Formalising Multi-layer Corpora in OWL DL ?
Lexicon Modelling, Querying and Consistency Control
Aljoscha Burchardt1, Sebastian Pad?2?, Dennis Spohr3?, Anette Frank4?and Ulrich Heid3
1Dept. of Comp. Ling. 2Dept. of Linguistics 3Inst. for NLP 4Dept. of Comp. Ling.
Saarland University Stanford University University of Stuttgart University of Heidelberg
Saarbr?cken, Germany Stanford, CA Stuttgart, Germany Heidelberg, Germany
albu@coli.uni-sb.de pado@stanford.edu spohrds,heid@ims.uni-stuttgart.de frank@cl.uni-heidelberg.de
Abstract
We present a general approach to formally
modelling corpora with multi-layered anno-
tation, thereby inducing a lexicon model in a
typed logical representation language, OWL
DL. This model can be interpreted as a graph
structure that offers flexible querying func-
tionality beyond current XML-based query
languages and powerful methods for consis-
tency control. We illustrate our approach by
applying it to the syntactically and semanti-
cally annotated SALSA/TIGER corpus.
1 Introduction
Over the years, much effort has gone into the creation
of large corpora with multiple layers of linguistic an-
notation, such as morphology, syntax, semantics, and
discourse structure. Such corpora offer the possibility
to empirically investigate the interactions between
different levels of linguistic analysis.
Currently, the most common use of such corpora
is the acquisition of statistical models that make use
of the ?more shallow? levels to predict the ?deeper?
levels of annotation (Gildea and Jurafsky, 2002; Milt-
sakaki et al, 2005). While these models fill an im-
portant need for practical applications, they fall short
of the general task of lexicon modelling, i.e., creat-
ing an abstracted and compact representation of the
corpus information that lends itself to ?linguistically
informed? usages such as human interpretation or
integration with other knowledge sources (e.g., deep
grammar resources or ontologies). In practice, this
task faces three major problems:
?At the time of writing, Sebastian Pad? and Dennis Spohr
were affiliated with Saarland University, and Anette Frank with
DFKI Saarbr?cken and Saarland University.
Ensuring consistency. Annotation reliability and
consistency are key prerequisites for the extraction of
generalised linguistic knowledge. However, with the
increasing complexity of annotations for ?deeper? (in
particular, semantic) linguistic analysis, it becomes
more difficult to ensure that all annotation instances
are consistent with the annotation scheme.
Querying multiple layers of linguistic annotation.
A recent survey (Lai and Bird, 2004) found that cur-
rently available XML-based corpus query tools sup-
port queries operating on multiple linguistic levels
only in very restricted ways. Particularly problematic
are intersecting hierarchies, i.e., tree-shaped analyses
on multiple linguistic levels.
Abstractions and application interfaces. A per-
vasive problem in annotation is granularity: The gran-
ularity offered by a given annotation layer may di-
verge considerably from the granularity that is needed
for the integration of corpus-derived data in large
symbolic processing architectures or general lexical
resources. This problem is multiplied when more
than one layer of annotation is considered, for exam-
ple in the characterisation of interface phenomena.
While it may be possible to obtain coarser-grained
representations procedurally by collapsing categories,
such procedures are not flexibly configurable.
Figure 1 illustrates these difficulties with a sentence
from the SALSA/TIGER corpus (Burchardt et al,
2006), a manually annotated German newspaper cor-
pus which contains role-semantic analyses in the
FrameNet paradigm (Fillmore et al, 2003) on top
of syntactic structure (Brants et al, 2002).1 The se-
1While FrameNet was originally developed for English, the
majority of frames has been found to generalise well to other
389
which the official Croatia but in significant international-law difficulties bring would
Figure 1: Multi-layer annotation of a German phrase with syntax and frame semantics (?which would bring
official Croatia into significant difficulties with international law?)
mantic structure consists of frames, semantic classes
assigned to predicating expressions, and the semantic
roles introduced by these classes. The verb bringen
(?to bring?) is used metaphorically and is thus analy-
sed as introducing one frame for the ?literal? reading
(PLACING) and one for the ?understood? reading
(CAUSATION), both with their own role sets.
The high complexity of the semantic structure even
on its own shows the necessity of a device for con-
sistency checking. In conjunction with syntax, it
presents exactly the case of intersecting hierarchies
which is difficult to query. With respect to the issue of
abstraction, note that semantic roles are realised vari-
ously as individual words (was (?which?) ) and con-
stituents (NPs, PPs), a well-known problem in deriv-
ing syntax-semantics mappings from corpora (Frank,
2004; Babko-Malaya et al, 2006).
Our proposal. We propose that the problems in-
troduced above can be addressed by formalising cor-
pora in an integrated, multi-layered corpus and lexi-
con model in a declarative logical framework, more
specifically, the description logics-based OWL DL
formalism. The major benefits of this approach are
that all relevant properties of the annotation and the
underlying model are captured in a uniform represen-
tation and, moreover, that the formal semantics of the
model makes it possible to use general and efficient
knowledge representation techniques for consistency
control. Finally, we can extract specific subsets from
a corpus by defining task-specific views on the graph.
After a short discussion of related approaches in
languages (Burchardt et al, 2006; Boas, 2005).
Section 2, Section 3 provides details on our method-
ology. Sections 4 and 5 demonstrate the benefits of
our strategy on a model of the SALSA/TIGER data.
Section 6 concludes.
2 Related Work
One recent approach to lexical resource modelling
is the Lexical Systems framework (Polgu?re, 2006),
which aims at providing a highly general represen-
tation for arbitrary kinds of lexica. While this is
desirable from a representational point of view, the
resulting models are arguably too generic to support
strong consistency checks on the encoded data.
A further proposal is the currently evolving Lex-
ical Markup Framework (LMF; Francopoulo et
al. (2006)), an ISO standard for lexical resource mod-
elling, and an LMF version of FrameNet exists. How-
ever, we believe that our usage of a typed formalism
takes advantage of a strong logical foundation and
the notions of inheritance and entailment (cf. Schef-
fczyk et al (2006)) and is a crucial step beyond the
representational means provided by LMF.
Finally, the closest neighbour to our proposal is
the ATLAS project (Laprun et al, 2002), which
combines annotations with a descriptive meta-model.
However, to our knowledge, ATLAS only models
basic consistency constraints, and does not capture
dependencies between different layers of annotation.
390
3 Modelling Multilevel Corpora in OWL DL
3.1 A formal graph-based Lexicon
This section demonstrates how OWL DL, a strongly
typed representation language, can serve to transpar-
ently formalise corpora with multi-level annotation.
OWL DL is a logical language that combines the
expressivity of OWL2 with the favourable computa-
tional properties of Description Logics (DL), notably
decidability and monotonicity (Baader et al, 2003).
The strongly typed, well-defined model-theoretic se-
mantics distinguishes OWL DL from recent alterna-
tive approaches to lexicon modelling.
Due to the fact that OWL DL has been defined
in the Resource Description Framework (RDF3), the
first central benefit of using OWL DL is the possibil-
ity to conceive of the lexicon as a graph ? a net-like
entity with a high degree of interaction between lay-
ers of linguistic description, with an associated class
hierarchy. Although OWL DL itself does not have a
graph model but a model-theoretic semantics based
on First Order Logic, we will illustrate our ideas with
reference to a graph-like representation, since this is
what we obtain by transforming our OWL DL files
into an RDFS database.
Each node in the graph instantiates one or more
classes that determine the properties of the node. In
a straightforward sense, properties correspond to la-
belled edges between nodes. They are, however, also
represented as nodes in the graph which instantiate
(meta-)classes themselves.
The model is kept compact by OWL?s support for
multiple instantiation, i.e., the ability of instances
to realise more than one class. For example, in a
syntactically and semantically annotated corpus, all
syntactic units (constituents, words, or even parts
of words) can instantiate ? in addition to a syntac-
tic class ? one or more semantic classes. Multiple
instantiation enables the representation of informa-
tion about several annotation layers within single
instances.
As we have argued in Section 2, we believe that
having one generic model that can represent all cor-
pora is problematic. Instead, we propose to construct
lexicon models for specific types of corpora. The
2http://www.w3.org/2004/OWL/
3http://www.w3.org/RDF/
design of such models faces two central design ques-
tions: (a) Which properties of the annotated instances
should be represented?; (b) How are different types
of these annotation properties modelled in the graph?
Implicit features in annotations. Linguistic anno-
tation guidelines often concentrate on specifying the
linguistic data categories to be annotated. However,
a lot of linguistically relevant information often re-
mains implicit in the annotation scheme. Examples
from the SALSA corpus include, e.g., the fact that
the annotation in Figure 1 is metaphorical. This in-
formation has to be inferred from the configuration
that one predicate evokes two frames. As such infor-
mation about different annotation types is useful in
final lexicon resources, e.g. to define clean generali-
sations over the data (singling out ?special cases?), to
extract information about special data categories, and
to define formally grounded consistency constraints,
we include it in the lexicon model.
Form of representation. All relevant information
has to be represented either as assertional statements
in the model graph (i.e., nodes connected by edges),
or as definitional axioms in the class hierarchy.4
This decision involves a fundamental trade-off be-
tween expressivity and flexibility. Modelling features
as axioms in the class hierarchy imposes definitional
constraints on all instances of these classes and is
arguably more attractive from a cognitive perspec-
tive. However, modelling features as entities in the
graph leads to a smaller class hierarchy, increased
querying flexibility, and more robustness in the face
of variation and noise in the data.
3.2 Modelling SALSA/TIGER Data
We now illustrate these decisions concretely by de-
signing a model for a corpus with syntactic and
frame-semantic annotation, more concretely the
SALSA/TIGER corpus. However, the general points
we make are valid beyond this particular setting.
As concerns implicit annotation features, we have
designed a hierarchy of annotation types which now
explicitly expresses different classes of annotation
phenomena and which allows for the definition of
annotation class-specific properties. For example,
frame targets are marked as a multi-word target if
4This choice corresponds to the DL distinction between TBox
(?intensional knowledge?) and ABox (?extensional knowledge?).
391
L
in
gu
is
ti
c
m
od
el
? Frames
w Intentionally_affect
w Placing
w Motion, . . .
? Roles
w Intentionally_affect.Act
w Placing.Means
? TIGER edge labels and POS
w SB, OA, PPER, ADJA, . . .
? Generalised functions and categories
w subj, obj, NounP, AdjP, . . .
A
nn
ot
at
io
n
ty
pe
s
? Frame Annotations
w Simple
w Metaphoric
w Underspecified
? Role Annotations
w Simple
w Underspecified
? Target Annotations
w Single-word targets
w Multi-word targets
? Sentences, syntactic units, . . .
Figure 2: Schema of the OWL DL model?s class hierarchy (?TBox?)
their span contains at least two terminal nodes. The
hierarchy is shown on the right of Figure 2, which
shows parts of the bipartite class hierarchy.
The left-hand side of Figure 2 illustrates the lin-
guistic model, in which frames and roles are organ-
ised according to FrameNet?s inheritance relation.
Although this design seems to be straightforward, it
is the result of careful considerations concerning the
second design decision. Since FrameNet is a hierar-
chically structured resource with built-in inheritance
relations, one important question is whether to model
individual frames, such as SELF_MOTION or LEAD-
ERSHIP, and their relations either as instances of a
general class Frame and as links between these in-
stances, or as hierarchically structured classes with
richer axiomatisation. In line with our focus on con-
sistency checking, we adopt the latter option, which
allows us to use built-in reasoning mechanisms of
OWL DL to ensure consistency.
Annotation instances from the corpus instantiate
multiple classes in both hierarchies (cf. Figure 2): On
the annotation side according to their types of phe-
nomena; on the linguistic side based on their frames,
roles, syntactic functions, and categories.
Flexible abstraction. Section 1 introduced granu-
larity as a pervasive problem in the use of multi-level
corpora. Figure 2 indicates that the class hierarchy
of the OWL DL model offers a very elegant way
of defining generalised data categories that provide
abstractions over model classes, both for linguistic
categories and annotation types. Moreover, proper-
ties can be added to each abstracting class and then
be used, e.g., for consistency checking. In our case,
Figure 2 shows (functional) edge labels and part-of-
speech tags provided by TIGER, as well as sets of
(largely theory-neutral) grammatical functions and
categories that subsume these fine-grained categories
and support the extraction of generalised valence in-
formation from the lexicon.
An annotated corpus sentence. To substantiate
the above discussion, Figure 3 shows a partial lexicon
representation of the example in Figure 1. The boxes
represent instance nodes, with classes listed above
the horizontal line, and datatype properties below
it.5 The links between these instances indicate OWL
object properties which have been defined for the
instantiated classes. For example, the metaphorical
PLACING frame is shown as a grey box in the middle.
Multiple inheritance is indicated by instances
carrying more than one class, such as the in-
stance in the left centre, which instantiates the
classes SyntacticUnit, NP, OA, NounP and
obj. Multi-class instances inherit the properties
of each of these classes, so that e.g., the meta-
phoric frame annotation of the PLACING frame
in the middle has both the properties defined for
frames (hasCoreRole) and for frame annotations
(hasTarget). The generalised syntactic categories
discussed above are given in italics (e.g., NounP).
The figure highlights the model?s graph-based
structure with a high degree of interrelation between
the lexicon entities. For example, the grey PLAC-
ING frame instance is directly related to its roles
(left, bottom), its lexical anchor (right), the surround-
ing sentence (top), and a flag (top left) indicating
metaphorical use.
5For the sake of simplicity, we excluded explicit ?is-a? links.
392
MetaphoricFrameAnnotationUspFrameAnnotationCausation
SyntacticUnitPRELSSB
hasTigerIDhasContent
NounPsubj
SyntacticUnitNENKhasTigerIDhasContent s2910_17"Kroatien"
Source
SyntacticUnitNPOA
hasTigerIDhasContent
SimpleRoleAnnotationPlacing.ThemehasContent
UspFrameAnnotationSupport LemmahasLemma
LexicalUnitrdf:ID bringen.Placing
SingleWordTargethasContent
SyntacticUnitVVINFHDhasTigerIDhasContent
SyntacticUnitNNNKhasTigerIDhasContent
NounPobj
s2910_15
"das"
s2910_502
"das offizielle Kroatien"
s2910_14
"was"SyntacticUnitARTNKhasTigerIDhasContent
"bringen"
"bringen"
SimpleRoleAnnotationPlacing.CausehasContent "was"
SentenceAnnotationhasSentenceIDhasContent s2910"Die Ausrufung des ..."
MetaphoricFrameAnnotationPlacing
"das offizielle Kroatien"
SimpleRoleAnnotationPlacing.GoalhasContent "in betr?chtliche v?lker..."
"bringen"s2910_23
"Schwierigkeiten"s2910_22
consistsOf
isAssignedTo hasFlag
hasFrameAnnotation hasFrameAnnotation
isUspWith
hasFrameAnnotation
hasCoreRole
hasCoreRoleisAssignedTo
hasCoreRole
hasTargetisTargetOf isAssignedTo hasHead
hasAnnotation?Instance
hasReadingisReadingOf
isAnnotationInstanceOf
isAssignedToSyntacticUnitADJANKhasTigerIDhasContent s2910_16"offizielle"
consistsOf consistsOf
...
Figure 3: Partial lexicon representation of an annotated corpus sentence
4 Querying the Model
We now address the second desideratum introduced
in Section 1, namely a flexible and powerful query
mechanism. For OWL DL models, such a mecha-
nism is available in the form of the Sesame (Broekstra
et al, 2002) SeRQL query language. Since SeRQL
makes it possible to extract and view arbitrary sub-
graphs of the model, querying of intersective hierar-
chies is possible in an intuitive manner.
An interesting application for this querying mecha-
nism is to extract genuine lexicon views on the corpus
annotations, e.g., to extract syntax-semantics map-
ping information for particular senses of lemmas, by
correlating role assignments with deep syntactic in-
formation. These can serve both for inspection and
for interfacing the annotation data with deep gram-
matical resources or general lexica. Applied to our
complete corpus, this ?lexicon? contains on average
8.5 role sets per lemma, and 5.6 role sets per frame.
The result of such a query is illustrated in Table 1 for
the lemma senken (?to lower?).
From such view, frame- or lemma-specific role
sets, i.e., patterns of role-category-function assign-
ments can easily be retrieved. A typical example is
given in Table 2, with additional frequency counts.
The first row indicates that the AGENT role has been
realised as a (deep) subject noun phrase and the ITEM
as (deep) object noun phrase.
We found that generalisations over corpus cate-
gories encoded in the class hierarchies are central
Role Cat Func Freq
Item NounP obj 26
Agent NounP subj 15
Difference PrepP mod-um 6
Cause NounP subj 4
Value_2 PrepP mod-auf 3
Value_2 PrepP pobj-auf 2
Value_1 PrepP mod-von 1
Table 1: Role-category-function assignments for
senken / CAUSE_CHANGE_OF_SCALAR_POSITION (CCSP)
Role set for senken / CCSP Freq
Agent Item 11
subj obj
NounP NounP
Cause Item 4
subj obj
NounP NounP
Item 4
obj
NounP
Agent Item Difference 2
subj obj mod-um
NounP NounP PrepP
Table 2: Sample of role sets for senken / CCSP
to the usefulness of the resulting patterns. For ex-
ample, the number of unique mappings between se-
mantic roles and syntactic categories in our corpus
is 5,065 for specific corpus categories, and 2,289 for
abstracted categories. Thus, the definition of an ab-
straction layer, in conjunction with a flexible query
mechanism, allows us to induce lexical characterisa-
tions of the syntax-semantics mapping ? aggregated
393
and generalised from disparate corpus annotations.
Incremental refinements. Querying, and the re-
sulting lexical views, can serve yet another purpose:
Such aggregates make it possible to conduct a data-
driven search for linguistic generalisations which
might not be obvious from a theoretical perspective,
and allow quick inspection of the data for counterex-
amples to plausible regularities.
In the case of semantic roles, for example, such
a regularity would be that semantic roles are not
assigned to conflicting grammatical functions (e.g.,
deep subject and object) within a given lemma. How-
ever, some of the role sets we extracted contained
exactly such configurations. Further inspection re-
vealed that these irregularities resulted from either
noise introduced by errors in the automatic assign-
ment of grammatical functions, or instances with
syntactically non-local role assignments.
Starting from such observations, our approach sup-
ported a semi-automatic, incremental refinement of
the linguistic and annotation models, in this case in-
troducing a distinction between local and non-local
role realisations.
Size of the lexicon. Using a series of SeRQL
queries, we have computed the size of the cor-
pus/lexicon model for the SALSA/TIGER data (see
Table 3). The lexicon model architecture as described
in Section 3 results in a total of more than 304,000
instances in the lexicon, instantiating 581 different
frame classes and 1,494 role classes.
5 Consistency Control
The first problem pointed out in Section 1 was the
need for efficient consistency control mechanisms.
Our OWL DL-based model in fact offers two mech-
anisms for consistency checking: axiom-based and
query-based checking.
Axiom-based checking. Once some constraint has
been determined to be universally applicable, it can
be formulated in Description Logics in the form of
axiomatic expressions on the respective class in the
model. Although the general interpretation of these
axioms in DL is that they allow for inference of new
statements, they can still be used as a kind of well-
formedness ?constraint?. For example, if an individ-
ual is asserted as an instance of a particular class, the
Type No. of instances
Lemmas 523
Lemma-frame pairs (LUs) 1,176
Sentences 13,353
Syntactic units 223,302
Single-word targets 16,268
Multi-word targets 258
Frame annotations 16,526
Simple 14,700
Underspecified 995
Metaphoric 785
Elliptic 107
Role annotations 31,704
Simple 31,112
Underspecified 592
Table 3: Instance count based on the first SALSA
release
reasoner will detect an inconsistency if this instance
does not adhere to the axiomatic class definition. For
semantic role annotations, axioms can e.g. define the
admissible relations between a particular frame and
its roles. This is illustrated in the DL statements be-
low, which express that an instance of PLACING may
at most have the roles GOAL, PATH, etc.
Placing v ?.hasRole (Placing.Goal unionsq Placing.Path unionsq . . .)
Placing v ?.hasRole (Placing.Goal unionsq Placing.Path unionsq . . .)
Relations between roles can be formalised in a
similar way. An example is the excludes relation in
FrameNet, which prohibits the co-occurrence of roles
like CAUSE and AGENT of the PLACING frame. This
can be expressed by the following statement.
Placing v ?((?.hasRole Placing.Cause)u
(?.hasRole Placing.Agent))
The restrictions are used in checking the consistency
of the semantic annotation; violations of these con-
straints lead to inconsistencies that can be identified
by theorem provers. Although current state-of-the-art
reasoners do not yet scale to the size of entire cor-
pora, axiom-based checking still works well for our
data due to SALSA?s policy of dividing the original
TIGER corpus into separate subcorpora, each deal-
ing with one particular lemma (cf. Scheffczyk et al
(2006)).
394
Query-based checking. Due to the nature of our
graph representation, constraints can combine dif-
ferent types of information to control adherence to
annotation guidelines. Examples are the assignment
of the SUPPORTED role of support verb constructions,
which ought to be assigned to the maximal syntactic
constituent projected by the supported noun, or the
exclusion of reflexive pronouns from the span of the
target verb. However, the consistency of multi-level
annotation is often difficult to check: Not only are
some types of classification (e.g. assignment of se-
mantic classes) inherently difficult; the annotations
also need to be considered in context. For such cases,
axiom-based checking is too strict. In practice, it is
important that manual effort can be reduced by auto-
matically extracting subsets of ?suspicious? data for
inspection. This can be done using SeRQL queries
which ? in contrast to the general remarks on the
scalability of reasoners ? are processed and evaluated
very quickly on the entire annotated corpus data.
Example queries that we formulated examine sus-
picious configurations of annotation types, such as
target words evoking two or more frame annota-
tions which are neither marked as underspecified nor
tagged as a pair of (non-)literal metaphorical frame
annotations. Here, we identified 8 cases of omitted
annotation markup, namely 4 missing metaphor flags
and 4 omitted underspecification links.
On the semantic level, we extracted annotation
instances (in context) for metaphorical vs. non-
metaphorical readings, or frames that are involved
in underspecification in certain sentences, but not in
others. While the result sets thus obtained still re-
quire manual inspection, they clearly illustrate how
the detection of inconsistencies can be enhanced by
a declarative formalisation of the annotation scheme.
Another strategy could be to concentrate on frames
or lemmas exhibiting proportionally high variation
in annotation (Dickinson and Meurers, 2003).
6 Conclusion
In this paper, we have constructed a Description
Logics-based lexicon model directly from multi-layer
linguistic corpus annotations. We have shown how
such a model allows for explicit data modelling, and
for flexible and fine-grained definition of various de-
grees of abstractions over corpus annotations.
Furthermore, we have demonstrated that a pow-
erful logical formalisation which integrates an un-
derlying annotation scheme can be used to directly
control consistency of the annotations using general
KR techniques. It can also overcome limitations
of current XML-based search tools by supporting
queries which are able to connect multiple levels of
linguistic analysis. These queries can be used vari-
ously as an additional means of consistency control,
to derive quantitative tendencies from the data, to
extract lexicon views tailored to specific purposes,
and finally as a general tool for linguistic research.
Acknowledgements
This work has been partly funded by the German
Research Foundation DFG (grant PI 154/9-2). We
also thank the two anonymous reviewers for their
valuable comments and suggestions.
References
Franz Baader, Diego Calvanese, Deborah L. McGuinness,
Daniele Nardi, and Peter F. Patel-Schneider. 2003.
The Description Logic Handbook: Theory, Implemen-
tation and Applications. CUP.
Olga Babko-Malaya, Ann Bies, Ann Taylor, Szuting Yi,
Martha Palmer, Mitch Marcus, Seth Kulick, and Li-
bin Shen. 2006. Issues in Synchronizing the English
Treebank and PropBank. In Proceedings of the COL-
ING/ACL Workshop on Frontiers in Linguistically An-
notated Corpora, Sydney.
Hans C. Boas. 2005. Semantic frames as interlingual
representations for multilingual lexical databases. In-
ternational Journal of Lexicography, 18(4):445?478.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Jeen Broekstra, Arjohn Kampman, and Frank van Herme-
len. 2002. Sesame: A generic architecture for storing
and querying RDF and RDF Schema. In Proceedings
of the 1st ISWC, Sardinia.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pad?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th LREC,
Genoa.
Markus Dickinson and W. Detmar Meurers. 2003. De-
tecting errors in part-of-speech annotation. In Pro-
ceedings of the 10th EACL, Budapest.
395
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to FrameNet.
International Journal of Lexicography, 16:235?250.
Gil Francopoulo, Monte George, Nicoletta Calzolari,
Monica Monachini, Nuria Bel, Mandy Pet, and Clau-
dia Soria. 2006. LMF for multilingual, specialized
lexicons. In Proceedings of the 5th LREC, Genoa.
Anette Frank. 2004. Generalisations over corpus-
induced frame assignment rules. In Proceedings of the
LREC Workshop on Building Lexical Resources From
Semantically Annotated Corpora, Lisbon.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Catherine Lai and Steven Bird. 2004. Querying and up-
dating treebanks: A critical survey and requirements
analysis. In Proceedings of the Australasian Language
Technology Workshop, Sydney.
Christophe Laprun, Jonathan Fiscus, John Garofolo, and
Sylvain Pajot. 2002. Recent Improvements to the AT-
LAS Architecture. In Proceedings of HLT 2002, San
Diego.
Eleni Miltsakaki, Nikhil Dinesh, Rashmi Prasad, Ar-
avind Joshi, and Bonnie Webber. 2005. Exper-
iments on sense annotations and sense disambigua-
tion of discourse connectives. In Proceedings of the
Fourth Workshop on Treebanks and Linguistic Theo-
ries, Barcelona, Spain.
Alain Polgu?re. 2006. Structural properties of lexi-
cal systems: Monolingual and multilingual perspec-
tives. In Proceedings of the COLING/ACL Workshop
on Multilingual Language Resources and Interoper-
ability, Sydney.
Jan Scheffczyk, Collin F. Baker, and Srini Narayanan.
2006. Ontology-based reasoning about lexical re-
sources. In Proceedings of the 5th OntoLex, Genoa.
396
Projection-based Acquisition of a Temporal Labeller
Kathrin Spreyer?
Department of Linguistics
University of Potsdam
Germany
spreyer@uni-potsdam.de
Anette Frank
Dept. of Computational Linguistics
University of Heidelberg
Germany
frank@cl.uni-heidelberg.de
Abstract
We present a cross-lingual projection frame-
work for temporal annotations. Auto-
matically obtained TimeML annotations in
the English portion of a parallel corpus
are transferred to the German translation
along a word alignment. Direct projection
augmented with shallow heuristic knowl-
edge outperforms the uninformed baseline
by 6.64% F
1
-measure for events, and by
17.93% for time expressions. Subsequent
training of statistical classifiers on the (im-
perfect) projected annotations significantly
boosts precision by up to 31% to 83.95% and
89.52%, respectively.
1 Introduction
In recent years, supervised machine learning has be-
come the standard approach to obtain robust and
wide-coverage NLP tools. But manually annotated
training data is a scarce and expensive resource. An-
notation projection (Yarowsky and Ngai, 2001) aims
at overcoming this resource bottleneck by scaling
conceptually monolingual resources and tools to a
multilingual level: annotations in existing monolin-
gual corpora are transferred to a different language
along the word alignment to a parallel corpus.
In this paper, we present a projection framework
for temporal annotations. The TimeML specifica-
tion language (Pustejovsky et al, 2003a) defines an
annotation scheme for time expressions (timex for
? The first author was affiliated with Saarland University
(Saarbru?cken, Germany) at the time of writing.
John [met]
event
Mary [last night]
timex
.
John [traf]
event
Mary [gestern Abend]
timex
.
Figure 1: Annotation projection.
short) and events, and there are tools for the auto-
matic TimeML annotation of English text (Verha-
gen et al, 2005). Similar rule-based systems exist
for Spanish and Italian (Saquete et al, 2006). How-
ever, such resources are restricted to a handful of
languages.
We employ the existing TimeML labellers to an-
notate the English portion of a parallel corpus, and
automatically project the annotations to the word-
aligned German translation. Fig. 1 shows a simple
example. The English sentence contains an event
and a timex annotation. The event-denoting verb met
is aligned with the German traf, hence the latter also
receives the event tag. Likewise, the components of
the multi-word timex last night align with German
gestern and abend, respectively, and the timex tag is
transferred to the expression gestern abend.
Projection-based approaches to multilingual an-
notation have proven adequate in various domains,
including part-of-speech tagging (Yarowsky and
Ngai, 2001), NP-bracketing (Yarowsky et al, 2001),
dependency analysis (Hwa et al, 2005), and role se-
mantic analysis (Pado? and Lapata, 2006). To our
knowledge, the present proposal is the first to apply
projection algorithms to temporal annotations.
489
Cross-lingually projected information is typically
noisy, due to errors in the source annotations as
well as in the word alignment. Moreover, success-
ful projection relies on the direct correspondence
assumption (DCA, Hwa et al (2002)) which de-
mands that the annotations in the source text be
homomorphous with those in its (literal) transla-
tion. The DCA has been found to hold, to a sub-
stantial degree, for the above mentioned domains.
The results we report here show that it can also
be confirmed for temporal annotations in English
and German. Yet, we cannot preclude divergence
from translational correspondence; on the contrary,
it occurs routinely and to a certain extent systemat-
ically (Dorr, 1994). We employ two different tech-
niques to filter noise. Firstly, the projection process
is equipped with (partly language-specific) knowl-
edge for a principled account of typical alignment
errors and cross-language discrepancies in the reali-
sation of events and timexes (section 3.2). Secondly,
we apply aggressive data engineering techniques to
the noisy projections and use them to train statistical
classifiers which generalise beyond the noise (sec-
tion 5).
The paper is structured as follows. Section 2
gives an overview of the TimeML specification lan-
guage and compatible annotation tools. Section 3
presents our projection models for temporal annota-
tions, which are evaluated in section 4. Section 5
describes how we induce temporal labellers for Ger-
man from the projected annotations; section 6 con-
cludes.
2 Temporal Annotation
2.1 The TimeML Specification Language
The TimeML specification language (Pustejovsky
et al, 2003a)1 and annotation framework emerged
from the TERQAS workshop2 in the context of the
ARDA AQUAINT programme. The goal of the pro-
gramme is the development of question answering
(QA) systems which index content rather than plain
keywords. Semantic indexing based on the identifi-
cation of named entities in free text is an established
1A standardised version ISO-TimeML is in preparation, cf.
Schiffrin and Bunt (2006).
2See http://www.timeml.org/site/terqas/in
dex.html
method in QA and related applications. Recent years
have also seen advances in relation extraction, a vari-
ant of event identification, albeit restricted in terms
of coverage: the majority of systems addressing
the task use a pre-defined set of?typically domain-
specific?templates. In contrast, TimeML models
events in a domain-independent manner and pro-
vides principled definitions for various event classes.
Besides the identification of events, it addresses their
relative ordering and anchoring in time by integrat-
ing timexes in the annotation. The major contri-
bution of TimeML is the explicit representation of
dependencies (so-called links) between timexes and
events.
Unlike traditional accounts of events (e.g.,
Vendler (1967)), TimeML adopts a very broad
notion of eventualities as ?situations that happen
or occur? and ?states or circumstances in which
something obtains or holds true? (Pustejovsky et
al., 2003a); besides verbs, this definition includes
event nominals such as accident, and stative mod-
ifiers (prepared, on board). Events are annotated
with EVENT tags. TimeML postulates seven event
classes: REPORTING, PERCEPTION, ASPECTUAL, I-
ACTION, I-STATE, STATE, and OCCURRENCE. For
definitions of the individual classes, the reader is re-
ferred to Saur?? et al (2005b).
Explicit timexes are marked by the TIMEX3 tag.
It is modelled on the basis of Setzer?s (2001) TIMEX
tag and the TIDES TIMEX2 annotation (Ferro et al,
2005). Timexes are classified into four types: dates,
times, durations, and sets.
Events and timexes are interrelated by three kinds
of links: temporal, aspectual, and subordinating.
Here, we consider only subordinating links (slinks).
Slinks explicate event modalities, which are of cru-
cial importance when reasoning about the certainty
and factuality of propositions conveyed by event-
denoting expressions; they are thus directly rel-
evant to QA and information extraction applica-
tions. Slinks relate events in modal, factive, counter-
factive, evidential, negative evidential, or condi-
tional relationships, and can be triggered by lexical
or structural cues.
2.2 Automatic Labellers for English
The basis of any projection architecture are high-
quality annotations of the source (English) portion
490
e ? E temporal entity
l ? E ? E (subordination) link
ws ? Ws, wt ? Wt source/target words
al ? Al : Ws ? Wt word alignment
As ? as : E ? 2Ws source annotation
At ? at : projected target
(E ? As ? Al) ? 2Wt annotation
Table 1: Notational conventions.
of the parallel corpus. However, given that the pro-
jected annotations are to provide enough data for
training a target language labeller (section 5), man-
ual annotation is not an option. Instead, we use the
TARSQI tools for automatic TimeML annotation of
English text (Verhagen et al, 2005). They have been
modelled and evaluated on the basis of the Time-
Bank (Pustejovsky et al, 2003b), yet for the most
part rely on hand-crafted rules. To obtain a full tem-
poral annotation, the modules are combined in a cas-
cade. We are using the components for timex recog-
nition and normalisation (Mani and Wilson, 2000),
event extraction (Saur?? et al, 2005a), and identifica-
tion of modal contexts (Saur?? et al, 2006).3
3 Informed Projection
3.1 The Core Algorithm
Recall that TimeML represents temporal entities
with EVENT and TIMEX3 tags which are anchored
to words in the text. Slinks, on the other hand, are
not anchored in the text directly, but rather relate
temporal entities. The projection of links is there-
fore entirely determined by the projection of the en-
tities they are defined on (see Table 1 for the nota-
tion used throughout this paper): a link l = (e, e?)
in the source annotation as projects to the target an-
notation at iff both e and e? project to non-empty
sequences of words. The projection of the enti-
ties e, e? themselves, however, is a non-trivial task.
3TARSQI also comprises a component that introduces tem-
poral links (Mani et al, 2003); we are not using it here because
the output includes the entire tlink closure. Although Mani et al
(2006) use the links introduced by closure to boost the amount
of training data for a tlink classifier, this technique is not suit-
able for our learning task since the closure might easily propa-
gate errors in the automatic annotations.
a.. . . [ ws ]e . . . b. . . . [ ws ]e . . .
. . . [ wt ]e . . . . . . [ wtj wtj+1 ]e . . .
c. . . . [ wsi wsi+1 ]e . . .
. . . [ wtj wtj+1 wtj+2 ]e . . .
Figure 2: Projection scenarios: (a) single-word 1-to-
1, (b) single-word 1-to-many, (c) multi-word.
a. [ . . . ]e b. [ . . . ]e . . .[ . . . ]e?
wtj?2 wtj?1 wtj wtj+1 wt
Figure 3: Problematic projection scenarios: (a) non-
contiguous aligned span, (b) rivalling tags.
Given a temporal entity e covering a sequence as(e)
of tokens in the source annotation, the projection
model needs to determine the extent at(e, as, al) of
e in the target annotation, based on the word align-
ment al . Possible projection scenarios are depicted
in Fig. 2. In the simplest case (Fig. 2a), e spans a
single word ws which aligns with exactly one word
wt in the target sentence. In this case, the model
predicts e to project to wt. A single tagged word
with 1-to-many alignments (as in Fig. 2b) requires
a more thorough inspection of the aligned words. If
they form a contiguous sequence, e can be projected
onto the entire sequence as a multi-word unit. This
is problematic in a scenario such as the one shown in
Fig. 3a, where the aligned words do not form a con-
tiguous sequence. There are various strategies, de-
scribed in section 3.2, to deal with non-contiguous
cases. For the moment, we can adopt a conservative
approach which categorically blocks discontinuous
projections. Finally, Fig. 2c illustrates the projec-
tion of an entity spanning multiple words. Here, the
model composes the projection span of e from the
alignment contribution of each individual word ws
covered by e. Again, the final extent of the projected
entity is required to be contiguous.
With any of these scenarios, a problem arises
when two distinct entities e and e? in the source an-
491
1. project(as, al ):
2. at,C = ?
3. for each entity e defined by as:
4. at,C(e, as, al) =
SC
ws?as(e) proj(ws, e, as, al)
5. for each link l = (e, e?) defined over as:
6. if at,C(e, as, al) 6= ? and at,C(e?, as, al) 6= ?
7. then define l to hold for at,C
8. return at,C
where
proj(ws, e, as, al) = {wt ? Wt | (ws, wt) ? al ?
?e? ? as. e? 6= e ? wt 6? at,C(e?, as, al)}
and
[C
S =
?
S
S :
S
S is convex
? : otherwise
Figure 4: The projection algorithm.
notation have conflicting projection extents, that is,
when at(e, as, al) ? at(e?, as, al ) 6= ?. This is il-
lustrated in Fig. 3b. The easiest strategy to resolve
conflicts like these is to pick an arbitrary entity and
privilege it for projection to the target word(s) wt in
question. All other rivalling entities e? project onto
their remaining target words at(e?, as, al) \ {wt}.
Pseudocode for this word-based projection of
temporal annotations is provided in Fig. 4.
3.2 Incorporating Additional Knowledge
The projection model described so far is extremely
susceptible to errors in the word alignment. Re-
lated efforts (Hwa et al, 2005; Pado? and Lapata,
2006) have already suggested that additional lin-
guistic information can have considerable impact on
the quality of the projected annotations. We there-
fore augment the baseline model with several shal-
low heuristics encoding linguistic or else topologi-
cal constraints for the choice of words to project to.
Linguistically motivated filters refer to the part-of-
speech (POS) tags of words in the target language
sentence, whereas topological criteria investigate the
alignment topology.
Linguistic constraints. Following Pado? and La-
pata (2006), we implement a filter which discards
alignments to non-content words, for two reasons:
(i) alignment algorithms are known to perform
poorly on non-content words, and (ii) events as
well as timexes are necessarily content-bearing and
hence unlikely to be realised by non-content words.
This non-content (NC) filter is defined in terms of
POS tags and affects conjunctions, prepositions and
punctuation. In the context of temporal annotations,
we extend the scope of the filter such that it effec-
tively applies to all word classes that we deem un-
likely to occur as part of a temporal entity. There-
fore, the NC filter is actually defined stronger for
events than for timexes, in that it further blocks
projection of events to pronouns, whereas pronouns
may be part of a timex such as jeden Freitag ?ev-
ery Friday?. Moreover, events prohibit the projec-
tion to adverbs; this restriction is motivated by the
fact that events in English are frequently translated
in German as adverbials which lack an event read-
ing (cf. head switching translations like prefer to X
vs. German lieber X ?rather X?). We also devise an
unknown word filter: it applies to words for which
no lemma could be identified in the preprocessing
stage. Projection to unknown words is prohibited
unless the alignment is supported bidirectionally.
The strictness concerning unknown words is due to
the empirical observation that alignments which in-
volve such words are frequently incorrect.
In order to adhere to the TimeML specification, a
simple transformation ensures that articles and con-
tracted prepositions such as am ?on the? are included
in the extent of timexes. Another heuristics is de-
signed to remedy alignment errors involving auxil-
iary and modal verbs, which are not to be annotated
as events. If an event aligns to more than one word,
then this filter singles out the main verb or noun and
discards auxiliaries.
Topological constraints. In section 3.1, we de-
scribed a conservative projection principle which re-
jects the transfer of annotations to non-contiguous
sequences. That model sets an unnecessarily modest
upper bound on recall; but giving up the contiguity
requirement entirely is not sensible either, since it is
indeed highly unlikely for temporal entities to be re-
alised discontinuously in either source or target lan-
guage (noun phrase cohesion, Yarowsky and Ngai
(2001)). Based on these observations, we propose
two refined models which manipulate the projected
annotation span so as to ensure contiguity. One
492
model identifies and discards outlier alignments,
which actively violate contiguity; the other one adds
missing alignments, which form gaps. Technically,
both models establish convexity in non-convex sets.
Hence, we first have to come up with a backbone
model which is less restrictive than the baseline, so
that the convexation models will have a basis to op-
erate on. A possible backbone model at,0 is pro-
vided in (1).
(1) at,0(e, as, al) =
?
ws?as(e)
proj(ws, e, as, al )
This model simply gathers all words aligned with
any word covered by e in the source annotation, ir-
respective of contiguity in the resulting sequence of
words. Discarding outlier alignments is then for-
malised as a reduction of at,0?s output to (one of)
its greatest convex subset(s) (GCS). Let us call this
model at,GCS. In terms of a linear sequence of
words, at,GCS chooses the longest contiguous sub-
sequence. The GCS-model thus serves a filtering
purpose similar to the NC filter. However, whereas
the latter discards single alignment links on linguis-
tic grounds, the former is motivated by topological
properties of the alignment as a whole.
The second model, which fills gaps in the word
alignment, constructs the convex hull of at,0 (cf.
Pado? and Lapata (2005)). We will refer to this model
as at,CH. The example in (2) illustrates both models.
(2)
[ . . . ]e
?C : ?
GCS : {1, 2}
1 2 3 4 5 CH : {1, 2, 3, 4, 5}
Here, entity e aligns to the non-contiguous token
sequence [1, 2, 5], or equivalently, the non-convex
set {1, 2, 5}(= at,0(e)). The conservative base-
line at,C rejects the projection altogether, whereas
at,GCS projects to the tokens 1 and 2. The additional
padding introduced by the convex hull (at,CH) fur-
ther extends the projected extent to {1, 2, 3, 4, 5}.
Alignment selection. Although bi-alignments are
known to exhibit high precision (Koehn et al, 2003),
in the face of sparse annotations we use unidirec-
tional alignments as a fallback, as has been proposed
in the context of phrase-based machine translation
(Koehn et al, 2003; Tillmann, 2003). Furthermore,
we follow Hwa et al (2005) in imposing a limit on
the maximum number of words that a single word
may align to.
4 Experiments
Our evaluation setup consists of experiments con-
ducted on the English-German portion of the Eu-
roparl corpus (Koehn, 2005); specifically, we work
with the preprocessed and word-aligned version
used in Pado? and Lapata (2006): the source-target
and target-source word alignments were automati-
cally established by GIZA++ (Och and Ney, 2003),
and their intersection achieves a precision of 98.6%
and a recall of 52.9% (Pado?, 2007). The preprocess-
ing consisted of automatic POS tagging and lemma-
tisation.
To assess the quality of the TimeML projec-
tions, we put aside and manually annotated a de-
velopment set of 101 and a test set of 236 bi-
sentences.4 All remaining data (approx. 960K bi-
sentences) was used for training (section 5). We
report the weighted macro average over all possi-
ble subclasses of timexes/events, and consider only
exact matches. The TARSQI annotations exhibit
an F
1
-measure of 80.56% (timex), 84.64% (events),
and 43.32% (slinks) when evaluated against the En-
glish gold standard.
In order to assess the usefulness of the linguis-
tic and topological parameters presented in section
3.2, we determined the best performing combination
of parameters on the development set. Not surpris-
ingly, event and timex models benefit from the var-
ious heuristics to different degrees. While the pro-
jection of events can benefit from the NC filter, the
projection of timexes is rather hampered by it. In-
stead, it exploits the flexibility of the GCS convexa-
tion model together with a conservative limit of 2 on
per-word alignments. In the underlying data sample
of 101 sentences, the English-to-German alignment
direction appears to be most accurate for timexes.
Table 2 shows the results of evaluating the optimised
models on the test set, along with the baseline from
section 3.1 and a ?full? model which activates all
4The unconventional balance of test and development data is
due to the fact that a large portion of the annotated data became
available only after the parameter estimation phase.
493
events slinks time expressions
model prec recall F prec recall F prec recall F
timex-optimised 48.53 33.73 39.80 30.09 10.71 15.80 71.01 52.76 60.54
event-optimised 50.94 44.23 47.34 30.96 14.29 19.55 56.55 42.52 48.54
combined 50.98 44.36 47.44 30.96 14.29 19.55 71.75 52.76 60.80
baseline 52.26 33.46 40.80 26.98 10.71 15.34 49.53 37.80 42.87
full 51.10 40.42 45.14 29.95 13.57 18.68 73.74 54.33 62.56
Table 2: Performance of projection models over test data.
[. . .] must today decide [. . .]: [. . .] (108723)
[. . .] hat heute u?ber
1
[. . .] zu entscheiden, na?mlich u?ber
2
[. . .]
APPR VVINF APPR
Figure 5: Amending alignment errors.
heuristics. The results confirm our initial assump-
tion that linguistic and topological knowledge does
indeed improve the quality of the projected annota-
tions. The model which combines the optimal set-
tings for timexes and events outperforms the un-
informed baseline by 17.93% (timexes) and 6.64%
(events) F
1
-measure. However, exploration of the
model space on the basis of the (larger and thus pre-
sumably more representative) test set shows that the
optimised models do not generalise well. The test
set-optimised model activates all linguistic heuris-
tics, and employs at,CH convexation. For events,
projection considers bi-alignments with a fallback to
unidirectional alignments, preferably from English
to German; timex projection considers all alignment
links. This test set-optimised model, which we will
use to project the training instances for the maxi-
mum entropy classifier, achieves an F
1
-measure of
48.82% (53.15% precision) for events and 62.04%
(73.74% precision) for timexes.5
With these settings, our projection model is ca-
pable of repairing alignment errors, as shown in
Fig. 5, where the automatic word alignments are rep-
resented as arrows. The conservative baseline con-
sidering only bidirectional alignments discards all
5The model actually includes an additional strategy to ad-
just event and timex class labels on the basis of designated
FrameNet frames; the reader is referred to Spreyer (2007), ch.
4.5 for details.
event timex
data prec recall prec recall
all 53.15 45.14 73.74 53.54
best 75% 54.81 47.06 74.61 62.82
Table 3: Correlation between alignment probability
and projection quality.
alignments but the (incorrect) one to u?ber
1
. The op-
timised model, on the other hand, does not exclude
any alignments in the first place; the faulty align-
ments to u?ber
1
and u?ber
2
are discarded on linguistic
grounds by the NC filter, and only the correct align-
ment to entscheiden remains for projection.
5 Robust Induction
The projected annotations, although noisy, can be
exploited to train a temporal labeller for German.
As Yarowsky and Ngai (2001) demonstrate for POS
tagging, aggressive filtering techniques applied to
vast amounts of (potentially noisy) training data are
capable of distilling relatively high-quality data sets,
which may then serve as input to machine learn-
ing algorithms. Yarowsky and Ngai (2001) use the
Model-3 alignment score as an indicator for the
quality of (i) the alignment, and therefore (ii) the
projection. In the present study, discarding 25% of
the sentences based on this criterion leads to gains
in both recall and precision (Table 3). In accor-
dance with the TimeML definition, we further re-
strict training instances on the basis of POS tags by
basically re-applying the NC filter (section 3.2). But
even so, the proportion of positive and negative in-
stances remains heavily skewed?an issue which we
will address below by formulating a 2-phase classi-
494
prec recall F F
model event slink
1-step 83.48 32.58 46.87 17.01
1-step unk 83.88 32.19 46.53 16.87
2-step 83.95 34.44 48.84 19.06
2-step unk 84.21 34.30 48.75 19.06
timex
1-step 87.77 49.11 62.98
1-step unk 87.22 49.55 63.20
2-step 89.52 51.79 65.62
2-step unk 88.68 50.89 64.67
Table 4: Classifier performance over test data.
fication task.
The remaining instances6 are converted to feature
vectors encoding standard lexical and grammatical
features such as (lower case) lemma, POS, govern-
ing prepositions, verbal dependents, etc.7 For slink
instances, we further encode the syntactic subordi-
nation path (if any) between the two events.
We trained 4 classifiers,8 with and without
smoothing with artificial unknowns (Collins, 2003),
and as a 1-step versus a 2-step decision in which
instances are first discriminated by a binary classi-
fier, so that only positive instances are passed on to
be classified for a subclass. The performance of the
various classifiers is given in Table 4. Although the
overall F
1
-measure does not notably differ from that
achieved by direct projection, we observe a drastic
gain in precision, albeit at the cost of recall. With
almost 84% and 90% precision, this is an ideal start-
ing point for a bootstrapping procedure.
6 Discussion and Future Work
Clearly, the?essentially unsupervised?projection
framework presented here does not produce state-
of-the-art annotations. But it does provide an inex-
6Note that slink instances are constructed for event pairs, as
opposed to event and timex instances, which are constructed for
individual words.
7The grammatical features have been extracted from analy-
ses of the German ParGram LFG grammar (Rohrer and Forst,
2006).
8We used the opennlp.maxent package,
http://maxent.sourceforge.net/.
pensive and largely language-independent basis (a)
for manual correction, and (b) for bootstrapping al-
gorithms. In the future, we will investigate how
weakly supervised machine learning techniques like
co-training (Blum and Mitchell, 1998) could further
enhance projection, e.g. taking into account a third
language in a triangulation setting (Kay, 1997).
Acknowledgements
We would like to thank Sebastian Pado? for provid-
ing us with the aligned Europarl data, Inderjeet Mani
and Marc Verhagen for access to the TARSQI tools,
and James Pustejovsky for clarification of TimeML
issues. We would also like to thank the three anony-
mous reviewers for helpful comments.
References
Avrim Blum and Tom Mitchell. 1998. Combining La-
beled and Unlabeled Data with Co-Training. In Pro-
ceedings of the 1998 Conference on Computational
Learning Theory, pages 92?100, July.
Michael Collins. 2003. Head-Driven Statistical Mod-
els for Natural Language Parsing. Computational Lin-
guistics, 29(4):589?637, December.
Bonnie J. Dorr. 1994. Machine Translation Divergences:
A Formal Description and Proposed Solution. Com-
putational Linguistics, 20(4):597?635.
Lisa Ferro, Laurie Gerber, Inderjeet Mani, Beth Sund-
heim, and George Wilson, 2005. TIDES 2005 Stan-
dard for the Annotation of Temporal Expressions,
September.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan
Kolak. 2002. Evaluating Translational Correspon-
dence using Annotation Projection. In Proceedings of
ACL-2002, Philadelphia, PA.
R. Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas,
and Okan Kolak. 2005. Bootstrapping Parsers via
Syntactic Projection across Parallel Texts. Natural
Language Engineering, 11(3):311?325.
Martin Kay. 1997. The Proper Place of Men and Ma-
chines in Language Translation. Machine Translation,
12(1-2):3?23.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of HLT/NAACL 2003, pages 127?133.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of the
MT Summit 2005.
495
Inderjeet Mani and George Wilson. 2000. Robust Tem-
poral Processing of News. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL-2000), pages 69?76, Hong Kong.
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.
2003. Inferring Temporal Ordering of Events in News.
In Proceedings of the Human Language Technology
Conference (HLT-NAACL-2003). Short paper.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine
Learning of Temporal Relations. In Proceedings of
ACL/COLING 2006, pages 753?760, Sydney, Aus-
tralia.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Sebastian Pado? and Mirella Lapata. 2005. Cross-lingual
projection of role-semantic information. In Proceed-
ings of HLT/EMNLP 2005, Vancouver, BC.
Sebastian Pado? and Mirella. Lapata. 2006. Optimal con-
stituent alignment with edge covers for semantic pro-
jection. In Proceedings of ACL-COLING 2006, Syd-
ney, Australia.
Sebastian Pado?. 2007. Cross-Lingual Annotation Pro-
jection Models for Role-Semantic Information. Ph.D.
thesis, Saarland University, Saarbru?cken, Germany.
James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2003a. TimeML: Robust Specification of Event
and Temporal Expressions in Text. In Proceedings
of the Fifth International Workshop on Computational
Semantics.
James Pustejovsky, Patrick Hanks, Roser Saur??, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro, and
Marcia Lazo. 2003b. The TimeBank Corpus. In Pro-
ceedings of Corpus Linguistics, pages 647?656.
Christian Rohrer and Martin Forst. 2006. Improving
coverage and parsing quality of a large-scale LFG for
German. In Proceedings of LREC 2006, pages 2206?
2211, Genoa, Italy, May.
Estela Saquete, Patricio Mart??nez-Barco, Rafael Mun?oz,
Matteo Negri, Manuela Speranza, and Rachele Sprug-
noli. 2006. Multilingual Extension of a Temporal
Expression Normalizer using Annotated Corpora. In
Proceedings of the EACL 2006 Workshop on Cross-
Language Knowledge Induction, Trento, Italy, April.
Roser Saur??, Robert Knippen, Marc Verhagen, and
James Pustejovsky. 2005a. Evita: A Robust Event
Recognizer For QA Systems. In Proceedings of
HLT/EMNLP 2005, pages 700?707.
Roser Saur??, Jessica Littman, Bob Knippen, Robert
Gaizauskas, Andrea Setzer, and James Pustejovsky,
2005b. TimeML Annotation Guidelines Version 1.2.1,
October.
Roser Saur??, Marc Verhagen, and James Pustejovsky.
2006. SlinkET: A Partial Modal Parser for Events. In
Proceedings of LREC-2006, Genova, Italy, May. To
appear.
Amanda Schiffrin and Harry Bunt. 2006. Defining a
preliminary set of interoperable semantic descriptors.
Technical Report D4.2, INRIA-Loria, Nancy, France,
August.
Andrea Setzer. 2001. Temporal Information in Newswire
Articles: an Annotation Scheme and Corpus Study.
Ph.D. thesis, University of Sheffield, Sheffield, UK.
Kathrin Spreyer. 2007. Projecting Temporal Annotations
Across Languages. Diploma thesis, Saarland Univer-
sity, Saarbru?cken, Germany.
Christoph Tillmann. 2003. A Projection Extension Algo-
rithm for Statistical Machine Translation. In Michael
Collins and Mark Steedman, editors, Proceedings of
the 2003 Conference on Empirical Methods in Natural
Language Processing (EMNLP-2003), pages 1?8.
Zeno Vendler, 1967. Linguistics in Philosophy, chapter
Verbs and Times, pages 97?121. Cornell University
Press, Ithaca, NY.
Marc Verhagen, Inderjeet Mani, Roser Sauri, Robert
Knippen, Jessica Littman, and James Pustejovsky.
2005. Automating Temporal Annotation with
TARSQI. In Proceedings of the ACL-2005.
David Yarowsky and Grace Ngai. 2001. Inducing Mul-
tilingual POS Taggers and NP Bracketers via Robust
Projection across Aligned Corpora. In Proceedings of
NAACL-2001, pages 200?207.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing Multilingual Text Analysis Tools via
Robust Projection across Aligned Corpora. In Pro-
ceedings of HLT 2001, First International Conference
on Human Language Technology Research.
496
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 37?40,
Suntec, Singapore, 3 August 2009. c?2009 ACL and AFNLP
A NLG-based Application for Walking Directions 
 
 
Michael Roth and Anette Frank 
Department of Computational Linguistics 
Heidelberg University 
69120 Heidelberg, Germany 
{mroth,frank}@cl.uni-heidelberg.de 
 
  
 
Abstract 
This work describes an online application 
that uses Natural Language Generation 
(NLG) methods to generate walking di-
rections in combination with dynamic 2D 
visualisation. We make use of third party 
resources, which provide for a given 
query (geographic) routes and landmarks 
along the way. We present a statistical 
model that can be used for generating 
natural language directions. This model 
is trained on a corpus of walking direc-
tions annotated with POS, grammatical 
information, frame-semantics and mark-
up for temporal structure. 
1 Introduction 
The purpose of route directions is to inform a 
person, who is typically not familiar with his cur-
rent environment, of how to get to a designated 
goal. Generating such directions poses difficul-
ties on various conceptual levels such as the 
planning of the route, the selection of landmarks 
along the way (i.e. easily recognizable buildings 
or structures) and generating the actual instruc-
tions of how to navigate along the route using the 
selected landmarks as reference points. 
As pointed out by Tom & Denis (2003), the 
use of landmarks in route directions allows for 
more effective way-finding than directions rely-
ing solely on street names and distance measures. 
An experiment performed in Tom & Denis? work 
also showed that people tend to use landmarks 
rather than street names when producing route 
directions themselves.  
The application presented here is an early re-
search prototype that takes a data-driven genera-
tion approach, making use of annotated corpora 
collected in a way-finding study. In contrast to 
previously developed NLG systems in this area 
(e.g. Dale et. al, 2002), one of our key features is 
the integration of a number of online resources to 
compute routes and to find salient landmarks. 
The information acquired from these resources 
can then be used to generate natural directions 
that are both easier to memorise and easier to 
follow than directions given by a classic route 
planner or navigation system. 
The remainder of this paper is structured as 
follows: In Section 2 we introduce our system 
and describe the resources and their integration 
in the architecture. Section 3 describes our cor-
pus-based generation approach, with Section 4 
outlining our integration of text generation and 
visualisation. Finally, Section 5 gives a short 
conclusion and discusses future work. 
2 Combining Resources  
The route planner used in our system is provided 
by the Google Maps API1. Given a route com-
puted in Google Maps, our system queries a 
number of online resources to determine land-
marks that are adjacent to this route. At the time 
of writing, these resources are: OpenStreetMaps2 
for public transportation, the Wikipedia WikiPro-
ject Geographical coordinates3 for salient build-
ings, statues and other objects, Google AJAX 
Search API4 for ?yellow pages landmarks? such 
as hotels and restaurants, and Wikimapia 5  for 
squares and other prominent places.  
All of the above mentioned resources can be 
queried for landmarks either by a single GPS 
                                                 
1 http://code.google.com/apis/maps/  
2 http://www.openstreetmap.org 
3 http://en.wikipedia.org/wiki/Wikipedia:WikiProject 
Geographical_coordinates 
4 http://code.google.com/apis/ajaxsearch 
5 http://www.wikimapia.org  
37
coordinate (using the LocalSearch method in 
Google AJAX Search and web tools in Wikipe-
dia) or an area of GPS coordinates (using URL 
based queries in Wikimapia and OpenStreet-
Maps). The following list describes the data for-
mats returned by the respective services and how 
they were integrated: 
? Wikimapia and OpenStreetMaps ? Both 
resources return landmarks in the queried 
area as an XML file that specifies GPS 
coordinates and additional information. 
The XML files are parsed using a Java-
Script implementation of a SAX parser. 
The coordinates and names of landmarks 
are then used to add objects within the 
Google Maps API. 
? Wikipedia ? In order to integrate land-
marks from Wikipedia, we make use of a 
community created tool called search-a-
place 6 , which returns landmarks from 
Wikipedia in a given radius of a GPS 
coordinate. The results are returned in an 
HTML table that is converted to an XML 
file similar to the output of Wikimapia. 
Both the query and the conversion are im-
plemented in a Yahoo! Pipe7 that can be 
accessed in JavaScript via its URL. 
? Google AJAX Search ? The results re-
turned by the Google AJAX Search API 
are JavaScript objects that can be directly 
inserted in the visualisation using the 
Google Maps API.   
3 Using Corpora for Generation 
A data-driven generation approach achieves a 
number of advantages over traditional ap-
proaches for our scenario. First of all, corpus 
data can be used to learn directly how certain 
events are typically expressed in natural lan-
guage, thus avoiding the need of manually speci-
fying linguistic realisations. Secondly, variations 
of discourse structures found in naturally given 
directions can be learned and reproduced to 
avoid monotonous descriptions in the generation 
part. Last but not least, a corpus with good cov-
erage can help us determine the correct selection 
restrictions on verbs and nouns occurring in di-
rections. The price to pay for these advantages is 
                                                 
6 http://toolserver.org/~kolossos/wp-
world/umkreis.php  
7 http://pipes.yahoo.com/pipes/pipe.info?_id=BBI0x8
G73RGbWzKnBR50VA  
the cost of annotation; however we believe that 
this is a reasonable trade-off, in view of the fact 
that a small annotated corpus and reasonable 
generalizations in data modelling will likely 
yield enough information for the intended navi-
gation applications. 
3.1 Data Collection 
We currently use the data set from (Marciniak & 
Strube, 2005) to learn linguistic expressions for 
our generation approach. The data is annotated 
on the following levels: 
? Token and POS level 
? Grammatical level (including annotations 
of main verbs, arguments and connectives) 
? Frame-semantics level (including semantic 
roles and frame annotations in the sense of 
(Fillmore, 1977)) 
? Temporal level (including temporal rela-
tions between discourse units) 
3.2 Our Generation Approach 
At the time of writing, our system only makes 
use of the first three annotation levels. The lexi-
cal selection is inspired by the work of Ratna-
parkhi (2000) with the overall process designed 
as follows: given a certain situation on a route, 
our generation component receives the respective 
frame name and a list of semantic role filling 
landmarks as input (cf. Section 4). The genera-
tion component then determines a list of poten-
tial lexical items to express this frame using the 
relative frequencies of verbs annotated as evok-
ing the particular frame with the respective set of 
semantic roles (examples in Table 1). 
 
SELF_MOTION 
PATH 
17% walk, 13% follow, 10% 
cross, 7% continue, 6% take, ? 
GOAL 
18% get, 18% enter, 9% con-
tinue, 7% head, 5% reach, ? 
SOURCE 14% leave, 14% start, ? 
DIRECTION 
25% continue, 13% make,  
13% walk, 6% go, 3% take, ? 
DISTANCE 15% continue, 8% go, ? 
PATH + GOAL 29% continue, 14% take, ? 
DISTANCE + 
GOAL 
100% walk 
DIRECTION + 
PATH 
23% continue, 23% walk,  
8% take, 6% turn, 6% face, ? 
Table 1: Probabilities of lexical items for the frame 
SELF_MOTION and different frame elements 
38
For frame-evoking elements and each associated 
semantic role-filler in the situation, the gram-
matical knowledge learned from the annotation 
level determines how these parts can be put to-
gether in order to generate a full sentence (cf. 
Table 2). 
 
SELF_MOTION 
walk +  
[building PATH] 
walk ? walk + PP 
PP ? along + NP  
NP ? the + building 
get +  
[building GOAL] 
get ? get + to + NP 
NP ? the + building 
take +  
[left DIRECTION] 
take ? take + NP 
NP ? a + left 
Table 2: Examples of phrase structures for the frame 
SELF_MOTION and different semantic role fillers 
4 Combining Text and Visualisation 
As mentioned in the previous section, our model 
is able to compute single instructions at crucial 
points of a route. At the time of writing the ac-
tual integration of this component consists of a 
set of hardcoded rules that map route segments to 
frames, and landmarks within the segment to role 
fillers of the considered frame. The rules are 
specified as follows: 
? A turning point given by the Google Maps 
API is mapped to the SELF_MOTION frame 
with the actual direction as the semantic 
role direction. If there is a landmark adja-
cent to the turning point, it is added to the 
frame as the role filler of the role source. 
? If a landmark is adjacent or within the 
starting point of the route, it will be 
mapped to the SELF_MOTION frame with 
the landmark filling the semantic role 
source. 
? If a landmark is adjacent or within the 
goal of a route, it will be mapped to the 
SELF_MOTION frame with the landmark 
filling the semantic role goal. 
? If a landmark is adjacent to a route or a 
route segment is within a landmark, the 
respective segment will be mapped to the 
SELF_MOTION frame with the landmark 
filling the semantic role path. 
5 Conclusions and Outlook 
We have presented the technical details of an 
early research prototype that uses NLG methods 
to generate walking directions for routes com-
puted by an online route planner. We outlined 
the advantages of a data-driven generation ap-
proach over traditional rule-based approaches 
and implemented a first-version application, 
which can be used as an initial prototype exten-
sible for further research and development. 
Our next goal in developing this system is to 
enhance the generation component with an inte-
grated model based on machine learning tech-
niques that will also account for discourse level 
phenomena typically found in natural language 
directions. We further intend to replace the cur-
rent hard-coded set of mapping rules with an 
automatically induced mapping that aligns 
physical routes and landmarks with the semantic 
representations. The application is planned to be 
used in web experiments to acquire further data 
for alignment and to study specific effects in the 
generation of walking instructions in a multimo-
dal setting.  
The prototype system described above will be 
made publicly available at the time of publica-
tion. 
Acknowledgements 
This work is supported by the DFG-financed in-
novation fund FRONTIER as part of the Excel-
lence Initiative at Heidelberg University (ZUK 
49/1).  
References 
Dale, R., Geldof, S., & Prost, J.-P. (2002). Generating 
more natural route descriptions. Proceedings of the 
2002 Australasian Natural Language Processing 
Workshop. Canberra, Australia. 
Fillmore, C. (1977). The need for a frame semantics 
in linguistics. Methods in Linguistics , 12, 2-29. 
Marciniak, T., & Strube, M. (2005). Using an 
annotated corpus as a knowledge source for 
language generation. Proceedings of the Workshop 
on Using Corpora for Natural Language 
Generation, (pp. 19-24). Birmingham, UK. 
Ratnaparkhi, A. (2000). Trainable Methods for 
Surface Natural Language Generation. Proceedings 
of the 6th Applied Natural Language Processing 
Conference. Seattle, WA, USA. 
Tom, A., & Denis, M. (2003). Referring to landmark 
or street information in route directions: What 
difference does it make? In W. Kuhn, M. Worboys, 
& S. Timpf (Eds.), Spatial Information Theory (pp. 
384-397). Berlin: Springer. 
39
 Figure 1: Visualised route from Rohrbacher Stra?e 6 to Hauptstrasse 22, Heidelberg. Left: GoogleMaps 
directions; Right: GoogleMaps visualisation enriched with landmarks and directions generated by our system 
(The directions were manually inserted here as they are actually presented step-by-step following the route) 
Script Outline 
Our demonstration is outlined as follows: At first 
we will have a look at the textual outputs of 
standard route planners and discuss at which 
points the respective instructions could be im-
proved in order to be better understandable or 
easier to follow. We will then give an overview 
of different types of landmarks and argue how 
their integration into route directions is a valu-
able step towards better and more natural instruc-
tions.  
Following the motivation of our work, we will 
present different online resources that provide 
landmarks of various sorts. We will look at the 
information provided by these resources, exam-
ine the respective input and output formats, and 
state how the formats are integrated into a com-
mon data representation in order to access the 
information within the presented application. 
Next, we will give a brief overview of the cor-
pus in use and point out which kinds of annota-
tions were available to train the statistical gen-
eration component. We will discuss which other 
annotation levels would be useful in this scenario 
and which disadvantages we see in the current 
corpus. Subsequently we outline our plans to 
acquire further data by collecting directions for 
routes computed via Google Maps, which would 
allow an easier alignment between the instruc-
tions and routes. 
Finally, we conclude the demonstration with a 
presentation of our system in action. During the 
presentation, the audience will be given the pos-
sibility to ask questions and propose routes for 
which we show our system?s computation and 
output (cf. Figure 1).  
System Requirements 
The system is currently developed as a web-
based application that can be viewed with any 
JavaScript supporting browser. A mid-end CPU 
is required to view the dynamic route presenta-
tion given by the application. Depending on the 
presentation mode, we can bring our own laptop 
so that the only requirements to the local organ-
isers would be a stable internet connection (ac-
cess to the resources mentioned in the system 
description is required) and presentation hard-
ware (projector or sufficiently large display). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
40
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 10?15,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Semantic Approach To Textual Entailment:
System Evaluation and Task Analysis
Aljoscha Burchardt, Nils Reiter, Stefan Thater
Dept. of Computational Linguistics
Saarland University
Saarbr?cken, Germany
 
		Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 72?76,
Suntec, Singapore, 6 August 2009.
c?2009 ACL and AFNLP
Creating an Annotated Corpus for Generating Walking Directions
Stephanie Schuldes
?
Michael Roth
?
?EML Research gGmbH
Heidelberg, Germany
http://www.eml-research.de/nlp
Anette Frank
?
Michael Strube
?
?Department of Computational Linguistics
University of Heidelberg, Germany
http://www.cl.uni-heidelberg.de
Abstract
This work describes first steps towards
building a system that synchronously gen-
erates multimodal (textual and visual)
route directions for pedestrians. We pur-
sue a corpus-based approach for building a
generation model that produces natural in-
structions in multiple languages. We con-
ducted an empirical study to collect ver-
bal route directions, and annotated the ac-
quired texts on different levels. Here we
describe the experimental setting and an
analysis of the collected data.
1 Introduction
Route directions guide a person unfamiliar with
the environment to their designated goal. We plan
to generate route instructions that are similar to
those given by humans by referring to landmarks
and by structuring the route in a way that it is easy
to memorize (Denis, 1997).
We develop a system for synchronously gen-
erating natural language route directions and 3D
scenes of a route. The core of the architecture
is a unified representation providing information
for both verbal and graphical output. The direct
correspondence between linguistic references and
shown objects facilitates the identification of the
visual scene in the real world and the choice of the
correct action while following the route. To cre-
ate a reusable system that is adaptable to different
navigational domains and languages, we use ma-
chine learning techniques to build a statistical gen-
eration model from annotated corpora. We report
on an empirical study to collect human-produced
walking directions to be used for statistical gener-
ation from underlying semantic structures. While
our scenario is ultimately multilingual, here we
give an analysis of the German dataset.
2 Related Work
The task of analyzing and generating cognitively
adequate route instructions has been addressed by
a number of authors (Taylor & Tversky, 1996;
Tappe, 2000; Habel, 2003; Richter, 2008; Vi-
ethen & Dale, 2008; Kelleher & Costello, 2009).
Marciniak & Strube (2005) showed that a system
for generating route directions can be successfully
trained on a small set of 75 route direction texts
(8418 tokens). In their approach directions are
represented in a graph, which encodes informa-
tion on various conceptual levels. While their ap-
proach is restricted to reproducing directions for
the learned graphs, we will generate directions for
a wide range of possible routes. Dale et al (2005)
developed a system that takes GIS data as input
and uses a pipeline architecture to generate verbal
route directions. In contrast to their approach, our
approach will be based on an integrated architec-
ture allowing for more interaction between the dif-
ferent stages of generation. The idea of combining
verbal directions with scenes from a virtual 3D en-
vironment has recently lead to a new framework
for evaluating NLG systems: The Challenge on
Generating Instructions in Virtual Environments
(GIVE) (Byron et al, 2009) is planned to become
a regular event for the NLG community.
3 Corpus Acquisition
For collecting naturally produced route instruc-
tions, we conducted a study with 29 native speak-
ers of German (66% female and 33% male). The
participants in our study were students from var-
ious fields aged between 20 and 34 years. We
designed two different settings: one on-site set-
ting, in which participants walked around in a real
world situation (specifically our university cam-
pus), and one desk-based setting, in which they
interacted with a web application. The former
was further divided into indoor and outdoor routes,
72
32
1
7
6
4
5
Figure 1: Example route from the indoor setting
(first task), leading from a room with photocopiers
(1) across an open space and downstairs (3) to a
students? union room (6), passing an information
board (4) and a coffee machine (5). A lecture room
(2) and a glass wall (7) are visible from the route.
while the latter was restricted to an outdoor sce-
nario. This design enables us to study possible
differences and commonalities between linguistic
realizations obtained for different environments as
well as different presentation modes.
For both scenarios, the task was to give written
directions to a person unfamiliar with the area as
to how to get to the destination the participants just
reached, taking the same route. First, participants
were led along a route to a given destination point
(on-site). Each participant was asked to give di-
rections for two routes inside buildings of the uni-
versity campus (e.g. from an office to a seminar
room, cf. Figure 1), and one outside route (e.g.
from the building entrance to a bus stop).
Second, participants were shown a web appli-
cation that guided them along a route by means of
a 2D animation (desk-based). Subjects were al-
lowed to use all information displayed by the web
application: named places, buildings, street and
bridge names, etc. (cf. Figure 2).
Setting GM CI CO Total
physical routes 9 6 3 18
directions 59 58 28 145
tokens 5353 4119 2674 12146
tokens/dir. (?) 91 71 96
Table 1: Number of routes, directions, and tokens
for the different settings. GM = Google Maps, CI
= Campus Indoor, CO = Campus Outdoor.
4 Corpus Annotation
The acquired texts were processed in several steps.
To ensure that all route directions consist of syn-
tactically and semantically correct sentences, we
Figure 2: Web application used in the second task.
Landmarks were introduced successively via pop-
ups as the animated walker encountered them.
manually corrected spelling mistakes, omissions
resulting in grammatical errors, and removed el-
liptical and unclear directions.
The preprocessed texts were annotated on the
following three levels:
pos lemma ? part-of-speech and lemma
syn dep ? dependency relations
sem frame ? frames and semantic roles
For the pos lemma and syn dep levels, we used
TreeTagger (Schmid, 1997) and XLE (Maxwell
& Kaplan, 1993). The corpus was parsed
with the German ParGram LFG grammar (Forst,
2007). The outputs were corrected manually
by two annotators. On the sem frame level an-
notation was carried out using the annotation
tool SALTO (Burchardt et al, 2006) and fol-
lowing the definiton of the FrameNet frames
SELF MOTION, PERCEPTION, BEING LOCATED
and LOCATIVE RELATION (Baker et al, 1998). In
terms of accuracy for unlabeled/labeled relations,
the annotation agreement was 78.88%/65.17% on
the syn dep level and 79.27%/68.39% for frames
and semantic roles.
5 Data Analysis
5.1 Corpus Statistics
We examined word frequencies with respect to the
experimental settings in order to determine simi-
larities and dissimilarities in lexical choice. Table
2 shows the three most frequent verbs and nouns
found in each corpus part.
The data reveals that the most frequent verbs are
typical among all settings. However, we found a
number of lower-frequency verbs that are rather
73
Top verbs (Campus) GM CI CO
gehen ?to walk? 11% 18% 14%
sein ?to be? 3.9% 8.2% 6.6%
stehen ?to stand? 0.0% 6.3% 5.3%
Top verbs (GM) GM CI CO
folgen ?to follow? 12% 2.9% 2.6%
gehen ?to walk? 11% 18% 14%
abbiegen ?to turn into? 9.0% 3.8% 8.9%
Top nouns (Campus) GM CI CO
T?ur ?door? 0.0% 12% 0.9%
Treppe ?stairs? 0.0% 8.3% 0.0%
Gang ?hallway? 0.0% 6.6% 0.0%
Top nouns (GM) GM CI CO
...stra?e ?. . . Street? 28% 0.0% 2.2%
Richtung ?direction? 3.5% 2.8% 2.6%
...platz ?. . . Square? 3.4% 0.0% 6.1%
Table 2: Relative frequency of the three most com-
mon verbs and nouns in both studies
scenario-specific. In many cases, the occurrence
or absence of a verb can be attributed to a verb?s
selectional restrictions. For example, some of
the verbs describing movements along streets (e.g.
folgen ?to follow?, abbiegen ?to turn into?) do not
occur within the indoor corpus whereas verbs de-
scribing ?3D movements? (e.g. durchqueren ?to
walk through?, hinuntergehen ?to walk down?) are
not mentioned with the Google Maps setting.
The most frequent nouns significantly differ be-
tween the indoor and outdoor settings. This corre-
lation does not come as a surprise, as most of the
mentioned objects cannot be found in all scenar-
ios. On the other hand, nouns that are common
to both indoor and outdoor scenarios can be di-
vided into two categories: Nouns denoting (1) ob-
jects that appear in both scenarios (e.g. Geb?aude
?building?) and (2) abstract concepts typical for
route directions in general, e.g. Richtung ?direc-
tion?, Nummer ?number?, Ziel ?goal?, and Start-
punkt ?starting point?.
5.2 Landmark Alignment
Landmark alignment serves the purpose of de-
tecting objects that are most frequently men-
tioned across directions, and how the same ob-
ject is referred to differently. We created a graph-
based representation of the landmarks mentioned
in each route instruction (single route representa-
tion, SRR) for use in two types of alignment. Fig-
ure 3 shows an example from the indoor study.
First, we created a combined graph for each phys-
ical route by merging the respective SRRs, taking
into account several criteria:
String matching of landmark names;
Semantic similarity using GermaNet (Lemnitzer
& Kunze, 2002), a lexical-semantic network
for German similar to WordNet;
Frequency of references across all directions;
Spatio-temporal proximity of references to the
same object;
Number of landmarks mentioned in a single di-
rection (i.e. length of the SRR).
The combined graphs show that there are strong
correspondences between the directions for the
same route. We also found that, in the campus
settings, there was a small number of frequently
used general objects and a large number of less
frequently used specific objects. This facilitates
merging and shows the importance of the objects
for people?s orientation, and at the same time sup-
ports our claim that other modalities are needed
to disambiguate references during navigation. For
generating informative referential expressions, the
combined graph needs to be refined so that object
properties are represented (Krahmer et al, 2003).
Second, we aligned the SRRs with the physical
route graph. Comparing the landmarks mentioned
in the campus settings revealed that, in 97.8% of
the cases, people adhere to the sequence in which
objects are encountered. Reversed order was only
found in special cases like distant objects.
5.3 Discourse Phenomena
We analyzed the use of anaphora, the temporal or-
der of instructions, and occurrences of prototypi-
cal event chains in the collected texts in order to
identify coherence-inducing elements.
Spatio-temporal adverbials: Most anaphors
mention intermediate goals on the route in order
to refer to the starting point of a new action (e.g.
da/hier ?here?, dort ?there?). This finding goes
hand in hand with the observation that the col-
lected route directions are typically structured in
a linear temporal order (cf. Table 3) as for ex-
ample indicated by the use of adverbs indicat-
ing temporal succession (e.g. jetzt ?now?, dann
?then? and danach ?afterwards?) and conjunctions
(e.g. bis ?until?, wenn ?when?). Interestingly, a re-
versed order can be found in a few cases, where
74
R?cken Raum Kopierer Treppe Treppe Brett Getr?nkeautomat
Treppe
H?rsaal
Treppe Kaffeeautomat T?r Fachschaft
Kopierer Treppe
H?rsaal
Brett S?ule
Glast?r
Getr?nkeautomat T?r
Fachschafts-
raum
Druckerraum Theoretikum 180-Grad-Kurve Fachschaftstafel
Glaswand Glaswand
Kopf
Medizin-
Fachschaft
Kopierzentrum R?cken Treppe Treppe Richtung
Glasfenster Glasfront
Eingang Fachschaft
3
2
1
7
64 5
Figure 3: Each line shows one SRR for the route in Figure 1. Correspondences are indicated by identical
node shapes, black dots substitute non-matched tokens. The bottom graph shows the physical route seen
as sequence of landmarks. Node size reflects the importance of the referred object as conveyed by SRRs.
Adverbs >
t
GM CI CO
dann ?then? 55 43 30
jetzt ?now? 4 7 5
danach ?afterwards? 12 5 3
Adverbs <
t
GM CI CO
vorher ?beforehand? 0 1 0
davor ?before? 1 0 2
Table 3: Frequencies of temporal adverbs indicat-
ing linear (>
t
) and reversed linear order (<
t
)
the following action or situation is not supposed
to take place (e.g. Gehen Sie vorher rechts ?be-
forehand turn right?).
Backward-looking event anaphors and refer-
ences to result states: We also found explicit
references to past events (e.g. Nach dem Durch-
queren ?after traversing?) and result states of
events, e.g. the adverbial phrase unten angekom-
men (here: ?downstairs?) was frequently used fol-
lowing an instruction to ?walk downstairs?.
6 Conclusions and Future Work
The lexical corpus analysis confirms our hypoth-
esis that there are strong commonalities in lexi-
cal choice for directions that persist across sce-
narios and presentation modes, with a small num-
ber of focused differences, and obvious domain-
dependent lexical differences regarding the nature
of objects in the respective scenarios. While our
current corpus data is rather broad, environment-
specific data can be extended quickly by setting up
web studies using 2D and 3D environments.
The alignment of the physical routes and ver-
bal instructions shows a clear tendency that linear
route structure is observed in verbal realization,
with only few exceptions. Since temporal order
is observed by default, temporal annotation can be
restricted to capture exceptional orderings, which
are recoverable from linguistic cues. The study of
discourse coherence effects yielded a number of
elements that will be given special attention in the
surface generation model. We observed a variety
of coherence-inducing elements that are generic
in nature and thus seem well-suited for a corpus-
based generation model. As other languages are
known to exhibit differences in verbal realization
of directions (von Stutterheim et al, 2002), we
have to extend our data collection in order to gen-
erate systematic linguistic variations from a single
underlying semantic structure for all languages.
The linguistic annotation levels of frames and
roles, syntactic dependencies, and basic word cat-
egories have been tested successfully with a sim-
ilar corpus (Roth & Frank, 2009). The next steps
will consist in the alignment of physical routes and
landmarks with semantic representations in an in-
tegrated generation architecture.
Acknowledgements: This work is supported by
the DFG-financed innovation fund FRONTIER as
part of the Excellence Initiative at Heidelberg Uni-
versity (ZUK 49/1) and partially funded by the
Klaus Tschira Foundation, Heidelberg, Germany.
We thank the participants in our study, our anno-
tators Tim Krones and Anna Schmidt, and student
assistants Jonathan Geiger and Carina Silberer.
75
References
Baker, Collin F., Charles J. Fillmore & John B. Lowe (1998).
The Berkeley FrameNet Project. In Proceedings of the
17th International Conference on Computational Linguis-
tics and 36th Annual Meeting of the Association for Com-
putational Linguistics, Montr?eal, Qu?ebec, Canada, 10?14
August 1998, pp. 86?90.
Burchardt, Aljoscha, Katrin Erk, Anette Frank, Andrea
Kowalski & Sebastian Pado (2006). SALTO: A versatile
multi-level annotation tool. In Proceedings of the 5th In-
ternational Conference on Language Resources and Eval-
uation, Genoa, Italy, 22?28 May 2006, pp. 517?520.
Byron, Donna, Alexander Koller, Kristina Striegnitz, Justine
Cassell, Robert Dale, Johanna Moore & Jon Oberlander
(2009). Report on the First NLG Challenge on Generating
Instructions in Virtual Environments (GIVE). In Proceed-
ings of the 12th European Workshop on Natural Language
Generation (ENLG 2009), Athens, Greece, 30-31 March
2009, pp. 165?173.
Dale, Robert, Sabine Geldof & Jean-Philippe Prost (2005).
Using natural language generation in automatic route de-
scription. Journal of Research and Practice in Information
Technology, 37(1):89?106.
Denis, Michel (1997). The description of routes: A cognitive
approach to the production of spatial discourse. Current
Psychology of Cognition, 16:409?458.
Forst, Martin (2007). Filling statistics with linguistics ?
Property design for the disambiguation of German LFG
parses. In Proceedings of the ACL 2007 Workshop on
Deep Linguistic Processing, Prague, Czech Republic, 28
June 2007, pp. 17?24.
Habel, Christopher (2003). Incremental generation of mul-
timodal route instructions. In Reva Freedman & Charles
Callaway (Eds.), Working Papers of the 2003 AAAI Spring
Symposium on Natural Language Generation in Spoken
and Written Dialogue, pp. 44?51. Menlo Park, California:
AAAI Press.
Kelleher, John D. & Fintan J. Costello (2009). Applying com-
putational models of spatial prepositions to visually situ-
ated dialog. Computational Linguistics, 35(2):271?306.
Krahmer, Emiel, Sebastiaan van Erk & Andr?e Verleg (2003).
Graph-based generation of referring expressions. Compu-
tational Linguistics, 29(1):53?72.
Lemnitzer, Lothar & Claudia Kunze (2002). GermaNet ? rep-
resentation, visualization, application. In Proceedings of
the 3rd International Conference on Language Resources
and Evaluation, Las Palmas, Canary Islands, Spain, 29?31
May 2002, pp. 1485?1491.
Marciniak, Tomacz & Michael Strube (2005). Beyond the
pipeline: Discrete optimization in NLP. In Proceedings of
the 9th Conference on Computational Natural Language
Learning, Ann Arbor, Mich., USA, 29?30 June 2005, pp.
136?145.
Maxwell, John T. & Ronald M. Kaplan (1993). The inter-
face between phrasal and functional constraints. Compu-
tational Linguistics, 19(4):571?590.
Richter, Kai-Florian (2008). Context-Specific Route Direc-
tions ? Generation of Cognitively Motivated Wayfinding
Instructions. Amsterdam: IOS Press.
Roth, Michael & Anette Frank (2009). A NLG-based appli-
cation for walking directions. In Companion Volume to
the Proceedings of the 47th Annual Meeting of the Associ-
ation for Computational Linguistics and the 4th Interna-
tional Joint Conference on Natural Language Processing
of the Asian Federation of Natural Language Processing,
Singapore, 2-7 August 2009. To appear.
Schmid, Helmut (1997). Probabilistic Part-of-Speech tagging
using decision trees. In Daniel Jones & Harold Somers
(Eds.), New Methods in Language Processing, pp. 154?
164. London, U.K.: UCL Press.
Tappe, Heike (2000). Perspektivenwahl in Beschreibun-
gen dynamischer und statischer Wegeskizzen. [Choice of
perspective in descriptions of dynamic and static sketch-
maps]. In Christopher Habel & Christiane v. Stutterheim
(Eds.), R?aumliche Konzepte und sprachliche Strukturen,
pp. 69?97. T?ubingen: Niemeyer.
Taylor, Holly & Barbara Tversky (1996). Perspective in
spatial descriptions. Journal of Memory and Language,
35:371?391.
Viethen, Jette & Robert Dale (2008). The use of spatial re-
lations in referring expression generation. In Proceedings
of the Fifth International Natural Language Generation
Conference, Salt Fork OH, USA, 12?14 June 2008, pp.
59?67.
von Stutterheim, Christiane, Ralf N?use & Jorge M. Serra
(2002). Crosslinguistic differences in the conceptuali-
sation of events. In Hilde Hasselg?ard, Stig Johansson,
Bergljot Behrens & Cathrine Fabricius-Hansen (Eds.), In-
formation Structure in a Cross-lingustic Perspective, pp.
179?198. Amsterdam: Rodopi.
76
  	
Constraint-based RMRS Construction from Shallow Grammars
Anette Frank
Language Technology Lab
German Research Center for Artificial Intelligence, DFKI GmbH
Stuhlsatzenhausweg 3, 66123 Saarbru?cken, Germany
Anette.Frank@dfki.de
Abstract
We present a constraint-based syntax-semantics
interface for the construction of RMRS (Robust
Minimal Recursion Semantics) representations
from shallow grammars. The architecture is de-
signed to allow modular interfaces to existing
shallow grammars of various depth ? ranging
from chunk grammars to context-free stochastic
grammars. We define modular semantics con-
struction principles in a typed feature structure
formalism that allow flexible adaptation to al-
ternative grammars and different languages.1
1 Introduction
Semantic formalisms such as MRS (Copestake et
al., 2003) provide elegant solutions for the treatment
of semantic ambiguities in terms of underspecifi-
cation ? most prominently scope. In recent work
Copestake (2003) has investigated a novel aspect
of underspecification in the design of semantic for-
malisms, which is concerned with the representation
of partial semantic information, as it might be ob-
tained from shallow, i.e. incomplete syntactic anal-
ysis. The main rationale for this type of underspeci-
fication is to ensure monotonicity, and thus upwards
compatibility of the output of shallow parsing with
semantic representations obtained from full syntac-
tic parsing. Thus, Copestake?s design of RMRS ?
Robust Minimal Recursion Semantics ? provides an
important contribution to a novel line of research to-
wards integration of shallow and deep NLP. While
previous accounts (Daum et al, 2003; Frank et al,
2003) focus on shallow-deep integration at the syn-
tactic level, Copestake aims at integration of shal-
low and deep NLP at the level of semantics.
In this paper we review the RMRS formalism de-
signed by Copestake (2003) and present an archi-
tecture for a principle-based syntax-semantics in-
terface for RMRS construction from shallow gram-
mars. We argue for a unification-based approach,
1The research reported here was conducted in the project
QUETAL, funded by the German Ministry for Education and
Research, BMBF, under grant no. 01 IW C02.
to account for (underspecified) argument binding
in languages with case-marking as opposed to
structural argument identification. The architec-
ture we propose is especially designed to support
flexible adaptation to different types of shallow
to intermediate-level syntactic grammars that may
serve as a basis for RMRS construction. A chal-
lenge for principle-based semantics construction
from shallow grammars is the flat and sometimes
non-compositional nature of the structures they typ-
ically produce. We present RMRS semantics con-
struction principles that can be applied to flat syn-
tactic structures with various degrees of partiality.
2 RMRS ? For Partial Semantic
Representation
Copestake (2003) presents a formalism for partial
semantic representation that is derived from MRS
semantics (Copestake et al, 2003). Robust Min-
imal Recursion Semantics is designed to support
novel forms of integrated shallow and deep NLP,
by accommodating semantic representations pro-
duced by NLP components of various degrees of
partiality and depth of analysis ? ranging from
PoS taggers and NE recognisers over chunk and
(non-)lexicalised context-free grammars to deep
grammars like HPSG with MRS output structures.
The potential of a variable-depth semantic anal-
ysis is most evident for applications with conflict-
ing requirements of robustness and accuracy. Given
a range of NLP components of different depths of
analysis that deliver compatible semantic represen-
tations, we can apply flexible integration methods:
apply voting techniques, or combine partial results
from shallow and deep systems (Copestake, 2003).
To allow intersection and monotonic enrichment
of the output representations from shallow systems
on one extreme of the scale with complete repre-
sentations of deep analysis on the other, the missing
specifications of the weakest system must be fac-
tored out from the most comprehensive deep repre-
sentations. In the RMRS formalism, this concerns
the following main aspects of semantic information:
Argument encoding. A ?Parsons style? notation
accommodates for partiality of shallow systems
wrt. argument identification. Instead of predicates
with fixed arity, e.g. l4:on(e? ,e,y), predicates and ar-
guments are represented as independent elementary
predications: on(l4,e?), ARG1(l4,e), ARG2(l4,y).
This accounts for uncertainty of argument identi-
fication in shallow grammars. Underspecification
wrt. the type of argument is modeled in terms of a
hierarchy over disjunctive argument types: ARG1 <
ARG12, ARG2 < ARG12, ARG12 < . . . < ARGn.
Variable naming and equalities. Constraints for
equality of variables in elementary predications are
to be added incrementally, to accommodate for
knowledge-poor systems like PoS taggers, where
the identity of referential variables of, e.g., adjec-
tives and nouns in potential NPs cannot be estab-
lished, or else chunkers, where the binding of argu-
ments to predicates is only partially established.
An example of corresponding MRS (1.a) and
RMRS (1.b) representations illustrate these differ-
ences, cf. Copestake (2003).
(1) Every fat cat sat on a mat
a. l0:every(x,h1,h2), l1:fat(x), l2:cat1(x),
l3:CONJ, l4:sit1(espast ,x), l14:on2(e? ,e,y),
l9:CONJ, l5:some(y,h6,h7), l6:table1(y),
qeq(h1,l3), qeq(h6,l6), in-g(l3,l1), in-g(l3,l2),
in-g(l9,l4), in-g(l9,l14)
b. l0:every(x0), RSTR(l0,h1), BODY(l0,h2),
l1:fat(x1), l2:cat1(x2), l3:CONJ,
l4:sit1(e3spast), ARG1(l4,x2), l14:on2(e4),
ARG1(l14,e3), ARG2(l14,x5), l9:CONJ,
l5:some(x5), RSTR(l5,h6), BODY(l5,h7),
l6:table1(x6), qeq(h1,l1), qeq(h6,l6), in-
g(l3,l1), in-g(l3,l2), in-g(l9,l4), in-g(l9,l14),
x0 = x1, x1 = x2, x5 = x6
3 RMRS from Shallow Grammars
We aim at a modular interface for RMRS construc-
tion that can be adapted to a wide range of exist-
ing shallow grammars such as off-the-shelf chunk
parsers or probabilistic (non-)lexicalised PCFGs.
Moreover, we aim at the construction of under-
specified, but maximally constrained (i.e., resolved)
RMRS representations from shallow grammars.
A unification-based account. Chunk-parsers and
PCFG parsers for sentential structure do in general
not provide functional information that can be used
for argument identification. While in languages
like English argument identification is to a large ex-
tent structurally determined, in other languages ar-
guments are (partially) identified by case marking.
In case-marking languages, morphological agree-
ment constraints can yield a high degree of com-
pletely disambiguated constituents. Morphological
disambiguation can thus achieve maximally con-
strained argument identification for shallow analy-
ses. We therefore propose a unification-based ap-
proach for RMRS construction, where agreement
constraints can perform morphological disambigua-
tion for partial (i.e. underspecified) argument identi-
fication. Moreover, by interfacing shallow analysis
with morphological processing we can infer impor-
tant semantic features for referential and event vari-
ables, such as PNG and TENSE information. Thus,
morphological processing is also beneficial for lan-
guages with structural argument identification.
A reparsing architecture. In order to realise a
modular interface to existing parsing systems, we
follow a reparsing approach: RMRS construction
takes as input the output structure of a shallow
parser. We index the nodes of the parse tree and
extract a set of rules and lexicon entries with cor-
responding node indices. Reparsing of the original
input string according to this set of rules determin-
istically replays the original parse. In the reparsing
process we apply RMRS construction principles.
Constraint-based RMRS construction. We define
constraint-based principles for RMRS construction
in a typed feature structure formalism. These con-
straints are applied to the input syntactic structures.
In the reparsing step the constraints are resolved, to
yield maximally specified RMRS representations.
The RMRS construction principles are defined
and processed in the SProUT processing platform
(Drozdzynski et al, 2004). The SProUT system
combines finite-state technology with unification-
based processing. It allows the definition of finite
state transduction rules that apply to (sequences of)
typed feature structures (TFS), as opposed to atomic
symbols. The left-hand side of a transduction rule
specifies a regular expression over TFS as a recog-
nition pattern; the right-hand side specifies the out-
put in terms of a typed feature structure. The sys-
tem has been extended to cascaded processing, such
that the output of a set of rule applications can pro-
vide the input for another set of rewrite rules. The
system allows several distinct rules to apply to the
same input substring, as long as the same (maxi-
mal) sequence of structures is matched by these dif-
ferent rules. The output structures defined by these
individual rules can be unified, by way of flexible
interpreter settings. These advanced configurations
allows us to state RMRS construction principles in
a modular way.
S1
NP11 VVFIN12 PP13
ART111 ADJA112 NN113 sa? APPR131 ART132 NN141
ein dicker Kater auf der Matte
Figure 1: Input syntactic tree: Ein dicker Kater sa?
auf der Matte ? A fat cat sat on the mat
phrase & [ID ?11?, CAT ?NP?, M-ID ?1?, M-CAT ?S?]
lex & [ID ?12?, CAT ?VVFIN?, M-ID ?1?, M-CAT ?S?]
phrase & [ID ?13?, CAT ?PP?, M-ID ?1?, M-CAT ?S?]
lex & [ID ?111?, CAT ?ART?, M-ID ?11?, M-CAT ?NP?]
lex & [ID ?112?, CAT ?ADJA?, M-ID ?11?, M-CAT ?NP?]
lex & [ID ?113?, CAT ?NN?, M-ID ?11?, M-CAT ?NP?]
lex & [ID ?131?, CAT ?APPR?, M-ID ?13?, M-CAT ?PP?]
lex & [ID ?132?, CAT ?ART?, M-ID ?13?, M-CAT ?PP?]
lex & [ID ?133?, CAT ?NN?, M-ID ?13?, M-CAT ?PP?]
Figure 2: TFS representations for lexical and
phrasal nodes (here for tree of Figure 1)
phrase :> synsem & [M-ID #1, M-CAT #mcat]+
?> phrase & [ID #1, CAT #mcat].
Figure 3: Reparsing rule
Cascaded Reparsing. We extract information
about phrase composition from the indexed input
parse trees. For each local subtree, we extract
the sequence of daughter nodes as TFS, recording
for each node its node identifier (ID) together with
the identifier (M-ID) and category (M-CAT) of its
mother node (cf. Figure 2). This implicitly en-
codes instructions for phrase composition that are
employed in the cascaded system to guide phrase
composition and concurrent semantics construction.
A general reparsing rule (cf. Figure 3) is applied
to an input sequence of TFS for lexical or phrasal
nodes and produces as output a TFS for the implic-
itly defined mother node. The rule specifies that
for all nodes in the matched input sequence, their
mother node identifier and category features (M-ID,
M-CAT) must be identical, and defines the output
(mother) node?s local identifier and category feature
(ID, CAT) by use of variable co-references (#var).
Since the system obeys a longest-match strategy,
the regular expression is constrained to apply to the
same constituents as in the original parse tree.
Cascaded reparsing first applies to the sequence
of leaf nodes. The output node sequence is enriched
with the phrase-building information from the origi-
nal parse tree, and is again input to the phrase build-
ing and semantics construction rules. Thus, we de-
fine a cyclic cascade, where the output of a cascade
is fed in as input to the same rules. The cycle termi-
nates when no phrase building rule could be applied
to the input, i.e. the root category has been derived.
agr :> lex & [M-ID #1]*
( lex & [M-ID #1, CAT ?NN?, MSYN [AGR #agr]]+
| lex & [M-ID #1, CAT ?ADJA?, MSYN [AGR #agr]]+
| lex & [M-ID #1, CAT ?ART?, MSYN [AGR #agr]]+ )
lex & [M-ID #1]*
?> phrase & [ID #1, MSYN [AGR #agr]].
Figure 4: Modular agreement projection rules
Morpho-syntactic disambiguation. Before rule
application, the SProUT system performs morpho-
logical lookup on the input words (Krieger and Xu,
2003). Morphological information is modeled in a
TFS hierarchy with disjunctive types to underspec-
ify ambiguities of inflectional features, e.g. case.
We define very general principles for morpho-
syntactic agreement, defining agreement between
daughter and mother constituents individually for
categories like determiner, adjective or noun (Figure
4). Since in our reparsing approach the constituents
are pre-defined, the agreement projection principles
can be stated independently for possible mother-
daughter relations, instead of specifying complex
precedence patterns for NPs. Defining morphologi-
cal agreement independently for possibly occurring
daughter constituents yields few and very general
(disjunctive) projection principles that can apply to
?unseen? constituent sequences.
The rule in Figure 4 again exploits the longest-
match strategy to constrain application to the pre-
defined constituents, by specifying coreferent M-ID
features for all nodes in the rule?s input sequence.
In reparsing, the (possibly disjunctive) morpho-
logical types in the output structure of the individ-
ual rule applications are unified, yielding partially
resolved inflectional features for the mother node.
For NP11, e.g., we obtain CASE nom by unifica-
tion of nom (from ART and ADJA) and nom-acc-
dat (from NN). The resolved case value of the NP
can be used for (underspecified) argument binding
in RMRS construction.
4 Semantics Projection Principles for
Shallow Grammars
Lexical RMRS conditions. Lexical entries for
RMRS construction are constrained by types for
PoS classes, with class-specific elementary predi-
cations (EP) in RMRS.RELS, cf. Figure 5. RELS
and CONS are defined as set-valued features instead
of lists. This allows for modular content projec-
tion principles (see below). We distinguish differ-
ent types of EPs: ep-rel, defining relation and la-
bel, ep-rstr and ep-body for quantifiers, with LB and
RSTR/BODY features. Arguments are encoded as a
type ep-arg, which expands to disjunctive subtypes
ep-arg-1, ep-arg-12, ep-arg-23, . . . , ep-arg-n.
rmrs-nn & [CAT ?NN?, MSYN [AGR #agr],STEM <#stem>,
RMRS [KEY #1, BIND-ARG [AGR #agr ],
RELS {ep-rel &[LB #lb, REL #stem] ,
ep-arg0 & #1 & [LB #lb, ARG0 var]},
CONS { }]].
Figure 5: Lexical types with RMRS EPs
cont proj :> [M-ID #1]*
[M-ID #1, RMRS [RELS #rels, CONS #cons]]
[M-ID #1]*
?> [ID #1, RMRS [RELS #rels, CONS #cons]].
Figure 6: Content projection
Content projection. The content projection rule
(Figure 6) assembles the RMRS conditions in RELS
and CONS features of the daughter constituents. In
SProUT, the unification of output structures with
set-valued features is defined as set union. While
the classical list representation would require multi-
ple content rules for different numbers of daughters,
the set representation allows us to state a single con-
tent principle: it applies to each individual daughter,
and yields the union of the projected set elements as
the semantic value for the mother constituent.
Argument and variable binding. Management
features (KEY, BIND-ARG) propagate values of labels
and variables for argument binding. The maximally
specific type ep-arg-x of the arguments to be bound
is determined by special bind-arg principles that de-
fine morpho-syntactic constraints (case, passive).
For languages with structural argument identifica-
tion we can employ precedence constraints in the
regular expression part of argument binding rules.
Content projection from flat structures. A chal-
lenge for principle-based RMRS construction from
shallow grammars are their flat syntactic struc-
tures. They do not, in general, employ strictly bi-
nary structures as assumed in HPSG (Flickinger et
al., 2003). Constituents may also contain multiple
heads (cf. the PP in Fig. 1). Finally, chunk parsers
do not resolve phrasal attachment, thus providing
discontinuous constituents to be accounted for.
With flat, non-binary structures, we need to as-
semble EP (ep-arg-x) conditions for argument bind-
ing for each potential argument constituent of a
phrase. In the SRroUT system, this can again be
done without explicit list operations, by application
of individual argument binding rules that project
binding EP conditions for each potential argument
to the RELS feature of the mother. Thus, simi-
lar to Figure 6, we can state general and modular
mother-daughter principles for argument binding.
For multiple-headed constituents, such as flat PPs,
we use secondary KEY and BIND-ARG features. For
argument binding with chunk parsers, where PP at-
tachment is not resolved, we will generate in-group
conditions that account for possible attachments.
5 Comparison to Related Work
Compared to the RMRS construction method
Copestake (2003) applies to the English PCFG
parser of Carroll and Briscoe (2002), the main
features of our account are argument identifica-
tion via morphological disambiguation and defini-
tion of modular semantics construction principles
in a typed unification formalism. The architecture
we propose can be applied to sentence- or chunk-
parsing. The rule-based SProUT system allows the
definition of modular projection rules that can be
tailored to specific properties of an underlying shal-
low grammar (e.g. identification of active/passive
voice, of syntactic NP/PP heads). In future work we
will compare our semantics construction principles
to the general model of Copestake et al (2001).
Acknowledgements I am greatly indebted to my
colleagues at DFKI, especially the SProUT team
members Witold Droz?dz?yn?ski, Hans-Ulrich Krieger,
Jakub Piskorski and Ulrich Scha?fer, for their techni-
cal support and advice. Special thanks go to Kathrin
Spreyer for support in grammar development.
References
A. Copestake, A. Lascarides, and D. Flickinger. 2001.
An Algebra for Semantic Construction in Constraint-
based Grammars. In Proceedings of the ACL 2001,
Toulouse, France.
A. Copestake, D. Flickinger, I. Sag, and C. Pollard.
2003. Minimal Recursion Semantics. Ms.
A. Copestake. 2003. Report on the Design of RMRS.
Technical Report D1.1a, University of Cambridge,
University of Cambridge, UK., October. 23 pages.
M. Daum, K.A. Foth, and W. Menzel. 2003. Constraint-
based Integration of Deep and Shallow Parsing Tech-
niques. In Proceedings of EACL 2003, Budapest,
Hungary.
W. Drozdzynski, H.-U. Krieger, J. Piskorski, U. Scha?fer,
and F. Xu. 2004. Shallow processing with unification
and typed feature structures ? foundations and appli-
cations. Ku?nstliche Intelligenz, 1:17?23.
D. Flickinger, E. M. Bender, and S. Oepen. 2003. MRS
in the LinGO Grammar Matrix: A Practical User?s
Guide. Technical report, Deep Thought Project De-
liverable 3.5.
A. Frank, M. Becker, B. Crysmann, B. Kiefer, and
U. Scha?fer. 2003. Integrated Shallow and Deep Pars-
ing: ToPP meets HPSG. In Proceedings of the ACL
2003, pages 104?111, Sapporo, Japan.
H.-U. Krieger and F. Xu. 2003. A type-driven method
for compacting mmorph resources. In Proceedings of
RANLP 2003, pages 220?224.
The TIGER 700 RMRS Bank:
RMRS Construction from Dependencies
Kathrin Spreyer
Computational Linguistics Department
Saarland University
66041 Saarbru?cken, Germany
kathrins@coli.uni-sb.de
Anette Frank
Language Technology Lab
DFKI GmbH
66123 Saarbru?cken, Germany
frank@dfki.de
Abstract
We present a treebank conversion
method by which we construct an
RMRS bank for HPSG parser evalu-
ation from the TIGER Dependency
Bank. Our method effectively per-
forms automatic RMRS semantics
construction from functional depen-
dencies, following the semantic alge-
bra of Copestake et al (2001). We
present the semantics construction
mechanism, and focus on some spe-
cial phenomena. Automatic conver-
sion is followed by manual valida-
tion. First evaluation results yield
high precision of the automatic se-
mantics construction rules.
1 Introduction
Treebanks are under development for many
languages. They are successfully exploited for
the induction of treebank grammars, train-
ing of stochastic parsers, and for evaluat-
ing and benchmarking competitive parsing
and grammar models. While parser evalu-
ation against treebanks is most natural for
treebank-derived grammars, it is extremely
difficult for hand-crafted grammars that rep-
resent higher-level functional or semantic in-
formation, such as LFG or HPSG grammars.
In a recent joint initiative, the TIGER
project provides dependency-based treebank
representations for German, on the basis of
the TIGER treebank (Brants et al, 2002).
Forst (2003) applied treebank conversion
methods to the TIGER treebank, to derive
an f-structure bank for stochastic training and
evaluation of a German LFG parser. A more
theory-neutral dependency representation is
currently derived from this TIGER-LFG tree-
bank for cross-framework parser evaluation
(Forst et al, 2004). However, while Penn-
treebank style grammars and LFG analyses
are relatively close to dependency represen-
tations, the output of HPSG parsing is diffi-
cult to match against such structures. HPSG
analyses do not come with an explicit repre-
sentation of functional structure, but directly
encode semantic structures, in terms of (Ro-
bust) Minimal Recursion Semantics (hence-
forth (R)MRS.1 This leaves a gap to be
bridged in terms of the encoding of argu-
ments vs. adjuncts, the representation of spe-
cial constructions like relative clauses, and
not least, the representation of quantifiers and
their (underspecified) scoping relations.
In order to bridge this gap, we construct
an RMRS ?treebank? from a subset of the
TIGER Dependency Bank (Forst et al, 2004),
which can serve as a gold standard for HPSG
parsing for evaluation, and for training of
stochastic HPSG grammar models. In con-
trast to treebanks constructed from analyses
of hand-crafted grammars, our treebank con-
1RMRS (Copestake, 2003) is a formalism for par-
tial semantic representation that is derived from MRS
(Copestake et al, 2005). It is designed for the in-
tegration of semantic representations produced by
NLP components of different degrees of partiality and
depth, ranging from chunk parsers and PCFGs to deep
HPSG grammars with (R)MRS output.
1
version approach yields a standard for com-
parative parser evaluation where the upper
bound for coverage is defined by the corpus
(here, German newspaper text), not by the
grammar.
Our method for treebank conversion effec-
tively performs priniciple-based (R)MRS se-
mantics construction from LFG-based depen-
dency representations, which can be extended
to a general parsing architecture for (R)MRS
construction from LFG f-structures.
The remainder of this paper is organised
as follows. Section 2 introduces the input
dependency representations provided by the
TIGER Dependency Bank, and describes the
main features of the term rewriting machinery
we use for treebank conversion, i.e., RMRS se-
mantics construction from dependency struc-
tures. Section 3 presents the core of the se-
mantics construction process. We show how
to adapt the construction principles of the se-
mantic algebra of Copestake et al (2001) to
RMRS construction from dependencies in a
rewrite scenario, and discuss the treatment of
some special phenomena, such as verbal com-
plementation, coordination and modification.
Section 4 reports on the treebank construc-
tion methodology, with first results of quality
control. Section 5 concludes.
2 From TIGER Dependency Bank
to TIGER RMRS Bank
2.1 The TIGER Dependency Bank
The input to the treebank conversion pro-
cess consists of dependency representations
of the TIGER Dependency Bank (TIGER-
DB). The TIGER-DB has been derived semi-
automatically from (a subset of) the TIGER-
LFG Bank of Forst (2003), which is in turn
derived from the TIGER treebank. The de-
pendency format is similar to the Parc 700
Dependency Bank (King et al, 2003). It ab-
stracts away from constituency in order to re-
main as theory-neutral as possible. So-called
dependency triples are sets of two-place pred-
icates that encode grammatical relations, the
arguments representing the head of the depen-
dency and the dependent, respectively. The
sb(mu?ssen~0, Museum~1)
oc inf(mu?ssen~0, weichen~3)
mood(mu?ssen~0, ind)
tense(mu?ssen~0, pres)
mod(Museum~1, privat~1001)
cmpd lemma(Museum~1, Privatmuseum)
case(Museum~1, nom)
gend(Museum~1, neut)
num(Museum~1, sg)
sb(weichen~3, Museum~1)
Figure 1: TIGER dependency representation
of sentence #8595: Privatmuseum muss wei-
chen ? Private museum deemed to vanish.
triples further retain a number of morpholog-
ical features from the LFG representations,
such as agreement features or tense informa-
tion. Figure 1 displays an example.
For the purpose of RMRS construction, the
triples format has advantages and disadvan-
tages. The LFG-derived dependencies offer
all the advantages of a functional as opposed
to a constituent-based representation. This
representation already filters out the seman-
tically inappropriate status of auxiliaries as
heads; their contribution is encoded by fea-
tures such as perf or fut, which can be
directly translated into features of semantic
event variables. Most importantly, the triples
localise dependencies which are not locally re-
alised in phrase structure (as in long-distance
constructions), so that there is no need for
additional mechanisms to identify the argu-
ments of a governing predicate. Moreover,
the dependency representation format is to a
large extent uniform across languages, in con-
trast to phrase-structural encoding. There-
fore, the dependency-based semantics con-
struction mechanism can be quickly ported to
other languages.
The challenges we face mainly concern a
lack of specific types of phrase structure in-
formation that are crucial for RMRS compo-
sition. Linear precedence, e.g., plays a crucial
role when it comes to multiple modification or
coordination. Yet, it is possible to reconstruct
the surface order from the indices attached to
the Pred values in the triples. Part-of-speech
information, which is useful to trigger differ-
ent types of semantics construction rules, can
be induced from the presence or absence of
2
certain morphological features, yet to a lim-
ited extent.
For our current purpose of treebank con-
version, we are dependent on the specific in-
put format of the TIGER-DB, while in a more
general parsing context, one could ensure that
missing information of this type is included in
the input to semantics construction.
2.2 Treebank Conversion
Similar to the TIGER to TIGER-DB conver-
sion (Forst, 2003; Forst et al, 2004), we are
using the term rewriting system of Crouch
(2005) for treebank conversion. Originally de-
signed for machine translation, the system is a
powerful rewriting tool that has been applied
to other tasks, such as frame semantics con-
struction (Frank and Erk, 2004), or induction
of knowledge representations (Crouch, 2005).
The input to the system consists of a
set of facts in a prolog-like term representa-
tion. The rewrite rules refer to these facts in
the left-hand side (LHS), either conjunctively
(marked by ?,?) or disjunctively (marked by
?|?). Expressions on the LHS may be negated
(by prefix ?-?), thereby encoding negative con-
straints for matching. A rule applies if and
only if all facts specified on the LHS are satis-
fied by the input set of facts. The right-hand
side (RHS) of a rewrite rule defines a conjunc-
tion of facts which are added to the input set
of facts if the rule applies. The system further
allows the user to specify whether a matched
fact will be consumed (i. e., removed from the
set of facts) or whether it will be retained in
the output set of facts (marked by prefix ?+?).2
The system offers powerful rule encoding
facilities in terms of macros and templates.
Macros are parameterized patterns of (possi-
bly disjunctive) facts; templates are parame-
terized abstractions over entire (disjunctive)
rule applications. These abstraction means
help the user to define rules in a perspicious
and modular way, and significantly enhance
2The system additionally features optional rules
(??=>?), as opposed to deterministic rewriting (?==>?).
However, given that the input structures for RMRS
construction are disambiguated, and since our target
structures are underspecified semantic structures, we
can define the semantics deterministically.
the maintainability of complex rule sets.
The processing of rules is strictly ordered.
The rules are applied in the order of textual
appearance. Each rule is tested against the
current input set of facts and, if it matches,
produces an output set of facts that provides
the input to the next rule in sequence. Each
rule applies concurrently to all distinct sets of
matching facts, i.e. it performs parallel appli-
cation in case of alternative matching facts.
3 RMRS Construction from
Dependencies
Within the framework of HPSG, every lex-
ical item defines a complete (R)MRS struc-
ture. The semantic representation of a phrase
is defined as the assembly and combination
of the RMRSs of its daughters, according to
semantic constraints, which apply in parallel
with syntactic constraints. In each compo-
sition step, the RMRSs of the daughters are
combined according to semantic composition
rules that define the semantic representation
of the phrase, cf. (Copestake et al, 2005). Fol-
lowing the scaffolding of the syntactic struc-
ture in this way finally yields the semantics of
the sentence.
For the present task, the input to semantics
construction is a dependency structure. As
established by work on Glue Semantics (Dal-
rymple, 1999), semantics construction from
dependency structures can in similar ways
proceed recursively, to deliver a semantic pro-
jection of the sentence. Note, however, that
the resource-based approach of Glue Seman-
tics leads to alternative derivations in case of
scope ambiguities, whereas RMRS targets an
underspecified semantic representation.
For (R)MRS construction from dependen-
cies we follow the algebra for semantics com-
position in Copestake et al (2001). In HPSG
implementations of this algebra, composition
is triggered by phrasal configurations. Yet,
the algebra is neutral with regard to the syn-
tactic representation, and can be transposed
to composition on the basis of dependency re-
lations, much alike the Glue framework.
However, the rewriting system we are using
is not suited for a typical recursive application
3
scheme: the rules are strictly ordered, and
each rule simultaneously applies to all facts
that satisfy the constraints in the LHS. That
is, the RMRS composition cannot recursively
follow the composition of dependents in the
input structure. In section 3.2 we present a
design of RMRS that is suited for this con-
current application scheme. Before, we briefly
sketch the semantic algebra.
3.1 An Algebra for Semantic
Construction
Copestake et al (2001) define a semantic en-
tity as a 5-tuple ?s1, s2, s3, s4, s5? such that
s1 is a hook, s2 is a (possibly empty) set of
holes, s3 and s4 are bags of Elementary Pred-
ications (EPs) and handle constraints, respec-
tively, and s5 is a set of equalities holding be-
tween variables. The hook is understood to
represent the externalised part of the seman-
tic entity as a pair of a handle and an index
(a variable). It is used for reference in compo-
sition: Hooks of semantic arguments fill holes
(or slots) of functors. Holes, in turn, record
gaps in a semantic representation which re-
main to be filled. They, too, are pairs of a
handle and an index; furthermore, holes are
labelled with the grammatical function they
bear syntactically. That is, the labels on holes
serve two purposes: They help determine the
appropriate operation of composition (see be-
low), and they link the semantics to syntax.3
EPs (predicate applications) represent the
binding of argument variables to their predi-
cators. An EP h : r(a1, . . . , an, sa1, . . . , sam)
consists of the EP?s handle (or label) h, a
relation r, and a list of zero or more vari-
able arguments a1, . . . , an, followed by zero or
more scopal arguments sa1, . . . , sam (denot-
ing handles) of the relation. Finally, the bag
3Copestake et al (2001) mention a third feature to
be included in the hook as an externally visible vari-
able, which they instantiate with the index of the con-
trolled subject in equi constructions and which is also
used to implement the semantics of predicative modifi-
cation. However, this feature is not crucial given that
the underlying syntactic structures represent depen-
dencies rather than immediate dominance relations,
and therefore make non-local information available lo-
cally. Likewise, the dependency scenario does not ne-
cessitate that modifiers externalise their ARG1 argu-
ment position (see section 3.3.3).
of handle constraints (Hcons) contains con-
ditions which (partially) specify the relations
between scopal arguments and their scope, i.e.
between the scopal argument and the handles
that may fill the hole.
The operators of semantic composition
opl1 , . . . , oplk are drawn from ? ? ? ? ?,
where ? is the set of all semantic entities, and
l1, . . . , lk correspond to the labels on holes:
An operator opli defines the composition of
a semantic head which has a hole labelled li
with the argument filling that hole as follows:
The result of opli(a1, a2) is undefined if a2 has
no hole labelled li, otherwise:
1. hook(opli(a1, a2)) = hook(a2);
2. holesl?(opli(a1, a2)) = holesl?(a1) ?
holesl?(a2) for all labels l? 6= li;
3. eps(opli(a1, a2)) = eps(a1)
? eps(a2);
4. eqs(opli(a1, a2)) = Tr(eqs(a1)? eqs(a2)?
{hook(a1) = holeli(a2)}); where Tr
stands for the transitive closure.
3.2 RMRS Design
As mentioned earlier, the concurrent nature of
rule application makes it impossible to pro-
ceed recursively in a scaffolding way, inher-
ent to tree-based analyses, since the rules ap-
ply simultaneously to all structures. RMRS
construction is therefore designed around one
designated ?global? RMRS. Instead of pro-
jecting and accumulating RMRS constraints
step-wise by recursive composition, we di-
rectly insert the meaning descriptions into a
single global RMRS. Otherwise, composition
strictly follows the semantic operations of the
algebra of Copestake et al (2001): the compo-
sition rules only refer to the hook and slots of
functors and arguments, to achieve the bind-
ing of argument variables and the encoding of
scope constraints.
Global and Lexical RMRSs. The global
RMRS features a top handle (Top, usually
the label of the matrix proposition), sets of
EPs (Rels) and handle constraints (Hcons),
respectively, as described in the algebra, and
a set of Ing constraints.4
4Whenever two labels are related via an Ing (in-
group) constraint, they can be understood to be con-
4
+pred(X,Pred),-mo( ,X), -spec( ,X),
+?s::?(X,SemX), +hook(SemX,Hook)
==> lb(Hook,Lb), var(Hook,Var)
&& add ep(Lb,ep rel,rel,Pred)
&& add ep(Lb,ep arg0,arg0,Var).
lexical RMRS: [
Hook
[
Lb Lb
Var Var
]]
global RMRS: ?
?
?
?
?
?
Rels
?
?
?
. . .,
[
Pred n
Lb Lb
Arg0 Var
]
, . . .
?
?
?
Hcons
{
. . .
}
Ing
{
. . .
}
?
?
?
?
?
?
Figure 2: A rule for nominals (top) with re-
sulting lexical and global RMRS (bottom).
In addition, every predicate in the depen-
dency structure projects a lexical RMRS. Lex-
ical RMRSs are semantic entities which con-
sist of only a hook (i.e. a label and a variable),
that makes the entity available for reference
by subsequent (composition) rules, whereas
the basic semantic content (which is deter-
mined on the basis of the predicate?s category,
and comprises, at least, EPs for the relation
and the ARG0)5 is uniformly maintained in
the bags of the global RMRS, yet still an-
chored to the lexical hook labels and variables.
Figure 2 shows an example of a lexical
RMRS with its links to the global RMRS, and
a simplified version of the corresponding rule:
The rule applies to predicates, i.e. pred fea-
tures, with a value Pred. It introduces the
lexical RMRS, i.e., the hook?s label and vari-
able, and adds the predicate?s basic semantic
content to the global RMRS, here the relation
represented by Pred and the ARG0 variable,
which is co-referent with the hook?s variable.
Composition. The semantic composition
of arguments and functors is driven by the
predicate arg(Fctor,N,Arg), where N en-
codes the argument position, Fctor and Arg
are indices of functor and argument, respec-
joined. This is relevant, e.g., for intersective modifi-
cation, since a quantifier that outscopes the modified
noun must also take scope over the modifier.
5The category information required to define the
concrete basic semantics is not explicit in the depen-
dencies, but is induced from the grammatical function
borne by the predicate, as well as the presence or ab-
sence of certain morphological features (section 2.1).
+arg(X,2,Arg), +g f(Arg,?oc fin?),
+comp form(Arg,dass)
get lb(X,LbX), get lb(Arg,LbA)
==> sort(Lb,h), sort(LbP,h)
&& add ep(LbX,ep arg2,argx,LbP)
&& add ep(LbP,ep rel,rel,?prpstn m rel?)
&& add ep(LbP,ep arg0,arg0,Lb)
&& add qeq(Lb,LbA).
lexical RMRSs:
X:
[
Hook
[
Lb LbX
]
]
Arg:
[
Hook
[
Lb LbA
]
]
global RMRS:
?
?
?
?
?
Rels
?
?
?
..,
?
?
X v
Lb LbX
Arg2 LbP
?
?,
?
?
prpstn m rel
Lb LbP
Arg0 Lb
?
?,
[
Arg v
Lb LbA
]
,..
?
?
?
Hcons
{
. . ., Lb qeq LbA , . . .
}
?
?
?
?
?
Figure 3: Sample argument binding rule trig-
gered by arg(X,2,Arg) (top), referred lexical
RMRSs and resulting global RMRS (bottom).
tively.6 We interpret the arg-predicate as a
slot/hole of the functor, such that the binding
of the argument to the functor comes down
to filling the hole, in the sense of the alge-
bra described above: This is steered by the
previously defined hooks of the two semantic
entities, in that the matching rule introduces
an EP with an attribute ARGN that is an-
chored to the externalised label in the func-
tor?s hook. The value of the attribute ARGN
is the hook variable or hook label of the argu-
ment, depending on the category. A slightly
more complicated example is shown in Figure
3, it involves the introduction of an additional
proposition and a scope constraint. This rule
performs the composition of a declarative fi-
nite clausal object (oc fin) with its verbal
head. It assigns a proposition relation as the
value of the verb?s ARG2, which in turn has
an ARG0 that takes scope over the hook label
of the matrix verb in the object clause.
In general, composition does not depend on
the order of rule applications. That is, the
fact that the system performs concurrent rule
6The arg predicates are introduced by a set of
preprocessing rules which reconstruct the argument
structure by referring to the local grammatical func-
tions of a predicate and testing for (morphological)
features typically borne by non-arguments. E.g.,
pron type( ,expl) identifies an expletive pronoun.
5
??
?
?
?
?
?
?
Rels
?
?
?
?
?
?
?
?
?
. . .,
?
?
?
?
wissen v
Lb h
Arg0 e
Arg1 x
Arg2 1
?
?
?
?
,
?
?
?
?
versammeln v
Lb 1
Arg0 e
Arg1 2
Arg2 x
?
?
?
?
,
?
?
?
?
pronoun q rel
Lb h
Arg0 2
Rstr 3
Body h
?
?
?
?
,
[
pron null u
Lb 4
Arg0 2
]
, . . .
?
?
?
?
?
?
?
?
?
Hcons
{
. . ., 3 qeq 4 , . . .
}
?
?
?
?
?
?
?
?
Figure 4: Control analysis with unspecified coreference in [. . .], als so gut wie er kaum
ein anderer die Studentenmassen [. . .] zu versammeln wu?te. ? [. . .] when hardly anybody
knew how to rally the crowd of students [. . .] as well as he did. (from corpus sentence # 8074).
applications in a cascaded rule set is not prob-
lematic for semantics construction. Though,
we have to ensure that every partial structure
is assigned a hook, prior to the application of
composition rules. This is ensured by stating
the rules for lexical RMRSs first.
Scope constraints. By introducing handle
constraints, we define restrictions on the pos-
sible scoped readings. This is achieved by
gradually adding qeq relations to the global
Hcons set. Typically, this constraint relates
a handle argument of a scopal element, e.g. a
quantifier, and the label of the outscoped el-
ement. However, we cannot always fully pre-
dict the interaction among several scoping el-
ements. This is the case, inter alia, for the
modification of verbs by more than one scopal
adverb. This ambiguity is modeled by means
of a UDRT-style underspecification, that is,
we leave the scope among the modifiers un-
specified, but restrict each to outscope the
verb handle.7
3.3 Selected Phenomena
3.3.1 Verbal complements.
The treebank distinguishes three kinds of ver-
bal complements: infinitival phrases govered
by a raising verb or by a control verb, and
finite clausal arguments.
Infinitival complements. Raising verbs
do not assign an ARG1, and the infinitival ar-
gument is bound via an additional proposition
which fills the ARG2 position of the governor.
A handle constraint requires the proposition
7This is in accordance with the German HPSG
grammar of Crysmann (2003), and will also be
adapted in the ERG (p.c. D. Flickinger).
to take scope over the label of the infinitive.
Modal verbs lend themselves most naturally
to the same analysis, by virtue of identical
annotation in the dependency triples.
The implementation of RMRS for equi
constructions relies on external lexicon re-
sources, since the underlying dependency
structures do not encode the coreference be-
tween the controlled subject and the exter-
nal controller. Instead, the controlee is anno-
tated as a null pronoun. In order to differ-
entiate subject from object control, we enrich
the transfer input with a list of static facts
s_control(Pred) and o_control(Pred), re-
spectively, which we extracted from the Ger-
man HPSG grammar (Crysmann, 2003). The
rules refer to these facts, and establish the ap-
propriate bindings. If no information about
coreference is available (due to sparse lexical
data), the controlled subject appears in the
RMRS as an unbound pronoun, as assumed
in the syntactic structure. This is shown in
Fig. 4. In the manual correction phase, these
cases are corrected in the output RMRS, by
introducing the missing control relation.
Finite complements. For finite clausal
complements we assume the basic analysis il-
lustrated in section 3.2. But finite clauses are
not necessarily declarative, they can also have
interrogative meaning. In RMRS, this dis-
tinction is typically drawn in a type hierarchy,
of which we assume a simplified version:
message m rel
prop ques m rel imp m rel
prpstn m rel int m rel
German embedded clauses are usually marked
by one of the complementizers dass (that)
6
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Rels
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
. . .,
?
?
?
?
?
def q rel
Lb 10
Arg0 4
Rstr 7
Body h
?
?
?
?
?
,
[
Achtung n
Lb 1
Arg0 4
]
,
?
?
?
?
?
?
?
?
und c
Lb h
Arg0 x
L hndl 13
R hndl 16
L index 4
R index 17
?
?
?
?
?
?
?
?
,
?
?
?
?
?
?
?
?
implicit conj rel
Lb 16
Arg0 17
L hndl 14
R hndl 15
L index 5
R index 6
?
?
?
?
?
?
?
?
,
?
?
?
?
?
def q rel
Lb 11
Arg0 5
Rstr 8
Body h
?
?
?
?
?
,
[
Zuneigung n
Lb 2
Arg0 5
]
,
?
?
?
?
?
def q rel
Lb 12
Arg0 6
Rstr 9
Body h
?
?
?
?
?
,
[
Liebe n
Lb 3
Arg0 6
]
, . . .
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Hcons
{
. . ., 7 qeq 1 , 8 qeq 2 , 9 qeq 3 , 13 qeq 10 , 14 qeq 11 , 15 qeq 12 , . . .
}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 5: RMRS for the coordinate NP ihre Achtung, ihre Zuneigung und Liebe ? their esteem,
their affection and love (from corpus sentence # 8345).
or ob (whether), in initial position, but
may occur without it, though less fre-
quently. If a complementizer is present,
this is recorded as comp_form(_,dass) (resp.
comp_form(_,ob)), and we can fully deter-
mine the kind of message relation from its
lexical form, i.e., prpstn m rel for declar-
ative and int m rel for interrogative ones.
In the absence of an overt complementizer,
we could introduce the underspecified type
prop ques m rel, but rather chose to use a
default rule for the declarative reading prp-
stn m rel, which occurs far more often. This
reduces the manual correction effort.
3.3.2 Coordination
The HPSG analysis of coordinate structures
takes the form of a binary, right-branching
structure. Since semantics construction in
HPSG proceeds along this tree, an RMRS for
a coordinate phrase likewise mirrors the recur-
sive organisation of conjuncts in the syntax.
Each partial coordination introduces an im-
plicit conj rel, while the meaning contributed
by the lexical conjunction is conveyed in the
EP which spans the entire coordination.
By contrast, the dependency structures
preserve the flat LFG-analysis of coordina-
tion as a set of conjuncts. To overcome this
discrepancy between source and target struc-
tures, we define specialised rules that mimic
recursion in that they process the conjuncts
from right to left, two at a time, thereby build-
ing the desired, binary-structure semantics for
the coordination. Fig. 5 shows a sample out-
put RMRS for coordinated NPs.8 Note that
we posit the L/R hndl handle arguments to
outscope each label that takes scope over the
noun. This accounts for scope ambiguities
among quantifiers and scopal adjectives.
3.3.3 Recursive Modification
The algebra of Copestake et al (2001) de-
fines modifiers to externalise the variable of
the ARG1. This, however, runs into problems
when a construction needs to incorporate the
inherent event variable (ARG0) of a modifier
as an argument, as e.g. in recursive modifica-
tion. In these cases, the ARG0 variable is not
accessible as a hook for composition.
In contrast, we identify the hook vari-
able of modifiers with their ARG0 variable.
This enables a uniform account of recur-
sive intersective modification, since the in-
herent variable is legitimatly accessible via
the hook, whereas the ARG1?like any other
argument?is bound in a slot-filling opera-
tion.9 The corresponding rule and an example
output RMRS are displayed in Fig. 6: When-
ever the dependency relation mo is encoun-
tered, no matter what the exact pred value,
the semantics contributed by the head of the
8The semantic contribution of the possessive pro-
nouns has been neglected for ease of exposition.
9Similarly, this treatment of modification correctly
accounts for modification in coordination structures,
as in the NP ihrer munteren und farbenfreudigen In-
szenierung ? of her lively and colourful production
(from corpus sentence # 9821).
7
+mo(X,M), +pred(M,Pred),
-scopal(Pred),
+?s::?(M,SemM), +hook(SemM,Hook),
+lb(Hook,LbM),
get var(X,VarX), get lb(X,LbX)
==> var(Hook,VarM)
&& add ep(LbM,ep rel,rel,Pred)
&& add ep(LbM,ep arg0,arg0,VarM)
&& add ep(LbM,ep arg1,argx,VarX)
&& add ing(LbM,LbX).
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Rels
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
. . .,
?
?
?
liegen v
Lb 1
Arg0 2
Arg1 x
?
?
?
,
?
?
?
hoch r
Lb 3
Arg0 4
Arg1 2
?
?
?
,
?
?
?
sehr r
Lb 5
Arg0 e
Arg1 4
?
?
?
, . . .
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Ing
{
. . ., 3 ing 1 , 5 ing 3 , . . .
}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 6: Rule defining the lexical RMRS for
modifiers (top), resulting global RMRS for
the recursive modification in liege [. . .] sehr
hoch ? [. . .] is at a very high level (from cor-
pus sentence # 8893).
dependency can be unambiguously identified
as the argument of the semantic head. In fact,
given that modifiers are in this way locally
annotated as mo dependents in the triples, we
can bind the ARG1 already when defining the
lexical RMRS of the modifier.
4 The TIGER 700 RMRS Bank
4.1 Design and methodology
Treebank Design. Our aim is to make avai-
lable manually validated RMRS structures for
700 sentences of the TIGER-DB. Since the
underlying data is contiguous newspaper text,
we chose to select a block of consecutive sen-
tences instead of a random sample. In this
way, the treebank can be further extended
by annotation of intersentential phenomena,
such as co-reference or discourse relations.
However, we have to accommodate for gaps,
due to sentences for which there are rea-
sonable functional-syntactic, but (currently)
no sound semantic analyses. This problem
arises for sentences involving, e.g., elliptical
constructions, or else ungrammatical or frag-
mented sentences. We will include, but ex-
plicitly mark such sentences for which we can
only obtain partial, but no fully sound seman-
tic analyses. We will correspondingly extend
the annotation set to yield a total of 700 cor-
rectly annotated sentences.
The composition rules are designed to
record their application by way of rule-specific
identifiers. These may serve as a filtering
means in case the analysis of certain phenom-
ena as assumed in the treebank is incompati-
ble with the grammar to be evaluated.
Quality Control. For compilation of a
manually controlled RMRS bank, we imple-
mented a cascaded approach for quality con-
trol, with a feedback loop between (i) and (ii):
(i) Manual sample-based error-detection.
We are using the application markers of
specific construction rules to select sample
RMRSs for phenomenon-based inspection, as
well as random sampling, in order to detect
problems that can be corrected by adjust-
ments of the automatic conversion procedure.
(ii) Adjustment of conversion rules. The
construction rules are modified to adjust er-
rors detected in the automatic conversion pro-
cess. Errors that cannot be covered by general
rules need to be manually corrected in (iii).
(iii) Manual control. Finally, we perform
manual control and correction of errors that
cannot be covered by automatic RMRS con-
struction. Here, we mark and separate the
phenomena that are not covered by the state-
of-the-art in RMRS-based semantic theory.
Viewing and editing support. The in-
spection of RMRSs is supported by convert-
ing the underlying XML format to HTML.
RMRSs can thus be comfortably viewed in
a browser, with highlighting of coreferences,
display of agreement features, and links of
EPs to the surface forms they originated from.
Correction is supported by an XSLT-based
interactive editing tool. It enables the user to
specify which EPs, arguments or constraints
are to be added/removed. With each change,
the HTML representation is updated, so that
the result is immediately visible for verifica-
tion. The tool features a simple mechanism
for version maintenance and retrieval, and
8
avg # of
abs # in % corrections/sent.
validated 700 100 2.24
odd 28 4
fully perfect 281 40.14 0
corrected 419 59.86 3.75
<5 corrections 601 85.86 0.96
avg # of
abs # in % corrections/sent.
validated 100 100 1.3
odd 5 5
fully perfect 68 68 0
corrected 32 32 4.2
<5 corrections 88 88 0.44
Table 1: Evaluation of current data set for
complete correction (top) and for correction
ignoring part-of-speech (bottom).
separate storage for fully validated structures.
4.2 First Results
The transfer grammar comprises 74 rewrite
rules for converting dependency structures to
RMRS, plus 34 macros and templates.
In a first validation experiment on the ba-
sis of 100 structures, we classified 20% of the
RMRSs as involving errors that can be cap-
tured by adjustments of the automatic con-
version rules (see step (ii) above), while 59%
were fully correct.10
After improvement of the rules we evalu-
ated the quality of the automatic construction
procedure by validating the 700 sentences of
the treebank. Average counts for this sam-
ple are 15.57 tokens/sentence, 15.92 depen-
dencies/sentence. Table 1 (top) summarises
the results. Of the 700 structures, 4% con-
tained phenomena which we do not analyse at
all. 40% required no correction at all. For the
59% that needed manual correction, the aver-
age count of units to be corrected per sentence
was 3.75. The number of RMRSs that needed
less than the average of corrections was 601,
i.e. 85.86%. The time needed for inspection
and correction was 5 mins 12 secs/sentence,
calculated on the entire data set.
Error analysis. A large portion of the er-
rors did not concern the RMRS as such, but
10This evaluation did not perform correction of part-
of-speech tags (cf. below, error analysis).
simply the part-of-speech tags, encoded in the
relation names. If part-of-speech errors are
ignored, the number of correct RMRSs in-
creases from 41% to 68%. The results of vali-
dation without part-of-speech correction, cal-
culated on a third sample of 100 sentences,
are given in Table 1 (bottom).
Significant structural errors arise primarily
in the context of modification. This is due
to the TIGER annotation scheme. For exam-
ple, certain adjunct clauses are embedded in
main clauses as mo dependents, yet the em-
bedding conjunction is, again, annotated as a
modifier of the embedded clause. This leads
to erroneous analyses. Refinement of the rules
could considerably improve accuracy, but dis-
tinguishing these cases from ordinary modifi-
cation is not always possible, due to missing
category information.
While modifiers turned out challenging in
the mapping from dependencies to semantics,
we did not observe many errors in the treat-
ment of arguments: the rules that map de-
pendents to semantic arg predicates yield a
very precise argument structure.
5 Conclusion
We presented a method for semantics con-
struction that converts dependency structures
to RMRSs as they are output by HPSG gram-
mars. By applying this method to the TIGER
Dependency Bank, we construct an RMRS
Bank that allows cross-framework parser eval-
uation for German. Our method for RMRS
construction can be transposed to dependency
banks for other languages, such as the PARC
700 Dependency Bank for English (King et
al., 2003). The choice of RMRS also ensures
that the semantic bank can be used for com-
parative evaluation of HPSG grammars with
low-level parsers that output partial seman-
tics in terms of RMRS, such as the RASP
parser of Carroll and Briscoe (2002).
While the formalism of (R)MRS has its ori-
gins in HPSG, we have shown that RMRS se-
mantics construction can be carried over to
dependency-based frameworks like LFG. In
future research, we will investigate how the
semantic algebra of Copestake et al (2001)
9
compares to Glue Semantics (Dalrymple,
1999). Our construction rules may in fact be
modified and extended to yield semantics con-
struction along the lines of Glue Semantics,
with hooks as resources and Rels, Hcons
and Ing sets as meaning language. In this sce-
nario, the composition rules would consume
the hook of the semantic argument, so that
resource-sensitivity is assured. Scope ambi-
guities would not result in alternative deriva-
tions, since RMRS makes use of scope under-
specification in the meaning language.
Related work in Dyvik et al (2005) presents
MRS construction from LFG grammars in
a correspondence architecture, where seman-
tics is defined as a projection in individ-
ual syntactic rules. Our architecture follows
a description-by-analysis (DBA) approach,
where semantics construction applies to fully
resolved syntactic structures. This architec-
ture is especially suited for the present task
of treebank creation, where grammars for a
given language may not have full coverage.
Also, in a DBA architecture, incomplete rule
sets can still yield partially annotated, i.e.,
unconnected semantic structures. Likewise,
this construction method can deal with par-
tially analysed syntactic input.
Finally, our method can be extended to a
full parsing architecture with deep semantic
output, where care should be taken to pre-
serve structural or categorial information that
we identified as crucial for the purpose of
principle-driven semantics construction.
Acknowledgements
The research reported here was conducted
in cooperation with the TIGER project and
has partially been supported by the project
QUETAL (DFKI), funded by the German
Ministry for Education and Research, grant
no. 01 IW C02. Special thanks go to Dan
Flickinger and Berthold Crysmann for advice
on theoretical and grammar-specific issues.
We also thank Martin Forst, who provided us
with the TIGER DB dependency structures,
Berthold Crysmann for providing HPSG lex-
ical resources, and Ulrich Scha?fer for XSLT-
scripts used for visualisation.
References
S. Brants, S. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER Treebank. In
Proceedings of the Workshop on Treebanks and
Linguistic Theories, Sozopol, Bulgaria.
C. Carroll and E. Briscoe. 2002. High precision
extraction of grammatical relations. In Proceed-
ings of COLING 2002, pages 134?140.
A. Copestake, A. Lascarides, and D. Flickinger.
2001. An Algebra for Semantic Construction in
Constraint-based Grammars. In Proceedings of
the ACL 2001, Toulouse, France.
A. Copestake, D. Flickinger, I. Sag, and C. Pol-
lard. 2005. Minimal Recursion Semantics. to
appear.
A. Copestake. 2003. Report on the Design of
RMRS. Technical Report D1.1a, University of
Cambridge, University of Cambridge, UK.
R. Crouch. 2005. Packed Rewriting for Mapping
Semantics to KR. In Proceedings of the Sixth
International Workshop on Computational Se-
mantics, IWCS-05, Tilburg, The Netherlands.
B. Crysmann. 2003. On the efficient implemen-
tation of German verb placement in HPSG. In
Proceedings of RANLP 2004, Bulgaria.
M. Dalrymple, editor. 1999. Semantics and Syn-
tax in Lexical Functional Grammar: The Re-
source Logic Approach. MIT Press.
H. Dyvik, V. Rose?n, and P. Meurer. 2005. LFG,
Minimal Recursion Semantics and Translation.
In Proceedings of the LFG 2005 Conference,
Bergen, Norway. to appear.
M. Forst, N. Bertomeu, B. Crysmann, F. Fouvry,
S. Hansen-Schirra, and V. Kordoni. 2004. To-
wards a Dependency-Based Gold Standard for
German Parsers: The Tiger Dependency Bank.
In S. Hansen-Schirra, S. Oepen, and H. Uszkor-
eit, editors, Proceedings of LINC 2004, Geneva,
Switzerland.
M. Forst. 2003. Treebank Conversion ? Estab-
lishing a testsuite for a broad-coverage LFG
from the TIGER treebank. In Proceedings of
LINC?03, Budapest, Hungary.
A. Frank and K. Erk. 2004. Towards an LFG
Syntax?Semantics Interface for Frame Seman-
tics Annotation. In A. Gelbukh, editor, Com-
putational Linguistics and Intelligent Text Pro-
cessing, LNCS, Vol. 2945. Springer, Heidelberg.
T.H. King, R. Crouch, S. Riezler, M. Dalrymple,
and R. Kaplan. 2003. The PARC 700 Depen-
dency Bank. In Proceedings of LINC 2003, Bu-
dapest, Hungary.
10
An Integrated Architecture for Shallow and Deep Processing
Berthold Crysmann, Anette Frank, Bernd Kiefer, Stefan Mu?ller,
Gu?nter Neumann, Jakub Piskorski, Ulrich Scha?fer, Melanie Siegel, Hans Uszkoreit,
Feiyu Xu, Markus Becker and Hans-Ulrich Krieger
DFKI GmbH
Stuhlsatzenhausweg 3
Saarbru?cken, Germany
whiteboard@dfki.de
Abstract
We present an architecture for the integra-
tion of shallow and deep NLP components
which is aimed at flexible combination
of different language technologies for a
range of practical current and future appli-
cations. In particular, we describe the inte-
gration of a high-level HPSG parsing sys-
tem with different high-performance shal-
low components, ranging from named en-
tity recognition to chunk parsing and shal-
low clause recognition. The NLP com-
ponents enrich a representation of natu-
ral language text with layers of new XML
meta-information using a single shared
data structure, called the text chart. We de-
scribe details of the integration methods,
and show how information extraction and
language checking applications for real-
world German text benefit from a deep
grammatical analysis.
1 Introduction
Over the last ten years or so, the trend in application-
oriented natural language processing (e.g., in the
area of term, information, and answer extraction)
has been to argue that for many purposes, shallow
natural language processing (SNLP) of texts can
provide sufficient information for highly accurate
and useful tasks to be carried out. Since the emer-
gence of shallow techniques and the proof of their
utility, the focus has been to exploit these technolo-
gies to the maximum, often ignoring certain com-
plex issues, e.g. those which are typically well han-
dled by deep NLP systems. Up to now, deep natural
language processing (DNLP) has not played a sig-
nificant role in the area of industrial NLP applica-
tions, since this technology often suffers from insuf-
ficient robustness and throughput, when confronted
with large quantities of unrestricted text.
Current information extractions (IE) systems
therefore do not attempt an exhaustive DNLP analy-
sis of all aspects of a text, but rather try to analyse or
?understand? only those text passages that contain
relevant information, thereby warranting speed and
robustness wrt. unrestricted NL text. What exactly
counts as relevant is explicitly defined by means
of highly detailed domain-specific lexical entries
and/or rules, which perform the required mappings
from NL utterances to corresponding domain knowl-
edge. However, this ?fine-tuning? wrt. a particular
application appears to be the major obstacle when
adapting a given shallow IE system to another do-
main or when dealing with the extraction of com-
plex ?scenario-based? relational structures. In fact,
(Appelt and Israel, 1997) have shown that the cur-
rent IE technology seems to have an upper perfor-
mance level of less than 60% in such cases. It seems
reasonable to assume that if a more accurate analy-
sis of structural linguistic relationships could be pro-
vided (e.g., grammatical functions, referential rela-
tionships), this barrier might be overcome. Actually,
the growing market needs in the wide area of intel-
ligent information management systems seem to re-
quest such a break-through.
In this paper we will argue that the quality of cur-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 441-448.
                         Proceedings of the 40th Annual Meeting of the Association for
rent SNLP-based applications can be improved by
integrating DNLP on demand in a focussed manner,
and we will present a system that combines the fine-
grained anaysis provided by HPSG parsing with a
high-performance SNLP system into a generic and
flexible NLP architecture.
1.1 Integration Scenarios
Owing to the fact that deep and shallow technologies
are complementary in nature, integration is a non-
trivial task: while SNLP shows its strength in the
areas of efficiency and robustness, these aspects are
problematic for DNLP systems. On the other hand,
DNLP can deliver highly precise and fine-grained
linguistic analyses. The challenge for integration is
to combine these two paradigms according to their
virtues.
Probably the most straightforward way to inte-
grate the two is an architecture in which shallow and
deep components run in parallel, using the results of
DNLP, whenever available. While this kind of ap-
proach is certainly feasible for a real-time applica-
tion such as Verbmobil, it is not ideal for processing
large quantities of text: due to the difference in pro-
cessing speed, shallow and deep NLP soon run out
of sync. To compensate, one can imagine two possi-
ble remedies: either to optimize for precision, or for
speed. The drawback of the former strategy is that
the overall speed will equal the speed of the slow-
est component, whereas in case of the latter, DNLP
will almost always time out, such that overall preci-
sion will hardly be distinguishable from a shallow-
only system. What is thus called for is an integrated,
flexible architecture where components can play at
their strengths. Partial analyses from SNLP can be
used to identify relevant candidates for the focussed
use of DNLP, based on task or domain-specific crite-
ria. Furthermore, such an integrated approach opens
up the possibility to address the issue of robustness
by using shallow analyses (e.g., term recognition)
to increase the coverage of the deep parser, thereby
avoiding a duplication of efforts. Likewise, integra-
tion at the phrasal level can be used to guide the
deep parser towards the most likely syntactic anal-
ysis, leading, as it is hoped, to a considerable speed-
up.
shallow
NLP
components
NLP
deep
components internal repr.
layer
multi
chart
annot.
XML
external repr.
generic OOP
component
interface
WHAM
application
specification
input and
result
Figure 1: The WHITEBOARD architecture.
2 Architecture
The WHITEBOARD architecture defines a platform
that integrates the different NLP components by en-
riching an input document through XML annota-
tions. XML is used as a uniform way of represent-
ing and keeping all results of the various processing
components and to support a transparent software
infrastructure for LT-based applications. It is known
that interesting linguistic information ?especially
when considering DNLP? cannot efficiently be
represented within the basic XML markup frame-
work (?typed parentheses structure?), e.g., linguistic
phenomena like coreferences, ambiguous readings,
and discontinuous constituents. The WHITEBOARD
architecture employs a distributed multi-level repre-
sentation of different annotations. Instead of trans-
lating all complex structures into one XML docu-
ment, they are stored in different annotation layers
(possibly non-XML, e.g. feature structures). Hyper-
links and ?span? information together support effi-
cient access between layers. Linguistic information
of common interest (e.g. constituent structure ex-
tracted from HPSG feature structures) is available in
XML format with hyperlinks to full feature struc-
ture representations externally stored in correspond-
ing data files.
Fig. 1 gives an overview of the architecture of
the WHITEBOARD Annotation Machine (WHAM).
Applications feed the WHAM with input texts and
a specification describing the components and con-
figuration options requested. The core WHAM en-
gine has an XML markup storage (external ?offline?
representation), and an internal ?online? multi-level
annotation chart (index-sequential access). Follow-
ing the trichotomy of NLP data representation mod-
els in (Cunningham et al, 1997), the XML markup
contains additive information, while the multi-level
chart contains positional and abstraction-based in-
formation, e.g., feature structures representing NLP
entities in a uniform, linguistically motivated form.
Applications and the integrated components ac-
cess the WHAM results through an object-oriented
programming (OOP) interface which is designed
as general as possible in order to abstract from
component-specific details (but preserving shallow
and deep paradigms). The interfaces of the actu-
ally integrated components form subclasses of the
generic interface. New components can be inte-
grated by implementing this interface and specifying
DTDs and/or transformation rules for the chart.
The OOP interface consists of iterators that walk
through the different annotation levels (e.g., token
spans, sentences), reference and seek operators that
allow to switch to corresponding annotations on a
different level (e.g., give all tokens of the current
sentence, or move to next named entity starting
from a given token position), and accessor meth-
ods that return the linguistic information contained
in the chart. Similarily, general methods support
navigating the type system and feature structures of
the DNLP components. The resulting output of the
WHAM can be accessed via the OOP interface or as
XML markup.
The WHAM interface operations are not only
used to implement NLP component-based applica-
tions, but also for the integration of deep and shallow
processing components itself.
2.1 Components
2.1.1 Shallow NL component
Shallow analysis is performed by SPPC, a rule-
based system which consists of a cascade of
weighted finite?state components responsible for
performing subsequent steps of the linguistic anal-
ysis, including: fine-grained tokenization, lexico-
morphological analysis, part-of-speech filtering,
named entity (NE) recognition, sentence bound-
ary detection, chunk and subclause recognition,
see (Piskorski and Neumann, 2000; Neumann and
Piskorski, 2002) for details. SPPC is capable of pro-
cessing vast amounts of textual data robustly and ef-
ficiently (ca. 30,000 words per second in standard
PC environment). We will briefly describe the SPPC
components which are currently integrated with the
deep components.
Each token identified by a tokenizer as a poten-
tial word form is morphologically analyzed. For
each token, its lexical information (list of valid read-
ings including stem, part-of-speech and inflection
information) is computed using a fullform lexicon
of about 700,000 entries that has been compiled out
from a stem lexicon of about 120,000 lemmas. Af-
ter morphological processing, POS disambiguation
rules are applied which compute a preferred read-
ing for each token, while the deep components can
back off to all readings. NE recognition is based on
simple pattern matching techniques. Proper names
(organizations, persons, locations), temporal expres-
sions and quantities can be recognized with an av-
erage precision of almost 96% and recall of 85%.
Furthermore, a NE?specific reference resolution is
performed through the use of a dynamic lexicon
which stores abbreviated variants of previously rec-
ognized named entities. Finally, the system splits
the text into sentences by applying only few, but
highly accurate contextual rules for filtering implau-
sible punctuation signs. These rules benefit directly
from NE recognition which already performs re-
stricted punctuation disambiguation.
2.1.2 Deep NL component
The HPSG Grammar is based on a large?scale
grammar for German (Mu?ller, 1999), which was
further developed in the VERBMOBIL project for
translation of spoken language (Mu?ller and Kasper,
2000). After VERBMOBIL the grammar was adapted
to the requirements of the LKB/PET system (Copes-
take, 1999), and to written text, i.e., extended with
constructions like free relative clauses that were ir-
relevant in the VERBMOBIL scenario.
The grammar consists of a rich hierarchy of
5,069 lexical and phrasal types. The core grammar
contains 23 rule schemata, 7 special verb move-
ment rules, and 17 domain specific rules. All rule
schemata are unary or binary branching. The lexicon
contains 38,549 stem entries, from which more than
70% were semi-automatically acquired from the an-
notated NEGRA corpus (Brants et al, 1999).
The grammar parses full sentences, but also other
kinds of maximal projections. In cases where no full
analysis of the input can be provided, analyses of
fragments are handed over to subsequent modules.
Such fragments consist of maximal projections or
single words.
The HPSG analysis system currently integrated
in the WHITEBOARD system is PET (Callmeier,
2000). Initially, PET was built to experiment
with different techniques and strategies to process
unification-based grammars. The resulting sys-
tem provides efficient implementations of the best
known techniques for unification and parsing.
As an experimental system, the original design
lacked open interfaces for flexible integration with
external components. For instance, in the beginning
of the WHITEBOARD project the system only ac-
cepted fullform lexica and string input. In collabora-
tion with Ulrich Callmeier the system was extended.
Instead of single word input, input items can now
be complex, overlapping and ambiguous, i.e. essen-
tially word graphs. We added dynamic creation of
atomic type symbols, e.g., to be able to add arbitrary
symbols to feature structures. With these enhance-
ments, it is possible to build flexible interfaces to
external components like morphology, tokenization,
named entity recognition, etc.
3 Integration
Morphology and POS The coupling between the
morphology delivered by SPPC and the input needed
for the German HPSG was easily established. The
morphological classes of German are mapped onto
HPSG types which expand to small feature struc-
tures representing the morphological information in
a compact way. A mapping to the output of SPPC
was automatically created by identifying the corre-
sponding output classes.
Currently, POS tagging is used in two ways. First,
lexicon entries that are marked as preferred by the
shallow component are assigned higher priority than
the rest. Thus, the probability of finding the cor-
rect reading early should increase without excluding
any reading. Second, if for an input item no entry is
found in the HPSG lexicon, we automatically create
a default entry, based on the part?of?speech of the
preferred reading. This increases robustness, while
avoiding increase in ambiguity.
Named Entity Recognition Writing HPSG gram-
mars for the whole range of NE expressions etc. is
a tedious and not very promising task. They typi-
cally vary across text sorts and domains, and would
require modularized subgrammars that can be easily
exchanged without interfering with the general core.
This can only be realized by using a type interface
where a class of named entities is encoded by a gen-
eral HPSG type which expands to a feature structure
used in parsing. We exploit such a type interface for
coupling shallow and deep processing. The classes
of named entities delivered by shallow processing
are mapped to HPSG types. However, some fine-
tuning is required whenever deep and shallow pro-
cessing differ in the amount of input material they
assign to a named entity.
An alternative strategy is used for complex syn-
tactic phrases containing NEs, e.g., PPs describ-
ing time spans etc. It is based on ideas from
Explanation?based Learning (EBL, see (Tadepalli
and Natarajan, 1996)) for natural language analy-
sis, where analysis trees are retrieved on the basis
of the surface string. In our case, the part-of-speech
sequence of NEs recognised by shallow analysis is
used to retrieve pre-built feature structures. These
structures are produced by extracting NEs from a
corpus and processing them directly by the deep
component. If a correct analysis is delivered, the
lexical parts of the analysis, which are specific for
the input item, are deleted. We obtain a sceletal
analysis which is underspecified with respect to the
concrete input items. The part-of-speech sequence
of the original input forms the access key for this
structure. In the application phase, the underspeci-
fied feature structure is retrieved and the empty slots
for the input items are filled on the basis of the con-
crete input.
The advantage of this approach lies in the more
elaborate semantics of the resulting feature struc-
tures for DNLP, while avoiding the necessity of
adding each and every single name to the HPSG lex-
icon. Instead, good coverage and high precision can
be achieved using prototypical entries.
Lexical Semantics When first applying the origi-
nal VERBMOBIL HPSG grammar to business news
articles, the result was that 78.49% of the miss-
ing lexical items were nouns (ignoring NEs). In
the integrated system, unknown nouns and NEs can
be recognized by SPPC, which determines morpho-
syntactic information. It is essential for the deep sys-
tem to associate nouns with their semantic sorts both
for semantics construction, and for providing se-
mantically based selectional restrictions to help con-
straining the search space during deep parsing. Ger-
maNet (Hamp and Feldweg, 1997) is a large lexical
database, where words are associated with POS in-
formation and semantic sorts, which are organized in
a fine-grained hierarchy. The HPSG lexicon, on the
other hand, is comparatively small and has a more
coarse-grained semantic classification.
To provide the missing sort information when re-
covering unknown noun entries via SPPC, a map-
ping from the GermaNet semantic classification to
the HPSG semantic classification (Siegel et al,
2001) is applied which has been automatically ac-
quired. The training material for this learning pro-
cess are those words that are both annotated with se-
mantic sorts in the HPSG lexicon and with synsets
of GermaNet. The learning algorithm computes a
mapping relevance measure for associating seman-
tic concepts in GermaNet with semantic sorts in the
HPSG lexicon. For evaluation, we examined a cor-
pus of 4664 nouns extracted from business news
that were not contained in the HPSG lexicon. 2312
of these were known in GermaNet, where they are
assigned 2811 senses. With the learned mapping,
the GermaNet senses were automatically mapped to
HPSG semantic sorts. The evaluation of the map-
ping accuracy yields promising results: In 76.52%
of the cases the computed sort with the highest rel-
evance probability was correct. In the remaining
20.70% of the cases, the correct sort was among the
first three sorts.
3.1 Integration on Phrasal Level
In the previous paragraphs we described strategies
for integration of shallow and deep processing where
the focus is on improving DNLP in the domain of
lexical and sub-phrasal coverage.
We can conceive of more advanced strategies for
the integration of shallow and deep analysis at the
length cover- complete LP LR 0CB   2CB
age match
  40 100 80.4 93.4 92.9 92.1 98.9
 40 99.8 78.6 92.4 92.2 90.7 98.5
Training: 16,000 NEGRA sentences
Testing: 1,058 NEGRA sentences
Figure 2: Stochastic topological parsing: results
level of phrasal syntax by guiding the deep syntac-
tic parser towards a partial pre-partitioning of com-
plex sentences provided by shallow analysis sys-
tems. This strategy can reduce the search space, and
enhance parsing efficiency of DNLP.
Stochastic Topological Parsing The traditional
syntactic model of topological fields divides basic
clauses into distinct fields: so-called pre-, middle-
and post-fields, delimited by verbal or senten-
tial markers. This topological model of German
clause structure is underspecified or partial as to
non-sentential constituent boundaries, but provides
a linguistically well-motivated, and theory-neutral
macrostructure for complex sentences. Due to its
linguistic underpinning the topological model pro-
vides a pre-partitioning of complex sentences that is
(i) highly compatible with deep syntactic structures
and (ii) maximally effective to increase parsing ef-
ficiency. At the same time (iii) partiality regarding
the constituency of non-sentential material ensures
the important aspects of robustness, coverage, and
processing efficiency.
In (Becker and Frank, 2002) we present a corpus-
driven stochastic topological parser for German,
based on a topological restructuring of the NEGRA
corpus (Brants et al, 1999). For topological tree-
bank conversion we build on methods and results
in (Frank, 2001). The stochastic topological parser
follows the probabilistic model of non-lexicalised
PCFGs (Charniak, 1996). Due to abstraction from
constituency decisions at the sub-sentential level,
and the essentially POS-driven nature of topologi-
cal structure, this rather simple probabilistic model
yields surprisingly high figures of accuracy and cov-
erage (see Fig.2 and (Becker and Frank, 2002) for
more detail), while context-free parsing guarantees
efficient processing.
The next step is to elaborate a (partial) map-
ping of shallow topological and deep syntactic struc-
tures that is maximally effective for preference-gui-
Topological Structure:
CL-V2
VF-TOPIC LK-FIN MF RK-t
NN VVFIN ADV NN PREP NN VVFIN
[ 	 [ 
	 Peter] [ 
 i?t] [ 
 gerne Wu?rstchen mit Kartoffelsalat] [ Integrated Shallow and Deep Parsing: TopP meets HPSG
Anette Frank, Markus Beckerz, Berthold Crysmann, Bernd Kiefer and Ulrich Scha?fer
DFKI GmbH School of Informaticsz
66123 Saarbru?cken, Germany University of Edinburgh, UK
firstname.lastname@dfki.de M.Becker@ed.ac.uk
Abstract
We present a novel, data-driven method
for integrated shallow and deep parsing.
Mediated by an XML-based multi-layer
annotation architecture, we interleave a
robust, but accurate stochastic topological
field parser of German with a constraint-
based HPSG parser. Our annotation-based
method for dovetailing shallow and deep
phrasal constraints is highly flexible, al-
lowing targeted and fine-grained guidance
of constraint-based parsing. We conduct
systematic experiments that demonstrate
substantial performance gains.1
1 Introduction
One of the strong points of deep processing (DNLP)
technology such as HPSG or LFG parsers certainly
lies with the high degree of precision as well as
detailed linguistic analysis these systems are able
to deliver. Although considerable progress has been
made in the area of processing speed, DNLP systems
still cannot rival shallow and medium depth tech-
nologies in terms of throughput and robustness. As
a net effect, the impact of deep parsing technology
on application-oriented NLP is still fairly limited.
With the advent of XML-based hybrid shallow-
deep architectures as presented in (Grover and Las-
carides, 2001; Crysmann et al, 2002; Uszkoreit,
2002) it has become possible to integrate the added
value of deep processing with the performance and
robustness of shallow processing. So far, integration
has largely focused on the lexical level, to improve
upon the most urgent needs in increasing the robust-
ness and coverage of deep parsing systems, namely
1This work was in part supported by a BMBF grant to the
DFKI project WHITEBOARD (FKZ 01 IW 002).
lexical coverage. While integration in (Grover and
Lascarides, 2001) was still restricted to morphologi-
cal and PoS information, (Crysmann et al, 2002) ex-
tended shallow-deep integration at the lexical level
to lexico-semantic information, and named entity
expressions, including multiword expressions.
(Crysmann et al, 2002) assume a vertical,
?pipeline? scenario where shallow NLP tools provide
XML annotations that are used by the DNLP system
as a preprocessing and lexical interface. The per-
spective opened up by a multi-layered, data-centric
architecture is, however, much broader, in that it en-
courages horizontal cross-fertilisation effects among
complementary and/or competing components.
One of the culprits for the relative inefficiency of
DNLP parsers is the high degree of ambiguity found
in large-scale grammars, which can often only be re-
solved within a larger syntactic domain. Within a hy-
brid shallow-deep platform one can take advantage
of partial knowledge provided by shallow parsers to
pre-structure the search space of the deep parser. In
this paper, we will thus complement the efforts made
on the lexical side by integration at the phrasal level.
We will show that this may lead to considerable per-
formance increase for the DNLP component. More
specifically, we combine a probabilistic topological
field parser for German (Becker and Frank, 2002)
with the HPSG parser of (Callmeier, 2000). The
HPSG grammar used is the one originally developed
by (Mu?ller and Kasper, 2000), with significant per-
formance enhancements by B. Crysmann.
In Section 2 we discuss the mapping problem
involved with syntactic integration of shallow and
deep analyses and motivate our choice to combine
the HPSG system with a topological parser. Sec-
tion 3 outlines our basic approach towards syntactic
shallow-deep integration. Section 4 introduces vari-
ous confidence measures, to be used for fine-tuning
of phrasal integration. Sections 5 and 6 report on
experiments and results of integrated shallow-deep
parsing, measuring the effect of various integra-
tion parameters on performance gains for the DNLP
component. Section 7 concludes and discusses pos-
sible extensions, to address robustness issues.
2 Integrated Shallow and Deep Processing
The prime motivation for integrated shallow-deep
processing is to combine the robustness and effi-
ciency of shallow processing with the accuracy and
fine-grainedness of deep processing. Shallow analy-
ses could be used to pre-structure the search space of
a deep parser, enhancing its efficiency. Even if deep
analysis fails, shallow analysis could act as a guide
to select partial analyses from the deep parser?s chart
? enhancing the robustness of deep analysis, and the
informativeness of the combined system.
In this paper, we concentrate on the usage of shal-
low information to increase the efficiency, and po-
tentially the quality, of HPSG parsing. In particu-
lar, we want to use analyses delivered by an effi-
cient shallow parser to pre-structure the search space
of HPSG parsing, thereby enhancing its efficiency,
and guiding deep parsing towards a best-first analy-
sis suggested by shallow analysis constraints.
The search space of an HPSG chart parser can
be effectively constrained by external knowledge
sources if these deliver compatible partial subtrees,
which would then only need to be checked for com-
patibility with constituents derived in deep pars-
ing. Raw constituent span information can be used
to guide the parsing process by penalizing con-
stituents which are incompatible with the precom-
puted ?shape?. Additional information about pro-
posed constituents, such as categorial or featural
constraints, provide further criteria for prioritis-
ing compatible, and penalising incompatible con-
stituents in the deep parser?s chart.
An obvious challenge for our approach is thus to
identify suitable shallow knowledge sources that can
deliver compatible constraints for HPSG parsing.
2.1 The Shallow-Deep Mapping Problem
However, chunks delivered by state-of-the-art shal-
low parsers are not isomorphic to deep syntactic
analyses that explicitly encode phrasal embedding
structures. As a consequence, the boundaries of
deep grammar constituents in (1.a) cannot be pre-
determined on the basis of a shallow chunk analy-
sis (1.b). Moreover, the prevailing greedy bottom-up
processing strategies applied in chunk parsing do not
take into account the macro-structure of sentences.
They are thus easily trapped in cases such as (2).
(1) a. [
CL
There was [
NP
a rumor [
CL
it was going
to be bought by [
NP
a French company [
CL
that
competes in supercomputers]]]]].
b. [
CL
There was [
NP
a rumor]] [
CL
it was going
to be bought by [
NP
a French company]] [
CL
that competes in supercomputers].
(2) Fred eats [
NP
pizza and Mary] drinks wine.
In sum, state-of-the-art chunk parsing does nei-
ther provide sufficient detail, nor the required accu-
racy to act as a ?guide? for deep syntactic analysis.
2.2 Stochastic Topological Parsing
Recently, there is revived interest in shallow anal-
yses that determine the clausal macro-structure of
sentences. The topological field model of (German)
syntax (Ho?hle, 1983) divides basic clauses into dis-
tinct fields ? pre-, middle-, and post-fields ? delim-
ited by verbal or sentential markers, which consti-
tute the left/right sentence brackets. This model of
clause structure is underspecified, or partial as to
non-sentential constituent structure, but provides a
theory-neutral model of sentence macro-structure.
Due to its linguistic underpinning, the topologi-
cal field model provides a pre-partitioning of com-
plex sentences that is (i) highly compatible with
deep syntactic analysis, and thus (ii) maximally ef-
fective to increase parsing efficiency if interleaved
with deep syntactic analysis; (iii) partiality regarding
the constituency of non-sentential material ensures
robustness, coverage, and processing efficiency.
(Becker and Frank, 2002) explored a corpus-
based stochastic approach to topological field pars-
ing, by training a non-lexicalised PCFG on a topo-
logical corpus derived from the NEGRA treebank of
German. Measured on the basis of hand-corrected
PoS-tagged input as provided by the NEGRA tree-
bank, the parser achieves 100% coverage for length
 40 (99.8% for all). Labelled precision and recall
are around 93%. Perfect match (full tree identity) is
about 80% (cf. Table 1, disamb +).
In this paper, the topological parser was provided
a tagger front-end for free text processing, using the
TnT tagger (Brants, 2000). The grammar was ported
to the efficient LoPar parser of (Schmid, 2000). Tag-
ging inaccuracies lead to a drop of 5.1/4.7 percent-
CL-V2
VF-TOPIC LK-VFIN MF RK-VPART NF
ART NN VAFIN ART ADJA NN VAPP CL-SUBCL
Der,1 Zehnkampf,2 ha?tte,3 eine,4 andere,5 Dimension,6 gehabt,7 ,
The decathlon would have a other dimension had LK-COMPL MF RK-VFIN
KOUS PPER PROAV VAPP VAFIN
wenn,9 er,10 dabei,11 gewesen,12 wa?re,13 .
if he there been had .
<TOPO2HPSG type=?root? id=?5608?>
<MAP CONSTR id=?T1? constr=?v2 cp? conf
ent
=?0.87? left=?W1? right=?W13?/>
<MAP CONSTR id=?T2? constr=?v2 vf? conf
ent
=?0.87? left=?W1? right=?W2?/>
<MAP CONSTR id=?T3? constr=?vfronted vfin+rk? conf
ent
=?0.87? left=?W3? right=?W3?/>
<MAP CONSTR id=?T6? constr=?vfronted rk-complex? conf
ent
=?0.87? left=?W7? right=?W7?/>
<MAP CONSTR id=?T4? constr=?vfronted vfin+vp+rk? conf
ent
=?0.87? left=?W3? right=?W13?/>
<MAP CONSTR id=?T5? constr=?vfronted vp+rk? conf
ent
=?0.87? left=?W4? right=?W13?/>
<MAP CONSTR id=?T10? constr=?extrapos rk+nf? conf
ent
=?0.87? left=?W7? right=?W13?/>
<MAP CONSTR id=?T7? constr=?vl cpfin compl? conf
ent
=?0.87? left=?W9? right=?W13?/>
<MAP CONSTR id=?T8? constr=?vl compl vp? conf
ent
=?0.87? left=?W10? right=?W13?/>
<MAP CONSTR id=?T9? constr=?vl rk fin+complex+finlast? conf
ent
=?0.87? left=?W12? right=?W13?/>
</TOPO2HPSG>
Der
D
Zehnkampf
N?
NP-NOM-SG
haette
V
eine
D
andere
AP-ATT
Dimension
N?
N?
NP-ACC-SG
gehabt
V
EPS
wenn
C
er
NP-NOM-SG
dabei
PP
gewesen
V
waere
V-LE
V
V
S
CP-MOD
EPS
EPS
EPS/NP-NOM-SG
S/NP-NOM-SG
S
Figure 1: Topological tree w/param. cat., TOPO2HPSG map-constraints, tree skeleton of HPSG analysis
dis- cove- perfect LP LR 0CB 2CB
amb rage match in % in % in % in %
+ 100.0 80.4 93.4 92.9 92.1 98.9
  99.8 72.1 88.3 88.2 87.8 97.9
Table 1: Disamb: correct (+) / tagger ( ) PoS input.
Eval. on atomic (vs. parameterised) category labels.
age points in LP/LR, and 8.3 percentage points in
perfect match rate (Table 1, disamb  ).
As seen in Figure 1, the topological trees abstract
away from non-sentential constituency ? phrasal
fields MF (middle-field) and VF (pre-field) directly
expand to PoS tags. By contrast, they perfectly ren-
der the clausal skeleton and embedding structure of
complex sentences. In addition, parameterised cate-
gory labels encode larger syntactic contexts, or ?con-
structions?, such as clause type (CL-V2, -SUBCL,
-REL), or inflectional patterns of verbal clusters (RK-
VFIN,-VPART). These properties, along with their
high accuracy rate, make them perfect candidates for
tight integration with deep syntactic analysis.
Moreover, due to the combination of scrambling
and discontinuous verb clusters in German syntax, a
deep parser is confronted with a high degree of local
ambiguity that can only be resolved at the clausal
level. Highly lexicalised frameworks such as HPSG,
however, do not lend themselves naturally to a top-
down parsing strategy. Using topological analyses to
guide the HPSG will thus provide external top-down
information for bottom-up parsing.
3 TopP meets HPSG
Our work aims at integration of topological and
HPSG parsing in a data-centric architecture, where
each component acts independently2 ? in contrast
to the combination of different syntactic formalisms
within a unified parsing process.3 Data-based inte-
gration not only favours modularity, but facilitates
flexible and targeted dovetailing of structures.
3.1 Mapping Topological to HPSG Structures
While structurally similar, topological trees are not
fully isomorphic to HPSG structures. In Figure 1,
e.g., the span from the verb ?ha?tte? to the end of the
sentence forms a constituent in the HPSG analysis,
while in the topological tree the same span is domi-
nated by a sequence of categories: LK, MF, RK, NF.
Yet, due to its linguistic underpinning, the topo-
logical tree can be used to systematically predict
key constituents in the corresponding ?target? HPSG
2See Section 6 for comparison to recent work on integrated
chunk-based and dependency parsing in (Daum et al, 2003).
3As, for example, in (Duchier and Debusmann, 2001).
analysis. We know, for example, that the span from
the fronted verb (LK-VFIN) till the end of its clause
CL-V2 corresponds to an HPSG phrase. Also, the
first position that follows this verb, here the leftmost
daughter of MF, demarcates the left edge of the tra-
ditional VP. Spans of the vorfeld VF and clause cat-
egories CL exactly match HPSG constituents. Cate-
gory CL-V2 tells us that we need to reckon with a
fronted verb in position of its LK daughter, here 3,
while in CL-SUBCL we expect a complementiser in
the position of LK, and a finite verb within the right
verbal complex RK, which spans positions 12 to 13.
In order to communicate such structural con-
straints to the deep parser, we scan the topological
tree for relevant configurations, and extract the span
information for the target HPSG constituents. The
resulting ?map constraints? (Fig. 1) encode a bracket
type name4 that identifies the target constituent and
its left and right boundary, i.e. the concrete span in
the sentence under consideration. The span is en-
coded by the word position index in the input, which
is identical for the two parsing processes.5
In addition to pure constituency constraints, a
skilled grammar writer will be able to associate spe-
cific HPSG grammar constraints ? positive or neg-
ative ? with these bracket types. These additional
constraints will be globally defined, to permit fine-
grained guidance of the parsing process. This and
further information (cf. Section 4) is communicated
to the deep parser by way of an XML interface.
3.2 Annotation-based Integration
In the annotation-based architecture of (Crysmann
et al, 2002), XML-encoded analysis results of all
components are stored in a multi-layer XML chart.
The architecture employed in this paper improves
on (Crysmann et al, 2002) by providing a central
Whiteboard Annotation Transformer (WHAT) that
supports flexible and powerful access to and trans-
formation of XML annotation based on standard
XSLT engines6 (see (Scha?fer, 2003) for more de-
tails on WHAT). Shallow-deep integration is thus
fully annotation driven. Complex XSLT transforma-
tions are applied to the various analyses, in order to
4We currently extract 34 different bracket types.
5We currently assume identical tokenisation, but could ac-
commodate for distinct tokenisation regimes, using map tables.
6Advantages we see in the XSLT approach are (i) minimised
programming effort in the target implementation language for
XML access, (ii) reuse of transformation rules in multiple mod-
ules, (iii) fast integration of new XML-producing components.
extract or combine independent knowledge sources,
including XPath access to information stored in
shallow annotation, complex XSLT transformations
to the output of the topological parser, and extraction
of bracket constraints.
3.3 Shaping the Deep Parser?s Search Space
The HPSG parser is an active bidirectional chart
parser which allows flexible parsing strategies by us-
ing an agenda for the parsing tasks.7 To compute pri-
orities for the tasks, several information sources can
be consulted, e.g. the estimated quality of the parti-
cipating edges or external resources like PoS tagger
results. Object-oriented implementation of the prior-
ity computation facilitates exchange and, moreover,
combination of different ranking strategies. Extend-
ing our current regime that uses PoS tagging for pri-
oritisation,8 we are now utilising phrasal constraints
(brackets) from topological analysis to enhance the
hand-crafted parsing heuristic employed so far.
Conditions for changing default priorities Ev-
ery bracket pair br
x
computed from the topological
analysis comes with a bracket type x that defines its
behaviour in the priority computation. Each bracket
type can be associated with a set of positive and neg-
ative constraints that state a set of permissible or for-
bidden rules and/or feature structure configurations
for the HPSG analysis.
The bracket types fall into three main categories:
left-, right-, and fully matching brackets. A right-
matching bracket may affect the priority of tasks
whose resulting edge will end at the right bracket
of a pair, like, for example, a task that would
combine edges C and F or C and D in Fig. 2.
Left-matching brackets work analogously. For fully
matching brackets, only tasks that produce an edge
that matches the span of the bracket pair can be af-
fected, like, e.g., a task that combines edges B and C
in Fig. 2. If, in addition, specified rule as well as fea-
ture structure constraints hold, the task is rewarded
if they are positive constraints, and penalised if they
are negative ones. All tasks that produce crossing
edges, i.e. where one endpoint lies strictly inside the
bracket pair and the other lies strictly outside, are
penalised, e.g., a task that combines edges A and B.
This behaviour can be implemented efficiently
when we assume that the computation of a task pri-
7A parsing task encodes the possible combination of a pas-
sive and an active chart edge.
8See e.g. (Prins and van Noord, 2001) for related work.
brxbrx
A
B C
D E
F
Figure 2: An example chart with a bracket pair of
type x. The dashed edges are active.
ority takes into account the priorities of the tasks it
builds upon. This guarantees that the effect of chang-
ing one task in the parsing process will propagate
to all depending tasks without having to check the
bracket conditions repeatedly.
For each task, it is sufficient to examine the start-
and endpoints of the building edges to determine if
its priority is affected by some bracket. Only four
cases can occur:
1. The new edge spans a pair of brackets: a match
2. The new edge starts or ends at one of the brack-
ets, but does not match: left or right hit
3. One bracket of a pair is at the joint of the build-
ing edges and a start- or endpoint lies strictly
inside the brackets: a crossing (edges A and B
in Fig. 2)
4. No bracket at the endpoints of both edges: use
the default priority
For left-/right-matching brackets, a match behaves
exactly like the corresponding left or right hit.
Computing the new priority If the priority of a
task is changed, the change is computed relative to
the default priority. We use two alternative confi-
dence values, and a hand-coded parameter (x), to
adjust the impact on the default priority heuristics.
conf
ent
(br
x
) specifies the confidence for a concrete
bracket pair br
x
of type x in a given sentence, based
on the tree entropy of the topological parse. conf
pr
specifies a measure of ?expected accuracy? for each
bracket type. Sec. 4 will introduce these measures.
The priority p(t) of a task t involving a bracket
br
x
is computed from the default priority ~p(t) by:
p(t) = ~p(t)  (1 conf
ent
(br
x
)  conf
pr
(x) (x))
4 Confidence Measures
This way of calculating priorities allows flexible pa-
rameterisation for the integration of bracket con-
straints. While the topological parser?s accuracy is
high, we need to reckon with (partially) wrong anal-
yses that could counter the expected performance
gains. An important factor is therefore the confi-
dence we can have, for any new sentence, into the
best parse delivered by the topological parser: If
confidence is high, we want it to be fully considered
for prioritisation ? if it is low, we want to lower its
impact, or completely ignore the proposed brackets.
We will experiment with two alternative confi-
dence measures: (i) expected accuracy of particular
bracket types extracted from the best parse deliv-
ered, and (ii) tree entropy based on the probability
distribution encountered in a topological parse, as
a measure of the overall accuracy of the best parse
proposed ? and thus the extracted brackets.9
4.1 Conf
pr
: Accuracy of map-constraints
To determine a measure of ?expected accuracy? for
the map constraints, we computed precision and re-
call for the 34 bracket types by comparing the ex-
tracted brackets from the suite of best delivered
topological parses against the brackets we extracted
from the trees in the manually annotated evalua-
tion corpus in (Becker and Frank, 2002). We obtain
88.3% precision, 87.8% recall for brackets extracted
from the best topological parse, run with TnT front
end. We chose precision of extracted bracket types
as a static confidence weight for prioritisation.
Precision figures are distributed as follows: 26.5%
of the bracket types have precision  90% (93.1%
in avg, 53.5% of bracket mass), 50% have pre-
cision  80% (88.9% avg, 77.7% bracket mass).
20.6% have precision  50% (41.26% in avg, 2.7%
bracket mass). For experiments using a threshold
on conf
pr
(x) for bracket type x, we set a threshold
value of 0.7, which excludes 32.35% of the low-
confidence bracket types (and 22.1% bracket mass),
and includes chunk-based brackets (see Section 5).
4.2 Conf
ent
: Entropy of Parse Distribution
While precision over bracket types is a static mea-
sure that is independent from the structural complex-
ity of a particular sentence, tree entropy is defined as
the entropy over the probability distribution of the
set of parsed trees for a given sentence. It is a use-
ful measure to assess how certain the parser is about
the best analysis, e.g. to measure the training utility
value of a data point in the context of sample selec-
tion (Hwa, 2000). We thus employ tree entropy as a
9Further measures are conceivable: We could extract brack-
ets from some n-best topological parses, associating them with
weights, using methods similar to (Carroll and Briscoe, 2002).
10
20
30
40
50
60
70
80
90
00.20.40.60.81
in
 %
Normalized entropy
precision
recall
coverage
Figure 3: Effect of different thresholds of normal-
ized entropy on precision, recall, and coverage
confidence measure for the quality of the best topo-
logical parse, and the extracted bracket constraints.
We carry out an experiment to assess the effect
of varying entropy thresholds  on precision and re-
call of topological parsing, in terms of perfect match
rate, and show a way to determine an optimal value
for . We compute tree entropy over the full prob-
ability distribution, and normalise the values to be
distributed in a range between 0 and 1. The normali-
sation factor is empirically determined as the highest
entropy over all sentences of the training set.10
Experimental setup We randomly split the man-
ually corrected evaluation corpus of (Becker and
Frank, 2002) (for sentence length  40) into a train-
ing set of 600 sentences and a test set of 408 sen-
tences. This yields the following values for the train-
ing set (test set in brackets): initial perfect match
rate is 73.5% (70.0%), LP 88.8% (87.6%), and LR
88.5% (87.8%).11 Coverage is 99.8% for both.
Evaluation measures For the task of identifying
the perfect matches from a set of parses we give the
following standard definitions: precision is the pro-
portion of selected parses that have a perfect match
? thus being the perfect match rate, and recall is the
proportion of perfect matches that the system se-
lected. Coverage is usually defined as the proportion
of attempted analyses with at least one parse. We ex-
tend this definition to treat successful analyses with
a high tree entropy as being out of coverage. Fig. 3
shows the effect of decreasing entropy thresholds
 on precision, recall and coverage. The unfiltered
set of all sentences is found at =1. Lowering  in-
10Possibly higher values in the test set will be clipped to 1.
11Evaluation figures for this experiment are given disregard-
ing parameterisation (and punctuation), corresponding to the
first row of figures in table 1.
82
84
86
88
90
92
94
96
0.160.180.20.220.240.260.280.3
in
 %
Normalized entropy
precision
recall
f-measure
Figure 4: Maximise f-measure on the training set to
determine best entropy threshold
creases precision, and decreases recall and coverage.
We determine f-measure as composite measure of
precision and recall with equal weighting (=0.5).
Results We use f-measure as a target function on
the training set to determine a plausible . F-measure
is maximal at =0.236 with 88.9%, see Figure 4.
Precision and recall are 83.7% and 94.8% resp.
while coverage goes down to 83.0%. Applying the
same  on the test set, we get the following results:
80.5% precision, 93.0% recall. Coverage goes down
to 80.6%. LP is 93.3%, LR is 91.2%.
Confidence Measure We distribute the comple-
ment of the associated tree entropy of a parse tree tr
as a global confidence measure over all brackets br
extracted from that parse: conf
ent
(br) = 1 ent(tr).
For the thresholded version of conf
ent
(br), we set
the threshold to 1   = 1  0:236 = 0:764.
5 Experiments
Experimental Setup In the experiments we use
the subset of the NEGRA corpus (5060 sents,
24.57%) that is currently parsed by the HPSG gram-
mar.12 Average sentence length is 8.94, ignoring
punctuation; average lexical ambiguity is 3.05 en-
tries/word. As baseline, we performed a run with-
out topological information, yet including PoS pri-
oritisation from tagging.13 A series of tests explores
the effects of alternative parameter settings. We fur-
ther test the impact of chunk information. To this
12This test set is different from the corpus used in Section 4.
13In a comparative run without PoS-priorisation, we estab-
lished a speed-up factor of 1.13 towards the baseline used in
our experiment, with a slight increase in coverage (1%). This
compares to a speed-up factor of 2.26 reported in (Daum et al,
2003), by integration of PoS guidance into a dependency parser.
end, phrasal fields determined by topological pars-
ing were fed to the chunk parser of (Skut and Brants,
1998). Extracted NP and PP bracket constraints are
defined as left-matching bracket types, to compen-
sate for the non-embedding structure of chunks.
Chunk brackets are tested in conjunction with topo-
logical brackets, and in isolation, using the labelled
precision value of 71.1% in (Skut and Brants, 1998)
as a uniform confidence weight.14
Measures For all runs we measure the absolute
time and the number of parsing tasks needed to com-
pute the first reading. The times in the individual
runs were normalised according to the number of
executed tasks per second. We noticed that the cov-
erage of some integrated runs decreased by up to
1% of the 5060 test items, with a typical loss of
around 0.5%. To warrant that we are not just trading
coverage for speed, we derived two measures from
the primary data: an upper bound, where we asso-
ciated every unsuccessful parse with the time and
number of tasks used when the limit of 70000 pas-
sive edges was hit, and a lower bound, where we
removed the most expensive parses from each run,
until we reached the same coverage. Whereas the
upper bound is certainly more realistic in an applica-
tion context, the lower bound gives us a worst case
estimate of expectable speed-up.
Integration Parameters We explored the follow-
ing range of weighting parameters for prioritisation
(see Section 3.3 and Table 2).
We use two global settings for the heuristic pa-
rameter . Setting  to 1
2
without using any confi-
dence measure causes the priority of every affected
parsing task to be in- or decreased by half its value.
Setting  to 1 drastically increases the influence of
topological information, the priority for rewarded
tasks is doubled and set to zero for penalized ones.
The first two runs (rows with  P  E) ignore
both confidence parameters (conf
pr=ent
=1), measur-
ing only the effect of higher or lower influence of
topological information. In the remaining six runs,
the impact of the confidence measures conf
pr=ent
is
tested individually, namely +P  E and  P +E, by
setting the resp. alternative value to 1. For two runs,
we set the resp. confidence values that drop below
a certain threshold to zero (PT, ET) to exclude un-
14The experiments were run on a 700 MHz Pentium III ma-
chine. For all runs, the maximum number of passive edges was
set to the comparatively high value of 70000.
factor msec (1st) tasks
low-b up-b low-b up-b low-b up-b
Baseline     524 675 3813 4749
Integration of topological brackets w/ parameters
 P  E  1
2
2.21 2.17 237 310 1851 2353
 P  E 1 2.04 2.10 257 320 2037 2377
+P  E  1
2
2.15 2.21 243 306 1877 2288
PT  E  1
2
2.20 2.30 238 294 1890 2268
 P +E  1
2
2.27 2.23 230 302 1811 2330
 P ET  1
2
2.10 2.00 250 337 1896 2503
+P  E 1 2.06 2.12 255 318 2021 2360
PT  E 1 2.08 2.10 252 321 1941 2346
PT with chunk and topological brackets
PT  E  1
2
2.13 2.16 246 312 1929 2379
PT with chunk brackets only
PT  E  1
2
0.89 1.10 589 611 4102 4234
Table 2: Priority weight parameters and results
certain candidate brackets or bracket types. For runs
including chunk bracketing constraints, we chose
thresholded precision (PT) as confidence weights
for topological and/or chunk brackets.
6 Discussion of Results
Table 2 summarises the results. A high impact on
bracket constraints (1) results in lower perfor-
mance gains than using a moderate impact ( 1
2
)
(rows 2,4,5 vs. 3,8,9). A possible interpretation is
that for high , wrong topological constraints and
strong negative priorities can mislead the parser.
Use of confidence weights yields the best per-
formance gains (with  1
2
), in particular, thresholded
precision of bracket types PT, and tree entropy
+E, with comparable speed-up of factor 2.2/2.3 and
2.27/2.23 (2.25 if averaged). Thresholded entropy
ET yields slightly lower gains. This could be due to
a non-optimal threshold, or the fact that ? while pre-
cision differentiates bracket types in terms of their
confidence, such that only a small number of brack-
ets are weakened ? tree entropy as a global measure
penalizes all brackets for a sentence on an equal ba-
sis, neutralizing positive effects which ? as seen in
+/ P ? may still contribute useful information.
Additional use of chunk brackets (row 10) leads
to a slight decrease, probably due to lower preci-
sion of chunk brackets. Even more, isolated use of
chunk information (row 11) does not yield signifi-
01000
2000
3000
4000
5000
6000
7000
0 5 10 15 20 25 30 35
baseline
+PT ?(0.5)
12867 12520 11620 9290
0
100
200
300
400
500
600
#sentences
msec
Figure 5: Performance gain/loss per sentence length
cant gains over the baseline (0.89/1.1). Similar re-
sults were reported in (Daum et al, 2003) for inte-
gration of chunk- and dependency parsing.15
For PT -E  1
2
, Figure 5 shows substantial per-
formance gains, with some outliers in the range of
length 25?36. 962 sentences (length >3, avg. 11.09)
took longer parse time as compared to the baseline
(with 5% variance margin). For coverage losses, we
isolated two factors: while erroneous topological in-
formation could lead the parser astray, we also found
cases where topological information prevented spu-
rious HPSG parses to surface. This suggests that
the integrated system bears the potential of cross-
validation of different components.
7 Conclusion
We demonstrated that integration of shallow topo-
logical and deep HPSG processing results in signif-
icant performance gains, of factor 2.25?at a high
level of deep parser efficiency. We show that macro-
structural constraints derived from topological pars-
ing improve significantly over chunk-based con-
straints. Fine-grained prioritisation in terms of con-
fidence weights could further improve the results.
Our annotation-based architecture is now easily
extended to address robustness issues beyond lexical
matters. By extracting spans for clausal fragments
from topological parses, in case of deep parsing fail-
15(Daum et al, 2003) report a gain of factor 2.76 relative to a
non-PoS-guided baseline, which reduces to factor 1.21 relative
to a PoS-prioritised baseline, as in our scenario.
ure the chart can be inspected for spanning anal-
yses for sub-sentential fragments. Further, we can
simplify the input sentence, by pruning adjunct sub-
clauses, and trigger reparsing on the pruned input.
References
M. Becker and A. Frank. 2002. A Stochastic Topological
Parser of German. In Proceedings of COLING 2002,
pages 71?77, Taipei, Taiwan.
T. Brants. 2000. Tnt - A Statistical Part-of-Speech Tag-
ger. In Proceedings of Eurospeech, Rhodes, Greece.
U. Callmeier. 2000. PET ? A platform for experimenta-
tion with efficient HPSG processing techniques. Nat-
ural Language Engineering, 6 (1):99 ? 108.
C. Carroll and E. Briscoe. 2002. High precision extrac-
tion of grammatical relations. In Proceedings of COL-
ING 2002, pages 134?140.
B. Crysmann, A. Frank, B. Kiefer, St. Mu?ller, J. Pisko-
rski, U. Scha?fer, M. Siegel, H. Uszkoreit, F. Xu,
M. Becker, and H.-U. Krieger. 2002. An Integrated
Architecture for Deep and Shallow Processing. In
Proceedings of ACL 2002, Pittsburgh.
M. Daum, K.A. Foth, and W. Menzel. 2003. Constraint
Based Integration of Deep and Shallow Parsing Tech-
niques. In Proceedings of EACL 2003, Budapest.
D. Duchier and R. Debusmann. 2001. Topological De-
pendency Trees: A Constraint-based Account of Lin-
ear Precedence. In Proceedings of ACL 2001.
C. Grover and A. Lascarides. 2001. XML-based data
preparation for robust deep parsing. In Proceedings of
ACL/EACL 2001, pages 252?259, Toulouse, France.
T. Ho?hle. 1983. Topologische Felder. Unpublished
manuscript, University of Cologne.
R. Hwa. 2000. Sample selection for statistical gram-
mar induction. In Proceedings of EMNLP/VLC-2000,
pages 45?52, Hong Kong.
S. Mu?ller and W. Kasper. 2000. HPSG analysis of
German. In W. Wahlster, editor, Verbmobil: Founda-
tions of Speech-to-Speech Translation, Artificial Intel-
ligence, pages 238?253. Springer, Berlin.
R. Prins and G. van Noord. 2001. Unsupervised pos-
tagging improves parsing accuracy and parsing effi-
ciency. In Proceedings of IWPT, Beijing.
U. Scha?fer. 2003. WHAT: An XSLT-based Infrastruc-
ture for the Integration of Natural Language Process-
ing Components. In Proceedings of the SEALTS Work-
shop, HLT-NAACL03, Edmonton, Canada.
H. Schmid, 2000. LoPar: Design and Implementation.
IMS, Stuttgart. Arbeitspapiere des SFB 340, Nr. 149.
W. Skut and T. Brants. 1998. Chunk tagger: statistical
recognition of noun phrases. In ESSLLI-1998 Work-
shop on Automated Acquisition of Syntax and Parsing.
H. Uszkoreit. 2002. New Chances for Deep Linguistic
Processing. In Proceedings of COLING 2002, pages
xiv?xxvii, Taipei, Taiwan.
Corpus-based Induction of an LFG Syntax-Semantics Interface
for Frame Semantic Processing
Anette Frank
Language Technology Lab
DFKI GmbH
Stuhlsatzenhausweg 3
66123 Saarbru?cken, Germany
Anette.Frank@dfki.de
Jir??? Semecky?
Institute of Formal and Applied Linguistics
Charles University
Malostranske? na?me?st?? 25
11800 Prague, Czech Republic
semecky@ufal.ms.mff.cuni.cz
Abstract
We present a method for corpus-based induc-
tion of an LFG syntax-semantics interface for
frame semantic processing in a computational
LFG parsing architecture. We show how to
model frame semantic annotations in an LFG
projection architecture, including special phe-
nomena that involve non-isomorphic mappings
between levels. Frame semantic annotations
are ported from a manually annotated corpus
to a ?parallel? LFG corpus. We extract func-
tional descriptions from the frame-annotated
LFG corpus, to derive general frame assign-
ment rules that can be applied to new sentences.
We evaluate the results by applying the induced
frame assignment rules to LFG parser output.1
1 Introduction
There is a growing insight that high-quality NLP
applications for information access are in need of
deeper, in particular, semantic analysis. A bottle-
neck for semantic processing is the lack of large
domain-independent lexical semantic resources.
There are now efforts for the creation of large lex-
ical semantic resources that provide information on
predicate-argument structure. FrameNet (Baker et
al., 1998), building on Fillmore?s theory of frame
semantics, provides definitions of frames and their
semantic roles, a lexical database and a manually
annotated corpus of example sentences. A strictly
corpus-based approach is carried out with ?Prop-
Bank? ? a manual predicate-argument annotation on
top of the Penn treebank (Kingsbury et al, 2002).
First approaches for learning stochastic models
for semantic role assignment from annotated cor-
pora have emerged with Gildea and Jurafsky (2002)
and Fleischman et al (2003). While current com-
petitions explore the potential of shallow parsing
1The research reported here was conducted in a coopera-
tion project of the German Research Center for Artificial Intel-
ligence, DFKI Saarbru?cken with the Computational Linguistics
Department of the University of the Saarland at Saarbru?cken.
for role labelling, Gildea and Palmer (2002) empha-
sise the role of deeper syntactic analysis for seman-
tic role labelling. We follow this line and explore
the potential of deep syntactic analysis for role la-
belling, choosing Lexical Functional Grammar as
underlying syntactic framework. We aim at a com-
putational interface for frame semantics processing
that can be used to (semi-)automatically extend the
size of current training corpora for learning stochas-
tic models for role labelling, and ? ultimately ? as a
basis for automatic frame assignment in NLP tasks,
based on the acquired stochastic models.
We discuss advantages of semantic role assign-
ment on the basis of functional syntactic analy-
ses as provided by LFG parsing, and present an
LFG syntax-semantics interface for frame seman-
tics, building on a first study in Frank and Erk
(2004). In the present paper we focus on the corpus-
based induction of a computational LFG interface
for frame semantics from a semantically annotated
corpus. We describe the methods used to derive an
LFG-based frame semantic lexicon, and discuss the
treatment of special (since non-isomorphic) map-
pings in the syntax-semantics interface. Finally, we
apply the acquired frame assignment rules in a com-
putational LFG parsing architecture.
The paper is structured as follows. Section 2
gives some background on the semantically anno-
tated corpus we are using, and the LFG resources
that provide the basis for automatic frame assign-
ment. In Section 3 we discuss advantages of deeper
syntactic analysis for a principle-based syntax-
semantics interface for semantic role labelling. We
present an LFG interface for frame semantics which
we realise in a modular description-by-analysis ar-
chitecture. Section 4 describes the method we apply
to derive frame assignment rules from corpus anno-
tations: we port the frame annotations to a ?paral-
lel? LFG corpus and induce general LFG frame as-
signment rules, by extracting syntactic descriptions
for the frame constituting elements. We use LFG?s
functional representations to distinguish local and
non-local role assignments. The derived frame as-
SPD requests that coalition talk about reform
Figure 1: SALSA/TIGER frame annotation
signment rules are reapplied to the original syntac-
tic LFG corpus to control the results. In Section 5
we apply and evaluate the frame projection rules in
an LFG parsing architecture. In Section 6 we sum-
marise our results and discuss future directions.
2 Corpus and Grammar Resources
Frame Semantic Corpus Annotations The basis
for our work is a corpus of manual frame annota-
tions, the SALSA/TIGER corpus (Erk et al, 2003).2
The annotation follows the FrameNet definitions of
frames and their semantic roles.3 Underlying this
corpus is a syntactically annotated corpus of Ger-
man newspaper text, the TIGER treebank (Brants
et al, 2002). TIGER syntactic annotations consist
of relatively flat constituent graph representations,
with edge labels that indicate functional informa-
tion, such as head (HD), subject (SB), cf. Figure 1.
The SALSA frame annotations are flat graphs
connected to syntactic constituents. Figure 1 dis-
plays frame annotations where the REQUEST frame
is triggered by the (discontinuous) frame evoking el-
ement (FEE) fordert ... auf (requests). The seman-
tic roles (or frame elements, FEs) are represented as
labelled edges that point to syntactic constituents in
the TIGER syntactic annotation: the noun SPD for
the SPEAKER, Koalition for the ADDRESSEE, and
the PP zu Gespra?ch u?ber Reform for the MESSAGE.
LFG Grammar Resources We aim at a computa-
tional syntax-semantics interface for frame seman-
tics, to be used for (semi-)automatic corpus annota-
tion for training of stochastic role assignment mod-
els, and ultimately as a basis for automatic frame as-
signment. As a grammar resource we chose a wide-
coverage computational LFG grammar for German
(developed at IMS, University of Stuttgart). This
German LFG grammar has already been used for
semi-automatic syntactic annotation of the TIGER
corpus, with reported coverage of 50%, and 70%
2http://www.coli.uni-sb.de/lexicon
3See http://www.icsi.berkeley.edu/?framenet
precision (Brants et al, 2002). The grammar runs
on the XLE grammar processing platform, which
provides stochastic training and online disambigua-
tion packages. Currently, the grammar is further ex-
tended, and will be enhanced with stochastic disam-
biguation, along the lines of (Riezler et al, 2002).
LFG Corpus Resource Next to the German LFG
grammar, (Forst, 2003) has derived a ?parallel? LFG
f-structure corpus from the TIGER treebank, by ap-
plying methods for treebank conversion. We make
use of the parallel treebank to induce LFG frame an-
notation rules from the SALSA/TIGER annotations.
3 LFG for Frame Semantics
Lexical Functional Grammar (Bresnan, 2001)
assumes multiple levels of representation. Most
prominent are the syntactic representations of
c(onstituent)- and f(unctional)-structure. The corre-
spondence between c- and f-structure is defined by
functional annotations of rules and lexical entries.
This architecture can be extended to semantics pro-
jection (Halvorsen and Kaplan, 1995).
LFG f-structure representations abstract away
from surface-syntactic properties, by localising ar-
guments in mid- and long-distance constructions,
and therefore allow for uniform reference to syntac-
tic dependents in diverse syntactic configurations.
This is important for the task of frame annotation,
as it abstracts away from aspects of syntax that are
irrelevant to frame (element) assignment.
In (1), e.g., the SELLER role can be uniformly as-
sociated with the local SUBJect of sell, even though
it is realized as (a.) a relative pronoun of come that
controls the SUBJect of sell, (b.) an implicit second
person SUBJ, (c.) a non-overt SUBJ controlled by
the OBLique object of hard, and (d.) a SUBJ (we) in
VP coordination.
(1) a. The woman who had come in to sell flowers
overheard their conversation.
b. Don?t sell the factory to another company.
c. It would be hard for him to sell newmont shares.
d. .. we decided to sink some of our capital, buy a
car, and sell it again before leaving.
LFG Semantics Projection for Frames As in a
standard LFG projection architecture, we define a
frame semantics projection ?f from the level of f-
structure. We define the ?f ?projection to introduce
elementary frame structures, with attributes FRAME,
FEE (frame-evoking element), and frame-specific
role attributes. Figure 2 displays the ?f?projection
for the sentence in Figure 1.4
4The MESSAGE role is coindexed with a lower frame, the
frame projection introduced by the noun Gespr a?ch.
??
?
?
?
?
?
?
?
?
PRED ?AUFFORDERN?(SUBJ)(OBJ)(OBL)??
SUBJ
[
PRED ?SPD?
]
OBJ
[
PRED ?KOALITION?
]
OBL
?
?
?
?
PRED ?ZU?(OBJ)??
OBJ
?
?
PRED ?GESPRA?CH?
ADJ
[
PRED ?U?BER?(OBJ)??
OBJ
[
PRED ?REFORM?
]
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?f
?
?
?
?
?
FRAME REQUEST
FEE AUFFORDERN
SPEAKER [ ]
ADDRESSEE [ ]
MESSAGE [ ]
?
?
?
?
?
?
?
?
FRAME CONVERSATION
FEE GESPRA?CH
INTERLOCUTOR 1 [ ]
TOPIC [ ]
?
?
?
Figure 2: LFG projection architecture for Frame Semantics
auffordern V,
(?PRED)=?AUFFORDERN?(?SUBJ)(?OBJ)(?OBL)??
...
(?f (?) FRAME) = REQUEST
(?f (?) FEE) = (? PRED FN)
(?f (?) SPEAKER) = ?f (? SUBJ)
(?f (?) ADDRESSEE) = ?f (? OBJ)
(?f (?) MESSAGE) = ?f (? OBL OBJ)
Figure 3: Frame projection by co-description
Figure 3 states the lexical entry for the REQUEST
frame. ?f is a function of f-structure. The verb
auffordern introduces a node ?f (?) in the semantics
projection of ?, its local f-structure, and defines its
attributes FRAME and FEE. The frame elements are
defined as ?f?projections of the verb?s SUBJ, OBJ
and OBL OBJ functions. E.g. the SPEAKER role,
referred to as (?f (?) SPEAKER), the SPEAKER at-
tribute in the projection ?f (?) of ?, is defined as
identical to the ?f?projection of the verb?s SUBJ,
?f (? SUBJ).
Frames in Context The projection of frames in
context can yield connected frame structures. In
Figure 2, Gespra?ch fills the MESSAGE role of
REQUEST, but it also introduces a frame of its
own, CONVERSATION. Thus, the CONVERSATION
frame, by coindexation, is an instantiation, in con-
text, of the MESSAGE of REQUEST.
Co-description vs. description-by-analysis In
the co-description architecture we just presented
f- and s-structure equations jointly determine the
valid analyses of a sentence. Analyses that do
not satisfy both f- and s-structure constraints are
inconsistent and ruled out.
An alternative to co-description is semantics
construction via description-by-analysis (DBA)
(Halvorsen and Kaplan, 1995). Here, semantics
is built on top of fully resolved f-structures. F-
structures that are consistent with semantic mapping
constraints are semantically enriched ? remaining
analyses are left untouched.
Both models are equally powerful ? yet while co-
pred(X,auffordern),
subj(X,A), obj(X,B), obl(X,C), obj(C,D)
==>
+?sf ::?(X,SemX), +frame(SemX,request),
+fee(X,auffordern),
+?sf ::?(A,SemA), +speaker(SemX,SemA),
+?sf ::?(B,SemB), +addressee(SemX,SemB),
+?sf ::?(D,SemD), +message(SemX,SemD).
Figure 4: Frame projection by DBA (via transfer)
description integrates the semantics projection into
the grammar and parsing process, DBA keeps it as a
separate module. Thus, with DBA, semantics does
not interfere with grammar design and can be de-
veloped separately. The DBA approach also facili-
tates the integration of external semantic knowledge
sources (such as word senses or named entity types).
DBA by transfer We realise the DBA approach
by way of a term-rewriting transfer system that is
part of the XLE grammar processing platform. The
system represents f-structures as sets of predicates
which take as arguments variables for f-structure
nodes or atomic values. Transfer is defined as a
sequence of ordered rules. If a rule applies to an
input set of predicates, it defines a new output set.
This output set is input to the next rule in the cas-
cade. A rule applies if all terms on its left-hand side
match some term in the input set. The terms on the
right hand side (prefixed ?+?) are added to the in-
put set. There are obligatory (==>) and optional
(?=>) rules. Optional rules introduce two output
sets: one results from application of the rule, the
other is equal to the input set.
Figure 4 displays a transfer rule that corresponds
to the co-description lexical entry of Figure 3. For
matched f-structure nodes (pred, subject, object,
oblique object) it defines a ?f?projection (by pred-
icate ?s::f ?) with new s-structure nodes. For these,
we define the frame information (FRAME, FEE) and
the linking of semantic roles (e.g., the ?f?projection
SemA of the SUBJ is defined as the SPEAKER role
of the head?s semantic projection SemX).
Frame FeeID Role(s) FeID(s)
Request 2 (from {2, 8}) Speaker 1
Addressee 3
Message 501
Figure 5: Core frame information for Fig. 1
% projecting frame information for FEE
project fee(FeeID, Frame) ::
ti-id(X,FeeID), pred(X,Pred) ==>
+?s::?(X,S X), +frame(S X,Frame), +fee(S X,Pred).
% semantic projection for (each) FE of FEE
project fe of fee(FeeID, Frame, FeID, Role) ::
ti-id(X,FeeID), ?s::?(X,S X), frame(S X,Frame),
ti-id(Y,FeID), pred(Y,Pred) ==>
+?s::?(Y,S Y), +Role(S X,S Y), +rel(S Y,Pred).
Figure 6: SALSA-2-LFG-TIGER transfer
4 Corpus-based induction of an LFG
frame semantics interface
4.1 Porting SALSA annotations to LFG
A challenge for corpus-based induction of a syntax-
semantics interface for frame assignment is the
transposition of the corpus annotations from a given
syntactic annotation scheme to the target syntactic
framework. The basis for our work are annotations
of the SALSA/TIGER corpus (Erk et al, 2003), en-
coded in an XML annotation scheme that extends
the syntactic TIGER XML annotation scheme.
The TIGER treebank has been converted to a par-
allel LFG f-structure corpus (Forst, 2003). The
SALSA/TIGER and LFG-TIGER corpora could be
used to learn corresponding syntactic paths in the
respective structures. Thus, we could establish
the paths of frame constituting elements in the
SALSA/TIGER corpus, and port the annotations to
the corresponding path in the LFG-TIGER corpus.
However, we could apply a more precise method,
by exploiting the fact that the LFG-TIGER cor-
pus preserves the original TIGER constituent iden-
tifiers, as f-structure features TI-ID (see Fig. 7). We
use these ?anchors? to port the SALSA annotations
to the parallel LFG-TIGER treebank. Thus, in a
first step we extend the latter to an LFG corpus with
frame semantics projection. From the extended cor-
pus we induce general LFG frame assignment rules.
This will be described in more detail in Section 4.2.
Porting annotations by transfer For each sen-
tence we extract the constituent identifiers of frame
constituting elements in the SALSA XML annota-
tions (cf. Figure 5). This information is coded into
transfer rules, where we refer to the corresponding
TI-ID features in the f-structure as anchors to project
the frame information for a given frame annotation
Figure 7: LFG-TIGER f-structure (w/ TI-ID)
Figure 8: Frame projection from f-str of Fig. 7
instance. The first transfer rule (template) in Figure
6 defines the semantic projection of the FEE, where
the correct f-structure location is referenced by the
feature TI-ID. Subsequent rules ? one for each role
to be assigned ? define the given semantic role as an
argument of the FEE?s semantic projection, again
using the TI-IDs of the FEE and FE as anchors.
We generate these frame projection rules for each
sentence in the SALSA/TIGER corpus, and apply
them to the corresponding f-structure in the LFG-
TIGER corpus. The result is an LFG corpus with
frame semantic anntations (cf. Figures 7 and 8).
The basic structure of frame-inducing rules in
Figure 6 was refined to account for special cases:
Coordination For frame elements that corre-
spond to coordinated constituents, as in Figure 9, we
project a semantic role that records a set of semantic
predicates (REL), one for each of the conjuncts.
Beamten, Politikern und Gescha?ftsleuten wird
Schmiergeld bezahlt ? Clerks, politicians and
businessmen are payed bribes
Figure 9: Frame with coordinated RECVR role
Underspecification The SALSA annotation
scheme allows for underspecification, to represent
unresolved word sense ambiguities or optionality
(Erk et al, 2003). In a given context, a predicate
may evoke alternative frames (i.e. word senses),
where it is impossible to decide between them.
E.g. the verb verlangen (demand) may convey
the meaning of REQUEST, but also COMMERCIAL
TRANSACTION. Such cases are annotated with
Figure 10: Underspecification as disjunction
4 Artikel gingen u?ber die Ladentheke?4 items were sold
Figure 11: Multiword expressions
alternative frames, which are marked as elements of
an ?underspecification group?. Underspecification
may also affect frame elements of a single frame.
A motion (Antrag), e.g., may be both MEDIUM and
SPEAKER of a REQUEST. Finally, a constituent
may or may not be interpreted as a frame element
of a given frame. It is then represented as a single
element of an underspecification group.
We model underspecification as disjunction,
which is encoded by optional transfer rules that cre-
ate alternative (disjunctive) contexts. Optionality is
modeled by a single optional rule. Figure 10 dis-
plays the result of underspecified frame element as-
signment in an f-structure chart (Maxwell and Ka-
plan, 1989). Context c1 displays the reading where
Antrag is assigned the SPEAKER role, alternatively,
in context c2, it is assigned the role MEDIUM.
In a symbolic account disjunction doesn?t cor-
rectly model the intended meaning of underspecifi-
cation. Yet, a stochastic model for frame assignment
should render the vagueness involved in underspec-
ification by close stochastic weights. Thus, under-
specified annotation instances provide alternative
frames in the training data and can be used for fine-
grained evaluation of frame assignment models.
Multiword Expressions The treatment of mul-
tiword expressions (idioms, support constructions)
requires special care. For idioms, the constituting
elements are annotated as multiple frame evoking
elements (cf. Figure 11 for u?ber die Ladentheke
gehen ? go over the counter (being sold)). We de-
fine semantic projections for the individual compo-
nents: the main frame evoking predicate (FEE) and
the idiom-constituting words, which are recorded in
a set-valued feature FEE-MWE. Otherwise, idioms
are treated like ordinary main verbs. E.g., like sell,
the expression triggers a COMMERCE SELL frame
with the appropriate semantic roles, here GOODS.
Asymmetric Embedding Another type of non-
isomorphism between syntactic and semantic rep-
Figure 12: Asymmetric embedding (example (2))
resentation occurs in cases where distinct syntactic
constituents are annotated as instantiation of a sin-
gle semantic role. In (2), PP and NP are annotated
as the MESSAGE of a STATEMENT, since they jointly
convey its content. Projecting distinct constituents
to a single semantic node can, however, lead to in-
consistencies, especially if both constituents inde-
pendently project semantic frames.
(2) Der Geschaeftsfuehrer gab [PP?MO als Grund
fuer die Absage] [NP?OBJ Terminnoete] an.
The director mentioned [time conflicts] [as a
reason for cancelling the appointment]
In the SALSA annotations asymmetric embedding
at the semantic level is the typical pattern for such
double-constituent annotations. I.e., for (2), we
assume a target frame structure where the MES-
SAGE of STATEMENT points to the PP ? which it-
self projects a frame REASON with semantic roles
CAUSE for Terminno?te, and EFFECT for Absage.
Such multiple-constituent annotations arise in
cases where frame annotations are partial: since
corpus annotation proceeds frame-wise, in (2) the
REASON frame may not have been treated yet.
Moreover, annotators are in general not shown com-
plete(d) sentence annotations.
We account for these cases by a simulation of
functional uncertainty equations, which accommo-
date for a potential embedded frame within either
one of the otherwise re-entrant constituents. We ap-
ply a transfer rule set that embeds one (or the other)
of the two constituent projections as an embedded
role of an unknown frame, to be evoked by the re-
spective ?dominating? node. We introduce an ?un-
known? role ROLE? for the embedded constituent,
which is to be interpreted as a functional uncertainty
path over variable semantic roles.
Figure 12 displays the alternative (hypothetical)
frame structures for (2), where the second one ?
with FRAME instantiated to REASON and ROLE? to
CAUSE ? corresponds to the actual reading.
Overview of data Our current data set comprises
12436 frame annotations for 11934 sentences. Ta-
ble 1 gives frequency figures for the special phe-
coord usp mwe asym >dbl all
abs 467 395 1287 421 97 12436
in % 3.76 3.18 10.34 3.39 0.78 100
Table 1: Overview of special annotation types
nomena: coordination, underspecification, multi-
word expressions and double constituents (asym).5
We successfully ported 11713 frame annotations
to the LFG-TIGER corpus, turning it into an LFG
corpus with frame annotations.
4.2 Inducing frame projection rules
From the enriched corpus we extract lexical frame
assignment rules that ? instead of node identifiers ?
use f-structure descriptions to identify constituents
and map them to frame semantic roles. These rules
can then be applied to the f-structure output of free
LFG parsing, i.e. to novel sentences.
We designed an algorithm for extracting f-struc-
ture paths between pairs of f-structure nodes that
correspond to the s-structure of the frame evoking
element and one of its semantic roles, respectively.
Table 2 gives an example for the frame projection
in Figure 13. Starting from the absolute f-structure
path (f-path) for (the f-structure projecting to) the
FEE MITTEILEN we extract relative f-paths leading
to the roles MESSAGE and SPEAKER. The f-path for
the MESSAGE (?OBJ) is local to the f-structure that
projects to the FEE. For the SPEAKER we identify
two paths: one local, the other non-local. The local
f-path (?SUBJ) leads to the local SUBJ of mitteilen
in Figure 13. By co-indexation with the SUBJ of
versprechen we find an alternative non-local path,
which we render as an inside-out functional equa-
tion ((XCOMP?) SUBJ).
Since f-structures are directed acyclic graphs, we
use graph accessibility to distinguish local from
non-local f-paths. In case of alternative local and
non-local paths, we choose the local one. From al-
ternative non-local paths, we chose the one(s) with
shortest inside-out subexpression.
Generating frame assignment rules We ex-
tracted f-path descriptions for frame assignment
from the enriched LFG-TIGER corpus. We com-
piled 9707 lexicalised frame assignment rules in the
format of Figure 4. The average number of distinct
assignment rules per FEE is 8.38. Abstracting over
the FEEs, we obtain 7317 FRAME-specific rules,
with an average of 41.34 distinct rules per frame.
Due to the surface-oriented TIGER annotation
format, the original annotations contain a high num-
ber of non-local frame element assignments that
5Role assignment to more than two constituents (>dbl) con-
stitute a rather disparate set of data we do not try to cover.
?
?
FRAME COMMUNICATION
FEE MITTEILEN
SPEAKER [ ]
MESSAGE [ ]
?
?
?
?
?
?
?
?
?
?
PRED VERSPRECHEN
SUBJ
[
PRED SPD
]
OBJ2
[
PRED WA?HLER
]
XCOMP
?
?
PRED MITTEILEN
SUBJ [ ]
OBJ
[
PRED BESCHLUSS
]
?
?
?
?
?
?
?
?
?
?
SPD verspricht Wa?hlern, Beschlu?sse mitzuteilen
SPD promises voters to report decisions
Figure 13: Local and non-local frame elements
absolute f-path relative f-path
FEE XCOMP PRED ?
MSG XCOMP OBJ ?OBJ local
SPKR SUBJ (XCOMP?)SUBJ nonlocal
XCOMP SUBJ ?SUBJ local
Table 2: Local and nonlocal path equations
are localised in LFG f-structures. The f-paths ex-
tracted from the enriched LFG corpus yield 12.82%
non-local (inside-out) vs. 87.18% local (outside-in)
frame element assignment rules.
As an alternative rule format, we split frame as-
signment into separate rules for projection of the
FEE and the individual FEs. This allows assignment
rules to apply in cases where the f-structure does not
satisfy the functional constraints for some FE. This
yields improved robustness, and accounts for syn-
tactic variability when applied to new data. For this
rule format, we obtain 960 FEE assignment rules,
and 8261 FEE-specific FE assignment rules. Ab-
stracting over the FEE, this reduces to 4804 rules.6
4.3 Reapplying frame assignment rules
We reapplied the induced frame assignment rules to
the original syntactic LFG-TIGER corpus, to con-
trol the results. The results are evaluated against the
frame-enriched LFG-TIGER corpus that was cre-
ated by explicit node anchoring (Sec. 4.1). We ap-
plied ?full frame rules? that introduce FEE and all
FEs in a single rule, as well as separated FEE and
FE rules. We applied all rules for a given frame to
any sentences that had received the same frame in
the corpus. We obtained 93.98% recall with 25.95%
precision (full frame rules), and 94.98% recall with
45.52% precision (split rules), cf. Table 3.a. The
low precision is due to overgeneration of the more
general abstracted rules, which are not yet con-
trolled by statistical selection. We measured an am-
biguity of 8.46/7.83 frames per annotation instance.
6In the future we will experiment with assignment rules that
are not conditioned to FEEs, but to frame-specific syntactic de-
scriptions, to assign frames to ?unknown? lexical items.
full frame rules FEE and FE rules
rec prec amb rec prec amb
(a) 93.98 25.95 8.46 94.98 45.52 7.83
(b) 52.21 6.93 13.35 76.41 18.32 9.00
Table 3: Evaluation of annotation results:
(a) on TIGER corpus, (b) on LFG parses
5 Applying frame assignment rules in an
LFG parsing architecture
We finally apply the frame assignment rules to orig-
inal LFG parses of the German LFG grammar. The
grammar produces f-structures that are compatible
with the LFG-TIGER corpus, thus the syntactic
constraints can match the parser?s f-structure output.
In contrast to the LFG-TIGER corpus, the grammar
delivers f-structures for alternative syntactic analy-
ses. We don?t expect frame projections for all syn-
tactic readings, but where they apply, they will cre-
ate ambiguity in the semantics projection.
We applied the rules to the parses of 6032 corpus
sentences. Compared to the LFG-TIGER corpus we
obtain lower recall and precision (Table 3.b) and a
higher ambiguity rate per sentence. Drop in preci-
sion and higher ambiguity are due to the higher am-
biguity in the syntactic input. Moreover, we now ap-
ply the complete rule set to any given sentence. The
rules can thus apply to new annotation instances,
and create more ambiguity. The drop in recall is
mainly due to overgenerations by automatic lem-
matisation and functional assignments to PPs in the
TIGER-LFG corpus, which are not matched by the
LFG parser output. These mismatches will be cor-
rected by refinements of the TIGER-LFG treebank.
6 Summary and Future Directions
We presented a method for corpus-based induction
of an LFG syntax-semantics interface for frame se-
mantic processing. We port frame annotations from
a manually annotated corpus to an LFG parsing ar-
chitecture that can be used to process unparsed text.
We model frame semantic annotations in an LFG
projection architecture, including phenomena that
involve non-isomorphic mappings between levels.
In future work we will train stochastic mod-
els for disambiguation of the assigned frame se-
mantic structures. We are especially interested in
exploring the potential of deeper, functional syn-
tactic analyses for frame assignment, in conjunc-
tion with additional semantic knowledge (e.g. word
senses, named entities). We will set up a bootstrap-
ping cycle for learning increasingly refined stochas-
tic models from growing training corpora, using
semi-supervised learning methods. We will explore
multi-lingual aspects of frame assignment, using
English FrameNet data and an English LFG gram-
mar with comparable f-structure output. Finally, we
will investigate how similar methods can be applied
to syntactic frameworks such as HPSG, which al-
ready embody a level of semantic representation.
Acknowledgements We thank the IMS Stuttgart for
allowing us to use the German LFG grammar. Spe-
cial thanks go to Martin Forst who provided us with
the TIGER-LFG corpus and added special features
to support our work. Finally, thanks go to Dick
Crouch who greatly enhanced the transfer system.
References
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998.
The Berkeley FrameNet project. In Proceedings of
COLING-ACL 1998, Montr e?al, Canada.
S.Brants, S.Dipper, S.Hansen, W.Lezius, G.Smith. 2002.
The TIGER Treebank. In Proc. of the Workshop on
Treebanks and Linguistic Theories, Sozopol, Bulgaria.
J. Bresnan. 2001. Lexical-Functional Syntax. Blackwell
Publishers, Oxford.
K. Erk, A. Kowalski, S. Pad o?, and M. Pinkal. 2003.
Towards a Resource for Lexical Semantics: A Large
German Corpus with Extensive Semantic Annotation.
In Proceedings of the ACL 2003, Sapporo, Japan.
M. Fleischman, N. Kwon, and E. Hovy. 2003. Maxi-
mum entropy models for FrameNet classification. In
Proceedings of EMNLP?03, Sapporo, Japan.
M. Forst. 2003. Treebank Conversion ? Establishing a
testsuite for a broad-coverage LFG from the TIGER
treebank. In A. Abeill e?, S. Hansen, and H. Uszkoreit
(eds), Proceedings of the 4th International Workshop
on Linguistically Interpreted Corpora, Budapest.
A. Frank and K. Erk. 2004. Towards an LFG Syntax?
Semantics Interface for Frame Semantics Annotation.
In A. Gelbukh (ed), Computational Linguistics and In-
telligent Text Processing, Springer, Heidelberg.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
D. Gildea and M. Palmer. 2002. The Necessity of Pars-
ing for Predicate Argument Recognition. In Proceed-
ings of ACL?02, Philadelphia, PA.
P.-K. Halvorsen and R.M. Kaplan. 1995. Projec-
tions and Semantic Description in Lexical-Functional
Grammar. In M. Dalrymple, R.M. Kaplan, J.T.
Maxwell, A. Zaenen (eds), Formal Issues in Lexical-
Functional Grammar, CSLI Lecture Notes, Stanford.
P. Kingsbury, M. Palmer, and M. Marcus. 2002. Adding
semantic annotation to the Penn TreeBank. In Pro-
ceedings of the HLT Conference, San Diego.
J. T. III Maxwell and R. M. Kaplan. 1989. An overview
of disjunctive constraint satisfaction. In Proceedings
of IWPT, pages 18?27.
S. Riezler, T. H. King, R. M. Kaplan, R. Crouch, J. T. III
Maxwell, and M. Johnson. 2002. Parsing the Wall
Street Journal using a Lexical-Functional Grammar
and Discriminative Estimation Techniques. In Pro-
ceedings of the ACL?02, Philadelphia, PA.
Proceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 1?8,
New York City, NY, USA. June 2006. c?2006 Association for Computational Linguistics
Contextual phenomena and thematic relations in database QA dialogues:
results from a Wizard-of-Oz Experiment
Nu?ria Bertomeu, Hans Uszkoreit
Saarland University
Saarbru?cken, Germany
uszkoreit|bertomeu@coli.uni-sb.de
Anette Frank, Hans-Ulrich Krieger and Brigitte Jo?rg
German Research Center of Artificial Intelligence
Saarbru?cken, Germany
frank|krieger|joerg@dfki.de
Abstract
Considering data obtained from a corpus
of database QA dialogues, we address the
nature of the discourse structure needed
to resolve the several kinds of contextual
phenomena found in our corpus. We look
at the thematic relations holding between
questions and the preceding context and
discuss to which extent thematic related-
ness plays a role in discourse structure.
1 Introduction
As pointed out by several authors (Kato et al, 2004),
(Chai and Ron, 2004), the information needs of
users interacting with QA systems often go beyond
a single stand-alone question. Often users want to
research about a particular topic or event or solve
a specific task. In such interactions we can expect
that the individual user questions will be themati-
cally connected, giving the users the possibility of
reusing part of the context when formulating new
questions.
That users implicitly refer to and even omit ma-
terial which can be recovered from the context
has already been replicated in several Wizard-of-
Oz experiments simulating natural language inter-
faces to databases, (Carbonell, 1983), (Dahlba?ck
and Jo?nsson, 1989), the most frequent contextual
phenomena being ellipsis, anaphora and definite de-
scriptions.
A big challenge for interactive QA systems is,
thus, the resolution of contextual phenomena. In or-
der to be able to do so a system has to keep track of
the user?s focus of attention as the interaction pro-
ceeds. The attentional state at a given point in the
interaction is given by the discourse structure. An
open issue, however, is the nature of the discourse
structure model needed in a QA system. Ahrenberg
et al (1990) argue that the discourse structure in NL
interfaces is, given the limited set of actions to be
performed by the system and the user, simpler than
the one underlying human-human dialogue. Upon
Ahrenberg et al (1990) this is given by the discourse
goals, rather than the overall goals of the user, as is
the case in task-oriented dialogues, (Grosz and Sid-
ner, 1986). Following Ahrenberg et al (1990), the
QA discourse is structured in segments composed
by a pair of initiative-response units, like question-
answer, or question-assertion, in the absence of an
answer. A segment can be embedded in another seg-
ment if it is composed by a clarification request and
its corresponding answer. The local context of a
segment is given by the immediately preceding seg-
ment. Upon Ahrenberg et al (1990), the latter re-
liably limits up the search space for antecedents of
anaphoric devices and ellipsis. However, as we will
see, there are few cases where the antecedents of
contextual phenomena are to be found beyond the
immediately preceding segments. This suggests that
a more complex approach to discourse structure for
QA systems is needed.
In more recent studies of interactive QA special
attention has been paid to the thematic relatedness of
questions, (Chai and Ron, 2004), (Kato et al, 2004).
Chai and Ron (2004) propose a discourse model-
ing for QA interactions in which they keep track
of thematic transitions between questions. Although
1
the applications of tracking thematic transitions be-
tween questions have not been investigated in depth,
Sun and Chai (2006) report on an experiment which
shows that the use of a model of topic transitions
based on Centering Theory improves query expan-
sion for context questions. However, these previous
studies on the thematic relations between questions
are not based on collections of interactive data, but
on questions centered around a topic that were col-
lected in non-interactive environments. This means
that they do not consider the answers to the ques-
tions, to which following questions can be related.
This paper presents data on different kinds of con-
textual phenomena found in a corpus of written nat-
ural language QA exchanges between human users
and a human agent representing an interactive infor-
mation service. We address two issues: the kinds
and frequencies of thematic relations holding be-
tween the user questions and the preceding context,
on the one hand, and the location of antecedents for
the different contextual phenomena, on the other.
We also discuss the question whether thematic rela-
tions can contribute to determine discourse structure
and, thus, to the resolution of the contextual phe-
nomena.
In the next section we present our data collection
and the aspects of the annotation scheme which are
relevant to the current work. In section 3 we present
data regarding the overall thematic cohesion of the
QA sessions. In section 4 we report on data regard-
ing the co-occurrence of discourse phenomena and
thematic relations and the distance between the phe-
nomena and their antecedents. Finally, we discuss
our findings with regard to their relevance with re-
spect to the nature of discourse structure.
2 Corpus and methodology
2.1 Experimental set-up
In order to obtain a corpus of natural QA inter-
actions, we designed a Wizard-of-Oz experiment.
The experiment was set up in such a way that the
exchanges between users and information system
would be as representative as possible for the inter-
action between users and QA systems. We chose an
ontology database instead of a text based closed do-
main QA system, however, because in order to simu-
late a real system short time responses were needed.
30 subjects took part in the experiment, which
consisted in solving a task by querying LT-WORLD,
an ontology containing information about language
technology1, in English. The modality of interaction
was typing through a chat-like interface.
Three different tasks were designed: two of them
concentrated on information browsing, the other one
on information gathering. In the first task sub-
jects had to find three traineeships at three different
projects in three different institutions each on a dif-
ferent topic, and obtain some information about the
chosen projects, like a contact address, a descrip-
tion, etc. In the second task, subjects had to find
three conferences in the winter term and three con-
ferences in the summer term, each one on a differ-
ent topic and they had to obtain some information on
the chosen conferences such as deadline, place, date.
etc. Finally, the third task consisted of finding infor-
mation for writing a report on European language
technology in the last ten years. To this end, subjects
had to obtain quantitative information on patents, or-
ganizations, conferences, etc.
The Wizard was limited to very few types of re-
sponses. The main response was answering a ques-
tion. In addition, she would provide intermediate
information about the state of processing if the re-
trieval took too long. She could also make state-
ments about the contents of the database when it did
not contain the information asked for or when the
user appeared confused about the structure of the
domain. Finally, she could ask for clarification or
more specificity when the question could not be un-
derstood. Yet the Wizard was not allowed to take
the initiative by offering information that was not
explicitely asked for. Thus all actions of the Wiz-
ard were directly dependent on those of the user.
As a result we obtained a corpus of 33 logs (30
plus 3 pilot experiments) containing 125.534 words
in 2.534 turns, 1.174 of which are user turns.
2.2 Annotation scheme
The corpus received a multi-layer annotaton2 con-
sisting of five levels. The levels of turns and part-of-
speech were automatically annotated. The level of
turns records information about the speaker and time
1See http://www.lt-world.org.
2We employed the annotation tool MMAX2 developed at
EML Research, Heidelberg.
2
stamp. For the other levels - the questions level, the
utterances level, and the entities level - a specific an-
notation scheme was developed. For these, we only
explain the aspects relevant for the present study.
2.2.1 Questions
This level was conceived to keep track of the
questions asked by the user which correspond to
queries to the database. With the aim of annotating
thematic relatedness between questions we distin-
guished two main kinds of thematic relations: those
holding between a question and a previous ques-
tion, quest(ion)-to-quest(ion)-rel(ation), and those
holding between a question and a previous answer,
quest(ion)-to-answ(er)-rel(ation).
Quest-to-quest-rels can be of the following types:
? refinement if the current question asks for the
same type of entity as some previous question,
but the restricting conditions are different, ask-
ing, thus, for a subset, superset or disjoint set
of the same class.
(1) US: How many projects on language tech-
nologies are there right now?
US: How many have been done in the
past?
? theme-entity if the current question is about the
same entity as some previous question.
(2) US: Where will the conference take place?
US: What is the dead-line for applicants?
? theme-property if the current question asks for
the same property as the immediately preced-
ing question but for another entity.
(3) US: Dates of TALK project?
US: Dates of DEREKO?
? paraphrase if the question is the rephrasing of
some previous question.
? overlap if the content of a question is subsumed
by the content of some previous question.
We distinguish the following quest-to-answ-rels:
? refinement if the current question asks for a
subset of the entities given in the previous an-
swer.
(4) LT: 3810.
US: How many of them do research on
language technology?
? theme if the current question asks about an en-
tity first introduced in some previous answer.
(5) LT: Semaduct, ...
US: What language technology topics
does the Semaduct project investigate?
Although Chai and Jin (2004) only consider tran-
sitions among questions in dialogues about events,
most of our relations have a correspondence with
theirs. Refinement corresponds to their constraint
refinement, theme-property to their participant-shift,
and theme-entity to their topic exploration.
2.2.2 Utterances
Utterances are classified according to their
speech-act: question, answer, assertion, or request.
Our annotation of discourse structure is identical in
spirit to the one proposed by Ahrenberg et al (1990).
A segment is opened with a user question to the
database and is closed with its corresponding an-
swer or an assertion by the system. Clarification
requests and their corresponding answers form seg-
ments which are embedded in other segments. Re-
quests to wait and assertions about the processing of
a question are also embedded in the segment opened
by the question.
Fragmentary utterances are annotated at this level.
We distinguish between fragments with a full lin-
guistic source, fragments with a partial source,
and fragments showing a certain analogy with the
source. The first group corresponds to fragments
which are structurally identical to the source and
can, thus, be resolved by substitution or extension.
(6) US: Are there any projects on spell checking in
Europe in the year 2006?
US: And in the year 2005?
Fragments with a partial source implicitly refer to
some entity previously introduced, but some infer-
ence must be done in order to resolve them.
(7) US: How is the contact for that project?
US: Homepage?
3
The last group is formed by fragments which show
some kind of parallelism with the source but which
cannot be resolved by substitution.
(8) US: Which conferences are offered in this win-
ter term in the subject of English language?
US: Any conferences concerning linguistics in
general?
2.2.3 Reference
We distinguish the following types of reference
to entities: identity or co-reference, subset/superset
and bridging.
Co-reference occurs when two or more expres-
sions denote the same entity. Within this group we
found the following types of implicit co-referring
expressions which involve different degrees of ex-
plicitness: elided NPs, anaphoric and deictic pro-
nouns, deictic NPs, and co-referent definite NPs.
Elided NPs are optional arguments, that is, they
don?t need to be in the surface-form of the sentence,
but are present in the semantic interpretation. In (9)
there is an anaphoric pronoun and an elided NP both
referring to the conference Speech TEK West 2006.
(9) US: The Speech TEK West 2006, when does it
take place?
LT: 2006-03-30 - 2006-04-01.
US: Until when can I hand in a paper [ ]?
Bridging is a definite description which refers to
an entity related to some entity in the focus of at-
tention. The resolution of bridging requires some
inference to be done in order to establish the con-
nection between the two entities. In example (2) in
subsection 2.2.1 there is an occurrence of bridging,
where the dead-line is meant to be the dead-line of
the conference currently under discussion.
Finally, subset/superset reference takes place
when a linguistic expression denotes a subset or su-
perset of the set of entities denoted by some previ-
ous linguistic expression. Subset/superset reference
is sometimes expressed through two interesting con-
textual phenomena: nominal ellipsis3, also called se-
mantic ellipsis, and one-NPs4. Nominal ellipsis oc-
curs within an NP and it is namely the noun what
3Note, however, that nominal ellipsis does not necessarily
always denote a subset, but sometimes it can denote a disjoint
set, or just lexical material which is omitted.
4One-NPs are a very rare in our corpus, so we are not con-
sidering them in the present study.
is missing and must be recovered from the context.
Here follows an example:
(10) US: Show me the three most important.
3 Thematic follow-up
When looking at the thematic relatedness of the
questions it?s striking how well structured the in-
teractions are regarding thematic relatedness. From
1047 queries to the database, 948 (90.54%) follow-
up on some previous question or answer, or both.
Only 99 questions (9.46%) open a new topic. 725
questions (69.25% of the total, 76.48% of the con-
nected questions) are related to other questions, 332
(31.71% of the total, 35.02% of the connected ques-
tions) are related to answers, and 109 (10.41% of the
total, 11.49% of the connected questions) are con-
nected to both questions and answers. These num-
bers don?t say much about how well structured the
discourse is, since the questions could be far away
from the questions or answers they are related to.
However, this is very seldom the case. In 60% of
the cases where the questions are thematically con-
nected, they immediately follow the question they
are related to, that is, the two questions are consecu-
tive5. In 16.56% of the cases the questions immedi-
ately follow the answer they are related to. 74.58%
of the questions, thus, immediately follow up the
question or/and answer they are thematically related
to6.
Table 1 shows the distribution of occurrences
and distances in segments for each of the rela-
tions described in subsection 2.2.1. We found that
the most frequent question-to-question relation is
theme-entity, followed by the question-to-answer re-
lation theme. As you can see, for all the relations ex-
cept theme, most occurrences are between very close
standing questions or questions and answers, most
of them holding between consecutive questions or
questions and answers. The occurrences of the re-
lation theme, however, are distributed along a wide
range of distances, 29.70% holding between ques-
tions and answers that are 2 and 14 turns away from
5By consecutive we mean that there is no intervening query
to the database between the two questions. This doesn?t imply
that there aren?t several intervening utterances and turns.
69 questions are consecutive to the question and answer they
are related to, respectively, that?s why the total percentage of
related consecutive questions is not 76.56%.
4
REF. Q. THEME E. Q. THEME P. Q. PARA. Q. OVERL. Q. REF. A. THEME A.
TOTAL 74 338 107 174 29 29 303
(7.80%) (35.65%) (11.29%) (18.35%) (3.06%) (3.06%) (31.96%)
1 SEGM. 88.73% 81.65% 100% 60.92% 78.57% 83.34% 46.39%
2 SEGM. 5.63% 1.86% 0% 8.09% 21.43% 13.33% 10.20%
Table 1: Occurrences of the different thematic relations
REL. / PHEN. THEME E. Q. THEME P. Q. THEME A. REF. Q. REF. A. CONNECTED TOTAL
FRAGMENT 53 (54.08%) 17 (16.32%) 3 (3.06%) 21 (21.42%) 0 97 (85.08%) 114
BRIDGING 40 (74.07%) 0 3 (5.55%) 1 (1.85%) 0 54 (58.69%) 92
DEFINITE NP 26 (78.78%) 0 4 (12.21%) 2 (6.10%) 0 33 (66%) 50
DEICTIC NP 19 (51.35%) 0 13 (35.13%) 2 (5.40%) 1 (2.70%) 37 (78.72%) 47
ANAPHORIC PRON. 13 (39.39%) 2 (6.06%) 10 (30.30%) 0 5 (15.15%) 33 (39.75%) 83
DEICTIC PRON. 2 (75%) 0 1 (25%) 0 0 3 (25%) 12
ELIDED NP 9 (69.23%) 0 2 (15.38%) 0 0 13 (61.90%) 21
NOMINAL ELLIPSIS 0 1 (7.69%) 6 (46.15%) 1 (7.69%) 5 (38.46%) 13 (81.25%) 16
Table 2: Contextual phenomena and the thematic relations holding between the questions containing them
and the questions or answers containing the antecedents.
each other. This is because often several entities
are retrieved with a single query and addressed later
on separately, obtaining all the information needed
about each of them before turning to the next one.
We found also quite long distances for paraphrases,
which means that the user probably forgot that he
had asked that question, since he could have also
scrolled back.
These particular distributions of thematic rela-
tions seem to be dependent on the nature of the
tasks. We found some differences across tasks: the
information gathering task elicited more refinement,
while the information browsing tasks gave rise to
more theme relations. It is possible that in an in-
teraction around an event or topic we may find ad-
ditional kinds of thematic relations and different
distributions. We also observed different strategies
among the subjects. The most common was to ask
everything about an entity before turning to the next
one, but some subjects preferred to ask about the
value of a property for all the entities under discus-
sion before turning to the next property.
4 Contextual phenomena: distances and
thematic relatedness
There are 1113 user utterances in our corpus, 409 of
which exhibit some kind of discourse phenomenon,
i.e., they are context-dependent in some way. This
amounts to 36.16% of the user utterances, a pro-
portion which is in the middle of those found in the
several corpora analyzed by Dahlba?ck and Jo?nsson
(1989)7. The amount of context-dependent user ut-
terances, as Dahlba?ck and Jo?nsson (1989) already
pointed out, as well as the distribution of the dif-
ferent relations among questions and answers ex-
plained above, may be dependent on the nature of
the task attempted in the dialogue.
Table 2 shows the distribution of the most fre-
quent thematic relations holding between the ques-
tions containing the contextual phenomena consid-
ered in our study and the questions or answers con-
taining their antecedents. The rightmost column
shows the number of occurrences of each of the con-
textual phenomena described in subsection 2.2.3.
The second column on the right shows the number
of occurrences in which the antecedent is located
in some previous segment and the question contain-
ing the contextual phenomenon is related through a
thematic relation to the question or answer contain-
ing the antecedent. The percentages shown for each
phenomenon are out of the total number of its oc-
currences. The remaining columns show frequen-
7They found a high variance according to the kind of task
carried out in the different dialogues. Dialogues from tasks
where there was the possibility to order something contained
a higher number of context-dependent user initiatives, up to
54.62%, while information browsing dialogues contained a
smaller number of context-dependent user initiatives, 16.95%
being the lowest amount found.
5
cies of co-occurrence for each of the phenomena and
thematic relations. The percentages shown for each
phenomenon are out of the total number of its con-
nected occurrences.
For the majority of investigated phenomena we
observe that most questions exhibiting them stand
in a thematic relation to the question or answer con-
taining the antecedent. Although there may be sev-
eral intermediate turns, the related questions are al-
most always consecutive, that is, the segment con-
taining the contextual phenomenon immediately fol-
lows the segment containing the antecedent. In the
remainder of the cases, the contextual phenomenon
and its antecedent are usually in the same segment.
However, this is not the case for deictic and
anaphoric pronouns. In most cases their antecedents
are in the same segment and even in the same utter-
ance or just one utterance away. This suggests that
pronouns are produced in a more local context than
other phenomena and their antecedents are first to be
looked for in the current segment.
For almost all the phenomena the most frequent
relation holding between the question containing
them and the question or answer containing the an-
tecedent is the question-to-question relation theme-
entity, followed by the question-to-answer relation
theme. This is not surprising, since we refer back to
entities because we keep speaking about them.
However, fragments and nominal ellipsis show a
different distribution. Fragments are related to their
sources through the question-to-question relations
theme-property and refinement, as well. Regarding
the distribution of relations across the three differ-
ent types of fragments we distinguish in our study,
we find that the relations refinement and theme-
property only hold between fragments with a full
source and fragments of type analogy, and their re-
spective sources. On the other hand, practically all
fragments with a partial-source stand in a theme-
entity relation to their source. Questions containing
nominal ellipsis are mostly related to the preceding
answer both through the relations theme and refine-
ment.
4.1 Antecedents beyond the boundaries of the
immediately preceding segment
As we have seen, the antecedents of more implicit
co-referring expressions, like pronouns, are very of-
ten in the same segment as the expressions. The
antecedents of less explicit co-referring expressions,
like deictic and definite NPs, are mostly in the im-
mediately preceding segment, but also often in the
same segment. About 50% are 2 utterances away,
20% between 3 and 5, although we find distances up
to 41 utterances for definite NPs.
However, there is a small number (11) of cases in
which the antecedents are found across the bound-
aries of the immediately preceding segment. This
poses a challenge to systems since the context
needed for recovering these antecedent is not as lo-
cal. The following example is a case of split an-
tecedents. The antecedent of the elided NP is to be
found across the two immediately preceding ques-
tions. Moreover, as you can see, the Wizard is not
sure about how to interpret the missing argument,
which can be because of the split antecedents, but
also because of the amount of time passed, and/or
because one of the answers is still missing, that is,
more than one segment is open at the same time.
(11) US: Which are the webpages for European
Joint Conferences on Theory and Practice
of Software and International Conference on
Linguistic Evidence?
LT: Please wait... (waiting time)
US: Which are the webpages for International
Joint Conference on Neural Networks and
Translating and the Computer 27?
LT: http://www.complang.ac, ... (1st answer)
US: Up to which date is it possible to send a
paper, an abstract [ ]?
LT: http://uwb.edu/ijcnn05/, ... (2nd answer)
LT: For which conference?
US: For all of the conferences I got the web-
pages.
In the following example the antecedent of the
definite NP is also to be found beyond the bound-
aries of the immediately preceding segment.
(12) US: What is the homepage of the project?
LT: http://dip.semanticweb.org
USER: What is the email address of Christoph
Bussler?
LT: The database does not contain this informa-
tion.
US: Where does the project take place?
6
Here the user asks about the email address of a per-
son who was previously introduced in the discourse
as the coordinator of the project under discussion
and then keeps on referring to the project with a def-
inite NP. The intervening question is somehow re-
lated to the project, but not directly. There is a topic
shift, as defined by Chai and Jin (2004), where the
main topic becomes an entity related to the entity the
preceding question was about. However, this topic
shift is only at a very local level, since the dialogue
participants keep on speaking about the project, that
is, the topic at a more general level keeps on being
the same. We can speak here of thematic nesting,
since the second question is about an entity intro-
duced in relation to the entity in focus of attention
in the first question, and the third question is again
about the same entity as the first. The project has not
completely left the focus, but has remained in sec-
ondary focus during the second segment, to become
again the main focus in the third segment. It seems
that as long as the entity to which the focus of atten-
tion has shifted is related to the entity previously in
focus of attention, the latter still also remains within
the focus of attention.
5 Conclusions
The possibility of using contextual phenomena is
given by certain types of thematic relatedness - espe-
cially theme-entity and theme, for co-reference and
bridging, and refinement, theme-entity and theme-
property, for fragments -, and contiguity of ques-
tions. As we have seen, the immediately preced-
ing segment is in most cases the upper limit of the
search space for the last reference to the entity, or
the elided material in fragments. The directions of
the search for antecedents, however, can vary de-
pending on the phenomena, since for more implicit
referring expressions antecedents are usually to be
found in the same segment, while for less implicit
referring expressions they are to be found in the pre-
ceding one.
These data are in accordance with what Ahren-
berg et al (1990) predict in their model. Just to
consider the immediately preceding segment as the
upper limit of the search space for antecedents is
enough and, thus, no tracking of thematic relations
is needed to resolve discourse phenomena. How-
ever, there are occurrences of more explicit types
of co-reference expressions, where the antecedent
is beyond the immediately preceding segment. As
we have observed, in these cases the intervening
segment/s shift the focus of attention to an entity
(maybe provided in some previous answer) closely
related to the one in focus of attention in the pre-
ceding segment. It seems that as long as this rela-
tion exists, even if there are many segments in be-
tween8, the first entity remains in focus of attention
and can be referred to by an implicit deictic or defi-
nite NP without any additional retrieval cue. We can
speak of thematic nesting of segments, which seems
to be analogous to the intentional structure in task-
oriented dialogues as in (Grosz and Sidner, 1986),
also allowing for reference with implicit devices to
entities in the superordinate segments after the sub-
ordinated ones have been closed. It seems, thus, that
thematic structure, like the discourse goals, also im-
poses structure on the discourse.
These cases, although not numerous, suggest that
a more complex discourse structure is needed for
QA interactions than one simply based on the dis-
course goals. The local context is given by the dis-
course segments, which are determined by the dis-
course goals, but a less local context may encompass
several segments. As we have seen, reference with
implicit devices to entities in the less local context
is still possible. What seems to determine this less
local context is a unique theme, about which all the
segments encompassed by the context directly or in-
directly are. So, although it does not seem necessary
to track all the thematic transitions between the seg-
ments, it seems necessary to categorize the segments
as being about a particular more global theme.
In a system like the one we simulated, having spe-
cific tasks in mind and querying structured data, a
possible approach to model this extended context,
or focus of attention, would be in terms of frames.
Every time a new entity is addressed a new frame
is activated. The frame encompasses the entity it-
self and the properties holding of it and other enti-
ties, as well as those entities. This would already
allow us to successfully resolve bridging and frag-
ments with a partial source. If the focus of atten-
8We found up to five intervening segments, one of them be-
ing a subsegment.
7
tion then shifts to one of the related entities, the user
demanding particular information about it, then its
frame is activated, but the previous frame also re-
mains somehow active, although to a lesser degree.
As long as there is a connection between the enti-
ties being talked about and a frame is not explicitly
closed, by switching to speak about a different en-
tity of the same class, for example, frames remain
somehow active and implicit references will be ac-
commodated within the activation scope.
In principle, the closer the relation to the entity
currently in focus, the higher the degree of activation
of the related entities. Yet, there may be cases of
ambiguity, where only inferences about the goals of
the user may help to resolve the reference, as in (13):
(13) US: How is the contact for that project?
LT: daelem@uia.ua.ac.be
US: What is the institute?
LT: Centrum voor Nederlandse Taal en Spraak.
US: Homepage?
Here the property ?Homepage? could be asked about
the institution or the project, the institution being
more active. However, the Wizard interpreted it as
referring to the project without hesitation because
she knew that subjects were interested in projects,
not in organizations. In order to resolve the ambigu-
ity, we would need a system customized for tasks or
make inferences about the goals of the users based
on the kind of information they?ve been asking for.
Determining at which level of nesting some expres-
sion has to be interpreted may involve plan recogni-
tion.
However, for open domain systems not having a
knowledge-base with structured data it may be much
more difficult to keep track of the focus of attention
beyond the strictly local context. For other kinds
of interactions which don?t have such a structured
nature as our tasks, this may also be the case. For
example, in the information browsing tasks in (Kato
et al, 2004), there is not a global topic encompass-
ing the whole interaction, but the information needs
of the user are given by the information he is en-
countering as the interaction proceeds, that is, he is
browsing the information in a free way, without hav-
ing particular goals or particular pieces of informa-
tion he wants to obtain in mind. In such cases it
may be difficult to determine how long frames are
active if the nesting goes very far, as well as making
any inferences about the user?s plans. However, it
might also be the case, that in that kind of interac-
tions no implicit referring expressions are used be-
yond the segmental level, because there is no such
an extended context. In order to find out, a study
with interactive data should be carried out.
Acknowledgements
The research reported here has been conducted in
the projects QUETAL and COLLATE II funded by
the German Ministry for Education and Research,
grants no. 01IWC02 and 01INC02, respectively. We
are also grateful to Bonnie Webber for her helpful
comments on the contents of this paper.
References
Ahrenberg Lars, Dahlba?ck Nils and Arne Jo?nsson 1990.
Discourse representation and discourse management
for natural language interfaces. Proceeding of the
Second Nordic Conference on Text Comprehension in
Man and Machine, Ta?by, Sweeden, 1990.
Jaime G. Carbonell. 1983. Discourse pragmatics and
ellipsis resolution in task-oriented natural language
interfaces. Proceedings of the 21st annual meeting
on Association for Computational Linguistics, Cam-
bridge, Massachusetts, 1983
Chai Joyce Y. and Ron Jin. 2004. Discourse Status
for Context Questions. HLT-NAACL 2004 Workshop
on Pragmatics in Question Answering (HLT-NAACL
2004) Boston, MA, USA, May 3-7, 2004
Dahlba?ck Nils and Arne Jo?nsson. 1989. Empirical
Studies of Discourse Representations for Natural Lan-
guage Interfaces. Proceedings of the Fourth Confer-
ence of the European Chapter of the ACL (EACL?89),
Manchester.
Grosz Barbara and Candance Sidner. 1986. Attention,
Intention and the Structure of Discourse. Computa-
tional Linguistics 12(3): 175-204.
Kato Tsuneaki, Fukumoto Junichi, Masui Fumito and
Noriko Kando. 2004. Handling Information Access
Dialogue through QA Technologies - A novel chal-
lenge for open-domain question answering. HLT-
NAACL 2004 Workshop on Pragmatics in Question
Answering (HLT-NAACL 2004) Boston, MA, USA,
May 3-7, 2004
Sun Mingyu and Joycie J. Chai. 2006. Towards Intel-
ligent QA Interfaces: Discourse Processing for Con-
text Questions. International Conference on Intelligent
User Interfaces, Sydney, Australia, January 2006
8
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 430?438,
Beijing, August 2010
A Structured Vector Space Model for Hidden Attribute Meaning
in Adjective-Noun Phrases
Matthias Hartung and Anette Frank
Computational Linguistics Department
Heidelberg University
{hartung, frank}@cl.uni-heidelberg.de
Abstract
We present an approach to model hid-
den attributes in the compositional se-
mantics of adjective-noun phrases in a
distributional model. For the represen-
tation of adjective meanings, we refor-
mulate the pattern-based approach for at-
tribute learning of Almuhareb (2006) in
a structured vector space model (VSM).
This model is complemented by a struc-
tured vector space representing attribute
dimensions of noun meanings. The com-
bination of these representations along the
lines of compositional semantic principles
exposes the underlying semantic relations
in adjective-noun phrases. We show that
our compositional VSM outperforms sim-
ple pattern-based approaches by circum-
venting their inherent sparsity problems.
1 Introduction
In formal semantic theory, the compositional se-
mantics of adjective-noun phrases can be modeled
in terms of selective binding (Pustejovsky, 1995),
i.e. the adjective selects one of possibly several
roles or attributes1 from the semantics of the noun.
(1) a. a blue car
b. COLOR(car)=blue
In this paper, we define a distributional frame-
work that models the compositional process un-
derlying the modification of nouns by adjectives.
1In the original statement of the theory, adjectives se-
lect qualia roles that can be considered as collections of at-
tributes.
We focus on property-denoting adjectives as they
are valuable for acquiring concept representations
for, e.g., ontology learning. An approach for au-
tomatic subclassification of property-denoting ad-
jectives is presented in Hartung and Frank (2010).
Our goal is to expose, for adjective-noun phrases
as in (1a), the attribute in the semantics of the
noun that is selected by the adjective, while not
being overtly realized on the syntactic level. The
semantic information we intend to capture for (1a)
is formalized in (1b).
Ideally, this kind of knowledge could be ex-
tracted from corpora by searching for patterns that
paraphrase (1a), e.g. the color of the car is blue.
However, linguistic patterns that explicitly relate
nouns, adjectives and attributes are very rare.
We avoid these sparsity issues by reducing
the triple r=?noun, attribute, adjective? that
encodes the relation illustrated in (1b) to tu-
ples r?=?noun, attribute? and r??=?attribute,
adjective?, as suggested by Turney and Pantel
(2010) for similar tasks. Both r? and r?? can be
observed much more frequently in text corpora
than r. Moreover, this enables us to model adjec-
tive and noun meanings as distinct semantic vec-
tors that are built over attributes as dimensions.
Based on these semantic representations, we make
use of vector composition operations in order to
reconstruct r from r? and r??. This, in turn, al-
lows us to infer complete noun-attribute-adjective
triples from individually acquired noun-attribute
and adjective-attribute representations.
The contributions of our work are as follows:
(i) We propose a framework for attribute selection
based on structured vector space models (VSM),
using as meaning dimensions attributes elicited
430
by adjectives; (ii) we complement this novel rep-
resentation of adjective meaning with structured
vectors for noun meanings similarly built on at-
tributes as meaning dimensions; (iii) we propose a
composition of these representations that mirrors
principles of compositional semantics in mapping
adjective-noun phrases to their corresponding on-
tological representation; (iv) we propose and eval-
uate several metrics for the selection of meaning-
ful components from vector representations.
2 Related Work
Adjective-noun meaning composition has not
been addressed in a distributional framework be-
fore (cf. Mitchell and Lapata (2008)). Our ap-
proach leans on related work on attribute learning
for ontology induction and recent work in distri-
butional semantics.
Attribute learning. Early approaches to at-
tribute learning include Hatzivassiloglou and
McKeown (1993), who cluster adjectives that de-
note values of the same attribute. A weakness
of their work is that the type of the attribute
cannot be made explicit. More recent attempts
to attribute learning from adjectives are Cimiano
(2006) and Almuhareb (2006). Cimiano uses at-
tributes as features to arrange sets of concepts in a
lattice. His approach to attribute acquisition har-
nesses adjectives that occur frequently as concept
modifiers in corpora. The association of adjec-
tives with their potential attributes is performed by
dictionary look-up in WordNet (Fellbaum, 1998).
Similarly, Almuhareb (2006) uses adjectives and
attributes as (independent) features for the pur-
pose of concept learning. He acquires adjective-
attribute pairs using a pattern-based approach.
As a major limitation, these approaches are
confined to adjective-attribute pairs. The poly-
semy of adjectives that can only be resolved in the
context of the modified noun is entirely neglected.
From a methodological point of view, our work
is similar to Almuhareb?s, as we will also build
on lexico-syntactic patterns for attribute selection.
However, we extend the task to involve nouns and
rephrase his approach in a distributional frame-
work based on the composition of structured vec-
tor representations.
Distributional semantics. We observe two re-
cent trends in distributional semantics research:
(i) The use of VSM tends to shift from mea-
suring unfocused semantic similarity to captur-
ing increasingly fine-grained semantic informa-
tion by incorporating more linguistic structure.
Following Baroni and Lenci (to appear), we re-
fer to such models as structured vector spaces.
(ii) Distributional methods are no longer confined
to word meaning, but are noticeably extended to
capture meaning on the phrase level. Prominent
examples for (i) are Pado? and Lapata (2007) and
Rothenha?usler and Schu?tze (2009) who use syn-
tactic dependencies rather than single word co-
occurrences as dimensions of semantic spaces.
Erk and Pado? (2008) extend this idea to the ar-
gument structure of verbs, while also accounting
for compositional meaning aspects by modelling
predication over arguments. Hence, their work is
also representative for (ii).
Baroni et al (2010) use lexico-syntactic pat-
terns to represent concepts in a structured VSM
whose dimensions are interpretable as empirical
manifestations of properties. We rely on similar
techniques for the acquisition of structured vec-
tors, whereas our work focusses on exposing the
hidden meaning dimensions involved in composi-
tional processes underlying concept modification.
The commonly adopted method for modelling
compositionality in VSM is vector composition
(Mitchell and Lapata, 2008; Widdows, 2008).
Showing the benefits of vector composition for
language modelling, Mitchell and Lapata (2009)
emphasize its potential to become a standard
method in NLP.
The approach pursued in this paper builds on
both lines of research sketched in (i) and (ii) in
that we model a specific meaning layer in the se-
mantics of adjectives and nouns in a structured
VSM. Vector composition is used to expose their
hidden meaning dimensions on the phrase level.
3 Structured Vector Representations for
Adjective-Noun Meaning
3.1 Motivation
Contrary to prior work, we model attribute selec-
tion as involving triples of nouns, attributes and
431
C
O
LO
R
D
IR
EC
TI
O
N
D
U
R
AT
IO
N
SH
A
PE
SI
ZE
SM
EL
L
SP
EE
D
TA
ST
E
TE
M
PE
R
AT
U
R
E
W
EI
G
H
T
ve 1 1 0 1 45 0 4 0 0 21
vb 14 38 2 20 26 0 45 0 0 20
ve ? vb 14 38 0 20 1170 0 180 0 0 420
ve + vb 15 39 2 21 71 0 49 0 0 41
Figure 1: Vectors for enormous (ve) and ball (vb)
adjectives, as in (2). The triple r can be bro-
ken down into tuples r? = ?noun, attribute? and
r?? = ?attribute, adjective?. Previous learning
approaches focussed on r? (Cimiano, 2006) or r??
(Almuhareb, 2006) only.
(2) a. a bluevalue carconcept
b. ATTR(concept) = value
In semantic composition of adjective-noun
compounds, the adjective (e.g. blue) contributes a
value for an attribute (here: COLOR) that charac-
terizes the concept evoked by the noun (e.g. car).
Thus, the attribute in (2) constitutes a ?hidden
variable? that is not overtly expressed in (2a), but
constitutes the central axis that relates r? and r??.
Structured vectors built on extraction patterns.
We model the semantics of adjectives and nouns
in a structured VSM that conveys the hidden re-
lationship in (2). The dimensions of the model
are defined by attributes, such as COLOR, SIZE
or SPEED, while the vector components are deter-
mined on the basis of carefully selected acquisi-
tion patterns that are tailored to capturing the par-
ticular semantic information of interest for r? and
r??. In this respect, lexico-syntactic patterns serve
a similar purpose as dependency relations in Pado?
and Lapata (2007) or Rothenha?usler and Schu?tze
(2009). The upper part of Fig. 1 displays exam-
ples of vectors we build for adjectives and nouns.
Composing vectors along hidden dimensions.
The fine granularity of lexico-syntactic patterns
that capture the triple r comes at the cost of their
sparsity when applied to corpus data. Therefore,
we construct separate vector representations for
r? and r??. Eventually, these representations are
joined by vector composition to reconstruct the
triple r. Apart from avoiding sparsity issues,
this compositional approach has several prospects
from a linguistic perspective as well.
Ambiguity and disambiguation. Building vec-
tors with attributes as meaning dimensions en-
ables us to model (i) ambiguity of adjectives with
regard to the attributes they select, and (ii) the dis-
ambiguation capacity of adjective and noun vec-
tors when considered jointly. Consider, for exam-
ple, the phrase enormous ball that is ambiguous
for two reasons: enormous may select a set of pos-
sible attributes (SIZE or WEIGHT, among others),
while ball elicits several attributes in accordance
with its different word senses2. As seen in Fig. 1,
these ambiguities are nicely captured by the sep-
arate vector representations for the adjective and
the noun (upper part); by composing these repre-
sentations, the ambiguity is resolved (lower part).
3.2 Building a VSM for Adjective-Noun
Meaning
In this section, we introduce the methods we ap-
ply in order to (i) acquire vector representations
for adjectives and nouns, (ii) select appropriate at-
tributes from them, and (iii) compose them.
3.2.1 Attribute Acquisition Patterns
We use the following patterns3 for the ac-
quisition of vectors capturing the tuple r?? =
?attribute, adjective?. Even though some of
these patterns (A1 and A4) match triples of nouns,
attributes and adjectives, we only use them for the
extraction of binary tuples (underlined), thus ab-
stracting from the modified noun.
(A1) ATTR of DT? NN is|was JJ
(A2) DT? RB? JJ ATTR
(A3) DT? JJ or JJ ATTR
(A4) DT? NN?s ATTR is|was JJ
(A5) is|was|are|were JJ in|of ATTR
To acquire noun vectors capturing the tuple
r? = ?noun, attribute?, we rely on the follow-
ing patterns. Again, we only extract pairs, as indi-
cated by the underlined elements.
(N1) NN with|without DT? RB? JJ? ATTR
(N2) DT ATTR of DT? RB? JJ? NN
(N3) DT NN?s RB? JJ? ATTR
(N4) NN has|had a|an RB? JJ? ATTR
2WordNet senses for the noun ball include, among others:
1. round object [...] in games; 2. solid projectile, 3. object
with a spherical shape, 4. people [at a] dance.
3Some of these patterns are taken from Almuhareb (2006)
and Sowa (2000). The descriptions rely on the Penn Tagset
(Marcus et al, 1999). ? marks optional elements.
432
3.2.2 Target Filtering
Some of the adjectives extracted by A1-A5 are
not property-denoting and thus represent noise.
This affects in particular pattern A2, which ex-
tracts adjectives like former or more, or relational
ones such as economic or geographic.
This problem may be addressed in different
ways: By target filtering, extractions can be
checked against a predicative pattern P1 that is
supposed to apply to property-denoting adjectives
only. Vectors that fail this test are suppressed.
(P1) DT NN is|was JJ
Alternatively, extractions obtained from low-
confidence patterns can be awarded reduced
weights by means of a pattern value function (de-
fined in 3.3; cf. Pantel and Pennacchiotti (2006)).
3.2.3 Attribute Selection
We intend to use the acquired vectors in order
to detect attributes that are implicit in adjective-
noun meaning. Therefore, we need a method
that selects appropriate attributes from each vec-
tor. While, in general, this task consists in dis-
tinguishing semantically meaningful dimensions
from noise, the requirements are different depend-
ing on whether attributes are to be selected from
adjective or noun vectors. This is illustrated in
Fig. 1, a typical configuration, with one vector
representing a typical property-denoting adjective
that exhibits relatively strong peaks on one or
more dimensions, whereas noun vectors show a
tendency for broad and flat distributions over their
dimensions. This suggests using a strict selection
function (choosing few very prominent dimen-
sions) for adjectives and a less restrictive one (li-
censing the inclusion of more dimensions of lower
relative prominence) for nouns. Moreover, we are
interested in finding a selection function that re-
lies on as few free parameters as possible in order
to avoid frequency or dimensionality effects.
MPC Selection (MPC). An obvious method
for attribute selection is to choose the most promi-
nent component from any vector (i.e., the highest
absolute value). If a vector exhibits several peaks,
all other components are rejected, their relative
importance notwithstanding. MPC obviously fails
to capture polysemy of targets, which affects ad-
jectives such as hot, in particular.
Threshold Selection (TSel). TSel recasts the
approach of Almuhareb (2006), in selecting all di-
mensions as attributes whose components exceed
a frequency threshold. This avoids the drawback
of MPC, but introduces a parameter that needs to
be optimized. Also, it is difficult to apply absolute
thresholds to composed vectors, as the range of
their components is subject to great variation, and
it is unclear whether the method will scale with
increased dimensionality.
Entropy Selection (ESel). In information the-
ory, entropy measures the average uncertainty in
a probability distribution (Manning and Schu?tze,
1999). We define the entropy H(v) of a
vector v=?v1, . . . , vn? over its components as
H(v) = ??ni=1 P (vi) log P (vi), where P (vi) =
vi/
?n
i=1 vi.
We use H(v) to assess the impact of singular
vector components on the overall entropy of the
vector: We expect entropy to detect components
that contribute noise, as opposed to those that con-
tribute important information.
We define an algorithm for entropy-based at-
tribute selection that returns a list of informa-
tive dimensions. The algorithm successively sup-
presses (combinations of) vector components one
by one. Given that a gain of entropy is equiva-
lent to a loss of information and vice versa, we as-
sume that every combination of components that
leads to an increase in entropy when being sup-
pressed is actually responsible for a substantial
amount of information. The algorithm includes a
back-off to MPC for the special case that a vector
contains a single peak (i.e., H(v) = 0), so that,
in principle, it should be applicable to vectors of
any kind. Vectors with very broad distributions
over their dimensions, however, pose a problem
to this method. For ball in Fig. 1, for instance, the
method does not select any dimension.
Median Selection (MSel). As a further method
we rely on the median m that can be informally
defined as the value that separates the upper from
the lower half of a distribution (Krengel, 2003).
It is less restrictive than MPC and TSel and over-
comes the particular drawback of ESel. Using this
measure, we choose all dimensions whose compo-
nents exceed m. Thus, for the vector representing
433
Pattern Label # Hits (Web) # Hits (ukWaC)
A1 2249 815
A2 36282 72737
A3 3370 1436
A4 ? 7672
A5 ? 3768
N1 ? 682
N2 ? 5073
N3 ? 953
N4 ? 56
Table 1: Number of pattern hits on the Web (Al-
muhareb, 2006) and on ukWaC
ball, WEIGHT, DIRECTION, SHAPE, SPEED and
SIZE are selected.
3.2.4 Vector Composition
We use vector composition as a hinge to com-
bine adjective and noun vectors in order to recon-
struct the triple r=?noun, attribute, adjective?.
Mitchell and Lapata (2008) distinguish two major
classes of vector composition operations, namely
multiplicative and additive operations, that can be
extended in various ways. We use their standard
definitions (denoted ? and +, henceforth). For
our task, we expect ? to perform best as it comes
closest to the linguistic function of intersective ad-
jectives, i.e. to select dimensions that are promi-
nent both for the adjective and the noun, whereas
+ basically blurs the vector components, as can
be seen in the lower part of Fig. 1.
3.3 Model Parameters
We follow Pado? and Lapata (2007) in defining a
semantic space as a matrix M = B?T relating a
set of target elements T to a set of basis elements
B. Further parameters and their instantiations we
use in our model are described below. We use p to
denote an individual lexico-syntactic pattern.
The basis elements of our VSM are nouns de-
noting attributes. For comparison, we use the at-
tributes selected by Almuhareb (2006): COLOR,
DIRECTION, DURATION, SHAPE, SIZE, SMELL,
SPEED, TASTE, TEMPERATURE, WEIGHT.
The context selection function cont(t) deter-
mines the set of patterns that contribute to the rep-
resentation of each target word t ? T . These are
the patterns A1-A5 and N1-N4 (cf. Section 3.2.1).
The target elements represented in the vector
space comprise all adjectives TA that match the
patterns A1 to A5 in the corpus, provided they ex-
ceed a frequency threshold n. During develop-
ment, n was set to 5 in order to filter noise.
As for the target nouns TN , we rely on a repre-
sentative dataset compiled by Almuhareb (2006).
It contains 402 nouns that are balanced with re-
gard to semantic class (according to the WordNet
supersenses), ambiguity and frequency.
As association measure that captures the
strength of the association between the elements
of B and T , we use raw frequency counts4 as ob-
tained from the PoS-tagged and lemmatized ver-
sion of the ukWaC corpus (Baroni et al, 2009).
Table 1 gives an overview of the number of hits
returned by these patterns.
The basis mapping function ? creates the di-
mensions of the semantic space by mapping each
extraction of a pattern p to the attribute it contains.
The pattern value function enables us to sub-
divide dimensions along particular patterns. We
experimented with two instantiations: pvconst
considers, for each dimension, all patterns, while
weighting them equally. pvf (p) awards the ex-
tractions of pattern p with weight 1, while setting
the weights for all patterns different from p to 0.
4 Experiments
We evaluate the performance of the structured
VSM on the task of inferring attributes from
adjective-noun phrases in three experiments: In
Exp1 and Exp2, we evaluate vector representa-
tions capturing r? and r?? independently of one an-
other. Exp3 investigates the selection of hidden
attributes from vector representations constructed
by composition of adjective and noun vectors.
We compare all results against different gold
standards. In Exp1, we follow Almuhareb (2006),
evaluating against WordNet 3.0. For Exp2 and
Exp3, we establish gold standards manually: For
Exp2, we construct a test set of nouns annotated
with their corresponding attributes. For Exp3, we
manually annotate adjective-noun phrases with
the attributes appropriate for the whole phrase. All
experiments are evaluated in terms of precision,
recall and F1 score.
4We experimented with the conditional probability ratio
proposed by Mitchell and Lapata (2009). As it performed
worse on our data, we did not consider it any further.
434
4.1 Exp1: Attribute Selection for Adjectives
The first experiment evaluates the performance of
structured vector representations on attribute se-
lection for adjectives. We compare this model
against a re-implementation of Almuhareb (2006).
Experimental settings and gold standard. To
reconstruct Almuhareb?s approach, we ran his pat-
terns A1-A3 on the ukWaC corpus. Table 1 shows
the number of hits when applied to the Web (Al-
muhareb, 2006) vs. ukWaC. A1 and A3 yield less
extractions on ukWaC as compared to the Web.5
We introduced two additional patterns, A4 and
A5, that contribute about 10,000 additional hits.
We adopted Almuhareb?s manually chosen thresh-
olds for attribute selection for A1-A3; for A4, A5
and a combination of all patterns, we manually se-
lected optimal thresholds.
We experiment with pvconst and all variants of
pvf (p) for pattern weighting (see sect. 3.3). For
attribute selection, we compare TSel (as used by
Almuhareb), ESel and MSel.
The gold standard consists of all adjectives that
are linked to at least one of the ten attributes
we consider by WordNet?s attribute relation
(1063 adjectives in total).
Evaluation results. Results for Exp1 are dis-
played in Table 2. The settings of pv are given in
the rows, the attribute selection methods (in com-
bination with target filtering6) in the columns.
The results for our re-implementation of Al-
muhareb?s individual patterns are comparable to
his original figures7, except for A3 that seems to
suffer from quantitative differences of the under-
lying data. Combining all patterns leads to an
improvement in precision over (our reconstruc-
tion of) Almuhareb?s best individual pattern when
TSel and target filtering are used in combina-
tion. MPC and MSel perform worse (not reported
here). As for target filtering, A1 and A3work best.
Both TSel and ESel benefit from the combina-
tion with the target filter, where the largest im-
provement (and the best overall result) is observ-
5The difference for A2 is an artifact of Almuhareb?s ex-
traction methodology.
6Regarding target filtering, we only report the best filter
pattern for each configuration.
7P(A1)=0.176, P(A2)=0.218, P(A3)=0.504
MPC ESel MSel
P R F P R F P R F
pvf (N1) 0.22 0.06 0.10 0.29 0.04 0.07 0.22 0.09 0.13
pvf (N2) 0.29 0.18 0.23 0.20 0.06 0.09 0.28 0.39 0.33
pvf (N3) 0.34 0.05 0.09 0.20 0.02 0.04 0.25 0.08 0.12
pvf (N4) 0.25 0.02 0.04 0.29 0.02 0.03 0.26 0.02 0.05
pvconst 0.29 0.18 0.22 0.20 0.06 0.09 0.28 0.43 0.34
Table 3: Evaluation results for Experiment 2
able for ESel on pattern A1 only. This is the
pattern that performs worst in Almuhareb?s orig-
inal setting. From this, we conclude that both
ESel and target filtering are valuable extensions
to pattern-based structured vector spaces if preci-
sion is in focus. This also underlines a finding
of Rothenha?usler and Schu?tze (2009) that VSMs
intended to convey specific semantic information
rather than mere similarity benefit primarily from
a linguistically adequate choice of contexts.
Similar to Almuhareb, recall is problematic.
Even though ESel leads to slight improvements,
the scores are far from satisfying. With Al-
muhareb, we note that this is mainly due to a
high number of extremely fine-grained adjectives
in WordNet that are rare in corpora.8
4.2 Exp2: Attribute Selection for Nouns
Exp2 evaluates the performance of attribute selec-
tion from noun vectors tailored to the tuple r??.
Construction of the gold standard. For eval-
uation, we created a gold standard by manually
annotating a set of nouns with attributes. This
gold standard builds on a random sample ex-
tracted from TN (cf. section 3.3). Running N1-
N4 on ukWaC returned semantic vectors for 216
concepts. From these, we randomly sampled 100
concepts that were manually annotated by three
human annotators.
The annotators were provided a matrix consist-
ing of the nouns and the set of ten attributes for
each noun. Their task was to remove all inappro-
priate attributes. They were free to decide how
many attributes to accept for each noun. In order
to deal with word sense ambiguity, the annotators
were instructed to consider all senses of a noun
and to retain every attribute that was acceptable
for at least one sense.
Inter-annotator agreement amounts to ?= 0.69
(Fleiss, 1971). Cases of disagreement were ad-
judicated by majority-voting. The gold standard
435
Almuhareb (reconstr.) VSM (TSel + Target Filter) VSM (ESel) VSM (ESel + Target Filter)
P R F Thr P R F Patt Thr P R F P R F Patt
pvf (A1) = 1 0.183 0.005 0.009 5 0.300 0.004 0.007 A3 5 0.231 0.045 0.076 0.519 0.035 0.065 A3
pvf (A2) = 1 0.207 0.039 0.067 50 0.300 0.033 0.059 A1 50 0.084 0.136 0.104 0.240 0.049 0.081 A3
pvf (A3) = 1 0.382 0.020 0.039 5 0.403 0.014 0.028 A1 5 0.192 0.059 0.090 0.375 0.027 0.050 A1
pvf (A4) = 1 0.301 0.020 0.036 A3 10 0.135 0.055 0.078 0.272 0.020 0.038 A1
pvf (A5) = 1 0.295 0.008 0.016 A3 24 0.105 0.056 0.073 0.315 0.024 0.045 A3
pvconst 0.420 0.024 0.046 A1 183 0.076 0.152 0.102 0.225 0.054 0.087 A3
Table 2: Evaluation results for Experiment 1
contains 424 attributes for 100 nouns.
Evaluation results. Results for Exp2 are given
in Table 3. Performance is lower in comparison to
Exp1. We hypothesize that the tuple r?? might not
be fully captured by overt linguistic patterns. This
needs further investigation in future research.
Against this background, MPC is relatively pre-
cise, but poor in terms of recall. ESel, being
designed to select more than one prominent di-
mension, counterintuitively fails to increase re-
call, suffering from the fact that many noun vec-
tors show a rather flat distribution without any
strong peak. MSel turns out to be most suitable
for this task: Its precision is comparable to MPC
(with N3 as an outlier), while recall is consider-
ably higher. Overall, these results indicate that at-
tribute selection for adjectives and nouns, though
similar, should be viewed as distinct tasks that re-
quire different attribute selection methods.
4.3 Exp3: Attribute Selection for
Adjective-Noun Phrases
In this experiment, we compose noun and adjec-
tive vectors in order to yield a new combined rep-
resentation. We investigate whether the seman-
tic information encoded by the components of this
new vector is sufficiently precise to disambiguate
the attribute dimensions of the original represen-
tations (see section 3.1) and, thus, to infer hidden
attributes from adjective-noun phrases (see (2)) as
advocated by Pustejovsky (1995).
Construction of the gold standard. For evalu-
ation, we created a manually annotated test set of
adjective-noun phrases. We selected a subset of
property-denoting adjectives that are appropriate
modifiers for the nouns from TN using the pred-
icative pattern P1 (see sect. 3) on ukWaC. This
8For instance: bluish-lilac, chartreuse or pink-lavender
as values of the attribute COLOR.
yielded 2085 adjective types that were further re-
duced to 386 by frequency filtering (n = 5). We
sampled our test set from all pairs in the carte-
sian product of the 386 adjectives and 216 nouns
(cf. Exp2) that occurred at least 5 times in a sub-
section of ukWaC. To ensure a sufficient number
of ambiguous adjectives in the test set, sampling
proceeded in two steps: First, we sampled four
nouns each for a manual selection of 15 adjectives
of all ambiguity levels in WordNet. This leads to
60 adjective-noun pairs. Second, another 40 pairs
were sampled fully automatically.
The test set was manually annotated by the
same annotators as in Exp2. They were asked to
remove all attributes that were not appropriate for
a given adjective-noun pair, either because it is not
appropriate for the noun or because it is not se-
lected by the adjective. Further instructions were
as in Exp2, in particular regarding ambiguity.
The overall agreement is ?=0.67. After adjudi-
cation by majority voting, the resulting gold stan-
dard contains 86 attributes for 76 pairs. 24 pairs
could not be assigned any attribute, either because
the adjective did not denote a property, as in pri-
vate investment, or the most appropriate attribute
was not offered, as in blue day or new house.
We evaluate the vector composition methods
discussed in section 3.2.4. Individual vectors for
the adjectives and nouns from the test pairs were
constructed using all patterns A1-A5 and N1-N4.
For attribute selection, we tested MPC, ESel and
MSel. The results are compared against three
baselines: BL-P implements a purely pattern-
based method, i.e. running the patterns that ex-
tract the triple r (A1, A4, N1, N3 and N4, with
JJ and NN instantiated accordingly) on the pairs
from the test set. BL-N and BL-Adj are back-offs
for vector composition, taking the respective noun
or adjective vector, as investigated in Exp1 and
Exp2, as surrogates for a composed vector.
436
MPC ESel MSel
P R F P R F P R F
? 0.60 0.58 0.59 0.63 0.46 0.54 0.27 0.72 0.39
+ 0.43 0.55 0.48 0.42 0.51 0.46 0.18 0.91 0.30
BL-Adj 0.44 0.60 0.50 0.51 0.63 0.57 0.23 0.83 0.36
BL-N 0.27 0.35 0.31 0.37 0.29 0.32 0.17 0.73 0.27
BL-P 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Table 4: Evaluation results for Experiment 3
Evaluation results. Results are given in Table
4. Attribute selection based on the composition of
adjective and noun vectors yields a considerable
improvement of both precision and recall as com-
pared to the individual results obtained in Exp1
and Exp2. Comparing the results of Exp3 against
the baselines reveals two important aspects of our
work. First, the complete failure of BL-P9 un-
derlines the attractiveness of our method to build
structured vector representations from patterns of
reduced complexity. Second, vector composition
is suitable for selecting hidden attributes from
adjective-noun phrases that are jointly encoded
by adjective and noun vectors: Both composition
methods we tested outperform BL-N.
However, the choice of the composition method
matters: ? performs best with a maximum pre-
cision of 0.63. This confirms our expectation
that vector multiplication is a good approxima-
tion for attribute selection in adjective-noun se-
mantics. Being outperformed by BL-Adj in most
categories, + is less suited for this task.
All selection methods outperform BL-Adj in
precision. Comparing MPC and ESel, ESel
achieves better precision when combined with the
?-operator, while doing worse for recall. The
robust performance of MPC is not surprising as
the test set contains only ten adjective-noun pairs
that are still ambiguous with regard to the at-
tributes they elicit. The stronger performance of
the entropy-based method with the ?-operator is
mainly due to its accuracy on detecting false posi-
tives, in that it is able to return ?empty? selections.
In terms of precision, MSel did worse in general,
while recall is decent. This underlines that vector
composition generally promotes meaningful com-
ponents, but MSel is too inaccurate to select them.
Given the performance of the baselines and
the noun vectors in Exp2, we consider this a
very promising result for our approach to attribute
9The patterns used yield no hits for the test pairs at all.
selection from structured vector representations.
The results also corroborate the insufficiency of
previous approaches to attribute learning from ad-
jectives alone.
5 Conclusions and Outlook
We proposed a structured VSM as a framework
for inferring hidden attributes from the composi-
tional semantics of adjective-noun phrases.
By reconstructing Almuhareb (2006), we
showed that structured vector representations of
adjective meaning consistently outperform sim-
ple pattern-based learning, up to 13 pp. in preci-
sion. A combination of target filtering and pat-
tern weighting turned out to be effective here, by
selecting particulary meaningful lexico-syntactic
contexts and filtering adjectives that are not
property-denoting. Further studies need to inves-
tigate this phenomenon and its most appropriate
formulation in a vector space framework.
Moreover, the VSM offers a natural represen-
tation for sense ambiguity of adjectives. Compar-
ing attribute selection methods on adjective and
noun vectors shows that they are sensitive to the
distributional structure of the vectors, and need to
be chosen with care. Future work will investigate
these selection methods in high-dimensional vec-
tors spaces, by using larger sets of attributes.
Exp3 shows that the composition of pattern-
based adjective and noun vectors robustly reflects
aspects of meaning composition in adjective-noun
phrases, with attributes as a hidden dimension.
It also suggests that composition is effective in
disambiguation of adjective and noun meanings.
This hypothesis needs to be substantiated in fur-
ther experiments.
Finally, we showed that composition of vectors
representing complementary meaning aspects can
be beneficial to overcome sparsity effects. How-
ever, our compositional approach meets its lim-
its if the patterns capturing adjective and noun
meaning in isolation are too sparse to acquire suf-
ficiently populated vector components from cor-
pora. For future work, we envisage using vector
similarity to acquire structured vectors for infre-
quent targets from semantic spaces that convey
less linguistic structure to address these remain-
ing sparsity issues.
437
References
Almuhareb, Abdulrahman. 2006. Attributes in Lexi-
cal Acquisition. Ph.D. Dissertation, Department of
Computer Science, University of Essex.
Baroni, Marco and Alessandro Lenci. to appear.
Distributional Memory. A General Framework for
Corpus-based Semantics. Computational Linguis-
tics.
Baroni, Marco, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide web:
A collection of very large linguistically processed
web-crawled corpora. Journal of Language Re-
sources and Evaluation, 43(3):209?226.
Baroni, Marco, Brian Murphy, Eduard Barbu, and
Massimo Poesio. 2010. Strudel. A Corpus-based
Semantic Model of Based on Properties and Types.
Cognitive Science, 34:222?254.
Cimiano, Philipp. 2006. Ontology Learning and Pop-
ulation from Text. Algorithms, Evaluation and Ap-
plications. Springer.
Erk, Katrin and Sebastian Pado?. 2008. A Structured
Vector Space Model for Word Meaning in Context.
In Proceedings of EMNLP, Honolulu, HI.
Fellbaum, Christiane, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press, Cam-
bridge, Mass.
Fleiss, Joseph L. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5):378?382.
Hartung, Matthias and Anette Frank. 2010. A
Semi-supervised Type-based Classification of Ad-
jectives. Distinguishing Properties and Relations.
In Proceedings of the 7th International Conference
on Language Resources and Evaluation, Valletta,
Malta, May.
Hatzivassiloglou, Vasileios and Kathleen McKeown.
1993. Towards the Automatic Identification of Ad-
jectival Scales. Clustering Adjectives According to
Meaning. In Proceedings of the 31st Annual Meet-
ing of the Association of Computational Linguistics,
pages 172?182.
Krengel, Ulrich. 2003. Wahrscheinlichkeitstheorie
und Statistik. Vieweg, Wiesbaden.
Manning, Christopher D. and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. The MIT Press, Cambridge, Mas-
sachusetts.
Marcus, Mitchell P., Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3,
ldc99t42. CD-ROM. Philadelphia, Penn.: Linguis-
tic Data Consortium.
Mitchell, Jeff and Mirella Lapata. 2008. Vector-based
Models of Semantic Composition. In Proceedings
of ACL-08: HLT, pages 236?244, Columbus, Ohio,
June.
Mitchell, Jeff and Mirella Lapata. 2009. Lan-
guage Models Based on Semantic Composition. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, Singa-
pore, August 2009, pages 430?439, Singapore, Au-
gust.
Pado?, Sebastian and Mirella Lapata. 2007.
Dependency-based Construction of Semantic Space
Models. Computational Linguistics, 33:161?199.
Pantel, Patrick and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, Sydney,
Australia, 17?21 July 2006, pages 113?120.
Pustejovsky, James. 1995. The Generative Lexicon.
MIT Press, Cambridge, Mass.
Rothenha?usler, Klaus and Hinrich Schu?tze. 2009. Un-
supervised Classification with Dependency Based
Word Spaces. In Proceedings of the EACL Work-
shop on Geometrical Models of Natural Language
Semantics (GEMS), pages 17?24, Athens, Greece,
March.
Sowa, John F. 2000. Knowledge Representation.
Logical, Philosophical, and Computational Foun-
dations. Brooks Cole.
Turney, Peter D. and Patrick Pantel. 2010. From Fre-
quency to Meaning. Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Widdows, Dominic. 2008. Semantic Vector Products.
Some Initial Investigations. In Proceedings of the
2nd Conference on Quantum Interaction, Oxford,
UK, March.
438
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 958?966,
Beijing, August 2010
Computing EM-based Alignments of Routes and Route Directions as a
Basis for Natural Language Generation
Michael Roth and Anette Frank
Department of Computational Linguistics
Heidelberg University
{mroth,frank}@cl.uni-heidelberg.de
Abstract
Route directions are natural language
(NL) statements that specify, for a given
navigational task and an automatically
computed route representation, a se-
quence of actions to be followed by the
user to reach his or her goal. A corpus-
based approach to generate route direc-
tions involves (i) the selection of elements
along the route that need to be mentioned,
and (ii) the induction of a mapping from
route elements to linguistic structures that
can be used as a basis for NL generation.
This paper presents an Expectation-Maxi-
mization (EM) based algorithm that aligns
geographical route representations with
semantically annotated NL directions, as
a basis for the above tasks. We formu-
late one basic and two extended models,
the latter capturing special properties of
the route direction task. Although our
current data set is small, both extended
models achieve better results than the sim-
ple model and a random baseline. The
best results are achieved by a combination
of both extensions, which outperform the
random baseline and the simple model by
more than an order of magnitude.
1 Introduction
The purpose of route directions is to inform a per-
son, who is typically not familiar with his cur-
rent environment, of how to get to a designated
goal. Generating such directions poses difficul-
ties on various conceptual levels such as planning
the route, selecting landmarks (i.e., recognizable
buildings or structures) and splitting the task into
appropriate single instructions of how to navigate
along the route using the selected landmarks as
reference points.
Previously developed natural language genera-
tion (NLG) systems make use of simple heuristics
for the task of content selection for route direc-
tions (Dale et al, 2005; Roth and Frank, 2009).
In our work, we aim for a corpus-based approach
that can be flexibly modeled after natural, human-
produced directions for varying subtasks (e.g., in-
door vs. outdoor navigation), and that facilitates
multilingual extensions. By employing salient
landmarks and allowing for variation in NL real-
ization, such a system is expected to generate nat-
ural sounding directions that are easier to memo-
rize and easier to follow than directions given by
a classical route planner or navigation system.
NLG for route directions crucially differs from
other generation tasks such as document summa-
rization (Mani, 2001) in that the selection and or-
dering of input structures for language generation
is heavily situation-dependent, i.e., dependent on
the specific properties of a given route to be fol-
lowed.
In line with a corpus-based NLG approach, we
propose to automatically align geographical route
representations as produced by a route planner
with an annotated corpus of NL directions given
by humans for the respective routes. The induced
alignments will (i) serve to identify which ele-
ments of a route to select for verbalization, and (ii)
deliver correspondences between route segments
and linguistic input structures that can be used as
a basis for statistical NL generation. We investi-
958
gate a minimally supervised method for inducing
such alignments to ensure maximal flexibility for
adaptations to different scenarios.
The remainder of this paper is structured as fol-
lows: In Section 2 we discuss related work. Sec-
tion 3 introduces the task, and the representation
formats and resources we use. Section 4 intro-
duces a basic Expectation-Maximization model
and two extensions for the alignment task. Sec-
tion 5 outlines the experiments and presents the
evaluation results. In Section 6 we conclude and
discuss future work.
2 Related Work
Various aspects of route directions have been sub-
ject of research in computational linguistics, rang-
ing from instructional dialogues in MapTask (An-
derson et al, 1991) to recent work on learning
to follow route directions (Vogel and Jurafsky,
2010). However, little work has been done on
generating NL directions based on data from Geo-
graphical Information Systems (Dale et al, 2005;
Roth and Frank, 2009).
NLG systems are typically realized as pipeline
architectures (Reiter and Dale, 2000). As a first
step, they compute a set of messages that rep-
resent the information to be conveyed to a user,
given a specific communicative task (Content Se-
lection). Selecting appropriate content for a task
can be defined heuristically, by manually crafted
rules or by learning content selection rules auto-
matically from corpus data. Previous work by
Dale et al (2005) and Roth and Frank (2009)
on generating NL directions used hand-crafted
heuristics. Duboue and McKeown (2003) were
the first to model content selection as a machine
learning task, in which selection rules are induced
from pairs of human-written text and associated
sets of database entries. They induce baseline se-
lection rules from exact matches of NL expres-
sions with database entries; in addition, class-
based rules are computed by matching database
entry types against NL expressions, using statis-
tical co-occurrence clusters. Barzilay and Lapata
(2005) incorporate the interplay between multiple
events and entities when learning content selec-
tion rules using a special link function.
Recent work by Liang et al (2009) focuses on
modeling grounded language, by aligning real-
world representations with NL text that references
corresponding world states. They show how a
generative model can be used to segment text into
utterances and to identify relevant facts with min-
imal supervision. Both tasks are handled jointly
in a unified framework by training a hierarchical
semi-Markov model on pairs of text and world
states, thereby modeling sequencing effects in the
presentation of facts. While their work is not pri-
marily concerned with NLG, the learned corre-
spondences and their probabilities could be ap-
plied to induce content selection rules and lin-
guistic mappings in a NLG task. The approach is
shown to be effective in scenarios typical for NLG
settings (weather forecasts, RoboCup sportscast-
ing, NFL recaps) that differ in the amount of avail-
able data, length of textual descriptions, and den-
sity of alignments.
In the following, we will adapt ideas from their
EM-based approach to align (segments of) route
representations and NL route directions in a min-
imally supervised manner. We will investigate in-
creasingly refined models that are tailored to the
nature of our task and underlying representations.
In particular, we extend their approach by exploit-
ing semantic markup in the NL direction corpus.
3 Aligning Routes and Directions
In this work we explore the possibility of using
an implementation of the EM algorithm (Demp-
ster et al, 1977) to learn correspondences between
(segments of) the geographical representation of
a route and linguistic instructions of how to fol-
low this route in order to arrive at a designated
goal. We are specifically interested in identifying
which parts of a route are realized in natural lan-
guage and which kinds of semantic constructions
are used to express them.
As a data source for inducing such correspon-
dences we use a parallel corpus of route repre-
sentations and corresponding route directions that
were collected in a controlled experiment for nav-
igation in an urban street network (cf. Schuldes
et al (2009)). For the alignment task, the routes
were compiled to a specification format that has
been realized in an internal version of an online
route planner. Figure 1 displays the route rep-
959
Figure 1: A (partial) route representation of the route segment displayed on the right.
resentation for a small route segment (a junction
connecting ?Hauptstra?e? and ?Leyergasse?). The
corresponding part of a NL route direction is dis-
played in Figure 2. The route representation and
the NL direction share some common concepts:
For example, both contain references to a land-
mark called ?Sudpfanne? (marked as [1]) and a
street named ?Leyergasse? (marked as [2]). Using
pairs of route representations and directions, we
aim to automatically induce alignments between
such correspondences. In the following we de-
scribe our data in more detail.
3.1 Route Representation Format
The route representation format we use (illus-
trated in Figure 1) is an extended version of
the OpenGIS Location Service (OpenLS) Imple-
mentation Standards, a set of XML-based rep-
resentations specified by the Open Geospatial
Consortium1. Previous approaches on extend-
ing the latter with landmarks in an interopera-
1http://www.opengeospatial.org/standards/is
ble way have been presented by Neis and Zipf
(2008). The representation format of our data
has been developed in close collaboration with re-
searchers from Geoinformatics at Heidelberg Uni-
versity2 and adopts ideas previously proposed in
the Cognitive OpenLS specification by Hansen et
al. (2006). The resulting specification will be im-
plemented in an extended (internal) version of the
online route planner OpenRouteService.org.
Our work revolves around two kinds of ele-
ments in this format: so-called maneuvers, i.e., el-
ements that describe a decision point including the
required action and the following route segment,
and landmarks that occur along the route. For the
alignment task we focus on the following types of
attributes that are part of the XML specification,
specified here as Attribute (Element):
directionOfTurn (Maneuver) ? the direction of
movement for the current maneuver, i.e.,
?left?, ?right? or ?straight?
2Chair of GIScience, Alexander Zipf,
http://www.geog.uni-heidelberg.de/lehrstuehle/gis/
960
Figure 2: Directions for the route segment displayed in Figure 1 annotated with frame-semantic markup
and alignment information. The directions translate to ?You start walking from Hauptstra?e towards
Gaststa?tte Sudpfanne, then you turn right onto Leyergasse?
junctionType (Maneuver) ? the type of junction
at the current maneuver, e.g., ?intersection?,
?crossing?
name (JunctionCategory) ? the name of the
junction at the current maneuver, e.g.,
?Hauptstra?e/Leyergasse?
name (NextSegment) ? the name of the street of
the next route segment, e.g., ?Hauptstra?e?
streetName (RouteBranch) ? the street name of
a branch along which the route continues,
e.g., ?Leyergasse?
streetName (NoRouteBranch) ? the street name
of a branch that is not part of the route, e.g.,
?Kisselgasse?
name (Landmark) ? the name of a landmark,
e.g., ?Hotel Sudpfanne?
spatialRelation (UsedLandmark) ? the spatial
relation between a landmark and the current
maneuver, e.g., ?left?, ?right?, ?before?
3.2 A Parallel Corpus of Route Directions
The corpus of route directions used in this work
is a subset of the data collected by Schuldes et al
(2009) in a desk-based experiment. To elicit NL
route directions, subjects were shown a web appli-
cation that guided them along a route by means of
a 2D animation. Subsequently they had to write
NL route directions in German for the shown
routes. The subjects were allowed to use all infor-
mation displayed by the web application: named
places, buildings, bridges and street names, etc.
The resulting directions were POS-tagged with
TreeTagger (Schmid, 1997), dependency-parsed
with XLE (Maxwell and Kaplan, 1993), and man-
ually revised. Additionally, we annotated frame-
semantic markup (Fillmore et al, 2003) and gold
standard alignments to the route representation us-
ing the SALTO annotation tool (Burchardt et al,
2006).
Frame semantic markup. The texts are an-
notated with an inventory of 4 frames relevant
for directions (SELF MOTION, PERCEPTION, BE-
ING LOCATED, LOCATIVE RELATION), with se-
mantic roles (frame elements) such as DIREC-
TION, GOAL, PATH, LOCATION. Figure 2 il-
lustrates a typical example for the use of the
SELF MOTION frame, once with the elements
SOURCE and DIRECTION, and once with the el-
ements DIRECTION and GOAL. Our alignment
model uses the frame semantic annotation as
structuring information.
Gold standard alignments. For evaluation we
constructed gold alignments. We asked two an-
notators to align text parts with corresponding
attributes in the respective route representation3.
The information about corresponding attributes
was added to a single word by manually insert-
3The alignments have not been double annotated, hence
no measure for inter-annotator agreement can be provided.
961
#S #W #FE #aligned FE
avg. per direction 8 98 28 14 (50%)
overall 412 5298 1519 750
Table 1: Corpus statistics: number of sentences
(S), words (W), frame elements (FE) and align-
ments.
#attributes #aligned attr.
avg. per route 115 14 (12%)
overall 921
Table 2: Corpus statistics: total number and per-
centage of relevant attribute alignments.
ing XPATH expressions that unambiguously refer
to the aligned attribute in the route representation
format. For learning the alignment model, the an-
notations were spread to all words in the span of
the respective frame element.
Corpus statistics. We made use of a corpus of
54 NL directions collected for 8 routes in an urban
street network. Tables 1 and 2 give some statis-
tics about the number of words (W) and frame
elements (FE) in the parallel corpus. Comparing
the total number of relevant attributes (as listed in
Section 3.1) and attributes annotated in the gold
alignments (aligned attr.) we note that only 12%
are actually mentioned in NL directions. Thus it
is necessary to select the most salient attributes to
avoid the generation of overly redundant text.
4 Alignment Model
For the induction of alignments between (parts of)
route structures and semantic representations, we
adopt ideas from the models presented in Liang et
al. (2009) (cf. Section 2).
We start from a basic frame alignment model.
It specifies a conditional probability distribution
p(f |a) for the alignment to a frame element f of
type ft (e.g., source, goal, direction) in the frame-
semantic annotation layer given an attribute a of
type at (e.g., streetName, directionOfTurn) in the
route representation format. Note that this model
does not take into account the actual value av of
the attribute a nor the words that are annotated as
part of f . We assume that the frame annotation
represents a reliable segmentation for this align-
ment. This allows us to omit modeling segmenta-
tion explicitly.
As extensions to the basic frame alignment
model, we specify two further models that cap-
ture properties that are specific to the task of di-
rection alignment. As route directions are typi-
cally presented in a linear order with respect to
the route, we incorporate an additional distance
model ? in our alignment. We further account
for word choice within a frame element as an ad-
ditional factor. The word choice model p(w|a)
will exploit attribute type and value information
in the route representations that are reflected in
word choice in the linguistic instructions. Both
extensions are inspired by and share similarities
with models that have been successfully applied
in work on text alignment for the task of machine
translation (Vogel et al, 1996; Tiedemann, 2003).
Our full model is a distribution over frame el-
ements f and words w that factorizes the three
above mentioned parts under the assumption of
independence between each component and each
attribute:
p(f, w|a) = p(f |a)?(dist(f, a)) p(w|a) (1)
The individual models are described in more
detail in the following subsections.
4.1 Frame Alignment Model
This basic frame alignment model specifies the
probabilities p(f |a) for aligning an attribute a of
type at (i.e., one of the types listed in Section 3.1)
to a frame element f labeled as type ft. This
alignment model is initialized as a uniform distri-
bution over f and trained using a straight-forward
implementation of the EM algorithm, following
the well-known IBM Model 1 for alignment in
machine translation (Brown et al, 1993). The ex-
pectation step (E-step) computes expected counts
given occurrences of ft and at under the assump-
tion that all alignments are independent 1:1 corre-
spondences:
count(ft, at) =
?
{?f ?,a??|f ?t=ft?a?t=at} p(f
?|a?)
?
{?f ?,y?|f ?t=ft} p(f
?|y)
(2)
The probabilities are re-estimated to maximize
the overall alignment probability by normalizing
962
the estimated counts (M-step):
p(f |a) = count(ft, at)?
x count(xt, at)
(3)
4.2 Distance Model
We hypothesize that the order of route directions
tends to be consistent with the order of maneuvers
encoded by the route representation. We include
this information in our alignment model by defin-
ing a distance measure dist(f, a) between the rel-
ative position of a frame element f in the text and
the relative position of an attribute a in the route
representation. The probabilities are specified in
form of a distance distribution ?(i) over normal-
ized distances i ? [0 : 1] and learned during EM
training. The weights are initialized as a uniform
distribution and re-estimated in each M-step by
normalizing the estimated counts:
?(i) =
?
{?x,y?| dist(x,y)=i} count(x, y)?
{?x,y?} count(x, y)
(4)
4.3 Word Choice Model
We define a word choice model for word us-
age within a frame element. This additional fac-
tor is necessary to distinguish between various
occurrences of the same type of frame element
with different surface realizations. For exam-
ple, assuming that the frame alignment model
correctly aligns directionOfTurn attributes to a
frame element of type DIRECTION, the word
choice model will provide an additional weight
for the alignment between the value of an attribute
(e.g., ?left?) and the corresponding words within
the frame element (e.g., ?links?). Similarly to
the word choice model within fields in (Liang
et al, 2009), our model specifies a distribution
over words given the attribute a. Depending on
whether the attribute is typed for strings or cate-
gorial values, two different distributions are used.
String Attributes. For string attributes, we de-
termine a weighting factor based on the longest
common subsequence ratio (LCSR). The reason
for using this measure is that we want to allow for
spelling variants and the use of synonymous com-
mon nouns in the description of landmarks and
street names (e.g., ?Main St.? vs. ?Main Street?,
?Texas Steakhouse? vs. ?Texas Restaurant?). The
weighting factor pstr(w|a) for an alignment pair
?f, a? is a constant in the E-step and is calculated
as the LCSR of the considered attribute value av
and the content words w = cw(f) in an anno-
tated frame element f divided by the sum over the
LCSR values of all alignment candidates for a:
pstr(w|a) =
LCSR(av, w)?
x LCSR(av, cw(x))
(5)
Categorial Attributes. We define categorial at-
tributes as attributes that can only take a finite
and prescribed set of values. For these we do
not expect to find matching strings in NL direc-
tions as the attribute values are defined indepen-
dently of the language in use (e.g., values for di-
rectionOfTurn are ?left?, ?right? and ?straight?.
However, the directions in our data set are in Ger-
man, thus containing the lexemes ?links?, ?rechts?
und ?geradeaus? instead). As the set of values
{av ? Dat} for a categorial attribute type at is
finite, we can define and train probability distri-
butions over words for each of them during EM
training. The models are initialized as uniform
distributions and are used as a weighting factor
in the E-Step. We re-calculate the parameters of
a distribution pcat(w|a) in each EM iteration by
normalizing the estimated counts during M-step:
pcat(w|a) =
count(av, w)?
x count(av, x)
(6)
5 Experiments and Results
5.1 Setting
We test the performance of different combinations
of these EM-based models on our data, starting
from a simple baseline model (EM), combined
with the distance (EM+dst) and word choice
models (EM+wrd) and finally the full model
(Full). We perform additional experiments to ex-
amine the impact of different corpus sizes and an
alignment threshold (+thld).
EM is a baseline model that consists of a simple
EM implementation for aligning attributes
and frame elements (equation (3)).
EM+dst consists of the simple EM model and the
additional distance factor (equation (4)).
963
Model P (+thld) R (+thld) F1 (+thld)
Random 2.7 (2.7) 3.9 (3.9) 3.2 (3.2)
EM 2.0 (3.6) 2.9 (3.7) 2.34 (3.6)
EM+dst 7.3 (11.6) 10.8 (11.7) 8.7 (11.6)
EM+wrd 26.8 (36.3) 39.5 (35.5) 32.0 (35.9)
Full 28.9 (38.9) 42.5 (37.9) 34.4 (38.4)
Table 3: Precision (P), Recall (R) and F1 measure
results with and without threshold (+thld) on the
alignment task (all numbers in percentages).
EM+wrd consists of the simple EM model with
the word choice model (equations (5) and
(6), respectively).
Full is the full alignment model including dis-
tance and word choice as described in Sec-
tion 4 (cf. equation (1)).
We use the data set described in Section 3. The
predictions made by the different models are eval-
uated against the gold standard alignments (cf. Ta-
bles 1 and 2). We run a total number of 30 iter-
ations4 of EM training on the complete data set
to learn the parameters of the probability distri-
butions. From the set of all possible 1-to-1 align-
ments, we select the most probable alignments ac-
cording to the model in a way that no attribute and
no frame element is aligned twice.
5.2 Results
We measure precision as the number of predicted
alignments also annotated in the gold standard di-
vided by the total number of alignments generated
by our model. Recall is measured as the number
of correctly predicted alignments divided by the
total number of alignment annotations. As base-
lines we consider a random baseline (obtained
from the average results measured over 1,000 ran-
dom alignment runs) and the simple EM model.
The results in Table 3 show that the simple
EM model performs below the random baseline.
The individual extended models achieve signifi-
cant improvement over the simple model and the
random baseline. While the distance model has a
smaller impact, the influence of the word choice
4This number was determined by experiments as a gen-
eral heuristics.
# directions Precison Recall F1
1 28.94% 42.31% 34.38%
2 29.04% 41.90% 34.31%
3 29.01% 42.18% 34.38%
4 28.75% 41.81% 34.07%
5 29.36% 42.69% 34.79%
6 30.18% 43.91% 35.77%
Table 4: Average results when using only a spe-
cific number of directions for each route with the
model Full (-thld).
model is considerable. Applying the full model
yields further performance gains. We note that for
all models recall is higher compared to precision.
One of the reasons for this phenomenon may be
that the EM-based models align as many attributes
as possible to frame elements in the route direc-
tions. In our gold standard, however, only around
12% of all relevant attributes correspond to frame
elements in the route directions (cf. Section 3.2).
We estimate this quota from a part of the corpus
and use it as an alignment threshold, i.e., for eval-
uation we select the best alignments proposed by
the models, until we reach the threshold. With this
we achieve a F1 measure of 38.40% in a 6-fold
cross validation test. This represents an improve-
ment of 3.97 points and considerably boosts preci-
sion, yielding overall balanced precision (38.90%)
and recall (37.92%).
A general problem of the current setup is the
small amount of available data. With a total of 54
route directions, the data consists of 6 to 8 direc-
tions for each route. We compute a learning curve
by using only exactly 1 to 6 directions per route to
examine whether performance improves with in-
creasing data size. The results are computed as
an average over multiple runs with different data
partitions (see Table 4). The results indicate small
but consistent improvements with increasing data
sizes, however, the differences are minimal. Thus
we are not able to conclude at this point whether
performance increases are possible with the addi-
tion of more data.
5.3 Error Analysis
In an error analysis on the results of the full model,
we found that 363 out of 784 (46%) misalign-
964
ments are related to attributes not aligned in our
gold standard. This is due to the fact that not
all relevant attributes are realized in natural lan-
guage directions. By addressing this problem in
the model Full+threshold, we are able to reduce
these errors, as evidenced by a gain of almost 10
points in precision and 4 points in F1 measure.
We further observe that the word choice model
does not correctly reflect the distribution of cat-
egorial attributes in the parallel corpus. In the
data, we observe that humans often aggregate
multiple occurrences of the same attribute value
into one single utterance. An example of such a
phenomenon can be seen with the attribute type
?directionOfTurn?: Even though ?straight? is the
most common value for this attribute, it is only re-
alized in directions in 33 (5%) cases (compared
to 65% and 47% for ?left? and ?right? respec-
tively). While our EM implementation maximizes
the likelihood for all alignment probabilities based
on expected counts, many pairs are not ? or not
frequently ? found in the corpus. This results in
the model often choosing incorrect alignments for
categorial attributes and makes up for 23% of the
misaligned attributes in total.
We found that further 5% of the attributes are
misaligned with frame elements containing pro-
nouns that actually refer to a different attribute.
As our word choice model does not account for
the use of anaphora, none of the affected frame
elements are aligned correctly. Given the genre
of our corpus, integrating simple heuristics to re-
solve anaphora (e.g., binding to the closest pre-
ceding mention) could solve this problem for the
majority of the cases.
6 Conclusion
We presented a weakly supervised method for
aligning route representations and natural lan-
guage directions on the basis of parallel corpora
using EM-based learning. Our models adopt ideas
from Liang et al (2009) with special adaptations
to the current application scenario. As a major
difference to their work, we make use of frame-
semantic annotations on the NL side as a basis for
segmentation.
While we can show that the extended mod-
els significantly outperform a simple EM-based
model, the overall results are still moderate. We
cannot draw a direct comparison to the results pre-
sented in Liang et al (2009) due to the different
scenarios and data sets. However, the corpus they
used for the NFL recaps scenario is the closest to
ours in terms of available data size and percentage
of aligned records (in our case attributes). For this
kind of corpus, they achieve an F1 score of 39.9%
with the model that is closest to ours (Model 2?).
Their model achieves higher performance for sce-
narios with more available data and a higher per-
centage of alignments. Thus we expect that our
model benefits from additional data sets, which
we plan to gather in web-based settings.
Still, we do not expect to achieve near to per-
fect alignments due to speaker variation, a factor
we also observe in the current data. As our ul-
timate goal is to generate NL instructions from
given route representations, we can nevertheless
make use of imperfectly aligned data for the com-
pilation of high-confidence rules to compute se-
mantic input structures for NLG. Following previ-
ous work by Barzilay and Lee (2002), we can also
exploit the fact that our data consists of multiple
directions for each route to identify alternative re-
alization patterns for the same route segments. In
addition, (semi-)supervised models could be used
to assess the gain we may achieve in comparison
to the minimally supervised setting.
However, we still see potential for improv-
ing our current models by integrating refinements
based on the observations outlined above: Miss-
ing alignment targets on the linguistic side ? es-
pecially due to anaphora, elliptical or aggregating
constructions ? constitute the main error source.
We aim to capture these phenomena within the
linguistic markup in order to provide hidden align-
ment targets. Also, our current model only consid-
ers frame elements as alignment targets. This can
be extended to include their verbal predicates.
Acknowledgements: This work is supported by
the DFG-financed innovation fund FRONTIER as
part of the Excellence Initiative at Heidelberg Uni-
versity (ZUK 49/1). We thank Michael Bauer and
Pascal Neis for the specification of the route repre-
sentation format and Carina Silberer and Jonathan
Geiger for annotation.
965
References
Anderson, Anne H., Miles Bader, Ellen Gurman Bard,
Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,
Stephen Isard, Jacqueline Kowtko, Jan McAllister,
Jim Miller, Catherine Sotillo, Henry Thompson, and
Regina Weinert. 1991. The HCRC Map Task cor-
pus. Language and Speech, 34(4):351?366.
Barzilay, Regina and Mirella Lapata. 2005. Collective
content selection for concept-to-text-generation. In
Proceedings of the Human Language Technology
Conference and the 2005 Conference on Empirical
Methods in Natural Language Processing, Vancou-
ver, B.C., Canada, 6?8 October 2005, pages 331?
338.
Barzilay, Regina and Lillian Lee. 2002. Bootstrapping
lexical choice via multiple-sequence alignment. In
Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, Philadel-
phia, Penn., 6?7 July 2002, pages 164?171.
Brown, Peter F., Vincent J. Della Pietra, Stephan
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19:263?311.
Burchardt, Aljoscha, Katrin Erk, Anette Frank, Andrea
Kowalski, and Sebastian Pado. 2006. SALTO: A
versatile multi-level annotation tool. In Proceedings
of the 5th International Conference on Language
Resources and Evaluation, Genoa, Italy, 22?28 May
2006, pages 517?520.
Dale, Robert, Sabine Geldof, and Jean-Philippe Prost.
2005. Using natural language generation in auto-
matic route description. Journal of Research and
Practice in Information Technology, 37(1):89?106.
Dempster, Arthur P., Nan M. Laird, and Donald B.
Rubin. 1977. Maximum likelihood from incom-
plete data via the EM algorithm. Journal of the
Royal Statistics Society, Series B (Methodological),
39(1):1?38.
Duboue, Pablo A. and Kathleen R. McKeown. 2003.
Statistical acquisition of content selection rules. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing, Sapporo,
Japan, 11?12 July 2003, pages 121?128.
Fillmore, Charles J., Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16(3):235?250.
Hansen, Stefan, Kai-Florian Richter, and Alexander
Klippel. 2006. Landmarks in OpenLS: A data
structure for cognitive ergonomic route directions.
In Proceedings of the 4th International Conference
on Geographic Information Science, Mu?nster, Ger-
many, 20-23 September 2006.
Liang, Percy, Michael Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In Proceedings of ACL-IJCNLP 2009, pages
91?99, August.
Mani, Inderjeet. 2001. Automatic Summarization.
John Benjamins, Amsterdam, Philadelphia.
Maxwell, John T. and Ronald M. Kaplan. 1993.
The interface between phrasal and functional con-
straints. Computational Linguistics, 19(4):571?
590.
Neis, Pascal and Alexander Zipf. 2008. Extending the
OGC OpenLS route service to 3D for an interoper-
able realisation of 3D focus maps with landmarks.
Journal of Location Based Services, 2(2):153?174.
Reiter, Ehud and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge, U.K.:
Cambridge University Press.
Roth, Michael and Anette Frank. 2009. A NLG-based
Application for Walking Directions. In Companion
Volume to the Proceedings of the Joint Conference
of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing,
Singapore, 2?7 August 2009, pages 37?40.
Schmid, Helmut. 1997. Probabilistic Part-of-Speech
tagging using decision trees. In Jones, Daniel and
Harold Somers, editors, New Methods in Language
Processing, pages 154?164. London, U.K.: UCL
Press.
Schuldes, Stephanie, Michael Roth, Anette Frank, and
Michael Strube. 2009. Creating an annotated cor-
pus for generating walking directions. In Proceed-
ings of the ACL-IJCNLP 2009 Workshop on Lan-
guage Generation and Summarisation, Singapore,
6 August 2009, pages 72?76.
Tiedemann, Jo?rg. 2003. Combining Clues for Word
Alignment. In Proceedings of the 10th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL), pages 339?346,
Budapest, Hungary.
Vogel, Adam and Dan Jurafsky. 2010. Learning to
Follow Navigational Directions. In Proceedings of
ACL-2010, Uppsala, Sweden.
Vogel, Stephan, Hermann Ney, and Christoph Till-
mann. 1996. HMM-based Word Alignment in Sta-
tistical Translation. In Proceedings of the 16h Inter-
national Conference on Computational Linguistics
(COLING), pages 836?841, Copenhagen, Denmark.
966
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 540?551,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Exploring Supervised LDA Models for Assigning Attributes to
Adjective-Noun Phrases
Matthias Hartung and Anette Frank
Computational Linguistics Department
Heidelberg University
{hartung,frank}@cl.uni-heidelberg.de
Abstract
This paper introduces an attribute selection
task as a way to characterize the inherent mea-
ning of property-denoting adjectives in adjec-
tive-noun phrases, such as e.g. hot in hot sum-
mer denoting the attribute TEMPERATURE,
rather than TASTE. We formulate this task
in a vector space model that represents adjec-
tives and nouns as vectors in a semantic space
defined over possible attributes. The vectors
incorporate latent semantic information ob-
tained from two variants of LDA topic mod-
els. Our LDA models outperform previous ap-
proaches on a small set of 10 attributes with
considerable gains on sparse representations,
which highlights the strong smoothing power
of LDA models. For the first time, we extend
the attribute selection task to a new data set
with more than 200 classes. We observe that
large-scale attribute selection is a hard prob-
lem, but a subset of attributes performs ro-
bustly on the large scale as well. Again, the
LDA models outperform the VSM baseline.
1 Introduction
Corpus-based statistical modeling of semantics is
gaining increased attention in computational linguis-
tics. This field of research includes distributional
vector space models (VSMs), i.e., models that rep-
resent the semantics of words or phrases as vectors
over high-dimensional cooccurrence data (Turney
and Pantel, 2010; Baroni and Lenci, 2010, i.a.), as
well as latent variable models (LVMs) which aggre-
gate distributional observations in ?hidden?, or latent
variables, thereby reducing the dimensionality of the
data. An example of the latter are topic models (Blei
et al, 2003), which have recently been applied to
modeling selectional preferences of verbs (Ritter et
al., 2010; ?O Se?aghdha, 2010), or word sense disam-
biguation (Li et al, 2010).
A topic that is increasingly studied in distribu-
tional semantics is the semantics of adjectives, both
in isolation (Almuhareb, 2006) and in compositional
adjective-noun phrases (Hartung and Frank, 2010;
Guevara, 2010; Baroni and Zamparelli, 2010).
In this paper, we propose a new approach to a
problem we denote as attribute selection: The task is
to predict the hidden attribute meaning expressed by
a property-denoting adjective in composition with
a noun. The adjective hot, e.g., may denote at-
tributes such as TEMPERATURE, TASTE or EMO-
TIONALITY. These adjective meanings can be com-
bined with nouns such as tea, soup or debate, which
can be characterized in terms of attributes as well.
The goal of the task is to determine the hidden at-
tribute meaning predicated over the noun in a given
adjective-noun phrase, as illustrated in (1).
(1) a. a hotvalue summerconcept
b. TEMPERATURE(summer) = hot
It is by way of the composition of adjective and
noun that specific attributes are selected from the ad-
jective?s space of possible attribute meanings, and
typically lead to a disambiguation of the adjective
and possibly the noun. Hartung and Frank (2010)
were the first to model this insight in a VSM by rep-
resenting the meaning of adjectives and nouns in se-
mantic vectors defined over attributes. The meaning
of adjective-noun phrases is computed by means of
540
C
O
L
O
R
D
IR
E
C
T
IO
N
D
U
R
A
T
IO
N
SH
A
PE
SI
Z
E
SM
E
L
L
SP
E
E
D
TA
ST
E
T
E
M
PE
R
A
T
U
R
E
W
E
IG
H
T
~e 1 1 0 1 45 0 4 0 0 21
~b 14 38 2 20 26 0 45 0 0 20
~e ?~b 14 38 0 20 1170 0 180 0 0 420
~e+~b 15 39 2 21 71 0 49 0 0 41
Figure 1: Vectors for enormous (~e) and ball (~b)
vector composition, such that the ?hidden? attribute
meaning of the phrase can be ?selected? as a promi-
nent component from the composed vector. This is
illustrated in Fig. 1 for the adjective enormous (~e)
in combination with the noun ball (~b), with alter-
native composition operations: vector multiplication
(?) and addition (+).1 Both yield SIZE as the most
prominent component in the composed vector.
In the present paper we offer a new approach to
this formalization of the compositional meaning of
adjectives and nouns that owes to both distributional
VSMs and LVMs. Through this combination, we
attempt to improve on earlier work in Almuhareb
(2006) and Hartung and Frank (2010), which are
both embedded in a purely distributional setting.
Specifically, we use Latent Dirichlet Allocation
(LDA; Blei et al (2003)) to train an attribute model
that captures semantic information encoded in ad-
jectives and nouns independently of one another.
Following Hartung and Frank (2010), this model is
embedded into a VSM that employs vector com-
position to combine the meaning of adjectives and
nouns. We present two variants of LDA that differ
in the way attributes are associated with the induced
LDA topics: Controled LDA (C-LDA) and Labeled
LDA (L-LDA; Ramage et al (2009)). Both will be
presented in detail in Section 3.
Our aims in this paper are two-fold: (i) We inves-
tigate LDA as a modeling framework in the attribute
selection task, as its use of topics as latent variables
may alleviate inherent sparsity problems faced by
prior work using pattern-based (Almuhareb, 2006)
or vector space models (Hartung and Frank, 2010).
(ii) While these prior approaches were restricted to
a confined set of 10 attributes, we will we apply our
1The figure is adopted from the distributional setting of Har-
tung and Frank (2010), with component values defined by pat-
tern frequency counts for the chosen attribute nouns.
models on a much larger space of attributes, to probe
their capacity on a more realistic data set.
The remainder of this paper is divided as fol-
lows. Section 2 reviews related work on distribu-
tional models of adjective semantics, and introduces
the two frameworks in which we ground our ap-
proach: LVMs and VSMs. In Section 3 we introduce
two LDA models for attribute selection: C-LDA and
L-LDA. Section 4 describes the settings for two ex-
periments: In the first experiment, we perform at-
tribute selection confined to a space of 10 attributes
to compare against prior work. In the second setting
we perform attribute selection on a large scale, using
206 attributes. Section 5 presents and discusses the
results. Section 6 concludes.
2 Related Work
Distributional models of adjective semantics.
Almuhareb (2006) aims at capturing the relationship
between adjectives and attributes based on lexico-
syntactic patterns, such as the ATTR of the * is ADJ.
Apart from inherent sparsity issues, his approach
does not account for the compositional nature of the
problem, as the contextual information contributed
by a noun is neglected: For instance, his model is
unable to predict that hot is unlikely to denote TASTE
in the context of summer, other than in hot meal.
Compositionality of adjective-noun phrases and
how it can be adequately modeled in VSMs is
the main concern in Baroni and Zamparelli (2010)
and Guevara (2010), who are in search of the
best composition operator for combining adjective
with noun meanings. While these works adhere
to a purely latent representation of meaning, Har-
tung and Frank (2010) include attributes as sym-
bolic ?hidden? meanings of adjectives, nouns and
adjective-noun phrases in a distributional VSM.
Finally, a large body of work dealing with com-
positionality in distributional frameworks is not con-
fined to the special case of adjective-noun composi-
tion (Mitchell and Lapata (2008), Rudolph and Gies-
brecht (2010), i.a.). All these approaches regard
composition as a process combining vectors (or ma-
trices, resp.) to yield a new, contextualized vector
representation within the same semantic space.
Latent Dirichlet Allocation, aka. Topic Models
(TMs). LDA is a generative probabilistic model
541
for document collections. Each document is repre-
sented as a mixture over latent topics, where each
topic is a probability distribution over words (Blei et
al., 2003). These topics can be used as dense fea-
tures for, e.g., document clustering. Depending on
the number of topics, which has to be pre-specified,
the dimensionality of the document representation
can be considerably reduced in comparison to sim-
ple bag-of-words models. The remainder of this pa-
per will assume some familiarity with LDA and the
LDA terminology as introduced in Blei et al (2003).
Recent work investigates ways of accommodating
supervision with LDA, e.g. supervised topic models
(Blei and McAuliffe, 2007), Labeled LDA (L-LDA)
(Ramage et al, 2009) or DiscLDA (Lacoste-Julien
et al, 2008). We will discuss L-LDA in Section 3.
Distributional VSMs and TMs. The idea to inte-
grate topic models and VSMs goes back to Mitchell
and Lapata (2009) who build a distributional model
with dimensions set to topics over bag-of-words fea-
tures. In their setting, LDA merely serves the pur-
pose of dimensionality reduction, whereas our par-
ticular motivation is to use topics as probabilistic
indicators for the prediction of attributes as seman-
tic target categories in adjective-noun composition.
Mitchell and Lapata (2010) compare VSMs defined
over bags of context words vs. latent topics in a sim-
ilarity judgement task. Their results indicate that a
multiplicative setting works best for vector compo-
sition in word-based models, while vector addition
is better suited for topic vectors.
3 Topic Models for Attribute Selection
3.1 Using LDA for modeling lexical semantics
Recently, LDA has been used for problems in lexical
semantics, where the primary goal is not document
modeling but the induction of semantic knowledge
from high-dimensional co-occurrence data. Ritter et
al. (2010) and ?O Se?aghdha (2010) model selectional
restrictions of verbs by inducing topic distributions
that characterize ?mixtures of topics? observed in
verb argument positions. As a basis for LDA mod-
eling, they collect pseudo-documents, i.e. bags of
words that co-occur in syntactic argument positions.
We apply a similar idea to the attribute selection
problem: we collect pseudo-documents that char-
acterize attributes by adjectives and nouns that co-
occur with the attribute nouns in local contextual re-
lations. The topic distributions obtained from fitting
an LDA model to the collection of these pseudo-
documents can then be injected into semantic vector
representations for adjectives and nouns.
In its original statement, LDA is a fully unsuper-
vised process (apart from the desired number of top-
ics which has to be specified in advance) that es-
timates topic distributions over documents ?d and
topic-word distributions ?t with topics represented
as latent variables. Estimating these parameters on a
document collection yields topic proportions P (t|d)
and topic distributions P (w|t) that can be used to
compute a smooth distribution P (w|d) as in (2),
where t denotes a latent topic, w a word and d a
document in the corpus.
P (w|d) =
?
t
P (w|t)P (t|d) (2)
Being designed for exploratory rather than dis-
criminative analysis, LDA does not intend condi-
tioning of words or topics on external categories.
That is, the resulting topics cannot be related to pre-
viously defined target categories. For attribute se-
lection, the LDA-inferred topics need to be linked
to semantic attributes. Therefore, we apply two ex-
tensions of standard LDA that are capable of taking
supervised category information into account, either
implicitly or directly, by including an additional ob-
servable variable into the generative process.
In general, LVMs can be expected to overcome
sparsity issues that are frequently encountered in
distributional models. This positive smoothing ef-
fect is achieved by marginalization over the latent
variables (cf. Prescher et al (2000)). For instance, it
is unlikely to observe a dependency path linking the
adjective mature to the attribute MATURITY. Such
a relation is more likely for young, for example. If
young co-occurs with mature in a different pseudo-
document (AGE might be a candidate), this results in
a situation where (i) young and mature share one or
more latent topics and (ii) the topic proportions for
the attributes MATURITY and AGE will become sim-
ilar to the extent of common words in their pseudo-
documents. Consequently, the final attribute model
is expected to assign a (small) positive probability to
the relation between mature and MATURITY without
observing it in the training data.
542
3.2 Controled LDA
The generative story behind C-LDA is equivalent to
standard LDA. However, the collection of pseudo-
documents used as input to C-LDA is structured in
a controled way such that each document conveys
semantic information that specifically characterizes
the individual categories of interest (attributes, in
our case). In line with the distributional hypothesis
(Harris, 1968), we consider the pseudo-documents
constructed in this way as distributional fingerprints
of the meaning of the corresponding attribute.
The contents of the pseudo-documents are se-
lected along syntactic dependency paths linking
each attribute noun to meaningful context words (ad-
jectives and nouns).2 A corpus consisting of the two
sentences in (3), e.g., yields a pseudo-document for
the attribute noun SPEED containing car and fast.
(3) What is the speed of this car? The machine
runs at a very fast speed.
Though we are ultimately interested in triples of
attributes, adjectives and nouns that define the com-
positional semantics of adjective-noun phrases (cf.
(1)), C-LDA is only exposed to binary tuples be-
tween attributes and adjectives or nouns, respec-
tively. This is in line with Hartung and Frank
(2010), who obtained substantial performance im-
provements by splitting the ternary relation into two
binary relations.
Presenting LDA with pseudo-documents that cha-
racterize individual target attributes imports super-
vision into the LDA process in two respects: the
estimated topic proportions P (t|d) will be highly
attribute-specific, and similarly so for the topic dis-
tributions P (w|t). This makes the model more ex-
pressive for the ultimate labeling task. Moreover,
since C-LDA collects pseudo-documents focused on
individual target attributes, we are able to link exter-
nal categories to the generative process by heuristi-
cally labeling pseudo-documents with their respec-
tive attribute as target category. Thus, we approx-
imate P (w|a), the probability of a word given an
attribute, by P (w|d) as obtained from LDA:
2The dependency paths, together with the set of attribute
nouns of interest, have to be manually specified. See the sup-
plementary material for the full list of dependency paths used.
1 For each topic k ? {1, . . . , K}:
2 Generate ?k = (?k,1, . . . , ?k,V )T ? Dir(? | ?)
3 For each document d:
4 For each topic k ? {1, . . . ,K}
5 Generate ?(d)k ? {0, 1} ? Bernoulli(? | ?k)
6 Generate ?(d) = L(d) ? ?
7 Generate ?(d) = (?l1 , . . . , ?lMd )
T ? Dir(? | ?(d))
8 For each i in {1, . . . , Nd}:
9 Generate zi ? {?(d)1 , . . . , ?(d)Md} ? Mult(? | ?
(d))
10 Generate wi ? {1, . . . , V } ? Mult(? | ?zi)
Figure 2: L-LDA generative process (Ramage et al 2009)
P (w|a) ? P (w|d) =
?
t
P (w|t)P (t|d) (4)
3.3 Labeled LDA
L-LDA (Ramage et al, 2009) extends standard LDA
to include supervision for specific target categories,
yet in a different way: (i) The generative process
includes a second observed variable, i.e. each doc-
ument is explicitly labeled with a target category.
A document may be labeled with an arbitrary num-
ber of categories; unlabeled documents are also pos-
sible. However, L-LDA permits only binary as-
signments of categories to documents; probabilistic
weights over categories are not intended. (ii) Con-
trary to LDA, where the number of topics has to be
specified in advance, L-LDA sets this parameter to
the number of unique target categories. Moreover,
the model is constrained such that documents may
be assigned only those topics that correspond to their
observable category label(s). That is, latent topics
t in the standard formulation of LDA (2) are con-
strained to correspond to explicit labels a.
More specifically, L-LDA extends the generative
process of LDA by constraining the topic distribu-
tions over documents ?(d) to only those topics that
correspond to the document?s set of labels ?(d). This
is done by projecting the parameter vector of the
Dirichlet topic prior ? to a lower-dimensional vec-
tor ?(d) whose topic dimensions correspond to the
document labels.
This extension is integrated in steps 5 and 6 of
Fig. 2: First, in step 5, the document?s labels ?(d)
are generated for each topic k. The resulting vector
of document?s labels ?(d) = {k | ?(d)k = 1} is used
to define a document-specific label projection matrix
543
L(d)|?(d)|?K , such that L
(d)
ij = 1 if ?
(d)
i = j, and 0 oth-
erwise. This matrix is used in step 6 to project the
Dirichlet topic prior ? to a lower-dimensional vec-
tor ?(d), whose topic dimensions correspond to the
document labels. Topic proportions are then, in step
7, generated for this reduced parameter space.
In our instantiation of L-LDA, we collect pseudo-
documents for attributes exactly as for C-LDA. Doc-
uments are labeled with exactly one category, the at-
tribute noun. Note that, even though the relationship
between documents and topics is fixed, the one be-
tween topics and words is not. Any word occurring
in more than one document will be assigned a non-
zero probability for each corresponding topic.
Thus, with regard to attribute modeling, C-LDA
and L-LDA build an interesting pair of opposites:
The L-LDA model assumes that attributes are se-
mantically primitive in the sense that they cannot
be decomposed into smaller topical units, whereas
words may be associated with several attributes at
the same time. C-LDA, at the other end of the spec-
trum, licenses semantic variability on both the at-
tribute and the word level. Particularly, a word might
be associated with some of the topics underlying an
attribute, but not with all of them, and an attribute
can be characterized by multiple topics.
3.4 Vector Space Framework
For integrating the information obtained from C-
LDA or L-LDA into a distributional VSM, we fol-
low Hartung and Frank (2010): Adjectives and
nouns are modeled as independent semantic vectors
along their relationship to attributes; the most promi-
nent attribute(s) that represent the hidden meaning
of adjective-noun phrases are selected from their
composition (cf. Fig. 1).
The dimensions of the VSM are set to the pre-
selected attributes. Semantic vectors are computed
for all adjectives and nouns occurring at least five
times in the pseudo-documents. Vector component
values v?w,a? are derived from the C-LDA and L-
LDA models in different ways: with C-LDA we
obtain P (w|a) by approximation from P (w|d) (cf.
equation (4)), while in L-LDA we obtain P (w|a) di-
rectly from the induced topic-word distribution ?t,
through labeled topics t = a (cf. equation (2)).
Vector composition is defined as vector multipli-
cation (?) or vector addition (+).
For attribute selection on the composed vector, we
use two methods we found to perform best in Har-
tung and Frank (2010): Entropy Selection (ESel)
and Most Prominent Component (MPC). ESel mea-
sures entropy over the vector components to identify
components that encode a high amount of informa-
tion. It selects all attributes that lead to an increase of
entropy when suppressed from the vector represen-
tation. If no informative components can be detected
in a vector due to a very broad, flat distribution of
the probability mass (cf. ~b in Fig. 1), ESel yields an
empty list. MPC always chooses exactly one vector
component, i.e. the one with the highest value.
4 Experimental Settings
Attribute selection over small and large semantic
spaces. We evaluate the performance of the VSMs
based on C-LDA and L-LDA in two experimental
settings, contrasting the problem of attribute selec-
tion on semantic spaces of radically different dimen-
sionality, using sets of 10 vs. 206 attributes.
Evaluation measures. We evaluate against two
gold standards consisting of adjective-noun phrases
(or adjective-noun pairs) and their associated at-
tribute meanings. We report precision, recall and
f1-score. Where appropriate, we test differences in
the performance of various model configurations for
statistical significance in a randomized permutation
test (Yeh, 2000), using the sigf tool (Pado?, 2006).
Baselines. We compare our models against two
baselines, PATTVSM and DEPVSM. PATTSVM is
reconstructed from Hartung and Frank (2010). It is
grounded in a selection of lexical patterns that iden-
tify the target elements (adjectives and nouns) for
the vector basis elements (i.e., the attribute nouns)
in a local context window. The component values
are defined using raw frequency counts over the ex-
tracted patterns. DEPVSM is similar to PATTVSM;
however, it relies on dependency paths that connect
the target elements and attributes in local contexts.
The paths are identical to the ones used for con-
structing pseudo-documents in C-LDA and L-LDA.
As in PATTVSM, the vector components are set to
raw frequencies over extracted paths.
Implementations. To implement our models, we
rely on MALLET (McCallum, 2002) for C-LDA and
544
the Stanford Topic Modeling Toolbox3 for L-LDA.
In both cases, we run 1000 iterations of Gibbs sam-
pling, using default values for all hyperparameters.
Data set for attribute selection over 10 attributes.
The first experiment is conducted on the data set
used in Hartung and Frank (2010). It consists of
100 adjective-noun pairs manually annotated for
ten attributes: COLOR, DIRECTION, DURATION,
SHAPE, SIZE, SMELL, SPEED, TASTE, TEMPER-
ATURE, WEIGHT. To enable comparison, the di-
mensions of our models are set to exactly these at-
tributes.
Data set for attribute selection over a large se-
mantic space (206 attributes). In the second ex-
periment, we max out the attribute selection task
to a much larger set of attributes in order to an-
alyze the difficulty of the task on more represen-
tative data. We automatically construct a data set
of adjective-noun phrases labeled with appropriate
attributes from WordNet 3.0 (Fellbaum, 1998), re-
lying on the assumption that examples given in
glosses correspond to the respective word sense of
the adjective. We first extract all adjectives that
are linked to at least one attribute synset by the
attribute relation. Next, we run the glosses of
these adjectives (3592 in number) through TreeTag-
ger (Schmid, 1994) to find examples of adjectives
modifying nouns in attributive constructions. The
resulting adjective-noun phrases are labeled with the
attribute label linked to the given adjective sense.
This method yields 7901 labeled adjective-noun
phrases. They are divided into development and test
data according to a sampling procedure that respects
the following criteria: (i) Both sets must contain
all attributes with an equal number of phrases for
each attribute; (ii) phrases with both elements con-
tained in CoreWordNet4 are preferred, while others
are only considered if necessary to satisfy the first
criterion. This procedure yields 496/345 phrases
in the development/test set, distributed over 206 at-
tributes5.
3http://nlp.stanford.edu/software/tmt/.
4A subset of WordNet restricted to the 5000 most fre-
quently used word senses. Available from: http://
wordnetcode.princeton.edu/standoff-files/
core-wordnet.txt
5If an attribute provides only one example, this was added
to the development set. Therefore, the test set only comprises
Training data. The pseudo-documents are collec-
ted from dependency paths obtained from section 2
of the parsed pukWaC corpus (Baroni et al, 2009).
5 Discussion of Results
5.1 Experiment 1
In Experiment 1, we evaluate the performance
of C-LDA and L-LDA on the attribute selection
task over 10 attributes against the pattern-based
and dependency-based models PATTVSM and DE-
PVSM as competitive baselines. Besides a com-
parison to standard VSMs, we are especially in-
terested in the relative performance of the LDA
models. Given that C-LDA and L-LDA estimate
attribute-specific topic distributions in the structured
pseudo-documents under different assumptions re-
garding the correspondence of attributes and topics
(cf. Sec. 3.2 and 3.3), we expect the two LDA vari-
ants to differ in their capability to capture the topic
distributions in the labeled pseudo-documents.
5.1.1 Attribute Selection for 10 Attributes
Tables 1 and 2 summarize the results for at-
tribute selection over 10 attributes against the la-
beled adjective-noun pairs in the test set, using ESel
and MPC as selection functions on vectors com-
posed by multiplication (Table 1) and addition (Ta-
ble 2). The results reported for C-LDA correspond
to the best performing model (with number of top-
ics set to 42, as this setting yields the best and most
constant results over both composition operators).
C-LDA shows highest f-scores and recall over all
settings, and highest precision with vector addition.6
In line with Mitchell and Lapata (2010) (cf. Sec. 2),
we obtain the best overall results with vector addi-
tion (ESel: P: 0.55, R: 0.66, F: 0.61; MPC: P: 0.59,
R: 0.71, F: 0.64). The difference between C-LDA
and L-LDA is small but significant for vector mul-
tiplication; for vector addition, it is not significant.
Compared to the LDA models, the VSM baselines
206 attributes, while all models were trained on 262 attributes
obtained from WordNet in the first extraction step.
6In Tables 1 and 2, statistical significance of the differences
between the models is marked by the superscripts L, D and P,
denoting a significant difference over L-LDA, DepVSM and
PattVSM, respectively. All differences reported are significant
at p < 0.05, except for the difference between C-LDA and L-
LDA in Table 3 (p < 0.1).
545
ESel MPC
P R F P R F
C-LDA 0.58 0.65 0.61L,P 0.57 0.64 0.60
L-LDA 0.68 0.54 0.60D 0.55 0.61 0.58D
DepVSM 0.48 0.58 0.53P 0.57 0.60 0.58
PattVSM 0.63 0.46 0.54 0.60 0.58 0.59
Table 1: Attribute selection over 10 attributes (?)
ESel MPC
P R F P R F
C-LDA 0.55 0.66 0.61D,P 0.59 0.71 0.64
L-LDA 0.53 0.57 0.55D,P 0.50 0.45 0.47D,P
DepVSM 0.38 0.65 0.48P 0.57 0.60 0.58
PattVSM 0.71 0.35 0.47 0.47 0.56 0.51
Table 2: Attribute selection over 10 attributes (+)
are competitive, but tend to perform lower. This ef-
fect is statistically significant for ESel with vector
multiplication: each of the LDA models statistically
significantly outperforms one of the VSM models,
DEPVSM and PATTVSM. With ESel and vector
addition, both LDA models outperform both VSM
models statistically significantly. The LDAESel,+
models outperform the PATTVSMESel,+ model of
Hartung and Frank (2010) by a high margin in
f-score: +0.14 for C-LDA; +0.08 for L-LDA.
Compared to the stronger multiplicative settings
PATTVSMESel,? and PATTVSMMPC,? this still
represents a plus of +0.07 and +0.02 in f-score, re-
spectively. We further observe a clear improvement
of the LDA models over the VSM models in terms of
recall (+0.20, C-LDAESel,+ vs. PATTVSMESel,?),
at the expense of some loss in precision (-0.08, C-
LDAESel,+ vs. PATTVSMESel,?). This clearly con-
firms a stronger generalization power of LDA com-
pared to VSM models.
With regard to selection functions, we observe
that MPC tends to perform better for the VSM mod-
els, while ESel is more suitable in the LDA models.
Figures 3 and 4 display the overall performance
curve ranging over different topic numbers for C-
LDAESel,+ and C-LDAESel,? ? compared to the
remaining models that are not dependent on topic
size. For topic numbers smaller than the attribute set
size, C-LDA underperforms, for obvious reasons.
Increasing ranges of topic numbers to 60 does not
show a linear effect on performance. Parameter set-
tings with performance drops below the VSM base-
lines are rare, which holds particularly for vector ad-
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0  10  20  30  40  50  60
F-
Sc
or
e
Num. Topics
C-LDA
L-LDA
DepVSM
PattVSM
Figure 3: Performance of C-LDAESel,? for different
topic numbers, compared against all other models
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0  10  20  30  40  50  60
F-
Sc
or
e
Num. Topics
C-LDA
L-LDA
DepVSM
PattVSM
Figure 4: Performance of C-LDAESel,+ for different
topic numbers, compared against all other models
dition at topic ranges larger than 10. With vector
addition, C-LDA outperforms L-LDA in almost all
configurations, yet at an overall lower performance
level of L-LDA (0.55 with addition vs. 0.6 with mul-
tiplication). Note that in the multiplicative setting,
C-LDA reaches the performance of L-LDA only in
its best configurations, while with vector addition it
obtains high performance that exceeds L-LDA?s top
f-score of 0.6 for topic ranges between 10 and 20.
Based on these observations, vector addition
seems to offer the more robust setting for C-LDA,
the model that is less strict with regard to topic-
attribute correspondences. Vector multiplication, on
the other hand, is more suitable for L-LDA and its
stricter association of topics with class labels.
5.1.2 Smoothing Power of LDA Models
Our hypothesis was that LDA models should be
better suited for dealing with sparse data, compared
546
ESel MPC
P R F P R F
C-LDA 0.39 0.31 0.35 0.37 0.27 0.32
L-LDA 0.30 0.18 0.23 0.20 0.18 0.19
DepVSM 0.20 0.10 0.13 0.37 0.26 0.30
PattVSM 0.00 0.00 0.00 0.00 0.00 0.00
Table 3: Performance figures on sparse vectors (?)
ESel MPC
P R F P R F
C-LDA 0.43 0.33 0.38 0.44 0.28 0.34
L-LDA 0.34 0.16 0.22 0.37 0.18 0.24
DepVSM 0.16 0.17 0.17 0.36 0.21 0.27
PattVSM 0.13 0.04 0.06 0.17 0.25 0.20
Table 4: Performance figures on sparse vectors (+)
to pattern-based or purely distributional approaches.
While this is broadly confirmed in the above results
by global gains in recall, we conduct a special evalu-
ation focused on those pairs in the test set that suffer
from sparse data. We selected all adjective and noun
vectors that did not yield any positive component
values in the PATTVSM model. The 22 adjective-
noun pairs in the test set affected by these ?zero vec-
tors? were evaluated using the remaining models.
The results in Tables 3 and 4 yield a very clear pic-
ture: C-LDA obtains highest precision, recall and
f-score across all settings, followed by L-LDA and
DEPVSMESel, while their ranks are reversed when
using MPC. Again, MPC works better for the VSM
models, ESel for the LDA models. Vector addition
performs best for C-LDA with f-scores of 0.38 and
0.34 ? outperforming the pattern-based results on
sparse vectors by orders of magnitude.
5.2 Experiment 2
Experiment 2 is designed to max out the space of
attributes to be modeled, to assess the capacity of
both LDA models and the DEPVSM baseline model
in the attribute selection task on a large attribute
space.7 In contrast to Experiment 1, with its con-
fined semantic space of 10 target attributes, this rep-
resents a huge undertaking.
5.2.1 Large-scale Attribute Selection
Table 5 (column all) displays the performance of
all models on attribute selection over a range of 206
7We did not apply PATTVSM to this large-scale experiment,
as only poor performance can be expected.
all property
? + ? +
C-LDA 0.04 0.02 0.18L,D 0.10D
L-LDA 0.03 0.04 0.15 0.15
DepVSM 0.02 0.02 0.12 0.07
Table 5: Performance figures (in f-score) of C-LDAESel
on 206 (all) and 73 property attributes (property)
all property
P R F P R F
WIDTH 0.67 1.00 0.80 1.00 0.50 0.67
WEIGHT 0.80 0.57 0.67 0.50 0.57 0.53
MAGNETISM 0.50 1.00 0.67
SPEED 0.50 0.50 0.50 1.00 0.50 0.67
TEXTURE 0.33 1.00 0.50 0.33 1.00 0.50
DURATION 0.50 0.50 0.50 1.00 1.00 1.00
TEMPERATURE 0.30 0.75 0.43 0.43 0.75 0.55
AGE 0.33 0.50 0.40
THICKNESS 1.00 0.25 0.40 0.50 0.13 0.20
DEGREE 1.00 0.20 0.33
LENGTH 0.17 1.00 0.29 0.50 1.00 0.67
DEPTH 1.00 0.14 0.25 1.00 0.86 0.92
ACTION 0.17 0.50 0.25
LIGHT 0.33 0.17 0.22 0.20 0.17 0.18
POSITION 0.14 0.25 0.18 0.20 0.25 0.22
SHARPNESS 1.00 1.00 1.00
SERIOUSNESS 0.50 1.00 0.67
COLOR 0.13 0.25 0.17 0.29 0.50 0.36
LOYALTY 1.00 1.00 1.00
average 0.49 0.54 0.51 0.63 0.63 0.63
Table 6: Attribute selection on 206 attributes (all) and 73
property attributes (property); performance figures of C-
LDAESel,? for best attributes (F>0)
dimensions, contrasting vector addition and multi-
plication. The number of topics was set to 400. As
the overall performance is close to 0 for both com-
position methods, no parameter setting can be iden-
tified as particularly suited for this large-scale at-
tribute selection task. The differences between the
three models are very small and not significant8.
5.2.2 Focused Evaluation and Data Analysis
To gain a deeper insight into the modeling capac-
ity of the LDA models for this large-scale selection
task, Table 6 (column all) presents a partial evalua-
tion of attributes that could be assigned to adjective-
noun pairs with an f-score >0 by C-LDAESel,?.
Despite the disappointing overall performance of
8Again, statistically significant differences are marked by
superscripts (cf. footnote 6). All differences reported are sig-
nificant at ? < 0.05.
547
prediction correct
thin layer THICKNESS THICKNESS
heavy load WEIGHT WEIGHT
shallow water DEPTH DEPTH
short holiday DURATION DURATION
attractive force MAGNETISM MAGNETISM
short hair LENGTH LENGTH
serious book DIFFICULTY MIND
blue line COLOR UNION
weak president POSITION POWER
fluid society REPUTE CHANGEABLENESS
short flight DISTANCE DURATION
rough bark TEXTURE EVENNESS
faint heart CONSTANCY COWARDICE
Table 7: Sample of correct and false predictions of C-
LDAESel,? in Experiment 2
the LDA models on this large attribute space, it is
remarkable that C-LDA is able to induce distinctive
topic distributions for a number of attributes with up
to 0.51 f-score with balanced precision and recall,
a moderate drop of only -0.10 relative to the corre-
sponding model induced over 10 attributes.
Raising the attribute selection task from 10 to 206
attributes poses a true challenge to our models, by
the sheer size and diversity of the semantic space
considered. Table 7 gives an insight into the nature
of the data and the difficulty of the task, by listing
correct and false preditions of C-LDA for a small
sample of adjective-noun pairs. Possible explana-
tions for false predictions are manifold, among them
near misses (e.g. serious book, weak president, short
flight, rough bark), idiomatic expressions (e.g. faint
heart, blue line) or questionable labels provided by
WordNet (e.g. serious book).
As seen above, C-LDA achieves relatively high
performance figures on selected attributes (cf. Table
6, col. all). In order to identify what makes these
attributes different from others that resist success-
ful modeling, we investigated three factors: (i) the
amount of training data available for each attribute,
(ii) the ambiguity rate per attribute, and (iii) their
ontological subtype.
(i) Measuring the dependence between training
data size and f-score per attribute shows that a large
amount of training data is generally helpful, but not
the decisive factor (Pearson?s r = 0.19, p < 0.01).
(ii) The ambiguity rate ARattr per attribute attr
is computed by averaging over all test pairs TPattr
labeled with attr, counting the total number of at-
tributes attr? that are associated with each adjective
in pairs ?adj, n? ? TPattr in WordNet:
ARattr =
?
attr?
?
?adj,n??TPattr |?adj, attr??WN |
|TPattr |
Correlating this figure with the performance per at-
tribute in terms of f-score yields only a small pos-
itive correlation (Pearson?s r = 0.23, p < 0.01).
In fact, the qualitative analysis in Table 7 shows that
C-LDA is capable of assigning meaningful attributes
to adjective-noun phrases not only in easy, but also
ambiguous cases (cf. shallow water, where DEPTH
is the only attribute provided for shallow in Word-
Net vs. short holiday, short hair or short flight).
(iii) Although the 206 attributes used in Exp. 2 are
rather diverse, including concepts such as HEIGHT,
KINDNESS or INDIVIDUALITY, we observe a high
number of attributes from Exp. 1 that are success-
fully modeled in Exp. 2 (5 out of 10, cf. column
all in Table 6). Given that they are categorized into
the property class in WordNet9, we presume that the
varying performance across attributes might be in-
fluenced by their ontological subtype. This hypoth-
esis is validated in a replication of Exp. 2, with train-
ing data limited to the 73 attributes pertaining to the
property subtype in WordNet. The test set was re-
stricted accordingly, resulting in 112 pairs that are
linked to a property attribute.
The overall performance of the models in this ex-
periment is shown in Table 5 (column property):
With vector multiplication, the best-performing op-
eration across all models, all models benefit consid-
erably (+0.10 or more). C-LDA shows the largest
improvement, significantly outperforming both L-
LDA and DEPVSM. With vector addition, the per-
formance gains are slightly lower in general. In
this setting, L-LDA shows higher f-score than C-
LDA, though this difference is not statistically sig-
nificant. Still, C-LDA significantly outranges DE-
PVSM. Note that we can not show a significant dif-
ference between C-LDAESel,? and L-LDAESel,+,
so the comparison between these models remains in-
conclusive here. Note further that the affinity of C-
LDA with vector addition and L-LDA with vector
multiplication, respectively, is inverted in the large-
scale experiment (cf. Table 5).
9WordNet separates attributes into properties, qualities and
states, among several others.
548
While these overall results are far from satisfac-
tory, they still clearly indicate that the LDA models
work effectively for at least a subset of attributes,
and outperform the VSM baseline.
Again, a more detailed analysis is given in Ta-
ble 6 (column property), showing the performance
of the best individual property attributes (F>0) in
the restricted experiment. Average performance of
the best property attributes with F>0, individually,
amounts to F=0.6310. In comparison to the unres-
tricted setting (cf. column all), nearly all property
attributes benefit from model training on selective
data. Exceptions are WIDTH, WEIGHT, THICKNESS,
AGE, DEGREE and LIGHT. Thus, apparently, some
of the adjectives associated with non-property at-
tributes in the full set provide some discriminative
power that is helpful to distinguish property types.
In a qualitative analysis of the 133 non-property
attributes filtered out in this experiment, we find that
the WordNet-SUMO mapping (Niles, 2003) does
not provide differentiating definitions for about 60%
of these attributes, linking them instead to a single
subjective assessment attribute. This suggests that
in many cases the distinctions drawn by WordNet
are too subtle even for humans to reproduce.
6 Conclusion
This paper explored the use of LDA topic models
in a semantic labeling task that predicts attributes
as ?hidden? meanings in the compositional seman-
tics of adjective-noun phrases. LDA topic models
are expected to alleviate sparsity problems of dis-
tributional VSMs as encountered in prior work, by
incorporating latent semantic information about at-
tribute nouns. We investigated two variants of LDA
that employ different degrees of supervision for as-
sociating topics with attributes.
Our contributions are as follows. We proposed
two LDA models for the attribute selection task that
import supervision for a target category parameter
in different ways: L-LDA (Ramage et al, 2009)
embeds the target categories into the LDA process,
by defining a 1:1 correspondence of topics and tar-
get categories. C-LDA, by contrast, does not af-
fect the LDA generative process. Here, we heuris-
10In comparison, L-LDAESel,? yields an average f-score of
0.47 for attributes with F>0 in the property setting.
tically equate pseudo-documents with target cate-
gories, to approximate category-specific word-topic
distributions. By adhering to standard LDA, C-LDA
accommodates a greater variety in the distributions
of topics to attribute-specific documents and words,
as compared to L-LDA. Combining standard LDA
topic modeling with a means of interpreting the in-
duced topics relative to a set of external categories,
C-LDA offers greater flexibility and expressiveness.
Our experimental results show that modeling at-
tributes as latent or explicit topics with C-LDA and
L-LDA, respectively, outperforms the purely distri-
butional baseline model DEPVSM and PATTVSM
of prior work. Targeted evaluation on sparse data
points confirms that LDA models help to overcome
inherent sparsity effects of VSMs. C-LDA and L-
LDA are close in performance in Experiment 1. C-
LDA outperforms L-LDA only with optimal topic
parameter settings.
Finally, we probed the modeling capacity of LDA
and VSM models on a vast space of 206 attributes.
This task proved to be extremely difficult. However,
we obtain respectable results on a subset of attributes
denoting properties, where C-LDA performs best in
quantitative performance measures. It yields high-
est f-scores in full and partial evaluation ? both with
the full-size attribute model, and when training and
testing is restricted to property attributes. The differ-
ences are small, but statistically significant between
the LDA models and the VSM baseline in a setting
restricted to property attributes.
Data analysis indicates that our models perform
more robustly on concrete attributes in contrast to
abstract attribute types that lack clear categorization.
This suggests that our approach to attribute selec-
tion is most appropriate for detecting attributes that
reflect clear ontological distinctions.
However, there is ample space for improvement.
In Hartung and Frank (2011), we show that the
quality of the noun vectors lags behind the adjec-
tive vectors. This clearly affects the performance
of our models in cases where the semantic contri-
bution of the noun is decisive for disambiguation.
Future work will focus on ways to enhance the noun
vector representations through additional contextual
features, to make them denser and more articulated
in structure.
549
References
Abdulrahman Almuhareb. 2006. Attributes in Lexical
Acquisition. Ph.D. Dissertation, Department of Com-
puter Science, University of Essex.
Marco Baroni and Alessandro Lenci. 2010. Distri-
butional Memory. A General Framework for Corpus-
based Semantics. Computational Linguistics, 36:673?
721.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, East
Stroudsburg, PA, pages 1183?1193.
M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta.
2009. The WaCky Wide Web: A Collection of Very
Large Linguistically Processed Web-Crawled Corpora.
Language Resources and Evaluation, 43:209?226.
D. Blei and J. McAuliffe. 2007. Supervised topic mod-
els. Neural Information Processing Systems, 21.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. JMLR, 3:993?1022.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Semantics,
Stroudsburg, PA. Association for Computational Lin-
guistics.
Zellig Harris. 1968. Mathematical Structures of Lan-
guage. Wiley.
Matthias Hartung and Anette Frank. 2010. A Structured
Vector Space Model for Hidden Attribute Meaning in
Adjective-Noun Phrases. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (COLING), Beijing, China, August.
Matthias Hartung and Anette Frank. 2011. Assessing in-
terpretable, attribute-related meaning representations
for adjective-noun phrases in a similarity prediction
task. In Proceedings of GEometrical Models of Nat-
ural Language Semantics (GEMS-2011), Edinburgh,
UK.
Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan.
2008. DiscLDA: Discriminative Learning for Dimen-
sionality Reduction and Classification. In NIPS, vol-
ume 22.
Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010.
Topic models for word sense disambiguation and
token-based idiom detection. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, ACL ?10, pages 1138?1147, Upp-
sala, Sweden.
Andrew Kachites McCallum. 2002. MAL-
LET: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
Models of Semantic Composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, Ohio, June.
Jeff Mitchell and Mirella Lapata. 2009. Language Mod-
els Based on Semantic Composition. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, Singapore, August 2009,
pages 430?439, Singapore, August.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34:1388?1429.
Ian Niles. 2003. Mapping WordNet to the SUMO Ontol-
ogy. In Proceedings of the IEEE International Knowl-
edge Engineering conference, pages 23?26, June.
Diarmuid ?O Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 435?444, Uppsala, Sweden, July.
Association for Computational Linguistics.
Sebastian Pado?, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
D. Prescher, S. Riezler, and M. Rooth. 2000. Using a
probabilistic class-based lexicon for lexical ambiguity
resolution. In Proceedings of the 18th COLING, pages
649?655.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: A
supervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, Singapore, August 2009, pages 248?256.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A La-
tent Dirichlet Allocation Method for Selectional Pref-
erences. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 424?434, Uppsala, Sweden, July. Association
for Computational Linguistics.
Sebastian Rudolph and Eugenie Giesbrecht. 2010. Com-
positional matrix-space models of language. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 907?916. Asso-
ciation for Computational Linguistics, July.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing (NeMLaP). Manchester, U.K., 14?
16 September 1994.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37:141?
188.
550
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the Fourth Conference on Computational Lan-
guage Learning (CoNLL-2000) and the Second Learn-
ing Language in Logic Workshop, Lisbon, Portugal,
pages 947?953.
551
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 171?182, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Aligning Predicates across Monolingual Comparable Texts
using Graph-based Clustering
Michael Roth and Anette Frank
Department of Computational Linguistics
Heidelberg University
Germany
{mroth,frank}@cl.uni-heidelberg.de
Abstract
Generating coherent discourse is an important
aspect in natural language generation. Our
aim is to learn factors that constitute coherent
discourse from data, with a focus on how to re-
alize predicate-argument structures in a model
that exceeds the sentence level. We present
an important subtask for this overall goal, in
which we align predicates across compara-
ble texts, admitting partial argument struc-
ture correspondence. The contribution of this
work is two-fold: We first construct a large
corpus resource of comparable texts, includ-
ing an evaluation set with manual predicate
alignments. Secondly, we present a novel ap-
proach for aligning predicates across compa-
rable texts using graph-based clustering with
Mincuts. Our method significantly outper-
forms other alignment techniques when ap-
plied to this novel alignment task, by a margin
of at least 6.5 percentage points in F1-score.
1 Introduction
Discourse coherence is an important aspect in natu-
ral language generation (NLG) applications. A num-
ber of theories have investigated coherence inducing
factors. A prominent example is Centering Theory
(Grosz et al1995), which models local coherence
by relating the choice of referring expressions to the
importance of an entity at a certain stage of a dis-
course. A data-driven model based on this theory
is the entity-based approach by Barzilay and Lap-
ata (2008), which models coherence phenomena by
observing sentence-to-sentence transitions of entity
occurrences.
Barzilay and Lapata show that their approach can
discriminate between a coherent and a non-coherent
set of ordered sentences. However, their model is
not able to generate alternative entity realizations by
itself. Furthermore, the entity-based approach only
investigates realization patterns for individual enti-
ties in discourse in terms of core grammatical func-
tions. It does not investigate the interplay between
entity transitions and realization patterns for full-
fledged semantic structures. This interplay, how-
ever, is an important factor for a semantics-based,
generative model of discourse coherence.
The main hypothesis of our work is that we can
automatically learn context-specific realization pat-
terns for predicate argument structures (PAS) from a
semantically parsed corpus of comparable text pairs.
Our assumption builds on the success of previous
research, where comparable and parallel texts have
been exploited for a range of related learning tasks,
e.g., unsupervised discourse segmentation (Barzilay
and Lee, 2004) and bootstrapping semantic analyz-
ers (Titov and Kozhevnikov, 2010).
For our purposes, we are interested in finding cor-
responding PAS across comparable texts that are
known to talk about the same events, and hence in-
volve the same set of underlying event participants.
By aligning predicates in such texts, we can inves-
tigate the factors that determine discourse coher-
ence in the realization patterns for the involved argu-
ments. These include the specific forms of argument
realization, as a pronoun or a specific type of refer-
ential expression, as studied in prior work in NLG
(Belz et al2009, inter alia). The specific set-up
we examine, however, allows us to further investi-
171
gate the factors that govern the non-realization of
an argument position, as a special form of coher-
ence inducing element in discourse. Example (1),
extracted from our corpus of aligned texts,illustrates
this point: Both texts report on the same event of
locating victims in an avalanche. While (1.a) explic-
itly talks about the location of this event, the role re-
mains implicit in the second sentence of (1.b), given
that it can be recovered from the preceding sentence.
In fact, realization of this argument role would im-
pede the fluency of discourse by being overly repet-
itive.
(1) a. . . . The official said that [no bodies]Arg1 had
been recovered [from the avalanches]Arg2 which
occurred late Friday in the Central Asian coun-
try near the Afghan border some 300 kilometers
(185 miles) southeast of the capital Dushanbe.
b. Three other victims were trapped in an
avalanche in the village of Khichikh. [None
of the victims bodies]Arg1 have been found
[ ]Argm-loc.
This phenomenon clearly relates to the problem
of discourse-linking of implicit roles, a very chal-
lenging task in discourse processing.1 In our work,
we consider this problem from a content-based gen-
eration perspective, concentrating on the discourse
factors that allow for the omission of a role.
Thus, our aim is to identify comparable predica-
tions across aligned texts, and to study the discourse
coherence factors that determine the realization pat-
terns of arguments in the respective discourses. This
can be achieved by considering the full set of argu-
ments that can be recovered from the aligned pred-
ications. This paper focuses on the first of these
tasks, henceforth called predicate alignment.2
In line with data-driven approaches in NLP, we
automatically align predicates in a suitable corpus of
paired texts. The induced alignments will (i) serve to
identify events described in both comparable texts,
and (ii) provide information about the underlying ar-
gument structures and how they are realized in each
context to establish a coherent discourse. We in-
vestigate a graph-based clustering method for induc-
1See the recent SemEval 2010 task: Linking Events and
their Participants in Discourse, (Ruppenhofer et al2010).
2Note that we provide details regarding the construction of
a suitable data set and further examples involving non-realized
arguments in a complementary paper (Roth and Frank, 2012).
ing such alignments as clustering provides a suitable
framework to implicitly relate alignment decisions
to one another, by exploiting global information en-
coded in a graph.
The remainder of this paper is structured as fol-
lows: In Section 2, we discuss previous work in re-
lated tasks. Section 3 describes our task and a suit-
able data set. Section 4 introduces a graph-based
clustering model using Mincuts for the alignment of
predicates. Section 5 outlines the experiments and
presents evaluation results. Finally, we conclude in
Section 6 and discuss future work.
2 Related Work
The task of aligning words in general has been stud-
ied extensively in previous work, for example as part
of research in statistical machine translation (SMT).
Typically, alignment models in SMT are trained by
observing and (re-)estimating co-occurrence counts
of word pairs in parallel sentences (Brown et al
1993). The same methods have also been applied
in monolingual settings, for example to align words
in paraphrases (Cohn et al2008). In contrast to
traditional word alignment tasks, our focus is not on
pairs of isolated sentences but on aligning predicates
within the discourse contexts in which they are sit-
uated. Furthermore, text pairs for our task should
not be strictly parallel as we are specifically inter-
ested in the impact of different discourse contexts.
In Section 5, we will show that this particular set-
ting indeed constitutes a more challenging task com-
pared to traditional word alignment in parallel or
paraphrasing sentences.
Another set of related tasks is found in the area of
textual inference. Since 2006, there have been reg-
ular challenges on the task of Recognizing Textual
Entailment (RTE). In the original task description,
Dagan et al2006) define textual entailment ?as a
directional relationship between pairs of text expres-
sions, denoted by T - the entailing ?Text? -, and H
- the entailed ?Hypothesis?. (. . . ) T entails H if the
meaning of H can be inferred from the meaning of
T, as would typically be interpreted by people.? Al-
though this relation does not necessarily require the
presence of corresponding predicates, previous work
by MacCartney et al2008) shows that word align-
ments can serve as a good indicator of entailment.
172
As a matter of fact, the same holds true for the task
of detecting paraphrases. In contrast to RTE, this lat-
ter task requires bi-directional entailments, i.e., each
of the two phrases must entail the other. Wan et al
(2006) show that a simple approach solely based on
word (and lemmatized n-gram) overlap can already
achieve an F1-score of up to 83% for detecting para-
phrases in the Microsoft Research Paraphrase Cor-
pus (Dolan and Brockett, 2005, MSRPC). In fact,
this is just 0.6% points below the state-of-the-art re-
sults recently reported by Socher et al2011).
The MSRPC and data sets from the first RTE
challenges only consisted of isolated pairs of sen-
tences. The Fifth PASCAL Recognizing Textual En-
tailment Challenge (Bentivogli et al2009) intro-
duced a ?Search Task?, where entailing sentences
for a hypothesis have to be found in a set of full
documents. This new task first opened the doors for
assessing the role of discourse (Mirkin et al2010a;
Mirkin et al2010b) in RTE. However, this setting is
still limited as discourse contexts are only provided
for the entailing part (T ) of each text pair but not for
the hypothesis H .
A further task related to ours is the detection
of event coreference. The goal of this task is to
identify all mentions of the same event within a
document and, in some settings, also across docu-
ments. However, the task setting is typically more
restricted than ours in that its focus lies on iden-
tical events/references (cf. Walker et al2006),
Weischedel et al2011), inter alia). In particular,
verbalizations of different aspects of an event (e.g.,
?buy???sell?, ?kill???die?, ?recover???find?) are gen-
erally not linked in this paradigm. In contrast to co-
reference methods that identify chains of events, we
are interested in pairs of corresponding predicates
(and their argument structure), for which we can ob-
serve alternative realizations in discourse.
3 Aligning Predicates Across Texts
This section summarizes how we built a large cor-
pus of comparable texts, as a basis for the predicate
alignment task. We motivate the choice of the cor-
pus and present a strategy for extracting comparable
text pairs. Subsequently, we report on the prepara-
tion of an evaluation data set with manual predicate
alignments across the paired texts. We conclude this
section with an example that showcases the poten-
tial of using aligned predicates for the study of co-
herence phenomena. More detailed information re-
garding corpus creation, annotation guidelines and
additional examples illustrating the potential of this
corpus can be found in Roth and Frank (2012).
3.1 Corpus Creation
The goal of our work is to investigate coherence fac-
tors for argument structure realization, using com-
parable texts that describe the same events, but that
include variation in textual presentation. This re-
quirement fits well with the news domain, for which
we can trace varying textual sources that describe
the same underlying events. The English Gigaword
Fifth Edition (Parker et al2011) corpus (henceforth
just Gigaword) is one of the largest corpus collec-
tions for English. It comprises a total of 9.8 million
newswire articles from seven distinct sources.
In previous work (Roth and Frank, 2012), we in-
troduced GigaPairs, a sub-corpus extracted from Gi-
gaword that includes over 160,000 pairs of newswire
articles from distinct sources. GigaPairs has been
derived from Gigaword using the pairwise similar-
ity method on headlines presented by Wubben et al
(2009). In addition to calculating the similarity of
news titles, we impose an additional date constraint
to further increase the precision of extracted pairs of
texts. Random inspection of about 100 documents
revealed only two texts describing different events.
Overall, we extracted 167,728 document pairs con-
taining a total of 50 million word tokens. Each doc-
ument in this corpus consists of up to 7.564 words
with a mean and median of 301 and 213 words, re-
spectively. All texts have been pre-processed us-
ing MATE tools (Bjo?rkelund et al2010; Bohnet,
2010), a pipeline of NLP modules including a state-
of-the-art semantic role labeler that computes Prop-
Bank/NomBank annotations (Palmer et al2005;
Meyers et al2008).
3.2 Gold Standard Annotation
We selected 70 text pairs from the GigaPairs cor-
pus for manual predicate alignment. All document
pairs were randomly chosen with the constraint that
each text consists of 100 to 300 words.3 Predi-
3This constraint is satisfied by 75.3% of all documents in
GigaPairs.
173
cates identified by the semantic parser are provided
as pre-labeled annotations for alignment. We asked
two students4 to tag corresponding predicates across
each text pair. Following standard practice in word
alignment tasks (cf. Cohn et al2008)) the annota-
tors were instructed to distinguish between sure and
possible alignments, depending on how certainly, in
their opinion, two predicates describe verbalizations
of the same event. The following examples show
predicate pairings marked as sure (2) and as possi-
ble alignments (3).
(2) a. The regulator ruled on September 27 that Nas-
daq too was qualified to bid for OMX [. . . ]
b. The authority [. . . ] had already approved a sim-
ilar application by Nasdaq.
(3) a. Myanmar?s military government said earlier this
year it has released some 220 political prisoners
[. . . ]
b. The government has been regularly releasing
members of Suu Kyi?s National League for
Democracy party [. . . ]
In total, the annotators (A/B) aligned 487/451 sure
and 221/180 possible alignments with a Kappa score
(Cohen, 1960) of 0.86.5 For the construction of a
gold standard, we merged the alignments from both
annotators by taking the union of all possible align-
ments and the intersection of all sure alignments.
Cases which involved a sure alignment on which the
annotators disagreed were resolved in a group dis-
cussion with the first author.
We split the final corpus into a development set
of 10 document pairs and a test set of 60 document
pairs. The test set contains a total of 3,453 predicates
(1,531 nouns and 1,922 verbs). Its gold standard an-
notation consists of 446 sure and 361 possible align-
ments, which corresponds to an average of 7.4 sure
(6.0 possible) alignments per document pair. Most
of the gold alignments (82.4%) are between predi-
cates of the same part-of-speech (242 noun and 423
verb pairs). A total of 383 gold alignments (47.5%)
have been annotated between predicates with iden-
tical lemma form. Diverging numbers of realized
arguments can be observed in 320 pairs (39.7%).
4Both annotators are students in computational linguistics,
one undergraduate (A) and one postgraduate (B) student.
5Following Brockett (2007), we computed agreement on la-
beled annotations, including unaligned predicate pairs as an ad-
ditional null category.
3.3 Potential for Discourse Coherence
This section presents an example of an aligned
predicate pair from our development set that il-
lustrates the potential of aggregating corresponding
PAS across comparable texts. The example repre-
sents one of eleven cases involving unrealized argu-
ments that can be found in our development set of
only ten document pairs.
(4) a. The Chadians said theyArg0 had fled in fear of
their lives.
b. The United Nations says some 20,000
refugeesArg0 have fled into CameroonArg1.
In both sentences, the Arg0 role of the predicate flee
is filled, but Arg1 (here: the goal) has not been real-
ized in (4.a). However, sentence (4.a) is still part of a
coherent discourse, as a role filler for the omitted ar-
gument can be inferred from the preceding context.
For the goal of our work, we are interested in factors
that license such omissions of an argument. Poten-
tial factors on the discourse level include the infor-
mation status of the entity filling an argument posi-
tion, and its salience at the corresponding point in
discourse. Roth and Frank (2012) discuss additional
examples that demonstrate the importance of fac-
tors on further linguistic levels, e.g., lexical choice
of predicates and their syntactic realization.
In the example above, the aggregation of aligned
PAS presents an effective means to identify appro-
priate fillers for unrealized roles. Hence, we can uti-
lize each such pair as one positive and one negative
training instance for a model of discourse coherence
that controls the omissibility of arguments. In what
follows, we introduce an alignment approach that
can be used to automatically acquire more training
data using the entire GigaPairs corpus.
4 Model
For the automatic induction of predicate alignments
across texts, we opt for an unsupervised graph-based
clustering method. In this section, we first define a
graph representation for pairs of documents. In par-
ticular, predicates are represented as nodes in such a
graph and similarities between predicates as edges.
We then proceed to describe various similarity mea-
sures that can be used to identify similar predicate
instances. Finally, we introduce the clustering algo-
rithm that we apply to graphs (representing pairs of
174
documents) in order to induce alignments between
corresponding predicates.
4.1 Graph representation
We build a bipartite graph representation for each
pair of texts, using as vertices the predicate argu-
ment structures assigned in pre-processing (cf. Sec-
tion 3.1). We represent each predicate as a node and
integrate information about arguments only implic-
itly. Given the sets of predicates P1 and P2 of two
comparable texts T1 and T2, respectively, we for-
mally define an undirected graph GP1,P2 as follows:
GP1,P2 = ?V,E? where
V = P1 ? P2
E = P1 ? P2
(1)
Edge weights. We specify the edge weight be-
tween two nodes representing predicates p1 ? P1
and p2 ? P2 as a weighted linear combination of
four similarity measures described in the next sec-
tion: WordNet and VerbNet similarity, Distributional
similarity and Argument similarity.
wp1p2 = ?1 ? simWN(p1, p2)
+ ?2 ? simVN(p1, p2)
+ ?3 ? simDist(p1, p2)
+ ?4 ? simArg(p1, p2)
(2)
Initially we set aleighting parameters ?1 . . . ?4 to
have uniform weights by default. In Section 5, we
define an optimized weighting setting for the indi-
vidual similarity measures.
4.2 Similarity Measures
We employ a number of similarity measures
that make use of complementary information
that is type-based (simWN/VN/Dist) or token-based
(simArg).6 Given two lemmatized predicates p1, p2
and their set of arguments A1 = args(p1), A2 =
args(p2), we define the following measures.
WordNet similarity. Given all pairs of synsets s1,
s2 that contain the predicates p1, p2, respectively,
we compute the maximal similarity using the infor-
mation theoretic measure described in Lin (1998).
Our implementation exploits the WordNet hierarchy
6All token-based frequency counts (i.e., freq() and idf())
are computed over all documents from the AFP and APW parts
of the English Gigaword Fifth Edition.
(Fellbaum, 1998) to find the synset of the least com-
mon subsumer (lcs) and uses the pre-computed In-
formation Content (IC) files from Pedersen et al
(2004) to compute Lin?s measure:
simWN(p1, p2) =
IC(lcs(s1, s2))
IC(s1) ? IC(s2)
(3)
In order to compute similarities between verbal and
nominal predicates, we further use derivation infor-
mation from NomBank (Meyers et al2008): if a
noun represents a nominalization of a verbal pred-
icate, we resort to the corresponding verb synset.
If no relation can be found between two predicates,
we set a default value of simWN = 0. This applies
in particular to all cases that involve a predicate not
present in WordNet.
VerbNet similarity. To overcome systematic
problems with the WordNet verb hierarchy (cf.
Richens (2008)), we further compute similarity
between verbal predicates using VerbNet (Kipper
et al2008). Verbs in VerbNet are categorized into
semantic classes according to their syntactic behav-
ior. A class C can recursively embed sub-classes
Cs ? sub(C) that represent finer semantic and
syntactic distinctions. We define a simple similarity
function that defines fixed similarity scores between
0 and 1 for pairs of predicates p1, p2 depending on
their relatedness within the VerbNet class hierarchy:
simVN(p1, p2) =
?
????
????
1.0 if ?C : p1, p2 ? C
0.8 if ?C,Cs : Cs ? sub(C)
? p1, p2 ? C ? Cs
0.0 else
(4)
Distributional similarity. As some predicates
may not be covered by the WordNet and VerbNet hi-
erarchies, we additionally calculate similarity based
on distributional meaning in a semantic space (Lan-
dauer and Dumais, 1997). Following the traditional
bag-of-words approach that has been applied in re-
lated tasks (Guo and Diab, 2011; Mitchell and La-
pata, 2010), we consider the 2,000 most frequent
context words c1, . . . , c2000 ? C as dimensions of
a vector space and define predicates as vectors using
their Pointwise Mutual Information (PMI):
~p = (PMI(p, c1), . . . ,PMI(p, c2000) (5)
175
with PMI(x, y) =
freq(x, y)
freq(x) ? freq(y)
Given the vector representations of two predicates,
we calculate their similarity as the cosine of the an-
gle between the two vectors:
simDist(p1, p2) =
~p1 ? ~p2
|~p1| ? |~p2|
(6)
Argument similarity. While the previous similar-
ity measures are purely type-based, argument simi-
larity integrates token-based, i.e., discourse-specific,
similarity information about predications by taking
into account the similarity of their arguments. This
measure calculates the association between the ar-
guments A1 of the first and the arguments A2 of the
second predicate by determining the ratio of over-
lapping words in both argument sets.
simArg(p1, p2) =
?
w?A1?A2 idf(w)?
w?A1 idf(w) +
?
w?A2 idf(w)
(7)
In order to give higher weight to (rare) content
words, we weight each word by its Inverse Docu-
ment Frequency (IDF), which we calculate over all
documents d from the AFP and APW sections of the
Gigaword corpus:
idf(w) = log
|D|
|{d : w ? D|}
(8)
Normalization. In order to make the outputs of all
similarity measures comparable, we normalize their
value ranges on the development set to have a mean
and standard deviation of 1.0.
4.3 Mincut-based Clustering
Our graph clustering method uses minimum cuts (or
Mincut) in order to partition the bipartite text graph
into clusters of aligned predicates. A Mincut op-
eration divides a given graph into two disjoint sub-
graphs. Each minimum cut is performed as a cut
between some source node s and some target node
t, such that (i) each of the two nodes will be in a
different sub-graph and (ii) the sum of weights of all
removed edges will be as small as possible. Our sys-
tem determines each Mincut using an implementa-
tion of the method by Goldberg and Tarjan (1986).7
7Basic graph operations are performed using the freely
available Java library JGraph, cf. http://jgrapht.org/.
function CLUSTER(G)
clusters? ?
E ? GETEDGES(G) . Step 1
e? GETEDGEWITHLOWESTWEIGHT(E)
s? GETSOURCENODE(e)
t? GETTARGETNODE(e)
G? ? MINCUT(G, s, t) . Step 2
C ? GETCONNECTEDCOMPONENTS(G?)
for all Gs ? C do . Step 3
if SIZE(Gs) <= 2 then
clusters? clusters ?Gs
else
clusters? clusters ? CLUSTER(Gs)
end if
end for
return clusters;
end function
Figure 2: Pseudo code of our clustering algorithm
As our goal is to induce clusters that correspond to
pairs of similar predicates, we set a maximum num-
ber of two nodes per cluster as stopping criterion.
Given an input graph G, our algorithm recursively
applies Mincuts in three steps as described in Figure
2. Step 1 identifies the edge e with lowest weight in
the given graph G. Step 2 performs the actual Min-
cut operation on G. Finally, the stopping criterion
and recursion are applied in Step 3. An example of
a clustered graph is illustrated in Figure 1.
The advantage of our method compared to off-
the-shelf clustering techniques is two-fold: On the
one hand, the clustering algorithm is free of any pa-
rameters, such as the number of clusters or a clus-
tering threshold, that require fine-tuning. On the
other hand, the approach makes use of a termina-
tion criterion that very well represents the nature of
the goal of our task, namely to align pairs of predi-
cates across comparable texts. The next section pro-
vides empirical evidence for the advantage of this
approach.
5 Experiments
This section evaluates our graph-clustering model
on the task of aligning predicates across compara-
ble texts. For comparison to related tasks and meth-
ods, we describe different evaluation settings, vari-
176
Figure 1: The predicates of two sentences (white: ?The company has said it plans to restate its earnings for 2000
through 2002.?; grey: ?The company had announced in January that it would have to restate earnings (. . . )?) from the
Microsoft Research Paragraph Corpus are aligned by computing clusters with minimum cuts.
ous baselines, as well as results for these baselines
and the model presented above.
5.1 Settings
In order to benchmark our model against tradi-
tional methods for word alignment, we first apply
our graph-based alignment model (Full) on three
sentence-based paraphrase corpora. This model uses
the similarity measures defined in Section 4.2 and
the clustering algorithm introduced in Section 4.3.
In a second experiment, we evaluate Full on our
novel task of inducing predicate alignments across
comparable monolingual texts, using the GigaPairs
data set described in Section 3. We evaluate against
the manually annotated gold alignments in the test
data set described in Section 3.2. To gain more in-
sight into the performance of the various similar-
ity measures included in the Full model, we eval-
uate simplified versions that omit individual similar-
ity measures (Full?[measure name]).
The relative differences in performance against
various baselines will help us quantify the differ-
ences and difficulties between a traditional sentence-
based word alignment setting and our novel align-
ment task that operates on full texts.
5.1.1 Sentence-level Alignment Setting
For sentence-based predicate alignment we make
use of the following three corpora that are word-
aligned subsets of the paraphrase collections de-
scribed in (Cohn et al2008): MTC consists of 100
sentence pairs from the Multiple-Translation Chi-
nese Corpus (Huang et al2002), Leagues contains
100 sentential paraphrases from two translations of
Jules Verne?s ?Twenty Thousand Leagues Under
the Sea?, and MSR is a sub-set of the Microsoft
Research Paraphrase Corpus (Dolan and Brockett,
2005), consisting of 130 sentence pairs. All three
paraphrase collections are in English.
Results for these experiments are reported in Sec-
tion 5.3.1. Note that in order to determine alignment
candidates, we apply the same pre-processing steps
as used for the annotation of our corpus. The se-
mantic parser identified an average number of 3.8,
5.1 and 4.7 predicates per text (i.e., per paraphrase
sentence) in MTC, Leagues and MSR, respectively.
All models are evaluated against the subset of gold
standard alignments (cf. Cohn et al2008)) between
pairs of words marked as predicates.
5.1.2 Text-level Alignment Setting
Results for our own data set, GigaPairs, are reported
in Section 5.3.2. In this setting, models are evaluated
against the annotated gold standard alignments be-
tween predicates as described in Section 3.2. Since
all text pairs in GigaPairs comprise multiple sen-
tences each, the average number of predicates per
text to consider (27.5) is much higher than in the
paraphrase settings. As the full graph representa-
tion becomes rather inefficient to handle (by default,
edges are inserted between all predicate pairs), we
use the development set of 10 text pairs to estimate
177
MTC Leagues MSR
Precision Recall F1 Precision Recall F1 Precision Recall F1
LemmaId 25.1** 74.9 37.6** 31.5** 67.2 42.9** 42.3** 90.8 57.7**
Greedy 74.8** 88.3** 81.0 75.0** 86.0** 80.1 80.7** 97.0** 88.1
WordAlign 99.3 86.6 92.5 98.7 78.5 87.4 99.5 96.0* 97.7*
Full 92.3 72.2 81.1 92.7 69.4 79.4 94.5 88.3 91.3
Table 1: Results for sentence-based predicte alignment in the three benchmark settings MTC, Leagues and MSR (all
numbers in %); results that significantly differ from Full are marked with asterisks (* p<0.05; ** p<0.01).
a threshold on predicate similarity for adding edges.
We tested all thresholds from 1.5 to 4.0 with a step-
size of 0.25 and found 2.5 to perform best. This
threshold is applied in the evaluation of all graph-
based models.
5.2 Baselines
A simple baseline for both settings is to align all
predicates whose lemmas are identical. This base-
line, henceforth called LemmaId, is computed as a
lower bound for all settings. In order to assess the
benefits of the clustering step, we propose a second
baseline that uses the same similarity measures and
thresholds as our Full model, but omits the cluster-
ing step described in Section 4.3. Instead, it greed-
ily computes as many 1-to-1 alignments as possible,
starting from the highest similarity to the learned
threshold (Greedy).
As a more sophisticated baseline, we make
use of alignment tools commonly used in sta-
tistical machine translation (SMT). For the three
sentence-based paraphrase settings MTC, Leagues
and MSR, Cohn et al2008) readily provide
GIZA++ (Och and Ney, 2003) alignments as part
of their word-aligned paraphrase corpus. For the
experiments in the GigaPairs setting, we train our
own word alignment model using the state-of-the-
art word alignment tool Berkeley Aligner (Liang et
al., 2006). As word alignment tools require pairs of
sentences as input, we first extract paraphrases in the
latter setting using a re-implementation of the para-
phrase detection system by Wan et al2006).8 In
the following section, we abbreviate both baselines
using SMT alignment tools as WordAlign.
8Note that the performance of this system lies slightly be-
low the state-of-the-art results reported by Socher et al2011)
However, we were not able to reproduce the results of Socher et
al. using the publicly available release of their software.
5.3 Results
We measure precision as the number of predicted
alignments that are annotated in the gold standard
divided by the total number of predictions. Recall
is measured as the number of correctly predicted
sure alignments divided by the total number of sure
alignments in the gold standard. This conforms to
evaluation measures used for word alignment mod-
els in SMT (Och and Ney, 2003). Following Cohn
et al2008), we subsequently compute the F1-score
as the harmonic mean between precision and recall.
We compute statistical significance of result dif-
ferences with a paired t-test (Cohen, 1995) over the
affected test set documents and provide correspond-
ing significance levels where appropriate.
5.3.1 Sentence-level Predicate Alignment
The results for MTC, Leagues and MSR are pre-
sented in Table 1. The numbers indicate that
WordAlign consistently outperforms all other mod-
els on the three data sets in terms of F1-score. Sta-
tistical significance of result differences between
WordAlign and Full can only be observed for recall
and F1-score on the MSR data set (p<0.05). Other
differences are not significant due to high variance
of results compared to data set sizes.
The overall performance of WordAlign does not
come much as a surprise, seeing that all three data
sets consist of highly parallel sentence pairs. In
fact, the results for LemmaId show that by align-
ing all predicates with identical lemmas, most of the
sure alignments in the three settings are already cov-
ered. The reason for the low precision lies in the
fact that the same lemma can occur multiple times
in the same paraphrase, a phenomenon that is bet-
ter handled by WordAlign, Greedy and Full. In-
terestingly, the Greedy model achieves the highest
recall in all settings but it performs below our Full
178
model in terms of precision and F1-score. The per-
formance differences between Greedy and Full are
statistically significant (p<0.01) regarding precision
and recall.
5.3.2 Text-level Predicate Alignment
We now turn to the experiments on our own data
set, GigaPairs, which comprises full documents
of unequal lengths instead of pairs of single sen-
tences. Table 2 presents the results for our full model
and the three baselines. From all four approaches,
WordAlign yields lowest performance. We observe
two main reasons for this: On the one hand, sen-
tence paraphrase detection does not perform per-
fectly. Hence, the extracted sentence pairs do not
always contain gold alignments. On the other hand,
even sentence pairs that contain gold alignments are
generally less parallel than in the previous settings,
which make them harder to align. The increased dif-
ficulty can also be seen in the results for the Greedy
baseline, which only achieves an F1-score of 20.1%
in this setting. In contrast, we observe that the ma-
jority of all sure alignments (60.3%) can be retrieved
by applying the LemmaId model.
The Full model achieves a recall of 46.6%, but
it significantly outperforms LemmaId (p<0.01) in
terms of precision (58.7%, +18.4 percentage points).
This is an important factor for us, as we plan to use
the alignments in subsequent tasks. With 52.0%,
Full achieves the best overall F1-score.
Ablating similarity measures. All aforemen-
tioned results were conducted in experiments with
a uniform weighting scheme of similarity measures
as introduced in Section 4.3. Table 3 shows the per-
formance impact of individual similarity measures
by removing them completely (i.e., setting their
weight to 0.0). The numbers indicate that not all
measures contribute positively to the overall perfor-
mance when using equal weights. However, a signif-
icant difference can only be observed when remov-
ing the argument similarity measure, which drasti-
cally reduces the results. This clearly highlights the
importance of incorporating the context of individ-
ual predications in this task.
Tuning weights. Subsequently, we tested various
combinations of weights on our development set in
order to estimate a good overall weighting scheme.
Precision Recall F1
LemmaId 40.3** 60.3** 48.3
Greedy 19.6** 20.6** 20.1**
WordAlign 19.7** 15.2** 17.2**
Full 58.7 46.6 52.0
Table 2: Results for GigaPairs (all numbers in %); re-
sults that significantly differ from Full are marked with
asterisks (* p<0.05; ** p<0.01).
Precision Recall F1
Full?WN 58.9 48.0 52.9
Full?VN 57.3 48.7 52.6
Full?Dist 54.3 42.8 47.9
Full?Args 40.1** 24.0** 30.0**
Full 58.7 46.6 52.0
Full+tuned 59.7** 50.7** 54.8**
Table 3: Impact of removing individual measures and us-
ing a tuned weighting scheme (all numbers in %); results
that significantly differ from Full are marked with aster-
isks (* p<0.05; ** p<0.01).
This tuning procedure is implemented as a brute-
force technique, in which we fix the weight of one
similarity measure and allow all other measures to
receive a weight assignment between 0.25 to 5.0
times the fixed weight. Finally, the resulting weights
are normalized to sum to 1.0. We found the best per-
forming weighting scheme to be 0.09, 0.48, 0.24 and
0.19 for ?1, . . . , ?4, respectively (cf. Eq. (2), Section
4). The performance gains of the resulting model
(Full+tuned) can be seen in Table 3. Comput-
ing statistical significance of the result differences
between Full+tuned and all baseline models con-
firmed significant improvements (p<0.01) for both
precision and F1-score.
5.4 Error Analysis
We perform an error analysis on the output of
Full+tuned on the development set of GigaPairs
in order to determine re-occurring problems. In to-
tal, the model missed 13 out of 35 sure alignments
(Type I errors) and predicted 23 alignments not an-
notated in the gold standard (Type II errors).
Six Type I errors (46%) occurred when the lemma
of an affected predicate occurred more than once in a
text and the model missed a correct link. Vice versa,
identical predicates that refer to different events have
179
been the source of 8 Type II errors (35%). We ob-
serve that these errors are frequently related to pred-
icates, such as ?say? and ?appear?, that often occur
in news texts. Altogether, we find 15 Type II errors
(65%) that are due to high predicate similarity de-
spite low argument overlap (cf. Example (5)).
(5) a. The US alert (. . . ) followed intelligence reports
that . . .
b. The Foreign Ministry announcement called on
Japanese citizens to be cautious . . .
We observe that argument overlap itself can be low
even for correct alignments. This clearly indicates
that a better integration of context is needed. Ex-
ample (6.a) illustrates a case in which the agent of
a warning event is not realized. Here, contextual in-
formation is required to correctly align it to the first
warning event in (6.b). This involves inference be-
yond the local PAS.
(6) a. The US alert (. . . ) is one step down from a full
[travel]Arg1 warning [ ]Arg0.
b. Japan has issued a travel alert . . . (which)
follows similar warnings [from Ameri-
can and British authorities]Arg0. (. . . ) An offi-
cial said it was highly unusual for [Tokyo]Arg0
to issue such a warning . . .
6 Conclusion
We presented a novel task for predicate alignment
across comparable monolingual texts, which we ad-
dress using graph-based clustering with Mincuts.
The motivation for this task is to acquire empirical
data for studying discourse coherence factors related
to argument structure realization.
As a first step, we constructed a data set of com-
parable texts that provide full discourse contexts
for alternative verbalizations of the same underlying
events. The data set is derived from all newswire
pairs found in the English Gigaword Fifth Edition
and contains a total of more than 160,000 paired
documents.
A subset of these pairs forms an evaluation set,
annotated with gold alignments that relate predica-
tions, which exhibit a (possibly partial) correspond-
ing argument structure. We established that the an-
notation task, while difficult, can be performed with
good inter-annotator agreement (? at 0.86).
Our main contribution is a novel clustering ap-
proach using Mincuts for aligning predications
across comparable texts. Our experiments estab-
lished that recursive clustering improves on greedy
selection methods by profiting from global infor-
mation encoded in the graph representation. While
the Mincut-based method is in itself unsupervised, a
small amount of development data is needed to tune
parameters for the construction of particularly suit-
able input graphs.
We tested our full model against two additional
baselines: simple heuristic alignment based on iden-
tical lemma forms and a combination of techniques
from SMT and paraphrase detection. The evalua-
tion for our novel task was complemented by a tra-
ditional word alignment task using established para-
phrase data sets. We determined clear differences in
performance for all models for the two types of task
settings. While word alignment methods from SMT
outperform the competing models in the sentence-
based alignment tasks, they perform poorly in the
discourse setting.
In future work, we will enhance our model by
incorporating more refined similarity measures in-
cluding discourse-based criteria. We will further ex-
plore tuning techniques, e.g., a more suitable pre-
selection method for edges in graph construction, in
order to increase either precision or recall. The deci-
sion of optimizing towards one measure or another
is clearly task-dependent. In our case, high preci-
sion is favorable as we plan to learn accurate dis-
course model parameters from the computed align-
ments. Even though such an optimization will result
in an overall lower recall, application of the align-
ment model on the entire GigaPairs corpus can still
provide us with a large amount of precise predicate
alignments. Using this set of alignments, we will
then proceed to exploit contextual information in or-
der to learn a semantic model for discourse coher-
ence in argument structure realization.
Acknowledgements
We are grateful to the Landesgraduiertenfo?rderung
Baden-Wu?rttemberg for funding within the research
initiative ?Coherence in language processing? at
Heidelberg University. We thank Danny Rehl and
Lukas Funk for annotation.
180
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A pi-
lot on semantic textual similarity. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tions, Montreal, Canada, June. to appear.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics, Boston, Mass., 2?7 May 2004,
pages 113?120.
Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.
2009. The grec main subject reference generation
challenge 2009: overview and evaluation results. In
Proceedings of the 2009 Workshop on Language Gen-
eration and Summarisation, pages 79?87.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
pascal recognizing textual entailment challenge. In
Proceedings of TAC.
Anders Bjo?rkelund, Bernd Bohnet, Love Hafdell, and
Pierre Nugues. 2010. A high-performance syntac-
tic and semantic dependency parser. In Coling 2010:
Demonstration Volume, pages 33?36, Beijing, China,
August. Coling 2010 Organizing Committee.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 89?97, Beijing, China,
August. Coling 2010 Organizing Committee.
Chris Brockett. 2007. Aligning the RTE 2006 Corpus.
Microsoft Research.
Peter F. Brown, Vincent J. Della Pietra, Stephan A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263?311.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20:37?46.
Paul R. Cohen. 1995. Empirical methods for artificial
intelligence. MIT Press, Cambridge, MA, USA.
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing Corpora for Development and
Evaluation of Paraphrase Systems. 34(4).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In J. Quin?onero-Candela, I. Dagan, and
B. Magnini, editors, Machine Learning Challenges,
pages 177?190. Springer, Heidelberg, Germany.
William B. Dolan and Chris Brockett. 2005. Automat-
ically constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop on
Paraphrasing.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Adrew V. Goldberg and Robert E. Tarjan. 1986. A
new approach to the maximum flow problem. In Pro-
ceedings of the eighteenth annual ACM symposium on
Theory of computing, pages 136?146, New York, NY,
USA.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Weiwei Guo and Mona Diab. 2011. Semantic topic mod-
els: Combining word distributional statistics and dic-
tionary definitions. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 552?561, July.
Shudong Huang, David Graff, and George Doddington.
2002. Multiple-Translation Chinese Corpus. Linguis-
tic Data Consortium, Philadelphia.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A Large-scale Classification
of English Verbs. 42(1):21?40.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to Plato?s problem: The Latent Semantic Anal-
ysis theory of the acquisition, induction, and represen-
tation of knowledge. Psychological Review, 104:211?
240.
Percy Liang, Benjamin Taskar, and Dan Klein. 2006.
Alignment by agreement. In North American Associ-
ation for Computational Linguistics (NAACL), pages
104?111.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th Inter-
national Conference on Machine Learning, Madison,
Wisc., 24?27 July 1998, pages 296?304.
Bill MacCartney, Michael Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, Waikiki, Honolulu, Hawaii, 25-
27 October 2008.
Adam Meyers, Ruth Reeves, and Catherine Macleod.
2008. NomBank v1.0. Linguistic Data Consortium,
Philadelphia.
Shachar Mirkin, Jonathan Berant, Ido Dagan, and Eyal
Shnarch. 2010a. Recognising entailment within dis-
181
course. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
Beijing, China, August. Coling 2010 Organizing Com-
mittee.
Shachar Mirkin, Ido Dagan, and Sebastian Pado?. 2010b.
Assessing the role of discourse references in entail-
ment inference. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, Uppsala, Sweden, 11?16 July 2010.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. 34(8):1388?
1429.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
29(1):19?51.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
105.
Robert Parker, David Graff, Jumbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion. Linguistic Data Consortium, Philadelphia.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity ? Measuring the re-
latedness of concepts. In Companion Volume to the
Proceedings of the Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, Boston, Mass.,
2?7 May 2004, pages 267?270.
Tom Richens. 2008. Anomalies in the wordnet verb hier-
archy. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 729?736. Association for Computational Lin-
guistics.
Michael Roth and Anette Frank. 2012. Aligning pred-
icate argument structures in monolingual comparable
texts: A new corpus for a new task. In Proceedings
of the First Joint Conference on Lexical and Computa-
tional Semantics, Montreal, Canada, June.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. SemEval-
2010 Task 10: Linking Events and Their Participants
in Discourse. In Proceedings of the 5th International
Workshop on Semantic Evaluations, pages 45?50, Up-
psala, Sweden, July.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural Infor-
mation Processing Systems (NIPS 2011).
Ivan Titov and Mikhail Kozhevnikov. 2010. Bootstrap-
ping semantic analyzers from non-contradictory texts.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, Uppsala, Swe-
den, 11?16 July 2010, pages 958?967.
Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. ACE 2005 Multilin-
gual Training Corpus. Linguistic Data Consortium,
Philadelphia.
Stephen Wan, Mark Dras, Robert Dale, and Cecile Paris.
2006. Using dependency-based features to take the
?Para-farce? out of paraphrase. In Proceedings of the
Australasian Language Technology Workshop, pages
131?138.
Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-
uard Hovy, Sameer Pradhan, Lance Ramshaw, Nian-
wen Xue, Ann Taylor, Jeff Kaufman, Michelle Fran-
chini, Mohammed El-Bachouti, Robert Belvin, and
Ann Houston. 2011. OntoNotes Release 4.0. Lin-
guistic Data Consortium, Philadelphia.
Sander Wubben, Antal van den Bosch, Emiel Krahmer,
and Erwin Marsi. 2009. Clustering and matching
headlines for automatic paraphrase acquisition. In
Proceedings of the 12th European Workshop on Nat-
ural Language Generation (ENLG 2009), pages 122?
125, Athens, Greece, March. Association for Compu-
tational Linguistics.
182
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 40?49,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Identifying Generic Noun Phrases
Nils Reiter and Anette Frank
Department of Computational Linguistics
Heidelberg University, Germany
{reiter,frank}@cl.uni-heidelberg.de
Abstract
This paper presents a supervised approach
for identifying generic noun phrases in
context. Generic statements express rule-
like knowledge about kinds or events.
Therefore, their identification is important
for the automatic construction of know-
ledge bases. In particular, the distinction
between generic and non-generic state-
ments is crucial for the correct encoding
of generic and instance-level information.
Generic expressions have been studied ex-
tensively in formal semantics. Building
on this work, we explore a corpus-based
learning approach for identifying generic
NPs, using selections of linguistically mo-
tivated features. Our results perform well
above the baseline and existing prior work.
1 Introduction
Generic expressions come in two basic forms:
generic noun phrases and generic sentences. Both
express rule-like knowledge, but in different ways.
A generic noun phrase is a noun phrase that
does not refer to a specific (set of) individual(s),
but rather to a kind or class of individuals. Thus,
the NP The lion in (1.a)1 is understood as a ref-
erence to the class ?lion? instead of a specific in-
dividual. Generic NPs are not restricted to occur
with kind-related predicates as in (1.a). As seen
in (1.b), they may equally well be combined with
predicates that denote specific actions. In contrast
to (1.a), the property defined by the verb phrase in
(1.b) may hold of individual lions.
(1) a. The lion was the most widespread mam-
mal.
b. Lions eat up to 30 kg in one sitting.
1All examples are taken from Wikipedia unless stated oth-
erwise.
Generic sentences are characterising sentences
that quantify over situations or events, expressing
rule-like knowledge about habitual actions or situ-
ations (2.a). This is in contrast with sentences that
refer to specific events and individuals, as in (2.b).
(2) a. After 1971 [Paul Erdo?s] also took am-
phetamines.
b. Paul Erdo?s was born [...] on March 26,
1913.
The genericity of an expression may arise from
the generic (kind-referring, class-denoting) inter-
pretation of the NP or the characterising interpre-
tation of the sentence predicate. Both sources may
concur in a single sentence, as illustrated in Ta-
ble 1, where we have cross-classified the exam-
ples above according to the genericity of the NP
and the sentence.
This classification is extremely difficult, be-
cause (i) the criteria for generic interpretation are
far from being clear-cut and (ii) both sources of
genericity may freely interact.
S[gen+] S[gen-]
NP[gen+] (1.b) (1.a)
NP[gen-] (2.a) (2.b)
Table 1: Generic NPs and generic sentences
The above classification of generic expressions
is well established in traditional formal semantics
(cf. Krifka et al (1995))2. As we argue in this
paper, these distinctions are relevant for semantic
processing in computational linguistics, especially
for information extraction and ontology learning
and population tasks. With appropriate semantic
analysis of generic statements, we can not only
formally capture and exploit generic knowledge,
2The literature draws some finer distinctions including as-
pects like specificity, which we will ignore in this work.
40
but also distinguish between information pertain-
ing to individuals vs. classes. We will argue that
the automatic identification of generic expressions
should be cast as a machine learning problem in-
stead of a rule-based approach, as there is (i) no
transparent marking of genericity in English (as in
most other European languages) and (ii) the phe-
nomenon is highly context dependent.
In this paper, we build on insights from for-
mal semantics to establish a corpus-based ma-
chine learning approach for the automatic classi-
fication of generic expressions. In principle our
approach is applicable to the detection of both
generic NPs and generic sentences, and in fact it
would be highly desirable and possibly advanta-
geous to cover both types of genericity simulta-
neously. Our current work is confined to generic
NPs, as there are no corpora available at present
that contain annotations for genericity at the sen-
tence level.
The paper is organised as follows. Section 2 in-
troduces generic expressions and motivates their
relevance for knowledge acquisition and semantic
processing tasks in computational linguistics. Sec-
tion 3 reviews prior and related work. In section 4
we motivate the choice of feature sets for the au-
tomatic identification of generic NPs in context.
Sections 5 and 6 present our experiments and re-
sults obtained for this task on the ACE-2 data set.
Section 7 concludes.
2 Generic Expressions & their Relevance
for Computational Linguistics
2.1 Interpretation of generic expressions
Generic NPs There are two contrasting views
on how to formally interpret generic NPs. Ac-
cording to the first one, a generic NP involves a
special form of quantification. Quine (1960), for
example, proposes a universally quantified read-
ing for generic NPs. This view is confronted with
the most important problem of all quantification-
based approaches, namely that the exact determi-
nation of the quantifier restriction (QR) is highly
dependent on the context, as illustrated in (3)3.
(3) a. Lions are mammals. QR: all lions
b. Mammals give birth to live young. QR:
less than half of all mammals
3Some of these examples are taken from Carlson (1977).
c. Rats are bothersome to people. QR: few
rats4
In view of this difficulty, several approaches
restrict the quantification to only ?relevant? (De-
clerck, 1991) or ?normal? (Dahl, 1975) individu-
als.
According to the second view, generic noun
phrases denote kinds. Following Carlson (1977),
a kind can be considered as an individual that has
properties on its own. On this view, the generic NP
cannot be analysed as a quantifier over individuals
pertaining to the kind. For some predicates, this
is clearly marked. (1.a), for instance, attributes a
property to the kind lion that cannot be attributed
to individual lions.
Generic sentences are usually analysed using a
special dyadic operator, as first proposed by Heim
(1982). The dyadic operator relates two semantic
constituents, the restrictor and the matrix:
Q[x1, ..., xi]([x1, ..., xi]
? ?? ?
Restrictor
; ?y1, ..., yi[x1, .., xi, y1, ..., yi]
? ?? ?
Matrix
)
By choosing GEN as a generic dyadic operator,
it is possible to represent the two readings (a) and
(b) of the characterising sentence (4) by variation
in the specification of restrictor and matrix (Krifka
et al, 1995).
(4) Typhoons arise in this part of the pacific.
(a) Typhoons in general have a common ori-
gin in this part of the pacific.
(b) There arise typhoons in this part of the pa-
cific.
(a?) GEN[x; y](Typhoon(x);this-part-of-the-
pacific(y)?arise-in(x, y))
(b?) GEN[x; y](this-part-of-the-
pacific(x);Typhoon(y)?arise-in(y, x))
In order to cope with characterising sentences
as in (2.a), we must allow the generic operator
to quantify over situations or events, in this case,
?normal? situations which were such that Erdo?s
took amphetamines.
2.2 Relevance for computational linguistics
Knowledge acquisition The automatic acquisi-
tion of formal knowledge for computational appli-
cations is a major endeavour in current research
4Most rats are not even noticed by people.
41
and could lead to big improvements of semantics-
based processing. Bos (2009), e.g., describes sys-
tems using automated deduction for language un-
derstanding tasks using formal knowledge.
There are manually built formal ontologies
such as SUMO (Niles and Pease, 2001) or Cyc
(Lenat, 1995) and linguistic ontologies like Word-
Net (Fellbaum, 1998) that capture linguistic and
world knowledge to a certain extent. However,
these resources either lack coverage or depth. Au-
tomatically constructed ontologies or taxonomies,
on the other hand, are still of poor quality (Cimi-
ano, 2006; Ponzetto and Strube, 2007).
Attempts to automatically induce knowledge
bases from text or encyclopaedic sources are cur-
rently not concerned with the distinction between
generic and non-generic expressions, concentrat-
ing mainly on factual knowledge. However, rule-
like knowledge can be found in textual sources in
the form of generic expressions5.
In view of the properties of generic expressions
discussed above, this lack of attention bears two
types of risks. The first concerns the distinction
between classes and instances, regarding the attri-
bution of properties. The second concerns mod-
elling exceptions in both representation and infer-
encing.
The distinction between classes and instances
is a serious challenge even for the simplest
methods in automatic ontology construction, e.g.,
Hearst (1992) patterns. The so-called IS-A pat-
terns do not only identify subclasses, but also in-
stances. Shakespeare, e.g., would be recognised
as a hyponym of author in the same way as temple
is recognised as a hyponym of civic building.
Such a missing distinction between classes and
instances is problematic. First, there are predicates
that can only attribute properties to a kind (1.a).
Second, even for properties that in principle can be
attributed to individuals of the class, this is highly
dependent on the selection of the quantifier?s re-
striction in context (3). In both cases, it holds that
properties attributed to a class are not necessarily
5In the field of cognitive science, research on the ac-
quisition of generic knowledge in humans has shown that
adult speakers tend to use generic expressions very often
when talking to children (Pappas and Gelman, 1998). We
are not aware of any detailed assessment of the proportion
of generic noun phrases in educational text genres or ency-
clopaedic resources like Wikipedia. Concerning generic sen-
tences, Mathew and Katz (2009) report that 19.9% of the sen-
tences in their annotated portion of the Penn Treebank are
habitual (generic) and 80.1% episodic (non-generic).
inherited by any or all instances pertaining to the
class.
Zirn et al (2008) are the first to present fully au-
tomatic, heuristic methods to distinguish between
classes and instances in the Wikipedia taxonomy
derived by Ponzetto and Strube (2007). They re-
port an accuracy of 81.6% and 84.5% for differ-
ent classification schemes. However, apart from a
plural feature, all heuristics are tailored to specific
properties of the Wikipedia resource.
Modelling exceptions is a cumbersome but
necessary problem to be handled in ontology
building, be it manually or by automatic means,
and whether or not the genericity of knowledge
is formalised explicitly. In artificial intelligence
research, this area has been tackled for many
years. Default reasoning (Reiter, 1980) is con-
fronted with severe efficiency problems and there-
fore has not extended beyond experimental sys-
tems. However, the emerging paradigm of Answer
Set Programming (ASP, Lifschitz (2008)) seems
to be able to model exceptions efficiently. In ASP
a given problem is cast as a logic program, and
an answer set solver calculates all possible answer
sets, where an answer set corresponds to a solution
of the problem. Efficient answer set solvers have
been proposed (Gelfond, 2007). Although ASP
may provide us with very efficient reasoning sys-
tems, it is still necessary to distinguish and mark
default rules explicitly (Lifschitz, 2002). Hence,
the recognition of generic expressions is an impor-
tant precondition for the adequate representation
and processing of generic knowledge.
3 Prior Work
Suh (2006) applied a rule-based approach to auto-
matically identify generic noun phrases. Suh used
patterns based on part of speech tags that iden-
tify bare plural noun phrases, reporting a precision
of 28.9% for generic entities, measured against
an annotated corpus, the ACE 2005 (Ferro et al,
2005). Neither recall nor f-measure are reported.
To our knowledge, this is the single prior work on
the task of identifying generic NPs.
Next to the ACE corpus (described in more de-
tail below), Herbelot and Copestake (2008) offer a
study on annotating genericity in a corpus. Two
annotators annotated 48 noun phrases from the
British National Corpus for their genericity (and
specificity) properties, obtaining a kappa value of
0.744. Herbelot and Copestake (2008) leave su-
42
pervised learning for the identification of generic
expressions as future work.
Recent work by Mathew and Katz (2009)
presents automatic classification of generic and
non-generic sentences, yet restricted to habitual
interpretations of generic sentences. They use a
manually annotated part of the Penn TreeBank
as training and evaluation set6. Using a selec-
tion of syntactic and semantic features operating
mainly on the sentence level, they achieved preci-
sion between 81.2% and 84.3% and recall between
60.6% and 62.7% for the identification of habitual
generic sentences.
4 Characterising Generic Expressions
for Automatic Classification
4.1 Properties of generic expressions
Generic NPs come in various syntactic forms.
These include definite and indefinite singular
count nouns, bare plural count and singular and
plural mass nouns as in (5.a-f). (5.f) shows a
construction that makes the kind reading unam-
biguous. As Carlson (1977) observed, the generic
reading of ?well-established? kinds seems to be
more prominent (g vs. h).
(5) a. The lion was the most widespread mam-
mal.
b. A lioness is weaker [...] than a male.
c. Lions died out in northern Eurasia.
d. Metals are good conductors.
e. Metal is also used for heat sinks.
f. The zoo has one kind of tiger.
g. The Coke bottle has a narrow neck.
h. The green bottle has a narrow neck.
Apart from being all NPs, there is no obvious
syntactic property that is shared by all examples.
Similarly, generic sentences come in a range of
syntactic forms (6).
(6) a. John walks to work.
b. John walked to work
(when he lived in California).
c. John will walk to work
(when he moves to California).
6The corpus has not been released.
Although generic NPs and generic sentences
can be combined freely (cf. Section 1; Table 1),
both phenomena highly interact and quite often
appear in the same sentence (Krifka et al, 1995).
Also, genericity is highly dependent on contex-
tual factors. Present tense, e.g., may be indica-
tive for genericity, but with appropriate temporal
modification, generic sentences may occur in past
or future tense (6). Presence of a copular con-
struction as in (5.a,b,d) may indicate a generic NP
reading, but again we find generic NPs with event
verbs, as in (5.e) or (1.b). Lexical semantic fac-
tors, such as the semantic type of the clause predi-
cate (5.c,e), or ?well-established? kinds (5.g) may
favour a generic reading, but such lexical factors
are difficult to capture in a rule-based setting.
In our view, these observations call for a corpus-
based machine learning approach that is able to
capture a variety of factors indicating genericity in
combination and in context.
4.2 Feature set and feature classes
In Table 2 we give basic information about the
individual features we investigate for identifying
generic NPs. In the following, we will structure
this feature space along two dimensions, distin-
guishing NP- and sentence-level factors as well as
syntactic and semantic (including lexical seman-
tic) factors. Table 3 displays the grouping into cor-
responding feature classes.
NP-level features are extracted from the local
NP without consideration of the sentence context.
Sentence-level features are extracted from the
clause (in which the NP appears), as well as sen-
tential and non-sentential adjuncts of the clause.
We also included the (dependency) relations be-
tween the target NP and its governing clause.
Syntactic features are extracted from a parse
tree or shallow surface-level features. The feature
set includes NP-local and global features.
Semantic features include semantic features
abstracted from syntax, such as tense and aspect
or type of modification, but also lexical semantic
features such as word sense classes, sense granu-
larity or verbal predicates.
Our aim is to determine indicators for genericity
from combinations of these feature classes.
43
Feature Description
Number sg, pl
Person 1, 2, 3
Countability ambig, no noun, count, uncount
Noun Type common, proper, pronoun
Determiner Type def, indef, demon
Granularity The number of edges in the WordNet hypernymy graph between the synset of the entity and
a top node
Part of Speech POS-tag (Penn TreeBank tagset; Marcus et al (1993)) of the head of the phrase
Bare Plural false, true
Sense[0-3] WordNet sense. Sense[0] represents the sense of the head of the entity, Sense[1] its direct
hypernym sense and so forth.
Sense[Top] The top sense in the hypernym hierarchy (often referred to as ?super sense?)
Dependency Relation [0-4] Dependency Relations. Relation[0] represents the relation between entity and its governor,
Relation[1] the relation between the governor and its governor and so forth.
Embedded Predicate.Pred Lemma of the head of the directly governing predicate of the entity
C.Tense past, pres, fut
C.Progressive false, true
C.Perfective false, true
C.Mood indicative, imperative, subjunctive
C.Passive false, true
C.Temporal Modifier? false, true
C.Number of Modifiers numeric
C.Part of Speech POS-tag (Penn TreeBank tagset; Marcus et al (1993)) of the head of the phrase
C.Pred Lemma of the head of the clause
C.Adjunct.Time true, false
C.Adjunct.VType main, copular
C.Adjunct.Adverbial Type vpadv, sadv
C.Adjunct.Degree positive, comparative, superlative
C.Adjunct.Pred Lemma of the head of the adjunct of the clause
XLE.Quality How complete is the parse by the XLE parser? fragmented, complete, no parse
Table 2: The features used in our system. C stands for the clause in which the noun phrase appears,
?Embedding Predicate? its direct predicate. In most cases, we just give the value range, if necessary, we
give descriptions. All features may have a NULL value.
Syntactic Semantic
NP-level Number, Person, Part of Speech, Determiner Type, Bare Plural Countability, Granularity, Sense[0-3, Top]
S-level Clause.{Part of Speech, Passive, Number of Modifiers}, De-
pendency Relation[0-4], Clause.Adjunct.{Verbal Type, Adver-
bial Type}, XLE.Quality
Clause.{Tense, Progressive, Perfective,
Mood, Pred, Has temporal Modifier},
Clause.Adjunct.{Time, Pred}, Embedded
Predicate.Pred
Table 3: Feature classes
Name Descriptions and Features
Set 1 Five best single features: Bare Plural, Person, Sense [0], Clause.Pred, Embedding Predicate.Pred
Set 2 Five best feature tuples:
a. Number, Part of Speech
b. Countability, Part of Speech
c. Sense [0], Part of Speech
d. Number, Countability
e. Noun Type, Part of Speech
Set 3 Five best feature triples:
a. Number, Clause.Tense, Part of Speech
b. Number, Clause.Tense, Noun Type
c. Number, Clause.Part of Speech, Part of Speech
d. Number, Part of Speech, Noun Type
e. Number, Clause.Part of Speech, Noun Type
Set 4 Features, that appear most often among the single, tuple and triple tests: Number, Noun Type,
Part of Speech, Clause.Tense, Clause.Part of Speech, Clause.Pred, Embedding Predicate.Pred, Person, Sense
[0], Sense [1], Sense[2]
Set 5 Features performing best in the ablation test: Number, Person, Clause.Part of Speech, Clause.Pred,
Embedding Predicate.Pred, Clause.Tense, Determiner Type, Part of Speech, Bare Plural, Dependency Relation
[2], Sense [0]
Table 4: Derived feature sets
44
5 Experiments
5.1 Dataset
As data set we are using the ACE-2 (Mitchell et
al., 2003) corpus, a collection of newspaper texts
annotated with entities marked for their genericity.
In this version of the corpus, the classification of
entities is a binary one.
Annotation guidelines The ACE-2 annotation
guidelines describe generic NPs as referring to an
arbitrary member of the set in question, rather than
to a particular individual. Thus, a property at-
tributed to a generic NP is in principle applicable
to arbitrary members of the set (although not to
all of them). The guidelines list several tests that
are either local syntactic tests involving determin-
ers or tests that cannot be operationalised as they
involve world knowledge and context information.
The guidelines give a number of criteria to iden-
tify generic NPs referring to specific properties.
These are (i) types of entities (lions in 3.a), (ii)
suggested attributes of entities (mammals in 3.a),
(iii) hypothetical entities (7) and (iv) generalisa-
tions across sets of entities (5.d).
(7) If a person steps over the line, they must be
punished.
The general description of generic NPs as de-
noting arbitrary members of sets obviously does
not capture kind-referring readings. However, the
properties characterised (i) can be understood to
admit kinds. Also, some illustrations in the guide-
lines explicitly characterise kind-referring NPs as
generic. Thus, while at first sight the guidelines
do not fully correspond to the characterisation of
generics we find in the formal semantics literature,
we argue that both characterisations have similar
extensions, i.e., include largely overlapping sets
of noun phrases. In fact, all of the examples
for generic noun phrases presented in this paper
would also be classified as generic according to
the ACE-2 guidelines.
We also find annotated examples of generic NPs
that are not discussed in the formal semantics liter-
ature (8.a), but that are well captured by the ACE-2
guidelines. However, there are also cases that are
questionable (8.b).
(8) a. ?It?s probably not the perfect world, but
you kind of have to deal with what you
have to work with,? he said.
b. Even more remarkable is the Internet,
where information of all kinds is available
about the government and the economy.
This shows that the annotation of generics is dif-
ficult, but also highlights the potential benefit of a
corpus-driven approach that allows us to gather a
wider range of realisations. This in turn can con-
tribute to novel insights and discussion.
Data analysis A first investigation of the corpus
shows that generic NPs are much less common
than non-generic ones, at least in the newspaper
genre at hand. Of the 40,106 annotated entities,
only 5,303 (13.2%) are marked as generic. In or-
der to control for bias effects in our classifier, we
will experiment with two different training sets, a
balanced and an unbalanced one.
5.2 Preprocessing
The texts have been (pre-)processed to add sev-
eral layers of linguistic annotation (Table 5). We
use MorphAdorner for sentence splitting and Tree-
Tagger with the standard parameter files for part
of speech tagging and lemmatisation. As we
do not have a word sense disambiguation system
available that outperforms the most frequent sense
baseline, we simply used the most frequent sense
(MFS). The countability information is taken from
Celex. Parsing was done using the English LFG
grammar (cf. Butt et al (2002)) in the XLE pars-
ing platform and the Stanford Parser.
Task Tool
Sentence splitting MorphAdorner 7
POS, lemmatisation TreeTagger (Schmid, 1994)
WSD MFS (according to WordNet 3.0)
Countability Celex (Baayen et al, 1996)
Parsing XLE (Crouch et al, 2010)
Stanford (Klein and Manning, 2003)
Table 5: Preprocessing pipeline
As the LFG-grammar produced full parses only
for the sentences of 56% of the entities (partial
parses: 37% of the entities), we chose to integrate
the Stanford parser as a fallback. If we are unable
to extract feature values from the f-structure pro-
duced by the XLE parser, we extract them from
the Stanford Parser, if possible. Experimentation
showed using the two parsers in tandem yields best
results, compared to individual use.
7http://morphadorner.northwestern.edu
45
Feature Set Generic Non generic Overall
P R F P R F P R F
Baseline Majority 0 0 0 86.8 100 92.9 75.3 86.8 80.6
Baseline Person 60.5 10.2 17.5 87.9 99.0 93.1 84.3 87.2 85.7
Baseline Suh 28.9
F
ea
tu
re
C
la
ss
es
U
nb
al
an
ce
d
NP 31.7 56.6 40.7 92.5 81.4 86.6 84.5 78.2 81.2
S 32.2 50.7 39.4 91.8 83.7 87.6 83.9 79.4 81.6
NP/Syntactic 39.2 58.4 46.9 93.2 86.2 89.5 86.0 82.5 84.2
S/Syntactic 31.9 22.1 26.1 88.7 92.8 90.7 81.2 83.5 82.3
NP/Semantic 28.2 53.5 36.9 91.8 79.2 85 83.4 75.8 79.4
S/Semantic 32.1 36.6 34.2 90.1 88.2 89.2 82.5 81.4 81.9
Syntactic 40.1 66.6 50.1 94.3 84.8 89.3 87.2 82.4 84.7
Semantic 34.5 56.0 42.7 92.6 83.8 88.0 84.9 80.1 82.4
All 37.0 72.1 49.0 81.3 87.6 87.4 80.1 80.1 83.6
B
al
an
ce
d
NP 30.1 71.0 42.2 94.4 74.8 83.5 85.9 74.3 79.7
S 26.9 73.1 39.3 94.4 69.8 80.3 85.5 70.2 77.1
NP/Syntactic 35.4 76.3 48.4 95.6 78.8 86.4 87.7 78.5 82.8
S/Syntactic 23.1 77.1 35.6 94.6 61.0 74.2 85.1 63.1 72.5
NP/Semantic 24.7 60.0 35.0 92.2 72.1 80.9 83.3 70.5 76.4
S/Semantic 26.4 66.3 37.7 93.3 71.8 81.2 84.5 71.1 77.2
Syntactic 30.8 85.3 45.3 96.9 70.8 81.9 88.2 72.8 79.7
Semantic 30.1 67.5 41.6 93.9 76.1 84.1 85.5 75.0 79.9
All 33.7 81.0 47.6 96.3 75.8 84.8 88.0 76.5 81.8
F
ea
tu
re
S
el
ec
ti
on
U
nb
al
an
ce
d Set 1 49.5 37.4 42.6 90.8 94.2 92.5 85.3 86.7 86.0
Set 2a 37.3 42.7 39.8 91.1 89.1 90.1 84.0 82.9 83.5
Set 3a 42.6 54.1 47.7 92.7 88.9 90.8 86.1 84.3 85.2
Set 4 42.7 69.6 52.9 94.9 85.8 90.1 88.0 83.6 85.7
Set 5 45.7 64.8 53.6 94.3 88.3 91.2 87.9 85.2 86.5
B
al
an
ce
d
Set 1 29.7 71.1 41.9 94.4 74.4 83.2 85.9 73.9 79.5
Set 2a 36.5 70.5 48.1 94.8 81.3 87.5 87.1 79.8 83.3
Set 3a 36.2 70.8 47.9 94.8 81.0 87.4 87.1 79.7 83.2
Set 4 35.9 83.1 50.1 96.8 77.4 86.0 88.7 78.2 83.1
Set 5 37.0 81.9 51.0 96.6 78.7 86.8 88.8 79.2 83.7
Table 6: Results of the classification, using different feature and training sets
5.3 Experimental setup
Given the unclear dependencies of features, we
chose to use a Bayesian network. A Bayesian net-
work represents the dependencies of random vari-
ables in a directed acyclic graph, where each node
represents a random variable and each edge a de-
pendency between variables. In fact, a number
of feature selection tests uncovered feature depen-
dencies (see below). We used the Weka (Witten
and Frank, 2002) implementation BayesNet in all
our experiments.
To control for bias effects, we created balanced
data sets by oversampling the number of generic
entities and simultaneously undersampling non-
generic entities. This results in a dataset of 20,053
entities with approx. 10,000 entities for each
class. All experiments are performed on balanced
and unbalanced data sets using 10-fold cross-
validation, where balancing has been performed
for each training fold separately (if any).
Feature classes We performed evaluation runs
for different combinations of feature sets: NP- vs.
S-level features (with further distinction between
syntactic and semantic NP-/S-level features), as
well as overall syntactic vs. semantic features.
This was done in order to determine the effect of
different types of linguistic factors for the detec-
tion of genericity (cf. Table 3).
46
Feature selection We experimented with two
methods for feature selection. Table 4 shows the
resulting feature sets.
In ablation testing, a single feature in turn is
temporarily omitted from the feature set. The fea-
ture whose omission causes the biggest drop in f-
measure is set aside as a strong feature. This pro-
cess is repeated until we are left with an empty
feature set. From the ranked list of features f1 to
fn we evaluate increasingly extended feature sets
f1..fi for i = 2..n. We select the feature set that
yields the best balanced performance, at 45.7%
precision and 53.6% f-measure. The features are
given as Set 5 in Table 4.
As ablation testing does not uncover feature de-
pendencies, we also experimented with single, tu-
ple and triple feature combinations to determine
features that perform well in combination. We
ran evaluations using features in isolation and each
possible pair and triple of features. We select the
resulting five best features, tuples and triples of
features. The respective feature sets are given as
Set 1 to Set 3 in Table 4. The features that appear
most often in Set 1 to Set 3 are grouped in Set 4.
Baseline Our results are evaluated against three
baselines. Since the class distribution is unequal,
a majority baseline consists in classifying each en-
tity as non-generic. As a second baseline we chose
the performance of the feature Person, as this fea-
ture gave the best performance in precision among
those that are similarly easy to extract. Finally, we
compare our results to (Suh, 2006).
6 Results and Discussion
The results of classification are summarised in Ta-
ble 6. The columns Generic and Non-generic give
the results for the respective class. Overall shows
the weighted average of the classes.
Comparison to baselines Given the bias for
non-generic NPs in the unbalanced data, the ma-
jority baseline achieves high performance overall
(F: 80.6). Of course, it does not detect any generic
NPs. The Person-based baseline also suffers from
very low recall (R: 10.2%), but achieves the high-
est precision (P: 60.5 %). (Suh, 2006) reported
only precision of the generic class, so we can only
compare against this value (28.9 %). Most of
the features and feature sets yield precision values
above the results of Suh.
Feature classes, unbalanced data For the
identification of generic NPs, syntactic features
achieve the highest precision and recall (P: 40.1%,
R: 66.6 %). Using syntactic features on the NP-
or sentence-level only, however, leads to a drop in
precision as well as recall. The recall achieved by
syntactic features can be improved at the cost of
precision by adding semantic features (R: 66.6 ?
72.1, P: 40.1 ? 37). Semantic features in sep-
aration perform lower than the syntactic ones, in
terms of recall and precision.
Even though our results achieve a lower pre-
cision than the Person baseline, in terms of f-
measure, we achieve a result of over 50%, which
is almost three times the baseline.
Feature classes, balanced data Balancing the
training data leads to a moderate drop in perfor-
mance. All feature classes perform lower than on
the unbalanced data set, yielding an increase in re-
call and a drop in precision. The overall perfor-
mance differences between the balanced and un-
balanced data for the best achieved values for the
generic class are -4.7 (P), +13.2 (R) and -1.7 (F).
This indicates that (i) the features prove to perform
rather effectively, and (ii) the distributional bias in
the data can be exploited in practical experiments,
as long as the data distribution remains constant.
We observe that generally, the recall for the
generic class improves for the balanced data. This
is most noticeable for the S-level features with
an increase of 55 (syntactic) and 29.7 (semantic).
This could indicate that S-level features are useful
for detecting genericity, but are too sparse in the
non-oversampled data to become prominent. This
holds especially for the lexical semantic features.
As a general conclusion, syntactic features
prove most important in both setups. We also ob-
serve that the margin between syntactic and se-
mantic features reduces in the balanced dataset,
and that both NP- and S-level features contribute
to classification performance, with NP-features
generally outperforming the S-level features. This
confirms our hypothesis that all feature classes
contribute important information.
Feature selection While the above figures were
obtained for the entire feature space, we now dis-
cuss the effects of feature selection both on per-
formance and the distribution over feature classes.
The results for each feature set are given in Ta-
ble 6. In general, we find a behaviour similar to
47
Syntactic Semantic
NP Number, Person, Part of
Speech, Determiner Type, Bare
Plural
Sense[0]
S Clause.Part of Speech, Depen-
dency Relation[2]
Clause.{Tense,
Pred}
Table 7: Best performing features by feature class
the homogeneous classes, in that balanced train-
ing data increases recall at the cost of precision.
With respect to overall f-measure, the best sin-
gle features are strong on the unbalanced data.
They even yield a relatively high precision for the
generic NPs (49.5%), the highest value among the
selected feature sets. This, however, comes at the
price of one of the lowest recalls. The best per-
forming feature in terms of f-measure on both bal-
anced and unbalanced data is Set 5 with Set 4 as a
close follow-up. Set 5 achieves an f-score of 53.6
(unbalanced) and 51.0 (balanced). The highest re-
call is achieved using Set 4 (69.6% on the unbal-
anced and 83.1% on the balanced dataset). The
results for Set 5 represent an improvement of 3.5
respectively 2.6 (unbalanced and balanced) over
the best achieved results on homogeneous feature
classes. In fact, Table 7 shows that these features,
selected by ablation testing, distribute over all ho-
mogeneous classes.
We trained a decision tree to gain insights into
the dependencies among these features. Figure 1
shows an excerpt of the obtained tree. The clas-
sifier learned to classify singular proper names
as non-generic, while the genericity of singular
nouns depends on their predicate. At this point,
the classifier can correctly classify some of the
NPs in (5) as kind-referring (given the training
data contains predicates like ?widespread?, ?die
out?, ...).
7 Conclusions and Future Work
This paper addresses a linguistic phenomenon that
has been thoroughly studied in the formal se-
mantics literature but only recently is starting to
be addressed as a task in computational linguis-
tics. We presented a data-driven machine learn-
ing approach for identifying generic NPs in con-
text that in turn can be used to improve tasks such
as knowledge acquisition and organisation. The
classification of generic NPs has proven difficult
even for humans. Therefore, a machine learning
approach seemed promising, both for the identifi-
cation of relevant features as for capturing contex-
Figure 1: A decision tree trained on feature Set 5
tual factors. We explored a range of features using
homogeneous and mixed classes gained by alter-
native methods of feature selection. In terms of
f-measure on the generic class, all feature sets per-
formed above the baseline(s). In the overall clas-
sification, the selected sets perform above the ma-
jority and close to or above the Person baseline.
The final feature set that we established charac-
terises generic NPs as a phenomenon that exhibits
both syntactic and semantic as well as sentence-
and NP-level properties. Although our results are
satisfying, in future work we will extend the range
of features for further improvements. In particular,
we will address lexical semantic features, as they
tend to be effected by sparsity. As a next step,
we will apply our approach to the classification
of generic sentences. Treating both cases simul-
taneously could reveal insights into dependencies
between them.
The classification of generic expressions is only
a first step towards a full treatment of the chal-
lenges involved in their semantic processing. As
discussed, this requires a contextually appropriate
selection of the quantifier restriction8, as well as
determining inheritance of properties from classes
to individuals and the formalisation of defaults.
References
R. Harald Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1996. CELEX2. Linguistic Data Consor-
tium, Philadelphia.
Johan Bos. 2009. Applying automated deduction to
natural language understanding. Journal of Applied
8Consider example (1.a), which is contextually restricted
to a certain time and space.
48
Logic, 7(1):100 ? 112. Special Issue: Empirically
Successful Computerized Reasoning.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Marsuichi, and Christian Rohrer. 2002. The
Parallel Grammar Project. In Proceedings of Gram-
mar Engineering and Evaluation Workshop.
Gregory Norman Carlson. 1977. Reference to Kinds in
English. Ph.D. thesis, University of Massachusetts.
Philipp Cimiano. 2006. Ontology Learning and Popu-
lating from Text. Springer.
Dick Crouch, Mary Dalrymple, Ron Ka-
plan, Tracy King, John Maxwell, and Paula
Newman, 2010. XLE Documentation.
www2.parc.com/isl/groups/nltt/xle/doc/xle toc.html.
O?sten Dahl. 1975. On Generics. In Edward
Keenan, editor, Formal Semantics of Natural Lan-
guage, pages 99?111. Cambridge University Press,
Cambridge.
Renaat Declerck. 1991. The Origins of Genericity.
Linguistics, 29:79?102.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Lisa Ferro, Laurie Gerber, Janet Hitzeman, Eliza-
beth Lima, and Beth Sundheim. 2005. ACE En-
glish Training Data. Linguistic Data Consortium,
Philadelphia.
Michael Gelfond. 2007. Answer sets. In Handbook of
Knowledge Representation. Elsevier Science.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th International Conference on Computational
Linguistics, pages 539?545.
Irene Heim. 1982. The Semantics of Definite and In-
definite Noun Phrases. Ph.D. thesis, University of
Massachusetts, Amherst.
Aurelie Herbelot and Ann Copestake. 2008. Anno-
tating genericity: How do humans decide? (a case
study in ontology extraction). In Sam Featherston
and Susanne Winkler, editors, The Fruits of Empiri-
cal Linguistics, volume 1. de Gruyter.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st
Meeting of the Association for Computational Lin-
guistics, pages 423?430.
Manfred Krifka, Francis Jeffry Pelletier, Gregory N.
Carlson, Alice ter Meulen, Gennaro Chierchia, and
Godehard Link. 1995. Genericity: An Introduc-
tion. In Gregory Norman Carlson and Francis Jeffry
Pelletier, editors, The Generic Book. University of
Chicago Press, Chicago.
Douglas B. Lenat. 1995. Cyc: a large-scale invest-
ment in knowledge infrastructure. Commun. ACM,
38(11):33?38.
Vladimir Lifschitz. 2002. Answer set programming
and plan generation. Artificial Intelligence, 138(1-
2):39 ? 54.
Vladimir Lifschitz. 2008. What is Answer Set Pro-
gramming? In Proceedings of AAAI.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Thomas Mathew and Graham Katz. 2009. Supervised
Categorization of Habitual and Episodic Sentences.
In Sixth Midwest Computational Linguistics Collo-
quium. Bloomington, Indiana: Indiana University.
Alexis Mitchell, Stephanie Strassel, Mark Przybocki,
JK Davis, George Doddington, Ralph Grishman,
Adam Meyers, Ada Brunstein, Lisa Ferro, and Beth
Sundheim. 2003. ACE-2 Version 1.0. Linguistic
Data Consortium, Philadelphia.
Ian Niles and Adam Pease. 2001. Towards a Standard
Upper Ontology. In Proceedings of the 2nd Interna-
tional Conference on Formal Ontology in Informa-
tion Systems.
Athina Pappas and Susan A. Gelman. 1998. Generic
noun phrases in mother?child conversations. Jour-
nal of Child Language, 25(1):19?33.
Simone Paolo Ponzetto and Michael Strube. 2007.
Deriving a large scale taxonomy from wikipedia.
In Proceedings of the 22nd Conference on the Ad-
vancement of Artificial Intelligence, pages 1440?
1445, Vancouver, B.C., Canada, July.
Willard Van Orman Quine. 1960. Word and Object.
MIT Press, Cambridge, Massachusetts.
Raymond Reiter. 1980. A logic for default reasoning.
Artificial Intelligence, 13:81?132.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. Proceedings of the
conference on New Methods in Language Process-
ing, 12.
Sangweon Suh. 2006. Extracting Generic Statements
for the Semantic Web. Master?s thesis, University of
Edinburgh.
Ian H. Witten and Eibe Frank. 2002. Data min-
ing: practical machine learning tools and techniques
with Java implementations. ACM SIGMOD Record,
31(1):76?77.
Ca?cilia Zirn, Vivi Nastase, and Michael Strube. 2008.
Distinguishing between instances and classes in the
Wikipedia taxonomy. In Proceedings of the 5th Eu-
ropean Semantic Web Conference.
49
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 1?10,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Casting Implicit Role Linking as an Anaphora Resolution Task
Carina Silberer?
School of Informatics
University of Edinburgh
Edinburgh, UK
c.silberer@ed.ac.uk
Anette Frank
Department of Computational Linguistics
Heidelberg University
Heidelberg, Germany
frank@cl.uni-heidelberg.de
Abstract
Linking implicit semantic roles is a challeng-
ing problem in discourse processing. Unlike
prior work inspired by SRL, we cast this prob-
lem as an anaphora resolution task and embed
it in an entity-based coreference resolution
(CR) architecture. Our experiments clearly
show that CR-oriented features yield strongest
performance exceeding a strong baseline. We
address the problem of data sparsity by apply-
ing heuristic labeling techniques, guided by
the anaphoric nature of the phenomenon. We
achieve performance beyond state-of-the art.
1 Introduction
A widespread phenomenon that is still poorly stud-
ied in NLP is the meaning contribution of unfilled
semantic roles of predicates in discourse interpreta-
tion. Such roles, while linguistically unexpressed,
can often be anaphorically bound to antecedent ref-
erents in the discourse context. Capturing such im-
plicit semantic roles and linking them to their an-
tecedents is a challenging problem. But it bears im-
mense potential for establishing discourse coherence
and for getting closer to the aim of true NLU.
Linking of implicit semantic roles in discourse
has recently been introduced as a shared task in
the SemEval 2010 competition Linking Events and
Their Participants in Discourse (Ruppenhofer et al,
2009, 2010). The task consists in detecting un-
filled semantic roles of events and determining an-
tecedents in the discourse context that these roles
? The work reported in this paper is based on a Master?s
Thesis conducted at Heidelberg University (Silberer, 2011).
can be understood to refer to. In (1), e.g., the pred-
icate jealousy introduces two implicit roles, one for
the experiencer, the other for the object of jealousy
involved. These roles can be bound to Watson and
the speaker (I) in the non-local preceding context.
(1) Watson won?t allow that I know anything of art but
that is mere jealousy because our views upon the
subject differ.
(2) IReader was sitting reading in the chairPlace.
In contrast to implicit roles that can be discourse-
bound to an antecedent as in (1), roles can be inter-
preted existentially, as in (2), with an unfilled TEXT
role of the READING frame that cannot be anchored
in prior discourse. The FrameNet paradigm (Fill-
more et al, 2003) that was used for annotation in
the SemEval task classifies these interpretation dif-
ferences as definite (DNI) vs. indefinite (INI) null
instantiations (NI) of roles, respectively.
2 Implicit Role Reference: A Short History
Early studies. The phenomenon of implicit role re-
ference is not new. It has been studied in a number
of early approaches. Palmer et al (1986) treated un-
filled semantic roles as special cases of anaphora and
coreference resolution (CR). Resolution was guided
by domain knowledge encoded in a knowledge-
based system. Similarly, Whittemore et al (1991)
analyzed the resolution of unexpressed event roles
as a special case of CR. A formalization in DRT was
fully worked out, but automation was not addressed.
Later studies emphasize the role of implicit role
reference in a frame-semantic discourse analysis.
Fillmore and Baker (2001) provide an analysis of
1
a newspaper text that indicates the importance of
frames and roles in establishing discourse coher-
ence. Burchardt et al (2005) offer a formalization
of the involved factors: the interplay of frames and
frame relations with factors of contextual contigu-
ity. The work includes no automation, but suggests a
corpus-based approach using antecedent-role coref-
erence patterns collected from corpora.
Tetreault (2002), finally, offers an automated anal-
ysis for resolving implicit role reference. The small-
scale study is embedded in a rule-based CR setup.
SemEval 2010 Task 10: Linking Roles. Trig-
gered by the SemEval 2010 competition (Ruppen-
hofer et al, 2010), research on resolving implicit
role reference has gained momentum again, in a field
where both semantic role labeling (SRL) and coref-
erence resolution have seen tremendous progress.
However, the systems that participated in the NI-
only task on implicit role resolution achieved mod-
erate success in the initial subtasks: (i) recog-
nition of implicit roles and (ii) classification as
discourse-bound vs. existential interpretation (DNI
vs. INI). Yet, (iii) identification of role antecedents
was bluntly unsuccessful, with around 1% F-score.
Ruppenhofer et al clearly relate the task to
coreference resolution. The participating systems,
though, framed the task as a special case of SRL.
Chen et al (2010) participated with their SRL sys-
tem SEMAFOR (Das et al, 2010). They cast the task
as one of extended SRL, by admitting constituents
from a larger context. To overcome the lack and
sparsity of syntactic path features, they include lex-
ical association and similarity scores for semantic
roles and role fillers; classical SRL order and dis-
tance features are adapted to larger distances.
VENSES++ by Tonelli and Delmonte (2010) is
a semantic processing system that includes lexico-
semantic processing, anaphora resolution and deep
semantic resolution components. Anaphora resolu-
tion is performed in a rule-based manner; pronom-
inals are replaced with their antecedents? lexical
information. For role linking, the system applies
diverse heuristics including search for predicate-
argument structures with compatible arguments, as
well as semantic relatedness scores between poten-
tial fillers of (overt and implicit) semantic roles.
More recently Tonelli and Delmonte (2011) recur
to a leaner approach for role binding, estimating a
relevance score for potential antecedents from role
fillers observed in training. They report an F-score
of 8 points for role binding on SemEval data. How-
ever, being strongly lexicalized, their trained model
seems heavily dependent on the training data.
Ruppenhofer et al (2011) use semantic types for
identifying DNI role antecedents, reporting an error
reduction of 14% on Chen et al (2010)?s results.
The poor performance results in the SemEval task
clearly indicate the difficulty of resolving implicit
role reference. A major factor seems to relate to data
sparsity: the training set covers only 245 DNI anno-
tations linked to an antecedent.
Linking implicit arguments of nominals. Ger-
ber and Chai (2010) (G&C henceforth) investigate a
closely related task of argument binding, tied to the
linking of implicit arguments for nominal predicates
using the PropBank role labeling scheme. In con-
trast to the SemEval task, which focuses on a verbs
and nouns, their system is only applied to nouns and
is restricted to 10 predicates with substantial training
set sizes (avg: 125, median: 103).
G&C propose a discriminative model that selects
an antecedent for an implicit role from an extended
context window. The approach incorporates some
aspects relating to CR that go beyond the SRL-
oriented SemEval systems: A candidate represen-
tation includes information about all the candidates?
coreferent mentions (determined by automatic CR),
in particular their semantic roles (provided by gold
annotations) and WordNet synsets. Patterns of se-
mantic associations between filler candidates and
implicit roles are learned for all mentions contained
in the candidate?s entity chain. They achieve an F-
score of 42.3, against a baseline of 26.5.
Gerber (2011) presents an extended model that in-
corporates strategies suggested in Burchardt et al
(2005): using frame relations as well as coreference
patterns acquired from large corpora. This model
achieves an F-score of 50.3 (baseline: 28.9).
3 Casting Implicit Role Linking as an
Anaphora Resolution Task
3.1 Implicit role = anaphora resolution
Recent models for role binding mainly draw on tech-
niques from SRL, enriched with concepts from CR.
2
In this paper, we explicitly formulate implicit role
linking as an anaphora resolution task. This is in
line with the predominant conception in early work,
and also highlights the close relationship with zero
anaphora (Kameyama, 1985). Computational treat-
ments of zero anaphora (e.g., Imamura et al (2009))
are in fact employing techniques well-known from
SRL. Recent work by Iida and Poesio (2011), by
contrast, offers an analysis of zero anaphora in a
CR architecture. Further support comes from psy-
cholinguistic studies in Garrod and Terras (2000),
who establish commonalities between implicit role
reference and other types of anaphora resolution.
The contributions of our work are as follows:
i. We cast implicit role binding as a CR task, us-
ing an entity-mention model and discriminative
classification for antecedent selection.
ii. We examine the effectiveness of model features
for classical SRL vs. CR features to clarify the
nature of this special phenomenon.
iii. We automatically acquire heuristically labeled
data to address the sparse data problem.
i. An entity-mention model for anaphoric role
resolution. In our model implicit roles that are
discourse-bound (i.e. classified as DNI) are treated
as anaphoric, similar to zero anaphora: the implicit
role will be bound to a discourse antecedent.
In line with recent research in CR, we adopt an
entity-mention model, where an entity is represented
by all mentions pertaining to a coreference chain
(see i.a. Rahman and Ng (2011), Cai and Strube
(2010)). Our model is based on binary classifier de-
cisions that take as input the anaphoric role and an
entity candidate from the preceding discourse. The
final classification of a role linking to an entity is ob-
tained by discriminative ranking of the binary clas-
sifiers? probability estimates. Details on the system
architecture are given in Section 3.2.
ii. SRL vs. CR: Analysis of feature sets. The
linking of implicit semantic roles represents an inter-
esting mixture of SRL and CR that displays excep-
tional characteristics of both types of phenomena.
In contrast to classical SRL, the relation between
a predicate?s semantic role and a candidate role filler
? being realized outside the local syntactic context ?
cannot be characterized by syntactic path features.
But similar to SRL we can compute a semantic class
type expected by the role and determine which can-
didate is most appropriate to fill the semantic role.
Anaphoric binding of unfilled roles also diverges
from classical CR in that the anaphoric element is
not overtly expressed. This excludes typical CR fea-
tures that refer to overt realization, such as agree-
ment or string overlap. Again, we can make use of a
semantic characterization of role fillers to determine
the role?s most appropriate antecedent entity in the
discourse. This closely relates to semantic class fea-
tures employed in CR (e.g., Rahman and Ng (2011)).
Thus, semantic association features are important
modeling aspects, but they do not contribute to clari-
fying the nature of the phenomenon. We will include
additional properties that are considered characteris-
tic for CR, such as the semantics of an entity (as op-
posed to individual mentions), or salience properties
of antecedents (cf. Section 4.3). Thus, the model we
propose substantially differs from prior work.
We classify the features of our models as SRL vs.
CR features, plus a mixture class that relates to both
phenomena. We examine which type of features is
most effective for resolving implicit role reference.
iii. Heuristic data acquisition. In response to the
sparse data problem encountered with the SemEval
data set and the general lack of annotated resources
for implicit role binding, we experiment with tech-
niques for heuristic data acquisition. The strategy
we apply builds on our working hypothesis that im-
plicit role reference is best understood as a special
case of (zero) anaphora resolution.
We process manually annotated coreference data
sets that are jointly labeled with semantic roles.
From these we extract entity chains that contain
anaphoric pronouns that fill a predicate?s semantic
role. We artificially delete the pronoun?s role label
and transfer it to its closest antecedent in its chain.
In this way, we convert the example to an instance
that is structurally similar to one involving a locally
unfilled semantic role that is bound to an overt an-
tecedent. An example is given below: in (3.a) we
identify a pronoun that fills the SPEAKER role of the
frame STATEMENT. We transfer this role label to its
closest antecedent (3.b).
3
(3) a. Riadyk spoke in hisk 21-story office building
on the outskirts of Jakarta. [...] The timing of
hisk,Speaker statementStatement is important.
b. Riadyk spoke in hisk,Speaker 21-story office
building on the outskirts of Jakarta. [...] The tim-
ing of ? statementStatement is important.
Clearly such artificially created annotation instances
are only approximations of naturally occurring cases
of implicit role binding. But we expect to acquire
numerous data points for relevant features: semantic
class information for the antecedent entity, the pred-
icate?s frame and roles and coherence properties.
3.2 System Architecture
Our approach is embedded in an architecture for su-
pervised CR using an entity-mention model. The
main processing steps of the system include: (1) en-
tity detection, (2) instance creation with feature ex-
traction and (3) classification. As we are focusing
on the resolution of implicit DNI roles, we assume
that the text is already augmented with standard CR
information (we make use of gold data and automati-
cally assigned coreference chains). Accordingly, the
description of modules focuses exclusively on the
resolution of DNIs.
(1) Entity Detection. We first collect the entire
entity set E mentioned in the discourse. This set
forms the overall set of candidates to consider for
DNI linking. For each DNI dk to be linked, a subset
of candidates Ek ? E is chosen as candidate search
space for resolving dk. We experiment with differ-
ent strategies for constructing Ek (cf. Section 4).
(2) Instance Creation. The next step consists in
the creation of (training) instances for classification
including the extraction of features for all instances.
An instance instej ,dk consists of the active DNI
dk, its frame and a candidate entity ej ? Ek. In-
stance creation follows an entity-based adaption of
the standard procedure of Soon et al (2001), which
has been applied by Yang et al (2004, 2008). Pro-
cessing the discourse from left to right, for each DNI
dk, instances Ik are created by processing Ek from
right to left according to each entity?s most recent
mention, starting with the entity closest to dk. Note
that, as entities instead of mentions are considered,
only one instance is created for an entity which is
mentioned several times in the search space.
In training, the instance creation stops when the
correct antecedent, i.e. a positive instance, as well as
at least one negative instance have been found.1
(3) Classification. From the acquired training in-
stances we learn a binary classifier that predicts for
an instance instej ,dk whether it is positive, i.e. en-
tity ej is a correct antecedent for DNI dk. Fur-
ther, the classifier provides a probability estimate for
instej ,dk being positive. We obtain classifications
for all instances in Ik. Among the positive classified
instances, we select the antecedent e with the high-
est estimate. That is, we apply the best-first strategy
(Ng and Cardie, 2002). In case of a tie, we choose
the antecedent which is closer to the target. If no
instance is classified as positive, dk is left unfilled.
4 Data and Experiments
4.1 SEMEVAL 2010 task and data set
We adhere to the SemEval 2010 task by Ruppen-
hofer et al (2009) as test bed for our experiments.
The main focus of our work is on part (iii), the iden-
tification of antecedents for DNIs. Subtasks (i) and
(ii), the recognition and interpretation of NIs will be
only tackled to enable comparison to the participat-
ing systems of the SemEval NI-only task.
The SemEval task is based on fiction stories by
A. C. Doyle, one story as training data and another
two chapters as test set, enriched with coreference
and FrameNet-style frame annotations. Information
about the training section is found in Table 1. The
test data comprise 710 NIs (349 DNIs, 361 INIs), of
which 259 DNIs are linked.
4.2 Heuristic data acquisition
Since the training data has a critically small amount
of linked DNIs, we heuristically labeled training
data on the basis of data sets with manually anno-
tated coreference information: OntoNotes 3.0 (Hovy
et al, 2006), as well as ACE-2 (Mitchell et al, 2003)
and MUC-6 (Chinchor and Sundheim, 2003).
OntoNotes 3.0 was merged with gold SRL an-
notations from the CoNLL-2005 shared task. By
means of SemLink-1.1 (Loper et al, 2007) and a
mapping included in the SemEval data, these Prop-
Bank (PB, Palmer et al (2005)) annotations were
1We additionally impose several restrictions, e.g., a valid
candidate must not already fill another role of the active frame.
4
#ent avg avg #frames #frame#DNI #DNI
#ent/doc size types types
SemEval 141 141 9 1,370 317 245 155
ONotes 7899 23 3 12,770 258 2,220 270
ACE-2 3564 11 4 58,204 757 4,265 578
MUC-6 1841 15 3 20,140 654 997 310
corpus coref semantic roles
ONotes manual manual PB CoNLL05, ported to FN
ACE-2 manual automatic FN (Semafor)
MUC-6 manual automatic FN (Semafor)
Table 1: SemEval vs. heuristically acquired data
mapped to their FrameNet (FN) counterparts, if ex-
istent. For the ACE-2 and MUC-6 corpora, we used
Semafor (Das and Smith, 2011) for automatic anno-
tation with FN semantic roles. From these data sets
we acquired heuristically annotated instances of role
linking using the strategy explained in 3.1.
Table 1 summarizes the resulting training data.
The heuristically labeled data extends the manually
labeled DNI instances by an order of magnitude.
4.3 Model parameters
Entity sets Edni. For definition of the set of can-
didate entities to consider for DNI linking, Edni,
we determined different parameter settings with re-
strictions on the types, distances and prominence of
candidate antecedents. For instance, unlike in noun
phrase CR, antecedents for a DNI can be realized by
a wide range of constituents other than NPs, such as
prepositional (PP), adverbial (ADVP), verb phrases
(VP) and even sentences (S) referring to proposi-
tions.
These settings, stated in Table 2, were inferred by
experiments on the training data and by examining
its statistics: AllChains is motivated by the fact that
72% of the DNIs are linked to referents with non-
singleton chains. On the other hand, the majority of
DNI antecedents ? not only non-singletons, but also
phrases of a certain type or terminals that overtly
fill other roles ? are located in the current and the
two preceding sentences (69.6%), which motivates
SentWin. However, antecedents are also located far
beyond this window span which is probably due to
the nature of the SemEval texts, with prominent en-
tities being accessible over longer stretches of dis-
course. Chains+Win is designed by taking into ac-
AllChains This set contains all the entities repre-
sented by non-singleton coreference chains that
were introduced in the discourse up to the cur-
rent DNI position, assuming that this way only
more salient entities are considered.
SentWin Comprises constituents with a certain
phrase type2 or terminals that overtly fill a role,
occurring within the current or the preceding
two sentences.
Chain+Win This set comprises SentWin plus all
entities mentioned at least five times up to the
current DNI position (i.e. salient entities).
Table 2: Entity set settings Edni
count all previous observations.
Training data sets. We made use of different mix-
tures of training data: SemEval plus different exten-
sions using the heuristically acquired data summa-
rized in Table 1.
4.4 Feature sets: SRL, mixed and CR-oriented
Table 3 lists the most important features used for
training our models. Features 1-13 were used in the
best model and are ordered by their strength based
on feature ablation experiments (cf. Section 5). All
features are marked for their general type; the last
column marks features employed by G&C.3
Below we give some details for selected features.
Feat. 1: Prominence. We first compute average
prominence of an entity e (Eq. 2) by summing over
the size (= nb. of mentions) of all entities e in a win-
dow w4 of preceding sentences and dividing by the
nb. of entities E in w. Prominence of e (Eq. 1) is
set to the difference between its size in w and the
average prominence score.5 The final feature value
records the relative rank of e?s prominence score
compared to the scores of the other candidates.
prom(e, w) = #mentions(e, w)? avg prom(w) (1)
avg prom(w) =
?
e?E #mentions(e, w)
|E|
(2)
2The phrase type must be NPB, S, VP, SBAR, or SG.
3? marks features that are similar to G&C features. Note
that their only CR features are distance features.
4We set w = 2 based on experiments on the training data.
5This prominence score was proposed by Dolata (2010)
within an entity grid approach to role linking.
5
nr feature type G&C
1 prominence prominence score of the entity in the current discourse position CR -
2 pos.dist mention PoS or phrase type of the most recent explicit mention (CR) -
concatenated with sentence distance to the target
3 dist mentions minimum distance between DNI and entity in mentions CR -
4 dist sentences minimum distance between DNI and entity in sentences CR +
5 vnroles dni.entity the counterparts of the DNI in VerbNet (VN, Kipper et al (2000)) mixed +
concatenated with the VN roles the entity already instantiates
6 roles dni.entity concatenation of the DNI with the FN roles the entity already instantiates mixed ?
7 semType dni.entity semantic type of the DNI concatenated with mixed -
the semantic types of the roles the entity already instantiates
8 avgDist sentences average sentence distance between the entity and the DNI CR +
9 sp supersense agreement of the selectional preferences for the DNI mixed -
and the most frequent supersense of the entity
10 function (target) grammatical function of the target SRL -
11 wnss ent.st dni pointwise mutual information between the entity?s WN supersense ss and mixed -
the DNI?s FN semantic type st: pmi(ss, st) = log2P (ss|st)/P (ss)
12 nbRoles dni.entity like feature 5, but with NomBank arguments 0 and 1 mixed ?
13 frame.dni frame name concatenated with the DNI SRL -
Table 3: Best features used for training. Feat. 11 was computed on the FN dataset and the SemEval training data.
Feat. 9: SelPrefs. We compute selectional prefer-
ences following the information-theoretic approach
of Resnik (1993, 1996). Similar to Erk (2007), we
used an adapted version which we computed for se-
mantic roles by means of the FN database rather than
for verb argument positions. The WordNet classes
over which the preferences are defined are WordNet
lexicographer?s files (supersenses).
The selectional association values ?(dni, ss) of
the DNI?s selectional preferences are retrieved for
the supersense ss of each candidate antecedent?s
head. As for Feat. 1, we define a candidate?s fea-
ture value by its rank in the ordered list of these ?s.
4.5 Experiments
Evaluation measures. We adopt the precision (P),
recall (R) and F1 measures in Ruppenhofer et al
(2010). A true positive is a DNI which has been
linked to the correct entity as given by the gold data.
Classifiers and feature selection. For DNI link-
ing, we use BayesNet (Cooper and Herskovits,
1992) as classifier, implemented in Weka (Witten
and Frank, 2000).6 For each parameter combination,
we perform feature selection by means of leave-one-
out 10-fold cross-validation on the SemEval train-
ing data with successively removing/determining the
6We experimented with different learners and selected the
algorithm that performed best for the different subtasks.
best features. The resulting models Mi are then eval-
uated on the SemEval test data in different setups:
Exp1: Linking DNIs. Exp1 evaluates our models
on the DNI linking task proper (NI-only step (iii)).
This setting uses the gold coreference, SRL and DNI
information in the test data.
Exp2: Full NI-only. For benchmarking on the
SemEval task, we perform the complete NI-only
task. Here, the test data is only enriched w/ SRL la-
beling. Each frame f in the test corpus is processed,
involving the following steps:
(i) Recognition of NIs is performed by consulting
the FN database7 and determining the FN core roles
that are unfilled. From this NI set, roles that are
conceptually redundant or competing with f?s overt
roles are rejected as they don?t need to or must not
be linked, respectively.
(ii) For predicting the interpretation of an NI, we
use LibSVM (Chang and Lin, 2001) as classifier
which further assigns each NI a probability estimate
of the NI being definite. We use a small set of fea-
tures: the FN semantic type of the NI and a boolean
feature indicating whether the target is in passive
voice and the agent (object) not realized. Further,
we use a statistical feature which gives the relative
7We used the FrameNetAPI by Reiter (2010).
6
model add. entity frame DNI Linking (%)
data set anno. P R F1
M0 - AllChains gold 25.6 25.1 25.3
M1 ON2-10 Chains+Win proj 30.8 25.1 27.7
M1? ON2-24 AllChains proj 35.6 20.1 25.7
M1?? ON2-24 SentWin proj 23.3 22.4 22.8
M2 MUC Chains+Win auto 26.1 24.3 25.3
M3 ACE AllChains auto 24.0 21.2 22.5
Prom ? Chains+Win ? 20.5 20.5 20.5
Table 4: Exp1: Best performing models for different en-
tity and data settings. Test data contain gold CR chains.
frequency of the role?s realization as DNI and INI,
respectively, in the training data.
(iii) DNI linking is performed for each of f?s pre-
dicted DNIs Df in descending order of their prob-
ability estimates. If an antecedent em can be de-
termined for a predicted DNI, the role is labeled
as such and linked to em. As the DNI?s role has
been filled now, competing or redundant DNIs are
removed from Df before moving to the next pre-
dicted DNI. Only DNIs for which an antecedent is
found are labeled as such.
Exp2 is evaluated on both gold coreference an-
notation and automatically assigned coreference
chains, using the CR system of Cai et al (2011).
5 Evaluation and Results
5.1 Exp1: DNI linking evaluation
Table 4 shows the best performing models for DNI
linking for each parameter setting8. We compare
them to a strong baseline Prom (last row) that links
each DNI to the antecedent candidate with highest
prominence score. Its F1-score is beaten by the other
models, with a gain of 7.2 points for model M1. The
high performance of the baseline can be taken as ev-
idence that salience factors are crucial for this task.
The best performing model M1 (27.7 F1) uses
about a fifth of the ON data with Chains+Win. When
using SentWin as entity set, F1 drops to 18.5 (not
shown). The best performing model using SentWin
(M1??) performs 4.9 points below M1. Hence, re-
liance on the Chains+Win set seems beneficial. Per-
formance of the AllChains setting varies over the
8We consider the 3 types of entity sets and different train-
ing setups ? additional data (Section 4.3); additional data with
gold, projected or automatic frame annotations. The ON data
was also evaluated with roughly a fifth of ON to evaluate the
effect of different amounts of data of the same type of data.
Features P ( %) R (%) F1 (%)
all 30.8 25.1 27.7
- 1-4,8 (CR) 21.6 8.1 11.8
- 10,13 (SRL) 31.0 25.9 28.2
- 5-7,9,11-12 (mixed) 20.6 20.5 20.5
Table 5: Results of ablation study.
different data sets: the strongest model is M0 with-
out additional data. An explanation could be the dif-
ferent data domains (story vs. news), leading to a
different nature (length and number) of the entities.
In general, the models seem to profit from heuris-
tically labeled training data. We note strong gains
(up to 10 pts) in precision for 3 of these 5 best mod-
els, compared to M0. Finally, we observe higher
performance when using additional data with gold/
projected semantic frame annotations (M1, M1?).
Analysis of the best model. Table 5 states the re-
sults for M1 when leaving out one of the feature
types at a time. The serious drop of F1 from 27.7%
to 11.8% when omitting CR features clearly demon-
strates that this feature type has by far the greatest
impact on the task performance. Rejection of the
mixed features decreases F1 to a score equal to the
prominence baseline, whereas leaving out the SRL-
features even slightly increases F1. The weakness of
Feature 13 could still be attributed to data sparsity.
5.2 Exp2: Full NI-only evaluation
Table 6 lists the results for the full NI-only task ob-
tained with the presented models with different addi-
tional training data sets (lines 2-5). When perform-
ing all three steps, the F1-score of the best model
M1 drops to 10.1% (-17.6 pts, col. 10) under us-
age of automatic coreference annotations in the test
data (i.e. under the real task conditions). When us-
ing gold coreference annotations, the F1-score is
at 18.1% (col. 11), which can be seen as an upper
bound for our current models on this task. The dif-
ference of 9.6 points between only performing DNI
linking (Table 4) and the full NI-only task reflects
the fact that recognizing (step i) and interpreting
(step ii) NIs bear difficulties on their own.9
Comparison of our models with the two SemEval
9When not performing step (iii), NI recognition achieves
77.6% recall and 67% relative precision.
7
Null Instantiations (%)
model add. entity frame recogn. interpret. (precision) DNI Linking (%)
data set anno. recall relative absolute P R F1 F1(crf)
M0 - AllChains gold 58 68 40 6.0 8.9 7.1 12.5
M1 ON2-10 Chains+Win proj 56 69 38 9.2 11.2 10.1 18.1
M2 MUC Chains+Win auto 52 70 36 7.0 8.5 7.6 11.0
M3 ACE AllChains auto 56 68 38 5.9 8.1 6.8 11.3
M3? ACE Chains+Win auto 56 68 38 6.9 9.7 8.0 9.5
SEMAFOR ? 63 55 35 1.40
VENSES++ ? 8 64 5 1.21
T&D ? 54 75 40 13.0 6.0 8
Table 6: Exp2 results obtained for our models (lines 1-5) and comparable systems (lines 6-8). Column 5 gives the
score for correctly recognized NIs. Cols. 6 and 7 report precision for correctly interpreted NIs on the basis of the
correctly recognized (relative) vs. all gold NIs to be recognized (absolute). The scores in the last column (F1(crf))
were obtained with gold CR annotations.
task participants10 (lines 7-8) shows that our models
clearly outperform these systems ? with a gain of
+5.7 and +8.89 points in F1-score in DNI linking.11
Compared to Tonelli and Delmonte (2011)
(T&D), M1 has a higher F1-score in linking of
+2.1 points. In contrast to our method, their link-
ing approach is (admittedly) heavily lexicalized and
strongly tailored to the domain of the used data.
6 Conclusion
We cast the problem of linking implicit semantic
roles as a special case of (zero) anaphora resolution,
drawing on insights from earlier work and parallels
observed with zero anaphora. Our results strongly
support this analysis: (i) Feature selection clearly
determines CR-related features as strongest support
for DNI linking. (ii) Our models beat a strong base-
line using a prominence score to determine DNI ref-
erence. (iii) We devise a method for heuristically la-
beling training data that simulates implicit role refer-
ence. Using this data we obtain system performance
beyond state-of-the-art, with high gains in precision.
While these findings clearly corroborate our con-
ceptual approach, overall performance is still mea-
ger. Comparison to G&C?s setting suggests that
training data is a serious issue. We addressed the
10The F1-scores are from http://semeval2.fbk.eu/
semeval2.php?location=Rankings/ranking10.html
11Moreover, note that Ruppenhofer et al describe a weaker
evaluation, that judges DNI linkings as correct if the span of the
linked referent contains the gold referent. Further, they consider
14 linked INIs in the test data, although linking INIs conflicts
with the definition of INIs.
problem of training set size using heuristic data ac-
quisition. The nature of semantic role annotations
may be another problem, as FrameNet-style roles do
not generalize well. Finally, implicit roles pertaining
to nominalizations tend to be more local than those
pertaining to verbs12 and might be less diverse.
Our model is closer in spirit to G&C than the Se-
mEval systems, but differs by being embedded in
an entity-based CR architecture using discriminative
antecedent selection. Also, we address a more prin-
cipled issue, by exploring the nature of the task using
a qualitative feature analysis. Our system compares
favorably to related work. Benchmarking against
the SemEval participants and T&D shows clear im-
provements. Also, T&D?s model is closely tied to
domain data, while ours is enhanced with out-of-
domain data. Exact comparison to G&C needs to be
conducted on the same data set and labeling scheme.
In sum, within the chosen setting we can show
that implicit role reference is best modeled as a spe-
cial case of anaphora resolution. We observe that
models trained on cleaner data perform better than
on larger, but more noisy data sets. Thus, it is es-
sential to further enhance the quality of heuristically
labeled data. Applying the classifiers for steps (i)
and (ii) as a filter could help to better constrain the
data to the target phenomenon.
Acknowledgements. We would like to thank Mateusz
Dolata for his help with salience and coherence features,
and Michael Roth for his server support.
12This is confirmed by analysis of the SemEval vs. NomBank
corpus of G&C.
8
References
Aljoscha Burchardt, Anette Frank, and Manfred Pinkal.
2005. Building Text Meaning Representations from
Contextually Related Frames ? A Case Study. In Pro-
ceedings of the 6th International Workshop on Com-
putational Semantics, IWCS-6, pages 66?77, Tilburg,
The Netherlands.
Jie Cai and Michael Strube. 2010. End-to-end coref-
erence resolution via hypergraph partitioning. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 143?151, Beijing,
China.
Jie Cai, Eva Mu?jdricza-Maydt, and Michael Strube.
2011. Unrestricted coreference resolution via global
hypergraph partitioning. In Proceedings of the Shared
Task of 15th Conference on Computational Natural
Language Learning, pages 56?60, Portland, Oregon.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a Library for Support Vector Machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
Desai Chen, Nathan Schneider, Dipanjan Das, and
Noah A. Smith. 2010. SEMAFOR: Frame Argument
Resolution with Log-Linear Models. In Proceedings
of the 5th International Workshop on Semantic Evalu-
ation, pages 264?267, Uppsala, Sweden, July.
Nancy Chinchor and Beth Sundheim, 2003. Message
Understanding Conference (MUC) 6. Linguistic Data
Consortium, Philadelphia.
Gregory F. Cooper and Edward Herskovits. 1992. A
Bayesian Method for the Induction of Probabilistic
Networks from Data. Machine Learning, 9(4):309?
347.
Dipanjan Das and Noah A. Smith. 2011. Semi-
supervised frame-semantic parsing for unknown pred-
icates. In Dekang Lin, Yuji Matsumoto, and Rada Mi-
halcea, editors, ACL, pages 1435?1444. The Associa-
tion for Computer Linguistics.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic Frame-Semantic
Parsing. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
948?956, Los Angeles, California, June.
Mateusz Dolata. 2010. Extending the Entity-Grid Model
for the Processing of Implicit Roles in Discourse.
Bachelor?s thesis, Department of Computational Lin-
guistics, Heidelberg University, Germany.
Katrin Erk. 2007. A Simple, Similarity-based Model for
Selectional Preferences. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics, ACL ?07, pages 216?223, Prague, Czech
Republic, June.
Charles J. Fillmore and Collin F. Baker. 2001. Frame Se-
mantics for Text Understanding. In Proceedings of the
NAACL 2001 Workshop on WordNet and Other Lexical
Resources, Pittsburgh, June.
Charles J. Fillmore, Christopher R. Johnson, and Miriam
R. L. Petruck. 2003. Background to Framenet. Inter-
national Journal of Lexicography, 16(3):235?250.
Simon Garrod and Melody Terras. 2000. The Contribu-
tion of Lexical and Situational Knowledge to Resolv-
ing Discourse Roles: Bonding and Resolution. Jour-
nal of Memory and Language, 42(4):526?544.
Matthew Gerber and Joyce Chai. 2010. Beyond Nom-
Bank: A Study of Implicit Arguments for Nominal
Predicates. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 1583?1592, Uppsala, Sweden, July.
Matthew Steven Gerber. 2011. Semantic Role Labeling
of Implicit Arguments for Nominal Predicates. Ph.D.
thesis, Michigan State University.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% Solution. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics, HLT-NAACL ?06, pages 57?60, New York, New
York, June.
Ryu Iida and Massimo Poesio. 2011. A Cross-Lingual
ILP Solution to Zero Anaphora Resolution. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 804?813,
Portland, Oregon.
Kenji Imamura, Kuniko Saito, and Tomoko Izumi.
2009. Discriminative Approach to Predicate-
Argument Structure Analysis with Zero-Anaphora
Resolution. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing of
the Asian Federation of Natural Language Processing,
ACL-IJCNLP ?09, pages 85?88, Suntec, Singapore,
August.
Megumi Kameyama. 1985. Zero Anaphora: The case of
Japanese. Ph.D. thesis, Stanford University.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-Based Construction of a Verb Lexicon.
In Proceedings of the 17th National Conference
on Artificial Intelligence and 12th Conference
on Innovative Applications of Artificial Intelli-
gence, pages 691?696, Austin, Texas. AAAI Press.
http://verbs.colorado.edu/?mpalmer/
projects/verbnet.html.
Edward Loper, Szu ting Yi, and Martha Palmer. 2007.
Combining Lexical Resources: Mapping between
9
PropBank and VerbNet. In Proceedings of the 7th In-
ternational Workshop on Computational Linguistics.
Alexis Mitchell, Stephanie Strassel, Mark Przybocki,
JK Davis, George Doddington, Ralph Grishman,
Adam Meyers, Ada Brunstein, Lisa Ferro, and Beth
Sundheim, 2003. ACE-2 Version 1.0. Linguistic Data
Consortium, Philadelphia.
Vincent Ng and Claire Cardie. 2002. Improving Ma-
chine Learning Approaches to Coreference Resolu-
tion. In Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics, ACL ?02,
pages 104?111, Philadelphia, Pennsylvania.
Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiff-
man, Lynette Hirschman, Marcia Linebarger, and John
Dowding. 1986. Recovering Implicit Information. In
Proceedings of the 24th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 10?19,
New York, New York, USA.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106, March.
Altaf Rahman and Vincent Ng. 2011. Narrowing the
modeling gap: A cluster-ranking approach to corefer-
ence resolution. Journal of Artificial Intelligence Re-
search, 40:469?521.
Nils Reiter. 2010. FrameNet API. http://www.cl.
uni-heidelberg.de/trac/FrameNetAPI.
Philip Resnik. 1996. Selectional Constraints: an
Information-theoretic Model and its Computational
Realization. Cognition, 61(1-2):127?159, November.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2009. SemEval-
2010 Task 10: Linking Events and Their Participants
in Discourse. In Proceedings of the NAACL-HLT 2009
Workshop on Semantic Evaluations: Recent Achieve-
ments and Future Directions (SEW-09), pages 106?
111, Boulder, Colorado, June.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. SemEval-
2010 Task 10: Linking Events and Their Participants
in Discourse. In Proceedings of the 5th International
Workshop on Semantic Evaluations, pages 45?50, Up-
psala, Sweden, July.
Josef Ruppenhofer, Philip Gorinski, and Caroline
Sporleder. 2011. In Search of Missing Arguments:
A Linguistic Approach. In Proceedings of the Inter-
national Conference Recent Advances in Natural Lan-
guage Processing, pages 331?338, Hissar, Bulgaria,
September.
Carina Silberer. 2011. Linking Implicit Semantic Roles
in Discourese Using Coreference Resolution Methods.
Master?s thesis, Department of Computational Lin-
guistics, Heidelberg University, Germany.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A Machine Learning Approach to Coref-
erence Resolution of Noun Phrases. Computational
Linguistics, 27:521?544, December.
Joel R. Tetreault. 2002. Implicit Role Reference. In
International Symposium on Reference Resolution for
Natural Language Processing, pages 109?115, Ali-
cante, Spain.
Sara Tonelli and Rodolfo Delmonte. 2010. VENSES++:
Adapting a Deep Semantic Processing System to the
Identification of Null Instantiations. In Proceedings of
the 5th International Workshop on Semantic Evalua-
tions, pages 296?299, Uppsala, Sweden, July.
Sara Tonelli and Rodolfo Delmonte. 2011. Desperately
Seeking Implicit Arguments in Text. In Proceedings of
the ACL 2011 Workshop on Relational Models of Se-
mantics, pages 54?62, Portland, Oregon, USA, June.
G. Whittemore, M. Macpherson, and G. Carlson. 1991.
Event-building through role filling and anaphora reso-
lution. In Proceedings of the 29th Annual Meeting on
Association for Computational Linguistics, pages 17?
24, Morristown, NJ, USA.
Ian H. Witten and Eibe Frank. 2000. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann, San Fran-
cisco, CA, USA.
Xiaofeng Yang, Jian Su, Guodong Zhou, and Chew Lim
Tan. 2004. An NP-cluster Based Approach to Coref-
erence Resolution. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics,
COLING ?04, pages 226?232, Geneva, Switzerland.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, Ting
Liu, and Sheng Li. 2008. An Entity-Mention Model
for Coreference Resolution with Inductive Logic Pro-
gramming. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, ACL ?08:HLT, pages
843?851, Columbus, Ohio, June.
10
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 218?227,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Aligning Predicate Argument Structures in Monolingual Comparable Texts:
A New Corpus for a New Task
Michael Roth and Anette Frank
Department of Computational Linguistics
Heidelberg University
Germany
{mroth,frank}@cl.uni-heidelberg.de
Abstract
Discourse coherence is an important aspect of
natural language that is still understudied in
computational linguistics. Our aim is to learn
factors that constitute coherent discourse from
data, with a focus on how to realize predicate-
argument structures (PAS) in a model that ex-
ceeds the sentence level. In particular, we aim
to study the case of non-realized arguments
as a coherence inducing factor. This task can
be broken down into two subtasks. The first
aligns predicates across comparable texts, ad-
mitting partial argument structure correspon-
dence. The resulting alignments and their con-
texts can then be used for developing a coher-
ence model for argument realization.
This paper introduces a large corpus of com-
parable monolingual texts as a prerequisite for
approaching this task, including an evaluation
set with manual predicate alignments. We il-
lustrate the potential of this new resource for
the empirical investigation of discourse coher-
ence phenomena. Initial experiments on the
task of predicting predicate alignments across
text pairs show promising results. Our findings
establish that manual and automatic predicate
alignments across texts are feasible and that
our data set holds potential for empirical re-
search into a variety of discourse-related tasks.
1 Introduction
Research in the fields of discourse and pragmatics
has led to a number of theories that try to explain and
formalize the effect of discourse coherence induc-
ing elements either locally or globally. For exam-
ple, Centering Theory (Grosz et al, 1995) provides
a framework to model local coherence by relating
the choice of referring expressions to the salience of
an entity at certain stages of a discourse. An exam-
ple for a global coherence model would be Rhetori-
cal Structure Theory (Mann and Thompson, 1988),
which addresses overall text structure by means of
coherence relations between the parts of a text.
In addition to such theories, computational ap-
proaches have been proposed to capture correspond-
ing phenomena empirically. A prominent example
is the entity-based model by Barzilay and Lapata
(2008). In their approach, local coherence is mod-
eled by the observation of sentence-to-sentence re-
alization patterns of individual entities. The learned
model reflects a key idea from Centering Theory,
namely that adjacent sentences in a coherent dis-
course are likely to involve the same entities.
One shortcoming of Barzilay and Lapata?s model
(and extensions of it) is that it only investigates overt
realization patterns in terms of grammatical func-
tions. These functions reflect explicit realizations of
predicate argument structures (PAS), but they do not
capture the full range of salience factors. In partic-
ular, the model does not reflect the importance of
discourse entities that fill core roles of the predicate,
but that remain implicit in the predicate?s local argu-
ment structure. We develop a specific set-up that al-
lows us to further investigate the factors that govern
such a null-instantiations of argument positions (cf.
Fillmore et al (2003)), as a special form of coher-
ence inducing element in discourse. We henceforth
refer to such cases as non-realized arguments.
Our main hypothesis is that context specific re-
alization patterns for PAS can be automatically
218
learned from a semantically parsed corpus of com-
parable text pairs. This assumption builds on
the success of previous research, where compara-
ble and parallel texts have been exploited for a
range of related learning tasks, e.g., unsupervised
discourse segmentation (Barzilay and Lee, 2004)
and bootstrapping semantic analyzers (Titov and
Kozhevnikov, 2010).
For our purposes, we are interested in finding cor-
responding PAS across comparable texts that are
known to talk about the same events, and hence in-
volve the same set of underlying event participants.
By aligning predicates in such texts, we can investi-
gate the factors that determine discourse coherence
in the realization patterns for the involved partici-
pants. As a first step towards this overall goal, we
describe the construction of a resource that contains
more than 160,000 document pairs that are known to
talk about the same events and participants. Exam-
ple (1), extracted from our corpus of aligned texts,
illustrates this point: Both texts report on the same
event, in particular the (aligned) event of locating
victims in an avalanche. While (1.a) explicitly talks
about the location of this event, the role remains im-
plicit in the second sentence of (1.b), given that it
can be recovered from the preceding sentence. In
fact, realization of this argument would impede the
fluency of discourse by being overly repetitive.
(1) a. . . . The official said that [no bodies]Arg1 had
been recovered [from the avalanches]Arg2 which
occurred late Friday in the Central Asian coun-
try near the Afghan border some 300 kilometers
(185 miles) southeast of the capital Dushanbe.
b. Three other victims were trapped in an
avalanche in the village of Khichikh. [None
of the victims bodies]Arg1 have been found
[ ]Argm-loc.
Our aim is to identify comparable predications
across pairs of texts, and to study the coherence
factors that determine the realization patterns of ar-
gument structures (including roles that remain im-
plicit) in discourse. This can be achieved by consid-
ering the full set of arguments that can be recovered
from the aligned predications, including both core
and non-core (i.e. adjunct) roles. However, in order
to relate PAS across texts to one another, we first
need to identify corresponding predicates.
In this paper, we construct a large data set to be
used for the induction of a coherence model for ar-
gument structure realization and related tasks. We
discuss the prospects of this data set for the study
of coherence factors in PAS realization. Finally, we
present first results on the initial task of predicate
alignment across comparable monolingual texts.
The remainder of this paper is structured as fol-
lows: In Section 2, we discuss previous work in re-
lated tasks. Section 3 introduces the new task to-
gether with a description of how we prepared a suit-
able data set. Section 4 discusses the potential bene-
fits of the created resource in more detail. Section 5
presents experiments on predicate alignment using
this new data set and outlines first results. Finally,
we conclude in Section 6 and discuss future work.
2 Related Work
Data sets comprising parallel texts have been re-
leased for various different tasks, including para-
phrase extraction and statistical machine translation
(SMT). While corpora for SMT are typically mul-
tilingual (e.g. Europarl, Koehn (2005)), there also
exist monolingual parallel corpora that consist of
multiple translations of one text into the same lan-
guage (Barzilay and McKeown, 2001; Huang et
al., 2002, inter alia). Each translation can pro-
vide alternative verbalizations of the same events
but little variation can be observed in context, as
the overall discourse remains the same. A higher
degree of variation can be found in the Microsoft
Research Paraphrase Corpus (e.g. MSRPC, Dolan
and Brockett (2005)), which consists of paraphrases
automatically extracted from different sources. In
the MSRPC, however, original discourse contexts
are not provided for each sentence. In contrast to
truly parallel monolingual corpora, there also exist
a range of comparable corpora that have been used
for tasks such as (multi-document) summarization
(McKeown and Radev, 1995, inter alia). Corpora for
this task are collected manually and hence are rather
small. Our work presents a method to automatically
construct a large corpus of text pairs describing the
same underlying events.
In this novel corpus, we identify common events
across texts and investigate the argument structures
that were realized in each context to establish a co-
219
herent discourse. Different aspects related to this
setting have been studied in previous work. For ex-
ample, Filippova and Strube (2007) and Cahill and
Riester (2009) examine factors that determine con-
stituent order and Belz et al (2009) study the con-
ditions for the use of different types of referring ex-
pressions. The specific set-up we examine allows
us to further investigate the factors that govern the
non-realization of an argument position, as a special
form of coherence inducing element in discourse.
As in the aforementioned work, we are specifically
interested in the generation of coherent discourses
(e.g. for summarization). Yet, our work also com-
plements research in discourse analysis. A recent
example for such work is the Semeval 2010 Task 10
(Ruppenhofer et al, 2010), which aims at linking
events and their participants in discourse. The pro-
vided data sets for this task, however, are critically
small (438 train and 525 test sentences). Eventu-
ally, the corpus we present in this paper could also
be beneficial for data-driven approaches to role link-
ing in discourse.
3 A Corpus for Aligning Predications
across Comparable Texts
Our aim is to construct a corpus of comparable texts
that can be assumed to be about the same events,
but include variation in textual presentation. This re-
quirement fits well with the news domain, for which
we can trace varying textual sources for the same
underlying events.
The English Gigaword Fifth Edition (Parker et al,
2011) corpus (henceforth just Gigaword) is one of
the largest corpus collections for English. It com-
prises a total of 9.8 million newswire articles from
seven distinct sources. For construction of our cor-
pus we make use of all combinations of agency pairs
in Gigaword.
3.1 Corpus Creation
In order to extract pairs of articles describing the
same news event, we implemented the pairwise sim-
ilarity method presented by Wubben et al (2009).
The method is based on measuring word overlap in
news headlines, weighting each word by its TF*IDF
score to give a higher impact to words occurring
with lower frequency. As our focus is to provide
a high-quality data set for predicate alignment and
follow-up tasks, we impose an additional date con-
straint to favor precision over recall. We apply this
constraint by requiring a pair of articles to be pub-
lished within a two-day time frame in order to be
considered as pairs of comparable news items.
Following this two-step procedure, we extracted a
total of 167,728 document pairs, an overall collec-
tion of 50 million word tokens. We inspected about
100 randomly selected document pairs and found
only two of them describing different events. This
is in line with the results of Wubben et al who re-
ported a precision of 93% without explicitly impos-
ing a date constraint. Overall, we found that most
text pairs share a high degree of similarity and vary
only in length (up to 7.564 words with a mean and
median of 301 and 213 words, respectively) and de-
tail. Closer examination of a development set of
10 document pairs (described below) revealed that
we can indeed find multiple cases where roles are
not locally filled in predicate argument structures.
We show instances of this phenomenon, in which
aligned PAS help to resolve implicit role references,
in Section 4.
3.2 Gold Standard Annotation
We pre-processed all texts using MATE tools
(Bohnet, 2010; Bjo?rkelund et al, 2010), a pipeline
of natural language processing modules including a
state-of-the-art semantic role labeler that computes
Prop/NomBank annotations (Palmer et al, 2005;
Meyers et al, 2008). The output was used to provide
pre-labeled verbal and nominal predicates for anno-
tation. We asked two students1 to tag alignments
of corresponding predicates in 70 text pairs derived
from the created corpus. All document pairs were
randomly chosen from the AFP and APW sections
of Gigaword with the constraint that each text con-
sists of 100 to 300 words2. We chose this constraint
as longer text pairs contain a high number of unre-
lated predicates, making this task difficult to manage
for the annotators.
Sure and possible links. Following standard prac-
tice in word alignment tasks (cf. Cohn et al (2008))
1Both annotators are students in Computational Linguistics,
one undergraduate (A) and one postgraduate (B) student.
2This constraint is satisfied by 75.3% of the documents.
220
the annotators were instructed to distinguish be-
tween sure (S) and possible (P) alignments, depend-
ing on how certainly, in their opinion, two predi-
cates (including their arguments) describe the same
event. The following examples show cases of predi-
cate pairings marked as sure (S link) (2) and as pos-
sible (P link) alignments (3):
(2) a. The regulator ruled on September 27 that Nas-
daq too was qualified to bid for OMX [. . . ]3
b. The authority [. . . ] had already approved a sim-
ilar application by Nasdaq.4
(3) a. Myanmar?s military government said earlier this
year it has released some 220 political prisoners
[. . . ]5
b. The government has been regularly releasing
members of Suu Kyi?s National League for
Democracy party [. . . ]6
Replaceability. As a guideline for deciding
whether two predicates are to be aligned, the
annotators were given the following two criteria: 1)
whether the predicates are replaceable in a given
context and 2) whether they share (potentially
implicit) arguments.
Missing context. In case one text does not provide
enough context to decide whether two predicates in
the paired documents refer to the same event, an
alignment should not be marked as sure.
Similar predicates. Annotators were told explic-
itly that sure links can be used even if two predicates
are semantically different but have the same mean-
ing in context. Example (4) illustrates such a case:
(4) a. The volcano roared back to life two weeks ago.
b. It began erupting last month.
1-to-1 vs. n-to-m. We asked the annotators to find
as many 1-to-1 correspondences as possible and to
prefer 1-to-1 matches over n-to-m alignments. In
case of multiple mentions (cf. Example (5)) of the
same event, we further asked the annotators to pro-
vide only one S link per predicate and mark remain-
ing cases as P links. If possible, the S link should
3Source document ID: AFP ENG 20071112.0235
4Source document ID: APW ENG 20071112.0645
5Source document ID: AFP ENG 20020301.0041
6Source document ID: APW ENG 20020301.0132
be used for the pairing of PAS with the highest in-
formation overlap (e.g. ?performa3???performb2? in
(5)). If there is no difference in information over-
lap, the predicate pair that occurs first in both texts
should be marked as a sure alignment (e.g. ?singa1??
?performb1? in (5)). The intuition behind this guide-
line is that the first mention introduces the actual
event while later mentions just (co-)refer or add fur-
ther information.
(5) a. Susan Boyle said she will singa1 in front of
Britain?s Prince Charles (. . . ) ?It?s going to be
a privilege to be performinga2 before His Royal
Highness,? the singer said (. . . ) British copy-
right laws will allow her to performa3 the hit in
front of the prince and his wife.7
b. British singing sensation Susan Boyle is going
to performb1 for Prince Charles (. . . ) The show
star will performb2 her version of Perfect Day
for Charles and his wife Camilla.8
3.3 Development and Evaluation Data Sets
In total, the annotators (A/B) aligned 487/451 sure
and 221/180 possible alignments with a Kappa score
(Cohen, 1960) of 0.86. Following Brockett (2007),
we computed agreement on labeled annotations, in-
cluding unaligned predicate pairs as an additional
null category. For the construction of a gold stan-
dard, we merged the alignments from both annota-
tors by taking the union of all possible alignments
and the intersection of all sure alignments. Cases
which involved a sure alignment on which the anno-
tators disagreed were resolved in a group discussion
with the first author. We split the final corpus into a
development set of 10 document pairs and a test set
of 60 document pairs.
Table 1 summarizes information about the result-
ing annotations in the development and test sets,
respectively. It gives information about the paired
texts (PT): number of predicates marked in prepro-
cessing (nouns and verbs), the set of manual predi-
cate alignments (PA): sure and possible, as well as
information about whether they were annotated for
predicates of the same PoS (N,V) or lemma.
Finally, as a rough indicator for diverging ar-
gument structures captured in the annotated align-
7Source document ID: AFP ENG 20101102.0028
8Source document ID: APW ENG 20101102.0923
221
Dev Set Test Set
nb. of PT 10 60
nb. marked predicates 395 3,453
nb. marked nouns 168 1,531
nb. marked verbs 227 1,922
sure PA/PT: avg. (total) 3.9 (35) 7.4 (446)
poss. PA/PT: avg. (total) 4.8 (43) 6.0 (361)
same PoS in PA (N/V) 88.5% (24/42) 82.4% (242/423)
same lemma in PA 53.8% (42) 47.5% (383)
unequal nb. args in PA 30.8% (24) 39.7% (320)
Table 1: Information on Paired Texts (PT) and manual
Predicate Alignments (PA) in development and test set
ments, we analyzed the number of PAs that involve
a different number of arguments.
4 Potential of Aggregation
In this section, we analyze the predicate alignments
in our manually annotated data set, to illustrate the
potential of aggregating corresponding PAS across
comparable texts.
We are particularly interested in cases of non-
realization of arguments, and thus take a closer look
at alignments involving roles that are not filled in
their local PAS. We extract a subset of such cases
by extracting pairs of aligned predicates that con-
tain a different number of realized arguments. We
deliberately focus on the more restricted core roles
in this exposition, but will consider the full range
of roles for developing a comprehensive coherence
model for argument structure realization.9 Our se-
lection of alignment examples is drawn from the de-
velopment set.
The following excerpts are from a pair of com-
parable texts describing a news report on Chadian
refugees crossing into Nigeria:
(6) a. The Chadians said [they]Arg0 had fled [ ]Arg1 in
fear of their lives.10
b. The United Nations says
[some 20,000 refugees]Arg0 have fled
[into Cameroon]Arg1.11
In both examples, the Arg0 role of the predicate fled
is filled, but Arg1 has not been realized in (6.a). Note
9Accordingly, the number of PAs involving diverging role
realizations in Table 1 is strongly underestimated.
10Source document ID: AFP ENG 20080205.0230
11Source document ID: APW ENG 20080206.0766
that the sentence is still part of a coherent discourse
as fillers for the omitted role can be inferred from
the preceding discourse context. Aggregating the
aligned PAS presents an effective means to identify
such appropriate fillers.
Example (7) presents another text pair, reporting
on elections in Iraq, in which role realizations differ
for the same hold event.
(7) a. He said (. . . ) [elections]Arg1 will be held [ ]Arg0
to form a government.12
b. The president (. . . ) said Wednesday
[his country]Arg0 will definitely hold
[elections]Arg1 in 2004.13
Here, the changes in argument realization go
along with a diathesis alternation, while the pair in
(6) exemplifies a case of lexical licensing for omis-
sion of a role.14
Example (8.b) illustrates a case in which the Arg1
of a decline event is involved in a preceding pred-
ication (rise) and thus has already been overtly re-
alized. The constructional properties of the subse-
quent predicates decline as a participle and noun, re-
spectively, are more adverse to overt realization of
the Arg1 role. Suppression of Arg1 in such cases
yields a much more coherent discourse as compared
to their realization. This is brought out by the con-
structed examples in (a?/b?), which are both highly
repetitive.
(8) a. The closely watched [index]Arg1 rose to 93.7
. . . after declining for . . . months.15
a?. ? . . . after the index declining for . . . months.
b. Consumer confidence rose . . . following three
months of dramatic decline [ ]Arg1.16
b?. ? . . . following three months of dramatic decline
[of consumer confidence]Arg1.
As showcased by the previous examples, the de-
cision on whether to realize a role filler in a lo-
cal PAS can be rather complex. Obviously, the
12Source document ID: AFP ENG 20031015.0353
13Source document ID: APW ENG 20031015.0236
14These different configurations are termed constructional
vs. lexical licensors in the SemEval 2010 Task 10 (Ruppen-
hofer et al, 2010).
15Source document ID: AFP ENG 20011228.0365
16Source document ID: APW ENG 20011228.0572
222
Figure 1: The predicates of two sentences (white: ?The company has said it plans to restate its earnings for 2000
through 2002.?; gray: ?The company had announced in January that it would have to restate earnings (. . . )?) from the
Microsoft Research Paragraph Corpus are aligned by computing clusters with minimum cuts.
above instances do not provide exhaustive informa-
tion for grounding all such decisions. A comprehen-
sive model of discourse coherence will need to esti-
mate the argument realization potential of different
predicates and roles from larger corpora. But as can
be seen from the discussed examples, training a se-
mantic model with suitable discourse features on all
predicate argument structures in a large corpus such
as ours will provide indicative range of realization
decisions.
5 Experiments
This section presents an initial experiment using an
unsupervised graph-based clustering method for the
task of aligning predicates across comparable texts.
We describe the alignment model, two baselines as
well as the experimental setting and results.17
5.1 Clustering Model
Similarity Measures. We define a number of sim-
ilarity measures between predicates, which make
use of complementary lexical information. One
source of information are token-based frequency
counts, which we compute over all documents
from the AFP and APW sections of Gigaword18.
Given two lemmatized predicates and their respec-
tive PAS, we employ the following four similarity
17The technicalities of this model, including detailed def-
initions of the similarity measures, are described elsewhere
(manuscript, under submission).
18These sections make up 56.6% of documents in Gigaword.
measures: Similarity in WordNet (simWN) and Verb-
Net (simVN), distributional similarity (simDist) and
bag-of-word similarity of arguments (simArgs). The
first three measures are type-based, whereas the lat-
ter is token-based.
Graph Representation. The input for graph clus-
tering is a bi-partite graph representation for pairs
of texts to be predicate-aligned. In this graph, each
node represents a PAS that was assigned during pre-
processing (cf. Section 3). Edges are inserted be-
tween pairs of predicates that are from two distinct
texts. A weight is assigned to each edge by a com-
bination of the introduced similarity measures.
Clustering algorithm. The graph clustering
method uses minimum cuts (or Mincuts) in order
to partition the bipartite text graph into clusters of
aligned predicates. Each Mincut operation divides
a graph into two disjoint sub-graphs, such that the
sum of weights of removed edges will be minimal.
As the goal is to induce clusters consisting of pairs
of similar predicates, a maximum number of two
nodes per cluster is set as stopping criterion. We
apply Mincut recursively to the input graph and
resulting sub-graphs until we reach the stopping
criterion. Figure 1 shows an example of a graph
clustered by the Mincut approach.
5.2 Setting
We perform evaluations of the graph-based align-
ment model (henceforth called Clustering) on the
223
task of inducing predicate alignments across com-
parable monolingual texts. We evaluate on the man-
ually annotated gold alignments in the test data set
described in Section 3.2.
Parameter Tuning. As the graph representation
becomes rather inefficient to handle using edges be-
tween all predicate pairs, we use the development
set of 10 text pairs to estimate a threshold for adding
edges. We found the best similarity threshold to be
an edge weight of 2.5. Note that the edge weights are
calculated as a weighted linear combination of four
different similarity measures. Subsequently, we also
tune the weighting scheme for similarity measures
on the development set. We found the best perform-
ing combination of weights to be 0.09, 0.19, 0.48
and 0.24 for simWN, simVN, simDist and simArgs, re-
spectively.
Baselines. A simple baseline for this task is to
align all predicates whose lemmas are identical
(SameLemma). As a more sophisticated baseline,
we make use of alignment tools commonly used in
statistical machine translation (SMT). We train our
own word alignment model using the state-of-the-art
tool Berkeley Aligner (Liang et al, 2006). As word
alignment tools require pairs of sentences as input,
we first extract paraphrases for this baseline using a
re-implementation of the paraphrase detection sys-
tem by Wan et al (2006). In the following sections,
we abbreviate this model as WordAlign.
5.3 Results
Following Cohn et al (2008) we measure precision
as the number of predicted alignments also anno-
tated in the gold standard divided by the total num-
ber of predictions. Recall is measured as the num-
ber of correctly predicted sure alignments devided
by the total number of sure alignments in the gold
standard. We subsequently compute the F1-score as
the harmonic mean between precision and recall.
Table 2 presents the results for our model and
the two baselines. From all four approaches,
WordAlign performs worst. We identify two main
reasons for this: On the one hand, the paraphrase
detection does not perform perfectly. Hence, the
extracted sentence pairs do not always contain gold
alignments. On the other hand, even sentence pairs
that contain gold alignments are generally less paral-
Precision Recall F1
WordAlign 19.7% 15.2% 17.2%
SameLemma 40.3% 60.3% 48.3%
Clustering 59.7% 50.7% 54.8%
Table 2: Results for all models on our test set; significant
improvements (p<0.005) over the results given in each
previous line are marked in bold face.
lel compared to a typical SMT setting, which makes
them harder to align.
We observe that the majority of all sure align-
ments (60.3%) can be retrieved by applying the
SameLemma model, yet at a low precision (40.3%).
While the Clustering model only recalls 50.7% of
all cases, it clearly outperforms SameLemma in
terms of precision (+19.4% points), an important
factor for us as we plan to use the alignments in
subsequent tasks. With 54.8%, Clustering also
achieves the best overall F1-score. We computed
statistical significance of result differences with a
paired t-test (Cohen, 1995), yielding significance at
the 99.5% level for precision and F1-score.
5.4 Analysis of Results
We perform an analysis of the output of the Clus-
tering model on the development set to categorize
correct and incorrect alignment decisions.19 In to-
tal, the model missed 13 out of 35 sure alignments
(Type I errors) and predicted 23 alignments not an-
notated in the gold standard (Type II errors). Six
Type I errors (46%) occurred when the lemma of an
affected predicate occurred more than once in a text
and the model missed the correct link. Vice versa,
we find 18 Type II errors (78%) that were made be-
cause of a high predicate similarity despite low ar-
gument overlap. An example is given in (9).
(9) a. The US alert (. . . ) followed intelligence reports
that . . . 20
b. The Foreign Ministry announcement called on
Japanese citizens to be cautious . . . 21
While argument overlap itself can be low even for
correct alignments, the results clearly indicate that
19We decided to leave the test set untouched for further exper-
iments. Due to parameter tuning, the results on the development
set alo provide us with an upper bound of the proposed model.
20Source document ID: AFP ENG 20101004.0367
21Source document ID: APW ENG 20101004.0207
224
a better integration of context is necessary: Exam-
ple (10.a) illustrates a case in which the agent of a
warning event is not realized. Here, contextual in-
formation is required to correctly align it to one of
the warning events in (10.b). This involves inference
beyond the local PAS.
(10) a. The US alert (. . . ) is one step down from a full
[travel]Arg1 warning [ ]Arg0.20
b. Japan has issued a travel alert . . . (which)
follows similar warnings [from Ameri-
can and British authorities]Arg0. (. . . ) An offi-
cial said it was highly unusual for [Tokyo]Arg0
to issue such a warning . . . 21
On the positive side, Clustering achieves a precision
of 61.4% and a recall of 65.7% on the development
set. Example (11) shows a correctly aligned PAS
pair that involves non-realized arguments:
(11) a. . . . the Governing Council has established
[a committee]Arg0 to draft [a constitution]Arg1.22
b. A .. resolution calls on the Governing
Council for elections and the drafting [ ]Arg0
[of a new constitution]Arg1.23
In (11.a), the follow-up sentences will refer back to
the committee that will draft the new Iraqi constitu-
tion, hence the institution has to be introduced in the
discourse at this point. In contrast, excerpt (11.b) is
the last sentence of a news report. Since it presents
a summary, introducing new (omissible) entities at
this point would not concord with general coherence
principles.
6 Conclusion
In this paper, we presented a novel corpus of compa-
rable texts that provides full discourse contexts for
alternative verbalizations. The motivation for the
construction of this corpus is to acquire empirical
data for studying discourse coherence factors related
to argument structure realization. A special phe-
nomenon we are interested in are discourse-related
factors that license the omission of argument roles.
Our data set satisfies two conditions that are es-
sential for the purported task: the texts are about
22Source document ID: AFP ENG 20031015.0353
23Source document ID: APW ENG 20031015.0236.
the same events and constitute alternative verbaliza-
tions. Selected from the Gigaword corpus, the doc-
uments pertain to the news domain, and satisfy the
further constraint that we have access to the full sur-
rounding discourse context. The constructed corpus
could thus be profitable for a range of other tasks
that need to investigate factors for knowledge aggre-
gation, such as summarization, or inference in dis-
course, such as textual entailment.
In total, we derived more than 160,000 document
pairs from all pairwise combinations of newswire
sources in the English Gigaword Fifth Edition. Us-
ing a subset of these pairs, we constructed a devel-
opment and an evaluation data set with gold align-
ments that relate predications with (possibly partial)
PAS correspondence. We established that the anno-
tation task, while difficult, can be performed with
good inter-annotator agreement (? at 0.86).
We presented first experiments on the task of au-
tomatically predicting predicate alignments. This
step is essential to gather empirical evidence of dif-
ferent PAS realizations for the same event, given
varying discourse contexts. Analysis of the data
shows that the aligned predications capture a wide
variety of sources and variations of coherence ef-
fects, including constructional, lexical and discourse
phenomena.
In future work, we will enhance our model by in-
corporating more refined semantic similarity mea-
sures including discourse-based criteria for estab-
lishing cross-document alignments. Given that our
data set includes sets of aligned documents from
several newswire sources, we will explore transitiv-
ity constraints across multiple document pairs in or-
der to further enhance the precision of the alignment
model. We will then proceed to the ultimate aim of
our work: the development of a coherence model for
argument structure realization, including the design
of an appropriate task and evaluation setting.
Acknowledgements
We are grateful to the Landesgraduiertenfo?rderung
Baden-Wu?rttemberg for funding within the research
initiative ?Coherence in language processing? of
Heidelberg University. We thank Danny Rehl and
Lukas Funk for annotation and Kathrin Spreyer, Tae-
Gil Noh and Carina Silberer for helpful discussion.
225
References
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics, Boston, Mass., 2?7 May 2004,
pages 113?120.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the Associ-
ation for Computational Linguistics, Toulouse, pages
50?57.
Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.
2009. The grec main subject reference generation
challenge 2009: overview and evaluation results. In
Proceedings of the 2009 Workshop on Language Gen-
eration and Summarisation, pages 79?87.
Anders Bjo?rkelund, Bernd Bohnet, Love Hafdell, and
Pierre Nugues. 2010. A high-performance syntac-
tic and semantic dependency parser. In Coling 2010:
Demonstration Volume, pages 33?36, Beijing, China,
August. Coling 2010 Organizing Committee.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 89?97, Beijing, China,
August. Coling 2010 Organizing Committee.
Chris Brockett. 2007. Aligning the RTE 2006 Corpus.
Microsoft Research.
Aoife Cahill and Arndt Riester. 2009. Incorporating in-
formation status into generation ranking. In Proceed-
ings of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the AFNLP,
pages 817?825, Suntec, Singapore, August. Associa-
tion for Computational Linguistics.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20:37?46.
Paul R. Cohen. 1995. Empirical methods for artificial
intelligence. MIT Press, Cambridge, MA, USA.
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing Corpora for Development and
Evaluation of Paraphrase Systems. 34(4).
William B. Dolan and Chris Brockett. 2005. Automat-
ically constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop on
Paraphrasing.
Katja Filippova and Michael Strube. 2007. Generat-
ing constituent order in German clauses. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, Prague, Czech Republic,
23?30 June 2007, pages 320?327.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16.3:235?250.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Shudong Huang, David Graff, and George Doddington.
2002. Multiple-Translation Chinese Corpus. Linguis-
tic Data Consortium, Philadelphia.
Philipp Koehn, 2005. Europarl: A parallel corpus for
statistical machine translation, volume 5, pages 79?
86.
Percy Liang, Benjamin Taskar, and Dan Klein. 2006.
Alignment by agreement. In North American Associ-
ation for Computational Linguistics (NAACL), pages
104?111.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory. Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Kathleen R. McKeown and Dragomir Radev. 1995. Gen-
erating summaries of multiple news articles. In Pro-
ceedings of the 18th Annual International ACM-SIGIR
Conference on Research and Development in Informa-
tion Retrieval, Seattle, Wash., 9?13 July 1995, pages
74?82. Reprinted in Advances in Automatic Text Sum-
marization, Mani, I. and Maybury, M.T. (Eds.), Cam-
bridge, Mass.: MIT Press, 1999, pp.381-389.
Adam Meyers, Ruth Reeves, and Catherine Macleod.
2008. NomBank v1.0. Linguistic Data Consortium,
Philadelphia.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
105.
Robert Parker, David Graff, Jumbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion. Linguistic Data Consortium, Philadelphia.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. SemEval-
2010 Task 10: Linking Events and Their Participants
in Discourse. In Proceedings of the 5th International
Workshop on Semantic Evaluations, pages 45?50, Up-
psala, Sweden, July.
Ivan Titov and Mikhail Kozhevnikov. 2010. Bootstrap-
ping semantic analyzers from non-contradictory texts.
226
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, Uppsala, Swe-
den, 11?16 July 2010, pages 958?967.
Stephen Wan, Mark Dras, Robert Dale, and Cecile Paris.
2006. Using dependency-based features to take the
?Para-farce? out of paraphrase. In Proceedings of the
Australasian Language Technology Workshop, pages
131?138.
Sander Wubben, Antal van den Bosch, Emiel Krahmer,
and Erwin Marsi. 2009. Clustering and matching
headlines for automatic paraphrase acquisition. In
Proceedings of the 12th European Workshop on Nat-
ural Language Generation (ENLG 2009), pages 122?
125, Athens, Greece, March. Association for Compu-
tational Linguistics.
227
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 306?316, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Automatically Identifying Implicit Arguments to
Improve Argument Linking and Coherence Modeling
Michael Roth and Anette Frank
Department of Computational Linguistics
Heidelberg University, Germany
{mroth,frank}@cl.uni-heidelberg.de
Abstract
Implicit arguments are a discourse-level phe-
nomenon that has not been extensively stud-
ied in semantic processing. One reason for
this lies in the scarce amount of annotated data
sets available. We argue that more data of
this kind would be helpful to improve exist-
ing approaches to linking implicit arguments
in discourse and to enable more in-depth stud-
ies of the phenomenon itself. In this paper, we
present a range of studies that empirically val-
idate this claim. Our contributions are three-
fold: we present a heuristic approach to auto-
matically identify implicit arguments and their
antecedents by exploiting comparable texts;
we show how the induced data can be used as
training data for improving existing argument
linking models; finally, we present a novel ap-
proach to modeling local coherence that ex-
tends previous approaches by taking into ac-
count non-explicit entity references.
1 Introduction
Semantic role labeling systems traditionally process
text in a sentence-by-sentence fashion, construct-
ing local structures of semantic meaning (Palmer et
al., 2010). Information relevant to these structures,
however, can be non-local in natural language texts
(Palmer et al, 1986; Fillmore, 1986, inter alia). In
this paper, we view instances of this phenomenon,
also referred to as implicit arguments, as elements
of discourse. In a coherent discourse, each utter-
ance focuses on a salient set of entities, also called
?foci? (Sidner, 1979) or ?centers? (Joshi and Kuhn,
1979). According to the theory of Centering (Grosz
et al, 1995), the salience of an entity in a discourse
is reflected by linguistic factors such as choice of
referring expression and syntactic form. Both ex-
tremes of salience, i.e., contexts of referential conti-
nuity (Brown, 1983) and irrelevance, can also be re-
flected by the non-realization of an entity. Altough
specific instances of non-realization, so-called zero
anaphora, have been well-studied in discourse anal-
ysis (Sag and Hankamer, 1984; Tanenhaus and Carl-
son, 1990, inter alia), this phenomenon has widely
been ignored in computational approaches to entity-
based coherence modeling. It could, however, pro-
vide an explanation for local coherence in cases that
are not covered by current models of Centering (cf.
Louis and Nenkova (2010)). In this work, we pro-
pose a new model to predict whether realizing an
argument contributes to local coherence in a given
position in discourse. Example (1) shows a text frag-
ment, in which argument realization is necessary in
the first sentence but redundant in the second.
(1) El Salvador is now the only Latin Ameri-
can country which still has troops in [Iraq].
Nicaragua, Honduras and the Dominican
Republic have withdrawn their troops [?].
From a semantic processing perspective, a human
reader can easily infer that ?Iraq?, the marked en-
tity in the first sentence of Example (1), is also an
implicit argument of the predicate ?withdraw? in the
second sentence. This inference step is, however,
difficult to model computationally as it involves an
interplay of two challenging sub-tasks: first, a se-
mantic processor has to determine that an argument
is not realized (but inferrable); and second, a suit-
306
able antecedent has to be found within the discourse
context. For the remainder of this paper, we refer to
these steps as identifying and linking implicit argu-
ments to discourse antecedents.
As indicated by Example (1), implicit arguments
are an important aspect in semantic processing, yet
they are not captured in traditional semantic role la-
beling systems. The main reasons for this are the
scarcity of annotated data, and the inherent difficulty
of inferring discourse antecedents automatically.
In this paper, we propose to induce implicit ar-
guments and discourse antecedents by exploiting
complementary (explicit) information obtained from
monolingual comparable texts (Section 3). We ap-
ply the empirically acquired data in argument link-
ing (Section 4) and coherence modeling (Section 5).
We conclude with a discussion on the advantages of
our data set and outline directions for future work
(Section 6).
2 Related work
The most prominent approach to entity-based coher-
ence modeling nowadays is the entity grid model by
Barzilay and Lapata (2005). It has originally been
proposed for automatic sentence ordering but has
also been applied in coherence evaluation and read-
ability assessment (Barzilay and Lapata, 2008; Pitler
and Nenkova, 2008), and story generation (McIntyre
and Lapata, 2009). Based on the original model,
a few extensions have been proposed: for exam-
ple, Filippova and Strube (2007) and Elsner and
Charniak (2011b) suggested additional features to
characterize semantic relatedness between entities
and features specific to single entities, respectively.
Other entity-based approaches to coherence model-
ing include the pronoun model by Charniak and El-
sner (2009) and the discourse-new model by Elsner
and Charniak (2008). All of these approaches are,
however, based on explicitly realized entity men-
tions only, ignoring references that are inferrable.
The role of implicit arguments has been studied
early on in the context of semantic processing (Fill-
more, 1986; Palmer et al, 1986). Yet, the phe-
nomenon has mostly been ignored in semantic role
labeling. First data sets, focusing on implicit argu-
ments, have only recently become available: Rup-
penhofer et al (2010) organized a SemEval shared
task on ?linking events and participants in dis-
course?, Gerber and Chai (2012) made available im-
plicit argument annotations for the NomBank corpus
(Meyers et al, 2008) and Moor et al (2013) pro-
vide annotations for parts of the OntoNotes corpus
(Weischedel et al, 2011). However, these resources
are very limited: The annotations by Moor et al and
Gerber and Chai are restricted to 5 and 10 predi-
cate types, respectively. The training set of the Se-
mEval task contains only 245 resolved implicit argu-
ments in total. As pointed out by Silberer and Frank
(2012), additional training data can be heuristically
created by treating anaphoric mentions as implicit
arguments. Their experimental results showed that
artificial training data can indeed improve results,
but only when obtained from corpora with manual
semantic role annotations (on the sentence level) and
gold coreference chains.
3 Identifying and linking implicit
arguments
The aim of this work is to automatically construct
a data set of implicit arguments and their discourse
antecedents. We propose an induction approach that
exploits complementary information obtained from
pairs of comparable texts. As a basis for this ap-
proach, we rely on several preparatory steps pro-
posed in the literature that first identify informa-
tion two documents have in common (cf. Figure 1).
In particular, we align corresponding predicate-
argument structures (PAS) using graph-based clus-
tering (Roth and Frank, 2012b). We then determine
co-referring entities across the texts using corefer-
ence resolution techniques on concatenated docu-
ment pairs (Lee et al, 2012). These preprocessing
steps are described in more detail in Section 3.1.
Given the preprocessed comparable texts and
aligned PAS, we propose to heuristically iden-
tify implicit arguments and link them to their
antecedents via the cross-document coreference
chains. We describe the details of this approach in
Section 3.2.
3.1 Data preparation
The starting point for our approach is the data set of
automatically aligned predicate pairs that has been
released by Roth and Frank (2012a).1 This data
1cf. http://www.cl.uni-heidelberg.de/%7Emroth/
307
Sentence that comprises a PAS with an (correctly predicted) implicit argument induced antecedent
The [?A0] [operatingA3] loss, as measured by . . . widened to 189 million euros . . . T-Online[?s]
It was handed over to Mozambican control . . . 33 years after [?A0] independence. Mozambique[?s]
. . . [local officials A0] failed to immediately report [the accident A1] [?A2] . . . [to] the government
Table 1: Three positive examples of automatically induced implicit argument and antecedent pairs.
Figure 1: Illustration of the induction approach: texts
consist of PAS (represented by overlapping circles);
we exploit alignments between corresponding predicates
across texts (marked by solid lines) and co-referring enti-
ties (marked by dotted lines) to infer implicit arguments
(marked by ?i?) and link antecedents (curly dashed line)
set, henceforth just R&F data, is a collection of
283,588 predicate pairs that have been aligned ?with
high precision?2 across comparable newswire arti-
cles from the Gigaword corpus (Parker et al, 2011).
To use these documents for our argument induc-
tion technique, we apply a couple of pre-processing
tools on each single document and perform cross-
document entity coreference on pairs of documents.
Single document pre-processing. We apply sev-
eral preprocessing steps to all documents in
the R&F data: we use the Stanford CoreNLP
package3 for tokenization and sentence split-
ting. We then apply MATE tools (Bohnet, 2010;
Bjo?rkelund et al, 2010), including the integrated
PropBank/NomBank-style semantic parser, to re-
construct local predicate-argument structures for
aligned predicates. Finally, we resolve pronouns that
occur in a PAS using the coreference resolution sys-
tem by Martschat et al (2012).
2The used method achieved a precision of 86.2% at a recall
of 29.1% on the Roth and Frank (2012a) test set.
3http://nlp.stanford.edu/software/
Cross-document coreference. We apply cross-
document coreference resolution to induce an-
tecedents for implicit arguments. In practice, we
use the Stanford Coreference System (Lee et al,
2013) and run it on pairs of texts by simply pro-
viding a single document as input, comprising of a
concatenation of the two texts. To perform this step
with high precision, we only use the most precise
resolution sieves: ?String Match?, ?Relaxed String
Match?, ?Precise Constructs?, ?Strict Head Match
[A-C]?, and ?Proper Head Noun Match?.
3.2 Identification and linking approach
Given a pair of aligned predicates from two compa-
rable texts, we examine the parser output to identify
the arguments in each predicate-argument structure
(PAS). We compare the set of realized argument po-
sitions in both structures to determine whether one
PAS contains an argument position (explicit) that
has not been realized in the other PAS (implicit).
For each implicit argument, we identify appropri-
ate antecedents by considering the cross-document
coreference chain of its explicit counterpart. As our
goal is to link arguments within discourse, we re-
strict candidate antecedents to mentions that occur
in the same document as the implicit argument.
We apply a number of restrictions to the resulting
pairs of implicit arguments and antecedents to mini-
mize the impact of errors from preprocessing:
- The aligned PAS should consist of a different
number of arguments (to minimize the impact
of argument labeling errors)
- The antecedent should not be a resolved pro-
noun (to avoid errors resulting from incorrect
pronoun resolution)
- The antecedent should not be in the same sen-
tence as the implicit argument (to circumvent
cases, in which an implicit argument is actu-
ally explicit but has not been recognized by the
parser)
308
3.3 Resulting data set
We apply the identification and linking approach to
the full R&F data set of aligned predicates. As a re-
sult, we induce a total of 701 implicit argument and
antecedent pairs, each in a separate document, in-
volving 535 different predicates. Examples are dis-
played in Table 1. Note that 701 implicit arguments
from 283,588 pairs of predicate-argument structures
seem to represent a fairly low recall. Most predicate
pairs in the high precision data set of Roth and Frank
(2012a) do, however, consist of identical argument
positions (84.5%). In the remaining cases, in which
an implicit argument can be identified (15.5%), an
antecedent in discourse cannot always be found us-
ing the high precision coreference sieves. This does
not mean that implicit arguments are a rare phe-
nomenon in general. In fact, 38.9% of all manually
aligned predicate pairs in Roth and Frank (2012a)
involved a different number of arguments.
We manually evaluated a subset of 90 induced im-
plicit arguments and found 80 discourse antecedents
to be correct (89%). Some incorrectly linked in-
stances still result from preprocessing errors. In Ta-
ble 2, we present a range of different error types that
occurred when extracting implicit arguments with-
out any restrictions.
4 Experiment 1: Linking implicit
arguments
Our first experiment assesses the utility of automat-
ically induced implicit arguments and antecedent
pairs for the task of implicit argument linking. For
evaluation, we use the data sets from the SemEval
2010 task on Linking Events and their Participants
in Discourse (Ruppenhofer et al, 2010, henceforth
just SemEval). For direct comparison with previous
results and heuristic acquisition techniques (cf. Sec-
tion 2), we apply the implicit argument identifica-
tion and linking model by Silberer and Frank (2012,
henceforth S&F) for training and testing.
4.1 Task summary
Both the training and test sets of the SemEval task
are text corpora extracted from Sherlock Holmes
novels, with manual frame semantic annotations in-
cluding implicit arguments. In the actual linking
task (?NI-only?), labels are provided for local argu-
ments and participating systems have to perform the
following three sub-tasks: (1) identify implicit argu-
ments (IA), (2) predict whether each IA is resolvable
and, if so, (3) find an appropriate antecedent.
The task organizers provide two versions of their
data sets: one based on FrameNet annotations and
one based on PropBank/NomBank annotations. We
found that the latter, however, only contains a sub-
set of the implicit argument annotations from the
FrameNet-based version. As all previous results in
this task have been reported on the FrameNet data
set, we adopt the same setting. Note that our addi-
tional training data is automatically labeled with a
PropBank/NomBank-style parser. That is, we need
to map our annotations to FrameNet. The organizers
of the SemEval shared task provide a manual map-
ping dictionary for predicates in the annotated data
set. We make use of this manual mapping and ad-
ditionally use SemLink 1.14 for mapping predicates
and arguments not in the dictionary.
4.2 Model details
We make use of the system by S&F to train a new
model for the NI-only task. As mentioned in the pre-
vious sub-section, this task consists of three steps:
In step (1), implicit arguments are identified as un-
filled FrameNet core roles that are not competing
with roles that are already filled; in step (2), a SVM
classifier is used to predict whether implicit argu-
ments are resolvable based on a small amount of
features ? semantic type of the affected Frame Ele-
ment, the relative frequency of its realization type in
the SemEval training corpus, and a boolean feature
that indicates whether the affected sentence is in pas-
sive voice and does not contain a (deep) subject. In
step (3), we apply the same features and classifier as
S&F, i.e., the BayesNet implementation from Weka
(Witten and Frank, 2005), to find appropriate an-
tecedents for (predicted) resolvable arguments. S&F
report that their best results were obtained when
considering all entities as candidate antecedents that
are syntactic constituents from the present and the
past two sentences, or entities that occurred at least
five times in the previous discourse (?Chains+Win?
setting). In their evaluation, the latter of these two
restrictions crucially depended on gold coreference
chains. As the automatic coreference chains in our
4http://verbs.colorado.edu/semlink/
309
Sentence that comprises a PAS with an (incorrectly predicted) implicit argument induced antecedent
(1) .. [Statistics?] released [Tuesday TMP ] [?A0] showed the death toll dropped . . . official statistics
(2) A [French LOC?] [?A0] draft resolution . . . demands full . . . compliance . . . France
(3) An earthquake . . . is capable of causing .. [heavy EXT ] damage [?A2?] major
Table 2: Examples of erroneous pairs of implicit arguments and antecedents. In (1), the parser did not recognize
?Statistics? as an argument of showed; in (2), the parser mislabeled ?French? as a locative modifier; both errors lead
to incorrectly identified implicit arguments. In (3), the implicit argument is correct but the wrong antecedent was
identified because ?major? had been mislabeled in the aligned predicate-argument structure
data are rather sparse (and noisy), we only consider
syntactic constituents from the present and the past
two sentences as antecedents (?SentWin? setting).
Before training and testing a new model with
our own data, we perform feature selection us-
ing 10-fold cross validation. We run the feature
selection on a combination of the SemEval train-
ing data and our additional data set in order to
find a set of features that generalizes best across
the two different corpora. We found these to be
features regarding ?prominence?, selectional pref-
erences (?sp supersense?), the POS tags of entity
mentions, and semantic types of argument positions
(?semType dni.entity?). Note that the S&F system
does not make use of any lexicalized information.
Instead, semantic features are computed based on
the highest abstraction level in WordNet (Fellbaum,
1998). For detailed description of all features, see
Silberer and Frank (2012).
4.3 Results
For direct comparison in the full task, both with
S&F?s model and other previously published results,
we adopt the precision, recall and F1 measures as
defined in Ruppenhofer et al (2010). We compare
our results with those previously reported on the Se-
mEval task (see Table 3 for a summary): Chen et
al. (2010) adapted SEMAFOR, the best performing
system that participated in the actual task in 2010.
Tonelli and Delmonte (2011) presented a revised
version of their SemEval system (Tonelli and Del-
monte, 2010), which outperformed SEMAFOR in
terms of recall (6%) and F1 score (8%). The best
results in terms of recall and F1 score up to date
have been reported by Laparra and Rigau (2012),
with 25% and 19%, respectively. Our model outper-
forms their state-of-the-art system in terms of preci-
sion (21%) but at a higher cost of recall (8%). Two
P R F
Chen et al (2010)5 0.25 0.01 0.02
Tonelli and Delmonte (2011) 0.13 0.06 0.08
Laparra and Rigau (2012) 0.15 0.25 0.19
Laparra and Rigau (2013) 0.14 0.18 0.16
Gorinski et al (2013)6 0.14 0.12 0.13
S&F (no additional data) 0.06 0.09 0.07
S&F (best additional data) 0.09 0.11 0.10
This paper 0.21 0.08 0.12
Table 3: Results in terms of precision (P), recall (R) and
F1 score (F) for identifying and linking implicit argu-
ments in the SemEval test set.
influencing factors for their high recall are probably
(1) their improved method for identifying (resolv-
able) implicit arguments, and (2) their addition of
lexicalized and ontological features.
Comparison to the original results reported by
S&F, whose system we use, shows that our addi-
tional data improves precision (from 6% to 21%)
and F1 score (from 7% to 12%). The loss in recall
is marginal (-1%) given the size of the test set (259
resolvable cases in total). The result in precision is
the second highest score reported on this task. Inter-
estingly, the improvements are higher than those of
the best training set used in the original study by Sil-
berer and Frank (2012), even though their additional
data set is three times bigger than ours and is based
on manual semantic annotations. We conjecture that
their low gain in precision could be a side effect trig-
gered by two factors: on the one hand, their model
crucially relies on coreference chains, which are au-
tomatically generated for the test set and hence are
rather noisy. On the other hand, their heuristically
created training data might not represent implicit ar-
gument instances adequately.
310
5 Experiment 2: Implicit arguments in
coherence modeling
In our second experiment, we examine the effect of
implicit arguments on local coherence, i.e., the ques-
tion of how well a local argument (non-)realization
fits into a given context. We approach this question
as follows: first, we assemble a data set of document
pairs that differ only with respect to a single realiza-
tion decision (Section 5.1). Given each pair in this
data set, we ask human annotators to indicate their
preference for the implicit or explicit argument re-
alization in the pre-specified context (Section 5.2).
Second, we attempt to emulate the decision pro-
cess computationally using a discriminative model
based on discourse and entity-specific features (Sec-
tion 5.3).
5.1 Data compilation
We use the induced data set (henceforth source
data), as described in Section 3, as a starting point
for composing a set of document pairs that involve
implicit and explicit arguments. To make sure that
each document pair in this data set only differs with
respect to a single realization decision, we first cre-
ate two copies of each document from the source
data: one copy remains in its original form, and the
other copy will be modified with respect to a sin-
gle argument realization. Example (2) illustrates an
example of an original and modified (marked by an
asterik) sentence:
(2) [The Dalai Lama?sA0] visit [to FranceA1] ends
on Tuesday.
* [The Dalai Lama?sA0] visit ends on Tuesday.
Note that adding and removing arguments at ran-
dom can lead to structures that are semantically
implausible. Hence, we restrict this procedure to
predicate-argument structures (PAS) that actually
occur and are aligned across two texts, and create
modifications by replacing a single argument posi-
tion in one text with the corresponding argument po-
sition in the comparable text. Examples (2) and (3)
5Results as reported in Tonelli and Delmonte (2011)
6Results computed as an average over the scores given for
both test files; rounded towards the number given for the test
file that contained more instances.
show two such comparable texts. The original PAS
in Example (2) contains an explicit argument that is
implicit in the aligned PAS and hence removed in
the modified version. Vice versa, the original text
in (3) involves an implicit argument, which is made
explicit in the modified version.
(3) [The Dalai Lama?sA0] visit coincides with the
Beijing Olympics.
* [The Dalai Lama?sA0] visit [to FranceA1] co-
incides with the Beijing Olympics.
We ensure that the modified structure fits into
the given context grammatically by only consid-
ering PAS with identical predicate form and con-
stituent order. We found that this restriction con-
strains affected arguments to be modifiers, prepo-
sitional phrases and direct objects. We argue that
this is actually a desirable property because more
complicated alternations could affect coherence by
themselves; resulting interplays would make it diffi-
cult to distinguish between the isolated effect of ar-
gument realization itself and other effects, triggered
for example by sentence order (Gordon et al, 1993).
5.2 Annotation
We set up a web experiment using the NLTK pack-
age (Belz and Kow, 2011) to collect (local) coher-
ence ratings for implicit and explicit arguments. For
this experiment, we compiled a data set of 150 doc-
ument pairs. As described in Section 5.1, each text
pair consists of mostly the same text, with the only
difference being one argument realization.
We presented all 150 pairs to two annotators7 and
asked them to indicate their preference for one al-
ternative over the other using a continuous slider
scale. The annotators got to see the full texts, with
the alternatives presented next to each other. To
make texts easier to read and differences easier to
spot, we collapsed all identical sentences into one
column and highlighted the aligned predicate (in
both texts) and the affected argument (in the explicit
case). An example is shown in Figure 2. To avoid
any bias in the annotation process, we shuffled the
sequence of text pairs and randomly assigned the
side of display (left/right) of each realization type
7Both annotators are undergraduate students in Computa-
tional Linguistics.
311
Figure 2: Texts as displayed to the annotators.
(explicit/implicit). Note that instead of providing a
definition of local coherence ourselves, we simply
asked the annotators to rate how ?natural? a realiza-
tion sounds given the discourse context.
We found that annotators made use of the full rat-
ing scale, which spans from -50 to +50, with the ex-
tremes indicating either a strong preference for the
text on the left hand side or the right hand side, re-
spectively. Most ratings are, however, concentrated
more towards the center of the scale (i.e., around
zero). This seems to imply that the use of im-
plicit or explicit arguments did not make a consid-
erable difference most of the time. The first author
confirmed this assumption and resolved disagree-
ments between annotators in several group discus-
sions. The annotators also affirmed that some cases
do not read naturally when a specific argument is or
is not realized at a given position in discourse. Ex-
amples (4) and (5) illustrate two cases, in which a
redundant argument is realized (A4, or destination)
or a coherence establishing argument has been omit-
ted (A2, or co-signer).8
(4) ? The remaining contraband was picked up at
Le Havre. The containers had arrived [in
Le Havre] from China.
(5) ? Lt.-Gen. Mohamed Lamari (. . . ) denied
his country wanted South African weapons
to fight Muslim rebels fighting the govern-
ment. ?We are not going to fight a flea with
8Note that both examples are only excerpts from the affected
texts. The annotators got to see the full context.
a hammer,? Lamari told reporters after sign-
ing the agreement of intent [?].
Following discussions with the annotators, we
discarded all items from the final data set, for which
no clear preference could be established (72%) or
the annotators had different preferences (9%). We
mapped all remaining items into two classes accord-
ing to whether the affected argument had to be im-
plicit (9 texts) or explicit (20 texts). All 29 uniquely
classified texts are used as a small gold standard test
set for evaluation.
5.3 Coherence model
We model the decision process that underlies the
(non-)realization of arguments using a SVM classi-
fier and a range of discourse features. The features
can be classified into three groups: features specific
to the affected predicate-argument structure (Parg),
the (automatic) coreference chain of the affected ar-
gument (Coref), and the discourse context (Disc).
Parg includes the absolute and relative number of
realized arguments; the number of modifiers in the
PAS; and the total length (in words) of the PAS and
the complete sentence.
Coref includes the number of previous/follow-up
mentions in a fixed sentence window; the distance
(in number of words/sentences) to the previous/next
mention; the distribution of occurrences over the
previous/succeeding two sentences;9 and the POS of
previous/follow-up mentions.
Disc includes the total number of coreference
chains in the text; the occurrence of pronouns
in the current sentence; lexical repetitions in the
previous/follow-up sentence; the current position in
discourse (begin, middle, end); and a feature indi-
cating whether the affected argument occured in the
first sentence.
Note that most of these features overlap with
those successfully applied in previous work. For
example, Pitler and Nenkova (2008) also use text
9This type of feature is very similar to the transition pat-
terns in the original entity grid. The only difference is that our
features are not typed with respect to the grammatical function
of explicit realizations. The reason for skipping this informa-
tion lies in the insignificant amount of relevant samples in our
(noisy) training data.
312
length, sentence-to-sentence transitions, word over-
lap and pronoun occurrences as features for predict-
ing readability. Our own contribution lies in the defi-
nition of PAS-specific features and the adaptation of
all features to the task of predicting (non-)realization
of arguments in a predicate-argument structure.
5.4 Training data
We do not make use of any manually annotated data
for training. Instead, our model relies solely on the
automatically induced source data, described in Sec-
tion 3, for learning. We prepare this data set as fol-
lows: first, we remove all data points that also occur
in the test set. Second, we split all pairs of texts into
two groups ? texts that contain a predicate-argument
structure in which an implicit argument has been
identified (IA), and their comparable counterparts
that contain the aligned PAS with an explicit argu-
ment (EA). All texts are labelled according to their
group. For all texts in group EA, we remove the ex-
plicit argument from the aligned PAS. This way, the
feature extractor always gets to see the text and au-
tomatic annotations as if the realization decision had
not been performed and can thus extract unbiased
feature values for the affected entity and argument
position.
5.5 Evaluation setting
The goal of this task is to correctly predict the re-
alization type (implicit or explicit) of an argument
that maximizes the coherence of the document. As
a proxy for coherence, we use the naturalness rat-
ings given by our annotators. We evaluate classifica-
tion performance on the part of our test set for which
clear preferences have been established. We report
results in terms of precision, recall and F1 score. We
compute precision as the fraction of correct classifier
decisions divided by the total number of classifica-
tions; and recall as the fraction of correct classifier
decisions divided by the total number of test items.
Note that precision and recall are identical when the
model provides a class label for every test item. We
compute F1 as the harmonic mean between precision
and recall.
For comparison with previous work, we further
apply a couple of previously proposed local co-
herence models: the original entity grid model by
Barzilay and Lapata (2005), a modified version that
uses topic models (Elsner and Charniak, 2011a) and
an extended version that includes entity-specific fea-
tures (Elsner and Charniak, 2011b). We further ap-
ply the discourse-new model by Elsner and Charniak
(2008) and the pronoun-based model by Charniak
and Elsner (2009). For all of the aforementioned
models, we use their respective implementation pro-
vided with the Brown Coherence Toolkit10. Note
that the toolkit only returns one coherence score for
each document. To use the toolkit for argument clas-
sification, we use two documents per data point ?
one that contains the affected argument explicitly
and one that does not (implicit argument) ? and treat
the higher scoring variant as classification output. If
both documents achieve the same score, we neither
count the test item as correctly nor as incorrectly
classified. In contrast, we apply our own model only
on the document that contains the implicit argument,
and use the classifier to predict whether this realiza-
tion type fits into the given context or not. Note that
our model has an advantage here because it is specif-
ically designed for this task. Yet, all models com-
pute local coherence ratings based on entity occur-
rences and should thus be able to predict which re-
alization type coheres best with the given discourse
context.11
5.6 Results
The results are summarized in Table 4. As all mod-
els provided class labels for almost all test instances,
we focus our discussion on F1 scores. The majority
class in our test set is the explicit realization type,
making up 20 of the 29 test items (69%).
The original entity grid model produced differing
scores for the two realization types only in 26 cases.
The model exhibits a strong preference for the im-
plicit realization type: it predicts this class in 22
cases, resulting in an F1 score of only 15%. Tak-
ing a closer look at the features of the model reveals
that this an expected outcome: in its original set-
ting, the entity grid learns realization patterns in the
form of sentence-to-sentence transitions. Most enti-
ties are, however, only mentioned a few times in a
10cf. http://www.ling.ohio-state.edu/%7Emelsner/
11Recall that input document pairs are identical except for the
affected argument position. Consequently, the resulting coher-
ence scores only differ with respect to affected entity realiza-
tions.
313
P R F
Entity grid models ? ? ?
Baseline entity grid 0.15** 0.14** 0.15**
Extended entity grid 0.19** 0.17** 0.18**
Topical entity grid 0.34** 0.34** 0.34**
Other models ? ? ?
Pronouns 0.43** 0.34** 0.38**
Discourse-newness 0.48** 0.48** 0.48**
This paper ? ? ?
Our (full) model 0.90 0.90 0.90
Simplified model 0.83 0.83 0.83
Majority class 0.69* 0.69* 0.69*
Table 4: Results in terms of precision (P), recall (R) and
F1 score for correctly predicting argument realization; re-
sults that significantly differ from our (full) model are
marked with asterisks (* p<0.1; ** p<0.01)
text, which means that non-realizations make up the
?most probable? class ? independently of whether
they are relevant in a given context or not. The mod-
els by Charniak and Elsner (2009) and Elsner and
Charniak (2011a), which are not based on an entity
grid, do not suffer from this effect and achieve bet-
ter results, with F1 scores of 38% and 48%, respec-
tively. The topical and entity-specific refinements to
the entity grid model also alleviate the bias towards
non-realizations, resulting in improved F1 scores of
18% and 34%, respectively.
To counter-balance this issue altogether, we train
a simplified version of our own model that only
uses features that involve occurrence patterns. The
main difference between this simplified model and
the original entity grid model lies in the different
use of training data: while entity grid models treat
all non-realized items equally, our model gets to
?see? actual examples of entities that are implicit.
In other words, our simplified model takes into ac-
count implicit mentions of entities, not only explicit
ones. The results show that this extra information
has a significant (p<0.01, using a randomization test
(Yeh, 2000)) impact on test set performance, basi-
cally raising F1 from 15% to 83%. Using all features
of our model further increases F1 score to 90%, the
highest score achieved overall.
The highest weighted features in our model in-
clude all three feature groups: for example, the
number of coreferent mentions within the preceed-
ing/following two sentences (Coref), the number
of words already realized in the affected predicate-
argument structure (Parg), and the total number of
coreference chains in the document (Disc).
6 Conclusions
In this paper, we presented a novel approach to ac-
curately induce implicit arguments and discourse an-
tecedents from comparable texts (cf. Section 3). We
demonstrated the benefit of this kind of data for link-
ing implicit arguments and modeling local coher-
ence. Our experiments revealed three particularly
interesting results.
Firstly, a small data set of (automatically induced)
implicit arguments can have a greater impact on ar-
gument linking models than a bigger data set of ar-
tificially created instances (cf. Section 4). Secondly,
the use of implicit vs. explicit arguments, while be-
ing a subtle difference in most contexts, can have a
clear impact on text ratings. Thirdly, our automat-
ically created training data enables models to learn
features that considerably improve prediction of lo-
cally coherent argument realizations (cf. Section 5).
For the task of implicit argument linking, more
training data will be needed to further advance
the state-of-the-art. Our method for inducing
this kind of data, by exploiting aligned predicate-
argument structures from comparable texts, has
shown promising results. Future work will have
to explore this direction more fully, for example,
by identifying ways to induce data with higher re-
call. Integrating argument (non-)realization into a
full model of local coherence also remains part of
future work. In this paper, we presented a suitable
basis for such work: a training set that contains em-
pirical data on implicit arguments in discourse; and
a feature set that models argument realization with
high accuracy.
Acknowledgments
We are grateful to the Landesgraduiertenfo?rderung
Baden-Wu?rttemberg for funding within the research
initiative ?Coherence in language processing? at
Heidelberg University. We thank our annotators and
four anonymous reviewers.
314
References
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, Ann Arbor, Michi-
gan, USA, 25?30 June 2005, pages 141?148.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Anja Belz and Eric Kow. 2011. Discrete vs. continuous
rating scales for language evaluation in nlp. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 230?235, Portland, Oregon, USA,
June.
Anders Bjo?rkelund, Bernd Bohnet, Love Hafdell, and
Pierre Nugues. 2010. A high-performance syntac-
tic and semantic dependency parser. In Coling 2010:
Demonstration Volume, pages 33?36, Beijing, China,
August. Coling 2010 Organizing Committee.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 89?97, Beijing, China,
August.
Cheryl Brown. 1983. Topic continuity in written english
narrative. In Talmy Givon, editor, Topic Continuity
in Discourse: A Quantitative Cross-Language Study.
John Benjamins, Amsterdam, The Netherlands.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 148?156, Athens, Greece,
March.
Desai Chen, Nathan Schneider, Dipanjan Das, and
Noah A. Smith. 2010. SEMAFOR: Frame argument
resolution with log-linear models. In Proceedings of
the 5th International Workshop on Semantic Evalua-
tion, pages 264?267, Uppsala, Sweden, July.
Micha Elsner and Eugene Charniak. 2008. Coreference-
inspired coherence modeling. In Proceedings of ACL-
08: HLT, Short Papers, pages 41?44, Columbus, Ohio,
June.
Micha Elsner and Eugene Charniak. 2011a. Disentan-
gling chat with local coherence models. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1179?1189, Portland, Oregon, USA,
June.
Micha Elsner and Eugene Charniak. 2011b. Extending
the entity grid with entity-specific features. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 125?129, Portland, Oregon, USA,
June.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, Mas-
sachusetts, USA.
Katja Filippova and Michael Strube. 2007. Extending
the entity-grid coherence model to semantically re-
lated entities. In Proceedings of the 11th European
Workshop on Natural Language Generation, Schloss
Dagstuhl, Germany, 17?20 June 2007, pages 139?142.
C. J. Fillmore. 1986. Pragmatically controlled zero
anaphora. In Proceedings of the twelfth annual meet-
ing of the Berkeley Linguistics Society, pages 95?107.
Matthew Gerber and Joyce Chai. 2012. Semantic Role
Labeling of Implicit Arguments for Nominal Predi-
cates. Computational Linguistics, 38(4):755?798.
Peter C. Gordon, Barbara J. Grosz, and Laura A. Gilliom.
1993. Pronouns, names, and the centering of attention
in discourse. Cognitive Science, 17:311?347.
Philip Gorinski, Josef Ruppenhofer, and Caroline
Sporleder. 2013. Towards weakly supervised resolu-
tion of null instantiations. In Proceedings of the 10th
International Conference on Computational Semantics
(IWCS 2013) ? Long Papers, pages 119?130, Potsdam,
Germany, March.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Aravind K. Joshi and Steve Kuhn. 1979. Centered logic:
The role of entity centered sentence representation in
natural language inferencing. In Proceedings of the
6th International Joint Conference on Artificial Intel-
ligence, Tokyo, Japan, August, pages 435?439.
Egoitz Laparra and German Rigau. 2012. Exploiting ex-
plicit annotations and semantic types for implicit argu-
ment resolution. In Proceedings of the Sixth IEEE In-
ternational Conference on Semantic Computing (ICSC
2010), pages 75?78, Palermo, Italy, September. IEEE
Computer Society.
Egoitz Laparra and German Rigau. 2013. Sources of ev-
idence for implicit argument resolution. In Proceed-
ings of the 10th International Conference on Compu-
tational Semantics (IWCS 2013) ? Long Papers, pages
155?166, Potsdam, Germany, March.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
489?500, Jeju Island, Korea, July.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
315
Deterministic coreference resolution based on entity-
centric, precision-ranked rules. Computational Lin-
guistics, 39(4). Accepted for publication.
Annie Louis and Ani Nenkova. 2010. Creating local
coherence: An empirical assessment. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 313?316, Los An-
geles, California, June.
Sebastian Martschat, Jie Cai, Samuel Broscheit, E?va
Mu?jdricza-Maydt, and Michael Strube. 2012. A
multigraph model for coreference resolution. In Joint
Conference on EMNLP and CoNLL - Shared Task,
pages 100?106, Jeju Island, Korea, July.
Neil McIntyre and Mirella Lapata. 2009. Learning to
tell tales: A data-driven approach to story generation.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the Association for Computational
Linguistics and the 4th International Joint Conference
on Natural Language Processing, Singapore, 2?7 Au-
gust 2009, pages 217?225.
Adam Meyers, Ruth Reeves, and Catherine Macleod.
2008. NomBank v1.0. Linguistic Data Consortium,
Philadelphia.
Tatjana Moor, Michael Roth, and Anette Frank. 2013.
Predicate-specific annotations for implicit role bind-
ing: Corpus annotation, data analysis and evaluation
experiments. In Proceedings of the 10th International
Conference on Computational Semantics (IWCS 2013)
? Short Papers, pages 369?375, Potsdam, Germany,
March.
Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiff-
man, Lynette Hirschman, Marcia Linebarger, and John
Dowding. 1986. Recovering implicit information. In
Proceedings of the 24th Annual Meeting of the Associ-
ation for Computational Linguistics, New York, N.Y.,
10?13 June 1986, pages 10?19.
Martha Palmer, Daniel Gildea, and Nianwen Xue. 2010.
Synthesis Lectures on Human Language Technolo-
gies. Morgan & Claypool.
Robert Parker, David Graff, Jumbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion. Linguistic Data Consortium, Philadelphia.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text qual-
ity. In Proceedings of the 2008 Conference on Empir-
ical Methods in Natural Language Processing, pages
186?195, Honolulu, Hawaii, October.
Michael Roth and Anette Frank. 2012a. Aligning pred-
icate argument structures in monolingual comparable
texts: A new corpus for a new task. In Proceedings
of the First Joint Conference on Lexical and Computa-
tional Semantics, pages 218?227, Montreal, Canada,
June.
Michael Roth and Anette Frank. 2012b. Aligning
predicates across monolingual comparable texts us-
ing graph-based clustering. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 171?182, Jeju Island, Ko-
rea, July.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. SemEval-
2010 Task 10: Linking Events and Their Participants
in Discourse. In Proceedings of the 5th International
Workshop on Semantic Evaluations, pages 45?50, Up-
psala, Sweden, July.
Ivan A. Sag and Jorge Hankamer. 1984. Towards a The-
ory of Anaphoric Processing. Linguistics and Philos-
ophy, 7:325?345.
Candace L. Sidner. 1979. Towards a computational the-
ory of definite anaphora comprehension in English.
Technical Report AI-Memo 537, Massachusetts Insti-
tute of Technology, AI Lab, Cambridge, Mass.
Carina Silberer and Anette Frank. 2012. Casting implicit
role linking as an anaphora resolution task. In Pro-
ceedings of the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012), pages 1?10,
Montre?al, Canada, 7-8 June.
Michael K. Tanenhaus and Greg N. Carlson. 1990. Com-
prehension of Deep and Surface Verbphrase Anaphors.
Language and Cognitive Processes, 5(4):257?280.
Sara Tonelli and Rodolfo Delmonte. 2010. VENSES++:
Adapting a deep semantic processing system to the
identification of null instantiations. In Proceedings of
the 5th International Workshop on Semantic Evalua-
tion, pages 296?299, Uppsala, Sweden, July.
Sara Tonelli and Rodolfo Delmonte. 2011. Desperately
seeking implicit arguments in text. In Proceedings of
the ACL 2011 Workshop on Relational Models of Se-
mantics, pages 54?62, Portland, Oregon, USA, June.
Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-
uard Hovy, Sameer Pradhan, Lance Ramshaw, Nian-
wen Xue, Ann Taylor, Jeff Kaufman, Michelle Fran-
chini, Mohammed El-Bachouti, Robert Belvin, and
Ann Houston. 2011. OntoNotes Release 4.0. Lin-
guistic Data Consortium, Philadelphia.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, San Francisco, California, USA, 2nd
edition.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Computa-
tional Linguistics, pages 947?953, Saarbru?cken, Ger-
many, August.
316
A Resource-Poor Approach for
Linking Ontology Classes to
Wikipedia Articles
Nils Reiter
Matthias Hartung
Anette Frank
University of Heidelberg (Germany)
email: reiter@cl.uni-heidelberg.de
Abstract
The applicability of ontologies for natural language processing depends
on the ability to link ontological concepts and relations to their realisa-
tions in texts. We present a general, resource-poor account to create such
a linking automatically by extracting Wikipedia articles corresponding to
ontology classes. We evaluate our approach in an experiment with the
Music Ontology. We consider linking as a promising starting point for
subsequent steps of information extraction.
381
382 Reiter, Hartung, and Frank
1 Introduction
Ontologies are becoming increasingly popular as a means for formal, machine-read-
able modelling of domain knowledge, in terms of concepts and relations. Linking
ontological concepts and relations to their natural language equivalents is of utmost
importance for ontology-based applications in natural language processing. Providing
larger quantities of text that clearly belongs to a given ontological concept is a pre-
requisite for further steps towards ontology population with relations and instances.
We thus consider this work as a point of departure for future work on populating and
lexicalizing ontologies, and their use in semantic processing.
In this paper we present a method that provides relevant textual sources for a do-
main ontology by linking ontological classes to the most appropriate Wikipedia arti-
cles describing the respective ontological class. The paper is structured as follows:
We discuss related work in Section 2. Section 3 presents our method for linking on-
tology classes to Wikipedia articles. The method is implemented and tested using the
music ontology (Raimond et al, 2007) and a Wikipedia dump of 2007. We present
this experiment in Section 4 and its evaluation in Section 5. Section 6 concludes and
gives an outlook on directions of future work.
2 Related Work
Our goal is to detect the most appropriateWikipedia article for a given ontology class.
As Wikipedia is a domain-independent resource, it usually contains many more senses
for one concept name than does a domain-specific ontology. Thus, one of the chal-
lenges we meet is the need for disambiguation between multiple candidate articles
with respect to one specific ontology class.1 Therefore, we compare our approach to
previous work on sense disambiguation. Since in our approach, we aim at minimiz-
ing the degree of language- and resource dependency, our focus is on the amount of
external knowledge used.
One method towards sense disambiguation that has been studied is to use different
kinds of text overlap: Ruiz-Casado et al (2005) calculate vector similarity between a
Wikipedia article and WordNet glosses based on term frequencies. Obviously, such
glosses are not available for all languages, domains and applications. Wu and Weld
(2007) and Cucerzan (2007) calculate the overlap between contexts of named entities
and candidate articles from Wikipedia, using overlap ratios or similarity scores in a
vector space model, respectively. Both approaches disambiguate named entities using
textual context. Since our aim is to acquire concept-related text sources, these methods
are not applicable.
A general corpus-based approach has been proposed by Reiter and Buitelaar (2008):
Using a domain corpus and a domain-independent reference corpus, they select the
article with the highest domain relevance score among multiple candidates. This ap-
proach works reasonably well but relies on the availability of domain-specific corpora
and fails at selecting the appropriate among multiple in-domain senses. In contrast,
our resource-poor approach does not rely on additional textual resources, as ontologies
usually do not contain contexts for classes.
1Mihalcea (2007) shows that Wikipedia can indeed be used as a sense inventory for sense
disambiguation.
A Resource-Poor Approach for Linking Ontology Classes to Wikipedia 383
3 Linking Ontology classes to Wikipedia articles
This section briefly reviews relevant information about Wikipedia and describes our
method for linking ontology classes to Wikipedia articles. Our algorithm consists of
two steps: (i) extracting candidate articles from Wikipedia and (ii) selecting the most
appropriate one. The algorithm is independent of the choice of a specific ontology.2
3.1 Wikipedia
The online encyclopedia Wikipedia currently comprises more than 2,382,000 articles
in about 250 languages. Wikipedia is interesting for our approach because it is semi-
structured and articles usually talk about one specific topic.
The structural elements in Wikipedia that we rely on are links between articles,
inter-language links, disambiguation and redirect pages. Inter-language links refer to
an article about the same topic in a different language. Disambiguation pages collect
the different senses of a given term. Redirect pages point to other pages, allowing for
spelling variations, abbreviations and synonyms.
3.2 Extracting the candidate articles
The first step of our algorithm is to extract candidate articles for ontology classes. The
method we employ is based on Reiter and Buitelaar (2008). The algorithm starts with
the English label LC of an ontology class C, and tries to retrieve the article that bears
the same title.3 Any Wikipedia page P retrieved by this approach falls into one of
three categories:
1. P is an article: The template {{otheruses}} in the article indicates that a
disambiguation page exists which lists further candidate articles for C. The
disambiguation page is then retrieved and we proceed with step 2. Otherwise,
P is considered to be the only article for C.
2. P is a disambiguation page: The algorithm extracts all links on P and considers
every linked page as a candidate article.4
3. P is a redirect page: The redirect is being followed and the algorithm checks
the different cases once again.
3.3 Features for the classifier
We now discuss the features we apply to disambiguate candidate articles retrieved by
our candidate extraction method with regard to the respective ontology class. Some
features use structural properties of both Wikipedia and the ontology, others are based
on shallow linguistic processing.
2It is still dependent on the language used for coding ontological concepts (here English). In future
work we aim at bridging between languages using Wikipedia?s inter-language links or other multi-lingual
resources.
3We use common heuristics to cope with CamelCase, underscore whitespace alternation etc.
4Note that, apart from pointing to different readings of a term, disambiguation pages sometimes include
pages that are clearly not a sense of the given term. Distinguishing these from true/appropriate readings of
the term is not trivial.
384 Reiter, Hartung, and Frank
Domain relevance
Wikipedia articles can be classified according to their domain relevance by computing
the proportion of domain terms they contain. In this paper, we explore several variants
of matching a set of domain terms against the article in question:
Class labels. The labels of all concepts in the ontology are used as a set of domain
terms.
? We extract the nouns from the POS-tagged candidate article. The relative fre-
quency of domain terms is then computed for the complete article and for nouns
only, both for types and for tokens.
? We compute the frequency of domain terms in the first paragraph only, assuming
it contains domain relevant key terms.
? The redirects pointing to the article in question, i.e., spelling variations and
synonyms, are extracted. We then compute their relative frequency in the set of
class labels.
Comments. As most ontologies contain natural language comments for classes, we
use them to retrieve domain terms. All class comments extracted from the ontology are
POS-tagged. We use all nouns as domain terms and compute their relative frequencies
in the article.
Class vs. Instance
It is intuitively clear that a class in the ontology needs to be linked to a Wikipedia
article representing a class rather than an instance.5 We extract the following features
in order to detect whether an article represents a class or an instance, thus being able
to reject certain articles as inappropriate link targets for a particular class.
Translation distance. Instances inWikipedia are usually named entities (NEs). Thus,
the distinction between concepts and instances can, to a great extent, be rephrased as
the problem of NE detection. As our intention is to develop a linking algorithm which
is, in principle, language-independent, we decided to rely on the inter-language links
provided by Wikipedia. The basic idea is that NEs are very similar across different
languages (at least in languages using the same script), while concepts show a greater
variation in their surface forms across different languages. Thus, for the inter-language
links on the article in question that use latin script, we compute the average string sim-
ilarity in terms of Levenshtein Distance (Levenshtein, 1966) between the title of the
page and its translations.
Templates. Wikipedia offers a number of structural elements that might be useful
in order to distinguish instances from concepts. In particular, the infobox template
is used to express structured information about instances of a certain type and some
of their properties. Thus, we consider articles containing an infobox template to
correspond to an instance.
5We are aware of the fact that the distinction between classes and instances is problematic on both sides:
Ontologies described in OWL Full or RDF do not distinguish clearly between classes and instances and
Wikipedia does not provide an explicit distinction either.
A Resource-Poor Approach for Linking Ontology Classes to Wikipedia 385
4 Experiment
4.1 The Music Ontology
We test our approach on the Music Ontology (MO) (Raimond et al, 2007). The MO
has been developed for the annotation of musical entities on the web and provides
capabilities to encode data about artists, their albums, tracks on albums and the process
of creating musical items.
The ontology defines 53 classes and 129 musical properties (e.g. melody) in its
namespace, 78 external classes are referenced. Most of the classes are annotated with
comments in natural language. TheMO is connected to several other ontologies (W3C
time6, timeline7, event8, FOAF9), making it an interesting resource for domain rele-
vant IE tasks and generalisation of the presented techniques to further domains. The
MO is defined in RDF and freely available10.
4.2 Experimental Setup
The experiment is divided into two steps: candidate page selection and classification
(see Section 3). For candidate selection we extract Wikipedia pages with titles that
are near-string identical to the 53 class labels. 28 of them are disambiguation pages.
From these pages, we extract the links and use them as candidates. The remaining 25
are directly linked to a single Wikipedia article.
To test our classification features, we divide the overall set of ontology classes in
training and test sets of 43 and 10 classes, respectively, that need to be associated
with their most appropriate candidate article. We restrict the linking to one most
appropriate article. For the classification step, we extract the features discussed in
Section 3.
Since the candidate set of pages shows a heavily skewed distribution in favour of
negative instances, we generate an additional training set by random oversampling
(Batista et al, 2004) in order to yield training data with a more uniform distribution
of positive and negative instances.
5 Evaluation
For evaluation, the ambiguous concepts in the ontology have been manually linked
to Wikipedia articles. The linking was carried out independently by three annotators,
all of them computational linguists. Each annotator was presented the class label,
its comment as provided by the ontology and the super class from which the class
inherits. On the Wikipedia side, all pages found by our candidate extraction method
were presented to the annotators.
The inter-annotator agreement is ? = 0.68 (Fleiss? Kappa). For eight concepts, all
three annotators agreed that none of the candidate articles is appropriate and for ten all
three agreed on the same article. These figures underline the difficulty of the problem,
as the information contained in domain ontologies and Wikipedia varies substantially
with respect to granularity and structure.
6www.w3.org/TR/owl-time/
7motools.sourceforge.net/timeline/timeline.html
8motools.sourceforge.net/event/event.html
9xmlns.com/foaf/spec/
10musicontology.com
386 Reiter, Hartung, and Frank
Candidate article selection. Candidate selection yields 16 candidate articles per
concept on average. These articles contain 1567 tokens on average. The minimal
and maximal number of articles per concepts are 3 and 38, respectively.
Candidate article classification. We train a decision tree11 using both the original
and the oversampled training sets as explained above.
Table 1: Results after training on original and over-sampled data
Positives Negatives Average
orig. samp. orig. samp. orig. samp.
P 1 0.63 0.87 0.97 0.94 0.80
R 0.17 0.83 1 0.91 0.58 0.87
F 0.27 0.71 0.93 0.94 0.75 0.83
Table 1 displays precision, recall and f-score results for positive and negative in-
stances as well as their average. As the data shows, oversampling can increase the
performance considerably. We suspect this to be caused not only by the larger training
set, but primarily by the more uniform distribution.
The table shows further that the negative instances can be classified reliably us-
ing the original or oversampled data set. However, as we intend to select positive
appropriate Wikipedia articles rather than to deselect inappropriate ones, we are par-
ticularly interested in good performance for the positive instances. We observe that
this approach identifies positive instances (i.e., appropriate Wikipedia articles) with a
reasonable performancewhen using the oversampled training set. It is noteworthy that
not a single feature performs better than with an f-measure of 0.6 when used alone.
The figures shown in Table 1 are obtained using the combination of all features.
Table 2: Results for combination of best features only
Positives Negatives
P 0.60 1.00
R 1.00 0.88
F 0.75 0.94
In Table 2, we present the results for the best performing features taken together
(using oversampling on the training set): nountypes-classlabels (F-measure: 0.6),
langlinks (0.5), redirects-classlabels (0.5), nountokens-classlabels (0.44),
fulltextclasslabels (0.44). Recall improves considerably, while there is a small
decrease in precision.
6 Conclusions
We have presented ongoing research on linking ontology classes to appropriate Wiki-
pedia articles. We consider this task a necessary step towards automatic ontology
lexicalization and population from texts.
11We used the ADTree implementation in the Weka toolkit www.cs.waikato.ac.nz/ml/weka/.
A Resource-Poor Approach for Linking Ontology Classes to Wikipedia 387
The crucial challenge in this task is to deal with the high degree of ambiguity that
is introduced by the fact that Wikipedia covers a large amount of fine-grained infor-
mation for numerous domains. This leads to a great number of potential candidate
articles for a given ontology class.
Our approach to this problem is independent of the particular ontology that is used
as a starting point. Moreover, it merely depends on a set of rather shallow but effec-
tive features which can be easily extracted from the domain ontology and Wikipedia,
respectively. From the results we derived in our experiments with the Music Ontol-
ogy, we conclude that our approach is feasible and yields reasonable results even for
small domain ontologies, provided we can overcome highly skewed distributions of
the training examples due to an overwhelming majority of negative instances. In fu-
ture work we will apply the methods described here to different domain ontologies and
use the selectedWikipedia articles as a starting point for extracting instances, relations
and attributes.
Acknowledgements. We kindly thank our annotators for their effort and R?diger
Wolf for technical support.
References
Batista, G., R. Prati, and M. C. Monard (2004). A Study of the Behavior of Several
Methods for BalancingMachine Learning Training Data. SIGKDD Explorations 6,
20?29.
Cucerzan, S. (2007). Large-Scale Named Entity Disambiguation Based on Wikipedia
Data. In Proc. of EMNLP, Prague.
Levenshtein, V. I. (1966). Binary codes capable of correcting deletions, insertions,
and reversals. Soviet Physics Doklady 10, 707?710.
Mihalcea, R. (2007). Using Wikipedia for Automatic Word Sense Disambiguation.
In Proc. of NAACL-07, Rochester, New York, pp. 196?203.
Raimond, Y., S. Abdallah, M. Sandler, and F. Giasson (2007). The Music Ontol-
ogy. In Proc. of the 8th International Conference on Music Information Retrieval,
Vienna, Austria.
Reiter, N. and P. Buitelaar (2008). Lexical Enrichment of Biomedical Ontologies. In
Information Retrieval in Biomedicine: Natural Language Processing for Knowl-
edge Integration. IGI Global, to appear.
Ruiz-Casado, M., E. Alfonseca, and P. Castells (2005). Automatic Assignment of
Wikipedia Encyclopedic Entries to WordNet Synsets. In Proc. of the 3rd Atlantic
Web Intelligence Conference, Volume 3528, Lodz, Poland, pp. 380?385.
Wu, F. and D. S. Weld (2007). Autonomously Semantifying Wikipedia. In Proc. of
the Conference on Information and Knowledge Management, Lisboa, Portugal.
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 93?101,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Assessing the Challenge of Fine-Grained
Named Entity Recognition and Classification
Asif Ekbal, Eva Sourjikova, Anette Frank and Simone Paolo Ponzetto
Department of Computational Linguistics
Heidelberg University, Germany
{ekbal,sourjikova,frank,ponzetto}@cl.uni-heidelberg.de
Abstract
Named Entity Recognition and Classi-
fication (NERC) is a well-studied NLP
task typically focused on coarse-grained
named entity (NE) classes. NERC for
more fine-grained semantic NE classes has
not been systematically studied. This pa-
per quantifies the difficulty of fine-grained
NERC (FG-NERC) when performed at
large scale on the people domain. We
apply unsupervised acquisition methods
to construct a gold standard dataset for
FG-NERC. This dataset is used to bench-
mark methods for classifying NEs at var-
ious levels of fine-grainedness using clas-
sical NERC techniques and global contex-
tual information inspired fromWord Sense
Disambiguation approaches. Our results
indicate high difficulty of the task and pro-
vide a ?strong? baseline for future research.
1 Introduction
Named Entity Recognition and Classification (cf.
Nadeau and Sekine (2007)) is a well-established
NLP task relevant for nearly all semantic process-
ing and information access applications. NERC
has been investigated using supervised (McCallum
and Li, 2003), unsupervised (Etzioni et al, 2005)
and semi-supervised (Pas?ca et al, 2006b) learning
methods. It has been investigated in multilingual
settings (Tjong Kim Sang, 2002; Tjong Kim Sang
and De Meulder, 2003) and special domains, e.g.
biomedicine (Ananiadou et al, 2004).
The classical NERC task is confined to coarse-
grained named entity (NE) classes established
in the MUC (MUC-7, 1998) or CoNLL (Tjong
Kim Sang, 2002) competitions, typically PERS,
LOC, ORG, MISC. While most recent work con-
centrates on feature engineering and robust statis-
tical models for various domains, few researchers
addressed the problem of recognizing and catego-
rizing fine-grained NE classes (such as biologist,
composer, or athlete) in an open-domain setting.
Fine-grained NERC is expected to be benefi-
cial for a wide spectrum of applications, includ-
ing Information Retrieval (Mandl and Womser-
Hacker, 2005), Information Extraction (Pas?ca et
al., 2006a) or Question-Answering (Pizzato et
al., 2006). However, manually compiling wide-
coverage gazetteers for fine-grained NE classes is
time-consuming and error-prone. Also, without an
extrinsic evaluation, it is difficult to define a priori
which classes are relevant for a particular domain
or task. Finally, prior research in FG-NERC is dif-
ficult to evaluate, due to the diversity of NE classes
and datasets used.
Accordingly, in the interest of a general ap-
proach, we address the challenge of capturing a
broad range of NE classes at various levels of con-
ceptual granularity. By turning FG-NERC into
a widely applicable task, applications are free to
choose relevant NE categories for specific needs.
Also, establishing a gold standard dataset for this
task enables comparative benchmarking of meth-
ods. However, the envisaged task is far from triv-
ial, given that the set of possible semantic classes
for a given NE comprises the full space of NE
classes, whereas descriptive nouns may be am-
biguous between a fixed set of meanings only.
The paper aims to establish a general frame-
work for FG-NERC by addressing two goals: (i)
we automatically build a gold standard dataset of
NE instances classified in context with fine-grain-
ed semantic class labels; (ii) we develop strong
baseline methods, to assess the aptness of standard
NLP approaches for this task. The two efforts are
strongly interleaved: a standardized dataset is not
only essential for (comparative) evaluation, but
also a prerequisite for classification approaches
based on supervised learning, the most successful
techniques for sequential labeling problems.
93
2 Related work
An early approach to FG-NERC is Alfonseca and
Manandhar (2002), who identify it as a problem
related to Word Sense Disambiguation (WSD).
They jointly address concept hierarchy learning
and instance classification using topic signatures,
yet the experiments are restricted to a small on-
tology of 9 classes. Similarly, Fleischman and
Hovy (2002) extend previous work from Fleis-
chman (2001) on locations and address the ac-
quisition of instances for 8 fine-grained person
classes. For supervised training they compile a
web corpus which is filtered using high-confident
classifications from an initial classifier trained on
seeds. Due to the limitations of their method to
create a good sample of training data, the perfor-
mance could not be generalized to held-out data.
Recent work takes the task of FG-NERC one
step further by (i) extending the number of classes,
(ii) relating them to reference concept hierar-
chies and (iii) exploring methods for building
training and evaluation data, or applying weakly
and unsupervised learning based on high-volume
data. Tanev and Magnini (2006) selected 10 NE-
subclasses of person and location using Word-
Net as a reference. Datasets were automatically
acquired and manually filtered. They compare
word and pattern-based supervised and a semi-
supervised approach based on syntactic features.
Giuliano & Gliozzo (2007, 2008) perform NE
classification against the People Ontology, an ex-
cerpt of the WordNet hierarchy, comprising 21
people classes populated with at least 40 instances.
Using minimally supervised lexical substitution
methods, they cast NE classification as an ontol-
ogy population task ? as opposed to recognition
and classification in context. In a similar setting,
Giuliano (2009) explores semi-supervised classifi-
cation of the People Ontology classes using latent
semantic kernels, comparing models built from
Wikipedia and from a news corpus. In a differ-
ent line of research Pas?ca (2007) and Pas?ca and
van Durme (2008) make use of query logs to ac-
quire NEs on a large scale. While Pas?ca (2007)
extracts NEs for 10 target classes, Pas?ca and van
Durme (2008) combine web query logs and web
documents to acquire both NE-concept pairs and
concept attributes using seeds.
But while these more recent approaches all of-
fer substantially novel contributions for many NE
acquisition subtasks, none of them addresses the
full task of FG-NERC, i.e., recognition and clas-
sification of NE tokens in context. Compared to
ontology population, focusing on types, classifica-
tion in raw texts needs to consider any token and
cannot rely on special contexts offering indicative
clues for class membership.
Bunescu and Pas?ca (2006) also perform dis-
ambiguation and classification of NEs in context,
yet in a different setup. Disambiguation is per-
formed into one of the known possible classes
for a NE, as determined from Wikipedia disam-
biguation pages. Contexts for training and testing
are acquired from Wikipedia pages, as opposed
to general text. Disambiguation is performed us-
ing vectors of co-occurring terms and a taxonomy-
based kernel that integrates word-category corre-
lations. Evaluation is performed on the task of
predicting, for a given NE in a Wikipedia page
context, the correct class from among its known
classes, including one experiment that included
10% of out-of-Wikipedia entities. The category
space was confined to People by occupation, with
8,202 subclasses. Classification considered 110
broad classes, 540 highly populated classes (w/o
out-of-Wikipedia entities), and 2,847 classes in-
cluding less populated ones. This setup is diffi-
cult to compare given the sense granularities em-
ployed and the special Wikipedia text genre. Even
though classification is performed in context, the
task does not evaluate recognition.
To summarize, the field has developed robust
methods for acquisition and fine-grained classifi-
cation of NEs on a large scale. But, the full task
of NE recognition and classification in context still
remains to be addressed for a wide-coverage, fine-
grained semantic class inventory that can serve as
a common benchmark for future research.
3 Fine-grained NERC on a large-scale
We present experiments that assess the difficulty
of open-domain FG-NERC pursued at a large
scale. We concentrate on instances and classes re-
ferring to people, since it is a well-studied domain
(see Section 2) and structured fine-grained infor-
mation can be readily applied to a well-defined
end-user task such as IR, cf. the Web People
Search task (Artiles et al, 2008). Our method
is general in that it requires only a (PoS tagged
and chunked) corpus and a reference taxonomy
to provide a concept hierarchy. Given a map-
ping between automatically extracted class labels
94
and concepts in a taxonomic resource, it can be
further extended to other domains, e.g. locations
or the biomedical domain by leveraging open-
domain taxonomies such as Yago (Suchanek et
al., 2008) or WikiTaxonomy (Ponzetto and Strube,
2007). The contribution of this work is two-fold:
(i) We develop an unsupervised method for ac-
quiring a comprehensive dataset for FG-NERC by
applying linguistically motivated patterns to a cor-
pus harvested from the Web (Section 4). Large
amounts of NEs are acquired together with their
contexts of occurrence and with their fine-grained
class labels which are mapped to synsets in Word-
Net. The controlled sense inventory and the tax-
onomic structure offered by WordNet enables an
evaluation of FG-NERC performance at different
levels of concept granularity, as given by the depth
at which the concepts are found. As our extraction
patterns reflect a wide-spread grammatical con-
struct, the method can be applied to many lan-
guages and extended to other domains.
(ii) Given this automatically acquired dataset,
we assess the problem of FG-NERC in a sys-
tematic series of experiments, exploring the per-
formance of NERC methods on different levels
of granularities. For recognition and classifica-
tion we apply standard sequential labeling tech-
niques ? i.e. a Maximum Entropy (MaxEnt) tag-
ger (Section 5.1) ? which we adapt to this hier-
archical classification problem (Section 5.2). To
test the hypothesis of whether a sequential la-
beler represents a valid choice to perform FG-
NERC, we compare the latter to a MaxEnt system
trained on a more semantically informed feature
set, and a gloss-overlap method inspired by WSD
approaches (Section 5.3).
4 Acquisition of a FG-NERC dataset
We present an unsupervised method that simul-
taneously acquires NEs, their semantic class and
contexts of occurrence from large textual re-
sources. In order to develop a clean resource of
properly disambiguated NEs, we develop acqui-
sition patterns for a grammatical construction that
unambiguously associates proper names with their
corresponding semantic class.
Pattern-based extraction of NE-concept pairs.
NEs are often introduced by so-called apposi-
tional structures as in (1), which overtly ex-
press which semantic class (here, painter) the NE
(Kandinsky) belongs to. Appositions involving
proper names can be captured by extraction pat-
terns as given in (2).
(1) . . . writings of the abstract painter Kandinsky
frequently explored similarities between . . .
(2) a. [the|The]? [JJ|NN]* [NN] [NP]
the abstract painter Kandinsky
b. [NP] [,]? [a|an|the]* [JJ|NN]* [NN]
W. Kandinsky, a Russian-born painter, ..
Contexts like (2.a) provide a less noisy se-
quence for extraction, due to the class and instance
labels being adjacent ? in contrast to (2.b) where
any number of modifiers can intervene between
the two. Accordingly, we apply in our experiments
only a restricted version of (2.a) ? with a deter-
miner ? to UKWAC, an English web-based cor-
pus (Baroni et al, 2009) that comes in a cleaned,
PoS-tagged and lemmatized form. Due to its size
(>2 billion tokens) and mixed genres, the corpus
is ideally suited for acquiring large quantities of
NEs pertaining to a broad variety of open-domain
semantic classes.
Filtering heuristics. The apposition patterns are
subject to noise, due to PoS-tagging errors, as
well as special constructions, e.g. reduced rela-
tive clauses. The former can be controlled by fre-
quency filters, the latter can be circumvented by
using chunk boundary information1. A more chal-
lenging problem is to recognize whether an ex-
tracted nominal is in fact a valid semantic class for
NEs. Besides, class labels can be ambiguous, so
there is uncertainty as to which class an extracted
entity should be assigned to. We apply two fil-
tering strategies: we set a frequency threshold ft
on the number of extracted NE tokens per class,
to remove infrequent class label extractions; we
then filter invalid semantic classes using informa-
tion from WordNet: given the WordNet PERSON
supersense, i.e. the lexicographer file for nouns de-
noting people, we check whether the first sense of
the class label candidate is found in PERSON.
Mapping to the WordNet person domain. In
order to perform a hierarchical classification of
people, we need a taxonomy for the domain at
hand. We achieve this by mapping the extracted
class labels to WordNet synsets. In our setting, we
map against all synsets found under person#n#1,
1We use YamCha (Kudo and Matsumoto, 2000) to per-
form phrase chunking.
95
which are direct hypernyms of at least one in-
stance in WordNet (CWN pers+Inst).2 Since our
goal is to map class labels to synsets (i.e. our fu-
ture NE classes), we check each class label candi-
date against all synonyms contained in the synset.
At this point we have to deal with two cases: two
extracted class label candidates (synonyms such
as doctor, physician) will map to a single synset,
while ambiguous class labels (e.g. director) can be
mapped to more than one synset. In the latter case,
we heuristically choose the synset which domi-
nates the highest number of instances in WordNet.
Mapping evaluation. We evaluated the cover-
age of our mapping for two sets of class labels
extracted for two different frequency thresholds:
ft = 40 and ft = 1. With ft = 40, we cover
31.1% of the synsets found under person#n#1 in
WordNet, i.e. the set of classes CWN pers+Inst;
conversely, 45.8% of the extracted class labels can
be successfully mapped to CWN pers+Inst. For
threshold ft = 1, we are able to map to 87.9%
of CWN pers+Inst, with only 20.1% of extracted
classes mapped to CWN pers+Inst. For the re-
maining 79.9% of class labels (e.g. goalkeeper,
chancellor, superstar) that have no instances in
WordNet, we manually inspected 20 classes, in 20
contexts each, and established that 76% of them
are appropriate NE person classes.
For threshold ft = 40, we obtain 153 class labels
which are mapped to 146 synsets. Ten class labels
are mapped to more than one synset. Using our
mapping heuristic based on the majority instance
class, we successfully disambiguate all of them.
However, since we only map to CWN pers+Inst,
we introduce errors for 5 classes. E.g. ?manager?
incorrectly gets mapped to manager#n#2, since
the latter is the only synset containing instances.
For these cases we manually corrected the auto-
matic mapping.
A taxonomy for FG-NERC. We create our gold
standard taxonomy of semantic classes by start-
ing with the 146 synsets obtained from the map-
ping, including the 5 classes that were manually
corrected. Since we concentrate on the people
domain, we additionally remove 5 classes that
can refer to other domains as well (e.g. carrier,
guide). Given the remaining 141 synsets, we se-
lect the portion of WordNet rooted at person#n#1
2We use WordNet version 3.0. With w#p#i we denote the
i-th sense of a wordw with part of speech p. E.g., person#n#1
is defined as { person, individual . . .}).
Level #C #C w/inst #inst #inst/C % of inst
1 1 0 0 - -
2 29 8 2,662 332 5.49
3 57 37 18,229 493 37.58
4 63 46 18,422 401 37.94
5 37 30 6,231 208 12.84
6 18 13 2,366 182 4.88
7 6 5 423 85 0.87
8 2 2 179 90 0.36
all 213 141 48,512 344 100
Table 1: Level-wise statistics of classes and in-
stances across the FG-NERC person taxonomy.
which contains them, together with any interven-
ing synset found along the WordNet hierarchy.
Given this WordNet excerpt, the extracted NE to-
kens are then appended to the respective synsets in
the hierarchy. Statistics of the resulting WordNet
fragment augmented with instances are given in
Table 1. The taxonomy has a maximum depth of 8,
and contains 213 synsets, i.e. NE classes (see col-
umn 2). 83.5% of the 31,819 extracted instances
(type-level) sit in leaf nodes. The classes automat-
ically refer back to the acquired appositional con-
texts. Table 1 gives statistics about the number of
instances (token-level) acquired for classes at dif-
ferent embedding levels. In total we have at our
disposal 48,512 instances (token-level) in apposi-
tional contexts. The type-token ratio is 1.52.
Gold standard validation. To create a gold
standard dataset of entities in context labeled with
fine-grained classes, we first randomly select 20
classes, as well as an additional 18 which are
also found in the People Ontology (Giuliano and
Gliozzo, 2008). For each class, we randomly se-
lect 40 occurrences of instances in context, i.e.
the words co-occurring in a window of 60 tokens
before and after the instance. We asked four an-
notators to label these extractions for correctness,
and to provide the correct label for the incorrect
cases, if one was available. Only 52 contexts out
of 1520 were labeled as incorrect, thus giving us
96.58% accuracy on our automatically extracted
data. The manually validated dataset is used to
provide a ground-truth for FG-NERC. However,
the noun (e.g. hunter) denoting the NE class is re-
moved from these contexts for training and testing
in all experiments. This is because, due to the ex-
traction method based on POS-patterns denoting
appositions, class labels are known a priori to oc-
cur in the context of an instance and thus identify
them with high precision.
96
5 Methodology for FG-NERC
We develop methods to perform FG-NERC using
standard techniques developed for coarse-grained
NERC and WSD. These are applied to our dataset
from Section 4, in order to measure performance
at different levels of semantic class granularity, i.e.
corresponding to the depth of the semantic classes
found in our WordNet fragment. We start in Sec-
tion 5.1 to present a Maximum Entropy model to
perform coarse-grained NERC and we extend it
to perform multiclass classification in a hierarchi-
cal taxonomy (Section 5.2). We then present in
Section 5.3 an alternative proposal to perform FG-
NERC using global context information, as found
in state-of-the-art approaches to supervised and
unsupervised WSD.
5.1 NERC using a MaxEnt tagger
Our baseline system is modeled following a Maxi-
mum Entropy approach (Bender et al, 2003, inter
alia). The MaxEnt model produces a probability
for each class label t (the NE tag) of a classifica-
tion instance, conditioned on its context of occur-
rence h. This probability is calculated by:
P (t|h) =
1
Z(h)
exp
?
?
n?
j=1
?jfj(h, t)
?
? (1)
where fj(h, t) is the j-th feature with associated
weight ?j and Z(h) is a normalization constant to
ensure a proper probability distribution.3 Given a
word wi to be classified as Beginning, Inside or
Outside (IOB) of a NE, we extract as features:
1. Context words. The words occurring within
the context window wi+2i?2 = wi?2 . . . wi+2.
2. Word prefix and suffix. Word prefix and suffix
character sequences of length up to n.
3. Infrequent word. A feature that fires if wi oc-
curs in the training set less frequently than a
given threshold (i.e. below 10 occurrences).
4. Part-of-Speech (PoS) and chunk informa-
tion. The PoS and chunk labels of wi.
5. Capitalization. A binary feature that checks
whether wi starts with a capital letter or not.
6. Word length. A binary feature that fires if
the length of wi is smaller than a pre-defined
threshold (i.e. less than 5 characters).
3In our implementation, we use the OpenNLP MaxEnt li-
brary (http://maxent.sourceforge.net).
7. Digit and symbol features. Three features
check whether wi contains digit strings, non-
characters (e.g. slashes) or number expressions.
8. Dynamic feature. The tag ti?1 of the word
wi?1 preceding wi in the sequence wn1 .
5.2 MaxEnt extension for FG-NERC
Extension to hierarchical classification. We
apply our baseline NERC system to FG-NERC.
Given a word in context, the task consists of recog-
nizing it as a NE, and classifying it into the appro-
priate semantic class from our person taxonomy.
We approach this as a hierarchical classification
task by generating a binary classifier4 with sepa-
rate training and test sets for each node in the tree.
To perform level-wise classification from coarse
to fine-grained classes, we need to adjust the class
labels and their corresponding training and test in-
stances for each experiment. For classification at
the deepest level, each concept contains the in-
stances of the original dataset. For classification
at higher levels we leverage the semantics of the
WordNet hyponym relations and expand the set
of target classes (i.e. synsets) of a given level to
contain all instances of hyponym synsets. Given
a set I of classification instances for a given tar-
get class c, we add all instances labeled with the
hyponyms of c to I . All other instances (not in
that subtree) are labeled as being Outside (O-) a
NE. This approach ensures that, for each node, the
dataset contains two classes (NE and O) only, and
implicitly ?propagates? the instances up the tree.
As a result, non-leaf nodes that did not have any
instance in the original dataset become populated.
Also, the classification of classes at higher levels
is based on larger datasets.
Extension to multiclass classification. Since
we train a binary classifier for each node of the
tree, we apply two methods to infer multiclass de-
cisions from these binary classifiers, namely level-
wise and global multiclass classification. In both
paradigms, we combine the single decisions of
the individual classifiers with the winner-takes-all
strategy, using weighted voting. The weights are
calculated based on the confidence value for the
corresponding class, i.e., its conditional probabil-
ity according to Equation (1). The output label is
selected randomly in case of ties.
4The IOB tagging scheme normally assigns three different
labels, i.e. Inside (I-), Outside (O-) and Beginning (B-) of
a chunk. However, our dataset does not have any instance
labeled as B-, since it does not contain any adjacent NEs.
97
For level-wise classification, we combine only
classifiers at the same level of embedding. Given
n concepts at level l, we have n possible out-
put labels for each word. The output label for a
classification instance is determined by the highest
weighted vote among all binary classifiers at level
l. For global classification we combine all binary
classifiers of the entire tree using weighted voting
to determine the winning class label. The weights
are calculated based on the product of confidence
value and depth of the corresponding class in the
tree.
5.3 FG-NERC using global contexts
FG-NERC is a more demanding task than ?classi-
cal? NERC, due to the larger amount of classes,
the paucity of examples for each class, and the
increasingly subtle semantic differences between
these classes. For such a task contextual informa-
tion is expected to be very informative ? e.g. if an
entity co-occurs in context with ?Nobel prize?, this
provides evidence that it is likely to be a scien-
tist or scholar. However, the context window used
by our baseline MaxEnt tagger is very local, in-
cluding at most the two preceding and succeeding
words. Hence, the classifier is not able to capture
informative contextual clues in a larger context.
Previous work has related FG-NERC to WSD
approaches (Alfonseca and Manandhar, 2002).
Accordingly, we investigate two context-sensitive
approaches inspired from WSD proposals, which
consider a more global context for classification.
We first define a new feature set to induce a new
MaxEnt model (MaxEnt-B) which only uses lexi-
cal features from a larger context window, as used
in standard supervised WSD (Lee and Ng, 2002):
1. PoS context. The part-of-speech occur-
ring within the context window posi+3i?3 =
posi?3 . . . posi+3.
2. Local collocation. Local collocations Cnm sur-
rounding wi. We use C?2,?1 and C1,2.
3. Content words in surrounding context. We
consider all unigrams in contexts wi+3i?3 =
wi?3 . . . wi+3 of wi (crossing sentence bound-
aries) for the entire training data. We convert to-
kens to lower case, remove stopwords, numbers
and punctuation symbols. We define a feature
vector of length 10 using the 10 most frequent
content words. Given a classification instance,
the feature corresponding to token t is set to 1
iff the context wi+3i?3 of wi contains t.
In addition, we use a Lesk-like method (Lesk,
1986) which labels instances in context with the
WordNet synset whose gloss has the maximum
overlap with the glosses of the senses of its words
in context. Given the small context provided by
theWordNet glosses, we follow Banerjee and Ped-
ersen (2003) and expand these to also include the
words from the glosses of the hypernym and hy-
ponym synsets.
6 Experiments
6.1 Benchmarking on coarse-grained NERC
We benchmark the performance of our baseline
MaxEnt classifier using the feature set from Sec-
tion 5.1 (MaxEnt-A henceforth) on the CoNLL-
2003 shared task dataset (Tjong Kim Sang and
De Meulder, 2003), the de-facto standard for eval-
uating coarse-grained NERC systems.
In MaxEnt modeling, feature selection is a cru-
cial problem and key to improving classification
performance. MaxEnt, however, does not provide
methods for automatic feature selection. We there-
fore experimented with various combinations of
features standardly used for NERC (1-8 of Section
5.1). Model parameters are computed with 200
iterations without feature frequency cutoff. The
best configuration is found by optimizing the F1
measure on the development data with various fea-
ture representations. The chosen features are: 1, 2
(with n = 3), 4, 5, 6, 7 and 8. Evaluation on the
test set is performed blindly, using this feature set.
The results are presented in Table 2.
TheMaxEnt labeler achieves performance com-
parable with the CoNLL-2003 task participants,
ranking 12th among the 16 systems participating
in the task, with a 2 point margin off the F1 of the
most similar system of Bender et al (2003) and
7 points below the best-performing system (Flo-
rian et al, 2003). The former used a relatively
complex set of features and different gazetteers
extracted from unannotated data. The latter com-
bined four diverse classifiers, namely a robust lin-
ear classifier, maximum entropy, transformation-
based learning and a hidden Markov model. They
used different feature sets, unannotated data and
an additional NE tagger. In comparison, our
NERC system is simpler and based on a small set
of features that can be easily obtained for many
languages. Besides, it does not make use of any
external resources and still shows state-of-the-art
performance on the overall data.
98
Recall Precision F?=1
PER 83.02% 81.40% 82.21%
LOC 88.47% 88.19% 88.23%
ORG 77.20% 68.03% 72.23%
MISC 81.20% 83.92% 82.54%
Overall 83.11% 80.47% 81.77%
Table 2: Results on the CoNLL-2003 test data.
Set # tokens # NEs
Training 2,431,041 38,810
Development 478,871 9,702
Test 181,490 1,520
Table 3: Statistics for training, dev and test sets.
6.2 Evaluating FG-NERC
Experimental setting. For the task of FG-
NERC, we compare the performance of MaxEnt-
A with the MaxEnt-B model from Section 5.3 and
the Lesk method. The data is partitioned into train-
ing and development sets by randomly selecting
80%-20% of the contexts in which the NEs occur.
We use the held-out, manually validated gold stan-
dard from Section 4 for blind evaluation. Statistics
for the dataset are reported in Table 3.
We build a MaxEnt model for each FG-NE
class, using the features that performed best on
the CoNLL task, except the digit and dynamic
NE features (MaxEnt-A), and context features 1-
3 of Section 5.3 (MaxEnt-B). Model parameters
are computed in the same way as for coarse-
grained NERC. Table 3 shows that our training set
is highly unbalanced. The ratio between positive
(NEs) and negative examples (i.e. O classification
instances) at the topmost level is 63:1. Also, with
increasing levels of fine-grainedness, the number
of negative (-O) NE classes is increasing for each
binary classifier. We observed on the develop-
ment set that this skewed distribution heavily bi-
ases the classifiers towards the negative category,
and accordingly investigated sampling techniques
to make the ratio of positive and negative examples
more balanced. We experiment with a sampling
strategy that over-samples the positive examples
and under-samples the negative ones. We define
various ratios of over-sampling depending upon
the number of positive examples in the original
training set. Table 4 lists the factors (f ) of over-
sampling applied to the original positive samples
(P ), with minimum and maximum sizes of the ob-
factor f size of P min P ? max P ?
20 ? P 1 ? 2K 20 40K
15 ? P 2K ? 5K 30K 75K
10 ? P 5K ? 10K 50K 100K
5 ? P 10K ? 20K 50K 100K
2 ? P 20K ? 50K 40K 100K
P 50K ? . . . 50K >50K
Table 4: Oversampling of positive samples.
MaxEnt-A MaxEnt-BLevel
R P F1 R P F1
1 98.7 85.0 91.4 95.1 83.0 88.6
2 96.0 65.5 77.9 48.1 46.3 47.2
3 95.3 54.3 69.3 43.3 41.1 42.2
4 86.8 52.8 65.6 41.1 37.2 39.1
5 90.4 45.9 60.9 49.2 21.5 29.9
6 91.6 36.9 52.6 51.7 13.2 21.1
7 89.5 31.8 46.9 42.2 10.2 16.4
8 100.0 19.9 66.7 87.1 8.1 14.7
global 85.1 43.2 57.3 61.9 26.6 37.2
hierarchical 87.7 44.8 59.4 64.5 29.5 40.5
Table 6: Level-wise NE recognition & classifica-
tion evaluation (in %).
tained oversampled sets P ? for different ranges of
original sizes of P .5 Oversampling is done with-
out replacement. The number of negative instan-
ces is always downsampled on the basis of P ? to
yield a 1:5 ratio of positive and negative samples,
a ratio we estimated from the CoNLL-2003 data.
Level-wise evaluation results on the FG-NE
classification-only (NEC) task for the MaxEnt
classifiers and Lesk are given in Table 5. Table
6 reports results for the evaluation of the MaxEnt
model performing both classification and recog-
nition. As for coarse-grained NERC, we evaluate
using the standard metrics of recall (R), precision
(P) and balanced F-measure (F1). As baseline, we
use a majority class assignment ? i.e. at each level,
we label all instances with the most frequent class
label. For global FG-NE classification, reported in
Table 5, the original fine-grained classes are con-
sidered, across the entire class hierarchy. Global
evaluation is performed by counting exact label
predictions on the entire hierarchy (global) and us-
ing the evaluation metric of Melamed and Resnik
(2000, hierarchical). As baseline we assume the
most frequent class label in the training set.
Discussion. All methods perform reasonably
well, indicating the feasibility of the task. For the
MaxEnt models, Table 5 shows a general high re-
call and decreasing precision as we move down the
hierarchy. Degradation in the overall F1 score is
5Sampling ratios are determined on the development set.
99
Baseline MaxEnt-A MaxEnt-B LeskLevel
R P F1 R P F1 R P F1 R P F1
1 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0
2 28.4 25.9 27.1 85.8 88.6 87.0 79.5 84.9 82.2 16.4 19.7 17.9
3 27.9 23.1 25.2 83.9 88.1 85.9 75.5 79.8 77.5 16.2 16.2 16.2
4 18.8 20.4 19.5 74.6 85.0 79.5 65.4 71.3 68.2 11.3 11.3 11.3
5 25.8 19.0 21.9 78.8 83.4 80.9 78.6 74.1 76.3 13.5 14 13.8
6 24.7 7.8 11.9 88.5 73.6 80.4 78.7 74.1 75.7 33.2 37.5 35.2
7 19.1 5.34 8.3 79.2 76.5 77.8 78.1 72.7 75.3 49.4 49.4 49.4
8 34.2 2.9 5.5 82.8 73.8 78.1 81.1 71.1 75.8 0.1 0.1 0.1
global 34.6 18.5 24.1 81.1 84.2 82.6 78.0 74.2 76.6 36.5 38.6 37.5
hierarchical 33.0 21.2 25.8 83.5 86.2 84.8 78.2 77.8 78.1 36.6 38.7 37.6
Table 5: Level-wise evaluation of fine-grained NE classification techniques (in %).
given by the increasingly limited amount of class
instances found towards the low regions of the tree
(down to an average of 85 and 90 instances per
class for levels 7 and 8, respectively) (cf. Table 1).
The ?classical? feature set (MaxEnt-A) yields bet-
ter performance compared to the semantic feature
set (MaxEnt-B). However MaxEnt-B still achieves
a respectable performance, given that it contains a
few semantic features only.
The MaxEnt classifiers achieve a far better per-
formance than Lesk. This is in-line with previ-
ous findings in WSD, namely unsupervised fine-
grained disambiguation methods rarely perform-
ing above the baseline, and suggests that Lesk can
be merely used as a ?strong? baseline. Error anal-
ysis showed that it performs poorly due to the lim-
ited context provided by the WordNet glosses, and
the limited impact of gloss expansions deriving
from the low connectivity between synsets.
Comparison of Tables 5 and 6 shows that per-
formance decreases considerably for a classifier
that not only assigns fine-grained classes, but also
detects which tokens actually are NEs. As for
the classification-only task, the performance de-
creases as one moves to lower levels. This in-
dicates that the complexity of the task is propor-
tional to the fine-grainedness of the class inven-
tory. MaxEnt-B, lacking ?classical? NER features,
shows dramatic losses, compared to MaxEnt-A.
Comparison to other work. We compared the
performance of our system based on global classi-
fication (one vs. rest) against the figures reported
for individual categories in Giuliano (2009). The
MaxEnt-A system compares favorably, although it
considers (i) more classes at each level ? i.e. 213
vs. 21 ? and (ii) classifies NEs at finer-grained lev-
els ? i.e. 8 vs. 4 maximum depth in the respec-
tive WordNet fragments. We achieve overall mi-
cro average R, P and F1 values of 87.5%, 85.7%
and 86.6%, respectively, compared to Giuliano?s
79.6%, 80.9% and 80.2%. Due to the different se-
tups and data used, these figures do not offer a ba-
sis for true comparison. However, the figures sug-
gest that our system achieves respectable perfor-
mance on a more complex classification problem.
7 Conclusions
We presented a method to perform FG-NERC on
a large scale. Our contribution lies in the def-
inition of a benchmarking setup for this task in
terms of gold standard datasets and strong base-
line methods provided by a MaxEnt classifier. We
proposed a pattern-based approach for the acqui-
sition of fined-grained NE semantic classes and
instances. This corpus-based method relies only
on the availability of large text corpora, such as
the WaCky corpora, in contrast to resources diffi-
cult to obtain, such as query-logs (Pas?ca and van
Durme, 2008). It makes use of a very large Web
corpus to extract instances from open-domain con-
texts ? in contrast to standard NERC approaches,
which are tailored for newswire data and do not
generalize well across domains. Our gold stan-
dard training and test datasets are currently based
only on appositional patterns6. Therefore, it does
not include the full spectrum of constructions in
which instances can be found in context. Future
work will investigate semi-supervised and heuris-
tics (e.g. ?one sense per discourse?) methods to ex-
pand the data with examples from follow-up men-
tions, e.g. co-occurring in the same document.
Our MaxEnt models still perform very local
classification decisions, relying on separate mod-
els for each semantic class. We accordingly plan to
explore both global models operating on the over-
all hierarchy, and more informative feature sets.
6The data are available for research purposes at http:
//www.cl.uni-heidelberg.de/fgnerc.
100
References
Enrique Alfonseca and Suresh Manandhar. 2002.
An unsupervised method for general named entity
recognition and automated concept discovery. In
Proc. of GWC-02.
S. Ananiadou, C. Friedman, and J.I. Tsujii. 2004.
Special issue on named entity recognition in
biomedicine. Journal of Biomedical Informatics,
37(6).
Javier Artiles, Satoshi Sekine, and Julio Gonzalo.
2008. Web people search. In Proc. of LREC ?08.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlap as a measure of semantic relatedness.
In Proc. of IJCAI-03, pages 805?810.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide web:
A collection of very large linguistically processed
web-crawled corpora. Journal of Language Re-
sources and Evaluation, 43(3):209?226.
Oliver Bender, Franz Josef Och, and Hermann Ney.
2003. Maximum entropy models for named entity
recognition. In Proc. of CoNLL-03, pages 148?151.
Razvan Bunescu and Marius Pas?ca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proc. of EACL-06, pages 9?16.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
an experimental study. Artificial Intelligence,
165(1):91?134.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proc. of
COLING-02, pages 1?7.
Michael Fleischman. 2001. Automated subcategoriza-
tion of named entities. In Proc. of the ACL 2001
Student Workshop.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In Proc. of CoNLL-
03, pages 168?171.
Claudio Giuliano and Alfio Gliozzo. 2007. Instance
based lexical entailment for ontology population. In
Proc. of ACL-07, pages 248?256.
Claudio Giuliano and Alfio Gliozzo. 2008. Instance-
based ontology population exploiting named-entity
substitution. In Proc. of COLING-ACL-08, pages
265?272.
Claudio Giuliano. 2009. Fine-grained classification of
named entities exploiting latent semantic kernels. In
Proc. of CoNLL-09, pages 201?209.
Taku Kudo and Yuji Matsumoto. 2000. Use of Support
Vector Machines for chunk identification. In Proc.
of CoNLL-00, pages 142?144.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empir-
ical evaluation of knowledge sources and learning
algorithms for word sense disambiguation. In Proc.
of EMNLP-02, pages 41?48.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a
pine cone from an ice cream cone. In Proceedings
of the ACL-SIGDOC Conference, pages 24?26.
Thomas Mandl and Christa Womser-Hacker. 2005.
The effect of named entities on effectiveness in
cross-language information retrieval evaluation. In
Proc. of ACM SAC 2005, pages 1059?1064.
Andrew McCallum and Andrew Li. 2003. Early re-
sults for named entity recognition with conditional
random fields, features induction and web-enhanced
lexicons. In Proc. of CoNLL-03, pages 188?191.
Dan Melamed and Philip Resnik. 2000. Tagger evalu-
ation given hierarchical tag sets. Computers and the
Humanities, pages 79?84.
MUC-7. 1998. Proceedings of the Seventh Mes-
sage Understanding Conference (MUC-7). Morgan
Kaufmann, San Francisco, Cal.
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Journal
of Linguisticae Investigationes, 30(1).
Marius Pas?ca and Benjamin van Durme. 2008.
Weakly-supervised acquisition of open-domain
classes and class attributes from web documents and
query logs. In Proc. of ACL-08, pages 19?27.
M. Pas?ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006a. Names and similarities on the web: Fact ex-
traction in the fast lane. In Proc. of COLING-ACL-
06, pages 809?816.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei
Lifchits, and Alpa Jain. 2006b. Organizing and
searching the world wide web of facts ? Step one:
The one-million fact extraction challenge. In Proc.
of AAAI-06, pages 1400?1405.
Marius Pas?ca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Proc. of
CIKM-2007, pages 683?690.
Luiz Augusto Pizzato, Diego Molla, and Ce?cile Paris.
2006. Pseudo relevance feedback using named enti-
ties for question answering. In Proc. of ALTW-2006,
pages 83?90.
Simone Paolo Ponzetto andMichael Strube. 2007. De-
riving a large scale taxonomy from Wikipedia. In
Proc. of AAAI-07, pages 1440?1445.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. YAGO: A Large Ontology from
Wikipedia and WordNet. Elsevier Journal of Web
Semantics, 6(3):203?217.
Hristo Tanev and Bernardo Magnini. 2006. Weakly
supervised approaches for ontology population. In
Proc. of EACL-06, pages 17?24.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 Shared
Task: Language-independent Named Entity Recog-
nition. In Proc. of CoNLL-03, pages 127?132.
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 Shared Task: Language-independent
Named Entity Recognition. In Proc. of CoNLL-02,
pages 155?158.
101
Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 52?61,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
Assessing Interpretable, Attribute-related Meaning Representations for
Adjective-Noun Phrases in a Similarity Prediction Task
Matthias Hartung and Anette Frank
Computational Linguistics Department
Heidelberg University
{hartung,frank}@cl.uni-heidelberg.de
Abstract
We present a distributional vector space model
that incorporates Latent Dirichlet Allocation
in order to capture the semantic relation hold-
ing between adjectives and nouns along inter-
pretable dimensions of meaning: The meaning
of adjective-noun phrases is characterized in
terms of ontological attributes that are promi-
nent in their compositional semantics. The
model is evaluated in a similarity prediction
task based on paired adjective-noun phrases
from the Mitchell and Lapata (2010) bench-
mark data. Comparing our model against a
high-dimensional latent word space, we ob-
serve qualitative differences that shed light
on different aspects of similarity conveyed
by both models and suggest integrating their
complementary strengths.
1 Introduction
This paper offers a comparative evaluation of two
types of accounts to the compositional meaning of
adjective-noun phrases. This comparison is embed-
ded in a similarity judgement task that determines
the semantic similarity of pairs of adjective-noun
phrases. All models we consider establish the sim-
ilarity of adjective-noun pairs by measuring simi-
larity between vectors representing the meaning of
the individual adjective-noun phrases. However, the
models we investigate differ in the type of interpreta-
tion they assign to adjectives, nouns and the phrases
composed from them.
One type of approach is represented by the clas-
sical vector space model (VSM) of Mitchell and La-
pata (2010; henceforth: M&L). It represents the se-
mantics of adjective-noun phrases in latent seman-
tic space, based on dimensions defined by bags of
context words. This classical model will be com-
pared against a compositional analysis of adjective-
noun phrases that represents adjectives and nouns
along interpretable dimensions of meaning, i.e. dis-
crete ontological attributes such as SIZE, COLOR,
SPEED, WEIGHT. Here, lexical vectors for adjec-
tives and nouns define possible attribute meanings as
component values; vector composition is intended
to elicit those attributes that are prominent in the
meaning of the whole phrase. For instance, a com-
posed vector representation of the phrase hot pep-
per is expected to yield high component values on
the dimensions TASTE and SMELL, rather than TEM-
PERATURE. The underlying relations between ad-
jectives and nouns, respectively, and the attributes
they denote is captured by way of latent semantic in-
formation obtained from Latent Dirichlet Allocation
(LDA; Blei et al (2003)). Thus, we treat attributes
as an abstract meaning layer that generalizes over
latent topics inferred by LDA and utilize this inter-
pretable layer as the dimensions of our VSM.
This approach has been shown to be effective
in an attribute selection task (Hartung and Frank,
2011), where the goal is to predict the most promi-
nent attribute(s) ?hidden? in the compositional se-
mantics of adjective-noun phrases. In this paper,
our main interest is to assess the potential of mod-
eling adjective semantics in terms of discrete, inter-
pretable attribute meanings in a similarity judgement
task, as opposed to a representation in latent seman-
tic space that is usually applied to tasks of this kind.
52
For this purpose, we rely on the evaluation data
set of M&L which serves as a shared benchmark in
the GEMS 2011 workshop. Their similarity judge-
ment task, being tailored to measuring latent simi-
larity, represents a true challenge for an analysis fo-
cused on discrete ontological attributes.
Our results show that the latent semantic model
of M&L cannot be beaten by an interpreted anal-
ysis based on LDA topic models. However, we
show substantial performance improvements of the
interpreted analysis in specific settings with adapted
training and test sets that enable focused compar-
ison. An interesting outcome of our investiga-
tions is that ? using an interpreted LDA analysis of
adjective-noun phrases ? we uncover divergences in
the notions of similarity underlying the judgement
task that go virtually unnoticed in a latent semantic
VSM, while they need to be clearly distinguished in
models focused on interpretable representations.
The paper is structured as follows: After a brief
summarization of related work, Section 3 introduces
Controled LDA, a weakly supervised extension to
standard LDA, and explains how it can be utilized to
inject interpretable meaning dimensions into VSMs.
In Section 4, we describe the parameters and exper-
imental settings for comparing our model to M&L?s
word-based latent VSM in a similarity prediction
task. Section 5 presents the results of this experi-
ment, followed by a thorough qualitative analysis of
the specific strengths and weaknesses of both mod-
els in Section 6. Section 7 concludes.
2 Related Work
Recent work in distributional semantics has engen-
dered different perspectives on how to character-
ize the semantics of adjectives and adjective-noun
phrases.
Almuhareb (2006) aims at capturing the seman-
tics of adjectives in terms of attributes they denote
using lexico-syntactic patterns. His approach suf-
fers from severe sparsity problems and does not ac-
count for the compositional nature of adjective-noun
phrases, as it disregards the meaning contributed by
the noun. It is therefore unable to perform disam-
biguation of adjectives in the context of a noun.
Baroni and Zamparelli (2010) and Guevara
(2010) focus on how best to represent composition-
ality in adjective-noun phrases considering differ-
ent types of composition operators. These works
adhere to a fully latent representation of mean-
ing, whereas Hartung and Frank (2010) assign sym-
bolic attribute meanings to adjectives, nouns and
composed phrases by incorporating attributes as di-
mensions in a compositional VSM. By holding the
attribute meaning of adjectives and nouns in dis-
tinct vector representations and combining them
through vector composition, their approach im-
proves on both weaknesses of Almuhareb?s work.
However, their account is still closely tied to Al-
muhareb?s pattern-based approach in that counts of
co-occurrence patterns linking adjectives and nouns
to attributes are used to populate the vector represen-
tations. These, however, are inherently sparse. The
resulting model therefore still suffers from sparsity
of co-occurrence data.
Finally, Latent Dirichlet Allocation, originally de-
signed for tasks such as text classification and doc-
ument modeling (Blei et al, 2003), found its way
into lexical semantics. Ritter et al (2010) and
?O Se?aghdha (2010), e.g., model selectional restric-
tions of verb arguments by inducing topic distribu-
tions that characterize mixtures of topics observed in
verb argument positions. Mitchell and Lapata (2009,
2010) were the first to use LDA-inferred topics as
dimensions in VSMs.
Hartung and Frank (2011) adopt a similar ap-
proach, by embedding LDA into a VSM for
adjective-noun meaning composition, with LDA
topics providing latent variables for attribute mean-
ings. That is, contrary to M&L, LDA is used to
convey information about interpretable semantic at-
tributes rather than latent topics. In fact, Hartung
and Frank (2011) are able to show that ?injecting?
topic distributions inferred from LDA into a VSM
alleviates sparsity problems that persisted with the
pattern-based VSM of Hartung and Frank (2010).
Baroni et al (2010) highlight two strengths of
VSMs that incorporate interpretable dimensions of
meaning: cognitive plausibility and effectiveness in
concept categorization tasks. In their model, con-
cepts are characterized in terms of salient proper-
ties and relations (e.g., children have parents, grass
is green). However, their approach concentrates on
nouns. Open questions are (i) whether it can be ex-
tended to further word classes, and (ii) whether the
53
interpreted meaning layers are interoperable across
word classes, to cope with compositionality. The
present paper extends their work by offering a test
case for an interpretable, compositional VSM, ap-
plied to adjective-noun composition with attributes
as a shared meaning layer. Moreover, to our knowl-
edge, we are the first to expose such a model to a
pairwise similarity judgement task.
3 Attribute Modeling based on LDA
3.1 Controled LDA
This section introduces Controled LDA (C-LDA), a
weakly supervised variant of LDA. We use C-LDA
to model attribute information that pertains to ad-
jectives and nouns individually. This information is
?injected? into a vector-space framework as a ba-
sis for computing the attributes that are prominent
in compositional adjective-noun phrases.
In its original statement, LDA is a fully unsu-
pervised process that estimates topic distributions
over documents ?d and word-topic distributions ?t
with topics represented as hidden variables. Esti-
mating these parameters on a document collection
yields topic proportions P (t|d) and topic distribu-
tions P (w|t) that can be used to compute a smooth
distribution P (w|d) as in (1), where t denotes a la-
tent topic, w a word and d a document in the corpus.
P (w|d) =
?
t
P (w|t)P (t|d) (1)
While the generative story underlying both mod-
els is identical, C-LDA extends standard LDA by
?implicitly? taking supervised category information
into account. This allows for linking latent topics to
interpretable semantic attributes. The idea is to col-
lect pseudo-documents in a controlled way such that
each document conveys semantic information about
one specific attribute. The pseudo-documents are
selected along syntactic dependency paths linking
the respective attribute noun to meaningful context
words (adjectives and nouns). A corpus consisting
of the two sentences in (2), e.g., yields a pseudo-
document for the attribute noun SPEED containing
car and fast.
(2) What is the speed of this car? The machine
runs at a very fast speed.
Note that, though we are ultimately interested
in triples between attributes, adjectives and nouns
that are conveyed by the compositional semantics
of adjective-noun phrases, C-LDA is only exposed
to binary tuples between attributes and adjectives or
nouns, respectively. This is in line with the findings
of Hartung and Frank (2010), who obtained sub-
stantial performance improvements by splitting the
triples into separate binary relations.
3.2 Embedding C-LDA into a VSM
The main difference of C-LDA compared to stan-
dard LDA is that the estimated topic proportions
P (t|d) of the former will be highly attribute-
specific, and similarly so for the topic distributions
P (w|t). We experiment with two variants of VSMs
that differ in the way they integrate attribute infor-
mation inferred from C-LDA, denoted as C-LDA-A
and C-LDA-T.
In C-LDA-A, the dimensions of the space are in-
terpretable attributes. The vector components re-
lating a target word w to an attribute a are set to
P (w|a). This probability is obtained from C-LDA
by constructing the pseudo-documents as distribu-
tional fingerprints of the respective attribute, as de-
scribed in Section 3.1 above:
P (w|a) ? P (w|d) =
?
t
P (w|t)P (t|d) (3)
C-LDA-T capitalizes on latent topics as dimen-
sions; the vector components are set to the topic pro-
portions P (w|t) as directly obtained from C-LDA.1
4 Parameters and Experimental Settings
Data. Our experiments are based on the adjective-
noun section of M&L?s 2010 evaluation data set2. It
consists of 108 pairs of adjective-noun phrases that
were rated for similarity by human judges.
1The ?topics as dimensions? approach has also been used
by Mitchell and Lapata (2010) for dimensionality reduction. In
their word space model, however, this setting leads to a decrease
in performance on adjective-noun phrases. Therefore, we do
not compare ourselves to this instantiation of their model in this
paper.
2Available from: http://homepages.inf.ed.ac.
uk/s0453356/share
54
Models. We contrast the two LDA-based models
(i, ii) C-LDA-A and C-LDA-T with two standard
VSMs: (iii) a re-implementation of the latent VSM
of M&L and (iv) a dependency-based VSM (De-
pVSM) which relies on dependency paths that con-
nect the target elements and attribute nouns in local
contexts. The paths are identical to the ones used
for constructing pseudo-documents in (i) and (ii).
Thus, DepVSM relies on the same information as
C-LDA-A and C-LDA-T, without capitalizing on the
smoothing power provided by LDA.
In the C-LDA models, we experiment with several
topic number settings. Depending on the number of
attributes |A| contained in the training material (see
below), we train one model instance for each topic
number in the range from 0.5 ? |A| to 2 ? |A|. For our
LDA implementations, we use MALLET (McCal-
lum, 2002). We run 1000 iterations of Gibbs sam-
pling with hyperparameters set to the default values.
Training data. For C-LDA-A, C-LDA-T and De-
pVSM we apply two different training scenarios:
In the first setting, we collect pseudo-documents
instantiating 262 attribute nouns that are linked to
adjectives by an attribute relation in WordNet
(Fellbaum, 1998). The topic distributions induced
from this data cover the broadest space of attribute
meanings we could produce from WordNet3. In a
second setting, we assume the presence of an ?or-
acle? that confines the training data to a subset of
33 attribute nouns that are linked to those adjectives
that actually occur in the M&L test set, to allow for
a focused evaluation. In both C-LDA variants, all
adjectives and nouns occurring at least five times in
the pseudo-documents become target elements in the
VSM. The pseudo-documents are collected along
dependency paths extracted from section 2 of the
pukWaC corpus (Baroni et al, 2009). The same set-
tings are used for training the DepVSM model.
As the M&L model is not intended to reflect at-
tribute meaning, the training data for this model re-
mains constant. Like M&L, we set the target el-
ements of this model to all types contained in the
complete evaluation data set (including nouns, ad-
3Note that in Hartung and Frank (2011) only a subset of
these attributes, mainly those characterized as properties in
WordNet, could be successfully modeled, at overall moderate
performance levels.
jectives and verbs) and select the 2000 context words
that co-occur most frequently with these targets in
pukWaC 2 as the dimensions of the space.
Filters on test set. Given the different types of
?semantic gist? of the models described above, we
expect that the LDA models perform best on those
test pairs that involve attributes known to the model.
To test this expectation, we compile a restricted test
set containing 43 pairs (adj1 n1, adj2 n2) where
both adj1 and adj2 bear an attribute meaning accord-
ing to WordNet.
Composition operators. In our experiments, we
use a subset of the operators proposed by Mitchell
and Lapata (2010) to obtain a compositional repre-
sentation of adjective-noun phrases from individual
vectors: vector multiplication (?; best operator in
M&L?s experiments on adjective-noun phrases) and
vector addition (+). Besides, in order to assess the
contribution of individual vectors in the composi-
tion process, we experiment with two ?composition
surrogates? by taking the individual adjective (ADJ-
only) or noun vector (N-only) as the result of the
composition process.
Evaluating the models. The models described
above are evaluated against the human similarity
judgements data provided by Mitchell and Lapata
(2010) as follows: We compute the cosine similar-
ity between the composed vectors representing the
adjective-noun phrases in each test pair. Next, we
measure the correlation between the model scores
and the human judgements in terms of Spearman?s
?, where each human rating is treated as an indi-
vidual data point. The correlation coefficient finally
reported is the average over all instances4 of one
model. For completeness, we also report the corre-
lation score of the best model instance and the stan-
dard deviation over all model instances.
5 Discussion of Results
Results on complete test set. Table 1 displays the
results achieved by the VSMs based on C-LDA and
4In fact, only those model instances resulting in a significant
correlation with the human judgements (p < 0.05) are taken
into account. This way, we eliminate both inefficient and overly
optimistic model instances.
55
+ ? ADJ-only N-only
avg best ? avg best ? avg best ? avg best ?
26
2
at
tr
s C-LDA-A 0.19 0.25 0.05 0.15 0.20 0.04 0.17 0.23 0.04 0.11 0.23 0.06
C-LDA-T 0.19 0.24 0.02 0.28 0.31 0.02 0.20 0.24 0.02 0.18 0.24 0.03
M&L 0.21 0.34 0.19 0.27
DepVSM -0.09 -0.09 -0.14 -0.08
33
at
tr
s C-LDA-A 0.23 0.27 0.02 0.21 0.24 0.01 0.27 0.29 0.01 0.17 0.22 0.02
C-LDA-T 0.21 0.28 0.03 0.14 0.23 0.04 0.22 0.27 0.03 0.10 0.21 0.06
M&L 0.21 0.34 0.19 0.27
DepVSM 0.21 0.20 0.27 0.19
Table 1: Correlation coefficients (Spearman?s ?) for different training sets, complete test set
+ ? ADJ-only N-only
avg best ? avg best ? avg best ? avg best ?
26
2
at
tr
s
(fi
lte
re
d) C-LDA-A 0.22 0.31 0.07 0.12 0.30 0.11 0.18 0.30 0.08 0.17 0.28 0.07C-LDA-T 0.25 0.30 0.03 0.26 0.35 0.04 0.24 0.29 0.04 0.19 0.23 0.04
M&L 0.38 0.40 0.24 0.43
DepVSM 0.08 -0.09 0.06 -0.07
33
at
tr
s
(fi
lte
re
d) C-LDA-A 0.29 0.32 0.02 0.31 0.36 0.02 0.34 0.38 0.02 0.09 0.18 0.04
C-LDA-T 0.26 0.36 0.05 0.14 0.30 0.09 0.28 0.38 0.07 0.03 0.18 0.08
M&L 0.38 0.40 0.24 0.43
DepVSM 0.34 0.32 0.35 0.19
Table 2: Correlation coefficients (Spearman?s ?) for different training sets and filtered test sets
the M&L word space model on the full adjective-
noun test set. The table is split into an upper and a
lower part containing the different results obtained
from training on 262 and 33 attributes, respectively.
Each multicolumn shows the performance achieved
by one of the different composition operators pre-
sented in Section 4, as well as results obtained from
predicting similarity on the basis of raw adjective
(ADJ-only) and noun (N-only) vectors.
First and foremost, we observe best overall per-
formance for the M&L model when combined with
multiplicative vector composition (? = 0.34), even
though the best results for this setting reported in
M&L (2010) (? = 0.46) cannot be reproduced.
Nevertheless, the C-LDA models show a consid-
erable performance improvement when the training
material is constrained to appropriate attributes by
an oracle (cf. Sect. 4). Another interesting obser-
vation is that the individual adjective and noun vec-
tors produced by M&L and the C-LDA models, re-
spectively, show diametrically opposed performance
(cf. 3rd and 4th multicolumn in Table 1).
More in detail, C-LDA-A achieves relative im-
provements across all composition operators when
comparing the 33-ATTR to the 262-ATTR setting.
Contrasting C-LDA-A and C-LDA-T, the latter is
clearly more effective on the larger training set, es-
pecially in combination with the ? operator (? =
0.28). This might be due to the intersective character
of multiplication, which requires densely populated
components in both the adjective and the noun vec-
tor. This requirement meets best with the C-LDA-T
model as long as the number of topics provided is
large. The + operator, on the other hand, combines
better with C-LDA-A. In the 33-ATTR setting, this
combination even outperforms vector addition un-
der the M&L model. Generally, C-LDA-A performs
better on the smaller training set, where it leaves C-
LDA-T behind in every configuration. This high-
lights that an interpretable, attribute-related meaning
layer generalizing over latent topics can be effective
if a small, discriminative set of attributes is available
for training. Otherwise, C-LDA-T seems to be more
powerful for the present similarity judgement task.
Analyzing the performance of the composition
surrogates ADJ-only and N-only in the restricted 33-
ATTR setting reveals an interesting twist in the qual-
ity of adjective vs. noun vectors: While M&L gen-
56
erally yields better results on noun vectors alone (as
compared to adjective vectors), C-LDA-A clearly
outperforms M&L in predicting similarity based on
adjective meanings in isolation. In this configura-
tion, M&L is also outperformed by the (very strong)
dependency baseline which is, in turn, only slightly
beaten by C-LDA-A in its best configuration. In
fact, it is the ADJ-only surrogate under the C-LDA-
A model in its best setting (? = 0.29) that comes
closest to the overall best-performing M&L model.
This indicates that modeling attributes in the latent
semantics of adjectives can be informative for the
present similarity prediction task. The poor quality
of the noun vectors, however, limits the overall per-
formance of the C-LDA models considerably.
Results on filtered test set. As can be seen from
Table 2, our expectation that C-LDA-A and C-
LDA-T should benefit from limiting the test set to
instances related to attribute meanings is largely
met. We observe overall improvement of correla-
tion scores; also the characteristics of the individual
models observed in Table 1 remain unchanged.
However, M&L benefits from filtering as well,
and in some configurations, e.g. under vector addi-
tion, the relative improvement is even bigger for the
latent word space models. This shows that M&L
and our C-LDA models are not fully complemen-
tary, i.e. some aspects of attribute similarity are also
covered by latent models.
Neverthelesss, the adjective/noun twist observed
for individual vector performance is corroborated:
C-LDA-A?s adjective vectors outperform those of
M&L by ten points (33 attributes, filtered setting;
compared to six points on the complete test set),
whereas the performance of the noun vectors drops
even further. Again, the DepVSM baseline performs
very strong on the adjective vectors in isolation,
which clearly underlines that our dependency-based
context selection procedure is effective. On the other
hand, the individual noun vectors produced by M&L
even yield the best overall result on the filtered test
data, thus outperforming both composition methods.
Differences in adjective and noun vectors. In or-
der to highlight qualitative differences of the indi-
vidual adjective and noun vectors across the various
models, we analyzed their informativeness in terms
of entropy. The intuition is as follows: The lower the
262 attrs 33 attrs
avg ? avg ?
C-LDA-A (JJ) 1.20 0.48 0.83 0.27
C-LDA-A (NN) 1.66 0.72 1.23 0.46
C-LDA-T (JJ) 0.92 0.04 0.50 0.04
C-LDA-T (NN) 1.10 0.06 0.60 0.02
M&L (JJ) 2.74 0.91 2.74 0.91
M&L (NN) 2.96 0.33 2.96 0.33
DepVSM (JJ) 0.48 0.61 0.65 0.32
DepVSM (NN) 0.38 0.67 0.96 0.21
Table 3: Average entropy of individual adjective and
noun vectors across different models
entropy exhibited by a vector, the more pronounced
are its most prominent components. On the contrary,
high entropy indicates a rather broad, less accen-
tuated distribution of the probability mass over the
vector components (cf. Hartung and Frank (2010)).
The results of this analysis are displayed in Ta-
ble 3. With regard to the C-LDA models, we observe
lower entropy in adjective vectors compared to noun
vectors across both training settings, which corre-
sponds to their relative performance in the similar-
ity prediction task. This indicates that C-LDA cap-
tures the relation between adjectives and attributes
in a very pronounced way, and that this information
proves valuable for similarity prediction.
The DepVSM model shows inconsistent results
with regard to the different training sets. While the
pattern observed for the C-LDA models is confirmed
on the limited training set, training on the full set of
262 attributes results in more accentuated noun vec-
tors. Given the huge standard deviations, however,
we suppose that these figures are not very reliable.5
The correspondence between lower entropy and
better performance we could observe for C-LDA-
A and C-LDA-T is, however, not confirmed by the
M&L word space model, as their adjective vectors
exhibit lower entropy on average6, while they per-
sistently underperform relative to the noun vectors
5In fact, unlike the C-LDA models and M&L, DepVSM
faces severe sparsity problems on the large training set, as be-
comes evident from the average total frequency mass per vector:
Noun vectors accumulate 704 cooccurrence counts over 262 di-
mensions on average, while adjective vectors are populated with
1555 counts on average (652 vs. 1052 counts over 33 dimen-
sions on the small training set).
6The entropy values of M&L are not directly comparable to
those of the C-LDA models and DepVSM; M&L entropies are
generally higher due to the higher dimensionality of the model.
57
(cf. Tables 1 and 2). Note, however, that the en-
tropy values of individual adjective vectors disperse
widely around the mean (?=0.91). This suggests
that a considerable proportion of M&L?s adjective
vectors is rather evenly distributed.
Analyzing the individual performance of noun
vectors in terms of entropy is less conclusive. While
the noun vectors consistently exhibit relatively high
entropy, their varying performance across the dif-
ferent models cannot be explained. We hypothesize
that the characteristics of the different models might
be more decisive instead: Apparently, attributes as
an abstract meaning layer are appropriate for mod-
eling the contribution of adjectives to phrase simi-
larity, whereas the contribution of nouns seems to
be captured more effectively by M&L-like distribu-
tions along bags of context words.
6 Error Analysis
In order to gain deeper insight into the strengths
and weaknesses of C-LDA-A and M&L, we
extracted the ten most similar/dissimilar pairs
(+Sim/?SimC-LDA-A/M&L; cf. Table 4) according
to system predictions, as well as the ten pairs
on which system and human raters show high-
est/lowest agreement in terms of similarity scores
(+Agr/?AgrC-LDA-A/M&L; cf. Table 5), for the best-
performing model instance of C-LDA-A and M&L
in the unfiltered 33-ATTR setting, respectively.
All pairs in +SimC-LDA-A and +SimM&L exhibit
matching attributes. +SimC-LDA-A contains two pairs
involving contrastive attribute values (vs. four in
+SimM&L): long period ? short time, hot weather
? cold air. Obviously, C-LDA-A is not prepared to
recognize this type of dissimilarity, as it does not
model the semantics and orientation of attribute val-
ues, and so assigns overly optimistic similarity rates.
While this deficiency is explained for C-LDA, it is
unexpected for M&L, where in +SimM&L we find
pairs such as old person ? elderly lady with similar-
ity ratings that are almost identical to antonymous
pairs discussed above, such as high price ? low cost.
We further observe a striking difference regarding
overall similarity ratings in both systems: We find
high scores of 0.88 on average within +SimC-LDA-A,
as opposed to 0.52 in +SimM&L. The difference
is less marked regarding ?Sim. Similarly, we
find overall low average similarity rates (0.2) in
+AgrM&L, whereas +AgrC-LDA-A achieves somewhat
higher rates (0.27). While all examples point to-
wards dissimilarity, C-LDA-A shows more discrim-
inative power, as exemplified by hot weather ? el-
derly lady (lowest rating) vs. central authority ? lo-
cal office (highest rating). This suggests that, over-
all, C-LDA-A disposes of a more discriminative se-
mantic representation to judge similarity ? which of
course can also go astray.
The disagreement set ?AgrC-LDA-A contains the
antonymous adjectives with high similarity ratings
from +SimC-LDA-A, of course. We also note a high
proportion (5/10) of pairs involving adjectives with
vague and highly ambiguous attribute meanings,
such as good, new, certain, general. These are dif-
ficult to capture, especially in combination with ab-
stract noun concepts such as information, effect or
circumstance.
An interesting type of similarity is represented by
early evening ? previous day. In this case, we ob-
serve a contrast in the semantics of the nouns in-
volved, while the pair exhibits strong similarity on
the attribute level, which is reflected in the system?s
similarity score. This type of similarity is reminis-
cent of relational analogies investigated in Turney
(2008). A related example is rural community ? fed-
eral assembly. Unlike the human judges, C-LDA
predicts high similarity for both pairs.
The examples given in ?AgrM&L, by contrast,
clearly point to a lack in capturing adjective seman-
tics, with misjudgements such as effective way ? effi-
cient use, large number ? vast amount or large quan-
tity ? great majority.
Turning to ?AgrC-LDA-A again, we find 9/10 items
exhibit values greater than 0.67 (average: 0.78).
This means the model yields a high number of
false positives in rating similarity (with explanations
and some reservations just discussed). All items in
?AgrM&L, by contrast, have values below 0.36 (av-
erage: 0.16). That is, we again observe that this
model assigns lower similarity scores. This is con-
firmed by a comparative analysis of average sim-
ilarity scores on the entire test set: C-LDA-A;+
yields an average similarity of 0.48 (?=0.05) over
all instances, while M&L;? yields 0.16 on average
(?=0.16). The human ratings (after normalization
to the scale from 0 to 1) amount to 0.39 (?=0.26).
58
SIMILARITY
C-LDA-A; + M&L; ?
+Sim
long period ? short time 0.95 important part ? significant role 0.66
hot weather ? cold air 0.95 certain circumstance ? particular case 0.60
different kind ? various form 0.91 right hand ? left arm 0.56
better job ? good place 0.89 long period ? short time 0.55
different part ? various form 0.88 old person ? elderly lady 0.54
social event ? special circumstance 0.88 high price ? low cost 0.54
better job ? good effect 0.88 black hair ? dark eye 0.48
similar result ? good effect 0.85 general principle ? basic rule 0.44
social activity ? political action 0.82 special circumstance ? particular case 0.43
early evening ? previous day 0.80 hot weather ? cold air 0.43
?Sim
early stage ? long period 0.11 old person ? right hand 0.03
northern region ? early age 0.11 new information ? further evidence 0.03
earlier work ? early evening 0.11 early stage ? dark eye 0.01
elderly woman ? black hair 0.10 practical difficulty ? cold air 0.01
practical difficulty ? cold air 0.08 left arm ? elderly woman 0.01
small house ? old person 0.07 hot weather ? elderly lady 0.00
left arm ? elderly woman 0.06 national government ? cold air 0.00
hot weather ? further evidence 0.06 black hair ? right hand 0.00
dark eye ? left arm 0.05 hot weather ? further evidence 0.00
national government ? cold air 0.03 better job ? economic problem 0.00
Table 4: Similarity scores predicted by optimal C-LDA-A and M&L model instances; 33-ATTR setting
AGREEMENT
C-LDA-A; + M&L; ?
+Agr
major issue ? american country 0.29 similar result ? good effect 0.29
efficient use ? little room 0.29 small house ? important part 0.14
economic condition ? american country 0.29 national government ? new information 0.12
public building ? central authority 0.29 major issue ? social event 0.26
northern region ? industrial area 0.28 new body ? significant role 0.11
new life ? economic development 0.42 social event ? special circumstance 0.25
new body ? significant role 0.13 economic development ? rural community 0.32
hot weather ? elderly lady 0.13 new technology ? public building 0.18
social event ? low cost 0.13 high price ? short time 0.10
central authority ? local office 0.44 new body ? whole system 0.24
?Agr
early evening ? previous day 0.80 effective way ? efficient use 0.29
rural community ? federal assembly 0.67 federal assembly ? national government 0.24
new information ? general level 0.68 vast amount ? high price 0.10
similar result ? good effect 0.85 different kind ? various form 0.24
better job ? good effect 0.88 vast amount ? large quantity 0.36
social event ? special circumstance 0.88 large number ? vast amount 0.31
better job ? good place 0.89 older man ? elderly woman 0.00
certain circumstance ? particular case 0.22 earlier work ? early stage 0.00
hot weather ? cold air 0.95 large number ? great majority 0.09
long period ? short time 0.95 large quantity ? great majority 0.04
Table 5: Test pairs showing high and low agreement between systems and human raters, together with system similarity
scores as obtained from optimal model instances; 33-ATTR setting
59
While these means are not fully comparable as they
are the result of different composition operations,
the standard deviations suggest that M&L?s similar-
ity predictions are dispersed over a larger range of
the scale, while the C-LDA scores show only small
variation. This missing spread might be one of the
reasons for C-LDA?s lower performance.
In summary, we note one obvious shortcoming in
the C-LDA-A model, in that it does not capture dis-
similarity due to distinct contrastive meanings of at-
tribute values in cases of similarity on the noun and
attribute levels. With its focus on attribute seman-
tics, however, C-LDA-A is able to capture similar-
ity due to relational analogies, as in early evening
? previous day (0.8), whereas the latent model of
M&L is clearly noun-oriented, and thus predicts a
low similarity of 0.2 for this pair.
We conclude that the proposed attribute analysis
of adjective-noun pairs implements an inherently re-
lational form of similarity. Noun semantics is cap-
tured only indirectly, through the range of attributes
found relevant for the noun. The current model also
fully neglects the meaning of scalar attribute values.
Whether a more comprehensive analysis of inter-
preted adjective-noun meanings is able to succeed
in a paired similarity prediction task is an open issue
to be explored in future work.
7 Conclusion
In this paper, we presented a distributional VSM
that incorporates latent semantic information char-
acterizing ontological attributes in the meaning of
adjective-noun phrases, as obtained from C-LDA, a
weakly supervised variant of LDA. Originally de-
signed for an attribute selection task (Hartung and
Frank, 2011), this model faces a true challenge when
evaluated in a pairwise similarity judgement task
against a high-dimensional word space model, such
as M&L?s VSM. In fact, our model is unable to com-
pete with M&L even in its best configurations.
Thorough analysis reveals, however, that the qual-
ity of individual adjective and noun vectors is dia-
metric across the two models: C-LDA, capitalizing
on interpretable ontological dimensions, produces
effective adjective vectors, whereas its noun repre-
sentations lag behind. The inverse situation is ob-
served for the word-based latent VSM of M&L.
One qualification is in order, though: In its cur-
rent state, the C-LDA model relies on an ?oracle?
that pre-selects the attributes involved in the test set
for the model to be trained on. Although one could
argue that tailoring the context words to the target
words has a similar effect in our re-implementation
of M&L, interferences of this kind are not desirable
in principle. Future work will need to explore in
more detail possible attribute ranges with regard to
their usefulness for different tasks and data sets.
Our comparative investigaton of the specific
strengths and weaknesses of the models indicates
that they focus on different aspects of similarity:
M&L, possibly due to its higher and more discrim-
inative dimensionality, tends to produce more ef-
ficient noun vectors. Overall, this model accords
better with human similarity judgements across di-
verse aspects of similarity than the more focused
attribute-oriented LDA models. The C-LDA mod-
els focus on a specific, interpretable meaning di-
mension shared by adjectives and nouns, with a ten-
dency for stronger modeling capacity for adjectives.
They are currently not prepared to capture dissimi-
larity in cases of contrastive attribute values, while
on the positive side, they effectively cope with re-
lational analogies, both with similar and dissimilar
noun meanings.
Our findings suggest that adding more discrimina-
tive power to the noun representations and scalar in-
formation about attribute values to the adjective vec-
tors might be beneficial. Further research is needed
to investigate how to combine interpretable seman-
tic representations tailored to specific relations, as
captured by C-LDA, with M&L-like bag-of-words
representations in a single distributional model.
Applying interpreted models to the present simi-
larity rating task will still remain a challenge, as it
involves mapping diverse mixtures of aspects and
grades of similarity to human judgements. How-
ever, if the performance of an integrated model can
compete with a purely latent semantic analysis, this
offers a clear advantage for more general tasks that
require linking phrase meaning to symbolic knowl-
edge bases such as (multilingual) ontologies, or for
application scenarios that involve discrete seman-
tic labels, such as text classification based on topic
modeling (Blei et al, 2003) or fine-grained named
entity classification (Ekbal et al, 2010).
60
References
Abdulrahman Almuhareb. 2006. Attributes in Lexical
Acquisition. Ph.D. Dissertation, Department of Com-
puter Science, University of Essex.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, East
Stroudsburg, PA, pages 1183?1193.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky Wide Web: A Col-
lection of Very Large Linguistically Processed Web-
crawled Corpora. Journal of Language Resources and
Evaluation, 43(3):209?226.
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2010. Strudel. A Corpus-based Seman-
tic Model based on Properties and Types. Cognitive
Science, 34:222?254.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet Allocation. JMLR, 3:993?1022.
Asif Ekbal, Eva Sourjikova, Anette Frank, and Simone
Ponzetto. 2010. Assessing the Challenge of Fine-
grained Named Entity Recognition and Classification.
In Proceedings of the ACL 2010 Named Entity Work-
shop (NEWS), Uppsala, Sweden.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Semantics,
Stroudsburg, PA. Association for Computational Lin-
guistics.
Matthias Hartung and Anette Frank. 2010. A Structured
Vector Space Model for Hidden Attribute Meaning in
Adjective-Noun Phrases. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (COLING), Beijing, China, August.
Matthias Hartung and Anette Frank. 2011. Exploring
Supervised LDA Models for Assigning Attributes to
Adjective-Noun Phrases. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, Edinburgh, UK.
Andrew Kachites McCallum. 2002. MAL-
LET: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
Models of Semantic Composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, Ohio, June.
Jeff Mitchell and Mirella Lapata. 2009. Language Mod-
els Based on Semantic Composition. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, Singapore, August 2009,
pages 430?439, Singapore, August.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive Sci-
ence, 34:1388?1429.
Diarmuid ?O Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 435?444, Uppsala, Sweden, July.
Association for Computational Linguistics.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alocation method for selectional preferences.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 424?434,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Peter D. Turney. 2008. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 905?912,
Manchester, UK.
61
