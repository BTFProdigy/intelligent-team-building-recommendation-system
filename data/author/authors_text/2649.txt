Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 177?184, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Inner-Outer Bracket Models for Word Alignment
using Hidden Blocks
Bing Zhao
School of Computer Science
Carnegie Mellon University
{bzhao}@cs.cmu.edu
Niyu Ge and Kishore Papineni
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598, USA
{niyuge, papineni}@us.ibm.com
Abstract
Most statistical translation systems are
based on phrase translation pairs, or
?blocks?, which are obtained mainly from
word alignment. We use blocks to infer
better word alignment and improved word
alignment which, in turn, leads to better
inference of blocks. We propose two new
probabilistic models based on the inner-
outer segmentations and use EM algorithms
for estimating the models? parameters. The
first model recovers IBM Model-1 as a spe-
cial case. Both models outperform bi-
directional IBM Model-4 in terms of word
alignment accuracy by 10% absolute on the
F-measure. Using blocks obtained from
the models in actual translation systems
yields statistically significant improvements
in Chinese-English SMT evaluation.
1 Introduction
Today?s statistical machine translation systems rely
on high quality phrase translation pairs to acquire
state-of-the-art performance, see (Koehn et al, 2003;
Zens and Ney, 2004; Och and Ney, 2003). Here,
phrase pairs, or ?blocks? are obtained automati-
cally from parallel sentence pairs via the underlying
word alignments. Word alignments traditionally are
based on IBM Models 1-5 (Brown et al, 1993) or on
HMMs (Vogel et al, 1996). Automatic word align-
ment is challenging in that its accuracy is not yet
close to inter-annotator agreement in some language
pairs: for Chinese-English, inter-annotator agree-
ment exceeds 90 on F-measure whereas IBM Model-
4 or HMM accuracy is typically below 80s. HMMs
assume that words ?close-in-source? are aligned to
words ?close-in-target?. While this locality assump-
tion is generally sound, HMMs do have limitations:
the self-transition probability of a state (word) is the
only control on the duration in the state, the length
of the phrase aligned to the word. Also there is no
natural way to control repeated non-contiguous vis-
its to a state. Despite these problems, HMMs remain
attractive for their speed and reasonable accuracy.
We propose a new method for localizing word
alignments. We use blocks to achieve locality in the
following manner: a block in a sentence pair is a
source phrase aligned to a target phrase. We assume
that words inside the source phrase cannot align to
words outside the target phrase and that words out-
side the source phrase cannot align to words inside
the target phrase. Furthermore, a block divides the
sentence pair into two smaller regions: the inner
part of the block, which corresponds to the source
and target phrase in the block, and the outer part of
the block, which corresponds to the remaining source
and target words in the parallel sentence pair. The
two regions are non-overlapping; and each of them is
shorter than the original parallel sentence pair. The
regions are thus easier to align than the original sen-
tence pairs (e.g., using IBM Model-1). While the
model uses a single block to split the sentence pair
into two independent regions, it is not clear which
block we should select for this purpose. Therefore,
we treat the splitting block as a hidden variable.
This proposed approach is far simpler than treat-
ing the entire sentence as a sequence of non-
overlapping phrases (or chunks) and considering such
sequential segmentation either explicitly or implic-
itly. For example, (Marcu and Wong, 2002) for a
joint phrase based model, (Huang et al, 2003) for
a translation memory system; and (Watanabe et
al., 2003) for a complex model of insertion, deletion
and head-word driven chunk reordering. Other ap-
proaches including (Watanabe et al, 2002) treat ex-
tracted phrase-pairs as new parallel data with limited
success. Typically, they share a similar architecture
of phrase level segmentation, reordering, translation
as in (Och and Ney, 2002; Koehn and Knight, 2002;
Yamada and Knight, 2001). The phrase level inter-
action has to be taken care of for the non-overlapping
sequential segmentation in a complicated way. Our
models model such interactions in a soft way. The
hidden blocks are allowed to overlap with each other,
177
while each block induced two non-overlapping re-
gions, i.e. the model brackets the sentence pair
into two independent parts which are generated syn-
chronously. In this respect, it resembles bilingual
bracketing (Wu, 1997), but our model has more lex-
ical items in the blocks with many-to-many word
alignment freedom in both inner and outer parts.
We present our localization constraints using
blocks for word alignment in Section 2; we detail our
two new probabilistic models and their EM train-
ing algorithms in Section 3; our baseline system, a
maximum-posterior inference for word alignment, is
explained in Section 4; experimental results of align-
ments and translations are in Section 5; and Section
6 contains discussion and conclusions.
2 Segmentation by a Block
We use the following notation in the remainder of
this paper: e and f denote the English and foreign
sentences with sentence lengthes of I and J , respec-
tively. ei is an English word at position i in e; fj is
a foreign word at position j in f . a is the alignment
vector with aj mapping the position of the English
word eaj to which fj connects. Therefore, we have
the standard limitation that one foreign word can-
not be connected to more than one English word. A
block ?[] is defined as a pair of brackets as follows:
?[] = (?e, ?f ) = ([il, ir], [jl, jr]), (1)
where ?e = [il, ir] is a bracket in English sentence de-
fined by a pair of indices: the left position il and the
right position ir, corresponding to a English phrase
eiril . Similar notations are for ?f = [jl, jr], which isone possible projection of ?e in f . The subscript l and
r are abbreviations of left and right, respectively.
?e segments e into two parts: (?e, e) = (?e?, ?e/?).The inner part ?e? = {ei, i ? [il, ir]} and the outer
part ?e/? = {ei, i /? [il, ir]}; ?f segments f similarly.
Thus, the block ?[] splits the parallel sentence pair
into two non-overlapping regions: the Inner ?[]? and
Outer ?[]/? parts (see Figure 1). With this segmen-tation, we assume the words in the inner part are
aligned to inner part only: ?[]? = ?e? ? ?f? : {ei, i ?
[il, ir]} ? {fj , j ? [jl, jr]}; and words in the outer
part are aligned to outer part only: ?[]/? = ?e/? ? ?f/? :{ei, i /? [il, ir]} ? {fj , j /? [jl, jr]}. We do not allow
alignments to cross block boundaries. Words inside
a block ?[] can be aligned using a variety of models
(IBM models 1-5, HMM, etc). We choose Model1 for
simplicity. If the block boundaries are accurate, we
can expect high quality word alignment. This is our
proposed new localization method.
Outer
Inner
li ri
rj
lj
e?
f?
Figure 1: Segmentation by a Block
3 Inner-Outer Bracket Models
We treat the constraining block as a hidden variable
in a generative model shown in Eqn. 2.
P (f |e) =
?
{?[]}
P (f , ?[]|e)
=
?
{?e}
?
{?f}
P (f , ?f |?e, e)P (?e|e), (2)
where ?[] = (?e, ?f ) is the hidden block. In the gen-
erative process, the model first generates a bracket
?e for e with a monolingual bracketing model of
P (?e|e). It then uses the segmentation of the En-
glish (?e, e) to generate the projected bracket ?f of f
using a generative translation model P (f , ?f |?e, e) =
P (?f/?, ?f?|?e/?, ?e?) ? the key model to implement ourproposed inner-outer constraints. With the hidden
block ?[] inferred, the model then generates word
alignments within the inner and outer parts sepa-
rately. We present two generating processes for the
inner and outer parts induced by ?[] and correspond-
ing two models of P (f , ?f |?e, e). These models are
described in the following secions.
3.1 Inner-Outer Bracket Model-A
The first model assumes that the inner part and the
outer part are generated independently. By the for-
mal equivalence of (f, ?f ) with (?f?, ?f/?), Eqn. 2 canbe approximated as:
P (f |e)?
?
{?e}
?
{?f}
P (?f?|?e?)P (?f/?|?e/?)P (?e|e)P (?f |?e),
(3)
where P (?f?|?e?) and P (?f/?|?e/?) are two independentgenerative models for inner and outer parts, respec-
178
tively and are futher decompsed into:
P (?f?|?e?) =
?
{aj??e?}
?
fj??f?
P (fj |eaj )P (eaj |?e?)
P (?f/?|?e/?) =
?
{aj??e/?}
?
fj??f/?
P (fj |eaj )P (eaj |?e/?), (4)
where {aJ1 } is the word alignment vector. Given the
block segmentation and word alignment, the genera-
tive process first randomly selects a ei according to
either P (ei|?e?) or P (ei|?e/?); and then generates fj in-dexed by word alignment aj with i = aj according to
a word level lexicon P (fj |eaj ). This generative pro-
cess using the two models of P (?f?|?e?) and P (?f/?|?e/?)must satisfy the constraints of segmentations induced
by the hidden block ?[] = (?e, ?f ). The English
words ?e? inside the block can only generate the words
in ?f? and nothing else; likewise ?e/? only generates
?f/?. Overall, the combination of P (?f?|?e?)P (?f/?|?e/?)in Eqn. 3 collaborates each other quite well in prac-
tice. For a particular observation ?f?, if ?e? is too
small (i.e., missing translations), P (?f?|?e?) will suf-
fer; and if ?e? is too big (i.e., robbing useful words
from ?e/?), P (?f/?|?e/?) will suffer. Therefore, our pro-posed model in Eqn. 3 combines the two costs and
requires both inner and outer parts to be explained
well at the same time.
Because the model in Eqn. 3 is essentially a two-
level (?[] and a) mixture model similar to IBM Mod-
els, the EM algorithm is quite straight forward as
in IBM models. Shown in the following are several
key E-step computations of the posteriors. The M-
step (optimization) is simply the normalization of
the fractional counts collected using the posteriors
through the inference results from E-step:
P?[]?(aj |?
f
?, ?e?) =
P (fj |eaj )?
ek??e? P (fj |ek)
P?[]/?(aj |?
f
/?, ?e/?) =
P (fj |eaj )?
ek??e/? P (fj |ek)
(5)
The posterior probability of P (aJ1 |f , ?f , ?e, e) =?J
j=1 P (aj |f , ?f , ?e, e), where P (aj |f , ?f , ?e, e) is ei-
ther P?[]?(aj |?
f
?, ?e?) when (fj , eaj ) ? ?[]?, or oth-
erwise P?[]/?(aj |?
f
/?, ?e/?) when (fj , eaj ) ? ?[]/?. As-
suming P (?e|e) to be a uniform distribution, the
posterior of selecting a hidden block given ob-
servations: P (?[] = (?e, ?f )|e, f) is proportional
to block level relative frequency Prel(?f?|?e?) up-
dated in each iteration; and can be smoothed
with P (?f |?e, f , e) = P (?f?|?e?)P (?f/?|?e/?)/
?
{??f}
P (??f? |?e?)P (?
?f
/? |?e/?) assuming Model-1 alignment inthe inner and outer parts independently to reduce
the risks of data sparseness in estimations.
In principle, ?e can be a bracket of any length
not exceeding the sentence length. If we restrict the
bracket length to that of the sentence length, we re-
cover IBM Model-1. Figure 2 summarizes the gener-
ation process for Inner-Outer Bracket Model-A.
f1 f2  f3  f4
e1 e2  e3
[e1] e2  e3 e1 [e2] e3 [e1 e2] e3 e1 [e2 e3]
?.
f1 f4
e1 e3
f2  f3
e2
f1 f3 f4
e1 e3
f2
e2
? ?
]3,2[=f? ]2,2[=f? [.,.]=f?
]2,2[=e?]1,1[=e? ]2,1[=e? ]3,2[=e?
innerouter innerouter
Figure 2: Illustration of generative Bracket Model-A
3.2 Inner-Outer Bracket Model-B
A block ?[] invokes both the inner and outer gener-
ations simultaneously in Bracket Model A (BM-A).
However, the generative process is usually more ef-
fective in the inner part as ?[] is generally small and
accurate. We can build a model focusing on gener-
ating only the inner part with careful inferences to
avoid errors from noisy blocks. To ensure that all
fJ1 are generated, we need to propose enough blocks
to cover each observation fj . This constraint can be
met by treating the whole sentence pair as one block.
The generative process is as follows: First the
model generates an English bracket ?e as before. The
model then generates a projection ?f in f to local-
ize all aj ?s for the given ?e according to P (?f |?e, e).
?e and ?f forms a hidden block ?[]. Given ?[], the
model then generates only the inner part fj ? ?f? via
P (f |?f , ?e, e) ' P (?f?|?f , ?e, e). Eqn. 6 summarizes
this by rewriting P (f , ?f |?e, e):
P (f , ?f |?e, e) = P (f |?f , ?e, e)P (?f |?e, e) (6)
= P (f |?f , ?e, e)P ([jl, jr]|?e, e)
' P (?f?|?f , ?e, e)P ([jl, jr]|?e, e).
P (?f?|?f , ?e, e) is a bracket level emission proba-
bilistic model which generates a bag of contiguous
words fj ? ?f? under the constraints from the given
hidden block ?[] = (?f , ?e). The model is simplified
in Eqn. 7 with the assumption of bag-of-words? inde-
pendence within the bracket ?f :
P (?f?|?f , ?e, e) =?
aJ1
?
j??f? P (fj |eaj )P (eaj |?
f , ?e, e). (7)
179
180
puting a pair (j, t)?:
(j, t)? = argmax
(j,t)
P (fj |et), (11)
that is, the point at which the posterior is maximum.
The pair (j, t) defines a word pair (fj , et) which is
then aligned. The procedure continues to find the
next maximum in the posterior matrix. Contrast
this with Viterbi alignment where one computes
f?T1 = argmax
{fT1 }
P (f1, f2, ? ? ? , fT |eT1 ), (12)
We observe, in parallel corpora, that when one
word translates into multiple words in another lan-
guage, it usually translates into a contiguous se-
quence of words. Therefore, we impose a conti-
guity constraint on word alignments. When one
word fj aligns to multiple English words, the En-
glish words must be contiguous in e and vice versa.
The algorithm to find word alignments using max-
posterior with contiguity constraint is illustrated in
Algorithm 1.
Algorithm 1 A maximum-posterior algorithm with
contiguity constraint
1: while (j, t) = (j, t)? (as computed in Eqn. 11)
do
2: if (fj , et) is not yet algned then
3: align(fj , et);
4: else if (et is contiguous to what fj is aligned)
or (fj is contiguous to what et is aligned) then
5: align(fj , et);
6: end if
7: end while
The algorithm terminates when there isn?t any
?next? posterior maximum to be found. By defi-
nition, there are at most JxT ?next? maximums in
the posterior matrix. And because of the contiguity
constraint, not all (fj , et) pairs are valid alignments.
The algorithm is sure to terminate. The algorithm
is, in a sense, directionless, for one fj can align to
multiple et?s and vise versa as long as the multiple
connections are contiguous. Viterbi, however, is di-
rectional in which one state can emit multiple obser-
vations but one observation can only come from one
state.
5 Experiments
We evaluate the performances of our proposed mod-
els in terms of word alignment accuracy and trans-
lation quality. For word alignment, we have 260
hand-aligned sentence pairs with a total of 4676 word
pair links. The 260 sentence pairs are randomly
selected from the CTTP1 corpus. They were then
word aligned by eight bilingual speakers. In this set,
we have one-to-one, one-to-many and many-to-many
alignment links. If a link has one target functional
word, it is considered to be a functional link (Ex-
amples of funbctional words are prepositions, deter-
miners, etc. There are in total 87 such functional
words in our experiments). We report the overall F-
measures as well as F-measures for both content and
functional word links. Our significance test shows
an overall interval of ?1.56% F-measure at a 95%
confidence level.
For training data, the small training set has 5000
sentence pairs selected from XinHua news stories
with a total of 131K English words and 125K Chi-
nese words. The large training set has 181K sentence
pairs (5k+176K); and the additional 176K sentence
pairs are from FBIS and Sinorama, which has in to-
tal 6.7 million English words and 5.8 million Chinese
words.
5.1 Baseline Systems
The baseline is our implementation of HMM with
the maximum-posterior algorithm introduced in sec-
tion 4. The HMMs are trained unidirectionally. IBM
Model-4 is trained with GIZA++ using the best re-
ported settings in (Och and Ney, 2003). A few pa-
rameters, especially the maximum fertility, are tuned
for GIZA++?s optimal performance. We collect bi-
directional (bi) refined word alignment by growing
the intersection of Chinese-to-English (CE) align-
ments and English-to-Chinese (EC) alignments with
the neighboring unaligned word pairs which appear
in the union similar to the ?final-and? approaches
(Koehn, 2003; Och and Ney, 2003; Tillmann, 2003).
Table 1 summarizes our baseline with different set-
tings. Table 1 shows that HMM EC-P gives the
F-measure(%) Func Cont Both
Small
HMM EC-P 54.69 69.99 64.78
HMM EC-V 31.38 53.56 55.59
HMM CE-P 51.44 69.35 62.69
HMM CE-V 31.43 63.84 55.45
Large
HMM EC-P 60.08 78.01 71.92
HMM EC-V 32.80 74.10 64.26
HMM CE-P 58.45 79.44 71.84
HMM CE-V 35.41 79.12 68.33
Small GIZA MH-bi 45.63 69.48 60.08GIZA M4-bi 48.80 73.68 63.75
Large GIZA MH-bi 49.13 76.51 65.67GIZA M4-bi 52.88 81.76 70.24
- Fully-Align 2 5.10 15.84 9.28
Table 1: Baseline: V: Viterbi; P: Max-Posterior
1LDC2002E17
181
best baseline, better than bidirectional refined word
alignments from GIZA M4 and the HMM Viterbi
aligners.
5.2 Inner-Outer Bracket Models
We trained HMM lexicon P (f |e) to initialize the
inner-outer Bracket models. Afterwards, up to 15?
20 EM iterations are carried out. Iteration starts
from the fully aligned2 sentence pairs, which give an
F-measure of 9.28% at iteration one.
5.2.1 Small Data Track
Figure 4 shows the performance of Model-A (BM-
A) trained on the small data set. For each English
bracket, Top-1 means only the fractional counts from
the Top-1 projection are collected, Top-all means
counts from all possible projections are collected. In-
side means the fractional counts are collected from
the inner part of the block only; and outside means
they are collected from the outer parts only. Using
the Top-1 projection from the inner parts of the block
(top-1-inside) gives the best performance: an F-
measure of 72.29%, or a 7.5% absolute improvement
over the best baseline at iteration 5. Figure 5 shows
BM-A with different settings on small data set
62
64
66
68
70
72
74
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16EM  Iterations
F-m
ea
su
re top-1 inside
top-all inside
top all
top-1
top-1 outside
top-all outside
Figure 4: BM-A with different settings on small data
the performance of Inner-Outer Bracket Model-B
(BM-B) over EM iterations. smoothing means when
collecting the fractional counts, we reweigh the up-
dated fractional count by 0.95 and give the remain-
ing 0.05 weight to original fractional count from the
links, which were aligned in the previous iteration.
w/null means we applied the proposed Null word
model in section 3.3 to infer null links. We also pre-
defined a list of 15 English function words, for which
there might be no corresponding Chinese words as
translations. These 15 English words are ?a, an, the,
of, to, for, by, up, be, been, being, does, do, did, -?.
In the drop-null experiments, the links containing
these predefined function words are simply dropped
2Every possible word pair is aligned
in the final word alignment (this means they are left
unaligned).
BM-B with different settings on small data set
65
67
69
71
73
75
77
1 2 3 4 5 6 7 8EM  Iterations
F-m
ea
su
re
top-1 smooth dropnulltop-1 smooth w/nulltop-1 smoothtop 1top all
Figure 5: BM-B with different settings on small data
Empirically we found that doing more than 5 it-
erations lead to overfitting. The peak performance
in our model is usually achieved around iteration
4?5. At iteration 5, setting ?BM-B Top-1? gives an
F-measure of 73.93% which is better than BM-A?s
best performance (72.29%). This is because Model
B leverages a local search for less noisy blocks and
hence the inner part is more accurately generated
(which in turn means the outer part is also more
accurate). From this point on, all of our experi-
ments are using Model B. With smoothing, BM-B
improves to 74.46%. After applying the null word
model, we get 75.20%. By simply dropping links
containing the 15 English functional words, we get
76.24%, which is significantly better than our best
baseline obtained from even the large training set
(HMM EC-P: 71.92%).
BM-B with different settings on large data set
69
71
73
75
77
79
81
83
1 2 3 4 5 6 7 8EM  Iterations
F-m
ea
su
re
top-1 smooth dropnull
top-1 smooth w/null
top-1 smooth
Figure 6: BM-B with different settings on large data
5.2.2 Large Data Track
Figure 6 shows performance pictures of model
BM-B on the large training set. Without dropping
English functional words, the best performance is
182
80.38% at iteration 4 using the Top-1 projection to-
gether with the null word models. By additionally
dropping the links containing the 15 functional En-
glish words, we get 81.47%. These results are all
significantly better than our strongest baseline sys-
tem: 71.92% F-measure using HMM EC-P (70.24%
using bidirectional Model-4 for comparisons).
On this data set, we experimented with different
maximum bracket length limits, from one word (un-
igram) to nine-gram. Results show that a maximum
bracket length of four is already optimal (79.3% with
top-1 projection), increased from 62.4% when maxi-
mum length is limited to one. No improvements are
observed using longer than five-gram.
5.3 Evaluate Blocks in the EM Iterations
Our intuition was that good blocks can improve word
alignment and, in turn, good word alignment can
lead to better block selection. The experimental re-
sults above support the first claim. Now we consider
the second claim that good word alignment leads to
better block selection.
Given reference human word alignment, we extract
reference blocks up to five-gram phrases on Chinese.
The block extraction procedure is based on the pro-
cedures in (Tillmann, 2003).
During EM, we output all the hidden blocks actu-
ally inferred at each iteration, then we evaluate the
precision, recall and F-measure of the hidden blocks
according to the extracted reference blocks. The re-
sults are shown in Figure 7. Because we extract all
10%
15%
20%
25%
30%
35%
40%
45%
F-m
ea
su
res
1 2 3 4 5 6 7 8EM  Iterations
A Direct Eval of blocks' accuracy in 'BM-B top-1 smooth w/null'
F-measure
Recall
Precision
Figure 7: A Direct Eval. of Blocks in BM-B
possible n-grams at each position in e, the precision
is low and the recall is relatively high as shown by
Figure 7. It also shows that blocks do improve, pre-
sumably benefiting from better word alignments.
Table 2 summarizes word alignment performances
of Inner-Outer BM-B in different settings. Overall,
without the handcrafted function word list, BM-B
gives about 8% absolute improvement in F-measure
on the large training set and 9% for the small set
F-measure(%) Func Cont Both
Small
Baseline 54.69 69.99 64.78
BM-B-drop 62.76 82.99 76.24
BM-B w/null 61.24 82.54 75.19
BM-B smooth 59.61 82.99 74.46
Large
Baseline 60.08 78.01 71.92
BM-B-drop 63.95 90.09 81.47
BM-B w/null 62.24 89.99 80.38
BM-B smooth 60.49 90.09 79.31
Table 2: BM-B with different settings
with a confidence interval of ?1.56%.
5.4 Translation Quality Evaluations
We also carried out the translation experiments using
the best settings for Inner-Outer BM-B (i.e. BM-B-
drop) on the TIDES Chinese-English 2003 test set.
We trained our models on 354,252 test-specific sen-
tence pairs drawn from LDC-supplied parallel cor-
pora. On this training data, we ran 5 iterations of
EM using BM-B to infer word alignments. A mono-
tone decoder similar to (Tillmann and Ney, 2003)
with a trigram language model3 is set up for trans-
lations. We report case sensitive Bleu (Papineni et
al., 2002) scoreBleuC for all experiments. The base-
line system (HMM ) used phrase pairs built from the
HMM-EC-P maximum posterior word alignment and
the corresponding lexicons. The baseline BleuC score
is 0.2276 ? 0.015. If we use the phrase pairs built
from the bracket model instead (but keep the HMM
trained lexicons), we get case sensitive BleuC score
0.2526. The improvement is statistically significant.
If on the other hand, we use baseline phrase pairs
with bracket model lexicons, we get a BleuC score
0.2325, which is only a marginal improvement. If we
use both phrase pairs and lexicons from the bracket
model, we get a case sensitive BleuC score 0.2750,
which is a statistically significant improvement. The
results are summarized in Table 3.
Settings BleuC
Baseline (HMM phrases and lexicon) 0.2276
Bracket phrases and HMM lexicon 0.2526
Bracket lexicon and HMM phrases 0.2325
Bracket (phrases and lexicon) 0.2750
Table 3: Improved case sensitive BleuC using BM-B
Overall, using Model-B, we improve translation
quality from 0.2276 to 0.2750 in case sensitive BleuC
score.
3Trained on 1-billion-word ViaVoice English data; the
same data is used to build our True Caser.
183
6 Conclusion
Our main contributions are two novel Inner-Outer
Bracket models based on segmentations induced by
hidden blocks. Modeling the Inner-Outer hidden seg-
mentations, we get significantly improved word align-
ments for both the small training set and the large
training set over the widely-practiced bidirectional
IBM Model-4 alignment. We also show significant
improvements in translation quality using our pro-
posed bracket models. Robustness to noisy blocks
merits further investigation.
7 Acknowledgement
This work is supported by DARPA under contract
number N66001-99-28916.
References
P.F. Brown, Stephen A. Della Pietra, Vincent. J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation:
Parameter estimation. In Computational Linguis-
tics, volume 19(2), pages 263?331.
Niyu Ge. 2004. A maximum posterior method
for word alignment. In Presentation given at
DARPA/TIDES MT workshop.
J.X. Huang, W.Wang, and M. Zhou. 2003. A unified
statistical model for generalized translation mem-
ory system. In Machine Translation Summit IX,
pages 173?180, New Orleans, USA, September 23-
27.
Philipp Koehn and Kevin Knight. 2002. Chunkmt:
Statistical machine translation with richer linguis-
tic knowledge. Draft, Unpublished.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based machine transla-
tion. In Proc. of HLT-NAACL 2003, pages 48?54,
Edmonton, Canada, May-June.
Philipp Koehn. 2003. Noun phrase translation. In
Ph.D. Thesis, University of Southern California,
ISI.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical ma-
chine translation. In Proc. of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 133?139, Philadelphia, PA, July 6-7.
Franz J. Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for
statistical machine translation. In Proceedings of
the 40th Annual Meeting of ACL, pages 440?447.
Franz J. Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment
models. In Computational Linguistics, volume 29,
pages 19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In Proc.
of the 40th Annual Conf. of the ACL (ACL 02),
pages 311?318, Philadelphia, PA, July.
Christoph Tillmann and Hermann Ney. 2003. Word
reordering and a dp beam search algorithm for
statistical machine translation. In Computational
Linguistics, volume 29(1), pages 97?133.
Christoph Tillmann. 2003. A projection extension
algorithm for statistical machine translation. In
Proc. of the Conference on Empirical Methods in
Natural Language Processing.
Kristina Toutanova, H. Tolga Ilhan, and Christo-
pher D. Manning. 2002. Extensions to hmm-based
statistical word alignment models. In Proc. of the
Conference on Empirical Methods in Natural Lan-
guage Processing, Philadelphia, PA, July 6-7.
S. Vogel, Hermann Ney, and C. Tillmann. 1996.
Hmm based word alignment in statistical machine
translation. In Proc. The 16th Int. Conf. on Com-
putational Lingustics, (Coling?96), pages 836?841,
Copenhagen, Denmark.
Taro Watanabe, Kenji Imamura, and Eiichiro
Sumita. 2002. Statistical machine translation
based on hierarchical phrases. In 9th International
Conference on Theoretical and Methodological Is-
sues, pages 188?198, Keihanna, Japan, March.
Taro Watanabe, Eiichiro Sumita, and Hiroshi G.
Okuno. 2003. Chunk-based statistical transla-
tion. In In 41st Annual Meeting of the ACL (ACL
2003), pages 303?310, Sapporo, Japan.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. In Computational Linguistics, volume 23(3),
pages 377?403.
K. Yamada and Kevin. Knight. 2001. Syntax-based
statistical translation model. In Proceedings of the
Conference of the Association for Computational
Linguistics (ACL-2001).
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Pro-
ceedings of the Human Language Technology Con-
ference (HLT-NAACL)s, pages 257?264, Boston,
MA, May.
184
Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 61?68,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
 
Multiple Reorderings in Phrase-based Machine Translation 
 
Niyu Ge,  Abe Ittycheriah 
IBM T.J.Watson Research 
1101 Kitchawan Rd.  
Yorktown Heights, NY 10598 
(niyuge, abei)@us.ibm.com 
 
Kishore Papineni 
Yahoo! Research 
45 West 18th St. 
New York, NY 10011 
kpapi@yahoo-inc.com 
 
 
Abstract  
This paper presents a method to integrate 
multiple reordering strategies in 
phrase-based statistical machine 
translation.  Recently there has been much 
research effort in reordering problems in 
machine translation.   State-of-the-art 
decoders incorporate sophisticated local 
reordering strategies, but there is little 
research on a unified approach to 
incorporate various kinds of reordering 
methods.  We present a phrase-based 
decoder which easily allows multiple 
reordering schemes.  We show how to use 
this framework to perform distance-based 
reordering and HIERO-style (Chiang 
2005) hierarchical reordering.  We also 
present two novel syntax-based reordering 
methods, one built on part-of-speech tags 
and the other based on parse trees.  We will 
give experimental results using these 
relatively easy to implement methods on 
standard tests.    
1 Introduction and Previous Work 
Given an input source sentence and guided by a 
translation model, language model, distortion 
model, etc., a machine translation decoder 
searches for a target sentence that is the best 
translation of the source.  There are usually two 
aspects of the search.  One tries to find target 
words for a given source segment.  The other 
searches for the order in which the source 
segments are to be translated.   A source segment 
here means a contiguous part of the source 
sentence. The former is largely controlled by 
language models and translation models and the 
latter by language models and distortion models.  
It is, in most cases, the latter, the search for the 
correct word order (which source segment to be 
translated next) that results in a large 
combinatorial search space.  State-of-the-art 
decoders use dynamic programming based 
beam-search with local reordering (Och 1999, 
Tillmann 2000).  Although local reordering to 
some degree is implicit in phrase-based 
decoding, the kind of reordering is very limited.   
The simplest distance-based reordering, from the 
current source position i, tries to defer the 
translation of the next n words (1 ? n ? N, N the 
maximum number of words to be delayed).  N is 
bounded by the computational requirements.    
 
Recent work on reordering has been on trying to 
find ?smart? ways to decide word order, using 
syntactic features such as POS tags (Lee and Ge 
2005) , parse trees (Zhang et.al, 2007, Wang et.al. 
2007,  Collins et.al. 2005, Yamada and Knight 
2001) to name just a few, and synchronized CFG 
(Wu 1997, Chiang 2005), again to name just a 
few.  These efforts have shown promising 
improvements in translation quality.  However, 
to use these features during decoding requires 
either a separate decoder to be written or some 
ad-hoc mechanisms to be invented to incorporate 
them into an existing decoder, or in some cases 
(Wang et. al. 2007) the input source is 
pre-ordered to be decoded monotonically.   
 
(Kanthak et. al. 2005) described a framework  in 
which different reordering methods are 
represented as search constraints to a finite state 
automata.   It is able to compute distance-based 
and ITG-style reordering automata.   We differ 
from that approach in a couple of ways.  One is 
that in (Kanthak et. al. 2005), an on-demand 
61
 reordering graph is pre-computed which is then 
taken as a input for monotonic decoding.    We 
compute the reordering as the sentence is being 
decoded.  The second is that it is not clear how to 
generate the permutation graphs under, say 
HIERO-type hierarchical constraints,  or other 
syntax-inspired reorderings such as those based 
on part-of-speech patterns.  Our approach differs 
in that we allow greater flexibility in capturing a 
wider range of reordering strategies. 
 
We will first give an overview of  the framework 
(?2).  We then describe how to implement four 
reordering methods in a single decoder in ?3.  ?4 
presents some Chinese-English results on the 
NIST MT test sets.  It also shows results on web 
log and broadcast news data.    
2 Reordering in Decoding 
2.1 Hypothesis 
The process of MT decoding can be thought of as 
a process of hypothesizing target translations.  
Given an input source sentence of length L, the 
decoding is done segment by segment.  A 
segment is simply an n-word source chunk, 
where 1 ? n ? L.  Decoding finishes when all 
source chunks are translated (some source words 
that have no target translations can be thought of 
as being translated into a special token NULL).   
The decoder at this point outputs its best 
hypothesis. 
 
2.2 Hypothesis with reorderings 
In order to facilitate various search strategies, a 
separation of duty is called for.    The decoder is 
composed of two major modules, a reordering 
module and a production module.  The reordering 
module decides which source segment to be 
translated next.   The production module 
produces the actual translations for a given 
segment.  Although most of the start-of-the-art 
decoders have these two modules, they are 
nevertheless tightly coupled.  Here they are 
separated.  This separation does not compromise 
the search space of the decoder.  Hypotheses that 
are explored in the traditional way are still 
explored in this framework.  This separation is 
essential if one were to design a decoder that 
incorporates phrase-based, syntax-based, and 
other types of decoding in a unified and 
disciplined way.   In the decoder, each hypothesis 
carries with it a sequence of source segments to 
be decoded at the current time step.   After the 
production module translates these segments and 
after beam pruning is applied to all the 
hypotheses produced at this time step, the 
hypotheses go back to the reordering module 
which determines the next source segments to be 
translated.  This process continues until all source 
words are translated.   
 
One can think of the reordering module as a black 
box whose sole responsibility is to determine the 
next sequence of source segments to be translated.   
Given this separation, the reordering module can 
be implemented in whichever way and the 
changes in it do not require changes to any other 
modules in the decoder.  There can be a suite of 
such modules, each exploring different features 
and implementing different search schemes.  A 
reordering module that implement basic 
distance-based reordering will take two 
parameters, the number of source words to be 
skipped and the window size that determines 
when the skipped words must be translated.  A 
reordering module that is based on HIERO rules 
will take the library of HIERO rules and select 
the subset that fire on a given input sentence.  The 
module will use this subset of rules to determine 
the source translation order.  A parse-inspired 
reordering module will take an input parse tree 
and based on either a trained model or 
hand-written rules  decide the next source 
sequence to be translated.  As long as all the 
reordering modules are written to a common 
interface,  they can be separately written and 
maintained.   
Figure 1 shows an example of how three 
reordering modules can be incorporated into a 
single decoder.  The input source is S1?Sn.   
Module
skip = 2
window = 3
S1 S2 X ?> T1 T2 X
S1
S2 S3
Sn?1 Sn
.....
Distance?based
Reordering
S1 X Sn ?> Tn X T1 HIERO?based
Reordering
Parse?based
Reordering
S1
S2
S3
Sn
S1
S1
S2
Sn?1
Production
 
Figure 1.  Reordering module example 
62
 Each reordering module has its own resources 
and parameters which are shown on the left side.  
Each reordering module produces a vector of 
next source positions.  The production module 
takes these positions and produces translations 
for them.  
3  Reordering Modules 
In this section, we describe four reordering 
modules implementing different reordering 
strategies.  The framework is not limited to these 
four methods.  We present these four to 
demonstrate the ability of the framework to 
incorporate a wide variety of reordering methods. 
 
3.1 Distance-based Skip Reordering 
 
This is the type of reordering first presented by 
(Brown et.al. 1993) and was briefly alluded to in 
the above Introduction section.  This method is 
controlled by 2 parameters: 
Skip = number of words whose 
translations are to be delayed.  Let us call these 
words skipped words. 
WindowWidth (ww) = maximum 
number of words allowed to be translated before 
translating the skipped words. 
 
This reordering module outputs all the possible 
next source words to be translated according to 
these two parameters.  For illustration purposes, 
let us use a bit vector B to represent which source 
words have been translated.  Thus those that have 
been translated have value 1 in the bit vector, and 
those un-translated have 0.   As an example, let 
skip = 2 and ww = 3, and an input sentence of 
length = 10.  Initially, all 10 entries of B are 0.  At 
the first time step, only the following are possible 
next positions: 
a) 1 0 0 0 0 0 0 0 0 0 :  translate 1st word 
b) 0 1 0 0 0 0 0 0 0 0 :  skip 1st word 
c) 0 0 1 0 0 0 0 0 0 0 :  skip 1st and 2nd words 
 
At the next time step,  if  we want to continue the 
path of c),  we have these choices: 
1) we can leave the first 2 words open and 
continue until we reach 3 words (because ww=3) 
c1) 0 0 1 1 0 0 0 0 0 0  
c2) 0 0 1 1 1 0 0 0 0 0 
2) or we can go back and translate either of the 
first 2 skipped words: 
 c3) 1 0 1 0 0 0 0 0 0 0 
 c4) 0 1 1 0 0 0 0 0 0 0 
 
It is clear that the search space easily blows up 
with large skip and window-width values.  
Therefore, a beam pruning step is performed after 
partial hypotheses are produced at every time 
step.   
 
3.2 HIERO Hierarchical Reordering 
 
In this section we show an example of how the 
Hiero decoding method (Chiang 2005) can be 
implemented as a reordering module in this 
framework.  This is not meant to show that our 
MT decoder is a synchronous CFG parser.  This 
is a conceptual demonstration of how the Hiero 
rules can be used in a reordering module to 
decide the source translation order and thus used 
in a traditional phrase-based decoder.  This 
module uses the Hiero rules to determine the next 
source segment to be translated.  The example is 
Chinese-English translation. Consider the 
following Chinese sentence (word position and 
English gloss are shown in parentheses): 
 
(1.Australia) (2. is)  (3. with) 
(4. North Korea) 	(5. have)  
(6. diplomatic 
relation)  (7. NULL)  (8. few)  (9. 
country) (10. one of) 
 
Suppose we have two following Hiero rules: 
 X ? Australia X  (1) 
 X  ? is one of X   (2) 
 
The left-hand-side of Hiero rules are source 
phrases and the right-hand-side is their English 
translation and the Xs are the non-terminals 
whose extent is determined by the source input 
against which the rules are tested for matching.  
A rule fires if its left-hand-side matches certain 
segments of the input. 
 
Given the above Chinese input and the two Hiero 
rules, the Hiero decoder as described in (Chiang 
2005) will produce a partial hypothesis 
?Australia is one of? by firing the two rules 
during parsing (see Chiang 2005 for decoding 
details).  We will show how to decode in the 
Hiero paradigm using the framework. 
63
  
The reordering module first decides a source 
segment based on rule (1).  Rule (1) generates a 
sequence of source segments in term of source 
ranges: <[1,1],[2,10]>.  This means the source 
segment spanning range [1,1] (word 1, 
/Australia) is to be translated first, and then the 
remaining segment spanning range [2,10] is to be 
translated next.  This is exactly what rule (1) 
dictates where  corresponds to source 
[1,1] in the reordering module?s output and the X 
is [2,10].  The range [1,1], after being given to the 
production module,  results in the production of a 
partial hypothesis where the target ?Australia? is 
produced.  The task now is to translate the next 
source range [2,10].  At this point,  the reordering 
module generates another source segment 
according to rule (2) where the left-hand-side ? 
X ? is matched against the input and three 
corresponding source ranges are found which are 
[2,2] (/is), [4,9] (X), and [10,10] (/one of).  
According to rule (2), this source sequence is to 
be translated in the order of [2,2] (is), [10,10] 
(one of), and then [4,9] (X).  Therefore the output 
of the reordering module at this stage is 
<[2,2],[10,10],[4,9]>.  This would then go on to 
be translated and results in a partial hypothesis to 
?Australia is one of?.  Thus ?Australia is one of? 
is a partial production which covers source 
segments [1,1] [2,2] and [10,10] in that order.  
Note that the source segments decoded so far are 
not contiguous and this is the effect of long-range 
reordering imposed by rule (2).  The next stage is 
<[4,9]> which is what the X in rule (2) 
corresponds to.  From here onwards, other rules 
will fire and the decoding sequence these rules 
dictate will be realized by the reordering module 
in the form of source ranges.  This process can 
also be viewed hierarchically in Figure 2. 
 
In Figure 2 the ranges (the bracketed numbers) 
are source segments and the leaves are English 
productions.  Initially we have the whole input 
sentence as one range [1,10].  According to rule 
(1), this initial range is refined to be 
<[1,1],[2,10]>,  the 2nd level in Figure 2.  The 
[2,10] is further refined by rule (2)  to generate  
the 3rd level ranges <[2,2],[10,10],[4,9]> and the 
process goes on.  Ranges that cannot be further 
refined go into the production module which 
 ...
[1,10]
[1,1]
Australia
[2,10]
[2,2] [10,10] [4,9]
is one of
 
 
Figure 2. Hiero-style decoding  
 
generates partial hypotheses which are the leaves 
in the figure.  In other words, the partial 
hypotheses are generated by traversing the tree in 
Figure 2 in a left-to-right depth-first fashion. 
 
3.3 Generalized Part-Of-Speech-based 
Reordering 
 
The aim of a generalized part-of-speech-based 
reordering method is to tackle the problem of 
long-range word movement.  Chinese is a 
pre-modification language in which the modifiers 
precede the head.  The following is an example 
with English gloss   in parentheses.  The 
prepositional modifier ?on the table'' follows the 
head ?the book'' in English (3.3b), but precedes it 
in Chinese (3.3a).  When the modifiers are long, 
word-based local reordering is inadequate to 
handle the movement. 
3.3a.  (table)  (on)  (NULL) (book) 
 3.3b.  the book on the table 
 
There have been several approaches to the 
problem some of which are mentioned in ?1.  
Compared to these methods, this approach is 
lightweight in that it requires only part-of-speech  
(POS) tagging on the source side. The idea is to 
capture general long-distance distortion 
phenomena by extracting reordering patterns 
using a mixture of words and part-of-speech tags 
on the source side.  The reordering patterns are 
extracted for every contiguously aligned source 
segment in the following form:  
source  sequence ? target sequence 
 
 
Both the source sequence and the target  
sequence are expressed using a combination of 
source words and POS tags.  The patterns are 
?generalized? not only because POS tags are used 
but also because variables or place-holders are 
64
 allowed.  Given a pair of source and target 
training sentences, their word alignments and 
POS tags on the source, we look for any 
contiguously aligned source segment and extract 
word reordering patterns around it.  Figure 3 
shows an example.   
 
Shown in Figure 3 are a pair of Chinese and 
English sentence, the Chinese POS tags and the 
word alignment indicated by the lines.  When 
multiple English words  are aligned to a single 
Chinese word, they are grouped by a rectangle for 
easy viewing.  Here we have a contiguously 
aligned source segment from position 3 to 8. 
Using the range notation, we say that source 
range [3,8] is aligned to target range [6, 14].  Let 
X denote the source  segment [3,8].   The source 
verb phrase (at positions 9 and 10) occur after X 
whereas the corresponding target verb phrase 
(target words 2,3, and 4) occur before the 
translation of X (which is target [6,14]). We thus 
extract the following pattern: 
  X V N ? V N  X       (1) 
 
where the left-hand side ? X V N? is the source 
word sequence and the right-hand side ?V N  X? 
is the target word sequence.   The X  in the pattern 
is meant to represent a variable, to be matched by 
a sequence of source words in the test data when 
this pattern fires during decoding.  Note that the 
pattern is a mixture of words and POS tags.  
Specifically, the word identity of the preposition 
 (position 2) is retained whereas the content 
words (the verb and the noun) are substituted by 
their POS tags.  This is because in general, for the 
reordering purpose the POS tags are good class 
representations for content words whereas 
different prepositions may have different word 
order patterns so that mapping them all to a single 
POS P masks the difference. Examples of 
patterns are shown in Table 1. 
 
In Chinese-English translation, the majority of 
the reorderings occur around verb modifiers 
(prepositions) and noun modifiers (usually 
around the Chinese part-of-speech DEG as in 
position 6).   Therefore we choose to extract only 
these 2 kinds of patterns that involve a 
preposition and/or a DEG.  In the example above, 
there are only 2 such patterns: 
    X V N ? V N  X              (1) 
X1 DEG X2 ? X2 DEG X1           (2)     
 
Figure 3. Chinese/English Alignment Example 
 
 
 
 Source Seq. Target Seq. Count P(tseq 
|sseq) 
1 X DEG NN X DEG NN 861 0.198 
2 X DEG NN X NN DEG 1322 0.305 
3 X DEG NN NN DEG X 2070 0.477 
4 X DEG NN NN X DEG 10 0.002 
5 X DEG NN DEG NN X 52 0.012 
6 X DEG NN DEG X NN 22 0.005 
7  X VV  X VV 15 0.118 
8  X VV VV  X 112 0.882 
9 X VV VV  X 2 0.041 
10 X VV X VV 47 0.959 
 
Table 1. Pattern examples 
 
 
In the table, we see that when the preposition is 
  (rows 7 and 8, translation: by), then the 
swapping is more likely (0.882 in row 8).  When 
the preposition is  (rows 9 and 10 translation: 
because), then the target most often stays the 
same order as the source (prob 0.959, last row). 
 
3.4 Parse-based Lexicalized Reordering 
  
Part-of-speech reordering patterns as described in 
?3.3 are crude approximation to the structure of 
the source sentence.  For example, in the source 
pattern ?X DEG NN?, the variable X can match a 
source segment of arbitrary length which is 
followed by ?DEG NN?.  Although it does 
capture very long range movement as a result of 
SrcPOS  Source              Target 
 
1.NNP Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 849?857,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
A Direct Syntax-Driven Reordering Model for Phrase-Based Machine 
Translation 
 
Niyu Ge 
IBM T.J.Watson Research 
Yorktown Heights, NY 10598 
niyuge@us.ibm.com 
 
Abstract 
This paper presents a direct word reordering 
model with novel syntax-based features for sta-
tistical machine translation.  Reordering models 
address the problem of reordering source lan-
guage into the word order of the target language.  
IBM Models 3 through 5 have reordering com-
ponents that use surface word information but 
very  little context information to determine the 
traversal order of the source sentence.  Since the 
late 1990s, phrase-based machine translation 
solves much of the local reorderings by using 
phrasal translations.  The problem of long-
distance reordering has become a central re-
search topic in modeling distortions.  We present 
a syntax driven maximum entropy reordering 
model that directly predicts the source traversal 
order and is able to model arbitrarily long dis-
tance word movement.  We show that this model 
significantly improves machine translation qual-
ity. 
1    Introduction 
Machine translation reordering models model the 
problem of the word order when translating a 
source language into a target language.  For exam-
ple in Spanish and Arabic, adjectives often come 
after the nouns they modify whereas in English 
modifying adjectives usually precede the nouns.  
When translating Spanish or Arabic into English, 
the position of the adjectives need to be properly 
reordered to be placed before the nouns to make 
fluent English.   
In this paper, we present a word reordering model 
that models the word reordering process in transla-
tion.  The paper is organized as follows.  ?2 out-
lines previous approaches to reordering.  ?3 details 
our model and its training and decoding process.  
?4 discusses experiments to evaluate the model 
and ?5 presents machine translation results.  ?6 is 
discussion and conclusion. 
2    Previous Work 
The word reordering problem has been one of the 
major problems in statistical machine translation 
(SMT).   Since exploring all possible reorderings 
of a source sentence is an NP-complete problem 
(Knight 1999), SMT systems limit words to be re-
ordered within a window of length k.  IBM Models 
3 through 5 (Brown et.al. 1993) model reorderings 
based on surface word information.  For example, 
Model 4 attempts to assign target-language posi-
tions to source-language words by modeling d(j | i, 
l, m) where j is the target-language position, i is the 
source-language position, l and m are respectively 
source and target sentence lengths.  These models 
are not effective in modeling reorderings because 
they don?t have enough context and lack structural 
information.   
 
Phrase-based SMT systems such as (Koehn et.al. 
2003) move from using words as translation units 
to using phrases.  One of the advantages of phrase-
based SMT systems is that local reorderings are 
inherent in the phrase translations.  However, 
phrase-based SMT systems capture reordering in-
stances and not reordering phenomena.  For exam-
ple, if the Arabic phrase ?the car red?  and its 
English translation ?the red car? is seen in the 
training data, phrase-based SMT is able to produce 
the correct English for the Arabic ?the car red?.  
However it will not be able to produce ?the blue 
car? for the Arabic ?the car blue? if the training 
data does not contain this phrase pair.  Phrases do 
not capture the phenomenon that Arabic adjectives 
and nouns need to be reordered.  Another problem 
with phrase-based SMT is the problem of long-
range reorderings.  Recent work on reordering has 
been focusing on capturing general reordering 
849
phenomena (as opposed to instances) and on solv-
ing long-range reordering problems.   
(Al-onaizan et.al. 2006) proposes 3 distor-
tion models, the inbound, outbound, and pair mod-
els.  They together model the likelihood of 
translating a source word at position i given that 
the source word at position j has just been trans-
lated.  These models perform better than n-gram 
based language models but are limited in their use 
of only  the surface strings.   
Instead of directly modeling the distance 
of word movement, phrasal level reordering mod-
els model how to move phrases,  also called orien-
tations.  Orientations typically apply to adjacent 
phrases.  Two adjacent phrases can be either 
placed monotonically (sometimes called straight) 
or swapped (non-monotonically or inverted).  
Early orientation models do not use lexical con-
tents such as (Zens et. al., 2004).  More recently, 
(Xiong et.al. 2006;  Zens 2006; Och et. al, 2004; 
Tillmann, 2004;  Kumar et al, 2005, Ni et al, 
2009) all presented models that use lexical features 
from the phrases to predict their orientations.   
These models are very powerful in predicting local 
phrase placements.  More recently (Galley et.al. 
2008) introduced a hierarchical orientation model 
that captures some non-local phrase reorderings by 
a shift reduce algorithm.  Because of the heavy use 
of lexical features, these models tend to suffer 
from data sparseness problems.  Another limitation 
is that these models are restricted to reorderings 
with no gaps and phrases that are adjacent. 
We present a probabilistic reordering model 
that models directly the source translation se-
quence and explicitly assigns probabilities to the 
reorderings of the source input with no restrictions 
on gap, length or adjacency.   This is different from 
the approaches of pre-order such as (Xia and 
McCord 2004; Collins et.al. 2005; Kanthak et. al. 
2005; Li et. al., 2007).   Although our model can 
be used to produce top N pre-ordered source, the 
experiments reported here do not use the model in 
the pre-order mode.  Instead, the reordering model 
is used to generate a reorder lattice which encodes 
many reorderings and their costs (negative log 
probability).  This reorder lattice is independent of 
the translation decoder.  In principle, any decoder 
can use this lattice for its reordering needs.  We 
have integrated the reorder lattice into a phrase-
based.  The experiments reported here are from the 
phrase-based decoder. 
We present the reordering model based on 
maximum entropy models.  We then describe the 
syntactic features in the context of Chinese to Eng-
lish translation. 
3    Maximum Entropy Reordering Model 
The model takes a source sequence of length n: 
 ],...,[ 21 nsssS =  
and models its translation or visit order according 
to the target language: 
],...,[ 21 nvvvV =  
where vj is the source position for target position j.  
For example, if the 2nd source word is to be trans-
lated first, then v1 = 2.  We find V such that 
)2(),|(max
)1()|(maxarg
1
1...1
}{
?
=
?
?
=
n
j
jj
V
vvSvp
SVp
?
 
In equation (1) {? } is the set of possible visit or-
ders.  We want to find a visit order V such that the 
probability p(V|S) is maximized.  Equation (2) is a 
component-wise decomposition of (1).   
 
Let 
)...,( 11 ?== jj vvShandvf  
We use the maximum entropy model to estimate 
equation (2): 
?=
k
kk hfhZhfp )3()),(exp()(
1)|( ??  
 
where Z(h) is the normalization constant 
)4(),(exp)( ? ?=
f k
kk hfhZ ??  
 
In equation (3), ?k(f, h) are binary-valued features.  
During training, instead of exploring all possible 
permutations,  samples are drawn given the correct 
path only. 
3.1   Feature Overview 
Most of our features ?k(f, h) are syntax-based.  
They examine how each parse node is reordered 
during translation.  We also have a few non-syntax 
features that inspect the surface words and part-of-
speech tags.  They complement syntax features by 
capturing lexical dependencies and guarding 
against parsing errors.  Instead of directly model-
850
 Step:                  1  2  3   4  5  6  7  8  9  10  11   
Visit Sequence:   1  9  10 2  8  7  6  3  4   5   11 
 
Figure 1.  A Chinese-English Parallel Sentence with Chinese Parse 
 
   
ing the absolute source position vj, we model the 
jump from the last source position vj-1.  All features 
share two common components: j (for jump), and 
cov (for coverage).  Jumps are bucketed and 
capped at 4 to prevent data sparsity.  Coverage is 
an integer indicating the visiting status of the 
words between the jump.  Coverage is 0 if none of 
the words was visited prior to this step, 1 if all 
were visited, and 2 if some but not all were visited.  
(j, cov) are present in all features and are removed 
from the descriptions below.  A couple of features 
use a variation of Jump and Coverage.  These will 
be described in the feature description. 
3.2    Parse-based Syntax Features 
We use the sentence pair in Figure 1. as a work-
ing example when describing the features.  Shown 
in the figure are a Chinese-English parallel sen-
tence pair, the word alignments between them, and 
the Chinese parse tree.   The parse tree is simpli-
fied.  Some details such as part-of-speech tags are 
omitted and denoted by triangles.  The first step is 
to determine the source visit sequence from the 
word alignment, also shown at the bottom of Fig-
ure 1.  If a target is aligned to more than one 
source, we assume the visit order is left to right.   
In Figure 1, source words 2 and 8 are aligned to the 
English ?at? and we define the visit sequence to be 
8 following 2. 
Chinese and  English differ in the positioning of 
the modifiers.  In English, non-adjectival modifiers 
follow the object they modify.  This is most 
prominent in the use of relative clauses and prepo-
sitional phrases.  Chinese in contrast is a pre-
modification language where modifiers whether 
adjectival, clausal or prepositional typically pre-
cede the object they modify.  In Figure 1.,  the 
Chinese prepositional phrase PP (in lightly shaded 
box in the parse tree) spanning range  [2,8] pre-
cedes the verb phrase VP2 at positions [9,10].  
These two phrases are swapped in English as 
shown by the two lightly shaded boxes in the 
alignment grid.  The relative clause CP (in dark 
shaded box in the parse tree) in Chinese spanning 
range [3,6] precedes the noun phrase NP3 at posi-
tion 7 whereas these two phrases are again 
swapped in English.  
851
The phenomenon for the reordering model to 
capture is that node VP1?s two children PP and 
VP2 (lightly shaded) need to be swapped regard-
less of how long the PP phrase is.  This is also true 
for node NP2 whose two children CP and NP3 
(dark shaded) need to be reversed. 
Parse-based features model how to reorder the 
constituents in the parse by learning how to walk 
the parse nodes.  For every non-unary node in the 
parse we learn such features as which of its child is 
visited first and for subsequent visits how to jump 
from one child to another.  For the treelet VP1  
PP VP2 in Figure 1, we learn to visit the child VP2 
first, then PP.  
We now define the notion of ?node visit?.  When 
a source word si is visited at step j, we find its path 
to root from the  leaf node denoted as PathToRooti.  
We say all the nodes contained in PathToRooti are 
being visited at that step.  Parse-based features are 
applied to every qualifying node in PathToRooti.  
Unary extensions do not qualify and are ignored.  
Since part-of-speech tags are unary branches, 
parse-based features apply from the lowest-level 
labels.  Another condition depends on the jump 
and is discussed in section ?3.4.  All our features 
are encoded by a vector of integers and are denoted 
as ? (?) in this paper.  We now describe the fea-
tures. 
3.2.1   First Child Features 
The first-child feature applies when a node is vis-
ited for the first time.  The feature learns which of 
the node?s child to visit first.   This feature learns 
such phenomena as translating the main verb first 
under a VP or translating the main NP first under 
an NP.  The feature is defined as ?(currentLabel, 
parentLabel, nthNode, j, cov) where 
currentLabel = label of the current parse node 
parentLabel = label of the parent node 
nthNode = an integer indicating the nth occurrence 
of the current node 
   In Figure 1, when source word 9 is visited at step 
2, its PathToRoot
 
is computed which is [VP2, VP1, 
IP1]. The first-child feature applied to VP2 is  
?(VP2, VP1, 1, 4, 1) since 
currentLabel = VP2;  parentLabel = VP1; 
nthChild = 1: VP2 is the 1st VP among its parent?s 
children 
j = 4: actual jump from 1 is 8 and is capped. 
cov = 0: words in between the jump [1,9] are not 
yet visited at this step. 
The semantics of this feature is that when a VP 
node is visited, the first VP child under it is visited 
first.  This feature learns to visit the first VP first 
which is usually the head VP no matter where it is 
positioned or how many modifiers precede it. 
 
3.2.2   Node Jump Features 
This feature applies on all subsequent visits to the 
parse node.  This feature models how to jump from 
one sibling to another sibling.  This feature has 
these components: ?(currentLabel, parentLabel, 
fromLable,  nodeJump,cov) where  
fromLabel = the node label where the jump is from 
nodeJump = node distance from that node 
This feature effectively captures syntactic reorder-
ings by looking at the node jump instead of surface 
distance jump.  In our example, a node-jump fea-
ture for jumping from source 10 to 2 at step 4 at 
VP1 level is ?(PP, VP1, VP2,  -1, 2) where 
currentLabel = PP where source word 2 is under 
parentLabel = VP1 
fromLabel = VP2 where source word 10 is under  
nodeJump = -1 since the jump is from VP2 to PP  
cov = 2 because in between [2,10] word 9 has been 
visited and other words have not.   
This feature captures the necessary information 
for the ?PP VP? reorderings regardless of how long 
the PP or VP phrase is.   
3.2.3   Jump Over Sibling Features 
To make a correct jump from one sibling to the 
other, siblings that are jumped over should also be 
considered.  For example in Chinese, while jump-
ing over a PP to cover a VP is a good jump, jump-
ing over an ADVP to cover a VP may not be 
because adverbs in both Chinese and English often 
precede the verb they modify.  The jump-over-
sibling features help distinguish these cases.  This 
feature?s components are ?(currentLabel, parent-
Label, jumpOverSibling, siblingCov, j) where jum-
pOverSibling is the label of the sibling that is 
jumped over and siblingCov is the coverage status 
of that sibling. 
   This feature applies to every sibling that is 
jumped over.  At step 2 where the jump is from 
source 1 to 9, this feature at VP1 level is ?(VP2, 
VP1, PP, 0, 4) because PP is a sibling of VP2 and 
852
is jumped over, PP is not covered at this step, and 
the jump is capped to be 4.   
3.2.4   Back Jump Sibling Features 
For every forward jump of length greater than 1, 
there is a backward jump to cover those words that 
were skipped.  In these situations we want to know 
how far we can move forward before we must 
jump backward.  The back-jump-sibling feature 
applies when the jump is backward (distance is 
negative) and inspects the sibling to the right.  It 
generates ?(currentLabel,  rightSiblingCov, j).  
When jumping from 10 to 2 at step 4, this feature 
is ?(PP, 1, -4) where -4 is the jump and 
currentLabel = PP where source word 2 is under 
rightSiblingCoverage = 1 since VP2 has been 
completed visited at this time.  This feature learns 
to go back to PP when its right sibling (VP2) is 
completed. 
     
3.2.5    Broken Features 
Translations do not always respect the constituent 
boundaries defined by the source parse tree.  Con-
sider the fragment in Figure 2.   
 
Figure 2. A ?Broken? Tree 
 
After the VV under VP2 is translated (?account 
for?),  a transition is made to translate the ADVP 
(?approximately?) leaving VP2 partially translated.  
We say that the node VP2 is  broken at this step.  
This type of feature has been shown to be useful 
for machine translation (Marton & Resnik 2008).   
Here, broken features model the context under 
which a node is broken by observing the feature 
?(curTag, prevTag, parentLabel, j, cov).  For the 
transition of source word 2 to source word 1 in 
Figure 2, a broken feature applies at VP2: ?(AD, 
VV, VP2, -1 ,1).  This feature learns that a VP can 
be broken when making a jump from a verb (VV) 
to an adverb (AD). 
 
3.3    Non-Parse Features 
Non-parse features do not use or use less fine-
grained information from the parse tree.   
3.3.1   Barrier Features 
Barrier features model the intuition that certain 
words such as punctuation should not move freely.  
This phenomenon has been observed and shown to 
be helpful in (Xiong et. al., 2008).  We call these 
words barrier words.  Barrier features are ?(barri-
erWord, cov, j).  All punctuations are barrier 
words.   
3.3.2    Number of Zero Islands Features 
Although word reorderings can involve words 
far apart, certain jump patterns are highly unlikely.  
For example, the coverage pattern ?1010101010? 
where every other source word is translated would 
be very improbable.  Let the right most covered 
source word be the frontier.  For every jump, the 
number-of-zero-islands feature computes the num-
ber of uncovered source islands to the left of the 
frontier.  Additionally it takes into account the 
number of parse nodes in between.  This feature is 
defined as ?(numZeroIslands, j, num-
ParseNodesInBetween).  The number of parse 
nodes is the number of maximum spanning nodes 
in between the jump.  The jump at step 2 from 
source 1 to 9 triggers this number-zero-island fea-
ture ?(1, 4, 1). The source coverage status at step 2 
is 10000000100 because the first source word has 
been visited and the current visit is source 9.  All 
words in between have not been visited.  There is 1 
contiguous sequence of 0?s between the first ?1? 
and the last ?1?, hence the numZeroIslands = 1.  
There is one parse node PP that spans all the 
source words from 2 to 8, therefore the last argu-
ment to the feature is 1.  If instead, the transition 
was from source 1 to 8, then there would be 2 
maximum spanning parse nodes for source [2,7] 
which are nodes P and NP2. The feature would be 
?(1, 4, 2).  This feature discourages scattered 
jumps that leave lots of zero islands and jump over 
lots of parse nodes. 
  
3.4    Training 
Training the maximum entropy reordering model 
needs word alignments and source-side parses.  We 
use hand alignments from LDC.  The training data 
853
statistics are shown in Table 1.  We use the (Levy 
and Manning 2003) parser on Chinese. 
Data #Sentences #Words 
LDC2006E93 10,408 230,764 
LDC2008E57 11,463 194,024 
Table 1.  Training Data 
 
From the word alignments we first determine the 
source visit sequence.  Table 2 details how the visit 
sequence is determined in various cases. 
Alignment Type S-T Visit Sequence 
1-1 Left to right from target 
m-1 Left to right from source 
1-m Left most target link 
? Attaches left  
Table 2.  Determining visit sequence 
 
The first column shows alignment type from 
source (S) to target (T).  1-1 means one source 
word aligns to one target word.  m-1 means many 
source words align to one target and vice versa.  ? 
means unaligned source words.    
 
After the source visit sequence is decided, fea-
tures are generated.  Note that the height of the tree 
is not uniform for all the words.  To preserve the 
structure and also alleviate the depth problem, we 
use the lowest-level-common-ancestor approach.  
For every jump, we generate features bottom up 
until we reach the node that is the common ances-
tor of the origin and the destination of the jump.  In 
Figure 1 there is a jump from source 7 to 6 at step 
7.  The lowest-level-common-ancestor for source 6 
and 7 is the node NP2 and features are generated 
up to the level of NP2.  Features on this training 
data are shown in the second column in Table 5. 
The MaxEnt model on this data is efficiently 
trained at 15 minutes per iteration (24 sen-
tences/sec or 471 words/sec). 
 
4   Experiments  
4.1   Reorder Evaluation  
To evaluate how accurate the reordering model is, 
we first compute its prediction accuracy.  We 
choose the first 100 sentences from NIST MT03 as 
our test set for this evaluation.  We manually word 
align them to the first set of reference using LDC 
annotation guidelines version 1.0 of April 2006.   
An average of 73% of the training sentences con-
tain unaligned source words and over 87% of the 
test sentences contain unaligned source words.  
The unaligned source words are mostly function 
words. Because the visit sequence of unaligned 
source words are determined not by truth but by 
heuristics (Table 2), they pose a problem in evalua-
tion.   
We thus evaluate the model by measuring the ac-
curacy of its decision conditioned on true history.  
We measure performance on the model?s top-N 
choices for N = 1,2, and 3.  Results are in Table 3.  
The table also shows the accuracy of  no reorder-
ing in the Monotone column. 
Top-N Accuracy Monotone 
1 80.56% 65.39% 
2 90.66% - 
3 93.05% - 
Table 3. Reordering model performance 
 
Figure 3 plots accuracy vs. MaxEnt training itera-
tion.  Accuracy starts low at 74.7% and reaches is 
highest at iteration 8 and fluctuates around 80.5% 
thereafter. 
71
72
73
74
75
76
77
78
79
80
81
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
 
Figure 3.  Accuracy vs.  MaxEnt Training Iteration 
 
We analyze 50 errors from the top-1 run.  The er-
rors are categorized and shown in Table 4. 
Error Category Percentage 
Lexical 34% 
Parse 30% 
Model 20% 
Reference 16% 
Table 4.  Error Analysis 
 
?Lexical? errors are those that rise from lexical 
choice of source words.  For example, an ?ADVP 
VP? structure would normally be visited mono-
tonically.  However, in case of  Chinese  phrase ?so 
do?, they should be swapped.  More than a third of 
the errors are of this nature.   Errors in the Refer-
854
ence category are those that are marked wrong be-
cause of the particular English reference.  The pro-
posed reorderings are correct but they don?t match 
the reference reorderings.  Another 30% of the er-
rors are due to parsing errors.  The Model errors 
are due to two sources.  One is the depth problem 
mentioned above.  Local statistics for some very 
deep treelets overwhelm the global statistics and 
local jumps win over the long jumps in these cases.  
Another problem is the data sparseness.  For ex-
ample, the model has learned to reorder the ?PP 
VP? structure but there is not much data for ?PP 
ADVP VP?.  The model fails to jump over PP into 
ADVP.   
4.2    Feature Utility  
We conduct ablation studies to see the utilities of 
each feature.  We take the best feature set which 
gives the performance in Table 3 and takes away 
one feature type at a time.   The results are in Table 
5.  The first row keeps all the features.  The Sub-
tract column shows performance after subtracting 
each feature while keeping all the other features.  
The Add column shows performance of adding the 
feature.  Using just first-child features gets 
75.97%.  Adding node-jump features moves the 
accuracy to 78.40% and so on. 
Features #Features Sub-
tract 
Add 
-  80.56% - 
First Child  7,559 79.87% 75.97% 
Node Jump  6,334 79.52% 78.40% 
JumpOver Sib.  2,403 80.52% 79.00% 
BackJump  602 80.48% 79.05% 
Broken  15,183 80.30% 79.13% 
Barrier  158 80.26% 79.22% 
NumZ Islands 200 79.52% 80.56% 
Table 5. Ablation study on features 
5    Translation Experiments  
5.1    Reorder Lattice Generation  
The reordering model is used to generate reorder 
lattices which are used by machine translation de-
coders.  Reorder lattices have been frequently used 
in decoding in works such as (Zhang et. al 2007, 
Kumar et.al. 2005, Hildebrand et.al. 2008), to 
name just a few.  The main difference here is that 
our lattices encode probabilities from the reorder-
ing model and are not used to preorder the source.  
The lattice contains reorderings and their cost 
(negative log probability).  Figure 4 shows a reor-
der lattice example.  Nodes are lattice states. Arcs 
store source word positions to be visited (trans-
lated) and their cost and they are delimited by 
comma in the figure.  Lower cost indicates better 
choice.  Figure 4 is much simplified for readability.  
It shows only the best path (highlighted) and a few 
neighboring arcs.  For example, it shows source 
words 1, 2, and 8 are the top 3 choices at step 1.  
Position 1 is the best choice with the lowest cost of 
0.302 and so on.   
 
Figure 4. A lattice example 
 
The sentence is shown at the bottom of the figure.  
The first part of the reference (true) path is indi-
cated by the alignment which is source sequence 1, 
8, 9, and 2.  We see that this matches the lattice?s 
top-1 choice.   
Lattice generation takes source sentence and 
source parse as input.  The lattice generation proc-
ess makes use of a beam search algorithm.  Every 
node in the lattice generates top-N next possible 
positions and the rest is pruned away.  A coverage 
vector is maintained on each path to ensure each 
source word is visited exactly once.     A wide 
beam width explores many source positions at any 
step and results in a bushy lattice.  This is needed 
for machine translation because the parses are er-
rorful. The structures that are hard for MT to reor-
der are also hard for parsers to parse.  Labels criti-
cal to reordering such as CP are among the least 
accurate labels.  Overall parsing accuracy is 
83.63% but CP accuracy is 73.11%.  We need a 
wide beam to include more long jumps to compen-
sate the parsing errors.  
5.2    Machine Translation  
We run MT experiments on NIST Chinese-English 
test sets MT03-05.  We compare the performance 
855
of using distance-based reordering and using maxi-
mum entropy reordering lattices.  The decoder is a 
log-linear phrase based decoder.  Translation mod-
els are trained from HMM alignments.  A 
smoothed 5-gram English LM is built on the Eng-
lish Gigaword corpus and English side of the Chi-
nese-English parallel corpora.   In the experiments, 
lexicalized distance-based reordering allows up to 
9 words to be jumped over.  MT performance is 
measured by BLEUr4n4 (Papineni et.al. 2001). 
The test set statistics and experiment results are 
show in Table 6.  Decoding with MaxEnt reorder 
lattices shows significant improvement for all con-
ditions.   
 
Data #Segs Lex 
Skip-9 
Reord   Lattice Gain 
MT03 919 0.3005 0.3315 +3.1 
MT04 1788 0.3250 0.3388 +1.38 
MT05 1082 0.2957 0.3236 +2.79 
Table 6. MT results 
 
Figures 5 shows an example from MT output 
with word alignments to the Chinese input. The 
MaxEnt reordering model correctly reorders two 
source modifiers at source positions 8 and 22.  The 
Skip9 output reorders locally whereas the MaxEnt 
lattice output shows much more complex reorder-
ings.  
 
6    Conclusions  
We present a direct syntax-based reordering model 
that captures source structural information.   The 
model is capable of handling reorderings of arbi-
trary length.  Long-range reorderings are essential 
in translation between languages with great word 
order differences such as Chinese-English and 
Arabic-English.  We have shown that phrase based 
SMT can benefit significantly from such a reorder-
ing model.   
The current model is not regularized and feature 
selection by thresholding the feature counts is quite 
primitive.  Regularizing the model will prevent 
overfitting, especially given the small training data 
set.  Regularization will also make the ablation 
study more meaningful. 
The reordering model presented here aims at 
capturing structural differences between source 
and target languages.  It does not have enough 
lexical features to deal with lexical idiosyncrasies.   
     
ME Lattice MT               Skip9 MT 
Figure 5.  MT comparison 
 
Our initial attempt at adding lexical pair jump fea-
tures ?(fromWord, toWord, j) has not proved use-
ful.  It hurt accuracy by 3% (from 80% to 77%).  
We see from Table 4 that 34% of the errors are due 
to source lexical choices which indicates the weak-
ness of the current lexical features.  Regularization 
of the model might also make a difference with the 
lexical features. 
Reordering and word choice in translation are not 
independent of each other.  We have shown some 
initial success with a separate reordering model.  In 
the future, we will build joint models on reordering 
and translation.  This approach will also address 
some of the reordering problems due to source 
lexical idiosyncrasies. 
7   Acknowledgement  
We would like to acknowledge the support of 
DARPA under Grant HR0011-08-C-0110 for fund-
ing part of this work.  The views, opinions, and/or 
findings contained in this article are those of the 
author and should not be interpreted as represent-
ing the official views or policies, either expressed 
or implied, of the Defense Advanced Research 
Projects Agency or the Department of Defense. 
References  
856
A.S.Hildebrand, K.Rottmann, Mohamed Noamany, Qin 
Gao, S. Hewavitharana, N. Bach and Stephan Voga.  
2008.  Recent Improvements in the CMU Large Scale 
Chinese-English SMT System.  In Proceedings of 
ACL 2008 (Short Papers) 
C. Wang, M. Collins, and Philipp Koehn.  2007.  Chi-
nese Syntactic Reordering for Statistical Machine 
Translation.  In Proceedings of EMNLP 2007 
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou, 
Minghui Li, and Yi Guan.  2007.  A Probabilistic 
Approach to syntax-based Reordering for Statistical 
Machine Translation.  In Proceedings of ACL 2007. 
Christoph Tillmannn.  2004.  A Block Orientation 
Model for Statistical Machine Translation.  In Pro-
ceedings of HLT-NAACL 2004. 
David Chiang.  2005.  A Hierarchical Phrase-based 
Model for Statistical Machine Translation.  In Pro-
ceedings of ACL 2005. 
Dekai Wu.  1997.  Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Cor-
pora.  Compuntational Linguistics, Vol. 23, pp 377-
404 
Deyi Xiong, Qun Liu, and Shouxun Lin.  2006.  Maxi-
mum Entropy Based Phrase Reordering Model for 
Statistical Machine Translation.  In Proceedings of 
ACL 2006. 
Deyi Xiong, Min Zhang, Aiti Aw, Haitao Mi, Qun Liu 
and Shouxun Lin.  2008.  Refinements in FTG-based 
Statistical Machine Translation.  In Proceedings of 
ICJNLP 2008 
Dongdong Zhang, Mu Li, Chi-Ho Li, and Ming Zhou. 
2007.  Phrase Reordering Model Integrating Syntac-
tic Knowledge for SMT.  In Proceedings of EMNLP 
2007 
Fei Xia and Michael McCord.  2004.  Improving a Sta-
tistical MT System with Automatically Learned Re-
write Patterns.  In Proceedings of COLING 2004. 
Franz Josef Och and Hermann Ney.  2004.  The Align-
ment Template Approach to Statistical Machine 
Translation.  Computational Linguistics, Vol. 30(4).  
pp. 417-449 
Kenji Yamada and Kevin Knight 2001.  A Syntax-based 
Statistical Translation Model.  In Proceedings of 
ACL 2001 
Kevin Knight.  1999.  Decoding Complexity in Word 
Replacement Translation Models.  Computational 
Linguistics, 25(4):607-615 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu.  2001.  A Method for Automatic Evaluation 
for MT.  In Proceedings of ACL 2001 
Michael Collins, Philipp Koehn, and Ivona Kucerova.   
2005.  Clause Restructuring for Statistical Machine 
Translation.  In Proceedings of ACL 2005. 
Michell Galley, Christoph D. Manning.  2008.  A Simple 
and Effective Hierarchical Phrase Reordering 
Model.  Proceedings of the EMNLP 2008 
Perter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer.  1993.  The 
Mathematics of Statistical Machine Translation.  
Computation Linguistics, 19(2). 
Philip Koehn, Franz Josef Och, and Daniel Marcu.  
2003.  Statistical Phrase-based Translation.  In Pro-
ceedings of NLT/NAACL 2003. 
Richard Zens, Hermann Ney, Taro Watanabe, and Eii-
chiro Sumita.  2004.  Reordering Constraints for 
Phrase-based Statistical Machine Translation.  In 
Proceedings of COLING 2004. 
Richard Zens and Hermann Ney.  2006. Discriminative 
Reordering Models for Statistical Machine Transla-
tion.  In Proceedings of the Workshop on Statistical 
Machine Translation, 2006. 
Roger Levy and Christoph Manning.  2003.  Is it harder 
to parse Chinese, or the Chinese Treebank?  In Pro-
ceedings of ACL 2003 
 Shankar Kumar and William Byrne.  2005.  Local 
Phrase Roerdering Models for Statistical Machine 
Translation.  In Proceedings of  HLT/EMNLP 2005 
Stephan Kanthak, David Vilar, Evgeny Matusov, Rich-
ard Zens, and Hermann Ney.  2005.  Novel Reorder-
ing Approaches in Phrase-based Statistical Machine 
Translation.  In Proceedings of the Workshop on 
Building and Using Parallel Texts 2005. 
Y. Al-Onaizan . K. 2006  Distortion Models for Statisti-
cal Machine Translation.  In Proceedings of ACL 
2006. 
Yizhao Ni, C.J.Saunders, S. Szedmak and M.Niranjan 
2009 Handling phrase reorderings for machine 
translation.  In Proceedings of ACL2009 
Yuqi Zhang, Richard Zens, and Hermann Ney.  2007.  
Improved Chunk-level Reordering for Statistical Ma-
chine Translation.  In Proceedings of HLT/NAACL 
2007. 
Yuval Marton and Philip Resnik.  2008.  Soft Syntactic 
Constraints for Hierarchical Phrased-based Transla-
tion.  In Proceedings of ACL 2008.  
857
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 61?69,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Improving Reordering for Statistical Machine Translation with Smoothed
Priors and Syntactic Features
Bing Xiang, Niyu Ge, and Abraham Ittycheriah
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598
{bxiang,niyuge,abei}@us.ibm.com
Abstract
In this paper we propose several novel ap-
proaches to improve phrase reordering for
statistical machine translation in the frame-
work of maximum-entropy-based modeling.
A smoothed prior probability is introduced to
take into account the distortion effect in the
priors. In addition to that we propose multi-
ple novel distortion features based on syntac-
tic parsing. A new metric is also introduced to
measure the effect of distortion in the transla-
tion hypotheses. We show that both smoothed
priors and syntax-based features help to sig-
nificantly improve the reordering and hence
the translation performance on a large-scale
Chinese-to-English machine translation task.
1 Introduction
Over the past decade, statistical machine translation
(SMT) has evolved into an attractive area in natural
language processing. SMT takes a source sequence,
S = [s1 s2 . . . sK ] from the source language, and
generates a target sequence, T ? = [t1 t2 . . . tL], by
finding the most likely translation given by:
T ? = arg max
T
p(T |S) (1)
In most of the existing approaches, following
(Brown et al, 1993), Eq. (1) is factored using the
source-channel model into
T ? = arg max
T
p(S|T )p?(T ), (2)
where the two models, the translation model,
p(S|T ), and the language model (LM), p(T ), are es-
timated separately: the former using a parallel cor-
pus and a hidden alignment model and the latter us-
ing a typically much larger monolingual corpus. The
weighting factor ? is typically tuned on a develop-
ment test set by optimizing a translation accuracy
criterion such as BLEU (Papineni et al, 2002).
In recent years, among all the proposed ap-
proaches, the phrase-based method has become
the widely adopted one in SMT due to its capa-
bility of capturing local context information from
adjacent words. Word order in the translation
output relies on how the phrases are reordered
based on both language model scores and distor-
tion cost/penalty (Koehn et al, 2003), among all
the features utilized in a maximum-entropy (log-
linear) model (Och and Ney, 2002). The distor-
tion cost utilized during the decoding is usually a
penalty linearly proportional to the number of words
in the source sentence that are skipped in a transla-
tion path.
In this paper, we propose several novel ap-
proaches to improve reordering in the phrase-based
translation with a maximum-entropy model. In Sec-
tion 2, we review the previous work that focused on
the distortion and phrase reordering in SMT. In Sec-
tion 3, we briefly review the baseline of this work.
In Section 4, we introduce a smoothed prior prob-
ability by taking into account the distortions in the
priors. In Section 5, we present multiple novel dis-
tortion features based on syntactic parsing. A new
distortion evaluation metric is proposed in Section
6 and experimental results on a large-scale Chinese-
English machine translation task are reported in Sec-
tion 7. Section 8 concludes the paper.
61
2 Previous Work
Significant amount of research has been conducted
in the past on the word reordering problem in SMT.
In (Brown et al, 1993) IBM Models 3 through 5
model reordering based on the surface word infor-
mation. For example, Model 4 attempts to assign
target-language positions to source-language words
by modeling d(j|i,K,L) where j is the target-
language position, i is the source-language position,
K and L are respectively source and target sentence
lengths. These models are not effective in modeling
reordering because they do not have enough context
and lack structural information.
Phrase-based SMT systems such as (Koehn et al,
2003) move from using words as translation units
to using phrases. One of the advantages of phrase-
based SMT systems is that the local reordering is in-
herent in the phrase translations. However, phrase-
based SMT systems capture reordering instances
and not reordering phenomena. It has trouble to pro-
duce the right translation order if the training data
does not contain the specific phrase pairs. For ex-
ample, phrases do not capture the phenomenon that
Arabic adjectives and nouns need to be reordered.
Instead of directly modeling the distance of word
movement, some phrase-level reordering models in-
dicate how to move phrases, also called orientations.
Orientations typically apply to the adjacent phrases.
Two adjacent phrases can be either placed mono-
tonically (sometimes called straight) or swapped
(non-monotonically or inverted). In (Och and Ney,
2004; Tillmann, 2004; Kumar and Byrne, 2005; Al-
Onaizan and Papineni, 2006; Xiong et al, 2006;
Zens and Ney, 2006; Ni et al, 2009), people pre-
sented models that use lexical features from the
phrases to predict their orientations. These models
are very powerful in predicting local phrase place-
ments. In (Galley and Manning, 2008) a hierar-
chical orientation model is introduced that captures
some non-local phrase reordering by a shift reduce
algorithm. Because of the heavy use of lexical fea-
tures, these models tend to suffer from data sparse-
ness problems.
Syntax information has been used for reordering,
such as in (Xia and McCord, 2004; Collins et al,
2005; Wang et al, 2007; Li et al, 2007; Chang et
al., 2009). More recently, in (Ge, 2010) a proba-
bilistic reordering model is presented to model di-
rectly the source translation sequence and explicitly
assign probabilities to the reordering of the source
input with no restrictions on gap, length or adja-
cency. The reordering model is used to generate a re-
ordering lattice which encodes many reordering and
their costs (negative log probability). Another recent
work is (Green et al, 2010), which estimates future
linear distortion cost and presents a discriminative
distortion model that predicts word movement dur-
ing translation based on multiple features.
This work differentiates itself from all the previ-
ous work on the phrase reordering as the following.
Firstly, we propose a smoothed distortion prior prob-
ability in the maximum-entropy-based MT frame-
work. It not only takes into account the distortion
in the prior, but also alleviates the data sparseness
problem. Secondly, we propose multiple syntactic
features based on the source-side parse tree to cap-
ture the reordering phenomena between two differ-
ent languages. The correct reordering patterns will
be automatically favored during the decoding, due to
the higher weights obtained through the maximum
entropy training on the parallel data. Finally, we
also introduce a new metric to quantify the effect on
the distortions in different systems. The experiments
on a Chinese-English MT task show that these pro-
posed approaches additively improve both the dis-
tortion and translation performance significantly.
3 Maximum-Entropy Model for MT
In this section we give a brief review of a special
maximum-entropy (ME) model as introduced in (It-
tycheriah and Roukos, 2007). The model has the
following form,
p(t, j|s) = p0(t, j|s)
Z
exp
?
i
?i?i(t, j, s), (3)
where s is a source phrase, and t is a target phrase.
j is the jump distance from the previously translated
source word to the current source word. During
training j can vary widely due to automatic word
alignment in the parallel corpus. To limit the sparse-
ness created by long jumps, j is capped to a win-
dow of source words (-5 to 5 words) around the last
translated source word. Jumps outside the window
are treated as being to the edge of the window. In
62
Eq. (3), p0 is a prior distribution, Z is a normalizing
term, and ?i(t, j, s) are the features of the model,
each being a binary question asked about the source
and target streams. The feature weights ?i can be
estimated with the Improved Iterative Scaling (IIS)
algorithm.
Several categories of features have been pro-
posed:
? Lexical features that examine source word, tar-
get word and jump;
? Lexical context features that examine the pre-
vious and next source words, and also the pre-
vious two target words;
? Segmentation features based on morphological
analysis;
? Part-of-speech (POS) features that collect the
syntactic information from the source and tar-
get words;
? Coverage features that examine the coverage
status of the source words to the left and to the
right. They fire only if the left source is open
(untranslated) or the right source is closed.
 
 
                <=-5          -4             -3             -2            -1            1              2             3              4           >=5 
                                                                                         jump 
Figure 1: Counts of jumps for words with POS NN.
4 Distortion Priors
Generally the prior distribution in Eq. (3) can con-
tain any information we know about the future.
 
 
                <=-5          -4             -3             -2            -1            1              2             3              4           >=5 
                                                                                        jump 
Figure 2: Counts of jumps for words with POS NT.
In (Ittycheriah and Roukos, 2007), the normalized
phrase count is utilized as the prior, i.e.
p0(t, j|s) ?
1
l
p0(t|s) =
C(s, t)
l ? C(s) (4)
where l is the jump window size (a constant), C(s, t)
is the co-ocurrence count of phrase pair (s, t), and
C(s) is the source phrase count of s. It can be seen
that distortion j is not taken into account in Eq. (4).
The contribution of distortion solely comes from the
features. In this work, we estimate the prior proba-
bility with distortion included,
p0(t, j|s) = p0(t|s)p(j|s, t) (5)
where p(j|s, t) is the distortion probability for a
given phrase pair (s, t).
Due to the sparseness issue in the estimation of
p(j|s, t), we choose to smooth it with the global dis-
tortion probability through
p(j|s, t) = ?pl(j|s, t) + (1 ? ?)pg(j), (6)
where pl is the local distortion probability estimated
based on the counts of jumps for each phrase pair
in the training, pg is the global distortion probability
estimated on all the training data, and ? is the inter-
polation weight. In this work, pg is estimated based
on either source POS (if it?s a single-word source
phrase) or source phrase size (if it?s more than one
word long), as shown below.
pg(j) =
{
Pg(j|POS), if |s| = 1
Pg(j||s|), if |s| > 1
(7)
63
In this way, the system can differentiate the distor-
tion distributions for single source words with differ-
ent POS tags, such as adjectives versus nouns. And
in the meantime, we also differentiate the distortion
distribution with different source phrase lengths. We
show several examples of the jump distributions in
Fig. 1 and 2 collected from 1M sentence pairs in
a Chinese-to-English parallel corpus with automatic
parsing and word alignment. Fig. 1 shows the count
histogram for single-word phrases with POS tag as
NN. The distortion with j = 1, i.e. monotone, domi-
nates the distribution with the highest count. The re-
ordering with j = ?1 has the second highest count.
Such pattern is shared by most of the other POS tags.
However, Fig. 2 shows that the distribution of jumps
for NT is quite different from NN. The jump with
j = ?1 is actually the most dominant, with higher
counts than monotone translation. This is due to the
different order in English when translating Chinese
temporal nouns.
5 Distortion Features
Although the maximum entropy translation model
has an explicit indicator of distortion, j, built into
the features, we discuss in this section some novel
features that try to capture the distortion phenomena
of translation. These features are questions about the
parse tree of the source language and in particular
about the local parse node neighborhood of the cur-
rent source word being translated. Figure 3 shows an
example sentence from the Chinese-English Parallel
Treebank (LDC2009E83) and the source language
parse is displayed on the left. The features below
can be viewed as either being within a parse node
or asking about the coverage status of neighborhood
nodes.
Since these features are asking about the current
coverage, they are specific to a path in the search lat-
tice during the decoding phase of translation. Train-
ing these features is done by evaluating on the path
defined by the automatic word alignment of the par-
allel corpus sentence.
5.1 Parse Tree Modifications
The ?de? construction in Chinese is by now famous.
In order to ask more coherent questions about the
parse neighborhood, we modify the parse structures
to ?raise? the ?de? structure. The parse trees anno-
tated by the LDC have a structure as shown in Fig.
4. After raising the ?de? structure we obtain the tree
in Fig. 5.
NP-OBJ
CP
IP
...
DEC
de
QP
...
NP
NN
Figure 4: Original parse tree from LDC.
DNP
CP
IP
...
DEC
de
QP
...
NP
NN
Figure 5: The parse tree after transformation.
The transformation has been applied to the exam-
ple shown in Figure 3. The resulting flat structure
facilitates the parse sibling feature discussed below.
5.2 Parse Coverage Feature
The first set of new features we will introduce is the
source parse coverage feature. This feature is in-
terior to a source parse node and asks if the leaves
under this parse node are covered (translated) or not
so far. The feature has the following components:
?i(SourceWord, TargetWord, SourceParseParent,
jump, Coverage).
Unary parents in the source parse tree are ex-
cluded since the feature has no ambiguity in cover-
age. In Figure 3, the ?PP? node above position 5 has
two children, P, NP. When translating source posi-
tion 6, this feature indicates that the PP node has a
leaf that is already covered.
5.3 Parse Sibling Feature
The second set of new features is the source parse
sibling feature. This feature asks whether the neigh-
64
 Figure 3: Chinese-English example.
boring parse node has been covered or not. The fea-
ture includes two types:
?i(SourceWord, TargetWord, SourceParseSibling,
jump, SiblingCoverage, SiblingOrientation)
and
?i(SourcePOS, TargetPOS, SourceParseSibling,
jump, SiblingCoverage, SiblingOrientation).
Some example features for the first type are
shown in Table 1, where ?i = e?i . The coverage
status (Cov) of the parse sibling node indicates if the
node is covered completely (1), partially (2) or not
covered (0). In order to capture the relationship of
the neighborhood node, we indicate the orientation
which can be either of {left (-1), right (1)}. Given
the example shown in Figure 3, at source position
10, the system can now ask about the ?CP? structure
to the left and the ?QP? and ?NP? structures to the
right. An ?i of greater than 1.0 (meaning ?i > 0)
indicates that the feature increases the probability of
the related target block. From these examples, it?s
clear that the system prefers to produce an empty
translation for the Chinese word ?de? when the ?QP?
and ?NP? nodes to the right of it are already covered
(the first two features in Table 1) and when the ?CP?
node to left is still uncovered (the third feature). The
last feature in the table shows ?i for the case when
?CP? has already been covered.
These features are able to capture neighborhoods
that are much larger than the original baseline model
which only asked questions about the immediate
lexical neighborhood of the current source word.
Cnt ?i Tgt Src Parse Cov Orien-
Node tation
18065 2.06 e0 de QP 1 1
366153 1.99 e0 de NP 1 1
143433 3.41 e0 de CP 0 -1
99297 1.05 e0 de CP 1 -1
Table 1: Parse Sibling Word Features (e0 represents
empty target).
6 A New Distortion Evaluation Metric
MT performance is usually measured by such met-
ric as BLEU which measures the MT output as a
whole including word choice and reordering. It is
useful to measure these components separately. Un-
igram BLEU (BLEUn1) measures the precision of
word choice. We need a metric for measuring re-
ordering accuracy. The naive way of counting accu-
racy at every source position does not account for the
case of the phrasal movement. If a phrase is moved
to the wrong place, every source word in the phrase
would be penalized whereas a more reasonable met-
ric would penalize the phrase movement only once
if the phrase boundary is correct.
We propose the following pair-wise distortion
metric. From an MT output, we first extract the
source visit sequence:
Hyp:{h1,h2, . . . hn}
where hi are the visit order of the source sentence.
From the reference, we extract the true visit se-
quence:
65
Ref:{r1,r2, . . . rn}
The Pair-wise Distortion metric PDscore can be
computed as follows:
PDscore(
??
H ) =
n
?
i=1
I(hi = rj ? hi?1 = rj?1)
n
(8)
It measures how often the translation output gets
the pair-wise source visit order correct. We notice
that an MT metric named LRscore was proposed in
(Birch and Osborne, 2010). It computes the distance
between two word order sequences, which is differ-
ent from the metric we proposed here.
7 Experiments
7.1 Data and Baseline
We conduct a set of experiments on a Chinese-to-
English MT task. The training data includes the UN
parallel corpus and LDC-released parallel corpora,
with about 11M sentence pairs, 320M words in to-
tal (counted at the English side). To evaluate the
smoothed distortion priors and different features, we
use an internal data set as the development set and
the NIST MT08 evaluation set as the test set, which
includes 76 documents (691 sentences) in newswire
and 33 documents (666 sentences) in weblog, both
with 4 sets of references for each sentence. Instead
of using all the training data, we sample the training
corpus based on the dev/test set to train the system
more efficiently. The most recent and good-quality
corpora are sampled first. For the given test set, we
obtain the first 20 instances of n-grams (length from
1 to 15) from the test that occur in the training uni-
verse and the resulting sentences then form the train-
ing sample. In the end, 1M sentence pairs are se-
lected for the sampled training for each genre of the
MT08 test set.
A 5-gram language model is trained from the En-
glish Gigaword corpus and the English portion of
the parallel corpus used in the translation model
training. The Chinese parse trees are produced
by a maximum entropy based parser (Ratnaparkhi,
1997). The baseline decoder is a phrase-based de-
coder that employs both normal phrases and also
non-contiguous phrases. The value of maximum
skip is set to 9 in all the experiments. The smoothing
parameter ? for distortion prior is set to 0.9 empiri-
cally based on the results on the development set.
7.2 Distortion Evaluation
We evaluate the MT distortion using the metric in
Eq. (8) on two hand-aligned test sets. Test-278 in-
cludes 278 held-out sentences. Test-52 contains the
first 52 sentences from the MT08 Newswire set, with
the Chinese input sentences manually aligned to the
first set of reference translations. From the hand
alignment, we extract the true source visit sequence
and this is the reference.
The evaluation results are in Table 2. It is shown
that the smoothed distortion prior, parse coverage
feature and parse sibling feature each provides im-
provement on the PDscore on Test-278 and Test-52.
The final system scores are 2 to 3 points absolute
higher than the baseline scores. The state visit se-
quence in the final system is closer to the true visit
sequence than that of the baseline. This indicates
the advantage of using both parse-based syntactic
features and also the smoothed prior that takes into
account of the distortion effect. We also provide
an upper-bound in the last row by computing the
PDscore between the first and second set of refer-
ences for Test-52. The number shows the agreement
between two human translators in terms of PDscore
is around 71%.
System Test-278 Test-52
ME Baseline 44.58 48.96
+Prior 45.12 49.22
+COV 45.00 49.03
+SIB 45.43 49.20
+COV+SIB 46.16 49.45
+Prior+COV+SIB 47.68 51.04
Ref1 vs. Ref2 - 70.99
Table 2: Distortion accuracy PDscore (Prior:smoothed
distortion prior; COV:parse coverage feature; SIB:parse
sibling feature).
7.3 Translation Results
Translation results on the MT08 Newswire set and
MT08 Weblog set are listed in Table 3 and Table 4
respectively. The MT performance is measured with
the widely adopted BLEU and TER (Snover et al,
2006) metrics. We also compare the results from
different configurations with a normal phrase-based
66
System Number of Features BLEU TER
PBT n/a 29.71 59.40
ME 9,008,382 32.12 56.78
+Prior 9,008,382 32.46 56.41
+COV 9,202,431 32.48 56.50
+SIB 10,088,487 32.73 56.26
+COV+SIB 10,282,536 32.94 55.97
+Prior+COV+SIB 10,282,536 33.15 55.62
Table 3: MT results on MT08 Newswire set (PBT:normal phrase-based MT; ME:Maximum-entropy baseline;
Prior:smoothed distortion prior; COV:parse coverage feature; SIB:parse sibling feature).
System Number of Features BLEU TER
PBT n/a 20.07 62.90
ME 9,192,617 22.42 60.36
+Prior 9,192,617 22.70 60.11
+COV 9,306,967 22.69 60.14
+SIB 9,847,445 22.91 59.92
+COV+SIB 9,961,795 23.04 59.78
+Prior+COV+SIB 9,961,795 23.25 59.56
Table 4: MT results on MT08 Weblog set (PBT:normal phrase-based MT; ME:Maximum-entropy baseline;
Prior:smoothed distortion prior; COV:parse coverage feature; SIB:parse sibling feature).
SMT system (Koehn et al, 2003) that is trained on
the same training data. The number of features used
in the systems are listed in the tables.
We start from the maximum-entropy baseline, a
system implemented similarly as in (Ittycheriah
and Roukos, 2007). It utilizes multiple features as
listed in Section 3, including lexical reordering fea-
tures, and produces an already significantly better
performance than the normal phrase-based MT sys-
tem (PBT). It is around 2.5 points better in both
BLEU and TER than the PBT baseline. By adding
smoothed priors, parse coverage features or parse
sibling features each separately, the MT perfor-
mance is improved by 0.3 to 0.6. The parse sibling
feature alone provides the largest individual contri-
bution. When adding both types of new features,
the improvement is around 0.6 to 0.8 on two gen-
res. Finally, applying all three results in the best
performance (the last row). On the Newswire set,
the final system is more than 3 points better than the
PBT baseline and 1 point better than the ME base-
line. On the Weblog set, it is more than 3 points
better than PBT and 0.8 better than the ME baseline.
All the MT results above are statistically significant
with p-value < 0.0001 by using the tool described in
(Zhang and Vogel, 2004).
7.4 Analysis
To better understand the distortion and translation
results, we take a closer look at the parse-based fea-
tures. In Table 5, we list the most frequent parse sib-
ling features that are related to the Chinese phrases
with ?PP VV? structures. It is known that in Chi-
nese usually the preposition phrases (?PP?) are writ-
ten/spoken before the verbs (?VV?), with a different
order from English. Table 5 shows how such re-
ordering phenomenon is captured by the parse sib-
ling features. Recall that when ?i is greater than 1,
the system prefers the reordering with that feature
fired. When ?i is smaller than 1, the system will
penalize the corresponding translation order during
the decoding search. When the coverage is equal to
1, it means ?PP? has been translated before translat-
ing current ?VV?. As shown in the table, those fea-
tures with coverage equal to 1 have ?i lower than 1,
which will result in penalties on incorrect translation
orders.
In Fig. 6, we show the comparison between the
67
Count ?i j TgtPOS SrcPOS ParseSib Cov Orien-
Node tation
3052 1.10 5 VBD VV PP 0 -1
2662 1.10 -1 VBD VV PP 0 -1
2134 1.25 4 VBD VV PP 0 -1
50 0.73 5 VBD VV PP 1 -1
39 0.84 -5 VBD VV PP 1 -1
18 0.95 -2 VBD VV PP 1 -1
Table 5: Parse Sibling Word Features related to Chinese ?PP VV?.
 
Src1 
 
 

  

 	


 

 

 

 , 1850  2005 

 , 
 
 

 1800    (were) (at) (annual) 3%  