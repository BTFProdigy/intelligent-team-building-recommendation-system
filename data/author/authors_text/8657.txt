Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1061?1069,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Information Retrieval Oriented Word Segmentation based on Character
Associative Strength Ranking
Yixuan Liu, Bin Wang, Fan Ding, Sheng Xu
Information Retrieval Group
Center for Advanced Computing Research
Institute of Computing Technology
Chinese Academy of Sciences
Beijing, 100190, P.R.China
{liuyixuan, wangbin, dingfan, xusheng}@ict.ac.cn
Abstract
This paper presents a novel, ranking-style
word segmentation approach, called RSVM-
Seg, which is well tailored to Chinese informa-
tion retrieval(CIR). This strategy makes seg-
mentation decision based on the ranking of the
internal associative strength between each pair
of adjacent characters of the sentence. On the
training corpus composed of query items, a
ranking model is learned by a widely-used tool
Ranking SVM, with some useful statistical
features, such as mutual information, differ-
ence of t-test, frequency and dictionary infor-
mation. Experimental results show that, this
method is able to eliminate overlapping am-
biguity much more effectively, compared to
the current word segmentation methods. Fur-
thermore, as this strategy naturally generates
segmentation results with different granular-
ity, the performance of CIR systems is im-
proved and achieves the state of the art.
1 Introduction
To improve information retrieval systems? perfor-
mance, it is important to comprehend both queries
and corpus precisely. Unlike English and other
western languages, Chinese does not delimit words
by white-space. Word segmentation is therefore a
key preprocessor for Chinese information retrieval
to comprehend sentences.
Due to the characteristics of Chinese, two main
problems remain unresolved in word segmentation:
segmentation ambiguity and unknown words, which
are also demonstrated to affect the performance of
Chinese information retrieval (Foo and Li, 2004).
Overlapping ambiguity and combinatory ambiguity
are two forms of segmentation ambiguity. The first
one refers to that ABC can be segmented into AB
C or A BC. The second one refers to that string
AB can be a word, or A can be a word and B can
be a word. In CIR, the combinatory ambiguity is
also called segmentation granularity problem (Fan
et al, 2007). There are many researches on the
relationship between word segmentation and Chi-
nese information retrieval (Foo and Li, 2004; Peng
et al, 2002a; Peng et al, 2002b; Jin and Wong,
2002). Their studies show that the segmentation
accuracy does not monotonically influence subse-
quent retrieval performance. Especially the overlap-
ping ambiguity, as shown in experiments of (Wang,
2006), will cause more performance decrement of
CIR. Thus a CIR system with a word segmenter bet-
ter solving the overlapping ambiguity, may achieve
better performance. Besides, it also showed that the
precision of new word identification was more im-
portant than the recall.
There are some researches show that when com-
pound words are split into smaller constituents, bet-
ter retrieval results can be achieved (Peng et al,
2002a). On the other hand, it is reasonable that the
longer the word which co-exists in query and cor-
pus, the more similarity they may have. A hypothe-
sis, therefore, comes to our mind, that different seg-
mentation granularity can be incorporated to obtain
better CIR performance.
In this paper we present a novel word segmenta-
tion approach for CIR, which can not only obviously
reduce the overlapping ambiguity, but also introduce
different segmentation granularity for the first time.
1061
In our method, we first predict the ranking result of
all internal association strength (IAS) between each
pair of adjacent characters in a sentence using Rank-
ing SVM model, and then, we segment the sentence
into sub-sentences with smaller and smaller granu-
larity by cutting adjacent character pairs according
to this rank. Other machine-learning based segmen-
tation algorithms (Zhang et al, 2003; Lafferty et al,
2001; Ng and Low, 2004) treat segmentation prob-
lem as a character sequence tagging problem based
on classification. However, these methods cannot di-
rectly obtain different segmentation granularity. Ex-
periments show that our method can actually im-
prove information retrieval performance.
This paper is structured as follows. It starts with
a brief introduction of the related work on the word
segmentation approaches. Then in Section 3, we in-
troduce our segmentation method. Section 4 evalu-
ates the method based on experimental results. Fi-
nally, Section 5 makes summary of this whole paper
and proposes the future research orientation.
2 Related Work
Various methods have been proposed to address
the word segmentation problem in previous studies.
They fall into two main categories, rule-based ap-
proaches that make use of linguistic knowledge and
statistical approaches that train on corpus with ma-
chine learning methods. In rule-based approaches,
algorithms of string matching based on dictionary
are the most commonly used, such as maximum
matching. They firstly segment sentences accord-
ing to a dictionary and then resort to some rules
to resolve ambiguities (Liu, 2002; Luo and Song,
2001). These rule-based methods are fast, how-
ever, their performances depend on the dictionary
which cannot include all words, and also on the rules
which cost a lot of time to make and must be up-
dated frequently. Recent years statistical approaches
became more popular. These methods take advan-
tage of various probability information gained from
large corpus to segment sentences. Among them,
Wang?s work (Wang, 2006) is the most similar to
our method, since both of us apply statistics infor-
mation of each gap in the sentence to eliminate over-
lapping ambiguity in methods. However, when com-
bining different statistics, Wang decided the weight
by a heuristic way which was too simply to be suit-
able for all sentences. In our method, we employ a
machine-learning method to train features? weights.
Many machine-learning methods, such as
HMM (Zhang et al, 2003), CRF (Lafferty et al,
2001), Maximum Entropy (Ng and Low, 2004),
have been exploited in segmentation task. To our
knowledge, machine-learning methods used in seg-
mentation treated word segmentation as a character
tagging problem. According to the model trained
from training corpus and features extracted from the
context in the sentence, these methods assign each
character a positional tag, indicating its relative po-
sition in the word. These methods are difficult to get
different granularity segmentation results directly.
Our method has two main differences with them.
Firstly, we tag the gap between characters rather
than characters themselves. Secondly, our method
is based on ranking rather than classification.
Then, we will present our ranking-based segmen-
tation method, RSVM-Seg.
3 Ranking based Segmentation
Traditional segmentation methods always take the
segmentation problem as classification problem and
give a definite segmentation result. In our approach,
we try to solve word segmentation problem from the
view of ranking. For easy understanding, let?s rep-
resent a Chinese sentence S as a character sequence:
C1:n = C1C2 . . . Cn
We also explicitly show the gap Gi(i = 1 . . . n? 1)
between every two adjacent characters Ci and Ci+1:
C1:n|G1:n?1 = C1G1C2G2 . . . Gn?1Cn
IASi(i = 1 . . . n) is corresponding to Gi(i =
1 . . . n), reflecting the internal association strength
between Ci and Ci+1. The higher the IAS value is,
the stronger the associative between the two charac-
ters is. If the association between two characters is
weak, then they can be segmented. Otherwise, they
should be unsegmented. That is to say we could
make segmentation based on the ranking of IAS
value. In our ranking-style segmentation method,
Ranking SVM is exploited to predict IAS ranking.
In next subsections, we will introduce how to
take advantage of Ranking SVM model to solve our
1062
problem. Then, we will describe features used for
training the Ranking SVM model. Finally, we will
give a scheme how to get segmentation result from
predicted ranking result of Ranking SVM.
3.1 Segmentation based on Ranking SVM
Ranking SVM is a classical algorithm for ranking,
which formalizes learning to rank as learning for
classification on pairs of instances and tackles the
classification issue by using SVM (Joachims, 2002).
Suppose that X?Rd is the feature space, where d is
the number of features, and Y = r1, r2, . . . , rK is
the set of labels representing ranks. And there exists
a total order between ranks r1 > r2 > . . . > rK ,
where > denotes the order relationship. The actual
task of learning is formalized as a Quadratic Pro-
gramming problem as shown below:
min?,???
1
2???
2 + C????
s.t.??, x? ? x?? > 1? ???,?x? ? x?, ??? ? 0
(1)
where ??? denotes l2 norm measuring the margin
of the hyperplane and ?ij denotes a slack variable.
xi ? xj means the rank class of xi has an order
prior to that of xj , i.e. Y (xi) > Y (xj). Suppose
that the solution to (1) is ??, then we can make the
ranking function as f(x) = ???, x?.
When applying Ranking SVM model to our prob-
lems, an instance (feature vector x) is created from
all bigrams (namely CiCi+1, i = 1 . . . n ? 1) of
a sentence in the training corpus. Each feature
is defined as a function of bigrams (we will de-
scribe features in detail in next subsection). The
instances from all sentences are then combined for
training. And Y refers to the class label of the
IAS degree. As we mentioned above, segmenta-
tion decision is based on IAS value. Therefore,
the number of IAS degree?s class label is also cor-
respondent to the number of segmentation class la-
bel. In traditional segmentation algorithms, they al-
ways label segmentation as two classes, segmented
and unsegmented. However, for some phrases, it is
a dilemma to make a segmentation decision based
on this two-class scheme. For example, Chinese
phrase ??????(Notepad)? can be segmented
as ????(Note)? and ???(computer)? or can
be viewed as one word. We cannot easily classify
the gap between ??? and ??? as segmented or un-
segmented. Therefore, beside these two class la-
bels, we define another class label, semisegmented,
which means that the gap between two characters
could be segmented or unsegmented, either will be
right. Correspondingly, IAS degree is also divided
into three classes, definitely inseparable (marked as
3), partially inseparable (marked as 2), and sepa-
rable (marked as 1). ?Separable? corresponds to
be segmented?; ?partially inseparable? corresponds
to semisegmented; ?definitely inseparable? corre-
sponds to be unsegmented. Obviously, there exists
orders between these labels? IAS values, namely
IAS(1) < IAS(2) < IAS(3), IAS(?) represents
the IAS value of different labels. Next, we will
describe the features used to train Ranking SVM
model.
3.2 Features for IAS computation
Mutual Information: Mutual information, mea-
suring the relationship between two variables, has
been extensively used in computational language re-
search. Given a Chinese character string ?xy? (as
mentioned above, in our method, ?xy? refers to bi-
gram in a sentence), mutual information between
characters x and y is defined as follows:
mi(x, y) = log2 p(x, y)p(x)p(y) (2)
where p(x, y) is the co-occurrence probability of x
and y, namely the probability that bigram ?xy? oc-
curs in the training corpus, and p(x), p(y) are the
independent probabilities of x and y respectively.
From (2), we conclude that mi(x, y) ? 0 means
that IAS is strong; mi(x, y) ? 0 means that it
is indefinite for IAS between characters x and y;
mi(x, y) ? 0 means that there is no association
been characters x and y. However, mutual infor-
mation has no consideration of context, so it can-
not solve the overlapping ambiguity effectively (Sili
Wang 2006). To remedy this defect, we introduce
another statistics measure, difference of t-test.
Difference of t-score (DTS): Difference of t-
score is proposed on the basis of t-score. Given
a Chinese character string ?xyz?, the t-score of the
character y relevant to character x and z is defined
1063
as:
tx,z(y) = p(z|y)? p(y|x)??2(p(z|y)) + ?2(p(y|x))
(3)
where p(y|x) is the conditional probability of y
given x, and p(z|y), of z given y, and ?2(p(y|x)),
?2(p(z|y)) are variances of p(y|x) and of p(z|y) re-
spectively. Sun et al gave the derivation formula of
?2(p(y|x)), ?2(p(z|y)) (Sun et al, 1997) as
?2(p(z|y)) ? r(y, z)r2(y) ?2(p(y|x)) ?
r(x, y)
r2(x) (4)
where r(x, y), r(y, z), r(y), r(z) are the frequency
of string xy, yz, y, and z respectively. Thus formula
(3) is deducted as
tx,z(y) =
r(y,z)
r(y) ?
r(x,y)
r(x)?
r(y,z)
r2(y) +
r(x,y)
r2(x)
(5)
tx,z(y) indicates the binding tendency of y in the
context of x and z: if tx,z(y) > 0 then y tends to
be bound with z rather than with x; if tx,z(y) < 0,
they y tends to be bound with x rather than with z.
To measure the binding tendency between two ad-
jacent characters ?xy? (also, it refers to bigram in a
sentence in our method), we use difference of t-score
(DTS) (Sun et al, 1998) which is defined as
dts(x, y) = tv,y(x)? tx,w(y) (6)
Higher dts(x, y) indicates stronger IAS between
adjacent characters x and y.
Dictionary Information: Both statistics mea-
sures mentioned above cannot avoid sparse data
problem. Then Dictionary Information is used to
compensate for the shortage of statistics informa-
tion. The dictionary we used includes 75784 terms.
We use binary value to denote the dictionary feature.
If a bigram is in the dictionary or a part of dictionary
term, we label it as ?1?, otherwise, we label is as ?0?.
Frequency: An important characteristic of new
word is its repeatability. Thus, we also use fre-
quency as another feature to train Ranking SVM
model. Here, the frequency is referred to the number
of times that a bigram occurs in the training corpus.
We give a training sentence for a better under-
standing of features mentioned above. The sentence
Algorithm 1 : Generate various granularity terms
1: Input: A Chinese sentence S = C1 : Cn
IAS = IAS1:n?1 LB = 1;RB = n
2: Iterative(S, IAS):
3: while length(S) ? 3 do
4: MB = FindMinIAS(IAS)
5: SL = CLB:MB
6: SR = CMB+1:RB
7: IASL = IASLB:MB
8: IASR = IASMB+1:RB
9: Iterative(SL, IASL)
10: Iterative(SR, IASR)
11: end while
is ????????(China Construction Bank net-
work)? We extract all bigrams in this sentence, com-
pute the four above features and give the IAS a la-
bel for each bigram. The feature vectors of all these
bigrams for training are shown in Table 1.
3.3 Segmentation scheme
In order to compare with other segmentation meth-
ods, which give a segmentation result based on two
class labels, segmented and unsegmented, it is nec-
essary to convert real numbers result given by Rank-
ing SVM to these two labels. Here, we make a
heuristic scheme to segment the sentence based on
IAS ranking result predicted by Ranking SVM. The
scheme is described in Algorithm 1. In each itera-
tion we cut the sentence at the gap with minimum
IAS value. Nie et.al. pointed out that the average
length of words in usage is 1.59 (Nie et al, 2000).
Therefore, we stop the segmentation iterative when
the length of sub sentence is 2 or less than 2. By
this method, we could represent the segmentation re-
sult as a binary tree. Figure 1 shows an example of
this tree. With this tree, we can obtain various gran-
ularity segmentations easily, which could be used
in CIR. This segmentation scheme may cause some
combinatory ambiguity. However, Nie et.al. (Nie
et al, 2000) also pointed out that there is no accu-
rate word definition, thus whether combinatory am-
biguity occurs is uncertain. What?s more, compared
to overlapping ambiguity, combinatory ambiguity is
not the fatal factor for information retrieval perfor-
mance as mentioned in introduction. Therefore, this
scheme is reasonable for Chinese information re-
1064
Bigram MI DTS Dictionary Frequency IAS
??(China) 6.67 1985.26 1 1064561 3
?? 2.59 -1447.6 0 14325 1
??(Construction) 8.67 822.64 1 200129 3
?? 5.94 -844.05 0 16098 2
??(Bank) 9.22 931.25 1 236976 3
?? 2.29 -471.24 0 15282 1
Table 1: Example of feature vector
???????
(Traffic map of JiangXi Province) 
???           ????
(JiangXi Province)     (Traffic map) 
???      ????
(JiangXi) (Province) (Traffic)    (Map)
Figure 1: Example 1
trieval.
4 Experiments and analysis
4.1 Data
Since the label scheme and evaluation measure (de-
scribed in next subsection) of our segmentation
method are both different from the traditional seg-
mentation methods, we did not carry out experi-
ments on SIGHAN. Instead, we used two query logs
(QueryLog1 and QueryLog2) as our experiment cor-
pus, which are from two Chinese search engine com-
panies. 900 queries randomly from QueryLog1 were
chosen as training corpus. 110 Chinese queries from
PKU Tianwang1 , randomly selected 150 queries
from QueryLog1 and 100 queries from QueryLog2
were used as test corpus. The train and test cor-
pus have been tagged by three people. They were
given written information need statements, and were
asked to judge the IAS of every two adjacent char-
acters in a sentence on a three level scale as men-
tioned above, separable, partially inseparable, and
definitely inseparable. The assessors agreed in 84%
of the sentences, the other sentences were checked
1Title field of SEWM2006 and SEWM2007 web retrieval
TD task topics. See http://www.cwirf.org/
by all assessors, and a more plausible alternative was
selected. We exploited SVM light2 as the toolkit to
implement Ranking SVM model.
4.2 Evaluation Measure
Since our approach is based on the ranking of IAS
values, it is inappropriate to evaluate our method by
the traditional method used in other segmentation
algorithms. Here, we proposed an evaluation mea-
sure RankPrecision based on Kendall?s ? (Joachims,
2002), which compared the similarity between the
predicted ranking of IAS values and the rankings
of these tags as descending order. RankPrecision
formula is as follows:
RankPrecision =
1? ?
n
i=1InverseCount(si)
?ni=1CompInverseCount(si)
(7)
where si represents the ith sentence (unsegmented
string), InverseCount(si) represents the number
of discordant pairs inversions in the ranking of the
predicted IAS value compared to the correct labeled
ranking. CompInverseCount(si) represents the
number of discordant pairs inversions when the la-
bels totally inverse.
4.3 Experiments Results
Contributions of the Features: We investi-
gated the contribution of each feature by gen-
erating many versions of Ranking SVM model.
RankPrecision as described above was used for
evaluations in these and following experiments.
We used Mutual Information(MI); Difference
of T-Score(DTS); Frequency(F); mutual informa-
tion and difference of t-score(MI+DTS); mu-
2http://svmlight.joachims.org/
1065
Feature Corpus
Train Query Query Tian
Log1 Log2 Wang
MI 0.882 0.8719 0.8891 0.9444
DTS 0.9054 0.8954 0.9086 0.9444
F 0.8499 0.8416 0.8563 0.9583
MI+DTS 0.9077 0.9117 0.923 0.9769
MI+DTS+F 0.8896 0.8857 0.9209 0.9815
MI+DTS+D 0.933 0.916 0.9384 0.9954
MI+DTS+F+D 0.932 0.93 0.9374 0.9954
Table 2: The segmentation performance with different
features
Features
MI DTS F MI+DTS MI+DTS+F MI+DTS+D MI+DTS+F+D
Ra
nk
Pr
ec
isi
on
.82
.84
.86
.88
.90
.92
.94
.96
.98
1.00
1.02
TrainCorpus
QueryLog1
QueryLog2
TianWang
Figure 2: Effects of features
tual information, difference of t-score and Fre-
quency(MI+DTS+F); mutual information, differ-
ence of t-score and dictionary(MI+DTS+D); mutual
information, difference of t-score, frequency and
Dictionary(MI+DTS+F+D) as features respectively.
The results are shown in Table 2 and Figure 2.
From the results, we can see that:
? Using all described features together, the Rank-
ing SVM achieved a good performance. And
when we added MI, DTS, frequency, dictio-
nary as features one by one, the RankPrecision
improved step by step. It demonstrates that the
features we selected are useful for segmenta-
tion.
Size of Corpus
Train Train Query Query Tian
Corpus Log1 Log2 Wang
100 0.9149 0.9070 0.9209 0.9630
200 0.9325 0.9304 0.9446 0.9907
400 0.9169 0.9057 0.9230 0.9630
500 0.9320 0.9300 0.9374 0.9954
600 0.9106 0.9050 0.9312 0.9907
700 0.9330 0.9284 0.9353 0.9954
900 0.9217 0.9104 0.9240 0.9907
Table 3: The segmentation performance with different
size training corpus
Number of Train Query
0 200 400 600 800 1000
Ra
nk
Pr
ec
isi
on
.80
.85
.90
.95
1.00
1.05
1.10
TrainCorpus
QueryLog1
QueryLog2
TianWang
Figure 3: Effects of Corpus Size
? The lowest RankPrecision is above 85%, which
suggests that the predicted rank result by our
approach is very close to the right rank. It is
shown that our method is effective.
? When we used each feature alone, difference
of t-score achieved highest RankPrecise, fre-
quency was worst on most of test corpus (ex-
cept TianWang). It is induced that difference
of t-test is the most effective feature for seg-
mentation. It is explained that because dts is
combined with the context information, which
eliminates overlapping ambiguity errors.
? It is surprising that when mutual information
and difference of t-score was combined with
1066
frequency, the RankPrecision was hurt on three
test corpus, even worse than dts feature. The
reason is supposed that some non-meaning but
common strings, such as ???? would be took
for a word with high IAS values. To correct
this error, we could build a stop word list, and
when we meet a character in this list, we treat
them as a white-space.
Effects of corpus size:We trained different Rank-
ing SVM models with different corpus size to in-
vestigate the effects of training corpus size to our
method performance. The results are shown in Ta-
ble 3 and Figure 3. From the results, we can see that
the effect of corpus size to the performance of our
approach is minors. Our segmentation approach can
achieve good performance even with small training
corpus, which indicates that Ranking SVM has gen-
eralization ability. Therefore we can use a relative
small corpus to train Ranking SVM, saving labeling
effort.
Effects on Finding Boundary: In algorithm
1, we could get different granularity segmentation
words when we chose different length as stop
condition. Figure 4 shows the ?boundary precision?
at each stop condition. Here, ?boundary precision?
is defined as
No.of right cut boundaries
No.of all cut boundaries (8)
From the result shown in figure 4, we can see
that as the segmentation granularity gets smaller, the
boundary precision gets lower. The reason is obvi-
ous, that we may segment a whole word into smaller
parts. However, as we analyzed in introduction, in
CIR, we should judge words boundaries correctly to
avoid overlapping ambiguity. As for combinatory
ambiguity, through setting different stop length con-
dition, we can obtain different granularity segmen-
tation result.
Effects on Overlapping Ambiguity: Due to the
inconsistency of train and test corpus, it is difficult to
keep fair for Chinese word segmentation evaluation.
Since ICTCLAS is considered as the best Chinese
word segmentation systems. We chose ICTCLAS
as the comparison object. Moreover, we chose
Maximum Match segmentation algorithm, which is
rule-based segmentation method, as the baseline.
Stop length 
2~3 4~5 6~7
Pr
ec
isi
on
 of
 B
ou
nd
ary
.88
.89
.90
.91
.92
.93
.94
.95
.96
Figure 4: Precision of boundary with different stop word
length conditions
Corpus NOA NOA NOA
(RSVM Seg) (ICTCLAS) (MM)
Query
Log1 7 10 21
Query
Log2 2 6 16
Tian
Wang 0 0 1
Table 4: Number of Overlapping Ambiguity
We compared the number of overlapping ambigu-
ity(NOA) among these three approaches on test cor-
pus QueryLog1, QueryLog2 and TianWang. The re-
sult is shown in Table 4. On these three test cor-
pus, the NOA of our approach is smallest, which
indicates our method resolve overlapping ambiguity
more effectively. For example, the sentence ???
??(basic notes)?, the segmentation result of ICT-
CLAS is ????(basic class)/?(article)?, the word
???(notes)? is segmented, overlapping ambiguity
occurring. However, with our method, the predicted
IAS value rank of positions between every two ad-
jacent characters in this sentence is ??3?1?2??,
which indicates that the character ??? has stronger
internal associative strength with the character ???
than with the character ???, eliminating overlap-
ping ambiguity according to this ISA rank results.
Effects on Recognition Boundaries of new
word: According to the rank result of all IAS values
1067
??????
(Hainan High School?s Entry Recruitme) 
??                ????
(Hainan) (High School?s Entry Recruitment) 
                   ??          ??
(High School?s Entry)(Recruitment)  
Figure 5: Example of New Word boundary
in a sentence, our method can recognize the bound-
aries of new words precisely, avoiding the overlap-
ping ambiguity caused by new words. For example,
the phrase ???????(Hainan High School?s
Entry Recruitment)?, the ICTCLAS segmentation
result is ???/?/??/??, because the new word
???? cannot be recognized accurately, thus the
character ??? is combined with its latter charac-
ter ???, causing overlapping ambiguity. By our
method, the segmentation result is shown as figure
5, in which no overlapping ambiguity occurs.
Performance of Chinese Information Re-
trieval: To evaluate the effectiveness of RSVM-Seg
method on CIR, we compared it with the FMM seg-
mentation. Our retrieval system combines differ-
ent query representations obtained by our segmen-
tation method, RSVM-Seg. In previous TREC Tere-
byte Track, Markov Random Field(MRF) (Metzler
and Croft, 2005) model has displayed better perfor-
mance than other information retrieval models, and
it can much more easily include dependence fea-
tures. There are three variants of MRF model, full
independence(FI), sequential dependence(SD), and
full dependence(FD). We chose SD as our retrieval
model, since Chinese words are composed by char-
acters and the adjacent characters have strong de-
pendence relationship. We evaluated the CIR per-
formance on the Chinese Web Corpora CWT200g
provided by Tianwang 3, which, as we know, is
the largest publicly available Chinese web corpus
till now. It consists of 37, 482, 913 web pages
with total size of 197GB. We used the topic set
3http://www.cwirf.org/
Segmentation
Method MAP R-P GMAP
FMM 0.0548 0.0656 0.0095
RSVM-Seg 0.0623 0.0681 0.0196
Table 5: Evaluation of CIR performance
for SEWM2007 and SEWM2006 Topic Distillation
(TD) task which contains 121 topics. MAP, R-
Precision and GMAP (Robertson, 2006) were as
main evaluation metrics. GMAP is the geometric
mean of AP(Average Precision) through different
queries, which was introduced to concentrate on dif-
ficult queries. The result is shown in 5. From the
table, we can see that our segmentation method im-
prove the CIR performance compared to FMM.
5 Conclusion and Future work
From what we have discussed above, we can safely
draw the conclusion that our work includes several
main contributions. Firstly, to our best known, this
is the first time to take the Chinese word segmenta-
tion problem as ranking problem, which provides a
new view for Chinese word segmentation. This ap-
proach has been proved to be able to eliminate over-
lapping ambiguity and also be able to obtain various
segmentation granularities. Furthermore, our seg-
mentation method can improve Chinese information
retrieval performance to some extent.
As future work, we would search another more
encouraging method to make a segmentation deci-
sion from the ranking result. Moreover, we will try
to relabel SIGHAN corpus on our three labels, and
do experiments on them, which will be more con-
venient to compare with other segmentation meth-
ods. Besides, we will carry out more experiments to
search the effectiveness of our segmentation method
to CIR.
Acknowledgments
This paper is supported by China Natural Science
Founding under No. 60603094 and China National
863 key project under No. 2006AA010105. We ap-
preciate Wenbin Jiang?s precious modification ad-
vices. Finally, we would like to thank the three
anonymous EMNLP reviewers for their helpful and
constructive comments.
1068
References
D. Fan, W. Bin, and W. Sili. 2007. A Heuristic Approach
for Segmentation Granularity Problem in Chinese In-
formation Retrieval. Advanced Language Processing
and Web Information Technology, 2007. ALPIT 2007.
Sixth International Conference on, pages 87?91.
S. Foo and H. Li. 2004. Chinese word segmentation and
its effect on information retrieval. volume 40, pages
161?190. Elsevier.
H. Jin and K.F. Wong. 2002. A Chinese dictionary
construction algorithm for information retrieval. ACM
Transactions on Asian Language Information Process-
ing (TALIP), 1(4):281?296.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. Proceedings of the eighth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 133?142.
J.D. Lafferty, A. McCallum, and F.C.N. Pereira. 2001.
Conditional Random Fields: Probabilistic Models for
Segmenting and Labeling Sequence Data. Proceed-
ings of the Eighteenth International Conference on
Machine Learning table of contents, pages 282?289.
Q. Liu. 2002. Review of Chinese lexical and syntactic
technology.
Z.Y. Luo and R. Song. 2001. Proper noun recognition in
Chinese word segmentation research. Conference of
international Chinese computer, 328:2001?323.
D. Metzler and W.B. Croft. 2005. A Markov random
field model for term dependencies. Proceedings of
the 28th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 472?479.
H.T. Ng and J.K. Low. 2004. Chinese part-of-speech
tagging: one-at-a-time or all-at-once? word-based or
character-based. Proc of EMNLP.
J.Y. Nie, J. Gao, J. Zhang, and M. Zhou. 2000. On
the use of words and n-grams for Chinese informa-
tion retrieval. Proceedings of the fifth international
workshop on on Information retrieval with Asian lan-
guages, pages 141?148.
F. Peng, X. Huang, D. Schuurmans, and N. Cercone.
2002a. Investigating the relationship between word
segmentation performance and retrieval performance
in Chinese IR. Proceedings of the 19th international
conference on Computational linguistics-Volume 1,
pages 1?7.
F. Peng, X. Huang, D. Schuurmans, N. Cercone, and S.E.
Robertson. 2002b. Using self-supervised word seg-
mentation in Chinese information retrieval. Proceed-
ings of the 25th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 349?350.
S. Robertson. 2006. On GMAP: and other transforma-
tions. Proceedings of the 15th ACM international con-
ference on Information and knowledge management,
pages 78?83.
Sili Wang. 2006. Research on chinese word segmenta-
tion for large scale information retrieval.
H.P. Zhang, H.K. Yu, D.Y. Xiong, and Q. Liu. 2003.
HHMM-based Chinese Lexical Analyzer ICTCLAS.
Proceedings of Second SIGHAN Workshop on Chinese
Language Processing, pages 184?187.
1069
A Study on Effectiveness of Syntactic Relationship in Dependence Re-
trieval Model 
Fan Ding1,2
1: Graduate University,  
Chinese Academy of Sciences  
Beijing, 100080, China  
dingfan@ict.ac.cn 
Bin Wang2
2: Institute of Computing Technology, 
Chinese Academy of Sciences  
Beijing, 100080, China 
wangbin@ict.ac.cn 
 
 
Abstract 
To relax the Term Independence Assump-
tion, Term Dependency is introduced and it 
has improved retrieval precision dramati-
cally. There are two kinds of term depend-
encies, one is defined by term proximity, 
and the other is defined by linguistic de-
pendencies. In this paper, we take a com-
parative study to re-examine these two 
kinds of term dependencies in dependence 
language model framework. Syntactic rela-
tionships, derived from a dependency 
parser, Minipar, are used as linguistic term 
dependencies. Our study shows: 1) Lin-
guistic dependencies get a better result than 
term proximity. 2) Dependence retrieval 
model achieves more improvement in sen-
tence-based verbose queries than keyword-
based short queries. 
1 Introduction 
For the sake of computational simplicity, Term 
Independence Assumption (TIA) is widely used in 
most retrieval models. It states that terms are statis-
tically independent from each other. Though un-
reasonable, TIA did not cause very bad perform-
ance. However, relaxing the assumption by adding 
term dependencies into the retrieval model is still a 
basic IR problem. Relaxing TIA is not easy be-
cause improperly relaxing may introduce much 
noisy information which will hurt the final per-
formance. Defining the term dependency is the 
first step in dependence retrieval model. Two re-
search directions are taken to define the term de-
pendency. The first is to treat term dependencies as 
term proximity, for example, the Bi-gram Model 
(F. Song and W. B. Croft, 1999) and Markov Ran-
dom Field Model (D. Metzler and W. B. Croft, 
2005) in language model. The second direction is 
to derive term dependencies by using some linguis-
tic structures, such as POS block (Lioma C. and 
Ounis I., 2007) or Noun/Verb Phrase (Mitra et al, 
1997), Maximum Spanning Tree (C. J. van 
Rijsbergen, 1979) and Linkage Model (Gao et al, 
2004) etc.  
Though linguistic information is intensively 
used in QA (Question Answering) and IE (Infor-
mation Extraction) task, it is seldom used in docu-
ment retrieval (T. Brants, 2004). In document re-
trieval, how effective linguistic dependencies 
would be compared with term proximity still needs 
to be explored thoroughly. 
In this paper, we use syntactic relationships de-
rived by a popular dependency parser, Minipar (D. 
Lin, 1998), as linguistic dependencies. Minipar is a 
broad-coverage parser for the English language. It 
represents the grammar as a network of nodes and 
links, where the nodes represent grammatical cate-
gories and the links represent types of dependency.  
We extract the dependencies between content 
words as term dependencies. 
To systematically compare term proximity with 
syntactic dependencies, we study the dependence 
retrieval models in language model framework and 
present a smooth-based dependence language 
model (SDLM). It can incorporate these two kinds 
of term dependencies. The experiments in TREC 
collections show that SDLM with syntactic rela-
tionships achieves better result than with the term 
proximity. 
The rest of this paper is organized as follows. 
Section 2 reviews some previous relevant work, 
197
Section 3 presents the definition of term depend-
ency using syntactic relationships derived by 
Minipar. Section 4 presents in detail the smooth-
based dependence language model. A series of ex-
periments on TREC collections are presented in 
Section 5. Some conclusions are summarized in 
Section 6. 
2 Related Work 
Generally speaking, when using term dependencies 
in language modeling framework, two problems 
should be considered: The first is to define and 
identify term dependencies; the second is to 
integrate term dependencies into a weighting 
schema. Accordingly, this section briefly reviews 
some recent relevant work, which is summarized 
into two parts: the definition of term dependencies 
and weight of term dependencies. 
2.1 Definition of Term Dependencies 
In definition of term dependencies, there are two 
main methods: shallow parsing by some linguistic 
tools and term proximity with co-occurrence in-
formation. Both queries and documents are repre-
sented as a set of terms and term dependencies 
among terms. Table 1 summarizes some recent 
related work according to the method they use to 
identify term dependencies in queries and docu-
ments. 
Methods Document 
Parsing 
Document Proximity
Query 
Parsing 
I: DM,LDM, 
etc. 
II: CULM, RP, etc. 
Query 
Proximity 
III: NIL IV: BG ,WPLM, 
MRF, etc. 
Table 1. Methods in identifying dependencies 
In the part I of table 1, DM is Dependence Lan-
guage Model (Gao et al, 2004). It introduces a de-
pendency structure, called linkage model. The 
linkage structure assumes that term dependencies 
in a sentence form an acyclic, planar graph, where 
two related terms are linked. LDM (Gao et al, 
2005) represents the related terms as linguistic 
concepts, which can be semantic chunks (e.g. 
named entities like person name, location name, 
etc.) and syntactic chunks (e.g. noun phrases, verb 
phrases, etc.).  
In the part II of table 1, CULM (M. Srikanth and 
R. Srihari, 2003) is a concept unigram language 
model. The parser tree of a user query is used to 
identify the concepts in the query. Term sequence 
in a concept is treated as bi-grams in the document 
model.  RP (Recognized Phrase, S. Liu et al, 2004) 
uses some linguistic tools and statistical tools to 
recognize four types of phrase in the query, includ-
ing proper names, dictionary phrase, simple phrase 
and complex phrase. A phrase is in a document if 
all its content words appear in the document within 
a certain window size. The four kinds of phrase 
correspond to variant window size. 
In the part IV of table 1, BG (bi-gram language 
model) is the simplest model which assumes term 
dependencies exist only between adjacent words 
both in queries and documents. WPLM (word pairs 
in language model, Alvarez et al, 2004) relax the 
co-occurrence window size in documents to 5 and 
relax the order constraint in bi-gram model. MRF 
(Markov Random Field) classify the term depend-
encies in queries into sequential dependence and 
full dependence, which respectively corresponds to 
ordered and unordered co-occurrence within a pre-
define-sized window in documents. 
From above discussion we can see that when the 
query is sentence-based, parsing method is pre-
ferred to proximity method. When the query is 
keyword-based, proximity method is preferred to 
parsing method. Thorsten (T. Brants, 2004) note: 
the longer the queries, the bigger the benefit of 
NLP. This conclusion also holds for the definition 
of query term dependencies. 
2.2 Weight of Term Dependencies 
In dependence retrieval model, the final relevance 
score of a query and a document consists of both 
the independence score and dependence score, 
such as Bahadur Lazarsfeld expansion (R. M. 
Losee, 1994) in classical probabilistic IR models. 
However, Spark Jones et al point out that without 
a theoretically motivated integration model, docu-
ments containing dependencies (e.g. phrases) may 
be over-scored if they are weighted in the same 
way as single words (Jones et al, 1998). Smooth-
ing strategy in language modeling framework pro-
vide such an elegant solution to incorporate term 
dependencies. 
In the simplest bi-gram model, the probability of 
bi-gram (qi-1,qi) in document D is smoothed by its 
unigram: 
198
)|(
)|(
),|(,
),|()1()|(),|(
1
1
1
11
DqP
DqqP
DqqPwhere
DqqPDqPDqqP
i
ii
ii
iiiiismoothed
?
?
?
??
?
??+?= ??
  (1) 
Further, the probability of bi-gram (qi-1,qi) in 
document P(qi|qi-1,D) can be smoothed by its prob-
ability in collection P(qi|qi-1,C). If P(qi|qi-1,D) is 
smoothed as Equation (1), the relevance score of 
query Q={q1q2?qm} and document D is: 
)|()|(
)|(
log)|,(,
)|,()|(log
)
)|()|(
)|(1
1log()|(log
)
)|(
),|(
)1(log()|(log
)),|()1()|(log()|(log
),|(log)|(log)|(log
1
1
1
...2
1
...1
...2 1
1
...1
...2
1
...1
...2
11
...2
11
DqPDqP
DqqP
DqqMIusually
DqqMIDqP
DqPDqP
DqqP
DqP
DqP
DqqP
DqP
DqqPDqPDqP
DqqPDqPDQP
ii
ii
ii
mi
iismoothed
mi
i
mi ii
ii
mi
i
mi i
ii
mi
i
mi
iii
mi
iismoothed
?
?
?
=
?
=
= ?
?
=
=
?
=
=
?
=
?
??
+=
??
?++?
??++=
??+?+=
+=
??
??
??
?
?
?
?
??
??
 (2) 
In Equation (2), the first score term is independ-
ence unigram score and the second score term is 
smoothed dependence score. Usually ? is set to 0.9, 
i.e., the dependence score is given a less weight 
compared with the independence score. 
DM (Gao et al, 2004), which can be regarded as 
the generalization of the bi-gram model, gives the 
relevance score of a document as:  
?
?
?
=
+
+=
Lji
ji
mi
i
DLqqMI
DLPDqPDQP
),(
...1
),|,(
)|(log)|(log)|(log
     (3) 
In Equation (3),L is the set of term dependencies 
in query Q. The score function consists of three 
parts: a unigram score, a smoothing factor 
logP(L|D), and a dependence score MI(qi,qj|L,D). 
MRF (D. Metzler and W. B. Croft, 2005) com-
bines the score of full independence, sequential 
dependence and full dependence in an interpolated 
way with the weight (0.8, 0.1, 0.1).  
Though these above models are derived from 
different theories, smoothing is an important part 
when incorporating term dependencies.  
3 Syntactic Parsing of Queries and 
Documents 
Term dependencies defined as term proximity may 
contain many ?noisy? dependencies. It?s our belief 
that parsing technique can filter out some of these 
noises and syntactic relationship is a clue to define 
parser, Minipar, to extract the syntactic depend-
ency between words.  In this section we will dis-
cuss the extraction of syntactic dependencies and 
the indexing schemes of term dependencies. 
3.1 Extraction of Syntactic Dependencie
term dependencies.  We use a popular dependency 
s 
ary 
an
des in the parsing result are single 
w
A dependency relationship is an asymmetric bin
relationship between a word called head (or 
governor, parent), and another word called 
modifier (or dependent, daughter). Dependency 
grammars represent sentence structures as a set of 
dependency relationships. For example, Figure 1 
takes the description field of TREC topic 651 as an 
example and shows part of the parsing result of 
Minipar. 
 
In Figure 1, Cat is the lexical category of word, 
d Rel is a label assigned to the syntactic depend-
encies, such as subject (sub), object (obj), adjunct 
(mod:A), prepositional attachment (Prep:pcomp-n), 
etc. Since function words have no meaning, the 
dependency relationships including function words, 
such as N:det:Det, are ignored. Only the depend-
ency relationships between content words are ex-
tracted. However, prepositional attachment is an 
exception. A prepositional noun phrase contains 
two parts: (N:mod:Prep) and (Prep:pcomp-n:N). 
We combine these two parts and get a relationship 
between nouns. 
Mostly, the no
ords. When the nodes are proper names, diction-
ary phrases, or compound words connected by hy-
phen, there are more than one word in the node. 
For example, the 5th and 6th relationship in Figure 1 
describes a compound word ?make up?.  We di-
vide these nodes into bi-grams, which assume de-
pendencies exist between adjacent words inside the 
Figure 1. Parsing Result of Minipar 
Node2 Node1 Cat1:Rel:Cat2 
TREC Topic 651: ?How is the ethnic make-
up of the U.S. population changing?? 
? 
3   makeup N:det:Det the 
4   makeup N:mod:A ethnic 
5   makeup N:lex-mod:U make 
6   makeup N:lex-mod:U - 
8   makeup N:mod:Prep of 
11 of  Prep:pcomp-n:N population 
9   population N:det:Det the 
10 population N:nn:N  U.S. 
? 
199
nodes.  If the compound-word node has a relation-
ship with other nodes, each word in the compound-
word node is assumed to have a relationship with 
the other nodes. Finally, the term dependencies are 
represented as word pairs. The direction of syntac-
tic dependencies is ignored. 
3.2 Indexing of Term Dependencies 
And the 
 
of
e that 
trieval status value (RSV) has the form: 
Parsing is a time-consuming process. 
documents parsing should be an off-line process. 
The parsing results, recognized as term dependen-
cies, should be organized efficiently to support the 
computation of relevance score at the retrieval step. 
As a supplement of regular documents?words 
inverted index, the indexing of term dependencies 
is organized as documents?dependencies lists. 
For example, Document A has n unique words; 
each of these n words has relationships with at 
least one other word. Then the term dependencies 
inside these n words can be represented as a half-
angle matrix as Figure 2 shows.  
 
The (i,j)-th element of the matrix is the number
 times that tidi and tidj have a dependency in 
document A. The matrix has the size of (n-1)*n/2 
and it is stored as list of size (n-1)*n/2. Each 
document corresponds to such a matrix. When ac-
cessing the term dependencies index, the global 
word id in the regular index is firstly converted to 
the internal id according to the word?s appearance 
order in the document. The internal id is the index 
of the half-angle matrix. Using the internal id pair, 
we can get its position in the matrix. 
4 Smooth-based Dependence Model 
From the discussion in section 2.2, we can se
smoothing is very important not only in unigram 
language model, but also in dependence language 
model. Taking the smoothed unigram model (C. 
Zhai and J. Lafferty, 2001) as the example, the re-
D
DQw D
DML
UG QCwp
Dwp
QwcDQRSV ?? log||)|(
)|(
log),(),( += ?
??
In Equation (4), c(w,Q) is the frequency of w
Q. The equation has three parts: PDML(w|D), ?D and 
P(
 (4) 
 in 
w|C). PDML(w|D) is the discounted maximum 
likelihood estimation of unigram P(w|D), ?D is the 
smoothing coefficient of document D, and P(w|C) 
is collection language model. If we use a smooth-
ing strategy as the smoothed MI in Equation (2), 
and replace term w with term pair (wi,wj), we can 
get the smoothed dependence model as: 
?
??
?+
=
DLww jismooth
j
ji
ji
Cwwp
D
Qwwc
),(
0 ))|,(
)|
1log(),,( ?  (5) 
In Equation (5), ?0 is the smoothing coefficient. 
Psm (w ,w |D) and Psm (w ,w |C) is the smoothed 
w
i j
e the Psmooth(wi,wj|D): 
 pair with relation-
sh
ismooth
DEP
wwp
DQRSV
,(
),(
ooth i j ooth i j
eight of term pair (wi,wj) in document D and col-
lection C.  
4.1 Smoothing P(w ,w |D) 
We use two parts to estimat
one is the weight of the term
ips in D, P(wi,wj|R,D), the other is the weight of 
the term co-occurrence in D, Pco(wi,wj|D). These 
two parts are defined as below: 
|D|)/(wCD)|P(w
D)|P(wD)|P(wD)|w,(wP
|D|R)/,w,(wCD)R,|w,P(w jiDji
?=
tid1 tid2 ? tidn-1 tidn
tid1
tid2
? 
 
tidn-1
tidn
 
??
?
?
??
?
?
?
??
?
?
??
?
?
?
0****
10...**
03...**
45..0*
20...10
 
iDi
jijiCO
=
=
   (6) 
|D| is the document length, CD(wi,wj,R) denotes 
the count of the dependency (wi,w ) in the docu-
m
jico1
j
ent D, and CD(wi) is the frequency of word wi in 
D. Psmooth(wi,wj|D) is defined as a combination of 
the two parts: 
P )?-(1
D)R,|w,P(w? D)|w,(wP ji1jismooth
?+ D)|w,(w
?=
 (7) 
Figure 2. Half-angle matrix of term dependencies
4.2 Smoothing P(wi,wj|C) 
bability of term pair 
. We use docu-
m
To directly estimate the pro
(w ,w ) in the collection is not easyi j
ent frequency of term pair (wi,wj) as its approxi-
mation. Same as Psmooth(wi,wj|D), Psmooth(wi,wj|C) 
consists of two parts: one is the document fre-
quency of term pair (wi,wj), DF(wi,wj), the other is 
the averaged document frequency of wi and wj. 
Then, Psmooth(wi,wj|C) is defined as: 
Dji
Djijismooth
CwDFwDF
CwwDFCwwP
||)()()1(
||),()|,( 2
???+ 2
?=
?
?
  (8) 
200
In Equation (8), |C|D is the count of Document in 
Collection C.  
Finally, if substituting Equation (7) and (8) into 
Eq
). The final retrieval status value of 
th
s 
To answer the question whether the syntactic de-
term proximity, 
,w ,R) in Equa-
tio
parameter 
is 
ns. Some statistics of the col-
lec
rameters (? ,? ,? ), 
SD
(MB) Doc.
uation (5), there are three parameters (?0,?1,?2) 
in RSVDEP(Q,D
e smooth-based dependence model, RSVSDLM, is 
the sum of RSVDEP and RSVUG: 
),(),(),( DQRSVDQRSVDQRSV UGDEPSDLM +=   (9) 
5 Experiments and Result
pendencies is more effective than 
we systematically compared their performance on 
two kinds of queries. One is verbose queries (the 
description field of TREC topics), the other is short 
queries (the title field of TREC topics). Since the 
verbose queries are sentence-level, they are parsed 
by Minipar to get the syntactic dependencies. In 
short queries, term proximity is used to define the 
dependencies, which assume every two words in 
the queries have a dependency.  
Our smooth-based dependence language model 
(SDLM) is used as dependence retrieval model in 
the experiments. If defining CD(wi j
n (6) to different meanings, we can get a de-
pendence model with syntactic dependencie, 
SDLM_Syn, or a dependence model with term 
proximity, SDLM_Prox. In SDLM_Syn, 
CD(wi,wj,R) is the count of syntactic dependencies 
between wi and wj in D. In SDLM_Prox, 
CD(wi,wj,R) is the number of times the terms wi 
and wj appear within a window N terms. 
We use Dirichlet-Prior smoothed KL-
Divergence model as the unigram model in Equa-
tion (9). The Dirichlet-Prior smoothing 
set to 2000. This unigram model, UG, is also the 
baseline in the experiments. The main evaluation 
metric in this study is the non-interpolated average 
precision (AvgPr.) 
We evaluated the smooth-based dependence 
language model in two document collections and 
four query collectio
tions are shown in Table 2. 
Three retrieval models are evaluated in the 
TREC collections: UG, SDLM_Syn and 
SDLM_Prox. Besides the pa 0 1 2
LM_Prox has one more parameter than 
SDLM_Syn. It is the window size N of 
CD(wi,wj,R). In the experiments, we tried the win-
dow size N of 5, 10, 20 and 40 to find the optimal 
setting. We find the optimal N is 10. This size is 
close to sentence length and it is used in the fol-
lowing experiments. 
Coll. Queries Documents Size # 
AP 51-200 Associated Press 
(1  
489 164,597
988,1989) in
Disk2 
TR -8EC7 351-450
Ro
Hard queries in
bust04 35 hard
351-450
Robust04
New 
651-700 
ex.672
Disk 4&5 
(no CR) 
3,120 528,155
Table 2.  TREC collections 
eter ,?2) were trained on three query 
se 700. Each query set 
was divided into two halves, and we applied two-
fo
Param s (?0,?1
ts: 51-200, 351-450 and 651-
ld cross validation to get the final result. We 
trained (?0,?1,?2) by directly maximizing MAP 
(mean average precision). Since the parameter 
range was limited, we used a linear search method 
at step 0.1 to find the optimal setting of (?0,?1,?2). 
 
Topic Index
0 20 40 60 80 100 120 140 160
A
vg
P
r
0.0
.2
.4
.6
.8
1.0
UG
SDLM_Syn
Topic Index
0 20 40 60 80 100
A
vg
P
r
0.0
.2
.4
.6
.8
1.0
UG
SDLM_Syn
Topic Index
0 10 20 30 40
A
vg
P
r
0.00
.05
.10
.15
.20
.25
.30
.35
UG
SDLM_Syn
Topic Index
0 10 20 30 40 50
A
vg
P
r
0.0
.2
.4
.6
.8
1.0
UG
SDLM_Syn
 
Figure 3 UG vs. SDLM_Syn in verbose queries: 
Top Left (51-200), Top Right (351-450), Bottom 
Left (hard topics in 351-450), and Bottom Right 
 Table 3 and Table 4 respectively. The 
settings of (?0,?1,?2) used in the experiments are 
al
(651-700) 
The results on verbose queries and short queries 
are listed in
so listed. A star mark after the change percent 
value indicates a statistical significant difference at 
the 0.05 level(one-sided Wilcoxon test). In verbose 
queries, we can see that SDLM has distinct 
201
 
UG SDLM_Prox SDLM_Syn collections 
AvgPr. AvgPr. %ch over UG (?0,?1,?2) AvgPr. %ch over UG (?0,?1,?2) 
AP 0.2159 0.2360 9.31* (1.8,0.6,0.9) 0.2393 10.84* (1.9,0.7,0.9)
TREC7-8 0.1893 0.2049 8.24* (1.2,0.1,0.2) 0.2061 8.87* (0.4,0.1,0.9)
Robust04_hard 0.0909 0.1049 15.40* (1.2,0.1,0.2) 0.1064 17.05* (0.4,0.1,0.9)
Robust04_new 0.2754 0.3022 9.73* (0.7,0.1,0.3) 0.3023 9.77* (0.7,0.1,0.3)
Table 3. Comparison results on verbose queries 
UG SDLM_Prox SDLM_Syn collections 
AvgPr. AvgPr. %ch over UG (?0,?1,?2) AvgPr. %ch over UG (?0,?1,?2) 
AP 0.2643 0.2644 0 (1.3,0.6,0.1) 0.2647 0.15 (1.1,0.5,0.2)
TREC7-8 0.2069 0.2076 0.34 (1.2,0.3,0.2) 0.2070 0 (1,0.1,0.2) 
Robust04_hard 0.1037 0.1044 0.68 (1.2,0.3,0.2) 0.1045 0.77 (1,0.1,0.2) 
Robust04_new 0.2771 0.2888 4.22* (1.3,0.3,0.4) 0.2869 3.54* (1.3,0.1,0.4)
Table 4. Comparison results on short queries 
Topic Index
0 20 40 60 80 100 120 140 160
A
vg
P
r
0.0
.2
.4
.6
.8
1.0
SDLM_Prox
SDLM_Syn
Topic Index
0 20 40 60 80 100
A
vg
P
r
0.0
.2
.4
.6
.8
1.0
SDLM_Prox
SDLM_Syn
 
Topic Index
0 10 20 30 40
A
vg
P
r
0.00
.05
.10
.15
.20
.25
.30
.35
SDLM_Prox
SDLM_Syn
Topic Index
0 10 20 30 40 50
A
vg
P
r
0.0
.2
.4
.6
.8
1.0
SDLM_Prox
SDLM_Syn
 
Figure 4. SDLM_Prox vs. SDLM_Syn in verbose 
queries: Top Left (51-200), Top Right (351-450), 
Bottom Left (hard topics in 351-450), Bottom 
Right (651-700) 
improvement over UG and SDLM_Syn has robust 
improvement over SDLM_Prox. In short queries, 
SDLM has slight improvement over UG and 
SDLM_Syn is comparative with SDLM_Prox. 
To study the effectiveness of syntactic depend-
encies in detail, Figure 3 and 4 compare 
SDLM_Syn and UG, SDLM_Syn and 
SDLM_Prox topic by topic in verbose queries. 
As shown in Figure 3 and Figure 4, SDLM_Syn 
achieves substantial improvements over UG in the 
majority of queries. While SDLM_Syn is com-
parative with SDLM_Prox in most of the queries, 
SDLM_Syn still get some noticeable improve-
ments over SDLM_Prox. 
From Table 3 and 4, we can see while the pa-
rameters (?0,?1,?2) change a lot in two different 
document collections, there is little change in the 
same document collection. This shows the robust-
ness of our smooth-based dependence language 
model. 
6 Conclusion 
In this paper we have systematically studied the 
effectiveness of syntactic dependencies compared 
with term proximity in dependence retrieval 
model. To compare the effectiveness of syntactic 
dependencies and term proximity, we develop a 
smooth-based dependence language model that 
can incorporate different term dependencies.  
Experiments on four TREC collections indicate 
the effectiveness of syntactic dependencies: In 
verbose queries, the improvement of syntactic 
dependencies over term proximity is noticeable; 
In short queries, the improvement is not notice-
able.  For keywords-based short queries with av-
erage length of 2-3 words, the term dependencies 
in the queries are very few. So the improvement 
of dependence retrieval model over independence 
unigram model is very limited. Meanwhile, the 
difference between syntactic dependencies and 
term proximity is not noticeable. For dependence 
retrieval model, we can get the same conclusion as 
Thorsten Brants: the longer the queries are, the 
bigger the benefit of NLP is. 
 
202
References 
ijsber R
worths, 1979. 
varez, s, Ji Nie
e g fo tion
s  200 686-
s for l e m  a
formation re ,
ges 334?342
b aluati MINI
 the E ion o
ndom 
field model for term dependencies, In Proceedings of 
SIGIR?05, Pages 472-479, 2005 
Fei Song and W. Bruce Croft. A general language 
model for information retrieval. In Proceedings of 
SIGIR?99, pages 279-280, 1999. 
Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu and Gu -
hong Cao, Dependence Language Model for Info -
mation Retrieval, In Proceedings of SIGIR?04, 
Pages:170-177, 2004 
Jianfeng Gao, Haoliang Qi, Xinsong Xia and Jian-Yun 
Nie. Linear Discriminant Model for Information Re-
trieval. In Proceedings of SIGIR?05, Pages:290-297, 
2005 
y Computer Laboratory. 1998 
Shuang Liu, Fang Liu, Clement Yu and Weiyi Meng, 
An Effective Approach to Document Retrieval via 
Utilizing WordNet an ses, In Pro-
in I
 Ter ence he 
u feld e n. Inf ess-
d men 3?3
 atur uage  In-
formation Retrieval . In Proceedings of 20th Interna-
tional Conference o nal Linguistics, 
e
C. J. van R g foen. In rmation etrieval. Butter-
Carmen Al  Philippe Langlai an-Yun , Bahad
Word Pairs in 
Retrieval, In Pr
Languag
oceeding
 Modelin
 of RIAO
r Informa
4, Pages 
 ing an
705, 2004 
Chengxiang 
ing method
Zhai an n Lafferty. A stud Joh
angua
dy of smooth-
lied to ad hoc g
trieval. In
odels
 Proceedi
pp
ngs of SIGIR?01in
pa
 
 , 2001 
Dekang Lin, Dep
PAR, Proceedin
endency-
gs of Wo
ased Ev
rkshop on
on of 
valuat
-
f 
Parsing Systems, Granada, Spain, May,
Donald Metzler and W. Bruce Croft, A Markov ra
 1998. 
i
r
K. Sparck Jones, S. Walker, and S. E. Robertson, A 
probabilistic model of information retrieval: devel-
opment and status. Technical Report TR-446, Cam-
bridge Universit
Lioma C. and Ounis I., A Syntactically-Based Query 
Reformulation Technique for Information Retrieval, 
Information Processing and Management (IPM), El-
sevier Science, 2007 
M. Mitra, C. Buckley, A. Singhal, and C. Cardie. An 
Analysis of Statistical and Syntactic Phrases. In 
Proceedings of RIAO-97, 5th International Confer-
ence ?Recherche d?Information Assistee par Ordi-
nateur?, pages 200-214, Montreal, CA, 1997. 
Munirathnam Srikanth and Rohini Srihari, Exploiting 
Syntactic Structure of Queries in a Language Mod-
eling Approach to IR, In Proceedings of CIKM?03, 
Pages: 476-483, 2003 
 
d Recognizing Phra
-2ceed gs of SIG R?04, Pages: 266 72, 2004 
Robert M. Losee. m depend : Truncating t
r Lazars xpansio ormation Proc
 Manage t, 30(2):29 03, 1994. 
Thorsten Brants. N al Lang Processing in
n Computatio
Antw rp, Belgium, 2004:1-13. 
 
203
Exploiting Parallel Texts for Word Sense Disambiguation:  
An Empirical Study 
Hwee Tou Ng 
Bin Wang 
Yee Seng Chan 
Department of Computer Science 
National University of Singapore 
3 Science Drive 2, Singapore 117543 
{nght, wangbin, chanys}@comp.nus.edu.sg 
 
Abstract 
A central problem of word sense disam-
biguation (WSD) is the lack of manually 
sense-tagged data required for supervised 
learning. In this paper, we evaluate an ap-
proach to automatically acquire sense-
tagged training data from English-Chinese 
parallel corpora, which are then used for 
disambiguating the nouns in the 
SENSEVAL-2 English lexical sample 
task. Our investigation reveals that this 
method of acquiring sense-tagged data is 
promising. On a subset of the most diffi-
cult SENSEVAL-2 nouns, the accuracy 
difference between the two approaches is 
only 14.0%, and the difference could nar-
row further to 6.5% if we disregard the 
advantage that manually sense-tagged 
data have in their sense coverage. Our 
analysis also highlights the importance of 
the issue of domain dependence in evalu-
ating WSD programs. 
1 Introduction 
The task of word sense disambiguation (WSD) is 
to determine the correct meaning, or sense of a 
word in context. It is a fundamental problem in 
natural language processing (NLP), and the ability 
to disambiguate word sense accurately is important 
for applications like machine translation, informa-
tion retrieval, etc. 
Corpus-based, supervised machine learning 
methods have been used to tackle the WSD task, 
just like the other NLP tasks. Among the various 
approaches to WSD, the supervised learning ap-
proach is the most successful to date. In this ap-
proach, we first collect a corpus in which each 
occurrence of an ambiguous word w has been 
manually annotated with the correct sense, accord-
ing to some existing sense inventory in a diction-
ary. This annotated corpus then serves as the 
training material for a learning algorithm. After 
training, a model is automatically learned and it is 
used to assign the correct sense to any previously 
unseen occurrence of w in a new context. 
While the supervised learning approach has 
been successful, it has the drawback of requiring 
manually sense-tagged data. This problem is par-
ticular severe for WSD, since sense-tagged data 
must be collected separately for each word in a 
language. 
One source to look for potential training data 
for WSD is parallel texts, as proposed by Resnik 
and Yarowsky (1997). Given a word-aligned paral-
lel corpus, the different translations in a target lan-
guage serve as the ?sense-tags? of an ambiguous 
word in the source language. For example, some 
possible Chinese translations of the English noun 
channel are listed in Table 1. To illustrate, if the 
sense of an occurrence of the noun channel is ?a 
path over which electrical signals can pass?, then 
this occurrence can be translated as ???? in Chi-
nese. 
WordNet 
1.7 sense id 
Lumped 
sense id 
Chinese translations WordNet 1.7 English sense descriptions 
1 1 ?? A path over which electrical signals can pass 
2 2 ?? ?? ??? A passage for water  
3 3 ? A long narrow furrow 
4 4 ?? A relatively narrow body of water  
5 5 ?? A means of communication or access 
6 6 ?? A bodily passage or tube 
7 1 ?? A television station and its programs 
 
Table 1: WordNet 1.7 English sense descriptions, the actual lumped senses, and Chinese translations 
of the noun channel used in our implemented approach 
 
 
Parallel corpora Size of English texts (in 
million words (MB)) 
Size of Chinese texts (in 
million characters (MB)) 
Hong Kong News 5.9 (39.4) 10.7 (22.8) 
Hong Kong Laws 7.0 (39.8) 14.0 (22.6) 
Hong Kong Hansards 11.9 (54.2) 18.0 (32.4) 
English translation of Chinese Treebank 0.1 (0.7) 0.2 (0.4) 
Xinhua News 3.6 (22.9) 7.0 (17.0) 
Sinorama 3.2 (19.8) 5.3 (10.2) 
Total 31.7 (176.8) 55.2 (105.4) 
 
Table 2: Size of English-Chinese parallel corpora 
 
This approach of getting sense-tagged corpus 
also addresses two related issues in WSD. Firstly, 
what constitutes a valid sense distinction carries 
much subjectivity. Different dictionaries define a 
different sense inventory. By tying sense distinc-
tion to the different translations in a target lan-
guage, this introduces a ?data-oriented? view to 
sense distinction and serves to add an element of 
objectivity to sense definition. Secondly, WSD has 
been criticized as addressing an isolated problem 
without being grounded to any real application. By 
defining sense distinction in terms of different tar-
get translations, the outcome of word sense disam-
biguation of a source language word is the 
selection of a target word, which directly corre-
sponds to word selection in machine translation.  
While this use of parallel corpus for word sense 
disambiguation seems appealing, several practical 
issues arise in its implementation: 
(i) What is the size of the parallel corpus 
needed in order for this approach to be able to dis-
ambiguate a source language word accurately? 
(ii) While we can obtain large parallel corpora 
in the long run, to have them manually word-
aligned would be too time-consuming and would 
defeat the original purpose of getting a sense-
tagged corpus without manual annotation. How-
ever, are current word alignment algorithms accu-
rate enough for our purpose? 
(iii) Ultimately, using a state-of-the-art super-
vised WSD program, what is its disambiguation 
accuracy when it is trained on a ?sense-tagged? 
corpus obtained via parallel text alignment, com-
pared with training on a manually sense-tagged 
corpus? 
Much research remains to be done to investi-
gate all of the above issues. The lack of large-scale 
parallel corpora no doubt has impeded progress in 
this direction, although attempts have been made to 
mine parallel corpora from the Web (Resnik, 
1999). 
However, large-scale, good-quality parallel 
corpora have recently become available. For ex-
ample, six English-Chinese parallel corpora are 
now available from Linguistic Data Consortium. 
These parallel corpora are listed in Table 2, with a 
combined size of 280 MB. In this paper, we ad-
dress the above issues and report our findings, ex-
ploiting the English-Chinese parallel corpora in 
Table 2 for word sense disambiguation. We evalu-
ated our approach on all the nouns in the English 
lexical sample task of SENSEVAL-2 (Edmonds 
and Cotton, 2001; Kilgarriff 2001), which used the 
WordNet 1.7 sense inventory (Miller, 1990). While 
our approach has only been tested on English and 
Chinese, it is completely general and applicable to 
other language pairs. 
2 
2.1 
2.2 
2.3 
2.4 
Approach 
Our approach of exploiting parallel texts for word 
sense disambiguation consists of four steps: (1) 
parallel text alignment (2) manual selection of tar-
get translations (3) training of WSD classifier (4) 
WSD of words in new contexts. 
Parallel Text Alignment 
In this step, parallel texts are first sentence-aligned 
and then word-aligned. Various alignment algo-
rithms (Melamed 2001; Och and Ney 2000) have 
been developed in the past. For the six bilingual 
corpora that we used, they already come with sen-
tences pre-aligned, either manually when the cor-
pora were prepared or automatically by sentence-
alignment programs. After sentence alignment, the 
English texts are tokenized so that a punctuation 
symbol is separated from its preceding word. For 
the Chinese texts, we performed word segmenta-
tion, so that Chinese characters are segmented into 
words. The resulting parallel texts are then input to 
the GIZA++ software (Och and Ney 2000) for 
word alignment. 
In the output of GIZA++, each English word 
token is aligned to some Chinese word token. The 
alignment result contains much noise, especially 
for words with low frequency counts. 
Manual Selection of Target Translations 
In this step, we will decide on the sense classes of 
an English word w that are relevant to translating w 
into Chinese. We will illustrate with the noun 
channel, which is one of the nouns evaluated in the 
English lexical sample task of SENSEVAL-2. We 
rely on two sources to decide on the sense classes 
of w: 
(i) The sense definitions in WordNet 1.7, which 
lists seven senses for the noun channel. Two 
senses are lumped together if they are translated in 
the same way in Chinese. For example, sense 1 and 
7 of channel are both translated as ???? in Chi-
nese, so these two senses are lumped together. 
(ii) From the word alignment output of 
GIZA++, we select those occurrences of the noun 
channel which have been aligned to one of the 
Chinese translations chosen (as listed in Table 1). 
These occurrences of the noun channel in the Eng-
lish side of the parallel texts are considered to have 
been disambiguated and ?sense-tagged? by the ap-
propriate Chinese translations. Each such occur-
rence of channel together with the 3-sentence 
context in English surrounding channel then forms 
a training example for a supervised WSD program 
in the next step. 
The average time taken to perform manual se-
lection of target translations for one SENSEVAL-2 
English noun is less than 15 minutes. This is a rela-
tively short time, especially when compared to the 
effort that we would otherwise need to spend to 
perform manual sense-tagging of training exam-
ples. This step could also be potentially automated 
if we have a suitable bilingual translation lexicon. 
Training of WSD Classifier 
Much research has been done on the best super-
vised learning approach for WSD (Florian and 
Yarowsky, 2002; Lee and Ng, 2002; Mihalcea and 
Moldovan, 2001; Yarowsky et al, 2001). In this 
paper, we used the WSD program reported in (Lee 
and Ng, 2002). In particular, our method made use 
of the knowledge sources of part-of-speech, sur-
rounding words, and local collocations. We used 
na?ve Bayes as the learning algorithm. Our previ-
ous research demonstrated that such an approach 
leads to a state-of-the-art WSD program with good 
performance. 
WSD of Words in New Contexts 
Given an occurrence of w in a new context, we 
then used the na?ve Bayes classifier to determine 
the most probable sense of w. 
noun No. of 
senses 
before 
lumping 
No. of 
senses 
after 
lumping 
M1 P1 P1-
Baseline 
M2 M3 P2 P2- 
Baseline 
child 4 1 - - - - - - - 
detention 2 1 - - - - - - - 
feeling 6 1 - - - - - - - 
holiday 2 1 - - - - - - - 
lady 3 1 - - - - - - - 
material 5 1 - - - - - - - 
yew 2 1 - - - - - - - 
bar 13 13 0.619 0.529 0.500 - - - - 
bum 4 3 0.850 0.850 0.850 - - - - 
chair 4 4 0.887 0.895 0.887 - - - - 
day 10 6 0.921 0.907 0.906 - - - - 
dyke 2 2 0.893 0.893 0.893 - - - - 
fatigue 4 3 0.875 0.875 0.875 - - - - 
hearth 3 2 0.906 0.844 0.844 - - - - 
mouth 8 4 0.877 0.811 0.846 - - - - 
nation 4 3 0.806 0.806 0.806 - - - - 
nature 5 3 0.733 0.756 0.522 - - - - 
post 8 7 0.517 0.431 0.431 - - - - 
restraint 6 3 0.932 0.864 0.864 - - - - 
sense 5 4 0.698 0.684 0.453 - - - - 
stress 5 3 0.921 0.921 0.921 - - - - 
art 4 3 0.722 0.494 0.424 0.678 0.562 0.504 0.424 
authority 7 5 0.879 0.753 0.538 0.802 0.800 0.709 0.538 
channel 7 6 0.735 0.487 0.441 0.715 0.715 0.526 0.441 
church 3 3 0.758 0.582 0.573 0.691 0.629 0.609 0.572 
circuit 6 5 0.792 0.457 0.434 0.683 0.438 0.446 0.438 
facility 5 3 0.875 0.764 0.750 0.874 0.893 0.754 0.750 
grip 7 7 0.700 0.540 0.560 0.655 0.574 0.546 0.556 
spade 3 3 0.806 0.677 0.677 0.790 0.677 0.677 0.677 
 
Table 3: List of 29 SENSEVAL-2 nouns, their number of senses, and various accuracy figures 
3 An Empirical Study 
We evaluated our approach to word sense disam-
biguation on all the 29 nouns in the English lexical 
sample task of SENSEVAL-2 (Edmonds and Cot-
ton, 2001; Kilgarriff 2001). The list of 29 nouns is 
given in Table 3. The second column of Table 3 
lists the number of senses of each noun as given in 
the WordNet 1.7 sense inventory (Miller, 1990). 
We first lump together two senses s1 and s2 of a 
noun if s1 and s2 are translated into the same Chi-
nese word. The number of senses of each noun 
after sense lumping is given in column 3 of Table 
3. For the 7 nouns that are lumped into one sense 
(i.e., they are all translated into one Chinese word), 
we do not perform WSD on these words. The aver-
age number of senses before and after sense lump-
ing is 5.07 and 3.52 respectively. 
After sense lumping, we trained a WSD classi-
fier for each noun w, by using the lumped senses in 
the manually sense-tagged training data for w pro-
vided by the SENSEVAL-2 organizers. We then 
tested the WSD classifier on the official 
SENSEVAL-2 test data (but with lumped senses) 
for w. The test accuracy (based on fine-grained 
scoring of SENSEVAL-2) of each noun obtained is 
listed in the column labeled M1 in Table 3. 
We then used our approach of parallel text 
alignment described in the last section to obtain the 
training examples from the English side of the par-
allel texts. Due to the memory size limitation of 
our machine, we were not able to align all six par-
allel corpora of 280MB in one alignment run of 
GIZA++. For two of the corpora, Hong Kong Han-
sards and Xinhua News, we gathered all English 
sentences containing the 29 SENSEVAL-2 noun 
occurrences (and their sentence-aligned Chinese 
sentence counterparts). This subset, together with 
the complete corpora of Hong Kong News, Hong 
Kong Laws, English translation of Chinese Tree-
bank, and Sinorama, is then given to GIZA++ to 
perform one word alignment run. It took about 40 
hours on our 2.4 GHz machine with 2 GB memory 
to perform this alignment. 
After word alignment, each 3-sentence context 
in English containing an occurrence of the noun w 
that is aligned to a selected Chinese translation 
then forms a training example. For each 
SENSEVAL-2 noun w, we then collected training 
examples from the English side of the parallel texts 
using the same number of training examples for 
each sense of w that are present in the manually 
sense-tagged SENSEVAL-2 official training cor-
pus (lumped-sense version). If there are insuffi-
cient training examples for some sense of w from 
the parallel texts, then we just used as many paral-
lel text training examples as we could find for that 
sense. We chose the same number of training ex-
amples for each sense as the official training data 
so that we can do a fair comparison between the 
accuracy of the parallel text alignment approach 
versus the manual sense-tagging approach. 
After training a WSD classifier for w with such 
parallel text examples, we then evaluated the WSD 
classifier on the same official SENSEVAL-2 test 
set (with lumped senses). The test accuracy of each 
noun obtained by training on such parallel text 
training examples (averaged over 10 trials) is listed 
in the column labeled P1 in Table 3. 
The baseline accuracy for each noun is also 
listed in the column labeled ?P1-Baseline? in Table 
3. The baseline accuracy corresponds to always 
picking the most frequently occurring sense in the 
training data. 
Ideally, we would hope M1 and P1 to be close 
in value, since this would imply that WSD based 
on training examples collected from the parallel 
text alignment approach performs as well as manu-
ally sense-tagged training examples. Comparing 
the M1 and P1 figures, we observed that there is a 
set of nouns for which they are relatively close. 
These nouns are: bar, bum, chair, day, dyke, fa-
tigue, hearth, mouth, nation, nature, post, re-
straint, sense, stress. This set of nouns is relatively 
easy to disambiguate, since using the most-
frequently-occurring-sense baseline would have 
done well for most of these nouns. 
The parallel text alignment approach works 
well for nature and sense, among these nouns. For 
nature, the parallel text alignment approach gives 
better accuracy, and for sense the accuracy differ-
ence is only 0.014 (while there is a relatively large 
difference of 0.231 between P1 and P1-Baseline of 
sense). This demonstrates that the parallel text 
alignment approach to acquiring training examples 
can yield good results. 
For the remaining nouns (art, authority, chan-
nel, church, circuit, facility, grip, spade), the 
accuracy difference between M1 and P1 is at least 
0.10. Henceforth, we shall refer to this set of 8 
nouns as ?difficult? nouns. We will give an analy-
sis of the reason for the accuracy difference be-
tween M1 and P1 in the next section. 
4 
4.1 
Analysis 
Sense-Tag Accuracy of Parallel Text 
Training Examples 
To see why there is still a difference between the 
accuracy of the two approaches, we first examined 
the quality of the training examples obtained 
through parallel text alignment. If the automati-
cally acquired training examples are noisy, then 
this could account for the lower P1 score. 
The word alignment output of GIZA++ con-
tains much noise in general (especially for the low 
frequency words). However, note that in our ap-
proach, we only select the English word occur-
rences that align to our manually selected Chinese 
translations. Hence, while the complete set of word 
alignment output contains much noise, the subset 
of word occurrences chosen may still have high 
quality sense tags. 
Our manual inspection reveals that the annota-
tion errors introduced by parallel text alignment 
can be attributed to the following sources: 
(i) Wrong sentence alignment: Due to errone-
ous sentence segmentation or sentence alignment, 
the correct Chinese word that an English word w 
should align to is not present in its Chinese sen-
tence counterpart. In this case, word alignment will 
align the wrong Chinese word to w. 
(ii) Presence of multiple Chinese translation 
candidates: Sometimes, multiple and distinct Chi-
nese translations appear in the aligned Chinese 
sentence. For example, for an English occurrence 
channel, both ???? (sense 1 translation) and ??
?? (sense 5 translation) happen to appear in the 
aligned Chinese sentence. In this case, word 
alignment may erroneously align the wrong Chi-
nese translation to channel. 
(iii) Truly ambiguous word: Sometimes, a word 
is truly ambiguous in a particular context, and dif-
ferent translators may translate it differently. For 
example, in the phrase ?the church meeting?, 
church could be the physical building sense (?
? ), or the institution sense (?? ). In manual 
sense tagging done in SENSEVAL-2, it is possible 
to assign two sense tags to church in this case, but 
in the parallel text setting, a particular translator 
will translate it in one of the two ways (?? or ?
?), and hence the sense tag found by parallel text 
alignment is only one of the two sense tags. 
By manually examining a subset of about 1,000 
examples, we estimate that the sense-tag error rate 
of training examples (tagged with lumped senses) 
obtained by our parallel text alignment approach is 
less than 1%, which compares favorably with the 
quality of manually sense tagged corpus prepared 
in SENSEVAL-2 (Kilgarriff, 2001). 
4.2 Domain Dependence and Insufficient 
Sense Coverage 
While it is encouraging to find out that the par-
allel text sense tags are of high quality, we are still 
left with the task of explaining the difference be-
tween M1 and P1 for the set of difficult nouns. Our 
further investigation reveals that the accuracy dif-
ference between M1 and P1 is due to the following 
two reasons: domain dependence and insufficient 
sense coverage. 
Domain Dependence The accuracy figure of 
M1 for each noun is obtained by training a WSD 
classifier on the manually sense-tagged training 
data (with lumped senses) provided by 
SENSEVAL-2 organizers, and testing on the cor-
responding official test data (also with lumped 
senses), both of which come from similar domains. 
In contrast, the P1 score of each noun is obtained 
by training the WSD classifier on a mixture of six 
parallel corpora, and tested on the official 
SENSEVAL-2 test set, and hence the training and 
test data come from dissimilar domains in this 
case. 
Moreover, from the ?docsrc? field (which re-
cords the document id that each training or test 
example originates) of the official SENSEVAL-2 
training and test examples, we realized that there 
are many cases when some of the examples from a 
document are used as training examples, while the 
rest of the examples from the same document are 
used as test examples. In general, such a practice 
results in higher test accuracy, since the test exam-
ples would look a lot closer to the training exam-
ples in this case. 
To address this issue, we took the official 
SENSEVAL-2 training and test examples of each 
noun w and combined them together. We then ran-
domly split the data into a new training and a new 
test set such that no training and test examples 
come from the same document. The number of 
training examples in each sense in such a new 
training set is the same as that in the official train-
ing data set of w. 
A WSD classifier was then trained on this new 
training set, and tested on this new test set. We 
conducted 10 random trials, each time splitting into 
a different training and test set but ensuring that 
the number of training examples in each sense (and 
thus the sense distribution) follows the official 
training set of w. We report the average accuracy 
of the 10 trials. The accuracy figures for the set of 
difficult nouns thus obtained are listed in the col-
umn labeled M2 in Table 3. 
We observed that M2 is always lower in value 
compared to M1 for all difficult nouns. This sug-
gests that the effect of training and test examples 
coming from the same document has inflated the 
accuracy figures of SENSEVAL-2 nouns. 
Next, we randomly selected 10 sets of training 
examples from the parallel corpora, such that the 
number of training examples in each sense fol-
lowed the official training set of w. (When there 
were insufficient training examples for a sense, we 
just used as many as we could find from the paral-
lel corpora.) In each trial, after training a WSD 
classifier on the selected parallel text examples, we 
tested the classifier on the same test set (from 
SENSEVAL-2 provided data) used in that trial that 
generated the M2 score. The accuracy figures thus 
obtained for all the difficult nouns are listed in the 
column labeled P2 in Table 3. 
Insufficient Sense Coverage We observed that 
there are situations when we have insufficient 
training examples in the parallel corpora for some 
of the senses of some nouns. For instance, no oc-
currences of sense 5 of the noun circuit (racing 
circuit, a racetrack for automobile races) could be 
found in the parallel corpora. To ensure a fairer 
comparison, for each of the 10-trial manually 
sense-tagged training data that gave rise to the ac-
curacy figure M2 of a noun w, we extracted a new 
subset of 10-trial (manually sense-tagged) training 
data by ensuring adherence to the number of train-
ing examples found for each sense of w in the cor-
responding parallel text training set that gave rise 
to the accuracy figure P2 for w. The accuracy fig-
ures thus obtained for the difficult nouns are listed 
in the column labeled M3 in Table 3. M3 thus gave 
the accuracy of training on manually sense-tagged 
data but restricted to the number of training exam-
ples found in each sense from parallel corpora. 
4.3 
5 
6 
Discussion 
The difference between the accuracy figures of 
M2 and P2 averaged over the set of all difficult 
nouns is 0.140. This is smaller than the difference 
of 0.189 between the accuracy figures of M1 and 
P1 averaged over the set of all difficult nouns. This 
confirms our hypothesis that eliminating the possi-
bility that training and test examples come from 
the same document would result in a fairer com-
parison. 
In addition, the difference between the accuracy 
figures of M3 and P2 averaged over the set of all 
difficult nouns is 0.065. That is, eliminating the 
advantage that manually sense-tagged data have in 
their sense coverage would reduce the performance 
gap between the two approaches from 0.140 to 
0.065. Notice that this reduction is particularly sig-
nificant for the noun circuit. For this noun, the par-
allel corpora do not have enough training examples 
for sense 4 and sense 5 of circuit, and these two 
senses constitute approximately 23% in each of the 
10-trial test set. 
We believe that the remaining difference of 
0.065 between the two approaches could be attrib-
uted to the fact that the training and test examples 
of the manually sense-tagged corpus, while not 
coming from the same document, are however still 
drawn from the same general domain. To illustrate, 
we consider the noun channel where the difference 
between M3 and P2 is the largest. For channel, it 
turns out that a substantial number of the training 
and test examples contain the collocation ?Channel 
tunnel? or ?Channel Tunnel?. On average, about 
9.8 training examples and 6.2 test examples con-
tain this collocation. This alone would have ac-
counted for 0.088 of the accuracy difference 
between the two approaches. 
That domain dependence is an important issue 
affecting the performance of WSD programs has 
been pointed out by (Escudero et al, 2000). Our 
work confirms the importance of domain depend-
ence in WSD. 
As to the problem of insufficient sense cover-
age, with the steady increase and availability of 
parallel corpora, we believe that getting sufficient 
sense coverage from larger parallel corpora should 
not be a problem in the near future for most of the 
commonly occurring words in a language. 
Related Work 
Brown et al (1991) is the first to have explored 
statistical methods in word sense disambiguation in 
the context of machine translation. However, they 
only looked at assigning at most two senses to a 
word, and their method only asked a single ques-
tion about a single word of context. Li and Li 
(2002) investigated a bilingual bootstrapping tech-
nique, which differs from the method we imple-
mented here. Their method also does not require a 
parallel corpus. 
The research of (Chugur et al, 2002) dealt with 
sense distinctions across multiple languages. Ide et 
al. (2002) investigated word sense distinctions us-
ing parallel corpora. Resnik and Yarowsky (2000) 
considered word sense disambiguation using mul-
tiple languages. Our present work can be similarly 
extended beyond bilingual corpora to multilingual 
corpora. 
The research most similar to ours is the work of 
Diab and Resnik (2002). However, they used ma-
chine translated parallel corpus instead of human 
translated parallel corpus. In addition, they used an 
unsupervised method of noun group disambigua-
tion, and evaluated on the English all-words task. 
Conclusion 
In this paper, we reported an empirical study to 
evaluate an approach of automatically acquiring 
sense-tagged training data from English-Chinese 
parallel corpora, which were then used for disam-
biguating the nouns in the SENSEVAL-2 English 
lexical sample task. Our investigation reveals that 
this method of acquiring sense-tagged data is pro-
mising and provides an alternative to manual sense 
tagging. 
 
Acknowledgements 
 
This research is partially supported by a research 
grant R252-000-125-112 from National University 
of Singapore Academic Research Fund. 
References 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1991. Word-
sense disambiguation using statistical methods. In 
Proceedings of the 29th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 264-
270. 
Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002. 
Polysemy and sense proximity in the Senseval-2 test 
suite. In Proceedings of the ACL SIGLEX Workshop 
on Word Sense Disambiguation: Recent Successes 
and Future Directions, pages 32-39. 
Mona Diab and Philip Resnik. 2002. An unsupervised 
method for word sense tagging using parallel cor-
pora. In Proceedings of the 40th Annual Meeting of 
the Association for Computational Linguistics, pages 
255-262. 
Philip Edmonds and Scott Cotton. 2001. SENSEVAL-2: 
Overview. In Proceedings of the Second Interna-
tional Workshop on Evaluating Word Sense 
Disambiguation Systems (SENSEVAL-2), pages 1-5. 
Gerard Escudero, Lluis Marquez, and German Rigau. 
2000. An empirical study of the domain dependence 
of supervised word sense disambiguation systems. In 
Proceedings of the Joint SIGDAT Conference on 
Empirical Methods in Natural Language Processing 
and Very Large Corpora, pages 172-180. 
Radu Florian and David Yarowsky. 2002. Modeling 
consensus: Classifier combination for word sense 
disambiguation. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language 
Processing, pages 25-32. 
Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002. Sense 
discrimination with parallel corpora. In Proceedings 
of the ACL SIGLEX Workshop on Word Sense Dis-
ambiguation: Recent Successes and Future Direc-
tions, pages 54-60. 
Adam Kilgarriff. 2001. English lexical sample task de-
scription. In Proceedings of the Second International 
Workshop on Evaluating Word Sense Disambigua-
tion Systems (SENSEVAL-2), pages 17-20. 
Yoong Keok Lee and Hwee Tou Ng. 2002. An empiri-
cal evaluation of knowledge sources and learning al-
gorithms for word sense disambiguation. In 
Proceedings of the 2002 Conference on Empirical 
Methods in Natural Language Processing, pages 41-
48. 
Cong Li and Hang Li. 2002. Word translation disam-
biguation using bilingual bootstrapping. In Proceed-
ings of the 40th Annual Meeting of the Association 
for Computational Linguistics, pages 343-351. 
I. Dan Melamed. 2001. Empirical Methods for Exploit-
ing Parallel Texts. MIT Press, Cambridge. 
Rada F. Mihalcea and Dan I. Moldovan. 2001. Pattern 
learning and active feature selection for word sense 
disambiguation. In Proceedings of the Second Inter-
national Workshop on Evaluating Word Sense Dis-
ambiguation Systems (SENSEVAL-2), pages 127-130. 
George A. Miller. (Ed.) 1990. WordNet: An on-line 
lexical database. International Journal of Lexicogra-
phy, 3(4):235-312. 
Franz Josef Och and Hermann Ney. 2000. Improved 
statistical alignment models. In Proceedings of the 
38th Annual Meeting of the Association for Computa-
tional Linguistics, pages 440-447. 
Philip Resnik. 1999. Mining the Web for bilingual text. 
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, pages 527-
534. 
Philip Resnik and David Yarowsky. 1997. A perspec-
tive on word sense disambiguation methods and their 
evaluation. In Proceedings of the ACL SIGLEX 
Workshop on Tagging Text with Lexical Semantics: 
Why, What, and How?, pages 79-86. 
Philip Resnik and David Yarowsky. 2000. Distinguish-
ing systems and distinguishing senses: New evalua-
tion methods for word sense disambiguation. Natural 
Language Engineering, 5(2):113-133. 
David Yarowsky, Silviu Cucerzan, Radu Florian, 
Charles Schafer, and Richard Wicentowski. 2001. 
The Johns Hopkins SENSEVAL2 system descrip-
tions. In Proceedings of the Second International 
Workshop on Evaluating Word Sense Disambigua-
tion Systems (SENSEVAL-2), pages 163-166. 
Coling 2010: Poster Volume, pages 1131?1139,
Beijing, August 2010
Using Clustering to Improve Retrieval Evaluation without 
Relevance Judgments 
Zhiwei Shi 
Institute of Computing Technology 
Chinese Academy of Science 
shizhiwei@ict.ac.cn
Peng Li 
Institute of Computing Technology 
Chinese Academy of Science 
lipeng01@ict.ac.cn
Bin Wang 
Institute of Computing Technology 
Chinese Academy of Science 
wangbin@ict.ac.cn
Abstract
Retrieval evaluation without relevance 
judgments is a hard but also very mean-
ingful work. In this paper, we use clus-
tering technique to improve the per-
formance of judgment free retrieval 
evaluation. By using one system to rep-
resent all the systems that are similar to 
it, we can largely reduce the negative ef-
fect of similar retrieval results in Re-
trieval evaluation. Experimental results 
demonstrated that our method outper-
formed all the previous judgment free 
evaluation methods significantly. Its 
overall average performance outper-
formed the best previous result by 
20.5%. Besides, our work is a general 
framework that can be applied to any 
other judgment free evaluation method 
for performance improvement. 
1 Introduction 
Generally, to compare the effectiveness of in-
formation retrieval systems, we need to prepare 
a test collection composed of a set of documents, 
a set of query topics, and a set of relevance 
judgments indicating which documents are rele-
vant to which topics. Among these requirements, 
relevance judgment is the most human resource 
exhausting and time consuming part. It even 
becomes infeasible when the test collection is 
extremely large. To address this problem, the 
TREC conferences used a pooling technology 
(Voorhees and Harman, 1999), where the top n
(e.g., n=100) documents retrieved by each par-
ticipating system are collected into a pool and 
then only the documents in the pool are judged 
for system comparison. Zobel (1998) has shown 
that this pooling method leads to reliable results 
in term of determining the effectiveness of re-
trieval systems and their relative rankings. Yet, 
the relevance determination process is still very 
resource intensive especially when the test col-
lection reaches or exceeds terabyte, or much 
more queries are included. More seriously, 
when we change to a new document collection, 
we have to redo the entire evaluation process.  
There are two possible solutions to the prob-
lem above, evaluation with incomplete rele-
vance judgments and evaluation without rele-
vance judgments. The former is well studied.  
Many well designed ranking methods with in-
complete judgments were carried out. Two of 
them, Minimal Test Collection (MTC) method 
(Carterette et al, 2006) and Statistical evalua-
tion (statMAP) method (Aslam et al, 2006), 
even got practical application in the Million 
Query (1MQ) track in TREC 2007 (Allan et al,
2007), and achieved satisfactory evaluation per-
formance. The latter is comparatively less stud-
ied. Only a few papers concentrate on the issue 
of evaluating retrieval systems without rele-
vance judgments. In Section 2 of this paper, we 
will briefly review some representative methods. 
We will see what they are and how they work.  
1131
In this paper, we focus our effort on the re-
trieval evaluation without relevance judgments. 
Although ?blind? evaluation is really a hard 
problem and its evaluation performance is far 
less than that of methods with incomplete 
judgments, it is undeniable that non-judgment 
evaluation has its own advantages. In some 
cases, relevance judgments are non-attainable. 
For example, when researchers compare their 
novel retrieval algorithms to existing methods, 
or search for optimal parameters of their algo-
rithms, or conduct data fusion in a dynamic en-
vironment, relevance judgment usually seems 
impossible. Besides, to construct a good evalua-
tion method without relevance judgments, re-
searchers need to mine the retrieval results thor-
oughly, and try to find laws that indicate the 
correlation between the effectiveness of a sys-
tem and features of its retrieval result. These 
laws are not only useful for ?blind? evaluation 
methods but also valuable for evaluation meth-
ods with incomplete judgments. 
One of the useful laws for ?blind? evaluation 
methods is Authority Effect (Spoerri, 2005). Yet 
it always ruined by multiple similar results. 
In this work, we use clustering technique to 
solve this problem. By selecting one system to 
represent all the systems that are similar to it, 
we can largely reduce the negative effect of 
similar retrieval results. Details of this method 
will be presented Section 3. Experimental re-
sults, which are reported in Section 4, also veri-
fied that our idea is feasible and effective. Our 
method outperformed all the previous judgment 
free evaluation methods on every test bed.  The 
overall average performance outperformed the 
best previous result by 20.5%. Finally, we con-
clude our work in Section 5. 
2 Related Work 
In 2001, Soboroff et al (2001) firstly proposed 
the concept of evaluating retrieval systems in 
the absence of relevance judgments. They 
generated a set of pseudo-relevance judgments 
by randomly selecting and declaring some 
documents from the pool of top 100 documents 
as relevant. This set of pseudo-relevance 
judgments (instead of a set of human relevance 
judgments) was then used to determine the 
effectiveness of the retrieval systems. Four 
versions of this random pseudo-relevance 
method were designed and tested on data from 
the ad hoc track in TREC 3, 5, 6, 7 and 8. They 
were simple random pseudo-relevance method, 
the variant with duplicate documents, the 
variant with Shallow pools and the variant with 
Exact-fraction sampling. All their resulting 
system assessments and rankings were well 
correlated with actual TREC rankings, and the 
variant with duplicate documents in pools got 
the best performance, with an average Kendall?s 
tau value 0.50 over the data of TREC 3, 5, 6, 7 
and 8. 
Soboroff et al?s idea came from two results 
in retrieval evaluation. One is that incomplete 
judgments do not harm evaluation results 
greatly. Zobel?s (1998) research had showed 
that the results obtained using pooling technol-
ogy were quite reliable given a pool depth of 
100. He also found that even though the pool 
depth was limited to 10, the relative perform-
ance among systems changed little, although 
actual precision scores did change for some sys-
tems. The other is that partially incorrect rele-
vance judgments do not harm evaluation results 
greatly. Voorhees (1998) ascertained that de-
spite a low average overlap between assessment 
sets, and wide variation in overlap among par-
ticular topics, the relative rankings of systems 
remained largely unchanged across the different 
sets of relevance judgments. These two points 
are bases of Soboroff et al?s random pseudo-
relevance method, and give explanation to the 
result that their rankings were positively related 
to that of the actual TRECs. As a matter of fact, 
the two points are bases of all the retrieval 
evaluation methods without or with incomplete 
relevance judgments. 
Aslam and Savell (2003) devised a method to 
measure the relative retrieval effectiveness of 
systems through system similarity computation. 
In their work, the similarity between two re-
trieval systems was the ratio of the number of 
documents in their intersection and union. Each 
system was scored by the average similarity 
between it and all other systems. This measure-
ment produced results that were highly corre-
lated with the random pseudo-relevance method. 
Aslam and Savell hypothesized that this was 
caused by ?tyranny of the masses? effect, and 
these two related methods were assessing the 
systems based on ?popularity? instead of ?per-
formance?. The analysis by Spoerri (2005) sug-
1132
gested that the ?popularity? effect was caused by 
considering all the runs submitted by a retrieval 
system, instead of only selecting one run per 
system. Our later experimental results will show 
that this point of view is partially correct. The 
?popularity? effect could not be avoided com-
pletely by only selecting one run per system. 
This is indeed a hard problem for all the evalua-
tion methods without relevance judgments. 
Wu and Crestani (2003) developed multiple 
?reference count? based methods to rank re-
trieval systems. They made the distinction be-
tween an ?original? document and its duplicates 
in all other lists, called the ?reference? docu-
ments, when computing a document?s score. A 
system?s score is the (weighted) sum of the 
scores of its ?original? documents. Several ver-
sions of reference count method were carried 
out and tested. The basic method (Basic) scored 
each ?original? document by the number of its 
?reference? documents. The first variant (V1) 
assigned different weights to ?reference? docu-
ments based on their ranking positions. The 
second variant (V2) assigned different weights 
to the ?original? document based on its ranking 
position. The third variant (V3) assigned differ-
ent weights to both the ?original? documents and 
the ?reference? documents based on their rank-
ing positions. The fourth variant (V4) was simi-
lar to V3, except that it normalized the weights 
to ?reference? documents. Wu and Crestani?s 
method output similar evaluation performance 
to that of the random pseudo-relevance method. 
Their work also showed that the similarity be-
tween the multiple runs submitted by the same 
retrieval system affected the ranking process. If 
only one run was selected for any of the partici-
pant system for any query, for 3-9 systems, V3 
outperformed random pseudo-relevance method 
by 45.6%; for 10-15 systems, random pseudo-
relevance method outperformed V3 by 6.5%. 
Nuray and Can (2006) introduced a method 
to rank retrieval systems automatically using 
data fusion. Their method consists of two parts. 
One is selecting systems for data fusion, and the 
other is selecting documents as pseudo relevant 
documents as the fusion result. In the former 
part, they hypothesized that systems returning 
documents different from the majority could 
provide better discrimination among the docu-
ments and systems. In return, this could lead to 
a more accurate pseudo relevant documents and 
more accurate rankings. To find proper systems, 
they introduced the ?bias? concept for system 
selection. In their work, bias was 1 minus the 
similarity between a system and the majority, 
where the similarity is a normalized dot product 
of two vectors. In the latter part, Nuray and Can 
tested three criterions, namely Rank position, 
Borda count and Condorcet. Experimental re-
sults on data from TREC 3, 5, 6 and 7 showed 
that bias plus Condorcet got the best evaluation 
results and it outperformed the reference count 
method and random pseudo relevance method 
greatly. 
More recently, Spoerri (2007) proposed a 
method using the structure of overlap between 
search results to rank retrieval systems. This 
method provides us a new view on how to rank 
retrieval systems without relevance judgments. 
He used local statistics of retrieval results as 
indicators of relative effectiveness of retrieval 
systems. Concretely, if there are N systems to be 
ranked, N groups are constructed randomly with 
the constraint that each group contains five sys-
tems and each system will appear in five groups; 
then the percentages of a system?s documents 
not found by other systems (Single%) as well as 
the difference between the percentages of docu-
ments found by a single system and all five sys-
tems (Single%-AllFive%) are calculated as in-
dicators of relative effectiveness respectively. 
Spoerri found that these two local statistics were 
highly and negatively correlated with the mean 
average precision and precision at 1000 scores 
of the systems. By utilizing the two statistics to 
rank systems from subsets of TREC 3, 6, 7 and 
8, Spoerri obtained appealing evaluation results. 
The overlap structure of the top 50 documents 
were sufficient to rank retrieval systems and 
produced the best results, which outperformed 
previous attempts to rank retrieval systems 
without relevance judgments significantly. 
So far, we have reviewed 5 representatives of 
non-judgment evaluation methods. All these 
methods faced the same serious problem: simi-
lar runs harmed the effectiveness of ranking 
process. Different methods handled this prob-
lem differently. Aslam and Savell (2003) called 
this the ?tyranny of the masses? and provided no 
solution. Wu and Crestani (2003) addressed this 
problem by selecting only one run for any of the 
participant system for any query. Nuray and 
Can (2006) selected systems that were less simi-
1133
lar to the majority for data fusion. Spoerri (2007) 
performed his method on a selected subset of all 
the systems. All these treatments led to evalua-
tion performance improvement. Yet we will say 
it could be improved more. In the next section, 
we will present a new solution to this problem. 
Its performance is examined in Section 4. 
3 Using Clustering to Improve Re-
trieval Evaluation without Relevance 
Judgments
3.1 Problem
As we reviewed in Section 2, previous research 
had shown that incomplete relevance judgments 
and partially incorrect relevance judgments do 
not harm retrieval evaluation greatly. This is 
why pooling technique can lead to reliable 
retrieval evaluation results. It is also the 
theoretical foundation of evaluation without 
relevance judgments.
Besides, non-judgments methods armed with 
more laws inside retrieval results. These laws 
indicate the correlation between retrieval effec-
tiveness of a system and features in its retrieval 
results. One of the most important laws used in 
non-judgments evaluation is Authority Effect 
(Spoerri, 2005): document, which is retrieval by 
more systems, is more likely being relevant. 
Unfortunately, similar retrieval results ruined 
this law. Aslam and Savell (2003) called this the 
?tyranny of the masses?. So, how to alleviate the 
negative effect of similar retrieval results is a 
big issue in non-judgments evaluation.  
3.2 Solution
Generally, our solution to the ?tyranny of the 
masses? is removing similar systems by cluster-
ing. The whole process is as follows: 
Firstly, all systems to be evaluated are clus-
tered into several subsets. 
Secondly, for each subset, one system is se-
lected as a representative. 
Thirdly, all the information used for system 
evaluation comes from these representatives. 
Finally, score every system according to the 
information collected in the previous step.
This is the general framework of our method-
ology. Notice that, in the third step, only se-
lected systems contribute to the information 
required for system evaluation. So we can elimi-
nate the negative effect caused by similar re-
trieval results. 
This solution can be applied to any method of 
retrieval evaluation without relevance judg-
ments. To illustrate how to apply it to a retrieval 
evaluation method, we will describe using clus-
tering to improve Average System Similarity, 
which is proposed by Aslam and Savell (2003), 
in detail as an example. 
3.3 Average System Similarity Based on 
Clustering
In Aslam and Savell?s (2003) method, each sys-
tem is evaluated based on a criterion named Av-
erage System Similarity. The average system 
similarity of a given system S0 is calculated ac-
cording to formula (1). 
?
z
 
0
),(
1
1
)(AvgSysSim
0
0
SS
SSSysSim
n
S
(1)
where n is the number of systems to be evalu-
ated, and similarity between two systems S and 
S0, SysSim(S, S0), is calculated based on for-
mula (2). 
21
21
21 RetRet
RetRet
),(SysSim
?
?
 SS (2)
where Reti indicates the set of documents re-
turned by System i (i = 1, 2). 
When applying clustering technique to the 
system similarity method, we need to define an 
equivalence relation first. 
Definition 1 (System Equivalence): Suppose 
that all systems are clustered into m clusters 
namely C1, C2, ?, Cm. Two systems S1 and S2
are equivalent if and only if there exists k (1 ? k
? m) so that S1?Ck and S2?Ck.
kk CSCSmkk
iff
SS
??dd
 
21
21
,,1,
(3)
Given the definition of System Equivalence, 
we get the average system similarity based on 
clustering as follows: 
?
z
 
0
),(
1
1
)(AvgSysSim
0
0
SR
SRSysSim
m
S
(4)
where m is the number of clusters and R is the 
representative system of a cluster. 
1134
Replacing formula (1) with formula (4), we 
get the retrieval evaluation method Average 
System Similarity Based on Clustering, shortly 
ASSBC.
There are two important issues for ASSBC 
that need to be addressed. Issue 1: How to select 
representative system from a cluster? Issue 2: 
How to decide the number of clusters we need? 
Before we address Issue 1, we introduce an-
other definition, Cluster Similarity. 
Definition 2 (Cluster Similarity): for any 
given two clusters C1 and C2, with their respec-
tive representative systems S1 and S2, the cluster 
similarity between C1 and C2 is the system simi-
larity between S1 and S2.
),(SysSim),(ClusterSim 2121 SSCC  (5)
Now we come to selecting representative sys-
tems for clusters. Here, we utilize a hierarchical 
bottom up clustering technique. The entire clus-
tering process is as follows. 
Initially, each system forms a cluster.
Loop Until the number of clusters is m 
Two most similar clusters merge, and 
one of their representatives with higher 
average system similarity survives as 
the representative of the new cluster. 
End Loop. 
In the initial step, since every cluster contains 
only one system, the representative system is 
unquestionable. Within each loop, two represen-
tative systems of the old clusters are candidates 
of the new cluster, and the one with higher score, 
which means higher retrieval performance, be-
comes the representative of the new cluster. 
For Issue 2, technically, how to decide the 
number of clusters is always a problem for clus-
tering. Yet, we do not have to rush in the deci-
sion. Let us examine the evaluation perform-
ance on different values of m first. 
4 Experiments 
In this section, we will illustrate the evaluation 
performance of Average System Similarity 
Based on Clustering vs. different values of m.
Before we come to the experimental results, we 
would like to make some details clear first. 
4.1 Some Clarification 
4.1.1 Dataset
We perform our experiments on the ad hoc tasks 
of TREC-3, -5, -6 and -7. Most existing works 
on retrieval evaluation without judgments are 
tested on these tasks. To make a direct compari-
son with these work mentioned in Section 2 
later, we also choose these tasks as our test bed. 
4.1.2 Performance Measurement 
One of the measures of retrieval effectiveness 
used by TREC is mean non-interpolated average 
precision (MAP). Since average precision is 
based on much more information than other ef-
fectiveness measures such as R-precision or 
P(10) and known to be a more powerful and 
more stable effectiveness measure (Buckley and 
Voorhees, 2000), we utilize MAP as the effec-
tive measurement of retrieval systems in our 
experiments. 
The correlation of the ranking with our pro-
posed methods, as well as other methods, to the 
TREC official rankings is measured using the 
Spearman?s rank correlation coefficient. One 
reason is that it suits better for evaluating corre-
lation between ratio sequences, e.g. MAP, than 
Kendall?s tau. The other reason is that we can 
directly compare our results with those of pre-
vious attempts reviewed in Section 2, since 
most of them provided Spearman?s rank correla-
tion coefficient results. 
4.1.3 Substitute for Number of Clusters 
TREC Runs 
3 40 
5 61 
6 74 
7 103 
Table 1. Number of TREC runs 
As we know, the number of systems (runs) var-
ies in different TREC dataset (see Table 1 for 
details). Instead of examining the evaluation 
performance variation when absolute number of 
clusters m changes, we illustrate the evaluation 
performance vs. the percentage of m. Actually, 
for the sake of convenience, we will plot the 
correlation of our method to the TREC official 
rankings vs. the percentage of systems removed 
from the representative group in the following 
subsection.
1135
4.2 Experimental results 
Figure 1-4 show the plots of the correlation 
of our method to the TREC official rankings vs. 
the percentage of systems removed from the 
representative group on TREC-3, -5, -6 and -7 
respectively. The percentage of systems re-
moved goes from 0 to 85%, where 0 means no 
system removed and represents the original Av-
erage System Similarity method, and 85% is an 
up bound in our experiments. The horizontal 
line indicates the original performance. The 
tagged number on the curve says when the per-
formance curve reaches its peak and the peak 
value.
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1115?1126,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Regularized Competition Model for Question Difficulty Estimation in
Community Question Answering Services
Quan Wang
?
Jing Liu
?
Bin Wang
?
Li Guo
?
?
Institute of Information Engineering, Chinese Academy of Sciences, Beijing, P. R. China
{wangquan,wangbin,guoli}@iie.ac.cn
?
Harbin Institute of Technology, Harbin, P. R. China
jliu@ir.hit.edu.cn
Abstract
Estimating questions? difficulty levels is
an important task in community question
answering (CQA) services. Previous stud-
ies propose to solve this problem based
on the question-user comparisons extract-
ed from the question answering threads.
However, they suffer from data sparseness
problem as each question only gets a lim-
ited number of comparisons. Moreover,
they cannot handle newly posted question-
s which get no comparisons. In this pa-
per, we propose a novel question difficul-
ty estimation approach called Regularized
Competition Model (RCM), which natu-
rally combines question-user comparisons
and questions? textual descriptions into a
unified framework. By incorporating tex-
tual information, RCM can effectively deal
with data sparseness problem. We further
employ a K-Nearest Neighbor approach to
estimate difficulty levels of newly post-
ed questions, again by leveraging textu-
al similarities. Experiments on two pub-
licly available data sets show that for both
well-resolved and newly-posted question-
s, RCM performs the estimation task sig-
nificantly better than existing methods,
demonstrating the advantage of incorpo-
rating textual information. More interest-
ingly, we observe that RCM might provide
an automatic way to quantitatively mea-
sure the knowledge levels of words.
1 Introduction
Recent years have seen rapid growth in communi-
ty question answering (CQA) services. They have
been widely used in various scenarios, including
general information seeking on the web
1
, knowl-
1
http://answers.yahoo.com/
edge exchange in professional communities
2
, and
question answering in massive open online cours-
es (MOOCs)
3
, to name a few.
An important research problem in CQA is
how to automatically estimate the difficulty lev-
els of questions, i.e., question difficulty estima-
tion (QDE). QDE can benefit many applications.
Examples include 1) Question routing. Routing
questions to appropriate answerers can help ob-
tain quick and high-quality answers (Li and K-
ing, 2010; Zhou et al., 2009). Ackerman and
McDonald (1996) have demonstrated that rout-
ing questions by matching question difficulty lev-
el with answerer expertise level will make better
use of answerers? time and expertise. This is even
more important for enterprise question answering
and MOOCs question answering, where human
resources are expensive. 2) Incentive mechanism
design. Nam et al. (2009) have found that win-
ning point awards offered by reputation system-
s is a driving factor for user participation in C-
QA services. Assigning higher point awards to
more difficult questions will significantly improve
user participation and satisfaction. 3) Linguistics
analysis. Researchers in computational linguistics
are always interested in investigating the correla-
tion between language and knowledge, to see how
the language reflects one?s knowledge (Church,
2011). As we will show in Section 5.4, QDE pro-
vides an automatic way to quantitatively measure
the knowledge levels of words.
Liu et al. (2013) have done the pioneer work
on QDE, by leveraging question-user comparison-
s extracted from the question answering threads.
Specifically, they assumed that the difficulty lev-
el of a question is higher than the expertise level
of the asker (i.e. the user who asked the question),
but lower than that of the best answerer (i.e. the us-
er who provided the best answer). A TrueSkill al-
2
http://stackoverflow.com/
3
http://coursera.org/
1115
gorithm (Herbrich et al., 2006) was further adopt-
ed to estimate question difficulty levels as well as
user expertise levels from the pairwise compar-
isons among them. To our knowledge, it is the on-
ly existing work on QDE. Yang et al. (2008) have
proposed a similar idea, but their work focuses on
a different task, i.e., estimating difficulty levels of
tasks in crowdsourcing contest services.
There are two major drawbacks of previous
methods: 1) data sparseness problem and 2) cold-
start problem. By the former, we mean that un-
der the framework of previous work, each question
is compared only twice with the users (once with
the asker and the other with the best answerer),
which might not provide enough information and
contaminate the estimation accuracy. By the latter,
we mean that previous work only deals with well-
resolved questions which have received the best
answers, but cannot handle newly posted question-
s with no answers received. In many real-world
applications such as question routing and incentive
mechanism design, however, it is usually required
that the difficulty level of a question is known in-
stantly after it is posted.
To address the drawbacks, we propose further
exploiting questions? textual descriptions (e.g., ti-
tle, body, and tags) to perform QDE. Preliminary
observations have shown that a question?s difficul-
ty level can be indicated by its textual descrip-
tion (Liu et al., 2013). We take advantage of the
observations, and assume that if two questions are
close in their textual descriptions, they will also
be close in their difficulty levels, i.e., the smooth-
ness assumption. We employ manifold regular-
ization (Belkin et al., 2006) to characterize the
assumption. Manifold regularization is a well-
known technique to preserve local invariance in
manifold learning algorithms, i.e., nearby points
are likely to have similar embeddings (Belkin and
Niyogi, 2001). Then, we propose a novel Reg-
ularized Competition Model (RCM), which for-
malizes QDE as minimizing a loss on question-
user comparisons with manifold regularization on
questions? textual descriptions. As the smoothness
assumption offers extra information for inferring
question difficulty levels, incorporating it will ef-
fectively deal with data sparsity. Finally, we adopt
a K-Nearest Neighbor approach (Cover and Hart,
1967) to perform cold-start estimation, again by
leveraging the smoothness assumption.
Experiments on two publicly available data sets
collected from Stack Overflow show that 1) RCM
performs significantly better than existing meth-
ods in the QDE task for both well-resolved and
cold-start questions. 2) The performance of RCM
is insensitive to the particular choice of the term
weighting schema (determines how a question?s
textual description is represented) and the similar-
ity measure (determines how the textual similarity
between two questions is measured). The results
demonstrate the advantage of incorporating textu-
al information for QDE. Qualitative analysis fur-
ther reveals that RCM might provide an automatic
way to quantitatively measure the knowledge lev-
els of words.
The main contributions of this paper include: 1)
We take fully advantage of questions? textual de-
scriptions to address data sparseness problem and
cold-start problem which previous QDE methods
suffer from. To our knowledge, it is the first time
that textual information is introduced in QDE. 2)
We propose a novel QDE method that natural-
ly combines question-user comparisons and ques-
tions? textual descriptions into a unified frame-
work. The proposed method performs QDE sig-
nificantly better than existing methods. 3) We
demonstrate the practicability of estimating diffi-
culty levels of cold-start questions purely based on
their textual descriptions, making various applica-
tions feasible in practice. As far as we know, it is
the first work that considers cold-start estimation.
4) We explore how a word?s knowledge level can
be automatically measured by RCM.
The rest of the paper is structured as follows.
Section 2 describes the problem formulation and
the motivation of RCM. Section 3 presents the de-
tails of RCM. Section 4 discusses cold-start esti-
mation. Section 5 reports experiments and results.
Section 6 reviews related work. Section 7 con-
cludes the paper and discusses future work.
2 Preliminaries
2.1 Problem Formulation
A CQA service provides a platform where people
can ask questions and seek answers from others.
Given a CQA portal, consider a specific catego-
ry where questions on the same topic are asked
and answered, e.g., the ?C++ programming? cat-
egory of Stack Overflow. When an asker u
a
posts
a question q in the category, there will be sever-
al answerers to answer the question. Among all
the received answers, a best one will be chosen
1116
by the asker or voted by the community. The an-
swerer who provides the best answer is called the
best answerer u
b
. The other answerers are denoted
by O =
{
u
o
1
, u
o
2
, ? ? ? , u
o
M
}
. A question answering
thread (QA thread) is represented as a quadruplet
(
q, u
a
, u
b
,O
)
. Collecting all such QA threads in the
category, we get M users and N questions, denoted
byU = {u
1
, u
2
, ? ? ? , u
M
} and Q = {q
1
, q
2
, ? ? ? , q
N
}
respectively. Each user u
m
is associated with an
expertise score ?
m
, representing his/her expertise
level. A larger ?
m
indicates a higher expertise lev-
el of the user. Each question q
n
is associated with
a difficulty score ?
n
, representing its difficulty lev-
el. A larger ?
n
indicates a higher difficulty level
of the question. Difficulty scores (as well as ex-
pertise scores) are assumed to be comparable with
each other in the specified category. Besides, each
question q
n
has a textual description, and is repre-
sented as a V-dimensional term vector d
n
, where
V is the vocabulary size.
The question difficulty estimation (QDE) task
aims to automatically learn the question difficul-
ty scores (?
n
?s) by utilizing the QA threads T =
{
(
q, u
a
, u
b
,O
)
: q ? Q} as well as the question de-
scriptions D = {d
1
, d
2
, ? ? ? , d
N
} in the specified
category. Note that in Section 2 and Section 3, we
consider estimating difficulty scores of resolved
questions, i.e., questions with the best answers se-
lected or voted. Estimating difficulty scores of un-
resolved questions, e.g., newly posted ones, will
be discussed in Section 4.
2.2 Competition-based Methods
Liu et al. (2013) have proposed a competition-
based method for QDE. The key idea is to 1) ex-
tract pairwise competitions from the QA threads
and 2) estimate question difficulty scores based on
extracted competitions.
To extract pairwise competitions, it is assumed
that question difficulty scores and user expertise
scores are expressed on the same scale. Given a
QA thread
(
q, u
a
, u
b
,O
)
, it is further assumed that:
Assumption 1 (pairwise comparison assumption)
The difficulty score of question q is higher than the
expertise score of the asker u
a
, but lower than that
of the best answerer u
b
. Moreover, the expertise
score of the best answerer u
b
is higher than that
of the asker u
a
, as well as any answerer in O.
4
4
The difficulty score of question q is not assumed to be
lower than the expertise score of any answerer in O, since
such a user may just happen to see the question and respond
to it, rather than knowing the answer well.
Given the assumption, there are
(
|O| + 3
)
pairwise
competitions extracted from the QA thread, in-
cluding 1) one competition between the question
q and the asker u
a
, 2) one competition between
the question q and the best answerer u
b
, 3) one
competition between the best answerer u
b
and the
asker u
a
, and 4) |O| competitions between the best
answerer u
b
and each of the answerers in O. The
question q is the winner of the first competition,
and the best answerer u
b
is the winner of the re-
maining
(
|O| + 2
)
competitions. These pairwise
competitions are denoted by
C
q
=
{
u
a
?q, q?u
b
, u
a
?u
b
, u
o
1
?u
b
, ? ? ? , u
o
M
?u
b
}
,
where i ? j means that competitor j beats com-
petitor i in a competition. Let
C =
?
q?Q
C
q
(1)
be the set containing all the pairwise competitions
extracted from T .
Given the competition set C, Liu et al. (2013)
further adopted a TrueSkill algorithm (Herbrich
et al., 2006) to learn the competitors? skill level-
s (i.e. the question difficulty scores and the us-
er expertise scores). TrueSkill assumes that the
practical skill level of each competitor follows a
normal distribution N
(
?, ?
2
)
, where ? is the aver-
age skill level and ? is the estimation uncertain-
ty. Then it updates the estimations in an online
mode: for a newly observed competition with its
win-loss result, 1) increase the average skill level
of the winner, 2) decrease the average skill level
of the loser, and 3) shrink the uncertainties of both
competitors as more data has been observed. Yang
et al. (2008) have proposed a similar competition-
based method to estimate tasks? difficulty levels
in crowdsourcing contest services, by leveraging
PageRank (Page et al., 1999) algorithm.
2.3 Motivating Discussions
The methods introduced above estimate competi-
tors? skill levels based solely on the pairwise com-
petitions among them. The more competitions a
competitor participates in, the more accurate the
estimation will be. However, according to the
pairwise comparison assumption (Assumption 1),
each question participates in only two competi-
tions, one with the asker and the other with the
best answerer. Hence, there might be no enough
information to accurately infer its difficulty score.
We call this the data sparseness problem.
1117
(a) Low difficulty. (b) Medium difficulty. (c) High difficulty.
Figure 1: Tag clouds of SO/Math questions with different difficulty levels.
Taking advantage of additional metadata has
been demonstrated to be an effective way of deal-
ing with data sparsity in various applications such
as collaborative filtering (Claypool et al., 1999;
Schein et al., 2002) and personalized search (Dou
et al., 2007; Sugiyama et al., 2004). The ratio-
nale behind is to bridge the gap among users/items
by leveraging their similarities based on the meta-
data. As for QDE, preliminary observations have
shown that a question?s difficulty level can be in-
dicated by its textual description (Liu et al., 2013).
As an example, consider the QA threads in the
?mathematics? category of Stack Overflow. Di-
vide the questions into three groups: 1) low dif-
ficulty, 2) medium difficulty, and 3) high difficul-
ty, according to their difficulty scores estimated by
TrueSkill. Figure 1 visualizes the frequency dis-
tribution of tags in each group, where the size of
each tag is in proportion to its frequency in the
group. The results indicate that the tags associ-
ated with the questions do have the ability to re-
flect the questions? difficulty levels, e.g., low dif-
ficulty questions usually have tags such as ?home-
work? and ?calculus?, while high difficulty ones
usually have tags such as ?general topology? and
?number theory?. We further calculate the Pearson
correlation coefficient (Rodgers and Nicewander,
1988) between 1) the gap between the averaged
difficulty scores in each two groups and 2) the
Euclidean distance between the aggregated textu-
al descriptions in each two groups . The result is
r = 0.6424, implying that the difficulty gap is posi-
tively correlated with the textual distance. In other
words, the more similar two questions? textual de-
scriptions are, the more close their difficulty levels
are. Therefore, we take the textual information to
bridge the difficulty gap among questions, by as-
suming that
Assumption 2 (smoothness assumption) If two
questions q
i
and q
j
are close in their textual de-
scriptions d
i
and d
j
, they will also be close in their
difficulty scores ?
i
and ?
j
.
The smoothness assumption brings us additional
information about question difficulty scores by in-
ferring textual similarities. It serves as a supple-
ment to the pairwise competitions, and might help
address the data sparseness problem which previ-
ous methods suffer from.
3 Modeling Text Similarities for QDE
This section presents a novel Regularized Compe-
tition Model (RCM) for QDE, which combines the
pairwise competitions and the textual descriptions
into a unified framework. RCM can alleviate the
data sparseness problem and perform more accu-
rate estimation.
3.1 Regularized Competition Model
We start with several notations. As question dif-
ficulty scores can be directly compared with user
expertise scores, we take questions as pseudo user-
s. Let
?
? ? R
M+N
denote the skill levels (i.e. the
expertise scores and the difficulty scores) of all the
(pseudo) users:
?
?
i
=
{
?
i
, 1 ? i ? M,
?
i?M
, M < i ? M + N,
where
?
?
i
is the i-th entry of
?
?. The first M entries
are the user expertise scores, denoted by
?
?
u
? R
M
.
The last N entries are the question difficulty s-
cores, denoted by
?
?
q
? R
N
. Let
?
?
(u)
i
and
?
?
(q)
i
denote
the i-th entries of
?
?
u
and
?
?
q
respectively.
Exploiting Pairwise Competitions. We define
a loss on each pairwise competition i ? j:
?
(
?
?
i
,
?
?
j
)
= max
(
0, ? ?
(
?
?
j
?
?
?
i
))
p
, (2)
where p is either 1 or 2. The loss is defined on the
skill gap between the two competitors, i.e.,
?
?
j
?
?
?
i
,
1118
measuring the inconsistency between the expect-
ed outcome and the actual outcome. If the gap is
larger than a predefined threshold ?, competitor j
would probably beat competitor i in the compe-
tition, which coincides with the actual outcome.
Then the loss will be zero. Otherwise, there is a
higher chance that competitor j loses the competi-
tion, which goes against the actual outcome. Then
the loss will be greater than zero. The smaller the
gap is, the higher the chance of inconsistency be-
comes, and the greater the loss will be. Note that
the threshold ? can take any positive value since
we do not pose a norm constraint on
?
?.
5
Without
loss of generality we take ? = 1 throughout this
paper. As we will show in Section 3.2, the loss de-
fined in Eq. (2) has some similarity with the SVM
loss (Chapelle, 2007). We name it hinge loss when
p = 1, and quadratic loss when p = 2.
Given the competition set C, estimating skil-
l levels of (pseudo) users then amounts to solving
the following optimization problem:
min
?
?
?
(i? j)?C
?
(
?
?
i
,
?
?
j
)
+
?
1
2
?
?
T
?
?, (3)
where the first term is the empirical loss measur-
ing the total inconsistency; the second term is a
regularizer to prevent overfitting; and ?
1
? 0 is a
trade-off coefficient. It is also a competition-based
QDE method, called Competition Model (CM).
Exploiting Question Descriptions. Manifold
regularization is a well-known technique used in
manifold learning algorithms to preserve local in-
variance, i.e., nearby points are likely to have sim-
ilar embeddings (Belkin and Niyogi, 2001). In
QDE, the smoothness assumption expresses sim-
ilar ?invariance?, i.e., nearby questions (in terms
of textual similarities) are likely to have similar
difficulty scores. Hence, we characterize the as-
sumption with the following manifold regularizer:
R =
1
2
N
?
i=1
N
?
j=1
(
?
?
(q)
i
?
?
?
(q)
j
)
2
w
i j
=
?
?
T
q
D
?
?
q
?
?
?
T
q
W
?
?
q
=
?
?
T
q
L
?
?
q
, (4)
where w
i j
is the textual similarity between ques-
tion i and question j; W ? R
N?N
is the similarity
matrix with the (i, j)-th entry being w
i j
; D ? R
N?N
is a diagonal matrix with the i-th entry on the diag-
onal being d
ii
=
?
N
j=1
w
i j
; and L = D?W ? R
N?N
5
Given any
?
?
i
,
?
?
j
, and ?, there always exists a linear trans-
formation which keeps the sign of
(
? ?
(
?
?
j
?
?
?
i
))
unchanged.
is the graph Laplacian (Chung, 1997). Minimizing
R results in the smoothness assumption: for any
questions i and j, if their textual similarity w
i j
is
high, the difficulty gap
(
?
?
(q)
i
?
?
?
(q)
j
)
2
will be small.
A Hybrid Method. Combining Eq. (3) and
Eq. (4), we obtain RCM, which amounts to the
following optimization problem:
min
?
?
?
(i? j)?C
?
(
?
?
i
,
?
?
j
)
+
?
1
2
?
?
T
?
? +
?
2
2
?
?
T
q
L
?
?
q
. (5)
Here ?
2
? 0 is also a trade-off coefficient. The
advantages of RCM include 1) It naturally formal-
izes QDE as minimizing a manifold regularized
loss function, which seamlessly integrates both the
pairwise competitions and the textual description-
s. 2) By incorporating textual information, it can
address the data sparseness problem which previ-
ous methods suffer from, and perform significantly
better in the QDE task.
3.2 Learning Algorithm
Redefine the k-th pairwise competition (assumed
to be carried out between competitors i and j) as
(
x
k
, y
k
)
. x
k
? R
M+N
indicates the competitors:
x
(k)
i
= 1, x
(k)
j
= ?1, and x
(k)
l
= 0 for any l , i, j,
where x
(k)
l
is the l-th entry of x
k
. y
k
? {1,?1} is
the outcome: if competitor i beats competitor j,
y
k
= 1; otherwise, y
k
= ?1. The objective in Eq.
(5) can then be rewritten as
L
(
?
?
)
=
|C|
?
k=1
max
(
0, 1 ? y
k
(
?
?
T
x
k
))
p
+
1
2
?
?
T
Z
?
?,
where Z =
(
?
1
I
M
0
0 ?
1
I
N
+ ?
2
L
)
is a block matrix; I
M
?
R
M?M
and I
N
? R
N?N
are identity matrices; p =
1 corresponds to the hinge loss, and p = 2 the
quadratic loss. It is clear that the loss defined in
Eq. (2) has the same format as the SVM loss.
The objectiveL is differentiable for the quadrat-
ic loss but non-differentiable for the hinge loss.
We employ a subgradient method (Boyd et al.,
2003) to solve the optimization problem. The al-
gorithm starts at a point
?
?
0
and, as many iterations
as needed, moves from
?
?
t
to
?
?
t+1
in the direction
of the negative subgradient:
?
?
t+1
=
?
?
t
? ?
t
?L
(
?
?
t
)
,
1119
Algorithm 1 Regularized Competition Model
Require: competition set C and description setD
1:
?
?
0
? 1
2: for t = 0 : T ? 1 do
3: K
t
?
{
k : 1 ? y
k
(
?
?
T
t
x
k
)
> 0
}
4: ?L
(
?
?
t
)
? calculated by Eq. (6)
5:
?
?
t+1
?
?
?
t
? ?
t
?L
(
?
?
t
)
6: ?
t+1
?
{
?
?
0
,
?
?
1
, ? ? ? ,
?
?
t+1
}
7:
?
?
t+1
? argmin
?
???
t+1
L
(
?
?
)
8: end for
9: return
?
?
T
where ?
t
> 0 is the learning rate. The subgradient
is calculated as
?L
(
?
?
t
)
=
?
?
?
?
?
?
?
?
?
Z
?
?
t
?
?
k?K
t
y
k
x
k
, p=1,
Z
?
?
t
+ 2
?
k?K
t
x
k
x
T
k
?
?
t
? 2
?
k?K
t
y
k
x
k
, p=2,
(6)
where K
t
=
{
k : 1 ? y
k
(
?
?
T
t
x
k
)
> 0
}
. As it is not
always a descent method, we keep track of the best
point found so far (Boyd et al., 2003):
?
?
t+1
= arg min
?
???
t+1
L
(
?
?
)
,
where?
t+1
=
{
?
?
0
,
?
?
1
, ? ? ? ,
?
?
t+1
}
. The whole proce-
dure is summarized in Algorithm 1.
Convergence. For constant learning rate (i.e.,
?
t
= ?), Algorithm 1 is guaranteed to converge to
within some range of the optimal value, i.e.,
lim
t??
L
(
?
?
t
)
? L
?
< ?,
where L
?
denotes the minimum of L(?), and ? is a
constant defined by the learning rate ?. For more
details, please refer to (Boyd et al., 2003). During
our experiments, we set the iteration number as
T = 1000 and the learning rate as ?
t
= 0.001, and
convergence was observed.
Complexity. For both the hinge loss and the
quadratic loss, the time complexity (per itera-
tion) and the space complexity of RCM are both
O
(
|C| + ?N
2
)
. Here, |C| is the total number of
competitions, M and N are the numbers of user-
s and questions respectively, and ? is the ratio of
non-zero entries in the graph Laplacian L.
6
In the
analysis, we have assumed that M ? ?N
2
and
N ? ?N
2
.
6
Owing to the sparse nature of questions? textual descrip-
tions, the graph Laplacian L is usually sparse, with about
70% entries being zero according to our experiments.
4 Cold-Start Estimation
Previous sections discussed estimating difficulty s-
cores of resolved questions, from which pairwise
competitions could be extracted. However, for
newly posted questions without any answers re-
ceived, no competitions could be extracted and
none of the above methods work. We call it the
cold-start problem.
We heuristically apply a K-Nearest Neighbor
(KNN) approach (Cover and Hart, 1967) to cold-
start estimation, again by leveraging the smooth-
ness assumption. The key idea is to propagate
difficulty scores from well-resolved questions to
cold-start ones according to their textual simi-
larities. Specifically, suppose that there exists
a set of well-resolved questions whose difficul-
ty scores have already been estimated by a QDE
method. Given a cold-start question q
?
, we first
pick K well-resolved questions that are closest to
q
?
in textual descriptions, referred to as the near-
est neighbors. The difficulty score of question q
?
is then predicted as the averaged difficulty scores
of its nearest neighbors. The KNN method bridges
the gap between cold-start and well-resolved ques-
tions by inferring their textual similarities, and
might effectively deal with the cold-start problem.
5 Experiments
We have conducted experiments to test the effec-
tiveness of RCM in estimating difficulty scores of
both well-resolved and cold-start questions. More-
over, we have explored how a word?s difficulty lev-
el can be quantitatively measured by RCM.
5.1 Experimental Settings
Data Sets. We obtained a publicly available da-
ta set of Stack Overflow between July 31, 2008
and August 1, 2012
7
, containing QA threads in
various categories. We considered the categories
of ?C++ programming? and ?mathematics?, and
randomly sampled about 10,000 QA threads from
each category, denoted by SO/CPP and SO/Math
respectively. For each question, we took the title
and body fields as its textual description. For both
data sets, stop words in a standard list
8
and words
whose total frequencies are less than 10 were re-
moved. Table 1 gives the statistics of the data sets.
7
http://blog.stackoverflow.com/category/cc-wiki-dump/
8
http://jmlr.org/papers/volume5/lewis04a/a11-smart-
stop-list/english.stop
1120
# users # questions # competitions # words
SO/CPP 14,884 10,164 50,043 2,208
SO/Math 6,564 10,528 40,396 2,009
Table 1: Statistics of the data sets.
For evaluation, we randomly sampled 600 ques-
tion pairs from each data set, and asked annotators
to compare the difficulty levels of the questions
in each pair. We had two graduate students ma-
joring in computer science annotate the SO/CPP
questions, and two majoring in mathematics an-
notate the SO/Math questions. For each question,
only the title, body, and tags were exposed to the
annotators. Given a question pair
(
q
1
, q
2
)
, the an-
notators were asked to give one of the three labels:
q
1
? q
2
, q
2
? q
1
, or q
1
= q
2
, which respective-
ly means that question q
1
has a higher, lower, or
equal difficulty level compared with question q
2
.
We used Cohen?s kappa coefficient (Cohen, 1960)
to measure the inter-annotator agreement. The re-
sult is ? = 0.7533 on SO/CPP and ? = 0.8017
on SO/Math, indicating that the inter-annotator a-
greement is quite substantial on both data sets. Af-
ter removing the question pairs with inconsisten-
t labels, we got 521 annotated SO/CPP question
pairs and 539 annotated SO/Math question pairs.
We further randomly split the annotated ques-
tion pairs into development/test/cold-start sets,
with the ratio of 2:2:1. The first two sets were used
to evaluate the methods in estimating difficulty s-
cores of resolved questions. Specifically, the de-
velopment set was used for parameter tuning and
the test set was used for evaluation. The last set
was used to evaluate the methods in cold-start esti-
mation, and the questions in this set were excluded
from the learning process of RCM as well as any
baseline method.
Baseline Methods. We considered three base-
line methods: PageRank (PR), TrueSkill (TS), and
CM, which are based solely on the pairwise com-
petitions.
? PR first constructs a competitor graph, by
creating an edge from competitor i to com-
petitor j if j beats i in a competition. A
PageRank algorithm (Page et al., 1999) is
then utilized to estimate the relative impor-
tance of the nodes, i.e., question difficulty s-
cores and user expertise scores. The damping
factor was set from 0.1 to 0.9 in steps of 0.1.
? TS has been applied to QDE by Liu et al.
(2013). We set the model parameters in the
same way as they suggested.
? CM performs QDE by solving Eq. (3). We
set ?
1
in {0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1}.
We compared RCM with the above baseline meth-
ods. In RCM, both parameters ?
1
and ?
2
were set
in {0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1}.
Evaluation Metric. We employed accuracy
(ACC) as the evaluation metric:
ACC =
# correctly judged question pairs
# all question pairs
.
A question pair is regarded as correctly judged if
the relative difficulty ranking given by an estima-
tion method is consistent with that given by the
annotators. The higher the accuracy is, the better
a method performs.
5.2 Estimation for Resolved Questions
The first experiment tested the methods in estimat-
ing difficulty scores of resolved questions.
Estimation Accuracies. We first compared the
estimation accuracies of PR, TS, CM, and RCM
on the test sets of SO/CPP and SO/Math, obtained
with the best parameter settings determined by the
development sets. Table 2 gives the results, where
?H? denotes the hinge loss and ?Q? the quadratic
loss. In RCM, to calculate the graph Laplacian L,
we adopted Boolean term weighting schema and
took Jaccard coefficient as the similarity measure.
From the results, we can see that 1) RCM perform-
s significantly better than the baseline methods on
both data sets (t-test, p-value < 0.05), demonstrat-
ing the advantage of exploiting questions? textu-
al descriptions for QDE. 2) The improvements of
RCM over the baseline methods on SO/Math are
greater than those on SO/CPP, indicating that the
textual descriptions of the SO/Math questions are
more powerful in reflecting their difficulty level-
s. The reason is that the SO/Math questions are
much more heterogeneous, belonging to various
subfields of mathematics. The difficulty gaps a-
mong different subfields are sometimes obvious
(e.g., a question in topology in general has a high-
er difficulty level than a question in linear algebra),
making the textual descriptions more powerful in
distinguishing the difficulty levels.
Graph Laplacian Variants. We further inves-
tigated the performances of different term weight-
ing schemas and similarity measures in the graph
1121
PR TS
CM RCM
H Q H Q
SO/CPP 0.5876 0.6134 0.6340 0.6753 0.7371 0.7268
SO/Math 0.6067 0.6109 0.6527 0.6820 0.7699 0.7699
Table 2: ACC of different methods for well-
resolved questions.
Notation Definition
Boolean v(w, q) =
?
?
?
?
?
?
?
1, if word w occurs in question q
0, otherwise
TF-1 v(w, q) = f (w, q), the number of occurrences
TF-2 v(w, q) = log ( f (w, q) + 1)
TF-3 v(w, q) = 0.5 +
0.5 ? f (w, q)
max { f (w, q) : w ? q}
TFIDF-1 v(w, q) = TF-1 ? log
|Q|
|{q?Q:w?q}|
TFIDF-2 v(w, q) = TF-2 ? log
|Q|
|{q?Q:w?q}|
TFIDF-3 v(w, q) = TF-3 ? log
|Q|
|{q?Q:w?q}|
Cosine Sim (d
1
, d
2
) =
d
T
1
d
2
?d
1
???d
2
?
? [0, 1]
Jaccard Sim
(
d
1
, d
2
)
=
d
T
1
d
2
?d
1
?
2
+?d
2
?
2
??d
1
???d
2
?
? [0, 1]
Table 3: Different term weighting schemas and
similarity measures.
Laplacian. The term weighting schema deter-
mines how a question?s textual description is rep-
resented. We explored a Boolean schema, three
TF schemas, and three TFIDF schemas (Salton
and Buckley, 1988). The similarity measure de-
termines how the textual similarity between two
questions is calculated. We explored the Co-
sine similarity and the Jaccard coefficient (Huang,
2008). Detailed descriptions are given in Table 3.
Figure 2 and Figure 3 show the estimation ac-
curacies of the RCM variants on the test sets of
SO/CPP and SO/Math respectively, again obtained
with the best parameter settings determined by
the development sets. The performance of CM
is also given (the straight lines in the figures).
9
From the results, we can see that 1) All the RCM
variants can improve over CM on both data sets,
and most of the improvements are significant (t-
test, p-value < 0.05). This further demonstrates
that the effectiveness of incorporating textual de-
scriptions is not affected by the particular choice
of the term weighting schema or similarity mea-
sure. 2) Boolean term weighting schema performs
the best, considering different similarity measures,
loss types, and data sets collectively. 3) Jaccard
9
CM performs better than PR and TS on both data sets.
0.55
0.6
0.65
0.7
0.75 Cosine Jaccard CM(H)
(a) Hinge loss.
0.55
0.6
0.65
0.7
0.75 Cosine Jaccard CM(Q)
(b) Quadratic loss.
Figure 2: ACC of RCM variants for well-resolved
questions on SO/CPP.
0.6
0.65
0.7
0.75
0.8 Cosine Jaccard CM(H)
(a) Hinge loss.
0.6
0.65
0.7
0.75
0.8 Cosine Jaccard CM(Q)
(b) Quadratic loss.
Figure 3: ACC of RCM variants for well-resolved
questions on SO/Math.
coefficient performs as well as Cosine similari-
ty on SO/Math, but almost consistently better on
SO/CPP. Throughout the experiments, we adopted
Boolean term weighting schema and Jaccard coef-
ficient to calculate the graph Laplacian.
5.3 Estimation for Cold-Start Questions
The second experiment tested the methods in es-
timating difficulty scores of cold-start questions.
We employed Boolean term weighting schema to
represent a cold-start question, and utilized Jac-
card Coefficient to select its nearest neighbors.
Figure 4 and Figure 5 list the cold-start estima-
tion accuracies of different methods on SO/CPP
and SO/Math respectively, with different K val-
ues (the number of nearest neighbors). As the
accuracy oscillates drastically with a K value s-
maller than 11 on SO/CPP and smaller than 6 on
SO/Math, we report the results with K ? [11, 20]
on SO/CPP and K ? [6, 15] on SO/Math. The av-
eraged (over different K values) cold-start estima-
tion accuracies are further given in Table 4. All the
results are reported on the cold-start sets, with the
optimal parameter settings adopted in Section 5.2.
From the results, we can see that 1) Cold-start es-
timation is possible, and can achieve a consider-
ably high accuracy by choosing a proper method
(e.g. RCM), making applications such as better
question routing and better incentive mechanism
1122
0.45
0.55
0.65
0.75
11 12 13 14 15 16 17 18 19 20
Acc
urac
y
K
PR TS CM(H)CM(Q) RCM(H) RCM(Q)
Figure 4: ACC of different methods for cold-start
questions on SO/CPP.
0.5
0.6
0.7
0.8
6 7 8 9 10 11 12 13 14 15
Acc
urac
y
K
PR TS CM(H)CM(Q) RCM(H) RCM(Q)
Figure 5: ACC of different methods for cold-start
questions on SO/Math.
design feasible in practice. 2) As the value of K
varies, RCM (the red/blue solid line) performs al-
most consistently better than CM with the same
loss type (the red/blue dotted line), as well as PR
and TS (the gray dotted lines), showing the advan-
tages of RCM in the cold-start estimation. 3) The
cold-start estimation accuracies on SO/Math are
higher than those on SO/CPP, again demonstrating
that the textual descriptions of the SO/Math ques-
tions are more powerful in reflecting their difficul-
ty levels. This is consistent with the phenomenon
observed in Section 5.2.
5.4 Difficulty Levels of Words
The third experiment explored how a word?s diffi-
culty level can be measured by RCM automatical-
ly and quantitatively.
On both SO/CPP and SO/Math, we evenly split
the range of question difficulty scores (estimated
by RCM) into 10 buckets, and assigned questions
to the buckets according to their difficulty scores.
A larger bucket ID indicates a higher difficulty lev-
el. Then, given a word w, we calculated its fre-
quency in each bucket as follows:
f
i
(w) =
# questions in bucket i where w occurs
# all questions in bucket i
.
To make the frequency meaningful, buckets with
less than 50 questions were discarded. We picked
PR TS
CM RCM
H Q H Q
SO/CPP 0.5870 0.5413 0.6120 0.6304 0.6380 0.6609
SO/Math 0.6411 0.6305 0.6653 0.7263 0.6958 0.7442
Table 4: Averaged ACC of different methods for
cold-start questions.
0
0.4
0.8
1.2
3 3.5 4 4.5 5 5.5 6 6.5 7
Occ
urre
nce 
freq
uenc
y
Question buckets
array string virtual multithread
Figure 6: Frequencies of different words in the
buckets on SO/CPP.
four words from each data set as examples. Their
normalized frequencies in different buckets are
shown in Figure 6 and Figure 7. On SO/CPP,
we can observe that ?array? and ?string? occur
most frequently in questions with lower difficul-
ty levels, ?virtual? higher, and ?multithread? the
highest. It coincides with the intuition: ?array?
and ?string? are usually related to some basic con-
cepts in programming language, while ?virtual?
and ?multithread? usually discuss more advanced
topics. Similar phenomena can be observed on
SO/Math. The results indicate that RCM might
provide an automatic way to measure the difficul-
ty levels of words.
6 Related Work
QDE is relevant to the problem of estimating task
difficulty levels and user expertise levels in crowd-
sourcing services (Yang et al., 2008; Whitehill et
al., 2009). Studies on this problem fall into two
categories: 1) binary response based and 2) par-
tially ordered response based. In the first cate-
gory, binary responses (i.e. whether the solution
provided by a user is correct or not) are observed,
and techniques based on item response theory are
further employed (Whitehill et al., 2009; Welin-
der et al., 2010; Zhou et al., 2012). In the second
category, partially ordered responses (i.e. which
of the two given solutions is better) are observed,
and pairwise comparison based methods are fur-
ther adopted (Yang et al., 2008; Liu et al., 2013).
QDE belongs to the latter.
1123
00.4
0.8
1.2
4 5 6 7 8 9
Occ
urre
nce 
freq
uenc
y
Question buckets
homework calculus ring topology
Figure 7: Frequencies of different words in the
buckets on SO/Math.
The most relevant work to ours is a pairwise
comparison based approach proposed by Liu et al.
(2013) to estimate question difficulty levels in C-
QA services. They have also demonstrated that
a similar approach can be utilized to estimate us-
er expertise levels (Liu et al., 2011). Yang et al.
(2008) and Chen et al. (2013) have also proposed
pairwise comparison based methods, for task dif-
ficulty estimation and rank aggregation in crowd-
sourcing settings. Our work differs from previous
pairwise comparison based methods in that it fur-
ther utilizes textual information, formalized as a
manifold regularizer.
Manifold regularization is a geometrically mo-
tivated framework for machine learning, enforcing
the learning model to be smooth w.r.t. the geomet-
rical structure of data (Belkin et al., 2006). Within
the framework, dimensionality reduction (Belkin
and Niyogi, 2001; Cai et al., 2008) and semi-
supervised learning (Zhou et al., 2004; Zhu and
Lafferty, 2005) algorithms have been constructed.
In dimensionality reduction, manifold regulariza-
tion is utilized to guarantee that nearby points will
have similar low-dimensional representations (Cai
et al., 2008), while in semi-supervised learning it
is utilized to ensure that nearby points will have
similar labels (Zhou et al., 2004). In our work, we
assume that nearby questions (in terms of textual
similarities) will have similar difficulty levels.
Predicting reading difficulty levels of text is
also a relevant problem (Collins-Thompson and
Callan, 2004; Schwarm and Ostendorf, 2005). It
is a key to automatically finding materials at ap-
preciate reading levels for students, and also helps
in personalized web search (Collins-Thompson et
al., 2011). In the task of predicting reading dif-
ficulty levels, documents targeting different grade
levels are taken as ground truth, which can be eas-
ily obtained from the web. However, there is no
naturally annotated data for our QDE task on the
web. Other related problems include query dif-
ficulty estimation for search engines (Carmel et
al., 2006; Yom-Tov et al., 2005) and question dif-
ficulty estimation for automatic question answer-
ing systems (Lange et al., 2004). In these tasks,
query/question difficulty is system-oriented and ir-
relevant with human knowledge, which is a differ-
ent setting from ours.
7 Conclusion and Future Work
In this paper, we have proposed a novel method for
estimating question difficulty levels in CQA ser-
vices, called Regularized Competition Model (R-
CM). It takes fully advantage of questions? textu-
al descriptions besides question-user comparisons,
and thus can effectively deal with data sparsity and
perform more accurate estimation. A K-Nearest
Neighbor approach is further adopted to estimate
difficulty levels of cold-start questions. Experi-
ments on two publicly available data sets show
that RCM performs significantly better than exist-
ing methods in the estimation task, for both well-
resolved and cold-start questions, demonstrating
the advantage of incorporating textual informa-
tion. It is also observed that RCM might automat-
ically measure the knowledge levels of words.
As future work, we plan to 1) Enhance the ef-
ficiency and scalability of RCM. The complexity
analysis in Section 3.2 indicates that storing and
processing the graph Laplacian is a bottleneck of
RCM. We would like to investigate how to deal
with the bottleneck, e.g., via parallel or distribut-
ed computing. 2) Apply RCM to non-technical
domains. For non-technical domains such as the
?news and events? category of Yahoo! Answer-
s, there might be no strongly distinct notions of
?experts? and ?non-experts?, and it might be more
difficult to distinguish between ?hard questions?
and ?easy questions?. It is worthy investigating
whether RCM still works on such domains.
Acknowledgments
We would like to thank the anonymous review-
ers for their helpful comments. This work is
supported by the Strategic Priority Research Pro-
gram of the Chinese Academy of Sciences (grant
No. XDA06030200), the National Key Technolo-
gy R&D Program (grant No. 2012BAH46B03),
and the National Natural Science Foundation of
China (grant No. 61272427).
1124
References
Mark S. Ackerman and David W. McDonald. 1996.
Answer garden 2: merging organizational memory
with collaborative help. In Proceedings of the 1996
ACM Conference on Computer Supported Coopera-
tive Work, pages 97?105.
Mikhail Belkin and Partha Niyogi. 2001. Laplacian
eigenmaps and spectral techniques for embedding
and clustering. In Advances in Neural Information
Processing Systems, pages 585?591.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2006. Manifold regularization: a geometric frame-
work for learning from labeled and unlabeled ex-
amples. Journal of Machine Learning Research,
7:2399?2434.
Stephen Boyd, Lin Xiao, and Almir Mutapcic. 2003.
Subgradient methods. Lecture Notes of EE392o, S-
tanford University.
Deng Cai, Xiaofei He, Xiaoyun Wu, and Jiawei Han.
2008. Non-negative matrix factorization on mani-
fold. In Proceedings of the 8th IEEE International
Conference on Data Mining, pages 63?72.
David Carmel, Elad Yom-Tov, Adam Darlow, and Dan
Pelleg. 2006. What makes a query difficult? In
Proceedings of the 29th International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 390?397.
Olivier Chapelle. 2007. Training a support vec-
tor machine in the primal. Neural Computation,
19(5):1155?1178.
Xi Chen, Paul N. Bennett, Kevyn Collins-Thompson,
and Eric Horvitz. 2013. Pairwise ranking aggrega-
tion in a crowdsourced setting. In Proceedings of the
6th ACM International Conference on Web Search
and Data Mining, pages 193?202.
Fan RK. Chung. 1997. Spectral Graph Theory, vol-
ume 92.
Kenneth Church. 2011. How many multiword expres-
sions do people know. In Proceedings of the ACL-
HLT Workshop on Multiword Expressions: from
Parsing and Generation to the Real World, pages
137?144.
Mark Claypool, Anuja Gokhale, Tim Miranda, Pavel
Murnikov, Dmitry Netes, andMatthew Sartin. 1999.
Combining content-based and collaborative filters in
an online newspaper. In Proceedings of the ACM
SIGIR workshop on Recommender Systems.
Jacob Cohen. 1960. A coefficient of agreemen-
t for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
Kevyn Collins-Thompson and James P. Callan. 2004.
A language modeling approach to predicting reading
difficulty. In Proceedings of the 2004 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 193?200.
Kevyn Collins-Thompson, Paul N Bennett, Ryen W
White, Sebastian de la Chica, and David Sontag.
2011. Personalizing web search results by reading
level. In Proceedings of the 20th ACM Internation-
al Conference on Information and Knowledge Man-
agement, pages 403?412.
Thomas Cover and Peter Hart. 1967. Nearest neighbor
pattern classification. IEEE Transactions on Infor-
mation Theory, 13(1):21?27.
Zhicheng Dou, Ruihua Song, and Ji Rong Wen. 2007.
A large-scale evaluation and analysis of personal-
ized search strategies. In Proceedings of the 16th
International Conference on World Wide Web, pages
581?590.
Ralf Herbrich, Tom Minka, and Thore Graepel. 2006.
Trueskill: a bayesian skill rating system. In Ad-
vances in Neural Information Processing Systems,
pages 569?576.
Anna Huang. 2008. Similarity measures for text doc-
ument clustering. In Proceedings of the 6th New
Zealand Computer Science Research Student Con-
ference, pages 49?56.
Rense Lange, Juan Moran, Warren R. Greiff, and Lisa
Ferro. 2004. A probabilistic rasch analysis of ques-
tion answering evaluations. In Proceedings of the
2004 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 65?72.
Baichuan Li and Irwin King. 2010. Routing ques-
tions to appropriate answerers in community ques-
tion answering services. In Proceedings of the 19th
ACM International Conference on Information and
Knowledge Management, pages 1585?1588.
Jing Liu, Young-In Song, and Chin-Yew Lin. 2011.
Competition-based user expertise score estimation.
In Proceedings of the 34th International ACM SI-
GIR Conference on Research and Development in
Information Retrieval, pages 425?434.
Jing Liu, Quan Wang, Chin-Yew Lin, and Hsiao-Wuen
Hon. 2013. Question difficulty estimation in com-
munity question answering services. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 85?90.
Kevin Kyung Nam, Mark S. Ackerman, and Lada A.
Adamic. 2009. Questions in, knowledge in?: a s-
tudy of naver?s question answering community. In
Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems, pages 779?788.
Larry Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
bringing order to the web. Technical Report, Stan-
ford University.
1125
Joseph Lee Rodgers and W. Alan Nicewander. 1988.
Thirteen ways to look at the correlation coefficient.
The American Statistician, 42(1):59?66.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation Processing & Management, 24(5):513?
523.
Andrew I. Schein, Alexandrin Popescul, Lyle H. Ungar,
and David M. Pennock. 2002. Methods and met-
rics for cold-start recommendations. In Proceed-
ings of the 25th International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 253?260.
Sarah E. Schwarm and Mari Ostendorf. 2005. Reading
level assessment using support vector machines and
statistical language models. In Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 523?530.
Kazunari Sugiyama, Kenji Hatano, and Masatoshi
Yoshikawa. 2004. Adaptive web search based on
user profile constructed without any effort from user-
s. In Proceedings of the 13th International Confer-
ence on World Wide Web, pages 675?684.
Peter Welinder, Steve Branson, Serge Belongie, and
Pietro Perona. 2010. The multidimensional wis-
dom of crowds. In Advances in Neural Information
Processing Systems, pages 2424?2432.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier R Movellan. 2009. Whose
vote should count more: optimal integration of la-
bels from labelers of unknown expertise. In Ad-
vances in Neural Information Processing Systems,
pages 2035?2043.
Jiang Yang, Lada Adamic, and Mark Ackerman. 2008.
Competing to share expertise: the taskcn knowledge
sharing community. In Proceedings of the 2nd In-
ternational AAAI Conference on Weblogs and Social
Media.
Elad Yom-Tov, Shai Fine, David Carmel, and Adam
Darlow. 2005. Learning to estimate query difficulty:
including applications to missing content detection
and distributed information retrieval. In Proceed-
ings of the 28th International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 512?519.
Dengyong Zhou, Olivier Bousquet, Thomas Navin
Lal, Jason Weston, and Bernhard Sch?olkopf. 2004.
Learning with local and global consistency. In Ad-
vances in Neural Information Processing Systems,
pages 321?328.
Yanhong Zhou, Gao Cong, Bin Cui, Christian S.
Jensen, and Junjie Yao. 2009. Routing questions to
the right users in online communities. In Proceed-
ings of the 25th IEEE International Conference on
Data Engineering, pages 700?711.
Dengyong Zhou, John C Platt, Sumit Basu, and Y-
i Mao. 2012. Learning from the wisdom of crowds
by minimax entropy. In Advances in Neural Infor-
mation Processing Systems, pages 2204?2212.
Xiaojin Zhu and John Lafferty. 2005. Harmonic mix-
tures: combining mixture models and graph-based
methods for inductive and scalable semi-supervised
learning. In Proceedings of the 22nd Internation-
al Conference on Machine Learning, pages 1052?
1059.
1126
