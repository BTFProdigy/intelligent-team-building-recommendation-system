Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 136?144,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Transliteration Alignment
Vladimir Pervouchine, Haizhou Li
Institute for Infocomm Research
A*STAR, Singapore 138632
{vpervouchine,hli}@i2r.a-star.edu.sg
Bo Lin
School of Computer Engineering
NTU, Singapore 639798
linbo@pmail.ntu.edu.sg
Abstract
This paper studies transliteration align-
ment, its evaluation metrics and applica-
tions. We propose a new evaluation met-
ric, alignment entropy, grounded on the
information theory, to evaluate the align-
ment quality without the need for the gold
standard reference and compare the metric
with F -score. We study the use of phono-
logical features and affinity statistics for
transliteration alignment at phoneme and
grapheme levels. The experiments show
that better alignment consistently leads to
more accurate transliteration. In transliter-
ation modeling application, we achieve a
mean reciprocal rate (MRR) of 0.773 on
Xinhua personal name corpus, a signifi-
cant improvement over other reported re-
sults on the same corpus. In transliteration
validation application, we achieve 4.48%
equal error rate on a large LDC corpus.
1 Introduction
Transliteration is a process of rewriting a word
from a source language to a target language in a
different writing system using the word?s phono-
logical equivalent. The word and its translitera-
tion form a transliteration pair. Many efforts have
been devoted to two areas of studies where there
is a need to establish the correspondence between
graphemes or phonemes between a transliteration
pair, also known as transliteration alignment.
One area is the generative transliteration model-
ing (Knight and Graehl, 1998), which studies how
to convert a word from one language to another us-
ing statistical models. Since the models are trained
on an aligned parallel corpus, the resulting statisti-
cal models can only be as good as the alignment of
the corpus. Another area is the transliteration vali-
dation, which studies the ways to validate translit-
eration pairs. For example Knight and Graehl
(1998) use the lexicon frequency, Qu and Grefen-
stette (2004) use the statistics in a monolingual
corpus and the Web, Kuo et al (2007) use proba-
bilities estimated from the transliteration model to
validate transliteration candidates. In this paper,
we propose using the alignment distance between
the a bilingual pair of words to establish the evi-
dence of transliteration candidacy. An example of
transliteration pair alignment is shown in Figure 1.
e
5
e
1
e
2
e
3
e
4
c
1
c
2
c
3
A   L I  C E
? ? ?
source graphemes
target graphemes
e
1
e
2
e
3
grapheme tokens
Figure 1: An example of grapheme alignment (Al-
ice, ???), where a Chinese grapheme, a char-
acter, is aligned to an English grapheme token.
Like the word alignment in statistical ma-
chine translation (MT), transliteration alignment
becomes one of the important topics in machine
transliteration, which has several unique chal-
lenges. Firstly, the grapheme sequence in a word
is not delimited into grapheme tokens, resulting
in an additional level of complexity. Secondly, to
maintain the phonological equivalence, the align-
ment has to make sense at both grapheme and
phoneme levels of the source and target languages.
This paper reports progress in our ongoing spoken
language translation project, where we are inter-
ested in the alignment problem of personal name
transliteration from English to Chinese.
This paper is organized as follows. In Section 2,
we discuss the prior work. In Section 3, we in-
troduce both statistically and phonologically mo-
tivated alignment techniques and in Section 4 we
advocate an evaluation metric, alignment entropy
that measures the alignment quality. We report the
experiments in Section 5. Finally, we conclude in
Section 6.
136
2 Related Work
A number of transliteration studies have touched
on the alignment issue as a part of the translit-
eration modeling process, where alignment is
needed at levels of graphemes and phonemes. In
their seminal paper Knight and Graehl (1998) de-
scribed a transliteration approach that transfers the
grapheme representation of a word via the pho-
netic representation, which is known as phoneme-
based transliteration technique (Virga and Khu-
danpur, 2003; Meng et al, 2001; Jung et al,
2000; Gao et al, 2004). Another technique is
to directly transfer the grapheme, known as di-
rect orthographic mapping, that was shown to
be simple and effective (Li et al, 2004). Some
other approaches that use both source graphemes
and phonemes were also reported with good per-
formance (Oh and Choi, 2002; Al-Onaizan and
Knight, 2002; Bilac and Tanaka, 2004).
To align a bilingual training corpus, some take a
phonological approach, in which the crafted map-
ping rules encode the prior linguistic knowledge
about the source and target languages directly into
the system (Wan and Verspoor, 1998; Meng et al,
2001; Jiang et al, 2007; Xu et al, 2006). Oth-
ers adopt a statistical approach, in which the affin-
ity between phonemes or graphemes is learned
from the corpus (Gao et al, 2004; AbdulJaleel and
Larkey, 2003; Virga and Khudanpur, 2003).
In the phoneme-based technique where an in-
termediate level of phonetic representation is used
as the pivot, alignment between graphemes and
phonemes of the source and target words is
needed (Oh and Choi, 2005). If source and tar-
get languages have different phoneme sets, align-
ment between the the different phonemes is also
required (Knight and Graehl, 1998). Although
the direct orthographic mapping approach advo-
cates a direct transfer of grapheme at run-time,
we still need to establish the grapheme correspon-
dence at the model training stage, when phoneme
level alignment can help.
It is apparent that the quality of transliteration
alignment of a training corpus has a significant
impact on the resulting transliteration model and
its performance. Although there are many stud-
ies of evaluation metrics of word alignment for
MT (Lambert, 2008), there has been much less re-
ported work on evaluation metrics of translitera-
tion alignment. In MT, the quality of training cor-
pus alignment A is often measured relatively to
the gold standard, or the ground truth alignment
G, which is a manual alignment of the corpus or
a part of it. Three evaluation metrics are used:
precision, recall, and F -score, the latter being a
function of the former two. They indicate how
close the alignment under investigation is to the
gold standard alignment (Mihalcea and Pedersen,
2003). Denoting the number of cross-lingual map-
pings that are common in both A and G as CAG,
the number of cross-lingual mappings in A as CA
and the number of cross-lingual mappings in G as
CG, precision Pr is given as CAG/CA, recall Rc
as CAG/CG and F -score as 2Pr ?Rc/(Pr+Rc).
Note that these metrics hinge on the availability
of the gold standard, which is often not available.
In this paper we propose a novel evaluation metric
for transliteration alignment grounded on the in-
formation theory. One important property of this
metric is that it does not require a gold standard
alignment as a reference. We will also show that
how this metric is used in generative transliteration
modeling and transliteration validation.
3 Transliteration alignment techniques
We assume in this paper that the source language
is English and the target language is Chinese, al-
though the technique is not restricted to English-
Chinese alignment.
Let a word in the source language (English) be
{ei} = {e1 . . . eI} and its transliteration in the
target language (Chinese) be {cj} = {c1 . . . cJ},
ei ? E, cj ? C, and E, C being the English and
Chinese sets of characters, or graphemes, respec-
tively. Aligning {ei} and {cj} means for each tar-
get grapheme token c?j finding a source grapheme
token e?m, which is an English substring in {ei}
that corresponds to cj , as shown in the example in
Figure 1. As Chinese is syllabic, we use a Chinese
character cj as the target grapheme token.
3.1 Grapheme affinity alignment
Given a distance function between graphemes of
the source and target languages d(ei, cj), the prob-
lem of alignment can be formulated as a dynamic
programming problem with the following function
to minimize:
Dij = min(Di?1,j?1 + d(ei, cj),
Di,j?1 + d(?, cj),
Di?1,j + d(ei, ?))
(1)
137
Here the asterisk * denotes a null grapheme that
is introduced to facilitate the alignment between
graphemes of different lengths. The minimum dis-
tance achieved is then given by
D =
I?
i=1
d(ei, c?(i)) (2)
where j = ?(i) is the correspondence between the
source and target graphemes. The alignment can
be performed via the Expectation-Maximization
(EM) by starting with a random initial alignment
and calculating the affinity matrix count(ei, cj)
over the whole parallel corpus, where element
(i, j) is the number of times character ei was
aligned to cj . From the affinity matrix conditional
probabilities P (ei|cj) can be estimated as
P (ei|cj) = count(ei, cj)/
?
j
count(ei, cj) (3)
Alignment j = ?(i) between {ei} and {cj} that
maximizes probability
P =
?
i
P (c?(i)|ei) (4)
is also the same alignment that minimizes align-
ment distance D:
D = ? logP = ?
?
i
logP (c?(i)|ei) (5)
In other words, equations (2) and (5) are the same
when we have the distance function d(ei, cj) =
? logP (cj |ei). Minimizing the overall distance
over a training corpus, we conduct EM iterations
until the convergence is achieved.
This technique solely relies on the affinity
statistics derived from training corpus, thus is
called grapheme affinity alignment. It is also
equally applicable for alignment between a pair of
symbol sequences representing either graphemes
or phonemes. (Gao et al, 2004; AbdulJaleel and
Larkey, 2003; Virga and Khudanpur, 2003).
3.2 Grapheme alignment via phonemes
Transliteration is about finding phonological
equivalent. It is therefore a natural choice to use
the phonetic representation as the pivot. It is
common though that the sound inventory differs
from one language to another, resulting in differ-
ent phonetic representations for source and tar-
get words. Continuing with the earlier example,
?
AE L AH S
A L I C E
AY l i s iz
? ?
graphemes
phonemes
phonemes
graphemes
source
target
Figure 2: An example of English-Chinese translit-
eration alignment via phonetic representations.
Figure 2 shows the correspondence between the
graphemes and phonemes of English word ?Al-
ice? and its Chinese transliteration, with CMU
phoneme set used for English (Chase, 1997) and
IIR phoneme set for Chinese (Li et al, 2007a).
A Chinese character is often mapped to a unique
sequence of Chinese phonemes. Therefore, if
we align English characters {ei} and Chinese
phonemes {cpk} (cpk ? CP set of Chinese
phonemes) well, we almost succeed in aligning
English and Chinese grapheme tokens. Alignment
between {ei} and {cpk} becomes the main task in
this paper.
3.2.1 Phoneme affinity alignment
Let the phonetic transcription of English word
{ei} be {epn}, epn ? EP , where EP is the set of
English phonemes. Alignment between {ei} and
{epn}, as well as between {epn} and {cpk} can
be performed via EM as described above. We esti-
mate conditional probability of Chinese phoneme
cpk after observing English character ei as
P (cpk|ei) =
?
{epn}
P (cpk|epn)P (epn|ei) (6)
We use the distance function between English
graphemes and Chinese phonemes d(ei, cpk) =
? logP (cpk|ei) to perform the initial alignment
between {ei} and {cpk} via dynamic program-
ming, followed by the EM iterations until con-
vergence. The estimates for P (cpk|epn) and
P (epn|ei) are obtained from the affinity matrices:
the former from the alignment of English and Chi-
nese phonetic representations, the latter from the
alignment of English words and their phonetic rep-
resentations.
3.2.2 Phonological alignment
Alignment between the phonetic representations
of source and target words can also be achieved
using the linguistic knowledge of phonetic sim-
ilarity. Oh and Choi (2002) define classes of
138
phonemes and assign various distances between
phonemes of different classes. In contrast, we
make use of phonological descriptors to define the
similarity between phonemes in this paper.
Perhaps the most common way to measure the
phonetic similarity is to compute the distances be-
tween phoneme features (Kessler, 2005). Such
features have been introduced in many ways, such
as perceptual attributes or articulatory attributes.
Recently, Tao et al (2006) and Yoon et al (2007)
have studied the use of phonological features and
manually assigned phonological distance to mea-
sure the similarity of transliterated words for ex-
tracting transliterations from a comparable corpus.
We adopt the binary-valued articulatory at-
tributes as the phonological descriptors, which are
used to describe the CMU and IIR phoneme sets
for English and Chinese Mandarin respectively.
Withgott and Chen (1993) define a feature vec-
tor of phonological descriptors for English sounds.
We extend the idea by defining a 21-element bi-
nary feature vector for each English and Chinese
phoneme. Each element of the feature vector
represents presence or absence of a phonologi-
cal descriptor that differentiates various kinds of
phonemes, e.g. vowels from consonants, front
from back vowels, nasals from fricatives, etc1.
In this way, a phoneme is described by a fea-
ture vector. We express the similarity between
two phonemes by the Hamming distance, also
called the phonological distance, between the two
feature vectors. A difference in one descriptor
between two phonemes increases their distance
by 1. As the descriptors are chosen to differenti-
ate between sounds, the distance between similar
phonemes is low, while that between two very dif-
ferent phonemes, such as a vowel and a consonant,
is high. The null phoneme, added to both English
and Chinese phoneme sets, has a constant distance
to any actual phonemes, which is higher than that
between any two actual phonemes.
We use the phonological distance to perform
the initial alignment between English and Chi-
nese phonetic representations of words. After that
we proceed with recalculation of the distances be-
tween phonemes using the affinity matrix as de-
scribed in Section 3.1 and realign the corpus again.
We continue the iterations until convergence is
1The complete table of English and Chinese phonemes
with their descriptors, as well as the translitera-
tion system demo is available at http://translit.i2r.a-
star.edu.sg/demos/transliteration/
reached. Because of the use of phonological de-
scriptors for the initial alignment, we call this tech-
nique the phonological alignment.
4 Transliteration alignment entropy
Having aligned the graphemes between two lan-
guages, we want to measure how good the align-
ment is. Aligning the graphemes means aligning
the English substrings, called the source grapheme
tokens, to Chinese characters, the target grapheme
tokens. Intuitively, the more consistent the map-
ping is, the better the alignment will be. We can
quantify the consistency of alignment via align-
ment entropy grounded on information theory.
Given a corpus of aligned transliteration pairs,
we calculate count(cj , e?m), the number of times
each Chinese grapheme token (character) cj is
mapped to each English grapheme token e?m. We
use the counts to estimate probabilities
P (e?m, cj) = count(cj , e?m)/
?
m,j
count(cj , e?m)
P (e?m|cj) = count(cj , e?m)/
?
m
count(cj , e?m)
The alignment entropy of the transliteration corpus
is the weighted average of the entropy values for
all Chinese tokens:
H = ?
?
j
P (cj)
?
m
P (e?m|cj) logP (e?m|cj)
= ?
?
m,j
P (e?m, cj) logP (e?m|cj)
(7)
Alignment entropy indicates the uncertainty of
mapping between the English and Chinese tokens
resulting from alignment. We expect and will
show that this estimate is a good indicator of the
alignment quality, and is as effective as the F -
score, but without the need for a gold standard ref-
erence. A lower alignment entropy suggests that
each Chinese token tends to be mapped to fewer
distinct English tokens, reflecting better consis-
tency. We expect a good alignment to have a
sharp cross-lingual mapping with low alignment
entropy.
5 Experiments
We use two transliteration corpora: Xinhua cor-
pus (Xinhua News Agency, 1992) of 37,637
personal name pairs and LDC Chinese-English
139
named entity list LDC2005T34 (Linguistic Data
Consortium, 2005), containing 673,390 personal
name pairs. The LDC corpus is referred to as
LDC05 for short hereafter. For the results to be
comparable with other studies, we follow the same
splitting of Xinhua corpus as that in (Li et al,
2007b) having a training and testing set of 34,777
and 2,896 names respectively. In contrast to the
well edited Xinhua corpus, LDC05 contains erro-
neous entries. We have manually verified and cor-
rected around 240,000 pairs to clean up the corpus.
As a result, we arrive at a set of 560,768 English-
Chinese (EC) pairs that follow the Chinese pho-
netic rules, and a set of 83,403 English-Japanese
Kanji (EJ) pairs, which follow the Japanese pho-
netic rules, and the rest 29,219 pairs (REST) be-
ing labeled as incorrect transliterations. Next we
conduct three experiments to study 1) alignment
entropy vs. F -score, 2) the impact of alignment
quality on transliteration accuracy, and 3) how to
validate transliteration using alignment metrics.
5.1 Alignment entropy vs. F -score
As mentioned earlier, for English-Chinese
grapheme alignment, the main task is to align En-
glish graphemes to Chinese phonemes. Phonetic
transcription for the English names in Xinhua
corpus are obtained by a grapheme-to-phoneme
(G2P) converter (Lenzo, 1997), which generates
phoneme sequence without providing the exact
correspondence between the graphemes and
phonemes. G2P converter is trained on the CMU
dictionary (Lenzo, 2008).
We align English grapheme and phonetic repre-
sentations e? ep with the affinity alignment tech-
nique (Section 3.1) in 3 iterations. We further
align the English and Chinese phonetic represen-
tations ep ? cp via both affinity and phonological
alignment techniques, by carrying out 6 and 7 it-
erations respectively. The alignment methods are
schematically shown in Figure 3.
To study how alignment entropy varies accord-
ing to different quality of alignment, we would
like to have many different alignment results. We
pair the intermediate results from the e ? ep and
ep ? cp alignment iterations (see Figure 3) to
form e ? ep ? cp alignments between English
graphemes and Chinese phonemes and let them
converge through few more iterations, as shown
in Figure 4. In this way, we arrive at a total of 114
phonological and 80 affinity alignments of differ-
ent quality.
{cp
k
}
{e
i
}
English
graphemes
{ep
n
}
English
phonemes
Chinese
phonemes
affinity alignment affinity alignment
e? ep
iteration 1
e? ep
iteration 2
e? ep
iteration 3
ep? cp
iteration 1
ep? cp
iteration 2
...
ep? cp
iteration 6
phonological alignment
ep? cp
iteration 1
ep? cp
iteration 2
...
ep? cp
iteration 7
Figure 3: Aligning English graphemes to
phonemes e?ep and English phonemes to Chinese
phonemes ep?cp. Intermediate e?ep and ep?cp
alignments are used for producing e ? ep ? cp
alignments.
e? ep
alignments
ep? cp
affinity / 
phonological
alignments
iteration 1
iteration 2
iteration 3
iteration 1
iteration 2
iteration n
...
...
calculating
d(e
i
, cp
k
)
affinity
alignment
iteration 1
iteration 2
...
e? ep? cp
etc
Figure 4: Example of aligning English graphemes
to Chinese phonemes. Each combination of e?ep
and ep? cp alignments is used to derive the initial
distance d(ei, cpk), resulting in several e?ep?cp
alignments due to the affinity alignment iterations.
We have manually aligned a random set of
3,000 transliteration pairs from the Xinhua train-
ing set to serve as the gold standard, on which we
calculate the precision, recall and F -score as well
as alignment entropy for each alignment. Each
alignment is reflected as a data point in Figures 5a
and 5b. From the figures, we can observe a clear
correlation between the alignment entropy and F -
score, that validates the effectiveness of alignment
entropy as an evaluation metric. Note that we
don?t need the gold standard reference for report-
ing the alignment entropy.
We also notice that the data points seem to form
clusters inside which the value of F -score changes
insignificantly as the alignment entropy changes.
Further investigation reveals that this could be due
to the limited number of entries in the gold stan-
dard. The 3,000 names in the gold standard are not
enough to effectively reflect the change across dif-
ferent alignments. F -score requires a large gold
standard which is not always available. In con-
trast, because the alignment entropy doesn?t de-
pend on the gold standard, one can easily report
the alignment performance on any unaligned par-
allel corpus.
140
??????
??????
??????
???
??? ??? ??? ???
?? ? ?
???? ?? ? ? ?
?? ??
(a) 80 affinity alignments
??????
??????
??????
???
??? ??? ??? ???
???? ?? ? ? ?
?? ??
?? ? ?
(b) 114 phonological alignments
Figure 5: Correlation between F -score and align-
ment entropy for Xinhua training set algnments.
Results for precision and recall have similar trends
.
5.2 Impact of alignment quality on
transliteration accuracy
We now further study how the alignment affects
the generative transliteration model in the frame-
work of the joint source-channel model (Li et al,
2004). This model performs transliteration by
maximizing the joint probability of the source and
target names P ({ei}, {cj}), where the source and
target names are sequences of English and Chi-
nese grapheme tokens. The joint probability is
expressed as a chain product of a series of condi-
tional probabilities of token pairs P ({ei}, {cj}) =
P ((e?k, ck)|(e?k?1, ck?1)), k = 1 . . . N , where we
limit the history to one preceding pair, resulting in
a bigram model. The conditional probabilities for
token pairs are estimated from the aligned training
corpus. We use this model because it was shown
to be simple yet accurate (Ekbal et al, 2006; Li
et al, 2007b). We train a model for each of the
114 phonological alignments and the 80 affinity
alignments in Section 5.1 and conduct translitera-
tion experiment on the Xinhua test data.
During transliteration, an input English name
is first decoded into a lattice of all possible En-
glish and Chinese grapheme token pairs. Then the
joint source-channel transliteration model is used
to score the lattice to obtain a ranked list ofmmost
likely Chinese transliterations (m-best list).
We measure transliteration accuracy as the
mean reciprocal rank (MRR) (Kantor and
Voorhees, 2000). If there is only one correct
Chinese transliteration of the k-th English word
and it is found at the rk-th position in the m-best
list, its reciprocal rank is 1/rk. If the list contains
no correct transliterations, the reciprocal rank is
0. In case of multiple correct transliterations, we
take the one that gives the highest reciprocal rank.
MRR is the average of the reciprocal ranks across
all words in the test set. It is commonly used as
a measure of transliteration accuracy, and also
allows us to make a direct comparison with other
reported work (Li et al, 2007b).
We take m = 20 and measure MRR on Xinhua
test set for each alignment of Xinhua training set
as described in Section 5.1. We report MRR and
the alignment entropy in Figures 6a and 7a for the
affinity and phonological alignments respectively.
The highest MRR we achieve is 0.771 for affin-
ity alignments and 0.773 for phonological align-
ments. This is a significant improvement over the
MRR of 0.708 reported in (Li et al, 2007b) on the
same data. We also observe that the phonological
alignment technique produces, on average, better
alignments than the affinity alignment technique
in terms of both the alignment entropy and MRR.
We also report the MRR and F -scores for each
alignment in Figures 6b and 7b, from which we
observe that alignment entropy has stronger corre-
lation with MRR than F -score does. The Spear-
man?s rank correlation coefficients are ?0.89 and
?0.88 for data in Figure 6a and 7a respectively.
This once again demonstrates the desired property
of alignment entropy as an evaluation metric of
alignment.
To validate our findings from Xinhua corpus,
we further carry out experiments on the EC set
of LDC05 containing 560,768 entries. We split
the set into 5 almost equal subsets for cross-
validation: in each of 5 experiments one subset is
used for testing and the remaining ones for train-
ing. Since LDC05 contains one-to-many English-
Chinese transliteration pairs, we make sure that an
English name only appears in one subset.
Note that the EC set of LDC05 contains
many names of non-English, and, generally, non-
European origin. This makes the G2P converter
less accurate, as it is trained on an English pho-
netic dictionary. We therefore only apply the affin-
ity alignment technique to align the EC set. We
141
??????
??????
??????
??? ??? ??? ???
MRR
Alignment?entropy
(a) 80 affinity alignments
??????
??????
??????
??? ??? ??? ??? ??? ??? ???
MRR
F?score
(b) 80 affinity alignments
Figure 6: Mean reciprocal ratio on Xinhua test
set vs. alignment entropy and F -score for mod-
els trained with different affinity alignments.
use each iteration of the alignment in the translit-
eration modeling and present the resulting MRR
along with alignment entropy in Figure 8. The
MRR results are the averages of five values pro-
duced in the five-fold cross-validations.
We observe a clear correlation between the
alignment entropy and transliteration accuracy ex-
pressed by MRR on LDC05 corpus, similar to that
on Xinhua corpus, with the Spearman?s rank cor-
relation coefficient of ?0.77. We obtain the high-
est average MRR of 0.720 on the EC set.
5.3 Validating transliteration using
alignment measure
Transliteration validation is a hypothesis test that
decides whether a given transliteration pair is gen-
uine or not. Instead of using the lexicon fre-
quency (Knight and Graehl, 1998) or Web statis-
tics (Qu and Grefenstette, 2004), we propose vali-
dating transliteration pairs according to the align-
ment distance D between the aligned English
graphemes and Chinese phonemes (see equations
(2) and (5)). A distance function d(ei, cpk) is
established from each alignment on the Xinhua
training set as discussed in Section 5.2.
An audit of LDC05 corpus groups the corpus
into three sets: an English-Chinese (EC) set of
560,768 samples, an English-Japanese (EJ) set
of 83,403 samples and the REST set of 29,219
??????
??????
??????
??? ??? ??? ???
MRR
Alignment?entropy
(a) 114 phonological alignments
??????
??????
??????
??? ??? ??? ??? ??? ??? ???
MRR
F?score
(b) 114 phonological alignments
Figure 7: Mean reciprocal ratio on Xinhua test
set vs. alignment entropy and F -score for models
trained with different phonological alignments.
??????
??????
??????
??????
???
??? ??? ??? ???
??
???? ?? ? ? ?
Figure 8: Mean reciprocal ratio vs. alignment en-
tropy for alignments of EC set.
samples that are not transliteration pairs. We
mark the EC name pairs as genuine and the rest
112,622 name pairs that do not follow the Chi-
nese phonetic rules as false transliterations, thus
creating the ground truth labels for an English-
Chinese transliteration validation experiment. In
other words, LDC05 has 560,768 genuine translit-
eration pairs and 112,622 false ones.
We run one iteration of alignment over LDC05
(both genuine and false) with the distance func-
tion d(ei, cpk) derived from the affinity matrix of
one aligned Xinhua training set. In this way, each
transliteration pair in LDC05 provides an align-
ment distance. One can expect that a genuine
transliteration pair typically aligns well, leading
to a low distance, while a false transliteration pair
will do otherwise. To remove the effect of word
length, we normalize the distance by the English
name length, the Chinese phonetic transcription
142
length, and the sum of both, producing score1,
score2 and score3 respectively.
Miss?probability?(%)
F
a
l
s
e
?
a
l
a
r
m
?
p
r
o
b
a
b
i
l
i
t
y
?
(
%
)
2 5
10
1
2
5
1
0
1
20
2
0
score
2
EER:?4.48?%
score
1
EER:?7.13?%
score
3
EER:?4.80?%
(a) DET with score1, score2,
score3.
1 2 5 10
1
2
5
1
0
Miss?probability?(%)
F
a
l
s
e
?
a
l
a
r
m
?
p
r
o
b
a
b
i
l
i
t
y
?
(
%
)
Entropy:?2.396
MRR:?0.773
EER:?4.48?%
Entropy:?2.529
MRR:?0.764
EER:?4.52%
Entropy:?2.625
MRR:?0.754
EER:?4.70%
(b) DET results vs. three different
alignment quality.
Figure 9: Detection error tradeoff (DET) curves
for transliteration validation on LDC05.
We can now classify each LDC05 name pair as
genuine or false by having a hypothesis test. When
the test score is lower than a pre-set threshold, the
name pair is accepted as genuine, otherwise false.
In this way, each pre-set threshold will present two
types of errors, a false alarm and a miss-detect
rate. A common way to present such results is via
the detection error tradeoff (DET) curves, which
show all possible decision points, and the equal er-
ror rate (EER), when false alarm and miss-detect
rates are equal.
Figure 9a shows three DET curves based on
score1, score2 and score3 respectively for one
one alignment solution on the Xinhua training set.
The horizontal axis is the probability of miss-
detecting a genuine transliteration, while the verti-
cal one is the probability of false-alarms. It is clear
that out of the three, score2 gives the best results.
We select the alignments of Xinhua training
set that produce the highest and the lowest MRR.
We also randomly select three other alignments
that produce different MRR values from the pool
of 114 phonological and 80 affinity alignments.
Xinhua train 
set algnment
Alignment entropy 
of Xinhua train set
MRR on Xinhua 
test set
LDC 
classification 
EER, %
1
2
3
4
5
2.396 0.773 4.48
2.529 0.764 4.52
2.586 0.761 4.51
2.621 0.757 4.71
2.625 0.754 4.70
Table 1: Equal error ratio of LDC transliteration
pair validation for different alignments of Xinhua
training set.
We use each alignment to derive distance func-
tion d(ei, cpk). Table 1 shows the EER of LDC05
validation using score2, along with the alignment
entropy of the Xinhua training set that derives
d(ei, cpk), and the MRR on Xinhua test set in the
generative transliteration experiment (see Section
5.2) for all 5 alignments. To avoid cluttering Fig-
ure 9b, we show the DET curves for alignments
1, 2 and 5 only. We observe that distance func-
tion derived from better aligned Xinhua corpus,
as measured by both our alignment entropy met-
ric and MRR, leads to a higher validation accuracy
consistently on LDC05.
6 Conclusions
We conclude that the alignment entropy is a re-
liable indicator of the alignment quality, as con-
firmed by our experiments on both Xinhua and
LDC corpora. Alignment entropy does not re-
quire the gold standard reference, it thus can be
used to evaluate alignments of large transliteration
corpora and is possibly to give more reliable esti-
mate of alignment quality than the F -score metric
as shown in our transliteration experiment.
The alignment quality of training corpus has
a significant impact on the transliteration mod-
els. We achieve the highest MRR of 0.773 on
Xinhua corpus with phonological alignment tech-
nique, which represents a significant performance
gain over other reported results. Phonological
alignment outperforms affinity alignment on clean
database.
We propose using alignment distance to validate
transliterations. A high quality alignment on a
small verified corpus such as Xinhua can be effec-
tively used to validate a large noisy corpus, such
as LDC05. We believe that this property would be
useful in transliteration extraction, cross-lingual
information retrieval applications.
143
References
Nasreen AbdulJaleel and Leah S. Larkey. 2003. Sta-
tistical transliteration for English-Arabic cross lan-
guage information retrieval. In Proc. ACM CIKM.
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proc. ACL
Workshop: Computational Apporaches to Semitic
Languages.
Slaven Bilac and Hozumi Tanaka. 2004. A hybrid
back-transliteration system for Japanese. In Proc.
COLING, pages 597?603.
Lin L. Chase. 1997. Error-responsive feedback mech-
anisms for speech recognizers. Ph.D. thesis, CMU.
Asif Ekbal, Sudip Kumar Naskar, and Sivaji Bandy-
opadhyay. 2006. A modified joint source-channel
model for transliteration. In Proc. COLING/ACL,
pages 191?198
Wei Gao, Kam-Fai Wong, and Wai Lam. 2004.
Phoneme-based transliteration of foreign names for
OOV problem. In Proc. IJCNLP, pages 374?381.
Long Jiang, Ming Zhou, Lee-Feng Chien, and Cheng
Niu. 2007. Named entity translation with web min-
ing and transliteration. In IJCAI, pages 1629?1634.
Sung Young Jung, SungLim Hong, and Eunok Paek.
2000. An English to Korean transliteration model of
extended Markov window. In Proc. COLING, vol-
ume 1.
Paul. B. Kantor and Ellen. M. Voorhees. 2000. The
TREC-5 confusion track: comparing retrieval meth-
ods for scanned text. Information Retrieval, 2:165?
176.
Brett Kessler. 2005. Phonetic comparison algo-
rithms. Transactions of the Philological Society,
103(2):243?260.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
Jin-Shea Kuo, Haizhou Li, and Ying-Kuei Yang. 2007.
A phonetic similarity model for automatic extraction
of transliteration pairs. ACM Trans. Asian Language
Information Processing, 6(2).
Patrik Lambert. 2008. Exploiting lexical informa-
tion and discriminative alignment training in statis-
tical machine translation. Ph.D. thesis, Universitat
Polite`cnica de Catalunya, Barcelona, Spain.
Kevin Lenzo. 1997. t2p: text-to-phoneme converter
builder. http://www.cs.cmu.edu/?lenzo/t2p/.
Kevin Lenzo. 2008. The CMU pronounc-
ing dictionary. http://www.speech.cs.cmu.edu/cgi-
bin/cmudict.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proc. ACL, pages 159?166.
Haizhou Li, Bin Ma, and Chin-Hui Lee. 2007a. A
vector space modeling approach to spoken language
identification. IEEE Trans. Acoust., Speech, Signal
Process., 15(1):271?284.
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and Minghui
Dong. 2007b. Semantic transliteration of personal
names. In Proc. ACL, pages 120?127.
Linguistic Data Consortium. 2005. LDC Chinese-
English name entity lists LDC2005T34.
Helen M. Meng, Wai-Kit Lo, Berlin Chen, and Karen
Tang. 2001. Generate phonetic cognates to han-
dle name entities in English-Chinese cross-language
spoken document retrieval. In Proc. ASRU.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation
exercise for word alignment. In Proc. HLT-NAACL,
pages 1?10.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-
Korean transliteration model using pronunciation
and contextual rules. In Proc. COLING 2002.
Jong-Hoon Oh and Key-Sun Choi. 2005. Machine
learning based english-to-korean transliteration us-
ing grapheme and phoneme information. IEICE
Trans. Information and Systems, E88-D(7):1737?
1748.
Yan Qu and Gregory Grefenstette. 2004. Finding ideo-
graphic representations of Japanese names written in
Latin script via language identification and corpus
validation. In Proc. ACL, pages 183?190.
Tao Tao, Su-Youn Yoon, Andrew Fisterd, Richard
Sproat, and ChengXiang Zhai. 2006. Unsupervised
named entity transliteration using temporal and pho-
netic correlation. In Proc. EMNLP, pages 250?257.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proc. ACL MLNER.
Stephen Wan and Cornelia Maria Verspoor. 1998. Au-
tomatic English-Chinese name transliteration for de-
velopment of multilingual resources. In Proc. COL-
ING, pages 1352?1356.
M. M. Withgott and F. R. Chen. 1993. Computational
models of American speech. Centre for the study of
language and information.
Xinhua News Agency. 1992. Chinese transliteration
of foreign personal names. The Commercial Press.
LiLi Xu, Atsushi Fujii, and Tetsuya Ishikawa. 2006.
Modeling impression in probabilistic transliteration
into Chinese. In Proc. EMNLP, pages 242?249.
Su-Youn Yoon, Kyoung-Young Kim, and Richard
Sproat. 2007. Multilingual transliteration using fea-
ture based phonetic method. In Proc. ACL, pages
112?119.
144
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 136?144,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
CONE: Metrics for Automatic Evaluation of Named Entity              
Co-reference Resolution  
 
 
Bo Lin, Rushin Shah, Robert Frederking, Anatole Gershman 
Language Technologies Institute, School of Computer Science 
Carnegie Mellon University 
5000 Forbes Ave., PA 15213, USA 
 {bolin,rnshah,ref,anatoleg}@cs.cmu.edu 
 
 
Abstract 
Human annotation for Co-reference Resolu-
tion (CRR) is labor intensive and costly, and 
only a handful of annotated corpora are cur-
rently available. However, corpora with 
Named Entity (NE) annotations are widely 
available. Also, unlike current CRR systems, 
state-of-the-art NER systems have very high 
accuracy and can generate NE labels that are 
very close to the gold standard for unlabeled 
corpora.  We propose a new set of metrics col-
lectively called CONE for Named Entity Co-
reference Resolution (NE-CRR) that use a 
subset of gold standard annotations, with the 
advantage that this subset can be easily ap-
proximated using NE labels when gold stan-
dard CRR annotations are absent. We define 
CONE B3 and CONE CEAF metrics based on 
the traditional B3 and CEAF metrics and show 
that CONE B3 and CONE CEAF scores of any 
CRR system on any dataset are highly corre-
lated with its B3 and CEAF scores respectively. 
We obtain correlation factors greater than 0.6 
for all CRR systems across all datasets, and a 
best-case correlation factor of 0.8. We also 
present a baseline method to estimate the gold 
standard required by CONE metrics, and show 
that CONE B3 and CONE CEAF scores using 
this estimated gold standard are also correlated 
with B3 and CEAF scores respectively. We 
thus demonstrate the suitability of CONE 
B3and CONE CEAF for automatic evaluation 
of NE-CRR. 
1 Introduction 
Co-reference resolution (CRR) is the problem of 
determining whether two entity mentions in a 
text refer to the same entity in real world or not. 
Noun Phrase CRR (NP-CRR) considers all noun 
phrases as entities, while Named Entity CRR 
restricts itself to noun phrases that describe a 
Named Entity. In this paper, we consider the task 
of Named Entity CRR (NE-CRR) only. Most, if 
not all, recent efforts in the field of CRR have 
concentrated on machine-learning based ap-
proaches. Many of them formulate the problem 
as a pair-wise binary classification task, in which 
possible co-reference between every pair of men-
tions is considered, and produce chains of co-
referring mentions for each entity as their output. 
One of the most important problems in CRR is 
the evaluation of CRR results. Different evalua-
tion metrics have been proposed for this task. B-
cubed (Bagga and Baldwin, 1998) and CEAF 
(Luo, 2005) are the two most popular metrics; 
they compute Precision, Recall and F1 measure 
between matched equivalent classes and use 
weighted sums of Precision, Recall and F1 to 
produce a global score. Like all metrics, B3 and 
CEAF require gold standard annotations; howev-
er, gold standard CRR annotations are scarce, 
because producing such annotations involves a 
substantial amount of human effort since it re-
quires an in-depth knowledge of linguistics and a 
high level of understanding of the particular text. 
Consequently, very few corpora with gold stan-
dard CRR annotations are available (NIST, 2003; 
MUC-6, 1995; Agirre, 2007). By contrast, gold 
standard Named Entity (NE) annotations are easy 
to produce; indeed, there are many NE annotated 
corpora of different sizes and genres. Similarly, 
there are few CRR systems and even the best 
scores obtained by them are only in the region of 
F1 = 0.5 - 0.6. There are only four such CRR 
systems freely available, to the best of our know-
ledge (Bengston and Roth, 2007; Versley et al, 
2008; Baldridge and Torton, 2004; Baldwin and 
Carpenter, 2003). In comparison, there are nu-
merous Named Entity recognition (NER) sys-
tems, both general-purpose and specialized, and 
many of them achieve scores better than F1 = 
0.95 (Ratinov and Roth, 2009; Finkel et al, 
136
2005). Although these facts can be partly attri-
buted to the ?hardness? of CRR compared to 
NER, they also reflect the substantial gap be-
tween NER and CRR research. In this paper, we 
present a set of metrics, collectively called 
CONE, that leverage widely available NER sys-
tems and resources and tools for the task of eva-
luating co-reference resolution systems. The ba-
sic idea behind CONE is to predict a CRR sys-
tem?s performance for the task of full NE-CRR 
on some dataset using its performance for the 
subtask of named mentions extraction and group-
ing (NMEG) on that dataset. The advantage of 
doing so is that measuring NE-CRR performance 
requires the co-reference information of all men-
tions of a Named Entity, including named men-
tions, nominal and pronominal references, while 
measuring the NMEG performance only requires 
co-reference information of named mentions of a 
NE, and this information is relatively easy to ob-
tain automatically even in the absence of gold 
standard annotations. We compute correlation 
between CONE B3, B3, CONE CEAF and CEAF 
scores for various CRR systems on various gold-
standard annotated datasets and show that the 
CONE B3 and B3 scores are highly correlated for 
all such combinations of CRR systems and data-
sets, as are CONE CEAF and CEAF scores, with 
a best-case correlation of 0.8. We produce esti-
mated gold standard annotations for the Enron 
email corpus, since no actual gold standard CRR 
annotations exist for it, and then use CONE B3 
and CONE CEAF with these estimated gold 
standard annotations to compare the performance 
of various NE-CRR systems on this corpus. No 
such comparison has been previously performed 
for the Enron corpus. 
We adopt the same terminology as in (Luo, 
2005): a mention refers to each individual phrase 
and an entity refers to the equivalence class or 
co-reference chain with several mentions. This 
allows us to note some differences between NE-
CRR and NP-CRR. NE-CRR involves indentify-
ing named entities and extracting their co-
referring mentions; equivalences classes without 
any NEs are not considered. NE-CRR is thus 
clearly a subset of NP-CRR, where all co-
referring mentions and equivalence classes are 
considered. However, we focus on NE-CRR be-
cause it is currently a more active research area 
than NP-CRR and a better fit for target applica-
tions such as text forensics and web mining, and 
also because it is more amenable to the automatic 
evaluation approach that we propose. 
The research questions that motivate our work 
are:  
(1) Is it possible to use only NER resources to 
evaluate NE-CRR systems? If so, how is this 
problem formulated?  
(2) How does one perform evaluation in a way 
that is accurate and automatic with least hu-
man intervention?  
(3) How does one perform evaluation on large 
unlabeled datasets?  
We show that our CONE metrics achieve good 
results and represent a promising first step to-
ward answering these questions.  
 
The rest of the paper is organized as follows. We 
present related work in the field of automatic 
evaluation methods for natural language 
processing tasks in Section 2. In Section 3, we 
give an overview of the standard metrics current-
ly used for evaluating co-reference resolution. 
We define our new metrics CONE B3 and CONE 
CEAF in Section 4. In section 5, we provide ex-
perimental results that illustrate the performance 
of CONE B3 and CONE CEAF compared to B3 
and CEAF respectively. In Section 6, we give an 
example of the application of CONE metrics by 
evaluating NE-CRR systems on an unlabeled 
dataset, and discuss possible drawbacks and ex-
tensions of these metrics. Finally, in section 7 we 
present our conclusions and ideas for future 
work.  
2 Related Work 
There has been a substantial amount of research 
devoted to automatic evaluation for natural lan-
guage processing, especially tasks involving lan-
guage generation. The BLEU score (Papineni et 
al., 2002) proposed for evaluating machine trans-
lation results is the best known example of this. 
It uses n-gram statistics between machine gener-
ated results and references. It inspired the 
ROUGE metric (Lin and Hovy, 2003) and other 
methods (Louis and Nenkova, 2009) to perform 
automatic evaluation of text summarization. Both 
these metrics have show strong correlation be-
tween automatic evaluation results and human 
judgments. The two metrics successfully reduce 
the need for human judgment and help speed up 
research by allowing large-scale evaluation. 
Another example is the alignment entropy (Per-
vouchine et al, 2009) for evaluating translitera-
tion alignment. It reduces the need for alignment 
gold standard and highly correlates with transli-
teration system performance. Thus it is able to 
137
serve as a good metric for transliteration align-
ment. We contrast our work with (Stoyanov et al, 
2009), who show that the co-reference resolution 
problem can be separated into different parts ac-
cording to the type of the mention. Some parts 
are relatively easy to solve. The resolver per-
forms equally well in each part across datasets. 
They use the statistics of mentions in different 
parts with test results on other datasets as a pre-
dictor for unseen datasets, and obtain promising 
results with good correlations. We approach the 
problem from a different perspective. In our 
work, we show the correlation between the 
scores on traditional metrics and scores on our 
CONE metrics, and show how to automatically 
estimate the gold standard required by CONE 
metrics. Thus our method is able to predict the 
co-reference resolution performance without 
gold standard at all. We base our new metrics on 
the standard B3 and CEAF metrics used for com-
puting CRR scores. (Vilian et al, 1995; Bagga 
and Baldwin, 1998; Luo, 2005). B3 and CEAF 
are believed to be more discriminative and inter-
pretable than earlier metrics and are widely 
adopted especially for machine-learning based 
approaches.  
 
3 Standard Metrics: B3 and CEAF 
We now provide an overview of the standard B3 
and CEAF metrics used to evaluate CRR sys-
tems. Both metrics assume that a CRR system 
produces a set of equivalence classes {O} and 
assigns each mention to only one class. Let Oi be 
the class to which the ith mention was assigned 
by the system. We also assume that we have a set 
of correct equivalence classes {G} (the gold 
standard). Let Gi be the gold standard class to 
which the ith mention should belong. Let Ni de-
note the number of mentions in Oi which are also 
in Gi ? the correct mentions. B
3 computes the 
presence rate of correct mentions in the same 
equivalent classes. The individual precision and 
recall score is defined as follows: 
|| i
i
i O
NP ?
 
|| i
i
i G
NR ?
 
Here |Oi| and |Gi| are the cardinalities of sets Oi 
and Gi.   
The final precision and recall scores are: 
?
?
?
n
i
ii PwP
1
 ?
?
?
n
i
ii RwR
1
 
Here, in the simplest case the weight wi is set to 
1/n, equal for all mentions. 
CEAF (Luo, 2005) produces the optimal 
matching between output classes and true classes 
first, with the constraint that one true class, Gi, 
can be mapped to at most one output class, say 
Of(i) and vice versa. This can be solved by the 
KM algorithm (Kuhn, 1955; Munkres, 1957) for 
maximum matching in a bipartite graph. CEAF 
then computes the precision and recall score as 
follows: 
?
?
?
i
i
i
ifi
O
M
P
)(,      
?
?
?
i
i
i
ifi
G
M
R
)(,  
jiji GOM ??,
 
We use the terms Mi,j from CEAF to re-write B
3, 
its formulas then reduce to: 
???? i j i
ji
i
i O
M
O
P
2
,1  
???? i j i
ji
i
i G
M
G
R
2
,1  
We can see that B3 simply iterates through all 
pairs of matchings instead of considering the one 
to one mappings as CEAF does. Thus, B3 com-
putes the weighted sum of the F-measures for 
each individual mention which helps alleviate the 
bias in the pure link-based F-measure, while 
CEAF computes the same as B3 but enforces at 
most one matched equivalence class for every 
class in the system output and gold standard out-
put. 
4 CONE B3 and CONE CEAF Metrics:  
We now formally define the new CONE B3 and 
CONE CEAF metrics that we propose for 
automatic evaluation of NE-CRR systems. 
      Let G denote the set of gold standard 
annotations and O denote the output of an NE-
CRR system. Let Gi denote the equivalent class 
of entity i in the gold standard and Oj denote the 
equivalence class for entity j in the system output.  
Also let Gij denote the j
th mention in the 
equivalence class of entity i in the gold standard 
and Oij denote the j
th mention in the system 
output. 
As described earlier, the standard B3 and CEAF 
metrics evaluate scores using G and O and can 
be thought of as functions of the form B3(G, O). 
and CEAF(G, O) respectively. Let us use 
Score(G, O) to collectively refer to both these 
138
functions. An equivalence class Gi in G may 
contain three types of mentions: named mentions 
gNMij, nominal mentions g
NO
ij, and pronominal 
mentions gPRij. Similarly, we can define o
NM
ij, 
oNOij and o
PR
ij for a class Oi in O. Now for each 
gold standard equivalence class Gi and system 
output equivalence class Oi, we define the 
following sets GNMi  and  O
NM
i: 
iijNMijNMiNM GggGi ??? },{,  
iijNMijNMiNM OooOi ??? },{,  
In other words, GNMi and O
NM
i are the subsets of 
Gi and Oi containing all named mentions and no 
mentions of any other type.  
Let GNM denote the set of all such equivalance 
classes GNMi and O
NM denote the set of all 
equivalence classes ONMi. It is clear that G
NM and 
ONM are pruned versions of the gold standard 
annotations and system output respectively. 
We now define CONE B3 and CONE CEAF as 
follows: 
CONE B3 = B3(GNM, ONM) 
CONE CEAF = CEAF(GNM, ONM) 
 
Following our previous notation, we denote 
CONE B3 and CONE CEAF collectively as 
Score(GNM, ONM). We observe that Score(GNM, 
ONM) measures a NE-CRR system?s  
performance for the NE-CRR subtask of named 
mentions extraction and grouping (NMEG). We 
find that Score(GNM, ONM) is highly correlated 
with Score(G, O) for all the freely available NE-
CRR systems over various datasets. This 
provides the neccessary  justification for the use 
of Score(GNM, ONM).  
We use SYNERGY (Shah et al, 2010), an 
ensemble NER system that combines the UIUC 
NER (Ritanov and Roth, 2009) and Stanford 
NER (Finkel et al, 2005) systems, to produce 
GNM and ONM from G and O by  selecting named 
mentions. However, any other good NER system 
would serve the same purpose. 
We see that while standard evaluation metrics 
require the use of G, i.e. the full set of NE-CRR 
gold standard annotations including named, 
nominal and pronimal mentions, CONE metrics 
require only GNM, i.e. gold standard annotations 
consisting of named mentions only. The key 
advantage of using CONE metrics is that GNM 
can be automatically approximated using an 
NER system with a good degree of accuracy. 
This is because state-of-the-art NER systems 
achieve near-optimal performance, exceeding F1 
= 0.95 in many cases, and after obtaining their 
output, the task of estimating GNM reduces to 
simply clustering it to seperate mentions of 
diffrerent real-world entities. This clustering can 
be thought of as a form of named entity matching, 
which is not a very hard problem. There exist 
systems that perform such matching in a 
sophisticated manner with a high degree of 
accuracy. We use simple heuristics such as exact 
matching, word matches, matches between in-
itials, etc. to design such a matching system 
ourselves and use it to obtain estimates of GNM, 
say GNM-approx. We then calculate CONE B3 and 
CONE CEAF scores using GNM-approx instead of 
GNM; in other words, we perform fully automatic 
evaluation of NE-CRR systems by using 
Score(GNM-approx, ONM) instead of Score(GNM, 
ONM). In order to show the validity of this 
evaluation, we calculate the correlation between 
the Score(GNM-approx, ONM) and Score(G, O) for  
different NE-CRR systems across different 
datasets and find that they are indeed correlated. 
CONE thus makes automatic evaluation of NE-
CRR systems possible. By leveraging the widely 
available named entity resources, it reduces the 
need for gold standard annotations in the 
evaluation process. 
4.1 Analysis 
There are two major kinds of errors that affect 
the performance of NE-CRR systems for the full 
NE-CRR task: 
? Missing Named Entity (MNE): If a named 
mention is missing from the system output, 
it is very likely that its nearby nominal and 
anaphoric mentions will be lost, too 
? Incorrectly grouped Named Entity (IGNE): 
Even if the named mention is correctly iden-
tified with its nearby nominal and anaphoric 
mentions to form a chain, it is still possible 
to misclassify the named mentions and its 
co-reference chain 
Consider the following example of these two 
types of errors. Here, the alphabets represent the 
named mentions and numbers represent other 
type of mentions: 
 
Gold standard, G: (A, B, C, 1, 2, 3, 4) 
Output from System 1, O1: (A, B, 1, 2, 3) 
Output from System 2, O2: (A, C, 1, 2, 4), (B, 3) 
O1 shows an example of an MNE error, while 
O2 shows an example of an IGNE error.  
 
Both these types of errors are in fact rooted in 
named mention extraction and grouping 
(NMEG). Therefore, we hypothesize that they 
must be preserved in a NE-CRR system?s output 
139
for the subtask of named mentions extraction and 
grouping (NMEG) and will be reflected in the 
CONE B3 and CONE CEAF metrics that eva-
luate scores for this subtask. Consider the follow-
ing extension of the previous example:  
 
GNM: (A, B, C) 
O1NM: (A, B) 
O2NM: (A, C), (B) 
 
We observe that the MNE error in O1 is pre-
served in O1NM, and the IGNE error in O2 is pre-
served in O2NM. Empirically we sample several 
output files in our experiments and observe the 
same phenomena. Therefore, we argue that it is 
possible to capture the two major kinds of errors 
described by considering only GNM and ONM in-
stead of G and O.  
 
We now provide a more detailed theoretical 
analysis of the CONE metrics. For a given NE-
CRR system and dataset, consider the system 
output O and gold standard annotation G. Let P 
and R indicate precision and recall scores ob-
tained by evaluating O against G, using CEAF. If 
we replace both G and O with their subsets GNM 
and ONM respectively, such that GNM and ONM 
contain only named mentions, we can modify the 
equations for precision and recall for CEAF to 
derive the following equations for precision PNM 
and recall RNM for CONE CEAF: 
??
i
iNMOOSum NM }{
     
??
i
iNMNM GGSum }{
 
??
i
NM
ifi
NM
NM
OSum
M
P }{
)(,     
??
i
NM
ifi
NM
NM
GSum
M
R }{
)(, 
 
The corresponding equations for CONE B3 Pre-
cision are: 
?
?
?
?
i
NM
i
NM
j
ji
NM
NM
OSumO
M
P
}{
2
,
?
?
?
?
i
NM
i
NM
j
ji
NM
RSumR
M
R
}{
'
2
,
 
 
In order to support the hypothesis that CONE 
metrics evaluated using (GNM, ONM) represent an 
effective substitute for standard metrics that use 
(G, O), we compute entity level correlation be-
tween the corresponding CONE and standard 
metrics. For example, in the case of CEAF / 
CONE CEAF Precision, we calculate correlation 
between the following quantities: 
??? }{
)(,
NM
ifi
NM
NM
SSum
M
P?
 and 
??? }{
)(,
SSum
M
P ifi?  
We perform this experiment with the LBJ and 
BART CRR systems on the ACE Phase 2 corpus. 
We illustrate the correlation results in Figure 1.  
 
Figure 1. Correlation between NMP? andP?  - 
Entity Level CEAF Precision 
From Figure 1, we can see that the two 
measures are highly correlated. In fact, we find 
that the Pearson?s correlation coefficient (Soper 
et al, 1917; Cohen, 1988) is 0.73. The points 
lining up on the x-axis and y=1.0 represent very 
small equivalence classes and are a form of noise; 
their removal doesn?t affect this coefficient. To 
show that this strong correlation is not a 
statistical anomaly, we also compute entity-level 
correlation using (Gi - G
NM
i, Oj - O
NM
j) and (Gi, 
Oj) instead of (G
NM
i, O
NM
j) and (Gi, Oj) and find 
that the coefficient drops to 0.03, which is 
obviously not correlated at all.  
We now know NMP? andP?  are highly correlated. 
Assume the correlation is linear, with the 
following equation: 
?? ?? iNMi PP  
where ? and ? are the linear regression 
parameters. 
Thus 
? ? ???? nPnPPP NM
i
iNM
i i
????? ??
   
Here, n is the number of equivalence classes.    
We conclude that the overall CEAF Precision 
and CONE CEAF Precision should be highly 
140
correlated too. We repeat this experiment with 
CEAF / CONE CEAF Recall, B3 / CONE B3 
Precision and B3 / CONE B3 Recall and obtain 
similar results, allowing us to conclude that these 
sets of measures should also be highly correlated. 
We note here some generally accepted 
terminology regarding correlation: If two 
quantities have a Pearson?s correlation 
coefficient greater than 0.7, they are considered  
"strongly correlated", if their correlation is 
between 0.5 and 0.7, they are considered "highly 
correlated", if it is between 0.3 and 0.5, they are 
considered "correlated", and otherwise they are 
considered "not correlated".  
It is important to note that like all automatic 
evaluation metrics, CONE B3 and CONE CEAF 
too can be easily ?cheated?, e.g. a NE-CRR sys-
tem that performs NER and named entity match-
ing well but does not even detect and classify 
anaphora or nominal mentions would nonethe-
less score highly on these metrics. A possible 
solution to this problem would be to create gold 
standard annotations for a small subset of the 
data, call these annotations G?, and report two 
scores: B3 / CEAF (G?), and CONE B3 / CONE 
CEAF (GNM-approx). Discrepancies between these 
two scores would enable the detection of such 
?cheating?. A related point is that designers of 
NE-CRR systems should not optimize for CONE 
metrics alone, since by using GNM-approx (or GNM 
where gold standard annotations are available), 
these metrics are obviously biased towards 
named mentions. This issue can also be ad-
dressed by having gold standard annotations G? 
for a small subset. One could then train a system 
by optimizing both B3 / CEAF (G?) and CONE 
B3 / CONE CEAF (GNM-approx). This can be 
thought of as a form of semi-supervised learning, 
and may be useful in areas such as domain adap-
tation, where we could use some annotated test-
set in a standard domain, e.g. newswire as the 
smaller set and an unlabeled large testset from 
some other domain, such as e-mail or biomedical 
documents. An interesting future direction is to 
monitor the effectiveness of our metrics over 
time. As co-reference resolution systems evolve 
in strength, our metrics might be less effective, 
however this could be a good indicator to discri-
minate on different subtasks the improvements 
gained by the co-reference resolution systems. 
5 Experimental Results 
We present experimental results in support of the 
validity and effectiveness of CONE metrics. As 
mentioned earlier, we used the following four 
publicly available CRR systems: UIUC?s LBJ 
system (L), BART from JHU Summer Workshop 
(B), LingPipe from Alias-i (LP), and OpenNLP 
(OP) (Bengston and Roth, 2007; Versley et al, 
2008; Baldridge and Torton, 2004; Baldwin and 
Carpenter, 2003). All these CRR systems per-
form Noun Phrase co-reference resolution (NP-
CRR), not NE-CRR. So, we must first eliminate 
all equivalences classes that do not contain any 
named mentions. We do so using the SYNERGY 
NER system to separate named mentions from 
unnamed ones. Note that this must not be con-
fused with the use of SYNERGY to produce GNM 
and ONM from G and O respectively. For that task, 
all equivalence classes in G and O already con-
tain at least one named mention and we remove 
all unnamed mentions from each class. This 
process effectively converts the NP-CRR results 
of these systems into NE-CRR ones. We use the 
ACE Phase 2 NWIRE and ACE 2005 English 
datasets. We avoid using the ACE 2004 and 
MUC6 datasets because the UIUC LBJ system 
was trained on ACE 2004 (Bengston and Roth, 
2008), while BART and LingPipe were trained 
on MUC6. There are 29 files in the test set of 
ACE Phrase 2 and 81 files in ACE 2005, sum-
ming up to 120 files with around 50,000 tokens 
with 5000 valid co-reference mentions. Tables 1 
and 2 show the Pearson?s correlation coefficients  
between CONE metric scores of the type 
Score(GNM, ONM) and standard metric scores of 
the type Score(G, O) for combinations of various 
CRR systems and datasets.  
  B3/CONE B3  CEAF/CONE CEAF 
  P R F1 P R F1 
L 0.82 0.71 0.7 0.81 0.71 0.77 
B 0.85 0.5 0.66 0.71 0.61 0.68 
LP 0.84 0.66 0.67 0.74 0.71 0.73 
OP 0.31 0.57 0.61 0.79 0.72 0.79 
Table 1. GNM: Correlation on ACE Phase 2 
  B3/CONE B3  CEAF/CONE CEAF 
  P R F1 P R F1 
L 0.6 0.62 0.62 0.75 0.61 0.68 
B 0.74 0.82 0.84 0.72 0.68 0.67 
LP 0.91 0.65 0.73 0.44 0.57 0.53 
OP 0.48 0.77 0.8 0.54 0.67 0.65 
Table 2. GNM: Correlation on ACE 2005 
 
We observe from Tables 1 and 2 that CONE B3 
and CONE CEAF scores are highly correlated 
141
with B3 and CEAF scores respectively, and this 
holds true for Precision, Recall and F1 scores, for 
all combinations of CRR systems and datasets. 
This justifies our assumption that a system?s per-
formance for the subtask of NMEG is a good 
predictor of its performance for the full task of 
NE-CRR. These correlation coefficients are 
graphically illustrated in Figures 2 and 3. 
We now use our baseline named entity matching 
method to automatically generate estimated gold 
standard annotations GNM-approx and recalculate 
CONE CEAF and CONE B3 scores using GNM-
approx instead of GNM. Tables 3 and 4 show the 
correlation coefficients between the new CONE 
scores and the standard metric scores. 
 
  B3/CONE B3  CEAF/CONE CEAF 
  P R F1 P R F1 
L 0.31 0.23 0.22 0.33 0.55 0.56 
B 0.71 0.44 0.43 0.61 0.63 0.71 
LP 0.57 0.43 0.49 0.36 0.25 0.31 
OP 0.1 0.6 0.64 0.35 0.53 0.53 
Table 3. GNM-approx: Correlation on ACE Phase 2  
  B3/CONE B3  CEAF/CONE CEAF 
  P R F1 P R F1 
L 0.33 0.32 0.42 0.22 0.34 0.36 
B 0.25 0.66 0.65 0.2 0.45 0.37 
LP 0.19 0.33 0.34 0.77 0.68 0.72 
OP 0.26 0.66 0.67 0.28 0.42 0.38 
Table 4. GNM-approx: Correlation on ACE Phase 2 
We observe from Tables 3 and 4 that these corre-
lation factors are encouraging, but not as good as 
those in Tables 1 and 2. All the corresponding 
CONE B3 and CONE CEAF scores are corre-
lated, but very few are highly correlated. We 
should note however that our baseline system to 
create GNM-approx uses relatively simple clustering 
methods and heuristics. It is easy to observe that 
a sophisticated named entity matching system 
would produce a GNM-approx that better approx-
imates GNM than our baseline method, and CONE 
B3 and CONE CEAF scores calculated using this 
GNM-approx would be more correlated with stan-
dard B3 and CEAF scores.  
We note from the above results that correlations 
scores are very similar across different systems 
and datasets. In order to formalize this assertion, 
we calculate correlation scores in a system-
independent and data-independent manner. We 
combine all the data points across all four differ-
ent systems and plot them in Figure 2 and 3 for 
ACE Phase 2 NWIRE corpus and in Figure 4 and 
5 for ACE 2005 corpus respectively. We illu-
strate only F1 scores; the results for precision 
and recall are similar. 
 
Figure 2. Correlation between B3 F1 and CONE 
B3 F1 for all systems on ACE 2 
 
Figure 3. Correlation between CEAF F1 and 
CONE CEAF F1 for all systems on ACE 2 
 
Figure 2 reflects a Pearson?s correlation coeffi-
cient of 0.70, suggesting that all the B3 F1 and 
CONE B3 F1 scores for different systems are 
highly correlated and that CONE B3 F1 does not 
bias towards any particular system. Figure 3 re-
flects a Pearson?s correlation coefficient of 0.83, 
providing similar evidence for the system-
independence of correlation between CEAF F1 
and CONE CEAF F1 scores. Figures 4 and 5 
corresponding to ACE 2005 reflect similar corre-
lation coefficients of 0.89 and 0.82, and thus 
support the idea that the correlations between B3 
F1 and CONE B3 F1, as well as between CEAF 
F1and CONE CEAF F1, are dataset-independent 
in addition to being system-independent.  
 
142
 
Figure 4. Correlation between B3 F1 and CONE 
B3 F1 for all systems on ACE 2005 
 
 
Figure 5. Correlation between CEAF F1 and 
CONE CEAF F1 for all systems on ACE 2005 
6 Application and Discussion  
To illustrate the applicability of CONE metrics, 
we consider the Enron e-mail corpus. It is of a 
different genre than the newswire corpora that 
CRR systems are usually trained on, and no CRR 
gold standard annotations exist for it. Conse-
quently, no CRR systems have been evaluated on 
it so far. We used CONE B3 and CONE CEAF to 
evaluate and compare the NE-CRR performance 
of various CRR systems on a subset of the Enron 
e-mail corpus (Klimt and Yang, 2004) that was 
cleaned and stripped of spam messages. We re-
port the results in Table 5. 
 
  CONE B3  CONE CEAF 
  P R F1 P R F1 
L 0.43 0.21 0.23 0.31 0.17 0.21 
B 0.26 0.18 0.2 0.26 0.16 0.2 
LP 0.61 0.51 0.53 0.58 0.53 0.54 
OP 0.19 0.03 0.05 0.11 0.02 0.04 
Table 5. GNM-approx Scores on Enron corpus 
 
We find that LingPipe is the best of all the sys-
tems we considered, and LBJ is slightly ahead of 
BART in all measures. We suspect that since 
LingPipe is a commercial system, it may have 
extra training resources in the form of non-
traditional corpora. Nevertheless, we believe our 
method is robust and scalable for large corpora 
without NE-CRR gold standard annotations. 
 
7 Conclusion and Future Work 
We propose the CONE B3 and CONE CEAF me-
trics for automatic evaluation of Named Entity 
Co-reference Resolution (NE-CRR). These me-
trics measures a NE-CRR system?s performance 
on the subtask of named mentions extraction and 
grouping (NMEG) and use it to estimate the sys-
tem?s performance on the full task of NE-CRR. 
We show that CONE B3 and CONE CEAF 
scores of various systems across different data-
sets are strongly correlated with their standard B3 
and CEAF scores respectively. The advantage of 
CONE metrics compared to standard ones is that 
instead of the full gold standard data G, they only 
require a subset GNM of named mentions which 
even if not available can be closely approximated 
by using a state-of-the-art NER system and clus-
tering its results. Although we use a simple base-
line algorithm for producing the approximate 
gold standard GNM-approx, CONE B3 and CONE 
CEAF scores of various systems obtained using 
this GNM-approx still prove to be correlated with 
their standard B3 and CEAF scores obtained us-
ing the full gold standard G. CONE metrics thus 
reduce the need of expensive labeled corpora. 
We use CONE B3 and CONE CEAF to evaluate 
the NE-CRR performance of various CRR sys-
tems on a subset of the Enron email corpus, for 
which no gold standard annotations exist and no 
such evaluations have been performed so far. In 
the future, we intend to use more sophisticated 
named entity matching schemes to produce better 
approximate gold standards GNM-approx. We also 
intend to use the CONE metrics to evaluate NE-
CRR systems on new datasets in domains such as 
chat, email, biomedical literature, etc. where very 
few corpora with gold standard annotations exist. 
 
Acknowledgments 
We would like to thank Prof. Ani Nenkova from 
the University of Pennsylvania for her talk about 
automatic evaluation for text summarization at 
the spring 2010 CMU LTI Colloquium and ano-
nymous reviewers for insightful comments.  
143
References  
E. Agirre, L. M?rquez and R. Wicentowski, Eds. 
2007. Proceedings of the Fourth International 
Workshop on Semantic Evaluations (SemEval).   
A. Bagga and B. Baldwin. 1998. Algorithms for Scor-
ing Coreference Chains. Proceedings of LREC 
Workshop on Linguistic Coreference. 
J. Baldridge and T. Morton. 2004. OpenNLP. 
http://opennlp.sourceforge.net/. 
B. Baldwin and B. Carpenter. 2003. LingPipe. Alias-i. 
E. Bengtson and D. Roth. 2008. Understanding the 
Value of Features for Coreference Resolution. Pro-
ceedings of EMNLP. 
J. Cohen. 1988. Statistical power analysis for the be-
havioral sciences. (2nd ed.) 
A.K. Elmagarmid, P.G. Ipeirotis and V.S. Verykios. 
2007. Duplicate Record Detection: A Survey. IEEE 
Transactions on Knowledge and Data Engineering, 
v.19 n.1, 2007.   
J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating Non-local Information into Informa-
tion Extraction Systems by Gibbs Sampling. Pro-
ceedings of ACL. 
B. Klimt and Y. Yang. 2004. The Enron corpus: A 
new dataset for email classification research. Pro-
ceedings of ECML. 
H.W. Kuhn. 1955. The Hungarian method for the 
assignment problem. Naval Research Logistics 
Quarterly, 2(83). 
C. Lin and E. Hovy. 2003. Automatic evaluation of 
summaries using n-gram co-occurrence statistics. 
Proceedings of HLT-NAACL.    
C. Lin and F.J. Och. 2004. Automatic evaluation of 
machine translation quality using longest common 
subsequence and skip-bigram statistics. Proceed-
ings of ACL.  
A. Louis and A. Nenkova. 2009. Automatically Eva-
luating Content Selection in Summarization with-
out Human Models. Proceedings of EMNLP, pages 
306?314, Singapore, 6-7 August 2009. 
X. Luo. 2005. On coreference resolution performance 
metrics. Proceedings of EMNLP. 
MUC-6. 1995. Proceedings of the Sixth Understand-
ing Conference (MUC-6). 
J. Munkres. 1957. Algorithms for the assignment and 
transportation problems. Journal of SIAM, 5:32-38. 
NIST. 2003. The ACE evaluation plan. 
www.nist.gov/speech/tests/ace/index.htm. 
K Papineni, S Roukos, T Ward and W.J. Zhu. 2002. 
BLEU: a method for automatic evaluation of ma-
chine translation. Proceedings of ACL. 
V. Pervouchine, H. Li and B. Lin. 2009. Translitera-
tion alignment. Proceedings of ACL. 
L. Ratinov and D. Roth. 2009. Design Challenges and 
Misconceptions in Named Entity Recognition. 
Proceedings of CoNLL. 
R. Shah, B. Lin, A. Gershman and R. Frederking. 
2010. SYNERGY: a named entity recognition sys-
tem for resource-scarce languages such as Swahili 
using online machine translation. Proceedings of 
LREC Workshop on African Language Technology. 
H.E. Soper, A.W. Young, B.M. Cave, A. Lee and K. 
Pearson. 1917. On the distribution of the correla-
tion coefficient in small samples. Appendix II to 
the papers of "Student" and R. A. Fisher. A co-
operative study. Biometrika, 11, 328-413. 
V. Stoyanov, N. Gilbert, C. Cardie and E. Riloff. 
2009. Conundrums in Noun Phrase Coreference 
Resolution: Making Sense of the State-of-the-Art. 
Proceedings of ACL. 
Y. Versley, S.P. Ponzetto, M. Poesio, V. Eidelman, A. 
Jern, J. Smith, X. Yang and A. Moschitti. 2008. 
BART: A Modular Toolkit for Coreference Reso-
lution. Proceedings of EMNLP. 
M. Vilain, J. Burger, J. Aberdeen, D. Connolly and L. 
Hirschman. 1995. A model-theoretic coreference 
scoring scheme. Proceedings of MUC 6. 
 
 
144
