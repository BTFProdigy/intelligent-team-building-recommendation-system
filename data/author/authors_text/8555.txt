22
23
24
25
26
27
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 13?16,
Suntec, Singapore, 3 August 2009.
c?2009 ACL and AFNLP
ProLiV - a Tool for Teaching by Viewing Computational Linguistics
Monica Gavrila
Hamburg University, NATS
Vogt-K?olln Str 30, 20251, Germany
gavrila@informatik.
uni-hamburg.de
Cristina Vertan
Hamburg University, NATS
Vogt-K?olln Str 30, 20251, Germany
vertan@informatik.
uni-hamburg.de
Abstract
ProLiV - Animated Process-modeler of
Complex (Computational) Linguistic
Methods and Theories - is a fully modular,
flexible, XML-based stand-alone Java
application, used for computer-assisted
learning in Natural Language Processing
(NLP) or Computational Linguistics (CL).
Having a flexible and extendible architec-
ture, the system presents the students, by
means of text, of visual elements (such as
pictures and animations) and of interactive
parameter set-up, the following topics:
Latent Semantics Analysis (LSA), (com-
putational) lexicons, question modeling,
Hidden-Markov-Models (HMM), and
Topic-Focus. These topics are addressed
to first-year students in computer science
and/or linguistics.
1 Introduction
The role of multimedia in teaching Natural
Language Processing (NLP) is demonstrated
by constant development of software packages
such as GATE (http://gate.ac.uk) and
NLTK (http://nltk.sourceforge.net/
index.html). Detailed information about vi-
sual tools for NLP, in particular about GATE, is
to be found in (Gaizauskas et al 2001).
ProLiV is a Java application framework, devel-
oped in a three-year project (2005-2008) at the
University of Hamburg. It helps first-year stu-
dents to understand and learn, in an easier man-
ner, either complex linguistic theories used in NLP
(e.g. question modeling) or statistical approaches
for computational linguistics (e.g. LSA, HMM).
The learning process is supported by modules
integrating text, visual and interactive elements. In
its first released version, ProLiV contains the fol-
lowing modules:
? the Latent Semantic Analysis (LSA) module
and the computational lexicons module - for
linguists,
? the question modeling module - for computer
scientists,
? the Hidden-Markov-Models (HMM) module
and Topic-Focus module - for both computer
scientists and linguists.
2 The Learning Path
For each module, the learning path is guided by
lessons, a terminology dictionary and interactive
activities. Exercises and small tests can also be
integrated.
The lessons include text, pictures and ani-
mations. Hyperlinks between lessons ensure a
concept-oriented navigation through the learning
content. Additionally key terms within the content
are linked with dictionary entries.
Three central issues guided the development of
the ProLiV software:
1. choosing the most adequate means (text / pic-
ture / animation) to represent lessons content,
2. designing the layout (quantity and size of
text, colors) in order to increase the learning
success,
3. in case of the animations, defining its com-
ponents and parameters (speed, animation
steps, and graphical elements) to maximize
their impact on users.
Regarding the second issue above-mentioned,
the layout of the modules follows part of the
guidelines found in (Orr et al, 1994) and (Thi-
bodeau, 1997).
Considering the current multimedia develop-
ment, the trend is using animations to improve the
learning process. Animations are assumed to be
13
a promising educational tool, although their effi-
ciency is not fully proved. Researchers, such as
(Morrison, 2000), showed that animations can
convey more information and be helpful when
showing details in intermediate steps of a process,
but when building an animation it is very impor-
tant to consider the background of the student (e.g.
linguistics, natural sciences) and his/her psycho-
logical functioning. The educational effectiveness
of the animations depends on how they interact
with the learner. Depending on the student?s back-
ground, in order to have a helpful material, one
has to carefully decide what information the ani-
mation contains. As our experiment showed (see
Section 2.1), depending on the student and his/her
background, an animation can improve the learn-
ing process, or bring nothing to it. We found no
cases when the animation slowed down the learn-
ing process.
The system was experimentally used in semi-
nars at the University of Hamburg. Part of the
lessons content was adapted following the user?s
feedback.
2.1 Animations in ProLiV
Animations are not integrated in all modules of
the ProLiV system, but only in the LSA, computa-
tional lexicons and question modeling modules.
In order to decide how to organize the informa-
tion in an animation, we evaluated the animations
for the matrix multiplication in the LSA module
by asking 11 high-school pupils (between 16 and
19 years old) to choose between the several repre-
sentations.
We showed the pupils three animations that de-
scribe the multiplication of matrices, a static pic-
ture and the text representation of the definition.
The animations differ in the way the process is
presented (abstract vs. concrete) and in user in-
teraction authorization.
The pupils were asked to evaluate all the rep-
resentations. The question they had to answer
was: ?Which of the following representations
helps more, when learning about matrix multipli-
cation??. The scale given was from 1 = very help-
ful to 5 = not helpful at all.
Analyzing the results, we could not conclude
that one representation is a ?real winner?. The
best representation was considered the most flex-
ible animation, that allows the student go back-
wards and forwards whenever the user needs it,
Representation Average Result
Definition (formula) 3.5
Picture 2.91
Animation 1 3.64
Animation 2 2.09
Animation 3 2.45
Table 1: Evaluation of the animations in the ma-
trix multiplication (Animations 1 and 3 have no
user interaction; Animations 1 and 2 are more ab-
stract)
the learning process being adapted to the user?s
rhythm. All the evaluation results can be seen in
Table 1. In order to better see the influence of these
representations in the learning process, statistical
tests should be run.
3 System Architecture
In Figure 1 we present the ProLiV System archi-
tecture, consisting of:
? a file repository (lessons, dictionary, tests,
and exercises),
? a tool repository,
? an aggregating module combining elements
from file and tool repository (Main Unit),
? the graphical user interface (G.U.I.)
For each topic a stand-alone module is con-
nected with the G.U.I module via the Main Unit.
Modules related to new topics can be inserted any
time with no particular changes of the system.
The ProLiV architecture follows the guideline
considerations found in (Galitz, 1997).
Figure 1: The ProLiV Architecture
14
The flexibility of the system is also given by the
fact that the G.U.I.
1
is generated according to an
XML
2
description, developed within the project
(see DTD Description).
The XML description contains the information
in the lessons (definitions, theory, examples, etc.)
and the G.U.I. specifications (colors, fonts, links,
arrangement in the interface, etc.). Having an
XML file as input, the system generates automat-
ically the G.U.I. presented to the student. The in-
formation shown to the user can be extended or
modified with almost no implementation effort.
New lessons or modules can be integrated, by ex-
tending or adding XML files. Due to the same fact,
also the content adaptation of the system to other
languages
3
is very easy.
The DTD Description:
<?xml version=??1.0???>
<DOCTYPE LESSONS[
<!ELEMENT LESSONS (LESSON+)>
<!ELEMENT LESSON (TITLE+, (TEXT|FORMULA|
INDEXI|INDEX|BOLD|
ITALIC|TERM|LINK|DEF|
EXM|OBS|T|OTHER)+>
<!ELEMENT TITLE (#PCDATA)>
<!ELEMENT TEXT (#PCDATA)>
<!ELEMENT FORMULA (#PCDATA)>
<!ELEMENT INDEX (#PCDATA)>
<!ELEMENT INDEXI (#PCDATA)>
<!ELEMENT BOLD (#PCDATA)>
<!ELEMENT ITALIC (#PCDATA)>
<!ELEMENT TERM (#PCDATA)>
........................
<!ELEMENT T (#PCDATA)>
<!ELEMENT OTHER (#PCDATA)>
<!ATTLIST LESSON NO CDATA #REQUIRED>
<!ATTLIST DEF NO CDATA #REQUIRED>
<!ATTLIST EXM NO CDATA #REQUIRED>
<!ATTLIST OBS NO CDATA #REQUIRED>
<!ATTLIST QUIZZ NO CDATA #REQUIRED>
<!ATTLIST EX NO CDATA #REQUIRED>
<!ATTLIST T NO CDATA #REQUIRED>
<!ATTLIST OTHER STYLE CDATA #REQUIRED>
The G.U.I. follows the same design rules in all
modules and the layout and format decisions are
consistent. A color and a font style are associated
to only one kind of information (e.g. color red as-
sociated to definitions, etc.).
1
The G.U.I. is automatically generated not only for the
lessons, but also for the term dictionary associated to each
module.
2
XML = Extensible Markup Language. More details to
be found on http://en.wikipedia.org/wiki/XML
3
For the moment ProLiV contains lessons in German and
English
3.1 Integrated external software packages
The learning process is also sustained by in-
teractive elements, such as the possibility of
changing parameters for the LSA algorithm
and visualizing the results, or as the inte-
grated programs for the computational lexicons
tool: ManageLex (http://nats-www.
informatik.uni-hamburg.de/view/
Main/ManageLex) and G.E.R.L. (http://
nats-www.informatik.uni-hamburg.
de/view/Main/GerLexicon). This way
the students have the possibility, not only to read
the theory, but also to see the impact of their
modifications in an algorithm that is described in
the lessons.
Due to its architecture, other such external pro-
grams can be easily integrated within ProLiV.
4 LSA Module in ProLiV
In order to have a better overview of what a mod-
ule contains and how it is organized, this section
presents some aspects of the LSA module.
The LSA module makes an introduction to the
topic. It gives an overview of the LSA algo-
rithm, principles, application areas, and of the
main mathematical notions used in the algorithm.
Initially thought for being used mostly by students
from linguistics (or linguists) - due to the mathe-
matical algorithms -, the tool can be exploited by
anybody who wants to have an introductory course
on LSA.
The content is organized in four Units:
1. LSA: General Knowledge - It gives the LSA
definition, a short overview of the history, its
semantics, and how LSA can be used in the
study of cognitive processes.
2. Mathematical Fundamentals - It describes the
LSA algorithm
3. LSA Applications - It presents the applica-
tion areas for the LSA, LSA limitations and
critics. Also a comparison with other similar
algorithms is made.
4. Compendium of Mathematics - It gives the
user the mathematical background: defini-
tions, theorems, etc.
The course has also an introduction, a motivation,
conclusion and references.
15
The LSA module is offering not only a textual
representation of the information, but also sev-
eral visualization methods (as images and anima-
tions
4
). Beside the lessons, there are implemented
a term dictionary and an environment for testing
LSA parameters.
4.1 The LSA Test Environment
Probably the most interesting part of the LSA
module is the test environment. After learning
about LSA, in this environment the user has the
possibility to actually see how LSA is working,
and what results can be obtained when compar-
ing the meaning of two words. The user can set
several parameters of the algorithm - e.g. the
analysis mode (simple/frequency based vs. ad-
vanced/entropy based), the minimum word occur-
rences, the analysis dimension, the similarity mea-
sure (Cosine, Euclidean, Pearson, Dot-Product),
etc. - and decide which words are not considered
in the analysis. The analyzed text, the initial co-
occurrence matrix and the one obtained after ap-
plying the Singular Value Decomposition (SVD)
algorithm are shown in the G.U.I. The similarity
measure, when comparing two words, is calcu-
lated in both unreduced and reduced cases.
5 Conclusions
The paper presents a course-ware software, Pro-
LiV. It is a collection of (interactive) multimedia
tools used mainly for the consolidation of first-
years courses in computational linguistics and lit-
erary computing. Its goal is to help the humanist
scientists to make use of complex formal methods,
and the computer specialists to understand human-
ist facts and interpretations.
The main feature of the system, in the context
of the conference, is not the content of the lessons,
but the system?s extendible and adaptable architec-
ture. Another important aspect is the way in which
the information is presented to the student.
The system runs on any platform supporting
Java 1.5 or newer. It was developed on Linux and
tested on Windows and Mac OS X.
Being Java-based and having as input Unicode
files (XML encoded information), the system can
be embedded in the future in a Web environment.
More about ProLiV can be found in (Gavrila
et al 2006) or in (Gavrila et al TBA) and on
4
The animations integrated are for the LSA algorithm
tested on an example and for matrix multiplication
the ProLiV homepage: http://nats-www.
informatik.uni-hamburg.de/view/
PROLIV/WebHome.
Acknowledgments
We would like to thank all people that helped in
the development of our software: Project Coor-
dinator Prof. Dr. Walther v. Hahn (Computer
Science Department, Natural Language Systems
Group), Prof. Dr. Angelika Redder (Depart-
ment of Language, Linguistics and Media Stud-
ies, Institute for German Studies I), Dr. Shinichi
Kameyama (Department of Language, Linguistics
and Media Studies, Institute for German Stud-
ies I), Christina von Bremen (Computer Science
Department, Natural Language Systems Group),
Olga Szczepanska (Computer Science Depart-
ment, Natural Language Systems Group), Irina
Aleksenko (Computer Science Department, Nat-
ural Language Systems Group), Svetla Boytcheva
(Academy of Sciences Sofia).
References
Wilbert O. Galitz. 1997 The Essential Guide to User
Interface Design: an Introduction to GUI Design
principles and Techniques, Wiley Computer Pub-
lishing, New York.
Robert J. Gaizauskas, Peter J. Rodgers, and Kevin
Humphreys. 2001 Visual Tools for Natural Lan-
guage Processing, Journal of Visual Languages and
Computing, Vol. 12, Number 4, p. 375-411, Aca-
demic Press
Monica Gavrila, Cristina Vertan. 2006 Visualization
of Complex Linguistic Theories, in the Proceed-
ings of the ICDML 2006 Conference, p. 158-163,
Bangkok, Thailand, March 13-14
Monica Gavrila, Cristina Vertan, and Walther von
Hahn. To be published during 2009 ProLiV - Learn-
ing Terminology with animated Models for Visualiz-
ing Complex Linguistics Theories, in the Proceed-
ings of the LSP 2007 Conference, Hamburg, Ger-
many, August,
Julie Bauer Morrison, Barbara Twersky, and Mireille
Betrancourt. 2000 Animation: Does It Facilitate
Learning?, in the Proc. of the Workshop on Smart
Graphics, AAAI Press, Menlo Park, CA.
Kay L .Orr, Katharine C. Golas, and Katy Yao. 1994
Storyboard Development for Interactive Multimedia
Training, Journal of Interactive Instruction Devel-
opment, Volume 6, Number 3, p. 18-31
Pete Thibodeau. 1997 Design Standards for Visual
Elements and Interactivity for Courseware, T.H.E.
Journal, Volume 24, Number 7, p. 84-86
16
Language Resources for the Semantic Web ? perspectives for Machine 
Translation ? 
Cristina VERTAN 
Natural Language Systems Division, University of Hamburg 
Vogt-K?lln Strasse 30 
22527 Hamburg, Germany 
cri@nats.informatik.uni-hamburg.de 
 
Abstract 
In this paper we present a possible solution for 
improving the quality of on-line translation 
systems, using mechanisms and standards from 
Semantic Web. We focus on Example based 
machine translation and the automatization of the 
translation examples extraction by means of RDF-
repositories. 
 
1. Introduction 
 
Machine Translation  (MT) was nominated on the 
first place among the 10 emerging technologies 
who will change the world (Technical Review 
2004). It is expected that with the increased 
number of official language in Europe, and the 
continuous growth of non-English Internet 
resources, machine translation systems will 
become an indispensable tool in everyday work. 
For the moment high-quality MT-systems are on 
one hand expensive and on the other hand domain 
oriented. The on-line existent tools produce poor-
quality translation, and very often offer a false 
image of current translation engines capabilities. 
The main reason why on-line machine translation 
tools offer so poor results is that they rely either 
on corpus-based methods trained on a limited 
number of examples or they infer rules from a 
limited linguistic knowledge base (Gaspari 2002). 
Following the statistics published in 
(McLaughlin and Schwall 1998) already in 1998 
there were at least 25 countries with more than 
500 000 Internet users, and in at least half of these 
countries English is neither the first nor the 
second spoken language. This statistic shows 
clearly that access to on-line information can be 
guaranteed only through high-quality on-line 
machine translation tools. However, an on-line 
translation system has a number of specific 
requirements (i.e. different from the ?traditional? 
ones): 
- It has to be fast but not always perfect. 
The translation of web-documents is more 
a kind of ?translation for assimilation? in 
the Carbonell?s classification  (Carbonell 
1994). However it has to go beyond the 
word-to-word quality offered by the 
actual on-line systems 
- A large number of languages / pair of 
languages have to be covered 
- The system has to be a ?fully integrated 
black box?. Most part of the users do not 
have the expertise to tune different 
parameters. 
 
There are different approaches to automatic 
translation, however not all of them are suited to 
be used for on-line translation. 
 
1. Rule-based MT systems are based on 
complex linguistic modules both in the 
analysis and generation phase 
(morphology, syntax, semantics, 
pragmatics). Such modules are developed 
for only few languages and they are not 
commercially ?free available. The 
implementation of such modules requires 
deep linguistic knowledge in both 
languages (especially for the transfer 
rules) 
2. Knowledge-based MT systems are  
strongly domain dependent and rely on 
domain-specific ontologies. Most part of 
the ontologies were developed previously 
only for commercial products, and 
therefore are not free available 
3. Corpus-based MT systems (example ?
based and statistical-based) are younger 
on the market, and provide good 
translation quality, especially for 
assimilation purposes. They are based on 
large parallel aligned corpora, or on 
translation databases. In the first case 
considerable amount of text is aligned 
usually at the paragraph level; in the latter 
translation chunks are collected (usually 
the chunks are sentences or even smaller 
units.) 
 
Most part of the currently existent on-line 
translation systems adopt a very simplistic rule-
based approach, i.e. the translation is reduced to 
dictionary look-up followed by a morphological 
processing, and very simple syntactic transfer 
rules.  
Within the Semantic Web activities it is 
assumed that a big amount of internet resources 
will be semantically annotated.  This opens new 
perspectives for the corpus-based MT Systems, 
and makes them a serious candidate for on-line 
translation. 
This paper is organised as follows: in section 
2 we present the main principles of semantic web. 
In section 3 we describe a type of MT-System 
who can benefit from the Semantic Web activities. 
and show how Semantic Web technologies can be 
used to improve the quality of on-line Machine 
Translation systems. In section 4 we present 
directions of future work. 
2. The Semantic Web 
Following the definition of Tim-Berners-Lee, 
?The Semantic Web will bring structure to the 
meaningful content of the web pages, creating an 
environment where software agents roaming from 
page to page can readily carry out sophisticated 
tasks for users?(Berners-Lee and Hendler and 
Lasilla 1999) 
The WWW, was developed for humans; 
the documents on the web are machine readable 
but not machine understandable.  The main aim of 
Semantic Web is to  enrich documents with 
semantic information about the content and to 
develop powerful mechanisms capable of 
interpreting this information. These goals are 
achieved through implementation of models, 
standards as well as annotation of resources at the 
following layers (Berners-Lee 2003) presented in 
Figure 1:  
 
 
 
Figure 1 . Layer ?cake architecture of  Semantic 
Web (from Tim-Berners-Lee) 
 
Unicode and URI?s are the basic ?bricks? in this 
schema, the first ensuring internationalization, the 
latter unique identification of any resource on the 
Web. XML together with its syntactic validation 
language XMLschema and the Name Spaces 
mechanism are the standard way of encoding 
resources. However XML tags cannot describe 
contents of documents. Therefore RDF (Resource 
Description Framework) model has to be used, 
and the concepts used for semantic description 
have to be organised in ontologies. Inference on 
these concepts are made at the Logic  and Proof  
levels. 
For the purposes of this article we will 
concentrate on the Data-levels, i.e. annotations of 
documents (RDF)  and  structure of the semantic 
information (Ontologies) 
 
2.1.  Document annotation with RDF 
The Resource Description Framework (RDF) [is 
an entity relationship model used for representing 
information about resources in the World Wide 
Web. The main principle is that everything on the 
web can be unique identified with URI?s 
(Uniforme Resource Identifier) and then described 
in terms of triples representing the resources, their 
properties and values.  For the purposes of 
Semantic Web the serialization was done in XML; 
in this way the model benefits also from the 
Namespace property of XML and the RDF 
properties can be unique identified, independent 
of the users 
2.2. Ontologies for Semantic Web 
Ontology, a well-known Knowledge-
Representation mechanism  was rediscovered 
for the purposes of Semantic Web. The RDF 
properties can be organised in classes and 
subclasses, with attributes and values. 
Languages as RDFS, DAML+OIL, or 
recently OWL, permit complete description of 
complicated ontological relations between 
RDF properties, in an RDF/XML format. For 
the moment there are  already hundreds of 
Semantic Web ontolgies for different 
domains, most part of them free available. 
3. On-line Machine Translation and the 
Semantic Web 
In this section we will explain first the main 
principles of example-based machine translation. 
Then we will have a closer look on how it can 
benefit from the Semantic Web activities. 
3.1. Example-based Machine Translation 
(EBMT) 
The basic idea in EBMT is quite simple: for the 
translation of a sentence previous translation 
examples are used. The main assumption behind 
this idea is that many translations are simple 
modifications of previous translations 
[CarlWay03]. In contrast with the translation 
memories, the selection between more possible 
translations is completely automatic. 
  
A typical EBMT System is based on the following 
components (Trujillo 1999) 
 
1. A database of aligned sentences in the 
source and target languages. The contents 
of the database, as well as its dimension 
are essential for the quality of the 
selection. The examples have to be 
domain-relevant, long enough to capture 
specific particularities of a construction 
and short enough to be retrieved in 
common texts  
2. A matching algorithm that identifies the 
examples that most closely resemble all or 
part of the input sentence 
3. A combination algorithm which rebuilds 
the input sentence, through a combination 
of retrieved fragments  
4. A transfer and composition algorithm that 
extracts corresponding target fragments 
and combines them into  a sentence in the 
target language. 
 
It turned out that information about the syntactic 
structure of the fragments in both languages as 
well as pattern transfer rules, can improve 
significantly the performance of the example-
based MT system.[Carlway03]. Therefore it is 
quite usual that the example database contains, 
together with parallel aligned strings, also 
syntactic structures and their correspondences. 
3.2. Language Resources for Semantic Web 
and their role in Machine Translation 
Between the main activities in the Semantic Web 
at the moment we encounter:  
- the description and annotation of a large 
number of web resources following the 
RDF model  
- the creation of repositories of RDF 
properties, organised in  ontologies. 
 
Every resource (document piece of document or 
even sentence) is described via a triple (Subject, 
Predicate, Object). All three elements of the triple 
refer to the logical structure of the resource and 
not the syntactic one. It is expected that in the 
near future, a big part of the documents in Internet 
will be annotated following the RDF model. 
Machine Translation, and in particular 
Example-based Machine Translation can make 
use of these additional annotations for three 
purposes: 
1. For the achievement of parallel aligned 
corpora. Small languages still suffer from 
lack of linguistic resources, and especially 
multilingual resources. On-line 
documents are main source for machine-
readable corpora, however, with few 
exceptions (explicitly translations of the 
same Web page) it is difficult to 
determine automatically which part of a 
document is a translation of another 
document. RDF annotations can be used 
for such purposes 
2. For Example based rough translation: As 
mentioned in section 1 on-line translation 
is made for assimilation purposes, 
therefore, meaning preservation is much 
more important as an exact translation. 
RDF model aims to enrich documents 
with information about their content. This 
can help in the process of ?example based 
rough translation?. Until now, the trials in 
this field were done only on the basis of 
retrieval and translation of content-words 
[ShimhataSumitaMatsumoto03].  
3.  For disambiguation: the current example 
based translation systems make use only 
of syntactic annotation. These can be 
insufficient in disambiguation cases like 
the following: 
Let us assume that we have in the database of  
translation examples: 
Gro?e Besonderheiten ? important 
peculiarities 
Gro?e St?dte ? big cities 
The translation choice for gro?e  Schl??er 
as important castles or big castles is context 
depending. For the moment the disambiguation is 
done only statistical. Semantic annotation of the 
examples , as well as the input text would increase 
the translation accuracy. This makes sense 
especially for translation of on-line resources 
which are supposed to be  correspondingly 
annotated 
Although the advantages of Semantic 
Web annotations (in particular RDF-model) are 
transparent from the points mentioned above, the 
main question which arises is 
Who will decide which semantic information 
has to be included, at what level (sentence 
/paragraph/document), and in which 
language? 
Following information is needed for increasing 
the translation quality : 
- translation equivalents of words 
/expressions 
- transfer rules for syntactic structures 
- semantic classes for the candidate 
solutions. 
The main problem to be solved is the consistency 
between different RDF annotations corresponding 
to different users. Let us assume that in the 
German text the annotation for Gro?e St?dte  is . 
<rdf.description rdf. about:?http?..> 
  <user1: Messung > Gro?e  </user1: 
Messung > 
 
and in the English one  
<rdf.description rdf. about:?http?..> 
  <user2: size >big</user2: size > 
A relationship between ?size? and ?Messung? has 
to be established showing that they refer to the 
same concept. This has to be done via mapping on 
an ontology. The main challenge in the design of 
ontologies with multilingual instances is that, very 
often words in one language overlap concepts in 
the ontology, and there is no one-to-one mapping 
to the meaning in the other language 
The architecture in figure 2 proposes a 
framework for extracting translation 
correspondences, taking into account their RDF 
annotations. We propose the organisation of the  
RDF annotation scheme in two parts: syntactic 
annotation and semantic annotation. The concepts 
to be instantiated for this annotations will be 
organised in two correspondent ontologies.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2: Extraction of Translation Equivalents 
from RDF annotated texts. 
 
Assuming that input is a text A in language L1, a 
search process will identify fragments from A in 
the translation database and obtain  one or more 
translations, namely Texts B1, B2,?Bn. During 
the next step the RDF descriptions of the input 
text and the translation candidates are compared 
by mapping the RDF annotations on the syntactic 
and semantic ontology, and the most similar one 
is chosen as output.  
At the University of Hamburg we are 
currently implementing this schema within a 
Demo-System for German and English texts, in 
tourist domain. Approximately 30 documents in 
both languages are currently annotated with 
linguistic properties in RDF format, mapped on a 
syntactic respectively semantic ontology.  
4. Conclusions and Further Work 
In this article we presented the main principles of 
semantic Web as well as its possible contributions 
to the improvement of on-line translation systems. 
A solution for automatic extraction of translation 
examples from RDF-annotated texts is also 
presented. However the architecture supposes the 
existence of the repositories for syntactic and 
semantic annotations as well as the both 
ontologies.  In order to ensure the viability of the 
principle for on-line translation systems, such 
repositories have to be created for different 
languages, texts have to be annotated and the 
ontologies have to cover a broad spectrum  of 
linguistic phenomena. 
After the complete implementation of the 
demo system we intend to perform an evaluation 
of the translation quality, and to analyse also the 
accuracy of the extraction mechanism,  
 
References 
T. Berners-Lee 2003, Foreword to ?Spinning the 
Semantic Web-Bringing the World wide Web t 
Its Full Potential?, in  D. Fensel, D., J. Hendler,, 
H.Lieberman, and W. Wahlster, (eds.), MIT 
Press, 2003 
T. Berners-Lee, and J. Hendler, and O. Lasilla, 
1999, ?The Semantic Web?, Scientific 
American, 1999 
J. Carbonell 1994,  Slides of a tutorial on MT 
Saarbr?cken 1994. unpublished  
A-Way,  and M. Carl 2003, ?Introduction to 
Example-based machine Translation?, Kluwer 
Academic Press, 2003  
F. Gaspari 2002 ?Using free on-line services in 
MT teaching,? in  Proceedings of the 6th EAMT 
Workshop on Teaching Machine Translation, 
November 14-15, 2002, Manchester, pp.145-
153 
Text 
A  
Search of 
translation 
equivalents 
RDF 
Description  
Text  A 
RDF Descr 
Text  B1, 
B2,..in  
Semantic 
Ontology 
Syntactic 
Ontology 
Translatio
n DB 
Text 
B1 B2
Translation 
from A in 
Language l2 
S. McLaughli, and U. Schwall 1998, ?Machine 
Translation and the Information Soup:?, in 
Third Conference of the Association for 
Machine Translation in the Americas, 
Proceedings of the AMTA'98, LNAI 1529, D. 
Farwell et. Al. (Eds.) Langhorne, PA, USA, 
October 1998, pp. 384-397 
M. Shimohata, and E Sumita, and Y. Matsumoto 
2003, ?Retrieving Meaning-equivalent 
Sentences for Example-based Rough 
Translation?, HLT-NAACL Workshop: 
Building and using Parallel Texts. Data Driven 
Machine Translation and Beyond, Edmonton, 
May-June 2003, pp. 50-56 
Technology Review 2003, ?10 emerging 
Technologies who will change the World?, 
retrieved at http://www.technologyreview.com/
A Trujillo 1999, Translation Engines: Techniques 
for Machine Translation, Springer Verlag, 1999  
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 6?10,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
 
 
Harnessing NLP Techniques in the Processes of  
Multilingual Content Management 
 
 
Anelia Belogay Diman Karagyozov 
Tetracom IS Ltd. Tetracom IS Ltd. 
anelia@tetracom.com diman@tetracom.com 
Svetla Koeva Cristina Vertan 
Institute for Bulgarian Language Universitaet Hamburg 
svetla@dcl.bass.bg cristina.vertan@uni-hamburg.de 
Adam Przepi?rkowski Polivios Raxis 
Instytut Podstaw Informatyki Polskiej 
Akademii Nauk 
Atlantis Consulting SA 
adamp@ipipan.waw.pl raxis@atlantisresearch.gr 
Dan Cristea  
Universitatea Alexandru Ioan Cuza  
dcristea@info.uaic.ro  
 
 
Abstract 
The emergence of the WWW as the main 
source of distributing content opened the 
floodgates of information. The sheer 
volume and diversity of this content 
necessitate an approach that will reinvent 
the way it is analysed. The quantitative 
route to processing information which 
relies on content management tools 
provides structural analysis. The 
challenge we address is to evolve from 
the process of streamlining data to a level 
of understanding that assigns value to 
content. 
We present an open-source multilingual 
platform ATALS that incorporates 
human language technologies in the 
process of multilingual web content 
management. It complements a content 
management software-as-a-service 
component i-Publisher, used for creating, 
running and managing dynamic content-
driven websites with a linguistic 
platform. The platform enriches the 
content of these websites with revealing 
details and reduces the manual work of 
classification editors by automatically 
categorising content. The platform 
ASSET supports six European languages. 
We expect ASSET to serve as a basis for 
future development of deep analysis tools 
capable of generating abstractive 
summaries and training models for 
decision making systems. 
Introduction 
The advent of the Web revolutionized the way in 
which content is manipulated and delivered. As a 
result, digital content in various languages has 
become widely available on the Internet and its 
sheer volume and language diversity have 
presented an opportunity for embracing new 
methods and tools for content creation and 
distribution. Although significant improvements 
have been made in the field of web content 
management lately, there is still a growing 
demand for online content services that 
incorporate language-based technology. 
Existing software solutions and services such 
as Google Docs, Slingshot and Amazon 
implement some of the linguistic mechanisms 
addressed in the platform. The most used open-
source multilingual web content management 
6
  
systems (Joomla, Joom!Fish, TYPO3, Drupal)1 
offer low level of multilingual content 
management,   providing abilities for building 
multilingual sites. However, the available 
services are narrowly focused on meeting the 
needs of very specific target groups, thus leaving 
unmet the rising demand for a comprehensive 
solution for multilingual content management 
addressing the issues posed by the growing 
family of languages spoken within the EU. 
We are going to demonstrate the open-source 
content management platform ATLAS and as 
proof of concept, a multilingual library i-
librarian, driven by the platform. The 
demonstration aims to prove that people reading 
websites powered by ATLAS can easily find 
documents, kept in order via the automatic 
classification, find context-sensitive content, find 
similar documents in a massive multilingual data 
collection, and get short summaries in different 
languages that help the users to discern essential 
information with unparalleled clarity. 
The ?Technologies behind the system? chapter 
describes the implementation and the integration 
approach of the core linguistic processing 
framework and its key sub-components ? the 
categorisation, summarisation and machine-
translation engines. The chapter ?i-Librarian ? a 
case study? outlines the functionalities of an 
intelligent web application built with our system 
and the benefits of using it. The chapter 
?Evaluation? briefly discusses the user 
evaluation of the new system. The last chapter 
?Conclusion and Future Work? summarises the 
main achievements of the system and suggests 
improvements and extensions. 
Technologies behind the system 
The linguistic framework ASSET employs 
diverse natural language processing (NLP) tools 
technologically and linguistically in a platform, 
based on UIMA 2 . The UIMA pluggable 
component architecture and software framework 
are designed to analyse content and to structure 
it. The ATLAS core annotation schema, as a 
uniform representation model, normalizes and 
harmonizes the heterogeneous nature of the NLP 
tools3. 
                                                          
1 http://www.joomla.org/, http://www.joomfish.net/, 
http://typo3.org/, http://drupal.org/ 
2 http://uima.apache.org/ 
3 The system exploits heterogeneous NLP tools, for 
the supported natural languages, implemented in Java, 
C++ and Perl. Examples are: 
The processing of text in the system is split 
into three sequentially executed tasks. 
Firstly, the text is extracted from the input 
source (text or binary documents) in the ?pre-
processing? phase.  
Secondly, the text is annotated by several NLP 
tools, chained in a sequence in the ?processing? 
phase. The language processing tools are 
integrated in a language processing chain (LPC), 
so that the output of a given NLP tool is used as 
an input for the next tool in the chain. The 
baseline LPC for each of the supported languages 
includes a sentence and paragraph splitter, 
tokenizer, part of speech tagger, lemmatizer, 
word sense disambiguation, noun phrase chunker 
and named entity extractor (Cristea and Pistiol, 
2008). The annotations produced by each LPC 
along with additional statistical methods are 
subsequently used for detection of keywords and 
concepts, generation of summary of text, multi-
label text categorisation and machine translation.  
Finally, the annotations are stored in a fusion 
data store, comprising of relational database and 
high-performance Lucene4 indexes. 
The architecture of the language processing 
framework is depicted in Figure 1. 
 
 
 
Figure 1. Architecture and communication channels in 
our language processing framework. 
 
The system architecture, shown in Figure 2, is 
based on asynchronous message processing 
                                                                                        
OpenNLP (http://incubator.apache.org/opennlp/), 
RASP (http://ilexir.co.uk/applications/rasp/), 
Morfeusz (http://sgjp.pl/morfeusz/),  Panterra 
(http://code.google.com/p/pantera-tagger/), ParsEst 
(http://dcl.bas.bg/), TnT Tagger (http://www.coli.uni-
saarland.de/~thorsten/tnt/). 
4 http://lucene.apache.org/ 
7
  
patterns (Hohpe and Woolf, 2004) and thus 
allows the processing framework to be easily 
scaled horizontally. 
 
 
 
Figure 2. Top-level architecture of our CMS and its 
major components. 
Text Categorisation 
We implemented a language independent text 
categorisation tool, which works for user-defined 
and controlled classification hierarchies. The 
NLP framework converts the texts to a series of 
natural numbers, prior sending the texts to the 
categorisation engine. This conversion allows 
high level compression of the feature space. The 
categorisation engine employs different 
algorithms, such as Na?ve Bayesian, relative 
entropy, Class-Feature Centroid (CFC) (Guan et. 
al., 2009), and SVM. New algorithms can be 
easily integrated because of the chosen OSGi-
based architecture (OSGi Alliance, 2009). A 
tailored voting system for multi-label multi-class 
tasks consolidates the results of each of the 
categorisation algorithms. 
Summarisation (prototype phase) 
The chosen implementation approach for 
coherent text summarisation combines the well-
known LexRank algorithm (Erkan and Radev, 
2004) and semantic graphs and word-sense 
disambiguation techniques (Plaza and Diaz, 
2011). Furthermore, we have automatically built 
thesauri for the top-level domains in order to 
produce domain-focused extractive summaries. 
Finally, we apply clause-boundaries splitting in 
order to truncate the irrelevant or subordinating 
clauses in the sentences in the summary.  
Machine Translation (prototype phase) 
The machine translation (MT) sub-component 
implements the hybrid MT paradigm, combining 
an example-based (EBMT) component and a 
Moses-based statistical approach (SMT). Firstly, 
the input is processed by the example-based MT 
engine and if the whole or important chunks of it 
are found in the translation database, then the 
translation equivalents are used and if necessary 
combined (Gavrila, 2011). In all other cases the 
input is processed by the categorisation sub-
component in order to select the top-level 
domain and respectively, the most appropriate 
SMT domain- and POS-translation model 
(Niehues and Waibel, 2010). 
The translation engine in the system, based on 
MT Server Land (Federmann and Eisele, 2010),  
is able to accommodate and use different third 
party translation engines, such as the Google, 
Bing, Lusy or Yahoo translators. 
Case Study: Multilingual Library  
i-Librarian5  is a free online library that assists 
authors, students, young researchers, scholars, 
librarians and executives to easily create, 
organise and publish various types of documents 
in English, Bulgarian, German, Greek, Polish 
and Romanian. Currently, a sample of the 
publicly available library contains over 20 000 
books in English. 
On uploading a new document to i-Librarian, 
the system automatically provides the user with 
an extraction of the most relevant information 
(concepts and named entities, keywords). Later 
on, the retrieved information is used to generate 
suggestions for classification in the library 
catalogue, containing 86 categories, as well as a 
list of similar documents. Finally, the system 
compiles a summary and translates it in all 
supported languages. Among the supported 
formats are Microsoft Office documents, PDF, 
OpenOffice documents, books in various 
electronic formats, HTML pages and XML 
documents. Users have exclusive rights to 
manage content in the library at their discretion.   
The current version of the system supports 
English and Bulgarian. In early 2012 the Polish, 
Greek, German and Romanian languages will be 
in use. 
                                                          
5 i-Librarian web site is available at http://www.i-
librarian.eu/. One can access the i-Librarian demo content 
using ?demo@i-librarian.eu? for username and ?sandbox? 
for password. 
8
  
Evaluation 
The technical quality and performance of the 
system is being evaluated as well as its appraisal 
by prospective users. The technical evaluation 
uses indicators that assess the following key 
technical elements: 
? overall quality and performance 
attributes (MTBF6, uptime, response 
time); 
? performance of specific functional 
elements (content management, machine 
translation, cross-lingual content 
retrieval, summarisation, text 
categorisation).  
The user evaluation assesses the level of 
satisfaction with the system. We measure non 
functional elements such as: 
? User friendliness and satisfaction, clarity 
in responses and ease of use; 
? Adequacy and completeness of the 
provided data and functionality; 
? Impact on certain user activities and the 
degree of fulfilment of common tasks. 
We have planned for three rounds of user 
evaluation; all users are encouraged to try online 
the system, freely, or by following the provided 
base-line scenarios and accompanying exercises. 
The main instrument for collecting user feedback 
is an online interactive electronic questionnaire7. 
The second round of user evaluation is 
scheduled for Feb-March 2012, while the first 
round took place in Q1 2011, with the 
participation of 33 users. The overall user 
impression was positive and the Mean value of 
each indicator (in a 5-point Likert scale) was 
measured on AVERAGE or ABOVE 
AVERAGE.  
 
 
Figure 3. User evaluation ? UI friendliness and ease 
of use. 
                                                          
6 Mean Time Between Failures 
7 The electronic questionnaire is available at 
http://ue.atlasproject.eu 
 
Figure 4. User evaluation ? user satisfaction with the 
available functionalities in the system. 
 
 
Figure 5. User evaluation ? users productivity 
incensement. 
Acknowledgments 
ATLAS (Applied Technology for Language-
Aided CMS) is a European project funded under 
the CIP ICT Policy Support Programme, Grant 
Agreement 250467. 
Conclusion and Future Work 
The abundance of knowledge allows us to widen 
the application of NLP tools, developed in a 
research environment. The tailor made voting 
system maximizes the use of the different 
categorisation algorithms. The novel summary 
approach adopts state of the art techniques and 
the automatic translation is provided by a cutting 
edge hybrid machine translation system. 
The content management platform and the 
linguistic framework will be released as open-
source software. The language processing chains 
for Greek, Romanian, Polish and German will be 
fully implemented by the end of 2011. The 
summarisation engine and machine translation 
tools will be fully integrated in mid 2012. 
We expect this platform to serve as a basis for 
future development of tools that directly support 
decision making and situation awareness. We 
will use categorical and statistical analysis in 
order to recognise events and patterns, to detect 
opinions and predictions while processing 
The user interface is friendly and 
easy to use 
Excellent
28%
Good 
35%
Average
28%
Below 
Average
9%
Poor
Below Average
Average
Good 
Excellent
I am satisfied with the functionalities 
Below 
Average
3%
Average
38%
Excellent
31%
Good 
28%
Poor
Below
Average
Average
Good 
Excellent
The system increases y ur 
productivity 
Excellent
13%
Below 
Averag
9%
Average
31%
Good 
47%
Poor
Below
Average
Average
Good 
Excellent
9
  
extremely large volumes of disparate data 
resources. 
Demonstration websites 
The multilingual content management platform is 
available for testing at http://i-
publisher.atlasproject.eu/atlas/i-publisher/demo . 
One can access the CMS demo content using 
?demo? for username and ?sandbox2? for 
password. 
The multilingual library web site is available 
at http://www.i-librarian.eu/. One can access the 
i-Librarian demo content using ?demo@i-
librarian.eu? for username and ?sandbox? for 
password. 
References  
Dan Cristea and Ionut C. Pistol, 2008. Managing 
Language Resources and Tools using a Hierarchy 
of Annotation Schemas. In the proceedings of 
workshop 'Sustainability of Language Resources 
and Tools for Natural Language Processing', 
LREC, 2008 
Gregor Hohpe and Bobby Woolf. 2004. Enterprise 
Integration Patterns: Designing, Building, and 
Deploying Messaging Solutions. Addison-Wesley 
Professional. 
Hu Guan, Jingyu Zhou and Minyi Guo. A Class-
Feature-Centroid Classifier for Text 
Categorization. 2009. WWW 2009 Madrid, Track: 
Data Mining / Session: Learning, p201-210. 
OSGi Alliance. 2009. OSGi Service Platform, Core 
Specification, Release 4, Version 4.2. 
Gunes Erkan and Dragomir R. Radev. 2004. 
LexRank: Graph-based Centrality as Salience in 
Text Summarization. Journal of Artificial 
Intelligence Research 22 (2004), p457?479. 
Laura Plaza and Alberto Diaz. 2011. Using Semantic 
Graphs and Word Sense Disambiguation 
Techniques to Improve Text Summarization. 
Procesamiento del Lenguaje Natural, Revista n? 47 
septiembre de 2011 (SEPLN 2011), pp 97-105. 
Monica Gavrila. 2011. Constrained Recombination in 
an Example-based Machine Translation System, In 
the Proceedings of the EAMT-2011: the 15th 
Annual Conference of the European Association 
for Machine Translation, 30-31 May 2011, Leuven, 
Belgium, p. 193-200 
 Jan Niehues and Alex Waibel. 2010. Domain 
adaptation in statistical machine translation using 
factored translation models. EAMT 2010: 
Proceedings of the 14th Annual conference of the 
European Association for Machine Translation, 27-
28 May 2010, Saint-Rapha?l, France. 
Christian Federmann and Andreas Eisele. 2010. MT 
Server Land: An Open-Source MT Architecture. 
The Prague Bulletin of Mathematical Linguistics. 
NUMBER 94, 2010, p57?66 
10
Proceedings of the ACL Student Research Workshop, pages 130?135,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A New Syntactic Metric for Evaluation of Machine Translation 
 
Melania Duma 
Department of Computer 
Science 
University of Hamburg 
Vogt-K?lln-Stra?e 30 
22527 Hamburg 
duma@informatik.uni
-hamburg.de 
Cristina Vertan 
Faculty for Language, 
Literature and Media 
University of Hamburg 
Von Melle Park 6 
20146 Hamburg 
cristina.vertan@uni
-hamburg.de 
Wolfgang Menzel 
Department of Computer 
Science 
University of Hamburg 
Vogt-K?lln-Stra?e 30 
22527 Hamburg 
menzel@informatik.uni
-hamburg.de 
 
  
Abstract 
Machine translation (MT) evaluation aims at 
measuring the quality of a candidate 
translation by comparing it with a reference 
translation. This comparison can be 
performed on multiple levels: lexical, 
syntactic or semantic. In this  paper, we 
propose a new syntactic metric for MT 
evaluation based on the comparison of the 
dependency structures of the reference and 
the candidate translations. The dependency 
structures are obtained by means of a 
Weighted Constraints Dependency Grammar 
parser. Based on  experiments performed on 
English to German translations, we show that 
the new metric correlates well with human 
judgments at the system level. 
1 Introduction 
Research in automatic machine translation (MT) 
evaluation has the goal of developing a set of 
computer-based methods that measure accurately 
the correctness of the output generated by a MT 
system. However, this task is a difficult one 
mainly because there is no unique reference 
output that can be used in the comparison with 
the candidate translation. One sentence can have 
several correct translations. Thus, it is difficult to 
decide if the deviation from an existing reference 
translation is a matter of style (the use of 
synonymous words, different syntax etc.) or a 
real translation error.  
Most of the automatic evaluation metrics 
developed so far are focused on the idea of 
lexical matching between the tokens of one or 
more reference translations and the tokens of a 
candidate translation. However, structural 
similarity between a reference translation and a 
candidate one cannot be captured by lexical 
features. Therefore, research in MT evaluation 
experiences a gradual shift of focus from lexical  
metrics to structural ones, whether they are 
syntactic or semantic or a combination of both.  
This paper introduces a new syntactic 
automatic MT evaluation method. At this stage 
of research the new metric is evaluating 
translations from any source language into 
German. Given that a set of constraint-based 
grammar rules are available for that language, 
extensions to other target languages are anytime 
possible. The chosen tool for providing syntactic 
information for German is the Weighted 
Constraints Dependency Grammar (WCDG) 
parser (Menzel and Schr?der, 1998), which is 
preferred over other parsers because of its 
robustness to ungrammatical input, as it is typical 
for MT output. The rest of this paper is organized 
as follows. In Section 2 the state of the art in MT 
evaluation is presented, while in Section 3 the 
new syntactic metric is described. The 
experimental setup and results are presented in 
Section 4. The last section deals with the 
conclusions and future work. 
2 State of the art 
Automatic evaluation of MT systems relies on 
the existence of at least one reference1 created by 
a human annotator. Using an automatic method 
of evaluation a score is computed, based on the 
similarity between the output of the MT system 
and the reference. This similarity can be 
computed at different levels: lexical, syntactic or 
semantic. At the lexical level, the metrics 
developed so far can be divided into two major 
categories: n-gram based and edit distance based. 
                                                          
1 We will use the term reference for the reference 
translation and the term translation for the candidate 
translation. 
130
Among the n-gram based metrics, one of the 
most popular methods of evaluation is BLEU 
(Papineni et al, 2001). It provides a score that is 
computed as the summed number of n-grams 
shared by the references and the output, divided 
by the total number of n-grams. Lexical metrics 
that use the edit distance are constructed using 
the Levenshtein distance applied at the word 
level. Among these metrics, WER (Niessen et 
al., 2000) is the one which is used more 
frequently; it calculates the minimal number of 
insertion, substitutions and deletions needed to 
transform the candidate translation into a 
reference.  
Metrics based on lexical matching suffer from 
not being able to consider the variation  
encountered in natural language. Thus, they 
reward a low score to an otherwise fluent and 
syntactically correct candidate translation, if it 
does not share a certain number of words with 
the set of references. Because of this, major 
disagreements between the scores assigned by 
BLEU and human judgments have been reported 
in Koehn and Monz (2006) and Callison-Burch 
et al (2006). Another disadvantage is that many 
of them cannot be applied at the segment level, 
which is often needed in order to better assess 
the quality of MT output and to determine which 
improvements should be made to the MT system. 
Because of these disadvantages there is an 
increasing need for other approaches to MT 
evaluation that go beyond the lexical level of the 
phrases compared. 
 In Liu and Gildea (2005),  three syntactic 
evaluation metrics are presented. The first of 
these metrics, the Subtree Metric (SMT), is 
based on determining the number of subtrees that 
can be found in both the candidate translation 
and the reference phrase structure trees. The 
second metric, which is a kernel-based subtree 
metric, is defined as the maximum of the cosine 
measure between the MT output and the set of 
references. The third metric proposed computes 
the number of matching n-grams between the 
headword chains of the reference and the 
candidate translation dependency trees obtained 
using the parser described in (Collins, 1999).  
The idea of syntactic similarity is further 
exploited in Owczarzak et al (2007) which uses 
a Lexical Functional Grammar (LFG) parser. 
The similarity between the translation and the 
reference is computed using the precision and the 
recall of the dependencies that illustrate the pair 
of sentences. Furthermore, paraphrases are used 
in order to improve the correlation with human 
judgments. Another  set of syntactic metrics has 
been  introduced in Gimenez (2008); some of 
them are based on analyzing different types of 
linguistic information (i.e. part-of-speech or 
lemma).  
3 A new syntactic automatic metric 
In this section we introduce the  new syntactic 
metric  which is based on constraint dependency 
parsing. In the first subsection, the WCDG parser 
is presented, together with the advantages of 
using this parser over the other ones available, 
while the second subsection provides a detailed 
description of the new metric. 
3.1 Weighted Constraint Dependency 
Grammar Parser 
Our research was performed using a dependency 
parser. We decided on this type of parser 
because, as opposed to constituent parsers, it 
offers the possibility of better representing non-
projective structures. Moreover, it has been 
shown in Kuebler and Prokic (2006) that, at least 
in the case of German, the results achieved by a 
dependency parser are more accurate than the 
ones obtained when parsing using constituent 
parsers, and this is because dependency parsers 
can handle better long distance relations and 
coordination. 
   The goal of constraint dependency 
grammars (CDG) is to create dependency 
structures that represent a given phrase (Schr?der 
et al, 2000) on parallel levels of analysis. A 
relation between two words in a sentence is 
represented using an edge, which connects the 
regent and the dependent. Edges are annotated 
using labels in order to distinguish between 
different types of relations. A constraint is made 
up of a logical formula that describes properties 
of the tree. One property, for example, that is 
always enforced is that no word can have more 
than one regent on any level at a time. During the 
analysis, each of the constraints is applied to 
every edge or every pair of edges belonging to 
the constructed dependency parse tree. The main 
advantage of using constraint dependency 
grammars over dependency grammars based on 
generative rules is that they can deal better with 
free word order languages (Foth, 2004). 
Weighted Constraint Dependency Grammar 
(WCDG) (Menzel and Schr?der, 1998) assigns 
different weights to the constraints of the 
grammar. Every constraint in WCDG is assigned 
a score which is a number between 0.0 and 1.0, 
131
while the general score of a parse is calculated as 
the product of all the scores of all the instances 
of constraints that have not been satisfied. Rules 
that have a score of 0 are called hard rules, 
meaning that they cannot be ignored, which is 
the case of the one regent only rule mentioned 
earlier. The advantage of using graded 
constraints, as opposed to crisp ones, stems from 
the fact that weights allow the parser to tolerate 
constraint violations, which, in turn, makes the 
parser robust against ungrammaticality. The 
parser was evaluated using different types of 
texts, and the results show that it has an accuracy 
between 80% and 90% in computing correct 
dependency attachments depending on the type 
of text (Foth et al, 2004a). 
The benefit of using WCDG over other parsers 
is that it provides further information on a parse, 
like the general score of the parse and the 
constraints that are violated by the final result. 
This information can be further explored in order 
to perform an error analysis. Moreover, because 
of the fact that the candidate translations are 
sometimes not well-formed, parsing them 
represents a challenge. However, WCDG will 
always provide a final result, in the form of a 
dependency structure, even though it might have 
a low score due to the violated constraints. 
3.2 Description of the metric 
In order to define a  new syntactic metric for MT 
evaluation, we have incorporated the WCDG 
parser in the process of evaluation. Because the 
output of the WCDG parser is a dependency tree, 
we have looked into techniques of measuring 
how similar two trees are. Our aim was to 
determine whether a tree similarity metric 
applied on the two dependency parse trees would 
prove to be an efficient way of capturing the 
similarity between the reference and the 
translation. Let us consider this example, in 
which the reference sentence is ?Die schwarze 
Katze springt schnell auf den roten Stuhl.?(engl. 
The black cat jumps quickly on the red chair) 
and the candidate translation is?Auf den roten 
Stuhl schnell springt die schwarze Katze?(engl. 
On the red chair quickly jumps the red cat). Even 
though the word order of the two segments is 
quite different, and the translation has an 
incorrect syntax, they roughly have the same 
meaning. We present in Figure 1 the dependency 
parse trees obtained using WCDG for the 
sentences considered. We can observe that the 
general structure of the translation is similar to 
that of the reference, the only difference being 
the reverse order between the left subtree and the 
right subtree. The tree similarity measure that we 
chose to use was the All Common Embedded 
Subtrees (ACET) (Lin et al, 2008) similarity. 
Given a tree T, an embedded subtree is obtained 
by removing one or more nodes, except for the 
root, from the tree T. The idea behind ACET is 
that, the more substructures two trees share, the 
more similar they are. Therefore, ACET is 
defined as the number of common embedded 
subtrees shared between two trees. The results 
reported in Lin et al (2008) show that ACET 
outperforms tree edit distance (Zhang and 
Shasha, 1989) in terms of efficiency. 
   
 
Figure 1.  Example of dependency parse trees for 
reference and candidate translations 
 
In our experiments, we have applied the ACET 
algorithm, and computed the number of common 
embedded subtrees between the dependency 
parse trees of the hypothesis and the reference. 
Because of the additional information provided 
by the parsing, pre-processing of the output of 
the WCDG parser was necessary in order to 
transform the dependency tree into a general tree. 
We first removed the labels assigned to every 
edge, but maintained the nodes and the left to 
right order between them. 
In the following, we will refer to the new 
proposed metric using CESM (Common 
Embedded Subtree Metric). CESM was 
computed using the precision, the recall and the 
F-measure of the common embedded subtrees of 
the reference and the translation: 
 
           
                      
                     
 
         
                      
                     
 
132
          
                  
                
 
 
where treeref and treehyp represent the 
preprocessed dependency trees of the reference 
and the hypothesis translations.  
4 Experimental setup and evaluation 
In order to determine how accurate CESM is in 
capturing the similarity between references and 
translations, we evaluated it at the system level 
and at the segment level. The evaluation was 
conducted using data provided by the NAACL 
2012 WMT workshop (Callison-Burch et al, 
2012). The test data for the workshop consisted 
of 99 translated news articles in English, 
German, French, Spanish and Czech.  
At the system level, the initial German test set 
provided at the workshop was filtered according 
to the length of segments. This was done in order 
to limit the time requirements of WCDG. As a 
result, 500 segments with a length between 50 
and 80 characters were extracted from the 
German reference file. In the next step, we 
arbitrarily selected the outputs of 7 of the 15 
systems that were submitted for evaluation in the 
English to German translation task: DFKI  
(Vilar, 2012), JHU (Ganitkevitch et al, 2012), 
KIT (Niehues et al, 2012), UK (Zeman, 2012) 
and three anonymized system outputs referred to 
as OnlineA, OnlineB, OnlineC.  
After this initial step of filtering the data, the 7 
systems were evaluated by calculating the CESM 
score for every pair of reference and translation 
segments corresponding to a system. The 
average scores obtained are depicted in Table 1. 
Evaluation of the metric at the system level was 
performed by measuring the correlation of the 
CESM metric with human judgments using 
Spearman's rank correlation coefficient ?:  
 
    
    
 
       
 
 
where n represents the number of MT systems 
considered during evaluation, and di
2 represents 
the difference between the ranks, assigned to a 
system, by the metric and the human judgments. 
The minimum value of ? is -1, when there is no 
correlation between the two rankings, while the 
maximum value is 1, when the two rankings 
correlate perfectly (Callison-Burch et al, 2012).  
In order to compute the ? score, the scores 
attributed to every system by CESM, were 
converted into ranks. From the different ranking 
strategies that were presented by the WMT12 
workshop, the standard ranking order was 
chosen. The ? rank correlation coefficient was 
calculated as being ? = 0.92, which shows there 
is a strong correlation between the results of 
CESM and the human judgments. In order to 
better assess the quality of CESM, the test set 
was also evaluated using NIST (Doddington, 
2002), which managed to obtain the same rank 
correlation coefficient of ? = 0.92. 
 
No. System 
name 
CESM 
score 
NIST 
score 
1 DFKI  0.069 4.7709 
2 JHU 0.073 4.9904 
3 KIT 0.090 5.1358 
4 OnlineA 0.093 5.3039 
5 OnlineB 0.091 5.3039 
6 OnlineC 0.085 4.8022 
7 UK 0.075 4.6579 
Table 1. System level evaluation results 
 
The first step in evaluating at the segment level 
was filtering the initial test set provided by the 
WMT12 workshop. For this purpose, 2500 
reference and translation segments were selected 
with a length between 50 and 80 characters. The 
Kendall tau rank correlation coefficient was 
calculated in order to measure the correlation 
with human judgments, where Kendall tau 
(Callison-Burch et al, 2012) is defined as: 
 
   
                                     
                  
 
 
In order to compute the value of Kendall tau, we 
determined the number of concordant pairs and 
the number of discordant pairs of judgments. 
Similarly to the guideline followed during the 
WMT12 workshop (Callison-Burch et al, 2012), 
we penalized ties given by CESM and ignored 
ties assigned by the human judgments. The 
obtained result was a correlation of 0.058. As a 
term of comparison, the highest correlation for 
segment level reported in Callinson-Burch et al 
(2012) was 0.19 obtained by TerrorCat (Fishel et 
al., 2012) and the lowest was BlockErrCats 
(Popovic, 2012) with 0.040. However, these 
results were obtained by evaluating on the entire 
test set. The rather low correlation result we 
obtained can be partially explained by the fact 
that only one judgment of a pair of reference and 
translation was taken into account. It will be 
133
interesting to see how the averaging of the ranks 
of a translation influences the correlation 
coefficient.  
5 Conclusions and future work 
In this paper, a new evaluation metric for MT 
was introduced, which is based on the 
comparison of dependency parse trees. The 
dependency trees were obtained using the 
WCDG German parser. The reason why we 
chose this parser was that, due to its architecture, 
it is able to handle  ungrammatical and 
ambiguous input data. The experiments 
conducted so far show that using the data made 
available at the NAACL 2012 WMT workshop, 
CESM correlates well with the human judgments 
at the system level. One of the future 
experiments that we intend to perform is to 
assess metric quality on the entire evaluation set. 
Moreover, we plan to compare CESM with other 
tree-based MT metrics. Furthermore, the 
WMT12 workshop offers different ranking 
possibilities, like the ones presented in Bojar et 
al (2011) and in Lopez (2012). It will be 
determined how much are the segment level 
evaluation results influenced by these ranking 
orders. 
One limitation of the proposed metric is that, 
at the moment it is restricted to translations from 
any source language to German as a target 
language. Because of this reason, we plan to 
extend the metric to other languages and see how 
well it performs in different settings. In further 
experiments we also intend to test CESM using 
statistical based dependency  parsers, like the 
Malt Parser (Nivre et al, 2007) and the MST 
parser (McDonald et al, 2006), in order to 
decide whether the choice of parser influences 
the performance of the metric.  
Another approach that we will explore for 
improving CESM is to compare dependency 
parse trees using the base form and the part-of-
speech of the tokens, instead of the exact lexical 
match. We will try this approach in order to 
avoid penalizing lexical variation. 
The accuracy of CESM can be further 
increased by the use of paraphrases, which can 
be obtained by using a German thesaurus or a 
lexical resource like GermaNet (Hamp and 
Feldweg, 1997). Furthermore, a technique like 
the one described in Owczarzak (2008) can be 
implemented for generating domain specific 
paraphrases. The results reported show that the 
use of this kind of paraphrases in order to 
produce new references has increased the BLEU 
score, therefore this is an approach that will be 
further investigated. 
 
Acknowledgments 
 
This work was funded by the University of 
Hamburg Doctoral Fellowships in accordance 
with the Hamburg Act for the Promotion of 
Young Researchers and Artists (HmbNFG), and 
the EAMT Project ?Using Syntactic and 
Semantic Information in the Evaluation of 
Corpus-based Machine Translation?.   
Reference 
O. Bojar, M. Ercegov?evi?, M Popel and O. Zaidan. 
2011. A Grain of Salt for the WMT Manual 
Evaluation. Proceedings of the Sixth Workshop 
on Statistical Machine Translation. 
C. Callison-Burch, M. Osborne and P. Koehn. 2006. 
Re-evaluating the Role of Bleu in Machine 
Translation Research. Proceedings of EACL-
2006. 
C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. 
Soricut and L. Specia. 2012.  Findings of the 
2012 Workshop on Statistical Machine 
Translation. Proceedings of WMT12. 
M. J. Collins. 1999. Head-driven Statistical 
Models for Natural Language Parsing. Ph.D. 
thesis, University of Pennsylvania. 
G. Doddington. 2002. Automatic Evaluation of 
Machine Translation Quality Using N-gram 
Co-Occurrence Statistics. Proceedings of the 
2nd International Conference on Human Language 
Technology. 
K. Foth. 2004. Writing weighted constraints for 
large de-pendency grammars. Recent Advances 
in De-pendency Grammar, Workshop COLING 
2004. 
K. Foth, M. Daum and W. Menzel. 2004a. A broad-
coverage parser for German based on 
defeasible constraints. KONVENS 2004, 
Beitr?ge zur 7, Konferenz zur Verarbeitung 
nat?rlicher Sprache, Wien. 
K. Foth, M. Daum and W. Menzel. 2004b. 
Interactive grammar development with 
WCDG. Proc. of the 42nd Annual Meeting of the 
Association for Com-putational Linguistics. 
K. Foth, T. By and W. Menzel. 2006. Guiding a 
con-straint dependency parser with supertags. 
Proceedings of the 21st Int. Conf. on 
Computational Linguistics. 
134
M. Fishel, R. Sennrich, M. Popovic and O. Bojar. 
2012. TerrorCat: a translation error 
categorization-based MT quality metric. 
Proceedings of the Seventh Workshop on 
Statistical Machine Translation. 
J. Ganitkevitch, Y. Cao, J. Weese, M. Post and C. 
Callison-Burch. 2012. Joshua 4.0: Packing, 
PRO, and paraphrases. Proceedings of the 
Seventh Workshop on Statistical Machine 
Translation. 
J. Gimenez. 2008. Empirical Machine Translation 
and its Evaluation. Ph. D. thesis. 
B. Hamp and H. Feldweg. 1997. GermaNet - a 
Lexical-Semantic Net for German. Proc. of 
ACL workshop Automatic Information Extraction 
and Building of Lexical Semantic Resources for 
NLP Applications. 
P. Koehn and C. Monz. 2006. Manual and 
Automatic Evaluation of Machine Translation 
between European Languages. NAACL 2006 
Workshop on Statistical Machine Translation. 
P. Koehn. 2010. Statistical Machine Translation. 
Cambridge University Press. 
S. K?bler and J. Prokic. 2006. Why is German 
Dependency Parsing more Reliable than 
Constituent Parsing?. Proceedings of the Fifth 
International Work-shop on Treebanks and 
Linguistic Theories. 
Z. Lin, H. Wang, S. McClean and C. Liu. 2008. All 
Common Embedded Subtrees for Measuring 
Tree Similarity. International Symposium on 
Computational Intelligence and Design. 
D. Liu and D. Gildea. 2005. Syntactic Features for 
Evaluation of Machine Translation. ACL 2005 
Workshop on Intrinsic and Extrinsic Evaluation 
Measures for Machine Translation and/or 
Summarization. 
A. Lopez. 2012. Putting human assessments of 
machine translation systems in order. 
Proceedings of the Seventh Workshop on 
Statistical Machine Translation.  
R. McDonald, K. Lerman and F. Pereira. 2006. 
Multilingual Dependency Parsing with a Two-
Stage Discriminative Parser. Tenth Conference 
on Computational Natural Language Learning. 
W. Menzel and I. Schr?der. 1998. Decision 
Procedures for Dependency Parsing Using 
Graded Constraints. Workshop On Processing 
Of Dependency-Based Grammars. 
J. Niehues, Y. Zhang, M. Mediani, T. Herrmann, E. 
Cho and A. Waibel. 2012. The karlsruhe institute 
of technology translation systems for the WMT 
2012. Proceedings of the Seventh Workshop on 
Statistical Machine Translation. 
S. Niessen, F. J. Och, G. Leusch  and H. Ney. 2000. 
An Evaluation Tool for Machine Translation: 
Fast Evaluation for MT Research. Proceedings 
of the 2nd International Conference on Language 
Resources and Evaluation (LREC). 
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. 
K?bler, S. Marinov  and E. Marsi. 2007. 
MaltParser: A language-independent system 
for data-driven dependency parsing. Natural 
Language Engineering. 
K. Owczarzak, J. van Genabith and A. Way. 2007. 
Dependency-based automatic evaluation for 
machine translation. Proceedings of SSST, 
NAACL-HLT 2007 / AMTA Workshop on Syntax 
and Structure in Statistical Translation. 
K. Owczarzak. 2008. A Novel Dependency-Based 
Evaluation Metric for Machine Translation, 
Ph.D. thesis. 
K. Papineni, S. Roukos, T. Ward and W.-J. Zhu. 
2001. Bleu: a method for automatic evaluation 
of machine translation. RC22176 (Technical 
Report), IBM T.J. Watson Research Center. 
M. Popovic. 2012. Class error rates for evaluation 
of machine translation output. Proceedings of 
the Seventh Workshop on Statistical Machine 
Translation. 
I. Schr?der, W. Menzel, K. Foth and M. Schulz. 2000. 
Modeling dependency grammar with 
restricted constraints.  Traitement Automatique 
des Langues. 
I. Schr?der, H. Pop, W. Menzel and K. Foth. 2001. 
Learning grammar weights using genetic 
algorithms. Proceedings Euroconference Recent 
Advances in Natural Language Processing. 
I. Schr?der. 2002. Natural Language Parsing with 
Graded Constraints. Ph.D. thesis, Dept. of 
Computer Science, University of Hamburg. 
D. Vilar. 2012.  DFKI?s SMT system for WMT 
2012. Proceedings of the Seventh Workshop on 
Statistical Machine Translation. 
D. Zeman. 2012. Data issues of the multilingual 
translation matrix. Proceedings of the Seventh 
Workshop on Statistical Machine Translation. 
K. Zhang and D. Shasha. 1989. Simple fast 
algorithms for the editing distance between 
trees and related problems. SIAM Journal on 
Computing. 
 
135
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 59?64,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Two approaches for integrating translation and retrieval in real applications 
Cristina Vertan University of Hamburg Research Group ?Computerphilology? Von-Mell Park 6, 20146 Hamburg, germany cristina.vertan@uni-hamburg.de   	 ?   Abstract 
In this paper we present two approaches for integrating translation into cross-lingual search engines: the first approach relies on term translation via a language ontology, the other one is based on machine translation of specific information. 1 Introduction The explosion of on-line available multilingual information during the last years, raised the necessity of building applications able to manage this type of content.  People are more and more used to search for information not only in English, but also in their mother tongue and often in some other languages they understand.  Moreover there are dedicated web-platforms where the information is a-priori multilingual, like eLearning Systems and Content Management Systems. eLearning systems are used more an more as real alternatives to face-to-face courses and include often materials in the mother languages and also English (either because a lot of literature is available in English or because the content  should be made available to exchange students). Content management systems used by multinational corporates, share materials in several languages as well.  On such platforms the search facility is an essential one: usually the implemented methods are based on term indexes, which are created per language. This prohibits or at least makes very difficult the access to multilingual material: the user is forced to repeat the query in several languages, which is time consuming and error ?prone. Cross-lingual retrieval methods are only slowly introduced in real applications like those ones 
quoted above.  In this paper we will describe two applications and two different ways of combining term-translation and information retrieval. In the first one, an eLearning system, we implement a language ontology on which we map the multilingual lexical entries. The search engine makes then use of the mapping between the lexical material and the ontology. The second application is a content management system, in which we use machine translation as backbone to the search engine  The rest of the paper is organised as follows: in Section 2 we describe the eLearning environment in which we embedded the search engine and present this one. In Section 3 we describe the content management system and the symbiosis between the machine translation and the search engines. In Section 4 we conclude with some observations on these two approaches and introduce possible approaches for further work.  2 Crosslingual search based on language independent ontology and lexical information  The system we describe in this section was developed within the EU-Project LT4eL ? Language Technology for eLearning (http://www.lt4el.eu). The main goal of the project was to enhance an eLearning system with language technology tools. The system dealt with nine languages (Bulgarian, Czech, Dutch, English, German, Maltese, Polish, Portuguese, Romanian). eLearning documents were processed through language specific pipelines and keywords and definitions were automatic extracted.  The kernel of the system is however the crosslingual semantic search engine which makes use of a language 
59
independent ontology and  mapping of language specific lexicons.  As prototype we implemented a domain specific ontology of about 1000 concepts, from the field ?Computer Science for non computer science specialists?. The concepts were not collected from English texts, but from analyzed keywords from al involved languages. In this way we avoided a bias towards English specific concepts. For the keywords in each language each partner provided an English translation (one word, one expression or even several sentences). The analysis of these translations conducted to the construction of the ontology. The concepts were represented in OWL-DL. The domain specific ontology was mapped on the DOLCE-upper ontology as well as WordNeT to ensure consistency.   The two main components that define the ontology-to-text relation necessary to support the crosslingual retrieval are: (terminological) lexicon and concept annotation grammar (Lemnitzer et. Al, 2007).   The lexicon plays twofold role in the architecture. First, it interrelates the concepts in the ontology to the lexical knowledge used by the grammar in order to recognize the role of the concepts in the text. Second, the lexicon represents the main interface between the user and the ontology. This interface allows for the ontology to be navigated or represented in a natural way for the user. For example, the concepts and relations might be named with terms used by the users in their everyday activities and in their own natural language (e.g. Bulgarian). This could be considered as a first step to a contextualized usage of the ontology in a sense that the ontology could be viewed through different terms depending on the context. For example, the color names will vary from very specific terms within the domain of carpet production to more common names used when the same carpet is part of an interior design.   Thus, the lexical items contain the following information: a term, contextual information determining the context of the term usage, grammatical features determining the syntactic realization within the text. In the current implementation of the lexicons the contextual information is simplified to a list of a few types 
of users (producer, retailer, etc).  With respect to the relations between the terms in the lexicon and the concepts in the ontology, there are two main problems: (1) there is no lexicalized term for some of the concepts in the ontology, and (2) there are lexical terms in the language of the domain which lack corresponding concepts in the ontology, which represent the meaning of the terms. The first problem is overcomed by writing down in the lexicon also non-lexicalized (fully compositional) phrases to be represented. Even more, we encourage the lexicon builders to add more terms and phrases to the lexicons for a given concept in order to represent as many ways of expressing the concept in the language as possible.   These different phrases or terms for a given concept are used as a basis for construction of the annotation grammar. Having them, we might capture different wordings of the same meaning in the text. The concepts are language independent and they might be represented within a natural language as form(s) of a lexicalized term, or as a free phrase. In general, a concept might have a few terms connected to it and a (potentially) unlimited number of free phrases expressing this concept in the language   Some of the free phrases receive their meaning compositionally regardless their usage in the text, other free phrases denote the corresponding concept only in a particular context. In our lexicons we decided to register as many free phrases as possible in order to have better recall on the semantic annotation task.   In case of a concept that is not-lexicalized in a given language we require at least one free phrase to be provided for this concept. We could summarize the connection between the ontology and the lexicons in the following way: the ontology represents the semantic knowledge in form of concepts and relations with appropriate axioms; and the lexicons represent the ways in which these concepts can be realized in texts in the corresponding languages.   Of course, the ways in which a concept could be represented in the text are potentially infinite in number, thus, we could hope to represent in our lexicons only the most frequent and important terms and phrases. Here is an example of an entry from the Dutch lexicon:   
60
<entry id="id60"> <owl:Class rdf:about="lt4el:BarWithButtons"> <rdfs:subClassOf> <owl:Class rdf:about="lt4el:Window"/> </rdfs:subClassOf> </owl:Class> <def>A horizontal or vertical bar as a part of a window, that contains buttons, icons.</def> <termg lang="nl"> <term shead="1">werkbalk</term> <term>balk</term> <term type="nonlex">balk met knoppen</term> <term>menubalk</term> </termg> </entry>  Each entry of the lexicons contains three types of information: (1) information about the concept from the ontology which represents the meaning for the terms in the entry; (2) explanation of the concept meaning in English; and (3) a set of terms in a given language that have the meaning expressed by the concept. The concept part of the entry provides minimum information for formal definition of the concept.   The English explanation of the concept meaning facilitates the human understanding. The set of terms stands for different wordings of the concept in the corresponding language. One of the terms is the representative for the term set. Note that this is a somewhat arbitrary decision, which might depend on frequency of term usage or specialist?s intuition. This representative term will be used where just one of terms from the set is necessary to be used, for example as an item of a menu. In the example above we present the set of Dutch terms for the concept lt4el:BarWithButtons.   One of the term is non-lexicalized - attribute type with value nonlex. The first term is representative for the term set and it is marked-up with attribute shead with value 1. In this way we determine which term to be used for ontology browsing if there is no contextual information for the type of users. The second component of the ontology-to-text relation, the 
concept annotation grammar, is ideally considered as an extension of a general language deep grammar which is adopted to the concept annotation task. Minimally, the concept annotation grammar consists of a chunk grammar for concept annotation and (sense) disambiguation rules. The chunk grammar for each term in the lexicon contains at least one grammar rule for recognition of the term.   As a preprocessing step we consider annotation with grammatical features and lemmatization of the text. The disambiguation rules exploit the local context in terms of grammatical features, semantic annotation and syntactic structure, and alsp the global context such as topic of the text, discourse segmentation, etc. Currently we have implemented chunk grammars for several languages.   The disambiguation rules are under development. For the implementation of the annotation grammar we rely on the grammar facilities of the CLaRK System (Simov et al, 2001). The structure of each grammar rule in CLaRK is defined by the following DTD fragment:  <!ELEMENT line (LC?, RE, RC?, RM, Comment?) > <!ELEMENT LC (#PCDATA)> <!ELEMENT RC (#PCDATA)> <!ELEMENT RE (#PCDATA)> <!ELEMENT RM (#PCDATA)> <!ELEMENT Comment (#PCDATA)>  Each rule is represented as a line element. The rule consists of regular expression (RE) and category (RM = return markup). The regular expression is evaluated over the content of a given XML element and could recognize tokens and/or annotated data. The return markup is represented as an XML fragment which is substituted for the recognized part of the content of the element.  Additionally, the user could use regular expressions to restrict the context in which the regular expression is evaluated successfully. The LC element contains a regular expression for the left context and the RC for the right one. The element Comment is for human use. The application of the grammar is governed by Xpath expressions which provide additional mechanism for accurate annotation of a given XML document.  
61
Thus, the CLaRK grammar is a good choice for implementation of the initial annotation grammar. The creation of the actual annotation grammars started with the terms in the lexicons for the corresponding languages. Each term was lemmatized and the lemmatized form of the term was converted into regular expression of grammar rules. Each concept related to the term is stored in the return markup of the corresponding rule. Thus, if a term is ambiguous, then the corresponding rule in the grammar contains reference to all concepts related to the term.  The relations between the different elements of the models are as follows. A lexical item could have more than one grammar rule associated to it depending on the word order and the grammatical realization of the lexical item. Two lexical items could share a grammar rule if they have the same wording, but they are connected to different concepts in the ontology. Each grammar rule could recognize zero or several text chunks.  The relation ontology-to-text implemented in this way is the basis fort the crosslingual search engine which works in the following way: Words in any of the covered languages can be entered and are looked up in the lexicon; the concepts that are linked to the matching lexicon entries are used for ontology-based search in an automatic fashion.   Before lexicon lookup, the words are orthographically normalised, and combinations for multi-word terms are created (e.g. if the words "text" and "editor" are entered, the combinations "texteditor", "text editor" and "text-editor" are created and looked up, in addition to the individual words ). For each of the found concepts, the set of all its (direct or indirect) subconcepts is determined, and is used to retrieve  Learning Objects (Los) .   The use of these language-independent concepts as an intermediate step makes it possible to retrieve LOs in any of the covered languages, thus realising the crosslingual aspect of the retrieval. When the found LOs are displayed, at the same time the relevant parts of the ontology are presented in the language that the user prefers. In a second step, the user can select (by marking a checkbox) the concept(s) he wants to look for and repeat the search. If an entered 
word was ambiguous, the intended meaning can be explicated now by selecting the appropriate concept. Furthermore, by clicking on a concept, related concepts are displayed; navigation through the ontology is possible in this way. A list of retrieval languages (only LOs written in one of those languages will be found) is specified as an input parameter. The retrieved LOs are  sorted by language. The next ordering criterion is a ranking, based on the number of different search concepts and the number of occurrences of those concepts in the LO. For each found LO, its title, language, and matching concepts are shown.  3 Crosslingual search based on machine Translation  The second case study is the embedding of a crosslingual search engine into a web-based content management system. The system is currently implemented within the EU-PSP project ATLAS (http://www.atlasproject .eu) and aims to be domain independent. Thus, a model as presented in section 2 is impossible to be realised, as the automatic construction of a domain ontology is too unreliable and the human construction too cost effective.  Also a general lexicon coverage is practically impossible.  Therefore in this project we adopted a different solution (Karagiozov et al2011), namely we ensure the translation of keywords and short generated abstracts, and all these translations are part of the RDF-generate index. The ATLAS system ensures the lingustic processing of uploaded documents and extraction of most important keywords. A separate module generates short abstracts. These two elements can be further submitted for translation.  For the MT-Engine of the ATLAS ?System on a hybrid architecture combining example (EBMT) and statistical (SMT) machine translation on surface forms (no syntactic trees will be used) is chosen. For the SMT-component PoS and domain factored models as in (Niehues and Waibel 2010) are used, in order to ensure domain adaptability.  An original approach of our system is the interaction of the MT-engine with other modules of the system:  The document categorization module assigns to each document one or more domains. For each 
62
domain the system administrator has the possibility to store information regarding the availability of a correspondent specific training corpus. If no specific trained model for the respective domain exists, the user is provided with a warning, telling that the translation may be inadequate with respect to the lexical coverage.  The output of the summarization module is processed in such way that ellipses and anaphora are omitted, and lexical material is adapted to the training corpus.  The information extraction module is providing information about metadata of the document including publication age. For documents previous to 1900 we will not provide translation, explaining the user that in absence of a training corpus the translation may be misleading. The domain and dating restrictions can be changed at any time by the system administrator when an adequate training model is provides.  The translation results are then embedded in a document model which is used further for crosslingual search. Each document is thus converted to the following format  <foaf:Document rdf:about=http://atlas.eu/item#20> <dc:title>Internet Ethics </dc:title> <dc:creator rdf:resource=http://atlas.eu/pers#950 /> <atlas:summary xmnls:lang=?en?>  Default english summary <atlas:summary> <atlas:summary xmnls:lang=?de?>  Deutsche Zusammenfassung </atlas:summary> </foaf:Document> <foaf:Personrdf:about=http://atlas.eu/pers#950> <foaf:name>Name </foaf:name> <foaf:mbox> name@some.address.eu </foaf:mbox> </foaf:Person>  
This ist he basis for creation of the RDF-Index. The crosslingual serch engine is in this case a classic Lucene search engine, which operates however not with word-indexes but with these RDF-indexes, which automatically include multilingual information. This engine is currently under construction. 4 Conclusions In this paper we presented two approaches of embedding multilingual information into search engines.   One is based on the construction of a language independent ontology and corresponding lexical material, the other one on machine translation.   The first approach relies on a manual constructed ontology, therefore it is highly domain dependent and requires the involvement of domain specialists.   The second approach relies on machine translation quality, and also lacks a deep semantic analysis of the query.   However the mechanism can be implemented completely automatically, and is domain independent (assuming that the machine translation engine contains domain adaptation models)  Therefore it is difficult to asses which approach performs better. Further work concerns the selection of a certain domain and comparison of retrieval quality fort the two approaches.  5 Acknowledgements The work described here was realized within two projects: LT4eL (http://www.lt4el.eu) and ATLAS (http://www.atlasproject.eu).   The author is indebted especially to following persons who contributed essentially for the fulfillment of the described modules: Kiril Simov and Petya Osenova (Bulgarian Academy of Sciences), Eelco Mosel (former at the university of Hamburg), Ronald Winnem?ller (University of Hamburg).   
63
References  Karagiozov, D.  Koeva,S. Ogrodniczuk, M. and Vertan, C.  ATLAS ? A Robust Multilingual Platform for the Web. In Proceedings of the German Society for Computational Linguistics and Language Technology Conference (GSCL 2011), Hamburg, Germany, 2011  Lemnitzer, L. and Vertan, C. and Simov, K. and Monachesi, P. and Kiling A. and Cristea D. and Evans, D., ?Improving the search for learning objects with keywords and ontologies?. In Proceedings of Technologically enhanced learning conference 2007, p. 202-216  Niehues J. and Waibel,A. Domain Adaptation in Statistical Machine Translation using Factored Translation Models, Proceedings of EAMT 2010 Saint-Raphael, 2010  Simov, K., Peev, Z., Kouylekov, M., Simov, A., Dimitrov, M., Kiryakov, A. (2001). CLaRK - an XML-based System for Corpora Development. In: Proc. of the Corpus Linguistics 2001 Conference. Lancaster, UK.  
64
Proceedings of the Workshop on Automatic Text Simplification: Methods and Applications in the Multilingual Society, pages 64?68,
Dublin, Ireland, August 24th 2014.
Making historical texts accessible to everybody 
Cristina Vertan University of Hamburg Vogt-K?lln Strasse 30 22529 Hamburg cristina.vertan@uni-hamburg.de 
Walther v. Hahn University of Hamburg Vogt-K?lln Strasse 30 22529 Hamburg vhahn@informatik.uni-hamburg.de  Abstract In this paper we discuss the degree of readability of historical texts for a broad public. We argue that text simplification methods can improve significantly this aspect and bring an added value to historical texts. We present a specific example, a genuine multilingual historical texts, which should be available at least to researchers from different fields and propose a mechanism for simplifying the text.? 1 Introduction During the last decade there was a massive digitization campaign, which lead to a large number of electronicly available collections of historical documents. Most of these collections offer the possibility to navigate through the documents and display not only the associated metadata but also content. Thus researchers and students in various fields, which are related to one document?s topic, may have access to it. This, however, is not a barrier-free access as many historical languages either differ significantly from their modern correspondent or they are not at all in use any longer. Thus only scholars can understand such texts with deep knowledge in the respective language(s). We use the plural form ?languages? as most historical texts are multilingual, being composed from a mixture of paragraphs in one main text language and one or e more secondary languages which were either linguae francae at the time when the document has been written (e.g. Latin, Ancient Greek, Arabic) or reflect cultural or geographical particularities of the topic being described (e.g. a Latin Document written about the organisation of the Turkish empire). Text simplification is a technique used up to now for making modern texts accessible to groups with special requirements (persons with disabilities, language learners). Simplification means a broad range of techniques from lexical replacements of less used terms by more frequent ones, through syntactic adaptation (substitution of relative clauses, elimination of long distance dependencies) up to reshaping on the discourse level (e.g. eliminating anaphora) (Saggion et. al. 2013), (Dornescu et. al. 2013). However, it is assumed that the users know orthography and morphology of the language. In this paper we argue that text simplification is also an adequate method for making historical texts understandable for a broader public and we describe first approaches with a genuine multilingual historical text.  The paper is organised as follows: in section 2 we introduce simplification requirements for historical texts and exemplify them by means of a particular scenario, which we will describe. Section 3 is dedicated to our approach towards text simplification. Finally in section 4 we present our first conclusions and further work to be done. 
                                                           
?This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0/ 
64
2 The Need of Text Simplification for historical Texts As we mentioned in section 1, historical texts must suffer a certain transformation in order to be understood by non-trained readers.  These transformations are language dependent and should satisfy two criteria: ? They should try to bring the text as close as possible to the modern language form (if available) ? They should preserve the cultural and geographical setting of the time when they were written. In the following paragraph we will consider texts (originals or historical translations) for which a modern variant of the language is still in use.  As an example we will discuss the works of Dimitrie Cantemir, political figure, philosopher, historian, musician, geographer, who lived at the end of XVIIth. century and prepared two important works for the history of Eastern Europe for the Royal Academy of Sciences in Berlin. The first one, ?Decriptio Moldaviae? (The Description of Moldavia) is - as the title suggests - a detailed presentation of his (Cantemir?s) native country Moldavia (spreading today from the eastern part of Romania to the current Republic of Moldavia). Cantemir describes the history of the country, as well as its geography, the language and the traditions of people living there. It includes also the first detailed map of the region. The work was written in Latin and translated into German and French at the beginning of the XVIIIth century, later into Romanian. The second work is ?The History of Growth and Decay of the Ottoman Empire?. This was again written in Latin and translated more or less immediately into German, French, English and Russian. It remained a reference work for studies of the Ottoman Empire until the middle of XIXth century. Both works are thus relevant for historians but also for ethnographers, linguists, as well as for people interested in the history of these three territories.  The fact that they were translated seems to make their reception easier.  However we will show through several examples that this is a false assumption. The following examples illustrate also the need of text simplification at four linguistic levels: orthography morphology, syntax and semantics. The following examples are extracted from the German translation from 1771 of ?Descriptio Moldaviae? (Cantemir 1771). They are, however illustrative for a wide range of historical texts in other language combinations. A more detailed description can be found in XXXXXXXX 2.1 2.1. Orthographic level In this text we encounter passages in German, Romanian, Latin and Ancient Greek.  German Text is written in black-letter typeface. Latin and part of Romanian words  (see below) are written with roman typeface. Greek paragraphs are easyly to detect and to isolate due to their specific alphabet.  Two approaches of the writer were identified when dealing with local (Romanian) names ? Named Entities (geographical names, person names) as well as names for specific roles in the army or society are written with black-letter typeface. They are adapted to the German pronunciation, like in the following examples: e.g. The river Prut became Pruth; The ruler Drago? became Dragosch, the role of being a ?pivnicier? (person responsible for keeping wine and goods in the basement of a castle) became ?pivnichar? ? Lexical items illustrating the language, remained in Latin font and were not adapted phonetically. However, as at Cantemir?s time Romanian language was written in Cyrillic alphabet, the Latin-alphabet transcription is deviant from the current Romanian orthography e.g. ?muiere? (colloquial term for woman) appears in text as ?mujere?. 2.2 Morphological and syntactic level Old morphological forms deviant from those used in current German are present throughout the book   e.g. ?zweyten? or ?Theil? instead of ?zweiten? and ?Teil? 
65
For any modern reader unknown named entities appear: e.g. ?in dem bergigten Theile von Moramor, (*)?. Even in the text the ?Moramor? region is not clearly identified and the text passage contains two footnotes ?(*)?, one from Cantemir himself and one from the German translator from 1771, both commenting the word Moramor. 2.3  Semantic level There are either words which still exist in the modern vocabulary but mostly used with a different meaning. An example is the word ?fl?chtigen? used in the XVIIIth century exclusively with the meaning of ?running away from somebody? whereas nowadays it is predominantly used with the meaning of ?volatile substance?. The main challenge here is that both meanings were and are still valid through the whole period from XVIIIth century until now, just the usage frequency of one or the other meaning changed.  Time references are often relative. In an expression like ?von dem heutigen Ungarn? (engl. ?from Hungary nowadays?) one should understand and interpret the temporal expression ?nowadays? as referring to the time when the text was written (even not: published). This also implies that the corresponding political or geographical unit, in this case ?Hungary? may have changed since that time. 2.4 Knowledge level Additionally at knowledge level one can observe a different conceptual representation. We present here just one relevant example: It refers to geographical units / population groups, which changed their denomination or may refer to different entities depending of the historical/geographical context. In the sentence ?Die auf der andern Seite angr?nzende Polen und Russen nennen die Moldauer Wolochen, d. i. W?lsche oder Itali?ner, die Walachen aber, die auf dem Gebirge wohnen, heissen sie die Berg-Walachen, oder die Leute jenseits des Gebirges  we find the term ?W?lsche?. In Central Germany up to the last century ?W?lsche? was the name for French, in Southern Germany for Italians and still today in Eastern Austria it is the name for Slovenians. Thus the term depends on the historical and geographical context and is not fixed to a specific  population. However, readers may be confused without this background knowledge. From the examples above it is clear that a non-trained reader (i.e. a reader being not familiar with Early New High German, Romanian and old terms in Romanian, Romanian geography and history) will have difficulties in reading and interpreting the text. We should mention here that there does not exist any modern German Edition of the text.  3 Text simplification for historical texts We argue that a process of text simplification should take place at all above-mentioned levels. Some parts (the orthographical and morphological/syntactical level can be done semi-automatically through a rule-based process. Prerequisite is that the text is digitized and the information concerning the typeface and the font is preserved. ? STEP 1: Within the black-letter typeface paragraphs o Match each word against a German language model, o If a word is not matched but there are candidates from which it differs by 1 or 2 words try to apply normalization rules like (ey ? ei, ev ? eu), o In contrary match the word against a Romanian language model and try the same with a set of Romanian normalization rules. Words which could not be matched should be rendered to the user and proposed for manual correction. ? STEP 2: Within the Latin-typeface phrases o Match words against a Latin language model and a Romanian one 
66
o Word which could be found in both should be rendered for manual annotation and language disambiguation, o For words not found in the Latin model but with some close variants in the Romanian language model try to apply a Romanian normalization rule.  The output of Steps 1 and 2 will be a normalized text in with language is identified and marked for all paragraphs.  The paragraphs marked by ?Romanian? have to be manually translated, i.e. explained to the reader in German or English. Additional annotation is necessary to enable text processing for text simplification at upper levels. We propose an annotation scheme, aiming not only at marking words which could not be corrected throughout the normalization process, but enhancing also the meaning of the word (Vertan and v. Hahn 2014) The main unit of the annotation is called ?phrase?. By phrase we mean a word or a multi-word expression. For each phrase the semantic frame includes information about the named entity (if any) as well as the obsolete meaning and the modern meaning of the word. In figure 1 we present the structure of the annotation, while in figure 2 we present an example of an annotation as well as the possible linkage to a domain ontology  
   Figure 1 Structure of the annotation scheme  Following this annotation step, a replacement of each annotated phrase with its modern form or in case of Romanian or Latin words with its translation will be obtained. Our first attempts in applying a rule-based constraint dependency parser  (Beuck et. al. 2011) on such text were successful but this needs deeper investigation. The dependency parser can be used for identifying relative clauses and proposes candidates for further simplification.  
67
 Figure 2 Example of annotation scheme and ontology linking  4 Conclusions and further work In this paper we showed that text simplification is a useful technique for making historical texts understandable for modern readers.  We identified particularities of historical texts, which need special attention and pre-processing. In the second step we are able to apply state-of-the-art methods for text simplification. We propose an algorithm dealing with multilingual entries for text normalization.  Currently we are annotating manually the words rendered by the normalization process. Further work is planned for the application of the WCDG parser on the normalized text and selection of relative clauses, which can be either deleted or transformed into a main clause, in order to make sentences shorter and clearer. We intend also to exploit the existence of a comparable corpus containing translations of the same text in five languages (Vertan 2014). References Niels Beuck, Arne K?hn, and Wolfgang Menzel. Incremental parsing and the evaluation of partial dependency analyses In DepLing 2011, Proceedings of the 1st International Conference on Dependency Linguistics, 2011.  Cantemir Dimitire Beschreibung der Moldau. Faksimildruck der Original Ausgabe von 1771, Maciuca C. (Ed). , Bukarest, Kriterion Verlag, 1973 Iustin Dornescu, Richard Evans and Constantin Orasan, A tagging Approach to Identify Complex Constitutents for Text Simplification, in Proceedings of Recent Advances in Natural Language Processing (RANLP 2013), Hisar, Bulgaria, pp.221-229 Horacio Saggion, Elena Gomez-Martinez, Alberto Anula, Lorena Bourg, Esteban Etayo, Text Simplification in Simplext: Making texts more Accessible, last retrieved at www.simplext.es Cristina Vertan and Walther v. Hahn, Discovering and Explaining Knowledge in historical Documents, Proceedings of the Workshop on ?Language Technology for Historical Languages and Newspaper Archives?, Kristin Bjnadottir, Stewen Krauwer, Cristina Vertan and Martin Wyne Eds., Workshop associated with LREC 2014, Rejkyavik Mai 2014,  pages 76-79 Cristina Vertan, Less explored multilingual issues in the automatic processing of historical texts ? a case study, Proceedings of the Digital Humanites Conference 2014, Lausanne, pages 406-407 
68
