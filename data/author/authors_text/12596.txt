Coling 2008: Proceedings of the workshop on Knowledge and Reasoning for Answering Questions, pages 1?8
Manchester, August 2008
Semantic Chunk Annotation for complex questions using Conditional 
Random Field 
Shixi Fan 
Department of computer science  
Harbin Institute of Technology 
Shenzhen Graduate School, 
Shenzhen,518055, china 
fanshixi@hit.edu.cn 
Yaoyun Zhang 
Department of computer science  
Harbin Institute of Technology 
Shenzhen Graduate School, 
Shenzhen,518055, china 
Xiaoni5122@gmail.com 
Wing W. Y. Ng 
Department of computer science  
Harbin Institute of Technology 
Shenzhen Graduate School, 
Shenzhen,518055, china 
wing@hitsz.edu.cn 
Xuan Wang 
Department of computer science  
Harbin Institute of Technology 
Shenzhen Graduate School, 
Shenzhen,518055, china 
wangxuan@insun.hit.edu.cn 
Xiaolong Wang 
Department of computer science  
Harbin Institute of Technology 
Shenzhen Graduate School, 
Shenzhen,518055, china 
wangxl@insun.hit.edu.cn 
 
 
Abstract 
This paper presents a CRF (Conditional 
Random Field) model for Semantic 
Chunk Annotation in a Chinese Question 
and Answering System (SCACQA). The 
model was derived from a corpus of real 
world questions, which are collected 
from some discussion groups on the 
Internet. The questions are supposed to 
be answered by other people, so some of 
the questions are very complex. Mutual 
information was adopted for feature se-
lection.  The training data collection con-
sists of 14000 sentences and the testing 
data collection consists of 4000 sentences. 
The result shows an F-score of 93.07%. 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
1 Introduction 
1.1 Introduction of Q&A System 
Automated question answering has been a hot 
topic of research and development since the ear-
liest AI applications (A.M. Turing, 1950). Since 
then there has been a continual interest in proc-
essing knowledge and retrieving it efficiently to 
users automatically. The end of the 1980s saw a 
boost in information retrieval technologies and 
applications, with an unprecedented growth in 
the amount of digital information available, an 
explosion of growth in the use of computers for 
communications, and the increasing number of 
users that have access to all this information 
(Diego Moll?and Jose?Luis Vicedo, 2007).  
Search engines such as Google, Yahoo, Baidu 
and etc have made a great success for people?s 
information need. 
Anyhow, search engines are keywords-based 
which can only return links of relevant web 
pages, failing to provide a friendly user-interface 
with queries expressed in natural language sen-
tences or questions, or to return precise answers 
to users. Especially from the end of the 1990s, as 
1
information retrieval technologies and method-
ologies became mature and grew more slowly in 
pace, automated question answering(Q&A) sys-
tems which accept questions in free natural lan-
guage formations and return exactly the answer 
or a short paragraph containing relevant informa-
tion has become an urgent necessity. Major in-
ternational evaluations such as TREC, CLEF and 
NTCIR have attracted the participation of many 
powerful systems.  
The architecture of a Q&A system generally in-
cludes three modules: question processing, can-
didate answer/document retrieval, and answer 
extraction and re-ranking.      
1.2 Introduction of Question Analyzing      
Question Analyzing, as the premise and founda-
tion of the latter two modules, is of paramount 
importance to the integrated performance of a 
Q&A system. The reason is quite intuitive: a 
question contains all the information to retrieve 
the corresponding answer. Misinterpretation or 
too much loss of information during the process-
ing will inevitably lead to poor precision of the 
system. 
The early research efforts and evaluations in 
Q&A were focused mainly on factoid questions 
asking for named entities, such as time, numbers, 
and locations and so on. The questions in the test 
corpus of TREC and other organizations are also 
in short and simple form. Complex hierarchy in 
question types (Dragomir Radev et al 2001), 
question templates (Min-Yuh Day et al 2005), 
question parsing (Ulf Hermjakob, 2001) and 
various machine learning methods (Dell Zhang 
and Wee Sun Lee, 2003)are used for factoid 
question analysis, aiming to find what named 
entity is asked in the question. There are some 
questions which are very complicated or even 
need domain restricted knowledge and reasoning 
technique. Automatic Q&A system can not deal 
with such questions with current technique.    
In china, there is a new kind of web based Q&A 
system which is a special kind of discussion 
group. Unlike common discussion group, in the 
web based Q&A system one user posts a ques-
tion, other users can give answers to it. It is 
found that at least 50% percent questions 
(Valentin Jijkoun and Maarten de Rijke, 
2005)posted by users are non-factoid and surely 
more complicated both in question pattern and 
information need than those questions in the test 
set of TREC and other FAQ.  An example is as 
follows: 
 
This kind of Q&A system can complement the 
search engines effectively.  As the best search 
engines in china, Baidu open the Baidu Knowl-
edge2 Q&A system from 2003, and now it has 
more than 29 million question-answer pairs. 
There are also many other systems of this kind 
such as Google Groups, Yahoo Answers and 
Sina Knowledge3. This kind of system is a big 
question-answer pair database which can be 
treated as a FAQ database. How to search from 
the database and how to analyze the questions in 
the database needs new methods and techniques.   
More deeper and precise capture of the semantics 
in those complex questions is required. This phe-
nomenon has also been noticed by some re-
searchers and organizations. The spotlight gradu-
ally shifted to the processing and semantic un-
derstanding of complex questions. From 2006, 
TREC launched a new annually evaluation 
CIQ&A (complex, interactive Question Answer-
ing), aiming to promote the development of in-
teractive systems capable of addressing complex 
information needs. The targets of national pro-
grams AQUAINT and QUETAL are all at new 
interface and new enhancements to current state-
of-the-art Q&A systems to handle more complex 
inputs and situations. 
A few researchers and institutions serve as pio-
neers in complex questions study. Different tech-
nologies, such as definitions of different sets of 
question types, templates and sentence patterns 
(Noriko Tomuro, 2003) (Hyo-Jung Oh et al 
2005) machine learning methods (Radu Soricut 
and Eric Brill, 2004), language translation model 
(Jiwoon Jeon, W et al 2005), composition of 
information needs of the complex question 
(Sanda Harabagiu et al 2006) and so on, have 
been experimented on the processing of complex 
question, gearing the acquired information to the 
facility of other Q&A modules.  
Several major problems faced now by researcher 
of complex questions are stated as follow:  
First: Unlike factoid questions, it is very dif-
ficult to define a comprehensive type hierarchy 
for complex questions. Different domains under 
research may require definitions of different sets 
of question types, as shown in (Hyo-Jung Oh et 
al, 2005). Especially, the types of certain ques-
                                                 
2 http://zhidao.baidu.com/ 
3 http://iask.sina.com.cn/ 
2
tions are ambiguous and hard to identify. For 
example: 
 
This question type can be treated as definition, 
procedure or entity. 
Second: Lack of recognition of different seman-
tic chunks and the relations between them. 
FAQFinder (Radu Soricut and Eric Brill, 2004) 
also used semantic measure to credit the similar-
ity between different questions. Nevertheless, the 
question similarity is only a simple summation of 
the semantic similarity between words from the 
two question sentences. Question pattern are very 
useful and easy to implement, as justified by pre-
vious work. However, just like the problem with 
question types, question patterns have limitation 
on the coverage of all the variations of complex 
question formation. Currently, after the question 
processing step in most systems, the semantic 
meaning of large part of complex questions still 
remain vague. Besides, confining user?s input 
only within the selection of provided pattern may 
lead to unfriendly and unwelcome user interface. 
(Ingrid Zukerman and Eric Horvitz, 2001) used 
decision tree to model and recognize the infor-
mation need, question and answer coverage, 
topic, focus and restrictions of a question. Al-
though features employed in the experiments 
were described in detail, no selection process of 
those feature, or comparison between them was 
mentioned. 
This paper presents a general method for Chinese 
question analyzing. Our goal is to annotate the 
semantic chunks for the question automatically.  
2 Semantic Chunk Annotation 
Chinese language differs a lot from English in 
many aspects. Mature methodologies and fea-
tures well-justified in English Q&A systems are 
valuable sources of reference, but no direct copy 
is possible.  
The Ask-Answer system 4  is a Chinese online 
Q&A system where people can ask and answer 
questions like other web based Q&A system. The 
characteristic of this system is that it can give the 
answer automatically by searching from the 
asked question database when a new question is 
presented by people. The architecture of the 
automatically answer system is shown in figure 1.  
The system contains a list of question-answer 
pairs on particular subject. When users input a 
                                                 
 
 
 
4 http://haitianyuan.com/qa 
question from the web pages, the question is 
submitted to the system and then question-
answer pair is returned by searching from the 
questions asked before. The system includes four 
main parts: question pre-processing, question 
analyzing, searching and answer getting.  
The question pre-processing part will segment 
the input questions into words, label POS tags 
for every word.  Sometimes people ask two or 
more questions at one time, the questions should 
be made into simple forms by conjunctive struc-
ture detection. The question analyzing program 
will find out the question type, topic, focus and 
etc. The answer getting part will get the answer 
by computing the similarity between the input 
question and the questions asked before. The 
question analyzing part annotates the semantic 
chunks for the question. So that the question can 
be mapped into semantic space and the question 
similarity can be computed semantically. The 
Semantic chunk annotation is the most important 
part of the system. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Question Pre- processing 
 
Segmentation and  pos 
tagging 
Detect conjunctive structure  
Question Analyzing 
Semantic chunk annotationGet and extend key words
Question pattern and knowledge base 
Search reference question-answer pairs form database 
Answer getting 
Score the constituent 
answers 
Out put the top 
five answers 
 
Figure 1 the architecture of the automatically 
answer system 
Currently, no work has been reported yet on the 
question semantic chunk annotation in Chinese. 
The prosperity of major on-line discussion 
groups provides an abundant ready corpus for 
question answering research. Using questions 
collected from on-line discussion groups; we 
make a deep research on semantic meanings and 
build a question semantic chunk annotation 
model based on Conditional Random Field. 
Five types of semantic chunks were defined: 
Topic, Focus, Restriction, Rubbish information 
and Interrogative information. The topic of a 
3
question which is the topic or subject asked is the 
most important semantic chunk. The focus of a 
question is the asking point of the question. The 
restriction information can restrict the question?s 
information need and the answers. The rubbish 
information is those words in the question that 
has no semantic meanings for the question. Inter-
rogative information is a semantic tag set which 
corresponds to the question type. The interroga-
tive information includes interrogative words, 
some special verbs and nouns words and all these 
words together determine the question type. The 
semantic chunk information is shown in table 1.  
 
Semantic 
chunk   tag 
Abbreviation Meaning 
Topic T The question subject 
Focus F The additional information 
of topic 
Restrict 
 
Re Such as Time restriction and 
location restriction 
Rubbish 
information 
Ru Words no meaning for the 
question 
Other O other information without 
semantic meaning 
The following is interrogative information 
Quantity Wqua  
Description Wdes The answer need description
Yes/No Wyes The answer should be yes or 
no 
List Wlis The answer should be a list 
of entity 
Definition Wdef The answer is the definition 
of topic 
Location Wloc The answer is location 
Reason Wrea The answer can explain the 
question 
Contrast Wcon The answer is the compari-
son of the items proposed in 
the question 
People Wwho The answer is about the 
people?s information 
Choice Wcho The answer is one of the 
choice proposed in the ques-
tion 
Time Wtim The answer is the data or 
time length about the event 
in the question 
Entity Went The answer is the attribute 
of the topic. 
Table 1: Semantic chunks  
An annotation example question is as follows: 
 
This question can be annotated as follows: 
 
This kind of annotation is not convenient for CRF 
model, so the tags were transfer into the B I O 
form. (Shown as follows) 
 
Then the Semantic chunk annotation can be 
treated as a sequence tag problem.  
3 Semantic Chunk Annotation model 
3.1 Overview of the CRF model 
The conditional random field (CRF) is a dis-
criminative probabilistic model proposed by John 
Lafferty, et al(2001) to overcome the long-range 
dependencies problems associated with genera-
tive models. CRF was originally designed to la-
bel and segment sequences of observations, but 
can be used more generally. Let X, Y be random 
variables over observed data sequences and cor-
responding label sequences, respectively. For 
simplicity of descriptions, we assume that the 
random variable sequences X and Y have the 
same length, and use [ ]mxxxx ......, 21=   
and [ ]myyyy ......, 21=  to represent instances of 
X and Y, respectively. CRF defines the condi-
tional probability distribution P(Y |X) of label 
sequences given observation sequences as fol-
lows 
)),(exp(
)(
1
)|(
1
?
=
=
n
i
ii YXfXZ
XYP ?
?
?    (1) 
Where  is the normalizing factor that 
ensures equation 2. 
)(XZ?
 ? =y xyP 1)|(?                   (2) 
In equation 2 the i? is a model parameter and 
 is a feature function (often binary-
valued) that becomes positive (one for binary-
valued feature function) when X contains a cer-
tain feature in a certain position and Y takes a 
certain label, and becomes zero otherwise. 
Unlike Maximum Entropy model which use sin-
gle normalization constant to yield a joint distri-
bution, CRFs use the observation-dependent 
normalization  for conditional distribu-
tions. So CRFs can avoid the label biased prob-
lem. Given a set of training data 
),( YXfi
)(XZ?
}....2,1),,{( nkyxT kk ==  
 With an empirical distribution , CRF ),(
~
YXP
4
determines the model parameters }{ i?? =  by 
maximizing the log-likelihood of the training set 
)|(log),(
)|(log)(
,
~
1
xyPyxP
xyPP
yx
N
k
kk
?
??
?
?
?
=?
=                       (3) 
3.2 Features for the model 
The following features, which are used for train-
ing the CRF model, are selected according to the 
empirical observation and some semantic mean-
ings. These features are listed in the following 
table. 
 
Feature type in-
dex 
Feature type name 
1 Current word 
2 Current POS tag 
3 Pre-1 word POS tag 
4 Pre-2 word POS tag 
5 Post -1 word POS tag 
6 Post -2 word POS tag 
7 Question pattern 
8 Question type 
9 Is pattern key word 
10 Pattern tag 
Table 2: the Features for the model 
Current word: 
The current word should be considered when 
adding semantic tag for it. But there are too 
many words in Chinese language and only part 
of them will contribute to the performance, a set 
of words was selected. The word set includes 
segment note and some key words such as time 
key word and rubbish key word. When the cur-
rent word is in the word set the current word fea-
ture is the current word itself, and null on the 
other hand. 
Current POS tag: 
Current POS tag is the part of speech tag for the 
current word. 
Pre-1 word POS tag: 
Pre- 1 word POS tag is the POS tag of the first 
word before the labeling word in the sentence. If 
the Pre-1 word does not exit (the current is the 
first word in the sentence), the Pre- 1 word POS 
tag is set to null. 
Pre-2 word POS tag: 
Pre- 2 word POS tag is the POS tag of the second 
word before the labeling word in the sentence. If 
the Pre-2 word does not exit, the Pre- 2 word 
POS tag is set to null. 
Post -1 word POS tag: 
Post - 1 word POS tag is the POS tag of the first 
word after the labeling word in the sentence. If 
the Post -1 word does not exit (the current is the 
first word in the sentence), the Post - 1 word POS 
tag is set to null. 
Post -2 word POS tag: 
Post - 2 word POS tag is the POS tag of the sec-
ond word after the labeling word in the sentence. 
If the Post-2 word does not exit, the Pre- 2 word 
POS tag is set to null. 
Question pattern: 
Question pattern which is associated with ques-
tion type, can locate question topic, question fo-
cus by surface string matching. For example, 
(where is <topic>). The patterns are extracted 
from the training data automatically. When a pat-
tern is matched, it is treated as a feature. There 
are 1083 question patterns collected manually.  
Question type: 
Question type is an important feature for ques-
tion analyzing. The question patterns have the 
ability of deciding the question type. If there is 
no question pattern matching the question, the 
question type is defined by a decision tree algo-
rithm. 
Is pattern key word: 
For each question pattern, there are some key 
words. When the current word belongs to the 
pattern key word this feature is set to ?yes?, else 
it is set to ?no?. 
Pattern tag: 
When a pattern is matched, the topic, focus and 
restriction can be identified by the pattern. We 
can give out the tags for the question and the tags 
are treated as features. If there is no pattern is 
matched, the feature is set to null.   
4 Feature Selection experiment 
Feature selection is important in classifying sys-
tems such as neural networks (NNs), Maximum 
Entropy, Conditional Random Field and etc. The 
problem of feature selection has been tackled by 
many researchers. Principal component analysis 
(PCA) method and Rough Set Method are often 
used for feature selection. Recent years, mutual 
information has received more attention for fea-
ture selection problem.  
According to the information theory, the uncer-
tainty of a random variable X can be measured 
by its entropy . For a classifying problem, 
there are class label set represented by C and fea-
ture set represented by F. The conditional en-
tropy  measures the uncertainty about 
)(XH
)|( FCH
5
C when F is known, and the Mutual information 
I(C, F) is defined as:  
 F)|(C -(C));( HHFCI =                   (4) 
The feature set is known; so that the objective of 
training the model is to minimize the conditional 
entropy   equally maximize the mutual 
information . In the feature set F, some 
features are irrelevant or redundant. So that the 
goal of a feature selection problem is to find a 
feature S ( ), which achieve the higher 
values of . The set S is a subset of F and 
its size should be as small as possible. There are 
some algorithms for feature selection problem. 
The ideal greedy selection algorithm using mu-
tual information is realized as follows (Nojun 
Kwak and Chong-Ho Choi, 2002): 
)|( FCH
);( FCI
FS ?
);( FCI
 Input:   S- an empty set 
             F- The selected feature set 
Output:  a small reduced feature set S which is 
equivalent to F 
Step 1: calculate the MI with the Class 
set C , , compute  Ffi ?? );( ifCI
Step 2: select the feature that maximizes , 
set  
);( ifCI
}{},{\ ii fSfFF ??
Step 3: repeat until desired number of features 
are selected. 
1) Calculate the MI with the Class set C and S, 
Ffi ?? , compute  ),;( ifSCI
2) Select the feature that maximizes , 
set 
),;( ifSCI
}{},{\ ii fSfFF ??  
Step 4: Output the set S  that contains the se-
lected features 
To calculate MI the PDFs (Probability Distribu-
tion Functions) are required. When features and 
classing types are dispersing, the probability can 
be calculated statistically.  In our system, the 
PDFs are got from the training corpus statistically. 
The training corpus contains 14000 sentences. 
The training corpus was divided into 10 parts, 
with each part 1400 sentences.  And each part is 
divided into working set and checking set. The 
working set, which contains 90% percent data, 
was used to select feature by MI algorithm. The 
checking set, which contains 10% percent data, 
was used to test the performance of the selected 
feature sequence. When the feature sequence was 
selected by the MI algorithm, a sequence of CRF 
models was trained by adding one feature at each 
time. The checking data was used to test the per-
formance of these models.  
 The open test result 
Selected feature 
sequence 
1 2 3 4 5 6 7 8 9 10 
7, 10, 3, 1, 5, 2, 
4, 6, 8?9 
0.5104 0.8764 0.8864 0.8918 0.8925 0.8977 0.8992 0.9023 0.9025 0.9018 
7, 10, 1, 3, 5, 2, 
4?6?8?9 
0.5241 0.8775 0.8822 0.8911 0.8926 0.8956 0.8967 0.9010 0.9005 0.9007 
7, 10, 1, 3, 5, 2, 
4, 6?8?9 
0.5090 0.8691 0.8748 0.8851 0.8852 0.8914 0.8929 0.8955 0.8955 0.8949 
7, 10, 1, 3, 5, 2, 
4, 6?9?8 
0.5157 0.8769 0.8823 0.8913 0.8925 0.8978 0.8985 0.9017 0.9018 0.9010 
7, 10, 1, 3, 5, 2, 
4, 6?8?9 
0.5144 0.8821 0.8856 0.8921 0.8931 0.8972 0.8981 0.9010 0.9009 0.9007 
7, 10, 3, 1, 5, 2, 
4?6?8?9 
0.5086 0.8795 0.8876 0.8914 0.8919 0.8960 0.8967 0.9016 0.9013 0.9011 
7, 10, 1, 3, 5, 2, 
4, 6, 8, 9 
0.5202 0.8811 0.8850 0.8920 0.8931 0.8977 0.8980 0.9015 0.9013 0.9009 
7, 10, 1, 3, 5, 2, 
4, 6?8?9 
0.5015 0.8858 0.8879 0.8948 0.8942 0.8998 0.8992 0.9033 0.9027 0.9023 
7, 10, 1, 3, 5, 2, 
4, 6?8?9 
0.5179 0.8806 0.8805 0.8898 0.8908 0.8954 0.8958 0.8982 0.8982 0.8986 
7, 10, 1, 3, 5, 2, 
4, 6, 8?9 
0.5153 0.8921 0.8931 0.9006 0.9012 0.9041 0.9039 0.9071 0.9068 0.9067 
Table 3: the feature selection result and the test result 
In table 3, each row contains data corresponding 
to one part of the training corpus so there are ten 
rows with data in the table. The third row corre-
sponds to the first part and the last row corre-
sponds to the tenth part. There are eleven col-
umns in the table, the first columns is the fea-
tures sequence selected by the mutual informa-
tion algorithm for each part. The second column 
is the open test result with the first feature in the 
feature sequence. The third column is the open 
test result with the first two features in the fea-
ture sequence and so on. From the table, it is 
6
clear that the feature 7(Question pattern) and 
10(Pattern tag) are very important, while the fea-
ture 8(Question type) and 9(Is pattern key word) 
are not necessary. The explanation about this 
phenomenon is that the ?pattern key word? and 
?Question type? information can be covered by 
the Question patterns. So feature 8 and 9 are not 
used in the Conditional Random Field model. 
5 Semantic Chunk Annotation Experi-
ment 
The test and training data used in our system are 
collected from the website (Baidu knowledge 
and the Ask-Answer system), where people pro-
posed questions and answers. The training data 
consists of 14000 and the test data consists of 
4000 sentences. The data set consists of word 
tokens, POS and semantic chunk tags. The POS 
and semantic tags are assigned to each word to-
kens.  
The performance is measured with three rates: 
precision (Pre), recall (Rec) and F-score (F1). 
Pre = Match/Model                     (5) 
Rec=Match/Manual                    (6) 
F1=2*Pre*Rec/(Pre+Rec)              (7) 
Match is the count of the tags that was predicted 
right. Model is the count of the tags that was pre-
dicted by the model. Manual is the count of the 
tags that was labeled manually. 
Table 4 shows the performance of annotation of 
different semantic chunk types. The first column 
is the semantic chunk tag. The last three columns 
are precision, recall and F1 value of the semantic 
chunk performance, respectively.   
 
Label Manual Model Match Pre.() Rec.() F1 
B-T?I-T 17061?78462 16327?80488 14825?76461 90.80?95.00 86.89?97.45 88.80?96.21 
B-F?I-F 5072?13029 5079?13583 4657?12259 91.69?90.25 91.82?94.09 91.75?92.13 
B-Ru?I-Ru 775?30 11?0 2?0 18.18?0.00 0.26?0.00 0.51?0.00 
O 8354 8459 6676 78.92 79.91 79.41 
B-Wqua?I-Wqua 1363?934 1327?1028 1298?881 97.81?85.70 95.23?94.33 96.51?89.81 
B-Wyes?I-Wyes 5669?1162 5702?1098 5550?1083 97.33?98.63 97.90?93.20 97.62?95.84 
B-Wdes?I-Wdes 2907?278 2855?185 2779?184 97.34?99.46 95.60?66.19 96.46?79.48 
B-Wlis?I-Wlis 603?257 563?248 560?248 99.47?100 92.87?96.50 96.05?98.22 
B-Wdef?I-Wdef 1420?1813 1430?1878 1280?1695 89.51?90.26 90.14?93.49 89.82?91.85 
B-Wloc?I-Wloc 683?431 665?395 661?392 99.40?99.24 96.78?90.95 98.07?94.92 
B-Wrea?I-Wrea 902?159 873?83 843?82 96.56?98.80 93.46?51.57 94.99?67.77 
B-Wcon?I-Wcon 552?317 515?344 503?291 97.67?84.59 91.12?91.80 94.28?88.05 
B-Wwho?I-Wwho 420?364 357?350 348?336 97.48?96.00 82.86?92.31 89.58?94.12 
B-Wcho?I-Wcho 857?85 738?0 686?0 92.95?0.00 80.05?0.00 86.02?0.00 
B-Wtim?I-Wtim 408?427 401?419 355?380 88.53?90.69 87.01?88.99 87.76?89.83 
B-Went?I-Went 284?150 95?81 93?80 97.89?98.77 32.75?53.33 49.08?69.26 
Avg 145577 145577 135488 93.07 93.07 93.07 
Table 4: the performance of different semantic chunk
 
The semantic chunk type of ?Topic? and ?Focus? 
can be annotated well. Topic and focus semantic 
chunks have a large percentage in all the seman-
tic chunks and they are important for question 
analyzing. So the result is really good for the 
whole Q&A system. 
As for ?Rubbish? semantic chunk, it only has 
0.51 and 0.0 F1 measure for B-Ru and I-Ru. One 
reason is lacking enough training examples, for 
there are only 1031 occurrences in the training 
data. Another reason is sometimes restriction is 
complex. 
6 Conclusion and future work 
This paper present a new method for Chinese 
question analyzing based on CRF. The features 
are selected by using mutual information algo-
rithm. The selected features work effectively for 
the CRF model. The experiments on the test data 
set achieve 93.07% in F1 measure. In the future, 
new features should be discovered and new 
methods will be used.  
Acknowledgment  
This work is supported by Major Program of Na-
tional Natural Science Foundation of China 
(No.60435020 and No. 90612005) and the High 
Technology Research and Development Program 
of China (2006AA01Z197). 
 
References 
A.M. Turing. 1950. Computing Machinery and 
Intelligence. Mind, 236 (59): 433~460. 
Diego Moll?, Jose?Luis Vicedo. 2007. Question 
Answering in Restricted Domains: An Overview. 
Computational Linguistics, 33(1),  
7
Dragomir Radev, WeiGuo Fan, Leila Kosseim. 2001. 
The QUANTUM Question Answering System. 
TREC. 
Min-Yuh Day, Cheng-Wei Lee, Shih-Hung WU, 
Chormg-Shyong Ong,  Wen-Lian Hsu. 2005. An 
Integrated Knowledge-based and Machine 
Learning Approach for Chinese Question 
Classification. Proceedings of the IEEE 
International Conference on Natural Language 
Processing and Knowledge Engineering, Wuhan, 
China,:620~625. 
Ulf Hermjakob. 2001. Parsing and Question 
Classification for Question Answering.  
Proceedings of the ACL Workshop on Open-
Domain Question Answering, Toulouse,:19~25. 
Dell Zhang, Wee Sun Lee. 2003. Question 
classification using support vector machines. 
Proceedings of the 26th Annual International ACM 
Conference on Research and Development in 
Information Retrieval(SIGIR), Toronto, Canada,26 
~ 32. 
Valentin Jijkoun, Maarten de Rijke.2005. Retrieving 
Answers from Frequently Asked Questions Pages 
on the Web. CIKM?05, Bermen, Germany. 
Noriko Tomuro. 2003. Interrogative Reformulation 
Patterns and Acquisition of Question Paraphrases. 
Proceeding of the Second International Workshop 
on Paraphrasing, :33~40. 
Hyo-Jung Oh, Chung-Hee Lee, Hyeon-Jin Kim, 
Myung-Gil Jang. 2005. Descriptive Question 
Answering in Encyclopedia. Proceedings of the 
ACL Interactive Poster and Demonstration Sessions, 
pages 21?24, Ann Arbor. 
Radu Soricut, Eric Brill. 2004, Automatic Question 
Answering: Beyond the Factoid.  Proceedings of 
HLT-NAACL ,:57~64. 
Jiwoon Jeon, W. Bruce Croft and Joon Ho Lee. 2005. 
Finding Similar Questions in Large Question and 
Answer Archives. CIKM?05, Bremen, Germany. 
Sanda Harabagiu, Finley Lacatusu and Andrew Hickl. 
2006 . Answering Complex Questions with Random 
Walk Models. SIGIR?06, Seattle, Washington, 
USA.pp220-227. 
Ingrid Zukerman, Eric Horvitz. 2001. Using Machine 
Learning Techniques to Interpret WH-questions. 
ACL. 
John Lafferty, Andrew McCallum, Fernando Pereira. 
2001. Conditional Random Fields: probabilistic 
Models for Segmenting and Labeling Sequence 
Data. Proceedings of the Eighteenth International 
Conference on Machine Learning, p.282-289. 
Nojun Kwak and Chong-Ho Choi. 2002. Input 
feature selection for classification problems. 
IEEE Trans on Neural Networks,,13(1):143-
159 
 
8
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 218?222
Manchester, August 2008
Discriminative Learning of Syntactic and Semantic Dependencies
Lu Li
1
, Shixi Fan
2
, Xuan Wang
1
, Xiaolong Wang
1
Shenzhen Graduate School, Harbin Institute of Technology,
Xili, Shenzhen 518055, China
1
{lli,wangxuan,wangxl}@insun.hit.edu.cn
2
fanshixi@hit.edu.cn
Abstract
A Maximum Entropy Model based system
for discriminative learning of syntactic and
semantic dependencies submitted to the
CoNLL-2008 shared task (Surdeanu, et al,
2008) is presented in this paper. The sys-
tem converts the dependency learning task
to classification issues and reconstructs the
dependent relations based on classification
results. Finally F1 scores of 86.69, 69.95
and 78.35 are obtained for syntactic depen-
dencies, semantic dependencies and the
whole system respectively in closed chal-
lenge. For open challenge the correspond-
ing F1 scores are 86.69, 68.99 and 77.84.
1 Introduction
Given sentences and corresponding part-of-speech
of each word, the learning of syntactic and seman-
tic dependency contains two separable goals: (1)
building a dependency tree that defines the syn-
tactic dependency relationships between separated
words; (2) specifying predicates (no matter verbs
or nouns) of the sentences and labeling the seman-
tic dependents for each predicate.
In this paper a discriminative parser is pro-
posed to implement maximum entropy (ME) mod-
els (Berger, et al, 1996) to address the learning
task. The system is divided into two main subsys-
tems: syntactic dependency parsing and semantic
dependency labeling. The former is used to find a
well-formed syntactic dependency tree that occu-
pies all the words in the sentence. If edges are
added between any two words, a full-connected
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
graph is constructed and the dependency tree could
be found using a maximum spanning tree (MST)
algorithm (McDonald, et al, 2005). The latter fo-
cuses on separable predicates whose semantic de-
pendents could be determined using classification
tools, such as ME models
1
etc..
We participated in both closed and open chal-
lenge of the CoNLL-2008 shared task (Surdeanu,
et al, 2008). Results are reported on both develop-
ment and test sets in this paper.
2 System Description
2.1 Syntactic Parsing
The goal of syntactic parsing is to create a la-
beled syntactic dependency parse y for input sen-
tence x including words and their parts of speech
(POS). Inspired by the parsing model that imple-
ments maximum spanning tree (MST) algorithm
to induce the dependency parsing tree (McDonald,
et al, 2005), the system employs the same frame-
work. The incorporated features are defined over
parts of speech of words occurring between and
around a possible head-dependent relation.
Suppose G = (V, E) is a directed graph, where
V is the set of vertices denoting the words in sen-
tence x and E is the set of directed edges between
any two vertices with some scores. The MST al-
gorithm is to find the most probable subgraph of G
that satisfies tree constraints over all vertices. The
score function of the parsing tree y is defined as
s(y) =
?
(i,j)?y
s(i, j) (1)
where (i, j) ? y indicates an edge in y from word
i to word j and s(i, j) denotes its score. Suppose Y
1
http://homepages.inf.ed.ac.uk/s0450736/maxent.html
218
wi
w
j
p
i
p
j
(w
i
, p
i
) (w
j
, p
j
)
(w
i
, w
j
) (p
i
, p
j
)
(w
i
, p
j
) (w
j
, p
i
)
(w
i
, w
j
, p
i
) (w
i
, w
j
, p
j
)
(p
i
, p
j
, w
i
) (p
i
, p
j
, w
j
)
(w
i
, w
j
, p
i
, p
j
) (p
i
, p
k
, p
j
), i < k < j
(p
i
, p
i+1
, p
j?1
, p
j
) (p
i?1
, p
i
, p
j?1
, p
j
)
(p
i
, p
i+1
, p
j
, p
j+1
) (p
i?1
, p
i
, p
j
, p
j+1
)
Table 1: Features for syntactic parsing.
is the set of syntactic dependency labels, the score
function of edges is defined as
s(i, j) = max
l?Y
Pr(l|x, i, j) (2)
ME models are used to calculate the value of
Pr(l|x, i, j), where the features are extracted from
input sentence x. Given i and j as the subscripts
of words in the sentence and word i is the parent
of word j, the features can be illustrated in table
1. w
i
and p
i
are denoted as the ith word and the
ith part of speech respectively in the sentence. The
tuples define integrated features, such as (w
i
, p
i
)
indicates the feature combining the ith word and
ith part of speech. Besides these features, the dis-
tant between word i and word j in sentence x is
considered as a single feature. The distant is also
combined with features in table 1 to produce com-
plex features.
2.2 Semantic Dependency Labeling
Semantic dependencies are always concerning
with specific predicates. Unlike syntactic depen-
dencies, semantic dependency relationships usu-
ally can not be represented as a tree. Thus, the
method used for semantic dependency labeling
is somewhat different from syntactic dependency
parsing. The work of semantic labeling can be di-
vided into two stages: predicate tagging and de-
pendents recognizing.
2.2.1 Predicate Tagging
According to PropBank (Palmer, et al, 2005)
and NomBank (Meyers, et al, 2004), predicates
usually have several rolesets corresponding to dif-
ferent meanings. For example, the verb abandon
has three rolesets marked as ordinal numbers 01,
02 and 03 as described below.
w
i
p
i
p
i?1
p
i+1
(p
i?1
, p
i
) (p
i
, p
i+1
)
(p
i?2
, p
i
) (p
i
, p
i+2
)
(p
i?3
, p
i
) (p
i
, p
i+3
)
(p
i?1
, p
i
, p
i+1
) (w
i
, p
i
)
(w
i
, p
i?1
, p
i
) (w
i
, p
i
, p
i+1
)
(w
i
, p
i?2
, p
i
) (w
i
, p
i
, p
i+2
)
(w
i
, p
i?3
, p
i
) (w
i
, p
i
, p
i+3
)
(w
i
, p
i?1
, p
i
, p
i+1
)
Table 2: Features used for predicate tagging.
<frameset>
<predicate lemma=?abandon?>
<roleset id=?abandon.01? name=?leave
behind? vncls=?51.2?>
. . .
</roleset>
<roleset id=?abandon.02?
name=?exchange? vncls=?51.2?>
. . .
</roleset>
<roleset id=?abandon.03?
name=?surrender, give over? vncls=?-
?>
. . .
</roleset>
</predicate>
</frameset>
The goal of this part is to identify the predicates
in the sentences and to determine the roleset for
each of them. It should be cleared that the ordi-
nal numbers are only used to distinguish different
meanings of a predicate. However, if these num-
bers are treated as tags for predicates, some statisti-
cal properties will be obtained as illustrated in Fig-
ure 1. As can be seen, the distribution of the train
data would be quite informative for representing
the distribution of other three data sets. Based on
this idea, a classification framework is introduced
for predicate tagging.
Suppose the tag set is chosen to be T =
{01, 02, ..., 22} according to the horizontal axis of
Figure 1 and 00 is added to indicate that the ex-
amining word is not a predicate. Suppose t
i
is a
variable indicating the tag of word at position i in
sentences x. ME models are implemented to tag
the predicates.
t
i
= argmax
t? T
Pr(t|x, i) (3)
219
0 5 10 15 200
2
4
6
8
10
12
Ordinal Numbers of Predicates
Log
arith
mic
al N
umb
er o
f Oc
curr
enc
e
traindevelbrownws j
Figure 1: Distribution of the ordinal numbers of
predicates on different data sets. 01 - 21 are at-
tached with the predicates in the corpus and 22
stands for ?SU?.
The features for predicate tagging are listed in ta-
ble 2, where the symbols share the same mean-
ing as in table 1. Experiments show that this pure
statistic processing method is effective for predi-
cate tagging.
2.2.2 Dependents Recognizing
This subtask depends deeply on the results of
syntactic parsing and predicate tagging described
earlier in the system. Predicate tagging identifies
central words and syntactic parsing provides syn-
tactic features for its dependents identification and
classification.
Generally speaking, given a specific predicate in
a sentence, only a few of words are associated as its
semantic dependents. By statistical analysis a list
of part of speech tuples that are appearing to be se-
mantic dependency are collected. All other tuples
are filtered out to improve system performance.
Suppose (p, d) is a couple of predicate and one
of its possible dependents, T is the dependency
tree generated by syntactic parsing, L is the set of
semantic dependency labels. The dependents can
be recognized by using a classification model, ME
models are chosen as before.
l
(p,d)
= argmax
l?L
Pr(l|p, d, T ) (4)
Besides the semantic dependency labels, null is in-
cluded as a special tag to indicate that there is no
semantic dependency between p and d. As a result,
dependents identification (binary classification)
and dependents tagging (multi-classification) can
be solved together within one multi-classification
framework.
The selected features are listed below.
1. Predicate Features
? Lemma and POS of predicate, pred-
icate?s parent in syntactic dependency
tree.
? Voice active or passive.
? Syntactic dependency label of edge be-
tween predicate and its parent.
? POS framework POS list of predicate?s
siblings, POS list of predicate?s children.
? Syntactic dependency framework syn-
tactic dependency label list of the edges
between predicate?s parent and its sib-
lings.
? Parent framework syntactic depen-
dency label list of edges connecting to
predicate?s parent.
2. Dependent Features
? Lemma and POS of dependent, depen-
dent?s parent.
? POS framework POS list of depen-
dent?s siblings.
? Number of children of dependent?s par-
ent.
3. In Between Features
? Position of dependent according to
predicate: before or after.
? POS pair of predicate and dependent.
? Family relation between predicate and
dependent: ancestor or descendant.
? Path length between predicate and de-
pendent.
? Path POS POS list of all words appear-
ing on the path from predicate to depen-
dent.
? Path syntactic dependency label list of
dependency label of edges of path be-
tween predicate and dependent.
3 Experiment results
The classification models were trained using all the
training data. The detailed information are shown
in table 3. All experiments ran on 32-bit Intel(R)
Pentium(R) D CPU 3.00GHz processors with 2.0G
memory.
220
Feature Number Training Time
Syn. 7,488,533 30h
Prd. 1,484,398 8h
Sem. 3,588,514 12h
Table 3: Details of ME models. Syn. is for syntac-
tic parsing, Prd. is for predicate tagging and Sem.
is for semantic dependents recognizing.
Syntactic Semantic Overall
devel 85.29 69.60 77.49
brown 80.80 59.17 70.01
wsj 87.42 71.27 79.38
brown+wsj 86.69 69.95 78.35
(a) Closed Challenge
Syntactic Semantic Overall
devel 85.29 68.45 76.87
brown 80.80 58.22 69.51
wsj 87.42 70.32 78.87
brown+wsj 86.69 68.99 77.84
(b) Open Challenge
Table 4: Scores for joint learning of syntactic and
semantic dependencies.
3.1 Closed Challenge
The system for closed challenge is designed as a
two-stage parser: syntactic parsing and semantic
dependency labeling as described previously. Ta-
ble 4(a) shows the results on different corpus. As
shown in table 4(a), the scores of semantic depen-
dency labeling are quite low, that are influencing
the overall scores. The reason could be inferred
from the description in section 2.2.2 since seman-
tic dependent labeling inherits the errors from the
output of syntactic parsing and predicate tagging.
Following evaluates each part independently.
Besides the multiple classification model de-
scribed in table 3, a binary classification model
was built based on ME for predicate tagging. The
binary model can?t distinguish different rolesets of
predicate, but can identify which words are predi-
cates in sentences. The precision and recall for bi-
nary model are 90.80 and 88.87 respectively, while
for multiple model, the values are 84.60 and 85.60.
For semantic dependent labeling, experiments
were performed under conditions that the gold syn-
tactic dependency tree and predicates list were
given as input. The semantic scores became 80.09,
77.08 and 82.25 for devel, brown and wsj respec-
tively. This implies that the error of syntactic pars-
ing and predicate tagging could be probably aug-
mented in semantic dependent labeling. In order to
improve the performance of the whole system, the
deep dependence between the two stages should be
broken up in future research.
3.2 Open Challenge
In open challenge, the same models are used for
syntactic parsing and predicate tagging as in closed
challenge and two other models are trained for se-
mantic dependent labeling. Suppose M
mst
, M
malt
and M
chunk
are denoted as these three semantic
models, where M
mst
is the model used in closed
challenge, M
malt
is trained on the syntactic de-
pendency tree provided by the open corpus with
the same feature set as M
mst
, and M
chunk
is
trained using features extracted from name entity
and wordnet super senses results provided by the
open corpus.
Considering a possible dependent given a spe-
cific predicate, the feature set used for M
chunk
contains only six elements:
? Whether the dependent is in name entity
chunk: True or False.
? Name entity label of the dependent.
? Whether the dependent is in BBN name entity
chunk: True or False.
? BBN name entity label of the dependent.
? Whether the dependent is in wordnet super
sense chunk: True or False.
? Wordnet super sense label of the dependent.
After implementing these three models on se-
mantic dependents recognizing, the results were
merged to generate the scores described in table
4(b).
The merging strategy is quite simple. Given a
couple of predicate and dependent (p, d), the sys-
tem produces three semantic dependency labels
denoting as l
mst
, l
malt
and l
chunk
, the result la-
bel is chosen to be most frequent semantic label
among the three.
Comparing the scores of open challenge and
closed challenge, it can be found that the score of
the former is less than the latter, which is quite
strange since more resources were used in open
challenge. To examine the influences of differ-
ent semantic dependents recognizing models, each
221
Mmst
M
malt
M
chunk
devel 69.60 64.48 41.72
brown 59.17 56.52 34.04
wsj 71.27 66.40 41.83
Table 5: Semantic scores of different models.
model was implemented in the closed challenge
and the results are shown in table 5. Specially,
model M
chunk
generated too low scores and gave a
heavy negative influence on the final results. Find-
ing a good way to combine several results requires
further research.
4 Conclusions
This paper have presented a simple discriminative
system submitted to the CoNLL-2008 shared task
to address the learning task of syntactic and se-
mantic dependencies. The system was divided into
syntactic parsing and semantic dependents label-
ing. Maximum spanning tree was used to find
a syntactic dependency tree in the full-connected
graph constructed over the words of a sentence.
Maximum entropy models were implemented to
classify syntactic dependency edges, predicates
and their semantic dependents. A brief analysis
has also been given on the results of both closed
challenge and open challenge.
Acknowledgement
This research has been partially supported by the
National Natural Science Foundation of China
(No. 60435020 and No. 90612005), the Goal-
oriented Lessons from the National 863 Program
of China (No.2006AA01Z197) and Project of Mi-
crosoft Research Asia. We would like to thank
Zhixin Hao, Xiao Xin, Languang He and Tao Qian
for their wise suggestion and great help. Thanks
also to Muhammad Waqas Anwar for English im-
provement.
References
Adam Berger, Stephen Della Pietra, Vincent Della
Pietra 1996. A Maximum Entropy Approach to Nat-
ural Language Processing. Computational Linguis-
tics, 22(1):39-71.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young
and Ralph Grishman 2004. The NomBank Project:
An Interim Report HLT-NAACL 2004 Workshop:
Frontiers in Corpus Annotation, 24-31.
Martha Palmer, Daniel Gildea, Paul Kingsbury 2005.
The Proposition Bank: An Annotated Corpus of Se-
mantic Roles Computational Linguistics, 31(1):71-
106.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s M`arquez and Joakim Nivre 2008. The
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008)
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji?c 2005. Non-projective Dependency Pars-
ing using Spanning Tree Algorithms. Proceedings of
HLT/EMNLP, 523-530.
222
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 13?17,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Cascade Method for Detecting Hedges and their Scope in Natural
Language Text
Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan, Shixi Fan
Key Laboratory of Network Oriented Intelligent Computation
Harbin Institute of Technology Shenzhen Graduate School
Shenzhen, Guangdong, China
{tangbuzhou,yuanbo.hitsz}@gmail.com
{wangxl,wangxuan,fanshixi}@insun.hit.edu.cn
Abstract
Detecting hedges and their scope in nat-
ural language text is very important for
information inference. In this paper,
we present a system based on a cascade
method for the CoNLL-2010 shared task.
The system composes of two components:
one for detecting hedges and another one
for detecting their scope. For detecting
hedges, we build a cascade subsystem.
Firstly, a conditional random field (CRF)
model and a large margin-based model are
trained respectively. Then, we train an-
other CRF model using the result of the
first phase. For detecting the scope of
hedges, a CRF model is trained according
to the result of the first subtask. The ex-
periments show that our system achieves
86.36% F-measure on biological corpus
and 55.05% F-measure on Wikipedia cor-
pus for hedge detection, and 49.95% F-
measure on biological corpus for hedge
scope detection. Among them, 86.36%
is the best result on biological corpus for
hedge detection.
1 Introduction
Hedge cues are very common in natural language
text. Vincze et al (2008) report that 17.70% of
the sentences in the abstract section and 19.94% of
sentences in the full paper section contain hedges
on BioScope corpus. As Vincze et al (2008)
suggest that information that falls in the scope
of hedges can not be presented as factual in-
formation. Detecting hedges and their scope in
natural language text is very important for in-
formation inference. Recently, relative research
has received considerable interest in the biomed-
ical NLP community, including detecting hedges
and their in-sentence scope in biomedical texts
(Morante and Daelemans, 2009). The CoNLL-
2010 has launched a shared task for exploiting the
hedge scope annotated in the BioScope (Vincze et
al., 2008) and publicly available Wikipedia (Gan-
ter and Strube, 2009) weasel annotations. The
shared task contains two subtasks (Farkas et al,
2010): 1. learning to detect hedges in sentences on
BioScope and Wikipedia; 2. learning to detect the
in-sentence scope of these hedges on BioScope.
In this paper, we present a system based on a
cascade method for the CoNLL-2010 shared task.
The system composes of two components: one
for detecting hedges and another one for detect-
ing their scope. For detecting hedges, we build
a cascade subsystem. Firstly, conditional ran-
dom field (CRF) model and a large margin-based
model are trained respectively. Then, we train
another CRF model using the result of the first
phase. For detecting the scope of hedges, a CRF
model is trained according to the result of the first
subtask. The experiments show that our system
achieves 86.36% F-measure on biological corpus
and 55.05% F-measure on Wikipedia corpus for
hedge detection, and 49.95% F-measure on bio-
logical corpus for hedge scope detection. Among
them, 86.36% is the best result on biological cor-
pus for hedge detection.
2 System Description
As there are two subtasks, we present a system
based on a cascade supervised machine learning
methods for the CoNLL-2010 shared task. The ar-
chitecture of our system is shown in Figure 1.
The system composes of two subsystems for
two subtasks respectively, and the first subsystem
is a two-layer cascaded classifier.
2.1 Hedge Detection
The hedges are represented by indicating whether
a token is in a hedge and its position in the
CoNLL-2010 shared task. Three tags are used for
13
Figure 1: System architecture
this scheme, where O cue indicates a token out-
side of a hedge, B cue indicates a token at the
beginning of a hedge and I cue indicates a to-
ken inside of a hedge. In this subsystem, we do
preprocessing by GENIA Tagger (version 3.0.1)1
at first, which does lemma extraction, part-of-
speech (POS), chunking and named entity recog-
nition (NER) for feature extraction. For the out-
put of GENIA Tagger, we convert the first char
of a lemma into lower case and BIO chunk tag
into BIOS chunk tag, where S indicates a token
is a chunk, B indicates a token at the beginning
of a chunk, I indicates a token inside of a chunk,
and O indicates a token outside of a chunk. Then
a two-layer cascaded classifier is built for pre-
diction. There are a CRF classifier and a large
margin-based classifier in the first layer and a CRF
classifier in the second layer.
In the first layer, the following features are used
in our system:
? Word andWord Shape of the lemma: we used
the similar scheme as shown in (Tsai et al,
2005).
1http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
? Prefix and Suffix with length 3-5.
? Context of the lemma, POS and the chunk in
the window [-2,2].
? Combined features including L0C0, LiP0
and LiC0, where ?1 ? i ? 1 L denotes the
lemma of a word, P denotes a POS and C
denotes a chunk tag.
? The type of a chunk; the lemma and POS se-
quences of it.
? Whether a token is a part of the pairs ?neither
... nor? and ?either ... or? as both tokens of a
pair are always labeled with the same tag.
? Whether a token can possibly be classified
into B cue, I cue or O cue; its lemma, POS
and chunk tag for each possible case: these
features are extracted according to a dictio-
nary extracted from training corpus, which
lists all possible hedge tag for each word in
the training corpus.
In the second layer, we used some features
about the result of the last layer besides those men-
tioned above. They are listed as follow:
? The lemma and POS sequences of the hedge
predicted by each classifier.
? The times of a token classified into B cue,
I cue and O cue by the first two classifiers.
? Whether a token is the last token of the hedge
predicted by each classifier.
2.2 Hedge Scope Detection
We follow the way of Morante and Daelemans
(2009) to represent the scope of a hedge, where
F scope indicates a token at the beginning of a
scope sequence, L scope indicates a token at the
last of a scope sequence, and NONE indicates
others. In this phase, we do preprocessing by
GDep Tagger (version beta1)2 at first, which does
lemma extraction, part-of-speech (POS), chunk-
ing, named entity recognition (NER) and depen-
dency parse for feature extraction. For the out-
put of GDep Tagger, we deal with the lemma and
chunk tag using the same way mentioned in the
last section. Then, a CRF classifier is built for pre-
diction, which uses the following features:
2http://www.cs.cmu.edu/ sagae/parser/gdep
14
? Word.
? Context of the lemma, POS, the chunk, the
hedge and the dependency relation in the
window [-2,2].
? Combined features including L0C0,
L0H0, L0D0, LiP0, PiC0,PiH0, CiH0,
PiD0,CiD0, where ?1 ? i ? 1 L denotes
the lemma of a word, P denotes a POS, C
denotes a chunk tag, H denotes a hedge tag
and D denotes a dependency relation tag.
? The type of a chunk; the lemma and POS se-
quences of it.
? The type of a hedge; the lemma, POS and
chunk sequences of it.
? The lemma, POS, chunk, hedge and depen-
dency relation sequences of 1st and 2nd de-
pendency relation edges; the lemma, POS,
chunk, hedge and dependency relation se-
quences of the path from a token to the root.
? Whether there are hedges in the 1st, 2nd de-
pendency relation edges or path from a token
to the root.
? The location of a token relative to the nega-
tion signal: previous the first hedge, in the
first hedge, between two hedge cues, in the
last hedge, post the last hedge.
At last, we provided a postprocessing system
for the output of the classifier to build the com-
plete sequence of tokens that constitute the scope.
We applied the following postprocessing:
? If a hedge is bracketed by a F scope and a
L scope, its scope is formed by the tokens be-
tween them.
? If a hedge is only bracketed by a F scope, and
there is no L scope in the sentence, we search
the first possible word from the end of the
sentence according to a dictionary, which ex-
tracted from the training corpus, and assign it
as L scope. The scope of the hedge is formed
by the tokens between them.
? If a hedge is only bracketed by a F scope, and
there are at least one L scope in the sentence,
we think the last L scope is the L scope of the
hedge, and its scope is formed by the tokens
between them.
? If a hedge is only bracketed by a L scope,
and there is no F scope in the sentence, we
search the first possible word from the begin-
ning of the sentence to the hedge according to
the dictionary, and assign it as F scope. The
scope of the hedge is formed by the tokens
between them.
? If a hedge is only bracketed by a L scope,
and there are at least one F scope in the sen-
tence, we search the first possible word from
the hedge to the beginning of the sentence ac-
cording to the dictionary, and think it as the
F scope of the hedge. The scope of the hedge
is formed by the tokens between them.
? If a hedge is bracketed by neither of them, we
remove it.
3 Experiments and Results
Two annotated corpus: BioScope and Wikipedia
are supplied for the CoNLL-2010 shared task. The
BioScope corpus consists of two parts: biological
paper abstracts and biological full papers, and it
is used for two subtasks. The Wikipedia corpus is
only used for hedge detection. The detailed infor-
mation of these two corpora is shown in Table 1
and Table 2, respectively.
Abstracts Papers Test
#Documents 1273 9 15
#Sentences 11871 2670 5003
%Hedge sent. 17.70 19.44 15.75
#Hedges 2694 682 1043
#AvL. of sent. 30.43 27.95 31.30
#AvL. of scopes 17.27 14.17 17.51
Table 1: The detailed information of BioScope
corpus. ?AvL.? stands for average length.
Train Test
#Documents 2186 2737
#Sentences 11111 9634
%Hedge sentences 22.36 23.19
#Hedges 3133 3143
#AvL. of sentences 23.07 20.82
Table 2: The detail information of Wikipedia cor-
pus. ?AvL.? stands for average length.
In our experiments, CRF++-0.533 implemen-
3http://crfpp.sourceforge.net/
15
tation is employed to CRF, and svm hmm 3.104
implementation is employed to the large margin
method. All parameters are default except C
(the trade-off between training error and margin,
C=8000, for selecting C, the training corpus is par-
titioned into three parts, two of them are used for
training and the left one is used as a development
dataset) in svm hmm. Both of them are state-of-
the-art toolkits for the sequence labeling problem.
3.1 Hedge Detection
We first compare the performance of each single
classifier with the cascaded system on two corpora
in domain, respectively. Each model is trained by
whole corpus, and the performance of them was
evaluated by the official tool of the CoNLL-2010
shared task. There were two kinds of measure:
one for sentence-level performance and another
one for cue-match performance. Here, we only
focused on the first one, and the results shown in
Table 3.
Corpus System Prec. Recall F1
CRF 87.12 86.46 86.79
BioScope LM 85.24 87.72 86.46
CAS 85.03 87.72 86.36
CRF 86.10 35.77 50.54
Wikipedia LM 82.28 41.36 55.05
CAS 82.28 41.36 55.05
Table 3: In-sentence performance of the hedge
detection subsystem for in-domain test. ?Prec.?
stands for precision, ?LM? stands for large mar-
gin, and ?CAS? stands for cascaded system.
From Table 3, we can see that the cascaded sys-
tem is not better than other two single classifiers
and the single CRF classifier achieves the best per-
formance with F-measure 86.79%. The reason for
selecting this cascaded system for our final sub-
mission is that the cascaded system achieved the
best performance on the two training corpus when
we partition each one into three parts: two of them
are used for training and the left one is used for
testing.
For cross-domain test, we train a cascaded clas-
sifier using BioScope+Wikipedia cropus. Table 4
shows the results.
As shown in Table 5, the performance of cross-
domain test is worse than that of in-domain test.
4http://www.cs.cornell.edu/People/tj/svm light/svm-
hmm.html
Corpus Precision Recall F1
BioScope 89.91 73.29 80.75
Wikipedia 81.56 40.20 53.85
Table 4: Results of the hedge detection for cross-
domain test. ?LM? stands for large margin, and
?CAS? stands for cascaded system.
3.2 Hedge Scope Detection
For test the affect of postprocessing for hedge
scope detection, we test our system using two eval-
uation tools: one for scope tag and the other one
for sentence-level scope (the official tool). In or-
der to evaluate our system comprehensively, four
results are used for comparison. The ?gold? is the
performance using golden hedge tags for test, the
?CRF? is the performance using the hedge tags
prediction of single CRF for test, the ?LM? is the
performance using the hedge tag prediction of sin-
gle large margin for test, and ?CAS? is the per-
formance of using the hedge tag prediction of cas-
caded subsystem for test. The results of scope tag
and scope sentence-level are listed in Table 5 and
Table 6, respectively. Here, we should notice that
the result listed here is different with that submit-
ted to the CoNLL-2010 shared task because some
errors for feature extraction in the previous system
are revised here.
HD tag Precision Recall F1
F scope 92.06 78.83 84.94
gold L scope 80.56 68.67 74.14
NONE 99.68 99.86 99.77
F scope 78.83 66.89 72.37
CRF L scope 72.52 60.50 65.97
NONE 99.56 99.75 99.65
F scope 77.25 67.57 72.09
LM L scope 72.33 61.41 66.42
NONE 99.56 99.73 99.31
F scope 77.32 67.86 72.29
CAS L scope 72.00 61.29 66.22
NONE 99.57 99.73 99.65
Table 5: Results of the hedge scope tag. ?HD?
stands for hedge detection subsystem we used,
?LM? stands for large margin, and ?CAS? stands
for cascaded system.
As shown in Table 5, the performance of
L scope is much lower than that of F scope.
Therefore, the first problem we should solve is
16
HD subsystem Precision Recall F1
gold 57.92 55.95 56.92
CRF 52.36 48.40 50.30
LM 51.06 48.89 49.95
CAS 50.96 48.98 49.95
Table 6: Results of the hedge scope in-sentence.
?HD? stands for hedge detection subsystem we
used, ?LM? stands for large margin, and ?CAS?
stands for cascaded system.
how to improve the prediction performance of
L scope. Moreover, compared the performance
shown in Table 5 and 6, about 15% (F1 of L scope
in Table 5 - F1 in Table 6) scope labels are mis-
matched. An efficient postprocessing is needed to
do F-L scope pair match.
As ?CRF? hedge detection subsystem is bet-
ter than the other two subsystems, our system
achieves the best performance with F-measure
50.30% when using the ?CRF? subsystem.
4 Conclusions
This paper presents a cascaded system for the
CoNLL-2010 shared task, which contains two
subsystems: one for detecting hedges and an-
other one for detecting their scope. Although
the best performance of hedge detection subsys-
tem achieves F-measure 86.79%, the best per-
formance of the whole system only achieves F-
measure 50.30%. How to improve it, we think
some complex features such as context free gram-
mar may be effective for detecting hedge scope.
In addition, the postprocessing can be further im-
proved.
Acknowledgments
We wish to thank the organizers of the CoNLL-
2010 shared task for preparing the datasets
and organizing the challenge shared tasks.
We also wish to thank all authors supply-
ing the toolkits used in this paper. This
research has been partially supported by the
National Natural Science Foundation of China
(No.60435020 and No.90612005), National 863
Program of China (No.2007AA01Z194) and the
Goal-oriented Lessons from the National 863 Pro-
gram of China (No.2006AA01Z197).
References
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, Gyo?rgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their
Scope in Natural Language Text. In Proceedings of
the Fourteenth Conference on Computational Nat-
ural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associ-
ation for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173?176, Suntec, Singapore,
August. Association for Computational Linguistics.
Roser Morante andWalter Daelemans. 2009. Learning
the scope of hedge cues in biomedical texts. In Pro-
ceedings of the BioNLP 2009 Workshop, pages 28?
36, Boulder, Colorado, June. Association for Com-
putational Linguistics.
Tzong-Han Tsai, Chia-Wei Wu, and Wen-Lian Hsu.
2005. Using Maximum Entropy to Extract Biomed-
ical Named Entities without Dictionaries. In Sec-
ond International Joint Conference on Natural Lan-
guage Processing, pages 268?273.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9.
17
