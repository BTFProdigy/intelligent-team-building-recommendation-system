Issues in the Transcription of English Conversational Grunts 
Nige l  Ward  
? Mech-In.fo Engineering, University of Tokyo, 
Bunkyo-ku, Tokyo 113-8656, Japan 
nigel@sanpo.t.u-tokyo.ac.jp 
ht tp: / /www.sanpo.t .u- tokyo.ac. jp /~ nigel/ 
Abst ract  
Conversational grunts, such as uh- 
huh, un-hn, rnrn, and oh are ubiq- 
uitous in spoken English, but no 
satisfactory scheme for transcrib- 
ing these items exists. This pa- 
per describes previous approaches, 
presents ome facts about the pho- 
netics of grunts, proposes a tran- 
scription scheme, and evaluates its 
accuracy. 1 
1 The  Impor tance  o f  
Conversat iona l  Grunts  
:Conversational grunts, such as uh-huh, un-hn~ 
ram, and oh are ubiquitous in spoken English. 
In our conversation data, these grunts occur 
an average of once every 5 seconds in Amer- 
ican English conversation. In a sample of 79 
conversations from a larger corpus, Switch- 
board, urn was the 6th most frequent item 
(after /, and, the, you, and a), and the four 
items uh, uh-huh, um and urn-hum accounted 
for 4% of the total. These sounds are not only 
frequent, they are important in language use. 
To mention just one example, people learn- 
ing English as a second language are handi- 
capped in informal interactions if they cannot 
produce and recognize these sounds. 
1I would like to tb.nlr Takeki Kamiyama for pho- 
netic label cross-checld-g, all those who let me record 
their conversations, and the anonymous referees; and 
also the Japanese 1Vr;nlqtry of Education, the Sound 
Technology Promotion Foundation, the Nakayama 
Foundation, the Inamori Foundation, the Interna- 
tional Communications Fonndation and the Okawa 
Foundation for support. 
Just to be clear about definitions, in this 
paper 'grunts 2' means sounds which are ~not 
words', where a prototypical "word" is a 
sound having 1. a clear meaning, 2. the abil- 
ity to participate in syntactic onstructions, 
and 3. a phonotactically normal pronuncia- 
tion. For example, uh-huh is a grunt since it 
has no referential meaning, has no syntactic 
affinities, and has salient breathiness. In this 
paper 'conversational' refers to sounds which 
occur in conversation and are at least in part 
directed at the interlocutor, rather than be- 
ing purely self-directed 3. Both of these defi- 
nitions have flaws, but they provide a fairly 
objective criterion for delimiting the set of 
items which any transcription scheme should 
be able to handle. 
The phenomena circumscribed by this def- 
inition are a subset of "vocal segregates" 
(Trager, 1958) and of "interjections": the dif- 
ference is that it limits attention to sounds 
occurring in conversations. This definition 
also roughly delimits the subset of "discourse 
markers" or "discourse particles" which occur 
in informal spoken discourse. 
As the phonetics and meanings of conver- 
sational grunts are currently not well under- 
stood, we have begun a project aiming to elu- 
cidate, model, and eventually exploit them. 
The current paper is a report on an approach 
2 It may seem that the negative connotations ofthe 
word 'grunt' maire it inappropriate for use as a tech- 
nical term, but the phenomenon itself is often stlg- 
matised, and so the term is appropriate in that sense 
too. 
STwo rules of thnmh were adopted to help in cases 
which were difllcult to judge: consider laughter as not 
conversational, nd consider as conversational every- 
thing else that might possibly be playing some com- 
municative role, even if it isn't clear what that role 
might be. 
29 
to the preliminary problem of how to tran- 
scribe these sounds. 
A generally usable, standardized transcrip- 
tion scheme would be of great value. Im- 
mediate applications include screenplay writ- 
ing and court recording. It would also fa- 
cilitate the systematic corpns-based study of 
the meanings and functions of these sounds 4.
There are also prospects for applications in 
systems. One could imagine a dialog tran- 
scription system that produces output with 
the grunts represented in enough detail to 
show whether a listener is being enthusias- 
tic, reluctant, non-committal, bored, etc., as 
these states are often indicated by grunts 
rather than by words. One could imagine 
spoken dialog systems which prompt and con- 
firm concisely with such grunts, instead of full 
words or phrases. And one could imagine spo- 
ken dialog systems which adjust their output 
based on barge-in feedback from the user such 
as uh-huh meaning "go on, don't talk so slow", 
uh-hum meaning "stop, I need to think", and 
ah meaning "I have something to say". 
Section 2 surveys previous approaches to 
grunt transcription, Section 3 proposes a 
slightly new scheme, Section 4 discusses its 
adequacy, and Section 5 points out some open 
issues. 
2 Prev ious  Schemes  for  Grunt  
T ranscr ip t ion  
This section points out the problems with pre- 
vious approaches to grunt translation. 
2.1 Phonet ica l ly  Accurate  Schemes 
One tradition in labeling grunts is to use a 
completely general scheme. The central inspi- 
ration here is the fact that grunts are unlike 
words, in that they contain sounds which are 
never seen in the lexical items of the language. 
As such, they can fall outside the coverage 
of even the International Phonetic Alphabet, 
which is only designed to handle those sounds 
4This is not to say that there can be a strict order- 
ing of activities here: on the contrary, it is not pos- 
sible to fix a transcription standard without at least 
a tacit theory of the meanings and functions of the 
items being t ra~ibed .  Some thoughts on this ap- 
pear elsewhere (Ward, 2000). 
which occur contrastively in some words in 
some language. Thus there have been pro- 
posals for richer, more complete transcription 
schemes, capable of handling just about any 
communicative noise that people have been 
observed to produce, including moans, cries 
and belches (Trager, 1958; Poyatos, 1975). 
One disadvantage of these notations i  that 
they are not usable without raining. 
A second isadvantage is that their gener- 
ality is excessive for everyday use. As seen 
below, the vast majority of conversational 
grunts are drawn from a much smaller inven- 
tory of sounds. 
A third disadvantage is that they provide 
more accuracy than is needed. For exam- 
ple, in English there appear to be no grunts 
in which the difference between an alveolar 
nasal, a velar nasal, or nasalization of a vowel 
conveys a difference in meaning, and so these 
do not need to be distinguished in transcrip- 
tion. 
2.2 A Funct lon-based Schemes  
An alternative approach is seen in some 
schemes used for labeling corpora for pur- 
poses of training and evaluating speech rec- 
ognizers. A quote from the most recent 
Switchboard labeling standard (Hamaker et 
al., 1998) gives the flavor: 
20. Hesitation Sounds: Use "uh" 
or "ah" for hesitations consisting 
of a vowel sound, and "urn" or 
"hm" for hesitations with a nasal 
sound, depending upon which tran- 
scription the actual sound is closest 
to. Use "huh" for aspirated version 
of the hesitation as in "huh? <other 
speaker responds> um ok, I see your 
point." 
21: yes/no sounds: Use "uh-huh" 
or "um-hum" (yes) and "huh-uh" 
or "hum-tun" (no) for anything re- 
motely resembling these sounds of 
assent or denial" 
Another scheme (Lander, 1996) lists several 
"miscellaneous words", including: 
30 
"nuh uh" (no), "ram hmm" (yes), 
"hmm mmm" (no), 'hnm ram" (no), 
"uh huh" (yes), "huh uh" (no), "uh 
uh" (no) 
The inspiration behind these schemes 
seems to be the idea that grunts are just like 
words. This leads to two assumptions, both 
of which are questionable. First, there is the 
assumption that each grunt has some fixed 
meaning and some fixed functional role (filler, 
back-channel, etc). However, many specific 
grunt sounds can be found in more than one 
functional role, as seen in Table 1. Second, 
there is the assumption that the set of conver- 
sational grunts is small. However the number 
of observed grunts is not small~ as seen in Ta- 
ble 2, and the set of possible grunts is prob- 
ably not even finite: for example, it would 
not be surprising at all to hear the sound 
hura-ha-har~ in conversation, or hem-ha-an, or 
hurn-ha-un, and so on, and so on. (However, 
not every possible sound seems likely to be a 
conversational grunt; for example ziflug would 
seem a surprising novelty, and would be down- 
right weird in any of the functional positions 
typical for grunts.) 
One concrete problem with these schemes 
is that they are not designed to allow pho- 
netically accurate representations of grunts 5. 
In particular, they make the task of the la- 
beler a rather strange one. Given a grunt, 
first he must examine the context to deter- 
mine whether it is a back-channel or a filler, 
then determine whether it sounds affirmative 
or negative, and only then can he consider 
what the actual sound is, and his options are 
limited to picking one of the labels in the func- 
tional/semantic category. The relation be- 
tween the letters of the label and the phonet- 
ics of the grunt becomes omewhat arbitrary. 
This would be more tolerable if there was a 
clear tendency for each grunt to occur in only 
one functional position, but this is not the 
case, as noted above. The use of the aifirma- 
tive/negatlve distinction as a primary classi- 
ficatory feature is also also open to question. 
In our corpus, only 1% of the grunts were neg- 
ative in meaning, and these were all in con- 
texts where a negative answer was expected 
or likely, so this distinction is a strange choice 
for a top-level dividing principle. Moreover, 
negative grunts are, in fact, characterized by 
two-syllables with a sharp syllable boundary, 
often a glottal stop, and/or a sharp down- 
step in pitch, and/or a lack of breathiness, 
but these features are reflected only tenuously 
in the spellings listed as possible for negative 
grunts in these schemes. 
2.3 Naive Transcription 
The third tradition in transcribing grunts is to 
allow labelers to just spell them in the 'usual' 
way, as one might see them written in the 
comics or in a detective novel. The inspiration 
behind this is that native speakers generally 
have had a lot of exposure to orthographic 
representations of grunts, and can be trusted 
to do the right thing. 
One  problem with this tradition is that the 
mapping from letter sequences to the actual 
sounds is not clear. For example, a conversa- 
tion transcription given as a textbook exam- 
ple of good practice includes "u" and "uh", 
and "oh" and "oo" (Hutchby and Wooffitt, 
1999), without footnoting. Presumably the 
%o" means /u / ,  but it could also possibly 
mean a version of "oh" with strong lip round- 
hag, or a longer form of "oh", or perhaps a 
shorter form (if the labeler was trying to avoid 
confusion with the archaic vocative "o') .  En- 
glish orthography is phonetically ambiguous 
and not standardized for grunts. 
A second problem with this tradition is that 
creaky voice (vocal fry), although pragmati- 
cally significant, is generally not represented 
(although many practitioners are surprisingly 
diligent at noting occurrences of breathiness). 
2.4 Summary  o f  Des iderata  
Ideally we want a scheme for transcribing 
grunts which 
I. is easy to learn and use, 
5 Th.ls is acceptable if the only aim is to train speech 
recognizers, where the speech recognizers' acoustic 
models will end up capturing the possible phonetic 
variation without human intervention, and if the 
speech recognition results are not intended for actual 
use, but merely to be fed into an algorithm for COl- 
puting recognition scores. 
31 
total back- 
channel filler 
dis- 
fluency 
\[clear-throat\] 2 1 
tsk 22 . 12 2 
ah 7 1 3 3 
aum 5 4 1 
hh 3 
hmm 2 
huh 2 
m-hm 2 :i 
r-am 2 2 
mmm 3 ',.) 
myeah 2 2 
nn-hn 4 4 
oh 20 6 
oh-okay 2 1 
okay 8 2 2 
u-uh 4 2 
uh 38 14 21 
uh-hn 2 
uh-huh 3 3 
uh-uh 2 1 1 
nhh 2 2 
ukay 2 1 I 
um 20 10 8 
,,ram 5 5 
uu 5 2 2 
uum 5 3 2 
yeah 71 27 19 1 
(other) 72 34 19 3 
Total 317 91 108 45 
isolate response confirm- ation 
6 6 6 
8 3 
20 13 8 
final other 
1 
7 
1 
i 
5 
1 
1 1 
1 1 
1 
2 4 
I 4 
6 26 
Table 1: Counts  of Grunt  Occurrences in var ious pos i t ions  and  funct iona l  roles, for all g runts  
occurr ing  2 or more  t imes in  our  corpus 
\[clear-throat\] 2 
tsk 23 
tsk-naa 1 
tsk-neeu 1 
tsk-ooh 1 
tsk-yeah 1 
\[inhale\] 1 
\[unsticking\] 4 
aa 
achh 1 
ah 7 
ahh 1 
ai 1 
am 
BO 
aDO 
aum 
eah 
ehh 
h-Ylllrllq~ 
haah 
hh 
hh-ae~h 
hhh 
hhh-uuuh 
hhn 
hmm 
hmm'ml'nrn 
1 Im 
lm-lm 
huh 
i 
iiyeah 
1 m-hm 
1 mm 
I ~m-hm 
1fflffn-IYiYrt 
1 vn'rnrn 
1 myeah 
1 nn-hn 
nn-nnn I 
nu 1 
nuuuuu 1 
nyaa-haao 1 
nyeah 1 
o-w 1 
oa 1 
oh 20 
oh-eh I 
oh-kay 1 
oh-okay 2 
oh-yeah 1 
okay 8 
okay-hh I 
ooa I 
ookay 1 
oooh I 
ooooh I 
oop-ep-oop I 
u-kay 1 
Tab le  2: Al l  Grunts  in our  Corpus ,  with 
u-uh 4 
u-uun 1 
uam 1 
uh 38 
u.h-hn 2 
uh-hn-uh-hn 1 
uh-hu.h 3 
uh-~ 1 
uh.-uh 2 
u.h-uhmmm I 
nhh 2 
uhbh 1 
.hhm 1 
ulmy 2 
21 
um-hm-u.h-hm 1 
Rl-lr11'n 
~----n,,Hn 1 
au-lm 1 
un\]my 1 
unununu 1 
uu 5 
uum 5 
unmm 1 
uun 1 
uutth 1 
uuuuuuu 1 
WOW 1 
yah-yeah 1 
ye 1 
yeah 71 
yeah-oksy 1 
yeah-yeah I 
yeahaah I 
yeah.h 1 
yegh 1 
yeh-yeah I 
yei I 
yo 1 
yyeah I 
numbers  of occurrences 
32 
2. can represent all observed grunts, and 
3. unambiguously represents all meaningful 
differences in sound. 
While it is not possible to devise a single 
transcription scheme which is perfect for all 
purposes (Barry and Fourcin, 1992), it is clear 
that the current schemes all have room for 
improvement. 
3 P roposa l  
The basic idea is to start with the naive tran- 
scription tradition and then tighten it up. 
The advantages of using this as a starting 
point are two. First, it's convenient, since 
it is ASCII, familiar, and requires no special 
training. Second, as the result of the cumu- 
lative result of many years of novelists' and 
cartoonists' efforts to represent dialog, it has 
presumably evolved to be fairly adequate for 
capturing those sounds variations which are 
significant to meaning. 
The biggest need is to clarify and regular- 
ize the mapping from transcription to sound. 
This is the primary contribution of this paper: 
a specification of the actual phonetic values 
of each of the letters commonly used in tran- 
Scribing conversational grunts, as follows: 
u means schwa. This causes no confusion be- 
cause high vowels, including/u/, are van- 
ishingly rare in conversational grunts. 
n generally means nasalization. This is un- 
familiar in that English, unlike French, 
has no nasalized vowels in the words of 
the lexicon. However in grunts nasaliza- 
tion is common, as in ~n-hn and nyeah, 
and meaning-bearing. Occasionally there 
may be nasal consonants, and n can also 
be used for such cases, without confusion, 
because they appear to bear the same se- 
mantic value. 
h generally means breathiness. This often oc- 
curs at syllable boundaries, as in nh-huh. 
Some items involve breathiness through- 
out a syllable, others involve a consonan- 
ta l /h / ,  while others seem ambiguous be- 
tween these two. 
A single syllable-final 'h' bears no pho- 
netic value. 
tsk indicates an alveolar tongue click. These 
occur often in isolation, and occasionally 
grunt-initially 6.
- (hyphen) indicates a fairly strong syllable 
boundary. Phonetically this means a ma- 
jor dip in energy level, a sharp disconti- 
nuity in pitch, or a significant region of 
breathy or creaky voice. 
\ [repetit ion\]  Repetition of a letter indicates 
length and/or multiple weakly-separated 
syllables. 
uu as a syllable is a special case, indicating a 
creaky schwa 
All other letters have the normal values. 
There are two things that standard En- 
glish orthography provides no way to express. 
These are expressed as annotations, following 
the basic transcription and separated from it 
by a comma. 
cr indicates creaky voice, as in yeah:er. For 
further precision numbers from 1 to 3 
can be postposed, as in :crl for slightly 
creaky and :cr3 for extremely creaky. 
{nllrnhers~ numbers after a colon indicate 
anchor points for the pitch contour, on 
the standard 1 to 5 scale. Thus uh- 
uh:~-22 is a negative response or warn- 
ing, but uh-huh:43-22 is an blatantly un- 
interested back-channel, and uh-huh:32- 
34 is the standard, polite back-channeL 
Table 3 summarizes these letter-sound 
mappings. Table 4 suggests which sounds are 
most common. 
4 Adequacy  
This scheme does fairly well by the criteria of 
?2.4. 
?There are cases where the click is followed by a 
voiced sound without any perceptible pause (with a 
delay from the onset of the click to the onset of voicing 
of 50 to 170 milliseconds). 
33 
notation \[ p\]~onetic value 
non-trivial mappings 
h a single syllable-final 'h' bears no phonetic value, 
elsewhere 'h' indicates/h/or breathiness 
nasalization, occasionally a nasal consonant (other than/m/ )  
tsk alveolar tongue click 
u ~ (schwa) 
repetition of a letter length and/or multiple weakly-separated syllables 
- (hyphen) a fairly strong boundary between syllables or words 
standard mappings common in grunts 
m /m/  
o /o/  
a /a/  
y /jl, as in yeah and variants 
idiosyncratic spellings 
yeah / je~/  
kay /keI/, as in okay, ukay, llnkay, mkay etc. 
uu as a syllable, indicates a short creaky or glottalized schwa 
annotations 
:cr creaky voice (vocal fry) 
:1~5 pitch level 
Table 3: Regularized English Orthography for Conversational Grunts 
",7 
sound number 
/m/  
nasalization 
/h/and breathiness 
clicks 
creaky voice 
/schwa/  
/o/ 
/a/  
56 
20 
38 
25 
53 
109 
35 
5 
Table 4: Nllmbers of grunts in our corpus 
which include the various sound components 
1. As far as clarity and usability, this 
scheme has a direct and simple mapping from 
representation to the actual phonetics. It has 
been trivial to learn and easy to use (at least 
for the author; other labelers have not yet 
been trained). 
2. As far as representational coverage, this 
scheme is adequate for some 97% (=306/317) 
of the grunts which occur in our corpus. Thus 
it is not truly complete, and labelers must 
be allowed to escape into standard lexical 
orthography (for things like oop-ep-oop and 
wow), into IPA (for eases like achh and yegh, 
palatal and velar fricatives, respectively), and 
into ad hoc notion (for cases like throat clear- 
ings and noisy exhalations). 
3. As far as precision, the scheme allows 
sumciently detailed representation; at least to 
a first appro~mation. In particular, it covers 
all known meaningful phonetic variations. It 
is, however possible that other phonetic dis- 
tinctions are also significant. For example, 
it may be that the exact height of a vowel 
34 
matters, or the exact time point at which a 
vowel starts getting creaky, or the presence 
of glottal stops, lip rounding, glottalization, 
falsetto, and so on matter, or the precise de- 
tails of pitch and energy contours matter. 
Conversely, the scheme is not over-precise: 
all the phonetic elements represented in the 
scheme appear to bear meanings (Ward, 
2000). 
Regarding unambignity, the scheme is an 
improvement but has one failing: repetition 
of a letter represents either extended uration 
or the presence of multiple syllables. As these 
two phonetic features are generally correlated, 
and the difference in meaning between them 
is anyway subtle, this may not be a major 
problem. 
5 Open I ssues  
This notation assumes that the component 
sounds are categorical (except for creakiness 
and pitch), but this may in fact not be the 
case. Rather it may be that the phonetic 
components of grunts have a "gradual, rather 
than binary, oppositional character" (3akob- 
son and Waugh, 1979). This is a problem 
especially for nasalization and for vowels: it 
may be that there is an infinite number of 
slightly but significantly different variations. 
Further study is required. 
Experiments with multiple independent la- 
belers are needed to evaluate usability and 
measure cross-labeler agreement. 
Applying this notation can be complicated 
by dialect and individual differences. For ex- 
ample, the primary filler for one speaker in 
our corpus was aura. Right now it is not 
known whether this is a mere pronunciation 
variation, perhaps dialect-related, or signif- 
icantly different from urn. More study is 
needed. 
Other languages also have conversational 
grunts, for example, oua/s and hien in French, 
ja and hm in German, and un, he and ya in 
Japanese (Ward, 1998), and it may be pos- 
sible to use or adapt the present scheme for 
these and other languages. 
Re ferences  
W. J. Barry and A. 3. Fourcin. 1992. Levels of la- 
beling. Computer Speech and Language, pages 
1-14. 
J. Hamaker, Y. Zeng, and J. Picone. 1998. Rules 
and guidelines for transcription and segmenta- 
tion of the switchboard large vocabulary con- 
versational speech recognition corpus, version 
7.1. Technical report, Institute for Signal and 
Information Processing, Mississippi State Uni- 
versity. 
Inn Hutchby and Robin Wooflltt. 1999. Conver- 
sation Analysis. Blackwell. 
Roman Jakobson and Linda Waugh. 1979. The 
Sound Shape of Language. Indiana University 
Press. 
T. Lander. 1996. The CSLU labeling uide. Tech- 
nical Report CSLU-014--96, Center for Spoken 
Language Understanding, Oregon Graduate In- 
stitute of Science and Technology. 
Fernando Poyatos. 1975. Cross-cultural study of 
paralingulstic "alternants" in face-to-face inter- 
action. In Adam Kendon, Richard M. Harris, 
and Mary tL Key, editors, Organization of Be- 
havior in Face-to-Face Interaction, pages 285-- 
314. Mouton. 
George L. Trager. 
approximation. 
1-12. 
1958. Paralanguage: A first 
Studies in Linguistics, pages 
Nigel Ward. 1998. The relationship between 
sound and me~nlng in Japanese back-channel 
grunts. In Proceedings ofthe ~th Annual Meet- 
ing of the (Japanese) Association for Natural 
Language Processing, pages 464-467. 
Nigel Ward. 2000. The challenge of non-lexical 
speech sounds. In International Conference on 
Spoken Language Processing. to appear. 
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 182?189,
Columbus, June 2008. c?2008 Association for Computational Linguistics
A Framework for Model-based Evaluation of Spoken Dialog Systems
Sebastian Mo?ller
Deutsche Telekom Laboratories
Technische Universita?t Berlin
10587 Berlin, Germany
sebastian.moeller@telekom.de
Nigel G. Ward
Computer Science Department
University of Texas at El Paso
El Paso, Texas 79968, USA
nigelward@acm.org
Abstract
Improvements in the quality, usability and ac-
ceptability of spoken dialog systems can be
facilitated by better evaluation methods. To
support early and efficient evaluation of dia-
log systems and their components, this paper
presents a tripartite framework describing the
evaluation problem. One part models the be-
havior of user and system during the interac-
tion, the second one the perception and judg-
ment processes taking place inside the user,
and the third part models what matters to sys-
tem designers and service providers. The pa-
per reviews available approaches for some of
the model parts, and indicates how anticipated
improvements may serve not only developers
and users but also researchers working on ad-
vanced dialog functions and features.
1 Introduction
Despite the utility of many spoken dialog systems
today, the user experience is seldom satisfactory.
Improving this is a matter of great intellectual in-
terest and practical importance. However improve-
ments can be difficult to evaluate effectively, and this
may be limiting the pace of innovation: today, valid
and reliable evaluations still require subjective ex-
periments to be carried out, and these are expensive
and time-consuming. Thus, the needs of system de-
velopers, of service operators, and of the final users
of spoken dialog systems argue for the development
of additional evaluation methods.
In this paper we focus on the prospects for an
early and model-based evaluation of dialog systems.
Doing evaluation as early as possible in the de-
sign and development process is critical for improv-
ing quality, reducing costs and fostering innovation.
Early evaluation renders the process more efficient
and less dependent on experience, hunches and intu-
itions. With the help of such models predicting the
outcome of user tests, the need for subjective test-
ing can be reduced, restricting it to that subset of the
possible systems which have already been vetted in
an automatic or semi-automatic way.
Several approaches have already been presented
for semi-automatic evaluation. For example, the
PARADISE framework (Walker et al, 1997) predicts
the effects of system changes, quantified in terms of
interaction parameters, on an average user judgment.
Others (Araki and Doshita, 1997; Lo?pez-Co?zar et
al., 2003; Mo?ller et al, 2006) have developed dialog
simulations to aid system optimization. However the
big picture has been missing: there has been no clear
view of how these methods relate to each other, and
how they might be improved and joined to support
efficient early evaluation.
The remainder of this paper is organized as fol-
lows. Section 2 gives a brief review of different
evaluation purposes and terminology, and outlines a
new tripartite decomposition of the evaluation prob-
lem. One part of our framework models the behav-
ior of user and system during the interaction, and
describes the impact of system changes on the inter-
action flow. The second part models the perception
and judgment processes taking place inside the user,
and tries to predict user ratings on various percep-
tual dimensions. The third part models what mat-
ters to system designers and service providers for
182
a specific application. Sections 3, 4, and 5 go into
specifics on the three parts of the framework, dis-
cussing which components are already available or
conceivable. Finally, Section 6 discusses the poten-
tial impact of the approach, and Section 7 lists the
issues to be resolved in future work.
2 Performance, Quality, Usability and
Acceptability Evaluation
Developers tend to use indices of performance to as-
sess their systems. The performance indicates the
?ability of a system to provide the function it has
been designed for? (Mo?ller, 2005). The function
and an appropriate measure for quantifying the de-
gree of fulfillment may easily be determined for cer-
tain components ? e.g. word accuracy for a speech
recognizer or concept error rate for a speech under-
standing module ? but it is harder to specify for
other components, such as a dialog manager or an
output generation module. However, definitive mea-
sures of component quality are not always neces-
sary: what matters for such a module is its contri-
bution to the quality of the entire interaction, as it is
perceived by the user.
We follow the definition of the term quality as
introduced by Jekosch (2000) and now accepted
for telephone-based spoken dialog services by the
International Telecommunication Union in ITU-T
Rec. P.851 (2003): ?Result of judgment of the per-
ceived composition of an entity with respect to its
desired composition?. Quality thus involves a per-
ception process and a judgment process, during
which the perceiving person compares the percep-
tual event with a (typically implicit) reference. It is
the comparison with a reference which associates a
user-specific value to the perceptual event. The per-
ception and the comparison processes take place in a
particular context of use. Thus, both perception and
quality should be regarded as ?events? which hap-
pen in a particular personal, spatial, temporal and
functional context.
Usability is one sub-aspect of the quality of the
system. Following the definition in ISO 9241 Part
11 (1998), usability is considered as the ?extent to
which a product can be used by specified users to
achieve specified goals with effectiveness, efficiency
and satisfaction in a specified context of use?. Us-
ability is degraded when interaction problems oc-
cur. Such problems influence the perceptual event
of the user interacting with the system, and conse-
quently the quality s/he associates with the system
as a whole. This may have consequences for the
acceptability or the system or service, that is, how
readily a customer will use the system or service.
This can be quantified, for example as the ratio of
the potential user population to the size of the target
group.
It is the task of any evaluation to quantify as-
pects of system performance, quality, usability or
acceptability. The exact target depends on the pur-
pose of the evaluation (Paek, 2007). For example,
the system developer might be most interested in
quantifying the performance of the system and its
components; s/he might further need to know how
the performance affects the quality perceived by the
user. In contrast, the service operator might instead
be most interested in the acceptability of the ser-
vice. S/he might further want to know about the
satisfaction of the user, influenced by the usability
of the system, and also by other (e.g. hedonic) as-
pects like comfort, joy-of-use, fashion, etc. Differ-
ent evaluation approaches may be complementary,
in the sense that metrics determined for one purpose
may be helpful for other purposes as well. Thus, it is
useful to describe the components of different eval-
uation approaches in a single framework.
Figure 1 summarizes our view of the evaluation
landscape. At the lower left corner is what we can
change (the dialog system), at the right is what the
service operator might be interested in (a metric for
the value of the system). In between are three com-
ponents of a model of the processes taking place in
the evaluation. The behavior model describes how
system and user characteristics determine the flow
of the interaction and translate this to quantitative
descriptors. The perception and judgment model
describes how the interaction influences the percep-
tual and quality events felt by the user, and trans-
lates these to observable user judgments. Finally the
value model associates a certain value to the qual-
ity judgments, depending on the application. The
model properties have been grouped in three layers:
aspects of the user and his/her behavior, aspects of
the system in its context-of-use, and the work of an
external observer (expert) carrying out the evalua-
183
BehaviorModel Perception and Judgment Model Value Model
UserBehavior
SystemBehavior
InteracionBehavior
Use
r La
yer
Exp
ert
Lay
er
Sys
tem
Lay
er
InteractionPhenomena
InteractionParameters
PerceptualEvent
DimensionDescriptors
QualityJudgments
Reference
QualityEvent
QualityAspects ValueDescription
PerceptualQualityDimensions
Designerand OperatorRequirements
Figure 1: Tripartite view of a model-based evaluation. Observable properties are in boxes, inferred or hidden properties
are in ovals. The layers organize the properties as mostly user-related, mostly system-related, and mostly expert-
related, and mostly system-related.
tion. They have further been classified as to whether
they are observable (boxes) or hidden from the eval-
uator (ovals).
The next three sections go through the three parts
of the model left-to-right, explaining the needs, cur-
rent status, and prospects.
3 Behavior Model
The behavior model translates the characteristics of
the system and the user into predicted interaction be-
havior. In order to be useful, the representations of
this behavior must be concise.
One way to describe dialog behavior is with in-
teraction parameters which quantify the behavior of
the user and/or the system during the interaction.
Such parameters may be measured instrumentally or
given by expert annotation. In an attempt to sys-
tematize best practice, the ITU-T has proposed a
common set of interaction parameters suitable for
the evaluation of telephone-based spoken dialog sys-
tems in ITU-T Suppl. 24 (2005). These parameters
have been developed bottom-up from a collection of
evaluation reports over the last 15 years, and include
metrics related to dialog and communication in gen-
eral, meta-communication, cooperativity, task, and
speech-input performance (Mo?ller, 2005). Unfortu-
nately, it as is yet unclear which of these parameters
relate to quality from a user?s point-of-view. In addi-
tion, some metrics are missing which address critical
aspects for the user, e.g. parameters for the quality
and attractiveness of the speech output.
Another manageable way to describe system be-
havior is to focus on interaction phenomena. Sev-
eral schemes have been developed for classifying
such phenomena, such as system errors, user errors,
points of confusion, dead time, and so on (Bernsen et
al., 1998; Ward et al, 2005; Oulasvirta et al, 2006).
Patterns of interaction phenomena may be reflected
in interaction parameter values, and may be identi-
fied on that basis. Otherwise, they have to be deter-
mined by experts and/or users, by means of obser-
vation, interviews, thinking-aloud, and other tech-
niques from usability engineering. (Using this ter-
minology we can understand the practice of usability
testing as being the identification of interaction phe-
nomena, also known as ?usability events? or ?criti-
cal incidences?, and using these to estimate specific
quality aspects or the overall value of the system.)
Obtaining the interaction parameters and classi-
fying the interaction phenomena can be done, ob-
viously, from a corpus of user-system interactions.
The challenge for early evaluation is to obtain these
without actually running user tests. Thus, we would
like to have a system behavior model and a user be-
havior model to simulate interaction behavior, and
to map from system parameters and user properties
to interaction parameters or phenomena. The value
of such models for a developer is clear: they could
184
enable estimation of how a change in the system
(e.g. a change in the vocabulary) might affect the
interaction properties. In addition to the desired ef-
fects, the side-effects of system changes are also im-
portant. Predicting such side-effects will substan-
tially decrease the risk and uncertainty involved in
dialogue design, thereby decreasing the gap between
research and commercial work on dialog system us-
ability (Heisterkamp, 2003; Pieraccini and Huerta,
2005).
Whereas modeling system behavior in response to
user input is clearly possible (since in the last resort
it is possible to fully implement the system), user be-
havior can probably not be modeled in closed form,
because it unavoidably relates to the intricacies of
the user and reflects the time-flow of the interaction.
Thus, it seems necessary to employ a simulation
of the interaction, as has been proposed by Araki
and Doshita (1997) and Lo?pez-Co?zar et al (2003),
among others.
One embodiment of this idea is the MeMo work-
bench (Mo?ller et al, 2006), which is based on
the idea of running models of the system and of
the user in a dedicated usability testing workbench.
The system model is a description of the possi-
ble tasks (system task model) plus a description of
the system?s interaction behavior (system interac-
tion model). The user model is a description of the
tasks a user would want to carry out with the sys-
tem (user task model) plus a description of the steps
s/he would take to reach the goal when faced with
the system (user interaction model). Currently the
workbench uses simple attribute-value descriptions
of tasks the system is able to carry out. From these,
user-desired tasks may be derived, given some back-
ground knowledge of the domain and possible tasks.
The system interaction model is described by a state
diagram which models interactions as paths through
a number of dialog states. The system designer pro-
vides one or several ?intended paths? through the in-
teraction, which lead easily and/or effectively to the
task goal.
The user?s interaction behavior will strongly de-
pend on the system output in the previous turn.
Thus, it is reasonable to build the user interaction
model on top of the system interaction model: The
user mainly follows the ?intended path?, but at cer-
tain points deviations from this path are generated in
a probabilistic rule-based manner. For example, the
user might deviate from the intended path, because
s/he does not understand a long system prompt, or
because s/he is irritated by a large number of op-
tions. Each deviation from the intended path has
an associated probability; these are calculated from
system characteristics (e.g. prompt length, number
of options) and user characteristics (e.g. experience
with dialog systems, command of foreign languages,
assumed task and domain knowledge).
After the models have been defined, simulations
of user-system interactions can be generated. These
interactions are logged and annotated on different
levels in order to detect interaction problems. Us-
ability predictions are obtained from the (simulated)
interaction problems. The simulations can also sup-
port reinforcement learning or other methods for au-
tomatically determining the best dialog strategy.
Building user interaction models by hand is
costly. As an alternative to explicitly defining rules
and probabilities, simulations can be based on data
sets of actual interactions, augmented with annota-
tions such as indications of the dialog state, current
subtask, inferred user state, and interaction phenom-
ena. Annotations can be generated by the dialog
participants themselves, e.g. by re-listening after the
fact (Ward and Tsukahara, 2003), or by top com-
municators, decision-makers, trend-setters, experts
in linguistics and communication, and the like. Ma-
chine learning techniques can help by providing pre-
dictions of how users tend to react in various situa-
tions from lightly annotated data.
4 Perception and Judgment Model
Once the interaction behavior is determined, the
evaluator needs to know about the impact it has on
the quality perceived by the user. As pointed out in
Section 2, the perception and judgments processes
take place in the human user and are thus hidden
from the observer. The evaluator may, however, ask
the user to describe the perceptual event and/or the
quality event, either qualitatively in an open form or
quantitatively on rating scales. Provided that the ex-
periment is properly planned and carried out, user
quality judgments can be considered as direct qual-
ity measurements, reflecting the user?s quality per-
ception.
185
Whereas user judgments on quality will reflect the
internal reference and thus depend heavily on the
specific context and application, it may be assumed
that the characteristics of the perceptual event are
more universal. For example, it is likely that sam-
ples of observers and/or users would generally agree
on whether a given system could be characterized
as responsive, smooth, or predictable, etc. regardless
of what they feel about the importance of each such
quality aspect. We may take advantage of this by
defining a small set of universal perceptual quality
dimensions, that together are sufficient for predict-
ing system value from the user?s point-of-view.
In order to quantify the quality event and to iden-
tify perceptual quality dimensions, psychometric
measurement methods are needed, e.g. interaction
experiments with appropriate measurement scales.
Several attempts have been made to come up with
a common questionnaire for user perception mea-
surement related to spoken dialog systems, for ex-
ample the SASSI questionnaire (Hone and Graham,
2000) for systems using speech input, and the ITU-
standard augmented framework for questionnaires
(ITU-T Rec. P.851, 2003) for systems with both
speech-input and speech-output capabilities. Studies
of the validity and the reliability of these question-
naires (Mo?ller et al, 2007) show that both SASSI
and P.851 can cover a large number of different qual-
ity and usability dimensions with a high validity, and
mainly with adequate reliability, although the gener-
alizability of these results remains to be shown.
On the basis of batteries of user judgments ob-
tained with these questionnaires, dimension descrip-
tors of the perceptual quality dimensions can be ex-
tracted by means of factor analysis. A summary of
such multidimensional analyses in Mo?ller (2005b)
reveals that users? perceptions of quality and usabil-
ity can be decomposed into around 5 to 8 dimen-
sions. The resulting dimensions include factors such
as overall acceptability, task effectiveness, speed,
cognitive effort, and joy-of-use. It should be noted
that most such efforts have considered task-oriented
systems, where effectiveness, efficiency, and suc-
cess are obviously important, however these dimen-
sions may be less relevant to systems designed for
other purposes, for example tutoring or ?edutain-
ment? (Bernsen et al, 2004), and additional factors
may be needed for such applications.
In order to describe the impact of the interac-
tion flow on user-perceived quality, or on some of
its sub-dimensions, we would ideally model the hu-
man perception and judgment processes. Such an
approach has the clear advantage that the resulting
model would be generic, i.e. applicable to differ-
ent systems and potentially for different user groups,
and also analytic, i.e. able to explain why certain in-
teraction characteristics have a positive or negative
impact on perceived quality. Unfortunately, the per-
ception and judgment processes involved in spoken-
dialog interaction are not yet well understood, as
compared, for example, to those involved in listen-
ing to transmitted speech samples and judging their
quality. For the latter, models are available which
estimate quality with the help of peripheral audi-
tory perception models and a signal-based compar-
ison of representations of the perceptual event and
the assumed reference (Rix et al, 2006). They are
able to estimate user judgments on ?overall quality?
with an average correlation of around 0.93, and are
widely used for planning, implementing and moni-
toring telephone networks.
For interactions with spoken dialog systems, the
situation is more complicated, as the perceptual
events depend on the interaction between user and
systems, and not on one speech signal alone. A way
out is not to worry about the perception processes,
and instead to use simple linear regression models
for predicting an average user judgment from vari-
ous interaction parameters. The most widely used
framework designed to support this sort of early
evaluation is PARADISE (Walker et al, 1997). The
target variable of PARADISE is an average of several
user judgments (labeled ?user satisfaction?) of dif-
ferent system and interaction aspects, such as system
voice, perceived system understanding, task ease,
interaction pace, or the transparency of the interac-
tion. The interaction parameters are of three types,
those relating to efficiency (including elapsed time
and the number of turns), those relating to ?dialog
quality? (including mean recognition score and the
number of timeouts and rejections), and a measure
of effectiveness (task success). The model can be
trained on data, and the results are readily inter-
pretable: they can indicate which features of the in-
teraction are most critical for improving user satis-
faction.
186
PARADISE-style models can be very helpful tools
for system developers. For example, a recent inves-
tigation showed that the model can be used to ef-
fectively determine the minimum acceptable recog-
nition rate for a smart-home system, leading to
the same critical threshold as that obtained from
user judgments (Engelbrecht and Mo?ller, 2007).
However, experience also shows that the PARADISE
framework does not reliably give valid predictions of
individual user judgments, typically covering only
around 40-50% of the variance in the data it is
trained on. The generality is also limited: cross-
system extrapolation works sometimes but other
times has low accuracy (Walker et al, 2000; Mo?ller,
2005). These limitations are easy to understand in
terms of Figure 1: over-ambitious attempts to di-
rectly relate interaction parameters to a measure of
overall system value seem unlikely to succeed in
general. Thus it seems wise to limit the scope of the
perception and judgment component to the predic-
tion of values on the perceptual quality dimensions.
In any case, there are several ways in which such
models could be improved. One issue is that a linear
combination of factors is probably not generally ad-
equate. For example, parameters like the number of
turns required to execute a specific task will have a
non-zero optimum value, at least for inexperienced
users. An excessively low number of turns will be
as sure a sign of interaction problems as an exces-
sively large number. Such non-linear effects can-
not be handled by linear models which only support
relationships like ?the-more-the-better? or ?the-less-
the-better?. Non-linear algorithms may overcome
these limitations. A second issue is that of tempo-
ral context: instead of using a single input vector
of interaction parameters for each dialog, it may be
possible to apply a sequence of feature vectors, one
for each exchange (user-system utterance pair). The
features may consist not only of numeric measures
but also of categories encoding interaction phenom-
ena. Using this input one could then perhaps use a
neural network or Hidden-Markov Model to predict
various user judgments at the end of the interaction.
5 Value Model
Even if a model can predict user judgments of ?over-
all quality? with high validity and reliability, this is
not necessarily a good indicator of the acceptability
of a service. For example, systems with a sophis-
ticated and smooth dialog flow may be unaccept-
able for frequent users because what counts for them
is effectiveness and efficiency only. Different users
may focus on different quality dimensions in differ-
ent contexts, and weight them according to the task,
context of use, price, etc.
A first step towards addressing this problem
is to define quality aspects that a system devel-
oper or service operator might be concerned about.
There can be many such, but in usability engineer-
ing they are typically categorized into ?effective-
ness?, ?efficiency? and ?satisfaction?. A more de-
tailed taxonomy of quality aspects can be found in
Mo?ller (2005). On the basis of this or other tax-
onomizations, value prediction models can be de-
veloped. For example, a system enabling 5-year
old girls to ?talk to Barbie? might ascribe little im-
portance to task completion, speech recognition ac-
curacy, or efficiency, but high importance to voice
quality, responsiveness, and unpredictability. The
value model will derive a value description which
takes such a weighting into account. A model for
systems enabling police officers on patrol to obtain
information over the telephone would have very dif-
ferent weights.
Unfortunately, there appear to be no published de-
scriptions of value prediction models, perhaps be-
cause they are very specific or even proprietary, de-
pending on a company?s business logic and cus-
tomer base. Such models probably need not be very
complex: it likely will suffice to ascribe weights to
the perceptual quality dimensions, or to quality as-
pects derived from system developer and/or service
operator requirements. Appropriate weights may be
uncovered in stakeholder workshops, where design-
ers, vendors, usability experts, marketing strategists,
user representatives and so on come together and
discuss what they desire or expect.
6 Broader Impacts
We have presented a tripartite evaluation framework
which shows the relationship between user and sys-
tem characteristics, interaction behavior, perceptual
and quality events, their descriptions, and the final
value of the system or service. In doing so, we
187
have mainly considered the needs of system devel-
opers. However, an evaluation framework that sup-
ports judgments of perceived quality could provide
additional benefits for users. We can imagine user-
specific value models, representing what is impor-
tant to specified user groups. These could be so-
licited for an entire group, or inferred from each
user?s own personal history of interactions and deci-
sions, e.g, through a personalization database avail-
able to the service operator. The models could also
be used to support system selection, or to inform
real-time system customization or adaptation.
Better evaluation will also support the needs of
the research community. With the help of model-
based evaluation, it will become easier for re-
searchers not only to do evaluation more efficiently,
but also to to produce more meaningful evaluation
results; saying not just ?this feature was useful? but
also providing quantitative statements of how much
the feature affects various interaction parameters,
and from that how much it impacts the various qual-
ity dimensions, and ultimately the value itself. This
will make evaluation more meaningful and make it
easy for others to determine when an innovation is
worth adopting, speeding technology transfer.
One might worry that a standardized framework
might only be useful for evaluating incremental im-
provements, thereby discouraging work on radically
different dialog design concepts. However well-
designed evaluation components should enable this
framework to work for systems of any type, meaning
that it may be easier to explore new regions of the
design space. In particular it may enable more ac-
curate prediction of the value of design innovations
which in isolation may not be effective, but which in
combination may be.
7 Future Work
Although examples of some model components are
available today, notably several interaction simula-
tions and the PARADISE framework for predicting
user judgments from interaction parameters, these
are limited. To realize a complete and generally use-
ful evaluation model will require considerable work,
for example, on:
? User behavior model: Of the three compo-
nents, perhaps the greatest challenges are in
the development of user behavior models. We
need to develop methods which produce simu-
lated behavior which is realistic (congruent to
the behavior of real users), and/or which pro-
duce interaction parameters and/or quality in-
dicators comparable to those obtained by sub-
jective interaction experiments. It is yet un-
clear whether realistic user behavior can also be
generated for more advanced systems and do-
mains, such as computer games, collaborative
problem solving systems, or educational sys-
tems. We also need to develop models that ac-
curately represent the behavior patterns of var-
ious user groups.
? Interaction parameters: Several quality aspects
are still not reflected in the current parameter
sets, e.g. indices for the quality of speech out-
put. Some approaches are described in Mo?ller
and Heimansberg (2006), but the predictive
power is still too limited. In addition, many pa-
rameters still have to be derived by expert an-
notation. It may be possible to automatically
infer values for some parameters from proper-
ties of the user?s and system?s speech signals,
and such analyses may be a source for new pa-
rameters, covering new quality aspects.
? Perceptual and quality events and reference:
These items are subject of ongoing research in
related disciplines, such as speech quality as-
sessment, sound quality assessment, and prod-
uct sound design. Ideas for better, more realis-
tic modeling may be derived from cooperations
with these disciplines.
? Quality judgments and dimension descriptors:
In addition to the aspects covered by the SASSI
and P.851 questionnaires, psychologists have
defined methods for assessing cognitive load,
affect, affinity towards technology, etc. Input
from such questionnaires may provide a better
basis for developing value models.
Although a full model may be out of reach for the
next decade, a more thorough understanding of hu-
man behavior, perception and judgment processes is
not only of intrinsic interest but promises benefits
enough to make this a goal worth working towards.
188
Acknowledgments
This work was supported in part by NSF Grant No.
0415150.
References
M. Araki, and S. Doshita. 1997. Automatic Evaluation
Environment for Spoken Dialogue Systems. Dialogue
Processing in Spoken Language Systems, ECAI?96
Workshop Proceedings, Springer Lecture Notes in
Artificial Intelligence No. 1236, 183-194, Springer,
Berlin.
N. O. Bernsen, H. Dybkj?r, and L. Dybkj?r. 1998. De-
signing Interactive Speech Systems: From First Ideas
to User Testing. Springer, Berlin.
N. O. Bernsen, L. Dybkj?r, L., and S. Kiilerich. 2004.
Evaluating Conversation with Hans Christian Ander-
sen. Proc. 4th Int. Conf. on Language Resources and
Evaluation (LREC 2004), 3, pp. 1011-1014, Lisbon.
K.-P. Engelbrecht, and S. Mo?ller. 2007. Using Linear Re-
gression Models for the Prediction of Data Distribu-
tions. Proc. 8th SIGdial Workshop on Discourse and
Dialogue, Antwerp, pp. 291-294.
P. Heisterkamp. 2003. ?Do not attempt to light with
match!?: Some Thoughts on Progress and Research
Goals in Spoken Dialog Systems. Proc. 8th Europ.
Conf. on Speech Communication and Technology (Eu-
rospeech 2003 ? Switzerland).
K. S. Hone, and R. Graham. 2000. Towards a Tool for the
Subjective Assessment of Speech System Interfaces
(SASSI). Natural Language Engineering, 3(3-4): 287-
303.
ITU-T Rec. P.851. 2003. Subjective Quality Eval-
uation of Telephone Services Based on Spoken
Dialogue Systems. International Telecommunication
Union, Geneva.
ITU-T Suppl. 24 to P-Series Rec. 2005. Parameters
Describing the Interaction with Spoken Dialogue
Systems. International Telecommunication Union,
Geneva.
ISO Standard 9241 Part 11. 1998. Ergonomic Require-
ments for Office Work with Visual Display Terminals
(VDTs) ? Part 11: Guidance on Usability. Interna-
tional Organization for Standardization, Geneva.
U. Jekosch. 2000. Sprache ho?ren und beur-
teilen: Ein Ansatz zur Grundlegung der
Sprachqualita?tsbeurteilung. Habilitation thesis
(unpublished), Universita?t/Gesamthochschule, Essen.
R. Lo?pez-Co?zar, A. De la Torre, J. Segura, and A. Rubio.
2003. Assessment of Dialog Systems by Means of a
New Simulation Technique. Speech Communication,
40: 387-407.
S. Mo?ller, P. Smeele, H. Boland, and J. Krebber. 2007.
Evaluating Spoken Dialogue Systems According to
De-Facto Standards: A Case Study. Computer Speech
and Language, 21: 26-53.
S. Mo?ller, R. Englert, K.-P. Engelbrecht, V. Hafner,
A. Jameson, A. Oulasvirta, A. Raake, and N. Rei-
thinger. 2006. MeMo: Towards Automatic Usability
Evaluation of Spoken Dialogue Services by User Er-
ror Simulations. Proc. 9th Int. Conf. on Spoken Lan-
guage Processing (Interspeech 2006 ? ICSLP), Pitts-
burgh PA, pp. 1786-1789.
S. Mo?ller, and J. Heimansberg. 2006. Estimation of
TTS Quality in Telephone Environments Using a
Reference-free Quality Prediction Model. Proc. 2nd
ISCA/DEGA Tutorial and Research Workshop on Per-
ceptual Quality of Systems, Berlin, pp. 56-60.
S. Mo?ller. 2005. Quality of Telephone-Based Spoken Di-
alogue Systems. Springer, New York NY.
S. Mo?ller. 2005b. Perceptual Quality Dimensions of Spo-
ken Dialogue Systems: A Review and New Exper-
imental Results. Proc. 4th European Congress on
Acoustics (Forum Acusticum Budapest 2005), Bu-
dapest, pp. 2681-2686.
A. Oulasvirta, S. Mo?ller, K.-P. Engelbrecht, and A. Jame-
son. 2006. The Relationship of User Errors to Per-
ceived Usability of a Spoken Dialogue System. Proc.
2nd ISCA/DEGA Tutorial and Research Workshop on
Perceptual Quality of Systems, Berlin, pp. 61-67.
T. Paek. 2007. Toward Evaluation that Leads to Best
Practices: Reconciling Dialog Evaluation in Research
and Industry. Bridging the Gap: Academic and Indus-
trial Research in Dialog Technologies Workshop Pro-
ceedings, Rochester, pp. 40-47.
R. Pieraccini, J. Huerta. 2005. Where Do We and Com-
mercial Spoken Dialog Systems. Proc. 6th SIGdial
Workshop on Discourse and Dialogue, Lisbon, pp. 1-
10.
A. W. Rix, J. G. Beerends, D.-S. Kim, P. Kroon, and
O. Ghitza. 2006. Objective Assessment of Speech and
Audio Quality ? Technology and Applications. IEEE
Trans. Audio, Speech, Lang. Process, 14: 1890-1901.
M. Walker, C. Kamm, and D. Litman. 2000. Towards
Developing General Models of Usability with PAR-
ADISE. Natural Language Engineering, 6: 363-377.
M. A. Walker, D. J. Litman, C. A. Kamm, and A. Abella.
1997. PARADISE: A Framework for Evaluating Spo-
ken Dialogue Agents. Proc. of the ACL/EACL 35th
Ann. Meeting of the Assoc. for Computational Linguis-
tics, Madrid, Morgan Kaufmann, San Francisco CA,
pp. 271-280.
N. Ward, A. G. Rivera, K. Ward, and D. G. Novick. 2005.
Root Causes of Lost Time and User Stress in a Simple
Dialog System. Proc. 9th European Conf. on Speech
Communication and Technology (Interspeech 2005),
Lisboa.
N. Ward, and W. Tsukahara. 2003. A Study in Respon-
siveness in Spoken Dialogue. International Journal of
Human-Computer Studies, 59: 603-630.
189
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 198?206,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
A Bottom-Up Exploration of
the Dimensions of Dialog State in Spoken Interaction
Nigel G. Ward
Department of Computer Science
University of Texas at El Paso
El Paso, Texas, 79968, USA
nigelward@acm.org
Alejandro Vega
Department of Computer Science
University of Texas at El Paso
El Paso, Texas, 79968, USA
avega5@miners.utep.edu
Abstract
Models of dialog state are important, both
scientifically and practically, but today?s best
build strongly on tradition. This paper
presents a new way to identify the impor-
tant dimensions of dialog state, more bottom-
up and empirical than previous approaches.
Specifically, we applied Principal Compo-
nent Analysis to a large number of low-level
prosodic features to find the most important
dimensions of variation. The top 20 out of
76 dimensions accounted for 81% of the vari-
ance, and each of these dimensions clearly re-
lated to dialog states and activities, including
turn taking, topic structure, grounding, empa-
thy, cognitive processes, attitude and rhetori-
cal structure.
1 Introduction
What set of things should a dialog manager be re-
sponsible for? In other words, which aspects of the
current dialog state should the dialog manager track?
These questions are fundamental: they define the
field of computational dialog modeling and deter-
mine the basic architectures of our dialog systems.
However the answers common in the field today
arise largely from tradition, rooted in the concerns
of precursor fields such as linguistics and artificial
intelligence (Traum and Larsson, 2003; McGlashan
et al, 2010; Bunt, 2011).
We wish to provide a new perspective on these
fundamental questions, baed on a bottom-up, empir-
ical investigations of dialog state. We hope thereby
to discover new facets of dialog state and to obtain
estimates of which aspects of dialog state are most
important.
2 Aims
There are many ways to describe dialog state, but in
this paper we seek a model with 7 properties:
Orthogonal to Content. While the automatic
discovery of content-related dialog states has seen
significant advances, we are interested here in the
more general aspects of dialog state, those that oc-
cur across many if not all domains.
Scalar. While it is descriptively convenient to re-
fer to discrete states (is-talking, is-waiting-for-a-yes-
no-answer, and so on), especially for human ana-
lysts, in general it seems that scales are more natural
for many or all aspects of dialog state, for example,
one?s degree of confidence, the strength of desire to
take the turn, or the solidity of grounding.
Non-Redundant. While various levels and an-
gles are used in describing aspects of dialog state ?
and many of these are interrelated, correlated, and
generally tangled ? we would like a set of dimen-
sions which is as concise as possible and mutually
orthogonal.
Continuously Varying. While it is common to
label dialog states only at locally stable times, for
example when neither party is speaking, or only
over long spans, for example, utterances, we want a
model that can support incremental dialog systems,
able to describe the instantaneous state at any point
in time, even in the middle of an utterance.
Short-Term. While aspects of dialog state can
involve quite distant context, we here focus on the
aspects important in keeping the dialog flowing over
198
short time-scales.
Non-Exhaustive. While dialog states can be ar-
bitrarily complex, highly specific, and intricately re-
lated to content, a general model can only be ex-
pected to describe the frequently important aspects
of state.
Prioritized. While no aspects of dialog are unin-
teresting, we want to know which aspects of dialog
state are more important and commonly relevant.
3 Approach
To be as empirical as possible, we want to consider
as much data as possible. We accordingly needed to
use automatic techniques. In particular, we chose to
base our analysis on objective manifestations of di-
alog state. Among the many possible such manifes-
tations ? discourses markers, gesture, gaze, and so
on ? we chose to use only prosody. This is because
the importance of prosody in meta-communication
and dialog control has often been noted, because the
continuous nature of (most) prosodic features is con-
venient for our aims, and because prosodic features
are relatively easy to compute.
Given our aims and such features, it is natural
to do Principal Components Analysis (PCA). This
well-known method automatically identifies the fac-
tors underlying the observed variations across mul-
tiple features. We also hoped that PCA would sep-
arate out, as orthogonal factors, aspects of prosody
that truly relate to dialog from aspects with lexical,
phrasal, or other significance.
4 Related Research
While dialog states have apparently not previ-
ously been tackled using PCA, other dimensionality-
reduction methods have been used. Clustering
has previously been applied as a way to catego-
rize user intention-types and goals, using lexical-
semantic features and neighboring-turn features as
inputs (Lefevre and de Mori, 2007; Lee et al, 2009),
among other methods (Gasic and Young, 2011).
Hidden Markov Models have been used to identify
dialog ?modes? that involve common sequences of
dialog-acts (Boyer et al, 2009). There is also work
that uses PCA to reduce multi-factor subjective eval-
uations of emotion, style, or expressiveness into a
few underlying dimensions, for example (Barbosa,
2009). In addition, clustering over low-level patterns
of turn-taking has been used to identify a continuum
of styles (Grothendieck et al, 2011). However anal-
ysis of dialog states based on prosodic features has
not previously been attempted, nor has analysis of
dialog behaviors over time frames shorter than the
discourse or the turn sequence.
Reducing the multiplicity of prosodic features to
a smaller underlying set has long been a goal for
linguists. The traditional method is to start with
percepts (for example, that some syllables sound
louder) and then look for the acoustic-prosodic fea-
tures that correlate with these perceptions. More re-
cently the opposite tack has also been tried, start-
ing with acoustic-prosodic features, and trying to in-
fer a higher or deeper level of description. For ex-
ample, if we discover that for many syllables pitch
height, higher volume, and increased duration all
correlate, then we can infer some deeper factor un-
derlying all of these, namely stress or prominence.
PCA provides a systematic way of doing this for
many features at once, and it has been used for
various prosodic investigations, including an explo-
ration of the prosodic and other vocal parameters
relevant to emotional dimensions (Goudbeek and
Scherer, 2010) or levels of vocal effort (Charfue-
lan and Schro?eder, 2011), categorizing glottal-flow
waveforms (Pfitzinger, 2008), finding the factors in-
volved in boundaries and accents (Batliner et al,
2001), identifying the key dimensions of variation in
pitch contours using Functional Data Analysis (Gu-
bian et al, 2010), and for purely practical purposes
(Lee and Narayanan, 2005; Jurafsky et al, 2012). In
our own laboratory, Justin McManus applied PCA
to 4 left-context, single-speaker prosodic features,
and identified the first PC with a continuum from
silence to cheerful speech, and the second PC with
the continuum from back-channeling to storytelling.
However PCA has never before been applied to large
set of features, thus we hoped it might reveal im-
portant underlying factors in prosody that have not
previously been noticed: factors interactionally im-
portant, even if not salient.
5 Method
Using Switchboard, a large corpus of smalltalk be-
tween strangers over the telephone recorded in two
199
  we  don?t  go  camping      a           lot   lately        mostly   because          uh 
                                                                                      uh-huh 
Figure 1: The 16 pitch-height feature windows, centered about a hypothetical occurrence of the word lot .
channels (Godfrey et al, 1992), we collected data-
points from both sides of 20 dialogs, totaling almost
two hours, taking a sample every 10 milliseconds.
This gave us 600,000 datapoints.
For each datapoint we computed 76 prosodic fea-
tures. These features were taken from both the im-
mediate past and the immediate future, since dialog
state, by any definition, relates to both: being depen-
dent on past context and predictive of future actions.
The features were taken from both the speaker of in-
terest and his or her interlocutor, since dialog states
intrinsically involve the behavior of both parties.
Because our interest is in short-term dialog states,
features were computed over only the 3-4 seconds
before and after each point of interest. The sequenc-
ing of the prosodic features being obviously impor-
tant, this context was split up into a sequence of
windows. Wishing to give more precision and more
weight to close context than more distant context,
the windows closest to the point of interest were
smallest, with the more distant being wider, as il-
lustrated in Figure 1. The window sizes were fixed,
not aligned with utterances, words, nor syllables.
The specific features we computed were chosen
for convenience, based on a basic set previously
found useful for language modeling (Ward et al,
2011). These were 1. a speaking-rate measure, over
325 millisecond windows, 2. volume, over 50 ms
windows, 3. pitch height, over 150 ms windows,
and 4. pitch range, over 225 ms windows. All were
speaker-normalized. The values for the longer re-
gions were obtained by simply averaging the values
over two more more adjacent basic features.
In total there were 76 features: 24 volume, 20
pitch range, 16 pitch height, and 16 speaking rate.
At times where there was no pitch, the average pitch
value was used as substitute. All features were nor-
malized to have mean 0 and standard deviation 1.
PCA was then done. As hoped, a few dimensions
explained most of the variance, with the top 4 ex-
plaining 55%, the top 10 explaining 70%, and the
top 20 explaining 81%.
We then set out to determine, for each of the di-
mensions, what dialog states or situations, if any,
were associated with it.
Our first approach was to examine extreme data-
points. Because we thought that it would be infor-
mative to see which words tended to occur at the
extremes, we filtered our datapoints to select only
those which were at word onsets. For each dimen-
sion we then computed, for all of these, the values
on that dimension. We then sorted these to find the
highest 20 and the lowest 20. Looking at these word
lists however was generally not informative, as no
word or even word type predominated in any group,
in fact, the words were invariably highly diverse.
This perhaps indicates that the dimensions of dialog
state expressed by prosody do not aligne with those
expressed by words, and perhaps confirm that words
can correlate with social and dialog functions in un-
suspected ways (Tausczik and Pennebaker, 2010).
We next listened to some of some of these dat-
apoints in context. First we listened to a few low-
valued ones and came up with informal hypotheses
about what they had in common. We then listened
to more examples, winnowing and revising hypothe-
ses as we went, until we were satisfied that we had
a generalization that held for at least the majority of
the cases. Then we did the same thing for the high-
valued times. Finally we put the two together and
found an opposition, and used this to describe the
significance of the dimension as a whole. Some-
times this came easily, but sometimes it required
more listening to verify or refine. This was in gen-
eral easy for the top few dimensions, but more chal-
lenging for the lower ones, where the shared proper-
ties were generally weaker and more variable.
This process was unavoidably subjective, and
must be considered only exploratory. We did not
start out with any strong expectations, other than
200
that many of the dimensions would relate to aspects
of dialog. Our backgrounds may have predisposed
us to be extra alert to turn-taking processes, but of-
ten initial hypotheses relating to turn-taking were
superseded by others that explained the data bet-
ter. We did not limit ourselves to terminology from
any specific theoretical framework, rather we chose
whichever seemed most appropriate for the phenom-
ena.
Our second approach was to look at the loading
factors, to see for each dimension which of the in-
put prosodic features were highly correlated with it,
both positively and negatively. In every case these
confirmed or were compatible with our interpreta-
tions, generally revealing heavy loadings on features
which previous research or simple logic suggested
would relate to the dialog activities and states we
had associated with the dimension.
6 Interpretations of the Top Dimensions
The results of our analyses were as follows. These
must be taken as tentative, and the summary descrip-
tions in the headings and in the tables must be read
as mere mnemonics for the more complex reality
that our fuller descriptions capture better, although
still far from perfectly.
Dimension 1: Who?s speaking?
At points with low values on this dimension the
speaker of interest is speaking loudly and continu-
ously without pause while the other is completely
silent. At points with high values on this dimen-
sion the speaker of interest is producing only back-
channels, while the other speaker is speaking con-
tinuously. (Points with complete silence on the part
of the speaker of interest probably would have been
even more extreme, but were not examined since
our sample set only included timepoints where the
speaker of interest was starting a word.) Unsurpris-
ingly the features with the highest loadings were the
volumes for the two speakers. Thus we identify this
dimension with ?who?s speaking.? Interestingly, of
all the dimensions, this was the only with a bimodal
distribution.
Dimension 2: How much involvement is there?
At points with low values on this dimension
the dialog appeared to be faltering or awkward,
with the lone speaker producing words slowly in-
terspersed with non-filled pauses. High-value points
were places where both speakers appeared highly in-
volved, talking at once for several seconds, or one
laughing while the other talked. Again the volume
features had the highest loadings. Thus we identify
this dimension with the amount of involvement.
Dimension 3: Is there a topic end?
At points with low values on this dimension there
is generally a quick topic closing, in situations where
the speaker had a new topic cued up and wanted to
move on to it. An extreme example was when, af-
ter hearing clicks indicating call waiting, the speaker
said she needed to take the other call. At points with
high values on this dimension the topic was constant,
sometimes with the less active participant indicating
resigned boredom with a half-hearted back-channel.
The features with the highest positive loadings were
speaking-rate features: fast speech by the interlocu-
tor in the near future correlated with a topic close,
whereas fast speech by the current speaker about 1?
2 seconds ago correlated with topic continuity. Thus
we identify this dimension with topic ending.
Dimension 4: Is the referent grounded yet?
At points with low values on this dimension the
speaker is often producing a content word after a
filler or disfluent region, and this is soon followed
by a back-channel by the other speaker. At points
with high values on this dimension the speaker of in-
terest is adding more information to make the point
he wanted (starting the comment part of a topic-
comment pair) sometimes after the interlocutor had
responded with oh. Thus this dimension relates to
the continuum between trying to ground something
and continuing on with something already grounded.
Trying to ground correlated with an upcoming fast
speaking rate, while proceeding after grounding cor-
related with a high volume. Thus we identify this
dimension with the degree of grounding.
Dimension 5: Does the speaker want to start or
stop?
At points with low values on this dimension the
speaker of interest is starting a turn strongly, some-
times as a turn-grab or even cutting-off the other
speaker. At points with high values on this dimen-
201
sion the speaker is strongly yielding the turn, cou-
pled with the interlocutor very swiftly taking up the
turn. Often the turn yield occurs when the speaker
is soliciting a response, either explicitly or by ex-
pressing an opinion that seems intended to invoke
a response. As might be expected, cut-offs corre-
late with high volume on the part of the interrupting
speaker, while clear turn yields correlate with past
high volume on the part of the speaker who is end-
ing. Thus we identify this dimension with starting
versus stopping.
Dimension 6: Has empathy been expressed yet?
At points with low values on this dimension the
speaker is continuing shortly after a high-content,
emotionally-colored word that has just been ac-
knowledged by the interlocutor. At points with
high values on this dimension, the speaker is ac-
knowledging a feeling or attitude just expressed by
the other, by expressing agreement with a short
turn such as that?s right or yeah, Arizona?s beau-
tiful!. Continuing after empathic grounding corre-
lated with high volume after a couple of seconds;
expressing empathy with a short comment corre-
lated with the interlocutor recently having produced
a word with high pitch. Thus we identify this dimen-
sion with the degree of empathy established.
Dimension 7: Are the speakers synchronized?
At points with low values on this dimension both
speakers inadvertently start speaking at the same
time. At points with high values on this dimension
the speakers swiftly and successfully interleave their
speaking, for example by completing each other?s
turns or with back-channels. The features with the
highest positive loadings were those of pitch range
and speaking rate with the volume factors having
mostly negative loadings. Thus we identify this di-
mension with the degree of turn synchronization.
Dimension 8: Is the turn end unambiguous?
At points with low values on this dimension
the speaker is dragging out a turn which appears,
content-wise, to be already finished, producing post-
completions, such as uh or or anything like that. At
points with high values on this dimension, often the
speaker is definitively ending a turn. The feature
with the highest positive loading was pitch range,
unsurprisingly since clear turn ends often involve a
sharp pitch fall. Thus we identify this dimension
with the degree of ambiguity of the turn end.
Dimension 9: Is the topic exhausted?
At points with low values on this dimension a
speaker is closing out a topic due to running out of
things to say. Often at points with high values on this
dimension the speaker is staying with one topic, with
continuing interest also from the interlocutor. The
most positively correlated feature was the interloc-
tor?s volume 400?800 ms ago, for example during
a back-channel or comment showing interest. Thus
we identify this dimension with the degree of inter-
est in the current topic.
Dimension 10: Is the speaker thinking?
At points with low values on this dimension the
speaker is looking for a word, choosing her words
carefully, or recalling something, typically inside
a turn but preceded by a short pause or an um.
At points with high values on this dimension the
speaker seems to be giving up on the topic, declaim-
ing any relevant knowledge and/or yielding the turn.
The features correlating most with the memory-
search/lexical-access state were those of high vol-
ume by the speaker 50?1500 milliseconds later; the
features correlating most with the giving-up state
were speaking rate. Thus we identify this dimen-
sion with the degree to which the speaker is putting
mental effort into continuing.
Dimension 11: How quick-thinking is the
speaker?
Points with low values on this dimension included
two types: first where a speaker is ending a false start
and about to start over, and second where the speaker
is about to be cut off by the interlocutor while say-
ing something noncommittal to end a turn, such as I
guess. Points with high values included swift echos
and confirmations, which seemed to reflect quick-
ness and dominance. Thus we identify this dimen-
sion with quickness, confidence and dominance ver-
sus the lack thereof.
Dimension 12: Is the speaker claiming or
yielding the floor?
Points with low values on this dimension gener-
ally seemed to be staking a claim to the floor, re-
202
vealing the intention to talk on for several seconds,
sometimes as topic resumptions. Points with high
were generally floor yields, and sometimes sounded
negative or distancing. Slow future speaking rate, by
both speakers, aligned with the low values, and fast
rate with the high values. We identify this dimension
with the floor claim/yield continuum.
Dimension 13: How compatible is the
proposition with the context?
Points with low values on this dimension occurred
in the course of a self-narrative at the beginning of
something contradicting what the listener may have
inferred, or actually did think and say, for example
with no, we actually don?t. Points with high values
of this dimension generally involved a restatement
of something said before either by the speaker or
the interloctor, for example restating a question after
the other failed to answer, or opining that a football
team can now expect a few bad years, just a dozen
seconds after the interlocutor had already expressed
essentially the same thought. The low, contradicting
side had high volume and slow speaking rate for a
fraction of a second; the restatements were the oppo-
site. Thus we identify this dimension with the con-
tinuum between a contrast-type rhetorical structure
and a repetition-type one.
Dimension 14: Are the words being said
important?
Points with low values on this dimension occur
when the speaker is rambling: speaking with fre-
quent minor disfluencies while droning on about
something that he seems to have little interested in,
in part because the other person seems to have noth-
ing better to do than listen. Points with high values
on this dimension occur with emphasis and seemed
bright in tone. Slow speaking rate correlated highest
with the rambling, boring side of the dimension, and
future interlocutor pitch height with the emphasiz-
ing side. Thus we identify this dimension with the
importance of the current word or words, and the de-
gree of mutual engagement.
Dimension 15: Are the words premature or
delayed?
Points with low values on this dimension included
examples where the speaker is strongly holding the
floor despite a momentary disfluency, for example
uh and or well it?s it?s difficult, using creaky voice
and projecting authority. Points with high value on
this dimension overlapped substantially with those
high on dimension 14, but in addition seemed to
come when the speaker starts sharing some infor-
mation he had been wanting to talk about but sav-
ing up, for in a drawn-out political discussion, a new
piece of evidence supporting an opinion expressed
much earlier. Thus we identify this dimension with
the continuum between talking as soon as you have
something to say (or even slightly before) versus
talking about something when the time is ripe.
Dimension 16: How positive is the speaker?s
stance?
Points with low values on this dimension were on
words spoken while laughing or near such words, in
the course of self-narrative while recounting a hu-
morous episode. Points with high values on this
dimension also sometimes occurred in a self nar-
ratives, but with negative affect, as in brakes were
starting to fail, or in deploring statements such as
subject them to discriminatory practices. Low val-
ues correlated with a slow speaking rate; high values
with the pitch height. This we identify this a humor-
ous/regrettable continuum.
Other Dimensions
Space does not permit the discussion of further
dimensions here, but the end of Table 1 and Table
2 summarize what we have seen in some other di-
mensions that we have examined for various rea-
sons, some discussed elsewhere (dimensions 25, 62,
and 72 in (Ward and Vega, 2012 submitted) and 17,
18, 21, 24, 26, and 72 in (Ward et al, 2012 sub-
mitted)). Of course, not all dimensions are mostly
about dialog, for example dimension 29 appears to
be described best as relating simply to the presence
or absence of a stressed word (Ward et al, 2012 sub-
mitted), although that of course is not without impli-
cations for what dialog activities may cooccur.
7 Discussion
Although prosody is messy and multifunctional, this
exploration shows that PCA can derive from raw
features a set of dimensions which explain much of
the data, and which are surprisingly interpretable.
203
1 this speaker talking vs. other speaker talking 32%
2 neither speaking vs. both speaking 9%
3 topic closing vs. topic continuation 8%
4 grounding vs. grounded 6%
5 turn grab vs. turn yield 3%
6 seeking empathy vs. expressing empathy 3%
7 floor conflict vs. floor sharing 3%
8 dragging out a turn vs. ending confidently and crisply 3%
9 topic exhaustion vs. topic interest 2%
10 lexical access or memory retrieval vs. disengaging 2%
11 low content and low confidence vs. quickness 1%
12 claiming the floor vs. releasing the floor 1%
13 starting a contrasting statement vs. starting a restatement 1%
14 rambling vs. placing emphasis 1%
15 speaking before ready vs. presenting held-back information 1%
16 humorous vs. regrettable 1%
17 new perspective vs. elaborating current feeling 1%
18 seeking sympathy vs. expressing sympathy 1%
19 solicitous vs. controlling 1%
20 calm emphasis vs. provocativeness 1%
Table 1: Interpretations of top 20 dimensions, with the variance explained by each
21 mitigating a potential face threat vs. agreeing, with humor
24 agreeing and preparing to move on vs. jointly focusing
25 personal experience vs. second-hand opinion
26 signalling interestingness vs. downplaying things
62 explaining/excusing oneself vs. blaming someone/something
72 speaking awkwardly vs. speaking with a nicely cadenced delivery
Table 2: Interpretations of some other dimensions
Overall, the top dimensions covered a broad sam-
pling of the topics generally considered important in
dialog research. This can be taken to indicate that
the field of dialog studies is mostly already work-
ing on the important things after all. However pre-
viously unremarked aspects of dialog behavior do
appear to surface in some of the lower dimensions;
here further examination is needed.
We had hoped that PCA would separate out the
dialog-relevant aspects of prosody from the aspects
of prosody serving other functions. Generally this
was true, although in part because the non-dialog
functions of prosody didn?t show up strongly at all.
While this was probably due in part to the spe-
cific feature set used, it still suggests that dialog
factors are overwhelmingly important for prosody.
Partial exceptions were emotion, attitude, rhetorical
structure, speaking styles and interaction styles, all
of which appeared as aspects of some dimensions.
Some dimensions also seemed to relate to dialects,
personality traits, or individuals; for example, many
of the most unambiguous turn endings (dimension
8) were by the same few speakers, who seemed to
us to be businesslike and dominant.
204
8 Potential Applications
These dimensions, and similar empirically-derived
sets, are potentially useful for various applications.
First, the inferred dimensions could serve as a
first-pass specification of the skills needed for a
competent dialog agent: suggesting a dialog man-
ager whose core function is to monitor, predict, and
guide the development of the dialog in terms of the
top 10 or so dimensions. This technique could be
very generally useful: since it supports the discov-
ery of dialog dimensions in a purely data-driven way
(apart from the subjective interpretations, which are
not always needed), this may lead to methods for the
automatically generation of dialog models and dia-
log managers for arbitrary new domains.
Second, for generation and synthesis, given the
increased interest in going beyond intelligibility
to also give utterances dialog-appropriate wordings
and realizations, the inferred dimensions suggest
what is needed for dialog applications: we may have
identified the most important parameters for adapt-
ing and controlling a speech synthesizer?s prosodic
behavior for dialog applications.
Third, dimensional representations of dialog state
could be useful for predicting the speaker?s upcom-
ing word choices, that is, useful for language mod-
eling and thus speech recognition, as an improve-
ment on dialog-act descriptions of state or descrip-
tions in terms of raw, non-independent prosodic fea-
tures (Shriberg and Stolcke, 2004; Ward et al, 2011;
Stoyanchev and Stent, 2012). Initial results of con-
ditioning on 25 dimensions gave a 26.8% perplexity
reduction (Ward and Vega, 2012 submitted).
These dimensions could also be used for other
purposes, including a more-like-this function for
audio search based on similarity in terms of dia-
log context; better characterizing the functions of
discourse markers; tracking the time course of ac-
tion sequences leading to impressions of dominance,
friendliness and the like; finding salient or signifi-
cant events in meeting recordings; and teaching sec-
ond language learners the prosodic patterns of dia-
log.
9 Future Work
Our study was exploratory, and there are many ob-
vious ways to improve on it. It would be good to ap-
ply this method using richer feature sets, including
for example voicing fraction, pitch slope, pitch con-
tour features, spectral tilt, voicing properties, and
syllable- and word-aligned features, to get a more
complete view of what prosody contributes to di-
alog. Going further, one might also use temporal
features (Ward et al, 2011), features of gaze, ges-
ture, and words, perhaps in a suitable vector-space
representation (Bengio et al, 2003). Better feature
weighting could also be useful for refining the rank-
ing of the dimensions: while our method treated
one standard deviation of variance in one feature
as equal in importance to one standard deviation in
any other, in human perception this is certainly not
the case. It would also be interesting to apply this
method to other corpora in other domains: for ex-
ample in task-oriented dialogs we might expect it
to find additional important dimensions relating to
task structure, question type, recovery from mis-
understandings, uncertainty, and so on. Finally, it
would be interesting to explore which of these di-
mensions of state actually matter most for dialog
success (Tetreault and Litman, 2006).
In addition to the identification of specific dimen-
sions of dialog in casual conversations, this paper
contributes a new method: that of using PCA over
low-level, observable features to identify important
dimensions of dialog state, which could be applied
more generally.
While we see numerous advantages for quantita-
tive, dimensional dialog state modeling, we do not
think that this obsoletes more classical methods. In-
deed, it would be interesting to explore how com-
monly used dialog states and acts relate to these di-
mensions; for example, to take the set of utterances
labeled wh-questions in NXT Switchboard and ex-
amine where they are located in the ?dialog space?
defined by these dimensions (Calhoun et al, 2010;
Ward et al, 2012 submitted).
Acknowledgments
This work was supported in part by NSF Award IIS-
0914868. We thank Olac Fuentes for suggesting
PCA, Justin McManus for the prototype analysis,
Shreyas Karkhedkar for help with the basic features,
and David Novick for discussion.
205
References
Plinio Barbosa. 2009. Detecting changes in speech ex-
pressiveness in participants of a radio program. In In-
terspeech, pages 2155?2158.
Anton Batliner, Jan Buckow, Richard Huber, Volker
Warnke, Elmar No?th, and Heinrich Niemann. 2001.
Boiling down prosody for the classification of bound-
aries and accents in German and English. In Eu-
rospeech, pages 2781?2784.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137?1155.
Kristy Elizabeth Boyer, Eun Young Ha, Robert Phillips,
Michael D. Wallis, Mladen A. Vouk, and James C.
Lester. 2009. Inferring tutorial dialogue structure
with hidden Markov modeling. In Proc. NAACL-HLT
Workshop on Innovative Uses of NLP for Building Ed-
ucational Applications, pages 19?26.
Harry Bunt. 2011. Multifunctionality in dialogue. Com-
puter Speech and Language, 25:222?245.
Sasha Calhoun, Jean Carletta, Jason M. Brenier, Neil
Mayo, Dan Jurafsky, et al 2010. The NXT-format
Switchboard corpus: a rich resource for investigating
the syntax, semantics, pragmatics and prosody of dia-
logue. Language Resources and Evaluation, 44:387?
419.
Marcela Charfuelan and Marc Schro?eder. 2011. Investi-
gating the prosody and voice quality of social signals
in scenario meetings. In Proc. Affective Computing
and Intelligent Interaction.
Milica Gasic and Steve Young. 2011. Effective han-
dling of dialogue state in the hidden information state
POMDP-based dialogue manager. ACM Transactions
on Speech and Language Processing, 7.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
Switchboard: Telephone speech corpus for research
and development. In Proceedings of ICASSP, pages
517?520.
Martijn Goudbeek and Klaus Scherer. 2010. Beyond
arousal: Valence and potency/control cues in the vo-
cal expression of emotion. Journal of the Acoustical
Society of America, 128:1322?1336.
John Grothendieck, Allen L. Gorin, and Nash M. Borges.
2011. Social correlates of turn-taking style. Computer
Speech and Language, 25:789?801.
Michelle Gubian, Francesco Cangemi, and Lou Boves.
2010. Automatic and data driven pitch contour ma-
nipulation with functional data analysis. In Speech
Prosody.
Dan Jurafsky, Rajesh Ranganath, and Dan McFarland.
2012. Detecting friendly, flirtatious, awkward, and as-
sertive speech in speed-dates. Computer Speech and
Language, in press.
Chul Min Lee and Shrikanth Narayanan. 2005. Toward
detecting emotions in spoken dialogs. IEEE Transac-
tions on Speech and Audio Processing, 13:293?303.
Cheongjae Lee, Sangkeun Jung, Kyungduk Kim, and
Gary Geunbae Lee. 2009. Automatic agenda graph
construction from human-human dialogs using clus-
tering method. In Proc. NAACL-HLT 2009: Short Pa-
pers, pages 89?92.
Fabrice Lefevre and Renato de Mori. 2007. Unsuper-
vised state clustering for stochastic dialog manage-
ment. In ASRU, pages 550?553.
Scott McGlashan, Daniel C. Burnett, et al 2010. Voice
extensible markup language (VoiceXML) 3.0. Techni-
cal report, W3C.
Hartmut R. Pfitzinger. 2008. Segmental effects on
the prosody of voice quality. In Acoustics?08, pages
3159?3164.
Elizabeth Shriberg and Andreas Stolcke. 2004. Prosody
modeling for automatic speech recognition and un-
derstanding. In Mathematical Foundations of Speech
and Language Processing, IMA Volumes in Mathe-
matics and Its Applications, Vol. 138, pages 105?114.
Springer-Verlag.
Svetlana Stoyanchev and Amanda Stent. 2012. Concept
type prediction and responsive adaptation in a dialogue
system. Dialogue and Discourse, 3.
Yla R. Tausczik and James W. Pennebaker. 2010. The
psychological meaning of words: Liwc and computer-
ized text analysis methods. Journal of Language and
Social Psychology, 29:24?54.
Joel R. Tetreault and Diane J. Litman. 2006. Comparing
the utility of state features in spoken dialogue using
reinforcement learning. In HLT-NAACL, pages 272?
279.
David Traum and S. Larsson. 2003. The informa-
tion state approach to dialogue management. In Jan
van Kuppevelt and Ronnie Smith, editors, Current
and New Directions in Discourse and Dialogue, pages
325?353. Kluwer.
Nigel G.Ward and Alejandro Vega. 2012, submitted. To-
wards empirical dialog-state modeling and its use in
language modeling. In Interspeech.
Nigel G. Ward, Alejandro Vega, and Timo Baumann.
2011. Prosodic and temporal features for language
modeling for dialog. Speech Communication, 54:161?
174.
Nigel G. Ward, David G. Novick, and Alejandro Vega.
2012, submitted. Where in dialog space does uh-huh
occur? In Interdisciplinary Workshop on Feedback
Behaviors in Dialog at Interspeech 2012.
206
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 3?4,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Directions for Research on Spoken Dialog Systems, Broadly Defined?
Nigel G. Ward
University of Texas at El Paso
El Paso, Texas 79968, USA
nigelward@acm.org
Abstract
To increase impact and accelerate progress,
the spoken dialog systems research commu-
nity should work on four shareable things that
will also engage and support sister fields of
science and engineering.
1 To Reach Out to the VoiceXML
Community, a Commercial-Dialogs
Corpus
Although many people are frustrated with the com-
mercial dialog systems they use every day, spoken
dialog systems research has been only sporadically
relevant to these issues. Although service inter-
actions are pervasive in everyday life, and can be
rich and interesting, the vast majority of attempts
to model and engineer them have attempted to op-
timize efficiency and surface-goal completion. The
results are all around us, from crudely scripted up-
selling attempts at fast food restaurants to stilted di-
alog systems that tediously elicit the pieces of infor-
mation needed to complete a database query. One
reason is that the research community has come to
shun most practical dialog types, perhaps to avoid
seeming old-fashioned or being tainted by low ex-
pectations, or perhaps due to a misperception that
industry is addressing these issues. A resource that
would help progress here would be a commercial-
dialogs corpora that is shareable by all.
Personally, I would like this corpus to be one with
a truly exemplary person in the service role, some-
one who puts customers at ease, develops rapport,
?This work was supported by NSF Award IIS-0914868.
brings humor and sparkle, and makes them want to
call back. Having several thousand short dialogs
where diverse customers call in to that person, and
modeling how she handles them, would take us a
long way to understanding responsive and adaptive
behaviors. Even prototype systems built on such di-
alogs could help set the agenda for future genera-
tions of commercial dialog systems.
2 To Reach Out to the Applied Linguistics
Communities, Dialog Analysis Tools
Although many people are fascinated by language
and dialog, spoken dialog systems research has only
sporadically tapped this enthusiasm. For example,
researchers in the conversation analysis tradition and
teachers of foreign languages, not to mention many
undergraduates, love to explore patterns of dialog.
However spoken dialog research so far has produced
scant findings about language behavior that are in-
teresting to and graspable by non-engineers.
Personally, I think the biggest opportunity here in-
volves tools to support non-technical people in dis-
covering things themselves. Even amateurs, such
as high school science fair participants, should be
able to satisfy curiosity or confirm hunches, and ex-
perience the joy of systematically examining dia-
log phenomena. Our community ought to be pro-
ducing tools and toolsets that support the complete
workflow in such inquiries, eclectically supporting
tagging, searching, juxtaposing clips and so on,
and supporting both perceptually-based analysis and
quantitative analysis in an integrated way. In par-
ticular we need to go beyond in-lab solutions (Ward
and Al Bayyari, 2006) to develop robust toolsets that
3
can be used effectively without months of training.
3 To Reach Out to the Psycholinguistics
Community, Modeling-Related Goals
Although many people are curious about how com-
munication feats are achieved daily by human
minds, spoken dialog research has only sporadically
raised questions of real scientific interest. The spo-
ken dialog community ought to formulate one or two
high-profile grand-challenge problems that would
inspire and bring people together, either coopera-
tively or in competition. Rather than ?dialog man-
agement? and systems-type problems, these should
be framed as ?dialog modeling? problems, to make
it clear that they are true scientific problems, and
formulated so that they can be addressed more em-
pirically and/or more theoretically, without requir-
ing researchers to work with end-to-end systems.
Such purer formulations should also help focus on
questions of the fundamental human perceptions and
abilities involved here, and how they vary with age,
personality, language and culture.
Personally I think the most central and dialog-
specific issues in our field are those relating to inter-
personal coordination. Topics here have been nib-
bled at, perhaps most saliently in the study of turn-
taking phenomena. Possible grand challenges may
relate to topics such as ?dialog dynamics? and ?pre-
diction of the interlocutor?s actions,? but formulating
these problems so that they are general, and yet rel-
evant and tractable, has been difficult (Ward, 2010;
Ward et al, 2010).
4 To Reach Out to the Speech Processing
Community, More Open Models
First, although speech generation and speech synthe-
sis researchers are currently looking for new chal-
lenges, beyond correctness and intelligibilty, the di-
alog systems community has only sporadically of-
fered them interesting goals. These systems need
somehow to be able to express the richness of the
attitudes, structures, and intentions people convey in
dialog, in real time, and we ought to provide spec-
ifications for this. Personally I think that multi-
dimensional vector-space models of dialog states,
situations, and intentions have promise here, and
that these can best be developed by bottom-up em-
pirical studies (Ward and Vega, 2012 submitteda),
one of which suggests that the important dimensions
of dialog include, at least, in rough order of impor-
tance: who has the floor, the activity level, topic
aging and transition, turn taking, seeking vs. estab-
lishing grounding, empathy, and sympathy, lexical
access and planning processes, dominance, confi-
dence, affect and attitude, rhetorical structure and
strategy, and indications of concentration and in-
volvement.
Second, although research on emotion and other
nonverbal aspects of speech is advancing, this has
only sporadically been guided by the needs of dia-
log systems. We ought to be thinking more about
how emotion, attitude, stance, and related dimen-
sions of communication are used in dialog. Person-
ally I think that empirical studies of prosody, again,
can be informative.
Third, although speech recognition researchers
are adding flexibility and incrementality, speech rec-
ognizers? interactions with the dialog manager are
still very limited. In particular, the role of the dialog
model in telling the recognizer what words are likely
to come next, that is, its role in language modeling,
is still underdeveloped. Personally I think we need
dialog models that track more aspects of the dialog,
and do so continuously, and supply that information
to the recognizer (Ward and Vega, 2012 submittedb).
References
Nigel Ward and Yaffa Al Bayyari. 2006. A case study
in the identification of prosodic cues to turn-taking:
Back-channeling in Arabic. In Interspeech 2006 Pro-
ceedings.
Nigel G. Ward and Alejandro Vega. 2012, submitteda.
A bottom-up exploration of the dimensions of dialog
state in spoken interaction. In Sigdial.
Nigel G. Ward and Alejandro Vega. 2012, submittedb.
Towards empirical dialog-state modeling and its use in
language modeling. In Interspeech.
Nigel G. Ward, Olac Fuentes, and Alejandro Vega. 2010.
Dialog prediction for a general model of turn-taking.
In Interspeech.
Nigel G. Ward. 2010. The challenge of modeling dialog
dynamics. In Workshop on Modeling Human Commu-
nication Dynamics, at Neural Information Processing
Systems.
4
Proceedings of the SIGDIAL 2013 Conference, pages 107?111,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Patterns of Importance Variation in Spoken Dialog
Nigel G. Ward
University of Texas at El Paso
El Paso, Texas, 79968 USA
nigelward@acm.org
Karen A. Richart-Ruiz
University of Texas at El Paso
El Paso, Texas, 79968 USA
karichart@miners.utep.edu
Abstract
Some things people say are more impor-
tant, and some less so. Importance varies
from moment to moment in spoken dialog,
and contextual prosodic features and pat-
terns signal this. A simple linear regres-
sion model over such features gave esti-
mates that correlated well, 0.83, with hu-
man importance judgments.
1 Importance in Language and Dialog
Not everything people say to each other is equally
important, for example many ums and uhs have al-
most no significance, in comparison to those con-
tent words or nuances that are critical in one way
or another.
Many language processing applications need to
detect what is important in the input stream, in-
cluding dialog systems and systems for summa-
rization, information retrieval, information extrac-
tion, and so on. Today this is primarily done
using task-specific heuristics, such as discarding
stopwords, giving more weight to low frequency
words, or favoring utterances with high average
pitch. In this paper, however, we explore a gen-
eral, task-independent notion of importance, tak-
ing a dialog perspective.
Section 2 explains our empirical approach. Sec-
tions 3 and 4 explore the individual prosodic fea-
tures and longer prosodic patterns that dialog par-
ticipants use to signal to each other what is impor-
tant and unimportant. Section 5 describes predic-
tive models that use this information to automat-
ically estimate importance and Section 6 summa-
rizes the significance and future work needed.
2 Annotating Importance
No standard definition of importance is useful for
describing what happens, moment-by-moment, in
spoken dialog. The closest contender would be
entropy, as defined in information theory. For
text we can measure the difficulty of guessing let-
ters or words, as a measure of their unpredictabil-
ity and thus informativeness (Shannon, 1951), but
this is indirect, time-consuming, and impossible
to apply to non-symbolic aspects of language. We
can also measure the value of certain information,
such as prosody, for improving the accuracy of
predictions, but again this is indirect and time-
consuming (Ward and Walker, 2009).
We therefore chose to do an empirical study. We
hired a student to annotate importance. Wanting
to capture her naive judgments, atheoretically, we
did not precisely define importance for her. In-
stead we discussed the concept briefly, noting that
importance may be judged: not just by content
but also by value for directing the future course
of the dialog, not just from the speaker?s perspec-
tive but also from the listener?s, and not just from
the words said but also from how they were said.
The labeling tool used enabled the annotator to
navigate back and forth in the dialogs, listen to the
speakers together in stereo or independently, de-
limit regions of any desired size including words
and word fragments, and ascribe to each region an
importance value. While importance is continu-
ous, for convenience we used the whole numbers
from 0 to 5, with 5 indicating highest importance,
4 typical importance, 3 somewhat less importance,
2 and 1 even less, and 0 silence. To have a variety
of speakers, topics, and speaking styles, the mate-
rial was from the Switchboard corpus (Godfrey et
al., 1992).
107
Figure 1: Importance versus Time, in milliseconds. Rectangular line: Annotator judgments; Jagged line:
Predictions (discussed below). The words are all by one speaker, horizontally positioned by approximate
occurrence.
In total, she labeled both tracks of just over 100
minutes of dialog. There was diversity in labels,
supporting our belief that importance is not mono-
tone: the largest fraction of non-zero-labeled re-
gions, covering 38% of the total time, was at level
4, but there were also 20% at level 3 and 37% at
level 5. In general importance was variable, on
average staying at the same level for only 1.5 sec-
onds. Figure 1 illustrates.
In parallel, the second author labeled 17 min-
utes of the same dialogs1. The agreement in terms
of Kappa was .80 (?very good?) across all cate-
gories, and .67 (?good?) excluding the zero-level
labels, which were mostly for silent regions and
thus easy to agree on. In terms ofWeighted Kappa,
appropriate here since the labels are ordered (and
thus, for example, a 1-point difference matters
much less than a 5-point difference), the agree-
ment levels were .92 and .71, for all and for the
zero-excluding sets, respectively. The differences
were mainly due to minor variations in boundary
placement, missing labels for small quiet sounds
such as inbreaths and quiet overlapping backchan-
nels, and different ratings of repeated words, and
of backchannels (Ward and Richart-Ruiz, 2013).
3 Correlating Prosodic Factors
First we briefly examined lexical correlates of im-
portance, by examining the average importance
of words in this corpus (Ward and Richart-Ruiz,
2013). To summarize some key findings: Less fre-
quent words tend to have higher average per-word
importance, however ratings vary widely, depend-
ing on context. Some words have effects at a dis-
tance, for example, because tends to indicate that
1All labels are freely available at
http://www.cs.utep.edu/nigel/importance/
whatever is said one second later will be impor-
tant. The interlocutor?s words can also be infor-
mative, for example oh and uh-huh tend to indi-
cate that whatever the interlocutor said one second
ago was important. The ?words? with the most
extreme average importance ? notably uh-huh,
um-hum, um and laughter ? are fillers, backchan-
nels and other vocalizations of types which can
be detected well from the prosodic and inter-
actional contexts (Neiberg and Gustafson, 2011;
Truong and van Leeuwen, 2007). Thus a word-
based model of importance would be challenging
to build and might not have much value. We there-
fore turned our attention to prosody.
While prosody-importance connections have
not been considered directly, several studies have
found correlations between prosodic features and
various importance-related constructs, such as
predictability, involvement, engagement, activa-
tion, newness, and interest (Bell et al, 2009; Yu
et al, 2004; Batliner et al, 2011; Roehr and Bau-
mann, 2010; Oertel et al, 2011; Hsiao et al, 2012;
Kahn and Arnold, 2012; Kawahara et al, 2010).
However these studies have all been limited to spe-
cific features, functions, or hypotheses. Our aims
being instead exploratory, we looked for features,
from among a broad inventory, which correlate
with importance, as it occurs in a broad variety of
contexts.
Our feature inventory included features of 8
classes: four basic types ? volume, pitch height,
pitch range, and speaking-rate ? each computed
for both participants: the speaker and the inter-
locutor. Within each class, features were com-
puted over windows of various widths and at var-
ious offsets, for a total of 78 features (Ward and
Richart-Ruiz, 2013).
108
The speaker features correlating most strongly
with importance were volume and speaking rate.
Although the very strongest correlations were with
volume slightly in the past, volume both before
and after the current moment was strongly cor-
related over all windows, with one exception.
Speaker pitch height, in contrast, correlated neg-
atively with importance across all windows, con-
trary to what is often seen in monolog data.
The interlocutor features correlating most
strongly with importance were again volume and
speaking rate, but only over windows close to the
point of interest, perhaps due to co-construction
or supportive back-channeling; over more distant
windows, both past and future, these correlate neg-
atively. Interlocutor pitch range correlated nega-
tively over all windows.
4 Correlating Dialog-Activity Patterns
Thus we find that some prosodic features have dif-
ferent effects depending on their offset from the
frame of interest. Perhaps prosody is not just
marking importance vaguely somewhere in the
area, but more precisely indicating important and
unimportant moments.
To explore this we used Principal Components
Analysis (PCA), as described in detail in (Ward
and Vega, 2012). In short, this method finds
patterns of prosodic features which co-occur fre-
quently in the data, and so provides an unsuper-
vised way to discover the latent structure underly-
ing the observed regularities. We correlated the di-
mensions resulting with PCA with the importance
values. Many dimensions had significant correla-
tions, indicating that importance relates to many
prosodic structures and contexts. Each dimension
had two characteristic patterns, one corresponding
to high values on that dimension and one to low
values. We were able to interpret most of these in
terms of dialog activities (Ward and Vega, 2012).
Tending to be more important was: speech in
the middle of other speech (dimension 1), rather
than words snuck in while the other has the floor;
simultaneous speech (dimension 2), understand-
ably as such times tended to be high in involve-
ment and/or backchannels; times of encountering
and resolving turn conflicts (dimension 7), more
than places where the participants were support-
ively interleaving turns, which in this corpus were
generally more phatic than contentful; crisp turn
ends (dimension 8), rather than slow repetitious
model correlation m.a.e.
m5pTree decision tree .38 1.21
neural network .66 1.20
simple linear regression .79 .89
linear regression .83 .75
ditto, past-only features .83 .79
Table 1: Prediction Quality in terms of correlation
and mean absolute error, for various learning algo-
rithms.
wind-downs; ?upgraded assessments,? in which a
speaker agrees emphatically with an assessment
made by the other (dimension 6); and times when
speakers were solicitous, rather than controlling
(dimension 19). Dimension 6 is interesting in
that it matches an interaction pattern described as
an exemplar of prosodic co-construction (Ogden,
2012). Dimension 19 was one of those underlying
the exception noted above: the negative correla-
tion between importance and speaker volume over
the window from 0?50 milliseconds after the point
of prediction. Upon examination, low volume at
this offset often occurred when seeking agreement
and during quiet filled pauses in the vicinity of
high-content words.
5 Predictive Models
We next set out to build predictive models, for two
reasons: to judge whether the features discussed
above are adequate for building useful models, and
to determine what additional factors would be re-
quired in a more complete model.
The task is, given a timepoint in a track in a dia-
log, to predict the importance of what the speaker
is saying at that moment. Our performance met-
rics were the mean absolute error and the correla-
tion coefficient, computed over all frames; thus a
predictor is better to the extent that its predictions
are close to and correlate highly with the annota-
tor?s labels, including the implicit zero labels in
regions of silence or noise.
We built models using four algorithms in Weka.
All models performed poorly on dialogs for which
there was cross-track bleeding or other noise. As
these are artifacts of this corpus and would not be
relevant for most applications, our main evaluation
used only the five tracks with good audio quality.
These all had different speakers. We did five-fold
cross-validation on this; Table 1 gives the results.
Linear regression was best, by both measures and
109
past future all
?400 ?200 0
speaker .55 .64 .66 .59 .70
interloc. .37 .43 .43 .37 .47
both .62 .70 .71 .65 .74
Table 2: Model Quality, in terms of R2, as a func-
tion of the features used.
across every fold, and this was consistent for all
the other training and test sets tried.
To compare the performance of this predictor to
human performance, we also trained a model us-
ing 5 tracks to predict performance over two test
tracks, a total of 224495 test datapoints, which
the second judge also had annotated. Over these
the predictor did almost as well as second judge
in correlation (.88 versus .92), but not so well in
terms of mean absolute error (.75 versus .31).
Analyzing the errors, we noted several types of
cause (Ward and Richart-Ruiz, 2013). First, per-
formance varied widely across tracks, with mean
absolute errors from .55 to .97, even though all the
features were speaker-normalized. The high value
was for a speaker who was an outlier in two re-
spects: the only female among four males, and the
only East-Coast speaker among four Texans. Thus
results might be improved by separately model-
ing different genders and dialects. Second, predic-
tions were often off in situations like those where
the two human judges disagreed. Third, most of
the errors were due to feature-set issues: robust-
ness, poor loudness features, and not enough fine-
grained features. Fourth, our prosodic-feature-
only model did very poorly at distinguishing be-
tween the highest importance levels, 4 and 5, but
was otherwise generally good.
Table 2 shows how performance varies with the
features used; here quality is measured using sim-
ply the R2 of a linear regression over all the data.
Performance is lower with only the left-context
features, as would be required for real-time appli-
cations, but not drastically so; as seen also in the
last line of Table 1. Performance is only slightly
lower when predicting slightly in advance, without
using any features closere than 200 ms prior to the
prediction point, but notably worse 400 ms before.
Features of the interlocutor?s behavior are helpful,
partially why explaining dialog can be easier to
understand than monolog (Branigan et al, 2011).
6 Broader Significance and Future Work
Sperber and Wilson argue that ?attention and
thought processes . . . automatically turn toward in-
formation that seems relevant: that is, capable of
yielding cognitive effects? (Sperber and Wilson,
1987). This paper has identified some of the cues
that systems can use to ?automatically turn to-
ward? the most important parts of the input stream.
Overall, these findings show that task-independent
importance can be identified fairly reliably, and
that it can be predicted fairly well using simple
prosodic features and a simple model. Signifi-
cantly, we find that importance is frequently not
signaled or determined by one participant alone,
but is often truly a dialog phenomenon. We see
three main directions for future work:
First, there is ample scope to build better models
of importance, not only by pursuing the prosodic-
feature improvements noted above, but in exam-
ining lexical, semantic, rhetorical-structure and
dialog-structure correlates of importance.
Second, one could work to put our pretheoreti-
cal notion of importance on a firmer footing, per-
haps by relating it to entropy, or to the time course
of the psychological processes involved in retriev-
ing, creating, managing, and packaging informa-
tion into speech; or to the design and timing of
dialog contributions so as not to overload the lis-
tener?s processing capacity.
Third, there are applications. For example, a
dialog system needing to definitely convey some
information to the user could use an appropriate
prosodic lead-in to signal it properly, doing an in-
teractional dance (Gratch et al, 2007; Brennan et
al., 2010) to prepare the recipient to be maximally
receptive at the moment when the critical word
is said. Another potential application is in voice
codecs, as used in telecommunications. Today?s
codecs treat all speech as equally valuable. In-
stead we would like to transmit more important
words and sounds at higher quality, and less im-
portant ones at lower quality, thereby increasing
perceived call quality without increasing the aver-
age datarate, of course while properly considering
all perceptual factors (Voran and Catellier, 2013).
Acknowledgments
This work was supported in part by the NSF un-
der projects IIS-0914868 and CNS-0837556. We
thank Timo Baumann, Alejandro Vega, Shreyas
Karkhedkar, Gabriela Almeida and David Novick.
110
References
Anton Batliner, Stefan Steidl, Bjorn Schuller, et al
2011. Whodunnit: Searching for the most important
feature types signalling emotion-related user states
in speech. Computer Speech and Language, 25:4?
28.
Alan Bell, Jason M. Brenier, Michelle Gregory, Cyn-
thia Girand, and Dan Jurafsky. 2009. Predictability
effects on durations of content and function words
in conversational English. Journal of Memory and
Language, 60:92?111.
Holly P. Branigan, C.M. Catchpole, andM.J. Pickering.
2011. What makes dialogues easy to understand?
Language and Cognitive Processes, 26:1667?1686.
Susan E. Brennan, Alexia Galati, and Anna K. Kuhlen.
2010. Two minds, one dialog: Coordinating speak-
ing and understanding. In Brian H. Ross, editor, The
Psychology of Learning and Motivation, volume 53,
pages 301?344. Elsevier.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Proceedings
of ICASSP, pages 517?520.
Jonathan Gratch, Ning Wang, Jillian Gerten, Edward
Fast, and Robin Duffy. 2007. Creating rapport with
virtual agents. In Intelligent Virtual Agents, pages
125?138. Springer.
Joey Chiao-yin Hsiao, Wan-rong Jih, and Jane Yung-
jen Hsu. 2012. Recognizing continuous social en-
gagement level in dyadic conversation by using turn-
taking and speech emotion patterns. In Activity Con-
text Representation Workshop at AAAI.
Jason M. Kahn and Jennifer E. Arnold. 2012.
A processing-centered look at the contribution of
givenness to durational reduction. Journal of Mem-
ory and Language, 67:311?325.
Tatsuya Kawahara, K.Sumi, Z.Q. Chang, and
K.Takanashi. 2010. Detection of hot spots in poster
conversations based on reactive tokens of audience.
In Interspeech, pages 3042?3045.
Daniel Neiberg and Joakim Gustafson. 2011. A dual
channel coupled decoder for fillers and feedback. In
Interspeech 2011, pages 3097?3100.
Catharine Oertel, Stefan Scherer, and Nick Campbell.
2011. On the use of multimodal cues for the predic-
tion of degrees of involvment in spontaneous con-
versation. In Interspeech.
Richard Ogden. 2012. Prosodies in conversation. In
Oliver Niebuhr, editor, Understanding Prosody: The
role of context, function, and communication, pages
201?217. De Gruyter.
Christine Tanja Roehr and Stefan Baumann. 2010.
Prosodic marking of information status in German.
In Speech Prosody Conference.
Claude E. Shannon. 1951. Prediction and entropy
of printed English. Bell System Technical Journal,
30:50?64.
Dan Sperber and Deirdre Wilson. 1987. Pre?cis of Rel-
evance: Communication and cognition. Behavioral
and Brain Sciences, 10(04):697?710.
Khiet P. Truong and David A. van Leeuwen. 2007. Au-
tomatic discrimination between laughter and speech.
Speech Communication, 49:144?158.
Stephen D. Voran and Andrew A. Catellier. 2013.
When should a speech coding quality increase be al-
lowed within a talk-spurt? In IEEE ICASSP.
Nigel G. Ward and Karen A. Richart-Ruiz. 2013. Lex-
ical and prosodic indicators of importance in spoken
dialog. Technical Report UTEP-CS-13-41, Univer-
sity of Texas at El Paso, Department of Computer
Science.
Nigel G. Ward and Alejandro Vega. 2012. A bottom-
up exploration of the dimensions of dialog state in
spoken interaction. In 13th Annual SIGdial Meeting
on Discourse and Dialogue.
Nigel G. Ward and Benjamin H. Walker. 2009. Esti-
mating the potential of signal and interlocutor-track
information for language modeling. In Interspeech,
pages 160?163.
Chen Yu, Paul M. Aoki, and Alison Woodruff. 2004.
Detecting user engagement in everyday conversa-
tions. In Interspeech, pages 1329?1332.
111
