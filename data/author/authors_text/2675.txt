Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 875?882, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Inducing a multilingual dictionary
from a parallel multitext in related languages
Dmitriy Genzel
Department of Computer Science
Box 1910
Brown University
Providence, RI 02912, USA
dg@cs.brown.edu
Abstract
Dictionaries and word translation models
are used by a variety of systems, espe-
cially in machine translation. We build
a multilingual dictionary induction system
for a family of related resource-poor lan-
guages. We assume only the presence
of a single medium-length multitext (the
Bible). The techniques rely upon lexical
and syntactic similarity of languages as
well as on the fact that building dictionar-
ies for several pairs of languages provides
information about other pairs.
1 Introduction and Motivation
Modern statistical natural language processing tech-
niques require large amounts of human-annotated
data to work well. For practical reasons, the required
amount of data exists only for a few languages of
major interest, either commercial or governmental.
As a result, many languages have very little com-
putational research done in them, especially outside
the borders of the countries in which these languages
are spoken. Some of these languages are, however,
major languages with hundreds of millions of speak-
ers. Of the top 10 most spoken languages, Lin-
guistic Data Consortium at University of Pennsyl-
vania, the premier U.S. provider of corpora, offers
text corpora only in 7 (The World Factbook (2004),
2000 estimate) Only a few of the other languages
(French, Arabic, and Czech) have resources pro-
vided by LDC. Many Asian and Eastern European
languages number tens of millions of speakers, yet
very few of these seem to have any related compu-
tational linguistics work, at least as presented at the
international conferences, such as the ACL.1
The situation is not surprising, nor is it likely to
significantly change in the future. Luckily, most
of these less-represented languages belong to lan-
guage families with several prominent members. As
a result, some of these languages have siblings with
more resources and published research. 2 Inter-
estingly, the better-endowed siblings are not always
the ones with more native speakers, since political
considerations are often more important.3 If one
is able to use the resources available in one lan-
guage (henceforth referred to as source) to facilitate
the creation of tools and resource in another, related
language (target), this problem would be alleviated.
This is the ultimate goal of this project, but in the
first stage we focus on multi-language dictionary in-
duction.
Building a high-quality dictionary, or even bet-
ter, a joint word distribution model over all the lan-
guages in a given family is very important, because
using such a model one can use a variety of tech-
niques to project information across languages, e.g.
to parse or to translate. Building a unified model for
more than a pair of languages improves the quality
over building several unrelated pairwise models, be-
cause relating them to each other provides additional
information. If we know that word a in language A
has as its likely translation word b in language B,
and b is translated as c in C, then we also know that
a is likely to be translated as c, without looking at
1The search through ACL Anthology, for e.g., Telugu (?70
million speakers) shows only casual mention of the language.
2Telugu?s fellow Dravidian language Tamil (?65 million
speakers) has seen some papers at the ACL
3This is the case with Tamil vs. Telugu.
875
the A to C model.
2 Previous Work
There has been a lot of work done on building dic-
tionaries, by using a variety of techniques. One
good overview is Melamed (2000). There is work
on lexicon induction using string distance or other
phonetic/orthographic comparison techniques, such
as Mann and Yarowsky (2001) or semantic com-
parison using resources such as WordNet (Kondrak,
2001). Such work, however, primarily focuses on
finding cognates, whereas we are interested in trans-
lations of all words. Moreover, while some tech-
niques (e.g., Mann and Yarowsky (2001)) use mul-
tiple languages, the languages used have resources
such as dictionaries between some language pairs.
We do not require any dictionaries for any language
pair.
An important element of our work is focusing on
more than a pair of languages. There is an active
research area focusing on multi-source translation
(e.g., Och and Ney (2001)). Our setting is the re-
verse: we do not use multiple dictionaries in order
to translate, but translate (in a very crude way) in
order to build multiple dictionaries.
Many machine translation techniques require dic-
tionary building as a step of the process, and there-
fore have also attacked this problem. They use a va-
riety of approaches (a good overview is Koehn and
Knight (2001)), many of which require advanced
tools for both languages which we are not able to
use. They also use bilingual (and to some extent
monolingual) corpora, which we do have available.
They do not, however, focus on related languages,
and tend to ignore lexical similarity 4, nor are they
able to work on more than a pair of languages at a
time.
It is also worth noting that there has been some
MT work on related languages which explores lan-
guage similarity in an opposite way: by using dic-
tionaries and tools for both languages, and assum-
ing that a near word-for-word approach is reasonable
(Hajic et al, 2000).
4Much of recent MT research focuses on pairs of languages
which are not related, such as English-Chinese, English-Arabic,
etc.
3 Description of the Problem
Let us assume that we have a group of related lan-
guages, L1 . . . Ln, and a parallel sentence-aligned
multitext C, with corresponding portions in each
language denoted as C1 . . . Cn. Such a multitext ex-
ists for virtually all the languages in the form of the
Bible. Our goal is to create a multilingual dictionary
by learning the joint distribution P (x1 . . . xn)xi?Li
which is simply the expected frequency of the n-
tuple of words in a completely word-aligned mul-
titext. We will approach the problem by learning
pairwise language models, although leaving some
parameters free, and then combine the models and
learn the remaining free parameters to produce the
joint model.
Let us, therefore, assume that we have a set of
models {P (x, y|?ij)x?Li,y?Lj}i6=j where ?ij is a
parameter vector for pairwise model for languages
Li and Lj . We would like to learn how to combine
these models in an optimal way. To solve this prob-
lem, let us first consider a simpler and more general
setting.
3.1 Combining Models of Hidden Data
Let X be a random variable with distribution
Ptrue(x), such that no direct observations of it exist.
However, we may have some indirect observations
of X and have built several models of X?s distri-
bution, {Pi(x|?i)}ni=1, each parameterized by some
parameter vector ?i. Pi also depends on some other
parameters that are fixed. It is important to note that
the space of models obtained by varying ?i is only a
small subspace of the probability space. Our goal is
to find a good estimate of Ptrue(x).
The main idea is that if some Pi and Pj are close
(by some measure) to Ptrue, they have to be close
to each other as well. We will therefore make the
assumption that if some models of X are close to
each other (and we have reason to believe they are
fair approximations of the true distribution) they are
also close to the true distribution. Moreover, we
would like to set the parameters ?i in such a way
that P (xi|?i) is as close to the other models as pos-
sible. This leads us to look for an estimate that is
as close to all of our models as possible, under the
876
optimal values of ?i?s, or more formally:
Pest = argmin
P? (?)
min
?1
. . .min
?n
d(P? (?), P1(?|?1), . . . Pn(?|?n))
where d measures the distance between P? and all the
Pi under the parameter setting ?i. Since we have no
reason to prefer any of the Pi, we choose the follow-
ing symmetric form for d:
n?
i=1
D(P? (?)||Pi(?|?i))
where D is a reasonable measure of distance be-
tween probability distributions. The most appro-
priate and the most commonly used measure in
such cases in the Kullback-Leibler divergence, also
known as relative entropy:
D(p||q) =
?
x
p(x) log
p(x)
q(x)
It turns out that it is possible to find the optimal P?
under these circumstances. Taking a partial deriva-
tive and solving, we obtain:
P? (x) =
?n
i=1 Pi(x|?i)
1/n
?
x??X
?n
i=1 Pi(x?|?i)1/n
Substituting this value into the expression for
function d, we obtain the following distance mea-
sure between the Pi?s:
d?(P1(X|?1) . . . Pn(X|?n))
= minP? d(P? , P1(X|?1), . . . Pn(X|?n))
= ? log
?
x?X
?n
i=1 Pi(x|?i)
1/n
This function is a generalization of the well-
known Bhattacharyya distance for two distributions
(Bhattacharyya, 1943):
b(p, q) =
?
i
?
piqi
These results suggest the following Algorithm 1
to optimize d (and d?):
? Set al ?i randomly
? Repeat until change in d is very small:
? Compute P? according to the above for-
mula
? For i from 1 to n
? Set ?i in such a way as to minimize
D(P? (X)||Pi(X|?i))
? Compute d according to the above for-
mula
Each step of the algorithm minimizes d. It is also
easy to see that minimizing D(P? (X)||Pi(X|?i)) is
the same as setting the parameters ?i in order to max-
imize
?
x?X Pi(x|?i)
P? (x)
, which can be interpreted
as maximizing the probability under Pi of a cor-
pus in which word x appears P? (x) times. In other
words, we are now optimizing Pi(X) given an ob-
served corpus of X , which is a much easier problem.
In many types of models for Pi the Expectation-
Maximization algorithm is able to solve this prob-
lem.
3.2 Combining Pairwise Models
Following the methods outlined in the previous
section, we can find an optimal joint probability
P (x1 . . . xn)xi?Li if we are given several models
Pj(x1 . . . xn|?j). Instead, we have a number of pair-
wise models. Depending on which independence as-
sumptions we make, we can define a joint distribu-
tion over all the languages in various ways. For ex-
ample, for three languages, A, B, and C, and we can
use the following set of models:
P1(A,B,C) = P (A|B)P (B|C)P (C)
P2(A,B,C) = P (C|A)P (A|B)P (B)
P3(A,B,C) = P (B|C)P (C|A)P (A)
and
d?(P? , P1, P2, P3)
= D(P? ||P1) + D(P? ||P2) + D(P? ||P3)
= 2H(P? (A,C), P (A,C))
+ 2H(P? (A,B), P (A,B))
+ 2H(P? (B,C), P (B,C)) ? 3H(P? )
? H(P? (A), P (A)) ?H(P? (B), P (B))
? H(P? (C), P (C))
where H(?) is entropy, H(?, ?) is cross-entropy, and
P? (A,B) means P? marginalized to variables A,B.
The last three cross-entropy terms involve monolin-
gual models which are not parameterized. The en-
tropy term does not involve any of the pairwise dis-
tributions. Therefore, if P? is fixed, to maximize d?
877
we need to maximize each of the bilingual cross-
entropy terms.
This means we can apply the algorithm from
the previous section with a small modification
(Algorithm 2):
? Set al ?ij (for each language pair i, j) ran-
domly
? Repeat until change in d is very small:
? Compute Pi for i = 1 . . . k where k is the
number of the joint models we have cho-
sen
? Compute P? from {Pi}
? For i, j such that i 6= j
? Marginalize P? to (Li, Lj)
? Set ?ij in such a way as to minimize
D(P? (Li, Lj)||Pi(Li, Lj |?ij))
? Compute d according to the above for-
mula
Most of the ? parameters in our models can be
set by performing EM, and the rest are discrete with
only a few choices and can be maximized over by
trying all combinations of them.
4 Building Pairwise Models
We now know how to combine pairwise translation
models with some free parameters. Let us now dis-
cuss how such models might be built.
Our goal at this stage is to take a parallel bitext
in related languages A and B and produce a joint
probability model P (x, y), where x ? A, y ? B.
Equivalently, since the models PA(x) and PB(y)
are easily estimated by maximum likelihood tech-
niques from the bitext, we can estimate PA?B(y|x)
or PB?A(x|y). Without loss of generality, we will
build PA?B(y|x).
The model we are building will have a number of
free parameters. These parameters will be set by the
algorithm discussed above. In this section we will
assume that the parameters are fixed.
Our model is a mixture of several components,
each discussed in a separate section below:
PA?B(y|x) = ?fw(x)PfwA?B(y|x)
+ ?bw(x)PbwA?B(y|x)
+ ?char(x)PcharA?B(y|x)
+ ?pref (x)PprefA?B(y|x)
+ ?suf (x)PsufA?B(y|x)
+ ?cons(x)PconsA?B(y|x)
(1)
where all ?s sum up to one. The ?s are free pa-
rameters, although to avoid over-training we tie the
?s for x?s with similar frequencies. These lambdas
form a part of the ?ij parameter mentioned previ-
ously, where Li = A and Lj = B.
The components represent various constraints that
are likely to hold between related languages.
4.1 GIZA (forward)
This component is in fact GIZA++ software, origi-
nally created by John Hopkins University?s Summer
Workshop in 1999, improved by Och (2000). This
software can be used to create word alignments for
sentence-aligned parallel corpora as well as to in-
duce a probabilistic dictionary for this language pair.
The general approach taken by GIZA is as fol-
lows. Let LA and LB be the portions of the par-
allel text in languages A and B respectively, and
LA = (xi)i=1...n and LB = (yi)i=1...m. We can
define P (LB|LA) as
max
PA?B
max
Paligns
n?
i=1
m?
j=1
PA?B (yj |xi)Paligns (xi|j)
The GIZA software does the maximization by
building a variety of models, mostly described by
Brown et al (1993). GIZA can be tuned in various
ways, most importantly by choosing which models
to run and for how many iterations. We treat these
parameters as free, to be set alng with the rest at a
later stage.
As a side effect of GIZA?s optimization, we obtain
the PA?B(y|x) that maximizes the above expres-
sion. It is quite reasonable to believe that a model
of this sort is also a good model for our purposes.
This model is what we refer to as PfwA?B(y|x) in
the model overview.
GIZA?s approach is not, however, perfect. GIZA
builds several models, some quite complex, yet it
878
does not use all the information available to it, no-
tably the lexical similarity between the languages.
Furthermore, GIZA tries to map words (especially
rare ones) into other words if possible, even if the
sentence has no direct translation for the word in
question.
These problems are addressed by using other
models, described in the following sections.
4.2 GIZA (backward)
In the previous section we discussed using GIZA to
try to optimize P (LB|LA). It is, however, equally
reasonable to try to optimize P (LA|LB) instead. If
we do so, we can obtain PfwB?A(x|y) that pro-
duces maximal probability for P (LA|LB). We,
however need a model of PA?B(y|x). This is easily
obtained by using Bayes? rule:
PbwA?B(y|x) =
PfwB?A(x|y)PB(y)
PA(x)
which requires us to have PB(y) and PA(x). These
models can be estimated directly from LB and LA,
by using maximum likelihood estimators:
PA(x) =
?
i ?(xi, x)
n
and
PB(y) =
?
i ?(yi, y)
m
where ?(x, y) is the Kronecker?s delta function,
which is equal to 1 if its arguments are equal, and
to 0 otherwise.
4.3 Character-based model
This and the following models all rely on having a
model of PA?B(y|x) to start from. In practice it
means that this component is estimated following
the previous components and uses the models they
provide as a starting point.
The basic idea behind this model is that in related
languages words are also related. If we have a model
Pc of translating characters in language A into char-
acters in language B, we can define the model for
translating entire words.
Let word x in language A consists of characters
x1 through xn, and word y in language B consist of
characters y1 through ym.
Let us define (the unnormalized) character model:
Puchar(y|x) = Pcharlen(y|x,m)Plength(m|x)
i.e., estimating the length of y first, and y itself af-
terward. We make an independence assumption that
the length of y depends only on length of x, and are
able to estimate the second term above easily. The
first term is harder to estimate.
First, let us consider the case where lengths of x
and y are the same (m = n). Then,
Pcharlen(y|x, n) =
n?
i=1
Pc(yi|xi)
Let yj be word y with j?s character removed. Let
us now consider the case when m > n. We define
(recursively):
Pcharlen(y|x,m) =
m?
i=1
1
m
Pcharlen(y
i|x,m? 1)
Similarly, if n > m:
Pcharlen(y|x) =
n?
i=1
1
n
Pcharlen(y|x
i,m)
It is easy to see that this is a valid probability
model over all sequences of characters. However,
y is not a random sequence of characters, but a word
in language B, moreover, it is a word that can serve
as a potential translation of word x. So, to define a
proper distribution over words y given a word x and
a set of possible translations of x, T (x)
Pchar(y|x) = Puchar (y|x, y ? T (x))
= ?y??T (x)
Puchar(y,y?T (x)|x)?
y??T (x)
Puchar(y?|x)
This is the complete definition of Pchar, except
for the fact that we are implicitly relying upon the
character-mapping model, Pc, which we need to
somehow obtain. To obtain it, we rely upon GIZA
again. As we have seen, GIZA can find a good word-
mapping model if it has a bitext to work from. If we
have a PA?B word-mapping model of some sort, it
is equivalent to having a parallel bitext with words y
and x treated as a sequence of characters, instead of
indivisible tokens. Each (x, y) word pair would oc-
cur PA?B(x, y) times in this corpus. GIZA would
then provide us with the Pc model we need, by opti-
mizing the probability B language part of the model
given the language A part.
879
4.4 Prefix Model
This model and the two models that follow are built
on the same principle. Let there be a function f :
A ? CA and a function g : B ? CB . These func-
tions group words in A and B into some finite set of
classes. If we have some PA?B(y|x) to start with,
we can define
PfgA?B(y|x)
= P (y|g(y))P (g(y)|f(x))P (f(x)|x)
= P (y)
?
x?:f(x?)=f(x)
?
y?:g(y?)=g(y)
P (x?,y?)
(?
x?:f(x?)=f(x)
P (x?)
)(?
y?:g(y?)=g(y)
P (y?)
)
For the prefix model, we rely upon the following
idea: words that have a common prefix often tend to
be related. Related words probably should translate
as related words in the other language as well. In
other words, we are trying to capture word-level se-
mantic information. So we define the following set
of f and g functions:
fn(x) = prefix(x, n)
gm(y) = prefix(y,m)
where n and m are free parameters, whose values we
will determine later. We therefore define PprefA?B
as Pfg with f and g specified above.
4.5 Suffix Model
Similarly to a prefix model mentioned above, it is
also useful to have a suffix model. Words that have
the same suffixes are likely to be in the same gram-
matical case or share some morphological feature
which may persist across languages. In either case,
if a strong relationship exists between the result-
ing classes, it provides good evidence to give higher
likelihood to the word belonging to these classes. It
is worth noting that this feature (unlike the previous
one) is unlikely to be helpful in a setting where lan-
guages are not related.
The functions f and g are defined based on a set of
suffixes SA and SB which are learned automatically.
f(x) is defined as the longest possible suffix of x
that is in the set SA, and g is defined similarly, for
SB .
The sets SA and SB are built as follows. We start
with all one-character suffixes. We then consider
two-letter suffixes. We add a suffix to the list if it
occurs much more often than can be expected based
on the frequency of its first letter in the penultimate
position, times the frequency of its second letter in
the last position. We then proceed in a similar way
for three-letter suffixes. The threshold value is a free
parameter of this model.
4.6 Constituency Model
If we had information about constituent boundaries
in either language, it would have been useful to
make a model favoring alignments that do not cross
constituent boundaries. We do not have this infor-
mation at this point. We can assume, however, that
any sequence of three words is a constituent of sorts,
and build a model based on that assumption.
As before, let LA = (xi)i=1...n and LB =
(yi)i=1...m. Let us define as CA(i) a triple
of words (xi?1, xi, xi+1) and as CB(j) a triple
(yj?1, yj , yj+1). If we have some model PA?B , we
can define
PCA?CB (j|i) =
1
CPA?B(yj?1|xi?1)PA?B(yj |xi)
? PA?B(yj+1|xi+1)
where C is the sum over j of the above products, and
serves to normalize the distribution.
PconsA?B(y|x)
=
?n
i=1
?m
j=1
P (y|CB(j))PCA?CB (j|i)P (CA(i)|x)
=
?
i:xi=x
?m
j=1 P (y|CB(j))PCA?CB (j|i)
= 1?
j=1
?(yj ,y)
?
i:xi=x
?
j:yi=y PCA?CB (j|i)
5 Evaluation
The output of the system so far is a multi-lingual
word translation model. We will evaluate it by pro-
ducing a tri-lingual dictionary (Russian-Ukrainian-
Belorussian), picking a highest probability transla-
tion for each word, from the corresponding Bibles.
Unfortunately, we do not have a good hand-built tri-
lingual dictionary to compare it to, but only one
good bilingual one, Russian-Ukrainian5. We will
therefore take the Russian-Ukrainian portion of our
dictionary and compare it to the hand-built one.
Our evaluation metric is the number of entries that
match between these dictionaries. If a word has sev-
eral translations in the hand-built dictionary, match-
5The lack of such dictionaries is precisely why we do this
work
880
ing any of them counts as correct. It is worth not-
ing that for all the dictionaries we generate, the to-
tal number of entries is the same, since all the words
that occur in the source portion of the corpus have an
entry. In other words, precision and recall are pro-
portional to each other and to our evaluation metric.
Not all of the words that occur in our dictionary
occur in the hand-built dictionary and vice versa. An
absolute upper limit of performance, therefore, for
this evaluation measure is the number of left-hand-
side entries that occur in both dictionaries.
In fact, we cannot hope to achieve this number.
First, because the dictionary translation of the word
in question might never occur in the corpus. Second,
even if it does, but never co-occurs in the same sen-
tence as its translation, we will not have any basis
to propose it as a translation.6. Therefore we have
a ?achievable upper limit?, the number of words
that have their ?correct? translation co-occur at least
once. We will compare our performance to this up-
per limit.
Since there is no manual tuning involved we do
not have a development set, and use the whole bible
for training (the dictionary is used as a test set, as
described above).
We evaluate the performance of the model with
just the GIZA component as the baseline, and add
all the other components in turn. There are two pos-
sible models to evaluate at each step. The pairwise
model is the model given in equation 1 under the
parameter setting given by Algorithm 2, with Be-
lorussian used as a third language. The joint model
is the full model over these three languages as es-
timated by Algorithm 2. In either case we pick a
highest probability Ukrainian word as a translation
of a given Russian word.
The results for Russian-Ukrainian bibles are pre-
sented in Table 1. The ?oracle? setting is the set-
ting obtained by tuning on the test set (the dictio-
nary). We see that using a third language to tune
works just as well, obtaining the true global max-
imum for the model. Moreover, the joint model
(which is more flexible than the model in Equation
1) does even better. This was unexpected for us, be-
6Strictly speaking, we might be able to infer the word?s exis-
tence in some cases, by performing morphological analysis and
proposing a word we have not seen, but this seems too hard at
the moment
Table 1: Evaluation for Russian-Ukrainian (with Be-
lorussian to tune)
Stage Pair Joint
Forward (baseline) 62.3% 71.7%
Forward+chars 77.1% 84.2%
Forward+chars+backward 81.3% 84.1%
Fw+chars+bw+prefix 83.5% 84.5%
Fw+chars+bw+prefix+suffix 84.5% 85%
Fw+chars+bw+pref+suf+const 84.5% 85.2%
?Oracle? setting for ??s 84.6%
Table 2: Evaluation for Russian-Ukrainian (with Be-
lorussian and Polish)
Tuned by Pair Joint
Belorussian (prev. table) 84.5% 85.2% &
Polish 84.6% 78.6%
Both 84.5% 85.2%
?Oracle? tuning 84.5%
cause the joint model relies on three pairwise mod-
els equally, and Russian-Belorussian and Ukrainian-
Belorussian models are bound to be less reliable for
Russian-Ukrainian evaluation. It appears, however,
that our Belorussian bible is translated directly from
Russian rather than original languages, and parallels
Russian text more than could be expected.
To insure our results are not affected by this fact
we also try Polish separately and in combination
with Belorussian (i.e. a model over 4 languages),
as shown in Table 2.
These results demonstrate that the joint model
is not as good for Polish, but it still finds the
optimal parameter setting. This leads us to pro-
pose the following extension: let us marginalize
joint Russian-Ukrainian-Belorussian model into just
Russian-Ukrainian, and add this model as yet an-
other component to Equation 1. Now we cannot use
Belorussian as a third language, but we can use Pol-
ish, which we know works just as well for tuning.
The resulting performance for the model is 85.7%,
our best result to date.
881
6 Discussion and Future Work
We have built a system for multi-dictionary in-
duction from parallel corpora which significantly
improves quality over the standard existing tool
(GIZA) by taking advantage of the fact that lan-
guages are related and we have a group of more
than two of them. Because the system attempts to
be completely agnostic about the languages it works
on, it might be used successfully on many language
groups, requiring almost no linguistic knowledge on
the part of the user. Only the prefix and suffix com-
ponents are somewhat language-specific, but even
they are sufficiently general to work, with varying
degree of success, on most inflective and agglutina-
tive languages (which form a large majority of lan-
guages). For generality, we would also need a model
of infixes, for languages such as Hebrew or Arabic.
We must admit, however, that we have not tested
our approach on other language families yet. It is
our short term plan to test our model on several Ro-
mance languages, e.g. Spanish, Portuguese, French.
Looking at the first lines of Table 1, one can see
that using more than a pair of languages with a
model using only a small feature set can dramat-
ically improve performance (compare second and
third columns), while able to find the optimal val-
ues for all internal parameters.
As discussed in the introduction, the ultimate goal
of this project is to produce tools, such as a parser,
for languages which lack them. Several approaches
are possible, all involving the use of the dictionary
we built. While working on this project, we would
no longer be treating all languages in the same way.
We would use the tools available for that language to
further improve the performance of pairwise mod-
els involving that language and, indirectly, even the
pairs not involving this language. Using these tools,
we may be able to improve the word translation
model even further, simply as a side effect.
Once we build a high-quality dictionary for a spe-
cial domain such as the Bible, it might be possible to
expand to a more general setting by mining the Web
for potential parallel texts.
Our technique is limited in the coverage of the
resulting dictionary which can only contain words
which occur in our corpus. Whatever the corpus
may be, however, it will include the most common
words in the target language. These are the words
that tend to vary the most between related (and even
unrelated) languages. The relatively rare words (e.g.
domain-specific and technical terms) can often be
translated simply by inferring morphological rules
transforming words of one language into another.
Thus, one may expand the dictionary coverage us-
ing non-parallel texts in both languages, or even in
just one language if its morphology is sufficiently
regular.
References
The Central Intelligence Agency. 2004. The world fact-
book.
A. Bhattacharyya. 1943. On a measure of divergence be-
tween two statistical populations defined by their prob-
ability distributions. Bull. Calcutta Math. Soc., 35:99?
109.
P.F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
J. Hajic, J. Hric, and V. Kubon. 2000. Machine transla-
tion of very close languages. In Proccedings of the 6th
Applied Natural Language Processing Conference.
P. Koehn and K. Knight. 2001. Knowledge sources
for word-level translation models. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
G. Kondrak. 2001. Identifying cognates by phonetic
and semantic similarity. In Proceedings of the Second
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics, Pittsburgh, PA,
pages 103?110.
G. Mann and D. Yarowsky. 2001. Multipath transla-
tion lexicon induction via bridge languages. In Pro-
ceedings of the Second Meeting of the North American
Chapter of the Association for Computational Linguis-
tics, Pittsburgh, PA, pages 151?158.
I. D. Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26:221?249, June.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of the 38th Annual Meet-
ing of the Association for Computational Linguistics,
pages 440?447, Hongkong, China, October.
F. J. Och and H. Ney. 2001. Statistical multi-source
translation. In Proccedings of MT Summit VIII, pages
253?258.
882
Entropy Rate Constancy in Text
Dmitriy Genzel and Eugene Charniak
Brown Laboratory for Linguistic Information Processing
Department of Computer Science
Brown University
Providence, RI, USA, 02912
fdg,ecg@cs.brown.edu
Abstract
We present a constancy rate princi-
ple governing language generation. We
show that this principle implies that lo-
cal measures of entropy (ignoring con-
text) should increase with the sentence
number. We demonstrate that this is
indeed the case by measuring entropy
in three dierent ways. We also show
that this eect has both lexical (which
words are used) and non-lexical (how
the words are used) causes.
1 Introduction
It is well-known from Information Theory that
the most ecient way to send information
through noisy channels is at a constant rate. If
humans try to communicate in the most ecient
way, then they must obey this principle. The
communication medium we examine in this pa-
per is text, and we present some evidence that
this principle holds here.
Entropy is a measure of information rst pro-
posed by Shannon (1948). Informally, entropy
of a random variable is proportional to the di-
culty of correctly guessing the value of this vari-
able (when the distribution is known). Entropy
is the highest when all values are equally prob-
able, and is lowest (equal to 0) when one of the
choices has probability of 1, i.e. deterministi-
cally known in advance.
In this paper we are concerned with entropy
of English as exhibited through written text,
though these results can easily be extended to
speech as well. The random variable we deal
with is therefore a unit of text (a word, for our
purposes1) that a random person who has pro-
duced all the previous words in the text stream
is likely to produce next. We have as many ran-
dom variables as we have words in a text. The
distributions of these variables are obviously dif-
ferent and depend on all previous words pro-
duced. We claim, however, that the entropy of
these random variables is on average the same2.
2 Related Work
There has been work in the speech community
inspired by this constancy rate principle. In
speech, distortion of the audio signal is an extra
source of uncertainty, and this principle can by
applied in the following way:
A given word in one speech context might be
common, while in another context it might be
rare. To keep the entropy rate constant over
time, it would be necessary to take more time
(i.e., pronounce more carefully) in less common
situations. Aylett (1999) shows that this is in-
deed the case.
It has also been suggested that the principle
of constant entropy rate agrees with biological
evidence of how human language processing has
evolved (Plotkin and Nowak, 2000).
Kontoyiannis (1996) also reports results on 5
consecutive blocks of characters from the works
1It may seem like an arbitrary choice, but a word is a
natural unit of length, after all when one is asked to give
the length of an essay one typically chooses the number
of words as a measure.
2Strictly speaking, we want the cross-entropy between
all words in the sentences number n and the true model
of English to be the same for all n.
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 199-206.
                         Proceedings of the 40th Annual Meeting of the Association for
of Jane Austen which are in agreement with our
principle and, in particular, with its corollary as
derived in the following section.
3 Problem Formulation
Let fXig, i = 1 . . . n be a sequence of random
variables, with Xi corresponding to word wi in
the corpus. Let us consider i to be xed. The
random variable we are interested in is Yi, a ran-
dom variable that has the same distribution as
XijX1 = w1, . . . ,Xi?1 = wi?1 for some xed
words w
1
. . . wi?1. For each word wi there will
be some word wj , (j  i) which is the start-
ing word of the sentence wi belongs to. We will
combine random variables X
1
. . . Xi?1 into two
sets. The rst, which we call Ci (for context),
contains X
1
through Xj?1, i.e. all the words
from the preceding sentences. The remaining
set, which we call Li (for local), will contain
words Xj through Xi?1 . Both Li and Ci could
be empty sets. We can now write our variable
Yi as XijCi, Li.
Our claim is that the entropy of Yi , H(Yi)
stays constant for all i. By the denition of rel-
ative mutual information between Xi and Ci,
H(Yi) = H(XijCi, Li)
= H(XijLi) ? I(XijCi, Li)
where the last term is the mutual information
between the word and context given the sen-
tence. As i increases, so does the set Ci. Li, on
the other hand, increases until we reach the end
of the sentence, and then becomes small again.
Intuitively, we expect the mutual information
at, say, word k of each sentence (where Li has
the same size for all i) to increase as the sen-
tence number is increasing. By our hypothesis
we then expect H(XijLi) to increase with the
sentence number as well.
Current techniques are not very good at es-
timating H(Yi), because we do not have a
very good model of context, since this model
must be mostly semantic in nature. We have
shown, however, that if we can instead estimate
H(XijLi) and show that it increases with the
sentence number, we will provide evidence to
support the constancy rate principle.
The latter expression is much easier to esti-
mate, because it involves only words from the
beginning of the sentence whose relationship
is largely local and can be successfully cap-
tured through something as simple as an n-gram
model.
We are only interested in the mean value of
the H(Xj jLj) for wj 2 Si, where Si is the ith
sentence. This number is equal to 1
jS
i
j
H(Si),
which reduces the problem to the one of esti-
mating the entropy of a sentence.
We use three dierent ways to estimate the
entropy:
 Estimate H(Si) using an n-gram probabilis-
tic model
 Estimate H(Si) using a probabilistic model
induced by a statistical parser
 Estimate H(Xi) directly, using a non-para-
metric estimator. We estimate the entropy
for the beginning of each sentence. This
approach estimates H(Xi), not H(XijLi),
i.e. ignores not only the context, but also
the local syntactic information.
4 Results
4.1 N-gram
N-gram models make the simplifying assump-
tion that the current word depends on a con-
stant number of the preceding words (we use
three). The probability model for sentence S
thus looks as follows:
P (S) = P (w
1
)P (w
2
jw
1
)P (w
3
jw
2
w
1
)

n
?
i=4
P (wnjwn?1wn?2wn?3)
To estimate the entropy of the sentence S, we
compute log P (S). This is in fact an estimate of
cross entropy between our model and true distri-
bution. Thus we are overestimating the entropy,
but if we assume that the overestimation error is
more or less uniform, we should still see our esti-
mate increase as the sentence number increases.
Penn Treebank corpus (Marcus et al, 1993)
sections 0-20 were used for training, sections 21-
24 for testing. Each article was treated as a sep-
arate text, results for each sentence number were
grouped together, and the mean value reported
on Figure 1 (dashed line). Since most articles
are short, there are fewer sentences available for
larger sentence numbers, thus results for large
sentence numbers are less reliable.
The trend is fairly obvious, especially for
small sentence numbers: sentences (with no con-
text used) get harder as sentence number in-
creases, i.e. the probability of the sentence given
the model decreases.
4.2 Parser Model
We also computed the log-likelihood of the sen-
tence using a statistical parser described in
Charniak (2001)3. The probability model for
sentence S with parse tree T is (roughly):
P (S) =
?
x2T
P (xjparents(x))
where parents(x) are words which are parents
of node x in the the tree T . This model takes
into account syntactic information present in
the sentence which the previous model does not.
The entropy estimate is again log P (S). Overall,
these estimates are lower (closer to the true en-
tropy) in this model because the model is closer
to the true probability distribution. The same
corpus, training and testing sets were used. The
results are reported on Figure 1 (solid line). The
estimates are lower (better), but follow the same
trend as the n-gram estimates.
4.3 Non-parametric Estimator
Finally we compute the entropy using the esti-
mator described in (Kontoyiannis et al, 1998).
The estimation is done as follows. Let T be our
training corpus. Let S = fw
1
. . . wng be the test
sentence. We nd the largest k  n, such that
sequence of words w
1
. . . wk occurs in T . Then
log S
k is an estimate of the entropy at the word
w
1
. We compute such estimates for many rst
sentences, second sentences, etc., and take the
average.
3This parser does not proceed in a strictly left-to-right
fashion, but this is not very important since we estimate
entropy for the whole sentence, rather than individual
words
For this experiment we used 3 million words of
the Wall Street Journal (year 1988) as the train-
ing set and 23 million words (full year 1987) as
the testing set4. The results are shown on Fig-
ure 2. They demonstrate the expected behavior,
except for the strong abnormality on the second
sentence. This abnormality is probably corpus-
specic. For example, 1.5% of the second sen-
tences in this corpus start with words \the terms
were not disclosed", which makes such sentences
easy to predict and decreases entropy.
4.4 Causes of Entropy Increase
We have shown that the entropy of a sentence
(taken without context) tends to increase with
the sentence number. We now examine the
causes of this eect.
These causes may be split into two categories:
lexical (which words are used) and non-lexical
(how the words are used). If the eects are en-
tirely lexical, we would expect the per-word en-
tropy of the closed-class words not to increase
with sentence number, since presumably the
same set of words gets used in each sentence.
For this experiment we use our n-gram estima-
tor as described in Section 4.2. We evaluate
the per-word entropy for nouns, verbs, deter-
miners, and prepositions. The results are given
in Figure 3 (solid lines). The results indicate
that entropy of the closed class words increases
with sentence number, which presumably means
that non-lexical eects (e.g. usage) are present.
We also want to check for presence of lexical
eects. It has been shown by Kuhn and Mohri
(1990) that lexical eects can be easily captured
by caching. In its simplest form, caching in-
volves keeping track of words occurring in the
previous sentences and assigning for each word
w a caching probability Pc(w) =
C(w)
?
w
C(w)
, where
C(w) is the number of times w occurs in the
previous sentences. This probability is then
mixed with the regular probability (in our case
- smoothed trigram) as follows:
Pmixed(w) = (1 ? ?)Pngram(w) + ?Pc(w)
4This is not the same training set as the one used in
two previous experiments. For this experiment we needed
a larger, but similar data set
0 5 10 15 20 25
6.8
7
7.2
7.4
7.6
7.8
8
8.2
8.4
sentence number
e
n
tro
py
 e
st
im
at
e
parser
n?gram
Figure 1: N-gram and parser estimates of entropy (in bits per word)
0 5 10 15 20 25
8
8.1
8.2
8.3
8.4
8.5
8.6
8.7
8.8
8.9
9
sentence number
e
n
tro
py
 e
st
im
at
e
Figure 2: Non-parametric estimate of entropy
where ? was picked to be 0.1. This new prob-
ability model is known to have lower entropy.
More complex caching techniques are possible
(Goodman, 2001), but are not necessary for this
experiment.
Thus, if lexical eects are present, we expect
the model that uses caching to provide lower
entropy estimates. The results are given in Fig-
ure 3 (dashed lines). We can see that caching
gives a signicant improvement for nouns and a
small one for verbs, and gives no improvement
for the closed-class parts of speech. This shows
that lexical eects are present for the open-class
parts of speech and (as we assumed in the previ-
ous experiment) are absent for the closed-class
parts of speech. Since we have proven the pres-
ence of the non-lexical eects in the previous
experiment, we can see that both lexical and
non-lexical eects are present.
5 Conclusion and Future Work
We have proposed a fundamental principle of
language generation, namely the entropy rate
constancy principle. We have shown that en-
tropy of the sentences taken without context in-
creases with the sentence number, which is in
agreement with the above principle. We have
also examined the causes of this increase and
shown that they are both lexical (primarily for
open-class parts of speech) and non-lexical.
These results are interesting in their own
right, and may have practical implications as
well. In particular, they suggest that language
modeling may be a fruitful way to approach is-
sues of contextual influence in text.
Of course, to some degree language-modeling
caching work has always recognized this, but
this is rather a crude use of context and does
not address the issues which one normally thinks
of when talking about context. We have seen,
however, that entropy measurements can pick
up much more subtle influences, as evidenced
by the results for determiners and prepositions
where we see no caching influence at all, but nev-
ertheless observe increasing entropy as a func-
tion of sentence number. This suggests that
such measurements may be able to pick up more
obviously semantic contextual influences than
simply the repeating words captured by caching
models. For example, sentences will dier in
how much useful contextual information they
carry. Are there useful generalizations to be
made? E.g., might the previous sentence always
be the most useful, or, perhaps, for newspa-
per articles, the rst sentence? Can these mea-
surements detect such already established con-
textual relations as the given-new distinction?
What about other pragmatic relations? All of
these deserve further study.
6 Acknowledgments
We would like to acknowledge the members of
the Brown Laboratory for Linguistic Informa-
tion Processing and particularly Mark Johnson
for many useful discussions. Also thanks to
Daniel Jurafsky who early on suggested the in-
terpretation of our data that we present here.
This research has been supported in part by
NSF grants IIS 0085940, IIS 0112435, and DGE
9870676.
References
M. P. Aylett. 1999. Stochastic suprasegmentals: Re-
lationships between redundancy, prosodic struc-
ture and syllabic duration. In Proceedings of
ICPhS?99, San Francisco.
E. Charniak. 2001. A maximum-entropy-inspired
parser. In Proceedings of ACL?2001, Toulouse.
J. T. Goodman. 2001. A bit of progress in lan-
guage modeling. Computer Speech and Language,
15:403{434.
I. Kontoyiannis, P. H. Algoet, Yu. M. Suhov, and
A.J. Wyner. 1998. Nonparametric entropy esti-
mation for stationary processes and random elds,
with applications to English text. IEEE Trans.
Inform. Theory, 44:1319{1327, May.
I. Kontoyiannis. 1996. The complexity and en-
tropy of literary styles. NSF Technical Report No.
97, Department of Statistics, Stanford University,
June. [unpublished, can be found at the author?s
web page].
R. Kuhn and R. De Mori. 1990. A cache-based
natural language model for speech reproduction.
IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 12(6):570{583.
2 4 6 8 10
8
8.5
9
9.5
Nouns
normal 
caching
2 4 6 8 10
9.5
10
10.5
11
Verbs
normal 
caching
2 4 6 8 10
4.6
4.8
5
5.2
5.4
Prepositions
normal 
caching
2 4 6 8 10
3.7
3.8
3.9
4
4.1
4.2
4.3
4.4
Determiners
normal 
caching
Figure 3: Comparing Parts of Speech
M. P. Marcus, B. Santorini, and M. A. Marcin-
kiewicz. 1993. Building a large annotated cor-
pus of English: the Penn treebank. Computational
Linguistics, 19:313{330.
J. B. Plotkin and M. A. Nowak. 2000. Language
evolution and information theory. Journal of The-
oretical Biology, pages 147{159.
C. E. Shannon. 1948. A mathematical theory of
communication. The Bell System Technical Jour-
nal, 27:379{423, 623{656, July, October.
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 376?384,
Beijing, August 2010
Automatically Learning Source-side Reordering Rules for Large Scale
Machine Translation
Dmitriy Genzel
Google, Inc.
dmitriy@google.com
Abstract
We describe an approach to automatically
learn reordering rules to be applied as a
preprocessing step in phrase-based ma-
chine translation. We learn rules for 8 dif-
ferent language pairs, showing BLEU im-
provements for all of them, and demon-
strate that many important order trans-
formations (SVO to SOV or VSO, head-
modifier, verb movement) can be captured
by this approach.
1 Introduction
One of the major problems of modern statisti-
cal machine translation relates to its difficulties
in producing the correct word order on the target
side of the translation where the source side or-
der is not the same as the target side. In many
cases where the translation is spectacularly bad, if
one only enters the source sentence in the word or-
der of the target language the translation becomes
near-perfect (largely because the language model
can now make sense of it). The word order prob-
lems are especially extensive for languages that
have major differences, such as SOV vs. SVO
languages, but also cause insidious, but entirely
avoidable errors for the language pairs where the
word order is almost right, but not quite1. For
practical reasons all phrase-based decoders limit
the amount of reordering allowed and thus are
completely unable to produce correct translations
when the necessary movement is over a large dis-
tance. Furthermore, where the actual systematic
reordering for the two languages is within the de-
coder?s search space, it is penalized just as any
1For example of the latter kind, verb movement for
English-German and similar language pairs often causes
verbs to be aligned to nothing and to be altogether dropped
in translation.
other kind of reordering, whereas doing anything
other than this systematic reordering should in fact
be penalized.
It has been argued that this is a fundamental
flaw in phrase-based decoding systems and hier-
archical and syntax-based systems have been pro-
posed to solve this problem. These systems can
in principle resolve a part of this problem, but at
a significant time cost during training, and even
worse, during translation, making it less practical
for realtime systems. Instead we propose a system
for learning pre-ordering rules automatically from
data and demonstrate that it can capture many dif-
ferent kinds of reordering phenomena and do so at
no additional online cost.
2 Related Work
Many solutions to the reordering problem have
been proposed, e.g. syntax-based models (Chi-
ang, 2005), lexicalized reordering (Och et al,
2004), and tree-to-string methods (Zhang et al,
2006). All these methods try to solve the reorder-
ing problem in different ways, but have the fol-
lowing problems in common: word alignment is
not affected by them and they tend to introduce
significant additional work to be done at transla-
tion time. Most state of the art systems use HMM
or IBM Model 4 word alignment, both of which
have a penalty term associated with long distance
jumps, and tend to misalign words which move far
from their expected positions.
We are going to focus on the approaches where
reordering is done as a preprocessing step (some-
times called pre-ordering). These approaches
have the advantage that they are independent of
the actual MT system used, are often fast to ap-
ply, and tend to decrease (due to improved quality
of heuristic estimates) rather than dramatically in-
crease the time spent in actual decoding, unlike
376
some of the previously mentioned approaches.
The downside of these methods is that the reorder-
ing is fixed, and if it is wrong it can hurt the quality
of translations. We will discuss solutions for this
problem later.
Even in the relatively limited space of
preprocessing-based reordering solutions, there
has been a large amount of previous work, as far
back as Brown et al (1992). Most approaches
focus on utilizing manually written rules for dif-
ferent languages. A common language pair for
which rules were proposed is German-English
(Nie?en and Ney, 2001; Collins et al, 2005).
There is similar work for Chinese-English (Wang
et al, 2007) and quite a few other languages.
Clearly, such methods work quite well, but require
linguistic expertise to produce. Our goal, how-
ever, is to learn reordering from parallel data that
is already available to an MT system in an entirely
unsupervised manner.
We are not the first to attempt this task. In
particular, Xia and McCord (2004) proposed a
way to automatically learn reordering patterns for
French-English. Their system parses parallel data
both on the source and target side and then uses
a variety of heuristics to extract reordering rules
which are then applied during training. More
recently, Li et al (2007) use a maximum en-
tropy system to learn reordering rules for binary
trees (i.e., whether to keep or reorder for each
node). An approach most similar to ours is that
of Rottmann and Vogel (2007) where they learn
reordering rules based on sequences of part-of-
speech tags (but do not use parse trees). All of
these approaches show improvements in transla-
tion quality, but are applied on a single language
pair. Our goal is to find a method that works
well for many language pairs, regardless of the
word order transformations needed, and without
language-specific tuning. Unlike our predeces-
sors, we use a systematic search through the space
of possible permutation rules to minimize a spe-
cific metric, related to the monotonicity of result-
ing alignments.
3 Our Approach
We limit ourselves to reorderings of the source
side of training and test data. To constrain our
reorderings, we first produce a parse tree, using
a dependency parser similar to that of Nivre and
Scholz (2004). The above parser is much faster
than the time spent in translating the same sen-
tence and thus creates almost no overhead. In
our experiments where the source language is En-
glish the training data for the parser is the Penn
Treebank (Marcus et al, 1993). For German, we
use TIGER treebank (Brants et al, 2002). We
then convert the dependency tree to a shallow con-
stituent tree. The trees are annotated by both
Penn Treebank part of speech tags and by Stan-
ford dependency types (de Marneffe et al, 2006;
de Marneffe and Manning, 2008). For an exam-
ple, see Figure 1a.
Our reorderings are constrained by reordering
of nodes in a parse tree of the source sentence.
Thus, the full space of reorderings we consider
consists of all reorderings that would produce a
parse tree with the same set of child-parent rela-
tionships. For an example of a valid reordering,
see Figure 1b.
Each reordering is described by a series of
rules and we learn one such series for each lan-
guage pair automatically. Each source sentence is
parsed, and the tree is transformed sequentially,
one rule at a time applying to the entire tree, top
down. The reordered sentence is read off the
leaves of the tree and training and evaluation pro-
ceeds as normal. We are using a state-of-the-art
phrase-based statistical machine translation sys-
tem to perform the actual translation. The system
is itself capable of further local reordering during
translation limited by the maximum distance of 4
words.
3.1 Rule Space
Each rule consists of two parts: conditioning
context and action. For every internal node in
the parse tree, traversed top-down, the node is
matched against the conditioning context, and if a
match is found, the associated action applies. All
actions are limited to reordering children of the
matching node. Furthermore, if a rule applies at a
node, its descendants are not traversed for the pur-
pose of matching to avoid modifying the same part
of the sentence twice by the same rule. A differ-
ent rule may apply on this node or its descendants
377
_VBD
PRPI
nsubj
VBDsaw
head
_NN
dobj
DTa
det
NNman
head
RBeasily
advmod
(a) A sample parse tree
_VBD
PRPI
nsubj
VBDsaw
head
RBeasNoT
adtmid
_ll
dibj
Dya
dev
llman
head
(b) After reordering (moving RB over NN)
Figure 1: Parse tree of a sentence and its reordering
Feature Description
nT POS tag of this node
nL Syntactic label of this node
pT POS tag of the parent of this node
pL Syntactic label of the parent
1T POS tag of the first child
1L Label of the first child
2T POS tag of the second child
2L Label of the second child
... ...
Table 1: Set of features used as conditioning vari-
ables
later in the sequence.
A conditioning context is a conjunction of con-
ditions. Each condition is a (feature, value) pair.
List of features is given in table 1. In practice,
we limit ourselves to no more than 4 conditions in
a given context to avoid combinatorial explosion
and sparsity as well as contexts that fail to gen-
eralize. However, we may exhaustively generate
every possible conjunction of up to 5 conditions
from this list that covers up to 4 children that we
actually observe in training.
For example, the following contexts would be
valid for transformation in Fig. 1:
? nT = VBD
? 1T = PRP
? 1L = nsubj
? 3T = dobj
? etc.
or any conjunction of these. The action performed
in this example is swapping children 3 and 4 of
the VBD node, and can be denoted as the permu-
tation (1,2,4,3).
When processing a rule sequence, once a rule
applies, the action is performed, and that rule is
no longer applied on the same node or its descen-
dants (but can be further applied elsewhere in the
tree). Another rule (even an identical one) starts
from the top and can apply to nodes modified by
previous rules.
3.2 Reordering metrics
To evaluate the quality of a given reordering rule,
we need to have reliable metrics that, for each sen-
tence pair, can evaluate whether an improvement
in monotonicity has been made.
The easiest metric to use is the number of cross-
ing alignment links for a given aligned sentence
pair. For instance, in Figure 2, there are 2 cross-
ing links. This metric is trivial to compute and has
some nice properties. For instance, moving a sin-
gle word one position out of place causes one link
378
I have a dog
have? dog? I?
Figure 2: Counting crossing alignment links
to cross, moving it farther away from its correct
position would cause more links to cross. We will
refer to this metric as crossing score.
An ideal metric would be the actual BLEU
score that the system would obtain under this re-
ordering rule on the development set. However,
since each rule affects word alignment, phrase
extraction, optimal feature weights, and the ac-
tual translation, it would be necessary to retrain
the entire phrase-based system for each possible
rule, which is impractical. It is, however, practi-
cal, to retranslate the development set, keeping the
phrase table and feature weights constant. Nor-
mally, however, phrase tables contain multi-word
phrases, such as ?a b? which may no longer match
after the reordering, and this biases the system to-
ward the original word order. To avoid this, for
this computation only, we use a phrase table that
only contains single words and is therefore inde-
pendent of the source sentence word order. This
lets us test whether a given reordering improves
the search space for the phrase-based decoder at
the relatively small computational cost of trans-
lating the development set. We obtain a differ-
ence of the BLEU scores with and without a given
rule, which we hope to be a reasonable estimate
of the true gain in BLEU score that one would ob-
tain, by retraining the full system, including word
alignment, full-length phrase extraction, and tun-
ing the feature weights. We refer to this score as
estimated BLEU gain.
Note that these two scores are used to obtain an
estimate of utility of any given rule, and are not
used for evaluation of the entire system. Those
metrics are discussed in detail in the evaluation
section.
3.3 Algorithm
We propose a straightforward algorithm to au-
tomatically learn reordering rules. The input
data for all algorithms is word-aligned sentence
pairs. We have found that sophisticated align-
ment models introduce a bias toward alignment
between certain kinds of nodes (usually ones that
are close), and this has undesirable effects. In
practical terms this means that neither HMM nor
Model 4 alignments are useful (even though they
are better as alignments), but Model 1 alignments
are. However, to compensate for poor quality of
the alignments, we simply delete those alignment
links that have posterior probabilities under 0.52
and remove sentence pairs which have very few
alignments left. The crossing score works quite
well even when only a portion of the words in a
sentence are aligned.
The algorithm?s outline is given as Alg. 1.
The algorithm proceeds by considering all rules
after the best sequence of rules so far, and ap-
pends the best new rule (according to the metric)
to the sequence. In practice, some changes are
needed, and we describe some variations. Each
of these variations produces a different sequence
of rules, but they are interchangeable, and we can
simply pick one that performs best on the devel-
opment set, or to combine them through multi-
source translation or consensus.
In all variations, we are unable to generate all
possible rules for every sentence, as the number
can easily be 104-106 per sentence. It is sufficient,
however, to take a random sample of the input,
extract top candidates, and reevaluate those on the
entire set.
We also limit the kinds of rules we are allowed
to generate. The number of possible actions on a
node with n children is n! ? 1 and our trees are
quite shallow, often containing 5, 6, or even more
children per node. To avoid dealing with explo-
sion of rules and the resulting sparsity of the rule
space, we modify the process slightly, so that in-
stead of matching a node, we match a node and a
consecutive subsequence of its children of a given
size, as a sliding window. For example, in Figure
1a, node VBD has 4 children. If we limit our-
2This guarantees only one alignment per word
379
Algorithm 1 Optimizing alignment links
input: A set of aligned sentence pairs
base = <empty sequence>;
for several iterations do
candidate rules = GenerateAllCandidateRules(input, base);
base.append(MinCost(candidate rules))
end for
selves to 3 children at a time we would attempt to
match this node twice: with its children 1,2,3 and
2,3,4. In other words, we pretend to consider two
nodes, one with the first set of children, and one
with the second, proceeding left to right. If either
one matches, we apply the action to the subset of
children in the window and stop processing the
node further.
It is also useful to produce more than one rule
per iteration, although this can be problematic,
since the rules may interfere with each other.
3.3.1 Variant 1: Optimizing crossing score
We start with the initially empty base sequence.
As described above, we generate every possible
rule from a subset of sentences, and evaluate them
on the entire input, with the base sequence always
applied first. We use crossing score as a met-
ric. However, instead of extracting only one best-
scoring rule, we extract K best. Now we need to
obtain a decorrelated set: for every pair of rules,
we count the number of sentences where they both
apply. For every rule we consider all rules that are
ranked higher, and if the percentage of matches
between these two rules is high, the rules may
interfere with each other, and the current rule is
dropped. We thus obtain a small ordered set of
rules that tend to apply on different sentences, and
should not interfere with each other. From this
ordered set we produce all candidate rule subse-
quences and evaluate them, to ensure there really
is no interference. The one with the best score is
then appended to the base sequence. The process
is then repeated with a new base sequence.
3.3.2 Variant 2: Optimizing Estimated
BLEU gain
We proceed as in the previous variant, but final
evaluation of potential sequences to be appended
is done differently. Instead of using a crossing
score, we reorder the development set with each
candidate rule sequence and score it using a trans-
lation system with a fixed phrase table with sin-
gle word phrases only (to avoid bias for a spe-
cific word order). The sequence with the highest
BLEU is then appended to base sequence, and the
process is repeated.
3.3.3 Variant 3: Optimizing Estimated
BLEU gain in sequence
In this variant, once we obtain a set of
decorrelated candidate rules {a1, a2, . . . an} or-
dered by crossing score, we evaluate the fol-
lowing rule sequences (where b is base se-
quence): (b), (b, a1), (b, a1, a2) . . . (b, a1, . . . an)
using estimated BLEU gain, as above. If we
find that for some k, score(b, a1, . . . ak?1) >
score(b, a1, . . . ak?1, ak), that means that ak in-
terferes with preceding rules. We remove all
such ak, and retranslate/rescore until the score se-
quence is monotonically non-decreasing. At this
point, we append all surviving rules to the base
sequence, and repeat the process.
4 Evaluation
As described above, our base system is a phrase-
based statistical MT system, similar to that of
Och and Ney (2004). The baseline decoder is
capable of local reordering of up to 4 words.
Our training data is extracted by mining from the
Web, as well as from other published sources.
We train systems from English to 7 other lan-
guages, as well as German-English. We chose
them as follows: SOV languages (Japanese, Ko-
rean, Hindi), VSO language (Welsh), long dis-
tance verb movement (German), noun-modifier
issues (Russian and Czech). The amount of train-
ing data varies from 28 million words (for Hindi)
to 260 million (for German). The baseline sys-
380
tem is a production-quality system used by a large
number of users.
For the first set of experiments for German-
English and English-German we use WMT-09
data sets for development and testing (Callison-
Burch et al, 2009). We report BLEU scores for
each of the algorithms along with the best score
from the WMT-09 workshop for reference in Ta-
ble 2.
Unfortunately, there is no standard data set for
most of the languages we would like to experi-
ment with. For the second set of experiments, we
use an unpublished data set, containing data in En-
glish and 7 languages mentioned above. Our test
data comes from two sources: news articles from
WikiNews3 (996 sentences) and a set of random
sentences from the web (9000 sentences). From
these, we create 3 sets: dev1: 3000 sentences from
web and 486 sentences from wiki; dev2: 1000 sen-
tences from web; and test: the remainder of web
(5000 sentences) and wiki (510 sentences). The
dev1 set is used for tuning the system, both dev1
and dev2 for tuning consensus, and the test set for
evaluation. These sets are the same for all 7 lan-
guages.
Discriminative minimum error rate training
(Macherey et al, 2008) was applied to optimize
the feature weights for each system.
We evaluate the three variants of the algorithm
mentioned above. Each algorithm outputs a re-
ordering rule sequence (40-50 rules long) which
is applied to all the training and test data, and a
complete system is trained from scratch.
There is no need for us to pick a single al-
gorithm for all language pairs, since each algo-
rithm produces rules that are compatible with each
other. We are able to pick the algorithm that works
best on the development set for each language
pair.
In addition, we can use a decoder that is capa-
ble of performing a multi-input translation which
is given the unreordered input as well as the three
reordered inputs produced by the above algorithm.
This decoder is able to learn separate feature
weights for each feature/algorithm combination.
Finally, we can use consensus translation
3http://en.wikinews.org
Table 4: Manual vs. automatic reordering. Auto-
matic score is the combined score from Table 3.
Language Base Manual Auto-
matic
Diff
Hindi 16.85 19.25 19.36 0.11
Japanese 25.91 28.78 29.12 0.34
Korean 23.61 27.99 27.91 -0.08
(Macherey and Och, 2007) to produce the best
possible translation for each sentence.
Results using BLEU score (character-level for
Japanese and Korean, word-level for other lan-
guages) for English to X systems are given in Ta-
ble 3, along with the score of Google Translate as
of Feb 15, 2010, for expected quality reference.
All gains in the combined and consensus columns
are statistically significant using a bootstrap re-
sampling test (Noreen, 1989).
We should also note that the parsing and re-
ordering overhead was an average of 10msec per
sentence, and had no appreciable impact on the
speed of the system.
4.1 Comparison with manual reordering
We also compared our automatic method with a
manually written reordering rule set for SOV lan-
guages (Xu et al, 2009) (rules initially written for
Korean) for comparison with our approach. The
results are given in Table 4. The results are mostly
comparable, with automatic rules being better for
two of the three languages.
4.2 Turning off decoder reordering
All of the above experiments allowed the decoder
to further reorder the sentence as needed. Re-
ordering in the decoder creates an exponential in-
crease in the search space, and for a typical de-
coding strategy can lead to increase in decoding
time, search errors, or both. Since we already pre-
order the sentence, it should be possible to avoid
reordering in the decoder altogether.
Results for the combined decoder are given in
Table 5. It contains the gain of the combined de-
coder against the baseline from Table 3, and the
gain when decoder reordering is turned off against
the same baseline (which has decoder reordering
on). For many languages it is indeed now possi-
381
Table 2: Results for 3 algorithms on WMT-09 data with best individual system score from the workshop:
for EN to DE, Edinburgh, for DE to EN, Google
Language Base Var. 1 Var. 2 Var. 3 Best workshop
EN to DE 16.09 16.30 16.35 16.40 14.76
DE to EN 21.00 22.45 22.13 22.05 20.23
Table 3: Results on internal test set for 3 systems (Variant 1,2,3), the variant which performed best on
the development set, the combined system, and the consensus run, along with Google Translate scores
(Feb 15, 2010) for reference
Language Google Base Var. 1 Var. 2 Var. 3 Best on dev Combined Consensus
%BLEU %BLEU gain gain gain gain gain gain
Czech 16.68 15.35 -0.08 0.13 0.19 0.19 0.21 0.21
German 20.34 18.65 0.47 0.30 0.39 0.39 0.72 0.73
Hindi 19.15 16.85 2.25 2.08 0.15 2.08 2.51 2.47
Japanese 30.74 25.91 3.05 2.60 3.05 3.05 3.21 3.03
Korean 27.99 23.61 3.34 3.77 4.16 4.16 4.30 4.30
Russian 16.80 15.33 0.08 0.10 0.10 0.08 0.14 0.23
Welsh 27.38 25.48 1.25 0.77 1.43 1.43 1.34 1.63
Table 5: Disallowing decoder reordering: differ-
ence against baseline in %BLEU gain
Language Decoder
reordering
No decoder
reordering
Czech 0.21 0.08
German 0.72 0.55
Hindi 2.51 2.27
Japanese 3.21 3.21
Korean 4.30 4.15
Russian 0.14 -0.10
Welsh 1.34 0.98
ble to avoid decoder reordering altogether which
leads to a significant speedup.
5 Analysis
We looked at the rules being learned as well as at
the differences in the output to see if the gains in
BLEU are in fact due to the reordering phenomena
being resolved. The top rules for each language
are given in Table 6.
One can observe that the top rules for German
and Slavic languages are as expected: verb move-
ment and noun modifier reordering. Other top
rules for German cover other specific cases of verb
movement, other rules for Czech include, for ex-
ample, movement of the subject of the passive
sentence to the right and movement of the pos-
sessive (which is similar to the noun compound
case).
The rules for Welsh include movement of the
adjective modifier over its head (given in the ta-
ble above) and other rules moving noun modifiers,
moving a modal verb left over its subject, moving
determiners to the right of nouns, etc.
For Japanese and Korean, there are many rules
with dramatic impact, such as a rule moving all
heads to the right, reversing a sequence of three
nodes starting with a modal (e.g. can do some-
thing to something do can), moving numerical
modifiers to the right of their heads, and many oth-
ers.
Hindi is also an SOV language, but its gram-
mar is not as similar to Japanese or Korean as they
are to each other. Still, Hindi also has some simi-
lar rules, but there are many more involving verb
movement, such as a rule directly moving the verb
to the final position.
By looking at the sentences produced by the
system we can see that the differences are dra-
matic for SOV and VSO languages, as expected,
382
Table 6: Examples of top rules and their application
Languages Context Order Example
Hindi 1L:head 3L:none 2,1,3 I see him? I him see
Japanese, Korean 2L:prep 2,1 eat with a spoon? eat a spoon with
German 1T:VBN 2L:prep 2,1 struck with a ball? with a ball struck
Russian, Czech 1L:nn 2L:head 2,1 a building entrance? a entrance building
Welsh 1L:amod 2L:head 2,1 blue ball? ball blue
but more interestingly, most German sentences
now have a verb where the baseline had none. An-
other profound effect can be observed for Rus-
sian: the baseline almost invariably translated
noun compounds incorrectly: e.g. group leaders
may be translated as group of-leaders since this
requires no reordering and no preposition inser-
tion. This is especially problematic, since the user
of the translation system often cannot detect this:
the resulting sentence is not ungrammatical and
can even make sense. Our algorithm learns a rule
that prevents this from happening. Now the de-
coder must pay a cost to keep the order the same
as in English.
6 Discussion and Future Work
We have demonstrated a general technique which
requires only access to a parser for the source lan-
guage (in addition to parallel data which already
exists for an MT system) and is capable of re-
ducing reordering problems endemic in a phrase-
based system. No linguists or even native speakers
of any of these languages were needed to write the
rules. The algorithm is quite robust and performs
well on noisy web data, much of it being ungram-
matical.
All variants turned out to perform well, al-
though variants 1 and 3 were better most of the
time. We consider all variants to be useful, since
they find different local maxima under different
objective functions, and in practice use all of them
and pick a rule sequence that performs best on the
development set for any specific language pair.
We plan to explore this research area further in
several ways. First, it would be interesting to ex-
periment with applying rules learned for one lan-
guage to a related language, e.g. Portuguese for
Spanish or German for Dutch. This would let us
use rules learned from a major language for a mi-
nor one with less available training data.
We have only used English and German as
source languages. There is training data for
parsers in other languages, and this approach
should work well for most source languages.
Where a source language parser is not available,
we can still improve quality, by learning rules
from the target side and applying them only for the
purpose of improving word alignment. Improv-
ing word alignment alone would not help as much
as also using the reordering in the decoder, but it
will probably help in extracting better phrases. We
also plan to use parser projection to induce a rea-
sonable quality parser for other languages.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The tiger
treebank. In In Proceedings of the Workshop on
Treebanks and Linguistic Theories, pages 24?41.
Peter F. Brown, Stephen A. Della, Pietra Vincent,
J. Della Pietra, John D. Lafferty Robert, and L. Mer-
cer. 1992. Analysis, statistical transfer, and syn-
thesis in machine translation. In Proceedings of
the Fourth International Conference on Theoretical
and Methodological Issues in Machine Translation,
pages 83?100.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the ACL?05, pages 263?270, Ann Arbor,
Michigan, June.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
383
translation. In Proceedings of the ACL?05, pages
531?540, Ann Arbor, Michigan, June.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentations. In COLING?08 Workshop on Cross-
framework and Cross-domain Parser Evaluation,
Manchester, England, August.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure trees. In
LREC.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A probabilistic
approach to syntax-based reordering for statistical
machine translation. In Proceedings of the ACL-07,
pages 720?727, Prague, Czech Republic, June.
Wolfgang Macherey and Franz J. Och. 2007. An em-
pirical study on computing consensus translations
from multiple machine translation systems. In Pro-
ceedings of the EMNLP-CoNLL?07, pages 986?995,
Prague, Czech Republic, June.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation.
In Proceedings of the EMNLP-2008, pages 725?
734, Honolulu, Hawaii, October.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Sonja Nie?en and Hermann Ney. 2001. Morpho-
syntactic analysis for reordering in statistical ma-
chine translation. In Machine Translation Summit,
pages 247?252, Santiago de Compostela, Spain,
September.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings
of Coling 2004, pages 64?70, Geneva, Switzerland,
Aug 23?Aug 27. COLING.
Eric W. Noreen. 1989. Computer-Intensive Meth-
ods for Testing Hypotheses. John Wiley & Sons,
Canada.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004.
A smorgasbord of features for statistical machine
translation. In HLT-NAACL 2004: Main Proceed-
ings, pages 161?168, Boston, Massachusetts, USA,
May 2 - May 7.
Kay Rottmann and Stephan Vogel. 2007. Word re-
ordering in statistical machine translation with a
pos-based distortion model. In Proceedings of TMI,
Skovde, Sweden.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proceedings of the EMNLP-
CoNLL?2007, pages 737?745, Prague, Czech Re-
public, June.
Fei Xia and Michael McCord. 2004. Improving a
statistical MT system with automatically learned
rewrite patterns. In Proceedings of Coling 2004,
pages 508?514, Geneva, Switzerland, Aug 23?Aug
27. COLING.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In Pro-
ceedings of NAACL-HLT?09, Boulder, Colorado.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 256?263, New York City,
USA, June. Association for Computational Linguis-
tics.
384
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 158?166,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
?Poetic? Statistical Machine Translation: Rhyme and Meter
Dmitriy Genzel Jakob Uszkoreit Franz Och
Google, Inc.
1600 Amphitheatre Pkwy
Mountain View, CA 94043, USA
{dmitriy,uszkoreit,och}@google.com
Abstract
As a prerequisite to translation of poetry, we
implement the ability to produce translations
with meter and rhyme for phrase-based MT,
examine whether the hypothesis space of such
a system is flexible enough to accomodate
such constraints, and investigate the impact of
such constraints on translation quality.
1 Introduction
Machine translation of poetry is probably one of the
hardest possible tasks that can be considered in com-
putational linguistics, MT, or even AI in general. It
is a task that most humans are not truly capable of.
Robert Frost is reported to have said that poetry is
that which gets lost in translation. Not surprisingly,
given the task?s difficulty, we are not aware of any
work in the field that attempts to solve this problem,
or even discuss it, except to mention its difficulty,
and professional translators like to cite it as an exam-
ple of an area where MT will never replace a human
translator. This may well be true in the near or even
long term. However, there are aspects of the prob-
lem that we can already tackle, namely the problem
of the poetic form.
Vladimir Nabokov, in his famous translation of
Eugene Onegin (Nabokov, 1965), a poem with a
very strict meter and rhyming scheme, heavily dis-
parages those translators that attempt to preserve the
form, claiming that since it is impossible to perfectly
preserve both the form and the meaning, the form
must be entirely sacrificed. On the other hand, Dou-
glas Hofstadter, who spends 600 pages describing
how to translate a 60 word poem in 80 different ways
in Le Ton beau de Marot (1998), makes a strong case
that a poem?s form must be preserved in translation,
if at all possible. Leaving the controversy to the pro-
fessional translators, we investigate whether or not
it is possible to produce translations that conform to
certain metrical constraints common in poetry.
Statistical machine translation techniques, unlike
their traditional rule-based counterparts, are in fact
well-suited to the task. Because the number of po-
tential translation hypotheses is very large, it is not
unreasonable to expect that some of them should
conform to an externally imposed standard. The
goal of this paper is to investigate how these hy-
potheses can be efficiently identified, how often they
are present, and what the quality penalty for impos-
ing them is.
2 Related Work
There has been very little work related to the transla-
tion of poetry. There has been some work where MT
techniques were used to produce poetry (Jiang and
Zhou, 2008). In other computational poetry work,
Ramakrishnan et al(2009) generate song lyrics from
melody and various algorithms for poetry gener-
ation (Manurung et al, 2000; D??az-Agudo et al,
2002). There are books (Hartman, 1996) and arti-
cles (Bootz, 1996) on the subject of computer poetry
from a literary point of view. Finally, we must men-
tion Donald Knuth?s seminal work on complexity of
songs (Knuth, 1984).
158
3 Statistical MT and Poetry
We can treat any poetic form as a constraint on the
potential outputs. A naive approach to ensure that an
output of the MT system is, say, a haiku, is to create
a haiku detector and to examine a (very large) n-best
list of translations. This approach would not suc-
ceed very often, since the haikus that may be among
the possible translations are a very small fraction of
all translations, and the MT decoder is not actively
looking for them, since it is not part of the cost it
attempts to minimize. Instead, we would want to re-
cast ?Haikuness? as a feature function, such that a
real haiku has 0 cost, and those outputs that are not,
have large cost. This feature function must be local,
rather than global, so as to guide the decoder search.
The concept of feature functions as used in sta-
tistical MT is described by Och and Ney (Och and
Ney, 2002). For a phrase based system, a feature
function is a function whose inputs are a partial hy-
pothesis state sin, and a phrase pair p, and whose
outputs are the hypothesis state after p is appended
to the hypothesis: sout, and the cost incurred, c. For
hierarchical, tree-to-string and some other types of
MT systems which combine two partial hypotheses
and are not generating translations left-to-right, one
instead has two partial hypotheses states sleft and
sright as inputs, and the outputs are the same. Our
first goal is to describe how these functions can be
efficiently implemented.
The feature function costs are multiplied by fixed
weights and added together to obtain the total hy-
pothesis cost. Normally feature functions include
the logarithm of probability of target phrase given
source, source given target and other phrase-local
features which require no state to be kept, as well
as features like language model, which require non-
trivial state. The weights are usually learned auto-
matically, however we will set them manually for
our feature functions to be effectively infinite, since
we want them to override all other sources of infor-
mation.
We will now examine some different kinds of po-
etry and consider the properties of such feature func-
tions, especially with regard to keeping necessary
state. We are concerned with minimizing the amount
of information to be kept, both due to memory re-
quirements, and especially to ensure that compati-
ble hypotheses can be efficiently recombined by the
decoder.
3.1 Line-length constrained poetry
Some poetic genres, like the above-mentioned
haiku, require that a poem contain a certain num-
ber of lines (3 for haiku), each containing a certain
number of syllables (5,7,5 for haiku). These gen-
res include lanternes, fibs, tankas, and many others.
These genres impose two constraints. The first con-
straint is on total length. This requires that each hy-
pothesis state contain the current translation length
(in syllables). In addition, whenever a hypothesis is
expanded, we must keep track of whether or not it
would be possible to achieve the desired final length
with such an expansion. For example, if in the ini-
tial state, we have a choice between two phrases, and
picking the longer of the two would make it impos-
sible to have a 17-syllable translation later on, we
must impose a high cost on it, so as to avoid going
down a garden path.
The second constraint is on placing line breaks:
they must come at word boundaries. Therefore the
5th and 12th (and obviously 17th) syllable must end
words. This also requires knowing the current hy-
pothesis? syllable length, but unlike the first con-
straint, it can be scored entirely locally, without con-
sidering possible future expansions. For either con-
straint, however, the sentence has to be assembled
strictly left-to-right, which makes it impossible to
build partial hypotheses that do not start the sen-
tence, which hierarchical and tree-to-string decoders
require.
3.2 Rhythmic poetry
Some famous Western poetry, notably Shakespeare,
is written in rhythmic poetry, also known as blank
verse. This poetry imposes a constraint on the pat-
tern of stressed and unstressed syllables. For exam-
ple, if we use 0 to indicate no stress, and 1 to indicate
stress, blank verse with iambic foot obeys the regu-
lar expression (01)?, while one with a dactylic foot
looks like (100)?. This genre is the easiest to han-
dle, because it does not require current position, but
only its value modulo foot length (e.g. for an iamb,
whether the offset is even or odd). It is even possi-
ble, as described in Section 4, to track this form in a
decoder that is not left-to-right.
159
3.3 Rhythmic and rhyming poetry
The majority of English poetry that was written un-
til recently has both rhythm and rhyme. Generally
speaking, a poetic genre of this form can be de-
scribed by two properties. The first is a rhyming
scheme. A rhyming scheme is a string of letters,
each corresponding to a line of a poem, such that
the same letter is used for the lines that rhyme.
E.g. aa is a scheme for a couplet, a 2-line poem
whose lines rhyme. A sonnet might have a com-
plicated scheme like abbaabbacdecde. The second
property concerns meter. Usually lines that rhyme
have the same meter (i.e. the exact sequence of
stressed and unstressed syllables). For example, an
iambic pentameter is an iamb repeated 5 times, i.e.
0101010101. We can describe a genre completely
by its rhyming scheme and a meter for each letter
used in the rhyming scheme. We will refer to this ob-
ject as genre description. E.g. {abab, a : 010101, b :
10101010} is a quatrain with trimeter iambic and
tetrameter trochaic lines. Note that the other two
kinds of poetry can also be fit by this structure, if
one permits another symbol (we use *) to stand for
the syllables whose stress is not important, e.g. a
haiku: {abc, a : ?????, b : ???????, c : ?????}.
For this type of genre, we need to obey the same two
constraints as in the line-based poetry, but also to en-
sure that rhyming constraints hold. This requires us
to include in a state, for any outstanding rhyme let-
ter, the word that occurred at the end of that line. It
is not sufficient to include just the syllable that must
rhyme, because we wish to avoid self-rhymes (word
rhyming with an identical word).
4 Stress pattern feature function
We will first discuss an easier special case, namely
a feature function for blank verse, which we will re-
fer to as stress pattern feature function. This feature
function can be used for both phrase-based and hier-
archical systems.
In addition to a statistical MT system (Och and
Ney, 2004; Koehn et al, 2007), it is necessary to
have the means to count the syllables in a word and
to find out which ones are stressed. This can be done
with a pronunciation module of a text-to-speech
system, or a freely available pronunciation dictio-
nary, such as CMUDict (Rudnicky, 2010). Out-of-
vocabulary words can be treated as always imposing
a high cost.
4.1 Stress pattern for a phrase-based system
In a phrase based system, the feature function state
consists of the current hypothesis length modulo
foot length. For a 2-syllable foot, it is either 0 or
1, for a 3-syllable foot, 0, 1, or 2. The proposed
target phrase is converted into a stress pattern using
the pronunciation module, and the desired stress pat-
tern is left shifted by the current offset. The cost is
the number of mismatches of the target phrase vs.
the pattern. For example, if the desired pattern is
010, current offset is 1, and the proposed new phrase
has pattern 10011, we shift the desired pattern by 1,
obtaining 100 and extend it to length 5, obtaining
10010, matching it against the proposal. There is
one mismatch, at the fifth position, and we report a
cost of 1. The new state is simply the old state plus
phrase length, modulo foot length, 0 in this example.
4.2 Stress pattern for a hierarchical system
In a hierarchical system, we in general do not know
how a partial hypothesis might be combined on the
left. A hypothesis that is a perfect fit for pattern 010
would be horrible if it is placed at an offset that is
not divisible by 3, and vice versa, an apparently bad
hypothesis might be perfectly good if placed at such
an offset. To solve this problem, we create states
that track how well a partial hypothesis fits not only
the desired pattern, but all patterns obtained by plac-
ing this pattern at any offset, and also the hypothesis
length (modulo foot length, as usual). For instance,
if we observe a pattern 10101, we record the fol-
lowing state: {length: 1, 01 cost: 5, 10 cost: 0}.
If we now combine this state with another, such as
{length: 0, 01 cost: 1, 10 cost: 0}, we simply add
the lengths, and combine the costs either of the same
kind (if left state?s length is even), or the opposite (if
it is odd). In this instance we get {length: 1, 01
cost: 5, 10 cost: 1}. If both costs are greater than 0,
we can subtract the minimum cost and immediately
output it as cost: this is the unavoidable cost of this
combination. For this example we get cost of 1, and
a new state: {length: 1, 01 cost: 4, 10 cost: 0}. For
the final state, we output the remaining cost for the
pattern we desire. The approach is very similar for
feet of length 3.
160
4.3 Stress pattern: Whatever fits
With a trivial modification we can output transla-
tions that can fit any one of the patterns, as long
as we do not care which. The approach is identical
for both hierarchical and phrase-based systems. We
simply track all foot patterns (length 2 and length
3 are the only ones used in poetry) as in the above
algorithm, taking care to combine the right pattern
scores based on length offset. The length offset now
has to be tracked modulo 2*3.
This feature function can now be used to trans-
late arbitrary text into blank verse, picking whatever
meter fits best. If no meters can fit completely, it
will produce translations with the fewest violations
(assuming the weight for this feature function is set
high).
5 General poetic form feature function
In this section we discuss a framework for track-
ing any poetic genre, specified as a genre descrip-
tion object (Section 3.3 above). As in the case of
the stress pattern function, we use a statistical MT
system, which is now required to be phrase-based
only. We also use a pronunciation dictionary, but
in addition to tracking the number and stress of syl-
lables, we must now be able to provide a function
that classifies a pair of words as rhyming or non-
rhyming. This is in itself a non-trivial task (Byrd
and Chodorow, 1985), due to lack of a clear defini-
tion of what constitutes a rhyme. In fact rhyming is
a continuum, from very strong rhymes to weak ones.
We use a very weak definition which is limited to a
single syllable: if the final syllables of both words
have the same nucleus and coda1, we say that the
words rhyme. We accept this weak definition be-
cause we prefer to err on the side of over-generation
and accept even really bad poetry.
5.1 Tracking the target length
The hardest constraint to track efficiently is the
range of lengths of the resulting sentence. Phrase-
based decoders use a limited-width beam as they
build up possible translations. Once a hypothesis
drops out of the beam, it cannot be recovered, since
no backtracking is done. Therefore we cannot afford
1In phonology, nucleus and coda together are in fact called
rhyme or rime
to explore a part of the hypothesis space which has
no possible solutions for our constraints, we must be
able to prune a hypothesis as soon as it leads us to
such a subspace, otherwise we will end up on an un-
recoverable garden path. To avoid this problem, we
need to have a set of possible sentence lengths avail-
able at any point in the search, and to impose a high
cost if the desired length is not in that set.
Computing this set exactly involves a standard dy-
namic programming sweep over the phrase lattice,
including only uncovered source spans. If the maxi-
mum source phrase size is k, source sentence length
is n and maximum target/source length ratio for a
phrase is l (and therefore target sentence is limited
to at most ln words), this sweep requires going over
O(n2) source ranges, each of which can be produced
in k ways, and tracking ln potential lengths in each,
resulting in O(n3kl) algorithm. This is unaccept-
ably slow to be done for each hypothesis (even not-
ing that hypotheses with the same set of already cov-
ered source position can share this computation).
We will therefore solve this task approximately.
First, we can note that in most cases the set of possi-
ble target lengths is a range. This is due to phrase
extraction constraints, which normally ensure that
the lengths of target phrases form a complete range.
This means that it is sufficient to track only a mini-
mum and maximum value for each range, reducing
time to O(n2k). Second, we can note that whenever
a source range is interrupted by a covered phrase and
split into two ranges, the minimal and maximal sen-
tence length is simply the sum of the correspond-
ing lengths over the two uncovered subranges, plus
the current hypothesis length. Therefore, if we pre-
compute the minimum and maximum lengths over
all ranges, using the same dynamic programming al-
gorithm in advance, it is only necessary to iterate
over the uncovered ranges (at most O(n), and O(1)
in practice, due to reordering constraints) at runtime
and sum their minimum and maximum values. As a
result, we only need to do O(n2k) work upfront, and
on average O(1) extra work for each hypothesis.
5.2 State space
A state for the feature function must contain the fol-
lowing elements:
? Current sentence length (in syllables)
161
? Set of uncovered ranges (as needed for the
computation above)
? Zero or more letters from the rhyming scheme
with the associated word that has an outstand-
ing rhyme
5.3 The combination algorithm
To combine the hypothesis state sin with a phrase
pair p, do the following
1. Initialize cost as 0, sout as sin
2. Update sout: increment sentence length by tar-
get phrase length (in syllables), update cover-
age range
3. Compute minimum and maximum achievable
sentence length; if desired length not in range,
increment cost by a penalty
4. For each word in the target phrase
(a) If the word?s syllable pattern does not
match against desired pattern, add number
of mismatches to cost
(b) If at the end of a line:
i. If the line would end mid-word, incre-
ment cost by a penalty
ii. Let x be this line?s rhyme scheme let-
ter
iii. If x is present in the state sout, check
if the word associated with x rhymes
with the current word, if not, incre-
ment cost by a penalty
iv. Remove x with associated word from
the state sout
v. If letter x occurs further in the
rhyming scheme, add x with the cur-
rent word to the state sout
5.4 Tracking multiple patterns
The above algorithm will allow to efficiently search
the hypothesis space for a single genre description
object. In practice, however, there may be several
desirable patterns, any one of which would be ac-
ceptable. A naive approach, to use multiple fea-
ture functions, one with each pattern, does not work,
since the decoder is using a (log-)linear model, in
which costs are additive. As a result, a pattern that
matches one pattern, but not another, will still have
high cost, perhaps as high as a pattern that partially
matches both. We need to combine feature functions
not linearly, but with a min operator. This is easily
achieved by creating a combined state that encodes
the union of each individual function?s states (which
can share most of the information), and in addition
each feature function?s current total cost. As long
as at least one function has zero cost (i.e. can po-
tentially match), no cost is reported to the decoder.
As soon as all costs become positive, the minimum
over all costs is reported to the decoder as unavoid-
able cost, and should be subtracted from each fea-
ture function cost, bringing the minimum stored in
the output state back to 0.
It is also possible to prune the set of functions that
are still viable, based on their cost, to avoid keeping
track of patterns that cannot possibly match. Using
this approach we can translate arbitrary text, provide
a large number of poetic patterns and expect to get
some sort of poem at the end. Given a wide variety
of poetic genres, it is not unreasonable to expect that
for most inputs, some pattern will apply. Of course,
for translating actual poetry, we would likely have a
specific form in mind, and a positive outcome would
be less likely.
6 Results
We train a baseline phrase-based French-English
system using WMT-09 corpora (Callison-Burch et
al., 2009) for training and evaluation. We use a pro-
prietary pronunciation module to provide phonetic
representation of English words.
6.1 Stress Pattern Feature Function
We have no objective means of ?poetic? quality eval-
uation. We are instead interested in two metrics:
percentage of sentences that can be translated while
obeying a stress pattern constraint, and the impact
of this constraint on BLEU score (Papineni et al,
2002). Obviously, WMT test set is not itself in any
way poetic, so we use it merely to see if arbitrary
text can be forced into this constraint.
The BLEU score impact on WMT has been fairly
consistent during our experimentation: the BLEU
score is roughly halved. In particular, for the
above system the baseline score is 35.33, and stress
162
Table 1: Stress pattern distribution
Name Pattern % of matches
Iamb 01 9.6%
Trochee 10 7.2%
Anapest 001 27.1%
Amphibrach 010 32.1%
Dactyl 100 23.8%
pattern-constrained system only obtains 18.93.
The proportion of sentences successfully matched
is 85%, and if we permit a single stress error, it is
93%, which suggests that the constraint can be sat-
isfied in the great majority of cases. The distribution
of stress patterns among the perfect matches is given
in Table 1.
Some of the more interesting example translations
with stress pattern enforcement enabled are given in
table 2.
6.2 Poetic Form Feature Function
For poetic form feature function, we perform the
same evaluation as above, to estimate the impact of
forcing prose into an arbitrary poetic form, but to get
more relevant results we also translate a poetic work
with a specific genre requirement.
Our poetic form feature function is given a list
of some 210 genre descriptions which vary from
Haikus to Shakespearean sonnets. Matching any one
of them satisfies the constraint. We translate WMT
blind set and obtain a BLEU score of 17.28 with the
baseline of 35.33 as above. The proportion of sen-
tences that satisfied one of the poetic constraints is
87%. The distribution of matched genres is given
in Table 3. Some of the more interesting example
translations are given in table 2.
For a proper poetic evaluation, we use a French
translation of Oscar Wilde?s Ballad of Reading Gaol
by Jean Guiloineau as input, and the original Wilde?s
text as reference. The poem consists of 109 stanzas
of 6 lines each, with a genre description of {abcbdb,
a/c/d: 01010101, b: 010101}. The French version
obeys the same constraint. We treat each stanza as a
sentence to be translated. The baseline BLEU score
is 10.27. This baseline score is quite low, as can
be expected for matching a literal MT translation
against a professional poetical translation. We eval-
uate our system with a poetic constraint given above.
Table 3: Genre distribution for WMT corpus.
(Descriptions of these genres can be found in Wikipedia,
http://en.wikipedia.org)
Genre Number Percentage
No poem 809 13.1%
Blank verse 5107 82.7%
Couplet 81 1.3%
Haiku 42 0.7%
Cinquain 33 0.5%
Dodoitsu 24 0.4%
Quinzaine 23 0.4%
Choka 18 0.3%
Fib 15 0.2%
Tanka 14 0.2%
Lanterne 4 0.1%
Triplet 1 0.02%
Quatrain 1 0.02%
Total 6172 100%
The resulting score is 7.28. Out of 109 stanzas, we
found 12 translations that satisfy the genre constraint
(If we allow any poetic form, 108 out of 109 stanzas
match some form). Two sample stanzas that satisfied
the constraints are given in Table 4.
7 Discussion and Future Work
In this work we demonstrate how modern-day sta-
tistical MT system can be constrained to search for
translations obeying particular length, meter, and
rhyming constraints, whether a single constraint, or
any one of a set. We further demonstrate that the hy-
pothesis space is often rich enough that these con-
straints can be satisfied. The impact on translation
quality, however, is quite profound, as is to be ex-
pected. It seems that at the present state of machine
translation, one does indeed have to choose between
getting either the form or the meaning right. In the
present form, however, we can already find good
translations, as a sort of found poetry (Drury, 2006),
by translating a large quantity of text, whether poetic
or not.
This is the first attempt to deal with poetry trans-
lation, and the great majority of work to achieve rea-
sonable quality in form and meaning still remains to
be done. One major problem with the current fea-
ture function is that while it can avoid the areas of
the search space where length constraints cannot be
163
Table 2: Example translations. Stressed syllables are italicized
Reference A police spokesman said three people had been arrested and the
material was being examined.
Baseline A policeman said that three people were arrested and that the ma-
terial is currently being analyzed.
Stress Pattern (001) A police said that three were arrested and that the equipment is
currently being examined.
Poetic: Couplet in amphi-
brachic tetrameter
An of ficer stated that three were arrested
and that the equipment is currently tested.
Reference A trio of retired generals launched a mutiny in the Lords, protest-
ing against cuts in military spending: being armed-forces minister
is, they claimed, a part-time job.
Baseline A trio of retired generals have launched a revolt among the Lords,
protesting against cuts in military spending: they have proclaimed
only Minister of Defence is for them, a part-time employment.
Stress Pattern (010) A trio of general retirement launched a rebellion among Lords,
protesting the spending cuts troops: they claimed Minister only
defense is for them, a part-time job.
Poetic: Blank Verse in amphi-
brachic trimeter
A trio of generals retired
have launched an uprising among Lords,
protesting the spending cuts members:
they minister only proclaimed the
defense is for them, a part-time job.
Reference We must continue to condemn human rights abuses in Burma.
Baseline We must continue to denounce the violations of human rights
abuses in Burma.
Stress Pattern (100) We must continue to speak out against rights abuses committed in
Burma.
Poetic: Haiku: 5-7-5 syllables We must continue
denounce violations of
human rights Burma.
164
Table 4: Sample translations from Oscar Wilde?s Ballad of Reading Gaol.
Wilde?s original Our translation
He did not wring his hands, as do Without hands twisted like these men,
Those witless men who dare Poor men without hope, dare
To try to rear the changeling Hope To nourish hope in our vault
In the cave of black Despair: Of desperation there
He only looked upon the sun, And looked toward the sun, drink cool
And drank the morning air. Until the evening air.
With slouch and swing around the ring We are in our circle we
We trod the Fool?s Parade! Dragged like the Fools? Parade!
We did not care: we knew we were It mattered little, since we were
The Devil?s Own Brigade: The Devil?s sad Brigade:
And shaven head and feet of lead A shaved head and the feet of lead
Make a merry masquerade. Regardless gay charade!
satisfied, it cannot avoid the areas where rhyming
constraints are impossible to satisfy. As a result, we
need to allow a very wide hypothesis beam (5000 per
each source phrase coverage), to ensure that enough
hypotheses are considered, so that there are some
that lead to correct solutions later. We do not cur-
rently have a way to ensure that this happens, al-
though we can attempt to constrain the words that
end lines to have possible rhymes, or employ other
heuristics. A more radical solution is to create an
entirely different decoding algorithm which places
words not left-to-right, or in a hierarchical fashion,
but first placing words that must rhyme, and build-
ing hypotheses around them, like human translators
of poetry do.
As a result, the system at present is too slow, and
we cannot make it available online as a demo, al-
though we may be able to do so in the future.
The current approach relies on having enough lex-
ical variety in the phrase table to satisfy constraints.
Since our goal is not to be literal, but to obtain a
satisfactory compromise between form and mean-
ing, it would clearly be beneficial to augment target
phrases with synonyms and paraphrases, or to allow
for words to be dropped or added.
8 Acknowledgements
We would like to thank all the members of the MT
team at Google, especially Richard Zens and Moshe
Dubiner, for their help. We are thankful to the
anonymous reviewers for their comments, especially
to the one that to our amazement did the entire re-
view in verse2.
References
P. Bootz. 1996. Poetic machinations. Visible Language,
30(2):118?37.
Roy J. Byrd and Martin S. Chodorow. 1985. Using an
on-line dictionary to find rhyming words and pronun-
ciations for unknown words. In Proceedings of the
23rd Annual Meeting of the Association for Compu-
tational Linguistics, pages 277?283, Chicago, Illinois,
USA, July. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
B. D??az-Agudo, P. Gerva?s, and P. Gonza?lez-Calero.
2002. Poetry generation in colibriza. In Advances in
Case-Based Reasoning, pages 157?159. Springer.
John Drury. 2006. The poetry dictionary. Writer?s Di-
gest Books.
C.O. Hartman. 1996. Virtual muse: experiments in com-
puter poetry. Wesleyan University Press.
Douglas R. Hofstadter. 1998. Le Ton Beau De Marot:
In Praise of the Music of Language. Perseus Books
Group.
2With the reviewer?s permission, we feel that the ex-
tra work done by the reviewer deserves to be seen by
more than a few people, and make it available for you to
view at: http://research.google.com/archive/
papers/review_in_verse.html
165
Long Jiang and Ming Zhou. 2008. Generating Chi-
nese couplets using a statistical MT approach. In
Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 377?
384, Manchester, UK, August. Coling 2008 Organiz-
ing Committee.
D.E. Knuth. 1984. The complexity of songs. Communi-
cations of the ACM, 27(4):344?346.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
H.M. Manurung, G. Ritchie, and H. Thompson. 2000.
Towards a computational model of poetry generation.
In Proceedings of AISB Symposium on Creative and
Cultural Aspects and Applications of AI and Cognitive
Science, pages 79?86. Citeseer.
Vladimir Nabokov. 1965. Eugene Onegin: A Novel in
Verse by Alexandr Pushkin, Translated from the Rus-
sian. Bollingen Foundation.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295?302, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Ananth Ramakrishnan A., Sankar Kuppan, and Sobha
Lalitha Devi. 2009. Automatic generation of Tamil
lyrics for melodies. In Proceedings of the Workshop
on Computational Approaches to Linguistic Creativity,
pages 40?46, Boulder, Colorado, June. Association for
Computational Linguistics.
Alex Rudnicky. 2010. The Carnegie Mellon pronounc-
ing dictionary, version 0.7a. Online.
166
Variation of Entropy and Parse Trees of Sentences as a Function of the
Sentence Number
Dmitriy Genzel and Eugene Charniak
Brown Laboratory for Linguistic Information Processing
Department of Computer Science
Brown University
Providence, RI, USA, 02912
{dg,ec}@cs.brown.edu
Abstract
In this paper we explore the variation of
sentences as a function of the sentence
number. We demonstrate that while the
entropy of the sentence increases with the
sentence number, it decreases at the para-
graph boundaries in accordance with the
Entropy Rate Constancy principle (intro-
duced in related work). We also demon-
strate that the principle holds for differ-
ent genres and languages and explore the
role of genre informativeness. We investi-
gate potential causes of entropy variation
by looking at the tree depth, the branch-
ing factor, the size of constituents, and the
occurrence of gapping.
1 Introduction and Related Work
In many natural language processing applications,
such as parsing or language modeling, sentences are
treated as natural self-contained units. Yet it is well-
known that for interpreting the sentences the dis-
course context is often very important. The later
sentences in the discourse contain references to the
entities in the preceding sentences, and this fact is
often useful, e.g., in caching for language model-
ing (Goodman, 2001). The indirect influence of the
context, however, can be observed even when a sen-
tence is taken as a stand-alone unit, i.e., without its
context. It is possible to distinguish between a set
of earlier sentences and a set of later sentences with-
out any direct comparison by computing certain lo-
cal statistics of individual sentences, such as their
entropy (Genzel and Charniak, 2002). In this work
we provide additional evidence for this hypothesis
and investigate other sentence statistics.
1.1 Entropy Rate Constancy
Entropy, as a measure of information, is often used
in the communication theory. If humans have
evolved to communicate in the most efficient way
(some evidence for that is provided by Plotkin and
Nowak (2000)), then they would communicate in
such a way that the entropy rate would be constant,
namely, equal to the channel capacity (Shannon,
1948).
In our previous work (Genzel and Charniak,
2002) we propose that entropy rate is indeed con-
stant in human communications. When read in con-
text, each sentence would appear to contain roughly
the same amount of information, per word, whether
it is the first sentence or the tenth one. Thus the
tenth sentence, when taken out of context, must ap-
pear significantly more informative (and therefore
harder to process), since it implicitly assumes that
the reader already knows all the information in the
preceding nine sentences. Indeed, the greater the
sentence number, the harder to process the sentence
must appear, though for large sentence numbers this
would be very difficult to detect. This makes intu-
itive sense: out-of-context sentences are harder to
understand than in-context ones, and first sentences
can never be out of context. It is also demonstrated
empirically through estimating entropy rate of vari-
ous sentences.
In the first part of the present paper (Sections 2
and 3) we extend and further verify these results. In
the second part (Section 4), we investigate the poten-
tial reasons underlying this variation in complexity
by looking at the parse trees of the sentences. We
also discuss how genre and style affect the strength
of this effect.
1.2 Limitations of Preceding Work
In our previous work we demonstrate that the word
entropy rate increases with the sentence number; we
do it by estimating entropy of Wall Street Journal
articles in Penn Treebank in three different ways. It
may be the case, however, that this effect is corpus-
and language-specific. To show that the Entropy
Rate Constancy Principle is universal, we need to
confirm it for different genres and different lan-
guages. We will address this issue in Section 3.
Furthermore, if the principle is correct, it should
also apply to the sentences numbered from the be-
ginning of a paragraph, rather than from the begin-
ning of the article, since in either case there is a shift
of topic. We will discuss this in Section 2.
2 Within-Paragraph Effects
2.1 Implications of Entropy Rate Constancy
Principle
We have previously demonstrated (see Genzel and
Charniak (2002) for detailed derivation) that the
conditional entropy of the ith word in the sentence
(Xi), given its local context Li (the preceding words
in the same sentence) and global context Ci (the
words in all preceding sentences) can be represented
as
H(Xi|Ci, Li) = H(Xi|Li)? I(Xi, Ci|Li)
where H(Xi|Li) is the conditional entropy of the
ith word given local context, and I(Xi, Ci|Li) is
the conditional mutual information between the ith
word and out-of-sentence context, given the local
context. Since Ci increases with the sentence num-
ber, we will assume that, normally, it will provide
more and more information with each sentence. This
would cause the second term on the right to increase
with the sentence number, and since H(Xi|Ci, Li)
must remain constant (by our assumption), the first
term should increase with sentence number, and it
had been shown to do so (Genzel and Charniak,
2002).
Our assumption about the increase of the mutual
information term is, however, likely to break at the
paragraph boundary. If there is a topic shift at the
boundary, the context probably provides more infor-
mation to the preceding sentence, than it does to the
new one. Hence, the second term will decrease, and
so must the first one.
In the next section we will verify this experimen-
tally.
2.2 Experimental Setup
We use the Wall Street Journal text (years 1987-
1989) as our corpus. We take all articles that con-
tain ten or more sentences, and extract the first ten
sentences. Then we:
1. Group extracted sentences according to their
sentence number into ten sets of 49559 sen-
tences each.
2. Separate each set into two subsets, paragraph-
starting and non-paragraph-starting sentences1.
3. Combine first 45000 sentences from each set
into the training set and keep all remaining data
as 10 testing sets (19 testing subsets).
We use a simple smoothed trigram language model:
P (xi|x1 . . . xi?1) ? P (xi|xi?2xi?1)
= ?1P? (xi|xi?2xi?1)
+ ?2P? (xi|xi?1)
+ (1? ?1 ? ?2)P? (xi)
where ?1 and ?2 are the smoothing coefficients2,
and P? is a maximum likelihood estimate of the cor-
responding probability, e.g.,
P? (xi|xi?2xi?1) =
C(xi?2xi?1xi)
C(xi?2xi?1)
where C(xi . . . xj) is the number of times this se-
quence appears in the training data.
We then evaluate the resulting model on each of
the testing sets, computing per-word entropy of the
set:
H?(X) =
1
|X|
?
xi?X
logP (xi|xi?2xi?1)
1First sentences are, of course, all paragraph-starting.
2We have arbitrarily chosen the smoothing coefficients to be
0.5 and 0.3, correspondingly.
1 2 3 4 5 6 7 8 9 10
5.8
5.9
6.0
6.1
6.2
6.3
6.4
6.5
6.6
Sentence number
En
tro
py
 (b
its
)
all sentences
paragraph?starting
non?paragraph?starting
Figure 1: Entropy vs. Sentence number
2.3 Results and Discussion
As outlined above, we have ten testing sets, one for
each sentence number; each set (except for the first)
is split into two subsets: sentences that start a para-
graph, and sentences that do not. The results for full
sets, paragraph-starting subsets, and non-paragraph-
starting subsets are presented in Figure 1.
First, we can see that the the entropy for full
sets (solid line) is generally increasing. This re-
sult corresponds to the previously discussed effect
of entropy increasing with the sentence number. We
also see that for all sentence numbers the paragraph-
starting sentences have lower entropy than the non-
paragraph-starting ones, which is what we intended
to demonstrate. In such a way, the paragraph-
starting sentences are similar to the first sentences,
which makes intuitive sense.
All the lines roughly show that entropy increases
with the sentence number, but the behavior at the
second and the third sentences is somewhat strange.
We do not yet have a good explanation of this phe-
nomenon, except to point out that paragraphs that
start at the second or third sentences are probably
not ?normal? because they most likely do not indi-
cate a topic shift. Another possible explanation is
that this effect is an artifact of the corpus used.
We have also tried to group sentences based on
their sentence number within paragraph, but were
unable to observe a significant effect. This may be
due to the decrease of this effect in the later sen-
tences of large articles, or perhaps due to the relative
weakness of the effect3.
3 Different Genres and Languages
3.1 Experiments on Fiction
3.1.1 Introduction
All the work on this problem so far has focused
on the Wall Street Journal articles. The results are
thus naturally suspect; perhaps the observed effect
is simply an artifact of the journalistic writing style.
To address this criticism, we need to perform com-
parable experiments on another genre.
Wall Street Journal is a fairly prototypical exam-
ple of a news article, or, more generally, a writing
with a primarily informative purpose. One obvious
counterpart of such a genre is fiction4. Another al-
ternative might be to use transcripts of spoken dia-
logue.
Unfortunately, works of fiction, are either non-
homogeneous (collections of works) or relatively
short with relatively long subdivisions (chapters).
This is crucial, since in the sentence number experi-
ments we obtain one data point per article, therefore
it is impossible to use book chapters in place of arti-
cles.
3.1.2 Experimental Setup and Results
For our experiments we use War and Peace (Tol-
stoy, 1869), since it is rather large and publicly avail-
able. It contains only about 365 rather long chap-
ters5. Unlike WSJ articles, each chapter is not writ-
ten on a single topic, but usually has multiple topic
shifts. These shifts, however, are marked only as
paragraph breaks. We, therefore, have to assume
that each paragraph break represents a topic shift,
3We combine into one set very heterogeneous data: both 1st
and 51st sentence might be in the same set, if they both start
a paragraph. The experiment in Section 2.2 groups only the
paragraph-starting sentences with the same sentence number.
4We use prose rather than poetry, which presumably is
even less informative, because poetry often has superficial con-
straints (meter); also, it is hard to find a large homogeneous
poetry collection.
5For comparison, Penn Treebank contains over 2400 (much
shorter) WSJ articles.
1 2 3 4 58.05
8.1
8.15
8.2
8.25
8.3
Sentence number since beginning of paragraph
En
tro
py 
in b
its
Real run    
Control runs
Figure 2: War and Peace: English
and treat each paragraph as being an equivalent of a
WSJ article, even though this is obviously subopti-
mal.
The experimental setup is very similar to the one
used in Section 2.2. We use roughly half of the data
for training purposes and split the rest into testing
sets, one per each sentence number, counted from
the beginning of a paragraph.
We then evaluate the results using the same
method as in Section 2.2. We expect that the en-
tropy would increase with the sentence number, just
as in the case of the sentences numbered from the
article boundary. This effect is present, but is not
very pronounced. To make sure that it is statistically
significant, we also do 1000 control runs for com-
parison, with paragraph breaks inserted randomly at
the appropriate rate. The results (including 3 ran-
dom runs) can be seen in Figure 2. To make sure
our results are significant we compare the correla-
tion coefficient between entropy and sentence num-
ber to ones from simulated runs, and find them to be
significant (P=0.016).
It is fairly clear that the variation, especially be-
tween the first and the later sentences, is greater
than it would be expected for a purely random oc-
currence. We will see further evidence for this in the
next section.
3.2 Experiments on Other Languages
To further verify that this effect is significant and
universal, it is necessary to do similar experiments
in other languages. Luckily, War and Peace is also
digitally available in other languages, of which we
pick Russian and Spanish for our experiments.
We follow the same experimental procedure as in
Section 3.1.2 and obtain the results for Russian (Fig-
ure 3(a)) and Spanish (Figure 3(b)). We see that re-
sults are very similar to the ones we obtained for
English. The results are again significant for both
Russian (P=0.004) and Spanish (P=0.028).
3.3 Influence of Genre on the Strength of the
Effect
We have established that entropy increases with the
sentence number in the works of fiction. We ob-
serve, however, that the effect is smaller than re-
ported in our previous work (Genzel and Charniak,
2002) for Wall Street Journal articles. This is to be
expected, since business and news writing tends to
be more structured and informative in nature, grad-
ually introducing the reader to the topic. Context,
therefore, plays greater role in this style of writing.
To further investigate the influence of genre and
style on the strength of the effect we perform exper-
iments on data from British National Corpus (Leech,
1992) which is marked by genre.
For each genre, we extract first ten sentences of
each genre subdivision of ten or more sentences.
90% of this data is used as training data and 10%
as testing data. Testing data is separated into ten
sets: all the first sentences, all the second sentences,
and so on. We then use a trigram model trained on
the training data set to find the average per-word en-
tropy for each set. We obtain ten numbers, which
in general tend to increase with the sentence num-
ber. To find the degree to which they increase, we
compute the correlation coefficient between the en-
tropy estimates and the sentence numbers. We report
these coefficients for some genres in Table 1. To en-
sure reliability of results we performed the described
process 400 times for each genre, sampling different
testing sets.
The results are very interesting and strongly sup-
port our assumption that informative and struc-
tured (and perhaps better-written) genres will have
1 2 3 4 59.2
9.3
9.4
9.5
9.6
9.7
9.8
Sentence number since beginning of paragraph
En
tro
py 
in b
its
Real run    
Control runs
(a) Russian
1 2 3 4 58.2
8.25
8.3
8.35
8.4
8.45
8.5
8.55
8.6
8.65
En
tro
py 
in b
its
Sentence number since beginning of paragraph
Real run    
Control runs
(b) Spanish
Figure 3: War and Peace
stronger correlations between entropy and sentence
number. There is only one genre, tabloid newspa-
pers6, that has negative correlation. The four gen-
res with the smallest correlation are all quite non-
informative: tabloids, popular magazines, advertise-
ments7 and poetry. Academic writing has higher
correlation coefficients than non-academic. Also,
humanities and social sciences writing is probably
more structured and better stylistically than science
and engineering writing. At the bottom of the table
we have genres which tend to be produced by pro-
fessional writers (biography), are very informative
(TV news feed) or persuasive and rhetorical (parlia-
mentary proceedings).
3.4 Conclusions
We have demonstrated that paragraph boundaries of-
ten cause the entropy to decrease, which seems to
support the Entropy Rate Constancy principle. The
effects are not very large, perhaps due to the fact
6Perhaps, in this case the readers are only expected to look
at the headlines.
7Advertisements could be called informative, but they tend
to be sets of loosely related sentences describing various fea-
tures, often in no particular order.
that each new paragraph does not necessarily rep-
resent a shift of topic. This is especially true in a
medium like the Wall Street Journal, where articles
are very focused and tend to stay on one topic. In
fiction, paragraphs are often used to mark a topic
shift, but probably only a small proportion of para-
graph breaks in fact represents topic shifts. We also
observed that more informative and structured writ-
ing is subject to stronger effect than speculative and
imaginative one, but the effect is present in almost
all writing.
In the next section we will discuss the potential
causes of the entropy results presented both in the
preceding and this work.
4 Investigating Non-Lexical Causes
In our previous work we discuss potential causes
of the entropy increase. We find that both lexical
(which words are used) and non-lexical (how the
words are used) causes are present. In this section
we will discuss possible non-lexical causes.
We know that some non-lexical causes are
present. The most natural way to find these causes is
to examine the parse trees of the sentences. There-
fore, we collect a number of statistics on the parse
BNC genre Corr. coef.
Tabloid newspapers ?0.342? 0.014
Popular magazines 0.073? 0.016
Print advertisements 0.175? 0.015
Fiction: poetry 0.261? 0.013
Religious texts 0.328? 0.012
Newspapers: commerce/finance 0.365? 0.013
Non-acad: natural sciences 0.371? 0.012
Official documents 0.391? 0.012
Fiction: prose 0.409? 0.011
Non-acad: medicine 0.411? 0.013
Newspapers: sports 0.433? 0.047
Acad: natural sciences 0.445? 0.010
Non-acad: tech, engineering 0.478? 0.011
Non-acad: politics, law, educ. 0.512? 0.004
Acad: medicine 0.517? 0.007
Acad: tech, engineering 0.521? 0.010
Newspapers: news reportage 0.541? 0.009
Non-acad: social sciences 0.541? 0.008
Non-acad: humanities 0.598? 0.007
Acad: politics, laws, educ. 0.619? 0.006
Newspapers: miscellaneous 0.622? 0.009
Acad: humanities 0.676? 0.007
Commerce/finance, economics 0.678? 0.007
Acad: social sciences 0.688? 0.004
Parliamentary proceedings 0.774? 0.002
TV news script 0.850? 0.002
Biographies 0.894? 0.001
Table 1: Correlation coefficient for different genres
trees and investigate if any statistics show a signifi-
cant change with the sentence number.
4.1 Experimental Setup
We use the whole Penn Treebank corpus (Marcus et
al., 1993) as our data set. This corpus contains about
50000 parsed sentences.
Many of the statistics we wish to compute are very
sensitive to the length of the sentence. For example,
the depth of the tree is almost linearly related to the
sentence length. This is important because the aver-
age length of the sentence varies with the sentence
number. To make sure we exclude the effect of the
sentence length, we need to normalize for it.
We proceed in the following way. Let T be the set
of trees, and f : T ? R be some statistic of a tree.
Let l(t) be the length of the underlying sentence for
0 2 4 6 8 100.985
0.99
0.995
1
1.005
1.01
1.015
Bucket number (for sentence number)
Ad
just
ed 
tree
 de
pth
Figure 4: Tree Depth
tree t. Let L(n) = {t|l(t) = n} be the set of trees of
size n. Let Lf (n) be defined as 1|L(n)|
?
t?L(n) f(t),
the average value of the statistic f on all sentences
of length n. We then define the sentence-length-
adjusted statistic, for all t, as
f ?(t) =
f(t)
Lf (l(t))
The average value of the adjusted statistic is now
equal to 1, and it is independent of the sentence
length.
We can now report the average value of each
statistic for each sentence number, as we have done
before, but instead we will group the sentence num-
bers into a small number of ?buckets? of exponen-
tially increasing length8. We do so to capture the
behavior for all the sentence numbers, and not just
for the first ten (as before), as well as to lump to-
gether sentences with similar sentence numbers, for
which we do not expect much variation.
4.2 Tree Depth
The first statistic we consider is also the most nat-
ural: tree depth. The results can be seen in Figure
4.
In the first part of the graph we observe an in-
crease in tree depth, which is consistent with the in-
creasing complexity of the sentences. In the later
8For sentence number n we compute the bucket number as
blog1.5 nc
0 2 4 6 8 100.96
0.98
1
1.02
1.04
1.06
1.08
1.1
1.12
1.14
Bucket number (for sentence number)
Ad
jus
ted
 bra
nch
ing
 fac
tor
Branching factor
NPs only        
Base NPs only   
Figure 5: Branching factor
sentences, the depth decreases slightly, but still stays
above the depth of the first few sentences.
4.3 Branching Factor and NP Size
Another statistic we investigate is the average
branching factor, defined as the average number of
children of all non-leaf nodes in the tree. It does
not appear to be directly correlated with the sentence
length, but we normalize it to make sure it is on the
same scale, so we can compare the strength of re-
sulting effect.
Again, we expect lower entropy to correspond to
flatter trees, which corresponds to large branching
factor. Therefore we expect the branching factor to
decrease with the sentence number, which is indeed
what we observe (Figure 5, solid line).
Each non-leaf node contributes to the average
branching factor. It is likely, however, that the
branching factor changes with the sentence num-
ber for certain types of nodes only. The most obvi-
ous contributors for this effect seem to be NP (noun
phrase) nodes. Indeed, one is likely to use several
words to refer to an object for the first time, but only
a few words (even one, e.g., a pronoun) when refer-
ring to it later. We verify this intuitive suggestion,
by computing the branching factor for NP, VP (verb
phrase) and PP (prepositional phrase) nodes. Only
NP nodes show the effect, and it is much stronger
(Figure 5, dashed line) than the effect for the branch-
0 2 4 6 8 100.98
0.99
1
1.01
1.02
1.03
1.04
1.05
Bucket number (for sentence number)
Ad
jus
ted
 bra
nch
ing
 fac
tor
Branching factor             
Branching factor w/o base NPs
Figure 6: Branching Factor without Base NPs
ing factor.
Furthermore, it is natural to expect that most of
this effect arises from base NPs, which are defined
as the NP nodes whose children are all leaf nodes.
Indeed, base NPs show a slightly more pronounced
effect, at least with regard to the first sentence (Fig-
ure 5, dotted line).
4.4 Further Investigations
We need to determine whether we have accounted
for all of the branching factor effect, by proposing
that it is simply due to decrease in the size of the base
NPs. To check, we compute the average branching
factor, excluding base NP nodes.
By comparing the solid line in Figure 6 (the origi-
nal average branching factor result) with the dashed
line (base NPs excluded), you can see that base NPs
account for most, though not all of the effect. It
seems, then, that this problem requires further in-
vestigation.
4.5 Gapping
Another potential reason for the increase in the sen-
tence complexity might be the increase in the use of
gapping. We investigate whether the number of the
ellipsis constructions varies with the sentence num-
ber. We again use Penn Treebank for this experi-
0 2 4 6 8 100.4
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
Bucket number (for sentence number)
Ad
just
ed 
num
ber
 of 
gap
s
Figure 7: Number of ellipsis nodes
ment9.
As we can see from Figure 7, there is indeed a sig-
nificant increase in the use of ellipsis as the sentence
number increases, which presumably makes the sen-
tences more complex. Only about 1.5% of all the
sentences, however, have gaps.
5 Future Work and Conclusions
We have discovered a number of interesting facts
about the variation of sentences with the sentence
number. It has been previously known that the com-
plexity of the sentences increases with the sentence
number. We have shown here that the complexity
tends to decrease at the paragraph breaks in accor-
dance with the Entropy Rate Constancy principle.
We have verified that entropy also increases with the
sentence number outside of Wall Street Journal do-
main by testing it on a work of fiction. We have also
verified that it holds for languages other than En-
glish. We have found that the strength of the effect
depends on the informativeness of a genre.
We also looked at the various statistics that show
a significant change with the sentence number, such
as the tree depth, the branching factor, the size of
noun phrases, and the occurrence of gapping.
Unfortunately, we have been unable to apply these
results successfully to any practical problem so far,
9Ellipsis nodes in Penn Treebank are marked with *?* .
See Bies et al (1995) for details.
primarily because the effects are significant on av-
erage and not in any individual instances. Finding
applications of these results is the most important
direction for future research.
Also, since this paper essentially makes state-
ments about human processing, it would be very ap-
propriate to to verify the Entropy Rate Constancy
principle by doing reading time experiments on hu-
man subjects.
6 Acknowledgments
We would like to acknowledge the members of the
Brown Laboratory for Linguistic Information Pro-
cessing and particularly Mark Johnson for many
useful discussions. This research has been supported
in part by NSF grants IIS 0085940, IIS 0112435, and
DGE 9870676.
References
A. Bies, M. Ferguson, K. Katz, and R. MacIntyre, 1995.
Bracketing Guidelines for Treebank II Style Penn Tree-
bank Project. Penn Treebank Project, University of
Pennsylvania.
D. Genzel and E. Charniak. 2002. Entropy rate con-
stancy in text. In Proceedings of ACL?2002, Philadel-
phia.
J. T. Goodman. 2001. A bit of progress in language mod-
eling. Computer Speech and Language, 15:403?434.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn treebank. Computational Linguistics,
19:313?330.
J. B. Plotkin and M. A. Nowak. 2000. Language evo-
lution and information theory. Journal of Theoretical
Biology, pages 147?159.
C. E. Shannon. 1948. A mathematical theory of commu-
nication. The Bell System Technical Journal, 27:379?
423, 623?656, July, October.
L. Tolstoy. 1869. War and Peace. Available online,
in 4 languages (Russian, English, Spanish, Italian):
http://www.magister.msk.ru/library/tolstoy/wp/wp00.htm.
