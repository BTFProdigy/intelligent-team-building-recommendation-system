Automatic Recognition of Chinese Unknown Words1 Based on Roles Tagging2 
 
Kevin Zhang (Hua-Ping ZHANG)  Qun LIU   Hao ZHANG    Xue-Qi CHENG 
Software Division, Institute of Computing Technology, Chinese Academy of Sciences 
NO. 6, South Road, Kexueyuan, Zhongguancun, Haidian Dist. P.O. BOX 2704, Beijing, P.R. China, 100080 
Email: {zhanghp,liuqun, zhanghao,cxq}@software.ict.ac.cn 
 
Abstract 
This paper presents a unified solution, which 
is based on the idea of ?roles tagging?, to the 
complicated problems of Chinese unknown words 
recognition. In our approach, an unknown word is 
identified according to its component tokens and 
context tokens. In order to capture the functions of 
tokens, we use the concept of roles. Roles are 
tagged through applying the Viterbi algorithm in 
the fashion of a POS tagger. In the resulted most 
probable roles sequence, all the eligible unknown 
words are recognized through a maximum patterns 
matching. We have got excellent precision and 
recalling rates, especially for person names and 
transliterations. The result and experiments in our 
system ICTCLAS shows that our approach based 
on roles tagging is simple yet effective. 
Keywords: Chinese unknown words recognition, 
roles tagging, word segmentation, Viterbi 
algorithm. 
Introduction 
It is well known that word segmentation is a 
prerequisite to Chinese information processing. 
Previous research and work in word segmentation 
have made great progresses. However, cases with 
unknown words are not satisfactory. In general, 
any lexicon is limited and unable to cover all the 
words in real texts or speeches. According to our 
statistics on a 2,305,896-character news corpus 
from the People's Daily, there are about 1.19% 
unknown words. But they are difficult to be 
recalled and often greatly reduce the recognition 
rate of known words close to them. For example, 
the sentence ? ? ? ? ? ? ? ? ? ? ? 
(Pronunciation: ?Bu Zhang Sun Jia Zheng Zai 
Gong Zuo.?)  has two valid segmentations:  ??
?/???/?/??? (The minister Sun Jiazheng is 
at work) and ??? /?? /?? /?? ? (The 
minister Sun Jia now is at work). ????? is a 
person name in the first, while ???? is another 
name in the latter. Meanwhile, the string ????
?? will lead to overlapping ambiguity and bring a 
collision between the unknown word ???
?? (Sun Jiazheng) and ????(zheng zai; now). 
What?s more, the recognizing precision rates of 
person names, place names, and transliterations are 
91.26%, 69.12%, and 82.83%, respectively, while 
the recalling rates of them are just 68.77%, 60.47%, 
and 78.29%, respectively. (Data from official 
testing in 1999) [Liu (1999)] In a word, unknown 
words recognition has become one of the biggest 
stumbling blocks on the way of Chinese lexical 
analysis. A proper solution is important and 
urgent.  
Various approaches are taken in Chinese 
unknown words recognition. They can be broadly 
categorized into ?one-for-one?, ?one-for-several? 
and ?one-for-all? based on the number of 
categories of unknown words, which can be 
recognized. One-for-one solutions solve a 
particular problem, such as person name 
recognition [Song (1993); Ji (2001)], place name 
recognition [Tan (1999)] and transliteration 
recognition [Sun (1993)]. Similarly, 
one-for-several approaches provide one solution 
for several specific categories of unknown words 
[Lv (2001); Luo (2001)]. One-for-all solutions, as 
far as we know, have not been applicable yet 
[Chen (1999); He (2001)].  
Although currently practicable methods could 
achieve great precision or recalling rates in some 
special cases, they have their inherent deficiencies. 
First of all, rules applied are mostly summarized 
by linguists through painful study of all kinds of 
huge ?special name libraries? [Luo (2001)]. It?s 
time-consuming, expensive and inflexible. The 
categories of unknown words are diverse and the 
amount of such words is huge. With the rapid 
development of the Internet, this situation is 
becoming more and more serious. Therefore, it?s 
very difficult to summarize simple yet thorough 
rules about their compositions and contexts. 
Secondly, the recognition process cannot be 
activated until some ?indicator? tokens are 
scanned in. For instance, possible surnames or 
titles often trigger person name recognition on the 
following 2 or more characters. In the case of 
place name recognition, the postfixes such as 
? ? ?(county), ? ? ?(city) will activate the 
recognition on the previous characters. What?s 
more, these methods tend to work only on the 
monosyllabic tokens, which are obvious fragments 
after tokenization [Luo (2001); Lv (2001)]. It takes 
the risk of losing lots of unknown words without 
any explicit features. Furthermore, this trigger 
mechanism cannot resolve the ambiguity. For 
example, unknown word ????? (Fang Lin Shan) 
maybe a person name ??/???(Fang Linshan) or 
a place name ???/??(Fanglin Mountain). 
This paper presents a one-for-all approach 
based on roles tagging to avoid such problems. 
The process is: tagging tokens after word 
segmentation with the most probable roles and 
making unknown words recognition based on roles 
sequence. The mechanism of roles tagging is just 
like that of a small and simple Part-Of-Speech 
tagger.  
The paper is organized as follows: In section 
2, we will describe the approach in general. 
Following that, we will present the solution in 
practice. In the final part, we provide recognition 
experiments using roles-tagging methods. The 
result and possible problems are discussed as well. 
1 Unknown words recognition based on roles 
tagging  
1.1 Lexical roles of unknown words 
Unknown words are often made up of 
distinctive components, most of which are 
monosyllabic characters or short words; in 
addition, there are some regular relations between 
unknown words and their locality, especially with 
their left and right context. As we often write or 
speak, a Chinese person name is usually comprised 
of a one-or-two-character surname and a following 
given name of one or two characters, like ???
??(Xiao Jianqun) and ?????(Zhu-Ge Liang). 
The  previous words are mostly titles, 
occupations or some conjunctive words, such as 
????(Manager), ????(Driver) and ???(To). 
The following words tend to be verbs such as ??? 
(to say) , ????(to express). Similar components, 
contexts and relations can be discovered in place 
name, transliteration, organization name, or other 
types of unknown words.   
We define unknown word roles with respect 
to varied internal components, previous and 
succeeding contexts and other tokens in a 
particular sentence. Various roles are extracted 
according to their functions in the forming of 
different unknown words. Person names roles and 
transliterations roles set are shown in table 1a and 
1b respectively. Using the roles set of person name, 
the tokens sequence ??/?/??/?/?/?/?/?/
?/??/?/??/?/?/??/? (What Zhou Enlai 
and Deng Yunchao used before death are 
presented in the museum) will be tagged as ??/A 
?/A ??/K ?/B ?/C ?/D ?/M ?/B ?/C 
??/V ?/A ??/A ?/A ?/A??/A?. 
Role Significance Examples 
B Surname or family 
name. 
?/?/?/???
??/? 
C First Chinese char in 
the 2-char given name 
?/?/?/?? 
D Last Chinese char in 
the 2-char given name. 
?/?/?/?? 
E Given name with a 
single Chinese char. 
?/? 
F Prefix in the name. ?/???/? 
G Postfix in the name. ?/???/???/? 
K Previous context before 
person name. 
?/??/?/?/
?/?/? 
L Succeeding context 
following person name. 
???/???
?/? 
M Parts between two 
person names. 
??/?/?/?/
?/?/?/?/?
U Known words 
generated by previous 
context and the first 
component of name. 
?? /?? /? /
?/?/??/??
? /?? /?? /
?/?/?/ 
V Known words 
generated by the last 
component and next 
context. 
? /? /?? /?
? /, ? /? /?
?/? 
..... 
A Others tokens not 
mentioned above. 
??/??/??/
??/?/?/?/ 
Table 1a: Roles set of Chinese person names 
 
Role Significance Examples 
B The first 
component of 
transliteration 
?/?/? 
C Middle component ?/?/?/?/?/?
/?/?/? 
D Last component  ?/?/? 
..... 
  
Table 1b: Roles set of transliterations 
1.2 Roles tagging and unknown words recognition 
On the one hand, the sentence include words 
with different roles for a particular category of 
unknown words, on the other hand, such words 
can be recognized after identifying their roles 
sequence. That is: tagging tokens after word 
segmentation with the most probable roles 
sequence, then recognizing unknown words by 
maximum patterns matching on the final roles 
sequence. 
Roles tagging is similar to Part-Of-Speech 
tagging. Our tagging process is based on Viterbi 
Algorithm [Rabiner and Juang (1989)], which is to 
select the optimum with maximum probability 
from all possible tag sequences. The methodology 
and its deduction is given as below: 
Suppose that T is the tokens sequence after 
word segmentation and R is the roles sequence for 
T. We take the role sequence R# with the 
maximum probability as the best choice. That is: 
T=(t1, t 2, ? , t m), 
R=(r1, r2, ? , rm), m>0, 
R#= arg P(R|T)...................?........E1    
R
max
According to the Bayes equation, we can get: 
P(R|T)= P(R)P(T|R)/P(T) ...................E2 
For a particular token sequence, P(T) is a 
constant. So, We can get E3 based on E1 and E2: 
R#= arg P(R)P(T|R) ......................E3 
R
max
We may consider T as the observation value 
sequence while R as the state sequence hidden 
behind the observation. Now we introduce Hidden 
Markov Model [Rabiner and Juang (1986)] to 
resolve such a typical problem:  
P(R) P(T|R)?  ?
=
?
m
i
iiii rrprtp
0
1 )|()|(
?R#? .......E4 
R
maxarg ?
=
?
m
i
iiii rrprtp
0
1 )|()|(
?R#? 
? ......E5 
R
minarg ?
=
?+
m
i
iiii rrprtp
0
1)}|(ln)|({ln
E5 is simpler for computation than E4. 
Now, we can find the most possible token 
sequence with equation E5. It?s a simple 
application of Viterbi Algorithm. 
The final recognition through maximum pattern 
matching is not performed on the original texts but 
performed on roles sequence. The person patterns 
are {BBCD, BBE, BBZ, BCD, BE, BG, BXD, BZ, CD, FB, 
Y, XD}. Before matching, we should split the 
tokens whose roles are like ?U? or ?V?(which 
indicate that the related token is generated by 
internal components and the outside contexts of 
unknown words) into two proper parts. Such a 
processing can recall more unknown words and 
reduce the overlapping collision. As for the above 
sample sentence, the final roles sequence after 
splitting is ?AAKBCDMBCDLAAAAAA?. Therefore, 
we can identify the possible person names ???
?? and ????? according to the recognition 
pattern ?BCD?. 
1.3  Automatic acquisition of roles knowledge 
 As described in E5, the tag sequence R#  is 
decided by two kinds of factors: and 
.  is the probability of a 
token t
)|( ii rtp
)|( 1?ii rrp )|( ii rtp
( irp
i given the condition of being tagged with 
role ri, while  is the transitive 
probability from role r
)| 1?ir
i-1 to role ri. Both factors are 
useful lexical knowledge for tagging and final 
recognition. According to laws of large numbers, if 
the training corpus is large enough, we can acquire 
the roles knowledge as following: 
)|( ii rtp ?C(ti,ri)/C(ri) ................??......... E6 
Where C(ti, ri) is the count of token ti  being role ri; 
and C(ri) is the count of role ri. 
)|( 1?ii rrp ?C(ri-1,ri)/C(ri-1) ........?.?.....?E7 
Where C(ri-1,ri) is the count of role ri-1  followed 
by role ri. 
 C(ti,ri), C(ri) and C(ri-1,ri) are extracted from 
corpus through a training process. The training 
corpus came from one-month news from the 
People?s Daily with 2,305,896 Chinese characters, 
which are manually checked after word 
segmentation and POS tagging (It can be 
downloaded at icl.pku.edu.cn, the homepage of the 
Institute of Computational Linguistics, Peking 
University).  
However, the corpus is tagged with the 
Part-Of-Speech set. Before training, the original 
POS tags should be converted to the proper roles 
by analysing every token in the sentence.  
2 Algorithm and implementation 
The unknown words recognition based on 
roles tagging has three main steps: automatic 
acquisition of roles knowledge from the corpus; 
roles tagging with Viterbi algorithm and unknown 
words recognition through maximum pattern 
matching. 
  Viterbi algorithm is a classic approach in 
statistics. It aims to select the optimum roles 
sequence with maximum possibility from all 
possible results. Our evaluation function for 
decision-making is E5 given in sub-section 1.2. 
Considering the length limitation of this paper, we 
skip the details.  
Therefore, we only provide algorithms for 
roles knowledge learning. In the last part, the 
entire process of unknown words recognition will 
be listed. 
2.1 Roles knowledge learning 
Input: Corpus which is segmented and POS 
tagged 
T: the type of unknown words; 
R: Roles set of T 
Output: C(ti,ri), C(ri) and C(ri-1,ri) 
Algorithm: 
(1) Get one sentence S from corpus C;  
(2) Extract all tokens and POS tags from S; 
(3) Convert all POS tags to roles in T after role 
analysis.  
(4) Store the tokens whose role is not ?A? into the 
recognition lexicons of unknown words T, where 
?A? is not internal components nor context role.  
(5) Calculate the total number C(ti,ri) of token ti  
being role ri. At the same time, count C(ri), which is 
the number of role ri appearances. 
(6) Sum C(ri-1,ri) which is the times of role ri-1 
followed by role ri. 
(7) If no more sentences in the corpus C, exit; else 
go to (1)  
   First of all, we must explain step (3). Our 
corpus is tagged with POS and person, place or 
organization name are tagged with ?nr?, ?ns? or ?nt? 
respectively; Such POS are unique and different 
from noun. Transliterations can be extracted from 
words tagged with ?nr? or ?ns? and through 
analysing its component chars. So we can easily 
locate such kinds of words. Meanwhile, we can 
judge whether a word is unknown by looking it up 
in the core lexicon. Then we can identify roles of 
words according to their locality, which are before 
or following a particular unknown word. 
Here we provide a sample sentence from our 
corpus like ???/r  ??/ns  ??/t  ??/t  
?/n  ??/n  ?/nr  ??/nr  ?/w  ?/nr  ?
?/nr  ??/v?. In step (2), we can extract tokens 
and tags like ????/ ?r?; ????/ ?ns? and so on. 
When we train person recognition roles, firstly, we 
locate person name ??/nr  ??/nr? and ??/nr  
??/nr? just by searching POS ?nr?; Secondly, 
judge whether they are unknown after looking 
them up in the core lexicon; At last we can tag 
unknown words component and their context near 
their locality. So the final roles after conversion 
are ???/A  ??/A ??/A 1?/A ?/A ??
/K ?/B  ?/C?/D ?/M ?/B ?/C?/D ??
/L?. Then we can train the parameters based on 
new segmentation and person recognition roles 
sequence.  
In addition, we train every different kind of 
unknown word on the same corpus individually. 
That is: person roles, place roles and other roles 
are acquired respectively. Therefore, the unknown 
place recognition roles sequence of the above 
sentence may like ???/K  ?/B?/D  ??/L  
??/A  ?/A  ??/K  ?/A ??/A  ?/A  
?/A ??/A  ??/A?. Such a mechanism can 
greatly reduce the problem of sparse data. 
2.2 The entire process of Unknown words 
recognition 
Input: Original sentence S; 
R: the roles set of unknown words; 
P: pattern sets for recognition. 
Output: Possible unknown words of type T. 
Algorithm: 
(1) Word segmentation (we segment words on 
sentence S with N-shortest paths method 
[Hua-Ping ZHANG, Qun LIU (2002)]); 
(2) Tag tokens sequence with roles in set R using 
Viterbi algorithm. Get the roles sequence R# 
with maximum possibility. 
(3) Split tokens whose role is like ?U? or ?V? in the 
person roles. These roles indicate that the 
internal components glue together with their 
context. 
(4) Maximum match final roles sequence to the 
recognition patterns P and record their 
position. 
(5) Generate the candidate unknown words 
according to the result of pattern matching. 
(6) Exclude those candidates which are fit for the 
exclusive rules.(For example, Chinese person 
name can not include non-Chinese chars. ) 
(7) Output the possible unknown words. 
Now, we take person recognition on the 
sentence ?????????????????
????? as exemplification. In the first place, 
we can get the sequence ???/??/??/??/?
/??/?/?/?/?/?/?/?/??/? after rough 
word segmentation; Then we tag it with Viterbi 
algorithm using person recognition roles lexicon 
and transitive array. So, the most probable roles 
sequence is ?AAAAAKBCDMBCDL?.Therefore, 
candidate perosn names ????? and ????? 
can be recognized after maximum string matching. 
3 Experiments and Discussions 
Both close and open recognition test were 
conducted. In the close test, we tested our system 
within the training corpus, which is the knowledge 
base for recognition. Open test, however, is more 
realistic, because it is performed on arbitrary real 
texts outside the training corpus. The corpus in our 
experiments is from 2-months news in 1998 from 
the People?s Daily. 
In this paper, we only provide the recognition 
results of Chinese person and transliterations. The 
recognition of place names and other kind of 
unknown words can get similar performance. 
3.1 Recognition experiment of Chinese person name  
Test Type Close Open 
Corpus (news date) 1.1-2.20 2.20-2.28
Corpus Size  14,446K 2,605K 
Num of Chinese 
person names 
21,256 3,149 
Num of recognized 
person names 
27,813 4,130 
Num of correctly 20,865 2,886 
recognized names 
Precision rate 75.02% 69.88% 
Recalling rate 98.17% 91.65% 
F-measurement  85.05% 79.30% 
Table 2 Experiment results of Chinese person 
names recognition 
In Tables 2, precision rate and recalling rate are 
defined as equations E6 and E7 respectively. In 
addition, F-measurement is a uniformly weighted 
harmonic mean of precision rate and recalling rate 
as shown in E8. 
Precision rate=  
 wordsrecognized of num
 wordsrecognizedcorrectly  of num
??..E6 
Recalling rate=  
wordsunknown   totalofnum
 wordsrecognizedcorrectly  of num
??..E7 
F-measurement = 
ratePrecision rate Recalling
2ratePrecision rate Recalling
+
??
...?.E8 
3.2 Recognition Experiments of transliterations 
Test Type Close Open 
Corpus (news date) 1.1-2.20 2.20-2.28 
Corpus Size  14,446K 2,605K 
Num of 
transliterations  
9,059 1,592 
Num of recognized 
transliterations 
10,013 1,930 
Num of correctly 
recognized 
transliterations 
8,946 1,496 
Precision rate 89.35% 77.52% 
Recalling rate 98.75% 93.97% 
F-measurement  93.85% 84.96% 
Table 3 Results of transliterations recognition 
3.3 Discussions 
The traditional ways to test unknown words 
recognition is to collect sentences including 
unknown words and to make recognition 
experiments. Those sentences that haven?t the type 
of unknown words will be excluded from 
experiments in the pre-processing. In our 
experiments, we just take the realistic corpus and 
make no filtering. Therefore, the precision rates 
may be lower but closer to the realistic linguistic 
environment than previous tests. We have made 
experiments in the traditional way and the 
precision rate can be improved by less than 15%. 
In a word, there is no comparable with precision 
rates of previous unknown words recognition 
experiment. 
  In addition, our experiments show that the 
unknown words recognition based on role tagging 
can achieve very high recalling rates. For such a 
problem, recalling is more essential than precision. 
Low recalling rate means that we have no chance 
to recognize many unknown words through any 
efforts in the following steps, although words 
recognized are mostly valid; However, precision 
rate can be greatly improved in other processes, 
such as POS tagging or sentence simple parsing. In 
our system ICTCLAS (Institute of Computing 
Technology, Chinese Lexical System), we can 
exclude most invalid unknown words during POS 
tagging. The precision rate of Chinese person 
names recognition can achieve over 95% after 
POS tagging while the recalling rate is not 
reduced. 
  Our approach is purely corpus-based. We all 
know that, in any usual corpus, unknown words 
are sparsely distributed. If we depend totally on the 
corpus, the problem of sparse data is inevitable. 
But in the fine-tuning of our system, we found 
some countermeasures and successfully solved the 
problem. 
Lexical knowledge from linguists can be 
incorporated into the system. This does not mean 
that we fall back to the old ways. We just demand 
for those general rules about name formation to 
avoid apparent mistakes. As to person name 
recognition, there are several strict restrictions, 
such as the length of name, the order between 
surname and given name. 
Except for enlarging the training corpus, we 
provide two more counteractions: 
Firstly, a ?best n? approach [Hua-Ping ZHANG, 
Qun LIU (2002)], which provides n (n>1) possible 
tag sequences with leading probabilities, is feasible. 
Usually the desired tag sequence could be 
re-targeted or constructed from the best n 
sequences. In this way, we improved the recalling 
rate at the cost of precision rate. But given a better 
recalling, we could remedy in latter stages of 
language processing. When 3 most probable 
sequences are employed, the recalling and 
precision of unknown words in ICTCLAS can be 
enhanced obviously. 
The second resolution is training on a name 
library in addition to training on a corpus. As we 
all know, it?s easier and cheaper to get a person 
name library or other special name libraries than to 
segment and tag a corpus. We could extract the 
inner components relations from the unknown 
words libraries, and then merge these data into the 
roles information from the original corpus. When 
the special name libraries were introduced, both 
precision and recalling rates can be improved. 
Conclusion 
The paper presents a one-for-all approach for 
Chinese unknown words recognition based on 
roles tagging. At first, we define roles set for every 
category of unknown words according to the 
function of tokens, such as internal component or 
contexts. Unknown words are recognized on roles 
sequence, tagged with the roles set using Viterbi 
algorithm. The knowledge about roles is extracted 
from the learning on corpus. Experiments on large 
size corpus verify that the approach based on role 
tagging is simple and applicable. 
Acknowledgements 
First of all, our thanks go to the Institute of 
Computational Linguistics, Peking University for 
providing the corpus. And we owe lots of thanks to 
our colleagues: Zougang, Li Jifeng, Li Sujian,Li 
Shengtao, Zhu Hailong, Zhao Hongchao, Wang 
Shuxi and Dr. Zhou Lixin. We would especially 
express gratitude to the chief scientist Bai Shuo. 
References  
K.Y. Liu (1999)  The Assessment to Automatic Word 
Segmentation and POS Tagging Software. 
Proceedings of the 4th Conference on Chinese 
Computer Intelligent Interface and Application, 
Beijing. 
Z. Luo and R. Song (2001)  Integrated and Fast 
Recognition of Proper Noun in Modern Chinese Word 
Segmentation.  Proceedings of International 
Conference on Chinese Computing 2001, Singapore, 
pp. 323-328. 
H. Luo and Z. Ji (2001)  Inverse Name Frequency 
Model and Rules Based on Chinese Name Identifying.  
In "Natural Language Understanding and Machine 
Translation", C. N. Huang & P. Zhang, ed., Tsinghua 
Univ. Press, Beijing, China, pp. 123-128. 
R. Song (1993)  Person Name Recognition Method 
Based on Corpus and Rule.  In ?Computational 
Language Research and Development", L. W. Chen 
& Q. Yuan, ed., Beijing Institute of Linguistic Press. 
H. Y. Tan (1999)  Chinese Place Automatic 
Recognition Research.  In "Proceedings of 
Computational Language ", C. N. Huang & Z.D. 
Dong, ed., Tsinghua Univ. Press, Beijing, China. 
M.S. Sun (1993)  English Transliteration Automatic 
Recognition.  In "Computational Language 
Research and Development", L. W. Chen & Q. Yuan, 
ed., Beijing Institute of Linguistic Press. 
Y.J. Lv, T. J. Zhao (2001)  Levelled Unknown Chinese 
Words Resolution by Dynamic Programming.  
Journal of Chinese Information Processing. 15, 1, pp. 
28-33. 
X. H. Chen (1999)  One-for-all Solution for Unknown 
Word in Chinese Segmentation. Application of 
Language and Character, 3. 
Y. He (2001)  Identification of Unlisted Words on 
Transitive Probability of Monosyllabic Words.  In 
"Natural Language Understanding and Machine 
Translation", C. N. Huang & P. Zhang, ed., Tsinghua 
Univ. Press, Beijing, China, pp. 123-128. 
Hua-Ping  ZHANG, Qun LIU (2002)  Model of 
Chinese Words Rough Segmentation Based on 
N-Shortest-Paths Method.  Journal of Chinese 
Information Processing. 16, 5, pp. 77-83. 
L. R.Rabiner (1989)  A Tutorial on Hidden Markov 
Models and Selected Applications in Speech 
Recognition.  Proceedings of IEEE 77(2): 
pp.257-286. 
L.R. Rabiner and B.H. Juang, (Jun. 1986) An 
Introduction to Hidden Markov Models.  IEEE 
ASSP Mag., Pp.4-166. 
???????????????????3 
???   ? ?   ? ?   ??? 
Email : zhanghp@software.ict.ac.cn 
??????????????? 
?? 2704????????????? 6?, ??  
??: ?????????????????
?????????????????????
?????????????????????
?????????????????????
?? Viterbi??????? Token?????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
????????????????????
????? ICTCLAS????????????
?????????????????????
????????????? 
???:?????????????????
Viterbi?? 
                                                        
1 We define unknown words to be those not included in 
the core lexicon and unable to be generated by FSA, 
such as person name, place name. But numeric or 
common time word is not unknown because they can 
generate by a simple FSA. 
2 Related research in this paper is supported by 
Foundation of National Key Basic Research Project (ID: 
G1998030507-4 and G1998030510). 
3 ??????????????????(???
G1998030507-4?G1998030510)??? 
	



Syntax-Based Alignment: Supervised or Unsupervised?
Hao Zhang and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
Tree-based approaches to alignment model
translation as a sequence of probabilistic op-
erations transforming the syntactic parse tree
of a sentence in one language into that of the
other. The trees may be learned directly from
parallel corpora (Wu, 1997), or provided by a
parser trained on hand-annotated treebanks (Ya-
mada and Knight, 2001). In this paper, we
compare these approaches on Chinese-English
and French-English datasets, and find that au-
tomatically derived trees result in better agree-
ment with human-annotated word-level align-
ments for unseen test data.
1 Introduction
Statistical approaches to machine translation, pio-
neered by Brown et al (1990), estimate parame-
ters for a probabilistic model of word-to-word cor-
respondences and word re-orderings directly from
large corpora of parallel bilingual text. In re-
cent years, a number of syntactically motivated ap-
proaches to statistical machine translation have been
proposed. These approaches assign a parallel tree
structure to the two sides of each sentence pair, and
model the translation process with reordering oper-
ations defined on the tree structure. The tree-based
approach allows us to represent the fact that syn-
tactic constituents tend to move as unit, as well as
systematic differences in word order in the gram-
mars of the two languages. Furthermore, the tree
structure allows us to make probabilistic indepen-
dence assumptions that result in polynomial time
algorithms for estimating a translation model from
parallel training data, and for finding the highest
probability translation given a new sentence.
Wu (1997) modeled the reordering process with
binary branching trees, where each production
could be either in the same or in reverse order going
from source to target language. The trees of Wu?s
Inversion Transduction Grammar were derived by
synchronously parsing a parallel corpus, using a
grammar with lexical translation probabilities at the
leaves and a simple grammar with a single nonter-
minal providing the tree structure. While this gram-
mar did not represent traditional syntactic categories
such as verb phrases and noun phrases, it served
to restrict the word-level alignments considered by
the system to those allowable by reordering opera-
tions on binary trees. This restriction corresponds
to intuitions about the alignments that could be pro-
duced by systematic differences between the two
language?s grammars, and allows for a polynomial
time algorithm for finding the highest-probability
alignment, and for re-estimation of the lexical trans-
lation and grammar probabilities using the Expecta-
tion Maximization algorithm.
Yamada and Knight (2001) present an algorithm
for estimating probabilistic parameters for a simi-
lar model which represents translation as a sequence
of re-ordering operations over children of nodes in
a syntactic tree, using automatic parser output for
the initial tree structures. This gives the translation
model more information about the structure of the
source language, and further constrains the reorder-
ings to match not just a possible bracketing as in Wu
(1997), but the specific bracketing of the parse tree
provided.
In this paper, we make a direct comparison
of a syntactically unsupervised alignment model,
based on Wu (1997), with a syntactically super-
vised model, based on Yamada and Knight (2001).
We use the term syntactically supervised to indicate
that the syntactic structure in one language is given
to the training procedure. It is important to note,
however, that both algorithms are unsupervised in
that they are not provided any hand-aligned train-
ing data. Rather, they both use Expectation Maxi-
mization to find an alignment model by iteratively
improving the likelihood assigned to unaligned par-
allel sentences. Our evaluation is in terms of agree-
ment with word-level alignments created by bilin-
gual human annotators. We describe each of the
models used in more detail in the next two sections,
including the clone operation of Gildea (2003). The
reader who is familiar with these models may pro-
ceed directly to our experiments in Section 4, and
further discussion in Section 5.
2 The Inversion Transduction Grammar
The Inversion Transduction Grammar of Wu (1997)
can be thought as a a generative process which si-
multaneously produces strings in both languages
through a series of synchronous context-free gram-
mar productions. The grammar is restricted to bi-
nary rules, which can have the symbols in the right
hand side appear in the same order in both lan-
guages, represented with square brackets:
X ? [Y Z]
or the symbols may appear in reverse order in the
two languages, indicated by angle brackets:
X ? ?Y Z?
Individual lexical translations between English
words e and French words f take place at the leaves
of the tree, generated by grammar rules with a single
right hand side symbol in each language:
X ? e/f
Given a bilingual sentence pair, a synchronous
parse can be built using a two-dimensional exten-
sion of chart parsing, where chart items are indexed
by their nonterminal Y and beginning and ending
positions l, m in the source language string, and be-
ginning and ending positions i, j in the target lan-
guage string. For Expectation Maximization train-
ing, we compute inside probabilities ?(Y, l, m, i, j)
from the bottom up as outlined below:
for all l,m, n such that 1 ? l < m < n < Ns do
for all i, j, k such that 1 < i < j < k < Nt do
for all rules X ? Y Z ? G do
?(X, l, n, i, k)+=
P ([Y Z]|X)?(Y, l,m, i, j)?(Z,m, n, j, k)
?(X, l, n, i, k)+=
P (?Y Z?|X)?(Y,m, n, i, j)?(Z, l,m, j, k)
end for
end for
end for
A similar recursion is used to compute outside
probabilities for each chart item, and the inside
and outside probabilities are combined to derive ex-
pected counts for occurrence of each grammar rule,
including the rules corresponding to individual lex-
ical translations. In our experiments we use a gram-
mar with a start symbol S, a single preterminal C,
and two nonterminals A and B used to ensure that
only one parse can generate any given word-level
alignment (ignoring insertions and deletions) (Wu,
1997; Zens and Ney, 2003). The individual lexical
translations produced by the grammar may include
a NULL word on either side, in order to represent
insertions and deletions.
3 The Tree-To-String Model
The model of Yamada and Knight (2001) can be
thought of as a generative process taking a tree in
one language as input and producing a string in
the other through a sequence of probabilistic oper-
ations. If we follow the process of an English sen-
tence?s transformation into French, the English sen-
tence is first given a syntactic tree representation by
a statistical parser (Collins, 1999). As the first step
in the translation process, the children of each node
in the tree can be re-ordered. For any node with
m children, m! re-orderings are possible, each of
which is assigned a probability Porder conditioned
on the syntactic categories of the parent node and its
children. As the second step, French words can be
inserted at each node of the parse tree. Insertions are
modeled in two steps, the first predicting whether an
insertion to the left, an insertion to the right, or no
insertion takes place with probability Pins , condi-
tioned on the syntactic category of the node and that
of its parent. The second step is the choice of the in-
serted word Pt(f |NULL), which is predicted with-
out any conditioning information. The final step,
a French translation of each original English word,
at the leaves of the tree, is chosen according to a
distribution Pt(f |e). The French word is predicted
conditioned only on the English word, and each En-
glish word can generate at most one French word,
or can generate a NULL symbol, representing dele-
tion. Given the original tree, the re-ordering, inser-
tion, and translation probabilities at each node are
independent of the choices at any other node. These
independence relations are analogous to those of a
stochastic context-free grammar, and allow for effi-
cient parameter estimation by an inside-outside Ex-
pectation Maximization algorithm. The computa-
tion of inside probabilities ?, outlined below, con-
siders possible reorderings of nodes in the original
tree in a bottom-up manner:
for all nodes ?i in input tree T do
for all k, l such that 1 < k < l < N do
for all orderings ? of the children ?1...?m of ?i
do
for all partitions of span k, l into
k1, l1...km, lm do
?(?i, k, l)+=
Porder (?|?i)
?m
j=1 ?(?j , kj , lj)
end for
end for
end for
end for
As with Inversion Transduction Grammar, many
alignments between source and target sentences are
not allowed. As a minimal example, take the tree:
A
B
X Y
Z
Of the six possible re-orderings of the three ter-
minals, the two which would involve crossing the
bracketing of the original tree (XZY and YZX)
are not allowed. While this constraint gives us a
way of using syntactic information in translation,
it may in many cases be too rigid. In part to deal
with this problem, Yamada and Knight (2001) flat-
ten the trees in a pre-processing step by collapsing
nodes with the same lexical head-word. This allows,
for example, an English subject-verb-object (SVO)
structure, which is analyzed as having a VP node
spanning the verb and object, to be re-ordered as
VSO in a language such as Arabic. Larger syntactic
divergences between the two trees may require fur-
ther relaxation of this constraint, and in practice we
expect such divergences to be frequent. For exam-
ple, a nominal modifier in one language may show
up as an adverbial in the other, or, due to choices
such as which information is represented by a main
verb, the syntactic correspondence between the two
sentences may break down completely. While hav-
ing flatter trees can make more reorderings possible
than with the binary Inversion Transduction Gram-
mar trees, fixing the tree in one language generally
has a much stronger opposite effect, dramatically re-
stricting the number of permissible alignments.
3.1 Tree-to-String With Cloning
In order to provide more flexibility in alignments, a
cloning operation was introduced for tree-to-string
alignment by Gildea (2003). The model is modified
to allow for a copy of a (translated) subtree from the
English sentences to occur, with some cost, at any
point in the resulting French sentence. For example,
in the case of the input tree
A
B
X Y
Z
a clone operation making a copy of node 3 as a new
child of B would produce the tree:
A
B
X Z Y
Z
This operation, combined with the deletion of the
original node Z, produces the alignment (XZY)
that was disallowed by the original tree reordering
model.
The probability of adding a clone of original node
?i as a child of node ?j is calculated in two steps:
first, the choice of whether to insert a clone under
?j , with probability Pins(clone|?j), and the choice
of which original node to copy, with probability
Pclone(?i|clone = 1) =
Pmakeclone(?i)
?
k Pmakeclone(?k)
where Pmakeclone is the probability of an original
node producing a copy. In our implementation,
Pins(clone) is estimated by the Expectation Max-
imization algorithm conditioned on the label of the
parent node ?j , and Pmakeclone is a constant, mean-
ing that the node to be copied is chosen from all the
nodes in the original tree with uniform probability.
4 Experiments
We trained our translation models on a parallel
corpus of Chinese-English newswire text. We re-
stricted ourselves to sentences of no more than 25
words in either language, resulting in a training cor-
pus of 18,773 sentence pairs with a total of 276,113
Chinese words and 315,415 English words. The
Chinese data were automatically segmented into to-
kens, and English capitalization was retained. We
replace words occurring only once with an unknown
word token, resulting in a Chinese vocabulary of
23,783 words and an English vocabulary of 27,075
words. Our hand-aligned data consisted of 48 sen-
tence pairs also with less than 25 words in either
language, for a total of 788 English words and 580
Chinese words. A separate development set of 49
sentence pairs was used to control overfitting. These
sets were the data used by Hwa et al (2002). The
hand aligned test data consisted of 745 individual
aligned word pairs. Words could be aligned one-
to-many in either direction. This limits the perfor-
mance achievable by our models; the IBM models
allow one-to-many alignments in one direction only,
while the tree-based models allow only one-to-one
alignment unless the cloning operation is used.
Our French-English experiments were based on
data from the Canadian Hansards made available by
Ulrich German. We used as training data 20,000
sentence pairs of no more than 25 words in ei-
ther language. Our test data consisted of 447 sen-
tence pairs of no more than 30 words, hand aligned
by Och and Ney (2000). A separate development
set of 37 sentences was used to control overfitting.
We used of vocabulary of words occurring at least
10 times in the entire Hansard corpus, resulting in
19,304 English words and 22,906 French words.
Our test set is that used in the alignment evalua-
tion organized by Mihalcea and Pederson (2003),
though we retained sentence-initial capitalization,
used a closed vocabulary, and restricted ourselves
to a smaller training corpus. We parsed the English
side of the data with the Collins parser. As an ar-
tifact of the parser?s probability model, it outputs
sentence-final punctuation attached at the lowest
level of the tree. We raised sentence-final punctu-
ation to be a daughter of the tree?s root before train-
ing our parse-based model. As our Chinese-English
test data did not include sentence-final punctuation,
we also removed it from our French-English test set.
We evaluate our translation models in terms of
agreement with human-annotated word-level align-
ments between the sentence pairs. For scoring
the viterbi alignments of each system against gold-
standard annotated alignments, we use the align-
ment error rate (AER) of Och and Ney (2000),
which measures agreement at the level of pairs of
words:
AER = 1 ? |A ? GP | + |A ? GS ||A| + |GS |
where A is the set of word pairs aligned by the
automatic system, GS is the set marked in the
gold standard as ?sure?, and GP is the set marked
as ?possible? (including the ?sure? pairs). In our
Chinese-English data, only one type of alignment
was marked, meaning that GP = GS . For a better
understanding of how the models differ, we break
this figure down into precision:
P = |A ? GP ||A|
and recall:
R = |A ? GS ||GS |
Since none of the systems presented in this com-
parison make use of hand-aligned data, they may
differ in the overall proportion of words that are
aligned, rather than inserted or deleted. This affects
the precision/recall tradeoff; better results with re-
spect to human alignments may be possible by ad-
justing an overall insertion probability in order to
optimize AER.
Table 1 provides a comparison of results using the
tree-based models with the word-level IBM models.
IBM Models 1 and 4 refer to Brown et al (1993).
We used the GIZA++ package, including the HMM
model of Och and Ney (2000). We ran Model 1 for
three iterations, then the HMM model for three iter-
ations, and finally Model 4 for two iterations, train-
ing each model until AER began to increase on our
held-out cross validation data. ?Inversion Transduc-
tion Grammar? (ITG) is the model of Wu (1997),
?Tree-to-String? is the model of Yamada and Knight
(2001), and ?Tree-to-String, Clone? allows the node
cloning operation described above. Our tree-based
models were initialized from uniform distributions
for both the lexical translation probabilities and the
tree reordering operations, and were trained until
AER began to rise on our held-out cross-validation
data, which turned out to be four iterations for the
tree-to-string models and three for the Inversion
Transduction Grammar. French-English results are
shown in Table 2. Here, IBM Model 1 was trained
for 12 iterations, then the HMM model for 5 iter-
ations and Model 4 for 5 iterations. The ITG and
tree-to-string models were both trained for 5 itera-
tions. A learning curve for the Inversion Transduc-
tion Grammar, is shown in Figure 1, showing both
perplexity on held-out data and alignment error rate.
In general we found that while all models would in-
crease in AER if trained for too many iterations, the
increases were of only a few percent.
5 Discussion
The Inversion Transduction Grammar significantly
outperforms the syntactically supervised tree-to-
string model of Yamada and Knight (2001). The
tree-to-string and IBM models are roughly equiva-
lent. Adding the cloning operation improves tree-
to-string results by 2% precision and recall. It is
particularly significant that the ITG gets higher re-
call than the other models, when it is the only model
entirely limited to one-to-one alignments, bounding
the maximum recall it can achieve.
Our French-English experiments show only small
differences between the various systems. Overall,
performance on French-English is much better than
for Chinese-English. French-English has less re-
ordering overall, as shown by the percentage of pro-
ductions in the viterbi ITG parses that are inverted:
14% for French-English in comparison to 23% for
Chinese-English.
One possible explanation for our results is parser
error. While we describe our system as ?syntacti-
Alignment
Precision Recall Error Rate
IBM Model 1 .56 .42 .52
IBM Model 4 .67 .43 .47
Inversion Transduction Grammar .68 .52 .40
Tree-to-String w/ Clone .65 .43 .48
Tree-to-String w/o Clone .63 .41 .50
Table 1: Alignment results on Chinese-English corpus. Higher precision and recall correspond to lower
alignment error rate.
Alignment
Precision Recall Error Rate
IBM Model 1 .63 .71 .34
IBM Model 4 .83 .83 .17
Inversion Transduction Grammar .82 .87 .16
Tree-to-String w/ Clone .84 .85 .15
Table 2: French-English results.
cally supervised?, in fact this supervision comes in
the form of the annotation of the Wall Street Journal
treebank on which the parser is trained, rather than
parses for our parallel training corpus. In particular,
the text we are parsing has a different vocabulary
and style of prose from the WSJ treebank, and often
the fluency of the English translations leaves some-
thing to be desired. While both corpora consist of
newswire text, a typical WSJ sentence
Pierre Vinken, 61 years old, will join the
board as a nonexecutive director Nov. 29.
contrasts dramatically with
In the past when education on opposing
Communists and on resisting Russia was
stressed, retaking the mainland and uni-
fying China became a slogan for the au-
thoritarian system, which made the uni-
fication under the martial law a tool for
oppressing the Taiwan people.
a typical sentence from our corpus.
While we did not have human-annotated gold-
standard parses for our training data, we did have
human annotated parses for the Chinese side of our
test data, which was taken from the Penn Chinese
Treebank (Xue et al, 2002). We trained a second
tree-to-string model in the opposite direction, us-
ing Chinese trees and English strings. The Chi-
nese training data was parsed with the Bikel (2002)
parser, and used the Chinese Treebank parses for
our test data. Results are shown in Table 3. Because
the ITG is a symmetric, generative model, the ITG
results in Table 3 are identical to those in Table 1.
While the experiment does not show a significant
improvement, it is possible that better parses for the
training data might be equally important.
Even when the automatic parser output is correct,
the tree structure of the two languages may not cor-
respond. Dorr (1994) categorizes sources of syntac-
tic divergence between languages, and Fox (2002)
analyzed a parallel French-English corpus, quanti-
fying how often parse dependencies cross when pro-
jecting an English tree onto a French string. Even
in this closely related language pair with gener-
ally similar word order, crossed dependencies were
caused by such common occurrences as adverb
modification of a verb, or the correspondence of
?not? to ?ne pas?. Galley et al (2004) extract trans-
lation rules from a large parsed parallel corpus that
extend in scope to tree fragments beyond a single
node; we believe that adding such larger-scale op-
erations to the translation model is likely to signifi-
cantly improve the performance of syntactically su-
pervised alignment.
The syntactically supervised model has been
found to outperform the IBM word-level alignment
models of Brown et al (1993) for translation by
Yamada and Knight (2002). An evaluation for the
alignment task, measuring agreement with human
judges, also found the syntax-based model to out-
perform the IBM models. However, a relatively
small corpus was used to train both models (2121
Japanese-English sentence pairs), and the evalua-
tions were performed on the same data for training,
meaning that one or both models might be signifi-
cantly overfitting.
Zens and Ney (2003) provide a thorough analy-
sis of alignment constraints from the perspective of
decoding algorithms. They train the models of Wu
1 2 3 4 5 6 7 8 9
400
500
600
700
Pe
rp
le
xi
ty
0.4
0.45
0.5
0.55
Iterations
AER
Figure 1: Training curve for ITG model, showing perplexity on cross-validation data, and alignment error
rate on a separate hand-aligned dataset.
Alignment
Precision Recall Error Rate
Inversion Transduction Grammar .68 .52 .40
Tree-to-String, automatic parses .61 .48 .46
Tree-to-String, gold parses .61 .52 .44
Table 3: Chinese Tree to English String
(1997) as well as Brown et al (1993). Decoding,
meaning exact computation of the highest probabil-
ity translation given a foreign sentence, is not pos-
sible in polynomial time for the IBM models, and
in practice decoders search through the space of hy-
pothesis translations using a set of additional, hard
alignment constraints. Zens and Ney (2003) com-
pute the viterbi alignments for German-English and
French-English sentences pairs using IBM Model
5, and then measure how many of the resulting
alignments fall within the hard constraints of both
Wu (1997) and Berger et al (1996). They find
higher coverage for an extended version of ITG than
for the IBM decoding constraint for both language
pairs, with the unmodified ITG implementation cov-
ering about the same amount of German-English
data as IBM, and significantly less French-English
data. These results show promise for ITG as a ba-
sis for efficient decoding, but do not address which
model best aligns the original training data, as IBM-
derived alignments were taken as the gold standard,
rather than human alignments. We believe that our
results show that syntactically-motivated models are
a promising general approach to training translation
models as well to searching through the resulting
probability space.
Computational complexity is an issue for the tree-
based models presented here. While training the
IBM models with the GIZA++ software takes min-
utes, the tree-based EM takes hours. With our C im-
plementation, one iteration of the syntactically su-
pervised model takes 50 CPU hours, which can be
parallelized across machines. Our tree-based mod-
els are estimated with complete EM, while the train-
ing procedure for the IBM models samples from a
number of likely alignments when accumulating ex-
pected counts. Because not every alignment is legal
with the tree-based models, the technique of sam-
pling by choosing likely alignments according to a
simpler model is not straightforward. Nonetheless,
we feel that training times can be improved with the
right pruning and sampling techniques, as will be
necessary to train on the much larger amounts data
now available, and on longer sentences.
6 Conclusion
We present a side-by-side comparison of syntacti-
cally supervised and unsupervised tree-based align-
ment, along with the non tree-based IBM Model 4.
For Chinese-English, using trees helps the align-
ment task, but a data-derived tree structure gives
better results than projecting automatic English
parser output onto the Chinese string. The French-
English task is easier overall, and exhibits smaller
differences between the systems.
Acknowledgments We are very grateful to Re-
becca Hwa for assistance with the Chinese-English
data, and to everyone who helped make the re-
sources we used available to the research commu-
nity. This work was partially supported by NSF ITR
IIS-09325646.
References
Adam Berger, Peter Brown, Stephen Della Pietra,
Vincent Della Pietra, J. R. Fillett, Andrew Kehler,
and Robert Mercer. 1996. Language transla-
tion apparatus and method of using context-
based tanslation models. United States patent
5,510,981.
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In
Proceedings ARPA Workshop on Human Lan-
guage Technology.
Peter F. Brown, John Cocke, Stephen A. Della
Pietra, Vincent J. Della Pietra, Frederick Je-
linek, John D. Lafferty, Robert L. Mercer, and
Paul S. Roossin. 1990. A statistical approach to
machine translation. Computational Linguistics,
16(2):79?85, June.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation:
Parameter estimation. Computational Linguis-
tics, 19(2):263?311.
Michael John Collins. 1999. Head-driven Statisti-
cal Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania, Philadelphia.
Bonnie J. Dorr. 1994. Machine translation diver-
gences: A formal description and proposed solu-
tion. Computational Linguistics, 20(4):597?633.
Heidi J. Fox. 2002. Phrasal cohesion and statisti-
cal machine translation. In In Proceedings of the
2002 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP 2002), pages
304?311.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation
rule? In Proceedings of the Human Language
Technology Conference/North American Chapter
of the Association for Computational Linguistics
(HLT/NAACL).
Daniel Gildea. 2003. Loosely tree-based alignment
for machine translation. In Proceedings of the
41th Annual Conference of the Association for
Computational Linguistics (ACL-03), pages 80?
87, Sapporo, Japan.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and
Okan Kolak. 2002. Evaluating translational cor-
respondence using annotation projection. In Pro-
ceedings of the 40th Annual Conference of the
Association for Computational Linguistics (ACL-
02).
Rada Mihalcea and Ted Pederson. 2003. An eval-
uation exercise for word alignment. In HLT-
NAACL 2003 Workshop on Building and Using
Parallel Texts: Data Driven Machine Translation
and Beyond, pages 1?10, Edmonton, Alberta.
Franz Josef Och and Hermann Ney. 2000. Im-
proved statistical alignment models. In Proceed-
ings of the 38th Annual Conference of the Asso-
ciation for Computational Linguistics (ACL-00),
pages 440?447, Hong Kong, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?403.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese
corpus. In Proceedings of the 19th. International
Conference on Computational Linguistics (COL-
ING 2002), Taipei, Taiwan.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceed-
ings of the 39th Annual Conference of the Asso-
ciation for Computational Linguistics (ACL-01),
Toulouse, France.
Kenji Yamada and Kevin Knight. 2002. A de-
coder for syntax-based statistical MT. In Pro-
ceedings of the 40th Annual Conference of the
Association for Computational Linguistics (ACL-
02), Philadelphia, PA.
Richard Zens and Hermann Ney. 2003. A compar-
ative study on reordering constraints in statistical
machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computa-
tional Linguistics, Sapporo, Japan.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1081?1088
Manchester, August 2008
Extracting Synchronous Grammar Rules
From Word-Level Alignments in Linear Time
Hao Zhang and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627, USA
David Chiang
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292, USA
Abstract
We generalize Uno and Yagiura?s algo-
rithm for finding all common intervals of
two permutations to the setting of two
sequences with many-to-many alignment
links across the two sides. We show how
to maximally decompose a word-aligned
sentence pair in linear time, which can be
used to generate all possible phrase pairs
or a Synchronous Context-Free Grammar
(SCFG) with the simplest rules possible.
We also use the algorithm to precisely
analyze the maximum SCFG rule length
needed to cover hand-aligned data from
various language pairs.
1 Introduction
Many recent syntax-based statistical machine
translation systems fall into the general formalism
of Synchronous Context-Free Grammars (SCFG),
where the grammar rules are found by first align-
ing parallel text at the word level. From word-
level alignments, such systems extract the gram-
mar rules consistent either with the alignments
and parse trees for one of languages (Galley et
al., 2004), or with the the word-level alignments
alone without reference to external syntactic anal-
ysis (Chiang, 2005), which is the scenario we ad-
dress here.
In this paper, we derive an optimal, linear-time
algorithm for the problem of decomposing an ar-
bitrary word-level alignment into SCFG rules such
that each rule has at least one aligned word and is
minimal in the sense that it cannot be further de-
composed into smaller rules. Extracting minimal
rules is of interest both because rules with fewer
words are more likely to generalize to new data,
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
and because rules with lower rank (the number of
nonterminals on the right-hand side) can be parsed
more efficiently.
This algorithm extends previous work on factor-
ing permutations to the general case of factoring
many-to-many alignments. Given two permuta-
tions of n, a common interval is a set of numbers
that are consecutive in both. The breakthrough
algorithm of Uno and Yagiura (2000) computes
all K common intervals of two length n permu-
tations in O(n + K) time. This is achieved by
designing data structures to index possible bound-
aries of common intervals as the computation pro-
ceeds, so that not all possible pairs of beginning
and end points need to be considered. Landau et
al. (2005) and Bui-Xuan et al (2005) show that all
common intervals can be encoded in O(n) space,
and adapt Uno and Yagiura?s algorithm to produce
this compact representation in O(n) time. Zhang
and Gildea (2007) use similar techniques to factor-
ize Synchronous Context Free Grammars in linear
time.
These previous algorithms assume that the input
is a permutation, but in machine translation it is
common to work with word-level alignments that
are many-to-many; in general any set of pairs of
words, one from each language, is a valid align-
ment for a given bilingual sentence pair. In this
paper, we consider a generalized concept of com-
mon intervals given such an alignment: a common
interval is a pair of phrases such that no word pair
in the alignment links a word inside the phrase
to a word outside the phrase. Extraction of such
phrases is a common feature of state-of-the-art
phrase-based and syntax-based machine transla-
tion systems (Och and Ney, 2004a; Chiang, 2005).
We generalize Uno and Yagiura?s algorithm to this
setting, and demonstrate a linear time algorithm
for a pair of aligned sequences. The output is a tree
representation of possible phrases, which directly
provides a set of minimal synchronous grammar
1081
rules for an SCFG-based machine translation sys-
tem. For phrase-based machine translation, one
can also read all phrase pairs consistent with the
original alignment off of the tree in time linear in
the number of such phrases.
2 Alignments and Phrase Pairs
Let [x, y] denote the sequence of integers between
x and y inclusive, and [x, y) the integers between
x and y ? 1 inclusive. An aligned sequence pair
or simply an alignment is a tuple (E,F,A), where
E = e
1
? ? ? e
n
and F = f
1
? ? ? f
m
are strings, and
A is a set of links (x, y), where 1 ? x ? n and
1 ? y ? m, connecting E and F . For most of this
paper, since we are not concerned with the identity
of the symbols in E and F , we will assume for
simplicity that e
i
= i and f
j
= j, so that E =
[1, n] and F = [1,m].
In the context of statistical machine translation
(Brown et al, 1993), we may interpretE as an En-
glish sentence, F its translation in French, and A
a representation of how the words correspond to
each other in the two sentences. A pair of sub-
strings [s, t] ? E and [u, v] ? F is a phrase pair
(Och and Ney, 2004b) if and only if the subset of
links emitted from [s, t] in E is equal to the sub-
set of links emitted from [u, v] in F , and both are
nonempty.
Figure 1a shows an example of a many-to-
many alignment, where E = [1, 6], F =
[1, 7], and A = {(1, 6), (2, 5), (2, 7), (3, 4),
(4, 1), (4, 3), (5, 2), (6, 1), (6, 3)}. The eight
phrase pairs in this alignment are:
([1, 1], [6, 6]), ([1, 2], [5, 7]),
([3, 3], [4, 4]), ([1, 3], [4, 7]),
([5, 5], [2, 2]), ([4, 6], [1, 3]),
([3, 6], [1, 4]), ([1, 6], [1, 7]).
In Figure 1b, we show the alignment matrix rep-
resentation of the given alignment. By default, the
columns correspond to the tokens in E, the rows
correspond to the tokens in F , and the black cells
in the matrix are the alignment links in A. Using
the matrix representation, the phrase pairs can be
viewed as submatrices as shown with the black-
lined boundary boxes. Visually, a submatrix rep-
resents a phrase pair when it contains at least one
alignment link and there are no alignment links di-
rectly above, below, or to the right or left of it.
e
1
e
2
e
3
e
4
e
5
e
6
f
1
f
2
f
3
f
4
f
5
f
6
f
7
1
1
2
2
3
3
4
4
5
5
6
6
7
(a) (b)
Figure 1: An example of (a) a many-to-many
alignment and (b) the same alignment as a matrix,
with its phrase pairs marked.
2.1 Number of Phrase Pairs
In this section, we refine our definition of phrase
pairs with the concept of tightness and give an
asymptotic upper bound on the total number of
such phrase pairs as the two sequences? lengths
grow. In the original definition, the permissive
many-to-many constraint allows for unaligned to-
kens in both sequences E and F . If there is an un-
aligned token adjacent to a phrase pair, then there
is also a phrase pair that includes the unaligned
token. We say that a phrase pair ([s, t], [u, v]) is
tight if none of e
s
, e
t
, f
u
and f
v
is unaligned. By
focusing on tight phrase pairs, we eliminate the
non-tight ones that share the same set of alignment
links with their tight counterpart.
Given [s, t] in E, let l be the first member of
F that any position in [s, t] links to, and let u be
the last. According to the definition of tight phrase
pair, [l, u] is the only candidate phrase in F to pair
up with [s, t] in E. So, the total number of tight
phrase pairs is upper-bounded by the total number
of intervals in each sequence, which is O(n
2
).
If we do not enforce the tightness constraint, the
total number of phrase pairs can grow much faster.
For example, if a sentence contains only a single
alignment link between the midpoint of F and the
midpoint of E, then there will be O(n
2
m
2
) possi-
ble phrase pairs, but only a single tight phrase pair.
From now on, term phrase pair always refers to a
tight phrase pair.
2.2 Hierarchical Decomposition of Phrase
Pairs
In this section, we show how to encode all the tight
phrase pairs of an alignment in a tree of sizeO(n).
Lemma 2.1. When two phrase pairs overlap, the
intersection, the differences, and the union of the
two are also phrase pairs.
The following picture graphically represents the
two possible overlapping structures of two phrase
1082
([1, 6], [1, 7])
([1, 3], [4, 7])
([1, 2], [5, 7])
([1, 1], [6, 6])
([3, 3], [4, 4])
([4, 6], [1, 3])
([5, 5], [2, 2])
Figure 2: The normalized decomposition tree of
the alignment in Figure 1.
pairs: ([s, t], [u, v]) and ([s
?
, t
?
], [u
?
, v
?
]).
s s? t t?
u
u?
v
v?
s s? t t?
u?
u
v?
v
Let AB and BC be two overlapping English
phrases, with B being their overlap. There are six
possible phrases, A, B, C, AB, BC, and ABC,
but if we omit BC, the remainder are nested and
can be represented compactly by ((AB)C), from
which BC can easily be recovered. If we system-
atically apply this to the whole sentence, we obtain
a hierarchical representation of all the phrase pairs,
which we call the normalized decomposition tree.
The normalized decomposition tree for the exam-
ple is shown in Figure 2.
Bui-Xuan et al (2005) show that the family of
common intervals is weakly partitive, i.e. closed
under intersection, difference and union. This al-
lows the family to be represented as a hierarchi-
cal decomposition. The normalized decomposi-
tion focuses on the right strong intervals, those
that do not overlap with any others on the right.
Lemma 2.1 shows that the family of phrase pairs
is also a weakly partitive family and can be hierar-
chically decomposed after normalization. A minor
difference is we prefer left strong intervals since
our algorithms scan F from left to right. Another
difference is that we binarize a linearly-arranged
sequence of non-overlapping phrase pairs instead
of grouping them together.
In the following sections, we show how to pro-
duce the normalized hierarchical analysis of a
given alignment.
3 Shift-Reduce Algorithm
In this section, we present anO(n
2
+m+|A|) algo-
rithm that is similar in spirit to a shift-reduce algo-
rithm for parsing context-free languages. This al-
gorithm is not optimal, but its left-to-right bottom-
up control will form the basis for the improved al-
gorithm in the next section.
First, we can efficiently test whether a span
[x, y] is a phrase as follows. Define a pair of func-
tions l(x, y) and u(x, y) that record the minimum
and maximum, respectively, of the positions on the
French side that are linked to the positions [x, y]:
l(x, y) = min{j | (i, j) ? A, i ? [x, y]}
u(x, y) = max{j | (i, j) ? A, i ? [x, y]}
Note that l(?, y) is monotone increasing and u(?, y)
is monotone decreasing. Define a step of l(?, y)
(or u(?, y)) to be a maximal interval over which
l(?, y) (resp., u(?, y)) is constant. We can compute
u(x, y) in constant time from its value on smaller
spans:
u(x, y) = max{u(x, z), u(z + 1, y)}
and similarly for l(x, y).
We define the following functions to count the
number of links emitted from prefixes of F and E:
F
c
(j) = |{(i
?
, j
?
) ? A | j
?
? j}|
E
c
(i) = |{(i
?
, j
?
) ? A | i
?
? i}|
Then the difference F
c
(u) ? F
c
(l ? 1) counts the
total number of links to positions in [l, u], and
E
c
(y)?E
c
(x?1) counts the total number of links
to positions in [x, y]. E
c
and F
c
can be precom-
puted in O(n + m + |A|) time.
Finally, let
f(x, y) = F
c
(u(x, y))? F
c
(l(x, y)? 1)
? (E
c
(y)? E
c
(x? 1))
Note that f is non-negative, but not monotonic in
general. Figure 4 provides a visualization of u, l,
and f for the example alignment from Section 2.
This gives us our phrase-pair test:
Lemma 3.1. [x, y] and [l(x, y), u(x, y)] are a
phrase pair if and only if f(x, y) = 0.
This test is used in the following shift-reduce-
style algorithm:
X ? {1}
for y ? [2, n] from left to right do
append y to X
for x ? X from right to left do
compute u(x, y) from u(x + 1, y)
compute l(x, y) from l(x + 1, y)
if f(x, y) = 0 then
[x, y] is a phrase
1083
remove [x+ 1, y] from X
end if
end for
end for
In the worst case, at each iteration we traverse
the entire stack X without a successful reduction,
indicating that the worst case time complexity is
O(n
2
).
4 A Linear Algorithm
In this section, we modify the shift-reduce algo-
rithm into a linear-time algorithm that avoids un-
necessary reduction attempts. It is a generalization
of Uno and Yagiura?s algorithm.
4.1 Motivation
The reason that our previous algorithm is quadratic
is that for each y, we try every possible combina-
tion with the values in X . Uno and Yagiura (2000)
point out that in the case of permutations, it is not
necessary to examine all spans, because it is pos-
sible to delete elements from X so that f(?, y) is
monotone decreasing on X . This means that all
the x ? X such that f(x, y) = 0 can always be
conveniently found at the end of X . That this can
be done safely is guaranteed by the following:
Lemma 4.1. If x
1
< x
2
< y and f(x
1
, y) <
f(x
2
, y), then for all y
?
? y, f(x
2
, y
?
) > 0 (i.e.,
[x
2
, y
?
] is not a phrase).
Let us say that x
2
violates monotonicity if x
1
is the predecessor of x
2
in X and f(x
1
, y) <
f(x
2
, y). Then by Lemma 4.1, we can safely re-
move x
2
from X .
Furthermore, Uno and Yagiura (2000) show that
we can enforce monotonicity at all times in such a
way that the whole algorithm runs in linear time.
This is made possible with a shortcut based on the
following:
Lemma 4.2. If x
1
< x
2
< y and u(x
1
, y ? 1) >
u(x
2
, y ? 1) but u(x
1
, y) = u(x
2
, y), then for all
y
?
? y, f(x
2
, y
?
) > 0 (i.e., [x
2
, y
?
] is not a phrase).
The same holds mutatis mutandis for l.
Let us say that y updates a step [x
?
, y
?
] of u (or
l) if u(x
?
, y) > u(x
?
, y ? 1) (resp., l(x
?
, y) <
l(x
?
, y?1)). By Lemma 4.2, if [x
1
, y
1
] and [x
2
, y
2
]
are different steps of u(?, y ? 1) (resp., l(?, y ? 1))
and y updates both of them, then we can remove
from X all x
?
such that x
2
? x
?
< y.
u(?, y ? 1)
l(?, y ? 1)
u(?, y)
l(?, y)
x
?
1
y
?
2
y
x
?
2
y
?
1
Figure 3: Illustration of step (3) of the algorithm.
The letters indicate substeps of (3).
4.2 Generalized algorithm
These results generalize to the many-to-many
alignment case, although we must introduce a few
nuances. The new algorithm proceeds as follows:
Initialize X = {1}. For y ? [2, n] from left to
right:
1. Append y to X .
2. Update u and l:
(a) Traverse the steps of u(?, y ? 1) from
right to left and compute u(?, y) until we
have found the leftmost step [x
?
, y
?
] of
u(?, y ? 1) that gets updated by y.
(b) Do the same for l.
We have computed two values for x
?
; let x
?
1
be the smaller and x
?
2
be the larger. Similarly,
let y
?
1
be the smaller y
?
.
3. Enforce monotonicity of f(?, y) (see Fig-
ure 3):
(a) The positions left of the smaller x
?
al-
ways satisfy monotonicity, so do noth-
ing.
(b) For x ? [x
?
1
, x
?
2
) ? X while x violates
monotonicity, remove x from X .
(c) For x ? [x
?
2
, y
?
1
] ? X while x violates
monotonicity, remove x from X .
(d) The steps right of y
?
1
may or may not
violate monotonicity, but we use the
stronger Lemma 4.2 to delete all of them
(excluding y).
1
1
In the special case where [x
?
, y
?
] is updated by y to the
1084
y = 1 :
1
1
2
2
3
3
4
4
5
5
6
6
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
6
f
x
y = 2 :
1
1
2
2
3
3
4
4
5
5
6
6
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
6
f
x
y = 3 :
1
1
2
2
3
3
4
4
5
5
6
6
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
6
f
x
y = 4 :
1
1
2
2
3
3
4
4
5
5
6
6
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
6
f
x
y = 5 :
1
1
2
2
3
3
4
4
5
5
6
6
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
6
f
x
y = 6 :
1
1
2
2
3
3
4
4
5
5
6
6
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
6
f
x
Figure 4: The evolution of u(x, y) , l(x, y), and f(x, y) as y goes from 1 to 6 for the example alignment.
Each pair of diagrams shows the state of affairs between steps (3) and (4) of the algorithm. Light grey
boxes are the steps of u, and darker grey boxes are the steps of l. We use solid boxes to plot the values
of remaining x?s on the list but also show the other values in empty boxes for completeness.
(e) Finally, if y violates monotonicity, re-
move it from X .
4. For x ? X from right to left until f(x, y) >
0, output [x, y] and remove x?s successor in
X .
2
An example of the algorithm?s execution is
shown in Figure 4. The evolution of u(x, y),
l(x, y), and f(x, y) is displayed for increasing y
(from 1 to 6). We point out the interesting steps.
When y = 2, position 2 is eliminated due to step
(3e) of our algorithm to ensure monotonicity of
f at the right end, and [1, 2] is reduced. When
same value as the step to its left, we can use Lemma 4.2 to
delete [x
?
, y
?
] and y as well, bypassing steps (3b),(3c), and
(3e).
2
If there are any such x, they must lie to the left of x
?
1
.
Therefore a further optimization would be to perform step (4)
before step (3), starting with the predecessor of x
?
1
. If a re-
duction is made, we can jump to step (3e).
y = 3, two reductions are made: one on [3, 3] and
the other on [1, 3]. Because of leftmost normaliza-
tion, position 3 is deleted. When y = 6, we have
x
?
1
= x
?
2
= y
?
1
= 5, so that position 5 is deleted by
step (3c) and position 6 is deleted by step (3e).
4.3 Correctness
We have already argued in Section 4.1 that the
deletion of elements fromX does not alter the out-
put of the algorithm. It remains to show that step
(3) guarantees monotonicity:
Claim 4.3. For all y, at the end of step (3), f(?, y)
is monotone decreasing.
Proof. By induction on y. For y = 1, the claim
is trivially true. For y > 1, we want to show
that for x
1
, x
2
adjacent in X such that x
1
< x
2
,
f(x
1
, y) ? f(x
2
, y). We consider the five regions
of X covered by step (3) (cf. Figure 3), and then
1085
the boundaries between them.
Region (a): x
1
, x
2
? [1, x
?
1
]. Since u(x
i
, y) =
u(x
i
, y ? 1) and l(x
i
, y) = l(x
i
, y ? 1), we have:
f(x
i
, y)?f(x
i
, y?1) = 0? (E
c
(y)?E
c
(y?1))
i.e., in this region, f shifts down uniformly from
iteration y ? 1 to iteration y. Hence, if f(?, y ?
1) was monotonic, then f(?, y) is also monotonic
within this region.
Region (b): x
1
, x
2
? [x
?
1
, x
?
2
). Since u(x
1
, y ?
1) = u(x
2
, y ? 1) and u(x
1
, y) = u(x
2
, y) and
similarly for l, we have:
f(x
1
, y)? f(x
1
, y? 1) = f(x
2
, y)? f(x
2
, y? 1)
i.e., in this region, f shifts up or down uniformly.
3
Hence, if f(?, y ? 1) was monotonic, then f(?, y)
is also monotonic within this region.
Region (c): x
1
, x
2
? [x
?
2
, y
?
1
]. Same as Case 2.
Region (d) and (e): Vacuous (these regions have at
most one element).
The remaining values of x
1
, x
2
are those that
straddle the boundaries between regions. But
step (3) of the algorithm deals with each of
these boundaries explicitly, deleting elements until
f(x
1
) ? f(x
2
). Thus f(?, y) is monotonic every-
where.
4.4 Implementation and running time
X should be implemented in a way that allows
linear-time traversal and constant-time deletion;
also, u and l must be implemented in a way that
allows linear-time traversal of their steps. Doubly-
linked lists are appropriate for all three functions.
Claim 4.4. The above algorithm runs in O(n +
m + |A|) time.
We can see that the algorithm runs in linear time
if we observe that whenever we traverse a part of
X , we delete it, except for a constant amount of
work per iteration (that is, per value of y): the steps
traversed in (2) are all deleted in (3d) except four
(two for u and two for l); the positions traversed in
(3b), (3c), and (4) are all deleted except one.
4.5 SCFG Rule extraction
The algorithm of the previous section outputs the
normalized decomposition tree depicted in Fig-
ure 2. From this tree, it is straightforward to obtain
3
It can be shown further that in this region, f shifts up or
is unchanged. Therefore any reductions in step (4) must be in
region (a).
A? B
(1)
C
(2)
, C
(2)
B
(1)
B ? D
(1)
E
(2)
, E
(2)
D
(1)
D ? G
(1)
e
2
, f
5
G
(1)
f
6
G? e
1
, f
6
E ? e
3
, f
4
C ? e
4
F
(1)
e
6
, f
1
F
(1)
f
3
F ? e
5
, f
2
Figure 5: Each node from the normalized decom-
position tree of Figure 2 is converted into an SCFG
rule.
a set of maximally-decomposed SCFG rules. As
an example, the tree of Figure 2 produces the rules
shown in Figure 5.
We adopt the SCFG notation of Satta and Pe-
serico (2005). Each rule has a right-hand side se-
quence for both languages, separated by a comma.
Superscript indices in the right-hand side of gram-
mar rules such as:
A? B
(1)
C
(2)
, C
(2)
B
(1)
indicate that the nonterminals with the same index
are linked across the two languages, and will even-
tually be rewritten by the same rule application.
The example above inverts the order of B and C
when translating from the source language to the
target language.
The SCFG rule extraction proceeds as follows.
Assign a nonterminal label to each node in the tree.
Then for each node (S, T ) in the tree top-down,
where S and T are sequences of positions,
1. For each child (S
?
, T
?
), S
?
and T
?
must be
subsequences of S and T , respectively. Re-
place their occurrences in S and T with a pair
of coindexed nonterminals X
?
, where X
?
is
the nonterminal assigned to the child.
2. For each remaining position i in S, replace i
with e
i
.
3. For each remaining position j in T , replace j
with f
j
.
4. Output the rule X ? S, T , where X is the
nonterminal assigned to the parent.
As an example, consider the node ([4, 6], [1, 3])
in Figure 2. After step 1, it becomes
(4F
(1)
6, 1F
(1)
3)
and after steps 2 and 3, it becomes
(e
4
F
(1)
e
6
, f
1
F
(1)
f
3
)
1086
0 1 2 3 4 5 6
Hindi/English 52.8 53.5 99.9 99.9 100.0
Chinese/English 51.0 52.4 99.7 99.8 100.0 100.0 100.0
French/English 52.1 53.5 99.9 100.0 100.0 100.0
Romanian/English 50.8 52.6 99.9 99.9 100.0 100.0
Spanish/English 50.7 51.8 99.9 100.0 100.0 100.0
Table 1: Cumulative percentages of rule tokens by number of nonterminals in right-hand side. A blank
indicates that no rules were found with that number of nonterminals.
Finally, step 4 outputs
C ? e
4
F
(1)
e
6
, f
1
F
(1)
f
3
A few choices are available to the user depend-
ing on the application intended for the SCFG ex-
traction. The above algorithm starts by assigning
a nonterminal to each node in the decomposition
tree; one could assign a unique nonterminal to each
node, so that the resulting grammar produces ex-
actly the set of sentences given as input. But for
machine translation, one may wish to use a single
nonterminal, such that the extracted rules can re-
combine freely, as in Chiang (2005).
Unaligned words in either language (an empty
row or column in the alignment matrix, not present
in our example) will be attached as high as possi-
ble in our tree. However, other ways of handling
unaligned words are possible given the decompo-
sition tree. One can produce all SCFG rules con-
sistent with the alignment by, for each unaligned
word, looping through possible attachment points
in the decomposition tree. In this case, the num-
ber of SCFG rules produced may be exponential
in the size of the original input sentence; however,
even in this case, the decomposition tree enables a
rule extraction algorithm that is linear in the output
length (the number of SCFG rules).
4.6 Phrase extraction
We briefly discuss the process of extracting all
phrase pairs consistent with the original alignment
from the normalized decomposition tree. First of
all, every node in the tree gives a valid phrase
pair. Then, in the case of overlapping phrase pairs
such as the example in Section 2.1, the decom-
position tree will contain a left-branching chain
of binary nodes all performing the same permuta-
tion. While traversing the tree, whenever we iden-
tify such a chain, let ?
1
, . . . , ?
k
be the sequence of
all the children of the nodes in the chain. Then,
each of the subsequences {?
i
, . . . , ?
j
| 1 < i <
j ? k} yields a valid phrase pair. In our exam-
ple, the root of the tree of Figure 2 and its left
child form such a chain, with three children; the
subsequence {([3, 3], [4, 4]), ([4, 6], [1, 3])} yields
the phrase ([3, 6], [1, 4]). In the case of unaligned
words, we can also consider all combinations of
their attachments, as discussed for SCFG rule ex-
traction.
5 Experiments on Analyzing Word
Alignments
One application of our factorization algorithm
is analyzing human-annotated word alignments.
Wellington et al (2006) argue for the necessity
of discontinuous spans (i.e., for a formalism be-
yond Synchronous CFG) in order for synchronous
parsing to cover human-annotated word alignment
data under the constraint that rules have a rank
of no more than two. In a related study, Zhang
and Gildea (2007) analyze the rank of the Syn-
chronous CFG derivation trees needed to parse the
same data. The number of discontinuous spans
and the rank determine the complexity of dynamic
programming algorithms for synchronous parsing
(alignment) or machine translation decoding.
Both studies make simplifying assumptions on
the alignment data to avoid dealing with many-to-
many word links. Here, we apply our alignment
factorization algorithm directly to the alignments
to produce a normalized decomposition tree for
each alignment and collect statistics on the branch-
ing factors of the trees.
We use the same alignment data for the
five language pairs Chinese-English, Romanian-
English, Hindi-English, Spanish-English, and
French-English as Wellington et al (2006). Ta-
ble 1 reports the number of rules extracted by the
rank, or number of nonterminals on the right-hand
side. Almost all rules are binary, implying both
that binary synchronous grammars are adequate
for MT, and that our algorithm can find such gram-
mars. Table 2 gives similar statistics for the num-
ber of terminals in each rule. The phrases we ex-
tract are short enough that they are likely to gener-
alize to new sentences. The apparent difficulty of
1087
0 1 2 3 4 5 6 7 8 9 ?10 max
Hindi/English 39.6 92.2 97.7 99.5 99.7 99.9 99.9 100.0 7
Chinese/English 39.8 87.2 96.2 99.0 99.7 99.9 100.0 100.0 100.0 100.0 100.0 12
French/English 44.5 89.0 93.4 95.8 97.5 98.4 99.0 99.3 99.6 99.8 100.0 18
Romanian/English 42.9 89.8 96.9 98.9 99.5 99.8 99.9 100.0 100.0 9
Spanish/English 47.5 91.8 97.7 99.4 99.9 99.9 100.0 100.0 100.0 9
Table 2: Cumulative percentages of rule tokens by number of terminals in right-hand side. A blank
indicates that no rules were found with that number of terminals.
the French-English pair is due to the large number
of ?possible? alignments in this dataset.
6 Conclusion
By extending the algorithm of Uno and Yagiura
(2000) from one-to-one mappings to many-to-
many mappings, we have shown how to construct a
hierarchical representation of all the phrase pairs in
a given aligned sentence pair in linear time, which
yields a set of minimal SCFG rules. We have also
illustrated how to apply the algorithm as an analyt-
ical tool for aligned bilingual data.
Acknowledgments Thanks to Bob Moore for
suggesting the extension to phrase extraction at
SSST 2007. This work was supported in part
by NSF grants IIS-0546554 and ITR-0428020,
and DARPA grant HR0011-06-C-0022 under BBN
Technologies subcontract 9500008412.
References
Brown, Peter F., Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The math-
ematics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Bui-Xuan, Binh Minh, Michel Habib, and Christophe
Paul. 2005. Revisiting T. Uno and M. Yagiura?s al-
gorithm. In The 16th Annual International Sympo-
sium on Algorithms and Computation (ISAAC ?05),
pages 146?155.
Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL 2005, pages 263?270.
Galley, Michel, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of NAACL 2004.
Landau, Gad M., Laxmi Parida, and Oren Weimann.
2005. Gene proximity analysis across whole
genomes via PQ trees. Journal of Computational Bi-
ology, 12(10):1289?1306.
Och, Franz Josef and Hermann Ney. 2004a. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4).
Och, Franz Josef and Hermann Ney. 2004b. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30:417?449.
Satta, Giorgio and Enoch Peserico. 2005. Some
computational complexity results for synchronous
context-free grammars. In Proceedings of EMNLP
2005, pages 803?810, Vancouver, Canada, October.
Uno, Takeaki and Mutsunori Yagiura. 2000. Fast al-
gorithms to enumerate all common intervals of two
permutations. Algorithmica, 26(2):290?309.
Wellington, Benjamin, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the
complexity of translational equivalence. In Proceed-
ings of COLING-ACL 2006.
Zhang, Hao and Daniel Gildea. 2007. Factorization
of synchronous context-free grammars in linear time.
In Proceedings of the NAACL Workshop on Syntax
and Structure in Statistical Translation (SSST).
1088
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 256?263,
New York, June 2006. c?2006 Association for Computational Linguistics
Synchronous Binarization for Machine Translation
Hao Zhang
Computer Science Department
University of Rochester
Rochester, NY 14627
zhanghao@cs.rochester.edu
Liang Huang
Dept. of Computer & Information Science
University of Pennsylvania
Philadelphia, PA 19104
lhuang3@cis.upenn.edu
Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
gildea@cs.rochester.edu
Kevin Knight
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
knight@isi.edu
Abstract
Systems based on synchronous grammars
and tree transducers promise to improve
the quality of statistical machine transla-
tion output, but are often very computa-
tionally intensive. The complexity is ex-
ponential in the size of individual gram-
mar rules due to arbitrary re-orderings be-
tween the two languages, and rules ex-
tracted from parallel corpora can be quite
large. We devise a linear-time algorithm
for factoring syntactic re-orderings by bi-
narizing synchronous rules when possible
and show that the resulting rule set signif-
icantly improves the speed and accuracy
of a state-of-the-art syntax-based machine
translation system.
1 Introduction
Several recent syntax-based models for machine
translation (Chiang, 2005; Galley et al, 2004) can
be seen as instances of the general framework of
synchronous grammars and tree transducers. In this
framework, both alignment (synchronous parsing)
and decoding can be thought of as parsing problems,
whose complexity is in general exponential in the
number of nonterminals on the right hand side of a
grammar rule. To alleviate this problem, we investi-
gate bilingual binarization to factor the synchronous
grammar to a smaller branching factor, although it is
not guaranteed to be successful for any synchronous
rule with arbitrary permutation. In particular:
? We develop a technique called synchronous bi-
narization and devise a fast binarization algo-
rithm such that the resulting rule set alows ef-
ficient algorithms for both synchronous parsing
and decoding with integrated n-gram language
models.
? We examine the effect of this binarization
method on end-to-end machine translation
quality, compared to a more typical baseline
method.
? We examine cases of non-binarizable rules in a
large, empirically-derived rule set, and we in-
vestigate the effect on translation quality when
excluding such rules.
Melamed (2003) discusses binarization of multi-
text grammars on a theoretical level, showing the
importance and difficulty of binarization for efficient
synchronous parsing. One way around this diffi-
culty is to stipulate that all rules must be binary
from the outset, as in inversion-transduction gram-
mar (ITG) (Wu, 1997) and the binary synchronous
context-free grammar (SCFG) employed by the Hi-
ero system (Chiang, 2005) to model the hierarchical
phrases. In contrast, the rule extraction method of
Galley et al (2004) aims to incorporate more syn-
tactic information by providing parse trees for the
target language and extracting tree transducer rules
that apply to the parses. This approach results in
rules with many nonterminals, making good bina-
rization techniques critical.
Suppose we have the following SCFG, where su-
perscripts indicate reorderings (formal definitions of
256
S
NP
Baoweier
PP
yu
Shalong
VP
juxing le
huitan
S
NP
Powell
VP
held
a meeting
PP
with
Sharon
Figure 1: A pair of synchronous parse trees in the
SCFG (1). The dashed curves indicate pairs of syn-
chronous nonterminals (and sub trees).
SCFGs can be found in Section 2):
(1)
S? NP(1) VP(2) PP(3), NP(1) PP(3) VP(2)
NP? Powell, Baoweier
VP? held a meeting, juxing le huitan
PP? with Sharon, yu Shalong
Decoding can be cast as a (monolingual) parsing
problem since we only need to parse the source-
language side of the SCFG, as if we were construct-
ing a CFG projected on Chinese out of the SCFG.
The only extra work we need to do for decoding
is to build corresponding target-language (English)
subtrees in parallel. In other words, we build syn-
chronous trees when parsing the source-language in-
put, as shown in Figure 1.
To efficiently decode with CKY, we need to bi-
narize the projected CFG grammar.1 Rules can be
binarized in different ways. For example, we could
binarize the first rule left to right or right to left:
S? VNP-PP VP
VNP-PP? NP PP or
S? NP VPP-VP
VPP-VP ? PP VP
We call those intermediate symbols (e.g. VPP-VP) vir-
tual nonterminals and corresponding rules virtual
rules, whose probabilities are all set to 1.
These two binarizations are no different in the
translation-model-only decoding described above,
just as in monolingual parsing. However, in the
source-channel approach to machine translation, we
need to combine probabilities from the translation
model (an SCFG) with the language model (an n-
gram), which has been shown to be very impor-
tant for translation quality (Chiang, 2005). To do
bigram-integrated decoding, we need to augment
each chart item (X, i, j) with two target-language
1Other parsing strategies like the Earley algorithm use an
internal binary representation (e.g. dotted-rules) of the original
grammar to ensure cubic time complexity.
boundary words u and v to produce a bigram-item
like
( u ??? vX
i j
)
, following the dynamic program-
ming algorithm of Wu (1996).
Now the two binarizations have very different ef-
fects. In the first case, we first combine NP with PP:
( Powell ??? PowellNP
1 2
)
: p
( with ??? SharonPP
2 4
)
: q
( Powell ??? Powell ??? with ??? Sharon
VNP-PP
1 4
)
: pq
where p and q are the scores of antecedent items.
This situation is unpleasant because in the target-
language NP and PP are not contiguous so we can-
not apply language model scoring when we build the
VNP-PP item. Instead, we have to maintain all fourboundary words (rather than two) and postpone the
language model scoring till the next step where VNP-PP
is combined with ( held ??? meetingVP
2 4
) to form an S item.
We call this binarization method monolingual bina-
rization since it works only on the source-language
projection of the rule without respecting the con-
straints from the other side.
This scheme generalizes to the case where we
have n nonterminals in a SCFG rule, and the decoder
conservatively assumes nothing can be done on lan-
guage model scoring (because target-language spans
are non-contiguous in general) until the real nonter-
minal has been recognized. In other words, target-
language boundary words from each child nonter-
minal of the rule will be cached in all virtual non-
terminals derived from this rule. In the case of
m-gram integrated decoding, we have to maintain
2(m ? 1) boundary words for each child nontermi-
nal, which leads to a prohibitive overall complex-
ity of O(|w|3+2n(m?1)), which is exponential in rule
size (Huang et al, 2005). Aggressive pruning must
be used to make it tractable in practice, which in
general introduces many search errors and adversely
affects translation quality.
In the second case, however:
( with ??? SharonPP
2 4
)
: r
( held ??? meetingVP
4 7
)
: s
( held ??? Sharon
VPP-VP
2 7
)
: rs ? Pr(with | meeting)
Here since PP and VP are contiguous (but
swapped) in the target-language, we can include the
257
NP
NP
PP
VP
VP
PP
target (English)
source (Chinese)
VPP-VP
NP
PP
VP
Chinese indices
English
boundary
w
o
rds 1 2 4 7Powell
Powellheld
meetingwith
Sharon
VPP-VP
Figure 2: The alignment pattern (left) and alignment
matrix (right) of the synchronous production.
language model score by adding Pr(with | meeting),
and the resulting item again has two boundary
words. Later we add Pr(held | Powell) when the
resulting item is combined with ( Powell ??? PowellNP
1 2
) to
form an S item. As illustrated in Figure 2, VPP-VP hascontiguous spans on both source and target sides, so
that we can generate a binary-branching SCFG:
(2) S? NP(1) VPP-VP(2), NP(1) VPP-VP(2)VPP-VP ? VP(1) PP(2), PP(2) VP(1)
In this case m-gram integrated decoding can be
done in O(|w|3+4(m?1)) time which is much lower-
order polynomial and no longer depends on rule size
(Wu, 1996), allowing the search to be much faster
and more accurate facing pruning, as is evidenced in
the Hiero system of Chiang (2005) where he restricts
the hierarchical phrases to be a binary SCFG. The
benefit of binary grammars also lies in synchronous
parsing (alignment). Wu (1997) shows that parsing
a binary SCFG is in O(|w|6) while parsing SCFG is
NP-hard in general (Satta and Peserico, 2005).
The same reasoning applies to tree transducer
rules. Suppose we have the following tree-to-string
rules, following Galley et al (2004):
(3)
S(x0:NP, VP(x2:VP, x1:PP))? x0 x1 x2NP(NNP(Powell))? Baoweier
VP(VBD(held), NP(DT(a) NPS(meeting)))
? juxing le huitan
PP(TO(with), NP(NNP(Sharon)))? yu Shalong
where the reorderings of nonterminals are denoted
by variables xi.Notice that the first rule has a multi-level left-
hand side subtree. This system can model non-
isomorphic transformations on English parse trees
to ?fit? another language, for example, learning that
the (S (V O)) structure in English should be trans-
formed into a (V S O) structure in Arabic, by look-
ing at two-level tree fragments (Knight and Graehl,
2005). From a synchronous rewriting point of view,
this is more akin to synchronous tree substitution
grammar (STSG) (Eisner, 2003). This larger locality
is linguistically motivated and leads to a better pa-
rameter estimation. By imagining the left-hand-side
trees as special nonterminals, we can virtually cre-
ate an SCFG with the same generative capacity. The
technical details will be explained in Section 3.2.
In general, if we are given an arbitrary syn-
chronous rule with many nonterminals, what are the
good decompositions that lead to a binary grammar?
Figure 2 suggests that a binarization is good if ev-
ery virtual nonterminal has contiguous spans on both
sides. We formalize this idea in the next section.
2 Synchronous Binarization
A synchronous CFG (SCFG) is a context-free
rewriting system for generating string pairs. Each
rule (synchronous production) rewrites a nontermi-
nal in two dimensions subject to the constraint that
the sequence of nonterminal children on one side is
a permutation of the nonterminal sequence on the
other side. Each co-indexed child nonterminal pair
will be further rewritten as a unit.2 We define the
language L(G) produced by an SCFG G as the pairs
of terminal strings produced by rewriting exhaus-
tively from the start symbol.
As shown in Section 3.2, terminals do not play
an important role in binarization. So we now write
rules in the following notation:
X ? X(1)1 ...X(n)n , X
(pi(1))
pi(1) ...X
(pi(n))
pi(n)
where each Xi is a variable which ranges over non-terminals in the grammar and pi is the permutation
of the rule. We also define an SCFG rule as n-ary
if its permutation is of n and call an SCFG n-ary if
its longest rule is n-ary. Our goal is to produce an
equivalent binary SCFG for an input n-ary SCFG.
2In making one nonterminal play dual roles, we follow the
definitions in (Aho and Ullman, 1972; Chiang, 2005), origi-
nally known as Syntax Directed Translation Schema (SDTS).
An alternative definition by Satta and Peserico (2005) allows
co-indexed nonterminals taking different symbols in two di-
mensions. Formally speaking, we can construct an equivalent
SDTS by creating a cross-product of nonterminals from two
sides. See (Satta and Peserico, 2005, Sec. 4) for other details.
258
(2,3,5,4)
(2,3)
2 3
(5,4)
5 4
(2,3,5,4)
2 (3,5,4)
3 (5,4)
5 4
(a) (b) (c)
Figure 3: (a) and (b): two binarization patterns
for (2, 3, 5, 4). (c): alignment matrix for the non-
binarizable permuted sequence (2, 4, 1, 3)
However, not every SCFG can be binarized. In
fact, the binarizability of an n-ary rule is determined
by the structure of its permutation, which can some-
times be resistant to factorization (Aho and Ullman,
1972). So we now start to rigorously define the bi-
narizability of permutations.
2.1 Binarizable Permutations
A permuted sequence is a permutation of consec-
utive integers. For example, (3, 5, 4) is a permuted
sequence while (2, 5) is not. As special cases, single
numbers are permuted sequences as well.
A sequence a is said to be binarizable if it is a
permuted sequence and either
1. a is a singleton, i.e. a = (a), or
2. a can be split into two sub sequences, i.e.
a = (b; c), where b and c are both binarizable
permuted sequences. We call such a division
(b; c) a binarizable split of a.
This is a recursive definition. Each binarizable
permuted sequence has at least one hierarchical bi-
narization pattern. For instance, the permuted se-
quence (2, 3, 5, 4) is binarizable (with two possible
binarization patterns) while (2, 4, 1, 3) is not (see
Figure 3).
2.2 Binarizable SCFG
An SCFG is said to be binarizable if the permu-
tation of each synchronous production is binariz-
able. We denote the class of binarizable SCFGs as
bSCFG. This set represents an important subclass
of SCFG that is easy to handle (parsable in O(|w|6))
and covers many interesting longer-than-two rules.3
3Although we factor the SCFG rules individually and de-
fine bSCFG accordingly, there are some grammars (the dashed
SCFG bSCFG SCFG-2
O(|w|6) parsable
Figure 4: Subclasses of SCFG. The thick arrow de-
notes the direction of synchronous binarization. For
clarity reasons, binary SCFG is coded as SCFG-2.
Theorem 1. For each grammar G in bSCFG, there
exists a binary SCFG G?, such that L(G?) = L(G).
Proof. Once we decompose the permutation of n
in the original rule into binary permutations, all
that remains is to decorate the skeleton binary parse
with nonterminal symbols and attach terminals to
the skeleton appropriately. We explain the technical
details in the next section.
3 Binarization Algorithms
We have reduced the problem of binarizing an SCFG
rule into the problem of binarizing its permutation.
This problem can be cast as an instance of syn-
chronous ITG parsing (Wu, 1997). Here the parallel
string pair that we are parsing is the integer sequence
(1...n) and its permutation (pi(1)...pi(n)). The goal
of the ITG parsing is to find a synchronous tree that
agrees with the alignment indicated by the permu-
tation. In fact, as demonstrated previously, some
permutations may have more than one binarization
patterns among which we only need one. Wu (1997,
Sec. 7) introduces a non-ambiguous ITG that prefers
left-heavy binary trees so that for each permutation
there is a unique synchronous derivation (binariza-
tion pattern).
However, this problem has more efficient solu-
tions. Shapiro and Stephens (1991, p. 277) infor-
mally present an iterative procedure where in each
pass it scans the permuted sequence from left to right
and combines two adjacent sub sequences whenever
possible. This procedure produces a left-heavy bi-
narization tree consistent with the unambiguous ITG
and runs in O(n2) time since we need n passes in the
worst case. We modify this procedure and improve
circle in Figure 4), which can be binarized only by analyzing
interactions between rules. Below is a simple example:
S? X(1) X(2) X(3) X(4), X(2) X(4) X(1) X(3)
X? a , a
259
iteration stack input action
1 5 3 4 2
1 5 3 4 2 shift
1 1 5 3 4 2 shift
2 1 5 3 4 2 shift
3 1 5 3 4 2 shift
1 5 3-4 2 reduce [3, 4]
1 3-5 2 reduce ?5, [3, 4]?
4 1 3-5 2 shift
1 2-5 reduce ?2, ?5, [3, 4]??
1-5 reduce [1, ?2, ?5, [3, 4]??]
Figure 5: Example of Algorithm 1 on the input
(1, 5, 3, 4, 2). The rightmost column shows the
binarization-trees generated at each reduction step.
it into a linear-time shift-reduce algorithm that only
needs one pass through the sequence.
3.1 The linear-time skeleton algorithm
The (unique) binarization tree bi(a) for a binariz-
able permuted sequence a is recursively defined as
follows:
? if a = (a), then bi(a) = a;
? otherwise let a = (b; c) to be the rightmost
binarizable split of a. then
bi(a) =
{
[bi(b), bi(c)] b1 < c1
?bi(b), bi(c)? b1 > c1.
For example, the binarization tree for (2, 3, 5, 4)
is [[2, 3], ?5, 4?], which corresponds to the binariza-
tion pattern in Figure 3(a). We use [] and ?? for
straight and inverted combinations respectively, fol-
lowing the ITG notation (Wu, 1997). The rightmost
split ensures left-heavy binary trees.
The skeleton binarization algorithm is an instance
of the widely used left-to-right shift-reduce algo-
rithm. It maintains a stack for contiguous subse-
quences discovered so far, like 2-5, 1. In each it-
eration, it shifts the next number from the input and
repeatedly tries to reduce the top two elements on
the stack if they are consecutive. See Algorithm 1
for details and Figure 5 for an example.
Theorem 2. Algorithm 1 succeeds if and only if the
input permuted sequence a is binarizable, and in
case of success, the binarization pattern recovered
is the binarization tree of a.
Proof. ?: it is obvious that if the algorithm suc-
ceeds then a is binarizable using the binarization
pattern recovered.
?: by a complete induction on n, the length of a.
Base case: n = 1, trivial.
Assume it holds for all n? < n.
If a is binarizable, then let a = (b; c) be its right-
most binarizable split. By the induction hypothesis,
the algorithm succeeds on the partial input b, reduc-
ing it to the single element s[0] on the stack and re-
covering its binarization tree bi(b).
Let c = (c1; c2). If c1 is binarizable and trig-gers our binarizer to make a straight combination
of (b; c1), based on the property of permutations, itmust be true that (c1; c2) is a valid straight concate-nation. We claim that c2 must be binarizable in thissituation. So, (b, c1; c2) is a binarizable split to theright of the rightmost binarizable split (b; c), which
is a contradiction. A similar contradiction will arise
if b and c1 can make an inverted concatenation.
Therefore, the algorithm will scan through the
whole c as if from the empty stack. By the in-
duction hypothesis again, it will reduce c into s[1]
on the stack and recover its binarization tree bi(c).
Since b and c are combinable, the algorithm re-
duces s[0] and s[1] in the last step, forming the bi-
narization tree for a, which is either [bi(b), bi(c)] or
?bi(b), bi(c)?.
The running time of Algorithm 1 is linear in n, the
length of the input sequence. This is because there
are exactly n shifts and at most n?1 reductions, and
each shift or reduction takes O(1) time.
3.2 Binarizing tree-to-string transducers
Without loss of generality, we have discussed how
to binarize synchronous productions involving only
nonterminals through binarizing the corresponding
skeleton permutations. We still need to tackle a few
technical problems in the actual system.
First, we are dealing with tree-to-string trans-
ducer rules. We view each left-hand side subtree
as a monolithic nonterminal symbol and factor each
transducer rule into two SCFG rules: one from
the root nonterminal to the subtree, and the other
from the subtree to the leaves. In this way we can
uniquely reconstruct the tree-to-string derivation us-
ing the two-step SCFG derivation. For example,
260
Algorithm 1 The Linear-time Binarization Algorithm
1: function BINARIZABLE(a)
2: top? 0 . stack top pointer
3: PUSH(a1, a1) . initial shift4: for i? 2 to |a| do . for each remaining element
5: PUSH(ai, ai) . shift6: while top > 1 and CONSECUTIVE(s[top], s[top? 1]) do . keep reducing if possible
7: (p, q)? COMBINE(s[top], s[top? 1])
8: top? top? 2
9: PUSH(p, q)
10: return (top = 1) . if reduced to a single element then the input is binarizable, otherwise not
11: function CONSECUTIVE((a, b), (c, d))
12: return (b = c? 1) or (d = a? 1) . either straight or inverted
13: function COMBINE((a, b), (c, d))
14: return (min(a, c), max(b, d))
consider the following tree-to-string rule:
ADJP
x0:RB JJ
responsible
PP
IN
for
NP-C
NPB
DT
the
x2:NN
x1:PP
? x0 fuze x1 de x2
We create a specific nonterminal, say, T859, whichis a unique identifier for the left-hand side subtree
and generate the following two SCFG rules:
ADJP ? T859 (1), T859 (1)
T859 ? RB
(1) resp. for the NN(2) PP(3),
RB(1) fuze PP(3) de NN(2)
Second, besides synchronous nonterminals, ter-
minals in the two languages can also be present, as
in the above example. It turns out we can attach the
terminals to the skeleton parse for the synchronous
nonterminal strings quite freely as long as we can
uniquely reconstruct the original rule from its binary
parse tree. In order to do so we need to keep track of
sub-alignments including both aligned nonterminals
and neighboring terminals.
When binarizing the second rule above, we first
run the skeleton algorithm to binarize the under-
lying permutation (1, 3, 2) to its binarization tree
[1, ?3, 2?]. Then we do a post-order traversal to the
skeleton tree, combining Chinese terminals (one at
a time) at the leaf nodes and merging English termi-
nals greedily at internal nodes:
[1, ?3, 2?]
1 ?3, 2?
3 2
?
T859 [1,?3,2?]
V[RB, fuze]1
RB fuze
V?V[PP, de], resp. for the NN??3,2?
V[PP, de]3
PP de
NN2
A pre-order traversal of the decorated binarization
tree gives us the following binary SCFG rules:
T859 ? V1(1) V2(2), V1(1) V2(2)
V1 ? RB(1), RB(1) fuze
V2 ? resp. for the NN(1) V(2)3 , V(2)3 NN(1)V3 ? PP(1), PP(1) de
where the virtual nonterminals are:
V1: V[RB, fuze]V2: V?V[PP, de], resp. for the NN?V3: V[PP, de]
Analogous to the ?dotted rules? in Earley pars-
ing for monolingual CFGs, the names we create
for the virtual nonterminals reflect the underlying
sub-alignments, ensuring intermediate states can be
shared across different tree-to-string rules without
causing ambiguity.
The whole binarization algorithm still runs in time
linear in the number of symbols in the rule (includ-
ing both terminals and nonterminals).
4 Experiments
In this section, we answer two empirical questions.
261
 0
 2e+06
 4e+06
 6e+06
 8e+06
 1e+07
 0  5  10  15  20  25  30  35  40
 0
 20
 40
 60
 80
 100
# 
of
 ru
le
s
pe
rc
en
ta
ge
 (%
)
length
Figure 6: The solid-line curve represents the distribution of all rules against permutation lengths. The
dashed-line stairs indicate the percentage of non-binarizable rules in our initial rule set while the dotted-line
denotes that percentage among all permutations.
4.1 How many rules are binarizable?
It has been shown by Shapiro and Stephens (1991)
and Wu (1997, Sec. 4) that the percentage of binariz-
able cases over all permutations of length n quickly
approaches 0 as n grows (see Figure 6). However,
for machine translation, it is more meaningful to
compute the ratio of binarizable rules extracted from
real text. Our rule set is obtained by first doing word
alignment using GIZA++ on a Chinese-English par-
allel corpus containing 50 million words in English,
then parsing the English sentences using a variant
of Collins parser, and finally extracting rules using
the graph-theoretic algorithm of Galley et al (2004).
We did a ?spectrum analysis? on the resulting rule
set with 50,879,242 rules. Figure 6 shows how the
rules are distributed against their lengths (number
of nonterminals). We can see that the percentage
of non-binarizable rules in each bucket of the same
length does not exceed 25%. Overall, 99.7% of
the rules are binarizable. Even for the 0.3% non-
binarizable rules, human evaluations show that the
majority of them are due to alignment errors. It is
also interesting to know that 86.8% of the rules have
monotonic permutations, i.e. either taking identical
or totally inverted order.
4.2 Does synchronous binarizer help decoding?
We did experiments on our CKY-based decoder with
two binarization methods. It is the responsibility of
the binarizer to instruct the decoder how to compute
the language model scores from children nontermi-
nals in each rule. The baseline method is mono-
lingual left-to-right binarization. As shown in Sec-
tion 1, decoding complexity with this method is ex-
ponential in the size of the longest rule and since we
postpone all the language model scorings, pruning
in this case is also biased.
system bleu
monolingual binarization 36.25
synchronous binarization 38.44
alignment-template system 37.00
Table 1: Syntax-based systems vs. ATS
To move on to synchronous binarization, we first
did an experiment using the above baseline system
without the 0.3% non-binarizable rules and did not
observe any difference in BLEU scores. So we
safely move a step further, focusing on the binariz-
able rules only.
The decoder now works on the binary translation
rules supplied by an external synchronous binarizer.
As shown in Section 1, this results in a simplified de-
coder with a polynomial time complexity, allowing
less aggressive and more effective pruning based on
both translation model and language model scores.
We compare the two binarization schemes in
terms of translation quality with various pruning
thresholds. The rule set is that of the previous sec-
tion. The test set has 116 Chinese sentences of no
longer than 15 words. Both systems use trigram as
the integrated language model. Figure 7 demon-
strates that decoding accuracy is significantly im-
proved after synchronous binarization. The number
of edges proposed during decoding is used as a mea-
sure of the size of search space, or time efficiency.
Our system is consistently faster and more accurate
than the baseline system.
We also compare the top result of our syn-
chronous binarization system with the state-of-the-
art alignment-template approach (ATS) (Och and
Ney, 2004). The results are shown in Table 1. Our
system has a promising improvement over the ATS
262
 33.5
 34.5
 35.5
 36.5
 37.5
 38.5
 3e+09  4e+09  5e+09  6e+09  7e+09
bl
eu
 s
co
re
s
# of edges proposed during decoding
synchronous binarization
monolingual binarization
Figure 7: Comparing the two binarization methods
in terms of translation quality against search effort.
system which is trained on a larger data-set but tuned
independently.
5 Conclusion
Modeling reorderings between languages has been a
major challenge for machine translation. This work
shows that the majority of syntactic reorderings, at
least between languages like English and Chinese,
can be efficiently decomposed into hierarchical bi-
nary reorderings. From a modeling perspective, on
the other hand, it is beneficial to start with a richer
representation that has more transformational power
than ITG or binary SCFG. Our work shows how to
convert it back to a computationally friendly form
without harming much of its expressiveness. As a
result, decoding with n-gram models can be fast and
accurate, making it possible for our syntax-based
system to overtake a comparable phrase-based sys-
tem in BLEU score. We believe that extensions of
our technique to more powerful models such as syn-
chronous tree-adjoining grammar (Shieber and Sch-
abes, 1990) is an interesting area for further work.
Acknowledgments Much of this work was done
when H. Zhang and L. Huang were visiting
USC/ISI. The authors wish to thank Wei Wang,
Jonathan Graehl and Steven DeNeefe for help with
the experiments. We are also grateful to Daniel
Marcu, Giorgio Satta, and Aravind Joshi for discus-
sions. This work was partially supported by NSF
ITR IIS-09325646 and NSF ITR IIS-0428020.
References
Albert V. Aho and Jeffery D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling, volume 1.
Prentice-Hall, Englewood Cliffs, NJ.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL-05, pages 263?270, Ann Arbor, Michigan.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of ACL-
03, companion volume, Sapporo, Japan.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT/NAACL-04.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005. Ma-
chine translation as lexicalized parsing with hooks. In
Proceedings of IWPT-05, Vancouver, BC.
Kevin Knight and Jonathan Graehl. 2005. An overview
of probabilistic tree transducers for natural language
processing. In Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLing). LNCS.
I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of NAACL-03, Ed-
monton.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4).
Giorgio Satta and Enoch Peserico. 2005. Some computa-
tional complexity results for synchronous context-free
grammars. In Proceedings of HLT/EMNLP-05, pages
803?810, Vancouver, Canada, October.
L. Shapiro and A. B. Stephens. 1991. Bootstrap percola-
tion, the Schro?der numbers, and the n-kings problem.
SIAM Journal on Discrete Mathematics, 4(2):275?
280.
Stuart Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In COLING-90, volume III,
pages 253?258.
Dekai Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In 34th Annual Meeting
of the Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
263
Proceedings of the 43rd Annual Meeting of the ACL, pages 475?482,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Stochastic Lexicalized Inversion Transduction Grammar for Alignment
Hao Zhang and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
We present a version of Inversion Trans-
duction Grammar where rule probabili-
ties are lexicalized throughout the syn-
chronous parse tree, along with pruning
techniques for efficient training. Align-
ment results improve over unlexicalized
ITG on short sentences for which full EM
is feasible, but pruning seems to have a
negative impact on longer sentences.
1 Introduction
The Inversion Transduction Grammar (ITG) of Wu
(1997) is a syntactically motivated algorithm for
producing word-level alignments of pairs of transla-
tionally equivalent sentences in two languages. The
algorithm builds a synchronous parse tree for both
sentences, and assumes that the trees have the same
underlying structure but that the ordering of con-
stituents may differ in the two languages.
This probabilistic, syntax-based approach has in-
spired much subsequent reasearch. Alshawi et
al. (2000) use hierarchical finite-state transducers.
In the tree-to-string model of Yamada and Knight
(2001), a parse tree for one sentence of a transla-
tion pair is projected onto the other string. Melamed
(2003) presents algorithms for synchronous parsing
with more complex grammars, discussing how to
parse grammars with greater than binary branching
and lexicalization of synchronous grammars.
Despite being one of the earliest probabilistic
syntax-based translation models, ITG remains state-
of-the art. Zens and Ney (2003) found that the con-
straints of ITG were a better match to the decod-
ing task than the heuristics used in the IBM decoder
of Berger et al (1996). Zhang and Gildea (2004)
found ITG to outperform the tree-to-string model for
word-level alignment, as measured against human
gold-standard alignments. One explanation for this
result is that, while a tree representation is helpful
for modeling translation, the trees assigned by the
traditional monolingual parsers (and the treebanks
on which they are trained) may not be optimal for
translation of a specific language pair. ITG has the
advantage of being entirely data-driven ? the trees
are derived from an expectation maximization pro-
cedure given only the original strings as input.
In this paper, we extend ITG to condition the
grammar production probabilities on lexical infor-
mation throughout the tree. This model is reminis-
cent of lexicalization as used in modern statistical
parsers, in that a unique head word is chosen for
each constituent in the tree. It differs in that the
head words are chosen through EM rather than de-
terministic rules. This approach is designed to retain
the purely data-driven character of ITG, while giving
the model more information to work with. By condi-
tioning on lexical information, we expect the model
to be able capture the same systematic differences in
languages? grammars that motive the tree-to-string
model, for example, SVO vs. SOV word order or
prepositions vs. postpositions, but to be able to do
so in a more fine-grained manner. The interaction
between lexical information and word order also ex-
plains the higher performance of IBM model 4 over
IBM model 3 for alignment.
We begin by presenting the probability model in
the following section, detailing how we address is-
sues of pruning and smoothing that lexicalization in-
troduces. We present alignment results on a parallel
Chinese-English corpus in Section 3.
475
2 Lexicalization of Inversion Transduction
Grammars
An Inversion Transduction Grammar can generate
pairs of sentences in two languages by recursively
applying context-free bilingual production rules.
Most work on ITG has focused on the 2-normal
form, which consists of unary production rules that
are responsible for generating word pairs:
X ? e/f
and binary production rules in two forms that are
responsible for generating syntactic subtree pairs:
X ? [Y Z]
and
X ? ?Y Z?
The rules with square brackets enclosing the right
hand side expand the left hand side symbol into the
two symbols on the right hand side in the same order
in the two languages, whereas the rules with pointed
brackets expand the left hand side symbol into the
two right hand side symbols in reverse order in the
two languages.
One special case of ITG is the bracketing ITG that
has only one nonterminal that instantiates exactly
one straight rule and one inverted rule. The ITG we
apply in our experiments has more structural labels
than the primitive bracketing grammar: it has a start
symbol S, a single preterminal C, and two interme-
diate nonterminals A and B used to ensure that only
one parse can generate any given word-level align-
ment, as discussed by Wu (1997) and Zens and Ney
(2003).
As an example, Figure 1 shows the alignment and
the corresponding parse tree for the sentence pair Je
les vois / I see them using the unambiguous bracket-
ing ITG.
A stochastic ITG can be thought of as a stochastic
CFG extended to the space of bitext. The indepen-
dence assumptions typifying S-CFGs are also valid
for S-ITGs. Therefore, the probability of an S-ITG
parse is calculated as the product of the probabili-
ties of all the instances of rules in the parse tree. For
instance, the probability of the parse in Figure 1 is:
P (S ? A) ? P (A ? [CB])
? P (B ? ?CC?) ? P (C ? I/Je)
? P (C ? see/vois) ? P (C ? them/les)
It is important to note that besides the bottom-
level word-pairing rules, the other rules are all non-
lexical, which means the structural alignment com-
ponent of the model is not sensitive to the lexical
contents of subtrees. Although the ITG model can
effectively restrict the space of alignment to make
polynomial time parsing algorithms possible, the
preference for inverted or straight rules only pas-
sively reflect the need of bottom level word align-
ment. We are interested in investigating how much
help it would be if we strengthen the structural align-
ment component by making the orientation choices
dependent on the real lexical pairs that are passed up
from the bottom.
The first step of lexicalization is to associate a lex-
ical pair with each nonterminal. The head word pair
generation rules are designed for this purpose:
X ? X(e/f)
The word pair e/f is representative of the lexical
content of X in the two languages.
For binary rules, the mechanism of head selection
is introduced. Now there are 4 forms of binary rules:
X(e/f) ? [Y (e/f)Z]
X(e/f) ? [Y Z(e/f)]
X(e/f) ? ?Y (e/f)Z?
X(e/f) ? ?Y Z(e/f)?
determined by the four possible combinations of
head selections (Y or Z) and orientation selections
(straight or inverted).
The rules for generating lexical pairs at the leaves
of the tree are now predetermined:
X(e/f) ? e/f
Putting them all together, we are able to derive a
lexicalized bilingual parse tree for a given sentence
pair. In Figure 2, the example in Figure 1 is revisited.
The probability of the lexicalized parse is:
P (S ? S(see/vois))
? P (S(see/vois) ? A(see/vois))
? P (A(see/vois) ? [CB(see/vois)])
? P (C ? C(I/Je))
476
Isee
them
Je les vois
C
B
C
A
see/vois them/les
I/Je
S
C
Figure 1: ITG Example
I
see
them
Je les vois
S(see/vois)
C(see/vois)C(I/Je)
C
S
C(them/les)
C
B(see/vois)
A(see/vois)
Figure 2: Lexicalized ITG Example. see/vois is the headword of both the 2x2 cell and the entire alignment.
? P (B(see/vois) ? ?C(see/vois)C?)
? P (C ? C(them/les))
The factors of the product are ordered to show
the generative process of the most probable parse.
Starting from the start symbol S, we first choose
the head word pair for S, which is see/vois in the
example. Then, we recursively expand the lexical-
ized head constituents using the lexicalized struc-
tural rules. Since we are only lexicalizing rather than
bilexicalizing the rules, the non-head constituents
need to be lexicalized using head generation rules
so that the top-down generation process can proceed
in all branches. By doing so, word pairs can appear
at all levels of the final parse tree in contrast with the
unlexicalized parse tree in which the word pairs are
generated only at the bottom.
The binary rules are lexicalized rather than bilexi-
calized.1 This is a trade-off between complexity and
expressiveness. After our lexicalization, the number
of lexical rules, thus the number of parameters in the
statistical model, is still at the order of O(|V ||T |),
where |V | and |T | are the vocabulary sizes of the
1In a sense our rules are bilexicalized in that they condition
on words from both languages; however they do not capture
head-modifier relations within a language.
two languages.
2.1 Parsing
Given a bilingual sentence pair, a synchronous parse
can be built using a two-dimensional extension of
chart parsing, where chart items are indexed by their
nonterminal X , head word pair e/f if specified, be-
ginning and ending positions l,m in the source lan-
guage string, and beginning and ending positions i, j
in the target language string. For Expectation Max-
imization training, we compute lexicalized inside
probabilities ?(X(e/f), l,m, i, j), as well as un-
lexicalized inside probabilities ?(X, l,m, i, j), from
the bottom up as outlined in Algorithm 1.
The algorithm has a complexity of O(N4sN4t ),
where Ns and Nt are the lengths of source and tar-
get sentences respectively. The complexity of pars-
ing for an unlexicalized ITG is O(N3sN3t ). Lexical-
ization introduces an additional factor of O(NsNt),
caused by the choice of headwords e and f in the
pseudocode.
Assuming that the lengths of the source and target
sentences are proportional, the algorithm has a com-
plexity of O(n8), where n is the average length of
the source and target sentences.
477
Algorithm 1 LexicalizedITG(s, t)
for all l,m such that 0 ? l ? m ? Ns do
for all i, j such that 0 ? i ? j ? Nt do
for all e ? {el+1 . . . em} do
for all f ? {fi+1 . . . fj} do
for all n such that l ? n ? m do
for all k such that i ? k ? j do
for all rules X ? Y Z ? G do
?(X(e/f), l,m, i, j) +=
 straight rule, where Y is head
P ([Y (e/f)Z] | X(e/f)) ??(Y (e/f), l, n, i, k) ? ?(Z, n,m, k, j)
 inverted rule, where Y is head
+ P (?Y (e/f)Z? | X(e/f)) ??(Y (e/f), n,m, i, k) ? ?(Z, l, n, k, j)
 straight rule, where Z is head
+ P ([Y Z(e/f)] | X(e/f)) ??(Y, l, n, i, k) ? ?(Z(e/f), n,m, k, j)
 inverted rule, where Z is head
+ P (?Y Z(e/f)? | X(e/f)) ??(Y, n,m, i, k) ? ?(Z(e/f), l, n, k, j)
end for
end for
end for
 word pair generation rule
?(X, l,m, i, j) += P (X(e/f) | X) ??(X(e/f), l,m, i, j)
end for
end for
end for
end for
2.2 Pruning
We need to further restrict the space of alignments
spanned by the source and target strings to make the
algorithm feasible. Our technique involves comput-
ing an estimate of how likely each of the n4 cells in
the chart is before considering all ways of building
the cell by combining smaller subcells. Our figure
of merit for a cell involves an estimate of both the
inside probability of the cell (how likely the words
within the box in both dimensions are to align) and
the outside probability (how likely the words out-
side the box in both dimensions are to align). In
including an estimate of the outside probability, our
technique is related to A* methods for monolingual
parsing (Klein and Manning, 2003), although our
estimate is not guaranteed to be lower than com-
plete outside probabity assigned by ITG. Figure 3(a)
displays the tic-tac-toe pattern for the inside and
outside components of a particular cell. We use
IBM Model 1 as our estimate of both the inside and
outside probabilities. In the Model 1 estimate of
the outside probability, source and target words can
align using any combination of points from the four
outside corners of the tic-tac-toe pattern. Thus in
Figure 3(a), there is one solid cell (corresponding
to the Model 1 Viterbi alignment) in each column,
falling either in the upper or lower outside shaded
corner. This can be also be thought of as squeezing
together the four outside corners, creating a new cell
whose probability is estimated using IBM Model
1. Mathematically, our figure of merit for the cell
(l,m, i, j) is a product of the inside Model 1 proba-
bility and the outside Model 1 probability:
P (f (i,j) | e(l,m)) ? P (f(i,j) | e(l,m)) (1)
= ?|(l,m)|,|(i,j)|
?
t?(i,j)
?
s?{0,(l,m)}
t(ft | es)
? ?|(l,m)|,|(i,j)|
?
t?(i,j)
?
s?{0,(l,m)}
t(ft | es)
478
lm
i j i j
l
m
i j(a) (b) (c)
Figure 3: The tic-tac-toe figure of merit used for pruning bitext cells. The shaded regions in (a) show
alignments included in the figure of merit for bitext cell (l,m, i, j) (Equation 1); solid black cells show the
Model 1 Viterbi alignment within the shaded area. (b) shows how to compute the inside probability of a
unit-width cell by combining basic cells (Equation 2), and (c) shows how to compute the inside probability
of any cell by combining unit-width cells (Equation 3).
where (l,m) and (i, j) represent the complementary
spans in the two languages. ?L1,L2 is the probability
of any word alignment template for a pair of L1-
word source string and L2-word target string, which
we model as a uniform distribution of word-for-
word alignment patterns after a Poisson distribution
of target string?s possible lengths, following Brown
et al (1993). As an alternative, the ? operator can
be replaced by the max operator as the inside opera-
tor over the translation probabilities above, meaning
that we use the Model 1 Viterbi probability as our
estimate, rather than the total Model 1 probability.2
A na??ve implementation would take O(n6) steps
of computation, because there are O(n4) cells, each
of which takes O(n2) steps to compute its Model 1
probability. Fortunately, we can exploit the recur-
sive nature of the cells. Let INS(l,m, i, j) denote
the major factor of our Model 1 estimate of a cell?s
inside probability,
?
t?(i,j)
?
s?{0,(l,m)} t(ft | es). It
turns out that one can compute cells of width one
(i = j) in constant time from a cell of equal width
and lower height:
INS(l,m, j, j) =
?
t?(j,j)
?
s?{0,(l,m)}
t(ft | es)
=
?
s?{0,(l,m)}
t(fj | es)
= INS(l,m? 1, j, j)
+ t(fj | em) (2)
Similarly, one can compute cells of width greater
than one by combining a cell of one smaller width
2The experimental difference of the two alternatives was
small. For our results, we used the max version.
with a cell of width one:
INS(l,m, i, j) =
?
t?(i,j)
?
s?{0,(l,m)}
t(ft | es)
=
?
t?(i,j)
INS(l,m, t, t)
= INS(l,m, i, j ? 1)
? INS(l,m, j, j) (3)
Figure 3(b) and (c) illustrate the inductive compu-
tation indicated by the two equations. Each of the
O(n4) inductive steps takes one additive or mul-
tiplicative computation. A similar dynammic pro-
graming technique can be used to efficiently com-
pute the outside component of the figure of merit.
Hence, the algorithm takes just O(n4) steps to com-
pute the figure of merit for all cells in the chart.
Once the cells have been scored, there can be
many ways of pruning. In our experiments, we ap-
plied beam ratio pruning to each individual bucket of
cells sharing a common source substring. We prune
cells whose probability is lower than a fixed ratio be-
low the best cell for the same source substring. As a
result, at least one cell will be kept for each source
substring. We safely pruned more than 70% of cells
using 10?5 as the beam ratio for sentences up to 25
words. Note that this pruning technique is applica-
ble to both the lexicalized ITG and the conventional
ITG.
In addition to pruning based on the figure of merit
described above, we use top-k pruning to limit the
number of hypotheses retained for each cell. This
is necessary for lexicalized ITG because the number
of distinct hypotheses in the two-dimensional ITG
479
chart has increased to O(N3sN3t ) from O(N2sN2t )
due to the choice one of O(Ns) source language
words and one of O(Nt) target language words as
the head. We keep only the top-k lexicalized items
for a given chart cell of a certain nonterminal Y con-
tained in the cell l,m, i, j. Thus the additional com-
plexity of O(NsNt) will be replaced by a constant
factor.
The two pruning techniques can work for both the
computation of expected counts during the training
process and for the Viterbi-style algorithm for ex-
tracting the most probable parse after training. How-
ever, if we initialize EM from a uniform distribution,
all probabilties are equal on the first iteration, giving
us no basis to make pruning decisions. So, in our
experiments, we initialize the head generation prob-
abilities of the form P (X(e/f) | X) to be the same
as P (e/f | C) from the result of the unlexicalized
ITG training.
2.3 Smoothing
Even though we have controlled the number of pa-
rameters of the model to be at the magnitude of
O(|V ||T |), the problem of data sparseness still ren-
ders a smoothing method necessary. We use back-
ing off smoothing as the solution. The probabilities
of the unary head generation rules are in the form of
P (X(e/f) | X). We simply back them off to the
uniform distribution. The probabilities of the binary
rules, which are conditioned on lexicalized nonter-
minals, however, need to be backed off to the prob-
abilities of generalized rules in the following forms:
P ([Y (?)Z] | X(?))
P ([Y Z(?)] | X(?))
P (?Y (?)Z? | X(?))
P (?Y Z(?)? | X(?))
where ? stands for any lexical pair. For instance,
P ([Y (e/f)Z] | X(e/f)) =
(1 ? ?)PEM ([Y (e/f)Z] | X(e/f))
+ ?P ([Y (?)Z] | X(?))
where
? = 1/(1 + Expected Counts(X(e/f)))
The more often X(e/f) occurred, the more reli-
able are the estimated conditional probabilities with
the condition part being X(e/f).
3 Experiments
We trained both the unlexicalized and the lexical-
ized ITGs on a parallel corpus of Chinese-English
newswire text. The Chinese data were automati-
cally segmented into tokens, and English capitaliza-
tion was retained. We replaced words occurring only
once with an unknown word token, resulting in a
Chinese vocabulary of 23,783 words and an English
vocabulary of 27,075 words.
In the first experiment, we restricted ourselves to
sentences of no more than 15 words in either lan-
guage, resulting in a training corpus of 6,984 sen-
tence pairs with a total of 66,681 Chinese words and
74,651 English words. In this experiment, we didn?t
apply the pruning techniques for the lexicalized ITG.
In the second experiment, we enabled the pruning
techniques for the LITG with the beam ratio for the
tic-tac-toe pruning as 10?5 and the number k for the
top-k pruning as 25. We ran the experiments on sen-
tences up to 25 words long in both languages. The
resulting training corpus had 18,773 sentence pairs
with a total of 276,113 Chinese words and 315,415
English words.
We evaluate our translation models in terms of
agreement with human-annotated word-level align-
ments between the sentence pairs. For scoring the
Viterbi alignments of each system against gold-
standard annotated alignments, we use the alignment
error rate (AER) of Och and Ney (2000), which mea-
sures agreement at the level of pairs of words:
AER = 1 ? |A ?GP | + |A ?GS ||A| + |GS |
where A is the set of word pairs aligned by the
automatic system, GS is the set marked in the
gold standard as ?sure?, and GP is the set marked
as ?possible? (including the ?sure? pairs). In our
Chinese-English data, only one type of alignment
was marked, meaning that GP = GS .
In our hand-aligned data, 20 sentence pairs are
less than or equal to 15 words in both languages,
and were used as the test set for the first experiment,
and 47 sentence pairs are no longer than 25 words in
either language and were used to evaluate the pruned
480
Alignment
Precision Recall Error Rate
IBM Model 1 .59 .37 .54
IBM Model 4 .63 .43 .49
ITG .62 .47 .46
Lexicalized ITG .66 .50 .43
Table 1: Alignment results on Chinese-English corpus (? 15 words on both sides). Full ITG vs. Full LITG
Alignment
Precision Recall Error Rate
IBM Model 1 .56 .42 .52
IBM Model 4 .67 .43 .47
ITG .68 .52 .40
Lexicalized ITG .69 .51 .41
Table 2: Alignment results on Chinese-English corpus (? 25 words on both sides). Full ITG vs. Pruned
LITG
LITG against the unlexicalized ITG.
A separate development set of hand-aligned sen-
tence pairs was used to control overfitting. The sub-
set of up to 15 words in both languages was used for
cross-validating in the first experiment. The subset
of up to 25 words in both languages was used for the
same purpose in the second experiment.
Table 1 compares results using the full (unpruned)
model of unlexicalized ITG with the full model of
lexicalized ITG.
The two models were initialized from uniform
distributions for all rules and were trained until AER
began to rise on our held-out cross-validation data,
which turned out to be 4 iterations for ITG and 3
iterations for LITG.
The results from the second experiment are shown
in Table 2. The performance of the full model of un-
lexicalized ITG is compared with the pruned model
of lexicalized ITG using more training data and eval-
uation data.
Under the same check condition, we trained ITG
for 3 iterations and the pruned LITG for 1 iteration.
For comparison, we also included the results from
IBM Model 1 and Model 4. The numbers of itera-
tions for the training of the IBM models were cho-
sen to be the turning points of AER changing on the
cross-validation data.
4 Discussion
As shown by the numbers in Table 1, the full lexical-
ized model produced promising alignment results on
sentence pairs that have no more than 15 words on
both sides. However, due to its prohibitive O(n8)
computational complexity, our C++ implementation
of the unpruned lexicalized model took more than
500 CPU hours, which were distributed over multi-
ple machines, to finish one iteration of training. The
number of CPU hours would increase to a point that
is unacceptable if we doubled the average sentence
length. Some type of pruning is a must-have. Our
pruned version of LITG controlled the running time
for one iteration to be less than 1200 CPU hours, de-
spite the fact that both the number of sentences and
the average length of sentences were more than dou-
bled. To verify the safety of the tic-tac-toe pruning
technique, we applied it to the unlexicalized ITG us-
ing the same beam ratio (10?5) and found that the
AER on the test data was not changed. However,
whether or not the top-k lexical head pruning tech-
nique is equally safe remains a question. One no-
ticeable implication of this technique for training is
the reliance on initial probabilities of lexical pairs
that are discriminative enough. The comparison of
results for ITG and LITG in Table 2 and the fact that
AER began to rise after only one iteration of train-
ing seem to indicate that keeping few distinct lex-
ical heads caused convergence on a suboptimal set
481
of parameters, leading to a form of overfitting. In
contrast, overfitting did not seem to be a problem for
LITG in the unpruned experiment of Table 1, despite
the much larger number of parameters for LITG than
for ITG and the smaller training set.
We also want to point out that for a pair of long
sentences, it would be hard to reflect the inherent
bilingual syntactic structure using the lexicalized bi-
nary bracketing parse tree. In Figure 2, A(see/vois)
echoes IP (see/vois) and B(see/vois) echoes
V P (see/vois) so that it means IP (see/vois) is not
inverted from English to French but its right child
V P (see/vois) is inverted. However, for longer sen-
tences with more than 5 levels of bracketing and the
same lexicalized nonterminal repeatedly appearing
at different levels, the correspondences would be-
come less linguistically plausible. We think the lim-
itations of the bracketing grammar are another rea-
son for not being able to improve the AER of longer
sentence pairs after lexicalization.
The space of alignments that is to be considered
by LITG is exactly the space considered by ITG
since the structural rules shared by them define the
alignment space. The lexicalized ITG is designed
to be more sensitive to the lexical influence on the
choices of inversions so that it can find better align-
ments. Wu (1997) demonstrated that for pairs of
sentences that are less than 16 words, the ITG align-
ment space has a good coverage over all possibili-
ties. Hence, it?s reasonable to see a better chance
of improving the alignment result for sentences less
than 16 words.
5 Conclusion
We presented the formal description of a Stochastic
Lexicalized Inversion Transduction Grammar with
its EM training procedure, and proposed specially
designed pruning and smoothing techniques. The
experiments on a parallel corpus of Chinese and En-
glish showed that lexicalization helped for aligning
sentences of up to 15 words on both sides. The prun-
ing and the limitations of the bracketing grammar
may be the reasons that the result on sentences of up
to 25 words on both sides is not better than that of
the unlexicalized ITG.
Acknowledgments We are very grateful to Re-
becca Hwa for assistance with the Chinese-English
data, to Kevin Knight and Daniel Marcu for their
feedback, and to the authors of GIZA. This work
was partially supported by NSF ITR IIS-09325646
and NSF ITR IIS-0428020.
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computa-
tional Linguistics, 26(1):45?60.
Adam Berger, Peter Brown, Stephen Della Pietra, Vin-
cent Della Pietra, J. R. Fillett, Andrew Kehler, and
Robert Mercer. 1996. Language translation apparatus
and method of using context-based tanslation models.
United States patent 5,510,981.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Dan Klein and Christopher D. Manning. 2003. A* pars-
ing: Fast exact viterbi parse selection. In Proceed-
ings of the 2003 Meeting of the North American chap-
ter of the Association for Computational Linguistics
(NAACL-03).
I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of the 2003 Meeting
of the North American chapter of the Association for
Computational Linguistics (NAACL-03), Edmonton.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Conference of the Association for Compu-
tational Linguistics (ACL-00), pages 440?447, Hong
Kong, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Conference of the Association for Com-
putational Linguistics (ACL-01), Toulouse, France.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
Sapporo, Japan.
Hao Zhang and Daniel Gildea. 2004. Syntax-based
alignment: Supervised or unsupervised? In Proceed-
ings of the 20th International Conference on Compu-
tational Linguistics (COLING-04), Geneva, Switzer-
land, August.
482
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 279?286,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Factoring Synchronous Grammars By Sorting
Daniel Gildea
Computer Science Dept.
University of Rochester
Rochester, NY 14627
Giorgio Satta
Dept. of Information Eng?g
University of Padua
I-35131 Padua, Italy
Hao Zhang
Computer Science Dept.
University of Rochester
Rochester, NY 14627
Abstract
Synchronous Context-Free Grammars
(SCFGs) have been successfully exploited
as translation models in machine trans-
lation applications. When parsing with
an SCFG, computational complexity
grows exponentially with the length of the
rules, in the worst case. In this paper we
examine the problem of factorizing each
rule of an input SCFG to a generatively
equivalent set of rules, each having the
smallest possible length. Our algorithm
works in time O(n log n), for each rule
of length n. This improves upon previous
results and solves an open problem about
recognizing permutations that can be
factored.
1 Introduction
Synchronous Context-Free Grammars (SCFGs)
are a generalization of the Context-Free Gram-
mar (CFG) formalism to simultaneously produce
strings in two languages. SCFGs have a wide
range of applications, including machine transla-
tion, word and phrase alignments, and automatic
dictionary construction. Variations of SCFGs go
back to Aho and Ullman (1972)?s Syntax-Directed
Translation Schemata, but also include the In-
version Transduction Grammars in Wu (1997),
which restrict grammar rules to be binary, the syn-
chronous grammars in Chiang (2005), which use
only a single nonterminal symbol, and the Multi-
text Grammars in Melamed (2003), which allow
independent rewriting, as well as other tree-based
models such as Yamada and Knight (2001) and
Galley et al (2004).
When viewed as a rewriting system, an SCFG
generates a set of string pairs, representing some
translation relation. We are concerned here with
the time complexity of parsing such a pair, accord-
ing to the grammar. Assume then a pair with each
string having a maximum length of N , and con-
sider an SCFG G of size |G|, with a bound of n
nonterminals in the right-hand side of each rule in
a single dimension, which we call below the rank
of G. As an upper bound, parsing can be carried
out in time O(|G|Nn+4) by a dynamic program-
ming algorithm maintaining continuous spans in
one dimension. As a lower bound, parsing strate-
gies with discontinuous spans in both dimensions
can take time ?(|G|N c?n) for unfriendly permu-
tations (Satta and Peserico, 2005). A natural ques-
tion to ask then is: What if we could reduce the
rank of G, preserving the generated translation?
As in the case of CFGs, one way of doing this
would be to factorize each single rule into several
rules of rank strictly smaller than n. It is not diffi-
cult to see that this would result in a new grammar
of size at most 2 ? |G|. In the time complexities
reported above, we see that such a size increase
would be more than compensated by the reduction
in the degree of the polynomial in N . We thus
conclude that a reduction in the rank of an SCFG
would result in more efficient parsing algorithms,
for most common parsing strategies.
In the general case, normal forms with bounded
rank are not admitted by SCFGs, as shown in (Aho
and Ullman, 1972). Nonetheless, an SCFG with a
rank of n may not necessarily meet the worst case
of Aho and Ullman (1972). It is then reasonable
to ask if our SCFG G can be factorized, and what
is the smallest rank k < n that can be obtained
in this way. This paper answers these two ques-
tions, by providing an algorithm that factorizes the
rules of an input SCFG, resulting in a new, genera-
tively equivalent, SCFG with rank k as low as pos-
sible. The algorithm works in time O(n log n) for
each rule, regardless of the rank k of the factorized
rules. As discussed above, in this way we achieve
an improvement of the parsing time for SCFGs,
obtaining an upper bound of O(|G|N k+4) by us-
ing a parsing strategy that maintains continuous
279
1,2
1,2
2,1
2 1
1,2
3 4
3,1,4,2
7 5 8 6
4,1,3,5,2
7 1 2,4,1,3
4 6 3 5
8 2
Figure 1: Two permutation trees. The permuta-
tions associated with the leaves can be produced
by composing the permutations at the internal
nodes.
spans in one dimension.
Previous work on this problem has been pre-
sented in Zhang et al (2006), where a method is
provided for casting an SCFG to a form with rank
k = 2. If generalized to any value of k, that algo-
rithm would run in time O(n2). We thus improve
existing factorization methods by almost a factor
of n. We also solve an open problem mentioned
by Albert et al (2003), who pose the question of
whether irreducible, or simple, permutations can
be recognized in time less than ?(n2).
2 Synchronous CFGs and permutation
trees
We begin by describing the synchronous CFG for-
malism, which is more rigorously defined by Aho
and Ullman (1972) and Satta and Peserico (2005).
Let us consider strings defined over some set of
nonterminal and terminal symbols, as defined for
CFGs. We say that two such strings are syn-
chronous if some bijective relation is given be-
tween the occurrences of the nonterminals in the
two strings. A synchronous context-free gram-
mar (SCFG) is defined as a CFG, with the dif-
ference that it uses synchronous rules of the form
[A1 ? ?1, A2 ? ?2], with A1, A2 nonterminalsand ?1, ?2 synchronous strings. We can use pro-duction [A1 ? ?1, A2 ? ?2] to rewrite any syn-chronous strings [?11A1?12, ?21A2?22] into thesynchronous strings [?11?1?12, ?21?2?22], un-der the condition that the indicated occurrences
of A1 and A2 be related by the bijection asso-ciated with the source synchronous strings. Fur-
thermore, the bijective relation associated with the
target synchronous strings is obtained by compos-
ing the relation associated with the source syn-
chronous strings and the relation associated with
synchronous pair [?1, ?2], in the most obviousway.
As in standard constructions that reduce the
rank of a CFG, in this paper we focus on each
single synchronous rule and factorize it into syn-
chronous rules of lower rank. If we view the bijec-
tive relation associated with a synchronous rule as
a permutation, we can further reduce our factoriza-
tion problem to the problem of factorizing a per-
mutation of arity n into the composition of several
permutations of arity k < n. Such factorization
can be represented as a tree of composed permuta-
tions, called in what follows a permutation tree.
A permutation tree can be converted into a set of
k-ary SCFG rules equivalent to the input rule. For
example, the input rule:
[ X ? A(1)B(2)C(3)D(4)E(5)F (6)G(7)H(8),
X ? B(2)A(1)C(3)D(4)G(7)E(5)H(8)F (6) ]
yields the permutation tree of Figure 1(left). In-
troducing a new grammar nonterminal Xi for eachinternal node of the tree yields an equivalent set of
smaller rules:
[ X ? X(1)1 X
(2)
2 , X ? X
(1)
1 X
(2)
2 ]
[ X1 ? X(1)3 X
(2)
4 , X1 ? X
(1)
3 X
(2)
4 ]
[ X3 ? A(1)B(2), X3 ? B(2)A(1) ]
[ X4 ? C(1)D(2), X4 ? C(1)D(2) ]
[ X2 ? E(1)F (2)G(3)H(4),
X2 ? G(3)E(1)H(4)F (2) ]
In the case of stochastic grammars, the rule cor-
responding to the root of the permutation tree is
assigned the original rule?s probability, while all
other rules, associated with new grammar nonter-
minals, are assigned probability 1. We process
each rule of an input SCFG independently, pro-
ducing an equivalent grammar with the smallest
possible arity.
3 Factorization Algorithm
In this section we specify and discuss our factor-
ization algorithm. The algorithm takes as input a
permutation defined on the set {1, ? ? ? , n}, repre-
senting a rule of some SCFG, and provides a per-
mutation tree of arity k ? n for that permutation,
with k as small as possible.
Permutation trees covering a given input permu-
tation are unambiguous with the exception of se-
quences of binary rules of the same type (either
inverted or straight) (Albert et al, 2003). Thus,
when factorizing a permutation into a permutation
280
tree, it is safe to greedily reduce a subsequence
into a new subtree as soon as a subsequence is
found which represents a continuous span in both
dimensions of the permutation matrix1 associated
with the input permutation. For space reasons, we
omit the proof, but emphasize that any greedy re-
duction turns out to be either necessary, or equiv-
alent to the other alternatives.
Any sequences of binary rules can be rear-
ranged into a normalized form (e.g. always left-
branching) as a postprocessing step, if desired.
The top-level structure of the algorithm exploits
a divide-and-conquer approach, and is the same as
that of the well-known mergesort algorithm (Cor-
men et al, 1990). We work on subsequences of
the original permutation, and ?merge? neighbor-
ing subsequences into successively longer subse-
quences, combining two subsequences of length
2i into a subsequence of length 2i+1 until we have
built one subsequence spanning the entire permu-
tation. If each combination of subsequences can
be performed in linear time, then the entire permu-
tation can be processed in time O(n log n). As in
the case of mergesort, this is an application of the
so-called master theorem (Cormen et al, 1990).
As the algorithm operates, we will maintain the
invariant that we must have built all subtrees of
the target permutation tree that are entirely within
a given subsequence that has been processed. This
is analogous to the invariant in mergesort that all
processed subsequences are in sorted order. When
we combine two subsequences, we need only build
nodes in the tree that cover parts of both sub-
sequences, but are entirely within the combined
subsequence. Thus, we are looking for subtrees
that span the midpoint of the combined subse-
quence, but have left and right boundaries within
the boundaries of the combined subsequence. In
what follows, this midpoint is called the split
point.
From this invariant, we will be guaranteed to
have a complete, correct permutation tree at the
end of last subsequence combination. An example
of the operation of the general algorithm is shown
in Figure 2. The top-level structure of the algo-
rithm is presented in function KARIZE of Figure 3.
There may be more than one reduction neces-
sary spanning a given split point when combin-
ing two subsequences. Function MERGE in Fig-
1A permutation matrix is a way of representing a permuta-
tion, and is obtained by rearranging the row (or the columns)
of an identity matrix, according to the permutation itself.
2 1 3 4 7 5 8 6
2,1
2 1
1,2
3 4 7 5 8 6
1,2
2,1
2 1
1,2
3 4
3,1,4,2
7 5 8 6
1,2
1,2
2,1
2 1
1,2
3 4
3,1,4,2
7 5 8 6
Figure 2: Recursive combination of permutation
trees. Top row, the input permutation. Second row,
after combination into sequences of length two, bi-
nary nodes have been built where possible. Third
row, after combination into sequences of length
four; bottom row, the entire output tree.
ure 3 initializes certain data structures described
below, and then checks for reductions repeatedly
until no further reduction is possible. It looks first
for the smallest reduction crossing the split point
of the subsequences being combined. If SCAN,
described below, finds a valid reduction, it is com-
mitted by calling REDUCE. If a reduction is found,
we look for further reductions crossing either the
left or right boundary of the new reduction, repeat-
ing until no further reductions are possible. Be-
cause we only need to find reductions spanning
the original split point at a given combination step,
this process is guaranteed to find all reductions
needed.
We now turn to the problem of identifying a
specific reduction to be made across a split point,
which involves identifying the reduction?s left and
right boundaries. Given a subsequence and can-
didate left and right boundaries for that subse-
quence, the validity of making a reduction over
this span can be tested by verifying whether the
span constitutes a permuted sequence, that is,
a permutation of a contiguous sequence of inte-
gers. Since the starting permutation is defined
on a set {1, 2, ? ? ? , n}, we have no repeated in-
tegers in our subsequences, and the above condi-
281
function KARIZE(pi)
. initialize with identity mapping
h? hmin? hmax? (0..|pi|);
. mergesort core
for size? 1; size ? |pi|; size? size * 2 do
for min? 0;
min < |pi|-size+1;
min? min + 2 * size do
div = min + size - 1;
max? min(|pi|, min + 2*size - 1);
MERGE(min, div, max);
function MERGE(min, div, max)
. initialize h
sort h[min..max] according to pi[i];
sort hmin[min..max] according to pi[i];
sort hmax[min..max] according to pi[i];
. merging sorted list takes linear time
. initialize v
for i? min; i ? max; i? i + 1 do
v [ h[i] ]? i;
. check if start of new reduced block
if i = min or
hmin[i] 6= hmin[i-1] then
vmin? i;
vmin[ h[i] ]? vmin;
for i? max; i ? min; i? i - 1 do
. check if start of new reduced block
if i = max or
hmax[i] 6= hmax[i+1] then
vmax? i ;
vmax[ h[i] ]? vmax;
. look for reductions
if SCAN(div) then
REDUCE(scanned reduction);
while SCAN(left) or SCAN(right) do
REDUCE(smaller reduction);
function REDUCE(left, right, bot, top)
for i? bot..top do
hmin[i]? left;
hmax[i]? right;
for i? left..right do
vmin[i]? bot;
vmax[i]? top;
print ?reduce:? left..right ;
Figure 3: KARIZE: Top level of algorithm, iden-
tical to that of mergesort. MERGE: combines two
subsequences of size 2i into new subsequence of
size 2i+1. REDUCE: commits reduction by updat-
ing min and max arrays.
tion can be tested by scanning the span in ques-
tion, finding the minimum and maximum integers
in the span, and checking whether their difference
is equal to the length of the span minus one. Be-
low we call this condition the reduction test. As
an example of the reduction test, consider the sub-
sequence (7, 5, 8, 6), and take the last three ele-
ments, (5, 8, 6), as a candidate span. We see that
5 and 8 are the minimum and maximum integers
in the corresponding span, respectively. We then
compute 8 ? 5 = 3, while the length of the span
minus one is 2, implying that no reduction is possi-
ble. However, examining the entire subsequence,
the minimum is 5 and the maximum is 8, and
8 ? 5 = 3, which is the length of the span minus
one. We therefore conclude that we can reduce
that span by means of some permutation, that is,
parse the span by means of a node in the permuta-
tion tree. This reduction constitutes the 4-ary node
in the permutation tree of Figure 2.
A trivial implementation of the reduction test
would be to tests all combinations of left and right
boundaries for the new reduction. Unfortunately,
this would take time ?(n2) for a single subse-
quence combination step, whereas to achieve the
overall O(n log n) complexity we need linear time
for each combination step.
It turns out that the boundaries of the next re-
duction, covering a given split point, can be com-
puted in linear time with the technique shown in
function SCAN of Figure 5. We start with left and
right candidate boundaries at the two points imme-
diately to the left and right of the split point, and
then repeatedly check whether the current left and
right boundaries identify a permuted sequence by
applying the reduction test, and move the left and
right boundaries outward as necessary, as soon as
?missing? integers are identified outside the cur-
rent boundaries, as explained below. We will show
that, as we move outward, the number of possible
configurations achieved for the positions of the left
and the right boundaries is linearly bounded in the
length of the combined subsequence (as opposed
to quadratically bounded).
In order to efficiently implement the above idea,
we will in fact maintain four boundaries for the
candidate reduction, which can be visualized as
the left, right, top and bottom boundaries in the
permutation matrix. No explicit representation
of the permutation matrix itself is constructed, as
that would require quadratic time. Rather, we
282
7 1 4 6 3 5 8 2pi 4
7
2
1
1
3
2
4
4
3
6
1
6
3
8
7
5
5
8
8
6
5
2
7
v
pi
h
Figure 4: Permutation matrix for input permuta-
tion pi (left) and within-subsequence permutation
v (right) for subsequences of size four.
maintain two arrays: h, which maps from vertical
to horizontal positions within the current subse-
quence, and v which maps from horizontal to ver-
tical positions. These arrays represent the within-
subsequence permutation obtained by sorting the
elements of each subsequence according to the
input permutation, while keeping each element
within its block, as shown in Figure 4.
Within each subsequence, we alternate between
scanning horizontally from left to right, possibly
extending the top and bottom boundaries (Figure 5
lines 9 to 14), and scanning vertically from bottom
to top, possibly extending the left and right bound-
aries (lines 20 to 26). Each extension is forced
when, looking at the within-subsequence permuta-
tion, we find that some element is within the cur-
rent boundaries in one dimension but outside the
boundaries in the other. If the distance between
vertical boundaries is larger in the input permu-
tation than in the subsequence permutation, nec-
essary elements are missing from the current sub-
sequence and no reduction is possible at this step
(line 18). When all necessary elements are present
in the current subsequence and no further exten-
sions are necessary to the boundaries (line 30), we
have satisfied the reduction test on the input per-
mutation, and make a reduction.
The trick used to keep the iterative scanning lin-
ear is that we skip the subsequence scanned on the
previous iteration on each scan, in both the hori-
zontal and vertical directions. Lines 13 and 25 of
Figure 5 perform this skip by advancing the x and y
counters past previously scanned regions. Consid-
ering the horizontal scan of lines 9 to 14, in a given
iteration of the while loop, we scan only the items
between newleft and left and between right and
newright. On the next iteration of the while loop,
the newleft boundary has moved further to the left,
1: function SCAN (div)
2: left???;
3: right???;
4: newleft? div;
5: newright? div + 1 ;
6: newtop???;
7: newbot??;
8: while 1 do
. horizontal scan
9: for x? newleft; x ? newright ; do
10: newtop? max(newtop, vmax[x]);
11: newbot? min(newbot, vmin[x]);
. skip to end of reduced block
12: x? hmax[vmin[x]] + 1;
. skip section scanned on last iter
13: if x = left then
14: x? right + 1;
15: right? newright;
16: left? newleft;
. the reduction test
17: if newtop - newbot <
18: pi[h[newtop]] - pi[h[newbot]] then
19: return (0);
. vertical scan
20: for y? newbot; y ? newtop ; do
21: newright?
22: max(newright, hmax[y]);
23: newleft? min(newleft, hmin[y]);
. skip to end of reduced block
24: y? vmax[hmin[y]] + 1;
. skip section scanned on last iter
25: if y = bot then
26: y? top + 1;
27: top? newtop;
28: bot? newbot;
. if no change to boundaries, reduce
29: if newright = right
30: and newleft = left then
31: return (1, left, right, bot, top);
Figure 5: Linear time function to check for a sin-
gle reduction at split point div.
283
while the variable left takes the previous value of
newleft, ensuring that the items scanned on this it-
eration are distinct from those already processed.
Similarly, on the right edge we scan new items,
between right and newright. The same analysis
applies to the vertical scan. Because each item in
the permutation is scanned only once in the verti-
cal direction and once in the horizontal direction,
the entire call to SCAN takes linear time, regard-
less of the number of iterations of the while loop
that are required.
We must further show that each call to MERGE
takes only linear time, despite that fact that it
may involve many calls to SCAN. We accom-
plish this by introducing a second type of skipping
in the scans, which advances past any previously
reduced block in a single step. In order to skip
past previous reductions, we maintain (in func-
tion REDUCE) auxiliary arrays with the minimum
and maximum positions of the largest block each
point has been reduced to, in both the horizontal
and vertical dimensions. We use these data struc-
tures (hmin, hmax, vmin, vmax) when advancing to
the next position of the scan in lines 12 and 24 of
Figure 5. Because each call to SCAN skips items
scanned by previous calls, each item is scanned
at most twice across an entire call to MERGE,
once when scanning across a new reduction?s left
boundary and once when scanning across the right
boundary, guaranteeing that MERGE completes in
linear time.
4 An Example
In this section we examine the operation of the
algorithm on a permutation of length eight, re-
sulting in the permutation tree of Figure 1(right).
We will build up our analysis of the permutation
by starting with individual items of the input per-
mutation and building up subsequences of length
2, 4, and finally 8. In our example permutation,
(7, 1, 4, 6, 3, 5, 8, 2), no reductions can be made
until the final combination step, in which one per-
mutation of size 4 is used, and one of size 5.
We begin with the input permutation along the
bottom of Figure 6a. We represent the interme-
diate data structures h, hmin, and hmax along the
vertical axis of the figure; these three arrays are all
initialized to be the sequence (1, 2, ? ? ? , 8).
Figure 6b shows the combination of individual
items into subsequences of length two. Each new
subsequence of the h array is sorted according to
a)
7
1
1
1
111
1
2
2
2
222
4
3
3
3
333
6
4
4
4
444
3
5
5
5
555
5
6
6
6
666
8
7
7
7
777
2
8
8
8
888
pi
v
vmin
vmax
hhm
in
hm
ax
1
7
1
2
1
2
3
4
3
4
6
4
5
3
5
6
5
6
7
8
7
8
2
8
v
pi
h
b)
7
2
2
2
222
1
1
1
1
111
4
3
3
3
333
6
4
4
4
444
3
5
5
5
555
5
6
6
6
666
8
8
8
8
888
2
7
7
7
777
pi
v
vmin
vmax
hhm
in
hm
ax
2
7
2
1
1
1
3
4
3
4
6
4
5
3
5
6
5
6
8
8
8
7
2
7
v
pi
h
c)
7
4
4
4
222
1
1
1
1
333
4
2
2
2
444
6
3
3
3
111
3
6
6
6
888
5
7
7
7
555
8
8
8
8
666
2
5
5
5
777
pi
v
vmin
vmax
hhm
in
hm
ax
4
7
2
1
1
3
2
4
4
3
6
1
6
3
8
7
5
5
8
8
6
5
2
7
v
pi
h
Figure 6: Steps in an example computation,
with input permutation pi on left and within-
subsequence permutation described by v array on
right. Panel (a) shows initial blocks of unit size,
(b) shows combination of unit blocks into blocks
of size two, and (c) size two into size four. No
reductions are possible in these stages; example
continued in next figure.
284
a)
7
7
7
7
222
1
1
1
1
888
4
4
4
4
555
6
6
6
6
333
3
3
3
3
666
5
5
5
5
444
8
8
8
8
111
2
2
2
2
777
pi
v
vmin
vmax
hhm
in
hm
ax b)
7
7
7
7
222
1
1
1
1
888
4
4
3
6
536
6
6
3
6
336
3
3
3
6
636
5
5
3
6
436
8
8
8
8
111
2
2
2
2
777
pi
v
vmin
vmax
hhm
in
hm
ax
Left and right boundaries are initialized
to be adjacent to horizontal split point.
Vertical scan shows left and right bound-
aries must be extended. Permutation of
size four is reduced.
c)
7
7
7
7
222
1
1
1
1
888
4
4
3
6
536
6
6
3
6
336
3
3
3
6
636
5
5
3
6
436
8
8
8
8
111
2
2
2
2
777
pi
v
vmin
vmax
hhm
in
hm
ax d)
7
7
7
7
222
1
1
1
1
888
4
4
3
6
536
6
6
3
6
336
3
3
3
6
636
5
5
3
6
436
8
8
8
8
111
2
2
2
2
777
pi
v
vmin
vmax
hhm
in
hm
ax
Search for next reduction: left and right
boundaries initialized to be adjacent to
left edge of previous reduction.
Vertical scan shows right boundary must
be extended.
e)
7
7
7
7
222
1
1
1
1
888
4
4
3
6
536
6
6
3
6
336
3
3
3
6
636
5
5
3
6
436
8
8
8
8
111
2
2
2
2
777
pi
v
vmin
vmax
hhm
in
hm
ax f)
7
7
1
8
218
1
1
1
8
818
4
4
1
8
518
6
6
1
8
318
3
3
1
8
618
5
5
1
8
418
8
8
1
8
118
2
2
1
8
718
pi
v
vmin
vmax
hhm
in
hm
ax
Horizontal scan shows top boundary must
be extended.
Vertical scan shows left boundary must
be extended. Permutation of size five is
reduced.
Figure 7: Steps in scanning for final combination of subsequences, where v = pi. Area within current
left, right, top and bottom boundaries is shaded; darker shading indicates a reduction. In each scan, the
span scanned in the previous panel is skipped over.
285
the vertical position of the dots in the correspond-
ing columns. Thus, because pi[7] = 8 > pi[8] = 2,
we swap 7 and 8 in the h array. The algorithm
checks whether any reductions can be made at this
step by computing the difference between the in-
tegers on each side of each split point. Because
none of the pairs of integers in are consecutive, no
reductions are made at this step.
Figure 6c shows the combination the pairs
into subsequences of length four. The two split
points to be examined are between the second and
third position, and the sixth and seventh position.
Again, no reductions are possible.
Finally we combine the two subsequences of
length four to complete the analysis of the entire
permutation. The split point is between the fourth
and fifth positions of the input permutation, and
in the first horizontal scan of these two positions,
we see that pi[4] = 6 and pi[5] = 3, meaning our
top boundary will be 6 and our bottom boundary
3, shown in Figure 7a. Scanning vertically from
position 3 to 6, we see horizontal positions 5, 3,
6, and 4, giving the minimum, 3, as the new left
boundary and the maximum, 6, as the new right
boundary, shown in Figure 7b. We now perform
another horizontal scan starting at position 3, but
then jumping directly to position 6, as horizontal
positions 4 and 5 were scanned previously. Af-
ter this scan, the minimum vertical position seen
remains 3, and the maximum vertical position is
still 6. At this point, because we have the same
boundaries as on the previous scan, we can stop
and verify whether the region determined by our
current boundaries has the same length in the ver-
tical and horizontal dimensions. Both dimensions
have length four, meaning that we have found a
subsequence that is continuous in both dimensions
and can safely be reduced, as shown in Figure 6d.
After making this reduction, we update the hmin
array to have all 3?s for the newly reduced span,
and update hmax to have all sixes. We then check
whether further reductions are possible covering
this split point. We repeat the process of scan-
ning horizontally and vertically in Figure 7c-f,
this time skipping the span just reduced. One fur-
ther reduction is possible, covering the entire input
permutation, as shown in Figure 7f.
5 Conclusion
The algorithm above not only identifies whether
a permutation can be factored into a composi-
tion of permutations, but also returns the factor-
ization that minimizes the largest rule size, in time
O(n log n). The factored SCFG with rules of size
at most k can be used to synchronously parse
in time O(Nk+4) by dynamic programming with
continuous spans in one dimension.
As mentioned in the introduction, the optimal
parsing strategy for SCFG rules with a given
permutation may involve dynamic programming
states with discontinuous spans in both dimen-
sions. Whether these optimal parsing strategies
can be found efficiently remains an interesting
open problem.
Acknowledgments This work was partially sup-
ported by NSF ITR IIS-09325646 and NSF ITR
IIS-0428020.
References
Albert V. Aho and Jeffery D. Ullman. 1972. The
Theory of Parsing, Translation, and Compiling, vol-
ume 1. Prentice-Hall, Englewood Cliffs, NJ.
M. H. Albert, M. D. Atkinson, and M. Klazar. 2003.
The enumeration of simple permutations. Journal
of Integer Sequences, 6(03.4.4):18 pages.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL-05, pages 263?270.
Thomas H. Cormen, Charles E. Leiserson, and
Ronald L. Rivest. 1990. Introduction to algorithms.
MIT Press, Cambridge, MA.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT/NAACL.
I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of HLT/NAACL.
Giorgio Satta and Enoch Peserico. 2005. Some com-
putational complexity results for synchronous
context-free grammars. In Proceedings of
HLT/EMNLP, pages 803?810.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of ACL-01.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of HLT/NAACL.
286
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 953?960,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Inducing Word Alignments with Bilexical Synchronous Trees
Hao Zhang and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
This paper compares different bilexical
tree-based models for bilingual alignment.
EM training for the new model bene-
fits from the dynamic programming ?hook
trick?. The model produces improved de-
pendency structure for both languages.
1 Introduction
A major difficulty in statistical machine translation
is the trade-off between representational power
and computational complexity. Real-world cor-
pora for language pairs such as Chinese-English
have complex reordering relationships that are not
captured by current phrase-based MT systems, de-
spite their state-of-the-art performance measured
in competitive evaluations. Synchronous gram-
mar formalisms that are capable of modeling
such complex relationships while maintaining the
context-free property in each language have been
proposed for many years, (Aho and Ullman, 1972;
Wu, 1997; Yamada and Knight, 2001; Melamed,
2003; Chiang, 2005), but have not been scaled to
large corpora and long sentences until recently.
In Synchronous Context Free Grammars, there
are two sources of complexity, grammar branch-
ing factor and lexicalization. In this paper we fo-
cus on the second issue, constraining the gram-
mar to the binary-branching Inversion Transduc-
tion Grammar of Wu (1997). Lexicalization seems
likely to help models predict alignment patterns
between languages, and has been proposed by
Melamed (2003) and implemented by Alshawi et
al. (2000) and Zhang and Gildea (2005). However,
each piece of lexical information considered by a
model multiplies the number of states of dynamic
programming algorithms for inference, meaning
that we must choose how to lexicalize very care-
fully to control complexity.
In this paper we compare two approaches to
lexicalization, both of which incorporate bilexical
probabilities. One model uses bilexical probabil-
ities across languages, while the other uses bilex-
ical probabilities within one language. We com-
pare results on word-level alignment, and investi-
gate the implications of the choice of lexicaliza-
tion on the specifics of our alignment algorithms.
The new model, which bilexicalizes within lan-
guages, allows us to use the ?hook trick? (Eis-
ner and Satta, 1999) and therefore reduces com-
plexity. We describe the application of the hook
trick to estimation with Expectation Maximization
(EM). Despite the theoretical benefits of the hook
trick, it is not widely used in statistical monolin-
gual parsers, because the savings do not exceed
those obtained with simple pruning. We speculate
that the advantages may be greater in an EM set-
ting, where parameters to guide pruning are not
(initially) available.
In order to better understand the model, we an-
alyze its performance in terms of both agreement
with human-annotated alignments, and agreement
with the dependencies produced by monolingual
parsers. We find that within-language bilexical-
ization does not improve alignment over cross-
language bilexicalization, but does improve recov-
ery of dependencies. We find that the hook trick
significantly speeds training, even in the presence
of pruning.
Section 2 describes the generative model. The
hook trick for EM is explained in Section 3. In
Section 4, we evaluate the model in terms of align-
ment error rate and dependency error rate. We
conclude with discussions in Section 5.
953
2 Bilexicalization of Inversion
Transduction Grammar
The Inversion Transduction Grammar of Wu
(1997) models word alignment between a transla-
tion pair of sentences by assuming a binary syn-
chronous tree on top of both sides. Using EM
training, ITG can induce good alignments through
exploring the hidden synchronous trees from in-
stances of string pairs.
ITG consists of unary production rules that gen-
erate English/foreign word pairs e/f :
X ? e/f
and binary production rules in two forms that gen-
erate subtree pairs, written:
X ? [Y Z]
and
X ? ?Y Z?
The square brackets indicate the right hand side
rewriting order is the same for both languages.
The pointed brackets indicate there exists a type of
syntactic reordering such that the two right hand
side constituents rewrite in the opposite order in
the second language.
The unary rules account for the alignment links
across two sides. Either e or f may be a special
null word, handling insertions and deletions. The
two kinds of binary rules (called straight rules and
inverted rules) build up a coherent tree structure
on top of the alignment links. From a modeling
perspective, the synchronous tree that may involve
inversions tells a generative story behind the word
level alignment.
An example ITG tree for the sentence pair Je
les vois / I see them is shown in Figure 1(left). The
probability of the tree is the product rule probabil-
ities at each node:
P (S ? A)
? P (A ? [C B])
? P (C ? I/Je)
? P (B ? ?C C?)
? P (C ? see/vois)
? P (C ? them/les)
The structural constraint of ITG, which is that
only binary permutations are allowed on each
level, has been demonstrated to be reasonable
by Zens and Ney (2003) and Zhang and Gildea
(2004). However, in the space of ITG-constrained
synchronous trees, we still have choices in making
the probabilistic distribution over the trees more
realistic. The original Stochastic ITG is the coun-
terpart of Stochastic CFG in the bitext space. The
probability of an ITG parse tree is simply a prod-
uct of the probabilities of the applied rules. Thus,
it only captures the fundamental features of word
links and reflects how often inversions occur.
2.1 Cross-Language Bilexicalization
Zhang and Gildea (2005) described a model in
which the nonterminals are lexicalized by English
and foreign language word pairs so that the inver-
sions are dependent on lexical information on the
left hand side of synchronous rules. By introduc-
ing the mechanism of probabilistic head selection
there are four forms of probabilistic binary rules
in the model, which are the four possibilities cre-
ated by taking the cross-product of two orienta-
tions (straight and inverted) and two head choices:
X(e/f) ? [Y (e/f) Z]
X(e/f) ? [Y Z(e/f)]
X(e/f) ? ?Y (e/f) Z?
X(e/f) ? ?Y Z(e/f)?
where (e/f) is a translation pair.
A tree for our example sentence under this
model is shown in Figure 1(center). The tree?s
probability is again the product of rule probabil-
ities:
P (S ? A(see/vois))
? P (A(see/vois) ? [C B(see/vois)])
? P (C ? C(I/Je))
? P (B(see/vois) ? ?C(see/vois) C?)
? P (C ? C(them/les))
2.2 Head-Modifier Bilexicalization
One disadvantage of the model above is that it
is not capable of modeling bilexical dependen-
cies on the right hand side of the rules. Thus,
while the probability of a production being straight
or inverted depends on a bilingual word pair, it
does not take head-modifier relations in either lan-
guage into account. However, modeling complete
bilingual bilexical dependencies as theorized in
Melamed (2003) implies a huge parameter space
of O(|V |2|T |2), where |V | and |T | are the vo-
cabulary sizes of the two languages. So, in-
stead of modeling cross-language word transla-
tions and within-language word dependencies in
954
CB
C
A
see/vois them/les
I/Je
S
C
C(I/Je)
C
C(them/les)
C
B(see/vois)
A(see/vois)
C(see/vois)
S S
C(I)
C(them)
them/les
I/Je C(see)
see/vois
B(see)
A(see)
Figure 1: Parses for an example sentence pair under unlexicalized ITG (left), cross-language bilexicaliza-
tion (center), and head-modifier bilexicaliztion (right). Thick lines indicate head child; crossbar indicates
inverted production.
a joint fashion, we factor them apart. We lexical-
ize the dependencies in the synchronous tree using
words from only one language and translate the
words into their counterparts in the other language
only at the bottom of the tree. Formally, we have
the following patterns of binary dependency rules:
X(e) ? [Y (e) Z(e?)]
X(e) ? [Y (e?) Z(e)]
X(e) ? ?Y (e) Z(e?)?
X(e) ? ?Y (e?) Z(e)?
where e is an English head and e? is an English
modifier.
Equally importantly, we have the unary lexical
rules that generate foreign words:
X(e) ? e/f
To make the generative story complete, we also
have a top rule that goes from the unlexicalized
start symbol to the highest lexicalized nonterminal
in the tree:
S ? X(e)
Figure 1(right), shows our example sentence?s
tree under the new model. The probability of a
bilexical synchronous tree between the two sen-
tences is:
P (S ? A(see))
? P (A(see) ? [C(I) B(see)])
? P (C(I) ? I/Je)
? P (B(see) ? ?C(see) C(them)?)
? P (C(see) ? see/vois)
? P (C(them) ? them/les)
Interestingly, the lexicalized B(see) predicts
not only the existence of C(them), but also that
there is an inversion involved going from C(see)
to C(them). This reflects the fact that direct ob-
ject pronouns come after the verb in English, but
before the verb in French. Thus, despite condi-
tioning on information about words from only one
language, the model captures syntactic reordering
information about the specific language pair it is
trained on. We are able to discriminate between
the straight and inverted binary nodes in our ex-
ample tree in a way that cross-language bilexical-
ization could not.
In terms of inferencing within the framework,
we do the usual Viterbi inference to find the best
bilexical synchronous tree and treat the depen-
dencies and the alignment given by the Viterbi
parse as the best ones, though mathematically the
best alignment should have the highest probabil-
ity marginalized over all dependencies constrained
by the alignment. We do unsupervised training to
obtain the parameters using EM. Both EM and
Viterbi inference can be done using the dynamic
programming framework of synchronous parsing.
3 Inside-Outside Parsing with the Hook
Trick
ITG parsing algorithm is a CYK-style chart pars-
ing algorithm extended to bitext. Instead of build-
ing up constituents over spans on a string, an ITG
chart parser builds up constituents over subcells
within a cell defined by two strings. We use
?(X(e), s, t, u, v) to denote the inside probabil-
ity of X(e) which is over the cell of (s, t, u, v)
where (s, t) are indices into the source language
string and (u, v) are indices into the target lan-
guage string. We use ?(X(e), s, t, u, v) to de-
note its outside probability. Figure 2 shows how
smaller cells adjacent along diagonals can be com-
bined to create a large cell. We number the sub-
cells counterclockwise. To analyze the complex-
ity of the algorithm with respect to input string
955
S
       
       
       
       
       
       
      
      
      
      
      
      
 





      
      
      
      
      
      
u
U
v
s t
2 1
3 4
U






     
     
     
     
     
     
u
s S
e
Figure 2: Left: Chart parsing over the bitext cell of (s, t, u, v). Right: One of the four hooks built for
four corners for more efficient parsing.
length, without loss of generality, we ignore the
nonterminal symbols X , Y , and Z to simplify the
derivation.
The inside algorithm in the context of bilexical
ITG is based on the following dynamic program-
ming equation:
? (e, s, t, u, v)
=
?
S,U,e?
?
?
?
?
?
?1(e) ? ?3(e?) ? P ([e?e] | e)
+?2(e) ? ?4(e?) ? P (?ee?? | e)
+?3(e) ? ?1(e?) ? P ([ee?] | e)
+?4(e) ? ?2(e?) ? P (?e?e? | e)
?
?
?
?
?
So, on the right hand side, we sum up all possi-
ble ways (S, U ) of splitting the left hand side cell
and all possible head words (e?) for the non-head
subcell. e, e?, s, t, u, v, S, and U all eight vari-
ables take O(n) values given that the lengths of
the source string and the target string are O(n).
Thus the entire DP algorithm takes O(n8) steps.
Fortunately, we can reduce the maximum num-
ber of interacting variables by factorizing the ex-
pression.
Let us keep the results of the summations over
e? as:
?+1 (e) =
?
e?
?1(e?) ? P ([ee?] | e)
?+2 (e) =
?
e?
?2(e?) ? P (?e?e? | e)
?+3 (e) =
?
e?
?3(e?) ? P ([e?e] | e)
?+4 (e) =
?
e?
?4(e?) ? P (?ee?? | e)
The computation of each ?+ involves four
boundary indices and two head words. So, we can
rely on DP to compute them in O(n6). Based on
these intermediate results, we have the equivalent
DP expression for computing inside probabilities:
? (e, s, t, u, v)
=
?
S,U
?
?
?
?
?
?1(e) ? ?+3 (e)
+ ?2(e) ? ?+4 (e)
+ ?3(e) ? ?+1 (e)
+ ?4(e) ? ?+2 (e)
?
?
?
?
?
We reduced one variable from the original ex-
pression. The maximum number of interacting
variables throughout the algorithm is 7. So the im-
proved inside algorithm has a time complexity of
O(n7).
The trick of reducing interacting variables in DP
for bilexical parsing has been pointed out by Eis-
ner and Satta (1999). Melamed (2003) discussed
the applicability of the so-called hook trick for
parsing bilexical multitext grammars. The name
hook is based on the observation that we combine
the non-head constituent with the bilexical rule to
create a special constituent that matches the head
like a hook as demonstrated in Figure 2. How-
ever, for EM, it is not clear from their discussions
how we can do the hook trick in the outside pass.
The bilexical rules in all four directions are anal-
ogous. To simplify the derivation for the outside
algorithm, we just focus on the first case: straight
rule with right head word.
The outside probability of the constituent
(e, S, t, U, v) in cell 1 being a head of such rules
is:
?
s,u,e?
(
?(e) ? ?3(e?) ? P ([e?e] | e)
)
=
?
s,u
(
?(e) ?
(
?
e?
?3(e?) ? P ([e?e] | e)
))
=
?
s,u
(
?(e) ? ?+3 (e)
)
which indicates we can reuse ?+ of the lower left
neighbors of the head to make the computation
feasible in O(n7).
On the other hand, the outside probability for
(e?, s, S, u, U) in cell 3 acting as a modifier of such
956
a rule is:
?
t,v,e
(
?(e) ? ?1(e) ? P ([e?e] | e)
)
=
?
e
?
?P ([e?e] | e) ?
?
?
?
t,v
?(e) ? ?1(e)
?
?
?
?
=
?
e
(
P ([e?, e] | e) ? ?+3 (e)
)
in which we memorize another kind of intermedi-
ate sum to make the computation no more complex
than O(n7).
We can think of ?+3 as the outside probabilityof the hook on cell 3 which matches cell 1. Gener-
ally, we need outside probabilities for hooks in all
four directions.
?+1 (e) =
?
s,u
?(e) ? ?3(e)
?+2 (e) =
?
t,u
?(e) ? ?4(e)
?+3 (e) =
?
t,v
?(e) ? ?1(e)
?+4 (e) =
?
s,v
?(e) ? ?2(e)
Based on them, we can add up the outside prob-
abilities of a constituent acting as one of the two
children of each applicable rule on top of it to get
the total outside probability.
We finalize the derivation by simplifying the ex-
pression of the expected count of (e ? [e?e]).
EC(e ? [e?e])
=
?
s,t,u,v,S,U
(
P ([e?e] | e) ? ?3(e?) ? ?(e) ? ?1(e)
)
=
?
s,S,u,U
?
?P ([e?e] | e) ? ?3(e?) ?
?
?
?
t,v
? ? ?1
?
?
?
?
=
?
s,S,u,U
(
P ([e?e] | e) ? ?3(e?) ? ?+3 (e)
)
which can be computed in O(n6) as long as we
have ?+3 ready in a table. Overall we can do theinside-outside algorithm for the bilexical ITG in
O(n7), by reducing a factor of n through interme-
diate DP.
The entire trick can be understood very clearly
if we imagine the bilexical rules are unary rules
that are applied on top of the non-head con-
stituents to reduce it to a virtual lexical constituent
(a hook) covering the same subcell while sharing
the head word with the head constituent. However,
if we build hooks looking for all words in a sen-
tence whenever a complete constituent is added to
the chart, we will build many hooks that are never
used, considering that the words outside of larger
cells are fewer and pruning might further reduce
the possible outside words. Blind guessing of what
might appear outside of the current cell will off-
set the saving we can achieve. Instead of actively
building hooks, which are intermediate results, we
can build them only when we need them and then
cache them for future use. So the construction of
the hooks will be invoked by the heads when the
heads need to combine with adjacent cells.
3.1 Pruning and Smoothing
We apply one of the pruning techniques used in
Zhang and Gildea (2005). The technique is gen-
eral enough to be applicable to any parsing algo-
rithm over bitext cells. It is called tic-tac-toe prun-
ing since it involves an estimate of both the inside
probability of the cell (how likely the words within
the box in both dimensions are to align) and the
outside probability (how likely the words outside
the box in both dimensions are to align). By scor-
ing the bitext cells and throwing away the bad cells
that fall out of a beam, it can reduce over 70% of
O(n4) cells using 10?5 as the beam ratio for sen-
tences up to 25 words in the experiments, without
harming alignment error rate, at least for the un-
lexicalized ITG.
The hook trick reduces the complexity of bilex-
ical ITG from O(n8) to O(n7). With the tic-tac-
toe pruning reducing the number of bitext cells to
work with, also due to the reason that the grammar
constant is very small for ITG. the parsing algo-
rithm runs with an acceptable speed,
The probabilistic model has lots of parameters
of word pairs. Namely, there are O(|V |2) de-
pendency probabilities and O(|V ||T |) translation
probabilities, where |V | is the size of English vo-
cabulary and |T | is the size of the foreign lan-
guage vocabulary. The translation probabilities of
P (f |X(e)) are backed off to a uniform distribu-
tion. We let the bilexical dependency probabilities
back off to uni-lexical dependencies in the follow-
ing forms:
P ([Y (?) Z(e?)] | X(?))
P ([Y (e?) Z(?)] | X(?))
P (?Y (?) Z(e?)? | X(?))
P (?Y (e?) Z(?)? | X(?))
957
 0
 100
 200
 300
 400
 500
 600
 700
 0  5  10  15  20
se
co
n
ds
sentence length
without-hook
with-hook
 0
 20
 40
 60
 80
 100
 120
 140
 0  5  10  15  20  25
se
co
n
ds
sentence length
without-hook
with-hook
(a) (b)
Figure 3: Speedup for EM by the Hook Trick. (a) is without pruning. In (b), we apply pruning on the
bitext cells before parsing begins.
The two levels of distributions are interpolated
using a technique inspired by Witten-Bell smooth-
ing (Chen and Goodman, 1996). We use the ex-
pected count of the left hand side lexical nontermi-
nal to adjust the weight for the EM-trained bilexi-
cal probability. For example,
P ([Y (e) Z(e?)] | X(e)) =
(1 ? ?)PEM ([Y (e) Z(e?)] | X(e))
+ ?P ([Y (?) Z(e?)] | X(?))
where
? = 1/(1 + Expected Counts(X(e)))
4 Experiments
First of all, we are interested in finding out how
much speedup can be achieved by doing the hook
trick for EM. We implemented both versions in
C++ and turned off pruning for both. We ran the
two inside-outside parsing algorithms on a small
test set of 46 sentence pairs that are no longer than
25 words in both languages. Then we put the re-
sults into buckets of (1 ? 4), (5 ? 9), (10 ? 14),
(15?19), and (20?24) according to the maximum
length of two sentences in each pair and took av-
erages of these timing results. Figure 3 (a) shows
clearly that as the sentences get longer the hook
trick is helping more and more. We also tried to
turn on pruning for both, which is the normal con-
dition for the parsers. Both are much faster due
to the effectiveness of pruning. The speedup ratio
is lower because the hooks will less often be used
again since many cells are pruned away. Figure 3
(b) shows the speedup curve in this situation.
We trained both the unlexicalized and the lex-
icalized ITGs on a parallel corpus of Chinese-
English newswire text. The Chinese data were
automatically segmented into tokens, and English
capitalization was retained. We replaced words
occurring only once with an unknown word token,
resulting in a Chinese vocabulary of 23,783 words
and an English vocabulary of 27,075 words.
We did two types of comparisons. In the first
comparison, we measured the performance of five
word aligners, including IBM models, ITG, the
lexical ITG (LITG) of Zhang and Gildea (2005),
and our bilexical ITG (BLITG), on a hand-aligned
bilingual corpus. All the models were trained us-
ing the same amount of data. We ran the ex-
periments on sentences up to 25 words long in
both languages. The resulting training corpus had
18,773 sentence pairs with a total of 276,113 Chi-
nese words and 315,415 English words.
For scoring the Viterbi alignments of each sys-
tem against gold-standard annotated alignments,
we use the alignment error rate (AER) of Och
and Ney (2000), which measures agreement at the
level of pairs of words:
AER = 1 ? |A ? GP | + |A ? GS ||A| + |GS |
where A is the set of word pairs aligned by the
automatic system, GS is the set marked in thegold standard as ?sure?, and GP is the set markedas ?possible? (including the ?sure? pairs). In our
Chinese-English data, only one type of alignment
was marked, meaning that GP = GS .
In our hand-aligned data, 47 sentence pairs are
no longer than 25 words in either language and
were used to evaluate the aligners.
A separate development set of hand-aligned
sentence pairs was used to control overfitting. The
subset of up to 25 words in both languages was
used. We chose the number of iterations for EM
958
Alignment
Precision Recall Error Rate
IBM-1 .56 .42 .52
IBM-4 .67 .43 .47
ITG .68 .52 .41
LITG .69 .51 .41
BLITG .68 .51 .42
Dependency
Precision Recall Error Rate
ITG-lh .11 .11 .89
ITG-rh .22 .22 .78
LITG .13 .12 .88
BLITG .24 .22 .77
Table 1: Bilingual alignment and English dependency results on Chinese-English corpus (? 25 words on
both sides). LITG stands for the cross-language Lexicalized ITG. BLITG is the within-English Bilexical
ITG. ITG-lh is ITG with left-head assumption on English. ITG-rh is with right-head assumption.
Precision Recall AER
ITG .59 .60 .41
LITG .60 .57 .41
BLITG .58 .55 .44
Precision Recall DER
ITG-rh .23 .23 .77
LITG .11 .11 .89
BLITG .24 .24 .76
Table 2: Alignment and dependency results on a larger Chinese-English corpus.
training as the turning point of AER on the de-
velopment data set. The unlexicalized ITG was
trained for 3 iterations. LITG was trained for only
1 iteration, partly because it was initialized with
fully trained ITG parameters. BLITG was trained
for 3 iterations.
For comparison, we also included the results
from IBM Model 1 and Model 4. The numbers
of iterations for the training of the IBM models
were also chosen to be the turning points of AER
changing on the development data set.
We also want to know whether or not BLITG
can model dependencies better than LITG. For
this purpose, we also used the AER measurement,
since the goal is still getting higher precision/recall
for a set of recovered word links, although the de-
pendency word links are within one language. For
this reason, we rename AER to Dependency Error
Rate. Table 1(right) is the dependency results on
English side of the test data set. The dependency
results on Chinese are similar.
The gold standard dependencies were extracted
from Collins? parser output on the sentences. The
LITG and BLITG dependencies were extracted
from the Viterbi synchronous trees by following
the head words.
For comparison, we also included two base-line
results. ITG-lh is unlexicalized ITG with left-head
assumption, meaning the head words always come
from the left branches. ITG-rh is ITG with right-
head assumption.
To make more confident conclusions, we also
did tests on a larger hand-aligned data set used in
Liu et al (2005). We used 165 sentence pairs that
are up to 25 words in length on both sides.
5 Discussion
The BLITG model has two components, namely
the dependency model on the upper levels of the
tree structure and the word-level translation model
at the bottom. We hope that the two components
will mutually improve one another. The current
experiments indicate clearly that the word level
alignment does help inducing dependency struc-
tures on both sides. The precision and recall on
the dependency retrieval sub-task are almost dou-
bled for both languages from LITG which only
has a kind of uni-lexical dependency in each lan-
guage. Although 20% is a low number, given the
fact that the dependencies are learned basically
through contrasting sentences in two languages,
the result is encouraging. The results slightly im-
prove over ITG with right-head assumption for
English, which is based on linguistic insight. Our
results also echo the findings of Kuhn (2004).
They found that based on the guidance of word
alignment between English and multiple other lan-
guages, a modified EM training for PCFG on En-
glish can bootstrap a more accurate monolingual
probabilistic parser. Figure 4 is an example of the
dependency tree on the English side from the out-
put of BLITG, comparing against the parser out-
put.
We did not find that the feedback from the de-
959
are
accomplishments
Economic reform
bright
for
cities
China
?s
14 open frontier
accomplishments
Economic reform frontier
open cities 14
bright
for are
?s
China
Figure 4: Dependency tree extracted from parser output vs. Viterbi dependency tree from BLITG
pendencies help alignment. To get the reasons, we
need further and deeper analysis. One might guess
that the dependencies are modeled but are not yet
strong and good enough given the amount of train-
ing data. Since the training algorithm EM has the
problem of local maxima, we might also need to
adjust the training algorithm to obtain good pa-
rameters for the alignment task. Initializing the
model with good dependency parameters is a pos-
sible adjustment. We would also like to point out
that alignment task is simpler than decoding where
a stronger component of reordering is required to
produce a fluent English sentence. Investigating
the impact of bilexical dependencies on decoding
is our future work.
Acknowledgments This work was supported
by NSF ITR IIS-09325646 and NSF ITR IIS-
0428020.
References
Albert V. Aho and Jeffery D. Ullman. 1972. The
Theory of Parsing, Translation, and Compiling, vol-
ume 1. Prentice-Hall, Englewood Cliffs, NJ.
Hiyan Alshawi, Srinivas Bangalore, and Shona Dou-
glas. 2000. Learning dependency translation mod-
els as collections of finite state head transducers.
Computational Linguistics, 26(1):45?60.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Con-
ference of the Association for Computational Lin-
guistics (ACL-96), pages 310?318, Santa Cruz, CA.
ACL.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Conference of the As-
sociation for Computational Linguistics (ACL-05),
pages 263?270, Ann Arbor, Michigan.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In 37th Annual Meeting of the
Association for Computational Linguistics.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proceedings of the 42nd An-
nual Conference of the Association for Computa-
tional Linguistics (ACL-04).
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings
of the 43rd Annual Conference of the Association
for Computational Linguistics (ACL-05), Ann Ar-
bor, Michigan.
I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of the 2003 Meet-
ing of the North American chapter of the Associ-
ation for Computational Linguistics (NAACL-03),
Edmonton.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Conference of the Association for Com-
putational Linguistics (ACL-00), pages 440?447,
Hong Kong, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of the 39th Annual Conference of the Association
for Computational Linguistics (ACL-01), Toulouse,
France.
Richard Zens and Hermann Ney. 2003. A compara-
tive study on reordering constraints in statistical ma-
chine translation. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics, Sapporo, Japan.
Hao Zhang and Daniel Gildea. 2004. Syntax-based
alignment: Supervised or unsupervised? In Pro-
ceedings of the 20th International Conference on
Computational Linguistics (COLING-04), Geneva,
Switzerland, August.
Hao Zhang and Daniel Gildea. 2005. Stochastic lex-
icalized inversion transduction grammar for align-
ment. In Proceedings of the 43rd Annual Confer-
ence of the Association for Computational Linguis-
tics (ACL-05), Ann Arbor, MI.
960
Proceedings of ACL-08: HLT, pages 97?105,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Bayesian Learning of Non-compositional Phrases with Synchronous Parsing
Hao Zhang
Computer Science Department
University of Rochester
Rochester, NY 14627
zhanghao@cs.rochester.edu
Chris Quirk
Microsoft Research
One Microsoft Way
Redmond, WA 98052 USA
chrisq@microsoft.com
Robert C. Moore
Microsoft Research
One Microsoft Way
Redmond, WA 98052 USA
bobmoore@microsoft.com
Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
gildea@cs.rochester.edu
Abstract
We combine the strengths of Bayesian mod-
eling and synchronous grammar in unsu-
pervised learning of basic translation phrase
pairs. The structured space of a synchronous
grammar is a natural fit for phrase pair proba-
bility estimation, though the search space can
be prohibitively large. Therefore we explore
efficient algorithms for pruning this space that
lead to empirically effective results. Incorpo-
rating a sparse prior using Variational Bayes,
biases the models toward generalizable, parsi-
monious parameter sets, leading to significant
improvements in word alignment. This pref-
erence for sparse solutions together with ef-
fective pruning methods forms a phrase align-
ment regimen that produces better end-to-end
translations than standard word alignment ap-
proaches.
1 Introduction
Most state-of-the-art statistical machine transla-
tion systems are based on large phrase tables ex-
tracted from parallel text using word-level align-
ments. These word-level alignments are most of-
ten obtained using Expectation Maximization on the
conditional generative models of Brown et al (1993)
and Vogel et al (1996). As these word-level align-
ment models restrict the word alignment complex-
ity by requiring each target word to align to zero
or one source words, results are improved by align-
ing both source-to-target as well as target-to-source,
then heuristically combining these alignments. Fi-
nally, the set of phrases consistent with the word
alignments are extracted from every sentence pair;
these form the basis of the decoding process. While
this approach has been very successful, poor word-
level alignments are nonetheless a common source
of error in machine translation systems.
A natural solution to several of these issues is
unite the word-level and phrase-level models into
one learning procedure. Ideally, such a procedure
would remedy the deficiencies of word-level align-
ment models, including the strong restrictions on
the form of the alignment, and the strong inde-
pendence assumption between words. Furthermore
it would obviate the need for heuristic combina-
tion of word alignments. A unified procedure may
also improve the identification of non-compositional
phrasal translations, and the attachment decisions
for unaligned words.
In this direction, Expectation Maximization at
the phrase level was proposed by Marcu and Wong
(2002), who, however, experienced two major dif-
ficulties: computational complexity and controlling
overfitting. Computational complexity arises from
the exponentially large number of decompositions
of a sentence pair into phrase pairs; overfitting is a
problem because as EM attempts to maximize the
likelihood of its training data, it prefers to directly
explain a sentence pair with a single phrase pair.
In this paper, we attempt to address these two is-
sues in order to apply EM above the word level.
97
We attack computational complexity by adopting
the polynomial-time Inversion Transduction Gram-
mar framework, and by only learning small non-
compositional phrases. We address the tendency of
EM to overfit by using Bayesian methods, where
sparse priors assign greater mass to parameter vec-
tors with fewer non-zero values therefore favoring
shorter, more frequent phrases. We test our model
by extracting longer phrases from our model?s align-
ments using traditional phrase extraction, and find
that a phrase table based on our system improves MT
results over a phrase table extracted from traditional
word-level alignments.
2 Phrasal Inversion Transduction
Grammar
We use a phrasal extension of Inversion Transduc-
tion Grammar (Wu, 1997) as the generative frame-
work. Our ITG has two nonterminals: X and
C, where X represents compositional phrase pairs
that can have recursive structures and C is the pre-
terminal over terminal phrase pairs. There are three
rules with X on the left-hand side:
X ? [X X],
X ? ?X X?,
X ? C.
The first two rules are the straight rule and in-
verted rule respectively. They split the left-hand side
constituent which represents a phrase pair into two
smaller phrase pairs on the right-hand side and order
them according to one of the two possible permuta-
tions. The rewriting process continues until the third
rule is invoked. C is our unique pre-terminal for
generating terminal multi-word pairs:
C ? e/f .
We parameterize our probabilistic model in the
manner of a PCFG: we associate a multinomial dis-
tribution with each nonterminal, where each out-
come in this distribution corresponds to an expan-
sion of that nonterminal. Specifically, we place one
multinomial distribution ?X over the three expan-
sions of the nonterminalX , and another multinomial
distribution ?C over the expansions of C. Thus, the
parameters in our model can be listed as
?X = (P??, P[], PC),
where P?? is for the inverted rule, P[] for the straight
rule, PC for the third rule, satisfyingP??+P[]+PC =
1, and
?C = (P (e/f), P (e?/f ?), . . . ),
where
?
e/f P (e/f) = 1 is a multinomial distribu-
tion over phrase pairs.
This is our model in a nutshell. We can train
this model using a two-dimensional extension of the
inside-outside algorithm on bilingual data, assuming
every phrase pair that can appear as a leaf in a parse
tree of the grammar a valid candidate. However, it is
easy to show that the maximum likelihood training
will lead to the saturated solution where PC = 1 ?
each sentence pair is generated by a single phrase
spanning the whole sentence. From the computa-
tional point of view, the full EM algorithm runs in
O(n6) where n is the average length of the two in-
put sentences, which is too slow in practice.
The key is to control the number of parameters,
and therefore the size of the set of candidate phrases.
We deal with this problem in two directions. First
we change the objective function by incorporating
a prior over the phrasal parameters. This has the
effect of preferring parameter vectors in ?C with
fewer non-zero values. Our second approach was
to constrain the search space using simpler align-
ment models, which has the further benefit of signif-
icantly speeding up training. First we train a lower
level word alignment model, then we place hard con-
straints on the phrasal alignment space using confi-
dent word links from this simpler model. Combining
the two approaches, we have a staged training pro-
cedure going from the simplest unconstrained word
based model to a constrained Bayesian word-level
ITG model, and finally proceeding to a constrained
Bayesian phrasal model.
3 Variational Bayes for ITG
Goldwater and Griffiths (2007) and Johnson (2007)
show that modifying an HMM to include a sparse
prior over its parameters and using Bayesian esti-
mation leads to improved accuracy for unsupervised
part-of-speech tagging. In this section, we describe
a Bayesian estimator for ITG: we select parame-
ters that optimize the probability of the data given
a prior. The traditional estimation method for word
98
alignment models is the EM algorithm (Brown et
al., 1993) which iteratively updates parameters to
maximize the likelihood of the data. The drawback
of maximum likelihood is obvious for phrase-based
models. If we do not put any constraint on the dis-
tribution of phrases, EM overfits the data by mem-
orizing every sentence pair. A sparse prior over a
multinomial distribution such as the distribution of
phrase pairs may bias the estimator toward skewed
distributions that generalize better. In the context of
phrasal models, this means learning the more repre-
sentative phrases in the space of all possible phrases.
The Dirichlet distribution, which is parameter-
ized by a vector of real values often interpreted as
pseudo-counts, is a natural choice for the prior, for
two main reasons. First, the Dirichlet is conjugate
to the multinomial distribution, meaning that if we
select a Dirichlet prior and a multinomial likelihood
function, the posterior distribution will again be a
Dirichlet. This makes parameter estimation quite
simple. Second, Dirichlet distributions with small,
non-zero parameters place more probability mass on
multinomials on the edges or faces of the probabil-
ity simplex, distributions with fewer non-zero pa-
rameters. Starting from the model from Section 2,
we propose the following Bayesian extension, where
A ? Dir(B) means the random variable A is dis-
tributed according to a Dirichlet with parameter B:
?X | ?X ? Dir(?X),
?C | ?C ? Dir(?C),
[X X]
?X X?
C
X ? Multi(?X),
e/f | C ? Multi(?C).
The parameters ?X and ?C control the sparsity of
the two distributions in our model. One is the distri-
bution of the three possible branching choices. The
other is the distribution of the phrase pairs. ?C is
crucial, since the multinomial it is controlling has a
high dimension. By adjusting ?C to a very small
number, we hope to place more posterior mass on
parsimonious solutions with fewer but more confi-
dent and general phrase pairs.
Having defined the Bayesian model, it remains
to decide the inference procedure. We chose Vari-
ational Bayes, for its procedural similarity to EM
and ease of implementation. Another potential op-
tion would be Gibbs sampling (or some other sam-
pling technique). However, in experiments in un-
supervised POS tag learning using HMM structured
models, Johnson (2007) shows that VB is more ef-
fective than Gibbs sampling in approaching distribu-
tions that agree with the Zipf?s law, which is promi-
nent in natural languages.
Kurihara and Sato (2006) describe VB for PCFGs,
showing the only need is to change the M step of
the EM algorithm. As in the case of maximum like-
lihood estimation, Bayesian estimation for ITGs is
very similar to PCFGs, which follows due to the
strong isomorphism between the two models. Spe-
cific to our ITG case, the M step becomes:
P? (l+1)[] =
exp(?(E(X ? [X X]) + ?X))
exp(?(E(X) + s?X))
,
P? (l+1)?? =
exp(?(E(X ? ?X X?) + ?X))
exp(?(E(X) + s?X))
,
P? (l+1)C =
exp(?(E(X ? C) + ?X))
exp(?(E(X) + s?X))
,
P? (l+1)(e/f) = exp(?(E(e/f) + ?C))exp(?(E(C) +m?C))
,
where ? is the digamma function (Beal, 2003), s =
3 is the number of right-hand-sides for X , and m is
the number of observed phrase pairs in the data. The
sole difference between EM and VB with a sparse
prior ? is that the raw fractional counts c are re-
placed by exp(?(c + ?)), an operation that resem-
bles smoothing. As pointed out by Johnson (2007),
in effect this expression adds to c a small value that
asymptotically approaches ? ? 0.5 as c approaches
?, and 0 as c approaches 0. For small values of
? the net effect is the opposite of typical smooth-
ing, since it tends to redistribute probably mass away
from unlikely events onto more likely ones.
4 Bitext Pruning Strategy
ITG is slow mainly because it considers every pair of
spans in two sentences as a possible chart element.
In reality, the set of useful chart elements is much
99
smaller than the possible scriptO(n4), where n is
the average sentence length. Pruning the span pairs
(bitext cells) that can participate in a tree (either as
terminals or non-terminals) serves to not only speed
up ITG parsing, but also to provide a kind of ini-
tialization hint to the training procedures, encourag-
ing it to focus on promising regions of the alignment
space.
Given a bitext cell defined by the four boundary
indices (i, j, l,m) as shown in Figure 1a, we prune
based on a figure of merit V (i, j, l,m) approximat-
ing the utility of that cell in a full ITG parse. The
figure of merit considers the Model 1 scores of not
only the words inside a given cell, but also all the
words not included in the source and target spans, as
in Moore (2003) and Vogel (2005). Like Zhang and
Gildea (2005), it is used to prune bitext cells rather
than score phrases. The total score is the product of
the Model 1 probabilities for each column; ?inside?
columns in the range [l,m] are scored according to
the sum (or maximum) of Model 1 probabilities for
[i, j], and ?outside? columns use the sum (or maxi-
mum) of all probabilities not in the range [i, j].
Our pruning differs from Zhang and Gildea
(2005) in two major ways. First, we perform prun-
ing using both directions of the IBM Model 1 scores;
instead of a single figure of merit V , we have two:
VF and VB . Only those spans that pass the prun-
ing threshold in both directions are kept. Second,
we allow whole spans to be pruned. The figure of
merit for a span is VF (i, j) = maxl,m VF (i, j, l,m).
Only spans that are within some threshold of the un-
restricted Model 1 scores VF and VB are kept:
VF (i, j)
VF
? ?s and
VB(l,m)
VB
? ?s.
Amongst those spans retained by this first threshold,
we keep only those bitext cells satisfying both
VF (i, j, l,m)
VF (i, j)
? ?b and
VB(i, j, l,m)
VB(l,m)
? ?b.
4.1 Fast Tic-tac-toe Pruning
The tic-tac-toe pruning algorithm (Zhang and
Gildea, 2005) uses dynamic programming to com-
pute the product of inside and outside scores for
all cells in O(n4) time. However, even this can be
slow for large values of n. Therefore we describe an
Figure 1: (a) shows the original tic-tac-toe score for a
bitext cell (i, j, l,m). (b) demonstrates the finite state
representation using the machine in (c), assuming a fixed
source span (i, j).
improved algorithm with best case n3 performance.
Although the worst case performance is also O(n4),
in practice it is significantly faster.
To begin, let us restrict our attention to the for-
ward direction for a fixed source span (i, j). Prun-
ing bitext spans and cells requires VF (i, j), the score
of the best bitext cell within a given span, as well
as all cells within a given threshold of that best
score. For a fixed i and j, we need to search over
the starting and ending points l and m of the in-
side region. Note that there is an isomorphism be-
tween the set of spans and a simple finite state ma-
chine: any span (l,m) can be represented by a se-
quence of l OUTSIDE columns, followed bym?l+1
INSIDE columns, followed by n ? m + 1 OUT-
SIDE columns. This simple machine has the re-
stricted form described in Figure 1c: it has three
states, L, M , and R; each transition generates ei-
ther an OUTSIDE column O or an INSIDE column
I . The cost of generating an OUTSIDE at posi-
tion a is O(a) = P (ta|NULL) +
?
b 6?[i,j] P (ta|sb);
likewise the cost of generating an INSIDE column
is I(a) = P (ta|NULL) +
?
b?[i,j] P (ta|sb), with
100
O(0) = O(n+ 1) = 1 and I(0) = I(n+ 1) = 0.
Directly computing O and I would take time
O(n2) for each source span, leading to an overall
runtime of O(n4). Luckily there are faster ways to
find the inside and outside scores. First we can pre-
compute following arrays in O(n2) time and space:
pre[0, l] := P (tl|NULL)
pre[i, l] := pre[i? 1, l] + P (tl|si)
suf[n+ 1, l] := 0
suf[i, l] := suf[i+ 1, l] + P (tl|si)
Then for any (i, j), O(a) = P (ta|NULL) +
?
b 6?[i,j] P (ta|sb) = pre[i ? 1, a] + suf[j + 1, a].
I(a) can be incrementally updated as the source
span varies: when i = j, I(a) = P (ta|NULL) +
P (ta|si). As j is incremented, we add P (ta|sj) to
I(a). Thus we have linear time updates for O and I .
We can then find the best scoring sequence using
the familiar Viterbi algorithm. Let ?[a, ?] be the cost
of the best scoring sequence ending at in state ? at
time a:
?[0, ?] := 1 if ? = L; 0 otherwise
?[a, L] := ?[a? 1, L] ?O(a)
?[a,M ] := max
??L,M
{?[a? 1, ?]} ? I(a)
?[a,R] := max
??M,R
{?[a? 1, ?]} ?O(a)
Then VF (i, j) = ?[n + 1, R], using the isomor-
phism between state sequences and spans. This lin-
ear time algorithm allows us to compute span prun-
ing in O(n3) time. The same algorithm may be
performed using the backward figure of merit after
transposing rows and columns.
Having cast the problem in terms of finite state au-
tomata, we can use finite state algorithms for prun-
ing. For instance, fixing a source span we can enu-
merate the target spans in decreasing order by score
(Soong and Huang, 1991), stopping once we en-
counter the first span below threshold. In practice
the overhead of maintaining the priority queue out-
weighs any benefit, as seen in Figure 2.
An alternate approach that avoids this overhead is
to enumerate spans by position. Note that ?[m,R] ?
?n
a=m+1O(a) is within threshold iff there is a
span with right boundary m? < m within thresh-
old. Furthermore if ?[m,M ] ? ?na=m+1O(a) is
 0
 100
 200
 300
 400
 500
 600
 700
 800
 900
 10  20  30  40  50
Pr
un
in
g 
tim
e 
(th
ou
san
ds
 of
 se
co
nd
s)
Average sentence length
Baseline
k-best
Fast
Figure 2: Speed comparison of the O(n4) tic-tac-toe
pruning algorithm, the A* top-x algorithm, and the fast
tic-tac-toe pruning. All produce the same set of bitext
cells, those within threshold of the best bitext cell.
within threshold, thenm is the right boundary within
threshold. Using these facts, we can gradually
sweep the right boundary m from n toward 1 until
the first condition fails to hold. For each value where
the second condition holds, we pause to search for
the set of left boundaries within threshold.
Likewise for the left edge, ?[l,M ] ??ma=l+1 I(a) ?
?n
a=m+1O(a) is within threshold iff there is some
l? < l identifying a span (l?,m) within threshold.
Finally if V (i, j, l,m) = ?[l ? 1, L] ? ?ma=l I(a) ?
?n
a=m+1O(a) is within threshold, then (i, j, l,m)
is a bitext cell within threshold. For right edges that
are known to be within threshold, we can sweep the
left edges leftward until the first condition no longer
holds, keeping only those spans for which the sec-
ond condition holds.
The filtering algorithm behaves extremely well.
Although the worst case runtime is still O(n4), the
best case has improved to n3; empirically it seems to
significantly reduce the amount of time spent explor-
ing spans. Figure 2 compares the speed of the fast
tic-tac-toe algorithm against the algorithm in Zhang
and Gildea (2005).
101
Figure 3: Example output from the ITG using non-compositional phrases. (a) is the Viterbi alignment from the word-
based ITG. The shaded regions indicate phrasal alignments that are allowed by the non-compositional constraint; all
other phrasal alignments will not be considered. (b) is the Viterbi alignment from the phrasal ITG, with the multi-word
alignments highlighted.
5 Bootstrapping Phrasal ITG from
Word-based ITG
This section introduces a technique that bootstraps
candidate phrase pairs for phrase-based ITG from
word-based ITG Viterbi alignments. The word-
based ITG uses the same expansions for the non-
terminal X , but the expansions of C are limited to
generate only 1-1, 1-0, and 0-1 alignments:
C ? e/f,
C ? e/?,
C ? ?/f
where ? indicates that no word was generated.
Broadly speaking, the goal of this section is the same
as the previous section, namely, to limit the set of
phrase pairs that needs to be considered in the train-
ing process. The tic-tac-toe pruning relies on IBM
model 1 for scoring a given aligned area. In this
part, we use word-based ITG alignments as anchor
points in the alignment space to pin down the poten-
tial phrases. The scope of iterative phrasal ITG train-
ing, therefore, is limited to determining the bound-
aries of the phrases anchored on the given one-to-
one word alignments.
The heuristic method is based on the Non-
Compositional Constraint of Cherry and Lin (2007).
Cherry and Lin (2007) use GIZA++ intersections
which have high precision as anchor points in the
bitext space to constraint ITG phrases. We use ITG
Viterbi alignments instead. The benefit is two-fold.
First of all, we do not have to run a GIZA++ aligner.
Second, we do not need to worry about non-ITG
word alignments, such as the (2, 4, 1, 3) permutation
patterns. GIZA++ does not limit the set of permu-
tations allowed during translation, so it can produce
permutations that are not reachable using an ITG.
Formally, given a word-based ITG alignment, the
bootstrapping algorithm finds all the phrase pairs
according to the definition of Och and Ney (2004)
and Chiang (2005) with the additional constraint
that each phrase pair contains at most one word
link. Mathematically, let e(i, j) count the number of
word links that are emitted from the substring ei...j ,
and f(l,m) count the number of word links emit-
ted from the substring fl...m. The non-compositional
phrase pairs satisfy
e(i, j) = f(l,m) ? 1.
Figure 3 (a) shows all possible non-compositional
phrases given the Viterbi word alignment of the ex-
ample sentence pair.
6 Summary of the Pipeline
We summarize the pipeline of our system, demon-
strating the interactions between the three main con-
tributions of this paper: Variational Bayes, tic-tac-
toe pruning, and word-to-phrase bootstrapping. We
102
start from sentence-aligned bilingual data and run
IBM Model 1 in both directions to obtain two trans-
lation tables. Then we use the efficient bidirectional
tic-tac-toe pruning to prune the bitext space within
each of the sentence pairs; ITG parsing will be car-
ried out on only this this sparse set of bitext cells.
The first stage of training is word-based ITG, us-
ing the standard iterative training procedure, except
VB replaces EM to focus on a sparse prior. Af-
ter several training iterations, we obtain the Viterbi
alignments on the training data according to the fi-
nal model. Now we transition into the second stage
? the phrasal training. Before the training starts,
we apply the non-compositional constraints over the
pruned bitext space to further constrain the space
of phrase pairs. Finally, we run phrasal ITG itera-
tive training using VB for a certain number of itera-
tions. In the end, a Viterbi pass for the phrasal ITG is
executed to produce the non-compositional phrasal
alignments. From this alignment, phrase pairs are
extracted in the usual manner, and a phrase-based
translation system is trained.
7 Experiments
The training data was a subset of 175K sentence
pairs from the NIST Chinese-English training data,
automatically selected to maximize character-level
overlap with the source side of the test data. We put
a length limit of 35 on both sides, producing a train-
ing set of 141K sentence pairs. 500 Chinese-English
pairs from this set were manually aligned and used
as a gold standard.
7.1 Word Alignment Evaluation
First, using evaluations of alignment quality, we
demonstrate the effectiveness of VB over EM, and
explore the effect of the prior.
Figure 4 examines the difference between EM and
VB with varying sparse priors for the word-based
model of ITG on the 500 sentence pairs, both af-
ter 10 iterations of training. Using EM, because of
overfitting, AER drops first and increases again as
the number of iterations varies from 1 to 10. The
lowest AER using EM is achieved after the second
iteration, which is .40. At iteration 10, AER for EM
increases to .42. On the other hand, using VB, AER
decreases monotonically over the 10 iterations and
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 1e-009  1e-006  0.001  1
A
ER
Prior value
VB
EM
Figure 4: AER drops as ?C approaches zero; a more
sparse solution leads to better results.
stabilizes at iteration 10. When ?C is 1e ? 9, VB
gets AER close to .35 at iteration 10.
As we increase the bias toward sparsity, the AER
decreases, following a long slow plateau. Although
the magnitude of improvement is not large, the trend
is encouraging.
These experiments also indicate that a very sparse
prior is needed for machine translation tasks. Un-
like Johnson (2007), who found optimal perfor-
mance when ? was approximately 10?4, we ob-
served monotonic increases in performance as ?
dropped. The dimensionality of this MT problem is
significantly larger than that of the sequence prob-
lem, though, therefore it may take a stronger push
from the prior to achieve the desired result.
7.2 End-to-end Evaluation
Given an unlimited amount of time, we would tune
the prior to maximize end-to-end performance, us-
ing an objective function such as BLEU. Unfortu-
nately these experiments are very slow. Since we
observed monotonic increases in alignment perfor-
mance with smaller values of ?C , we simply fixed
the prior at a very small value (10?100) for all trans-
lation experiments. We do compare VB against EM
in terms of final BLEU scores in the translation ex-
periments to ensure that this sparse prior has a sig-
103
nificant impact on the output.
We also trained a baseline model with GIZA++
(Och and Ney, 2003) following a regimen of 5 it-
erations of Model 1, 5 iterations of HMM, and 5
iterations of Model 4. We computed Chinese-to-
English and English-to-Chinese word translation ta-
bles using five iterations of Model 1. These val-
ues were used to perform tic-tac-toe pruning with
?b = 1 ? 10?3 and ?s = 1 ? 10?6. Over the pruned
charts, we ran 10 iterations of word-based ITG using
EM or VB. The charts were then pruned further by
applying the non-compositional constraint from the
Viterbi alignment links of that model. Finally we ran
10 iterations of phrase-based ITG over the residual
charts, using EM or VB, and extracted the Viterbi
alignments.
For translation, we used the standard phrasal de-
coding approach, based on a re-implementation of
the Pharaoh system (Koehn, 2004). The output of
the word alignment systems (GIZA++ or ITG) were
fed to a standard phrase extraction procedure that
extracted all phrases of length up to 7 and esti-
mated the conditional probabilities of source given
target and target given source using relative fre-
quencies. Thus our phrasal ITG learns only the
minimal non-compositional phrases; the standard
phrase-extraction algorithm learns larger combina-
tions of these minimal units. In addition the phrases
were annotated with lexical weights using the IBM
Model 1 tables. The decoder also used a trigram lan-
guage model trained on the target side of the training
data, as well as word count, phrase count, and distor-
tion penalty features. Minimum Error Rate training
(Och, 2003) over BLEU was used to optimize the
weights for each of these models over the develop-
ment test data.
We used the NIST 2002 evaluation datasets for
tuning and evaluation; the 10-reference develop-
ment set was used for minimum error rate training,
and the 4-reference test set was used for evaluation.
We trained several phrasal translation systems, vary-
ing only the word alignment (or phrasal alignment)
method.
Table 1 compares the four systems: the GIZA++
baseline, the ITG word-based model, the ITG multi-
word model using EM training, and the ITG multi-
word model using VB training. ITG-mwm-VB is
our best model. We see an improvement of nearly
Development Test
GIZA++ 37.46 28.24
ITG-word 35.47 26.55
ITG-mwm (VB) 39.21 29.02
ITG-mwm (EM) 39.15 28.47
Table 1: Translation results on Chinese-English, using
the subset of training data (141K sentence pairs) that have
length limit 35 on both sides. (No length limit in transla-
tion. )
2 points dev set and nearly 1 point of improvement
on the test set. We also observe the consistent supe-
riority of VB over EM. The gain is especially large
on the test data set, indicating VB is less prone to
overfitting.
8 Conclusion
We have presented an improved and more efficient
method of estimating phrase pairs directly. By both
changing the objective function to include a bias
toward sparser models and improving the pruning
techniques and efficiency, we achieve significant
gains on test data with practical speed. In addition,
these gains were shown without resorting to external
models, such as GIZA++. We have shown that VB
is both practical and effective for use in MT models.
However, our best system does not apply VB to a
single probability model, as we found an apprecia-
ble benefit from bootstrapping each model from sim-
pler models, much as the IBM word alignment mod-
els are usually trained in succession. We find that
VB alone is not sufficient to counteract the tendency
of EM to prefer analyses with smaller trees using
fewer rules and longer phrases. Both the tic-tac-toe
pruning and the non-compositional constraint ad-
dress this problem by reducing the space of possible
phrase pairs. On top of these hard constraints, the
sparse prior of VB helps make the model less prone
to overfitting to infrequent phrase pairs, and thus
improves the quality of the phrase pairs the model
learns.
Acknowledgments This work was done while the
first author was at Microsoft Research; thanks to Xi-
aodong He, Mark Johnson, and Kristina Toutanova.
The last author was supported by NSF IIS-0546554.
104
References
Matthew Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Gatsby
Computational Neuroscience Unit, University College
London.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
In Proceedings of SSST, NAACL-HLT 2007 / AMTA
Workshop on Syntax and Structure in Statistical Trans-
lation, pages 17?24, Rochester, New York, April. As-
sociation for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270, Ann Arbor, Michigan, USA.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
744?751, Prague, Czech Republic, June. Association
for Computational Linguistics.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 296?305.
Philipp Koehn. 2004. Pharaoh: A beam search de-
coder for phrase-based statistical machine translation
models. In Proceedings of the 6th Conference of the
Association for Machine Translation in the Americas
(AMTA), pages 115?124, Washington, USA, Septem-
ber.
Kenichi Kurihara and Taisuke Sato. 2006. Variational
bayesian grammar induction for natural language. In
International Colloquium on Grammatical Inference,
pages 84?96, Tokyo, Japan.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In 2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Robert C. Moore. 2003. Learning translations of named-
entity phrases from parallel corpora. In Proceedings
of EACL, Budapest, Hungary.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449, De-
cember.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167, Sapporo, Japan.
Frank Soong and Eng Huang. 1991. A tree-trellis based
fast search for finding the n best sentence hypotheses
in continuous speech recognition. In Proceedings of
ICASSP 1991.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of COLING, pages 836?741,
Copenhagen, Denmark.
Stephan Vogel. 2005. PESA: Phrase pair extraction as
sentence splitting. In MT Summit X, Phuket, Thailand.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403, Septem-
ber.
Hao Zhang and Daniel Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment. In
Proceedings of ACL.
105
Proceedings of ACL-08: HLT, pages 209?217,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Efficient Multi-pass Decoding for Synchronous Context Free Grammars
Hao Zhang and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
We take a multi-pass approach to ma-
chine translation decoding when using syn-
chronous context-free grammars as the trans-
lation model and n-gram language models:
the first pass uses a bigram language model,
and the resulting parse forest is used in the
second pass to guide search with a trigram lan-
guage model. The trigram pass closes most
of the performance gap between a bigram de-
coder and a much slower trigram decoder, but
takes time that is insignificant in comparison
to the bigram pass. An additional fast de-
coding pass maximizing the expected count
of correct translation hypotheses increases the
BLEU score significantly.
1 Introduction
Statistical machine translation systems based
on synchronous grammars have recently shown
great promise, but one stumbling block to their
widespread adoption is that the decoding, or search,
problem during translation is more computationally
demanding than in phrase-based systems. This com-
plexity arises from the interaction of the tree-based
translation model with an n-gram language model.
Use of longer n-grams improves translation results,
but exacerbates this interaction. In this paper, we
present three techniques for attacking this problem
in order to obtain fast, high-quality decoders.
First, we present a two-pass decoding algorithm,
in which the first pass explores states resulting from
an integrated bigram language model, and the sec-
ond pass expands these states into trigram-based
states. The general bigram-to-trigram technique
is common in speech recognition (Murveit et al,
1993), where lattices from a bigram-based decoder
are re-scored with a trigram language model. We ex-
amine the question of whether, given the reordering
inherent in the machine translation problem, lower
order n-grams will provide as valuable a search
heuristic as they do for speech recognition.
Second, we explore heuristics for agenda-based
search, and present a heuristic for our second pass
that combines precomputed language model infor-
mation with information derived from the first pass.
With this heuristic, we achieve the same BLEU
scores and model cost as a trigram decoder with es-
sentially the same speed as a bigram decoder.
Third, given the significant speedup in the
agenda-based trigram decoding pass, we can rescore
the trigram forest to maximize the expected count of
correct synchronous constituents of the model, us-
ing the product of inside and outside probabilities.
Maximizing the expected count of synchronous con-
stituents approximately maximizes BLEU. We find
a significant increase in BLEU in the experiments,
with minimal additional time.
2 Language Model Integrated Decoding
for SCFG
We begin by introducing Synchronous Context Free
Grammars and their decoding algorithms when an
n-gram language model is integrated into the gram-
matical search space.
A synchronous CFG (SCFG) is a set of context-
free rewriting rules for recursively generating string
pairs. Each synchronous rule is a pair of CFG rules
209
with the nonterminals on the right hand side of one
CFG rule being one-to-one mapped to the other CFG
rule via a permutation pi. We adopt the SCFG nota-
tion of Satta and Peserico (2005). Superscript in-
dices in the right-hand side of grammar rules:
X ? X(1)1 ...X(n)n , X
(pi(1))
pi(1) ...X
(pi(n))
pi(n)
indicate that the nonterminals with the same index
are linked across the two languages, and will eventu-
ally be rewritten by the same rule application. Each
Xi is a variable which can take the value of any non-
terminal in the grammar.
In this paper, we focus on binary SCFGs and
without loss of generality assume that only the pre-
terminal unary rules can generate terminal string
pairs. Thus, we are focusing on Inversion Transduc-
tion Grammars (Wu, 1997) which are an important
subclass of SCFG. Formally, the rules in our gram-
mar include preterminal unary rules:
X ? e/f
for pairing up words or phrases in the two languages
and binary production rules with straight or inverted
orders that are responsible for building up upper-
level synchronous structures. They are straight rules
written:
X ? [Y Z]
and inverted rules written:
X ? ?Y Z?.
Most practical non-binary SCFGs can be bina-
rized using the synchronous binarization technique
by Zhang et al (2006). The Hiero-style rules of
(Chiang, 2005), which are not strictly binary but bi-
nary only on nonterminals:
X ? yu X(1) you X(2); have X(2) with X(1)
can be handled similarly through either offline bi-
narization or allowing a fixed maximum number of
gap words between the right hand side nonterminals
in the decoder.
For these reasons, the parsing problems for more
realistic synchronous CFGs such as in Chiang
(2005) and Galley et al (2006) are formally equiva-
lent to ITG. Therefore, we believe our focus on ITG
for the search efficiency issue is likely to generalize
to other SCFG-based methods.
Without an n-gram language model, decoding us-
ing SCFG is not much different from CFG pars-
ing. At each time a CFG rule is applied on the in-
put string, we apply the synchronized CFG rule for
the output language. From a dynamic programming
point of view, the DP states are X[i, j], where X
ranges over all possible nonterminals and i and j
range over 0 to the input string length |w|. Each
state stores the best translations obtainable. When
we reach the top state S[0, |w|], we can get the best
translation for the entire sentence. The algorithm is
O(|w|3).
However, when we want to integrate an n-gram
language model into the search, our goal is search-
ing for the derivation whose total sum of weights
of productions and n-gram log probabilities is
maximized. Now the adjacent span-parameterized
states X[i, k] and X[k, j] can interact with each
other by ?peeping into? the leading and trailing
n ? 1 words on the output side for each state.
Different boundary words differentiate the span-
parameterized states. Thus, to preserve the dynamic
programming property, we need to refine the states
by adding the boundary words into the parameter-
ization. The LM -integrated states are represented
as X[i, j, u1,..,n?1, v1,..,n?1]. Since the number of
variables involved at each DP step has increased to
3 + 4(n ? 1), the decoding algorithm is asymptoti-
cally O(|w|3+4(n?1)). Although it is possible to use
the ?hook? trick of Huang et al (2005) to factor-
ize the DP operations to reduce the complexity to
O(|w|3+3(n?1)), when n is greater than 2, the com-
plexity is still prohibitive.
3 Multi-pass LM-Integrated Decoding
In this section, we describe a multi-pass progres-
sive decoding technique that gradually augments the
LM -integrated states from lower orders to higher
orders. For instance, a bigram-integrated state
[X, i, j, u, v] is said to be a coarse-level state of a
trigram-integrate state [X, i, j, u, u?, v?, v], because
the latter state refines the previous by specifying
more inner words.
Progressive search has been used for HMM?s in
speech recognition (Murveit et al, 1993). The gen-
210
eral idea is to use a simple and fast decoding algo-
rithm to constrain the search space of a following
more complex and slower technique. More specif-
ically, a bigram decoding pass is executed forward
and backward to figure out the probability of each
state. Then the states can be pruned based on their
global score using the product of inside and outside
probabilities. The advanced decoding algorithm will
use the constrained space (a lattice in the case of
speech recognition) as a grammatical constraint to
help it focus on a smaller search space on which
more discriminative features are brought in.
The same idea has been applied to forests for pars-
ing. Charniak and Johnson (2005) use a PCFG to do
a pass of inside-outside parsing to reduce the state
space of a subsequent lexicalized n-best parsing al-
gorithm to produce parses that are further re-ranked
by a MaxEnt model.
We take the same view as in speech recognition
that a trigram integrated model is a finer-grained
model than bigram model and in general we can do
an n ? 1-gram decoding as a predicative pass for
the following n-gram pass. We need to do inside-
outside parsing as coarse-to-fine parsers do. How-
ever, we use the outside probability or cost informa-
tion differently. We do not combine the inside and
outside costs of a simpler model to prune the space
for a more complex model. Instead, for a given finer-
gained state, we combine its true inside cost with
the outside cost of its coarse-level counter-part to
estimate its worthiness of being explored. The use
of the outside cost from a coarser-level as the out-
side estimate makes our method naturally fall in the
framework of A* parsing.
Klein and Manning (2003) describe an A* pars-
ing framework for monolingual parsing and admis-
sible outside estimates that are computed using in-
side/outside parsing algorithm on simplified PCFGs
compared to the original PCFG. Zhang and Gildea
(2006) describe A* for ITG and develop admissible
heuristics for both alignment and decoding. Both
have shown the effectiveness of A* in situations
where the outside estimate approximates the true
cost closely such as when the sentences are short.
For decoding long sentences, it is difficult to come
up with good admissible (or inadmissible) heuris-
tics. If we can afford a bigram decoding pass, the
outside cost from a bigram model is conceivably a
very good estimate of the outside cost using a tri-
gram model since a bigram language model and a
trigram language model must be strongly correlated.
Although we lose the guarantee that the bigram-pass
outside estimate is admissible, we expect that it ap-
proximates the outside cost very closely, thus very
likely to effectively guide the heuristic search.
3.1 Inside-outside Coarse Level Decoding
We describe the coarse level decoding pass in this
section. The decoding algorithms for the coarse
level and the fine level do not necessarily have to
be the same. The fine level decoding algorithm is an
A* algorithm. The coarse level decoding algorithm
can be CKY or A* or other alternatives.
Conceptually, the algorithm is finding the short-
est hyperpath in the hypergraph in which the nodes
are states like X[i, j, u1,..,n?1, v1,..,n?1], and the hy-
peredges are the applications of the synchronous
rules to go from right-hand side states to left-hand
side states. The root of the hypergraph is a special
node S?[0, |w|, ?s?, ?/s?] which means the entire in-
put sentence has been translated to a string starting
with the beginning-of-sentence symbol and ending
at the end-of-sentence symbol. If we imagine a start-
ing node that goes to all possible basic translation
pairs, i.e., the instances of the terminal translation
rules for the input, we are searching the shortest hy-
per path from the imaginary bottom node to the root.
To help our outside parsing pass, we store the back-
pointers at each step of exploration.
The outside parsing pass, however, starts from the
root S?[|w|, ?s?, ?/s?] and follows the back-pointers
downward to the bottom nodes. The nodes need to
be visited in a topological order so that whenever
a node is visited, its parents have been visited and
its outside cost is over all possible outside parses.
The algorithm is described in pseudocode in Algo-
rithm 1. The number of hyperedges to traverse is
much fewer than in the inside pass because not ev-
ery state explored in the bottom up inside pass can
finally reach the goal. As for normal outside parsing,
the operations are the reverse of inside parsing. We
propagate the outside cost of the parent to its chil-
dren by combining with the inside cost of the other
children and the interaction cost, i.e., the language
model cost between the focused child and the other
children. Since we want to approximate the Viterbi
211
outside cost, it makes sense to maximize over all
possible outside costs for a given node, to be con-
sistent with the maximization of the inside pass. For
the nodes that have been explored in the bottom up
pass but not in the top-down pass, we set their out-
side cost to be infinity so that their exploration is
preferred only when the viable nodes from the first
pass have all been explored in the fine pass.
3.2 Heuristics for Fine-grained Decoding
In this section, we summarize the heuristics for finer
level decoding.
The motivation for combining the true inside
cost of the fine-grained model and the outside es-
timate given by the coarse-level parsing is to ap-
proximate the true global cost of a fine-grained state
as closely as possible. We can make the approx-
imation even closer by incorporating local higher-
order outside n-gram information for a state of
X[i, j, u1,..,n?1, v1,..,n?1] into account. We call this
the best-border estimate. For example, the best-
border estimate for trigram states is:
hBB(X, i, j, u1, u2, v1, v2)
=
[
max
s?S(i,j)
Plm(u2 | s, u1)
]
?
[
max
s?S(i,j)
Plm(s | v1, v2)
]
where S(i, j) is the set of candidate target language
words outside the span of (i, j). hBB is the prod-
uct of the upper bounds for the two on-the-border
n-grams.
This heuristic function was one of the admissible
heuristics used by Zhang and Gildea (2006). The
benefit of including the best-border estimate is to re-
fine the outside estimate with respect to the inner
words which refine the bigram states into the trigram
states. If we do not take the inner words into consid-
eration when computing the outside cost, all states
that map to the same coarse level state would have
the same outside cost. When the simple best-border
estimate is combined with the coarse-level outside
estimate, it can further boost the search as will be
shown in the experiments. To summarize, our recipe
for faster decoding is that using
?(X[i, j, u1,..,n?1, v1,..,n?1])
+ ?(X[i, j, u1, vn?1])
+ hBB(X, i, j, u1,...,n, v1,...,n) (1)
where ? is the Viterbi inside cost and ? is the Viterbi
outside cost, to globally prioritize the n-gram inte-
grated states on the agenda for exploration.
3.3 Alternative Efficient Decoding Algorithms
The complexity of n-gram integrated decoding for
SCFG has been tackled using other methods.
The hook trick of Huang et al (2005) factor-
izes the dynamic programming steps and lowers the
asymptotic complexity of the n-gram integrated de-
coding, but has not been implemented in large-scale
systems where massive pruning is present.
The cube-pruning by Chiang (2007) and the lazy
cube-pruning of Huang and Chiang (2007) turn the
computation of beam pruning of CYK decoders into
a top-k selection problem given two columns of
translation hypotheses that need to be combined.
The insight for doing the expansion top-down lazily
is that there is no need to uniformly explore every
cell. The algorithm starts with requesting the first
best hypothesis from the root. The request translates
into requests for the k-bests of some of its children
and grandchildren and so on, because re-ranking at
each node is needed to get the top ones.
Venugopal et al (2007) also take a two-pass de-
coding approach, with the first pass leaving the lan-
guage model boundary words out of the dynamic
programming state, such that only one hypothesis is
retained for each span and grammar symbol.
4 Decoding to Maximize BLEU
The ultimate goal of efficient decoding to find the
translation that has a highest evaluation score using
the least time possible. Section 3 talks about utiliz-
ing the outside cost of a lower-order model to esti-
mate the outside cost of a higher-order model, boost-
ing the search for the higher-order model. By doing
so, we hope the intrinsic metric of our model agrees
with the extrinsic metric of evaluation so that fast
search for the model is equivalent to efficient decod-
ing. But the mismatch between the two is evident,
as we will see in the experiments. In this section,
212
Algorithm 1 OutsideCoarseParsing()
for all X[i, j, u, v] in topological order do
for all children pairs pointed to by the back-pointers do
if X ? [Y Z] then
 the two children are Y [i, k, u, u?] and Z[k, j, v?, v]
?(Y [i, k, u, u?]) = max {?(Y [i, k, u, u?]),
?(X[i, j, u, v]) + ?(Z[k, j, v?, v]) + rule(X ? [Y Z]) + bigram(u?, v?)}
?(Z[k, j, v?, v]) = max {?(Z[k, j, v?, v]),
?(X[i, j, u, v]) + ?(Y [i, k, u, u?]) + rule(X ? [Y Z]) + bigram(u?, v?)}
end if
if X ? ?Y Z? then
 the two children are Y [i, k, v?, v] and Z[k, j, u, u?]
?(Y [i, k, v?, v]) = max {?(Y [i, k, v?, v]),
?(X[i, j, u, v]) + ?(Z[k, j, u, u?]) + rule(X ? ?Y Z?) + bigram(u?, v?)}
?(Z[k, j, u, u?]) = max {?(Z[k, j, u, u?]),
?(X[i, j, u, v]) + ?(Y [i, k, v?, v]) + rule(X ? ?Y Z?) + bigram(u?, v?)}
end if
end for
end for
we deal with the mismatch by introducing another
decoding pass that maximizes the expected count
of synchronous constituents in the tree correspond-
ing to the translation returned. BLEU is based on
n-gram precision, and since each synchronous con-
stituent in the tree adds a new 4-gram to the trans-
lation at the point where its children are concate-
nated, the additional pass approximately maximizes
BLEU.
Kumar and Byrne (2004) proposed the framework
of Minimum Bayesian Risk (MBR) decoding that
minimizes the expected loss given a loss function.
Their MBR decoding is a reranking pass over an n-
best list of translations returned by the decoder. Our
algorithm is another dynamic programming decod-
ing pass on the trigram forest, and is similar to the
parsing algorithm for maximizing expected labelled
recall presented by Goodman (1996).
4.1 Maximizing the expected count of correct
synchronous constituents
We introduce an algorithm that maximizes the ex-
pected count of correct synchronous constituents.
Given a synchronous constituent specified by the
state [X, i, j, u, u?, v?, v], its probability of being cor-
rect in the model is
EC([X, i, j, u, u?, v?, v])
= ?([X, i, j, u, u?, v?, v]) ? ?([X, i, j, u, u?, v?, v]),
where ? is the outside probability and ? is the in-
side probability. We approximate ? and ? using the
Viterbi probabilities. Since decoding from bottom
up in the trigram pass already gives us the inside
Viterbi scores, we only have to visit the nodes in
the reverse order once we reach the root to compute
the Viterbi outside scores. The outside-pass Algo-
rithm 1 for bigram decoding can be generalized to
the trigram case. We want to maximize over all
translations (synchronous trees) T in the forest af-
ter the trigram decoding pass according to
max
T
?
[X,i,j,u,u?,v?,v]?T
EC([X, i, j, u, u?, v?, v]).
The expression can be factorized and computed us-
ing dynamic programming on the forest.
5 Experiments
We did our decoding experiments on the LDC 2002
MT evaluation data set for translation of Chinese
newswire sentences into English. The evaluation
data set has 10 human translation references for each
sentence. There are a total of 371 Chinese sentences
of no more than 20 words in the data set. These
sentences are the test set for our different versions
of language-model-integrated ITG decoders. We
evaluate the translation results by comparing them
against the reference translations using the BLEU
metric.
213
The word-to-word translation probabilities are
from the translation model of IBM Model 4 trained
on a 160-million-word English-Chinese parallel cor-
pus using GIZA++. The phrase-to-phrase transla-
tion probabilities are trained on 833K parallel sen-
tences. 758K of this was data made available by
ISI, and another 75K was FBIS data. The language
model is trained on a 30-million-word English cor-
pus. The rule probabilities for ITG are trained using
EM on a corpus of 18,773 sentence pairs with a to-
tal of 276,113 Chinese words and 315,415 English
words.
5.1 Bigram-pass Outside Cost as Trigram-pass
Outside Estimate
We first fix the beam for the bigram pass, and change
the outside heuristics for the trigram pass to show
the difference before and after using the first-pass
outside cost estimate and the border estimate. We
choose the beam size for the CYK bigram pass to be
10 on the log scale. The first row of Table 1 shows
the number of explored hyperedges for the bigram
pass and its BLEU score. In the rows below, we
compare the additional numbers of hyperedges that
need to be explored in the trigram pass using differ-
ent outside heuristics. It takes too long to finish us-
ing uniform outside estimate; we have to use a tight
beam to control the agenda-based exploration. Us-
ing the bigram outside cost estimate makes a huge
difference. Furthermore, using Equation 1, adding
the additional heuristics on the best trigrams that can
appear on the borders of the current hypothesis, on
average we only need to explore 2700 additional hy-
peredges per sentence to boost the BLEU score from
21.77 to 23.46. The boost is so significant that over-
all the dominant part of search time is no longer the
second pass but the first bigram pass (inside pass ac-
tually) which provides a constrained space and out-
side heuristics for the second pass.
5.2 Two-pass decoding versus One-pass
decoding
By varying the beam size for the first pass, we can
plot graphs of model scores versus search time and
BLEU scores versus search time as shown in Fig-
ure 1. We use a very large beam for the second pass
due to the reason that the outside estimate for the
second pass is discriminative enough to guide the
Decoding Method Avg. Hyperedges BLEU
Bigram Pass 167K 21.77
Trigram Pass
UNI ? ?
BO + 629.7K=796.7K 23.56
BO+BB +2.7K =169.7K 23.46
Trigram One-pass,
with Beam 6401K 23.47
Table 1: Speed and BLEU scores for two-pass decoding.
UNI stands for the uniform (zero) outside estimate. BO
stands for the bigram outside cost estimate. BB stands for
the best border estimate, which is added to BO.
Decoder Time BLEU Model Score
One-pass agenda 4317s 22.25 -208.849
One-pass CYK 3793s 22.89 -207.309
Multi-pass, CYK first
agenda second pass 3689s 23.56 -205.344
MEC third pass 3749s 24.07 -203.878
Lazy-cube-pruning 3746s 22.16 -208.575
Table 2: Summary of different trigram decoding strate-
gies, using about the same time (10 seconds per sen-
tence).
search. We sum up the total number of seconds for
both passes to compare with the baseline systems.
On average, less than 5% of time is spent in the sec-
ond pass.
In Figure 1, we have four competing decoders.
bitri cyk is our two-pass decoder, using CYK as
the first pass decoding algorithm and using agenda-
based decoding in the second pass which is guided
by the first pass. agenda is our trigram-integrated
agenda-based decoder. The other two systems are
also one-pass. cyk is our trigram-integrated CYK
decoder. lazy kbest is our top-down k-best-style de-
coder.1
Figure 1(left) compares the search efficiencies of
the four systems. bitri cyk at the top ranks first. cyk
follows it. The curves of lazy kbest and agenda cross
1In our implementation of the lazy-cube-pruning based ITG
decoder, we vary the re-ranking buffer size and the the top-k
list size which are the two controlling parameters for the search
space. But we did not use any LM estimate to achieve early
stopping as suggested by Huang and Chiang (2007). Also, we
did not have a translation-model-only pruning pass. So the re-
sults shown in this paper for the lazy cube pruning method is
not of its best performance.
214
and are both below the curves of bitri cyk and cyk.
This figure indicates the advantage of the two-pass
decoding strategy in producing translations with a
high model score in less time.
However, model scores do not directly translate
into BLEU scores. In Figure 1(right), bitri cyk is
better than CYK only in a certain time window when
the beam is neither too small nor too large. But
the window is actually where we are interested ? it
ranges from 5 seconds per sentence to 20 seconds
per sentence. Table 2 summarizes the performance
of the four decoders when the decoding speed is at
10 seconds per sentence.
5.3 Does the hook trick help?
We have many choices in implementing the bigram
decoding pass. We can do either CYK or agenda-
based decoding. We can also use the dynamic pro-
gramming hook trick. We are particularly interested
in the effect of the hook trick in a large-scale system
with aggressive pruning.
Figure 2 compares the four possible combinations
of the decoding choices for the first pass: bitri cyk,
bitri agenda, bitri cyk hook and bitri agenda hook.
bitri cyk which simply uses CYK as the first pass
decoding algorithm is the best in terms of perfor-
mance and time trade-off. The hook-based de-
coders do not show an advantage in our experiments.
Only bitri agenda hook gets slightly better than bi-
tri agenda when the beam size increases. So, it is
very likely the overhead of building hooks offsets its
benefit when we massively prune the hypotheses.
5.4 Maximizing BLEU
The bitri cyk decoder spends little time in the
agenda-based trigram pass, quickly reaching the
goal item starting from the bottom of the chart. In
order to maximize BLEU score using the algorithm
described in Section 4, we need a sizable trigram
forest as a starting point. Therefore, we keep pop-
ping off more items from the agenda after the goal
is reached. Simply by exploring more (200 times
the log beam) after-goal items, we can optimize the
Viterbi synchronous parse significantly, shown in
Figure 3(left) in terms of model score versus search
time.
However, the mismatch between model score and
BLEU score persists. So, we try our algorithm
of maximizing expected count of synchronous con-
stituents on the trigram forest. We find signifi-
cant improvement in BLEU, as shown in Figure 3
(right) by the curve of bitri cyk epass me cons. bi-
tri cyk epass me cons beats both bitri cyk and cyk
in terms of BLEU versus time if using more than
1.5 seconds on average to decode each sentence. At
each time point, the difference in BLEU between
bitri cyk epass me cons and the highest of bitri cyk
and cyk is around .5 points consistently as we vary
the beam size for the first pass. We achieve the
record-high BLEU score 24.34 using on average 21
seconds per sentence, compared to the next-highest
score of 23.92 achieved by cyk using on average 78
seconds per sentence.
6 Conclusion
We present a multi-pass method to speed up n-
gram integrated decoding for SCFG. We use an in-
side/outside parsing algorithm to get the Viterbi out-
side cost of bigram integrated states which is used as
an outside estimate for trigram integrated states. The
coarse-level outside cost plus the simple estimate for
border trigrams speeds up the trigram decoding pass
hundreds of times compared to using no outside es-
timate.
Maximizing the probability of the synchronous
derivation is not equivalent to maximizing BLEU.
We use a rescoring decoding pass that maximizes the
expected count of synchronous constituents. This
technique, together with the progressive search at
previous stages, gives a decoder that produces the
highest BLEU score we have obtained on the data in
a very reasonable amount of time.
As future work, new metrics for the final pass may
be able to better approximate BLEU. As the bigram
decoding pass currently takes the bulk of the decod-
ing time, better heuristics for this phase may speed
up the system further.
Acknowledgments This work was supported by
NSF ITR-0428020 and NSF IIS-0546554.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In ACL.
215
-224
-222
-220
-218
-216
-214
-212
-210
-208
-206
-204
 10  100  1000  10000  100000
lo
g 
sc
or
e
total secs
bitri_cyk
cyk
agenda
lazy kbest
 0.17
 0.18
 0.19
 0.2
 0.21
 0.22
 0.23
 0.24
 10  100  1000  10000  100000
bl
eu
total secs
bitri_cyk
cyk
agenda
lazy kbest
Figure 1: We compare the two-pass ITG decoder with the one-pass trigram-integrated ITG decoders in terms of both
model scores vs. time (left) and BLEU scores vs. time (right). The model score here is the log probability of the
decoded parse, summing up both the translation model and the language model. We vary the beam size (for the first
pass in the case of two-pass) to search more and more thoroughly.
-222
-220
-218
-216
-214
-212
-210
-208
-206
-204
 100  1000  10000  100000
lo
g 
sc
or
e
total secs
bitri_cyk
bitri_cyk_hook
bitri_agenda
bitri_agenda_hook
 0.17
 0.18
 0.19
 0.2
 0.21
 0.22
 0.23
 0.24
 100  1000  10000  100000
bl
eu
total secs
bitri_cyk
bitri_cyk_hook
bitri_agenda
bitri_agenda_hook
Figure 2: We use different first-pass decoding algorithms, fixing the second pass to be agenda-based which is guided
by the outside cost of the first pass. Left: model score vs. time. Right: BLEU score vs. time.
-222
-220
-218
-216
-214
-212
-210
-208
-206
-204
-202
 100  1000  10000  100000
lo
g 
sc
or
e
total secs
bitri_cyk delayed-stopping
bitri_cyk
 0.17
 0.18
 0.19
 0.2
 0.21
 0.22
 0.23
 0.24
 0.25
 10  100  1000  10000  100000
bl
eu
total secs
bitri_cyk_epass_me_cons
bitri_cyk
cyk
Figure 3: Left: improving the model score by extended agenda-exploration after the goal is reached in the best-first
search. Right: maximizing BLEU by the maximizing expectation pass on the expanded forest.
216
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Conference of the Association for
Computational Linguistics (ACL-05), pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the International Conference on Computational
Linguistics/Association for Computational Linguistics
(COLING/ACL-06), pages 961?968, July.
Joshua Goodman. 1996. Parsing algorithms and metrics.
In Proceedings of the 34th Annual Conference of the
Association for Computational Linguistics (ACL-96),
pages 177?183.
Liang Huang and David Chiang. 2007. Faster algorithms
for decoding with integrated language models. In Pro-
ceedings of ACL, Prague, June.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005.
Machine translation as lexicalized parsing with hooks.
In International Workshop on Parsing Technologies
(IWPT05), Vancouver, BC.
Dan Klein and Christopher D. Manning. 2003. A* pars-
ing: Fast exact Viterbi parse selection. In Proceed-
ings of the 2003 Meeting of the North American chap-
ter of the Association for Computational Linguistics
(NAACL-03).
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Daniel Marcu Susan Dumais and Salim Roukos,
editors, HLT-NAACL 2004: Main Proceedings, pages
169?176, Boston, Massachusetts, USA, May 2 - May
7. Association for Computational Linguistics.
Hy Murveit, John W. Butzberger, Vassilios V. Digalakis,
and Mitchel Weintraub. 1993. Large-vocabulary dic-
tation using SRI?s decipher speech recognition system:
Progressive-search techniques. In Proceedings of the
IEEE International Conference on Acoustics, Speech,
& Signal Processing (IEEE ICASSP-93), volume 2,
pages 319?322. IEEE.
Giorgio Satta and Enoch Peserico. 2005. Some com-
putational complexity results for synchronous context-
free grammars. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 803?810, Vancouver, Canada,
October.
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In NAACL07,
Rochester, NY, April.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Hao Zhang and Daniel Gildea. 2006. Efficient search for
inversion transduction grammar. In 2006 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), Sydney.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the 2006 Meeting of the
North American chapter of the Association for Com-
putational Linguistics (NAACL-06), pages 256?263.
217
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 65?73,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Machine Translation as Lexicalized Parsing with Hooks
Liang Huang
Dept. of Computer & Information Science
University of Pennsylvania
Philadelphia, PA 19104
Hao Zhang and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
We adapt the ?hook? trick for speeding up
bilexical parsing to the decoding problem
for machine translation models that are
based on combining a synchronous con-
text free grammar as the translation model
with an n-gram language model. This
dynamic programming technique yields
lower complexity algorithms than have
previously been described for an impor-
tant class of translation models.
1 Introduction
In a number of recently proposed synchronous
grammar formalisms, machine translation of new
sentences can be thought of as a form of parsing on
the input sentence. The parsing process, however,
is complicated by the interaction of the context-free
translation model with an m-gram1 language model
in the output language. While such formalisms ad-
mit dynamic programming solutions having poly-
nomial complexity, the degree of the polynomial is
prohibitively high.
In this paper we explore parallels between transla-
tion and monolingual parsing with lexicalized gram-
mars. Chart items in translation must be augmented
with words from the output language in order to cap-
ture language model state. This can be thought of as
a form of lexicalization with some similarity to that
of head-driven lexicalized grammars, despite being
unrelated to any notion of syntactic head. We show
1We speak of m-gram language models to avoid confusion
with n, which here is the length of the input sentence for trans-
lation.
that techniques for parsing with lexicalized gram-
mars can be adapted to the translation problem, re-
ducing the complexity of decoding with an inversion
transduction grammar and a bigram language model
from O(n7) to O(n6). We present background on
this translation model as well as the use of the tech-
nique in bilexicalized parsing before describing the
new algorithm in detail. We then extend the al-
gorithm to general m-gram language models, and
to general synchronous context-free grammars for
translation.
2 Machine Translation using Inversion
Transduction Grammar
The Inversion Transduction Grammar (ITG) of Wu
(1997) is a type of context-free grammar (CFG) for
generating two languages synchronously. To model
the translational equivalence within a sentence pair,
ITG employs a synchronous rewriting mechanism to
relate two sentences recursively. To deal with the
syntactic divergence between two languages, ITG
allows the inversion of rewriting order going from
one language to another at any recursive level. ITG
in Chomsky normal form consists of unary produc-
tion rules that are responsible for generating word
pairs:
X ? e/f
X ? e/
X ? /f
where e is a source language word, f is a foreign lan-
guage word, and  means the null token, and binary
production rules in two forms that are responsible
for generating syntactic subtree pairs:
X ? [Y Z]
65
and
X ? ?Y Z?
The rules with square brackets enclosing the
right-hand side expand the left-hand side symbol
into the two symbols on the right-hand side in the
same order in the two languages, whereas the rules
with angled brackets expand the left hand side sym-
bol into the two right-hand side symbols in reverse
order in the two languages. The first class of rules
is called straight rule. The second class of rules is
called inverted rule.
One special case of 2-normal ITG is the so-called
Bracketing Transduction Grammar (BTG) which
has only one nonterminal A and two binary rules
A ? [AA]
and
A ? ?AA?
By mixing instances of the inverted rule with
those of the straight rule hierarchically, BTG can
meet the alignment requirements of different lan-
guage pairs. There exists a more elaborate version
of BTG that has 4 nonterminals working together
to guarantee the property of one-to-one correspon-
dence between alignments and synchronous parse
trees. Table 1 lists the rules of this BTG. In the
discussion of this paper, we will consider ITG in 2-
normal form.
By associating probabilities or weights with the
bitext production rules, ITG becomes suitable for
weighted deduction over bitext. Given a sentence
pair, searching for the Viterbi synchronous parse
tree, of which the alignment is a byproduct, turns out
to be a two-dimensional extension of PCFG parsing,
having time complexity of O(n6), where n is the
length of the English string and the foreign language
string. A more interesting variant of parsing over bi-
text space is the asymmetrical case in which only the
foreign language string is given so that Viterbi pars-
ing involves finding the English string ?on the fly?.
The process of finding the source string given its tar-
get counterpart is decoding. Using ITG, decoding is
a form of parsing.
2.1 ITG Decoding
Wu (1996) presented a polynomial-time algorithm
for decoding ITG combined with an m-gram lan-
guage model. Such language models are commonly
used in noisy channel models of translation, which
find the best English translation e of a foreign sen-
tence f by finding the sentence e that maximizes the
product of the translation model P (f |e) and the lan-
guage model P (e).
It is worth noting that since we have specified ITG
as a joint model generating both e and f , a language
model is not theoretically necessary. Given a foreign
sentence f , one can find the best translation e?:
e? = argmax
e
P (e, f)
= argmax
e
?
q
P (e, f, q)
by approximating the sum over parses q with the
probability of the Viterbi parse:
e? = argmax
e
max
q
P (e, f, q)
This optimal translation can be computed in using
standard CKY parsing over f by initializing the
chart with an item for each possible translation of
each foreign word in f , and then applying ITG rules
from the bottom up.
However, ITG?s independence assumptions are
too strong to use the ITG probability alone for ma-
chine translation. In particular, the context-free as-
sumption that each foreign word?s translation is cho-
sen independently will lead to simply choosing each
foreign word?s single most probable English trans-
lation with no reordering. In practice it is beneficial
to combine the probability given by ITG with a local
m-gram language model for English:
e? = argmax
e
max
q
P (e, f, q)Plm(e)?
with some constant language model weight ?. The
language model will lead to more fluent output by
influencing both the choice of English words and the
reordering, through the choice of straight or inverted
rules. While the use of a language model compli-
cates the CKY-based algorithm for finding the best
translation, a dynamic programming solution is still
possible. We extend the algorithm by storing in each
chart item the English boundary words that will af-
fect the m-gram probabilities as the item?s English
string is concatenated with the string from an adja-
cent item. Due to the locality of m-gram language
66
Structural Rules Lexical Rules
S ? A
S ? B
S ? C
A ? [AB]
A ? [BB]
A ? [CB]
A ? [AC]
A ? [BC]
A ? [CC]
B ? ?AA?
B ? ?BA?
B ? ?CA?
B ? ?AC?
B ? ?BC?
B ? ?CC?
C ? ei/fj
C ? /fj
C ? ei/
Table 1: Unambiguous BTG
model, only m?1 boundary words need to be stored
to compute the new m-grams produced by combin-
ing two substrings. Figure 1 illustrates the combi-
nation of two substrings into a larger one in straight
order and inverted order.
3 Hook Trick for Bilexical Parsing
A traditional CFG generates words at the bottom of
a parse tree and uses nonterminals as abstract rep-
resentations of substrings to build higher level tree
nodes. Nonterminals can be made more specific to
the actual substrings they are covering by associ-
ating a representative word from the nonterminal?s
yield. When the maximum number of lexicalized
nonterminals in any rule is two, a CFG is bilexical.
A typical bilexical CFG in Chomsky normal form
has two types of rule templates:
A[h] ? B[h]C[h?]
or
A[h] ? B[h?]C[h]
depending on which child is the head child that
agrees with the parent on head word selection.
Bilexical CFG is at the heart of most modern statisti-
cal parsers (Collins, 1997; Charniak, 1997), because
the statistics associated with word-specific rules are
more informative for disambiguation purposes. If
we use A[i, j, h] to represent a lexicalized con-
stituent, ?(?) to represent the Viterbi score function
applicable to any constituent, and P (?) to represent
the rule probability function applicable to any rule,
Figure 2 shows the equation for the dynamic pro-
gramming computation of the Viterbi parse. The two
terms of the outermost max operator are symmetric
cases for heads coming from left and right. Contain-
ing five free variables i,j,k,h?,h, ranging over 1 to
n, the length of input sentence, both terms can be
instantiated in n5 possible ways, implying that the
complexity of the parsing algorithm is O(n5).
Eisner and Satta (1999) pointed out we don?t have
to enumerate k and h? simultaneously. The trick,
shown in mathematical form in Figure 2 (bottom) is
very simple. When maximizing over h?, j is irrele-
vant. After getting the intermediate result of maxi-
mizing over h?, we have one less free variable than
before. Throughout the two steps, the maximum
number of interacting variables is 4, implying that
the algorithmic complexity is O(n4) after binarizing
the factors cleverly. The intermediate result
max
h?,B
[?(B[i, k, h?]) ? P (A[h] ? B[h?]C[h])]
can be represented pictorially as
C[h]
A
i k . The
same trick works for the second max term in
Equation 1. The intermediate result coming from
binarizing the second term can be visualized as
A
k
B[h]
j
. The shape of the intermediate re-
sults gave rise to the nickname of ?hook?. Melamed
(2003) discussed the applicability of the hook trick
for parsing bilexical multitext grammars. The anal-
ysis of the hook trick in this section shows that it is
essentially an algebraic manipulation. We will for-
mulate the ITG Viterbi decoding algorithm in a dy-
namic programming equation in the following sec-
tion and apply the same algebraic manipulation to
produce hooks that are suitable for ITG decoding.
4 Hook Trick for ITG Decoding
We start from the bigram case, in which each de-
coding constituent keeps a left boundary word and
67
tu11 u12 v12v11 u21 u22 v22v21
X
Y Z[ ]
Ss
u21
X
Y Z
Ss t
< >
v21 v22 u11 u12 v11 v12u22
(a) (b)
Figure 1: ITG decoding using 3-gram language model. Two boundary words need to be kept on the left (u)
and right (v) of each constituent. In (a), two constituents Y and Z spanning substrings s, S and S, t of the
input are combined using a straight rule X ? [Y Z]. In (b), two constituents are combined using a inverted
rule X ? ?Y Z?. The dashed line boxes enclosing three words are the trigrams produced from combining
two substrings.
?(A[i, j, h]) = max
?
?
?
?
?
max
k,h?,B,C
[
?(B[i, k, h?]) ? ?(C[k, j, h]) ? P (A[h] ? B[h?]C[h])
]
,
max
k,h?,B,C
[
?(B[i, k, h]) ? ?(C[k, j, h?]) ? P (A[h] ? B[h]C[h?])
]
?
?
?
?
?
(1)
max
k,h?,B,C
[
?(B[i, k, h?]) ? ?(C[k, j, h]) ? P (A[h] ? B[h?]C[h])
]
= max
k,C
[
max
h?,B
[
?(B[i, k, h?]) ? P (A[h] ? B[h?]C[h])
]
? ?(C[k, j, h])
]
Figure 2: Equation for bilexical parsing (top), with an efficient factorization (bottom)
a right boundary word. The dynamic programming
equation is shown in Figure 3 (top) where i,j,k range
over 1 to n, the length of input foreign sentence, and
u,v,v1,u2 (or u,v,v2,u1) range over 1 to V , the size
of English vocabulary. Usually we will constrain the
vocabulary to be a subset of words that are probable
translations of the foreign words in the input sen-
tence. So V is proportional to n. There are seven
free variables related to input size for doing the max-
imization computation. Hence the algorithmic com-
plexity is O(n7).
The two terms in Figure 3 (top) within the first
level of the max operator, corresponding to straight
rules and inverted rules, are analogous to the two
terms in Equation 1. Figure 3 (bottom) shows how to
decompose the first term; the same method applies
to the second term. Counting the free variables en-
closed in the innermost max operator, we get five: i,
k, u, v1, and u2. The decomposition eliminates one
free variable, v1. In the outermost level, there are
six free variables left. The maximum number of in-
teracting variables is six overall. So, we reduced the
complexity of ITG decoding using bigram language
model from O(n7) to O(n6).
The hooks k
X
Zu u2
i that we have built for de-
coding with a bigram language model turn out to be
similar to the hooks for bilexical parsing if we focus
on the two boundary words v1 and u2 (or v2 and u1)
68
?(X[i, j, u, v]) = max
?
?
?
?
?
?
?
?
?
max
k,v1,u2,Y,Z
[
?(Y [i, k, u, v1]) ? ?(Z[k, j, u2, v])
? P (X ? [Y Z]) ? bigram(v1, u2)
]
,
max
k,v2,u1,Y,Z
[
?(Y [i, k, u1, v]) ? ?(Z[k, j, u, v2])
? P (X ? ?Y Z?) ? bigram(v2, u1)
]
?
?
?
?
?
?
?
?
?
(2)
max
k,v1,u2,Y,Z
[
?(Y [i, k, u, v1]) ? ?(Z[k, j, u2, v]) ? P (X ? [Y Z]) ? bigram(v1, u2)
]
= max
k,u2,Z
[
max
v1,Y
[
?(Y [i, k, u, v1]) ? P (X ? [Y Z]) ? bigram(v1, u2)
]
? ?(Z[k, j, u2, v])
]
Figure 3: Equation for ITG decoding (top), with an efficient factorization (bottom)
that are interacting between two adjacent decoding
constituents and relate them with the h? and h that
are interacting in bilexical parsing. In terms of al-
gebraic manipulation, we are also rearranging three
factors (ignoring the non-lexical rules), trying to re-
duce the maximum number of interacting variables
in any computation step.
4.1 Generalization to m-gram Cases
In this section, we will demonstrate how to use the
hook trick for trigram decoding which leads us to a
general hook trick for any m-gram decoding case.
We will work only on straight rules and use icons
of constituents and hooks to make the equations eas-
ier to interpret.
The straightforward dynamic programming equa-
tion is:
i
X
u1u2 v1v2
j = maxv11,v12,u21,u22,
k,Y,Z
u22
i k j
X
Y Z
u1u2 v2v1
][
v11v12 u21
(3)
By counting the variables that are dependent
on input sentence length on the right hand side
of the equation, we know that the straightfor-
ward algorithm?s complexity is O(n11). The max-
imization computation is over four factors that
are dependent on n: ?(Y [i, k, u1, u2, v11, v12]),
?(Z[k, j, u21, u22, v1, v2]), trigram(v11, v12, u21),
and trigram(v12, u21, u22). As before, our goal is
to cleverly bracket the factors.
By bracketing trigram(v11, v12, u21) and
?(Y [i, k, u1, u2, v11, v12]) together and maximizing
over v11 and Y , we can build the the level-1 hook:
u21
i k
X
Z
u1u2
][
v12
= max
v11,Y
u21
i k
X
Y Z
u1u2
][
v11v12
The complexity is O(n7).
Grouping the level-1 hook and
trigram(v12, u21, u22), maximizing over v12,
we can build the level-2 hook:
u21
i k
X
Z
u1u2
][
u22
= max
v12
u21
i k
X
Z
u1u2
][
v12 u22
The complexity is O(n7). Finally,
we can use the level-2 hook to com-
bine with Z[k, j, u21, u22, v1, v2] to build
X[i, j, u1, u2, v1, v2]. The complexity is O(n9)
after reducing v11 and v12 in the first two steps.
i
X
u1u2 v1v2
j = max
u21,u22,k,Z
u22
i k j
X
Z
u1u2 v2v1
][
u21
(4)
Using the hook trick, we have reduced the com-
plexity of ITG decoding using bigrams from O(n7)
to O(n6), and from O(n11) to O(n9) for trigram
69
case. We conclude that for m-gram decoding of
ITG, the hook trick can change the the time com-
plexity from O(n3+4(m?1)) to O(n3+3(m?1)). To
get an intuition of the reduction, we can compare
Equation 3 with Equation 4. The variables v11 and
v12 in Equation 3, which are independent of v1 and
v2 for maximizing the product have been concealed
under the level-2 hook in Equation 4. In general,
by building m ? 1 intermediate hooks, we can re-
duce m ? 1 free variables in the final combination
step, hence having the reduction from 4(m ? 1) to
3(m ? 1).
5 Generalization to Non-binary Bitext
Grammars
Although we have presented our algorithm as a de-
coder for the binary-branching case of Inversion
Transduction Grammar, the same factorization tech-
nique can be applied to more complex synchronous
grammars. In this general case, items in the dy-
namic programming chart may need to represent
non-contiguous span in either the input or output
language. Because synchronous grammars with in-
creasing numbers of children on the right hand side
of each production form an infinite, non-collapsing
hierarchy, there is no upper bound on the number
of discontinuous spans that may need to be repre-
sented (Aho and Ullman, 1972). One can, however,
choose to factor the grammar into binary branching
rules in one of the two languages, meaning that dis-
continuous spans will only be necessary in the other
language.
If we assume m is larger than 2, it is likely that
the language model combinations dominate com-
putation. In this case, it is advantageous to factor
the grammar in order to make it binary in the out-
put language, meaning that the subrules will only
need to represent adjacent spans in the output lan-
guage. Then the hook technique will work in the
same way, yielding O(n2(m?1)) distinct types of
items with respect to language model state, and
3(m?1) free indices to enumerate when combining
a hook with a complete constituent to build a new
item. However, a larger number of indices point-
ing into the input language will be needed now that
items can cover discontinuous spans. If the gram-
mar factorization yields rules with at most R spans
in the input language, there may be O(n2R) dis-
tinct types of chart items with respect to the input
language, because each span has an index for its
beginning and ending points in the input sentence.
Now the upper bound of the number of free in-
dices with respect to the input language is 2R + 1,
because otherwise if one rule needs 2R + 2 in-
dices, say i1, ? ? ? , i2R+2, then there are R + 1 spans
(i1, i2), ? ? ? , (i2R+1, i2R+2), which contradicts the
above assumption. Thus the time complexity at the
input language side is O(n2R+1), yielding a total al-
gorithmic complexity of O(n3(m?1)+(2R+1)).
To be more concrete, we will work through a 4-
ary translation rule, using a bigram language model.
The standard DP equation is:
i
u v
j
A
= maxv3,u1,v1,u4,v4,u2,
k1,k2,k3,
B,C,D,E
B C D E
A
v3u u1 v1 u4 v4 u2 v
i k1 k2 k3 j (5)
This 4-ary rule is a representative difficult case.
The underlying alignment pattern for this rule is as
follows:
D
C
E
B
A
It is a rule that cannot be binarized in the bitext
space using ITG rules. We can only binarize it in
one dimension and leave the other dimension having
discontinuous spans. Without applying binarization
and hook trick, decoding parsing with it according
to Equation 5 requires time complexity of O(n13).
However, we can build the following partial con-
stituents and hooks to do the combination gradually.
The first step finishes a hook by consuming one
bigram. Its time complexity is O(n5):
C D E
A
u1u
k2 k3 = max
v3,B
B C D E
A
u v3 u1
k2 k3
The second step utilizes the hook we just built and
builds a partial constituent. The time complexity is
O(n7):
70
D E
A
u v1
i k1 k2 k3 = max
u1,C
C D E
A
u u1 v1
i k1 k2 k3
By ?eating? another bigram, we build the second
hook using O(n7):
D E
A
u u4
i k1 k2 k3 = max
v1
D E
A
u v1 u4
i k1 k2 k3
We use the last hook. This step has higher com-
plexity: O(n8):
E
A
u v4
i k1 k2 j = max
u4,k3,D
v4u4
k2 k3
D E
A
jk1i
u
The last bigram involved in the 4-ary rule is com-
pleted and leads to the third hook, with time com-
plexity of O(n7):
E
A
jk2k1i
u u2
= max
v4
E
A
u v4 u2
i k1 k2 j
The final combination is O(n7):
i
u v
j
A
= max
u2,k1,k2,E
u
i k1 k2
E
A
u2
j
v
The overall complexity has been reduced to
O(n8) after using binarization on the output side and
using the hook trick all the way to the end. The result
is one instance of our general analysis: here R = 2,
m = 2, and 3(m ? 1) + (2R + 1) = 8.
6 Implementation
The implementation of the hook trick in a practi-
cal decoder is complicated by the interaction with
pruning. If we build hooks looking for all words
in the vocabulary whenever a complete constituent
is added to the chart, we will build many hooks
that are never used, because partial hypotheses with
many of the boundary words specified by the hooks
may never be constructed due to pruning. In-
stead of actively building hooks, which are inter-
mediate results, we can build them only when we
need them and then cache them for future use. To
make this idea concrete, we sketch the code for bi-
gram integrated decoding using ITG as in Algo-
rithm 1. It is worthy of noting that for clarity we
are building hooks in shape of
v
k j
v?
Z
, instead
of
X
Y v
k j
v?
as we have been showing in the
previous sections. That is, the probability for the
grammar rule is multiplied in when a complete con-
stituent is built, rather than when a hook is created.
If we choose the original representation, we would
have to create both straight hooks and inverted hooks
because the straight rules and inverted rules are to be
merged with the ?core? hooks, creating more speci-
fied hooks.
7 Conclusion
By showing the parallels between lexicalization for
language model state and lexicalization for syntac-
tic heads, we have demonstrated more efficient al-
gorithms for previously described models of ma-
chine translation. Decoding for Inversion Transduc-
tion Grammar with a bigram language model can be
done in O(n6) time. This is the same complexity
as the ITG alignment algorithm used by Wu (1997)
and others, meaning complete Viterbi decoding is
possible without pruning for realistic-length sen-
tences. More generally, ITG with an m-gram lan-
guage model is O(n3+3(m?1)), and a synchronous
context-free grammar with at most R spans in the
input language is O(n3(m?1)+(2R+1)). While this
improves on previous algorithms, the degree in n
is probably still too high for complete search to
be practical with such models. The interaction of
the hook technique with pruning is an interesting
71
Algorithm 1 ITGDecode(Nt)
for all s, t such that 0 ? s < t ? Nt do
for all S such that s < S < t do
 straight rule
for all rules X ? [Y Z] ? G do
for all (Y, u1, v1) possible for the span of (s, S) do
 a hook who is on (S, t), nonterminal as Z, and outside expectation being v1 is required
if not exist hooks(S, t, Z, v1) then
build hooks(S, t, Z, v1)
end if
for all v2 possible for the hooks in (S, t, Z, v1) do
 combining a hook and a hypothesis, using straight rule
?(s, t, X, u1, v2) =
max
{
?(s, t, X, u1, v2), ?(s, S, Y, u1, v1) ? ?+(S, t, Z, v1, v2) ? P (X ? [Y Z])
}
end for
end for
end for
 inverted rule
for all rules X ? ?Y Z? ? G do
for all (Z, u2, v2) possible for the span of (S, t) do
 a hook who is on (s, S), nonterminal as Y , and outside expectation being v2 is required
if not exist hooks(s, S, Y, v2) then
build hooks(s, S, Y, v2)
end if
for all v1 possible for the hooks in (s, S, Y, v2) do
 combining a hook and a hypothesis, using inverted rule
?(s, t, X, u2, v1) =
max
{
?(s, t, X, u2, v1), ?(S, t, Z, u2, v2) ? ?+(s, S, Y, v2, v1) ? P (X ? ?Y Z?)
}
end for
end for
end for
end for
end for
routine build hooks(s, t, X, v?)
for all (X, u, v) possible for the span of (s, t) do
 combining a bigram with a hypothesis
?+(s, t, X, v?, v) =
max
{
?+(s, t, X, v?, v), bigram(v?, u) ? ?(s, t, X, u, v)
}
end for
72
area for future work. Building the chart items with
hooks may take more time than it saves if many of
the hooks are never combined with complete con-
stituents due to aggressive pruning. However, it may
be possible to look at the contents of the chart in or-
der to build only those hooks which are likely to be
useful.
References
Aho, Albert V. and Jeffery D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling, volume 1.
Englewood Cliffs, NJ: Prentice-Hall.
Charniak, Eugene. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of the Fourteenth National Conference on Arti-
ficial Intelligence (AAAI-97), pages 598?603, Menlo
Park, August. AAAI Press.
Collins, Michael. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Conference of the Association for Compu-
tational Linguistics (ACL-97), pages 16?23, Madrid,
Spain.
Eisner, Jason and Giorgio Satta. 1999. Efficient parsing
for bilexical context-free grammars and head automa-
ton grammars. In 37th Annual Meeting of the Associ-
ation for Computational Linguistics.
Melamed, I. Dan. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of the 2003 Meeting
of the North American chapter of the Association for
Computational Linguistics (NAACL-03), Edmonton.
Wu, Dekai. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In 34th Annual Meeting
of the Association for Computational Linguistics.
Wu, Dekai. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
73
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 224?231,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Efficient Search for Inversion Transduction Grammar
Hao Zhang and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
We develop admissible A* search heuris-
tics for synchronous parsing with Inver-
sion Transduction Grammar, and present
results both for bitext alignment and for
machine translation decoding. We also
combine the dynamic programming hook
trick with A* search for decoding. These
techniques make it possible to find opti-
mal alignments much more quickly, and
make it possible to find optimal transla-
tions for the first time. Even in the pres-
ence of pruning, we are able to achieve
higher BLEU scores with the same amount
of computation.
1 Introduction
The Inversion Transduction Grammar (ITG) of
Wu (1997) is a syntactically motivated algorithm
for producing word-level alignments of pairs of
translationally equivalent sentences in two lan-
guages. The algorithm builds a synchronous parse
tree for both sentences, and assumes that the trees
have the same underlying structure but that the or-
dering of constituents may differ in the two lan-
guages. ITG imposes constraints on which align-
ments are possible, and these constraints have
been shown to be a good match for real bitext data
(Zens and Ney, 2003).
A major motivation for the introduction of ITG
was the existence of polynomial-time algorithms
both for alignment and translation. Alignment,
whether for training a translation model using EM
or for finding the Viterbi alignment of test data,
is O(n6) (Wu, 1997), while translation (decod-
ing) is O(n7) using a bigram language model, and
O(n11) with trigrams. While polynomial-time al-
gorithms are a major improvement over the NP-
complete problems posed by the alignment models
of Brown et al (1993), the degree of these polyno-
mials is high, making both alignment and decod-
ing infeasible for realistic sentences without very
significant pruning. In this paper, we explore use
of the ?hook trick? (Eisner and Satta, 1999; Huang
et al, 2005) to reduce the asymptotic complexity
of decoding, and the use of heuristics to guide the
search.
Our search heuristics are a conservative esti-
mate of the outside probability of a bitext cell in
the complete synchronous parse. Some estimate
of this outside probability is a common element
of modern statistical (monolingual) parsers (Char-
niak et al, 1998; Collins, 1999), and recent work
has developed heuristics that are admissible for A*
search, guaranteeing that the optimal parse will
be found (Klein and Manning, 2003). We extend
this type of outside probability estimate to include
both word translation and n-gram language model
probabilities. These measures have been used to
guide search in word- or phrase-based MT sys-
tems (Wang and Waibel, 1997; Och et al, 2001),
but in such models optimal search is generally not
practical even with good heuristics. In this paper,
we show that the same assumptions that make ITG
polynomial-time can be used to efficiently com-
pute heuristics which guarantee us that we will
find the optimal alignment or translation, while
significantly speeding the search.
2 Inversion Transduction Grammar
An Inversion Transduction Grammar can generate
pairs of sentences in two languages by recursively
applying context-free bilingual production rules.
Most work on ITG has focused on the 2-normal
form, which consists of unary production rules
that are responsible for generating word pairs:
X ? e/f
224
and binary production rules in two forms that are
responsible for generating syntactic subtree pairs:
X ? [Y Z]
and
X ? ?Y Z?
The rules with square brackets enclosing the
right hand side expand the left hand side symbol
into the two symbols on the right hand side in the
same order in the two languages, whereas the rules
with pointed brackets expand the left hand side
symbol into the two right hand side symbols in re-
verse order in the two languages.
3 A* Viterbi Alignment Selection
A* parsing is a special case of agenda-based chart
parsing, where the priority of a node X[i, j] on the
agenda, corresponding to nonterminal X spanning
positions i through j, is the product of the node?s
current inside probability with an estimate of the
outside probability. By the current inside proba-
bility, we mean the probability of the so-far-most-
probable subtree rooted on the node X[i, j], with
leaves being iwj , while the outside probability isthe highest probability for a parse with the root
being S[0, N ] and the sequence 0wiXjwn formingthe leaves. The node with the highest priority is re-
moved from the agenda and added to the chart, and
then explored by combining with all of its neigh-
boring nodes in the chart to update the priorities
of the resulting nodes on the agenda. By using
estimates close to the actual outside probabilities,
A* parsing can effectively reduce the number of
nodes to be explored before putting the root node
onto the chart. When the outside estimate is both
admissible and monotonic, whenever a node is put
onto the chart, its current best inside parse is the
Viterbi inside parse.
To relate A* parsing with A* search for find-
ing the lowest cost path from a certain source
node to a certain destination node in a graph, we
view the forest of all parse trees as a hypergraph.
The source node in the hypergraph fans out into
the nodes of unit spans that cover the individual
words. From each group of children to their par-
ent in the forest, there is a hyperedge. The destina-
tion node is the common root node for all the parse
trees in the forest. Under the mapping, a parse is a
hyperpath from the source node to the destination
node. The Viterbi parse selection problem thus be-
comes finding the lowest-cost hyperpath from the
source node to the destination node. The cost in
this scenario is thus the negative of log probabil-
ity. The inside estimate and outside estimate natu-
rally correspond to the g? and h? for A* searching,
respectively.
A stochastic ITG can be thought of as a stochas-
tic CFG extended to the space of bitext. A node in
the ITG chart is a bitext cell that covers a source
substring and a target substring. We use the no-
tion of X[l, m, i, j] to represent a tree node in ITG
parse. It can potentially be combined with any
bitext cells at the four corners, as shown in Fig-
ure 1(a).
Unlike CFG parsing where the leaves are fixed,
the Viterbi ITG parse selection involves finding
the Viterbi alignment under ITG constraint. Good
outside estimates have to bound the outside ITG
Viterbi alignment probability tightly.
3.1 A* Estimates for Alignment
Under the ITG constraints, each source language
word can be aligned with at most one target lan-
guage word and vice versa. An ITG constituent
X[l, m, i, j] implies that the words in the source
substring in the span [l, m] are aligned with the
words in the target substring [i, j]. It further im-
plies that the words outside the span [l, m] in the
source are aligned with the words outside the span
[i, j] in the target language. Figure 1(b) displays
the tic-tac-toe pattern for the inside and outside
components of a particular cell. To estimate the
upper bound of the ITG Viterbi alignment proba-
bility for the outside component with acceptable
complexity, we need to relax the ITG constraint.
Instead of ensuring one-to-one in both directions,
we use a many-to-one constraint in one direction,
and we relax all constraints on reordering within
the outside component.
The many-to-one constraint has the same dy-
namic programming structure as IBM Model 1,
where each target word is supposed to be trans-
lated from any of the source words or the NULL
symbol. In the Model 1 estimate of the outside
probability, source and target words can align us-
ing any combination of points from the four out-
side corners of the tic-tac-toe pattern. Thus in
Figure 1(b), there is one solid cell (correspond-
ing to the Model 1 Viterbi alignment) in each col-
umn, falling either in the upper or lower outside
shaded corner. This can be also be thought of as
squeezing together the four outside corners, creat-
225
lm
0 i j N
l
m
0 i j N
l
m
n
0 i j k N
(a) (b) (c)
Figure 1: (a) A bitext cell X[l, m, i, j] (shaded) for ITG parsing. The inside cell can be combined with
adjacent cells in the four outside corners (lighter shading) to expand into larger cells. One possible
expansion to the lower left corner is displayed. (b) The tic-tac-toe pattern of alignments consistent with
a given cell. If the inner box is used in the final synchronous parse, all other alignments must come
from the four outside corners. (c) Combination of two adjacent cells shown with region for new outside
heuristic.
ing a new cell whose probability is estimated using
IBM Model 1. In contrast, the inside Viterbi align-
ment satisfies the ITG constraint, implying only
one solid cell in each column and each row. Math-
ematically, our Model 1 estimate for the outside
component is:
hM1(l, m, i, j) =
?
t<i,
t>j
max
s<l,
s>m
P (ft, es)
This Model 1 estimate is admissible. Maximiz-
ing over each column ensures that the translation
probability for each target word is greater than or
equal to the corresponding word translation prob-
ability under the ITG constraint. Model 1 virtually
assigns a probability of 1 for deleting any source
word. As a product of word-to-word translation
probabilities including deletions and insertions,
the ITG Viterbi alignment probability cannot be
higher than the product of maximal word-to-word
translation probabilities using the Model 1 esti-
mate.
The Model 1 estimate is also monotonic, a prop-
erty which is best understood geometrically. A
successor state to cell (l, m, i, j) in the search is
formed by combining the cell with a cell which
is adjacent at one of the four corners, as shown
in Figure 1(c). Of the four outside corner regions
used in calculating the search heuristic, one will
be the same for the successor state, and three will
be a subset of the old corner region. Without
loss of generality, assume we are combining a cell
(m, n, j, k) that is adjacent to (l, m, i, j) to the up-
per right. We define
HM1(l, m, i, j) = ? log hM1(l, m, i, j)
as the negative log of the heuristic in order to cor-
respond to an estimated cost or distance in search
terminology. Similarly, we speak of the cost of a
chart entry c(X[l, m, i, j]) as its negative log prob-
ability, and the cost of a cell c(l, m, i, j) as the
cost of the best chart entry with the boundaries
(l, m, i, j). The cost of the cell (m, n, j, k) which
is being combined with the old cell is guaranteed
to be greater than the contribution of the columns
j through k to the heuristic HM1(l, m, i, j). Thecontribution of the columns k through N to the
new heuristic HM1(l, n, i, k) is guaranteed to begreater in cost than their contribution to the old
heuristic. Thus,
HM1(l, m, i, j) ? c(m, n, j, k) + c(X ? Y Z)
+ HM1(l, n, i, k)
meaning that the heuristic is monotonic or consis-
tent.
The Model 1 estimate can be applied in both
translation directions. The estimates from both
directions are an upper bound of the actual ITG
Viterbi probability. By taking the minimum of the
two, we can get a tighter upper bound.
We can precompute the Model 1 outside esti-
mate for all bitext cells before parsing starts. A
na??ve implementation would take O(n6) steps of
computation, because there are O(n4) cells, each
of which takes O(n2) steps to compute its Model 1
probability. Fortunately, exploiting the recursive
226
ju v
</S><S>
i
Figure 2: The region within the dashed lines is the translation hypothesis X[i, j, u, v]. The word sequence
on the top is the Viterbi translation of the sentence on the bottom. Wide range word order change may
happen.
nature of the cells, we can compute values for the
inside and outside components of each cell using
dynamic programming in O(n4) time (Zhang and
Gildea, 2005).
4 A* Decoding
The of ITG decoding algorithm of Wu (1996) can
be viewed as a variant of the Viterbi parsing al-
gorithm for alignment selection. The task of stan-
dard alignment is to find word level links between
two fixed-order strings. In the decoding situation,
while the input side is a fixed sequence of words,
the output side is a bag of words to be linked with
the input words and then reordered. Under the ITG
constraint, if the target language substring [i, j] is
translated into s1 in the source language and thetarget substring [j, k] is translated into s2, then s1and s2 must be consecutive in the source languageas well and two possible orderings, s1s2 and s2s1,are allowed. Finding the best translation of the
substring of [i, k] involves searching over all pos-
sible split points j and two possible reorderings
for each split. In theory, the inversion probabilities
associated with the ITG rules can do the job of re-
ordering. However, a language model as simple as
bigram is generally stronger. Using an n-gram lan-
guage model implies keeping at least n?1 bound-
ary words in the dynamic programming table for a
hypothetical translation of a source language sub-
string. In the case of a bigram ITG decoder, a
translation hypothesis for the source language sub-
string [i, j] is denoted as X[i, j, u, v], where u and
v are the left boundary word and right boundary
word of the target language counterpart.
As indicated by the similarity of parsing item
notation, the dynamic programming property of
the Viterbi decoder is essentially the same as the
bitext parsing for finding the underlying Viterbi
alignment. By permitting translation from the null
target string of [i, i] into source language words as
many times as necessary, the decoder can translate
an input sentence into a longer output sentence.
When there is the null symbol in the bag of candi-
date words, the decoder can choose to translate a
word into null to decrease the output length. Both
insertions and deletions are special cases of the bi-
text parsing items.
Given the similarity of the dynamic program-
ming framework to the alignment problem, it is
not surprising that A* search can also be ap-
plied in a similar way. The initial parsing items
on the agenda are the basic translation units:
X[i, i + 1, u, u], for normal word-for-word trans-
lations and deletions (translations into nothing),
and also X[i, i, u, u], for insertions (translations
from nothing). The goal item is S[0, N, ?s?, ?/s?],
where ?s? stands for the beginning-of-sentence
symbol and ?/s? stands for the end-of-sentence
symbol. The exploration step of the A* search
is to expand the translation hypothesis of a sub-
string by combining with neighboring translation
hypotheses. When the outside estimate is admis-
sible and monotonic, the exploration is optimal
in the sense that whenever a hypothesis is taken
from the top of the agenda, it is a Viterbi transla-
tion of the corresponding target substring. Thus,
when S[0, N, ?s?, ?/s?] is added to the chart, we
have found the Viterbi translation for the entire
sentence.
227
?(X[i, j, u, v]) = max
{
???(X[i, j, u, v]), ?[](X[i, j, u, v])
}
?[](X[i, j, u, v]) = maxk,v1,u2,Y,Z
[
?(Y [i, k, u, v1]) ? ?(Z[k, j, u2, v]) ? P (X ? [Y Z]) ? Plm(u2 | v1)
]
= max
k,u2,Y,Z
[
max
v1
[
?(Y [i, k, u, v1]) ? Plm(u2 | v1)
]
? P (X ? [Y Z]) ? ?(Z[k, j, u2, v])
]
Figure 3: Top: An ITG decoding constituent can be built with either a straight or an inverted rule.
Bottom: An efficient factorization for straight rules.
4.1 A* Estimates for Translation
The key to the success of A* decoding is an out-
side estimate that combines word-for-word trans-
lation probabilities and n-gram probabilities. Fig-
ure 2 is the picture of the outside translations
and bigrams of a particular translation hypothesis
X[i, j, u, v].
Our heuristic involves precomputing two val-
ues for each word in the input string, involving
forward- and backward-looking language model
probabilities. For the forward looking value hf atinput position n, we take a maximum over the set
of words Sn that the input word tn can be trans-lated as:
hf (n) = maxs?Sn
[
Pt(s | tn) max
s??S
Plm(s? | s)
]
where:
S =
?
n
Sn
is the set of all possible translations for all words
in the input string. While hf considers lan-guage model probabilities for words following s,
the backward-looking value hb considers languagemodel probabilities for s given possible preceding
words:
hb(n) = maxs?Sn
[
Pt(s | tn) max
s??S
Plm(s | s?)
]
Our overall heuristic for a partial translation
hypothesis X[i, j, u, v] combines language model
probabilities at the boundaries of the input sub-
string with backward-looking values for the pre-
ceding words, and forward-looking values for the
following words:
h(i, j, u, v) =
[
max
s?S
Plm(u | s)
] [
max
s?S
Plm(s | v)
]
?
?
n<i,
n>j
max [hb(n), hf (n)]
Because we don?t know whether a given input
word will appear before or after the partial hypoth-
esis in the final translation, we take the maximum
of the forward and backward values for words out-
side the span [i, j].
4.2 Combining the Hook Trick with A*
The hook trick is a factorization technique for dy-
namic programming. For bilexical parsing, Eis-
ner and Satta (1999) pointed out we can reduce
the complexity of parsing from O(n5) to O(n4)
by combining the non-head constituents with the
bilexical rules first, and then combining the resul-
tant hook constituents with the head constituents.
By doing so, the maximal number of interactive
variables ranging over n is reduced from 5 to 4.
For ITG decoding, we can apply a similar factor-
ization trick. We describe the bigram-integrated
decoding case here, and refer to Huang et al
(2005) for more detailed discussion. Figure 3
shows how to decompose the expression for the
case of straight rules; the same method applies to
inverted rules. The number of free variables on the
right hand side of the second equation is 7: i, j, k,
u, v, v1, and u2.1 After factorization, counting thefree variables enclosed in the innermost max oper-
ator, we get five: i, k, u, v1, and u2. The decompo-sition eliminates one free variable, v1. In the out-ermost level, there are six free variables left. The
maximum number of interacting variables is six
overall. So, we reduced the complexity of ITG de-
coding using bigram language model from O(n7)
to O(n6). If we visualize an ITG decoding con-
stituent Y extending from source language posi-
tion i to k and target language boundary words u
and v1 with a diagram:
Y
i k
u v1
1X , Y , and Z range over grammar nonterminals, of which
there are a constant number.
228
 0
 50
 100
 150
 200
 250
 300
 350
 400
 450
 500
 0  10  20  30  40  50  60  70
se
co
n
ds
sentence length
full
uniform
ibm1encn
ibm1sym
 0
 1e+06
 2e+06
 3e+06
 4e+06
 5e+06
 6e+06
 0  10  20  30  40  50  60  70
# 
ar
cs
max sentence length
full
uniform
ibm1encn
ibm1sym
Figure 4: Speed of various techniques for finding the optimal alignment.
the hook corresponding to the innermost max op-
erator in the equation can be visualized as follows:
Y
i k
u u2
with the expected language model state u2 ?hang-ing? outside the target language string.
The trick is generic to the control strategies of
actual parsing, because the hooks can be treated
as just another type of constituent. Building hooks
is like applying special unary rules on top of non-
hooks. In terms of of outside heuristic for hooks,
there is a slight difference from that for non-hooks:
h(i, j, u, v) =
[
max
s?S
Plm(s | v)
]
?
?
n<i,
n>j
max [hb(n), hf (n)]
That is, we do not need the backward-looking es-
timate for the left boundary word u.
5 Experiments
We tested the performance of our heuristics for
alignment on a Chinese-English newswire corpus.
Probabilities for the ITG model were trained using
Expectation Maximization on a corpus of 18,773
sentence pairs with a total of 276,113 Chinese
words and 315,415 English words. For EM train-
ing, we limited the data to sentences of no more
than 25 words in either language. Here we present
timing results for finding the Viterbi alignment of
longer sentences using this fixed translation model
with different heuristics. We compute alignments
on a total of 117 test sentences, which are broken
down by length as shown in Table 1.
Length # sentences
0-9 5
10?19 26
20?29 29
30?39 22
40?49 24
50?59 10
60 1
Table 1: Length of longer sentence in each pair
from test data.
method time speedup
full 815s ?
uniform 547s 1.4
ibm1encn 269s 3.0
ibm1sym 205s 3.9
Table 2: Total time for each alignment method.
Results are presented both in terms of time and
the number of arcs added to the chart before the
optimal parse is found. Full refers to exhaus-
tive parsing, that is, building a complete chart
with all n4 arcs. Uniform refers to a best-first
parsing strategy that expands the arcs with the
highest inside probability at each step, but does
not incorporate an estimate of the outside proba-
bility. Ibm1encn denotes our heuristic based on
IBM model 1, applied to translations from English
to Chinese, while ibm1sym applies the Model 1
heuristic in both translation directions and takes
the minimum. The factor by which times were de-
creased was found to be roughly constant across
different length sentences. The alignment times
for the entire test set are shown in Table 2, the
best heuristic is 3.9 times faster than exhaustive
229
 0
 2e+06
 4e+06
 6e+06
 8e+06
 1e+07
 1.2e+07
 1.4e+07
 0  5  10  15  20
# 
hy
pe
re
dg
es
input sentence length
BI-UNIFORM
BI-HOOK-UNIFORM
BI-HOOK-A*
BI-HOOK-A*-BEAM
 13.7
 13.8
 13.9
 14
 14.1
 14.2
 14.3
 14.4
 14.5
 0  100  200  300  400  500  600  700
bl
eu
average number of arcs (unit is 1k)
BI-HOOK-A*+BEAM
BI-CYK-BEAM
Figure 5: On the left, we compare decoding speed for uniform outside estimate best-first decoding with
and without the hook trick, as well as results using our heuristic (labeled A*) and with beam pruning
(which no longer produces optimal results). On the right, we show the relationship between computation
time and BLEU scores as the pruning threshold is varied for both A* search and bottom-up CYK parsing.
dynamic programming.
We did our ITG decoding experiments on the
LDC 2002 MT evaluation data set for translation
of Chinese newswire sentences into English. The
evaluation data set has 10 human translation refer-
ences for each sentence. There are a total of 371
Chinese sentences of no more than 20 words in
the data set. These sentences are the test set for
our different versions of ITG decoders using both
a bigram language model and a trigram language
model. We evaluate the translation results by com-
paring them against the reference translations us-
ing the BLEU metric. The word-for-word transla-
tion probabilities are from the translation model
of IBM Model 4 trained on a 160-million-word
English-Chinese parallel corpus using GIZA++.
The language model is trained on a 30-million-
word English corpus. The rule probabilities for
ITG are from the same training as in the alignment
experiments described above.
We compared the BLEU scores of the A* de-
coder and the ITG decoder that uses beam ratio
pruning at each stage of bottom-up parsing. In the
case of bigram-integrated decoding, for each input
word, the best 2 translations are put into the bag of
output words. In the case of trigram-integrated de-
coding, top 5 candidate words are chosen. The A*
decoder is guaranteed to find the Viterbi transla-
tion that maximizes the product of n-grams prob-
abilities, translation probabilities (including inser-
tions and deletions) and inversion rule probabili-
ties by choosing the right words and the right word
order subject to the ITG constraint.
Figure 5 (left) demonstrates the speedup ob-
Decoder Combinations BLEU
BI-UNIFORM 8.02M 14.26
BI-HOOK-A* 2.10M 14.26
BI-HOOK-A*-BEAM 0.40M 14.43
BI-CYK-BEAM 0.20M 14.14
Table 3: Decoder speed and BLEU scores for bi-
gram decoding.
Decoder Cbns BLEU
TRI-A*-BEAM(10?3) 213.4M 17.83
TRI-A*-BEAM(10?2) 20.7M 17.09
TRI-CYK-BEAM(10?3) 21.2M 16.86
Table 4: Results for trigram decoding.
tained through the hook trick, the heuristic, and
pruning, all based on A* search. Table 3 shows the
improvement of BLEU score after applying the A*
algorithm to find the optimal translation under the
model. Figure 5 (right) investigates the relation-
ship between the search effort and BLEU score for
A* and bottom-up CYK parsing, both with prun-
ing. Pruning for A* works in such a way that we
never explore a low probability hypothesis falling
out of a certain beam ratio of the best hypothesis
within the bucket of X[i, j, ?, ?], where ? means
any word. Table 4 shows results for trigram-
integrated decoding. However, due to time con-
straint, we have not explored time/performance
tradeoff as we did for bigram decoding.
The number of combinations in the table is
the average number of hyperedges to be explored
in searching, proportional to the total number of
230
computation steps.
6 Conclusion
A* search for Viterbi alignment selection under
ITG is efficient using IBM Model 1 as an outside
estimate. The experimental results indicate that
despite being a more relaxed word-for-word align-
ment model than ITG, IBM Model 1 can serve
as an efficient and reliable approximation of ITG
in terms of Viterbi alignment probability. This is
more true when we apply Model 1 to both trans-
lation directions and take the minimum of both.
We have also tried to incorporate estimates of bi-
nary rule probabilities to make the outside esti-
mate even sharper. However, the further improve-
ment was marginal.
We are able to find the ITG Viterbi translation
using our A* decoding algorithm with an outside
estimate that combines outside bigrams and trans-
lation probabilities for outside words. The hook
trick gave us a significant further speedup; we be-
lieve this to be the first demonstrated practical ap-
plication of this technique.
Interestingly, the BLEU score for the opti-
mal translations under the probabilistic model is
lower than we achieve with our best bigram-
based system using pruning. However, this sys-
tem makes use of the A* heuristic, and our
speed/performance curve shows that the heuris-
tic allows us to achieve higher BLEU scores with
the same amount of computation. In the case of
trigram integrated decoding, there is 1 point of
BLEU score improvement by moving from a typ-
ical CYK plus beam search decoder to a decoder
using A* plus beam search.
However, without knowing what words will ap-
pear in the output language, a very sharp outside
estimate to further bring down the number of com-
binations is difficult to achieve.
The brighter side of the move towards optimal
decoding is that the A* search strategy leads us
to the region of the search space that is close to
the optimal result, where we can more easily find
good translations.
Acknowledgments This work was supported
by NSF ITR IIS-09325646 and NSF ITR IIS-
0428020.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Eugene Charniak, Sharon Goldwater, and Mark John-
son. 1998. Edge-based best-first chart parsing. In
Proceedings of the Sixth Workshop on Very Large
Corpora.
Michael John Collins. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In 37th Annual Meeting of the
Association for Computational Linguistics.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005.
Machine translation as lexicalized parsing with
hooks. In International Workshop on Parsing Tech-
nologies (IWPT05), Vancouver, BC.
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact viterbi parse selection. In Pro-
ceedings of the 2003 Meeting of the North American
chapter of the Association for Computational Lin-
guistics (NAACL-03).
Franz Josef Och, Nicola Ueffing, and Herman Ney.
2001. An efficient a* search algorithm for statis-
tical machine translation. In Proceedings of the
ACL Workshop on Data-Driven Machine Transla-
tion, pages 55?62, Toulouse, France.
Ye-Yi Wang and Alex Waibel. 1997. Decoding algo-
rithm in statistical machine translation. In 35th An-
nual Meeting of the Association for Computational
Linguistics.
Dekai Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In 34th Annual Meeting
of the Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Richard Zens and Hermann Ney. 2003. A compara-
tive study on reordering constraints in statistical ma-
chine translation. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics, Sapporo, Japan.
Hao Zhang and Daniel Gildea. 2005. Stochastic lex-
icalized inversion transduction grammar for align-
ment. In Proceedings of the 43rd Annual Confer-
ence of the Association for Computational Linguis-
tics (ACL-05), Ann Arbor, MI.
231
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 25?32,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Factorization of Synchronous Context-Free Grammars in Linear Time
Hao Zhang and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
Factoring a Synchronous Context-Free
Grammar into an equivalent grammar with
a smaller number of nonterminals in each
rule enables synchronous parsing algo-
rithms of lower complexity. The prob-
lem can be formalized as searching for the
tree-decomposition of a given permutation
with the minimal branching factor. In this
paper, by modifying the algorithm of Uno
and Yagiura (2000) for the closely related
problem of finding all common intervals
of two permutations, we achieve a linear
time algorithm for the permutation factor-
ization problem. We also use the algo-
rithm to analyze the maximum SCFG rule
length needed to cover hand-aligned data
from various language pairs.
1 Introduction
A number of recent syntax-based approaches to
statistical machine translation make use of Syn-
chronous Context Free Grammar (SCFG) as the un-
derlying model of translational equivalence. Wu
(1997)?s Inversion Transduction Grammar, as well
as tree-transformation models of translation such as
Yamada and Knight (2001), Galley et al (2004), and
Chiang (2005) all fall into this category.
A crucial question for efficient computation in ap-
proaches based on SCFG is the length of the gram-
mar rules. Grammars with longer rules can represent
a larger set of reorderings between languages (Aho
and Ullman, 1972), but also require greater compu-
tational complexity for word alignment algorithms
based on synchronous parsing (Satta and Peserico,
2005). Grammar rules extracted from large paral-
lel corpora by systems such as Galley et al (2004)
can be quite large, and Wellington et al (2006) ar-
gue that complex rules are necessary by analyzing
the coverage of gold-standard word alignments from
different language pairs by various grammars.
However, parsing complexity depends not only
on rule length, but also on the specific permutations
represented by the individual rules. It may be possi-
ble to factor an SCFG with maximum rule length
n into a simpler grammar with a maximum of k
nonterminals in any one rule, if not all n! permuta-
tions appear in the rules. Zhang et al (2006) discuss
methods for binarizing SCFGs, ignoring the non-
binarizable grammars; in Section 2 we discuss the
generalized problem of factoring to k-ary grammars
for any k and formalize the problem as permutation
factorization in Section 3.
In Section 4, we describe an O(k ? n) left-to-
right shift-reduce algorithm for analyzing permuta-
tions that can be k-arized. Its time complexity be-
comes O(n2) when k is not specified beforehand
and the minimal k is to be discovered. Instead of
linearly shifting in one number at a time, Gildea
et al (2006) employ a balanced binary tree as the
control structure, producing an algorithm similar in
spirit to merge-sort with a reduced time complex-
ity of O(n logn). However, both algorithms rely
on reduction tests on emerging spans which involve
redundancies with the spans that have already been
tested.
25
Uno and Yagiura (2000) describe a clever algo-
rithm for the problem of finding all common inter-
vals of two permutations in time O(n + K), where
K is the number of common intervals, which can
itself be ?(n2). In Section 5, we adapt their ap-
proach to the problem of factoring SCFGs, and show
that, given this problem definition, running time can
be improved to O(n), the optimum given the time
needed to read the input permutation.
The methodology in Wellington et al (2006) mea-
sures the complexity of word alignment using the
number of gaps that are necessary for their syn-
chronous parser which allows discontinuous spans
to succeed in parsing. In Section 6, we provide a
more direct measurement using the minimal branch-
ing factor yielded by the permutation factorization
algorithm.
2 Synchronous CFG and Synchronous
Parsing
We begin by describing the synchronous CFG for-
malism, which is more rigorously defined by Aho
and Ullman (1972) and Satta and Peserico (2005).
We adopt the SCFG notation of Satta and Peserico
(2005). Superscript indices in the right-hand side of
grammar rules:
X ? X(1)1 ...X(n)n , X
(pi(1))
pi(1) ...X
(pi(n))
pi(n)
indicate that the nonterminals with the same index
are linked across the two languages, and will eventu-
ally be rewritten by the same rule application. Each
Xi is a variable which can take the value of any non-
terminal in the grammar.
We say an SCFG is n-ary if and only if the max-
imum number of co-indexed nonterminals, i.e. the
longest permutation contained in the set of rules, is
of size n.
Given a synchronous CFG and a pair of input
strings, we can apply a generalized CYK-style bot-
tom up chart parser to build synchronous parse
trees over the string pair. Wu (1997) demonstrates
the case of binary SCFG parsing, where six string
boundary variables, three for each language as in
monolingual CFG parsing, interact with each other,
yielding an O(N6) dynamic programming algo-
rithm, where N is the string length, assuming the
two paired strings are comparable in length. For an
n-ary SCFG, the parsing complexity can be as high
as O(Nn+4). The reason is even if we binarize on
one side to maintain 3 indices, for many unfriendly
permutations, at most n + 1 boundary variables in
the other language are necessary.
The fact that this bound is exponential in the rule
length n suggests that it is advantageous to reduce
the length of grammar rules as much as possible.
This paper focuses on converting an SCFG to the
equivalent grammar with smallest possible maxi-
mum rule size. The algorithm processes each rule
in the input grammar independently, and determines
whether the rule can be factored into smaller SCFG
rules by analyzing the rule?s permutation pi.
As an example, given the input rule:
[X ? A(1)B(2)C(3)D(4)E(5)F (6)G(7),
X ? E(5)G(7)D(4)F (6)C(3)A(1)B(2) ] (1)
we consider the associated permutation:
(5, 7, 4, 6, 3, 1, 2)
We determine that this permutation can be fac-
tored into the following permutation tree:
(2,1)
(2,1)
(2,4,1,3)
5 7 4 6
3
(1,2)
1 2
We define permutation trees formally in the next
section, but note here that nodes in the tree corre-
spond to subsets of nonterminals that form a sin-
gle continuous span in both languages, as shown by
the shaded regions in the permutation matrix above.
This tree can be converted into a set of output rules
that are generatively equivalent to the original rule:
[X ? X(1)1 X
(2)
2 , X ? X
(2)
2 X
(1)
1 ]
[X1 ? A(1)B(2), X1 ? A(1)B(2) ]
[X2 ? C(1)X(2)3 , X2 ? X
(2)
3 C(1) ]
[X3 ? D(1)E(2)F (3)G(4),
X3 ? E(2)G(4)D(1)F (3) ]
where X1, X2 and X3 are new nonterminals used to
represent the intermediate states in which the syn-
chronous nodes are combined. The factorized gram-
mar is only larger than the original grammar by a
constant factor.
26
3 Permutation Trees
We define the notion of permutation structure in this
section. We define a permuted sequence as a per-
mutation of n (n ? 1) consecutive natural numbers.
A permuted sequence is said to be k-ary parsable
if either of the following conditions holds:
1. The permuted sequence only has one number.
2. It has more than one number and can be seg-
mented into k? (k ? k? ? 2) permuted se-
quences each of which is k-ary parsable, and
the k? subsequences are arranged in an order
identified by one of the k?! permutations of k?.
This is a recursive definition, and we call the cor-
responding recursive structure over the entire se-
quence a k-ary permutation tree.
Our goal is to find out the k-ary permutation tree
for a given permutation, where k is minimized.
4 Shift-reduce on Permutations
In this section, we present an O(n ? k) algorithm
which can be viewed as a need-to-be-optimized ver-
sion of the linear time algorithm to be presented in
the next section.
The algorithm is based on a shift-reduce parser,
which maintains a stack for subsequences that have
been discovered so far and loops over shift and re-
duce steps:
1. Shift the next number in the input permutation
onto the stack.
2. Go down the stack from the top to the bottom.
Whenever the top m subsequences satisfy the
partition property, which says the total length
of the m (k ? m ? 2) subsequences minus 1
is equal to the difference between the smallest
number and the largest number contained in the
m segments, make a reduction by gluing the
m segments into one subsequence and restart
reducing from the top of the new stack. Stop
when no reduction is possible.
3. If there are remaining numbers in the input per-
mutation, go to 1.
When we exit from the loop, if the height of the stack
is 1, the input permutation of n has been reduced to
Stack Input Operation
5, 7, 4, 6, 3, 1, 2 shift
5 7, 4, 6, 3, 1, 2 shift
5, 7 4, 6, 3, 1, 2 shift
5, 7, 4 6, 3, 1, 2 shift
5, 7, 4, 6 3, 1, 2 reduce by (2,4,1,3)
[4...7] 3, 1, 2 shift
[4...7], 3 1, 2 reduce by (2,1)
[3...7] 1, 2 shift
[3...7], 1 2 shift
[3...7], 1, 2 reduce by (1,2)
[3...7], [1...2] reduce by (2,1)
[1...7]
Table 1: The execution trace of the shift-reduce
parser on the input permutation 5, 7, 4, 6, 3, 1, 2.
a linear sequence of 1 to n, and parsing is success-
ful. Otherwise, the input permutation of n cannot be
parsed into a k-ary permutation tree.
An example execution trace of the algorithm is
shown in Table 1.
The partition property is a sufficient and neces-
sary condition for the top m subsequences to be re-
ducible. In order to check if the property holds, we
need to compute the sum of the lengths of subse-
quences under consideration and the difference be-
tween the largest and smallest number in the cov-
ered region. We can incrementally compute both
along with each step going down the stack. If m
is bounded by k, we need O(k) operations for each
item shifted onto the stack. So, the algorithm runs in
O(n ? k).
We might also wish to compute the minimum k
for which k-arization can be successful on an input
permutation of n. We can simply keep doing reduc-
tion tests for every possible top region of the stack
while going deeper in the stack to find the minimal
reduction. In the worst case, each time we go down
to the bottom of the increasingly higher stack with-
out a successful reduction. Thus, in O(n2), we can
find the minimum k-arization.
5 Linear Time Factorization
In this section, we show a linear time algorithm
which shares the left-to-right and bottom-up control
structure but uses more book-keeping operations to
reduce unnecessary reduction attempts. The reason
that our previous algorithm is asymptotically O(n2)
27
is that whenever a new number is shifted in, we have
to try out every possible new span ending at the new
number. Do we need to try every possible span? Let
us start with a motivating example. The permuted
sequence (5, 7, 4, 6) in Table 1 can only be reduced
as a whole block. However, in the last algorithm,
when 4 is shifted in, we make an unsuccessful at-
tempt for the span on (7, 4), knowing we are miss-
ing 5, which will not appear when we expand the
span no matter how much further to the right. Yet
we repeat the same mistake to try on 7 when 6 is
scanned in by attempting on (7, 4, 6). Such wasteful
checks result in the quadratic behavior of the algo-
rithm. The way the following algorithm differs from
and outperforms the previous algorithm is exactly
that it crosses out impossible candidates for reduc-
tions such as 7 in the example as early as possible.
Now we state our problem mathematically. We
define a function whose value indicates the re-
ducibility of each pair of positions (x, y) (1 ? x ?
y ? n):
f(x, y) = u(x, y)? l(x, y)? (y ? x)
where
l(x, y) = min
i?[x,y]
pi(i)
u(x, y) = max
i?[x,y]
pi(i)
l records the minimum of the numbers that are
permuted to from the positions in the region [x, y].
u records the maximum. Figure 1 provides the vi-
sualization of u, l, and f for the example permuta-
tion (5, 7, 4, 6, 3, 1, 2). u and l can be visualized as
stairs. u goes up from the right end to the left. l
goes down. f is non-negative, but not monotonic
in general. We can make a reduction on (x, y) if
and only if f(x, y) = 0. This is the mathemati-
cal statement of the partition property in step 2 of
the shift-reduce algorithm. u and l can be computed
incrementally from smaller spans to larger spans to
guarantee O(1) operations for computing f on each
new span of [x, y] as long as we go bottom up. In the
new algorithm, we will reduce the size of the search
space of candidate position pairs (x, y) to be linear
in n so that the whole algorithm is O(n).
The algorithm has two main ideas:
? We filter x?s to maintain the invariant that
f(x, y) (x ? y) is monotonically decreasing
with respect to x, over iterations on y (from 1
to n), so that any remaining values of x corre-
sponding to valid reductions are clustered at the
point where f tails off to zero. To put it another
way, we never have to test invalid reductions,
because the valid reductions have been sorted
together for us.
? We make greedy reductions as in the shift-
reduce algorithm.
In the new algorithm, we use a doubly linked list,
instead of a stack, as the data structure that stores
the candidate x?s to allow for more flexible main-
taining operations. The steps of the algorithm are as
follows:
1. Increase the left-to-right index y by one and ap-
pend it to the right end of the list.
2. Find the pivot x? in the list which is minimum
(leftmost) among x satisfying either u(x, y ?
1) < u(x, y) (exclusively) or l(x, y ? 1) >
l(x, y).
3. Remove those x?s that yield even smaller
u(x, y ? 1) than u(x?, y ? 1) or even larger
l(x, y ? 1) than l(x?, y ? 1). Those x?s must
be on the right of x? if they exist. They must
form a sub-list extending to the right end of the
original x list.
4. Denote the x which is immediately to the left
of x? as x?. Repeatedly remove all x?s such that
f(x, y) > f(x?, y) where x is at the left end of
the sub-list of x?s starting from x? extending to
the right.
5. Go down the pruned list from the right end, out-
put (x, y) until f(x, y) > 0. Remove x?s such
that f(x, y) = 0, sparing the smallest x which
is the leftmost among all such x?s on the list.
6. If there are remaining numbers in the input per-
mutation, go to 1.
The tricks lie in step 3 and step 4, where bad can-
didate x?s are filtered out. We use the following di-
agram to help readers understand the parts of x-list
that the two steps are filtering on.
28
x1, ..., x?,
step 4
? ?? ?
x?, ..., xi, ..., xj , ..., xk
? ?? ?
step 3
, y
The steps from 2 to 4 are the operations that main-
tain the monotonic invariant which makes the reduc-
tions in step 5 as trivial as performing output. The
stack-based shift-reduce algorithm has the same top-
level structure, but lacks steps 2 to 4 so that in step 5
we have to winnow the entire list. Both algorithms
scan left to right and examine potential reduction
spans by extending the left endpoint from right to
left given a right endpoint.
5.1 Example Execution Trace
An example of the algorithm?s execution is shown
in Figure 1. The evolution of u(x, y), l(x, y), and
f(x, y) is displayed for increasing y?s (from 2 to 7).
To identify reducible spans, we can check the plot of
f(x, y) to locate the (x, y) pairs that yield zero. The
pivots found by step 2 of the algorithm are marked
with ??s on the x-axis in the plot for u and l. The x?s
that are filtered out by step 3 or 4 are marked with
horizontal bars across. We want to point out the in-
teresting steps. When y = 3, x? = 1, x = 2 needs
to be crossed out by step 3 in the algorithm. When
y = 4, x? = 3, x = 3 itself is to be deleted by step 4
in the algorithm. x = 4 is removed at step 5 because
it is the right end in the first reduction. On the other
hand, x = 4 is also a bad starting point for future
reductions. Notice that we also remove x = 5 at
step 6, which can be a good starting point for reduc-
tions. But we exclude it from further considerations,
because we want left-most reductions.
5.2 Correctness
Now we explain why the algorithm works. Both al-
gorithms are greedy in the sense that at each scan
point we exhaustively reduce all candidate spans to
the leftmost possible point. It can be shown that
greediness is safe for parsing permutations.
What we need to show is how the monotonic in-
variant holds and is valid. Now we sketch the proof.
We want to show for all xi remaining on the list,
f(xi, y) ? f(xi+1, y). When y = 1, it is trivially
true. Now we do the induction on y step by case
analysis:
Case 1: If xi < xi+1 < x?, then f(xi, y) ?
f(xi, y ? 1) = ?1. The reason is if xi is on the
left of x?, both u(xi, y) and l(xi, y) are not changed
from the y ? 1-th step, so the only difference is that
y?xi has increased by one. Graphically, the f curve
extending to the left of x? shifts down a unit of 1. So,
the monotonic property still holds to the left of x?.
Case 2: If x? ? xi < xi+1, then f(xi, y) ?
f(xi, y ? 1) = c (c ? 0). The reason is that after
executing step 3 in the algorithm, the remaining xi?s
have either their u(xi, y) shifted up uniformly with
l(xi, y) being unchanged, or the symmetric case that
l(xi, y) is shifted down uniformly without changing
u(xi, y). In both cases, the difference between u and
l increases by at least one unit to offset the one unit
increase of y ? xi. The result is that the f curve ex-
tending from x? to the right shifts up or remains the
same.
Case 3: So the half curve of f on the left of x? is
shifting down and the half right curve on the right is
shifting up, making it necessary to consider the case
that xi is on the left and xi+1 on the right. Fortu-
nately, step 4 in the algorithm deals with this case
explicitly by cutting down the head of the right half
curve to smooth the whole curve into a monotoni-
cally decreasing one.
We still need one last piece for the proof, i.e., the
validity of pruning. Is it possible we winnow off
good x?s that will become useful in later stages of
y? The answer is no. The values we remove in step
3 and 4 are similar to the points indexing into the
second and third numbers in the permuted sequence
(5, 7, 4, 6). Any span starting from these two points
will not be reducible because the element 5 is miss-
ing.1
To summarize, we remove impossible left bound-
aries and keep good ones, resulting in the mono-
tonicity of f function which in turn makes safe
greedy reductions fast.
5.3 Implementation and Time Analysis
We use a doubly linked list to implement both the u
and l functions, where list element includes a span
of x values (shaded rectangles in Figure 1). Both
lists can be doubly linked with the list of x?s so that
1Uno and Yagiura (2000) prove the validity of step 3 and
step 4 rigorously.
29
we can access the u function and l function at O(1)
time for each x. At the same time, if we search for
x based on u or l, we can follow the stair functions,
skipping many intermediate x?s.
The total number of operations that occur at step
4 and step 5 is O(n) since these steps just involve
removing nodes on the x list, and only n nodes are
created in total over the entire algorithm. To find
x?, we scan back from the right end of u list or l
list. Due to step 3, each u (and l) element that we
scan over is removed at this iteration. So the total
number of operations accountable to step 2 and step
3 is bounded by the maximum number of nodes ever
created on the u and l lists, which is also n.
5.4 Related Work
Our algorithm is based on an algorithm for finding
all common intervals of two permutations (Uno and
Yagiura, 2000). The difference2 is in step 5, where
we remove the embedded reducible x?s and keep
only the leftmost one; their algorithm will keep all of
the reducible x?s for future considerations so that in
the example the number 3 will be able to involve in
both the reduction ([4?7], 3) and (3, [1?2]). In the
worst case, their algorithm will output a quadratic
number of reducible spans, making the whole algo-
rithm O(n2). Our algorithm is O(n) in the worst
case. We can also generate all common intervals by
transforming the permutation tree output by our al-
gorithm.
However, we are not the first to specialize the Uno
and Yagiura algorithm to produce tree structures for
permutations. Bui-Xuan et al (2005) reached a lin-
ear time algorithm in the definition framework of
PQ trees. PQ trees represent families of permuta-
tions that can be created by composing operations
of scrambling subsequences according to any per-
mutation (P nodes) and concatenating subsequences
in order (Q nodes). Our definition of permutation
tree can be thought of as a more specific version of a
PQ tree, where the nodes are all labeled with a spe-
cific permutation which is not decomposable.
2The original Uno and Yagiura algorithm also has the minor
difference that the scan point goes from right to left.
6 Experiments on Analyzing Word
Alignments
We apply the factorization algorithm to analyzing
word alignments in this section. Wellington et al
(2006) indicate the necessity of introducing discon-
tinuous spans for synchronous parsing to match up
with human-annotated word alignment data. The
number of discontinuous spans reflects the struc-
tural complexity of the synchronous rules that are
involved in building the synchronous trees for the
given alignments. However, the more direct and de-
tailed analysis would be on the branching factors of
the synchronous trees for the aligned data.
Since human-aligned data has many-to-one word
links, it is necessary to modify the alignments into
one-to-one. Wellington et al (2006) treat many-to-
one word links disjunctively in their synchronous
parser. We also commit to one of the many-one links
by extracting a maximum match (Cormen et al,
1990) from the bipartite graph of the alignment. In
other words, we abstract away the alternative links
in the given alignment while capturing the backbone
using the maximum number of word links.
We use the same alignment data for the five
language pairs Chinese/English, Romanian/English,
Hindi/English, Spanish/English, and French/English
(Wellington et al, 2006). In Table 2, we report the
number of sentences that are k-ary parsable but not
k ? 1-ary parsable for increasing k?s. Our analysis
reveals that the permutations that are accountable for
non-ITG alignments include higher order permuta-
tions such as (3, 1, 5, 2, 4), albeit sparsely seen.
We also look at the number of terminals the non-
binary synchronous nodes can cover. We are in-
terested in doing so, because this can tell us how
general these unfriendly rules are. Wellington et al
(2006) did a similar analysis on the English-English
bitext. They found out the majority of non-ITG
parsable cases are not local in the sense that phrases
of length up to 10 are not helpful in covering the
gaps. We analyzed the translation data for the five
language pairs instead. Our result differs. The right-
most column in Table 2 shows that only a tiny per-
cent of the non-ITG cases are significant in the sense
that we can not deal with them through phrases or
tree-flattening within windows of size 10.
30
y = 2:
1
*
1
2
2
3
3
4
4
5
5
6
6
7
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
7
6
f
x
y = 3:
1
*
1
2?
2
3
3
4
4
5
5
6
6
7
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
7
6
f
x
y = 4:
(1
1
2?
2
3?
*
3
4)
4
5
5
6
6
7
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
7
6
f
x
y = 5:
((1
*
1
2?
2
3?
3
4?)
4
5)
5
6
6
7
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
7
6
f
x
y = 6:
((1
*
1
2?
2
3?
3
4?)
4
5)
5
6
6
7
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
7
6
f
x
y = 7:
(((1
1
2?
2
3?
3
4?)
4
5)
5
(6
*
6
7))
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
7
6
f
x
Figure 1: Evolution of u(x, y), l(x, y), and f(x, y) as y goes from 2 to 7 for the permutation
(5, 7, 4, 6, 3, 1, 2). We use ? under the x-axis to indicate the x??s that are pivots in the algorithm. Use-
less x?s are crossed out. x?s that contribute to reductions are marked with either ( on its left or ) on its right.
For the f function, we use solid boxes to plot the values of remaining x?s on the list but also show the other
f values for completeness.
31
Branching Factor
1 2 4 5 6 7 10 ? 4 (and covering > 10 words)
Chinese/English 451 30 4 5 1 7(1.4%)
Romanian/English 195 4 0
Hindi/English 3 85 1 1 0
Spanish/English 195 4 1(0.5%)
French/English 425 9 9 3 1 6(1.3%)
Table 2: Distribution of branching factors for synchronous trees on various language pairs.
7 Conclusion
We present a linear time algorithm for factorizing
any n-ary SCFG rule into a set of k-ary rules where
k is minimized. The algorithm speeds up an easy-
to-understand shift-reduce algorithm, by avoiding
unnecessary reduction attempts while maintaining
the left-to-right bottom-up control structure. Em-
pirically, we provide a complexity analysis of word
alignments based on the concept of minimal branch-
ing factor.
References
Albert V. Aho and Jeffery D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling, volume 1.
Prentice-Hall, Englewood Cliffs, NJ.
Binh Minh Bui-Xuan, Michel Habib, and Christophe
Paul. 2005. Revisiting T. Uno and M. Yagiura?s algo-
rithm. In The 16th Annual International Symposium
on Algorithms and Computation (ISAAC?05), pages
146?155.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Conference of the Association for
Computational Linguistics (ACL-05), pages 263?270,
Ann Arbor, Michigan.
Thomas H. Cormen, Charles E. Leiserson, and Ronald L.
Rivest. 1990. Introduction to algorithms. MIT Press,
Cambridge, MA.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of the Human Language Technology Confer-
ence/North American Chapter of the Association for
Computational Linguistics (HLT/NAACL).
Daniel Gildea, Giorgio Satta, and Hao Zhang. 2006. Fac-
toring synchronous grammars by sorting. In Proceed-
ings of the International Conference on Computational
Linguistics/Association for Computational Linguistics
(COLING/ACL-06) Poster Session, Sydney.
Giorgio Satta and Enoch Peserico. 2005. Some com-
putational complexity results for synchronous context-
free grammars. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 803?810, Vancouver, Canada,
October.
Takeaki Uno and Mutsunori Yagiura. 2000. Fast algo-
rithms to enumerate all common intervals of two per-
mutations. Algorithmica, 26(2):290?309.
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the
complexity of translational equivalence. In Proceed-
ings of the International Conference on Computational
Linguistics/Association for Computational Linguistics
(COLING/ACL-06).
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Conference of the Association for Com-
putational Linguistics (ACL-01), Toulouse, France.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of the Human Lan-
guage Technology Conference/North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL).
32
Coling 2010: Poster Volume, pages 1345?1353,
Beijing, August 2010
An Empirical Study of Translation Rule Extraction with Multiple 
Parsers 
 
Tong Xiao??, Jingbo Zhu??, Hao Zhang?, Muhua Zhu?? 
 
?Natural Language Processing Lab., Northeastern University 
?Key Laboratory of Medical Image Computing, Ministry of Education 
{xiaotong,zhujingbo}@mail.neu.edu.cn 
zhanghao@ics.neu.edu.cn, zhumuhua@gmail.com 
 
Abstract 
Translation rule extraction is an impor-
tant issue in syntax-based Statistical Ma-
chine Translation (SMT). Recent studies 
show that rule coverage is one of the key 
factors affecting the success of syntax-
based systems. In this paper, we first 
present a simple and effective method to 
improve rule coverage by using multiple 
parsers in translation rule extraction, and 
then empirically investigate the effec-
tiveness of our method on Chinese-
English translation tasks. Experimental 
results show that extracting translation 
rules using multiple parsers improves a 
string-to-tree system by over 0.9 BLEU 
points on both NIST 2004 and 2005 test 
corpora. 
1 Introduction 
Recently various syntax-based models have been 
extensively investigated in Statistical Machine 
Translation (SMT), including models between 
source trees and target strings (Quirk et al, 2005; 
Liu et al, 2006; Huang et al, 2006), source 
strings and target trees (Yamada and Knight, 
2001; Galley et al, 2006; Shen et al, 2008), or 
source trees and target trees (Eisner, 2003; Ding 
and Palmer, 2005; Cowan et al, 2006; Zhang et 
al., 2008; Liu et al, 2009). In these models, au-
tomatic extraction of translation rules is an im-
portant issue, in which translation rules are typi-
cally extracted using parse trees on 
source/target-language side or both sides of the 
bilingual text. Exploiting the syntactic informa-
tion encoded in translation rules, syntax-based 
systems have shown to achieve comparable per-
formance with phrase-based systems, even out-
perform them in some cases (Marcu et al, 2006). 
Among all the factors contributing to the suc-
cess of syntax-based systems, rule coverage has 
been proved to be an important one that affects 
the translation accuracy of syntax-based systems 
(DeNeefe et al, 2007; Shen et al, 2008). How-
ever, these systems suffer from a problem that 
translation rules are extracted using only 1-best 
parse tree generated by a single parser, which 
generally results in relatively low rule coverage 
due to the limited scope in rule extraction (Mi 
and Huang, 2008). To alleviate this problem, a 
straightforward solution is to enlarge the scope 
of rule extraction, and obtain translation rules by 
using a group of diversified parse trees instead 
of a single parse tree. For example, Mi and 
Huang (2008) used k-best parses and forest to 
extract translation rules for improving the rule 
coverage in their forest-based SMT system, and 
achieved promising results. However, most pre-
vious work used the parse trees generated by 
only one parser, which still suffered somewhat 
from the relatively low diversity in the outputs 
of a single parser. 
Addressing this issue, we investigate how to 
extract diversified translation rules using multi-
ple parsers. As different parsers (or parsing 
models) can provide us with parse trees having 
relatively large diversity, we believe that it is 
beneficial to employ multiple different parsers to 
obtain diversified translation rules and thus en-
large the rule coverage. Motivated by this idea, 
we propose a simple and effective method to 
improve rule coverage by using multiple parsers 
1345
in rule extraction. Furthermore, we conduct an 
empirical study to investigate the effectiveness 
of our method on Chinese-English translation in 
a string-to-tree system. Experimental results 
show that our method improves the baseline sys-
tem by over 0.9 BLEU points on both NIST 
2004 and 2005 test corpora, even achieves a +1 
BLEU improvement when working with the k-
best extraction method. More interestingly, we 
observe that the MT performance is not very 
sensitive to the parsing performance of the pars-
ers used in rule extraction. Actually, the MT sys-
tem does not show different preferences for dif-
ferent parsers. 
2 Related Work 
In machine translation, some efforts have been 
made to improve rule coverage and advance the 
performance of syntax-based systems. For ex-
ample, Galley et al (2006) proposed the idea of 
rule composing which composes two or more 
rules with shared states to form a larger, com-
posed rule. Their experimental results showed 
that the rule composing method could signifi-
cantly improve the translation accuracy of their 
syntax-based system. Following Galley et al 
(2006)?s work, Marcu et al (2006) proposed 
SPMT models to improve the coverage of phras-
al rules, and demonstrated that the system per-
formance could be further improved by using 
their proposed models. Wang et al (2007) de-
scribed a binarization method that binarized 
parse trees to improve the rule coverage on non-
syntactic mappings. DeNeefe et al (2007) analy-
ized the phrasal coverage problem, and com-
pared the phrasal coverage as well as translation 
accuracy for various rule extraction methods 
(Galley et al, 2006; Marcu et al, 2006; Wang et 
al., 2007). 
As another research direction, some work is 
focused on enlarging the scope of rule extraction 
to improve rule coverage. For example, (Venu-
gopal et al, 2008) and (Mi and Huang, 2008) 
extracted rules from the k-best parses and forest 
generated by a single parser to alleviate the 
problem of the limited scope of 1-best parse, and 
achieved promising results. 
Our work differs from previous work in that 
we are concerned with obtaining diversified 
translation rules using multiple different parsers 
(or parsing models) instead of a single parser (or 
parsing model). It can be regarded as an en-
hancement of previous studies. As shown in the 
following parts of this paper, it works very well 
with the existing techniques, such as rule com-
posing (Galley et al, 2006), SPMT models 
(Marcu et al, 2006) and rule extraction with k-
best parses (Venugopal et al, 2008). 
3 Translation Rule Extraction 
In this work, the issue of translation rule extrac-
tion is studied in the string-to-tree model pro-
posed by Galley et al (2006).  We choose this 
model because it has been shown to be one of 
the state-of-the-art syntax-based models, and has 
been adopted in the most successful systems in 
NIST 2009 MT evaluation.  
Typically, (string-to-tree) translation rules are 
learned from the word-aligned bilingual text 
whose target-side has been parsed using a syn-
tactic parser. As the basic unit of translation, a 
translation rule consists of sequence words or 
variables in the source language, and a syntax 
tree in the target language having words (termi-
nals) and variables (non-terminals) at leaves. 
Figure 1 shows the translation rules extracted 
from a word-aligned sentence pair with a target-
side parse tree. 
 
Figure 1: Translation rules extracted from a 
string-tree pair. 
1346
 
Figure 2: Rule extraction using two different parsers (Berkeley Parser and Collins Parser). The 
shaded rectangles denote the translation rules that can be extracted from the parse tree generated by 
one parser but cannot be extracted from the parse tree generated by the other parser. 
 
To obtain basic translation rules, the (minimal) 
GHKM extraction method proposed in (Galley 
et al 2004) is utilized. The basic idea of GHKM 
extraction is to compute the set of the mini-
mally-sized translation rules that can explain the 
mappings between source-language string and 
target-language tree while respecting the align-
ment and reordering between the two languages. 
For example, from the string-tree pair shown at 
the top of Figure 1, we extract the minimal 
GHKM translation rules r1-6. In addition to 
GHKM extraction, the SPMT models (Marcu et 
al., 2006) are employed to obtain phrasal rules 
that are not covered by GHKM extraction.  For 
example, rule r8  in Figure 1 is a SPMT rule that 
is not obtained in GHKM extraction. Finally, the 
rule composing method (Galley et al, 2006) is 
used to compose two or more minimal GHKM 
or SPMT rules having shared states to form lar-
ger rules. For example, rule r7 in Figure 1 is gen-
erated by composing rules r2 and r6. 
4 Differences in Coverage between Rule 
Extractions with Different Parsers 
As described above, translation rule extraction 
relies on the outputs (parse trees) of parsers. As 
different parsers generally have large diversity 
between their outputs, rule extractions with dif-
ferent parsers generally result in very different 
sets of rules. For example, Figure 2 shows the 
rule extractions on a word-aligned sentence pair 
having two target-trees generated by Berkeley 
Parser and Collins Parser, respectively. It is ob-
served that Figure 2 (a) and (b) cover different 
sets of rule due to the different target-trees used 
in rule extraction. Particularly, well-formed rules 
ra7-a9 are extracted in Figure 2 (a), while they do 
not appear in Figure 2 (b). Also, rules rb7-b9 in 
Figure 2 (b) have the similar situation. This ob-
servation gives us an intuition that there is a 
?complementarity? between the rules extracted 
using different parsers. 
1347
We also conduct a quantitative study to inves-
tigate the impact of using different parsers 
(Berkeley Parser and Collins Parser) on rule 
coverage. Tables 1 shows the statistics of the 
rules extracted from 370K Chinese-English par-
allel sentence pairs1 using the method described 
in Section 3. In addition to the total number of 
rules extracted, the numbers of phrasal rules and 
useful rules are also reported to indicate the rule 
coverage of a rule set. Here phrasal rule refers 
to the rule whose source-side and the yield of its 
target-side contains only one phrase each, with 
optional surrounding variables. According to 
(DeNeefe et al, 2007), the number of phrasal 
rules is a good indicator of the coverage of a rule 
set. useful rule refers to the rule that can be ap-
plied when decoding the test sentences 2 . As 
shown in Table 1, the two resulting rule sets on-
ly have about 70% overlaps (Column 4), and the 
rule coverage increases by about 20% when we 
combine them together (Column 5). This finding 
confirms that the rule coverage can be improved 
by using multiple different parsers in rule extrac-
tion. 
 # of rules # of phrasal 
rules 
# of  
useful rules
Berkeley 3,538,332 2,515,243 549,783 
Collins 3,526,166 2,481,195 553,893 
Overlap 2,542,380 1,907,521 386,983 
Union 4,522,118 3,088,920 716,693 
Table 1: Comparison of rule coverage between 
different rule sets. 
5 Translation Rule Extraction with 
Multiple Parsers 
5.1 Rule Extraction Algorithm 
Motivated by the above observations, we pro-
pose a rule extraction method to improve the 
rule coverage by using multiple parsers.  
Let <f, e, a> be a tuple of <source sentence, 
target sentence, bi-directional word alignments>, 
                                                 
1 LDC2005T10, LDC2003E07, LDC2003E14 and 
LDC2005T06 
2 In this experiment, the test sentences come from 
NIST 2004 and 2005 MT evaluation sets. It should be 
noted that due to the pruning in decoding we cannot 
count the exact number of rules that can be used dur-
ing decoding. In this work, we use an alternative ? 
the number of rules matched with test sentences ? to 
estimate an upper-bound approximately. 
and {P1, ..., PN} be N syntactic parsers in target-
language. The following pseudocode formulizes 
the algorithm for extracting translation rules 
from <f, e, a> using parsers {P1, ..., PN}, where 
Pi(e) returns the parse tree generated by the i-th 
parser Pi. Function GENERATERULES() com-
putes the set of rules for <f, ti, a> by using vari-
ous rule extraction methods, such as  the method 
described in Section 3. 
Multi-Parser based Rule Extraction  
Input: <f, e, a> and P = {P1, ..., PN} 
Output: rule set R 
1 Function MULTIPAREREXTRACTOIN(<f, e, a>, P )
2     for i = 1 to N do                           <  for each parser
3        ti = Pi(e)                                      <  target-tree 
4       Ri = GENERATERULES (f, ti, a) <  rule extraction 
5       R.append(Ri) 
6     return R 
7 Function GENERATERULES ( f, ti, a ) 
8     return rules extracted from <f, ti, a> 
5.2 Learning Rule Probabilities 
In multi-parser based rule extraction, more than 
one parse trees are used, and each of them is as-
sociated with a parsing confidence (e.g. genera-
tive probability of the tree). Ideally, if the parse 
trees used in rule extraction can be accurately 
weighted, the rule probabilities will be better 
estimated according to the parse weights, for 
example, the rules extracted from a parse tree 
having a low weight should be penalized accord-
ingly in the estimation of rule probabilities. Un-
fortunately, the tree probabilities are generally 
incomparable between different parsers due to 
the different parsing models used and ways of 
implementation. Thus we cannot use the poste-
rior probability of a rule?s target-side to estimate 
the fractional count (Mi and Huang, 2008; Liu et 
al., 2009), which is used in maximum-likelihood 
estimation of rule probabilities. In this work, to 
simplify the problem, we assume that all the 
parsers have the same and maximum degrees of 
confidence on their outputs. For a rule r ex-
tracted from a string-tree pair, the count of r is 
defined to be: 
1
( , )
( )
N
i
r i
c r
N
?== ?                     (1) 
where ( , )r i? is 1 if r is extracted by using the i-
th parser, otherwise 0.  
1348
Following Mi and Huang (2008)?s work, three 
conditional rule probabilities are employed for 
experimenting with our method. 
': ( ') ( )
( )Pr( | ( ))
( )
r root r root r
c rr root r
c r=
= ?        (2) 
': ( ') ( )
( )Pr( | ( ))
( )
r lhs r lhs r
c rr lhs r
c r=
= ?             (3) 
': ( ') ( )
( )Pr( | ( ))
( )
r rhs r rhs r
c rr rhs r
c r=
= ?            (4) 
where lhs(r) and rhs(r) are the source-hand and 
target-hand sides of r respectively, and root(r) is 
the root of r?s target-tree. 
5.3 Parser Indicator Features 
For each rule, we define N indicator features (i.e. 
( , )r i? ) to indicate a rule is extracted by using 
which parsers, and add them into the translation 
model. By training the feature weights with Min-
imum Error Rate Training (MERT), the system 
can learn preferences for different parsers auto-
matically. 
6 Experiments 
The experiments are conducted on Chinese-
English translation in a state-of-the-art string-to-
tree SMT system.  
6.1 Experimental Setup 
Our bilingual data consists of 370K sentence 
pairs (9M Chinese words + 10M English words) 
which have been used in the experiment in Sec-
tion 4. GIZA++ is employed to perform the bidi-
rectional word alignment between the source and 
target sentences, and the final word alignment is 
generated using the inter-sect-diag-grow method. 
A 5-gram language model is trained on the tar-
get-side of the bilingual data and the Xinhua 
portion of English Gigaword corpus. The devel-
opment data set comes from NIST MT 2003 
evaluation set. To speed up MERT, sentences 
with more than 20 Chinese words are removed. 
The test sets are the NIST MT evaluation sets of 
2004 and 2005.  
Our baseline MT system is built based on the 
string-to-tree model proposed in (Galley et al, 
2006). In this system, both of minimal GHKM 
(Galley et al, 2004) and SPMT rules (Marcu et 
al., 2006) are extracted from the bilingual corpus, 
and the composed rules are generated by com-
posing two or three minimal GHKM and SPMT 
rules3. We use a CKY-style decoder with cube 
pruning (Huang and Chiang, 2007) and beam 
search to decode new Chinese sentences. By de-
fault, the beam size is set to 30. For integrating 
n-gram language model into decoding efficiently, 
rules containing more than two variables or 
source word sequences are binarized using the 
synchronous binarization method (Zhang et al, 
2006; Xiao et al, 2009).  
The system is evaluated in terms of the case-
insensitive NIST version BLEU (using the 
shortest reference length), and statistical signifi-
cant test is conducted using the re-sampling me-
thod proposed by Koehn (2004). 
6.2 The Parsers 
Four syntactic parsers are chosen for the ex-
periments. They are Stanford Parser4, Berkeley 
Parser 5 , Collins Parser (Dan Bikel?s reimple-
mentation of Collins Model 2) 6  and Charniak 
Parser7. The former two are state-of-the-art non-
lexicalized parsers, while the latter two are state-
of-the-art lexicalized parsers. All the parsers are 
trained on sections 02-21 of the Wall Street 
Journal (WSJ) Treebank, and tuned on section 
22. Table 2 summarizes the performance of the 
parsers. 
Parser Recall Precision F1 
Stanford 86.29% 87.21% 86.75% 
Berkeley 90.18% 90.45% 90.32% 
Collins 89.14% 88.85% 88.99% 
Charniak 89.99% 90.28% 90.13% 
Table 2: Performance of the four parsers on sec-
tion 23 of the WSJ Treebank. 
We parse the target-side of the bilingual data 
using the four parsers individually. From the 1-
best parses generated by these parsers, we obtain 
four baseline rule sets using the method de-
scribed in Section 3, as well as the rule sets usi- 
                                                 
3 Generally a higher baseline can be obtained by 
combining more (unit) rules. However, we find that 
using more composed rules does not affect the impact 
of using multiple parsers. Thus, we choose this set-
ting in order to finish all experiments in time. 
4 http://nlp.stanford.edu/software/lex-parser.shtml 
5 http://code.google.com/p/berkeleyparser/ 
6 http://www.cis.upenn.edu/~dbikel/download.html 
7 http://www.cs.brown.edu/people/ec/#software 
1349
Rule Coverage BLEU4 (%)  Rule set 
# of rules # of  
phrasal rules
# of  
useful rules 
Dev. MT04 MT05 
Stanford (S) 3,679 K 2,581 K 573 K 39.36 36.02 36.98 
Berkeley (B) 3,538 K 2,515 K 549 K 39.32 36.05 36.98 
Collins (Co) 3,526 K 2,481 K 553 K 39.16 36.07 36.91 
B
as
el
in
e 
Charniak (Ch) 3,450 K 2,435 K 540 K 39.24 35.90 36.89 
S + B 4,567 K 3,105 K 726 K 39.87+ 36.57+ 37.47+ 
S + Co 4,734 K 3,202 K 752 K 39.94+ 36.57+ 37.52+ 
S + Ch 4,764 K 3,258 K 751 K 40.01+ 36.51 37.59+ 
B + Co 4,522 K 3,088 K 716 K 39.84+ 36.60+ 37.46+ 
B +  Ch 4,562 K 3,129 K 717 K 39.81+ 36.49 37.41 
2 
pa
rs
er
s 
Co + Ch 4,592 K 3,125 K 727 K 39.75 36.55+ 37.43+ 
S + B + Co 5,331 K 3,543 K 852 K 40.14++ 36.83++ 37.78++ 
S + B + Ch 5,380 K 3,590 K 854 K 40.05+ 36.82++ 37.70+ 
S + Co + Ch 5,551 K 3,663 K 877 K 40.35++ 36.70+ 37.70+ 3 p
ar
se
rs
 
B + Co + Ch 5,294 K 3,544 K 840 K 40.04+ 36.76+ 37.65+ 
4 S + B + Co + Ch 6,005 K 3,940 K 958 K 40.28++ 36.99++ 37.89++ 
Table 5: Evaluation results. + or ++ = significantly better than all the baseline systems (using single 
parser) at the 95% or 99% confidence level. 
 
 Stanford Berkeley Collins Charniak
Stanford 100% 76.72% 73.32% 74.89% 
Berkeley 76.72% 100% 75.69% 76.76% 
Collins 73.32% 75.69% 100% 74.84% 
Charniak 74.89% 76.76% 74.84% 100% 
Table 3: Agreement between different parsers. 
 
ng the multi-parser based rule extraction method.  
Before conducting primary experiments, we first 
investigate the differences between the 1-best 
outputs of the parsers. Table 3 shows the agree-
ment between each pair of parsers. Here the de-
gree of agreement shown in each cell is com-
puted by using one parser?s output as a good 
standard to evaluate the other parser?s output in 
terms of F1 score, and a higher agreement score 
(i.e. F1 score) means that the 1-best outputs of 
the two parsers are more similar to each other. 
We see that the agreement scores between dif-
ferent parsers are always below 80%. This result 
reflects a large diversity in parse trees generated 
by different parsers, and thus confirms our ob-
servations in Section 4. 
We also examine the ?complementarity? be-
tween the baseline rule sets generated by using 
different parsers individually. Table 4 shows the 
results, where the degree of ?complementarity? 
between two rule sets is defined as the percent-
age of the rules in one rule set that are not cov-
ered by the other rule set. It can be regarded as a 
measure of the disagreement between two rule 
sets, and a higher number indicates large ?com-
plementarity?. For example, in Row 2, Column 3 
(Table 4), ?25.09%? means that 25.09% rules in 
the first rule set (using Stanford Parser) are not 
covered by the second rule set (using Berkeley 
Parser). Table 4 shows that there is always a dis-
agreement of over 25% between different rule 
sets. These results indicate that using different 
parsers can lead to a relatively large ?comple-
mentarity? between the rule sets.  
 Stanford Berkeley Collins Charniak
Stanford 0% 25.09% 29.91% 31.43% 
Berkeley 27.98% 0% 27.90% 29.68% 
Collins 32.84% 28.15% 0% 30.89% 
Charniak 35.70% 31.43% 32.37% 0% 
Table 4: Disagreement between the rule sets ob-
tained using different parsers individually. 
6.3 Evaluation of Translations 
We then study the impact of multi-parser based 
rule extraction on translation accuracy.  Table 5 
shows the BLEU scores as well as the rule cov-
erage for various rule extraction methods. We 
see, first of all, that the rule coverage is im-
proved significantly by multi-parser based rule 
extraction. Compared to the baseline method (i.e. 
single-parser based rule extraction), the multi-
parser based rule extraction achieves over 20% 
coverage improvements when only two parsers 
are used, even yields gains of over 50 percentage 
1350
points when all the four parsers are used together. 
Also, BLEU score is improved by multi-parser 
based rule extraction. When two parsers are em-
ployed in rule extraction, there is generally a 
gain of over 0.4 BLEU points on both MT04 and 
MT05 test sets. Further improvements are 
achieved when more parsers are involved. On 
both test sets, using three parsers in rule extrac-
tion generally yields a +0.7 BLEU improvement, 
and using all the parsers together yields a +0.9 
BLEU improvement which is the biggest im-
provement achieved in this set of experiment. 
All these results show that multi-parser based 
rule extraction is an effective way to improve the 
rule coverage as well as the BLEU score of the 
syntax-based MT system. 
An interesting finding is that there seems no 
significant differences in BLEU scores between 
the baseline systems (using single parsers), 
though the parsing performance of the corre-
sponding parsers is very different from each 
other. For example, the MT performance corre-
sponding to Berkeley Parser is very similar to 
that corresponding to Stanford Parser despite a 
4-point difference in F1 score between the two 
parsers. Another example is that Charniak parser 
performs slightly worse than the other three on 
MT task, though it achieves the 2nd best parsing 
performance in all the parsers. This interesting 
finding shows that the performance of syntax-
based MT systems is not very sensitive to the 
parsing performance of the parsers used in rule 
extraction. 
6.4 Preferences for Parsers 
We also investigate the preferences for different 
parsers in our system. Table 6 shows the weights 
of the parser indicator features learned by 
MERT, as well as the number of edges gener-
ated by applying the rules corresponding to dif-
ferent parsers during decoding. Both of the met-
rics are used to evaluate the contributions of the 
parsers to MT decoding. We see that though 
Stanford Parser and Berkeley Parser are shown 
to be relatively more preferred by the decoder, 
there are actually no significant differences in 
the degrees of the contributions of different 
parsers. This result also confirms the fact ob-
served in Table 5 that the MT system does not 
have special preferences for different parsers. 
 
Indicator Weight # of edges 
(Dev.) 
# of edges 
 (MT04) 
# of edges 
(MT05) 
Stanford 0.1990 7.7 M 169.2 M 101.7 M
Berkeley 0.1982 7.7 M 166.3 M 100.2 M
Collins 0.1690 6.9 M 149.9 M   93.1 M
Charniak 0.1729 7.1 M 156.5 M   97.2 M
Table 6: Preferences for different parsers. 
Though Table 6 provides some information 
about the contributions of different parsers, it 
still does not answer how often these rules are 
really used to generate final (1-best) translation. 
Table 7 gives an answer to this question. We see 
that, following the similar trend in Table 5, dif-
ferent parsers have nearly equal contributions in 
generating final translation. 
Indicator # of rules 
used in 1-best  
(Dev.) 
# of rules 
used in 1-best  
(MT04) 
# of rules 
used in 1-best  
(MT05) 
Stanford     2,410 23,513 14,357 
Berkeley     2,455 23,878 14,670 
Collins     2,309 22,654 13,815 
Charniak     2,269 22,406 13,731 
Table 7: Numbers of rules used in generating 
final (1-best) translation. 
6.5 Rule Extraction with k-best Parses 
We also conduct experiments to compare the 
effectiveness of multi-parser based rule extrac-
tion and rule extraction with k-best parses gener-
ated by a single parser. As Berkeley parser is 
one of the best-performing parsers in previous 
experiments, we employ it to generate k-best 
parses in this set of experiment. As shown in 
Figure 3, both of the methods improve the 
BLEU scores by enlarging the set of parse trees 
used in rule extraction. Compared to k-best ex-
traction, multi-parser extraction shows consiste- 
 36.8
 37
 37.2
 37.4
 37.6
 37.8
 38
 38.2
3.5 5.0 6.5
B
LE
U
4(
%
)
# of rules (million)
1-best
4-best
10-best
20-best
30-best 50-best
2 parsers
3 parsers
4 parsers
multi-parser extraction
k-best extraction
 
Figure 3: Multi-parser based rule extraction vs. 
rule extraction with k-best parses (MT05). 
1351
ntly better BLEU scores. Using 4 different pars-
ers, it achieves an improvement of 0.6 BLEU 
points over k-best extraction where even 50-best 
parses are used. 
Finally, we extend multi-parser based rule ex-
traction to extracting rules from the k-best parses 
generated by multiple parsers. Figure 4 shows 
the results on ?S + B + Co + Ch? system. We see 
that multi-parser based rule extraction can bene-
fit from k-best parses, and yields a modest (+0.2 
BLEU points) improvement when extracting 
from 10-best parses. However, since k-best ex-
traction generally results in much slower extrac-
tion speed, it might not be a good choice to use 
k-best parses to improve our method in practice. 
 37.7
 37.8
 37.9
 38
 38.1
 38.2
 38.3
 38.4
 38.5
5.5 6.5 7.5 8.5
B
LE
U
4(
%
)
# of rules (million)
1-best
2-best
5-best
10-best
multi-parser + k-best extraction
 
Figure 4: Multi-parser based rule extraction & 
rule extraction with k-best parses (MT05). 
7 Discussion and Future Work 
In this work, all the parsers are trained using the 
same treebank. To obtain diversified parse trees 
for multi-parser based rule extraction, an alterna-
tive way is to learn parsers on treebanks anno-
tated by different organizations (e.g. Penn Tree-
bank and ICE-GB corpus). Since different tree-
banks can provide us with more diversity in 
parsing, we believe that our system can benefit a 
lot from the parsers that are learned on multiple 
different treebanks individually. But here is a 
problem that due to the different annotation 
standards used, there is generally an incompati-
bility between treebanks annotated by different 
organizations. It will result in that we cannot 
straightforwardly mix the resulting rule sets (or 
heterogeneous grammars for short) for probabil-
ity estimation as well as the use for decoding. To 
solve this problem, a simple solution might be 
that we transform the incompatible rules into a 
unified form. Alternatively, we can use hetero-
geneous decoding (or parsing) techniques (Zhu 
et al, 2010) to make use of heterogeneous 
grammars in the stage of decoding. Both topics 
are very interesting and worth studying in our 
future work.  
Besides k-best extraction, our method can also 
be applied to other rule extraction schemes, such 
as forest-based rule extraction. As (Mi and 
Huang, 2008) has shown that forest-based ex-
traction is more effective than k-best extraction 
in improving translation accuracy, it is expected 
to achieve further improvements by using multi-
parser based rule extraction and forest-based rule 
extraction together. 
8 Conclusions  
In this paper, we present a simple and effective 
method to improve rule coverage by using mul-
tiple parsers in translation rule extraction. Ex-
perimental results show that  
z Using multiple parsers in rule extraction 
achieves large improvements of rule cover-
age over the baseline method where only a 
single parser is used, as well as a +0.9 
BLEU improvement on both NIST 2004 
and 2005 test corpora. 
z The MT system can be further improved by 
using multiple parsers and k-best parses to-
gether. However, with the consideration of 
extraction speed, it might not be a good 
choice to use k-best parses to improve mul-
ti-parser based rule extraction in practice. 
z The MT performance is not influenced by 
the parsing performance of the parsers used 
in rule extraction very much. Actually, the 
MT system does not show different prefer-
ences for different parsers. 
Acknowledgements 
This work was supported in part by the National 
Science Foundation of China (60873091) and 
the Fundamental Research Funds for the Central 
Universities (N090604008). The authors would 
like to thank the anonymous reviewers and Ton-
gran Liu for their pertinent comments for im-
proving the early version of this paper, and Ru-
shan Chen for building parts of the baseline sys-
tem. 
1352
References 
Brooke Cowan, Ivona Ku?erov? and Michael Collins. 
2006. A discriminative model for tree-to-tree 
translation. In Proc. of EMNLP 2006, pages 232-
241. 
Steve DeNeefe, Kevin Knight, Wei Wang and Daniel 
Marcu. 2007. What Can Syntax-based MT Learn 
from Phrase-based MT? In Proc. of EMNLP 2007, 
pages 755-763. 
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency 
insertion grammars. In Proc. of ACL 2005, Ann 
Arbor, Michigan, pages 541-548. 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for machine translation. In Proc. of ACL 
2003, pages 205-208. 
Michel Galley, Mark Hopkins, Kevin Knight and 
Daniel Marcu. 2004. What's in a translation rule? 
In Proc. of HLT-NAACL 2004, Boston, USA, 
pages 273-280. 
Michel Galley, Jonathan Graehl, Kevin Knight, Da-
niel Marcu, Steve DeNeefe, Wei Wang and Igna-
cio Thayer. 2006. Scalable inferences and training 
of context-rich syntax translation models. In Proc. 
of COLING/ACL 2006, Sydney, Australia, pages 
961-968. 
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language 
models. In Proc. of ACL 2007, Prague, Czech Re-
public, pages 144-151. 
Liang Huang, Kevin Knight and Aravind Joshi. 2006. 
Statistical syntax-directed translation with ex-
tended domain of locality. In Proc. of AMTA 2006, 
pages 66-73. 
Philipp Koehn. 2004. Statistical Significance Tests 
for Machine Translation Evaluation. In Proc. of 
EMNLP 2004, Barcelona, Spain, pages 388-395. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine 
translation. In Proc. of COLING/ACL 2006, Syd-
ney, Australia, pages 609-616. 
Yang Liu, Yajuan L? and Qun Liu. 2009. Improving 
Tree-to-Tree Translation with Packed Forest. In 
Proc. of ACL 2009, pages 558-566. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi and 
Kevin Knight. 2006. SPMT: Statistical machine 
translation with syntactified target language phras-
es. In Proc. of EMNLP 2006, Sydney, Australia, 
pages 44-52. 
Haitao Mi and Liang Huang. 2008. Forest-based 
Translation Rule Extraction. In Proc. of EMNLP 
2008, pages 206-214. 
Chris Quirk, Arul Menezes and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proc. of ACL 2005, pages 
271-279. 
Libin Shen, Jinxi Xu and Ralph Weischedel. 2008. A 
new string-to-dependency machine translation al-
gorithm with a target dependency language model. 
In Proc. of ACL/HLT 2008, pages 577-585. 
Ashish Venugopal, Andreas Zollmann, Noah A. 
Smith and Stephan Vogel. 2008. Wider Pipelines: 
K-best Alignments and Parses in MT Training. In 
Proc. of AMTA 2008, pages 192-201. 
Wei Wang, Kevin Knight and Daniel Marcu. 2007. 
Binarizing Syntax Trees to Improve Syntax-Based 
Machine Translation Accuracy. In Proc. of 
EMNLP-CoNLL 2007, Prague, Czech Republic, 
pages 746-754. 
Tong Xiao, Mu Li, Dongdong Zhang, Jingbo Zhu and 
Ming Zhou. 2009. Better Synchronous Binariza-
tion for Machine Translation. In Proc. of EMNLP 
2009, Singapore, pages 362-370. 
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical machine translation model. In 
Proc. of ACL 2001, pages 132-139. 
Hao Zhang, Liang Huang, Daniel Gildea and Kevin 
Knight. 2006. Synchronous Binarization for Ma-
chine Translation. In Proc. of HLT-NAACL 2006, 
New York, USA, pages 256- 263. 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, 
Chew Lim Tan and Sheng Li. 2008. A Tree Se-
quence Alignment-based Tree-to-Tree Translation 
Model. In Proc. of ACL/HLT 2008, pages 559-567. 
Muhua Zhu, Jingbo Zhu and Tong Xiao. 2010. Het-
erogeneous Parsing via Collaborative Decoding. In 
Proc. of COLING 2010. 
1353
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 320?331, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Generalized Higher-Order Dependency Parsing with Cube Pruning
Hao Zhang Ryan McDonald
Google, Inc.
{haozhang,ryanmcd}@google.com
Abstract
State-of-the-art graph-based parsers use fea-
tures over higher-order dependencies that rely
on decoding algorithms that are slow and
difficult to generalize. On the other hand,
transition-based dependency parsers can eas-
ily utilize such features without increasing the
linear complexity of the shift-reduce system
beyond a constant. In this paper, we attempt to
address this imbalance for graph-based pars-
ing by generalizing the Eisner (1996) algo-
rithm to handle arbitrary features over higher-
order dependencies. The generalization is at
the cost of asymptotic efficiency. To account
for this, cube pruning for decoding is utilized
(Chiang, 2007). For the first time, label tuple
and structural features such as valencies can
be scored efficiently with third-order features
in a graph-based parser. Our parser achieves
the state-of-art unlabeled accuracy of 93.06%
and labeled accuracy of 91.86% on the stan-
dard test set for English, at a faster speed than
a reimplementation of the third-order model of
Koo et al2010).
1 Introduction
The trade-off between rich features and exact de-
coding in dependency parsing has been well docu-
mented (McDonald and Nivre, 2007; Nivre and Mc-
Donald, 2008). Graph-based parsers typically trade-
off rich feature scope for exact (or near exact) de-
coding, whereas transition-based parsers make the
opposite trade-off. Recent research on both parsing
paradigms has attempted to address this.
In the transition-based parsing literature, the fo-
cus has been on increasing the search space of the
system at decoding time, as expanding the feature
scope is often trivial and in most cases only leads to
a constant-time increase in parser complexity. The
most common approach is to use beam search (Duan
et al2007; Johansson and Nugues, 2007; Titov and
Henderson, 2007; Zhang and Clark, 2008; Zhang
and Nivre, 2011), but more principled dynamic pro-
gramming solutions have been proposed (Huang and
Sagae, 2010). In all cases inference remains approx-
imate, though a larger search space is explored.
In the graph-based parsing literature, the main
thrust of research has been on extending the Eisner
chart-parsing algorithm (Eisner, 1996) to incorpo-
rate higher-order features (McDonald and Pereira,
2006; Carreras, 2007; Koo and Collins, 2010). A
similar line of research investigated the use of inte-
ger linear programming (ILP) formulations of pars-
ing (Riedel and Clarke, 2006; Martins et al2009;
Martins et al2010). Both solutions allow for exact
inference with higher-order features, but typically at
a high cost in terms of efficiency. Furthermore, spe-
cialized algorithms are required that deeply exploit
the structural properties of the given model. Upgrad-
ing a parser to score new types of higher-order de-
pendencies thus requires significant changes to the
underlying decoding algorithm. This is in stark con-
trast to transition-based systems, which simply re-
quire the definition of new feature extractors.
In this paper, we abandon exact search in graph-
based parsing in favor of freedom in feature scope.
We propose a parsing algorithm that keeps the back-
bone Eisner chart-parsing algorithm for first-order
parsing unchanged. Incorporating higher-order fea-
tures only involves changing the scoring function of
320
potential parses in each chart cell by expanding the
signature of each chart item to include all the non-
local context required to compute features. The core
chart-parsing algorithm remains the same regardless
of which features are incorporated. To control com-
plexity we use cube pruning (Chiang, 2007) with the
beam size k in each cell. Furthermore, dynamic pro-
gramming in the style of Huang and Sagae (2010)
can be done by merging k-best items that are equiv-
alent in scoring. Thus, our method is an applica-
tion of integrated decoding with a language model
in MT (Chiang, 2007) to dependency parsing, which
has previously been applied to constituent parsing
(Huang, 2008). However, unlike Huang, we only
have one decoding pass and a single trained model,
while Huang?s constituent parser maintains a sep-
arate generative base model from a following dis-
criminative re-ranking model. We draw connections
to related work in Section 6.
Our chart-based approximate search algorithm al-
lows for features on dependencies of an arbitrary or-
der ? as well as over non-local structural proper-
ties of the parse trees ? to be scored at will. In
this paper, we use first to third-order features of
greater varieties than Koo and Collins (2010). Ad-
ditionally, we look at higher-order dependency arc-
label features, which is novel to graph-based pars-
ing, though commonly exploited in transition-based
parsing (Zhang and Nivre, 2011). This is because
adding label tuple features would introduce a large
constant factor of O(|L|3), where |L| is the size of
the label set L, into the complexity for exact third-
order parsing. In our formulation, only the top-
ranked labelled arcs would survive in each cell. As
a result, label features can be scored without combi-
natorial explosion. In addition, we explore the use
of valency features counting how many modifiers a
word can have on its left and right side. In the past,
only re-rankers on k-best lists of parses produced by
a simpler model use such features due to the diffi-
culty of incorporating them into search (Hall, 2007).
The final parser with all these features is both ac-
curate and fast. In standard experiments for English,
the unlabeled attachment score (UAS) is 93.06%,
and the labeled attachment score (LAS) is 91.86%.
The UAS score is state-of-art. The speed of our
parser is 220 tokens per second, which is over 4
times faster than an exact third-order parser that at-
Figure 1: Example Sentence.
tains UAS of 92.81% and comparable to the state-of-
the-art transition-based system of Zhang and Nivre
(2011) that employs beam search.
2 Graph-based Dependency Parsing
Dependency parsers produce directed relationships
between head words and their syntactic modifiers.
Each word modifies exactly one head, but can have
any number of modifiers itself. The root of a sen-
tence is a designated special symbol which all words
in the sentence directly or indirectly modify. Thus,
the dependency graph for a sentence is constrained
to be a directed tree. The directed syntactic rela-
tionships, aka dependency arcs or dependencies for
short, can often be labeled to indicate their syntactic
role. Figure 1 gives an example dependency tree.
For a sentence x = x1 . . . xn, dependency pars-
ing is the search for the set of head-modifier depen-
dency arcs y? such that y? = argmaxy?Y(x) f(x, y),
where f is a scoring function. As mentioned before,
y? must represent a directed tree. |Y(x)| is then the
set of valid dependency trees for x and grows ex-
ponentially with respect to its length |x|. We fur-
ther define L as the set of possible arc labels and use
the notation (i l?? j) ? y to indicate that there is a
dependency from head word xi to modifier xj with
label l in dependency tree y.
In practice, f(x, y) is factorized into scoring func-
tions on parts of (x, y). For example, in first-
order dependency parsing (McDonald et al2005),
f(x, y) is factored by the individual arcs:
y? = argmax
y?Y(x)
f(x, y) = argmax
y?Y(x)
?
(i l??j)?y
f(i l?? j)
The factorization of dependency structures into arcs
enables an efficient dynamic programming algo-
rithm with running time O(|x|3) (Eisner, 1996), for
the large family of projective dependency structures.
Figure 2 shows the parsing logic for the Eisner
algorithm. It has two types of dynamic program-
ming states: complete items and incomplete items.
321
Complete items correspond to half-constituents, and
are represented as triangles graphically. Incomplete
items correspond to dependency arcs, and are repre-
sented as trapezoids. The Eisner algorithm is the ba-
sis for the more specialized variants of higher-order
projective dependency parsing.
Second-order sibling models (McDonald and
Pereira, 2006) score adjacent arcs with a common
head. In order to score them efficiently, a new state
corresponding to modifier pairs was introduced to
the chart-parsing algorithm. Due to the careful fac-
torization, the asymptotic complexity of the revised
algorithm remains O(|x|3). The resulting scoring
function is:
y? = argmax
y?Y(x)
?
(i l??j,i l???k)?y
f(i l?? j, i l
?
?? k)
where (i l?? j, i l
?
?? k) ? y indicates two adja-
cent head-modifier relationships in dependency tree
y, one from xi to xj with label l and another from
xi to xk with label l?. Words xj and xk are com-
monly referred to as siblings. In order to maintain
cubic parsing complexity, adjacent dependencies are
scored only if the modifiers occur on the same side
in the sentence relative to the head.
Second-order grandchild models (Carreras, 2007)
score adjacent arcs in length-two head-modifier
chains. For example, if word xi modifies word xj
with label l, but itself has a dependency to modi-
fier xk with label l?, then we would add a scoring
function f(j l?? i l
?
?? k). These are called grand-
child models as they can score dependencies be-
tween a word and its modifier?s modifiers, i.e., xk
is the grandchild of xj in the above example. The
states in the Eisner algorithm need to be augmented
with the indices to the outermost modifiers in order
to score the outermost grandchildren. The resulting
algorithm becomes O(|x|4).
Finally, third-order models (Koo and Collins,
2010) score arc triples such as three adjacent sib-
ling modifiers, called tri-siblings, or structures look-
ing at both horizontal contexts and vertical contexts,
e.g., grand-siblings that score a word, its modifier
and its adjacent grandchildren. To accommodate
the scorers for these sub-graphs, even more special-
ized dynamic programming states were introduced.
The Koo and Collins (2010) factorization enables
(a) = +
(b) = +
Figure 2: Structures and rules for parsing first-order mod-
els with the (Eisner, 1996) algorithm. This shows only
the construction of right-pointing dependencies and not
the symmetric case of left-pointing dependencies.
the scoring of certain types of third-order dependen-
cies with O(|x|4) decoder run-time complexity.
Each of these higher-order parsing algorithms
makes a clever factorization for the specific model
in consideration to keep complexity as low as possi-
ble. However, this results in a loss of generality.
3 Generalizing Eisner?s Algorithm
In this section, we generalize the Eisner algorithm
without introducing new parsing rules. The general-
ization is straight-forward: expand the dynamic pro-
gramming state to incorporate feature histories. This
is done on top of the two distinct chart items in the
O(|x|3) Eisner chart-parsing algorithm (Figure 2).
The advantage of this approach is that it maintains
the simplicity of the original Eisner algorithm. Un-
fortunately, it can increase the run-time complex-
ity of the algorithm substantially, but we will em-
ploy cube pruning to regain tractability. Because our
higher-order dependency parsing algorithm is based
the Eisner algorithm, it is currently limited to pro-
duce projective trees only.
3.1 Arbitrary n-th-order dependency parsing
We start with the simplest case of sibling models. If
we want to score sibling arcs, at rule (b) in Figure 2,
we can see that the complete item lying between
the head and the modifier (the middle of the three
items) does not contain information about the out-
ermost modifier of the head, which is the previous
dependency constructed and the sibling to the mod-
ifier of the dependency currently being constructed.
This fact suggests that, in order to score modifier
bigrams, the complete item states should be aug-
mented by the outermost modifier. We can aug-
ment the chart items with such information, which
322
(a) = +
(b) = +
Figure 3: Structures and rules for parsing models based
on modifier bigrams, with a generalized (Eisner, 1996)
algorithm. Here the dashed arrows indicate additional in-
formation stored in each chart-cell. Specifically the pre-
vious modifier in complete chart items.
is shown in Figure 3. It refines the complete items
by storing the previously constructed dependency to
the outermost modifiers. Note that now the signature
of the complete items is not simply the end-point in-
dexes, but contains the index of the outer modifier.
Using this chart item augmentation it is now pos-
sible to score both first-order arcs as well as second-
order sibling arcs. In fact, by symmetry, the new
dynamic program can also score the leftmost and
rightmost grandchildren of a head-modifier pair, in
rule (a) and rule (b) respectively. By counting the
number of free variables in each parsing rule, we
see that the parsing complexity is O(|x|5), which is
higher than both McDonald and Pereira (2006) and
Carreras (2007). The added complexity comes from
the fact that it is now possible to score a third-order
dependency consisting of the head, the modifier, the
sibling, and the outermost grandchild jointly.
We can go further to augment the complete and
incomplete states with more parsing history. Fig-
ure 4 shows one possible next step of generaliza-
tion. We generalize the states to keep track of the
latest two modifiers of the head. As a result, it be-
comes possible to score tri-siblings involving three
adjacent modifiers and grand-siblings involving two
outermost grandchildren ? both of which comprise
the third-order Model 2 of Koo and Collins (2010) ?
plus potentially any additional interactions of these
roles. Figure 5 shows another possible generaliza-
tion. We keep modifier chains up to length two in
the complete states. The added history enables the
computation of features for great-grandchildren re-
lationships: (h l?? m l
?
?? gc l
??
?? ggc).
In general, we can augment the complete and in-
complete states with n variables representing the
(a) = +
(b) = +
Figure 4: Structures and rules for parsing models based
on modifier trigrams in horizontal contexts, with a gener-
alized (Eisner, 1996) algorithm. Here the dashed arrows
indicate the previous two modifiers to the head in each
chart item.
(a) = +
(b) = +
Figure 5: Structures and rules for parsing models based
on modifier trigrams in vertical contexts, with a gener-
alized (Eisner, 1996) algorithm. Here the dashed arrows
indicate the modifier to the head and the modifier?s mod-
ifier, forming a modifier chain of length two.
possible parsing histories and loop over the cross
product of the histories in the innermost loop of Eis-
ner algorithm. The cardinality of the cross product
is |x|n ? |x|n. Thus, the complexity of the algo-
rithm augmented by n variables is O(|x|3 ? |x|2n) =
O(|x|3+2n), where n ? 0. Note that this complexity
is for unlabeled parsing. A factor of |L| for all or
a subset of the encoded arcs must be multiplied in
when predicting labeled parse structures.
3.2 History-based dependency parsing
The previous n modifiers, either horizontal or ver-
tical, is a potential signature of parsing history. We
can put arbitrary signatures of parsing history into
the chart items so that when we score a new item,
we can draw the distinguishing power of features
based on an arbitrarily deep history. For example,
consider the position of a modifier, which is the po-
sition in which it occurs amongst its siblings relative
to the location of the head. We can store the position
of the last modifier into both chart states. In com-
plete states, this signature tells us the position of the
outermost modifier, which is the valency of the head
in the left or right half-constituent.
323
In the extreme case, we can use full subtrees as
histories, although the cardinality of the set of his-
tories would quickly become exponential, especially
when one considers label ambiguity. Regardless, the
high complexity associated with this generalization,
even for second or third-order models, requires us to
appeal to approximate search algorithms.
3.3 Advantage of the generalization
The complexity analysis earlier in this section re-
veals the advantage of such a generalization scheme.
It factorizes a dynamic programming state for de-
pendency parsing into two parts: 1) the structural
state, which consists of the boundaries of incom-
plete and complete chart items, and accounts for the
O(|x|3) term in the analysis, and 2) the feature his-
tory, which is a signature of the internal content of a
sub-parse and accounts for the O(|x|2n) term. The
rules of the deductive parsing system ? the Eisner al-
gorithm ? stay the same as long as the structural rep-
resentation is unchanged. To generalize the parser
to handle richer features, one can simply enrich the
feature signature and the scoring function without
changing the structural state. A natural grouping of
states follows where all sub-parses sharing the same
chart boundaries are grouped together. This group-
ing will enable the cube pruning in Section 4 for ap-
proximate search.
There is another advantage of keeping the Eis-
ner parsing logic unchanged: derivations one-to-one
correspond to dependency parse trees. Augmenting
the complete and incomplete states does not intro-
duce spurious ambiguity. This grouping view is use-
ful for proving this point. Introducing higher order
features in each chart item will cause sub-derivations
to be re-ranked only. As a result, the final Viterbi
parse can differ from the one from the standard Eis-
ners algorithm. But the one-to-one correspondence
still holds.
4 Approximate Search with Cube Pruning
In machine translation decoding, an n-gram lan-
guage model can be incorporated into a translation
model by augmenting the dynamic programming
states for the translation model with the boundary
n ? 1 words on the target side. The complexity
for exact search involves a factor of |x|4n?4 in the
hierarchical phrase-based model of Chiang (2007),
where |x| is the input sentence length. The standard
technique is to force a beam size k on each transla-
tion state so that the possible combinations of lan-
guage model histories is bounded by k2. Further-
more, if the list of k language model states are sorted
from the lowest cost to the highest cost, we can as-
sume the best combinations will still be among the
combinations of the top items from each list, al-
though the incorporation of n-gram features breaks
the monotonic property of the underlying semi-ring.
Cube pruning is based on this approximation
(Chiang, 2007). It starts with the combination of the
top items in the lists to be combined. At each step, it
puts the neighbors of the current best combination,
which consists of going one position down in one of
the k-best lists, into a priority queue. The algorithm
stops when k items have been popped off from the
queue. At the final step, it sorts the popped items
since they can be out-of-order. It reduces the combi-
nation complexity from O(k2) to O(k ? log(k)).
Our history-augmented parsing is analogous to
MT decoding. The possible higher-order histories
can similarly be limited to at most k in each com-
plete or incomplete item. The core loop of the gener-
alized algorithm which has a complexity of O(|x|2n)
can similarly be reduced to O(k ?log(k)). Therefore,
the whole parsing algorithm remains O(|x|3) re-
gardless how deep we look into parsing history. Fig-
ure 6 illustrates the computation. We apply rule (b)
to combine two lists of augmented complete items
and keep the combinations with the highest model
scores. With cube pruning, we only explore cells at
(0, 0), (0, 1), (1, 0), (2, 0), and (1, 1), without the
need to evaluate scoring functions for the remaining
cells in the table. Similar computation happens with
rule (a).
In this example cube pruning does find the high-
est scoring combination, i.e., cell (1, 1). However,
note that the scores are not monotonic in the order in
which we search these cells as non-local features are
used to score the combinations. Thus, cube pruning
may not find the highest scoring combination. This
approximation is at the heart of cube pruning.
4.1 Recombination
The significance of using feature signatures is that
when two combinations result in a state with the
324
identical feature signature the one with the highest
score survives. This is the core principle of dynamic
programming. We call it recombination. It denotes
the same meaning as state-merging in Huang and
Sagae (2010) for transition-based parsers.
In cube pruning, with recombination, the k-best
items in each chart cell are locally optimal (in the
pruned search space) over all sub-trees with an
equivalent state for future combinations. The cube
pruning algorithm without recombination degener-
ates to a recursive k-best re-scoring algorithm since
each of the k-best items would be unique by itself
as a sub-tree. It should be noted that by working
on a chart (or a forest, equivalently) the algorithm is
already applying recombination at a coarser level.
In machine translation, due to its large search
space and the abstract nature of an n-gram language
model, it is more common to see many sub-trees
with the same language model feature signature,
making recombination crucial (Chiang, 2007). In
constituent parser reranking (Huang, 2008), recom-
bination is less likely to happen since the rerank-
ing features capture peculiarities of local tree struc-
tures. For dependency parsing, we hypothesize that
the higher-order features are more similar to the n-
gram language model features in MT as they tend to
be common features among many sub-trees. But as
the feature set becomes richer, recombination tends
to have a smaller effect. We will discuss the empiri-
cal results on recombination in Section 5.4.
5 Experiments
We define the scoring function f(x, y) as a linear
classifier between a vector of features and a corre-
sponding weight vector, i.e., f(x, y) = w ? ?(x, y).
The feature function ? decomposes with respect to
scoring function f . We train the weights to optimize
the first-best structure. We use the max-loss vari-
ant of the margin infused relaxed algorithm (MIRA)
(Crammer et al2006) with a hamming-loss margin
as is common in the dependency parsing literature
(Martins et al2009; Martins et al2010). MIRA
only requires a first-best decoding algorithm, which
in our case is the approximate chart-based parsing
algorithms defined in Sections 3 and 4. Because our
decoding algorithm is approximate, this may lead to
invalid updates given to the optimizer (Huang and
=
0 : 1 : 2 :
+
0 : f = 2.5 f = 1 f = 2
1 : f = 1.5 f = 3.2 f = 0.5
2 : f = 2.3 f = 3 f = 1.8
...
Figure 6: Combining two lists of complete items with
cube pruning.
Fayong, 2012). However, we found that ignoring or
modifying such updates led to negligible differences
in practice. In all our experiments, we train MIRA
for 8 epochs and use a beam of k = 5 during de-
coding. Both these values were determined on the
English development data.
5.1 Features
The feature templates we use are drawn from the
past work on graph-based parsing and transition-
based parsing. The base templates for the higher-
order dependencies are close to Koo and Collins
(2010), with the major exception that our features
include label-tuple information. The basic features
include identities, part of speech tags, and labels of
the words in dependency structures. These atomic
features are conjoined with the directions of arcs to
create composite n-gram features. The higher-order
dependency features can be categorized into the fol-
lowing sub-groups, where we use h to indicate the
head, m the modifier, s the modifier?s sibling and gc
a grandchild word in a dependency part.
? (labeled) modifier features: (h l?? m)
? (labeled) sibling features: (h l?? m,h l??? s)
? (labeled) outermost grandchild features:
(h l?? m l
?
?? gc)
? (labeled) tri-sibling features:
(h l?? m,h l
?
?? s, h l
??
?? s2)
? (labeled) grand-sibling features:
(h l?? m l
?
?? gc, h l?? m l
??
?? gc2),
325
? (labeled) sibling and grandchild conjoined features:
(h l?? m,h l
?
?? s,m l
??
?? gc)
The general history features include valencies of
words conjoined with the directions of the dominat-
ing arcs. The positions of the modifiers are also con-
joined with the higher-order dependency features in
the previous list.
The features that are new compared to Koo and
Collins (2010) are the label tuple features, the sib-
ling and grandchild conjoined features, and the va-
lency features. We determine this feature set based
on experiments on the development data for English.
In Section 5.3 we examine the impact of these new
features on parser performance.
5.2 Main Results
Our first set of results are on English dependen-
cies. We used the Penn WSJ Treebank converted to
dependencies with Penn2Malt1 conversion software
specifying Yamada and Matsumoto head rules and
Malt label set. We used the standard splits of this
data: sections 2-21 for training; section 22 for vali-
dation; and section 23 for evaluation. We evaluated
our parsers using standard labeled accuracy scores
(LAS) and unlabeled accuracy scores (UAS) exclud-
ing punctuation. We report run-times in tokens per
second. Part-of-speech tags are predicted as input
using a linear-chain CRF.
Results are given in Table 1. We compare our
method to a state-of-the-art graph-based parser (Koo
and Collins, 2010) as well as a state-of-the-art
transition-based parser that uses a beam (Zhang
and Nivre, 2011) and the dynamic programming
transition-based parser of Huang and Sagae (2010).
Additionally, we compare to our own implementa-
tion of exact first to third-order graph-based parsing
and the transition-based system of Zhang and Nivre
(2011) with varying beam sizes.
There are a number of points to make. First,
approximate decoding with rich features and cube
pruning gives state-of-the-art labeled and unlabeled
parsing accuracies relative to previously reported re-
sults. This includes the best graph-based parsing
results of Koo and Collins (2010), which has near
identical performance, as well as the best beam-
based and dynamic-programming-based transition
1http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
Parser UAS LAS Toks/Sec
Huang and Sagae (2010) 92.1- - -
Zhang and Nivre (2011) 92.9- 91.8- -
Zhang and Nivre (reimpl.) (beam=64) 92.73 91.67 760
Zhang and Nivre (reimpl.) (beam=256) 92.75 91.71 190
Koo and Collins (2010) 93.04 - -
1st-order exact (reimpl.) 91.80 90.50 2070
2nd-order exact (reimpl.) 92.40 91.12 1110
3rd-order exact (reimpl.) 92.81 -? 50
this paper 93.06 91.86 220
Table 1: Comparing this work in terms of parsing accu-
racy compared to state-of-the-art baselines on the English
test data. We also report results for a re-implementation
of exact first to third-order graph-based parsing and a re-
implementation of Zhang and Nivre (2011) in order to
compare parser speed. ?Our exact third-order implemen-
tation currently only supports unlabeled parsing.
parsers (Huang and Sagae, 2010; Zhang and Nivre,
2011). Second, at a similar toks/sec parser speed,
our method achieves better performance than the
transition-based model of Zhang and Nivre (2011)
with a beam of 256. Finally, compared to an im-
plementation of an exact third-order parser ? which
provides us with an apples-to-apples comparison in
terms of features and runtime ? approximate decod-
ing with cube pruning is both more accurate and
while being 4-5 times as fast. It is more accurate as
it can easily incorporate more complex features and
it is faster since its asymptotic complexity is lower.
We should point out that our third-order reimple-
mentation is a purely unlabeled parser as we do not
have an implementation of an exact labeled third-
order parser. This likely under estimates its accu-
racy, but also significantly overestimates its speed.
Next, we looked at the impact of our system
on non-English treebanks. Specifically we fo-
cused on two sets of data. The first is the Chi-
nese Treebank converted to dependencies. Here
we use the identical training/validation/evaluation
splits and experimental set-up as Zhang and Nivre
(2011). Additionally, we evaluate our system on
eight other languages from the CoNLL 2006/2007
shared-task (Buchholz and Marsi, 2006; Nivre et
al., 2007). We selected the following four data
sets since they are primarily projective treebanks
(<1.0% non-projective arcs): Bulgarian and Span-
ish from CoNLL 2006 as well as Catalan and Ital-
ian from CoNLL 2007. Currently our method is
restricted to predicting strictly projective trees as it
326
uses the Eisner chart parsing algorithm as its back-
bone. We also report results from four additional
CoNLL data sets reported in Rush and Petrov (2012)
in order to directly compare accuracy. These are
German, Japanese, Portuguese and Swedish. For all
data sets we measure UAS and LAS excluding punc-
tuation and use gold tags as input to the parser as is
standard for these data sets.
Results are given in Table 2. Here we compare to
our re-implementations of Zhang and Nivre (2011),
exact first to third-order parsing and Rush and Petrov
(2012) for the data sets in which they reported re-
sults. We again see that approximate decoding with
rich features and cube pruning has higher accu-
racy than transition-based parsing with a large beam.
In particular, for the ZH-CTB data set, our system
is currently the best reported result. Furthermore,
our system returns comparable accuracy with exact
third-order parsing, while being significantly faster
and more flexible.
5.3 Ablation studies
In this section, we analyze the contributions from
each of the feature groups. Each row in Table 3 uses
a super set of features than the previous row. All
systems use our proposed generalized higher-order
parser with cube-pruning. I.e., they are all using the
Eisner chart-parsing algorithm with expanded fea-
ture signatures. The only difference between sys-
tems is the set of features used. This allows us to see
the improvement from additional features.
The first row uses no higher-order features. It
is equivalent to the first-order model from Table 1.
The only difference is that it uses the k-best algo-
rithm to find the first-best, so it has additional over-
head compared to the standard Viterbi algorithm.
Each of the following rows gets a higher accuracy
than its previous row by adding more higher or-
der features. Putting in the sibling and grandchild
conjoined features and the valency features yields
a further improvement over the approximation of
Koo and Collins (2010). Thus, the addition of new
higher-order features, including valency, extra third-
order, and label tuple features, results in increased
accuracy. However, this is not without cost as the
run-time in terms of tokens/sec decreases (300 to
220). But this decrease is not asymptotic, as it would
be if one were to exactly search over our final model
Higher-order Features UAS LAS Toks/Sec
none 91.74 90.46 1510
McDonald (2006) features + labels 92.48 91.25 860
Carreras (2007) features + labels 92.85 91.66 540
Koo (2010) features + labels 92.92 91.75 300
all features 93.06 91.86 220
Table 3: Generalized higher-order parsing with cube
pruning using different feature sets.
Beam Recombination UAS LAS Toks/Sec
2 no 92.86 91.63 280
2 yes 92.89 91.65 260
5 no 93.05 91.85 240
5 yes 93.06 91.86 230
10 yes 93.05 91.85 140
Table 4: Showing the effect of better search on accuracy
and speed on the English test data with a fixed model.
with these additional features, e.g., valency would at
least multiply an additional O(n) factor.
5.4 Impact of Search Errors
Since our decoding algorithm is not exact, it could
return sub-optimal outputs under the current model.
We analyze the effect of search errors on accuracies
in Table 4. We vary the beam size at each cell and
switch the option for signature-based recombination
to make search better or worse to see how much im-
pact it has on the final accuracy.
The results indicate that a relatively small per-cell
beam is good enough. Going from a beam of 2 to
5 increases accuracy notably, but going to a larger
beam size has little effect but at a cost in terms of
efficiency. This suggests that most of the parser am-
biguity is represented in the top-5 feature signatures
at each chart cell. Furthermore, recombination does
help slightly, but more so at smaller beam sizes.
If we keep the beam size constant but enlarge
the feature scope from second-order to third-order,
one would expect more search errors to occur. We
measured this empirically by computing the num-
ber of sentences where the gold tree had a higher
model score than the predicted tree in the English
evaluation data. Indeed, larger feature scopes do
lead to more search errors, but the absolute num-
ber of search errors is usually quite small ? there
are only 19 search errors using second-order features
and 32 search errors using third-order plus valency
features out of 2416 English test sentences. Part
of the reason for this is that there are only 12 la-
327
Zhang and Nivre Zhang and Nivre Rush 1st-order 2nd-order 3rd-order
(reimpl.) (reimpl.) and exact exact exact
Language (beam=64) (beam=256) Petrov? (reimpl.) (reimpl.) (reimpl.) this paper
BG-CONLL 92.22 / 87.87 92.28 / 87.91 91.9- / - 91.98 / 87.13 93.02 / 88.13 92.96 / - 93.08 / 88.23
CA-CONLL 93.76 / 87.74 93.83 / 87.85 92.83 / 86.22 93.45 / 87.19 94.07 / - 94.00 / 88.08
DE-CONLL 89.18 / 86.50 88.94 / 86.58 90.8- / - 89.28 / 86.06 90.87 / 87.72 91.29 / - 91.35 / 88.42
ES-CONLL 86.64 / 83.25 86.62 / 83.11 85.35 / 81.53 86.80 / 82.91 87.26 / - 87.48 / 84.05
IT-CONLL 85.51 / 81.12 85.45 / 81.10 84.98 / 80.23 85.46 / 80.66 86.49 / - 86.54 / 82.15
JA-CONLL 92.70 / 91.03 92.76 / 91.09 92.3- / - 93.00 / 91.03 93.20 / 91.25 93.36 / - 93.24 / 91.45
PT-CONLL 91.32 / 86.98 91.28 / 86.88 91.5- / - 90.36 / 85.77 91.36 / 87.22 91.66 / - 91.69 / 87.70
SV-CONLL 90.84 / 85.30 91.00 / 85.42 90.1- / - 89.32 / 82.06 90.50 / 83.01 90.32 / - 91.44 / 84.58
ZH-CTB 86.04 / 84.48? 86.14 / 84.57 84.38 / 82.62 86.63 / 84.95 86.77 / - 86.87 / 85.19
AVG 89.80 / 86.03 89.81 / 86.06 89.05 / 84.74 90.14 / 85.89 90.46 / - 90.63 / 86.65
Table 2: UAS/LAS for experiments on non-English treebanks. Numbers in bold are the highest scoring system. Zhang
and Nivre is a reimplementation of Zhang and Nivre (2011) with beams of size 64 and 256. Rush and Petrov are
the UAS results reported in Rush and Petrov (2012). Nth-order exact are implementations of exact 1st-3rd order
dependency parsing. ?For reference, Zhang and Nivre (2011) report 86.0/84.4, which is previously the best result
reported on this data set. ?It should be noted that Rush and Petrov (2012) do not jointly optimize labeled and unlabeled
dependency structure, which we found to often help. This, plus extra features, accounts for the differences in UAS.
bels in the Penn2Malt label set, which results in lit-
tle non-structural ambiguity. In contrast, Stanford-
style dependencies contain a much larger set of la-
bels (50) with more fine-grained syntactic distinc-
tions (De Marneffe et al2006). Training and test-
ing a model using this dependency representation2
increases the number of search errors of the full
model to 126 out 2416 sentences. But that is still
only 5% of all sentences and significantly smaller
when measured per dependency.
6 Related Work
As mentioned in the introduction, there has been
numerous studies on trying to reconcile the rich-
features versus exact decoding trade-off in depen-
dency parsing. In the transition-based parsing lit-
erature this has included the use of beam search to
increase the search space (Duan et al2007; Johans-
son and Nugues, 2007; Titov and Henderson, 2007;
Zhang and Clark, 2008; Zhang and Nivre, 2011).
Huang and Sagae (2010) took a more principled ap-
proach proposing a method combining shift-reduce
parsing with dynamic programming. They showed
how feature signatures can be compiled into dy-
2This model gets 90.4/92.8 LAS/UAS which is comparable
to the UAS of 92.7 reported by Rush and Petrov (2012).
namic programming states and how best-first search
can be used to find the optimal transition sequence.
However, when the feature scope becomes large,
then the state-space and resulting search space can
be either intractable or simply non-practical to ex-
plore. Thus, they resort to an approximate beam
search that still exploring an exponentially-larger
space than greedy or beam-search transition-based
systems. One can view the contribution in this pa-
per as being the complement of the work of Huang
and Sagae (2010) for graph-based systems. Our ap-
proach also uses approximate decoding in order to
exploit arbitrary feature scope, while still exploring
an exponentially-large search space. The primary
difference is how the system is parameterized, over
dependency sub-graphs or transitions. Another criti-
cal difference is that a chart-based algorithm, though
still subject to search errors, is less likely to be hin-
dered by an error made at one word position be-
cause it searches over many parallel alternatives in a
bottom-up search as opposed to a left-to-right pass.
In the graph-based parsing literature, exact pars-
ing algorithms for higher-order features have been
studied extensively (McDonald and Pereira, 2006;
Carreras, 2007; Koo and Collins, 2010), but at a
high computational cost as increasing the order of a
model typically results in an asymptotic increase in
328
running time. ILP formulations of parsing (Riedel
and Clarke, 2006; Martins et al2009; Martins et
al., 2010) also allow for exact inference with higher-
order features, but again at a high computational
cost as ILP?s have, in the worst-case, exponential
run-time with respect to the sentence length. Stud-
ies that have abandoned exact inference have fo-
cused on sampling (Nakagawa, 2007), belief prop-
agation (Smith and Eisner, 2008), Lagrangian re-
laxation (Koo et al2010; Martins et al2011),
and more recently structured prediction cascades
(Weiss and Taskar, 2010; Rush and Petrov, 2012).
However, these approximations themselves are often
computationally expensive, requiring multiple de-
coding/sampling stages in order to produce an out-
put. All the methods above, both exact and approx-
imate, require specialized algorithms for every new
feature that is beyond the scope of the previous fac-
torization. In our method, the same parsing algo-
rithm can be utilized (Eisner?s + cube pruning) just
with slight different feature signatures.
Our proposed parsing model draws heavily on the
work of Huang (2008). Huang introduced the idea
of ?forest rescoring?, which uses cube pruning to
enable the incorporation of non-local features into
a constituency parsing model providing state-of-the
art performance. This paper is the extension of such
ideas to dependency parsing, also giving state-of-
the-art results. An important difference between our
formulation and forest rescoring is that we only have
one decoding pass and a single trained model, while
forest rescoring, as formulated by Huang (2008),
separates a generative base model from a follow-
ing discriminative re-ranking model. Hence, our
formulation is more akin to the one pass decoding
algorithm of Chiang (2007) for integrated decoding
with a language model in machine translation. This
also distinguishes it from previous work on depen-
dency parse re-ranking (Hall, 2007) as we are not
re-ranking/re-scoring the output of a base model but
using a single decoding algorithm and learned model
at training and testing.
This work is largely orthogonal to other attempts
to speed up chart parsing algorithms. This in-
cludes work on coarse-to-fine parsing (Charniak and
Johnson, 2005; Petrov and Klein, 2007; Rush and
Petrov, 2012), chart-cell closing and pruning (Roark
and Hollingshead, 2008; Roark and Hollingshead,
2009), and dynamic beam-width prediction (Boden-
stab et al2011). Of particular note, Rush and
Petrov (2012) report run-times far better than our
cube pruning system. At the heart of their system is
a linear time vine-parsing stage that prunes most of
the search space before higher-order parsing. This
effectively makes their final system linear time in
practice as the higher order models have far fewer
parts to consider. One could easily use the same
first-pass pruner in our cube-pruning framework.
In our study we use cube pruning only for de-
coding and rely on inference-based learning algo-
rithms to train model parameters. Gimpel and Smith
(2009) extended cube pruning concepts to partition-
function and marginal calculations, which would en-
able the training of probabilistic graphical models.
Finally, due to its use of the Eisner chart-parsing
algorithm as a backbone, our model is fundamen-
tally limited to predicting projective dependency
structures. Investigating extensions of this work to
the non-projective case is an area of future study.
Work on defining bottom-up chart-parsing algo-
rithms for non-projective dependency trees could
potentially serve as a mechanism to solving this
problem (Go?mez-Rodr??guez et al2009; Kuhlmann
and Satta, 2009; Go?mez-Rodr??guez et al2010).
7 Conclusion
In this paper we presented a method for general-
ized higher-order dependency parsing. The method
works by augmenting the dynamic programming
signatures of the Eisner chart-parsing algorithm and
then controlling complexity via cube pruning. The
resulting system has the flexibility to incorporate ar-
bitrary feature history while still exploring an ex-
ponential search space efficiently. Empirical results
show that the system gives state-of-the-art accura-
cies across numerous data sets while still maintain-
ing practical parsing speeds ? as much as 4-5 times
faster than exact third-order decoding.
Acknowledgments: We would like to thank Sasha Rush
and Slav Petrov for help modifying their hypergraph pars-
ing code. We would also like to thank the parsing team
at Google for providing interesting discussions and new
ideas while we conducted this work, as well as comments
on earlier drafts of the paper.
329
References
N. Bodenstab, A. Dunlop, K. Hall, and B. Roark. 2011.
Beam-width prediction for efficient context-free pars-
ing. In Proc. ACL.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of the CoNLL
Shared Task Session of EMNLP-CoNLL.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proc. ACL.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research.
M. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc. of LREC.
X. Duan, J. Zhao, and B. Xu. 2007. Probabilistic parsing
action models for multi-lingual dependency parsing.
In Proc. of EMNLP-CoNLL.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: an exploration. In Proc. of COL-
ING.
K. Gimpel and N.A. Smith. 2009. Cube summing,
approximate inference with non-local features, and
dynamic programming without semirings. In Proc.
EACL.
C. Go?mez-Rodr??guez, M. Kuhlmann, G. Satta, and
D. Weir. 2009. Optimal reduction of rule length in lin-
ear context-free rewriting systems. In Proc. NAACL.
C. Go?mez-Rodr??guez, M. Kuhlmann, and G. Satta. 2010.
Efficient parsing of well-nested linear context-free
rewriting systems. In Proc. NAACL.
K. Hall. 2007. K-best spanning tree parsing. In Proc. of
ACL.
L. Huang and S. Fayong. 2012. Structured perceptron
with inexact search. In Proc. of NAACL.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proc. of ACL.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In Proc. of ACL.
R. Johansson and P. Nugues. 2007. Incremental de-
pendency parsing using online learning. In Proc. of
EMNLP-CoNLL.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of ACL.
T. Koo, A. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proc. of EMNLP.
M. Kuhlmann and G. Satta. 2009. Treebank grammar
techniques for non-projective dependency parsing. In
Proc. EACL.
A. F. T. Martins, N. Smith, and E. P. Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proc. of ACL.
A. F. T. Martins, N. Smith, E. P. Xing, P. M. Q. Aguiar,
and M. A. T. Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference.
In Proc. of EMNLP.
A. F. T. Martins, N. Smith, M. A. T. Figueiredo, and
P. M. Q. Aguiar. 2011. Dual decomposition with
many overlapping components. In Proc of EMNLP.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of EMNLP-CoNLL.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
of EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
T. Nakagawa. 2007. Multilingual dependency parsing
using global features. In Proc. of EMNLP-CoNLL.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proc. of ACL.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc. of
EMNLP-CoNLL.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proc. NAACL.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In Proc. of EMNLP.
B. Roark and K. Hollingshead. 2008. Classifying chart
cells for quadratic complexity context-free inference.
In Proc. COLING.
B. Roark and K. Hollingshead. 2009. Linear complexity
context-free parsing pipelines via chart constraints. In
Proce. NAACL.
A. Rush and S. Petrov. 2012. Efficient multi-pass depen-
dency pruning with vine parsing. In Proc. of NAACL.
D. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. of EMNLP.
I. Titov and J. Henderson. 2007. Fast and robust mul-
tilingual dependency parsing with a generative latent
variable model. In Proc. of EMNLP-CoNLL.
D. Weiss and B. Taskar. 2010. Structured prediction cas-
cades. In Proc. of AISTATS.
330
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing. In Proc.
of EMNLP.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proc. of
ACL-HLT, volume 2.
331
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 908?913,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Online Learning for Inexact Hypergraph Search
Hao Zhang
Google
haozhang@google.com
Liang Huang Kai Zhao
City University of New York
{lhuang@cs.qc,kzhao@gc}.cuny.edu
Ryan McDonald
Google
ryanmcd@google.com
Abstract
Online learning algorithms like the percep-
tron are widely used for structured predic-
tion tasks. For sequential search problems,
like left-to-right tagging and parsing, beam
search has been successfully combined with
perceptron variants that accommodate search
errors (Collins and Roark, 2004; Huang et
al., 2012). However, perceptron training with
inexact search is less studied for bottom-up
parsing and, more generally, inference over
hypergraphs. In this paper, we generalize
the violation-fixing perceptron of Huang et
al. (2012) to hypergraphs and apply it to the
cube-pruning parser of Zhang and McDonald
(2012). This results in the highest reported
scores on WSJ evaluation set (UAS 93.50%
and LAS 92.41% respectively) without the aid
of additional resources.
1 Introduction
Structured prediction problems generally deal with
exponentially many outputs, often making exact
search infeasible. For sequential search problems,
such as tagging and incremental parsing, beam
search coupled with perceptron algorithms that ac-
count for potential search errors have been shown
to be a powerful combination (Collins and Roark,
2004; Daume? and Marcu, 2005; Zhang and Clark,
2008; Huang et al, 2012). However, sequen-
tial search algorithms, and in particular left-to-right
beam search (Collins and Roark, 2004; Zhang and
Clark, 2008), squeeze inference into a very narrow
space. To address this, Huang (2008) formulated
constituency parsing as approximate bottom-up in-
ference in order to compactly represent an exponen-
tial number of outputs while scoring features of ar-
bitrary scope. This idea was adapted to graph-based
dependency parsers by Zhang and McDonald (2012)
and shown to outperform left-to-right beam search.
Both these examples, bottom-up approximate de-
pendency and constituency parsing, can be viewed
as specific instances of inexact hypergraph search.
Typically, the approximation is accomplished by
cube-pruning throughout the hypergraph (Chiang,
2007). Unfortunately, as the scope of features at
each node increases, the inexactness of search and
its negative impact on learning can potentially be ex-
acerbated. Unlike sequential search, the impact on
learning of approximate hypergraph search ? as well
as methods to mitigate any ill effects ? has not been
studied. Motivated by this, we develop online learn-
ing algorithms for inexact hypergraph search by gen-
eralizing the violation-fixing percepron of Huang et
al. (2012). We empirically validate the benefit of
this approach within the cube-pruning dependency
parser of Zhang and McDonald (2012).
2 Structured Perceptron for Inexact
Hypergraph Search
The structured perceptron algorithm (Collins, 2002)
is a general learning algorithm. Given training in-
stances (x, y?), the algorithm first solves the decod-
ing problem y? = argmaxy?Y(x)w ? f(x, y) given
the weight vector w for the high-dimensional fea-
ture representation f of the mapping (x, y), where
y? is the prediction under the current model, y? is the
gold output and Y(x) is the space of all valid outputs
for input x. The perceptron update rule is simply:
w? = w + f(x, y?) ? f(x, y?).
The convergence of original perceptron algorithm
relies on the argmax function being exact so that
the conditionw ?f(x, y?) > w ?f(x, y?) (modulo ties)
always holds. This condition is called a violation
because the prediction y? scores higher than the cor-
rect label y?. Each perceptron update moves weights
908
A B C D E F
G H I J
K L
M
N
Figure 1: A hypergraph showing the union of the gold
and Viterbi subtrees. The hyperedges in bold and dashed
are from the gold and Viterbi trees, respectively.
away from y? and towards y? to fix such violations.
But when search is inexact, y? could be suboptimal
so that sometimes w ? f(x, y?) < w ? f(x, y?). Huang
et al (2012) named such instances non-violations
and showed that perceptron model updates for non-
violations nullify guarantees of convergence. To ac-
count for this, they generalized the original update
rule to select an output y? within the pruned search
space that scores higher than y?, but is not necessar-
ily the highest among all possibilities, which repre-
sents a true violation of the model on that training
instance. This violation fixing perceptron thus re-
laxes the argmax function to accommodate inexact
search and becomes provably convergent as a result.
In the sequential cases where y? has a linear struc-
ture such as tagging and incremental parsing, the
violation fixing perceptron boils down to finding
and updating along a certain prefix of y?. Collins
and Roark (2004) locate the earliest position in a
chain structure where y?pref is worse than y?pref by
a margin large enough to cause y? to be dropped
from the beam. Huang et al (2012) locate the po-
sition where the violation is largest among all pre-
fixes of y?, where size of a violation is defined as
w ? f(x, y?pref) ? w ? f(x, y?pref).
For hypergraphs, the notion of prefix must be gen-
eralized to subtrees. Figure 1 shows the packed-
forest representation of the union of gold subtrees
and highest-scoring (Viterbi) subtrees at every gold
node for an input. At each gold node, there are
two incoming hyperedges: one for the gold subtree
and the other for the Viterbi subtree. After bottom-
up parsing, we can compute the scores for the gold
subtrees as well as extract the corresponding Viterbi
subtrees by following backpointers. These Viterbi
subtrees need not necessarily to belong to the full
Viterbi path (i.e., the Viterbi tree rooted at node N ).
An update strategy must choose a subtree or a set of
subtrees at gold nodes. This is to ensure that the
model is updating its weights relative to the inter-
section of the search space and the gold path.
Our first update strategy is called single-node
max-violation (s-max). Given a gold tree y?, it tra-
verses the gold tree and finds the node n on which
the violation between the Viterbi subtree and the
gold subtree is the largest over all gold nodes. The
violation is guaranteed to be greater than or equal to
zero because the lower bound for the max-violation
on any hypergraph is 0 which happens at the leaf
nodes. Then we choose the subtree pair (y?n, y?n) and
do the update similar to the prefix update for the se-
quential case. For example, in Figure 1, suppose the
max-violation happens at node K , which covers the
left half of the input x, then the perceptron update
would move parameters to the subtree represented
by nodes B , C , H and K and away from A ,
B , G and K .
Our second update strategy is called parallel max-
violation (p-max). It is based on the observation that
violations on non-overlapping nodes can be fixed
in parallel. We define a set of frontiers as a set
of nodes that are non-overlapping and the union of
which covers the entire input string x. The frontier
set can include up to |x| nodes, in the case where the
frontier is equivalent to the set of leaves. We traverse
y? bottom-up to compute the set of frontiers such
that each has the max-violation in the span it cov-
ers. Concretely, for each node n, the max-violation
frontier set can be defined recursively,
ft(n) =
{
n, if n = maxv(n)
?
ni?children(n) ft(ni), otherwise
where maxv(n) is the function that returns the node
with the absolute maximum violation in the subtree
rooted at n and can easily be computed recursively
over the hypergraph. To make a perceptron update,
we generate the max-violation frontier set for the en-
tire hypergraph and use it to choose subtree pairs
?
n?ft(root(x))(y?n, y
?
n), where root(x) is the root of
the hypergraph for input x. For example, in Figure 1,
if the union of K and L satisfies the definition of
ft, then the perceptron update would move feature
909
weights away from the union of the two Viterbi sub-
trees and towards their gold counterparts.
In our experiments, we compare the performance
of the two violation-fixing update strategies against
two baselines. The first baseline is the standard up-
date, where updates always happen at the root node
of a gold tree, even if the Viterbi tree at the root node
leads to a non-violation update. The second baseline
is the skip update, which also always updates at the
root nodes but skips any non-violations. This is the
strategy used by Zhang and McDonald (2012).
3 Experiments
We ran a number of experiments on the cube-
pruning dependency parser of Zhang and McDonald
(2012), whose search space can be represented as a
hypergraph in which the nodes are the complete and
incomplete states and the hyperedges are the instan-
tiations of the two parsing rules in the Eisner algo-
rithm (Eisner, 1996).
The feature templates we used are a superset of
Zhang and McDonald (2012). These features in-
clude first-, second-, and third-order features and
their labeled counterparts, as well as valency fea-
tures. In addition, we also included a feature tem-
plate from Bohnet and Kuhn (2012). This tem-
plate examines the leftmost child and the rightmost
child of a modifier simultaneously. All other high-
order features of Zhang and McDonald (2012) only
look at arcs on the same side of their head. We
trained the parser with hamming-loss-augmented
MIRA (Crammer et al, 2006), following Martins et
al. (2010). Based on results on the English valida-
tion data, in all the experiments, we trained MIRA
with 8 epochs and used a beam of size 6 per node.
To speed up the parser, we used an unlabeled
first-order model to prune unlikely dependency arcs
at both training and testing time (Koo and Collins,
2010; Martins et al, 2013). We followed Rush and
Petrov (2012) to train the first-order model to min-
imize filter loss with respect to max-marginal filter-
ing. On the English validation corpus, the filtering
model pruned 80% of arcs while keeping the oracle
unlabeled attachment score above 99.50%. During
training only, we insert the gold tree into the hy-
pergraph if it was mistakenly pruned. This ensures
that the gold nodes are always available, which is
required for model updates.
3.1 English and Chinese Results
We report dependency parsing results on the Penn
WSJ Treebank and the Chinese CTB-5 Treebank.
Both treebanks are constituency treebanks. We gen-
erated two versions of dependency treebanks by ap-
plying commonly-used conversion procedures. For
the first English version (PTB-YM), we used the
Penn2Malt1 software to apply the head rules of Ya-
mada and Matsumoto and the Malt label set. For
the second English version (PTB-S), we used the
Stanford dependency framework (De Marneffe et
al., 2006) by applying version 2.0.5 of the Stan-
ford parser. We split the data in the standard way:
sections 2-21 for training; section 22 for validation;
and section 23 for evaluation. We utilized a linear
chain CRF tagger which has an accuracy of 96.9%
on the validation data and 97.3% on the evaluation
data2. For Chinese, we use the Chinese Penn Tree-
bank converted to dependencies and split into train/-
validation/evaluation according to Zhang and Nivre
(2011). We report both unlabeled attachment scores
(UAS) and labeled attachment scores (LAS), ignor-
ing punctuations (Buchholz and Marsi, 2006).
Table 1 displays the results. Our improved
cube-pruned parser represents a significant improve-
ment over the feature-rich transition-based parser of
Zhang and Nivre (2011) with a large beam size. It
also improves over the baseline cube-pruning parser
without max-violation update strategies (Zhang and
McDonald, 2012), showing the importance of up-
date strategies in inexact hypergraph search. The
UAS score on Penn-YM is slightly higher than the
best result known in the literature which was re-
ported by the fourth-order unlabeled dependency
parser of Ma and Zhao (2012), although we did
not utilize fourth-order features. The LAS score on
Penn-YM is on par with the best reported by Bohnet
and Kuhn (2012). On Penn-S, there are not many
existing results to compare with, due to the tradition
of reporting results on Penn-YM in the past. Never-
theless, our result is higher than the second best by
a large margin. Our Chinese parsing scores are the
highest reported results.
1http://stp.lingfil.uu.se//?nivre/research/Penn2Malt.html
2The data was prepared by Andre? F. T. Martins as was done
in Martins et al (2013).
910
Penn-YM Penn-S CTB-5
Parser UAS LAS Toks/Sec UAS LAS Toks/Sec UAS LAS Toks/Sec
Zhang and Nivre (2011) 92.9- 91.8- ?680 - - - 86.0- 84.4- -
Zhang and Nivre (reimpl.) (beam=64) 93.00 91.98 800 92.96 90.74 500 85.93 84.42 700
Zhang and Nivre (reimpl.) (beam=128) 92.94 91.91 400 93.11 90.84 250 86.05 84.50 360
Koo and Collins (2010) 93.04 - - - - - - - -
Zhang and McDonald (2012) 93.06 91.86 220 - - - 86.87 85.19 -
Rush and Petrov (2012) - - - 92.7- - 4460 - - -
Martins et al (2013) 93.07 - 740 92.82 - 600 - - -
Qian and Liu (2013) 93.17 - 180 - - - 87.25 - 100
Bohnet and Kuhn (2012) 93.39 92.38 ?120 - - - 87.5- 85.9- -
Ma and Zhao (2012) 93.4- - - - - - 87.4- - -
cube-pruning w/ skip 93.21 92.07 300 92.92 90.35 200 86.95 85.23 200
w/ s-max 93.50 92.41 300 93.59 91.17 200 87.78 86.13 200
w/ p-max 93.44 92.33 300 93.64 91.28 200 87.87 86.24 200
Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except
punctuations. We also include the tokens per second numbers for different parsers whenever available, although the
numbers from other papers were obtained on different machines. Speed numbers marked with ? were converted from
sentences per second.
The speed of our parser is around 200-300 tokens
per second for English. This is faster than the parser
of Bohnet and Kuhn (2012) which has roughly the
same level of accuracy, but is slower than the parser
of Martins et al (2013) and Rush and Petrov (2012),
both of which only do unlabeled dependency pars-
ing and are less accurate. Given that predicting la-
bels on arcs can slow down a parser by a constant
factor proportional to the size of the label set, the
speed of our parser is competitive. We also tried to
prune away arc labels based on observed labels for
each POS tag pair in the training data. By doing so,
we could speed up our parser to 500-600 tokens per
second with less than a 0.2% drop in both UAS and
LAS.
3.2 Importance of Update Strategies
The lower portion of Table 1 compares cube-pruning
parsing with different online update strategies in or-
der to show the importance of choosing an update
strategy that accommodates search errors. The max-
violation update strategies (s-max and p-max) im-
proved results on both versions of the Penn Treebank
as well as the CTB-5 Chinese treebank. It made
a larger difference on Penn-S relative to Penn-YM,
improving as much as 0.93% in LAS against the skip
update strategy. Additionally, we measured the per-
centage of non-violation updates at root nodes. In
the last epoch of training, on Penn-YM, there was
24% non-violations if we used the skip update strat-
egy; on Penn-S, there was 36% non-violations. The
portion of non-violations indicates the inexactness
 92
 92.2
 92.4
 92.6
 92.8
 93
 93.2
 93.4
 93.6
 93.8
 94
 1  2  3  4  5  6  7  8
UA
S
epochs
UAS on Penn-YM dev
s-max
p-max
skip
standard
Figure 2: Constrast of different update strategies on the
validation data set of Penn-YM. The x-axis is the number
of training epochs. The y-axis is the UAS score. s-max
stands for single-node max-violation. p-max stands for
parallel max-violation.
of the underlying search. Search is harder on Penn-S
due to the larger label set. Thus, as expected, max-
violation update strategies improve most where the
search is the hardest and least exact.
Figure 2 shows accuracy per training epoch on the
validation data. It can be seen that bad update strate-
gies are not simply slow learners. More iterations
of training cannot close the gap between strategies.
Forcing invalid updates on non-violations (standard
update) or simply ignoring them (skip update) pro-
duces less accurate models overall.
911
ZN 2011 (reimpl.) skip s-max p-max Best Published?
Language UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS
SPANISH 86.76 83.81 87.34 84.15 87.96 84.95 87.68 84.75 87.48 84.05
CATALAN 94.00 88.65 94.54 89.14 94.58 89.05 94.98 89.56 94.07 89.09
JAPANESE 93.10 91.57 93.40 91.65 93.26 91.67 93.20 91.49 93.72 91.7-
BULGARIAN 93.08 89.23 93.52 89.25 94.02 89.87 93.80 89.65 93.50 88.23
ITALIAN 87.31 82.88 87.75 83.41 87.57 83.22 87.79 83.59 87.47 83.50
SWEDISH 90.98 85.66 90.64 83.89 91.62 85.08 91.62 85.00 91.44 85.42
ARABIC 78.26 67.09 80.42 69.46 80.48 69.68 80.60 70.12 81.12 66.9-
TURKISH 76.62 66.00 76.18 65.90 76.94 66.80 76.86 66.56 77.55 65.7-
DANISH 90.84 86.65 91.40 86.59 91.88 86.95 92.00 87.07 91.86 84.8-
PORTUGUESE 91.18 87.66 91.69 88.04 92.07 88.30 92.19 88.40 93.03 87.70
GREEK 85.63 78.41 86.37 78.29 86.14 78.20 86.46 78.55 86.05 77.87
SLOVENE 84.63 76.06 85.01 75.92 86.01 77.14 85.77 76.62 86.95 73.4-
CZECH 87.78 82.38 86.92 80.36 88.36 82.16 88.48 82.38 90.32 80.2-
BASQUE 79.65 71.03 79.57 71.43 79.59 71.52 79.61 71.65 80.23 73.18
HUNGARIAN 84.71 80.16 85.67 80.84 85.85 81.02 86.49 81.67 86.81 81.86
GERMAN 91.57 89.48 91.23 88.34 92.03 89.44 91.79 89.28 92.41 88.42
DUTCH 82.49 79.71 83.01 79.79 83.57 80.29 83.35 80.09 86.19 79.2-
AVG 86.98 81.55 87.33 81.56 87.76 82.08 87.80 82.14
Table 2: Parsing Results for languages from CoNLL 2006/2007 shared tasks. When a language is in both years,
we use the 2006 data set. The best results with ? are the maximum in the following papers: Buchholz and Marsi
(2006), Nivre et al (2007), Zhang and McDonald (2012), Bohnet and Kuhn (2012), and Martins et al (2013), For
consistency, we scored the CoNLL 2007 best systems with the CoNLL 2006 evaluation script. ZN 2011 (reimpl.) is
our reimplementation of Zhang and Nivre (2011), with a beam of 64. Results in bold are the best among ZN 2011
reimplementation and different update strategies from this paper.
3.3 CoNLL Results
We also report parsing results for 17 languages from
the CoNLL 2006/2007 shared-task (Buchholz and
Marsi, 2006; Nivre et al, 2007). The parser in
our experiments can only produce projective depen-
dency trees as it uses an Eisner algorithm backbone
to generate the hypergraph (Eisner, 1996). So, at
training time, we convert non-projective trees ? of
which there are many in the CoNLL data ? to projec-
tive ones through flattening, i.e., attaching words to
the lowest ancestor that results in projective trees. At
testing time, our parser can only predict projective
trees, though we evaluate on the true non-projective
trees.
Table 2 shows the full results. We sort the
languages according to the percentage of non-
projective trees in increasing order. The Spanish
treebank is 98% projective, while the Dutch tree-
bank is only 64% projective. With respect to the
Zhang and Nivre (2011) baseline, we improved UAS
in 16 languages and LAS in 15 languages. The im-
provements are stronger for the projective languages
in the top rows. We achieved the best published
UAS results for 7 languages: Spanish, Catalan, Bul-
garain, Italian, Swedish, Danish, and Greek. As
these languages are typically from the more projec-
tive data sets, we speculate that extending the parser
used in this study to handle non-projectivity will
lead to state-of-the-art models for the majority of
languages.
4 Conclusions
We proposed perceptron update strategies for in-
exact hypergraph search and experimented with
a cube-pruning dependency parser. Both single-
node max-violation and parallel max-violation up-
date strategies signficantly improved parsing results
over the strategy that ignores any invalid udpates
caused by inexactness of search. The update strate-
gies are applicable to any bottom-up parsing prob-
lems such as constituent parsing (Huang, 2008) and
syntax-based machine translation with online learn-
ing (Chiang et al, 2008).
Acknowledgments: We thank Andre? F. T. Martins
for the dependency converted Penn Treebank with
automatic POS tags from his experiments; the re-
viewers for their useful suggestions; the NLP team
at Google for numerous discussions and comments;
Liang Huang and Kai Zhao are supported in part by
DARPA FA8750-13-2-0041 (DEFT), PSC-CUNY,
and a Google Faculty Research Award.
912
References
B. Bohnet and J. Kuhn. 2012. The best of bothworlds
- a graph-based completion model for transition-based
parsers. In Proc. of EACL.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proc. of EMNLP.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proc. of ACL.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of ACL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research.
H. Daume? and D. Marcu. 2005. Learning as search
optimization: Approximate large margin methods for
structured prediction. In Proc. of ICML.
M. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc. of LREC.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: an exploration. In Proc. of COL-
ING.
L. Huang, S. Fayong, and G. Yang. 2012. Structured
perceptron with inexact search. In Proc. of NAACL.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In Proc. of ACL.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of ACL.
X. Ma and H. Zhao. 2012. Fourth-order dependency
parsing. In Proc. of COLING.
A. F. T. Martins, N. Smith, E. P. Xing, P. M. Q. Aguiar,
and M. A. T. Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference.
In Proc. of EMNLP.
A. F. T. Martins, M. B. Almeida, and N. A. Smith. 2013.
Turning on the turbo: Fast third-order non-projective
turbo parsers. In Proc. of ACL.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc. of
EMNLP-CoNLL.
X. Qian and Y. Liu. 2013. Branch and bound algo-
rithm for dependency parsing with non-local features.
TACL, Vol 1.
A. Rush and S. Petrov. 2012. Efficient multi-pass depen-
dency pruning with vine parsing. In Proc. of NAACL.
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing. In Proc.
of EMNLP.
H. Zhang and R. McDonald. 2012. Generalized higher-
order dependency parsing with cube pruning. In Proc.
of EMNLP.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proc. of
ACL-HLT, volume 2.
913
Binarization of Synchronous
Context-Free Grammars
Liang Huang?
USC/Information Science Institute
Hao Zhang??
Google Inc.
Daniel Gildea?
University of Rochester
Kevin Knight?
USC/Information Science Institute
Systems based on synchronous grammars and tree transducers promise to improve the quality
of statistical machine translation output, but are often very computationally intensive. The
complexity is exponential in the size of individual grammar rules due to arbitrary re-orderings
between the two languages. We develop a theory of binarization for synchronous context-free
grammars and present a linear-time algorithm for binarizing synchronous rules when possible.
In our large-scale experiments, we found that almost all rules are binarizable and the resulting
binarized rule set significantly improves the speed and accuracy of a state-of-the-art syntax-
based machine translation system. We also discuss the more general, and computationally more
difficult, problem of finding good parsing strategies for non-binarizable rules, and present an
approximate polynomial-time algorithm for this problem.
1. Introduction
Several recent syntax-based models for machine translation (Chiang 2005; Galley et al
2004) can be seen as instances of the general framework of synchronous grammars
and tree transducers. In this framework, both alignment (synchronous parsing) and
decoding can be thought of as parsing problems, whose complexity is in general ex-
ponential in the number of nonterminals on the right-hand side of a grammar rule.
To alleviate this problem, we investigate bilingual binarization as a technique to fac-
tor each synchronous grammar rule into a series of binary rules. Although mono-
lingual context-free grammars (CFGs) can always be binarized, this is not the case
? Information Science Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: lhuang@isi.edu,
liang.huang.sh@gmail.com.
?? 1600 Amphitheatre Parkway, Mountain View, CA 94303. E-mail: haozhang@google.com.
? Computer Science Dept., University of Rochester, Rochester NY 14627. E-mail: gildea@cs.rochester.edu.
? Information Science Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu.
Submission received: 14 March 2007; revised submission received: 8 January 2009; accepted for publication:
25 March 2009.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 4
for all synchronous rules; we investigate algorithms for non-binarizable rules as well.
In particular:
r We develop a technique called synchronous binarization and devise a
linear-time binarization algorithm such that the resulting rule set alows
efficient algorithms for both synchronous parsing and decoding with
integrated n-gram language models.
r We examine the effect of this binarization method on end-to-end
translation quality on a large-scale Chinese-to-English syntax-based
system, compared to a more typical baseline method, and a state-of-the-art
phrase-based system.
r We examine the ratio of binarizability in large, empirically derived rule
sets, and show that the vast majority is binarizable. However, we also
provide, for the first time, real examples of non-binarizable cases verified
by native speakers.
r In the final, theoretical, sections of this article, we investigate the general
problem of finding the most efficient synchronous parsing or decoding
strategy for arbitrary synchronous context-free grammar (SCFG) rules,
including non-binarizable cases. Although this problem is believed to be
NP-complete, we prove two results that substantially reduce the search
space over strategies. We also present an optimal algorithm that runs
tractably in practice and a polynomial-time algorithm that is a good
approximation of the former.
Melamed (2003) discusses binarization of multi-text grammars on a theoretical
level, showing the importance and difficulty of binarization for efficient synchronous
parsing. One way around this difficulty is to stipulate that all rules must be binary
from the outset, as in Inversion Transduction Grammar (ITG) (Wu 1997) and the binary
SCFG employed by the Hiero system (Chiang 2005) to model the hierarchical phrases.
In contrast, the rule extraction method of Galley et al (2004) aims to incorporate more
syntactic information by providing parse trees for the target language and extracting
tree transducer rules that apply to the parses. This approach results in rules with many
nonterminals, making good binarization techniques critical.
We explain how synchronous rule binarization interacts with n-gram language
models and affects decoding for machine translation in Section 2. We define binarization
formally in Section 3, and present an efficient algorithm for the problem in Section 4.
Experiments described in Section 51 show that binarization improves machine trans-
lation speed and quality. Some rules cannot be binarized, and we present a decoding
strategy for these rules in Section 6. Section 7 gives a solution to the general theo-
retical problem of finding optimal decoding and synchronous parsing strategies for
arbitrary SCFGs, and presents complexity results on the nonbinarizable rules from our
Chinese?English data. These final two sections are of primarily theoretical interest, as
nonbinarizable rules have not been shown to benefit real-world machine translation sys-
tems. However, the algorithms presented may become relevant as machine translation
systems improve.
1 A preliminary version of Section 1?5 appeared in Zhang et al (2006).
560
Huang et al Binarization of Synchronous Context-Free Grammars
2. Motivation
Consider the following Chinese sentence and its English translation:
(1) ?
Ba`owe?ier
Powell

yu?
with
??
Sha?lo?ng
Sharon
>L
ju?x??ng
hold
?
le
[past.]
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 835?845,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Binarized Forest to String Translation
Hao Zhang
Google Research
haozhang@google.com
Licheng Fang
Computer Science Department
University of Rochester
lfang@cs.rochester.edu
Peng Xu
Google Research
xp@google.com
Xiaoyun Wu
Google Research
xiaoyunwu@google.com
Abstract
Tree-to-string translation is syntax-aware and
efficient but sensitive to parsing errors. Forest-
to-string translation approaches mitigate the
risk of propagating parser errors into transla-
tion errors by considering a forest of alterna-
tive trees, as generated by a source language
parser. We propose an alternative approach to
generating forests that is based on combining
sub-trees within the first best parse through
binarization. Provably, our binarization for-
est can cover any non-consitituent phrases in
a sentence but maintains the desirable prop-
erty that for each span there is at most one
nonterminal so that the grammar constant for
decoding is relatively small. For the purpose
of reducing search errors, we apply the syn-
chronous binarization technique to forest-to-
string decoding. Combining the two tech-
niques, we show that using a fast shift-reduce
parser we can achieve significant quality gains
in NIST 2008 English-to-Chinese track (1.3
BLEU points over a phrase-based system, 0.8
BLEU points over a hierarchical phrase-based
system). Consistent and significant gains are
also shown in WMT 2010 in the English to
German, French, Spanish and Czech tracks.
1 Introduction
In recent years, researchers have explored a wide
spectrum of approaches to incorporate syntax and
structure into machine translation models. The uni-
fying framework for these models is synchronous
grammars (Chiang, 2005) or tree transducers
(Graehl and Knight, 2004). Depending on whether
or not monolingual parsing is carried out on the
source side or the target side for inference, there are
four general categories within the framework:
? string-to-string (Chiang, 2005; Zollmann and
Venugopal, 2006)
? string-to-tree (Galley et al, 2006; Shen et al,
2008)
? tree-to-string (Lin, 2004; Quirk et al, 2005;
Liu et al, 2006; Huang et al, 2006; Mi et al,
2008)
? tree-to-tree (Eisner, 2003; Zhang et al, 2008)
In terms of search, the string-to-x models explore all
possible source parses and map them to the target
side, while the tree-to-x models search over the sub-
space of structures of the source side constrained
by an input tree or trees. Hence, tree-to-x mod-
els are more constrained but more efficient. Mod-
els such as Huang et al (2006) can match multi-
level tree fragments on the source side which means
larger contexts are taken into account for transla-
tion (Poutsma, 2000), which is a modeling advan-
tage. To balance efficiency and accuracy, forest-to-
string models (Mi et al, 2008; Mi and Huang, 2008)
use a compact representation of exponentially many
trees to improve tree-to-string models. Tradition-
ally, such forests are obtained through hyper-edge
pruning in the k-best search space of a monolin-
gual parser (Huang, 2008). The pruning parameters
that control the size of forests are normally hand-
tuned. Such forests encode both syntactic variants
and structural variants. By syntactic variants, we re-
fer to the fact that a parser can parse a substring into
either a noun phrase or verb phrase in certain cases.
835
We believe that structural variants which allow more
source spans to be explored during translation are
more important (DeNeefe et al, 2007), while syn-
tactic variants might improve word sense disam-
biguation but also introduce more spurious ambi-
guities (Chiang, 2005) during decoding. To focus
on structural variants, we propose a family of bina-
rization algorithms to expand one single constituent
tree into a packed forest of binary trees containing
combinations of adjacent tree nodes. We control the
freedom of tree node binary combination by restrict-
ing the distance to the lowest common ancestor of
two tree nodes. We show that the best results are
achieved when the distance is two, i.e., when com-
bining tree nodes sharing a common grand-parent.
In contrast to conventional parser-produced-forest-
to-string models, in our model:
? Forests are not generated by a parser but by
combining sub-structures using a tree binarizer.
? Instead of using arbitary pruning parameters,
we control forest size by an integer number that
defines the degree of tree structure violation.
? There is at most one nonterminal per span so
that the grammar constant is small.
Since GHKM rules (Galley et al, 2004) can cover
multi-level tree fragments, a synchronous grammar
extracted using the GHKM algorithm can have syn-
chronous translation rules with more than two non-
terminals regardless of the branching factor of the
source trees. For the first time, we show that simi-
lar to string-to-tree decoding, synchronous binariza-
tion significantly reduces search errors and improves
translation quality for forest-to-string decoding.
To summarize, the whole pipeline is as follows.
First, a parser produces the highest-scored tree for
an input sentence. Second, the parse tree is re-
structured using our binarization algorithm, result-
ing in a binary packed forest. Third, we apply the
forest-based variant of the GHKM algorithm (Mi
and Huang, 2008) on the new forest for rule extrac-
tion. Fourth, on the translation forest generated by
all applicable translation rules, which is not neces-
sarily binary, we apply the synchronous binarization
algorithm (Zhang et al, 2006) to generate a binary
translation forest. Finally, we use a bottom-up de-
coding algorithm with intergrated LM intersection
using the cube pruning technique (Chiang, 2005).
The rest of the paper is organized as follows. In
Section 2, we give an overview of the forest-to-
string models. In Section 2.1, we introduce a more
efficient and flexible algorithm for extracting com-
posed GHKM rules based on the same principle as
cube pruning (Chiang, 2007). In Section 3, we in-
troduce our source tree binarization algorithm for
producing binarized forests. In Section 4, we ex-
plain how to do synchronous rule factorization in a
forest-to-string decoder. Experimental results are in
Section 5.
2 Forest-to-string Translation
Forest-to-string models can be described as
e = Y( arg max
d?D(T ), T?F (f)
P (d|T ) ) (1)
where f stands for a source string, e stands for a tar-
get string, F stands for a forest, D stands for a set
of synchronous derivations on a given tree T , and
Y stands for the target side yield of a derivation.
The search problem is finding the derivation with
the highest probability in the space of all deriva-
tions for all parse trees for an input sentence. The
log probability of a derivation is normally a lin-
ear combination of local features which enables dy-
namic programming to find the optimal combination
efficiently. In this paper, we focus on the models
based on the Synchronous Tree Substitution Gram-
mars (STSG) defined by Galley et al (2004). In con-
trast to a tree-to-string model, the introduction of F
augments the search space systematically. When the
first-best parse is wrong or no good translation rules
are applicable to the first-best parse, the model can
recover good translations from alternative parses.
In STSG, local features are defined on tree-to-
string rules, which are synchronous grammar rules
defining how a sequence of terminals and nontermi-
nals on the source side translates to a sequence of
target terminals and nonterminals. One-to-one map-
ping of nonterminals is assumed. But terminals do
not necessarily need to be aligned. Figure 1 shows a
typical English-Chinese tree-to-string rule with a re-
ordering pattern consisting of two nonterminals and
different numbers of terminals on the two sides.
836
VP
VBD
was
VP-C
.x1:VBN PP
P
by
.x2:NP-C
? bei? x2 x1
Figure 1: An example tree-to-string rule.
Forest-to-string translation has two stages. The
first stage is rule extraction on word-aligned parallel
texts with source forests. The second stage is rule
enumeration and DP decoding on forests of input
strings. In both stages, at each tree node, the task on
the source side is to generate a list of tree fragments
by composing the tree fragments of its children. We
propose a cube-pruning style algorithm that is suit-
able for both rule extraction during training and rule
enumeration during decoding.
At the highest level, our algorithm involves three
steps. In the first step, we label each node in the in-
put forest by a boolean variable indicating whether it
is a site of interest for tree fragment generation. If it
is marked true, it is an admissible node. In the case
of rule extraction, a node is admissible if and only if
it corresponds to a phrase pair according to the un-
derlying word alignment. In the case of decoding,
every node is admissible for the sake of complete-
ness of search. An initial one-node tree fragment is
placed at each admissible node for seeding the tree
fragment generation process. In the second step,
we do cube-pruning style bottom-up combinations
to enumerate a pruned list of tree fragments at each
tree node. In the third step, we extract or enumerate-
and-match tree-to-string rules for the tree fragments
at the admissible nodes.
2.1 A Cube-pruning-inspired Algorithm for
Tree Fragment Composition
Galley et al (2004) defined minimal tree-to-string
rules. Galley et al (2006) showed that tree-to-string
rules made by composing smaller ones are impor-
tant to translation. It can be understood by the anal-
ogy of going from word-based models to phrase-
based models. We relate composed rule extraction
to cube-pruning (Chiang, 2007). In cube-pruning,
the process is to keep track of the k-best sorted lan-
guage model states at each node and combine them
bottom-up with the help of a priority queue. We
can imagine substituting k-best LM states with k
composed rules at each node and composing them
bottom-up. We can also borrow the cube pruning
trick to compose multiple lists of rules using a pri-
ority queue to lazily explore the space of combina-
tions starting from the top-most element in the cube
formed by the lists.
We need to define a ranking function for com-
posed rules. To simulate the breadth-first expansion
heuristics of Galley et al (2006), we define the fig-
ure of merit of a tree-to-string rule as a tuple m =
(h, s, t), where h is the height of a tree fragment,
s is the number of frontier nodes, i.e., bottom-level
nodes including both terminals and non-terminals,
and t is the number of terminals in the set of frontier
nodes. We define an additive operator +:
m1 + m2
= ( max{h1, h2} + 1, s1 + s2, t1 + t2 )
and a min operator based on the order <:
m1 < m2 ??
?
?
?
h1 < h2 ?
h1 = h2 ? s1 < s2 ?
h1 = h2 ? s1 = s2 ? t1 < t2
The + operator corresponds to rule compositions.
The < operator corresponds to ranking rules by their
sizes. A concrete example is shown in Figure 2,
in which case the monotonicity property of (+, <)
holds: if ma < mb, ma +mc < mb +mc. However,
this is not true in general for the operators in our def-
inition, which implies that our algorithm is indeed
like cube-pruning: an approximate k-shortest-path
algorithm.
3 Source Tree Binarization
The motivation of tree binarization is to factorize
large and rare structures into smaller but frequent
ones to improve generalization. For example, Penn
Treebank annotations are often flat at the phrase
level. Translation rules involving flat phrases are un-
likely to generalize. If long sequences are binarized,
837
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
VBD (1, 1, 0)
VBD
was
(2, 1, 1)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
VP-C (1, 1, 0)
VP-C
VPB PP
(2, 2, 0)
VP-C
VPB PP
P NP-C
(3, 3, 1)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
=
(1, 1, 0) (2, 2, 0) (3, 3, 1)
(1, 1, 0) VP
VBD VP-C
(2, 2, 0) VP
VBD VP-C
VPB PP
(3, 3, 0) VP
VBD VP-C
VPB PP
P NP-C
(4, 4, 1)
(2, 1, 1) VP
VBD
was
VP-C
(3, 2, 1) VP
VBD
was
VP-C
VPB PP
(3, 3, 1) VP
VBD
was
VP-C
VPB PP
P NP-C
(4, 4, 2)
Figure 2: Tree-to-string rule composition as cube-pruning. The left shows two lists of composed rules sorted by their
geometric measures (height, # frontiers,# frontier terminals), under the gluing rule of VP ? VBD VP?C.
The right part shows a cube view of the combination space. We explore the space from the top-left corner to the
neighbors.
the commonality of subsequences can be discov-
ered. For example, the simplest binarization meth-
ods left-to-right, right-to-left, and head-out explore
sharing of prefixes or suffixes. Among exponentially
many binarization choices, these algorithms pick a
single bracketing structure for a sequence of sibling
nodes. To explore all possible binarizations, we use
a CYK algorithm to produce a packed forest of bi-
nary trees for a given sibling sequence.
With CYK binarization, we can explore any span
that is nested within the original tree structure, but
still miss all cross-bracket spans. For example,
translating from English to Chinese, The phrase
?There is? should often be translated into one verb
in Chinese. In a correct English parse tree, however,
the subject-verb boundary is between ?There? and
?is?. As a result, tree-to-string translation based on
constituent phrases misses the good translation rule.
The CYK-n binarization algorithm shown in Al-
gorithm 1 is a parameterization of the basic CYK
binarization algorithm we just outlined. The idea is
that binarization can go beyond the scope of parent
nodes to more distant ancestors. The CYK-n algo-
rithm first annotates each node with its n nearest
ancestors in the source tree, then generates a bina-
rization forest that allows combining any two nodes
with common ancestors. The ancestor chain labeled
at each node licenses the node to only combine with
nodes having common ancestors in the past n gener-
ations.
The algorithm creates new tree nodes on the fly.
New tree nodes need to have their own states in-
dicated by a node label representing what is cov-
ered internally by the node and an ancestor chain
representing which nodes the node attaches to ex-
ternally. Line 22 and Line 23 of Algorithm 1 up-
date the label and ancestor annotations of new tree
nodes. Using the parsing semiring notations (Good-
man, 1999), the ancestor computation can be sum-
marized by the (?,?) pair. ? produces the ances-
tor chain of a hyper-edge. ? produces the ancestor
chain of a hyper-node. The node label computation
can be summarized by the (concatenate, min) pair.
concatenate produces a concatenation of node la-
bels. min yields the label with the shortest length.
A tree-sequence (Liu et al, 2007) is a sequence of
sub-trees covering adjacent spans. It can be proved
that the final label of each new node in the forest
corresponds to the tree sequence which has the min-
imum length among all sequences covered by the
node span. The ancestor chain of a new node is the
common ancestors of the nodes in its minimum tree
sequence.
For clarity, we do full CYK loops over all O(|w|2)
spans and O(|w|3) potential hyper-edges, where |w|
is the length of a source string. In reality, only de-
scendants under a shared ancestor can combine. If
we assume trees have a bounded branching factor
b, the number of descendants after n generations is
still bounded by a constant c = bn. The algorithm is
O(c3 ? |w|), which is still linear to the size of input
sentence when the parameter n is a constant.
838
VP
VBD+VBN
VBD
was
VBN
PP
P
by
NP-C
VP
VBD
was
VP-C
VBN+P
VBN P
by
NP-C
(a) (b)
VP
VBD+VBN+P
VBD+VBN
VBD
was
VBN
P
by
NP-C
VP
VBD+VBN+P
VBD
was
VBN+P
VBN P
by
NP-C
(c) (d)
1 2 3 4
0 VBD VBD+VBN VBD+VBN+P VP
1 VBN VBN+P VP-C
2 P PP
3 NP-C
Figure 3: Alternative binary parses created for the origi-
nal tree fragment in Figure 1 through CYK-2 binarization
(a and b) and CYK-3 binarization (c and d). In the chart
representation at the bottom, cells with labels containing
the concatenation symbol + hold nodes created through
binarization.
Figure 3 shows some examples of alternative trees
generated by the CYK-n algorithm. In this example,
standard CYK binarization will not create any new
trees since the input is already binary. The CYK-2
and CYK-3 algorithms discover new trees with an
increasing degree of freedom.
4 Synchronous Binarization for
Forest-to-string Decoding
In this section, we deal with binarization of transla-
tion forests, also known as translation hypergraphs
(Mi et al, 2008). A translation forest is a packed
forest representation of all synchronous derivations
composed of tree-to-string rules that match the
source forest. Tree-to-string decoding algorithms
work on a translation forest, rather than a source for-
est. A binary source forest does not necessarily al-
ways result in a binary translation forest. In the tree-
to-string rule in Figure 4, the source tree is already
ADJP
RB+JJ
x0:RB JJ
responsible
PP
IN
for
NP-C
NPB
DT
the
x1:NN
x2:PP
? x0
fuze
?? x2
de
? x1
ADJP
RB+JJ
x0:RB JJ
responsible
x1:PP
? x0
fuze
?? x1
PP
IN
for
NP-C
NPB
DT
the
x0:NN
x1:PP
? x1
de
? x0
Figure 4: Synchronous binarization for a tree-to-string
rule. The top rule can be binarized into two smaller rules.
binary with the help of source tree binarization, but
the translation rule involves three variables in the set
of frontier nodes. If we apply synchronous binariza-
tion (Zhang et al, 2006), we can factorize it into
two smaller translation rules each having two vari-
ables. Obviously, the second rule, which is a com-
mon pattern, is likely to be shared by many transla-
tion rules in the derivation forest. When beams are
fixed, search goes deeper in a factorized translation
forest.
The challenge of synchronous binarization for a
forest-to-string system is that we need to first match
large tree fragments in the input forest as the first
step of decoding. Our solution is to do the matching
using the original rules and then run synchronous
binarization to break matching rules down to factor
rules which can be shared in the derivation forest.
This is different from the offline binarization scheme
described in (Zhang et al, 2006), although the core
algorithm stays the same.
5 Experiments
We ran experiments on public data sets for English
to Chinese, Czech, French, German, and Spanish
839
Algorithm 1 The CYK-n Binarization Algorithm
1: function CYKBINARIZER(T,n)
2: for each tree node ? T in bottom-up topological order do
3: Make a copy of node in the forest output F
4: Ancestors[node] = the nearest n ancestors of node
5: Label [node] = the label of node in T
6: L? the length of the yield of T
7: for k = 2...L do
8: for i = 0, ..., L? k do
9: for j = i + 1, ..., i + k ? 1 do
10: lnode ? Node[i, j]; rnode ? Node[j, i + k]
11: if Ancestors[lnode] ? Ancestors[rnode] 6= ? then
12: pnode ? GETNODE(i, i + k)
13: ADDEDGE(pnode, lnode, rnode)
return F
14: function GETNODE(begin, end)
15: if Node[begin, end] /? F then
16: Create a new node for the span (begin, end)
17: Ancestors[node] = ?
18: Label [node] = the sequence of terminals in the span (begin, end) in T
19:
return Node[begin, end]
20: function ADDEDGE(pnode, lnode, rnode)
21: Add a hyper-edge from lnode and rnode to pnode
22: Ancestors[pnode] = Ancestors[pnode] ? (Ancestors[lnode] ?Ancestors[rnode])
23: Label [pnode] = min{Label[pnode], CONCATENATE(Label[lnode], Label[rnode])}
translation to evaluate our methods.
5.1 Setup
For English-to-Chinese translation, we used all the
allowed training sets in the NIST 2008 constrained
track. For English to the European languages, we
used the training data sets for WMT 2010 (Callison-
Burch et al, 2010). For NIST, we filtered out sen-
tences exceeding 80 words in the parallel texts. For
WMT, the filtering limit is 60. There is no filtering
on the test data set. Table 1 shows the corpus statis-
tics of our bilingual training data sets.
Source Words Target Words
English-Chinese 287M 254M
English-Czech 66M 57M
English-French 857M 996M
English-German 45M 43M
English-Spanish 216M 238M
Table 1: The Sizes of Parallel Texts.
At the word alignment step, we did 6 iterations
of IBM Model-1 and 6 iterations of HMM. For
English-Chinese, we ran 2 iterations of IBM Model-
4 in addition to Model-1 and HMM. The word align-
ments are symmetrized using the ?union? heuris-
tics. Then, the standard phrase extraction heuristics
(Koehn et al, 2003) were applied to extract phrase
pairs with a length limit of 6. We ran the hierar-
chical phrase extraction algorithm with the standard
heuristics of Chiang (2005). The phrase-length limit
is interpreted as the maximum number of symbols
on either the source side or the target side of a given
rule. On the same aligned data sets, we also ran the
tree-to-string rule extraction algorithm described in
Section 2.1 with a limit of 16 rules per tree node.
The default parser in the experiments is a shift-
reduce dependency parser (Nivre and Scholz, 2004).
It achieves 87.8% labelled attachment score and
88.8% unlabeled attachment score on the standard
Penn Treebank test set. We convert dependency
parses to constituent trees by propagating the part-
of-speech tags of the head words to the correspond-
ing phrase structures.
We compare three systems: a phrase-based sys-
tem (Och and Ney, 2004), a hierarchical phrase-
based system (Chiang, 2005), and our forest-to-
string systemwith different binarization schemes. In
the phrase-based decoder, jump width is set to 8. In
the hierarchical decoder, only the glue rule is applied
840
to spans longer than 10. For the forest-to-string sys-
tem, we do not have such length-based reordering
constraints.
We trained two 5-gram language models with
Kneser-Ney smoothing for each of the target lan-
guages. One is trained on the target side of the par-
allel text, the other is on a corpus provided by the
evaluation: the Gigaword corpus for Chinese and
news corpora for the others. Besides standard fea-
tures (Och and Ney, 2004), the phrase-based decoder
also uses a Maximum Entropy phrasal reordering
model (Zens and Ney, 2006). Both the hierarchi-
cal decoder and the forest-to-string decoder only use
the standard features. For feature weight tuning, we
do Minimum Error Rate Training (Och, 2003). To
explore a larger n-best list more efficiently in train-
ing, we adopt the hypergraph-based MERT (Kumar
et al, 2009).
To evaluate the translation results, we use BLEU
(Papineni et al, 2002).
5.2 Translation Results
Table 2 shows the scores of our system with the
best binarization scheme compared to the phrase-
based system and the hierarchical phrase-based sys-
tem. Our system is consistently better than the other
two systems in all data sets. On the English-Chinese
data set, the improvement over the phrase-based sys-
tem is 1.3 BLEU points, and 0.8 over the hierarchi-
cal phrase-based system. In the tasks of translat-
ing to European languages, the improvements over
the phrase-based baseline are in the range of 0.5 to
1.0 BLEU points, and 0.3 to 0.5 over the hierar-
chical phrase-based system. All improvements ex-
cept the bf2s and hier difference in English-Czech
are significant with confidence level above 99% us-
ing the bootstrap method (Koehn, 2004). To demon-
strate the strength of our systems including the two
baseline systems, we also show the reported best re-
sults on these data sets from the 2010 WMT work-
shop. Our forest-to-string system (bf2s) outperforms
or ties with the best ones in three out of four lan-
guage pairs.
5.3 Different Binarization Methods
The translation results for the bf2s system in Ta-
ble 2 are based on the cyk binarization algorithm
with bracket violation degree 2. In this section, we
BLEU
dev test
English-Chinese pb 29.7 39.4
hier 31.7 38.9
bf2s 31.9 40.7??
English-Czech wmt best - 15.4
pb 14.3 15.5
hier 14.7 16.0
bf2s 14.8 16.3?
English-French wmt best - 27.6
pb 24.1 26.1
hier 23.9 26.1
bf2s 24.5 26.6??
English-German wmt best - 16.3
pb 14.5 15.5
hier 14.9 15.9
bf2s 15.2 16.3??
English-Spanish wmt best - 28.4
pb 24.1 27.9
hier 24.2 28.4
bf2s 24.9 28.9??
Table 2: Translation results comparing bf2s, the
binarized-forest-to-string system, pb, the phrase-based
system, and hier, the hierarchical phrase-based system.
For comparison, the best scores from WMT 2010 are also
shown. ?? indicates the result is significantly better than
both pb and hier. ? indicates the result is significantly
better than pb only.
vary the degree to generate forests that are incremen-
tally augmented from a single tree. Table 3 shows
the scores of different tree binarization methods for
the English-Chinese task.
It is clear from reading the table that cyk-2 is the
optimal binarization parameter. We have verified
this is true for other language pairs on non-standard
data sets. We can explain it from two angles. At
degree 2, we allow phrases crossing at most one
bracket in the original tree. If the parser is reason-
ably good, crossing just one bracket is likely to cover
most interesting phrases that can be translation units.
From another point of view, enlarging the forests
entails more parameters in the resulting translation
model, making over-fitting likely to happen.
5.4 Binarizer or Parser?
A natural question is how the binarizer-generated
forests compare with parser-generated forests in
translation. To answer this question, we need a
841
BLEU
rules dev test
no binarization 378M 28.0 36.3
head-out 408M 30.0 38.2
cyk-1 527M 31.6 40.5
cyk-2 803M 31.9 40.7
cyk-3 1053M 32.0 40.6
cyk-? 1441M 32.0 40.3
Table 3: Comparing different source tree binarization
schemes for English-Chinese translation, showing both
BLEU scores and model sizes. The rule counts include
normal phrases which are used at the leaf level during
decoding.
parser that can generate a packed forest. Our fast
deterministic dependency parser does not generate
a packed forest. Instead, we use a CRF constituent
parser (Finkel et al, 2008) with state-of-the-art ac-
curacy. On the standard Penn Treebank test set, it
achieves an F-score of 89.5%. It uses a CYK algo-
rithm to do full dynamic programming inference, so
is much slower. We modified the parser to do hyper-
edge pruning based on posterior probabilities. The
parser preprocesses the Penn Treebank training data
through binarization. So the packed forest it pro-
duces is also a binarized forest. We compare two
systems: one is using the cyk-2 binarizer to generate
forests; the other is using the CRF parser with prun-
ing threshold e?p, where p = 2 to generate forests.1
Although the parser outputs binary trees, we found
cross-bracket cyk-2 binarization is still helpful.
BLEU
dev test
cyk-2 14.9 16.0
parser 14.7 15.7
Table 4: Binarized forests versus parser-generated forests
for forest-to-string English-German translation.
Table 4 shows the comparison of binarization for-
est and parser forest on English-German translation.
The results show that cyk-2 forest performs slightly
1All hyper-edges with negative log posterior probability
larger than p are pruned. In Mi and Huang (2008), the thresh-
old is p = 10. The difference is that they do the forest pruning
on a forest generated by a k-best algorithm, while we do the
forest-pruning on the full CYK chart. As a result, we need more
aggressive pruning to control forest size.
better than the parser forest. We have not done full
exploration of forest pruning parameters to fine-tune
the parser-forest. The speed of the constituent parser
is the efficiency bottleneck. This actually demon-
strates the advantage of the binarizer plus forest-to-
string scheme. It is flexible, and works with any
parser that generates projective parses. It does not
require hand-tuning of forest pruning parameters for
training.
5.5 Synchronous Binarization
In this section, we demonstrate the effect of syn-
chronous binarization for both tree-to-string and
forest-to-string translation. The experiments are on
the English-Chinese data set. The baseline systems
use k-way cube pruning, where k is the branching
factor, i.e., the maximum number of nonterminals on
the right-hand side of any synchronous translation
rule in an input grammar. The competing system
does online synchronous binarization as described in
Section 4 to transform the grammar intersected with
the input sentence to the minimum branching factor
k? (k? < k), and then applies k?-way cube pruning.
Typically, k? is 2.
BLEU
dev test
head-out cube pruning 29.2 37.0
+ synch. binarization 30.0 38.2
cyk-2 cube pruning 31.7 40.5
+ synch. binarization 31.9 40.7
Table 5: The effect of synchronous binarization for tree-
to-string and forest-to-string systems, on the English-
Chinese task.
Table 5 shows that synchronous binarization does
help reduce search errors and find better translations
consistently in all settings.
6 Related Work
The idea of concatenating adjacent syntactic cate-
gories has been explored in various syntax-based
models. Zollmann and Venugopal (2006) aug-
mented hierarchial phrase based systems with joint
syntactic categories. Liu et al (2007) proposed tree-
sequence-to-string translation rules but did not pro-
vide a good solution to place joint subtrees into con-
nection with the rest of the tree structure. Zhang et
842
al. (2009) is the closest to our work. But their goal
was to augment a k-best forest. They did not bina-
rize the tree sequences. They also did not put con-
straint on the tree-sequence nodes according to how
many brackets are crossed.
Wang et al (2007) used target tree binarization to
improve rule extraction for their string-to-tree sys-
tem. Their binarization forest is equivalent to our
cyk-1 forest. In contrast to theirs, our binarization
scheme affects decoding directly because we match
tree-to-string rules on a binarized forest.
Different methods of translation rule binarization
have been discussed in Huang (2007). Their argu-
ment is that for tree-to-string decoding target side
binarization is simpler than synchronous binariza-
tion and works well because creating discontinous
source spans does not explode the state space. The
forest-to-string senario is more similar to string-to-
tree decoding in which state-sharing is important.
Our experiments show that synchronous binariza-
tion helps significantly in the forest-to-string case.
7 Conclusion
We have presented a new approach to tree-to-string
translation. It involves a source tree binarization
step and a standard forest-to-string translation step.
The method renders it unnecessary to have a k-best
parser to generate a packed forest. We have demon-
strated state-of-the-art results using a fast parser and
a simple tree binarizer that allows crossing at most
one bracket in each binarized node. We have also
shown that reducing search errors is important for
forest-to-string translation. We adapted the syn-
chronous binarization technqiue to improve search
and have shown significant gains. In addition, we
also presented a new cube-pruning-style algorithm
for rule extraction. In the new algorithm, it is easy to
adjust the figure-of-merit of rules for extraction. In
the future, we plan to improve the learning of trans-
lation rules with binarized forests.
Acknowledgments
We would like to thank the members of the MT team
at Google, especially Ashish Venugopal, Zhifei Li,
John DeNero, and Franz Och, for their help and dis-
cussions. We would also like to thank Daniel Gildea
for his suggestions on improving the paper.
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on statisti-
cal machine translation and metrics for machine trans-
lation. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and Metrics(MATR),
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics. Revised August 2010.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Conference of the Association for
Computational Linguistics (ACL-05), pages 263?270,
Ann Arbor, MI.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 755?763,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, companion volume, pages 205?208, Sap-
poro, Japan.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL-08:
HLT, pages 959?967, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of the 2004 Meeting of the North American
chapter of the Association for Computational Linguis-
tics (NAACL-04), pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the International Conference on Computational
Linguistics/Association for Computational Linguistics
(COLING/ACL-06), pages 961?968, July.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proceedings of the 2004 Meeting of the
North American chapter of the Association for Compu-
tational Linguistics (NAACL-04).
843
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
Conference of the Association for Machine Translation
in the Americas (AMTA), Boston, MA.
Liang Huang. 2007. Binarization, synchronous bina-
rization, and target-side binarization. In Proceedings
of the NAACL/AMTA Workshop on Syntax and Struc-
ture in Statistical Translation (SSST), pages 33?40,
Rochester, NY.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of the
46th Annual Conference of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL-08:HLT), Columbus, OH. ACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Meeting of the North American chap-
ter of the Association for Computational Linguistics
(NAACL-03), Edmonton, Alberta.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In 2004 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 388?395, Barcelona, Spain, July.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 163?171, Sun-
tec, Singapore, August. Association for Computational
Linguistics.
Dekang Lin. 2004. A path-based transfer model for
machine translation. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics
(COLING-04), pages 625?630, Geneva, Switzerland.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the International Conference
on Computational Linguistics/Association for Compu-
tational Linguistics (COLING/ACL-06), Sydney, Aus-
tralia, July.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Pro-
ceedings of the 45th Annual Conference of the Associ-
ation for Computational Linguistics (ACL-07), Prague.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 206?214, Honolulu, Hawaii, Octo-
ber. Association for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of the 46th An-
nual Conference of the Association for Computational
Linguistics: Human Language Technologies (ACL-
08:HLT), pages 192?199.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings of
Coling 2004, pages 64?70, Geneva, Switzerland, Aug
23?Aug 27. COLING.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Conference of the Association for Com-
putational Linguistics (ACL-03).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Conference of the Association for Com-
putational Linguistics (ACL-02).
Arjen Poutsma. 2000. Data-oriented translation. In
Proceedings of the 18th International Conference on
Computational Linguistics (COLING-00).
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual Con-
ference of the Association for Computational Linguis-
tics (ACL-05), pages 271?279, Ann Arbor, Michigan.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Conference of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08:HLT), Columbus, OH.
ACL.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based ma-
chine translation accuracy. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 746?
754, Prague, Czech Republic, June. Association for
Computational Linguistics.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 55?63, New York City, June.
Association for Computational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the 2006 Meeting of the
844
North American chapter of the Association for Compu-
tational Linguistics (NAACL-06), pages 256?263, New
York, NY.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree sequence
alignment-based tree-to-tree translation model. In
Proceedings of ACL-08: HLT, pages 559?567, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and
Chew Lim Tan. 2009. Forest-based tree sequence to
string translation model. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 172?180,
Suntec, Singapore, August. Association for Computa-
tional Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation, pages 138?141, New York City, June. As-
sociation for Computational Linguistics.
845
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 19?24,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
NiuTrans: An Open Source Toolkit for  
Phrase-based and Syntax-based Machine Translation  
 
Tong Xiao? ? , Jingbo Zhu? ? , Hao Zhang?  and Qiang Li?  
?Natural Language Processing Lab, Northeastern University 
?Key Laboratory of Medical Image Computing, Ministry of Education 
{xiaotong,zhujingbo}@mail.neu.edu.cn 
{zhanghao1216,liqiangneu}@gmail.com 
 
 
 
Abstract 
We present a new open source toolkit for 
phrase-based and syntax-based machine 
translation. The toolkit supports several 
state-of-the-art models developed in 
statistical machine translation, including 
the phrase-based model, the hierachical 
phrase-based model, and various syntax-
based models. The key innovation provided 
by the toolkit is that the decoder can work 
with various grammars and offers different 
choices of decoding algrithms, such as 
phrase-based decoding, decoding as 
parsing/tree-parsing and forest-based 
decoding. Moreover, several useful utilities 
were distributed with the toolkit, including 
a discriminative reordering model, a simple 
and fast language model, and an 
implementation of minimum error rate 
training  for weight tuning. 
1 Introduction 
We present NiuTrans, a new open source machine 
translation toolkit, which was developed for 
constructing high quality machine translation 
systems. The NiuTrans toolkit supports most 
statistical machine translation (SMT) paradigms 
developed over the past decade, and allows for 
training and decoding with several state-of-the-art 
models, including: the phrase-based model (Koehn 
et al, 2003), the hierarchical phrase-based model 
(Chiang, 2007), and various syntax-based models 
(Galley et al, 2004; Liu et al, 2006). In particular, 
a unified framework was adopted to decode with 
different models and ease the implementation of 
decoding algorithms. Moreover, some useful 
utilities were distributed with the toolkit, such as: a 
discriminative reordering model, a simple and fast 
language model, and an implementation of 
minimum error rate training that allows for various 
evaluation metrics for tuning the system. In 
addition, the toolkit provides easy-to-use APIs for 
the development of new features. The toolkit has 
been used to build translation systems that have 
placed well at recent MT evaluations, such as the 
NTCIR-9 Chinese-to-English PatentMT task (Goto 
et al, 2011). 
We implemented the toolkit in C++ language, 
with special consideration of extensibility and 
efficiency. C++ enables us to develop efficient 
translation engines which have high running speed 
for both training and decoding stages. This 
property is especially important when the programs 
are used for large scale translation. While the 
development of C++ program is slower than that of 
the similar programs written in other popular 
languages such as Java, the modern compliers 
generally result in C++ programs being 
consistently faster than the Java-based counterparts. 
The toolkit is available under the GNU general 
public license 1 . The website of NiuTrans is   
http://www.nlplab.com/NiuPlan/NiuTrans.html. 
2 Motivation 
As in current approaches to statistical machine 
translation, NiuTrans is based on a log-linear 
                                                          
1 http://www.gnu.org/licenses/gpl-2.0.html 
19
model where a number of features are defined to 
model the translation process. Actually NiuTrans is 
not the first system of this kind. To date, several 
open-source SMT systems (based on either phrase-
based models or syntax-based models) have been 
developed, such as Moses (Koehn et al, 2007), 
Joshua (Li et al, 2009), SAMT (Zollmann and 
Venugopal, 2006), Phrasal (Cer et al, 2010), cdec 
(Dyer et al, 2010), Jane (Vilar et al, 2010) and 
SilkRoad 2 , and offer good references for the 
development of the NiuTrans toolkit. While our 
toolkit includes all necessary components as 
provided within the above systems, we have 
additional goals for this project, as follows: 
z It fully supports most state-of-the-art SMT 
models. Among these are: the phrase-based 
model, the hierarchical phrase-based model, 
and the syntax-based models that explicitly 
use syntactic information on either (both) 
source and (or) target language side(s). 
z It offers a wide choice of decoding 
algorithms. For example, the toolkit has 
several useful decoding options, including: 
standard phrase-based decoding, decoding 
as parsing, decoding as tree-parsing, and 
forest-based decoding. 
z It is easy-to-use and fast. A new system can 
be built using only a few commands. To 
control the system, users only need to 
modify a configuration file. In addition to 
the special attention to usability, the 
running speed of the system is also 
improved in several ways. For example, we 
used several pruning and multithreading 
techniques to speed-up the system. 
3 Toolkit 
The toolkit serves as an end-to-end platform for 
training and evaluating statistical machine 
translation models. To build new translation 
systems, all you need is a collection of word-
aligned sentences 3 , and a set of additional 
sentences with one or more reference translations 
for weight tuning and test. Once the data is 
prepared, the MT system can be created using a 
                                                          
2 http://www.nlp.org.cn/project/project.php?proj_id=14 
3 To obtain word-to-word alignments, several easy-to-use 
toolkits are available, such as GIZA++ and Berkeley Aligner. 
sequence of commands. Given a number of 
sentence-pairs and the word alignments between 
them, the toolkit first extracts a phrase table and 
two reordering models for the phrase-based system, 
or a Synchronous Context-free/Tree-substitution 
Grammar (SCFG/STSG) for the hierarchical 
phrase-based and syntax-based systems. Then, an 
n-gram language model is built on the target-
language corpus. Finally, the resulting models are 
incorporated into the decoder which can 
automatically tune feature weights on the 
development set using minimum error rate training 
(Och, 2003) and translate new sentences with the 
optimized weights. 
In the following, we will give a brief review of 
the above components and the main features 
provided by the toolkit. 
3.1 Phrase Extraction and Reordering Model 
We use a standard way to implement the phrase 
extraction module for the phrase-based model. 
That is, we extract all phrase-pairs that are 
consistent with word alignments. Five features are 
associated with each phrase-pair. They are two 
phrase translation probabilities, two lexical weights, 
and a feature of phrase penalty. We follow the 
method proposed in (Koehn et al, 2003) to 
estimate the values of these features. 
Unlike previous systems that adopt only one 
reordering model, our toolkit supports two 
different reordering models which are trained 
independently but jointly used during decoding. 
z The first of these is a discriminative 
reordering model. This model is based on 
the standard framework of maximum 
entropy. Thus the reordering problem is 
modeled as a classification problem, and 
the reordering probability can be efficiently 
computed using a (log-)linear combination 
of features. In our implementation, we use 
all boundary words as features which are 
similar to those used in (Xiong et al, 2006). 
z The second model is the MSD reordering 
model4 which has been successfully used in 
the Moses system. Unlike Moses, our 
toolkit supports both the word-based and 
phrase-based methods for estimating the 
                                                          
4 Term MSD refers to the three orientations (reordering types), 
including Monotone (M), Swap (S), and Discontinuous (D). 
20
probabilities of the three orientations 
(Galley and Manning, 2008). 
3.2 Translation Rule Extraction 
For the hierarchical phrase-based model, we follow 
the general framework of SCFG where a grammar 
rule has three parts ? a source-side, a target-side 
and alignments between source and target non-
terminals. To learn SCFG rules from word-aligned 
sentences, we choose the algorithm proposed in 
(Chiang, 2007) and estimate the associated feature 
values as in the phrase-based system. 
For the syntax-based models, all non-terminals 
in translation rules are annotated with syntactic 
labels. We use the GHKM algorithm to extract 
(minimal) translation rules from bilingual 
sentences with parse trees on source-language side 
and/or target-language side5 . Also, two or more 
minimal rules can be composed together to obtain 
larger rules and involve more contextual 
information. For unaligned words, we attach them 
to all nearby rules, instead of using the most likely 
attachment as in (Galley et al, 2006). 
3.3 N-gram Language Modeling 
The toolkit includes a simple but effective n-gram 
language model (LM). The LM builder is basically 
a ?sorted? trie structure (Pauls and Klein, 2011), 
where a map is developed to implement an array of 
key/value pairs, guaranteeing that the keys can be 
accessed in sorted order. To reduce the size of 
resulting language model, low-frequency n-grams 
are filtered out by some thresholds. Moreover, an 
n-gram cache is implemented to speed up n-gram 
probability requests for decoding. 
3.4 Weight Tuning 
We implement the weight tuning component 
according to the minimum error rate training 
(MERT) method (Och, 2003). As MERT suffers 
from local optimums, we added a small program 
into the MERT system to let it jump out from the 
coverage area. When MERT converges to a (local) 
optimum, our program automatically conducts the 
MERT run again from a random starting point near 
the newly-obtained optimal point. This procedure 
                                                          
5 For tree-to-tree models, we use a natural extension of the 
GHKM algorithm which defines admissible nodes on tree-
pairs and obtains tree-to-tree rules on all pairs of source and 
target tree-fragments. 
is repeated for several times until no better weights 
(i.e., weights with a higher BLEU score) are found. 
In this way, our program can introduce some 
randomness into weight training. Hence users do 
not need to repeat MERT for obtaining stable and 
optimized weights using different starting points.  
3.5 Decoding 
Chart-parsing is employed to decode sentences in 
development and test sets. Given a source sentence, 
the decoder generates 1-best or k-best translations 
in a bottom-up fashion using a CKY-style parsing 
algorithm. The basic data structure used in the 
decoder is a chart, where an array of cells is 
organized in topological order. Each cell maintains 
a list of hypotheses (or items). The decoding 
process starts with the minimal cells, and proceeds 
by repeatedly applying translation rules or 
composing items in adjunct cells to obtain new 
items. Once a new item is created, the associated 
scores are computed (with an integrated n-gram 
language model). Then, the item is added into the 
list of the corresponding cell. This procedure stops 
when we reach the final state (i.e., the cell 
associates with the entire source span). 
The decoder can work with all (hierarchical) 
phrase-based and syntax-based models. In 
particular, our toolkit provides the following 
decoding modes. 
z Phrase-based decoding. To fit the phrase-
based model into the CKY paring 
framework, we restrict the phrase-based 
decoding with the ITG constraint (Wu, 
1996). In this way, each pair of items in 
adjunct cells can be composed in either 
monotone order or inverted order. Hence 
the decoding can be trivially implemented 
by a three-loop structure as in standard 
CKY parsing. This algorithm is actually the 
same as that used in parsing with 
bracketing transduction grammars. 
z Decoding as parsing (or string-based 
decoding). This mode is designed for 
decoding with SCFGs/STSGs which are 
used in the hierarchical phrase-based and 
syntax-based systems. In the general 
framework of synchronous grammars and 
tree transducers, decoding can be regarded 
as a parsing problem. Therefore, the above 
chart-based decoder is directly applicable to 
21
the hierarchical phrase-based and syntax-
based models. For efficient integration of n-
gram language model into decoding, rules 
containing more than two variables are 
binarized into binary rules. In addition to 
the rules learned from bilingual data, glue 
rules are employed to glue the translations 
of a sequence of chunks.  
z Decoding as tree-parsing (or tree-based 
decoding). If the parse tree of source 
sentence is provided, decoding (for tree-to-
string and tree-to-tree models) can also be 
cast as a tree-parsing problem (Eisner, 
2003). In tree-parsing, translation rules are 
first mapped onto the nodes of input parse 
tree. This results in a translation tree/forest 
(or a hypergraph) where each edge 
represents a rule application. Then 
decoding can proceed on the hypergraph as 
usual. That is, we visit in bottom-up order 
each node in the parse tree, and calculate 
the model score for each edge rooting at the 
node. The final output is the 1-best/k-best 
translations maintained by the root node of 
the parse tree. Since tree-parsing restricts 
its search space to the derivations that 
exactly match with the input parse tree, it in 
general has a much higher decoding speed 
than a normal parsing procedure. But it in 
turn results in lower translation quality due 
to more search errors. 
z Forest-based decoding. Forest-based 
decoding (Mi et al, 2008) is a natural 
extension of tree-based decoding. In 
principle, forest is a data structure that can 
encode exponential number of trees 
efficiently. This structure has been proved 
to be helpful in reducing the effects caused 
by parser errors. Since our internal 
representation is already in a hypergraph 
structure, it is easy to extend the decoder to 
handle the input forest, with little 
modification of the code. 
4 Other Features 
In addition to the basic components described 
above, several additional features are introduced to 
ease the use of the toolkit. 
4.1 Multithreading 
The decoder supports multithreading to make full 
advantage of the modern computers where more 
than one CPUs (or cores) are provided. In general, 
the decoding speed can be improved when multiple 
threads are involved. However, modern MT 
decoders do not run faster when too many threads 
are used (Cer et al, 2010). 
4.2 Pruning 
To make decoding computational feasible, beam 
pruning is used to aggressively prune the search 
space. In our implementation, we maintain a beam 
for each cell. Once all the items of the cell are 
proved, only the top-k best items according to 
model score are kept and the rest are discarded. 
Also, we re-implemented the cube pruning method 
described in (Chiang, 2007) to further speed-up the 
system. 
In addition, we develop another method that 
prunes the search space using punctuations. The 
idea is to divide the input sentence into a sequence 
of segments according to punctuations. Then, each 
segment is translated individually. The MT outputs 
are finally generated by composing the translations 
of those segments. 
4.3 APIs for Feature Engineering 
To ease the implementation and test of new 
features, the toolkit offers APIs for experimenting 
with the features developed by users. For example, 
users can develop new features that are associated 
with each phrase-pair. The system can 
automatically recognize them and incorporate them 
into decoding. Also, more complex features can be 
activated during decoding. When an item is created 
during decoding, new features can be introduced 
into an internal object which returns feature values 
for computing the model score. 
5 Experiments 
5.1 Experimental Setup 
We evaluated our systems on NIST Chinese-
English MT tasks. Our training corpus consists of 
1.9M bilingual sentences. We used GIZA++ and 
the ?grow-diag-final-and? heuristics to generate 
word alignment for the bilingual data. The parse 
trees on both the Chinese and English sides were 
22
BLEU4[%] Entry 
 Dev  Test 
Moses: phrase  36.51  34.93
Moses: hierarchical phrase  36.65  34.79
 phrase  36.99  35.29
 hierarchical phrase  37.41  35.35
 parsing  36.48  34.71
 tree-parsing  35.54  33.99
 t2s 
 forest-based  36.14  34.25
 parsing  35.99  34.01
 tree-parsing  35.04  33.21
 t2t 
 forest-based  35.56  33.45
   
   
   
 N
iu
Tr
an
s 
 s2t  parsing  37.63  35.65
Table 1: BLEU scores of various systems. t2s, t2t, 
and s2t represent the tree-to-string, tree-to-tree, and 
string-to-tree systems, respectively. 
 
generated using the Berkeley Parser, which were 
then binarized in a head-out fashion 6. A 5-gram 
language model was trained on the Xinhua portion 
of the Gigaword corpus in addition to the English 
part of the LDC bilingual training data. We used 
the NIST 2003 MT evaluation set as our 
development set (919 sentences) and the NIST 
2005 MT evaluation set as our test set (1,082 
sentences). The translation quality was evaluated 
with the case-insensitive IBM-version BLEU4. 
For the phrase-based system, phrases are of at 
most 7 words on either source or target-side. For 
the hierarchical phrase-based system, all SCFG 
rules have at most two variables. For the syntax-
based systems, minimal rules were extracted from 
the binarized trees on both (either) language-
side(s). Larger rules were then generated by 
composing two or three minimal rules. By default, 
all these systems used a beam of size 30 for 
decoding. 
5.2 Evaluation of Translations 
Table 1 shows the BLEU scores of different MT 
systems built using our toolkit. For comparison, 
the result of the Moses system is also reported. We 
see, first of all, that our phrase-based and 
hierarchical phrase-based systems achieve 
competitive performance, even outperforms the 
Moses system over 0.3 BLEU points in some cases. 
Also, the syntax-based systems obtain very  
                                                          
6 The parse trees follow the nested bracketing format, as 
defined in the Penn Treebank. Also, the NiuTrans package 
includes a tool for tree binarization. 
BLEU4[%] Entry 
Dev Test 
Speed
(sent/sec)
Moses: phrase  36.69  34.99    0.11
+ cube pruning   36.51  34.93    0.47
NiuTrans: phrase  37.14  35.47    0.14
+ cube pruning  36.98  35.39    0.60
+ cube & punct pruning  36.99  35.29    3.71
+ all pruning & 8 threads  36.99  35.29  21.89
+ all pruning & 16 threads  36.99  35.29  22.36
Table 2: Effects of pruning and multithreading 
techniques. 
 
promising results. For example, the string-to-tree 
system significantly outperforms the phrase-based 
and hierarchical phrase-based counterparts. In 
addition, Table 1 gives a test of different decoding 
methods (for syntax-based systems). We see that 
the parsing-based method achieves the best BLEU 
score. On the other hand, as expected, it runs 
slowest due to its large search space. For example, 
it is 5-8 times slower than the tree-parsing-based 
method in our experiments. The forest-based 
decoding further improves the BLEU scores on top 
of tree-parsing. In most cases, it obtains a +0.6 
BLEU improvement but is 2-3 times slower than 
the tree-parsing-based method. 
5.3 System Speed-up 
We also study the effectiveness of pruning and 
multithreading techniques. Table 2 shows that all 
the pruning methods implemented in the toolkit is 
helpful in speeding up the (phrase-based) system, 
while does not result in significant decrease in 
BLEU score. On top of a straightforward baseline 
(only beam pruning is used), cube pruning and 
pruning with punctuations give a speed 
improvement of 25 times together7. Moreover, the 
decoding process can be further accelerated by 
using multithreading technique. However, more 
than 8 threads do not help in our experiments. 
6 Conclusion and Future Work 
We have presented a new open-source toolkit for 
phrase-based and syntax-based machine translation. 
It is implemented in C++ and runs fast. Moreover, 
it supports several state-of-the-art models ranging 
from phrase-based models to syntax-based models, 
                                                          
7 The translation speed is tested on Intel Core Due 2 E8500 
processors running at 3.16 GHz. 
23
and provides a wide choice of decoding methods. 
The experimental results on NIST MT tasks show 
that the MT systems built with our toolkit achieve 
state-of-the-art translation performance. 
The next version of NiuTrans will support 
ARPA-format LMs, MIRA for weight tuning and a 
beam-stack decoder which removes the ITG 
constraint for phrase decoding. In addition, a 
Hadoop-based MapReduce-parallelized version is 
underway and will be released in near future.  
Acknowledgments  
This research was supported in part by the National 
Science Foundation of China (61073140), the 
Specialized Research Fund for the Doctoral 
Program of Higher Education (20100042110031) 
and the Fundamental Research Funds for the 
Central Universities in China. 
References  
Daniel Cer, Michel Galley, Daniel Jurafsky and 
Christopher D. Manning. 2010. Phrasal: A Toolkit 
for Statistical Machine Translation with Facilities for 
Extraction and Incorporation of Arbitrary Model 
Features. In Proc. of HLT/NAACL 2010 
demonstration Session, pages 9-12. 
David Chiang. 2007. Hierarchical phrase-based 
translation. Computational Linguistics, 33(2):201?
228. 
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan 
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, 
Vladimir Eidelman, Philip Resnik. 2010. cdec: A 
Decoder, Alignment, and Learning Framework for 
Finite-State and Context-Free Translation Models. In 
Proc. of ACL 2010 System Demonstrations, pages 7-
12. 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for machine translation. In Proc. of ACL 
2003, pages 205-208. 
Michel Galley, Mark Hopkins, Kevin Knight and Daniel 
Marcu. 2004. What's in a translation rule? In Proc. of 
HLT-NAACL 2004, pages 273-280. 
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel 
Marcu, Steve DeNeefe, Wei Wang and Ignacio 
Thayer. 2006. Scalable inferences and training of 
context-rich syntax translation models. In Proc. of 
COLING/ACL 2006, pages 961-968. 
Michel Galley and Christopher D. Manning. 2008. A 
Simple and Effective Hierarchical Phrase Reordering 
Model. In Proc. of EMNLP2008, pages 848-856. 
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita and 
Benjamin K. Tsou. 2011. Overview of the Patent 
Machine Translation Task at the NTCIR-9 Workshop. 
In Proc. of NTCIR-9 Workshop Meeting, pages 559-
578. 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical phrase-based translation. In Proc. of 
HLT/NAACL 2003, pages 127-133. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. In 
Proc. of ACL 2007, pages 177?180. 
Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev 
Khudanpur, Lane Schwartz, Wren Thornton, 
Jonathan Weese, and Omar Zaidan. 2009. Joshua: An 
Open Source Toolkit for Parsing-Based Machine 
Translation. In Proc. of the Workshop on Statistical 
Machine Translation, pages 135?139. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. In Proc. of ACL 2006, pages 609-616. 
Haitao Mi, Liang Huang and Qun Liu. 2008. Forest-
Based Translation. In Proc. of ACL 2008, pages 192-
199. 
Franz Josef Och. 2003. Minimum error rate training in 
statistical machine translation. In Proc. of ACL 2003, 
pages 160-167. 
Adam Pauls and Dan Klein. 2011. Faster and Smaller 
N-Gram Language Models. In Proc. of ACL 2011, 
pages 258?267. 
David Vilar, Daniel Stein, Matthias Huck and Hermann 
Ney. 2010. Jane: Open Source Hierarchical 
Translation, Extended with Reordering and Lexicon 
Models. In Proc. of the Joint 5th Workshop on 
Statistical Machine Translation and MetricsMATR, 
pages 262-270. 
Dekai Wu. 1996. A polynomial-time algorithm for 
statistical machine translation. In Proc. of ACL1996, 
pages 152?158. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. 
Maximum Entropy Based Phrase Reordering Model 
for Statistical Machine Translation. In Proc. of ACL 
2006, pages 521-528. 
Andreas Zollmann and Ashish Venugopal. 2006. Syntax 
Augmented Machine Translation via Chart Parsing. 
In Proc. of HLT/NAACL 2006, pages 138-141. 
24
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92?97,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Universal Dependency Annotation for Multilingual Parsing
Ryan McDonald? Joakim Nivre?? Yvonne Quirmbach-Brundage? Yoav Goldberg??
Dipanjan Das? Kuzman Ganchev? Keith Hall? Slav Petrov? Hao Zhang?
Oscar Ta?ckstro?m?? Claudia Bedini? Nu?ria Bertomeu Castello?? Jungmee Lee?
Google, Inc.? Uppsala University? Appen-Butler-Hill? Bar-Ilan University?
Contact: ryanmcd@google.com
Abstract
We present a new collection of treebanks
with homogeneous syntactic dependency
annotation for six languages: German,
English, Swedish, Spanish, French and
Korean. To show the usefulness of such a
resource, we present a case study of cross-
lingual transfer parsing with more reliable
evaluation than has been possible before.
This ?universal? treebank is made freely
available in order to facilitate research on
multilingual dependency parsing.1
1 Introduction
In recent years, syntactic representations based
on head-modifier dependency relations between
words have attracted a lot of interest (Ku?bler et
al., 2009). Research in dependency parsing ? com-
putational methods to predict such representations
? has increased dramatically, due in large part to
the availability of dependency treebanks in a num-
ber of languages. In particular, the CoNLL shared
tasks on dependency parsing have provided over
twenty data sets in a standardized format (Buch-
holz and Marsi, 2006; Nivre et al, 2007).
While these data sets are standardized in terms
of their formal representation, they are still hetero-
geneous treebanks. That is to say, despite them
all being dependency treebanks, which annotate
each sentence with a dependency tree, they sub-
scribe to different annotation schemes. This can
include superficial differences, such as the renam-
ing of common relations, as well as true diver-
gences concerning the analysis of linguistic con-
structions. Common divergences are found in the
1Downloadable at https://code.google.com/p/uni-dep-tb/.
analysis of coordination, verb groups, subordinate
clauses, and multi-word expressions (Nilsson et
al., 2007; Ku?bler et al, 2009; Zeman et al, 2012).
These data sets can be sufficient if one?s goal
is to build monolingual parsers and evaluate their
quality without reference to other languages, as
in the original CoNLL shared tasks, but there are
many cases where heterogenous treebanks are less
than adequate. First, a homogeneous represen-
tation is critical for multilingual language tech-
nologies that require consistent cross-lingual anal-
ysis for downstream components. Second, consis-
tent syntactic representations are desirable in the
evaluation of unsupervised (Klein and Manning,
2004) or cross-lingual syntactic parsers (Hwa et
al., 2005). In the cross-lingual study of McDonald
et al (2011), where delexicalized parsing models
from a number of source languages were evalu-
ated on a set of target languages, it was observed
that the best target language was frequently not the
closest typologically to the source. In one stun-
ning example, Danish was the worst source lan-
guage when parsing Swedish, solely due to greatly
divergent annotation schemes.
In order to overcome these difficulties, some
cross-lingual studies have resorted to heuristics to
homogenize treebanks (Hwa et al, 2005; Smith
and Eisner, 2009; Ganchev et al, 2009), but we
are only aware of a few systematic attempts to
create homogenous syntactic dependency anno-
tation in multiple languages. In terms of auto-
matic construction, Zeman et al (2012) attempt
to harmonize a large number of dependency tree-
banks by mapping their annotation to a version of
the Prague Dependency Treebank scheme (Hajic?
et al, 2001; Bo?hmova? et al, 2003). Addition-
ally, there have been efforts to manually or semi-
manually construct resources with common syn-
92
tactic analyses across multiple languages using al-
ternate syntactic theories as the basis for the repre-
sentation (Butt et al, 2002; Helmreich et al, 2004;
Hovy et al, 2006; Erjavec, 2012).
In order to facilitate research on multilingual
syntactic analysis, we present a collection of data
sets with uniformly analyzed sentences for six lan-
guages: German, English, French, Korean, Span-
ish and Swedish. This resource is freely avail-
able and we plan to extend it to include more data
and languages. In the context of part-of-speech
tagging, universal representations, such as that of
Petrov et al (2012), have already spurred numer-
ous examples of improved empirical cross-lingual
systems (Zhang et al, 2012; Gelling et al, 2012;
Ta?ckstro?m et al, 2013). We aim to do the same for
syntactic dependencies and present cross-lingual
parsing experiments to highlight some of the bene-
fits of cross-lingually consistent annotation. First,
results largely conform to our expectations of
which target languages should be useful for which
source languages, unlike in the study of McDon-
ald et al (2011). Second, the evaluation scores
in general are significantly higher than previous
cross-lingual studies, suggesting that most of these
studies underestimate true accuracy. Finally, un-
like all previous cross-lingual studies, we can re-
port full labeled accuracies and not just unlabeled
structural accuracies.
2 Towards A Universal Treebank
The Stanford typed dependencies for English
(De Marneffe et al, 2006; de Marneffe and Man-
ning, 2008) serve as the point of departure for our
?universal? dependency representation, together
with the tag set of Petrov et al (2012) as the under-
lying part-of-speech representation. The Stanford
scheme, partly inspired by the LFG framework,
has emerged as a de facto standard for depen-
dency annotation in English and has recently been
adapted to several languages representing different
(and typologically diverse) language groups, such
as Chinese (Sino-Tibetan) (Chang et al, 2009),
Finnish (Finno-Ugric) (Haverinen et al, 2010),
Persian (Indo-Iranian) (Seraji et al, 2012), and
Modern Hebrew (Semitic) (Tsarfaty, 2013). Its
widespread use and proven adaptability makes it a
natural choice for our endeavor, even though ad-
ditional modifications will be needed to capture
the full variety of grammatical structures in the
world?s languages.
Alexandre re?side avec sa famille a` Tinqueux .
NOUN VERB ADP DET NOUN ADP NOUN P
NSUBJ ADPMOD
ADPOBJ
POSS
ADPMOD
ADPOBJ
P
Figure 1: A sample French sentence.
We use the so-called basic dependencies (with
punctuation included), where every dependency
structure is a tree spanning all the input tokens,
because this is the kind of representation that most
available dependency parsers require. A sample
dependency tree from the French data set is shown
in Figure 1. We take two approaches to generat-
ing data. The first is traditional manual annotation,
as previously used by Helmreich et al (2004) for
multilingual syntactic treebank construction. The
second, used only for English and Swedish, is to
automatically convert existing treebanks, as in Ze-
man et al (2012).
2.1 Automatic Conversion
Since the Stanford dependencies for English are
taken as the starting point for our universal annota-
tion scheme, we begin by describing the data sets
produced by automatic conversion. For English,
we used the Stanford parser (v1.6.8) (Klein and
Manning, 2003) to convert the Wall Street Jour-
nal section of the Penn Treebank (Marcus et al,
1993) to basic dependency trees, including punc-
tuation and with the copula verb as head in cop-
ula constructions. For Swedish, we developed a
set of deterministic rules for converting the Tal-
banken part of the Swedish Treebank (Nivre and
Megyesi, 2007) to a representation as close as pos-
sible to the Stanford dependencies for English.
This mainly consisted in relabeling dependency
relations and, due to the fine-grained label set used
in the Swedish Treebank (Teleman, 1974), this
could be done with high precision. In addition,
a small number of constructions required struc-
tural conversion, notably coordination, which in
the Swedish Treebank is given a Prague style anal-
ysis (Nilsson et al, 2007). For both English and
Swedish, we mapped the language-specific part-
of-speech tags to universal tags using the map-
pings of Petrov et al (2012).
2.2 Manual Annotation
For the remaining four languages, annotators were
given three resources: 1) the English Stanford
93
guidelines; 2) a set of English sentences with Stan-
ford dependencies and universal tags (as above);
and 3) a large collection of unlabeled sentences
randomly drawn from newswire, weblogs and/or
consumer reviews, automatically tokenized with a
rule-based system. For German, French and Span-
ish, contractions were split, except in the case of
clitics. For Korean, tokenization was more coarse
and included particles within token units. Annota-
tors could correct this automatic tokenization.
The annotators were then tasked with producing
language-specific annotation guidelines with the
expressed goal of keeping the label and construc-
tion set as close as possible to the original English
set, only adding labels for phenomena that do not
exist in English. Making fine-grained label dis-
tinctions was discouraged. Once these guidelines
were fixed, annotators selected roughly an equal
amount of sentences to be annotated from each do-
main in the unlabeled data. As the sentences were
already randomly selected from a larger corpus,
annotators were told to view the sentences in or-
der and to discard a sentence only if it was 1) frag-
mented because of a sentence splitting error; 2) not
from the language of interest; 3) incomprehensible
to a native speaker; or 4) shorter than three words.
The selected sentences were pre-processed using
cross-lingual taggers (Das and Petrov, 2011) and
parsers (McDonald et al, 2011).
The annotators modified the pre-parsed trees us-
ing the TrEd2 tool. At the beginning of the annota-
tion process, double-blind annotation, followed by
manual arbitration and consensus, was used itera-
tively for small batches of data until the guidelines
were finalized. Most of the data was annotated
using single-annotation and full review: one an-
notator annotating the data and another reviewing
it, making changes in close collaboration with the
original annotator. As a final step, all annotated
data was semi-automatically checked for annota-
tion consistency.
2.3 Harmonization
After producing the two converted and four an-
notated data sets, we performed a harmonization
step, where the goal was to maximize consistency
of annotation across languages. In particular, we
wanted to eliminate cases where the same label
was used for different linguistic relations in dif-
ferent languages and, conversely, where one and
2Available at http://ufal.mff.cuni.cz/tred/.
the same relation was annotated with different la-
bels, both of which could happen accidentally be-
cause annotators were allowed to add new labels
for the language they were working on. Moreover,
we wanted to avoid, as far as possible, labels that
were only used in one or two languages.
In order to satisfy these requirements, a number
of language-specific labels were merged into more
general labels. For example, in analogy with the
nn label for (element of a) noun-noun compound,
the annotators of German added aa for compound
adjectives, and the annotators of Korean added vv
for compound verbs. In the harmonization step,
these three labels were merged into a single label
compmod for modifier in compound.
In addition to harmonizing language-specific la-
bels, we also renamed a small number of relations,
where the name would be misleading in the uni-
versal context (although quite appropriate for En-
glish). For example, the label prep (for a mod-
ifier headed by a preposition) was renamed adp-
mod, to make clear the relation to other modifier
labels and to allow postpositions as well as prepo-
sitions.3 We also eliminated a few distinctions in
the original Stanford scheme that were not anno-
tated consistently across languages (e.g., merging
complm with mark, number with num, and purpcl
with advcl).
The final set of labels is listed with explanations
in Table 1. Note that relative to the universal part-
of-speech tagset of Petrov et al (2012) our final
label set is quite rich (40 versus 12). This is due
mainly to the fact that the the former is based on
deterministic mappings from a large set of annota-
tion schemes and therefore reduced to the granu-
larity of the greatest common denominator. Such a
reduction may ultimately be necessary also in the
case of dependency relations, but since most of our
data sets were created through manual annotation,
we could afford to retain a fine-grained analysis,
knowing that it is always possible to map from
finer to coarser distinctions, but not vice versa.4
2.4 Final Data Sets
Table 2 presents the final data statistics. The num-
ber of sentences, tokens and tokens/sentence vary
3Consequently, pobj and pcomp were changed to adpobj
and adpcomp.
4The only two data sets that were created through con-
version in our case were English, for which the Stanford de-
pendencies were originally defined, and Swedish, where the
native annotation happens to have a fine-grained label set.
94
Label Description
acomp adjectival complement
adp adposition
adpcomp complement of adposition
adpmod adpositional modifier
adpobj object of adposition
advcl adverbial clause modifier
advmod adverbial modifier
amod adjectival modifier
appos appositive
attr attribute
aux auxiliary
auxpass passive auxiliary
cc conjunction
ccomp clausal complement
Label Description
compmod compound modifier
conj conjunct
cop copula
csubj clausal subject
csubjpass passive clausal subject
dep generic
det determiner
dobj direct object
expl expletive
infmod infinitival modifier
iobj indirect object
mark marker
mwe multi-word expression
neg negation
Label Description
nmod noun modifier
nsubj nominal subject
nsubjpass passive nominal subject
num numeric modifier
p punctuation
parataxis parataxis
partmod participial modifier
poss possessive
prt verb particle
rcmod relative clause modifier
rel relative
xcomp open clausal complement
Table 1: Harmonized label set based on Stanford dependencies (De Marneffe et al, 2006).
source(s) # sentences # tokens
DE N, R 4,000 59,014
EN PTB? 43,948 1,046,829
SV STB? 6,159 96,319
ES N, B, R 4,015 112,718
FR N, B, R 3,978 90,000
KO N, B 6,194 71,840
Table 2: Data set statistics. ?Automatically con-
verted WSJ section of the PTB. The data release
includes scripts to generate this data, not the data
itself. ?Automatically converted Talbanken sec-
tion of the Swedish Treebank. N=News, B=Blogs,
R=Consumer Reviews.
due to the source and tokenization. For example,
Korean has 50% more sentences than Spanish, but
?40k less tokens due to a more coarse-grained to-
kenization. In addition to the data itself, anno-
tation guidelines and harmonization rules are in-
cluded so that the data can be regenerated.
3 Experiments
One of the motivating factors in creating such a
data set was improved cross-lingual transfer eval-
uation. To test this, we use a cross-lingual transfer
parser similar to that of McDonald et al (2011).
In particular, it is a perceptron-trained shift-reduce
parser with a beam of size 8. We use the features
of Zhang and Nivre (2011), except that all lexical
identities are dropped from the templates during
training and testing, hence inducing a ?delexical-
ized? model that employs only ?universal? proper-
ties from source-side treebanks, such as part-of-
speech tags, labels, head-modifier distance, etc.
We ran a number of experiments, which can be
seen in Table 3. For these experiments we ran-
domly split each data set into training, develop-
ment and testing sets.5 The one exception is En-
glish, where we used the standard splits. Each
row in Table 3 represents a source training lan-
guage and each column a target evaluation lan-
guage. We report both unlabeled attachment score
(UAS) and labeled attachment score (LAS) (Buch-
holz and Marsi, 2006). This is likely the first re-
liable cross-lingual parsing evaluation. In partic-
ular, previous studies could not even report LAS
due to differences in treebank annotations.
We can make several interesting observations.
Most notably, for the Germanic and Romance tar-
get languages, the best source language is from
the same language group. This is in stark contrast
to the results of McDonald et al (2011), who ob-
serve that this is rarely the case with the heteroge-
nous CoNLL treebanks. Among the Germanic
languages, it is interesting to note that Swedish
is the best source language for both German and
English, which makes sense from a typological
point of view, because Swedish is intermediate be-
tween German and English in terms of word or-
der properties. For Romance languages, the cross-
lingual parser is approaching the accuracy of the
supervised setting, confirming that for these lan-
guages much of the divergence is lexical and not
structural, which is not true for the Germanic lan-
guages. Finally, Korean emerges as a very clear
outlier (both as a source and as a target language),
which again is supported by typological consider-
ations as well as by the difference in tokenization.
With respect to evaluation, it is interesting to
compare the absolute numbers to those reported
in McDonald et al (2011) for the languages com-
5These splits are included in the release of the data.
95
Source
Training
Language
Target Test Language
Unlabeled Attachment Score (UAS) Labeled Attachment Score (LAS)
Germanic Romance Germanic Romance
DE EN SV ES FR KO DE EN SV ES FR KO
DE 74.86 55.05 65.89 60.65 62.18 40.59 64.84 47.09 53.57 48.14 49.59 27.73
EN 58.50 83.33 70.56 68.07 70.14 42.37 48.11 78.54 57.04 56.86 58.20 26.65
SV 61.25 61.20 80.01 67.50 67.69 36.95 52.19 49.71 70.90 54.72 54.96 19.64
ES 55.39 58.56 66.84 78.46 75.12 30.25 45.52 47.87 53.09 70.29 63.65 16.54
FR 55.05 59.02 65.05 72.30 81.44 35.79 45.96 47.41 52.25 62.56 73.37 20.84
KO 33.04 32.20 27.62 26.91 29.35 71.22 26.36 21.81 18.12 18.63 19.52 55.85
Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
mon to both studies (DE, EN, SV and ES). In that
study, UAS was in the 38?68% range, as compared
to 55?75% here. For Swedish, we can even mea-
sure the difference exactly, because the test sets
are the same, and we see an increase from 58.3%
to 70.6%. This suggests that most cross-lingual
parsing studies have underestimated accuracies.
4 Conclusion
We have released data sets for six languages with
consistent dependency annotation. After the ini-
tial release, we will continue to annotate data in
more languages as well as investigate further au-
tomatic treebank conversions. This may also lead
to modifications of the annotation scheme, which
should be regarded as preliminary at this point.
Specifically, with more typologically and morpho-
logically diverse languages being added to the col-
lection, it may be advisable to consistently en-
force the principle that content words take func-
tion words as dependents, which is currently vi-
olated in the analysis of adpositional and copula
constructions. This will ensure a consistent analy-
sis of functional elements that in some languages
are not realized as free words or are not obliga-
tory, such as adpositions which are often absent
due to case inflections in languages like Finnish. It
will also allow the inclusion of language-specific
functional or morphological markers (case mark-
ers, topic markers, classifiers, etc.) at the leaves of
the tree, where they can easily be ignored in appli-
cations that require a uniform cross-lingual repre-
sentation. Finally, this data is available on an open
source repository in the hope that the community
will commit new data and make corrections to ex-
isting annotations.
Acknowledgments
Many people played critical roles in the pro-
cess of creating the resource. At Google, Fer-
nando Pereira, Alfred Spector, Kannan Pashu-
pathy, Michael Riley and Corinna Cortes sup-
ported the project and made sure it had the re-
quired resources. Jennifer Bahk and Dave Orr
helped coordinate the necessary contracts. Andrea
Held, Supreet Chinnan, Elizabeth Hewitt, Tu Tsao
and Leigha Weinberg made the release process
smooth. Michael Ringgaard, Andy Golding, Terry
Koo, Alexander Rush and many others provided
technical advice. Hans Uszkoreit gave us per-
mission to use a subsample of sentences from the
Tiger Treebank (Brants et al, 2002), the source of
the news domain for our German data set. Anno-
tations were additionally provided by Sulki Kim,
Patrick McCrae, Laurent Alamarguy and He?ctor
Ferna?ndez Alcalde.
References
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora
Hladka?. 2003. The Prague Dependency Treebank:
A three-level annotation scenario. In Anne Abeille?,
editor, Treebanks: Building and Using Parsed Cor-
pora, pages 103?127. Kluwer.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Miriam Butt, Helge Dyvik, Tracy Holloway King,
Hiroshi Masuichi, and Christian Rohrer. 2002.
The parallel grammar project. In Proceedings of
the 2002 workshop on Grammar engineering and
evaluation-Volume 15.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with Chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation (SSST-3)
at NAACL HLT 2009.
96
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL-HLT.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
Marie-Catherine De Marneffe, Bill MacCartney, and
Chris D. Manning. 2006. Generating typed depen-
dency parses from phrase structure parses. In Pro-
ceedings of LREC.
Tomaz Erjavec. 2012. MULTEXT-East: Morphosyn-
tactic resources for Central and Eastern European
languages. Language Resources and Evaluation,
46:131?142.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction
via bitext projection constraints. In Proceedings of
ACL-IJCNLP.
Douwe Gelling, Trevor Cohn, Phil Blunsom, and Joao
Grac?a. 2012. The pascal challenge on grammar in-
duction. In Proceedings of the NAACL-HLT Work-
shop on the Induction of Linguistic Structure.
Jan Hajic?, Barbora Vidova Hladka, Jarmila Panevova?,
Eva Hajic?ova?, Petr Sgall, and Petr Pajas. 2001.
Prague Dependency Treebank 1.0. LDC, 2001T10.
Katri Haverinen, Timo Viljanen, Veronika Laippala,
Samuel Kohonen, Filip Ginter, and Tapio Salakoski.
2010. Treebanking finnish. In Proceedings of
The Ninth International Workshop on Treebanks and
Linguistic Theories (TLT9).
Stephen Helmreich, David Farwell, Bonnie Dorr, Nizar
Habash, Lori Levin, Teruko Mitamura, Florence
Reeder, Keith Miller, Eduard Hovy, Owen Rambow,
and Advaith Siddharthan. 2004. Interlingual anno-
tation of multilingual text corpora. In Proceedings
of the HLT-EACL Workshop on Frontiers in Corpus
Annotation.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of NAACL.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL.
Dan Klein and Chris D. Manning. 2004. Corpus-based
induction of syntactic structure: models of depen-
dency and constituency. In Proceedings of ACL.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan and Claypool.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.
Jens Nilsson, Joakim Nivre, and Johan Hall. 2007.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of ACL.
Joakim Nivre and Bea?ta Megyesi. 2007. Bootstrap-
ping a Swedish treebank using cross-corpus harmo-
nization and annotation projection. In Proceedings
of the 6th International Workshop on Treebanks and
Linguistic Theories.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on
dependency parsing. In Proceedings of EMNLP-
CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Mojgan Seraji, Bea?ta Megyesi, and Nivre Joakim.
2012. Bootstrapping a Persian dependency tree-
bank. Linguistic Issues in Language Technology,
7(18):1?10.
David A. Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the ACL.
Ulf Teleman. 1974. Manual fo?r grammatisk beskrivn-
ing av talad och skriven svenska. Studentlitteratur.
Reut Tsarfaty. 2013. A unified morpho-syntactic
scheme of stanford dependencies. Proceedings of
ACL.
Daniel Zeman, David Marecek, Martin Popel,
Loganathan Ramasamy, Jan S?tepa?nek, Zdene?k
Z?abokrtsky`, and Jan Hajic. 2012. Hamledt: To
parse or not to parse. In Proceedings of LREC.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL-HLT.
Yuan Zhang, Roi Reichart, Regina Barzilay, and Amir
Globerson. 2012. Learning to map into a universal
pos tagset. In Proceedings of EMNLP.
97
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 656?661,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Enforcing Structural Diversity in Cube-pruned Dependency Parsing
Hao Zhang Ryan McDonald
Google, Inc.
{haozhang,ryanmcd}@google.com
Abstract
In this paper we extend the cube-pruned
dependency parsing framework of Zhang
et al (2012; 2013) by forcing inference to
maintain both label and structural ambigu-
ity. The resulting parser achieves state-of-
the-art accuracies, in particular on datasets
with a large set of dependency labels.
1 Introduction
Dependency parsers assign a syntactic depen-
dency tree to an input sentence (K?ubler et al,
2009), as exemplified in Figure 1. Graph-based
dependency parsers parameterize models directly
over substructures of the tree, including single
arcs (McDonald et al, 2005), sibling or grand-
parent arcs (McDonald and Pereira, 2006; Car-
reras, 2007) or higher-order substructures (Koo
and Collins, 2010; Ma and Zhao, 2012). As the
scope of each feature function increases so does
parsing complexity, e.g., o(n
5
) for fourth-order
dependency parsing (Ma and Zhao, 2012). This
has led to work on approximate inference, typ-
ically via pruning (Bergsma and Cherry, 2010;
Rush and Petrov, 2012; He et al, 2013)
Recently, it has been shown that cube-pruning
(Chiang, 2007) can efficiently introduce higher-
order dependencies in graph-based parsing (Zhang
and McDonald, 2012). Cube-pruned dependency
parsing runs standard bottom-up chart parsing us-
ing the lower-order algorithms. Similar to k-best
inference, each chart cell maintains a beam of k-
best partial dependency structures. Higher-order
features are scored when combining beams during
inference. Cube-pruning is an approximation, as
the highest scoring tree may fall out of the beam
before being fully scored with higher-order fea-
tures. However, Zhang et al (2013) observe state-
of-the-art results when training accounts for errors
that arise due to such approximations.
John emailed April about one month ago
NSUBJ
IOBJ
ADVMOD
Q
U
A
N
T
M
O
D
N
U
M
N
P
A
D
V
M
O
D
Figure 1: A sample dependency parse.
In this work we extend the cube-pruning frame-
work of Zhang et al by observing that dependency
parsing has two fundamental sources of ambiguity.
The first, structural ambiguity, pertains to confu-
sions about the unlabeled structure of the tree, e.g.,
the classic prepositional phrase attachment prob-
lem. The second, label ambiguity, pertains to sim-
ple label confusions, e.g., whether a verbal object
is direct or indirect.
Distinctions between arc labels are frequently
fine-grained and easily confused by parsing mod-
els. For example, in the Stanford dependency
label set (De Marneffe et al, 2006), the labels
TMOD (temporal modifier), NPADVMOD (noun-
phrase adverbial modifier), IOBJ (indirect object)
and DOBJ (direct object) can all be noun phrases
that modify verbs to their right. In the context of
cube-pruning, during inference, the system opts to
maintain a large amount of label ambiguity at the
expense of structural ambiguity. Frequently, the
beam stores only label ambiguities and the result-
ing set of trees have identical unlabeled structure.
For example, in Figure 1, the aforementioned la-
bel ambiguity around noun objects to the right of
the verb (DOBJ vs. IOBJ vs. TMP) could lead one
or more of the structural ambiguities falling out of
the beam, especially if the beam is small.
To combat this, we introduce a secondary beam
for each unique unlabeled structure. That is,
we partition the primary (entire) beam into dis-
joint groups according to the identity of unla-
beled structure. By limiting the size of the sec-
ondary beam, we restrict label ambiguity and en-
force structural diversity within the primary beam.
The resulting parser consistently improves on the
state-of-the-art parser of Zhang et al (2013). In
656
(a)
l
=
l
+
l
1
(b)
l
=
l
1
+
l
2
Figure 2: Structures and rules for parsing with the
(Eisner, 1996) algorithm. Solid lines show only
the construction of right-pointing first-order de-
pendencies. l is the predicted arc label. Dashed
lines are the additional sibling modifier signatures
in a generalized algorithm, specifically the previ-
ous modifier in complete chart items.
particular, data sets with large label sets (and thus
a large number of label confusions) typically see
the largest jumps in accuracy. Finally, we show
that the same result cannot be achieved by simply
increasing the size of the beam, but requires ex-
plicit enforcing of beam diversity.
2 Structural Diversity in Cube-Pruning
Our starting point is the cube-pruned dependency
parsing model of Zhang and McDonald (2012). In
that work, as here, inference is simply the Eis-
ner first-order parsing model (Eisner, 1996) shown
in Figure 2. In order to score higher-order fea-
tures, each chart item maintains a list of signa-
tures, which represent subtrees consistent with the
chart item. The stored signatures are the relevant
portions of the subtrees that will be part of higher-
order feature calculations. For example, to score
features over adjacent arcs, we might maintain ad-
ditional signatures, again shown in Figure 2.
The scope of the signature adds asymptotic
complexity to parsing. Even for second-order sib-
lings, there will now be O(n) possible signatures
per chart item. The result is that parsing com-
plexity increases from O(n
3
) to O(n
5
). Instead
of storing all signatures, Zhang and McDonald
(2012) store the current k-best in a beam. This re-
sults in approximate inference, as some signatures
may fall out of the beam before higher-order fea-
tures can be scored. This general trick is known as
cube-pruning and is a common approach to deal-
ing with large hypergraph search spaces in ma-
chine translation (Chiang, 2007).
Cube-pruned parsing is analogous to k-best
parsing algorithmically. But there is a fundamen-
tal difference. In k-best parsing, if two subtrees
t
a
and t
b
belong to the same chart item, with t
a
l
=
0 : 1 : 2 :
+
l
1
l
2
l
1
0 :
l
1
1 :
l
2
2 :
l
3
l
=
0 : 1 : 2 :
+
l
1
l
2
l
1
0 :
l
1
1 :
l
2
2 :
l
1
Figure 3: Merging procedure in cube pruning. The
bottom shows that enforcing diversity in the k-best
lists can give chance to a good structure at (2, 2).
ranking higher than t
b
, then an extension of t
a
through combing with a subtree t
c
from another
chart item must also score higher than that of t
b
.
This property is called the monotonicity property.
Based on it, k-best parsing merges k-best subtrees
in the following way: given two chart items with
k-best lists to be combined, it proceeds on the two
sorted lists monotonically from beginning to end
to generate combinations. Cube pruning follows
the merging procedure despite the loss of mono-
tonicity due to the addition of higher-order feature
functions over the signatures of the subtrees. The
underlying assumption of cube pruning is that the
true k-best results are likely in the cross-product
space of top-ranked component subtrees. Figure 3
shows that the space is the top-left corner of the
grid in the binary branching cases.
As mentioned earlier, the elements in chart item
k-best lists are feature signatures of subtrees. We
make a distinction between labeled signatures and
unlabeled signatures. As feature functions are de-
fined on sub-graphs of the dependency trees, a fea-
ture signature is labeled if and only if feature func-
tions draw information from both the arcs in the
sub-graph and the labels on the arcs. Every la-
beled signature projects to an unlabeled signature
657
that ignores the arc labels.
The motivation for introducing unlabeled signa-
tures for labeled parsing is to enforce structural di-
versity. Figure 3 illustrates the idea. In the top
diagram, there is only one unlabeled signature in
one of the two lists. This is likely to happen when
there is label ambiguity so that all three labels have
similar scores. In such cases, alternative tree struc-
tures further down in the list that have the poten-
tial to be scored higher when incorporating higher-
order features, lose this opportunity due to prun-
ing. By contrast, if we introduce structural diver-
sity by limiting the number of label variants, such
alternative structures can come out on top.
More formally, when the feature signatures of
the subtrees include arc labels, the cardinality of
the set of all possible signatures grows by a poly-
nomial of the size of the label set. This factor has a
diluting effect on the diversity of unlabeled signa-
tures within the beam. The larger the label set is,
the greater the chance label ambiguity will dom-
inate the beam. Therefore, we introduce a sec-
ond level of beam specifically for labeled signa-
tures. We call it the secondary beam, relative to
the primary beam, i.e., the entire beam. The sec-
ondary beam limits the number of labeled signa-
tures for each unlabeled signature, a projection of
labeled signature, while the primary beam limits
the total number of labeled signatures. To illus-
trate this, consider an original primary beam of
length b and a secondary beam length of s
b
. Let
t
j
i
represent the i
th
highest scoring labeled variant
of unlabeled structure j. The table below shows a
specific example of beam configurations for b = 4
for all possible values of s
b
. The original beam is
the pathological case where all signatures have the
same unlabeled projection. When s
b
= 1, all sig-
natures in the beam now have a different unlabeled
projection. When s
b
= 4, the beam reverts to the
original without any structural diversity. Values
between balance structural and label diversity.
beam original b = 4 b = 4 b = 4 b = 4
rank b=4 s
b
= 1 s
b
= 2 s
b
= 3 s
b
= 4
1 t
1
1
t
1
1
t
1
1
t
1
1
t
1
1
2 t
1
2
t
2
1
t
1
2
t
1
2
t
1
2
3 t
1
3
t
3
1
t
2
1
t
1
3
t
1
3
4 t
1
4
t
4
1
t
3
1
t
2
1
t
1
4
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? beam cut-off ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
5 t
2
1
. . . . . . . . . . . .
6 t
3
1
. . . . . . . . . . . .
7 t
2
2
. . . . . . . . . . . .
8 t
3
2
. . . . . . . . . . . .
9 t
4
1
. . . . . . . . . . . .
To achieve this in cube pruning, deeper explo-
ration in the merging procedure becomes neces-
sary. In this example, originally the merging pro-
cedure stops when t
1
4
has been explored. When
s
b
= 1, the exploration needs to go further from
rank 4 to 9. When s
b
= 2, it needs to go from 4
to 6. When s
b
= 3, only one more step to rank
5 is necessary. The amount of additional compu-
tation depends on the value of s
b
, the composi-
tion of the incoming k-best lists, and the feature
functions which determine feature signatures. To
account for this we also compare to baselines sys-
tems that simply increase the size of the beam to a
comparable run-time.
In our experiments we found that s
b
= b/2 is
typically a good choice. As in most parsing sys-
tems, beams are applied consistently during learn-
ing and testing because feature weights will be ad-
justed according to the diversity of the beam.
3 Experiments
We use the cube-pruned dependency parser of
Zhang et al (2013) as our baseline system. To
make an apples-to-apples comparison, we use the
same online learning algorithm and the same fea-
ture templates. The feature templates include first-
to-third-order labeled features and valency fea-
tures. More details of these features are described
in Zhang and McDonald (2012). For online learn-
ing, we apply the same violation-fixing strategy
(so-called single-node max-violation) on MIRA
and run 8 epochs of training for all experiments.
For English, we conduct experiments on
the commonly-used constituency-to-dependency-
converted Penn Treebank data sets. The first one,
Penn-YM, was created by the Penn2Malt
1
soft-
ware. The second one, Penn-S-2.0.5, used the
Stanford dependency framework (De Marneffe et
al., 2006) by applying version 2.0.5 of the Stan-
ford parser. The third one, Penn-S-3.3.0 was con-
verted by version 3.3.0 of the Stanford parser. The
train/dev/test split was standard: sections 2-21 for
training; 22 for validation; and 23 for evaluation.
Automatic POS tags for Penn-YM and Penn-S-
2.0.5 are provided by TurboTagger (Martins et al,
2013) with an accuracy of 97.3% on section 23.
For Chinese, we use the CTB-5 dependency tree-
bank which was converted from the original con-
stituent treebank by Zhang and Nivre (2011) and
use gold-standard POS tags as is standard.
1
http://stp.lingfil.uu.se/?nivre/research/Penn2Malt.html
658
Berkeley Parser TurboParser Cube-pruned w/o diversity Cube-pruned w/ diversity
UAS LAS UAS LAS UAS LAS UAS LAS
PENN-YM - - 93.07 - 93.50 92.41 93.57 92.48
PENN-S-2.0.5 - - 92.82 - 93.59 91.17 93.71 91.37
PENN-S-3.3.0 93.31 91.01 92.20 89.67 92.91 90.52 93.01 90.64
PENN-S-3.3.0-GOLD 93.65 92.05 93.56 91.99 94.32 92.90 94.40 93.02
CTB-5 - - - - 87.78 86.13 87.96 86.34
Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of
structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is
from Martins et al (2013). Following Kong and Smith (2014), we trained our models on Penn-S-3.3.0
with gold POS tags and evaluated with both non-gold (Stanford tagger) and gold tags.
Table 1 shows the main results of the paper.
Both the baseline and the new system keep a beam
of size 6 for each chart cell. The difference is
that the new system enforces structural diversity
with the introduction of a secondary beam for la-
bel variants. We choose the secondary beam that
yields the highest LAS on the development data
sets for Penn-YM, Penn-S-2.0.5 and CTB-5. In-
deed we observe larger improvements for the data
sets with larger label sets. Penn-S-2.0.5 has 49 la-
bels and observes a 0.2% absolute improvement in
LAS. Although CTB-5 has a small label set (18),
we do see similar improvements for both UAS and
LAS. There is a slight improvement for Penn-YM
despite the fact that Penn-YM has the most com-
pact label set (12). These results are the highest
known in the literature. For the Penn-S-3.3.0 re-
sults we can see that our model outperforms Tur-
boPaser and is competitive with the Berkeley con-
stituency parser (Petrov et al, 2006). In particu-
lar, if gold tags are assumed, cube-pruning signif-
icantly outperforms Berkeley. This suggests that
joint tagging and parsing should improve perfor-
mance further in the non-gold tag setting, as that
is a differentiating characteristic of constituency
parsers. Table 2 shows the results on the CoNLL
2006/2007 data sets (Buchholz and Marsi, 2006;
Nivre et al, 2007). For simplicity, we set the sec-
ondary beam to 3 for all. We can see that over-
all there is an improvement in accuracy and this is
highly correlated with the size of the label set.
In order to examine the importance of balancing
structural diversity and labeled diversity, we let the
size of the secondary beam vary from one to the
size of the primary beam. In Table 3, we show the
results of all combinations of beam settings of pri-
mary beam sizes 4 and 6 for three data sets: Penn-
YM, Penn-S-2.0.5, and CTB-5 respectively. In the
table, we highlight the best results for each beam
size and data set on the development data. For 5
of the total of 6 comparison groups ? three lan-
w/o diversity w/ diversity
Language(labels) UAS LAS UAS LAS
CZECH(82) 88.36 82.16 88.36 82.02
SWEDISH(64) 91.62 85.08 91.85 85.26
PORTUGUESE(55) 92.07 88.30 92.23 88.50
DANISH(53) 91.88 86.95 91.78 86.93
HUNGARIAN(49) 85.85 81.02 86.55 81.79
GREEK(46) 86.14 78.20 86.21 78.45
GERMAN(46) 92.03 89.44 92.01 89.52
CATALAN(42) 94.58 89.05 94.91 89.54
BASQUE(35) 79.59 71.52 80.14 71.94
ARABIC(27) 80.48 69.68 80.56 69.98
TURKISH(26) 76.94 66.80 77.14 67.00
SLOVENE(26) 86.01 77.14 86.27 77.44
DUTCH(26) 83.57 80.29 83.39 80.19
ITALIAN(22) 87.57 83.22 87.38 82.95
SPANISH(21) 87.96 84.95 87.98 84.79
BULGARIAN(19) 94.02 89.87 93.88 89.63
JAPANESE(8) 93.26 91.67 93.16 91.51
AVG 87.76 82.08 87.87 82.20
Table 2: Results for languages from CoNLL
2006/2007 shared tasks. When a language is in
both years, the 2006 set is used. Languages are
sorted by the number of unique arc labels.
guages times two primary beams ? the best result
is obtained by choosing a secondary beam size that
is close to one half the size of the primary beam.
Contrasting Table 1 and Table 3, the accuracy im-
provements are consistent across the development
set and the test set for all three data sets.
A reasonable question is whether such improve-
ments could be obtained by simply enlarging the
beam in the baseline parser. The bottom row of
Table 3 shows the parsing results for the three data
sets when the beam is enlarged to 16. On Penn-
S-2.0.5, the baseline with beam 16 is at roughly
the same speed as the highlighted best system with
primary beam 6 and secondary beam 3. On CTB-
5, the beam 16 baseline is 30% slower. Table 3
indicates that simply enlarging the beam ? rela-
tive to parsing speed ? does not recover the wins
of structural diversity on Penn-S-2.0.5 and CTB-5,
though it does reduce the gap on Penn-S-2.0.5. On
Penn-YM, the beam 16 baseline is slightly better
than the new system, but 90% slower.
659
primary secondary PENN-YM PENN-S-2.0.5 CTB-5
beam beam UAS LAS UAS LAS UAS LAS
4
1 93.67 92.64 93.65 91.04 87.53 85.85
2 93.79 92.68 93.77 91.30 87.62 85.96
3 93.80 92.66 93.69 91.23 87.48 85.91
4 93.75 92.63 93.62 91.11 87.68 86.08
6
1 93.65 92.46 93.76 91.15 87.72 86.05
2 93.80 92.69 93.80 91.35 87.61 85.96
3 93.75 92.64 93.99 91.55 87.80 86.18
4 93.82 92.74 93.84 91.40 87.91 86.28
5 93.82 92.71 93.71 91.26 87.75 86.12
6 93.74 92.61 93.70 91.21 87.66 86.05
16 16 93.87 92.75 93.77 91.35 87.59 85.86
Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with
different primary beams. When the size of the secondary beam is equal to the primary beam, the parser
degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,
there is more structural diversity and less label diversity. Results are on development sets.
To better understand the behaviour of structural
diversity pruning relative to increasing the beam,
we looked at the unlabeled attachment F-score per
dependency label in the Penn-S-2.0.5 development
set
2
. Table 4 shows the 10 labels with the largest
increase in attachment scores for structural diver-
sity pruning relative to standard pruning. Impor-
tantly, the biggest wins are primarily for labels in
which unlabeled attachment is lower than average
(93.99, 8 out of 10). Thus, diversity pruning gets
most of its wins on difficult attachment decisions.
Indeed, many of the relations represent clausal
dependencies that are frequently structurally am-
biguous. There are also cases of relatively short
dependencies that can be difficult to attach. For
instance, quantmod dependencies are typically ad-
verbs occurring after verbs that modify quantities
to their right. But these can be confused as ad-
verbial modifiers of the verb to the left. These re-
sults support our hypothesis that label ambiguity
is causing hard attachment decisions to be pruned
and that structural diversity can ameliorate this.
4 Discussion
Keeping multiple beams in approximate search
has been explored in the past. In machine transla-
tion, multiple beams are used to prune translation
hypotheses at different levels of granularity (Zens
and Ney, 2008). However, the focus is improving
the speed of translation decoder rather than im-
proving translation quality through enforcement
of hypothesis diversity. In parsing, Bohnet and
Nivre (2012) and Bohnet et al (2013) propose a
model for joint morphological analysis, part-of-
speech tagging and dependency parsing using a
2
Using eval.pl from Buchholz and Marsi (2006).
w/o diversity w/ diversity
Label large beam small beam diff
quantmod 86.65 88.06 1.41
partmod 83.63 85.02 1.39
xcomp 87.76 88.74 0.98
tmod 89.75 90.72 0.97
appos 88.89 89.84 0.95
nsubjpass 92.53 93.31 0.78
complm 94.50 95.15 0.64
advcl 81.10 81.74 0.63
ccomp 82.64 83.17 0.54
number 96.86 97.39 0.53
Table 4: Unlabeled attachment F-score per de-
pendency relation. The top 10 score increases
for structural diversity pruning (beam 6 and la-
bel beam of 3) over basic pruning (beam 16) are
shown. Only labels with more than 100 instances
in the development data are considered.
left-to-right beam. With a single beam, token level
ambiguities (morphology and tags) dominate and
dependency level ambiguity is suppressed. This is
addressed by essentially keeping two beams. The
first forces every tree to be different at the depen-
dency level and the second stores the remaining
highest scoring options, which can include outputs
that differ only at the token level.
The present work looks at beam diversity in
graph-based dependency parsing, in particular la-
bel versus structural diversity. It was shown that
by keeping a diverse beam significant improve-
ments could be achieved on standard benchmarks,
in particular with respect to difficult attachment
decisions. It is worth pointing out that other
dependency parsing frameworks (e.g., transition-
based parsing (Zhang and Clark, 2008; Zhang and
Nivre, 2011)) could also benefit from modeling
structural diversity in search.
660
References
S. Bergsma and C. Cherry. 2010. Fast and accurate arc
filtering for dependency parsing. In Proc. of COL-
ING.
B. Bohnet and J. Nivre. 2012. A transition-based
system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In Proc. of
EMNLP/CoNLL.
B. Bohnet, J. Nivre, I. Boguslavsky, F. Ginter, Rich?ard
F., and J. Hajic. 2013. Joint morphological and syn-
tactic analysis for richly inflected languages. TACL,
1.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc.
of CoNLL.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of the CoNLL
Shared Task Session of EMNLP-CoNLL.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
M. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc. of LREC.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: an exploration. In Proc. of COL-
ING.
H. He, H. Daum?e III, and J. Eisner. 2013. Dynamic
feature selection for dependency parsing. In Proc.
of EMNLP.
L. Kong and N. A. Smith. 2014. An empirical compar-
ison of parsing methods for stanford dependencies.
In ArXiv:1404.4314.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of ACL.
S. K?ubler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Morgan & Claypool Publishers.
X. Ma and H. Zhao. 2012. Fourth-order dependency
parsing. In Proc. of COLING.
A. F. T. Martins, M. B. Almeida, and N. A. Smith.
2013. Turning on the turbo: Fast third-order non-
projective turbo parsers. In Proc. of ACL.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proc. of EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proc. of ACL.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
of EMNLP-CoNLL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In Proc. of ACL.
A. Rush and S. Petrov. 2012. Efficient multi-pass de-
pendency pruning with vine parsing. In Proc. of
NAACL.
R. Zens and H. Ney. 2008. Improvements in dynamic
programming beam search for phrase-based statisti-
cal machine translation. In Proc. IWSLT.
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing. In Proc.
of EMNLP.
H. Zhang and R. McDonald. 2012. Generalized
higher-order dependency parsing with cube pruning.
In Proc. of EMNLP.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proc.
of ACL-HLT, volume 2.
H. Zhang, L. Huang, K.Zhao, and R. McDonald. 2013.
Online learning for inexact hypergraph search. In
Proc. of EMNLP.
661
NEUNLPLab Chinese Word Sense Induction System for 
SIGHAN Bakeoff 2010 
Hao Zhang Tong Xiao Jingbo Zhu 
1. Key Laboratory of Medical Image Computing (Northeastern University), Ministry 
of Education 
2. Natural Language Processing Laboratory, Northeastern University 
zhanghao1216@gmail.com 
{xiaotong, zhujingbo}@mail.neu.edu.cn 
 
Abstract 
This paper describes a character-based 
Chinese word sense induction (WSI) sys-
tem for the International Chinese Lan-
guage Processing Bakeoff 2010. By 
computing the longest common sub-
strings between any two contexts of the 
ambiguous word, our system extracts 
collocations as features and does not de-
pend on any extra tools, such as Chinese 
word segmenters. We also design a con-
strained clustering algorithm for this task. 
Experiemental results show that our sys-
tem could achieve 69.88 scores of 
FScore on the development data set of 
SIGHAN Bakeoff 2010. 
1 Introduction 
The goal of word sense induction (WSI) is to 
group occurrences containing a given ambiguous 
word into clusters with respect to sense. Most 
researchers take the problem of word sense in-
duction as a clustering problem. Pantel & Lin 
(2002) clustered words on the basis of the dis-
tances of their co-occurrence vectors, and used 
global clustering as a solution. Neill (2002) used 
local clustering, and determined the senses of a 
given word by clustering its close associations. 
In this paper, we propose a simple but effec-
tive method to extract collocations as features 
from texts without pre-segmentations, and de-
sign a constrained clustering algorithm to ad-
dress the issue of Chinese word sense induction. 
By using our collocation extraction method, our 
Chinese WSI system is independent of any extra 
natural language processing tools, such as Chi-
nese word segmenters. On the development set 
of SIGHAN 2010 WSI task, the experimental 
results show that our system could achieve 69.88 
scores of FScore. In addition, the official results 
show that the performance of our system is 
67.15 scores of FScore on the test set of 
SIGHAN Bakeoff 2010. 
The rest of this paper is organized as follows. 
In Section 2, we present the task description of 
Chinese word sense induction. In Section 3, we 
first give an overview of our Chinese WSI sys-
tem, and then propose our feature extraction 
method and constrained clustering algorithm. In 
Section 4, we describe the evaluation method 
and show the experimental results on the devel-
opment and test data sets of the Bakeoff 2010. In 
Section 5, we conclude our work. 
2 Task Description 
Given the number of senses S and occurrences of 
the ambiguous word w, a word sense induction 
system is supposed to cluster the occurrences 
into S clusters, with each cluster representing a 
sense of the ambiguous word w. For example, 
suppose that there are some sentences containing 
the ambiguous word ???? (gloomy), and the 
sense number S is 2, the job of WSI system is to 
cluster these sentences into 2 clusters, with each 
cluster representing a sense of ????. Based on 
this task description, it is obvious to regard the 
problem of WSI as a clustering problem. 
Figures 1-2 shows example input and output 
of our WSI system , where there are 6 sentences 
and 2 resulting clusters. In Figure 1, the first 
column are the identifiers of sentences contain-
ing the word ????, and the second column are 
part of the sentences. In Figure 2, the first col-
umn represents the identifiers of sentences, and 
the second column represents the identifiers of 
clusters generated by our Chinese WSI system. 
 
Figure 1 Part of input of word ???? for our 
WSI system 
 
Figure 2 Output of our WSI system for word 
???? 
3 NEU Chinese WSI System 
3.1 System overview 
Our Chinese word sense induction system is 
built based on clustering work-frame. There are 
four major modules in the system, including 
data pre-processing, feature extraction, cluster-
ing and data post-processing modules. The ar-
chitecture of our Chinese WSI system is illus-
trated in Figure 3. 
3.2 Feature extraction 
Since there is no separators in Chinese like 
?space? in English to mark word boundaries, 
most Chinese natural language processing appli-
cations need to first apply a Chinese word seg-
menter to segment Chinese sentences. In our 
Chinese word sense induction system, we extract 
collocations from sentences containing the am-
biguous word as features. To extract collocations, 
we might first segment the sentences into word 
sequences, and then conduct feature extraction 
on the word-segmented corpus. However, errors 
might be induced in the procedure due to un-
avoidable incorrect segmentation results. Ad-
dressing this issue, we propose a method to di-
rectly extract collocations from sentences with-
out pre-segmentations. 
In our method, we extract two kinds of collo-
cations, namely ?global collocation? and ?local 
collocation?. Here global collocations are de-
fined to be the words (or character sequences) 
that frequently co-occur with the ambiguous 
word, and local collocations are defined to be 
the characters adjacent to the ambiguous word1. 
 
Figure 3 Architecture of our system 
To extract global collocations, we first com-
pute all the longest common substrings between 
any two of the sentences containing the ambigu-
ous word to form the set of candidate global col-
locations. For each candidate global collocation, 
we count the number of sentences containing it. 
We then reduce the size of the candidate set by 
eliminating candidates which contain only one 
character or functional words. We also remove 
the candidate with other candidates as its sub-
strings. Finally, we eliminate the candidates 
whose count of the number of sentences is below 
a certain threshold. The threshold equals to two 
in our experiments. We regard the candidates 
after the above processing as global collocations 
for WSI. 
To extract local collocations, we simply ex-
tract one character on both left and right sides of 
the ambiguous word to form the set of candidate 
local collocations. We then refine the candidate 
set by eliminating candidates which are func-
tional words or whose frequency is below a cer-
tain threshold. The threshold is set to two in our 
experiments. 
After extracting global collocations and local 
collocations, we put them together to form the 
                                                 
1
 Definitions of global collocation and local collocation 
might be different from those in other papers. 
start 
data pre-processing 
feature extraction 
clustering 
data post-processing 
end 
final set of collocations and use them as features 
of our system. For each collocation (or feature), 
we compute the list of indices of sentences that 
containing the collocation. Thus, every element 
of the set of collocations has the data structure of 
pair of ?key? and ?value?, where ?key? is the 
collocation itself, and the ?value? is the list of 
indices. 
3.3 Clustering algorithm 
We find that the high-confidence collocation is a 
very good indicator to distinguish the senses of 
an ambiguous word. However, the traditional 
clustering methods are based on the vector rep-
resentations of features, which probably de-
creases the effect of dominant features (i.e. high-
confidence collocations). To alleviate the prob-
lem, a nice way is to incorporate collocations 
into the clustering process as constraints. Moti-
vated by this idea, we design a constrained clus-
tering algorithm. In this algorithm, we could en-
sure that some occurrences of the ambiguous 
word must be in one cluster and some must not 
be in one cluster. The input for our constrained 
clustering algorithm is the set of collocations 
described in the previous section and the process 
of our clustering algorithm is shown in Table 1. 
Here the notation starting with character ?C? 
represents a collocation, and the notations of 
?Sin? and ?Srlt? represent the collocation set and 
the result set, respectively. 
Every element in the result set Srlt is regarded 
as one cluster for a given ambigous word, and 
the list of the element records the indices of the 
sentences belonging to the cluster. 
4 Evaluation of Our System 
The evaluation method is F-score which is pro-
vided within the Bakeoff 2010 (Zhao and 
Karypis, 2005). Suppose Cr is a class of the gold 
standard, and Si is a cluster of our system gener-
ated. FScore is computed with the formulas be-
low. 
( , ) 2 * * / ( )F score Cr Si P R P R? = +          (1) 
( ) max( ( , ))
Si
FScore Cr F score Cr Si= ?         (2) 
1
( )
c
r
nr
FScore FScore Cr
n
=
=?                (3) 
We evaluate our Chinese word sense induc-
tion system on the development data set and the 
test data set of the Bakeoff 2010. The details of 
the development data set and the test data set are 
summarized in Table 2. 
For comparison, we develop a baseline system 
that also uses the collocations as features and 
clustering based on the vector representations of 
features. On the development data set, we test 
our system and compare it with the baseline sys-
tem. The performance of our Chinese WSI sys-
tem and the baseline system are shown in Table 
3. From Table 3, we see that using our con-
strained clustering algorithm is better than using 
the traditional hierarchical clustering methods by 
7.06 scores of FScore for our Chinese WSI sys-
tem. It indicates that our constrained clustering 
algorithm could avoid reducing the effect of  
Input: collocation set Sin 
while there is available collocation Ci in the 
input set Sin 
 for each collocation Ct in the set Sin 
 if Ct not equals to Ci, and Ct is avail-
able 
 if list of Ct has intersection with 
that of Ci, or Ct and Ci have a 
meaningful substring (word or 
character), compose list of Ct into 
list of Ci, and mark Ct to be un-
available 
 end if 
 end if 
 end for 
 store Ci and its list into result set Srlt, and 
mark Ci to be unavailable 
end while 
if there are available collocations in the input 
set Sin 
 if the size of result set Srlt does not sat-
isfy the given cluster number, devide the 
rest collocations in Sin evenly into the 
rest clusters, and append their lists to 
their own clusters? lists respectively 
 else add the rest collocations into the last 
cluster, and append their list to the list of 
the last cluster 
 end if 
end if 
return the result set Srlt 
Output: result set Srlt 
Table 1 Constrained clustering algorithm 
high-confidence features (i.e. high-confidence 
collocations) and lead to better clustering results. 
This conclusion is also ensured by the compari-
son between our constrained clustering algo-
rithm and the traditional K-means clustering al-
gorithm. 
In addition, our system achieves 67.15 scores 
of FScore on the test data set reported by the 
SIGHAN Bakeoff 2010. 
data descriptions 
Dev set 
containing 50 ambiguous words, 
about 50 sentences for each am-
biguous word 
Test set 
containing 100 ambiguous words, 
about 50 sentences for each am-
biguous word 
Table 2 Data sets of SIGHAN Bakeoff 2010 
clustering methods 
FScore of 
our system 
(%) 
traditional hierarchical cluster-
ing 62.82 
traditional K-means clustering 62.48 
our constrained clustering 69.88 
Table 3 System performance on dev set of 
Bakeoff 2010 using different clustering methods 
5 Conclusions 
In this paper, we propose a collocation extrac-
tion method and a constrained clustering algo-
rithm for Chinese WSI task. By using the collo-
cation extraction method and the clustering algo-
rithm, our Chinese word sense induction system 
is independent of any extra tools. When tested 
on the test data set of the Bakeoff 2010, our sys-
tem achieves 67.15 scores of FScore. 
References 
Vickrey, David, Luke Biewald, Marc Teyssler, and 
Daphne Koller. 2005. Word-sense disambiguation 
for machine translation. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing, 
Morristown, NJ, USA, pages 771-778. 
Yarowsky, David. 1995. Unsupervised word sense 
disambiguation rivaling supervised methods. In 
Proceedings of 33rd Meeting of the Association for 
Computational Linguistics, Cambridge, MA, 189-
196. 
Schutze, Hinrich. 1998. Automatic word sense dis-
crimination. Computational Linguistics, Montreal, 
Canada, 24(1):97?123. 
Ng, Hwee Tou, Hian Beng Lee. 1996. Integrating 
Multiple Knowledge Sources to Disambiguate 
Word Sense: An Exemplar-Based Approach. In 
Proceedings of the 34th Meeting of the Association 
for Computational Linguistics, California, USA, 
pages 40-47. 
Daniel, Neill. 2002. Fully Automatic Word Sense 
Induction by Semantic Clustering. In Computer 
Speech, Cambridge University, Master?s Thesis. 
Pantel, Patrick, Dekang Lin. 2002. Discovering word 
senses from text. In Proceedings of ACM SIGKDD, 
Edmonton, 613-619. 
Rapp, Reinhard. 2004. A Practical Solution to the 
Problem of Automatic Word Sense Induction. In 
Proceedings of the 42nd Meeting of the Association 
for Computational Linguistics, Barcelona, Spain. 
Zhao, Ying, George Karypis. 2005. Hierarchical 
Clustering Algorithms for Document Datasets. 
Data Mining and Knowledge Discovery, 10:141-
168. 
