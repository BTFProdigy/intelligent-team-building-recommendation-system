Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 66?73, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
NTNU-CORE: Combining strong features for semantic similarity
Erwin Marsi, Hans Moen, Lars Bungum, Gleb Sizov, Bjo?rn Gamba?ck, Andre? Lynum
Norwegian University of Science and Technology
Department of Computer and Information and Science
Sem S?lands vei 7-9
NO-7491 Trondheim, Norway
{emarsi,hansmoe,bungum,sizov,gamback,andrely}@idi.ntnu.no
Abstract
The paper outlines the work carried out at
NTNU as part of the *SEM?13 shared task
on Semantic Textual Similarity, using an ap-
proach which combines shallow textual, dis-
tributional and knowledge-based features by
a support vector regression model. Feature
sets include (1) aggregated similarity based
on named entity recognition with WordNet
and Levenshtein distance through the calcula-
tion of maximum weighted bipartite graphs;
(2) higher order word co-occurrence simi-
larity using a novel method called ?Multi-
sense Random Indexing?; (3) deeper seman-
tic relations based on the RelEx semantic
dependency relationship extraction system;
(4) graph edit-distance on dependency trees;
(5) reused features of the TakeLab and DKPro
systems from the STS?12 shared task. The
NTNU systems obtained 9th place overall (5th
best team) and 1st place on the SMT data set.
1 Introduction
Intuitively, two texts are semantically similar if they
roughly mean the same thing. The task of formally
establishing semantic textual similarity clearly is
more complex. For a start, it implies that we have
a way to formally represent the intended meaning of
all texts in all possible contexts, and furthermore a
way to measure the degree of equivalence between
two such representations. This goes far beyond the
state-of-the-art for arbitrary sentence pairs, and sev-
eral restrictions must be imposed. The Semantic
Textual Similarity (STS) task (Agirre et al, 2012,
2013) limits the comparison to isolated sentences
only (rather than complete texts), and defines sim-
ilarity of a pair of sentences as the one assigned by
human judges on a 0?5 scale (with 0 implying no
relation and 5 complete semantic equivalence). It is
unclear, however, to what extent two judges would
agree on the level of similarity between sentences;
Agirre et al (2012) report figures on the agreement
between the authors themselves of about 87?89%.
As in most language processing tasks, there are
two overall ways to measure sentence similarity, ei-
ther by data-driven (distributional) methods or by
knowledge-driven methods; in the STS?12 task the
two approaches were used nearly equally much.
Distributional models normally measure similarity
in terms of word or word co-occurrence statistics, or
through concept relations extracted from a corpus.
The basic strategy taken by NTNU in the STS?13
task was to use something of a ?feature carpet bomb-
ing approach? in the way of first automatically ex-
tracting as many potentially useful features as possi-
ble, using both knowledge and data-driven methods,
and then evaluating feature combinations on the data
sets provided by the organisers of the shared task.
To this end, four different types of features were
extracted. The first (Section 2) aggregates similar-
ity based on named entity recognition with WordNet
and Levenshtein distance by calculating maximum
weighted bipartite graphs. The second set of features
(Section 3) models higher order co-occurrence sim-
ilarity relations using Random Indexing (Kanerva
et al, 2000), both in the form of a (standard) sliding
window approach and through a novel method called
?Multi-sense Random Indexing? which aims to sep-
arate the representation of different senses of a term
66
from each other. The third feature set (Section 4)
aims to capture deeper semantic relations using ei-
ther the output of the RelEx semantic dependency
relationship extraction system (Fundel et al, 2007)
or an in-house graph edit-distance matching system.
The final set (Section 5) is a straight-forward gath-
ering of features from the systems that fared best in
STS?12: TakeLab from University of Zagreb (S?aric?
et al, 2012) and DKPro from Darmstadt?s Ubiqui-
tous Knowledge Processing Lab (Ba?r et al, 2012).
As described in Section 6, Support Vector Regres-
sion (Vapnik et al, 1997) was used for solving the
multi-dimensional regression problem of combining
all the extracted feature values. Three different sys-
tems were created based on feature performance on
the supplied development data. Section 7 discusses
scores on the STS?12 and STS?13 test data.
2 Compositional Word Matching
Compositional word matching similarity is based
on a one-to-one alignment of words from the two
sentences. The alignment is obtained by maximal
weighted bipartite matching using several word sim-
ilarity measures. In addition, we utilise named entity
recognition and matching tools. In general, the ap-
proach is similar to the one described by Karnick
et al (2012), with a different set of tools used. Our
implementation relies on the ANNIE components in
GATE (Cunningham et al, 2002) and will thus be
referred to as GateWordMatch.
The processing pipeline for GateWordMatch
is: (1) tokenization by ANNIE English Tokeniser,
(2) part-of-speech tagging by ANNIE POS Tagger,
(3) lemmatization by GATE Morphological Anal-
yser, (4) stopword removal, (5) named entity recog-
nition based on lists by ANNIE Gazetteer, (6) named
entity recognition based on the JAPE grammar by
the ANNIE NE Transducer, (7) matching of named
entities by ANNIE Ortho Matcher, (8) computing
WordNet and Levenstein similarity between words,
(9) calculation of a maximum weighted bipartite
graph matching based on similarities from 7 and 8.
Steps 1?4 are standard preprocessing routines.
In step 5, named entities are recognised based on
lists that contain locations, organisations, compa-
nies, newspapers, and person names, as well as date,
time and currency units. In step 6, JAPE grammar
rules are applied to recognise entities such as ad-
dresses, emails, dates, job titles, and person names
based on basic syntactic and morphological features.
Matching of named entities in step 7 is based on
matching rules that check the type of named entity,
and lists with aliases to identify entities as ?US?,
?United State?, and ?USA? as the same entity.
In step 8, similarity is computed for each pair
of words from the two sentences. Words that are
matched as entities in step 7 get a similarity value
of 1.0. For the rest of the entities and non-entity
words we use LCH (Leacock and Chodorow, 1998)
similarity, which is based on a shortest path between
the corresponding senses in WordNet. Since word
sense disambiguation is not used, we take the simi-
larity between the nearest senses of two words. For
cases when the WordNet-based similarity cannot be
obtained, a similarity based on the Levenshtein dis-
tance (Levenshtein, 1966) is used instead. It is nor-
malised by the length of the longest word in the pair.
For the STS?13 test data set, named entity matching
contributed to 4% of all matched word pairs; LCH
similarity to 61%, and Levenshtein distance to 35%.
In step 9, maximum weighted bipartite matching
is computed using the Hungarian Algorithm (Kuhn,
1955). Nodes in the bipartite graph represent words
from the sentences, and edges have weights that cor-
respond to similarities between tokens obtained in
step 8. Weighted bipartite matching finds the one-to-
one alignment that maximizes the sum of similarities
between aligned tokens. Total similarity normalised
by the number of words in both sentences is used as
the final sentence similarity measure.
3 Distributional Similarity
Our distributional similarity features use Random
Indexing (RI; Kanerva et al, 2000; Sahlgren, 2005),
also employed in STS?12 by Tovar et al (2012);
Sokolov (2012); Semeraro et al (2012). It is an
efficient method for modelling higher order co-
occurrence similarities among terms, comparable to
Latent Semantic Analysis (LSA; Deerwester et al,
1990). It incrementally builds a term co-occurrence
matrix of reduced dimensionality through the use of
a sliding window and fixed size index vectors used
for training context vectors, one per unique term.
A novel variant, which we have called ?Multi-
67
sense Random Indexing? (MSRI), inspired by
Reisinger and Mooney (2010), attempts to capture
one or more ?senses? per unique term in an unsu-
pervised manner, each sense represented as an indi-
vidual vector in the model. The method is similar to
classical sliding window RI, but each term can have
multiple context vectors (referred to as ?sense vec-
tors? here) which are updated individually. When
updating a term vector, instead of directly adding the
index vectors of the neighbouring terms in the win-
dow to its context vector, the system first computes a
separate window vector consisting of the sum of the
index vectors. Then cosine similarity is calculated
between the window vector and each of the term?s
sense vectors. Each similarity score is in turn com-
pared to a set similarity threshold: if no score ex-
ceeds the threshold, the sentence vector is added as
a new separate sense vector for the term; if exactly
one score is above the threshold, the window vector
is added to that sense vector; and if multiple scores
are above the threshold, all the involved senses are
merged into one sense vector, together with the win-
dow vector. This accomplishes an incremental clus-
tering of senses in an unsupervised manner while re-
taining the efficiency of classical RI.
As data for training the models we used the
CLEF 2004?2008 English corpus (approx. 130M
words). Our implementation of RI and MSRI is
based on JavaSDM (Hassel, 2004). For classical
RI, we used stopword removal (using a customised
versions of the English stoplist from the Lucene
project), window size of 4+4, dimensionality set to
1800, 4 non-zeros, and unweighted index vector in
the sliding window. For MSRI, we used a simi-
larity threshold of 0.2, a vector dimensionality of
800, a non-zero count of 4, and window size of
5+5. The index vectors in the sliding window were
shifted to create direction vectors (Sahlgren et al,
2008), and weighted by distance to the target term.
Rare senses with a frequency below 10 were ex-
cluded. Other sliding-window schemes, including
unweighted non-shifted vectors and Random Permu-
tation (Sahlgren et al, 2008), were tested, but none
outperformed the sliding-window schemes used.
Similarity between sentence pairs was calcu-
lated as the normalised maximal bipartite similar-
ity between term pairs in each sentence, resulting
in the following features: (1) MSRI-Centroid:
each term is represented as the sum of its sense
vectors; (2) MSRI-MaxSense: for each term
pair, the sense-pair with max similarity is used;
(3) MSRI-Context: for each term, its neigh-
bouring terms within a window of 2+2 is used as
context for picking a single, max similar, sense
from the target term to be used as its represen-
tation; (4) MSRI-HASenses: similarity between
two terms is computed by applying the Hungarian
Algorithm to all their possible sense pair mappings;
(5) RI-Avg: classical RI, each term is represented
as a single context vector; (6) RI-Hungarian:
similarity between two sentences is calculated us-
ing the Hungarian Algorithm. Alternatively, sen-
tence level similarity was computed as the cosine
similarity between sentence vectors composed of
their terms? vectors. The corresponding features
are (1) RI-SentVectors-Norm: sentence vec-
tors are created by summing their constituent terms
(i.e., context vectors), which have first been normal-
ized; (2) RI-SentVectors-TFIDF: same as be-
fore, but TF*IDF weights are added.
4 Deeper Semantic Relations
Two deep strategies were employed to accompany
the shallow-processed feature sets. Two existing
systems were used to provide the basis for these fea-
tures, namely the RelEx system (Fundel et al, 2007)
from the OpenCog initiative (Hart and Goertzel,
2008), and an in-house graph-edit distance system
developed for plagiarism detection (R?kenes, 2013).
RelEx outputs syntactic trees, dependency graphs,
and semantic frames as this one for the sentence
?Indian air force to buy 126 Rafale fighter jets?:
Commerce buy:Goods(buy,jet)
Entity:Entity(jet,jet)
Entity:Name(jet,Rafale)
Entity:Name(jet,fighter)
Possibilities:Event(hyp,buy)
Request:Addressee(air,you)
Request:Message(air,air)
Transitive action:Beneficiary(buy,jet)
Three features were extracted from this: first, if
there was an exact match of the frame found in s1
with s2; second, if there was a partial match until the
first argument (Commerce buy:Goods(buy);
and third if there was a match of the frame category
68
(Commerce buy:Goods).
In STS?12, Singh et al (2012) matched Universal
Networking Language (UNL) graphs against each
other by counting matches of relations and univer-
sal words, while Bhagwani et al (2012) calculated
WordNet-based word-level similarities and created
a weighted bipartite graph (see Section 2). The
method employed here instead looked at the graph
edit distance between dependency graphs obtained
with the Maltparser dependency parser (Nivre et al,
2006). Edit distance is the defined as the minimum
of the sum of the costs of the edit operations (in-
sertion, deletion and substitution of nodes) required
to transform one graph into the other. It is approx-
imated with a fast but suboptimal algorithm based
on bipartite graph matching through the Hungarian
algorithm (Riesen and Bunke, 2009).
5 Reused Features
The TakeLab ?simple? system (S?aric? et al, 2012) ob-
tained 3rd place in overall Pearson correlation and
1st for normalized Pearson in STS?12. The source
code1 was used to generate all its features, that is,
n-gram overlap, WordNet-augmented word overlap,
vector space sentence similarity, normalized differ-
ence, shallow NE similarity, numbers overlap, and
stock index features.2 This required the full LSA
vector space models, which were kindly provided
by the TakeLab team. The word counts required for
computing Information Content were obtained from
Google Books Ngrams.3
The DKPro system (Ba?r et al, 2012) obtained first
place in STS?12 with the second run. We used the
source code4 to generate features for the STS?12
and STS?13 data. Of the string-similarity features,
we reused the Longest Common Substring, Longest
Common Subsequence (with and without normaliza-
tion), and Greedy String Tiling measures. From the
character/word n-grams features, we used Charac-
ter n-grams (n = 2, 3, 4), Word n-grams by Con-
tainment w/o Stopwords (n = 1, 2), Word n-grams
1http://takelab.fer.hr/sts/
2We did not use content n-gram overlap or skip n-grams.
3http://storage.googleapis.com/books/
ngrams/books/datasetsv2.html, version 20120701,
with 468,491,999,592 words
4http://code.google.com/p/
dkpro-similarity-asl/
by Jaccard (n = 1, 3, 4), and Word n-grams by Jac-
card w/o Stopwords (n = 2, 4). Semantic similarity
measures include WordNet Similarity based on the
Resnik measure (two variants) and Explicit Seman-
tic Similarity based on WordNet, Wikipedia or Wik-
tionary. This means that we reused all features from
DKPro run 1 except for Distributional Thesaurus.
6 Systems
Our systems follow previous submissions to the STS
task (e.g., S?aric? et al, 2012; Banea et al, 2012) in
that feature values are extracted for each sentence
pair and combined with a gold standard score in or-
der to train a Support Vector Regressor on the result-
ing regression task. A postprocessing step guaran-
tees that all scores are in the [0, 5] range and equal 5
if the two sentences are identical. SVR has been
shown to be a powerful technique for predictive data
analysis when the primary goal is to approximate a
function, since the learning algorithm is applicable
to continuous classes. Hence support vector regres-
sion differs from support vector machine classifica-
tion where the goal rather is to take a binary deci-
sion. The key idea in SVR is to use a cost function
for building the model which tries to ignore noise in
training data (i.e., data which is too close to the pre-
diction), so that the produced model in essence only
depends on a more robust subset of the extracted fea-
tures.
Three systems were created using the supplied
annotated data based on Microsoft Research Para-
phrase and Video description corpora (MSRpar and
MSvid), statistical machine translation system out-
put (SMTeuroparl and SMTnews), and sense map-
pings between OntoNotes and WordNet (OnWN).
The first system (NTNU1) includes all TakeLab and
DKPro features plus the GateWordMatch feature
with the SVR in its default setting.5 The training
material consisted of all annotated data available,
except for the SMT test set, where it was limited to
SMTeuroparl and SMTnews. The NTNU2 system is
similar to NTNU1, except that the training material
for OnWN and FNWN excluded MSRvid and that
the SVR parameter C was set to 200. NTNU3 is
similar to NTNU1 except that all features available
are included.
5RBF kernel,  = 0.1, C = #samples, ? = 1#features
69
Data NTNU1 NTNU2 NTNU3
MSRpar 0.7262 0.7507 0.7221
MSRvid 0.8660 0.8882 0.8662
SMTeuroparl 0.5843 0.3386 0.5503
SMTnews 0.5840 0.5592 0.5306
OnWN 0.7503 0.6365 0.7200
mean 0.7022 0.6346 0.6779
Table 1: Correlation score on 2012 test data
7 Results
System performance is evaluated using the Pearson
product-moment correlation coefficient (r) between
the system scores and the human scores. Results on
the 2012 test data (i.e., 2013 development data) are
listed in Table 1. This basically shows that except
for the GateWordMatch, adding our other fea-
tures tends to give slightly lower scores (cf. NTNU1
vs NTNU3). In addition, the table illustrates that op-
timizing the SVR according to cross-validated grid
search on 2012 training data (here C = 200), rarely
pays off when testing on unseen data (cf. NTNU1
vs NTNU2).
Table 2 shows the official results on the test data.
These are generally in agreement with the scores on
the development data, although substantially lower.
Our systems did particularly well on SMT, holding
first and second position, reasonably good on head-
lines, but not so well on the ontology alignment data,
resulting in overall 9th (NTNU1) and 12th (NTNU3)
system positions (5th best team). Table 3 lists the
correlation score and rank of the ten best individual
features per STS?13 test data set, and those among
the top-20 overall, resulting from linear regression
on a single feature. Features in boldface are gen-
uinely new (i.e., described in Sections 2?4).
Overall the character n-gram features are the most
informative, particularly for HeadLine and SMT.
The reason may be that these not only capture word
overlap (Ahn, 2011), but also inflectional forms and
spelling variants.
The (weighted) distributional similarity features
based on NYT are important for HeadLine and SMT,
which obviously contain sentence pairs from the
news genre, whereas the Wikipedia based feature is
more important for OnWN and FNWN. WordNet-
based measures are highly relevant too, with variants
NTNU1 NTNU2 NTNU3
Data r n r n r n
Head 0.7279 11 0.5909 59 0.7274 12
OnWN 0.5952 31 0.1634 86 0.5882 32
FNWN 0.3215 45 0.3650 27 0.3115 49
SMT 0.4015 2 0.3786 9 0.4035 1
mean 0.5519 9 0.3946 68 0.5498 12
Table 2: Correlation score and rank on 2013 test data
relying on path length outperforming those based on
Resnik similarity, except for SMT.
As is to be expected, basic word and lemma uni-
gram overlap prove to be informative, with overall
unweighted variants resulting in higher correlation.
Somewhat surprisingly, higher order n-gram over-
laps (n > 1) seem to be less relevant. Longest com-
mon subsequence and substring appear to work par-
ticularly well for OnWN and FNWN, respectively.
GateWordMatch is highly relevant too, in
agreement with earlier results on the development
data. Although treated as a single feature, it is ac-
tually a combination of similarity features where an
appropriate feature is selected for each word pair.
This ?vertical? way of combining features can po-
tentially provide a more fine-grained feature selec-
tion, resulting in less noise. Indeed, if two words are
matching as named entities or as close synonyms,
less precise types of features such as character-based
and data-driven similarity should not dominate the
overall similarity score.
It is interesting to find that MSRI outper-
forms both classical RI and ESA (Gabrilovich and
Markovitch, 2007) on this task. Still, the more ad-
vanced features, such as MSRI-Context, gave in-
ferior results compared to MSRI-Centroid. This
suggests that more research on MSRI is needed
to understand how both training and retrieval can
be optimised. Also, LSA-based features (see
tl.weight-dist-sim-wiki) achieve better
results than both MSRI, RI and ESA. Then again,
larger corpora were used for training the LSA mod-
els. RI has been shown to be comparable to LSA
(Karlgren and Sahlgren, 2001), and since a relatively
small corpus was used for training the RI/MSRI
models, there are reasons to believe that better
scores can be achieved by both RI- and MSRI-based
features by using more training data.
70
HeadLine OnWN FNWN SMT Mean
Features r n r n r n r n r n
CharacterNGramMeasure-3 0.72 2 0.39 2 0.44 3 0.70 1 0.56 1
CharacterNGramMeasure-4 0.69 3 0.38 5 0.45 2 0.67 6 0.55 2
CharacterNGramMeasure-2 0.73 1 0.37 9 0.34 10 0.69 2 0.53 3
tl.weight-dist-sim-wiki 0.58 14 0.39 3 0.45 1 0.67 5 0.52 4
tl.wn-sim-lem 0.69 4 0.40 1 0.41 5 0.59 10 0.52 5
GateWordMatch 0.67 8 0.37 11 0.34 11 0.60 9 0.50 6
tl.dist-sim-nyt 0.69 5 0.34 28 0.26 23 0.65 8 0.49 7
tl.n-gram-match-lem-1 0.68 6 0.36 16 0.37 8 0.51 14 0.48 8
tl.weight-dist-sim-nyt 0.57 17 0.37 14 0.29 18 0.66 7 0.47 9
tl.n-gram-match-lc-1 0.68 7 0.37 10 0.32 13 0.50 17 0.47 10
MCS06-Resnik-WordNet 0.49 26 0.36 22 0.28 19 0.68 3 0.45 11
TWSI-Resnik-WordNet 0.49 27 0.36 23 0.28 20 0.68 4 0.45 12
tl.weight-word-match-lem 0.56 18 0.37 16 0.37 7 0.50 16 0.45 13
MSRI-Centroid 0.60 13 0.36 17 0.37 9 0.45 19 0.45 14
tl.weight-word-match-olc 0.56 19 0.38 8 0.32 12 0.51 15 0.44 15
MSRI-MaxSense 0.58 15 0.36 15 0.31 14 0.45 20 0.42 16
GreedyStringTiling-3 0.67 9 0.38 6 0.31 15 0.34 29 0.43 17
ESA-Wikipedia 0.50 25 0.30 38 0.32 14 0.54 12 0.42 18
WordNGramJaccard-1 0.64 10 0.37 12 0.25 25 0.33 30 0.40 19
WordNGramContainment-1-stopword 0.64 25 0.38 7 0.25 24 0.32 31 0.40 20
RI-Hungarian 0.58 16 0.33 31 0.10 34 0.42 22 0.36 24
RI-AvgTermTerm 0.56 20 0.33 32 0.11 33 0.37 28 0.34 25
LongestCommonSubstring 0.40 29 0.30 39 0.42 4 0.37 27 0.37 26
ESA-WordNet 0.11 43 0.30 40 0.41 6 0.49 18 0.33 29
LongestCommonSubsequenceNorm 0.53 21 0.39 4 0.19 27 0.18 37 0.32 30
MultisenseRI-ContextTermTerm 0.39 31 0.33 33 0.28 21 0.15 38 0.29 33
MultisenseRI-HASensesTermTerm 0.39 32 0.33 34 0.28 22 0.15 39 0.29 34
RI-SentVectors-Norm 0.34 35 0.35 26 -0.01 51 0.24 35 0.23 39
RelationSimilarity 0.31 39 0.35 27 0.24 26 0.02 41 0.23 40
RI-SentVectors-TFIDF 0.27 40 0.15 50 0.08 40 0.23 36 0.18 41
GraphEditDistance 0.33 38 0.25 46 0.13 31 -0.11 49 0.15 42
Table 3: Correlation score and rank of the best features
8 Conclusion and Future Work
The NTNU system can be regarded as continuation
of the most successful systems from the STS?12
shared task, combining shallow textual, distribu-
tional and knowledge-based features into a support
vector regression model. It reuses features from the
TakeLab and DKPro systems, resulting in a very
strong baseline.
Adding new features to further improve
performance turned out to be hard: only
GateWordMatch yielded improved perfor-
mance. Similarity features based on both classical
and innovative variants of Random Indexing were
shown to correlate with semantic textual similarity,
but did not complement the existing distributional
features. Likewise, features designed to reveal
deeper syntactic (graph edit distance) and semantic
relations (RelEx) did not add to the score.
As future work, we would aim to explore a
vertical feature composition approach similar to
GateWordMatch and contrast it with the ?flat?
composition currently used in our systems.
Acknowledgements
Thanks to TakeLab for source code of their ?simple?
system and the full-scale LSA models. Thanks to the
team from Ubiquitous Knowledge Processing Lab
for source code of their DKPro Similarity system.
71
References
Agirre, E., Cer, D., Diab, M., and Gonzalez-Agirre,
A. (2012). SemEval-2012 Task 6: A pilot on se-
mantic textual similarity. In *SEM (2012), pages
385?393.
Agirre, E., Cer, D., Diab, M., Gonzalez-Agirre, A.,
and Guo, W. (2013). *SEM 2013 Shared Task:
Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second
Joint Conference on Lexical and Computational
Semantics. Association for Computational Lin-
guistics.
Ahn, C. S. (2011). Automatically detecting authors?
native language. PhD thesis, Monterey, Califor-
nia. Naval Postgraduate School.
Banea, C., Hassan, S., Mohler, M., and Mihalcea, R.
(2012). UNT: a supervised synergistic approach
to semantic text similarity. In *SEM (2012),
pages 635?642.
Ba?r, D., Biemann, C., Gurevych, I., and Zesch, T.
(2012). UKP: Computing semantic textual sim-
ilarity by combining multiple content similarity
measures. In *SEM (2012), pages 435?440.
Bhagwani, S., Satapathy, S., and Karnick, H. (2012).
sranjans : Semantic textual similarity using maxi-
mal weighted bipartite graph matching. In *SEM
2012: The First Joint Conference on Lexical and
Computational Semantics ? Volume 1: Proceed-
ings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation (Sem-
Eval 2012), pages 579?585, Montre?al, Canada.
Association for Computational Linguistics.
Cunningham, H., Maynard, D., Bontcheva, K., and
Tablan, V. (2002). GATE: A framework and
graphical development environment for robust
NLP tools and applications. In Proceedings of the
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 168?175, Philadel-
phia, Pennsylvania. ACL.
Deerwester, S., Dumais, S., Furnas, G., Landauer,
T., and Harshman, R. (1990). Indexing by latent
semantic analysis. Journal of the American Soci-
ety for Information Science, 41(6):391?407.
Fundel, K., Ku?ffner, R., and Zimmer, R. (2007).
RelEx - Relation extraction using dependency
parse trees. Bioinformatics, 23(3):365?371.
Gabrilovich, E. and Markovitch, S. (2007). Comput-
ing semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of The
Twentieth International Joint Conference for Ar-
tificial Intelligence., pages 1606?1611.
Hart, D. and Goertzel, B. (2008). Opencog: A soft-
ware framework for integrative artificial general
intelligence. In Proceedings of the 2008 confer-
ence on Artificial General Intelligence 2008: Pro-
ceedings of the First AGI Conference, pages 468?
472, Amsterdam, The Netherlands, The Nether-
lands. IOS Press.
Hassel, M. (2004). JavaSDM package.
Kanerva, P., Kristoferson, J., and Holst, A. (2000).
Random indexing of text samples for latent se-
mantic analysis. In Gleitman, L. and Josh, A.,
editors, Proceedings of the 22nd Annual Confer-
ence of the Cognitive Science Society, page 1036.
Erlbaum.
Karlgren, J. and Sahlgren, M. (2001). From Words
to Understanding. In Uesaka, Y., Kanerva, P., and
Asoh, H., editors, Foundations of real-world in-
telligence, chapter 26, pages 294?311. Stanford:
CSLI Publications.
Karnick, H., Satapathy, S., and Bhagwani, S. (2012).
sranjans: Semantic textual similarity using max-
imal bipartite graph matching. In *SEM (2012),
pages 579?585.
Kuhn, H. (1955). The Hungarian method for the as-
signment problem. Naval research logistics quar-
terly, 2:83?97.
Leacock, C. and Chodorow, M. (1998). Combin-
ing local context and WordNet similarity for word
sense identification. WordNet: An electronic lexi-
cal . . . .
Levenshtein, V. I. (1966). Binary codes capable of
correcting deletions, insertions and reversals. So-
viet Physics Doklady, 10(8):707?710.
Nivre, J., Hall, J., and Nilsson, J. (2006). Malt-
parser: A data-driven parser-generator for depen-
dency parsing. In In Proc. of LREC-2006, pages
2216?2219.
72
Reisinger, J. and Mooney, R. (2010). Multi-
prototype vector-space models of word meaning.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
number June, pages 109?117.
Riesen, K. and Bunke, H. (2009). Approximate
graph edit distance computation by means of bi-
partite graph matching. Image and Vision Com-
puting, 27(7):950?959.
R?kenes, H. (2013). Graph-Edit Distance Applied to
the Task of Detecting Plagiarism. Master?s the-
sis, Norwegian University of Science and Tech-
nology.
Sahlgren, M. (2005). An introduction to random in-
dexing. In Methods and Applications of Seman-
tic Indexing Workshop at the 7th International
Conference on Terminology and Knowledge En-
gineering, TKE, volume 5.
Sahlgren, M., Holst, A., and Kanerva, P. (2008). Per-
mutations as a Means to Encode Order in Word
Space. Proceedings of the 30th Conference of the
Cognitive Science Society.
S?aric?, F., Glavas?, G., Karan, M., S?najder, J., and
Bas?ic?, B. D. (2012). TakeLab: systems for mea-
suring semantic text similarity. In *SEM (2012),
pages 441?448.
*SEM (2012). Proceedings of the First Joint Con-
ference on Lexical and Computational Seman-
tics (*SEM), volume 2: Proceedings of the Sixth
International Workshop on Semantic Evaluation,
Montreal, Canada. Association for Computational
Linguistics.
Semeraro, G., Aldo, B., and Orabona, V. E. (2012).
UNIBA: Distributional semantics for textual sim-
ilarity. In *SEM (2012), pages 591?596.
Singh, J., Bhattacharya, A., and Bhattacharyya, P.
(2012). janardhan: Semantic textual similarity us-
ing universal networking language graph match-
ing. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics ? Vol-
ume 1: Proceedings of the main conference and
the shared task, and Volume 2: Proceedings of the
Sixth International Workshop on Semantic Evalu-
ation (SemEval 2012), pages 662?666, Montre?al,
Canada. Association for Computational Linguis-
tics.
Sokolov, A. (2012). LIMSI: learning semantic simi-
larity by selecting random word subsets. In *SEM
(2012), pages 543?546.
Tovar, M., Reyes, J., and Montes, A. (2012). BUAP:
a first approximation to relational similarity mea-
suring. In *SEM (2012), pages 502?505.
Vapnik, V., Golowich, S. E., and Smola, A. (1997).
Support vector method for function approxima-
tion, regression estimation, and signal process-
ing. In Mozer, M. C., Jordan, M. I., and Petsche,
T., editors, Advances in Neural Information Pro-
cessing Systems, volume 9, pages 281?287. MIT
Press, Cambridge, Massachusetts.
73
Proceedings of the TextGraphs-8 Workshop, pages 61?69,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Automatic Extraction of Reasoning Chains from Textual Reports
Gleb Sizov and Pinar O?ztu?rk
Department of Computer Science
Norwegian University of Science and Technology
Trondheim, Norway
{sizov, pinar}@idi.ntnu.no
Abstract
Many organizations possess large collections
of textual reports that document how a prob-
lem is solved or analysed, e.g. medical pa-
tient records, industrial accident reports, law-
suit records and investigation reports. Ef-
fective use of expert knowledge contained in
these reports may greatly increase productiv-
ity of the organization. In this article, we pro-
pose a method for automatic extraction of rea-
soning chains that contain information used
by the author of a report to analyse the prob-
lem at hand. For this purpose, we devel-
oped a graph-based text representation that
makes the relations between textual units ex-
plicit. This representation is acquired auto-
matically from a report using natural language
processing tools including syntactic and dis-
course parsers. When applied to aviation in-
vestigation reports, our method generates rea-
soning chains that reveal the connection be-
tween initial information about the aircraft in-
cident and its causes.
1 Introduction
Success of an organization is highly depend on its
knowledge which is generated and accumulated by
its employees over years. However, unless made
explicit and shareable, organizations have the risk
of losing this knowledge because employees may
change jobs at any time, or retire. It is common to
document such experience, also for evidence pur-
pose in case of legal problems and governmental
regulations. Consequently, many companies and in-
stitutions have large collections of textual reports
documenting their organizational experience on a
particular task, a client or a problem. Industrial in-
cident reports, law suit reports, electronic patient
records and investigation reports are the most intu-
itive examples. The effective use of the knowledge
contained in these reports can save substantial time
and resources. For example, incident reports can be
used to identify possible risks and prevent future in-
cidents, law suit reports constitute precedences for
future cases, and patient records might help to diag-
nose and find an appropriate treatment for a patient
with similar symptoms.
Existing search engines are effective at finding
relevant documents. However, after retrieval, inter-
pretation and reasoning with knowledge contained
in these documents is still done manually with no
computer assistance other than basic keyword-based
search. In our research, we are aiming to develop
methods that will assist users in interpretation and
reasoning with knowledge contained in textual re-
ports. The rationale behind our approach is that ex-
perts? line of reasoning for understanding and solv-
ing a problem can be reused for the analysis of a sim-
ilar problem. Reasoning knowledge can be extracted
from a report by analysing its syntactic and rhetor-
ical structure. When extracted and represented in a
computer-friendly way, this knowledge can be used
for automatic and computer-assisted reasoning.
In this article, we propose a method for auto-
matic extraction of reasoning chains from textual re-
ports. A reasoning chain is defined as a sequence
of transitions from one piece of information to an-
other starting from the problem description and lead-
ing to its solution. Our model is based on a novel
61
graph-based text representation, called Text Reason-
ing Network (TRN), which decomposes a document
into text units, discovers the connections between
these text units and makes them explicit. TRN is
acquired automatically from text using natural lan-
guage processing tools including a syntactic parser,
a discourse parser and a semantic similarity mea-
sure.
We tested our method on aviation investigation re-
ports from Transportation Board of Canada. These
reports are produced as a result of investigation of
aircraft incidents where experts are assigned the task
of analysing an incident and writing down their un-
derstanding of what and why it happened. Reason-
ing chains extracted from the investigation reports
reveal the connection between initial information
about the incident and its causes. When visualized,
this connection can be interpreted and analysed.
The rest of the paper is organized as follows. Sec-
tion 2 provides an overview of the related research.
In section 3, TRN representation is described. Gen-
eration of reasoning chains from aviation investiga-
tion reports is explained in section 4. Interesting ex-
amples of reasoning chains generated by our system
are demonstrated and analysed in section 5. In sec-
tion 6, we discuss the results and elaborate on future
work.
2 Related Work
To our knowledge, automatic extraction of reason-
ing chains from text has not been attempted before.
However, we were able to find several papers deal-
ing with text processing tasks relevant to our goal
that make use of graph-based representations.
The work done by Pechsiri and Piriyakul (2010)
is focused on extraction of causal relations from text
and construction of an explanation graph. The rela-
tions are extracted between clauses based on mined
cause-effect verb pairs, e.g. ?If the [aphids in-
fest rice pants], [the leaves will become yellow].?
with cause verb ?infest? and effect verb ?become?.
The explanation graph is constructed directly from
the extracted relations, which is different from our
approach where reasoning chains are extracted as
paths from the graph-based representation of a re-
port. There is only one example of the explanation
graph presented in the paper. This graph is gener-
ated from plant disease technical papers capturing
part of the domain knowledge. Manual inspection
of the graph revealed few mistakes.
An interesting research was conducted by Be-
rant (2012) for his PhD thesis. Unlike Pechsiri and
Piriyakul (2010), his approach relies on textual en-
tailment instead of causal relations. Entailment re-
lations are obtained between propositional patterns,
e.g. (X
subj
??? desire
obj
??? Y,X
subj
??? want
obj
???
Y ), using a classifier trained on distributional simi-
larity features. The focus of their work is to exploit
transitive nature of entailment relations in learning
of entailment graphs. As an application, the au-
thors developed a novel text exploration tool, where
a user can drill down/up from one statement to an-
other through the entailment graph. Entailment re-
lations alone, i.e. text
entails
????? hypothesis, are not
sufficient for extraction of reasoning chains because
the hypothesis often contains the information which
is already present in the text, making it impossible
to create a path from the problem description to the
solution. However, when combined with other types
of relations they might be useful for out task.
In the paper by Jin and Srihari (2007), authors
generate and evaluate evidence trails between con-
cepts across documents. An evidence trail is a
path connecting two concepts in a graph where
nodes are concepts that correspond to named enti-
ties and noun phrases participating in subject-verb-
object constructs. Three variations of the represen-
tation are tested, each with edges capturing differ-
ent types of information. In the first one, edges
capture word order in text. The second one cap-
tures co-occurrence of concepts. The third varia-
tion contains edges with weights corresponding to
the similarity between contexts of the concepts. Vec-
tor space model is used to represent and measure the
similarity between the contexts. The concept-based
representation is substantially different from TRN
but the idea of finding a shortest path between nodes
and use it as the evidence is similar. There is one
example of the evidence trail shown in the paper:
?bush - afghanistan - qaeda - bin ladin?, which re-
veals the connection between topics rather than con-
crete pieces of information.
A graph-based representation similar to (Jin and
Srihari, 2007) have been applied for Textual Case-
62
The aircraft stalled at a higher-than-normal airspeed due to accumulated ice on critical surfaces.
aircraft stalled at a higher-than-normal airspeedaccumulated ice on critical surfaces ice accumulation on critical surfaces
Ice accumulation on critical surfaces was possible.
ice accumulation critical surfaces 
Similar
accumulated ice
Cause
stalled at a higher-than-normal airspeedaircraft
higher-than-normal airspeed
Contains Contains Contains
Contains Contains Contains Contains
Contains
ContainsContains
Similar
Analysis
Contains Contains
Factual Information
Figure 1: Two sentences represented as Text Reasoning Network.
Based Reasoning (TCBR) (Lenz and Burkhard,
1996; Cunningham et al, 2004), a task of automat-
ically solving a new problem given a collection of
reports describing previous problems with solutions.
The dataset we use in our research can be considered
a TCBR dataset, since each report contains a prob-
lem description and a solution part. Problem-solving
based on knowledge represented in textual form is
a tough task and in practice TCBR approaches ei-
ther do classification of a problem into predefined
classes or retrieve a report that describes a problem
similar to a query problem. In the later case, in-
formation retrieval methods are utilized, including
graph-based representations for computing similar-
ity between documents as it is done by Cunning-
ham et al (2004). Their representation, inspired by
Schenker et al (2003), contains terms as nodes and
edges connecting the adjacent terms in text. Nodes
and edges are labelled with the frequency of their
appearance and with the section where they appear,
i.e. title or text. Infrequent terms are removed. In
addition, domain knowledge is introduced as a list
of important domain terms that are preserved even if
their frequency is low. The similarity used is based
on maximum common sub-graph. When tested on
summary documents from a law firm handling in-
surance cases, the results show improvement over
vector space model representations.
3 Text Reasoning Network
In our approach, a reasoning chain is extracted as a
path from the graph-based text representation. An
appropriate representation is crucial because chains
extracted from it are only as good as the represen-
tation itself. In this section we introduce a novel
graph-based text representation, called Text Reason-
ing Network (TRN), which is a graph with two types
of nodes: text nodes and section nodes; and three
types of edges: structural, similarity and causal. Fig-
ure 1 shows two sentences from a report represented
as TRN with section nodes on the top and all the text
nodes below them. The representation is acquired
automatically from text by the following procedure:
(1) syntax trees obtained from a syntactic parser are
added to the graph, (2) section nodes are attached to
sentence nodes, (3) similarity edges are added be-
tween similar text nodes, (4) cause relations identi-
fied by a discourse parser are added. The rest of this
section provides details on the structure of TRN and
methods used to generate it from text.
3.1 Nodes and Structural Relations
We are aiming to extract chains that capture the in-
formation used by the author of a report to reason
about the problem at hand. Graph-based text rep-
resentations described in section 2 use individual
terms or short phrases as nodes. Small text units
such as these are unable to capture sufficient infor-
mation for our purpose. Another popular choice
for a node in a textual graph is a sentence, which
captures a more or less complete piece of informa-
tion and is easy to interpret. However, a complex
sentence may contain several pieces of information
where only one is used in a reasoning chain.
A syntax tree provides a natural decomposition of
63
a sentence into its constituents. Since it is hard to
determine beforehand the size of constituents that
would be useful in a reasoning chain, we decided to
incorporate all the S (sentence, clause), NP (noun
phrase) and VP (verb phrase) nodes from syntax
trees produced by Stanford Parser (Klein and Man-
ning, 2003). These nodes are referred to as text
nodes. In addition to text nodes, the structure of a
syntax tree is also retained by adding structural re-
lations Contains and PartOf to TRN that correspond
to relations between parent and children text units
in the syntax tree. Figure 1 shows text nodes ex-
tracted from two sentences along with Contains re-
lations between them. PartOf edges are not shown
to avoid the clutter.
Graphs extracted from different sentences in a
document are combined into one. Each node has
a unique identity that is composed of a sequence of
stemmed words with stopwords removed. The ma-
jor implication of this is that if two sentences over-
lap, they will share one or several nodes, e.g. node
?critical surfaces? in figure 1.
In addition to text nodes, there are also section
nodes corresponding to parts of a document, e.g.
?Factual Information? and ?Analysis? nodes in fig-
ure 1. These nodes capture the structure of a doc-
ument. Text nodes containing a complete sentence,
also referred to as sentence nodes, are attached to
section nodes by structural relations.
3.2 Similarity Relations
In addition to structural relations, text nodes are con-
nected through similarity relations. To obtain these
relations, a similarity value is computed for each
pair of text nodes of the same category (S, VP, NP)
that are not in the same sentence. Similar edges are
added to the graph for node pairs with the similarity
value above a predefined threshold, e.g. nodes ?ice
accumulation? and ?accumulated ice? in figure 1.
Our similarity measure finds one-to-one align-
ment of words from two text units to maximize the
total similarity between them. For words we com-
pute LCH (Leacock et al, 1998) similarity, based on
a shortest path between the corresponding senses in
WordNet. A complete bipartite graph is constructed
and the maximum weighted bipartite matching is
computed using the Hungarian Algorithm (Kuhn,
1955). Nodes in this bipartite graph represent words
from the text units while edges have weights that
correspond to similarities between words. Maxi-
mum weighted bipartite matching finds a one-to-one
alignment that maximizes the sum of similarities be-
tween aligned words. This sum is normalized to lie
between 0 and 1 and is used as the final value for the
similarity between text units. If the value is higher
or equal 0.6 a Similar edge is added between the cor-
responding nodes.
3.3 Causal Relations
Causal relations are essential for analysis and deci-
sion making allowing inference about past and fu-
ture events (Garcia-Retamero et al, 2007). As seen
in (Pechsiri and Piriyakul, 2010) causal graphs ex-
tracted from domain-specific documents provide a
powerful representation of expert knowledge.
State-of-the-art techniques for extraction of
causal relations from text use automatic classi-
fiers trained on lexical features to recognize rela-
tions between subject and object in a clause or be-
tween verbs of different clauses (Chang and Choi,
2005; Bethard and Martin, 2008). Causal relations
are among discourse relations defined by Rhetori-
cal Structure Theory (Mann and Thompson, 1988).
Therefore, a discourse parser can be used to obtain
them from text. The advantage of this approach is
that a discourse parser recognizes relations between
larger text units. Few discourse parsers are avail-
able that can parse an entire document. For our work
we used PDTB-Styled End-to-End Discourse Parser
by Lin et al (2010), which makes use of machine
learning techniques trained on Penn Discourse Tree-
bank (PDTB) (Prasad et al, 2008). Cause relations
identified by the parser are added to TRN graph by
mapping arguments of the relations to text nodes and
then adding Cause edges between them.
4 Generation of Reasoning Chains from
Aviation Accident Reports
Generation of a reasoning chain from a report is a
three-stage process: (1) a report is converted from
text to a TRN graph, (2) given a start and an end
node, several paths are extracted from the graph, (3)
paths are combined, post-processed and visualized.
64
4.1 Dataset
In our work we use aviation investigation reports
from Transportation Safety Board of Canada1. Each
report in this collection documents an aircraft inci-
dent and contains the following sections: (1) ?Sum-
mary? is a brief description of the incident, (2) ?Fac-
tual Information? (further referred to as ?Factual?)
contains details about the aircraft, pilot, weather
conditions, terrain and communication with con-
trollers (3) ?Analysis? is a discussion of the incident
with the purpose to explain it based on the informa-
tion presented in the previous section, (4) ?Findings
as to Causes and Contributing Factor? (further re-
ferred to as ?Causes?) is a brief enumeration of find-
ings that most likely caused the incident.
The reports were downloaded from Transporta-
tion Board of Canada website as html documents.
Text and structure were extracted from html using
a custom Java component developed based on man-
ual analysis of the html source. Preprocessing steps
including tokenization, sentence splitting and part-
of-speech tagging were accomplished using ANNIE
components in GATE NLP platform (Cunningham
et al, 2002).
4.2 Extraction of Reasoning Chains
We define a reasoning chain as the shortest path
through a TRN representation of a report starting
from a sentence in ?Summary? and ending at one
of the sentences in ?Causes? section. The rationale
behind this decision is to reveal the author?s reason-
ing line starting from the initial information about
the incident contained in ?Summary? and leading to
incident causes in ?Causes? section. Hence, the path
finding process is constrained to follow the direction
from ?Summary? to ?Causes? through ?Factual? and
?Analysis? sections. The reasoning chain path with
constraints is defined by the following context-free
grammar in Backus-Naur Form (optional items in
[...]):
?path? ::= ?summary-path? [?edge? ?factual-path?]
[?edge? ?analysis-path?] ?edge? ?causes-path?
?summary-path? ::= ?summary-node?
| ?summary-node? ?contains-edge? ?summary-path?
1Aviation Investigation Reports are available at http://
goo.gl/k9mMV
?factual-path? ::= ?factual-node?
| ?factual-node? ?edge? ?factual-path?
?analysis-path? ::= ?analysis-node?
| ?analysis-node? ?edge? ?analysis-path?
?causes-path? ::= ?causes-node?
| ?causes-node? ?partof-edge? ?causes-path?
?edge? ::= ?partof-edge?
| ?contains-edge?
| ?similar-edge?
| ?cause-edge?
Several paths are obtained for each ?Summary?
sentence, each of which starting at one of the text
nodes contained in the sentence. These paths are
then combined into a reasoning graph. Before visu-
alization, a post-processing algorithm is applied to
make the reasoning graph more compact. The al-
gorithm collapses a sequence of structural edges of
the same type into a single edge, e.g. A
contains
??????
B
contains
?????? C is converted into A
contains
?????? C if
there is no other edge attached to B. The com-
pressed graph (shown in figures 2, 3 and 4) is visual-
ized using JGraphX library with hierarchical layout
for automatic positioning of nodes.
5 Examples and Analysis
In this section we analyse three reasoning graphs
generated by our system. These graphs were se-
lected mainly because of their compact size and ease
of interpretation even for someone who is not an avi-
ation expert. Every chain starts with a sentence from
?Summary? on the top of the figure and ends with
one or several sentences in ?Causes? on the bottom.
For each node a contained text unit is displayed, fol-
lowed by one or several letters in parenthesis indicat-
ing which section of the report this text node is ob-
served in: S - ?Summary?, F - ?Factual?, A - ?Anal-
ysis?, C - ?Causes?.
Figure 2 shows the reasoning graph with one
branch expressing that the captain?s focus on ?set-
ting climb power? and the ?landing gear? prevented
him from paying attention to the ?aircraft altitude?
so the ?sink rate was undetected and aircraft struck
the ground?. The start and the end sentences are not
similar and it is the sentence from the ?Analysis?
section that connects these two.
65
Figure 2: A reasoning graph from report A05O0225
(available at goo.gl/SZpTS)
The graph in figure 3 has two branches. The left
branch directly points to a sentence with ?the aux-
iliary fuel pump? but it does not explain ?a poor
electrical connection?. The right branch, however, is
longer and goes from ?switched fuel tank? to ?fuel
flow? and then to ?fuel pressure?, which is part of
a sentence in the ?Cause? section that includes this
text segment: ?reduction of fuel pressure, preventing
normal engine operation?.
The graph in figure 4 contains two branches
as well. The left branch picks up the location
of the flight ?Deer Lake? which relates to ?icing
conditions? although the text node suggesting ?a
lower altitude was requested to remain clear of ic-
ing conditions? makes this branch incoherent. The
right branch provides a connection between ?Provin-
cial Airlines Limited? and ?no requirement? in
their ?standard operating procedures? for a ?method
for ensuring the correct selection of AFCS climb
modes?. The chain goes through ?an inappropriate
AFCS mode? providing a good idea of the incident
cause.
Reasoning chains extracted by the system provide
a brief overview of the authors? reasoning line show-
ing how a basic information about the incident is
connected to its causes. However, some chains are
less informative than others (left branch in figure 3)
or incoherent (left branch of 4). In the former case
the chain could be made more informative if the sys-
tem will be queried to find evidence for ?poor elec-
trical connection? in addition to ?the auxiliary fuel
pump?. In the latter case, the chain becomes inco-
herent because ?icing conditions? is used in differ-
ent contexts where the first sentence states the lack
of ?icing conditions? and the second the presence of
?icing conditions?. It is possible to account for this
inconsistency by introducing a preference for larger
text units capturing more context or by recognizing
negations/absence.
6 Conclusion and Future Work
This paper presents a method for extraction of rea-
soning chains from textual reports. The method is
based on a graph-based text representation that cap-
tures both the structure and the content of the re-
port. Extracted reasoning chains provide a conve-
nient way to visualize information used by a domain
expert to reason about causes of an aircraft incident.
It may help in analysis of future incidents and opens
the possibility for automatic or computer-assisted
analysis. The methods can be adapted to other do-
mains and applications by defining appropriate start
nodes, end nodes and constraints like it is done in
section 4.2.
Extraction of reasoning chains is a new task and
there are yet no evaluation measures available. One
of the primary goals for our future work is to de-
velop a formal evaluation procedure for this task.
An intrinsic evaluation will require manually con-
structed reasoning chains as the gold standard to
compare the automatically extracted ones with. For
the extrinsic evaluation, reasoning chains can be
used in TCBR task for solution retrieval and eval-
uated with TCBR evaluation measures (Raghunan-
dan et al, 2008; Adeyanju et al, 2010). We also
plan to continue our work on the representation by
adding new types of relations to TRN and on the rea-
soning chain extraction algorithm by adapting flow
networks instead of shortest path for extraction of
reasoning chains.
66
Figure 3: A reasoning graph from report A05O0146 (available at goo.gl/MPMIq)
67
Figure 4: A reasoning graph from report A05A0059 (available at goo.gl/u1CXI)
68
References
Ibrahim Adeyanju, Nirmalie Wiratunga, Robert Lothian,
and Susan Craw. 2010. Applying machine translation
evaluation techniques to textual cbr. In Case-Based
Reasoning. Research and Development, pages 21?35.
Springer.
Jonathan Berant. 2012. Global Learning of Textual En-
tailment Graphs. Ph.D. thesis, Tel Aviv University.
Steven Bethard and James H Martin. 2008. Learning
semantic links from a corpus of parallel temporal and
causal relations. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics on Human Language Technologies: Short Papers,
pages 177?180. Association for Computational Lin-
guistics.
Du-Seong Chang and Key-Sun Choi. 2005. Causal rela-
tion extraction using cue phrase and lexical pair prob-
abilities. In Natural Language Processing?IJCNLP
2004, pages 61?70. Springer.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. Gate:
an architecture for development of robust hlt appli-
cations. In Proceedings of 40th Annual Meeting of
the Association for Computational Linguistics, pages
168?175, Philadelphia, Pennsylvania, USA, July.
Association for Computational Linguistics.
Colleen Cunningham, Rosina Weber, Jason M Proctor,
Caleb Fowler, and Michael Murphy. 2004. Inves-
tigating graphs in textual case-based reasoning. In
Advances in Case-Based Reasoning, pages 573?586.
Springer.
Rocio Garcia-Retamero, Annika Wallin, and Anja Dieck-
mann. 2007. Does causal knowledge help us be faster
and more frugal in our decisions? Memory & cogni-
tion, 35(6):1399?1409.
Wei Jin and Rohini K. Srihari. 2007. Graph-based text
representation and knowledge discovery. Proceedings
of the 2007 ACM symposium on Applied computing -
SAC ?07, page 807.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Associ-
ation for Computational Linguistics.
Harold W Kuhn. 1955. The hungarian method for the as-
signment problem. Naval research logistics quarterly,
2(1-2):83?97.
Claudia Leacock, George A Miller, and Martin
Chodorow. 1998. Using corpus statistics and word-
net relations for sense identification. Computational
Linguistics, 24(1):147?165.
Mario Lenz and Hans-Dieter Burkhard. 1996. Case re-
trieval nets: Basic ideas and extensions. In KI-96:
Advances in Artificial Intelligence, pages 227?239.
Springer.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A
pdtb-styled end-to-end discourse parser. Technical re-
port, Cambridge Univ Press.
William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Chaveevan Pechsiri and Rapepun Piriyakul. 2010.
Explanation knowledge graph construction through
causality extraction from texts. Journal of Computer
Science and Technology, 25(5):1055?1070.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bonnie L
Webber. 2008. The penn discourse treebank 2.0. In
LREC. Citeseer.
M. A. Raghunandan, Nirmalie Wiratunga, Sutanu
Chakraborti, Stewart Massie, and Deepak Khemani.
2008. Evaluation measures for tcbr systems. In
Advances in Case-Based Reasoning, pages 444?458.
Springer.
A. Schenker, M. Last, H. Bunke, and A. Kandel. 2003.
Clustering of web documents using a graph model.
Web Document Analysis: Challenges and Opportuni-
ties, pages 1?16.
69
