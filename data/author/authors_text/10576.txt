Search Result Clustering Using Label Language Model
Yeha Lee Seung-Hoon Na Jong-Hyeok Lee
Div. of Electrical and Computer Engineering
Pohang University of Science and Technology (POSTECH)
Advanced Information Technology Research Center (AITrc)
San 31, Hyoja-Dong, Pohang, Republic of Korea, 790-784
{sion,nsh1979,jhlee}@postech.ac.kr
Abstract
Search results clustering helps users to
browse the search results and locate what
they are looking for. In the search result
clustering, the label selection which anno-
tates a meaningful phrase for each cluster
becomes the most fundamental issue. In this
paper, we present a new method of using
the language modeling approach over Dmoz
for label selection, namely label language
model. Experimental results show that our
method is helpful to obtain meaningful clus-
tering labels of search results.
1 Introduction
Most contemporary search engines generate a long
flat list in response to a user query. This result can
be ranked by using criteria such as PageRank (Brin
and Page, 1998) or relevancy to the query. However,
this long flat list is uncomfortable to users, since it
forces users to examine each page one by one, and
to spend significant time and effort for finding the
really relevant information. Most users only look
into top 10 web pages in the list (Kummamuru et al,
2004). Thus many other relevant information can be
missed out as a result. Clustering method is pro-
posed in order to remedy the problem. Instead of
the flat list, it groups the search results to clusters,
and annotates a label with a representative words or
phrases to each cluster. Then, these labeled clusters
of search results are presented to users. Users can
benefit from labeled clusters because the size of in-
formation presented is significantly reduced.
Search result clustering has several specific re-
quirements that may not be required by other cluster
algorithms. First, search result clustering should al-
low fast clustering and fast generation of a label on
the fly, since it is an online process. This require-
ment can be met by adopting ?snippets? 1 rather than
entire documents of a search result set. Second, la-
bels annotated for clusters should be meaningful to
users because they are presented to users as a general
view of results. For this reason, recent search result
clustering researches focus on selecting meaningful
labels. This differs from general clustering which
focuses on the similarity of documents. In Zamir
and Etzioni (Zamir and Etzioni, 1998), a few other
key requirements of search result clustering are pre-
sented.
In this paper, we present a language modeling
approach with Dmoz for search result clustering.
Dmoz 2 is an Open Directory Project, and contains
manually tagged categories for web-sites. Since
these categories are built by human, they provide a
good basis to build labels for clusters. We can view
the problem of label selection for clusters as a prob-
lem of label generation by Dmoz.
We define a language model for each Dmoz-
category and select labels for clusters according to
the probability that this language model would gen-
erate candidate labels.
Thus, our method can select more meaningful la-
bels for clusters because we use labels generated by
human-tagged categories of Dmoz. The selected la-
1The term ?snippet? is used here to denote fragment of a
Web page returned by certain search engines
2Open Directory Project, http://www.dmoz.com/
637
bels enable users to quickly identify the desired in-
formation.
The paper is organized as follows. The next sec-
tion introduces related works. In Section 3, we for-
mulate the problem and show the detail of our ap-
proach. The experiment results and evaluations are
presented in Section 4. Finally, we conclude the
paper and discuss future works in Section 5.
2 RELATED WORKS
Many approaches have been suggested for organiz-
ing search results to improve browsing effectiveness.
Previous researches such as scatter/Gather (Hearst
and Pedersen, 1996) and Leuski (Leuski and Allan,
2000), Leouski (Leouski and Croft, 1996), cluster
documents using document-similarity, and generate
representative terms or phrases as labels. However,
these labels are often not meaningful, which compli-
cates user relevance judgment. They are also slow
in generating clusters and labels because they use
entire document contents in the process. Thus it is
difficult to apply these approaches to search engine
applications.
Due to the problems mentioned above, research
in search result clustering has focused on choosing
meaningful labels which is not usually addressed
in general document clustering. Zeng et al pre-
sented salient phrase ranking problem for label se-
lection, which ranks labels scored by a combination
of some properties of labels and documents (Zeng
et al, 2004). Kummamuru regarded label se-
lection as a problem making a taxonomy of the
search result, and proposed a label selecting crite-
rion based on taxonomy likelihood (Kummamuru et
al., 2004). Zamir presented a Suffix Tree Cluster-
ing (STC) which identifies sets of documents that
share common phrases, and clusters according to
these phrases (Zamir and Etzioni, 1998). Maarek
et al and Osinski presented a singular value de-
composition of the term-document matrix for search
result clustering (Maarek et al, 2000), (Osinski
and Weiss, 2004). The problem of these methods
is that SVD is extremely time-consuming when ap-
plied to a large number of snippets. Ferragina pro-
posed a method for generating hierarchical labels by
which entire search results are hierarchically clus-
tered (Ferragina and Gulli, 2005). This method pro-
duces a hierarchy of labeled clusters by constructing
a sequence of labeled and weighted bipartite graphs
representing the individual snippets on one side and
a set of labeled clusters on the other side.
3 LABEL LANGUAGE MODEL
The main purpose of Label Language Model(LLM)
is to generate meaningful labels on-the-fly from
search results, specifically snippets, for web-users.
The generated labels provide a view of the search re-
sult to users, and allow the users to navigate through
them for their search needs.
Our algorithm is composed of the four phases:
1. Search result fetching
2. Candidate Labels Generation
3. Label Score Calculation
4. Post-processing
Search result fetching. LLM operates as a meta-
search engine on top of established search engines.
Our engine first retrieves results from dedicated
search engines in response to user queries. The
search results are parsed through HTML parser,
and snippets are obtained as a result. We assume
that these snippets contain enough information
to provide user-relevance judgment. Hence, we
can generate meaningful labels using only those
snippets rather than the entire document contents of
the search result set.
Candidate Labels Generation. Candidate labels
are generated using the snippets obtained by search
result fetching. Snippets are processed by Porter?s
algorithm for stemming and stopword removing,
then every n-grams becomes a candidate label. Each
candidate label is tagged with a score calculated
by the Label Language Model. Finally, top N
candidate labels with highest scores are displayed
to users as labels for clusters of search result.
Label Score Calculation. Our model utilizes
Dmoz to select meaningful labels. Dmoz is the
largest, most comprehensive human-edited directory
of the Web and classifies more than 3,500,000 sites
in more than 460,000 categories. It is used for rank-
ing and retrieval by many search engines, such as
638
Google (Rerragina and Gulli, 2005).
Language model ranks documents according to
the probability that the language model of each doc-
ument would generate the user query.
Dmoz is a human-edited directory, which contains
meaningful categories. We can use the probability
that categories of Dmoz would generate candidate
labels as criteria to rank labels.
In our approach, the user query and the document
correspond to the candidate label and the Dmoz?s
category, respectively. We can obtain the probability
that LLM of each category would generate a label by
language model. We assume that the probability of
certain candidate label being generated can be esti-
mated by the maximum value of the probability that
LLM of each category would generate the candidate
label.
Let labeli be ith label, wij be jth word of labeli,
and Ck be kth category of Dmoz, respectively. If we
assume that the labels are drawn independently from
the distribution, then we can express the probability
that Dmoz generates labels as follows:
p(labeli|Dmoz) = max
k
p(labeli|Ck) (1)
p(labeli|Ck) =
?
p(wij |Ck) (2)
We use two smoothing methods, Jelinek-Mercer
smoothing and Dirichlet Priors smoothing (Zhai and
Lafferty, 2001), in order to handle unseen words.
The score of labeli is calculated as follows:
Si = max
k
?
j
log
(
1 + ?p(wij |Ck)
(1 ? ?)p(wij |Call)
)
(3)
Si = max
k
?
j
log
#(wkij) + ?p(wij |Call)
#(Ck) + ?
(4)
To solve the equation, p(wij |Ck) and p(wij |Call)
should be estimated. Let #(Ck) and #(Call)3 be
the number of words in kth category and the number
of words in Dmoz. Further, let #(wkij) and #(wallij )
be the number of word, wij , in kth category and the
number of word, wij , in Dmoz. Then p(wij |Ck) is
estimated as #(w
k
ij)
#(Ck) , and p(wij |Call) as
#(wallij )
#(Call) .
3Call denotes all categories of Dmoz
In Candidate Labels Generation phase, all can-
didate labels are scored. After post-processing,
candidate labels are shown in a descending order.
Post-processing. In post processing phase, labels
are refined through several rules. First, labels com-
posed of only query words are removed because they
do not provide better clues for users. Second, la-
bels that are contained in another label are removed.
Since every possible n gram is eligible for candidate
labels, multiple labels that differ only at the either
ends, i.e., one label contained in another, can be as-
signed a high score. In such cases, longer labels
are more specific and meaningful than shorter ones,
therefore shorter ones are removed. Users can ben-
efit from a more specific and meaningful label that
clarifies what a cluster contains. Finally, Top N La-
bels with highest scores produced by post processing
are presented to users.
4 EXPERIMENTS
We conducted several experiments with varying
smoothing parameter values, ?, ?. We investigated
the influence of the smoothing parameter on the la-
bel selection procedure.
4.1 Experiment Setup
Despite heavy researches on search result cluster-
ing, a standard test-set or evaluation measurement
does not exist. This paper adopts the methodology
of (Zeng et al, 2004) in order to evaluate the ex-
pressiveness of selected label and LLM
4.1.1 Test Data Set
We obtained Google?s search results that corre-
spond to fifty queries. The fifty queries are com-
prised of top 25 queries to Google and 25 from
(Zeng et al, 2004). For each query of the fifty, 200
snippets from Google are obtained. Table 1 summa-
rizes the query used in our experiment.
Search results obtained from Google are parsed
to remove html-tag and stopword, and stemming is
applied to obtain the snippets. Every n-gram of the
snippets, where n ? 3, becomes candidate labels.
Labels that do not occur more than 3 times are re-
moved from candidate set in order to reduce noise.
639
Type Queries
2005 Google
Top query
Myspace, Ares, Baidu, orkut,
iTumes, Sky News, World of War-
craft, Green Day, Leonardo da
Vinci, Janet Jackson, Hurricane
Katrina, tsunami, xbox 360, Brad
Pitt, Michael Jackson, American
Idol, Britney Spears, Angelina
Jolie, Harry Potter, ipod, digi-
tal camera, psp, laptop, computer
desk
(Zeng et al,
2004) query
jaguar, apple, saturn, jobs, jordan,
tiger, trec, ups, quotes, matrix, su-
san dumais, clinton, iraq, dell, dis-
ney, world war 2, ford, health, yel-
low pages, maps, flower, music,
chat, games, radio, jokes, graphic
design, resume, time zones, travel
Table 1: Queries used in experiment
4.1.2 Answer Label Set for Evaluation
In order to evaluate LLM, we manually created
labels for each query which are desired as outputs of
our test, and we refer them as answer labels. There
might be a case where an answer label and label se-
lected by our model are semantically equivalent but
lexically different; for example, car and automobile.
To mitigate the problem, we used Wordnet to han-
dle two different words with the same semantic. We
explain the use of Wordnet further in section 4.1.3.
4.1.3 Evaluation Measure & Method
We used precision at top N labels to evaluate the
model. Precision at top N is defined as P@N =
M@N
N , where is M@N is the number of relevant la-
bels among the top N generated labels to the answer
set. As explained in section 4.1.2, the labels gen-
erated by our model might not be equal to answer
labels even when they have the same semantic mean-
ing. It might be very time consuming for a human
to manually compare the two label set where one set
can vary due to the varying smoothing parameter if
semantic meaning also has to be considered.
We used WordNet?s synonyms and hypernyms
relationships in order to mitigate the problem ad-
dressed above. We regard a test label to be equal
to an answer label when WordNet?s synonyms or
hypernyms relationship allows them. Only the first
listed sense in Wordnet is used to prevent over-
generation.
We evaluated the overall effectiveness of LLM
with P@N and the effect of smoothing parameter
on P@N .
4.2 Experimental Result
We used P@5, P@10 and P@20 to evaluate the ef-
fectiveness of our model because most users disre-
gard snippets beyond 20.
First, for each query, we obtained each label?s
MAP4 for two smoothing methods. Figures 1 and
2 depicts MAP of Jelinke-Mercer smoothing and
Dirichlet Priors smoothing.
0 0.2 0.4 0.6 0.8 10.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
Lambda
Prec
ision
 
 
P@5P@10P@20
Figure 1: Jelinek-Mercer Smoothing
0 1000 2000 3000 4000 50000.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Mu
Prec
ision
 
 
P@5P@10P@20
Figure 2: Dirichlet priors Smoothing
In figures 1 and 2, X-axis denotes smoothing pa-
rameter, and Y -axis denotes MAP. The figures show
that the smaller the value of the smoothing is , the
4Mean Average Precision
640
higher the precision is. This indicates that a better
label is selected when the probability that a specific
category would generate the label is high. In our
test result, when using Dirichlet smoothing, the pre-
cision of top 5 and 10 labels are 82% and 80%, thus
users can benefit in browsing from our model using
5 or 10 labels. However, the precision rapidly drops
to 60% at P@20. The low precision at P@20 shows
the vulnerability of our model, indicating that our
model needs a refinement.
Figure 3 shows individual precisions of labels for
randomly selected five queries. The labels were gen-
erated by using Dirichlet priors smoothing.
Baidu tiger apple jaguar travel
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Queries
Prec
ision
 
 P@20P@5P@10
Figure 3: Using Dirichlet priors Smoothing
As shown in figure 1 and 2, the general order of
result precisions is as follows: P@20 ? P@10 ?
P@5. However, figure 3 shows that the precision
for query ?travel? is the lowest at P@5. This re-
sult indicates that words that appear many times in a
specific category of Dmoz might have higher proba-
bility regardless of snippet?s contents.
Average Baidu apple jaguar tiger travel0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Queries
Cov
erag
e
 
 
Top 5Top 10Top 20
Figure 4: Coverage
Figure 4 shows the average coverage of labels
generated by our model. The coverage of the labels
is about 0.32%, 0.51% and 0.66% at top 5, 10 and 20
labels respectively. This means that the labels allow
browsing over only 60% of the entire search results.
The lack of coverage is another pitfall of our model,
and further refining is needed.
Finally, in Table 2, we list top 10 labels for five
queries.
5 CONCLUSION & FUTURE WORKS
We proposed a LLM for label selection of search re-
sults, and analyzed the smoothing parameter?s effect
on the label selection. Experimental results showed
that LLM can pick up meaningful labels, and aid
users in browsing web search results. Experimental
results also validated our assumption that the high
probability that Dmoz categories generate a label in-
dicates meaningful labels. Further research direc-
tions remain as future works.
Our model is sensitive to Dmoz because we use
the language model based on Dmoz. Our model may
result in poor performance for labels that are not rep-
resented or over-represented in Dmoz. Therefore, it
is meaningful to study how sensitive to Dmoz the
performance of the LLM is, and how to mitigate sen-
sitivity. We used Google?s search results as an input
to our system. However, multiple engines offer a
better coverage of the web because of the low over-
lap of current search engines (Bharat and Broder,
1998). Further work can utilize multiple engines to
generate input to our system. In our test, snippet?s
title and content were assigned the same weight, and
titles and descriptions of Dmoz?s category were also
assigned the same weight. Future work might bene-
fit from varying the weights to them. We did not uti-
lize the information buried in the documents, such
as tf ? idf , but used only knowledge provided by the
external system, Dmoz. We believe that this also af-
fected LLM?s poor performance on over-represented
terms. Future work will benefit from incorporating
the information derivable from the documents.
6 ACKNOWLEDGMENTS
This work was supported by the Korea Science
and Engineering Foundation (KOSEF) through the
Advanced Information Technology Research Center
641
Queries Labels
Baidu language search set, Chinese
search engine, search engine com-
pany, Baidu.com, MP3 Search,
Baidu engine, Japanese Search
Engine, IPO, search market,
Mobile
apple Mac OS X, iPod, Apple Mac-
intosh, Apple products, language
character set, Music Store, Apple
develops, Apple Support, informa-
tion, San Francisco
jaguar Mac OS X, Jaguar Cars, Land
Rover, Jaguar XJ, Jaguar XK,
largest cat, Leopard, Photos
tagged jaguar, Jaguar dealer,
Jaguar Clubs
tiger Mac OS X, Tiger Woods, Tiger
Cats, Detroit Tigers, Security
tool, Parts PC Components, Paper
Tiger, Adventure Tour, National
Zoo, Tiger Beat
travel Car Rental, airline tickets, dis-
count hotels, Plan trip, Airfares,
package holidays, Visa, Travel
Cheap, Destination guides, Travel
news
Table 2: Queries used in experiment
(AITrc), also in part by the BK 21 Project and MIC
& IITA through IT Leading R&D Support Project in
2007.
References
P. Rerragina and A. Gulli. 2005. A personalized search
engine based on web-snippet hierarchical clustering.
In Special Interest Tracks and Poster Proceedings
of WWW-05, International Conference on the World
Wide Web, 801-810
H. Zeng, Q. He, Z. Chen, W. Ma and J. Ma 2004. Learn-
ing to cluster web search results. In Proceedings of the
27th ACM SIGIR Conference on Research and Devel-
opment of Information Retrieval
M. A. Hearst and J. O. Pedersen. 1996. Reexamining the
cluster hypothesis: Scatter/Gather on retrieval results.
In Proceedings of 19th ACM SIGIR Conference on Re-
search and Development in Information Retrieval, 76-
84
K. Kummamuru, R. Lotlikar, S. Roy, K. Signal and R.
Krishnapuram 2004. A hierarchical monothetic docu-
ment clustering algorithm for summarization browsing
search results. In Proceedings of 13th International
Conference on World Wide Web, 658-665
A. Leuski and J. Allan. 2000. Improving Interactive Re-
trieval by Combining Ranked List and Clustering. In
Proceedings of RIAOI, College de France, 665-681
A. V. Leouski and W. B. Croft. 1996. An Evaluation
of Techniques for Clustering Search Results. In Tech-
nical Report IR-76, Department of Computer Science,
University of Massachusetts, Amherst
O. Zamir and O. Etzioni. 1998. Web Document Clus-
tering: A Feasibility Demonstration. In Proceedings
of the 21th ACM SIGIR Conference on Research and
Development of Information Retrieval, 46-54
Y. Maarek, R. Fagin, I. Ben-Shaul and D. Pelleg. 2000.
Ephemeral document clustering for Web applications.
Technical Report RJ 10186, IBM, San Jose, US
S. Osinski and D. Weiss. 2004. Conceptual clustering
using Lingo algorithm: Evaluation on Open Directory
Project data In Proceedings of IIPWM-04, 5th Confer-
ence on Intelligent Information Processing and Web
Mining, 369-377
P. Ferragina and A. Gulli. 2005. A personalized search
engine based on Web-snippet hierarchical clustering.
In Special Interest Tracks and Poster Proceedings of
WWW-05, International conference on the World Wide
Web, 801-810
S. Brin and L. Page 1998. The anatomy of a large-scale
hypertextual(Web) Search Engine. In Proceedings of
the 7th International Conference on World Wide Web,
107-117
C. Zhai and J. Lafferty 2001. A study of smoothing
methods for language models applied to ad hoc infor-
mation retrieval. In Proceedings of the 24th ACM SI-
GIR Conference on Research and Development of In-
formation Retrieval, 334-342
K. Bharat and A. Broder. 1998. A technique for measur-
ing the relative size and overlap of public web search
engines. In Proceedings of the 7th International Con-
ference on World Wide Web
642
Automatic Extraction of English-Chinese Transliteration Pairs  
using Dynamic Window and Tokenizer 
Chengguo Jin 
Dept. of Graduate School for Information 
Technology, POSTECH, Korea 
chengguo@postech.ac.kr 
Dong-Il Kim 
Language Engineering Institute, YUST, 
China 
dongil@ybust.edu.cn 
Seung-Hoon Na 
Dept. of Computer Science & Engineering 
POSTECH, Korea 
nsh1979@postech.ac.kr 
Jong-Hyeok Lee 
Dept. of Computer Science & Engineering 
POSTECH, Korea  
jhlee@postech.ac.kr 
 
 
Abstract 
Recently, many studies have been focused 
on extracting transliteration pairs from bi-
lingual texts. Most of these studies are 
based on the statistical transliteration mod-
el. The paper discusses the limitations of 
previous approaches and proposes novel 
approaches called dynamic window and to-
kenizer to overcome these limitations. Ex-
perimental results show that the average 
rates of word and character precision are 
99.0% and 99.78%, respectively. 
1 Introduction 
Machine transliteration is a type of translation 
based on phonetic similarity between two lan-
guages. Chinese Named entities including foreign 
person names, location names and company names, 
etc are usually transliterated from foreign words. 
The main problem of transliteration resulted from 
complex relations between Chinese phonetic sym-
bols and characters. Usually, a foreign word can be 
transliterated into various Chinese words, and 
sometimes this will lead to transliteration complex-
ity.   In addition, dozens of Chinese characters cor-
respond to each pinyin which uses the Latin 
alphabet to represent sounds in Standard Mandarin. 
In order to solve these problems, Chinese 
government published the ?Names of the world's 
peoples?[12] containing 630,000 entries in 1993, 
which took about 40 years. However, some new 
foreign names still cannot be found in the diction-
ary. Constructing an unknown word dictionary is a 
difficult and time consuming job, so in this paper 
we propose a novel approach to automatically con-
struct the resource by efficiently extracting trans-
literation pairs from bilingual texts.  
Recently, much research has been conducted on 
machine transliteration. Machine transliteration is 
classified into two types. One is automatic genera-
tion of transliterated word from the source lan-
guage [6]; the other one is extracting transliteration 
pairs from bilingual texts [2]. Generally, the gen-
eration process performs worse than the extraction 
process. Especially in Chinese, people do not al-
ways transliterate foreign words only by sound but 
also consider the meanings. For example, the word 
?blog? is not transliterated into ???? ? (Bu-
LaoGe) which is phonetically equivalent to the 
source word, but transliterated into ????(BoKe) 
which means ?a lot of guests?. In this case, it is too 
difficult to automatically generate correct translit-
eration words.  Therefore, our approach is based on 
the method of extracting transliteration pairs from 
bilingual texts. 
The type of extraction of transliteration pairs can 
also be further divided into two types. One is ex-
tracting transliteration candidates from each lan-
guage respectively, and then comparing the pho-
netic similarities between those candidates of two 
languages [2, 8]. The other one is only extracting 
transliteration candidates from the source language, 
and using the candidates to extract corresponding 
transliteration words from the target language [1]. 
In Chinese, there is no space between two words 
and no special character set to represent foreign 
words such as Japanese; hence the candidate ex-
traction is difficult and usually results in a low pre-
cision. Therefore, the method presented in [2] 
which extracted transliteration candidates from 
9
Sixth SIGHAN Workshop on Chinese Language Processing
both English and Chinese result in a poor perform-
ance. Compared to other works, Lee[1] only ex-
tracts transliteration candidates from English, and 
finds equivalent Chinese transliteration words 
without extracting candidates from Chinese texts. 
The method works well, but the performance is 
required to be improved. In this paper we present a 
novel approaches to obtain a remarkable result in 
extracting transliteration word pairs from parallel 
texts.  
The remainder of the paper is organized as fol-
lows: Section 2 gives an overview of statistical 
machine transliteration and describes proposed 
approaches. Section 3 describes the experimental 
setup and a quantitative assessment of performance 
of our approaches. Conclusions and future work 
are presented in Section 4. 
 
2 Extraction of English-Chinese translit-
eration pairs 
In this paper, we first extract English named en-
tities from English-Chinese parallel texts, and se-
lect only those which are to be transliterated into 
Chinese. Next we extract Chinese transliteration 
words from corresponding Chinese texts. [Fig. 1] 
shows the entire process of extracting translitera-
tion word pairs from English-Chinese parallel texts. 
 
 
[Fig 1]. The process of extracting transliteration pairs from 
English-Chinese parallel corpus 
 
2.1 Statistical machine transliteration model 
  Generally, the Chinese Romanization system pin-
yin which is used to represent the pronunciation of 
each Chinese character is adopted in Chinese trans-
literation related studies. For example, the Chinese 
word ????? is first transformed to pinyin ?Ke 
Lin Dun?, and we compare the phonetic similarities 
between ?Clinton? and ?KeLinDun?. In this paper, 
we assume that E is written in English, while C is 
written in Chinese, and TU represents translitera-
tion units. So P(C|E), ??P( ? |Clinton) can be 
transformed to P(KeLinDun|Clinton). In this paper 
we define English TU as unigram, bigram, and tri-
gram; Chinese TU is pinyin initial, pinyin final and 
the entire pinyin. With these definitions we can 
further write the probability, ??P( ?|Clinton), as 
follows:  
(P ??? | Clinton ) ? )|( ClintonkelindunP   
 ?  )|()|()|()|()|( onunPdtPininPllPCkeP (1) 
 
 
[Fig 2]. TU alignment between English and Chinese pinyin 
 
[Fig 2] shows the possible alignment between Eng-
lish word ?Clinton? and Chinese word ??????s 
pinyin ?KeLinDun?.  
In [1], the authors add the match type informa-
tion in Eq. (1). The match type is defined with the 
lengths of TUs of two languages. For example, in 
the case of )|( CkeP the match type is 2-1, be-
cause the size of Chinese TU ke is 2 and the size 
of English TU C is 1. Match type is useful when 
estimating transliteration model?s parameters with-
out a pronunciation dictionary. In this paper, we 
use the EM algorithm to estimate transliteration 
model?s parameters without a pronunciation dic-
tionary, so we applied match type to our model. 
Add Match type(M) to Eq.(1) to formulate as fol-
lows: 
  
)|(),|(max)|( EMPEMCPECP
M
?  
    )(),|(max MPEMCP
M
?               (2) 
( )?
=
+?
N
i
iiiM
mPuvPECP
1
)(log)|((logmax)|(log  (3) 
 
10
Sixth SIGHAN Workshop on Chinese Language Processing
where u, v are English TU and Chinese TU, re-
spectively and m is the match type of u and v. 
 
[Fig 3]. The alignment of the English word and the Chinese 
sentence containing corresponding transliteration word 
 
[Fig 3] shows how to extract the correct Chinese 
transliteration ?? ??(KeLinDun) with the given 
English word ?Clinton? from a Chinese sentence.  
 
2.2 Proposed methods 
  When the statistical machine transliteration is 
used to extract transliteration pairs from a parallel 
text, the problems arise when there is more than 
one Chinese character sequence that is phonetically 
similar to the English word. In this paper we pro-
pose novel approaches called dynamic window and 
tokenizer to solve the problems effectively.  
 
2.2.1 Dynamic window method 
The dynamic window approach does not find the 
transliteration at once, but first sets the window 
size range according to the English word candi-
dates, and slides each window within the range to 
find the correct transliterations. 
 
[Fig 4]. Alignment result between English word ?Clinton? 
and correct Chinese transliteration, add a character into correct 
Chinese transliteration, and eliminate a character from correct 
Chinese transliteration. 
 
If we know the exact Chinese transliteration?s 
size, then we can efficiently extract Chinese trans-
literations by setting the window with the length of 
the actual Chinese transliteration word. For exam-
ple, in [Fig 4] we do alignment between the Eng-
lish word ?Clinton? and correct Chinese translit-
eration ?????(KeLinDun), add a character into 
correct Chinese transliteration ? ? ? ?
??(KeLinYiDun), and eliminate a character from 
correct Chinese transliteration ? ? ?(LinDun) 
respectively. The result shows that the highest 
score is the alignment with correct Chinese trans-
literation. This is because the alignment between 
the English word and the correct Chinese translit-
eration will lead to more alignments between Eng-
lish TUs and Chinese TUs, which will result in 
highest scores among alignment with other Chi-
nese sequences. This characteristic does not only 
exist between English and Chinese, but also exists 
between other language pairs. 
However, in most circumstances, we can hardly 
determine the correct Chinese transliteration?s 
length. Therefore, we analyze the distribution be-
tween English words and Chinese transliterations 
to predict the possible range of Chinese translitera-
tion?s length according to the English word. We 
11
Sixth SIGHAN Workshop on Chinese Language Processing
present the algorithm for the dynamic window ap-
proach as follows:  
Step 1: Set the range of Chinese transliteration?s 
length according to the extracted English word 
candidate.  
Step 2: Slide each window within the range to 
calculate the probability between an English word 
and a Chinese character sequence contained in the 
current window using Eq 3. 
Step 3: Select the Chinese character sequence 
with highest score and back-track the alignment 
result to extract the correct transliteration word. 
[Fig 5] shows the entire process of using the dy-
namic window approach to extract the correct 
transliteration word.  
 
English Word Ziegler 
Chinese Sentence ?? ? ??? ?? ?1963 ?? ??? 
English Sentence Ziegler and Italian Chemist Julio re-ceived the Nobel prize of 1963 together. 
Extracted translit-
eration without 
using dynamic 
window 
?? ? (JiaJuLiAo) 
Correct translitera-
tion ??  (QiGeLe) 
Steps 
1. Set Chinese transliteration?s range according to English 
word ?Ziegler? to [2, 7] (After analyzing the distribution be-
tween an English word and a Chinese transliteration word, we 
found that if the English word length is ?, then the Chinese 
transliteration word is between ?/3 and?.) 
2. Slide each window to find sequence with highest score. 
3 Select the Chinese character sequence with highest score and 
back-track the alignment result to extract a correct translitera-
tion word. 
Win-
dow 
size 
Chinese character sequence with high-
est score of each window (underline 
the back-tracking result) 
Score 
(normal-
ize with 
window 
size) 
2 ? (QiGe) -9.327 
3 ??  (QiGeLe) -6.290 
4 ?? ? (QiGeLeYu) -8.433 
5 ?? ?  (QiGeLeYuYi) -9.719 
6 ?? ??  (JiaJuLiAoGongTong) -10.458 
7 ?? ?  (QiGeLeYuYiDaLi) -10.721 
[Fig 5]. Extract the correct transliteration using the dynamic 
window method 
 
The dynamic window approach can effectively 
solve the problem shown in [Fig 5] which is the 
most common problem that arises from using sta-
tistical machine transliteration model to extract a 
transliteration from a Chinese sentence. However, 
it can not handle the case that a correct translitera-
tion with correct window size can not be extracted.   
Moreover, when the dynamic window approach is 
used, the processing time will increase severely. 
Hence, the following approach is presented to deal 
with the problem as well as to improve the per-
formance. 
 
2.2.2 Tokenizer method 
The tokenizer method is to divide a sentence 
with characters which have never been used in 
Chinese transliterations and applies the statistical 
transliteration model to each part to extract a cor-
rect transliteration.  
There are certain characters that are frequently 
used for transliterating foreign words, such as?
(shi)? (de)? (le)? (he) ??. On the other 
hand, there are other characters, such as ? (shi), 
(de)? (le)? (he),??, that have never been 
used for Chinese transliteration, while they are 
phonetically equivalent with the above characters. 
These characters are mainly particles, copulas and 
non-Chinese characters etc., and always come with 
named entities and sometimes also cause some 
problems. For example, when the English word 
?David? is transliterated into Chinese, the last pho-
neme is omitted and transliterated into ?
??(DaWei). In this case of a Chinese character 
such as ? ?(De) which is phonetically similar 
with the omitted syllable ?d?, the statistical translit-
eration model will incorrectly extract ? ?
?(DaWeiDe) as transliteration of ?David?. In [1], 
the authors deal with the problem through a post-
process using some linguistic rules. Lee and Chang 
[1] merely eliminate the characters which have 
never been used in Chinese transliteration such as 
? ?(De) from the results. Nevertheless, the ap-
proach cannot solve the problem shows in [Fig 6], 
because the copula ? ?(Shi) combines with the 
other character ? ?(zhe) to form the character 
sequence ? ?(ZheShi) which is phonetically 
similar with the English word ?Jacey?, and is in-
correctly recognized as a transliteration of ?Jacey?. 
Thus, in this case, although the copula ? ?(Shi) is 
12
Sixth SIGHAN Workshop on Chinese Language Processing
eliminated from the result through the post-process 
method presented in [1], the remaining part is not 
the correct transliteration. Compared with the 
method in [1], our tokenizer approach eliminates 
copula ? ?(Shi) at pre-processing time and then 
the phonetic similarity between ?Jacey? and the 
remaining part ? ?(Zhe) becomes very low; hence 
our approach overcomes the problem  prior to the 
entire process. In addition, the tokenizer approach 
also reduces the processing time dramatically due 
to separating a sentence into several parts. [Fig 6] 
shows the process of extracting a correct translit-
eration using the tokenizer method.  
 
English Word Jacey 
Chinese Sentence ? ? ?
? ? ? 
English Sentence The authors of this book are Peni-nah  Thomson and Jacey  Grahame. 
Incorrectly extracted 
transliteration (ZheShi) 
Correct transliteration ? (JieXi) 
Steps 
1. Separate the Chinese sentence with characters, ??, , , 
?? (including non-Chinese characters such as punctuation, 
number, English characters etc.), which have never been used 
in Chinese transliteration as follows: 
? ? ? ???? 
2. Apply statistical transliteration model to each part and se-
lect the part with highest score, and back-track the part to ex-
tract a correct transliteration.  
No. 
Chinese character sequence of 
each part (underline the back-
tracking result) 
Score 
(normalize with 
window size) 
1 ? (BenShu) -24.79 
2  (ZuoZhe) -15.83 
3 ?(PeiNiNaTangMuShen) -16.32 
4 ? ???? (JieXi) -10.29 
[Fig 6]. Extracting the correct transliteration using the to-
kenizer method. 
 
In conclusion, the two approaches complement 
each other; hence using them together will lead to 
a better performance. 
3  Experiments 
  In this section, we focus on the setup for the ex-
periments and a performance evaluation of the 
proposed approaches to extract transliteration word 
pairs from parallel corpora. 
3.1 Experimental setup 
We use 300 parallel English-Chinese sentences 
containing various person names, location names, 
company names etc. The corpus for training con-
sists of 860 pairs of English names and their Chi-
nese transliterations. The performance of translit-
eration pair extraction was evaluated based on pre-
cision and recall rates at the word and character 
levels. Since we consider exactly one proper name 
in the source language and one transliteration in 
the target language at a time, the word recall rates 
are the same as the word precision rates.  In order 
to demonstrate the effectiveness of our approaches, 
we perform the following experiments: firstly, only 
use STM(Statistical transliteration model) which is 
the baseline of our experiment; secondly, we apply 
the dynamic window and tokenizer method with 
STM respectively; thirdly, we apply these two 
methods together; at last, we perform experiment 
presented in [1] to compare with our methods. 
3.2 Evaluation of dynamic window and to-
kenizer methods 
 
  [table 1]. The experimental results of extracting 
transliteration pairs using proposed methods 
Methods Word  precision 
Character 
precision 
Character 
recall 
STM (baseline) 75.33% 86.65% 91.11% 
STM+DW 96.00% 98.51% 99.05% 
STM+TOK 78.66% 85.24% 86.94% 
STM+DW+TOK 99.00% 99.78% 99.72% 
STM+CW 98.00% 98.81% 98.69% 
STM+CW+TOK 99.00% 99.89% 99.61% 
 
As shown in table 1, the baseline STM achieves 
a word precision rate of 75%.  The STM works 
relatively well with short sentences, but as the 
length of sentences increases the performance sig-
nificantly decreases. The dynamic window ap-
proach overcomes the problem effectively. If the 
dynamic window method is applied with STM, the 
model will be tolerant with the length of sentences. 
The dynamic window approach improves the per-
formance of STM around 21%, and reaches the 
average word precision rate of 96% (STM+DW). 
In order to estimate the highest performance that 
the dynamic window approach can achieve, we 
apply the correct window size which can be ob-
tained from the evaluation data set with STM. The 
result (STM+CW) shows around 98% word preci-
13
Sixth SIGHAN Workshop on Chinese Language Processing
sion rate and about 23% improvement over the 
baseline. Therefore, dynamic window approach is 
remarkably efficient; it shows only 2% difference 
with theoretically highest performance.  However, 
the dynamic window approach increases the proc-
essing time too much.  
When using tokenizer method (STM+TOK), 
only about 3% is approved over the baseline. Al-
though the result is not considerably improved, it is 
extremely important that the problems that the dy-
namic window method cannot solve are managed 
to be solved. Thus, when using both dynamic win-
dow and tokenizer methods with STM (STM+ 
DW+TOK), it is found that around 3% improve-
ment is achieved over using only the dynamic win-
dow (STM+DW), as well as word precision rates 
of 99%.  
 
[table 2]. Processing time evaluation of proposed methods 
Methods Processing time 
STM (baseline) 5 sec (5751 milisec) 
STM+DW 2min 34sec (154893 milisec) 
STM+TOK 4sec (4574 milisec) 
STM+DW+TOK 32sec (32751 milisec) 
  
  Table 2 shows the evaluation of processing time 
of dynamic window and tokenizer methods. Using 
the dynamic window leads to 27 times more proc-
essing time than STM, while using the tokenizer 
method with the dynamic window method reduces 
the processing time around 5 times than the origi-
nal. Hence, we have achieved a higher precision as 
well as less processing time by combining these 
two methods.  
 
3.3 Comparing experiment 
  In order to compare with previous methods, we 
perform the experiment presented in [1]. Table 3 
shows using the post-processing method presented 
in [1] achieves around 87% of word precision rates, 
and about 12% improvement over the baseline. 
However, our methods are 11% superior to the 
method in [1].  
 
[Table 3] Comparing experiment with previous work 
4 Conclusions and future work 
  In this paper, we presented two novel approaches 
called dynamic window and tokenizer based on the 
statistical machine transliteration model. Our ap-
proaches achieved high precision without any post-
processing procedures. The dynamic window ap-
proach was based on a fundamental property, 
which more TUs aligned between correct translit-
eration pairs. Also, we reasonably estimated the 
range of correct transliteration?s length to extract 
transliteration pairs in high precision. The token-
izer method eliminated characters that have never 
been used in Chinese transliteration to separate a 
sentence into several parts. This resulted in a cer-
tain degree of improvement of precision and sig-
nificantly reduction of processing time.  These two 
methods are both based on common natures of all 
languages; thus our approaches can be readily port 
to other language pairs.  
In this paper, we only considered the English 
words that are to be transliterated into Chinese. 
Our work is ongoing, and in near future, we will 
extend our works to extract transliteration pairs 
from large scale comparable corpora. In compara-
ble corpora, there are many uncertainties, for ex-
ample, the extracted English word may be not 
transliterated into Chinese or there may be no cor-
rect transliteration in Chinese texts. However, with 
large comparable corpora, a word will appear sev-
eral times, and we can use the frequency or entropy 
information to extract correct transliteration pairs 
based on the proposed   perfect algorithm. 
 
Acknowledgement 
This work was supported by the Korea Science and Engineer-
ing Foundation (KOSEF) through the Advanced Information 
Technology Research Center (AITrc), also in part by the BK 
21 Project and MIC & IITA through IT Leading R&D Support 
Project in 2007. 
 
Reference 
 [1] C.-J. Lee, J.S. Chang, J.-S.R. Jang, Extraction of translit-
eration pairs from parallel corpora using a statistical translit-
eration model, in: Information Sciences 176, 67-90 (2006) 
[2] Richard Sproat, Tao Tao, ChengXiang Zhai, Named Entity 
Transliteration with Comparable Corpora, in: Proceedings of 
the 21st International Conference on Computational Linguis-
tics. (2006) 
[3] J.S. Lee and K.S. Choi, "English to Korean statistical 
transliteration for information retrieval," International Journal 
of Computer Processing of Oriental Languages, pp.17?37, 
(1998). 
Methods Word  Precision 
Character 
Precision 
Character 
Recall 
STM (baseline) 75.33% 86.65% 91.11% 
STM+DW+TOK 99.00% 99.78% 99.72% 
STM+[1]?s 
method 87.99% 90.17% 91.11% 
14
Sixth SIGHAN Workshop on Chinese Language Processing
[4] K. Knight, J. Graehl, Machine transliteration, Computa-
tional Linguistics 24 (4), 599?612, (1998). 
[5] W.-H. Lin, H.-H. Chen, Backward transliteration by learn-
ing phonetic similarity, in: CoNLL-2002, Sixth Conference on 
Natural Language Learning, Taipei, Taiwan, (2002). 
[6] J.-H. Oh, K.-S. Choi, An English?Korean transliteration 
model using pronunciation and contextual rules, in: Proceed-
ings of the 19th International Conference on Computational 
Linguistics (COLING), Taipei, Taiwan, pp. 758?764, (2002). 
[7] C.-J. Lee, J.S. Chang, J.-S.R. Jang, A statistical approach 
to Chinese-to-English Backtransliteration, in: Proceedings of 
the 17th Pacific Asia Conference on Language, Information, 
and Computation (PACLIC), Singapore, pp. 310?318, (2003). 
[8] Jong-Hoon Oh, Sun-Mee Bae, Key-Sun Choi, An Algo-
rithm for extracting English-Korean Transliteration pairs using 
Automatic E-K Transliteration In Proceedings of Korean In-
formation Science Socieity (Spring). (In Korean), (2004). 
[9] Jong-Hoon Oh, Jin-Xia Huang, Key-Sun Choi, An Align-
ment Model for Extracting English-Korean Translations of 
Term Constituents, Journal of Korean Information Science 
Society, SA, 32(4), (2005) 
[10] Chun-Jen Lee, Jason S. Chang, Jyh-Shing Roger Jang: 
Alignment of bilingual named entities in parallel corpora us-
ing statistical models and multiple knowledge sources. ACM 
Trans. Asian Lang. Inf. Process. 5(2): 121-145 (2006) 
[11] Lee, C. J. and Chang, J. S., Acquisition of English-
Chinese Transliterated Word Pairs from Parallel-Aligned 
Texts Using a Statistical Machine Transliteration Model, In. 
Proceedings of HLT-NAACL, Edmonton, Canada, pp. 96-103, 
(2003). 
[12] Xinhua Agency, Names of the world's peoples: a com-
prehensive dictionary of names in Roman-Chinese ( ?
? ? ), (1993) 
15
Sixth SIGHAN Workshop on Chinese Language Processing
