Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 21?30,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Regular Expression Learning for Information Extraction
Yunyao Li, Rajasekar Krishnamurthy, Sriram Raghavan, Shivakumar Vaithyanathan
IBM Almaden Research Center
San Jose, CA 95120
{yunyaoli, rajase, rsriram}@us.ibm.com, shiv@almaden.ibm.com
H. V. Jagadish?
Department of EECS
University of Michigan
Ann Arbor, MI 48109
jag@umich.edu
Abstract
Regular expressions have served as the dom-
inant workhorse of practical information ex-
traction for several years. However, there has
been little work on reducing the manual ef-
fort involved in building high-quality, com-
plex regular expressions for information ex-
traction tasks. In this paper, we propose Re-
LIE, a novel transformation-based algorithm
for learning such complex regular expressions.
We evaluate the performance of our algorithm
on multiple datasets and compare it against the
CRF algorithm. We show that ReLIE, in ad-
dition to being an order of magnitude faster,
outperforms CRF under conditions of limited
training data and cross-domain data. Finally,
we show how the accuracy of CRF can be im-
proved by using features extracted by ReLIE.
1 Introduction
A large class of entity extraction tasks can be ac-
complished by the use of carefully constructed reg-
ular expressions (regexes). Examples of entities
amenable to such extractions include email ad-
dresses and software names (web collections), credit
card numbers and social security numbers (email
compliance), and gene and protein names (bioinfor-
matics), etc. These entities share the characteristic
that their key representative patterns (features) are
expressible in standard constructs of regular expres-
sions. At first glance, it may seem that constructing
?Supported in part by NSF 0438909 and NIH 1-U54-
DA021519.
a regex to extract such entities is fairly straightfor-
ward. In reality, robust extraction requires the use
of rather complex expressions, as illustrated by the
following example.
Example 1 (Phone number extraction). An obvious
pattern for identifying phone numbers is ?blocks of
digits separated by hyphens? represented as R1 =
(\d+\-)+\d+.1 While R1 matches valid phone numbers
like 800-865-1125 and 725-1234, it suffers from both
?precision? and ?recall? problems. Not only does R1
produce incorrect matches (e.g., social security numbers
like 123-45-6789), it also fails to identify valid phone
numbers such as 800.865.1125, and (800)865-CARE. An
improved regex that addresses these problems is R2 =
(\d{3}[-.\ ()]){1,2}[\dA-Z]{4}.
While multiple machine learning approaches have
been proposed for information extraction in recent
years (McCallum et al, 2000; Cohen and McCal-
lum, 2003; Klein et al, 2003; Krishnan and Man-
ning, 2006), manually created regexes remain a
widely adopted practical solution for information
extraction (Appelt and Onyshkevych, 1998; Fukuda
et al, 1998; Cunningham, 1999; Tanabe and Wilbur,
2002; Li et al, 2006; DeRose et al, 2007; Zhu et al,
2007). Yet, with a few notable exceptions, which we
discuss later in Section 1.1, there has been very little
work in reducing this human effort through the use
of automatic learning techniques. In this paper, we
propose a novel formulation of the problem of learn-
1Throughout this paper, we use the syntax of the standard
Java regex engine (Java, 2008).
21
ing regexes for information extraction tasks. We
demonstrate that high quality regex extractors can be
learned with significantly reduced manual effort. To
motivate our approach, we first discuss prior work
in the area of learning regexes and describe some of
the limitations of these techniques.
1.1 Learning Regular Expressions
The problem of inducing regular languages from
positive and negative examples has been studied in
the past, even outside the context of information
extraction (Alquezar and Sanfeliu, 1994; Dupont,
1996; Firoiu et al, 1998; Garofalakis et al, 2000;
Denis, 2001; Denis et al, 2004; Fernau, 2005;
Galassi and Giordana, 2005; Bex et al, 2006).
Much of this work assumes that the target regex
is small and compact thereby allowing the learn-
ing algorithm to exploit this information. Consider,
for example, the learning of patterns motivated by
DNA sequencing applications (Galassi and Gior-
dana, 2005). Here the input sequence is viewed
as multiple atomic events separated by gaps. Since
each atomic event is easily described by a small and
compact regex, the problem reduces to one of learn-
ing simple regexes. Similarly, in XML DTD infer-
ence (Garofalakis et al, 2000; Bex et al, 2006), it
is possible to exploit the fact that the XML docu-
ments of interest are often described using simple
DTDs. E.g., in an online books store, each book
has a title, one or more authors and price. This in-
formation can be described in a DTD as ?book? ?
?title??author? + ?price?. However, as shown in Ex-
ample 1, regexes for information extraction rely on
more complex constructs.
In the context of information extraction, prior
work has concentrated primarily on learning regexes
over relatively small alphabet sizes. A common
theme in (Soderland, 1999; Ciravegna, 2001; Wu
and Pottenger, 2005; Feldman et al, 2006) is the
problem of learning regexes over tagged tokens
produced by other text-processing steps such as
POS tagging, morphological analysis, and gazetteer
matching. Thus, the alphabet is defined by the space
of possible tags output by these analysis steps. A
similar approach has been proposed in (Brill, 2000)
for POS disambiguation. In contrast, our paper ad-
dresses extraction tasks that require ?fine-grained?
control to accurately capture the structural features
of the entity of interest. Consequently, the domain
of interest consists of all characters thereby dramat-
ically increasing the size of the alphabet. To enable
this scale-up, the techniques presented in this paper
exploit advanced syntactic constructs (such as char-
acter classes and quantifiers) supported by modern
regex languages.
Finally, we note that almost all of the above de-
scribed work define the learning problem over a
restricted class of regexes. Typically, the restric-
tions involve either disallowing or limiting the use of
Kleene disclosure and disjunction operations. How-
ever, our work imposes no such restrictions.
1.2 Contributions
In a key departure from prior formulations, the
learning algorithm presented in this work takes as
input not just labeled examples but also an initial
regular expression. The use of an initial regex has
two major advantages. First, this expression pro-
vides a natural mechanism for a domain expert to
provide domain knowledge about the structure of the
entity being extracted. Second, as we show in Sec-
tion 2, the space of output regular expressions un-
der consideration can be meaningfully restricted by
appropriately defining their relationship to the input
expression. Such a principled approach to restrict
the search space permits the learning algorithm to
consider complex regexes in a tractable manner. In
contrast, prior work defined a tractable search space
by placing restrictions on the target class of regular
expressions. Our specific contributions are:
? A novel regex learning problem consisting of learn-
ing an ?improved? regex given an initial regex and
labeled examples
? Formulation of this learning task as an optimization
problem over a search space of regexes
? ReLIE, a regex learning algorithm that employs
transformations to navigate the search space
? Extensive experimental results over multiple
datasets to show the effectiveness of ReLIE and
a comparison study with the Conditional Random
Field (CRF) algorithm
? Finally, experiments that demonstrate the benefits
of using ReLIE as a feature extractor for CRF and
possibly other machine learning algorithms.
22
2 The Regex Learning Problem
Consider the task of identifying instances of some
entity E . Let R0 denote the input regex provided by
the user and let M(R0 ,D) denote the set of matches
obtained by evaluating R0 over a document col-
lection D. Let Mp(R0 ,D) = {x ? M(R0 ,D) :
x instance of E} and Mn(R0 ,D) = {x ? M(R0 ,D) :
x not an instance of E} denote the set of positive and
negative matches for R0 . Note that a match is pos-
itive if it corresponds to an instance of the entity of
interest and is negative otherwise. The goal of our
learning task is to produce a regex that is ?better?
than R0 at identifying instances of E .
Given a candidate regex R, we need a mechanism
to judge whether R is indeed a better extractor for
E than R0 . To make this judgment even for just the
original document collection D, we must be able to
label each instance matched byR (i.e., each element
of M(R,D)) as positive or negative. Clearly, this
can be accomplished if the set of matches produced
byR are contained within the set of available labeled
examples, i.e., if M(R,D) ? M(R0 ,D). Based on
this observation, we make the following assumption:
Assumption 1. Given an input regex R0 over some al-
phabet ?, any other regexR over ? is a candidate for our
learning algorithm only if L(R) ? L(R0 ). (L(R) denotes
the language accepted by R).
Even with this assumption, we are left with a po-
tentially infinite set of candidate regexes from which
our learning algorithm must choose one. To explore
this set in a principled fashion, we need a mecha-
nism to move from one element in this space to an-
other, i.e., from one candidate regex to another. In
addition, we need an objective function to judge the
extraction quality of each candidate regex. We ad-
dress these two issues below.
Regex Transformations To systematically ex-
plore the search space, we introduce the concept of
regex transformations.
Definition 1 (Regex Transformation). LetR? denote
the set of all regular expressions over some alphabet ?. A
regex transformation is a function T : R? ? 2R? such
that ?R? ? T (R), L(R?) ? L(R).
For example, by replacing different occurrences
of the quantifier + in R1 from Example 1 with
specific ranges (such as {1,2} or {3}), we obtain
expressions such as R3 = (\d+\-){1,2}\d+ and
R4 = (\d{3}\-)+\d+. The operation of replacing
quantifiers with restricted ranges is an example of a
particular class of transformations that we describe
further in Section 3. For the present, it is sufficient
to view a transformation as a function applied to a
regexR that produces, as output, a set of regexes that
accept sublanguages of L(R). We now define the
search space of our learning algorithm as follows:
Definition 2 (Search Space). Given an input regex R0
and a set of transformations T , the search space of our
learning algorithm is T (R0 ), the set of all regexes ob-
tained by (repeatedly) applying the transformations in T
to R0 .
For instance, if the operation of restricting quanti-
fiers that we described above is part of the transfor-
mation set, then R3 and R4 are in the search space
of our algorithm, given R1 as input.
Objective Function We now define an objective
function, based on the well known F-measure, to
compare the extraction quality of different candidate
regexes in our search space. Using Mp(R,D) (resp.
Mn(R,D)) to denote the set of positive (resp. nega-
tive) matches of a regex R, we define
precision(R,D) =
Mp(R,D)
Mp(R,D) + Mn(R,D)
recall(R,D) =
Mp(R,D)
Mp(R0,D)
F(R,D) =
2 ? precision(R,D) ? recall(R,D)
precision(R,D) + recall(R,D)
The regex learning task addressed in this paper
can now be formally stated as the following opti-
mization problem:
Definition 3 (Regex Learning Problem). Given
an input regex R0 , a document collection D, labeled
sets of positive and negative examples Mp(R0 ,D) and
Mn(R0 ,D), and a set of transformations T , compute the
output regex Rf = argmaxR?T (R0 ) F(R,D).
3 Instantiating Regex Transformations
In this section, we describe how transformations
can be implemented by exploiting the syntactic con-
structs of modern regex engines. To help with our
description, we introduce the following task:
Example 2 (Software name extraction). Consider the
task of identifying names of software products in text.
A simple pattern for this task is: ?one or more capital-
ized words followed by a version number?, represented
as R5 = ([A-Z]\w*\s*)+[Vv]?(\d+\.?)+.
23
When applied to a collection of University web
pages, we discovered that R5 identified correct in-
stances such as Netscape 2.0, Windows 2000 and
Installation Designer v1.1. However, R5 also ex-
tracted incorrect instances such as course numbers
(e.g. ENGLISH 317), room numbers (e.g. Room
330), and section headings (e.g. Chapter 2.2). To
eliminate spurious matches such as ENGLISH 317,
let us enforce the condition that ?each word is a
single upper-case letter followed by one or more
lower-case letters?. To accomplish this, we focus
on the sub-expression of R5 that identifies capital-
ized words, R51 = ([A-Z]\w*\s*)+, and replace it
with R51a = ([A-Z][a-z]*\s*)+. The regex result-
ing from R5 by replacing R51 with R51a will avoid
matches such as ENGLISH 317.
An alternate way to improve R5 is by explicitly
disallowing matches against strings like ENGLISH,
Room and Chapter. To accomplish this, we can
exploit the negative lookahead operator supported
in modern regex engines. Lookaheads are special
constructs that allow a sequence of characters
to be checked for matches against a regex with-
out the characters themselves being part of the
match. As an example, (?!Ra)Rb (??!? being
the negative lookahead operator) returns matches
of regex Rb but only if they do not match Ra.
Thus, by replacing R51 in our original regex with
R51b =(?! ENGLISH|Room|Chapter)[A-Z]\w*\s*,
we produce an improved regex for software names.
The above examples illustrate the general prin-
ciple of our transformation technique. In essence,
we isolate a sub-expression of a given regex R and
modify it such that the resulting regex accepts a sub-
language of R. We consider two kinds of modifica-
tions ? drop-disjunct and include-intersect. In drop-
disjunct, we operate on a sub-expression that corre-
sponds to a disjunct and drop one or more operands
of that disjunct. In include-intersect, we restrict the
chosen sub-expression by intersecting it with some
other regex. Formally,
Definition 4 (Drop-disjunct Transformation). Let
R ? R? be a regex of the form R = Ra?(X)Rb,
where ?(X) denotes the disjunction R1|R2| . . . |Rn of
any non-empty set of regexes X = {R1, R2, . . . , Rn}.
The drop-disjunct transformation DD(R,X, Y ) for some
Y ? X, Y 6= ? results in the new regex Ra?(Y )Rb.
Definition 5 (Include-Intersect Transformation). Let
.\W \s \w[a-zA-Z] \d|[0-9] _[a-z] [A-Z]
Figure 1: Sample Character Classes in Regex
R ? R? be a regex of the form R = RaXRb for some
X ? R?, X 6= ?. The include-intersect transformation
II(R,X, Y ) for some Y ? R?, Y 6= ? results in the new
regex Ra(X ? Y )Rb.
We state the following proposition (proof omit-
ted in the interest of space) that guarantees that both
drop-disjunct and include-intersect restrict the lan-
guage of the resulting regex, and therefore are valid
transformations according to Definition 1.
Proposition 1. Given regexes R,X1, Y1, X2 and Y2
from R? such that DD(R,X1, Y1) and II(R,X2, Y2)
are applicable, L(DD(R,X1, Y1)) ? L(R) and
L(II(R,X2, Y2)) ? L(R).
We now proceed to describe how we use differ-
ent syntactic constructs to apply drop-disjunct and
include-intersect transformations.
Character Class Restrictions Character
classes are short-hand notations for denoting
the disjunction of a set of characters (\d is
equivalent to (0|1...|9); \w is equivalent to
(a|. . .|z|A|. . .|Z|0|1. . .|9| ); etc.).2 Figure 1
illustrates a character class hierarchy in which
each node is a stricter class than its parent (e.g.,
\d is stricter than \w). A replacement of any of
these character classes by one of its descendants
is an instance of the drop-disjunct transformation.
Notice that in Example 2, when replacing R51 with
R51a , we were in effect applying a character class
restriction.
Quantifier Restrictions Quantifiers are used to
define the range of valid counts of a repetitive se-
quence. For instance, a{m,n} looks for a sequence
of a?s of length at least m and at most n. Since
quantifiers are also disjuncts (e.g., a{1,3} is equiv-
alent to a|aa|aaa), the replacement of an expres-
sion R{m,n} with an expression R{m1, n1} (m ?
m1 ? n1 ? n) is an instance of the drop-disjunct
transformation. For example, given a subexpres-
sion of the form a{1,3}, we can replace it with
2Note that there are two distinct character classes \W and \w
24
one of a{1,1}, a{1,2}, a{2,2}, a{2,3}, or a{3,3}.
Note that, before applying this transformation, wild-
card expressions such as a+ and a* are replaced by
a{0,maxCount} and a{1,maxCount} respectively,
where maxCount is a user configured maximum
length for the entity being extracted.
Negative Dictionaries Observe that the include-
intersect transformation (Definition 5) is applicable
for every possible sub-expression of a given regex
R. Note that a valid sub-expression in R is any
portion of R where a capturing group can be intro-
duced.3 Consider a regex R = RaXRb with a sub-
expression X; the application of include-intersect
requires another regex Y to yieldRa(X?Y )Rb. We
would like to construct Y such thatRa(X ?Y )Rb is
?better? than R for the task at hand. Therefore, we
construct Y as ?Y ? where Y ? is a regex constructed
from negative matches ofR. Specifically, we look at
each negative match of R and identify the substring
of the match that corresponds to X . We then apply
a greedy heuristic (see below) to these substrings to
yield a negative dictionary Y ?. Finally, the trans-
formed regexRa(X??Y ?)Rb is implemented using
the negative lookahead expression Ra(?! Y?)XRb.
Greedy Heuristic for Negative Dictionaries Im-
plementation of the above procedure requires cer-
tain judicious choices in the construction of the neg-
ative dictionary to ensure tractability of this trans-
formation. Let S(X) denote the distinct strings
that correspond to the sub-expression X in the neg-
ative matches of R.4 Since any subset of S(X)
is a candidate negative dictionary, we are left with
an exponential number of possible transformations.
In our implementation, we used a greedy heuris-
tic to pick a single negative dictionary consisting
of all those elements of S(X) that individually
improve the F-measure. For instance, in Exam-
ple 2, if the independent substitution of R51 with
(?!ENGLISH)[A-Z]\w*\s*, (?!Room)[A-Z]
\w*\s*, and (?!Chapter)[A-Z]\w*\s* each im-
proves the F-measure, we produce a nega-
tive dictionary consisting of ENGLISH, Room, and
Chapter. This is precisely how the disjunct
ENGLISH|Room|Chapter is constructed in R51b .
3For instance, the sub-expressions of ab{1,2}c are a,
ab{1,2}, ab{1,2}c, b, b{1,2}, b{1,2}c, and c.
4S(X) can be obtained automatically by identifying the sub-
string corresponding to the group X in each entry in Mn(R,D)
Procedure ReLIE(Mtr ,Mval,R0 ,T )
//Mtr : set of labeled matches used as training data
//Mval: set of labeled matches used as validation data
// R0 : user-provided regular expression
// T : set of transformations
begin
1. Rnew = R0
2. do {
3. for each transformation ti ? T
4. Candidatei=ApplyTransformations(Rnew, ti)
5. let Candidates =
?
i Candidatei
6. let R? = argmaxR?Candidates F(R,Mtr)
7. if (F(R?,Mtr) <= F(Rnew,Mtr)) return Rnew
8. if (F(R?,Mval) < F(Rnew,Mval)) return Rnew
9. Rnew = R?
10. } while(true)
end
Figure 2: ReLIE Search Algorithm
4 ReLIE Search Algorithm
Figure 2 describes the ReLIE algorithm for the
Regex Learning Problem (Definition 3) based on the
transformations described in Section 3. ReLIE is a
greedy hill climbing search procedure that chooses,
at every iteration, the regex with the highest F-
measure. An iteration in ReLIE consists of:
? Applying every transformation on the current regex
Rnew to obtain a set of candidate regexes
? From the candidates, choosing the regex R? whose
F-measure over the training dataset is maximum
To avoid overfitting, ReLIE terminates when either
of the following conditions is true: (i) there is no
improvement in F-measure over the training set;
(ii) there is a drop in F-measure when applying R?
on the validation set.
The following proposition provides an upper
bound for the running time of the ReLIE algorithm.
Proposition 2. Given any valid set of inputs Mtr,
Mval, R0 , and T , the ReLIE algorithm terminates in at
most |Mn(R0 ,Mtr )| iterations. The running time of the
algorithm TTotal(R0 ,Mtr ,Mval) ? |Mn(R0 ,Mtr )| ?
t0 , where t0 is the time taken for the first iteration of the
algorithm.
Proof. With reference to Figure 2, in each iteration, the
F-measure of the ?best? regex R? is strictly better than
Rnew. Since L(R?) ? L(Rnew), R? eliminates at least
one additional negative match compared toRnew. Hence,
the maximum number of iterations is |Mn(R0 ,Mtr )|.
For a regular expression R, let ncc(R) and nq(R) de-
note, respectively, the number of character classes and
quantifiers in R. The maximum number of possible sub-
expressions in R is |R|2, where |R| is the length of R.
Let MaxQ(R) denote the maximum number of ways in
25
which a single quantifier appearing in R can be restricted
to a smaller range. Let Fcc denote the maximum fanout5
of the character class hierarchy. Let TReEval(D) denote
the average time taken to evaluate a regex over datasetD.
Let Ri denote the regex at the beginning of iteration
i. The number of candidate regexes obtained by applying
the three transformations is
NumRE(Ri,Mtr) ? ncc(Ri)?Fcc+nq(Ri)?MaxQ(Ri)+|Ri|
2
The time taken to enumerate the character class and
quantifier restriction transformations is proportional to
the resulting number of candidate regexes. The time
taken for the negative dictionaries transformation is given
by the running time of the greedy heuristic (Section 3).
The total time taken to enumerate all candidate regexes is
given by (for some constant c)
TEnum(Ri,Mtr) ? c ? (ncc(Ri) ? Fcc + nq(Ri) ?MaxQ(Ri)
+ |Ri|
2 ?Mn(Ri,Mtr) ? TReEval(Mtr))
Choosing the best transformation involves evaluating
each candidate regex over the training and validation cor-
pus and the time taken for this step is
TPickBest(Ri,Mtr,Mval) = NumRE(Ri,Mtr)
?(TReEval(Mtr) + TReEval(Mval))
The total time taken for an iteration can be written as
TI(Ri,Mtr,Mval) =TEnum(Ri,Mtr)
+ TPickBest(Ri,Mtr,Mval)
It can be shown that the time taken in each iteration
decreases monotonically (details omitted in the interest of
space). Therefore, the total running time of the algorithm
is given by
TTotal(R0 ,Mtr ,Mval) =
?
TI(Ri,Mtr,Mval)
? |Mn(R0 ,Mtr )| ? t0 .
where t0 = TI(R0 ,Mtr ,Mval) is the running time
of the first iteration of the algorithm.
5 Experiments
In this section, we present an empirical study of
the ReLIE algorithm using four extraction tasks over
three real-life data sets. The goal of this study is to
evaluate the effectiveness of ReLIE in learning com-
plex regexes and to investigate how it compares with
standard machine learning algorithms.
5.1 Experimental Setup
Data Set The datasets used in our experiments are:
? EWeb: A collection of 50,000 web pages crawled
from a corporate intranet.
5Fanout is the number of ways in which a character class
may be restricted as defined by the hierarchy (e.g. Figure 1).
? AWeb: A set of 50,000 web pages obtained from
the publicly available University of Michigan Web
page collection (Li et al, 2006), including a sub-
collection of 10,000 pages (AWeb-S).
? Email: A collection of 10,000 emails obtained
from the publicly available Enron email collec-
tion (Minkov et al, 2005).
Extraction Tasks SoftwareNameTask, CourseNum-
berTask and PhoneNumberTask were evaluated on
EWeb, AWeb and Email, respectively. Since web
pages have large number of URLs, to keep the la-
beling task manageable, URLTask was evaluated on
AWeb-S.
Gold Standard For each task, the gold standard
was created by manually labeling all matches for the
initial regex. Note that only exact matches with the
gold standard are considered correct in our evalua-
tions. 6
Comparison Study To evaluate ReLIE for entity
extraction vis-a-vis existing algorithms, we used the
popular conditional random field (CRF). Specifi-
cally, we used the MinorThird (Cohen, 2004) imple-
mentation of CRF to train models for all four extrac-
tion tasks. For training the CRF we provided it with
the set of positive and negative matches from the ini-
tial regex with a context of 200 characters on either
side of each match7. Since it is unlikely that useful
features are located far away from the entity, we be-
lieve that 200 characters on either side is sufficient
context. The CRF used the base features described
in (Cohen et al, 2005). To ensure fair compari-
son with ReLIE, we also included the matches corre-
sponding to the input regex as a feature to the CRF.
In practice, more complex features (e.g., dictionar-
ies, simple regexes) derived by domain experts are
often provided to CRFs. However, such features can
also be used to refine the initial regex given to ReLIE.
Hence, with a view to investigating the ?raw? learn-
ing capability of the two approaches, we chose to
run all our experiments without any additional man-
ually derived features. In fact, the patterns learned
by ReLIE through transformations are often similar
6The labeled data will be made publicly available at
http://www.eecs.umich.edu/db/regexLearning/.
7Ideally, we would have preferred to let MinorThird extract
appropriate features from complete documents in the training-
set but could not get it to load our large datasets.
26
(a) SoftwareNameTask
0.50.6
0.70.8
0.91
10% 40% 80%Percentage of Data Used for Training
F
-
M
e
a
s
u
r
e
ReLIE CRF
(b) CourseNumberTask
0.50.6
0.70.8
0.91
10% 40% 80%Percentage of Data Used for Training
F
-
M
e
a
s
u
r
e
ReLIE CRF
(c) URLTask
0.50.6
0.70.8
0.91
10% 40% 80%Percentage of Data Used for Training
F
-
M
e
a
s
u
r
e
ReLIE CRF
(d) PhoneNumberTask
0.50.6
0.70.8
0.91
10% 40% 80%Percentage of Data Used for Training
F
-
M
e
a
s
u
r
e
ReLIE CRF
Figure 3: Extraction Qualitya
aFor SoftwareNameTask, with 80% training data we could not obtain results for CRF as the program
failed repeatedly during the training phase.
to the features that domain experts may provide to
CRF. We will revisit this issue in Section 5.4.
Evaluation We used the standard F-measure to
evaluate the effectiveness of ReLIE and CRF. We di-
vided each dataset into 10 equal parts and used X%
of the dataset for training (X=10, 40 and 80), 10%
for validation, and remaining (90-X)% for testing.
All results are reported on the test set.
5.2 Results
Four extraction tasks were chosen to reflect the enti-
ties commonly present in the three datasets.
? SoftwareNameTask: Extracting software names such
as Lotus Notes 8.0, Open Office Suite 2007.
? CourseNumberTask: Extracting university course
numbers such as EECS 584, Pharm 101.
? PhoneNumberTask: Extracting phone numbers such
as 1-800-COMCAST, (425)123 5678.
? URLTask: Extracting URLs such as
http:\\www.abc.com and lsa.umich.edu/ foo/.8
This section summarizes the results of our empir-
ical evaluation comparing ReLIE and CRF.
8URLTask may appear to be simplistic. However, extracting
URLs without the leading protocol definitions (e.g. http) can
be challenging.
Raw Extraction Quality The cross-validated re-
sults across all four tasks are presented in Figure 3.
? With 10% training data, ReLIE outperforms CRF
on three out of four tasks with a difference in F-
measure ranging from 0.1 to 0.2.
? As training data increases, both algorithms perform
better with the gap between the two reducing for
all the four tasks. For CourseNumberTask and URL-
Task, CRF does slightly better than ReLIE for larger
training dataset. For the other two tasks, ReLIE re-
tains its advantage over CRF.9
The above results indicate that ReLIE performs
comparably with CRF with a slight edge in condi-
tions of limited training data. Indeed, the capability
to learn high-quality extractors using a small train-
ing set is important because labeled data is often ex-
pensive to obtain. For precisely this same reason, we
would ideally like to learn the extractors once and
then apply them to other datasets as needed. Since
these other datasets may be from a different domain,
we next performed a cross-domain test (i.e., training
9For SoftwareNameTask, with 80% training data we could
not obtain results for CRF as the program failed repeatedly dur-
ing the training phase.
27
and testing on different domains).
Task(Training, Testing)
Data for Training 10% 40% 80%
ReLIE CRF ReLIE CRF ReLIE CRF
SoftwareNameTask(EWeb,AWeb) 0.920 0.297 0.977 0.503 0.971 N/A
URLTask(AWeb-S,Email) 0.690 0.209 0.784 0.380 0.801 0.507
PhoneNumberTask(Email,AWeb) 0.357 0.130 0.475 0.125 0.513 0.120
Table 1: Cross Domain Test (F-measure).
Technique
SoftwareNameTask CourseNumberTask URLTask PhoneNumberTask
training testing training testing training testing training testing
ReLIE 511.7 20.6 69.3 18.4 73.8 7.7 39.4 1.1
CRF 7597.0 2315.8 482.5 75.4 438.7 53.8 434.8 57.7
t(ReLIE)
t(CRF) 0.067 0.009 0.144 0.244 0.168 0.143 0.091 0.019
Table 2: Average Training/Testing Time (sec)(with 40% data for training)
Task(Extra Feature)
Data for Training 10% 40% 80%
CRF C+RL CRF C+RL CRF C+RL
CourseNumberTask(Negative Dictionary) 0.553 0.624 0.644 0.764 0.854 0.845
PhoneNumberTask(Quantifier) 0.695 0.893 0.820 0.937 0.821 0.964
Table 3: ReLIE as Feature Extractor (C+RL is CRF enhanced with
features learned by ReLIE).
Cross-domain Evaluation Table 1
summarizes the results of training
the algorithms on one data set and
testing on another. The scenarios
chosen are: (i) SoftwareNameTask
trained on EWeb and tested on
AWeb, (ii) URLTask trained on AWeb
and tested on Email, and (iii) Pho-
neNumberTask trained on Email
and tested on AWeb.10 We can
see that ReLIE significantly out-
performs CRF for all three tasks,
even when provided with a large
training dataset. Compared to test-
ing on the same dataset, there is a
reduction in F-measure (less than
0.1 in many cases) when the regex
learned by ReLIE is applied to a dif-
ferent dataset, while the drop for
CRF is much more significant (over 0.5 in many
cases).11
Training Time Another issue of practical consid-
eration is the efficiency of the learning algorithm.
Table 2 reports the average training and testing time
for both algorithms on the four tasks. On average Re-
LIE is an order of magnitude faster than CRF in both
building the model and applying the learnt model.
Robustness to Variations in Input Regexes The
transformations done by ReLIE are based on the
structure of the input regex. Therefore given differ-
ent input regexes, the final regexes learned by ReLIE
will be different. To evaluate the impact of the struc-
ture of the input regex on the quality of the regex
learned by ReLIE, we started with different regexes12
for the same task. We found that ReLIE is robust
to variations in input regexes. For instance, on Soft-
wareNameTask, the standard deviation in F-measure
10We do not report results for CourseNumberTask as course
numbers are specific to academic webpages and do not appear
in the other two domains
11Similar cross-domain performance deterioration for a ma-
chine learning approach has been observed by (Guo et al,
2006).
12Recall that the search space of ReLIE is limited by L(R0)
(Assumption 1). Thus to ensure meaningful comparison, for
the same task any two given input regexes R0 and R?0 are cho-
sen in such a way that although their structures are different,
Mp(R0,D) = Mp(R?0,D) and Mn(R0,D) = Mn(R
?
0,D).
of the final regexes generated from six different in-
put regexes was less than 0.05. Further details of this
experiment are omitted in the interest of space.
5.3 Discussion
The results of our comparison study (Figure 3) in-
dicates that for raw extraction quality ReLIE has a
slight edge over CRF for small training data. How-
ever, in cross-domain performance (Table 1) ReLIE
is significantly better than CRF (by 0.41 on aver-
age) . To understand this discrepancy, we examined
the final regex learned by ReLIE and compared that
with the features learned by CRF. Examples of ini-
tial regexes with corresponding final regexes learnt
by ReLIE with 10% training data are listed in Ta-
ble 4. Recall, from Section 3, that ReLIE transfor-
mations include character class restrictions, quanti-
fier restrictions and addition of negative dictionar-
ies. For instance, in the SoftwareNameTask, the final
regex listed was obtained by restricting [a-zA-Z]
to [a-z], \w to [a-zA-Z], and adding the nega-
tive dictionary (Copyright|Fall| ? ? ? |Issue). Sim-
ilarly, for the PhoneNumberTask, the final regex
involved two negative dictionaries (expressed as
(?![,]) and (?![,:])) 13 and quantifier restric-
tions (e.g. the first [A-Z\d]{2,4} was transformed
13To obtain these negative dictionaries, ReLIE not only
needs to correctly identify the dictionary entries from negative
matches but also has to place the corresponding negative looka-
head expression at the appropriate place in the regex.
28
SoftwareNameTask
R0 \b([A-Z][a-zA-Z]{1,10}\s){1,5}\s*(\w{0,2}\d[\.]?){1,4}\b
Rfinal
\b((?!(Copyright|Page|Physics|Question| ? ? ? |Article|Issue))[A-Z][a-z]{1,10}
\s){1,5}\s*([a-zA-Z]{0,2}\d[\.]?){1,4}\b
PhoneNumberTask R0 \b(1\W+)?\W?\d{3,3}\W*\s*\W?[A-Z\d]{2,4}\s*\W?[A-Z\d]{2,4}\bRfinal \b(1\W+)?\W?\d{3,3}((?![,])\W*)\s*\W?[A-Z\d]{3,3}\s*((?![,:])\W?)[A-Z\d]{3,4}\b
CourseNumberTask R0 \b([A-Z][a-zA-Z]+)\s+\d{3,3}\bRfinal \b(((?!(At|Between| ? ? ?Contact|Some|Suite|Volume))[A-Z][a-zA-Z]+))\s+\d{3,3}\b
URLTask R0 \b(\w+://)?(\w+\.){0,2}\w+\.\w+(/[
?\s]+){0,20}\b
Rfinal
\b((?!(Response 20010702 1607.csv| ? ? ?))((\w+://)?(\w+\.){0,2}\w+\.(?!(ppt
| ? ? ?doc))[a-zA-Z]{2,3}))(/[?\s]+){0,20}\b
Table 4: Sample Regular Expressions Learned by ReLIE(R0: input regex; Rfinal: final regex learned; the parts of R0
modified by ReLIE and the corresponding parts in Rfinal are highlighted.)
into [A-Z\d]{3,3}).
After examining the features learnt by CRF, it was
clear that while CRF could learn features such as the
negative dictionary it is unable to learn character-
level features. This should not be surprising since
our CRF was trained with primarily tokens as fea-
tures (cf. Section 5.1). While this limitation was less
of a factor in experiments involving data from the
same domain (some effects were seen with smaller
training data), it does explain the significant differ-
ence between the two algorithms in cross-domain
tasks where the vocabulary can be significantly dif-
ferent. Indeed, in practical usage of CRF, the main
challenge is to come up with additional complex fea-
tures (often in the form of dictionary and regex pat-
terns) that need to be given to the CRF (Minkov et
al., 2005). Such complex features are largely hand-
crafted and thus expensive to obtain. Since the Re-
LIE transformations are operations over characters,
a natural question to ask is: ?Can the regex learned
by ReLIE be used to provide features to CRF?? We
answer this question below.
5.4 ReLIE as Feature Extractor for CRF
To understand the effect of incorporating ReLIE-
identified features into CRF, we chose the two tasks
(CourseNumberTask and PhoneNumberTask) with the
least F-measure in our experiments to determine raw
extraction quality. We examined the final regex pro-
duced by ReLIE and manually extracted portions
to serve as features. For example, the negative
dictionary learned by ReLIE for the CourseNumber-
Task (At|Between| ? ? ? |Volume) was incorporated as
a feature into CRF. To help isolate the effects, for
each task, we only incorporated features correspond-
ing to a single transformation: negative dictionar-
ies for CourseNumberTask and quantifier restrictions
for PhoneNumberTask. The results of these experi-
ments are shown in Table 3. The first point worthy of
note is that performance has improved in all but one
case. Second, despite the F-measure on CourseNum-
berTask being lower than PhoneNumberTask (presum-
ably more potential for improvement), the improve-
ments on PhoneNumberTask are significantly higher.
This observation is consistent with our conjecture
in Section 5.1 that CRF learns token-level features;
therefore incorporating negative dictionaries as extra
feature provides only limited improvement. Admit-
tedly more experiments are needed to understand the
full impact of incorporating ReLIE-identified fea-
tures into CRF. However, we do believe that this is
an exciting direction of future research.
6 Summary and Future Work
We proposed a novel formulation of the problem of
learning complex character-level regexes for entity
extraction tasks. We introduced the concept of regex
transformations and described how these could be
realized using the syntactic constructs of modern
regex languages. We presented ReLIE, a powerful
regex learning algorithm that exploits these ideas.
Our experiments demonstrate that ReLIE is very ef-
fective for certain classes of entity extraction, partic-
ularly under conditions of cross-domain and limited
training data. Our preliminary results also indicate
the possibility of using ReLIE as a powerful feature
extractor for CRF and other machine learning algo-
rithms. Further investigation of this aspect of ReLIE
presents an interesting avenue of future work.
Acknowledgments
We thank the anonymous reviewers for their insight-
ful and constructive comments and suggestions. We
are also grateful for comments from David Gondek
and Sebastian Blohm.
29
References
R. Alquezar and A. Sanfeliu. 1994. Incremental gram-
matical inference from positive and negative data using
unbiased finite state automata. In SSPR.
Douglas E. Appelt and Boyan Onyshkevych. 1998. The
common pattern specification language. In TIPSTER
TEXT PROGRAM.
Geert Jan Bex et al 2006. Inference of concise DTDs
from XML data. In VLDB.
Eric Brill. 2000. Pattern-based disambiguation for natu-
ral language processing. In SIGDAT.
William W. Cohen and Andrew McCallum. 2003. Infor-
mation Extraction from the World Wide Web. in KDD
William W. Cohen. 2004. Minorthird: Methods for
identifying names and ontological relations in text
using heuristics for inducing regularities from data.
http://minorthird.sourceforge.net.
William W. Cohen et al 2005. Learning to Understand
Web Site Update Requests. In IJCAI.
Fabio Ciravegna. 2001. Adaptive information extraction
from text by rule induction and generalization. In IJ-
CAI.
H. Cunningham. 1999. JAPE ? a java annotation patterns
engine.
Francois Denis et al 2004. Learning regular languages
using RFSAs. Theor. Comput. Sci., 313(2):267?294.
Francois Denis. 2001. Learning regular languages
from simple positive examples. Machine Learning,
44(1/2):37?66.
Pedro DeRose et al 2007. DBLife: A Community In-
formation Management Platform for the Database Re-
search Community In CIDR
Pierre Dupont. 1996. Incremental regular inference. In
ICGI.
Ronen Feldman et al. 2006. Self-supervised Relation
Extraction from the Web. In ISMIS.
Henning Fernau. 2005. Algorithms for learning regular
expressions. In ALT.
Laura Firoiu et al 1998. Learning regular languages
from positive evidence. In CogSci.
K. Fukuda et al 1998. Toward information extraction:
identifying protein names from biological papers. Pac
Symp Biocomput., 1998:707?718
Ugo Galassi and Attilio Giordana. 2005. Learning regu-
lar expressions from noisy sequences. In SARA.
Minos Garofalakis et al 2000. XTRACT: a system for
extracting document type descriptors from XML doc-
uments. In SIGMOD.
Hong Lei Guo et al 2006. Empirical Study on the
Performance Stability of Named Entity Recognition
Model across Domains In EMNLP.
Java Regular Expressions. 2008. http://java.sun.com
/javase/6/docs/api/java/util/regex/package-
summary.html.
Dan Klein et al 2003. Named Entity Recognition with
Character-Level Models. In HLT-NAACL.
Vijay Krishnan and Christopher D. Manning. 2006. An
Effective Two-Stage Model for Exploiting Non-Local
Dependencies in Named Entity Recognition. In ACL.
Yunyao Li et al 2006. Getting work done on the web:
Supporting transactional queries. In SIGIR.
Andrew McCallum et al 2000. Maximum Entropy
Markov Models for Information Extraction and Seg-
mentation. In ICML.
Einat Minkov et al 2005. Extracting personal names
from emails: Applying named entity recognition to in-
formal text. In HLT/EMNLP.
Stephen Soderland. 1999. Learning information extrac-
tion rules for semi-structured and free text. Machine
Learning, 34:233?272.
Lorraine Tanabe and W. John Wilbur 2002. Tagging
gene and protein names in biomedical text. Bioinfor-
matics, 18:1124?1132.
Tianhao Wu and William M. Pottenger. 2005. A semi-
supervised active learning algorithm for information
extraction from textual data. JASIST, 56(3):258?271.
Huaiyu Zhu, Alexander Loeser, Sriram Raghavan, Shiv-
akumar Vaithyanathan 2007. Navigating the intranet
with high precision. In WWW.
30
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1002?1012,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Domain Adaptation of Rule-Based Annotators
for Named-Entity Recognition Tasks
Laura Chiticariu Rajasekar Krishnamurthy Yunyao Li Frederick Reiss Shivakumar Vaithyanathan
IBM Research ? Almaden
650 Harry Road, San Jose, CA 95120, USA
{chiti, rajase, yunyaoli, frreiss}@us.ibm.com, shiv@almaden.ibm.com
Abstract
Named-entity recognition (NER) is an impor-
tant task required in a wide variety of ap-
plications. While rule-based systems are ap-
pealing due to their well-known ?explainabil-
ity,? most, if not all, state-of-the-art results
for NER tasks are based on machine learning
techniques. Motivated by these results, we ex-
plore the following natural question in this pa-
per: Are rule-based systems still a viable ap-
proach to named-entity recognition? Specif-
ically, we have designed and implemented
a high-level language NERL on top of Sys-
temT, a general-purpose algebraic informa-
tion extraction system. NERL is tuned to the
needs of NER tasks and simplifies the pro-
cess of building, understanding, and customiz-
ing complex rule-based named-entity annota-
tors. We show that these customized annota-
tors match or outperform the best published
results achieved with machine learning tech-
niques. These results confirm that we can
reap the benefits of rule-based extractors? ex-
plainability without sacrificing accuracy. We
conclude by discussing lessons learned while
building and customizing complex rule-based
annotators and outlining several research di-
rections towards facilitating rule development.
1 Introduction
Named-entity recognition (NER) is the task of iden-
tifying mentions of rigid designators from text be-
longing to named-entity types such as persons, orga-
nizations and locations (Nadeau and Sekine, 2007).
While NER over formal text such as news articles
and webpages is a well-studied problem (Bikel et
al., 1999; McCallum and Li, 2003; Etzioni et al,
2005), there has been recent work on NER over in-
formal text such as emails and blogs (Huang et al,
2001; Poibeau and Kosseim, 2001; Jansche and Ab-
ney, 2002; Minkov et al, 2005; Gruhl et al, 2009).
The techniques proposed in the literature fall under
three categories: rule-based (Krupka and Hausman,
2001; Sekine and Nobata, 2004), machine learning-
based (O. Bender and Ney, 2003; Florian et al,
2003; McCallum and Li, 2003; Finkel and Manning,
2009; Singh et al, 2010) and hybrid solutions (Sri-
hari et al, 2001; Jansche and Abney, 2002).
1.1 Motivation
Although there are well-established rule-based sys-
tems to perform NER tasks, most, if not all, state-of-
the-art results for NER tasks are based on machine
learning techniques. However, the rule-based ap-
proach is still extremely appealing due to the associ-
ated transparency of the internal system state, which
leads to better explainability of errors (Siniakov,
2010). Ideally, one would like to benefit from the
transparency and explainability of rule-based tech-
niques, while achieving state-of-the-art accuracy.
A particularly challenging aspect of rule-based
NER in practice is domain customization ? cus-
tomizing existing annotators to produce accurate re-
sults in new domains. In machine learning-based
systems, adapting to a new domain has tradition-
ally involved acquiring additional labeled data and
learning a new model from scratch. However, recent
work has proposed more sophisticated approaches
that learn a domain-independent base model, which
can later be adapted to specific domains (Florian et
1002
BASEBALL - MAJOR LEAGUE STANDINGS AFTER TUESDAY 'S GAMES NEW YORK 1996-08-28?
?AMERICAN LEAGUE EASTERN DIVISION W L PCT GB NEW YORK 74 57 .565 -BALTIMORE 70 61 .534 4 BOSTON 68 65 .511 7 
?TEXAS AT KANSAS CITYBOSTON AT CALIFORNIANEW YORK AT SEATTLE
?
BASEBALL - ORIOLES WIN , YANKEESLOSE . BALTIMORE 1996-08-27
?In Seattle , Jay Buhner 's eighth-inning single snapped a tie as the Seattle Mariners edged the New York Yankees 2-1 in the opener of a three-game series .New York starter Jimmy Key left the game in the first inning after Seattle shortstop Alex Rodriguez lined a shot off his left elbow .
?
Document d1 Document d2
Customization Requirement : City, County or State names within sports articles may refer to a sports team  or to the location itself.Customization Solution (CS) :Within sports articles, Identify all occurrences of city/county/state as Organizations, Except when a contextual clue indicates that the reference is to the location
Organization Location
Figure 1: Example Customization Requirement
al., 2004; Blitzer et al, 2006; Jiang and Zhai, 2006;
Arnold et al, 2008; Wu et al, 2009). Implement-
ing a similar approach for rule-based NER typically
requires a significant amount of manual effort to (a)
identify the explicit semantic changes required for
the new domain (e.g., differences in entity type def-
inition), (b) identify the portions of the (complex)
core annotator that should be modified for each dif-
ference and (c) implement the required customiza-
tion rules without compromising the extraction qual-
ity of the core annotator. Domain customization of
rule-based NER has not received much attention in
the recent literature with a few exceptions (Petasis et
al., 2001; Maynard et al, 2003; Zhu et al, 2005).
1.2 Problem Statement
In this paper, we explore the following natural ques-
tion: Are rule-based systems still a viable approach
to named-entity recognition? Specifically, (a) Is it
possible to build, maintain and customize rule-based
NER annotators that match the state-of-the-art re-
sults obtained using machine-learning techniques?
and (b) Can this be achieved with a reasonable
amount of manual effort?
1.3 Contributions
In this paper, we address the challenges mentioned
above by (i) defining a taxonomy of the different
types of customizations that a rule developer may
perform when adapting to a new domain (Sec. 2), (ii)
identifying a set of high-level operations required
for building and customizing NER annotators, and
(iii) exposing these operations in a domain-specific
NER rule language, NERL, developed on top of Sys-
// Core rules identify Organization and Location candidates
// Begin customization// Identify articles covering sports event from article title CR1 <SportsArticle>  Evaluate Regular Expressions <R1>// Identify locations in sports articlesCR2 Retain <Location> As <LocationMaybeOrg> If ContainedWithin <SportsArticle>// City/County/State references (e.g., New York) may refer to the sports team in that cityCR3 Retain <LocationMaybeOrg> If Matches Dictionaries<?cities.dict?,?counties.dict?,?states.dict?>
// Some city references in sports articles may refer to the city (e.g., In Seattle )// These references should not be reclassified as OrganizationCR4 Discard <LocationMaybeOrg> If Matches Regular Expression <R2>on Left Context 2 Tokens
// City references to sports teams are added to Organization and removed from LocationCR5 Augment <Organization> With <LocationMaybeOrg>// End customization
// Continuation of core rules// Remove Locations that overlap with OrganizationsDiscard <Location> If Overlaps Concepts <Organization>Figure 2: Example Customization Rules in NERL
temT (Chiticariu et al, 2010), a general-purpose
algebraic information extraction system (Sec. 3).
NERL is specifically geared towards building and
customizing complex NER annotators and makes it
easy to understand a complex annotator that may
comprise hundreds of rules. It simplifies the iden-
tification of what portions need to be modified for
a given customization requirement. It also makes
individual customizations easier to implement, as il-
lustrated by the following example.
Suppose we have to customize a domain-
independent rule-based NER annotator for the
CoNLL corpus (Tjong et al, 2003). Consider the
two sports-related news articles in Fig. 1 from the
corpus, where city names such as ?New York? or
?Seattle? can refer to either a Location or an Orga-
nization (the sports team based in that city). In the
domain-independent annotator, city names were al-
ways identified as Location, as this subtle require-
ment was not considered during rule development.
A customization to address this issue is shown in
Fig. 1, which can be implemented in NERL with five
rules (Fig. 2). This customization (explained in de-
tail in Sec. 3) improved the F?=1 score for Organi-
zation and Location by approximately 9% and 3%,
respectively (Sec. 4).
We used NERL to customize a domain-
independent rule-based NER annotator for three
different domains ? CoNLL03 (Tjong et al, 2003),
Enron (Minkov et al, 2005) and ACE05 (NIST,
2005). Our experimental results (Sec. 4.3) demon-
strate that the customized annotators have extraction
quality better than the best-known results for
1003
Affects Single Affects Multiple
Entity Type Entity Types
Identify New Instances CS , CDD , CDSD BR
Modify Existing instances CEB , CDD CATA, CG
Table 1: Categorizing NER Customizations
individual domains, which were achieved with
machine learning techniques. The fact that we are
able to achieve such results across multiple domains
answers our earlier question and confirms that
we can reap the benefits of rule-based extractors?
explainability without sacrificing accuracy.
However, we found that even using NERL, the
amount of manual effort and expertise required in
rule-based NER may still be significant. In Sec. 5,
we report on the lessons learned and outline several
interesting research directions towards simplifying
rule development and facilitating the adoption of the
rule-based approach towards NER.
2 Domain Customization for NER
We consider NER tasks following the broad defini-
tion put forth by (Nadeau and Sekine, 2007), for-
mally defined as follows:
Definition 1 Named entity recognition is the task of
identifying and classifying mentions of entities with
one or more rigid designators, as defined by (Kripke,
1982).
For instance, the identification of proper nouns
representing persons, organizations, locations, prod-
uct names, proteins, drugs and chemicals are consid-
ered as NER tasks.
Based on our experience of customizing NER an-
notators for multiple domains, we categorize the
customizations involved into two main categories as
listed below. This categorization motivates the de-
sign of NERL (Sec. 3).
Data-driven (CDD): The most common NER cus-
tomization is data-driven, where the customizations
mostly involve the addition of new patterns and
dictionary entries, driven by observations from the
training data in the new domain. An example is
the addition of a new rule to identify locations from
the beginning of news articles (e.g., ?BALTIMORE
1995-08-27? and ?MURCIA , Spain 1996-09-10?).
Application-driven: What is considered a valid
named entity and its corresponding type can vary
across application domains. The most common di-
mensions on which the definition of a named entity
can vary are:
Entity Boundary (CEB): Different application do-
mains may have different definitions of where the
same entity starts or ends. For example, a Person
may (CoNLL03) or may not (Enron) include gener-
ational markers (e.g. ?Jr.? in ?Bush Jr.? or ?IV? in
?Henry IV?).
Ambiguous Type Assignment (CATA): The exact
type of a given named entity can be ambiguous.
Different applications may assign different types
for the same named entity. For instance, all in-
stances of ?White House? may be considered as Lo-
cation (CoNLL03), or be assigned as Facility or Or-
ganization based on their context (ACE05). In fact,
even within the same application domain, entities
typically considered as of the same type may be as-
signed differently. For example, given ?New York
beat Seattle? and ?Ethiopia beat Uganda?, both
?New York? and ?Ethiopia? are teams referred by their
locations. However, (Tjong et al, 2003) considers
the former, which corresponds to a city, as an Orga-
nization, and the latter, which corresponds to a coun-
try, as a Location.
Domain-Specific Definition (CDSD): Whether a
given term is even considered a named entity may
depend on the specific domain. As an example, con-
sider the text ?Commercialization Meeting - SBeck,
BHall, BSuperty, TBusby, SGandhi-Gupta?. Infor-
mal names such as ?SBeck? and ?BHall? may be con-
sidered as valid person names (Enron).
Scope(CS): Each type of named entity usually con-
tains several subtypes. For the same named en-
tity task, different applications may choose to in-
clude different sets of subtypes. For instance,
roads and buildings are considered part of Location
in CoNLL03, while they are not included in ACE05.
Granularity(CG): Name entity types are hierarchi-
cal. Different applications may define NER tasks
at different granularities. For instance, in ACE05,
Organization and Location entity types were split
into four entity types (Organization, Location, Geo-
Political Entity and Facility).
The different customizations are summarized as
shown in Tab. 1, based on the following criteria: (i)
whether the customization identifies new instances
or modifies existing instances; and (ii) whether the
1004
customization affects single or multiple entities. For
instance, CS identifies new instances for a single en-
tity type, as it adds instances of a new subtype for an
existing entity type. Note that BR in the table de-
notes the rules used to build the core annotator.
3 Named Entity Rule Language
3.1 Grammar vs. Algebraic NER
Traditionally, rule-based NER systems were based
on the popular CPSL cascading grammar specifi-
cation (Appelt and Onyshkevych, 1998). CPSL is
designed so that rules that adhere to the standard
can be executed efficiently with finite state transduc-
ers. Accordingly, the standard defines a rigid left-to-
right execution model where a region of text can be
matched by at most one rule according to a fixed rule
priority, and where overlapping annotations are dis-
allowed in the output of each grammar phase.
While it simplifies the design of CPSL engines,
the rigidity of the rule matching semantics makes
it difficult to express operations frequently used in
rule-based information extraction. These limitations
have been recognized in the literature, and several
extensions have been proposed to allow more flex-
ible matching semantics, and to allow overlapping
annotations (Cunningham et al, 2000; Boguraev,
2003; Drozdzynski et al, 2004). However, even
with these extensions, common operations such as
filtering annotations (e.g. CR4 in Fig. 2), are dif-
ficult to express in grammars and often require an
escape to custom procedural code.
Recently, several declarative algebraic languages
have been proposed for rule-based IE systems, no-
tably AQL (Chiticariu et al, 2010) and Xlog (Shen
et al, 2007). These languages are not constrained
by the requirement that all rules map onto finite state
transducers, and therefore can express a significantly
richer semantics than grammar-based languages. In
particular, the AQL rule language as implemented in
SystemT (Chiticariu et al, 2010) can express many
common operations used in rule-based information
extraction without requiring custom code. In addi-
tion, the separation of extraction semantics from ex-
ecution enables SystemT?s rule optimizer and effi-
cient runtime engine. Indeed, as shown in (Chiti-
cariu et al, 2010), SystemT can deliver an order of
magitude higher annotation throughput compared to
a state-of-the-art CPSL-based IE system.
Since AQL is a general purpose information ex-
traction rule language, similar to CPSL and JAPE,
it exposes an expressive set of capabilities that go
beyond what is required for NER tasks. These ad-
ditional capabilities can make AQL rules more ver-
bose than is necessary for implementing rules in the
NER domain. For example, Fig. 3 shows how the
same customization rule CR4 from Fig. 2 can be
implemented in JAPE or in AQL. Notice how im-
plementing even a single customization may lead to
defining complex rules (e.g. JAPE-R1, AQL-R1)
and sometimes even using custom code (e.g. JAPE-
R2). As illustrated by this example, the rules in AQL
and JAPE tend to be complex since some operations
? e.g., filtering the outputs of one rule based on the
outputs of another rule ? that are common in NER
rule sets require multiple rules in AQL or multiple
grammar phases in JAPE.
To make NER rules easier to develop and to
understand, we designed and implemented Named
Entity Rule Language (NERL) on top of SystemT.
NERL is a declarative rule language designed specif-
ically for named entity recognition. The design of
NERL draws on our experience with building and
customizing multiple complex NER annotators. In
particular, we have identified the operations required
in practice for such tasks, and expose these opera-
tions as built-in constructs in NERL. In doing so, we
ensure that frequently performed operations can be
expressed succinctly, so as not to complicate the rule
set unnecessarily. As a result, NERL rules for named
entity recognition tasks are significantly more com-
pact and easy to understand than the equivalent AQL
rules. At the same time, NERL rules can easily be
compiled to AQL, allowing our NER rule develop-
ment framework to take advantage of the capabilities
of the SystemT rule optimizer and efficient runtime
execution engine.
3.2 NERL
For the rest of this section, we focus on describ-
ing the types of rules supported in NERL. In Sec. 4,
we shall demonstrate empirically that NERL can be
successfully employed in building and customizing
complex NER annotators.
A NERL rule has the following form:
IntConcept ? RuleBody(IntConcept1, IntConcept2, . . .)
1005
// Some city references in sports articles may refer to the city (e.g., In Seattle )// These references should not be reclassified as OrganizationCR4 Discard <LocationMaybeOrg> If Matches Regular Expression <R2> on Left Context 2 Tokens
Rule in NERL
JAPE Phase 1Rule : AmbiguousLocationContext
({Token}[2]):context({AmbiguousLoc}): annot
  :annot.AmbiguousLoc = {lc = context.string}
JAPE Phase 2Rule : RetainValidLocation
({AmbiguousLoc.lc =~ R2}):ambiguousloc -->{  // rule to discard ambiguous locationsAnnotationSet loc = bindings.get(?ambiguousloc");
outputAS.removeAll(loc); }
Rule : RetainValidLocation({Token}[2]):context({AmbiguousLoc}):loc    -->{   // Action part in Java to test R2 on left context // and delete annotationAnnotationSet loc = bindings.get(?loc");AnnotationSet context = bindings.get(?context");int begOffset = context.firstNode().getOffset().intValue(); int endOffset = context.lastNode().getOffset().intValue(); String mydocContent = doc.getContent().toString(); String contextString =
mydocContent.substring(begOffset, endOffset);if (Pattern.matches(?R2?, contextString)) {
outputAS.removeAll(loc); }}
create view LocationMaybeOrgInvalid as
select LMO.value as valuefrom LocationMaybeOrg LMO 
where MatchesRegex(/R2/,LeftContextTok(LMO.value,2));
create view LocationMaybeOrgValid as(select LMO.value as value from LocationMaybeOrg LMO)
minus(select LMOI.value as value from LocationMaybeOrgInvalid LMOI);
Two Alternative Rule sets in JAPE Equivalent Rule set in AQL
JAPE-R1 JAPE-R2 AQL-R1
Figure 3: Single Customization Rule expressed in NERL, JAPE and AQL
Intuitively, a NERL rule creates an intermediate con-
cept or named entity (IntConcept for short) by ap-
plying a NERL rule on the input text and zero or
more previously defined intermediate concepts.
NERL Rule Types The types of rules supported in
NERL are summarized in Tab. 2. In what follows,
we illustrate these types by means of examples.
Feature definition (FD): FD rules identify basic
features from text (e.g., FirstName, LastName and
CapsWord features for identifying person names).
Candidate definition (CD): CD rules identify com-
plete occurrences of the target entity. For instance,
the Sequence rule ?LastName followed by ?,? fol-
lowed by FirstName? identifies person annotations
as a sequence of three tokens, where the first and
third tokens occur in dictionaries containing last and
first names.
Candidate Refinement (CR): CR rules are used to
refine candidates generated for different annotation
types. E.g., the Filter rule CR3 in Fig. 2 retains Loca-
tionMaybeOrg annotations that appear in one of sev-
eral dictionaries.
Consolidation (CO): CO rules are used to resolve
overlapping candidates generated by multiple CD
rules. For instance, consider the text ?Please see
the following request from Dr. Kenneth Lim of the
BAAQMD.?. A CD rule may identify ?Dr. Kenneth
Lim? as a person, while another CD rule may identify
?Kenneth Lim? as a candidate person. A consolidation
rule is then used to merge these two annotations to
produce a single annotation for ?Dr. Kenneth Lim?.
NERL Examples Within these categories, three
types of rules deserve special attention, as they cor-
respond to frequently used operations and are specif-
ically designed to ensure compactness of the rule-
set. In contrast, as discussed earlier (Fig. 3), each of
these operations require several rules and possibly
custom code in existing rule-based IE systems.
DynamicDict: The DynamicDict rule is used to create
customized gazetteers on the fly. The following ex-
ample shows the need for such a rule: While ?Clin-
ton? does not always refer to a person?s last name
(Clinton is the name of several cities in USA), in
documents containing a full person name with ?Clin-
ton? as a last name (e.g., ?Hillary Clinton?) it is rea-
sonable to annotate all references to the (possibly)
ambiguous word ?Clinton? as a person. This goal
can be accomplished using the rule <Create Dynamic
Dictionary using Person with length 1 to 2 tokens>,
which creates a gazetteer on a per-document basis.
Filter: The Filter rule is used to discard/retain cer-
tain intermediate annotations based on predicates on
the annotation text and its local context. Example
filtering predicates include
? Discard C If Matches Regular Expression R
? Retain C If Contains Dictionary D on Local Context LC
? Discard C If Overlaps Concepts C1, C2, . . .
ModifySpan: The ModifySpan rule is used to expand
or trim the span of a candidate annotation. For
instance, an Entity Boundary customization to in-
clude generational markers as part of a Person anno-
tation can be implemented using a ModifySpan rule
<Expand Person Using Dictionary ?generation.dict? on
RightContext 2 Tokens>.
Using NERL Tab. 2 shows how different types of
rules are used during rule building and customiza-
tions. Since BR and CS involve identifying one
1006
Rule Category Syntax BR CDD CG
CS CDSD CEB CATA
Dictionary FD Evaluate Dictionaries < D1, D2, . . . > with flags? X X
Regex FD Evaluate Regular Expressions < R1, R2, . . . > with flags? X X
PoS FD Evaluate Part of Speech < P1, P2, . . . > with language < L >? X X
DynamicDict FD Create Dynamic Dictionary using IntConcept with flags? X X
Sequence CD IntConceptorString multiplicity?
(followed by IntConceptorString multiplicity?)+ X X
Filter CR Discard/Retain IntConcept(As IntConcept)?
If SatisfiesPredicate on LocalContext X X X
ModifySpan CR Trim/Expand IntConcept Using Dictionary < D >
on LocalContext X X
Augment CO Augment IntConcept With IntConcept X X
Consolidate CO Consolidate IntConcept using ConsolidationPolicy X X
Table 2: Description of rules supported in NERL
or more entity (sub)types from scratch, all types
of rules are used. CDD and CDSD identify addi-
tional instances for an existing type and therefore
mainly rely on FD and CD rules. On the other hand,
the customizations that modify existing instances
(CEB ,CATA,CG) require CR and CO rules.
Revisiting the example in Fig. 2, CR rules were
used to implement a fairly sophisticated customiza-
tion in a compact fashion, as follows. Rule CR1
first identifies sports articles using a regular expres-
sion based on the article title. Rule CR2 marks
Locations within these articles as LocationMaybeOrg
and Rule CR3 only retains those occurrences that
match a city, county or state name (e.g., ?Seattle?).
Rule CR4 identifies occurrences that have a contex-
tual clue confirming that the mention was to a lo-
cation (e.g., ?In? or ?At?). These occurrences are al-
ready classified correctly as Location and do not need
to be changed. Finally, CR5 adds the remaining am-
biguous mentions to Organization, which would be
deleted from Location by a subsequent core rule.
4 Development and Customization of NER
extractors with NERL
Using NERL, we have developed CoreNER, a
domain-independent generic library for multiple
NER extraction tasks commonly encountered in
practice, including Person, Organization, Location,
EmailAddress, PhoneNumber, URL, and DateTime, but
we shall focus the discussion on the first three tasks
(see Tab. 3 for entity definitions), since they are the
most challenging. In this section, we first overview
the process of developing CoreNER (Sec. 4.1). We
then describe how we have customized CoreNER
for three different domains (Sec. 4.2), and present
a quality comparison with best published results ob-
tained with state-of-the-art machine learning tech-
niques (Sec. 4.3). The tasks we consider are not re-
stricted to documents in a particular language, but
due to limited availability of non-English corpora
and extractors for comparison, our evaluation uses
English-language text. In Sec. 5 we shall elaborate
on the difficulties encountered while building and
customizing CoreNER using NERL and the lessons
we learned in the process.
4.1 Developing CoreNER
We have built our domain independent CoreNER li-
brary using a variety of formal and informal text
(e.g. web pages, emails, blogs, etc.), and informa-
tion from public data sources such as the US Census
Bureau (Census, 2007) and Wikipedia.
The development process proceeded as follows.
We first collected dictionaries for each entity
type from different resources, followed by man-
ual cleanup when needed to categorize entries col-
lected into ?strong? and ?weak? dictionaries. For
instance, we used US Census data to create several
name dictionaries, placing ambiguous entries such
as ?White? and ?Price? in a dictionary of ambigu-
ous last names, while unambiguous entries such as
?Johnson? and ?Williams? went to the dictionary for
strict last names. Second, we developed FD and
CD rules to identify candidate entities based on the
way named entities generally occur in text. E.g.,
<Salutation CapsWord CapsWord> and <FirstName
1007
Type Subtypes
PER individual
LOC
Address, Boundary, Land-Region-Natural, Region-General,
Region-International, Airport, Buildings-Grounds, Path, Plant,
Subarea-Facility, Continent, Country-or-District, Nation,
Population-Center, State-or-Province
ORG Commercial, Educational, Government, Media, Medical-ScienceNon-Governmental
Table 3: NER Task Types and Subtypes
LastName> for Person, and <CapsWord{1,3} OrgSuf-
fix> and <CapsWord{1,2} Industry> for Organization.
We then added CR and CO rules to account for
contextual clues and overlapping annotations (e.g.,
Delete Person annotations appearing within an Orga-
nization annotation).
The final CoreNER library consists of 104 FD (in-
volving 68 dictionaries, 33 regexes and 3 dynamic
dictionaries), 74 CD, 123 CR and 102 CO rules.
4.2 Customizing CoreNER
In this section we describe the process of customiz-
ing our domain-independent CoreNER library for
several different datasets. We start by discussing our
choice of datasets to use for customization.
Datasets For a rigorous evaluation of CoreNER?s
customizability, we require multiple datasets satis-
fying the following criteria: First, the datasets must
cover diverse sources and styles of text. Second,
the set of the most challenging NER tasks Person,
Organization and Location (see Tab. 3) considered
in CoreNER should be applicable to them. Finally,
they should be publicly available and preferably
have associated published results, against which we
can compare our experimental results. Towards this
end, we chose the following public datasets.
? CoNLL03 (Tjong et al, 2003): a collection of
Reuters news stories. Consists of formal text.
? Enron (Minkov et al, 2005): a collection of
emails with meeting information from the Enron
dataset. Contains predominantly informal text.
? ACE05 (NIST, 2005)1 a collection of broadcast
news, broadcast conversations and newswire re-
ports. Consists of both formal and informal text.
Customization Process The goal of customization
1The evaluation test set is not publicly available. Thus, fol-
lowing the example of (Florian et al, 2006), the publicly avail-
able set is split into a 80%/20% data split, with the last 20% of
the data in chronological order selected as test data.
is to refine the original CoreNER (hence referred
to as CoreNERorig) in order to improve its extrac-
tion quality on the training set (in terms of F?=1)
for each dataset individually. In addition, a devel-
opment set is available for CoNLL03 (referred to as
CoNLL03dev), therefore we seek to improve F?=1 on
CoNLL03dev as well.
The customization process for each dataset pro-
ceeded as follows. First, we studied the entity defini-
tions and identified their differences when compared
with the definitions used for CoreNERorig (Tab. 3).
We then added rules to account for the differences.
For example, the definition of Organization in the
CoNLL03 dataset contained a sports organization
subtype, which was not considered when develop-
ing CoreNER. Therefore, we have used public data
sources (e.g., Wikipedia) to collect and curate dic-
tionaries of major sports associations and sport clubs
from around the world. The new dictionaries, along
with regular expressions identifying sports teams in
sports articles were used for defining FD and CD
rules such as CR1 (Fig. 2). Finally, CR and CO rules
were added to filter invalid candidates and augment
the Organization type with the new sports subtype
(similar in spirit to rules CR4 and CR5 in Fig. 2).
In addition to the train and development sets, the
customization process for CoNLL03 also involved
unlabeled data from the corpus as follows. 1) Since
data-driven rules (CDD) are often created based on a
few instances from the training data, testing them on
the unlabeled data helped fine tune the rules for pre-
cision. 2) CoNLL03 is largely dominated by sports
news, but only a subset of all sports were represented
in the train dataset. Using the unlabeled data, we
were able to add CDD rules for five additional types
of sports, resulting in 0.31% improvement in F?=1
score on CoNLL03dev. 3) Unlabeled data was also
useful in identifying domain-specific gazetteers by
using simple extraction rules followed by a man-
ual cleanup phase. For instance, for CoNLL03 we
collected five gazetteers of organization and person
names from the unlabeled data, resulting in 0.45%
improvement in recall for CoNLL03dev.
The quality of the customization on the train col-
lections is shown in Tab. 5. The total number of
rules added during customization for each of the
three domains is listed in Tab. 4. Notice how rules
of all four types are used both in the development
1008
FD CD CR CO
CoreNERorig 104 74 123 102
CoreNERconll 179 56 284 71
CoreNERenron 13 10 9 1
CoreNERace 83 35 117 26
Table 4: Rules added during customization
Precision Recall F?=1
CoreNERconll 97.64 95.60 96.61
CoreNERenron 91.15 92.58 91.86
CoreNERace 92.32 91.22 91.77
Table 5: Quality of customization on train datasets (%)
of the domain independent NER annotator, and dur-
ing customizations for different domains. A total of
8 person weeks were spent on customizations, and
we believe this effort is quite reasonable by rule-
based extraction standards. For example, (Maynard
et al, 2003) reports that customizing the ANNIE
domain independent NER annotator developed us-
ing the JAPE grammar-based rule language for the
ACE05 dataset required 6 weeks (and subsequent
tuning over the next 6 months), resulting in im-
proving the quality to 82% for this dataset. As we
shall discuss shortly, with similar manual effort, we
were able to achieve results outperforming state-of-
art published results on three different datasets, in-
cluding ACE05. However, one may rightfully ar-
gue that the process is still too lengthy impeding the
widespread deployment of rule-based NER extrac-
tion. We elaborate on the effort involved and the
lessons learned in the process in Sec. 5.
4.3 Evaluation of Customization
We now present an experimental evaluation of the
customizability of CoreNER. The main goals are
to investigate: (i) the feasibility of CoreNER cus-
tomization for different application domains; (ii)
the effectiveness of such customization compared to
state-of-the-art results; (iii) the impact of different
types of customization (Tab. 1); and (iv) how often
different categories of NERL rules (Tab. 2) are used
during customization.
We measured the effectiveness of customization
using the improvement in extraction quality of the
customized CoreNER over CoreNERorig. As shown
in Tab. 6, customization significantly improved
Precision Recall F?=1
CoNLL03dev
CoreNERorig 83.81 61.77 71.12
CoreNERconll 96.49 93.76 95.11
Improvement 12.68 31.99 13.99
CoNLL03test
CoreNERorig 77.21 54.87 64.15
CoreNERconll 93.89 89.75 91.77
Improvement 15.68 34.88 27.62
Enron
CoreNERorig 85.06 69.55 76.53
CoreNERenron 88.41 82.39 85.29
Improvement 3.35 12.84 8.76
ACE2005
CoreNERorig 57.23 57.41 57.32
CoreNERace 90.11 87.82 88.95
Improvement 32.88 30.41 31.63
Table 6: Overall Improvement due to Customization (%)
Precision Recall F?=1
LOC CoreNERconll 97.17 95.37 96.26
CoNLL03dev
Florian 96.59 95.65 96.12
ORG CoreNERconll 93.70 88.67 91.11Florian 90.85 89.63 90.24
PER CoreNERconll 97.79 95.87 96.82Florian 96.08 97.12 96.60
LOC CoreNERconll 93.11 91.61 92.35
CoNLL03test
Florian 90.59 91.73 91.15
ORG CoreNERconll 92.25 85.31 88.65Florian 85.93 83.44 84.67
PER CoreNERconll 96.32 92.39 94.32Florian 92.49 95.24 93.85
Enron PER CoreNERenron 87.27 81.82 84.46Minkov 81.1 74.9 77.9
Table 7: Comparison with state-of-the-art results(%)
F?=1 score for CoreNERorig across all datasets. 2
We note that the extraction quality of
CoreNERorig was low on CoNLL03 and ACE05
mainly due to differences in entity type definitions.
In particular, sports organizations, which occurred
frequently in the CoNLL03 collection, were not
considered during the development of CoreNERorig,
while in ACE05, ORG and LOC entity types were
split into four entity types (Organization, Location,
Geo-Political Entity and Facility). Customizations
such as CS and CG address the above changes
in named-entity type definition and substantially
improve the extraction quality of CoreNERorig.
Next, we compare the extraction quality of the
2CoNLL03dev and CoNLL03test correspond to the develop-
ment and test sets for CoNLL03 respectively.
1009
customized CoreNER for CoNLL03 and Enron3 with
the corresponding best published results by (Florian
et al, 2003) and (Minkov et al, 2005). Tab. 7 shows
that our customized CoreNER outperforms the cor-
responding state-of-the-art numbers for all the NER
tasks on both CoNLL03 and Enron. 4 These results
demonstrate that high-quality annotators can be built
by customizing CoreNERorig using NERL, with the
final extraction quality matching that of state-of-the-
art machine learning-based extractors.
It is worthwhile noting that the best pub-
lished results for CoNLL03 (Florian et al, 2003)
were obtained by using four different classifiers
(Robust Risk Minimization, Maximum Entropy,
Transformation-based learning, and Hidden Markov
Model) and trying six different classifier combi-
nation methods. Compared to the best published
result obtained by combining the four classifiers,
the individual classifiers performed between 2.5-
7.6% worse for Location, 5.6-15.2% for Organiza-
tion and 3.9-14.0% for Person5. Taking this into
account, the extraction quality advantage of cus-
tomized CoreNER is significant when compared
with the individual state-of-the-art classifiers.
Impact of Customizations by Type. While cus-
tomizing CoreNER for the three datasets, all types
of changes described in Sec. 2 were performed. We
measured the impact of each type of customization
by comparing the extraction quality of CoreNERorig
with CoreNERorig enhanced with all the customiza-
tions of that type. From the results for CoNLL03
(Tab. 8), we make the following observations.
? Customizations that identify additional subtypes
of entities (CS) or modify existing instances for
multiple types (CATA) have significant impact.
This effect can be especially high when the miss-
ing subtype appears very often in the new do-
main (E.g., over 50% of the organizations in
CoNLL03 are sports teams).
? Data-driven customizations (CDD) rely on the
aggregated impact of many rules. While individ-
ual rules may have considerable impact on their
3We cannot meaningfully compare our results against previ-
ously published results for ACE05, which is originally used for
mention detection while CoreNER considers only NER tasks.
4For Enron the comparison is reported only for Person, as
labeled data is available only for that type.
5Extended version obtained via private communication.
# rules added Precision Recall F?=1
CEB 3
CoNLL03dev
LOC ?0.21 ?0.22 ?0.22
ORG ?1.35 ?0.38 ?0.59
PER - - -
CoNLL03test
LOC ?0.30 ?0.36 ?0.33
ORG ?0.54 ?0.12 ?0.20
PER - - -
CATA 5
CoNLL03dev
LOC ?7.18 ?0.87 ?3.19
ORG ?1.37 ?10.67 ?9.04
PER ?0.04 - ?0.01
CoNLL03test
LOC ?7.73 ?1.20 ?3.77
ORG ?1.37 ?11.62 ?14.18
PER - - -
CDSD 2
CoNLL03dev
LOC ?0.85 - ?0.45
ORG ?1.00 ?0.07 ?0.01
PER - - -
CoNLL03test
LOC ?0.04 ?0.12 ?0.12
ORG ?0.64 - ?0.04
PER - - -
CS 149
CoNLL03dev
LOC ?1.63 ?0.21 ?0.85
ORG ?11.44 ?40.79 ?39.73
PER ?0.13 - ?0.05
CoNLL03test
LOC ?3.71 ?0.18 ?2.05
ORG ?9.2 ?36.24 ?37.96
PER ?0.58 - ?0.2
CDD 431
CoNLL03dev
LOC ?0.94 ?10.18 ?3.99
ORG ?9.63 ?11.93 ?14.71
PER ?6.12 ?28.5 ?18.84
CoNLL03test
LOC ?1.66 ?6.72 ?1.64
ORG ?8.84 ?12.40 ?15.90
PER ?9.15 ?31.48 ?22.21
Table 8: Impact by customization type on CoNLL03(%)
own (e.g., identifying all names that appear as
part of a player list increases the recall of PER by
over 6% on both CoNLL03dev and CoNLL03test),
the overall impact relies on the accumulative ef-
fect of many small improvements.
? Certain customizations (CEB and CDSD) pro-
vide smaller quality improvements, both per rule
and in aggregate.
5 Lessons Learned
Our experimental evaluation shows that rule-based
annotators can achieve quality comparable to that of
state-of-the-art machine learning techniques. In this
section we discuss three important lessons learned
regarding the human effort involved in developing
such rule-based extractors.
Usefulness of NERL We found NERL very helpful
in that it provided a higher-level abstraction catered
specifically towards NER tasks, thus hiding the com-
plexity inherent in a general-purpose IE rule lan-
guage. In doing so, NERL restricts the large space
of operations possible within a general-purpose lan-
guage to the small number of predefined ?templates?
1010
listed in Tab. 2. (We have shown empirically that our
choice of NERL rules is sufficient to achieve high
accuracy for NER tasks.) Therefore, NERL simpli-
fies development and maintenance of complex NER
extractors, since one does not need to worry about
multiple AQL statements or JAPE grammar phases
for implementing a single conceptual operation such
as filtering (see Fig. 3).
Is NERL Sufficient? Even using NERL, building
and customizing NER rules remains a labor inten-
sive process. Consider the example of designing the
filter rule CR4 from Fig. 3. First, one must exam-
ine multiple false positive Location entities to even
decide that a filter rule is appropriate. Second, one
must understand how those false positives were pro-
duced, and decide accordingly on the particular con-
cept to be used as filter (LocationMaybeOrg in this
case). Finally, one needs to decide how to build the
filter. Tab. 9 lists all the attributes that need to be
specified for a Filter rule, along with examples of
the search space for each rule attribute.
Rule Attributes Examples of Search Space
Location Intermediate Concept to filter
Predicate Type Matches Regex, Contains Dictionary, . . .
Predicate Parameter Regular Expressions, Dictionary Entries, . . .
Context Type Entity text, Left or Right context
Context Parameter k tokens, l characters
Table 9: Search space explored while adding a Filter rule
This search space problem is not unique to filter
rules. In fact, most rules in Tab. 2 have two or more
rule attributes. Therefore, designing an individual
NERL rule remains a time-consuming ?trial and er-
ror? process, in which multiple ?promising? combi-
nations are implemented and evaluated individually
before deciding on a satisfactory final rule.
Tooling for NERL The fact that NERL is a high-
level language exposing a restricted set of operators
can be exploited to reduce the human effort involved
in building NER annotators by enabling the follow-
ing tools:
Annotation Provenance Tools tracking prove-
nance (Cheney et al, 2009) for NERL rules can
help in explaining exactly which sequence of rules
is responsible for producing a given false positive,
thereby enabling one to quickly identify ?misbe-
haved? rules. For instance, one can quickly narrow
down the choices for the location where the filter
rule CR4 (Fig. 2) should be applied based on the
provenance of the false positives. Similarly, tools
for explaining false positives in the spirit of (Huang
et al, 2008), are also conceivable.
Automatic Parameter Learning The most time-
consuming part in building a rule often is to decide
the value of its parameters, especially for FD and
CR rules. For instance, while defining a CR rule,
one has to choose values for the Predicate parame-
ter and the Context parameter (see Tab. 9). Some
parameter values can be learned ? for example, dic-
tionaries (Riloff, 1993) and regular expressions (Li
et al, 2008).
Automatic Rule Refinement Tools automatically
suggesting entire customization rules to a complex
NERL program in the spirit of (Liu et al, 2010) can
further reduce human effort in building NER anno-
tators. With the help of such tools, one only needs
to consider good candidate NERL rules suggested
by the system without having to go through the
conventional manual ?trial and error? process.
6 Conclusion
In this paper, we described NERL, a high-level rule
language for building and customizing NER annota-
tors. We demonstrated that a complex NER annota-
tor built using NERL can be effectively customized
for different domains, achieving extraction quality
superior to the state-of-the-art numbers. However,
our experience also indicates that the process of de-
signing the rules themselves is still manual and time-
consuming. Finally, we discuss how NERL opens
up several interesting research directions towards the
development of sophisticated tooling for automating
some of the rule development tasks.
References
D. E. Appelt and B. Onyshkevych. 1998. The common
pattern specification language. In TIPSTER workshop.
A. Arnold, R. Nallapati, and W. W. Cohen. 2008.
Exploiting feature hierarchy for transfer learning in
named entity recognition. In ACL.
D. M. Bikel, R. L. Schwartz, and R. M. Weischedel.
1999. An algorithm that learns what?s in a name. In
Machine Learning, volume 34, pages 211?231.
J. Blitzer, R. Mcdonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
EMNLP.
1011
B. Boguraev. 2003. Annotation-based finite state pro-
cessing in a large-scale nlp arhitecture. In RANLP.
Census. 2007. U.S. Census Bureau.
http://www.census.gov.
J. Cheney, L. Chiticariu, and W. Tan. 2009. Provenance
in databases: Why, how, and where. Foundations and
Trends in Databases, 1(4):379?474.
L. Chiticariu, R. Krishnamurthy, Y. Li, S. Raghavan,
F. Reiss, and S. Vaithyanathan. 2010. SystemT: An
algebraic approach to declarative information extrac-
tion. In ACL.
H. Cunningham, D. Maynard, and V. Tablan. 2000.
JAPE: a Java Annotation Patterns Engine (Second Edi-
tion). Research Memorandum CS?00?10, Department
of Computer Science, University of Sheffield.
W. Drozdzynski, H. Krieger, J. Piskorski, U. Scha?fer, and
F. Xu. 2004. Shallow processing with unification and
typed feature structures ? foundations and applica-
tions. Ku?nstliche Intelligenz, 1:17?23.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005.
Unsupervised named-entity extraction from the web:
an experimental study. Artif. Intell., 165(1):91?134.
J. R. Finkel and C. D. Manning. 2009. Nested named
entity recognition. In EMNLP.
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named entity recognition through classifier combina-
tion. In CoNLL.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N. Nicolov, and S. Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In NAACL-HLT.
R. Florian, H. Jing, N. Kambhatla, and I. Zitouni. 2006.
Factorizing complex models: A case study in mention
detection. In ACL.
D. Gruhl, M. Nagarajan, J. Pieper, C. Robson, and
A. Sheth. 2009. Context and domain knowledge en-
hanced entity spotting in informal text. In ISWC.
J. Huang, G. Zweig, and M. Padmanabhan. 2001. Infor-
mation extraction from voicemail. In ACL.
Jiansheng Huang, Ting Chen, AnHai Doan, and Jeffrey F.
Naughton. 2008. On the Provenance of Non-Answers
to Queries over Extracted Data. PVLDB, 1(1):736?
747.
M. Jansche and S. Abney. 2002. Information extraction
from voicemail transcripts. In EMNLP.
J. Jiang and C. Zhai. 2006. Exploiting domain structure
for named entity recognition. In NAACL-HLT.
Saul Kripke. 1982. Naming and Necessity. Harvard Uni-
versity Press.
G. R. Krupka and K. Hausman. 2001. IsoQuest Inc.: De-
scription of the NetOwlTM extractor system as used
for MUC-7. In MUC-7.
Y. Li, R. Krishnamurthy, S. Raghavan, S. Vaithyanathan,
and H. V. Jagadish. 2008. Regular expression learning
for information extraction. In EMNLP.
B. Liu, L. Chiticariu, V. Chu, H. V. Jagadish, and F. Reiss.
2010. Automatic Rule Refinement for Information
Extraction. PVLDB, 3.
D. Maynard, K. Bontcheva, and H. Cunningham. 2003.
Towards a semantic extraction of named entities. In
RANLP.
A. McCallum and W. Li. 2003. Early results for named
entity recognition with conditional random fields, fea-
ture induction and web-enhanced lexicons. In CoNLL.
E. Minkov, R. C. Wang, and W. W. Cohen. 2005. Ex-
tracting personal names from emails: Applying named
entity recognition to informal text. In HLT/EMNLP.
D. Nadeau and S. Sekine. 2007. A survey of named
entity recognition and classification. Linguisticae In-
vestigationes, 30(1):3?26.
NIST. 2005. The ace evaluation plan.
F. J. Och O. Bender and H. Ney. 2003. Maximum en-
tropy models for named entity recognition. In CoNLL.
G. Petasis, F. Vichot, F. Wolinski, G. Paliouras,
V. Karkaletsis, and C. Spyropoulos. 2001. Using
machine learning to maintain rule-based named-entity
recognition and classification systems. In ACL.
T. Poibeau and L. Kosseim. 2001. Proper name ex-
traction from non-journalistic texts. In Computational
Linguistics in the Netherlands, pages 144?157.
E. Riloff. 1993. Automatically constructing a dictionary
for information extraction tasks. In KDD.
S. Sekine and C. Nobata. 2004. Definition, dictionaries
and tagger for extended named entity hierarchy. In
Conference on Language Resources and Evaluation.
W. Shen, A. Doan, J. F. Naughton, and R. Ramakrishnan.
2007. Declarative information extraction using data-
log with embedded extraction predicates. In VLDB.
S. Singh, D. Hillard, and C. Leggeteer. 2010. Minimally-
supervised extraction of entities from text advertise-
ments. In NAACL-HLT.
P. Siniakov. 2010. GROPUS - an adaptive rule-based al-
gorithm for information extraction. Ph.D. thesis, Freie
Universitat Berlin.
R. Srihari, C. Niu, and W. Li. 2001. A hybrid approach
for named entity and sub-type tagging. In ANLP.
E. F. Tjong, K. Sang, and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In CoNLL.
D. Wu, W. S. Lee, N. Ye, and H. L. Chieu. 2009. Domain
adaptive bootstrapping for named entity recognition.
In EMNLP.
J. Zhu, V. Uren, and E. Motta. 2005. Espotter: Adaptive
named entity recognition for web browsing. In WM.
1012
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 827?832,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Rule-based Information Extraction is Dead!
Long Live Rule-based Information Extraction Systems!
Laura Chiticariu
IBM Research - Almaden
San Jose, CA
chiti@us.ibm.com
Yunyao Li
IBM Research - Almaden
San Jose, CA
yunyaoli@us.ibm.com
Frederick R. Reiss
IBM Research - Almaden
San Jose, CA
frreiss@us.ibm.com
Abstract
The rise of ?Big Data? analytics over unstruc-
tured text has led to renewed interest in infor-
mation extraction (IE). We surveyed the land-
scape of IE technologies and identified a major
disconnect between industry and academia:
while rule-based IE dominates the commercial
world, it is widely regarded as dead-end tech-
nology by the academia. We believe the dis-
connect stems from the way in which the two
communities measure the benefits and costs of
IE, as well as academia?s perception that rule-
based IE is devoid of research challenges. We
make a case for the importance of rule-based
IE to industry practitioners. We then lay out a
research agenda in advancing the state-of-the-
art in rule-based IE systems which we believe
has the potential to bridge the gap between
academic research and industry practice.
1 Introduction
The recent growth of ?Big Data? analytics over large
quantities of unstructured text has led to increased
interest in information extraction technologies from
both academia and industry (Mendel, 2013).
Most recent academic research in this area starts
from the assumption that statistical machine learn-
ing is the best approach to solving information ex-
traction problems. Figure 1 shows empirical ev-
idence of this trend drawn from a survey of re-
cent published research papers. We examined the
EMNLP, ACL, and NAACL conference proceedings
from 2003 through 2012 and identified 177 different
EMNLP research papers on the topic of entity ex-
traction. We then classified these papers into three
categories, based on the techniques used: purely
Commercial*Vendors*(2013)*NLP*Papers*(200392012)*
100%$
50%$
0%$
3.5%*
21%$
75%$
Rule,$Based$
Hybrid$
Machine$Learning$Based$
45%*
22%$
33%$
Implementa@ons*of*En@ty*Extrac@on*
Large*Vendors*
67%*
17%$
17%$
All*Vendors*
Figure 1: Fraction of NLP conference papers from
EMNLP, ACL, and NAACL over 10 years that use ma-
chine learning versus rule-based techniques to perform
entity extraction over text (left); the same breakdown for
commercial entity extraction vendors one year after the
end of this 10-year period (right). The rule-based ap-
proach, although largely ignored in the research commu-
nity, dominates the commercial market.
rule-based, purely machine learning-based, or a hy-
brid of the two. We focus on entity extraction, as it
is a classical IE task, and most industrial IE systems
offer this feature.
The left side of the graph shows the breakdown
of research papers according to this categorization.
Only six papers relied solely on rules to perform the
extraction tasks described. The remainder relied en-
tirely or substantially on statistical techniques. As
shown in Figure 2, these fractions were roughly con-
stant across the 10-year period studied, indicating
that attitudes regarding the relative importance of the
different techniques have remained constant.
We found that distinguishing ?hybrid? systems
827
En@ty*Extrac@on*Papers*by*Year*
0%$
25%$
50%$
75%$
100%$
Year$of$PublicaAon$
FracAo
n$of$N
LP$Pap
ers$ Hybrid$
Machine$Learning$Based$
Rule,$Based$
Figure 2: The conference paper data (left-hand bar) from
Figure 1, broken down by year of publication. The rel-
ative fractions of the three different techniques have not
changed significantly over time.
from pure machine learning systems was quite chal-
lenging. The papers that use a mixture of rule-
based and machine learning techniques were gener-
ally written so as to obfuscate the use of rules, em-
phasizing the machine learning aspect of the work.
Authors hid rules behind euphemisms such as ?de-
pendency restrictions? (Mausam et al, 2012), ?en-
tity type constraints? (Yao et al, 2011), or ?seed dic-
tionaries? (Putthividhya and Hu, 2011).
In the commercial world, the situation is largely
reversed. The right side of Figure 1 shows the result
of a parallel survey of commercial entity extraction
products from 54 different vendors listed in (Yuen
and Koehler-Kruener, 2012). We studied analyst
reports and product literature, then classified each
product according to the same three categories. Ta-
ble 1 shows the 41 products considered in the study
1. We conducted this industry survey in 2013, one
year after the ten-year run of NLP papers we stud-
ied. One would expect the industrial landscape to
reflect the research efforts of the previous 10 years,
as mature technology moved from academia to in-
dustry. Instead, results of this second survey showed
the opposite effect, with rule-based systems com-
prising the largest fraction of those surveyed. Only
1/3 of the vendors relied entirely on machine learn-
ing. Among public companies and private compa-
1Other products do not offer entity extraction, or we did not
find sufficient evidence to classify the technology.
Table 1: Vendors and products considered in the study.
ai-one NathanApp
Attensity Command Center
Basis Technology Rosette
Clarabridge Analyze
Daedalus Stilus NER
GATE Information Extraction
General Sentiment
HP Autonomy IDOL Eduction
IBM InfoSphere BigInsights Text
Analytics
IBM InfoSphere Streams Text An-
alytics
IBM SPSS Text Analytics for Sur-
veys
IntraFind iFinder NAMER
IxReveal uHarmonize
Knime
Language Computer Cicero LITE
Lexanalytics Salience
alias-i LingPipe
Marklogic Analytics & Business Intelli-
gence
MeshLabs eZi CORE
Microsoft FAST Search Server
MotiveQuest
Nice Systems NiceTrack Open Source In-
telligence
OpenAmplify Insights
OpenText Content Analytics
Pingar
Provalis Research WordStat
Rapid-I Text Processing Extension
Rocket AeroText
salesforce.com Radian 6
SAP HANA Text Analysis
SAS Text Analytics
Serendio
Smartlogic Semaphore Classification
and Text Mining Server
SRA International NetOwl Text Analytics
StatSoft STATISTICA Text Miner
Temis Luxid Content Enrichment
Platform
Teradata (integration w/ Attensity)
TextKernel Extract!
Thompson Reuters OpenCalais
Veda Semantics Entity Identifier
ZyLab Text Mining&Analytics
828
Table 2: Pros and Cons
Pros Cons
R
ul
e-
ba
se
d
? Declarative ? Heuristic
? Easy to comprehend ? Requires tedious
? Easy to maintain manual labor
? Easy to incorporate
domain knowledge
? Easy to trace and fix
the cause of errors
M
L
-b
as
ed
? Trainable ? Requires labeled data
? Adaptable ? Requires retraining
? Reduces manual for domain adaptation
effort ? Requires ML expertise
to use or maintain
? Opaque
nies with more than $100 million in revenue, the sit-
uation is even more skewed towards rule-based sys-
tems, with large vendors such as IBM, SAP, and Mi-
crosoft being completely rule-based.
2 Explaining the Disconnect
What is the source of this disconnect between re-
search and industry? There does not appear to be
a lack of interaction between the two communities.
Indeed, many of the smaller companies we surveyed
were founded by NLP researchers, and many of the
larger vendors actively publish in the NLP literature.
We believe that the disconnect arises from a differ-
ence in how the two communities measure the costs
and benefits of information extraction.
Table 2 summarizes the pros and cons of machine
learning (ML) and rule-based IE technologies (Atz-
mueller and Kluegl, 2008; Grimes, 2011; Leung et
al., 2011; Feldman and Rosenfeld, 2006; Guo et
al., 2006; Krishnan et al, 2005; Yakushiji et al,
2006; Kluegl et al, 2009). On the surface, both
academia and commercial vendors acknowledge es-
sentially the same pros and cons for the two ap-
proaches. However, the two communities weight the
pros and cons significantly differently, leading to the
drastic disconnect in Figure 1.
Evaluating the benefits of IE. Academic papers
evaluate IE performance in terms of precision and
recall over standard labeled data sets. This simple,
clean, and objective measure is useful for judging
competitions, but the reality of the business world is
much more fluid and less well-defined.
In a business context, definitions of even basic en-
tities like ?product? and ?revenue? vary widely from
one company to another. Within any of these ill-
defined categories, some entities are more important
to get right than others. For example, in electronic
legal discovery, correctly identifying names of ex-
ecutives is much more important than finding other
types of person names.
In real-world applications, the output of extrac-
tion is often the input to a larger process, and it
is the quality of the larger process that drives busi-
ness value. This quality may derive from an aspect
of extracted output that is only loosely correlated
with overall precision and recall. For example, does
extracted sentiment, when broken down and aggre-
gated by product, produce an unbiased estimate of
average sentiment polarity for each product?
To be useful in a business context, IE must func-
tion well with metrics that are ill-defined and sub-
ject to change. ML-based IE models, which require
a careful up-front definition of the IE task, are poor
fit for these metrics. The commercial world greatly
values rule-based IE for its interpretability, which
makes IE programs easier to adopt, understand, de-
bug, and maintain in the face of changing require-
ments (Kluegl et al, 2009; Atzmueller and Kluegl,
2008). Furthermore, rule-based IE programs are val-
ued for allowing one to easily incorporate domain
knowledge, which is essential for targeting specific
business problems (Grimes, 2011). As an example,
an application may pose simple requirements to its
entity recognition component to output only full per-
son names, and not include salutation. With a rule-
based system, such a requirement translates to re-
moving a few rules. On the other hand, a ML-based
approach requires a complete retrain.
Evaluating the costs of IE. In a business setting,
the most significant costs of using information ex-
traction are the labor cost of developing or adapting
extractors for a particular business problem, and the
hardware cost of compute resources required by the
system.
NLP researchers generally have a well-developed
sense of the labor cost of writing extraction rules,
viewing this task as a ?tedious and time-consuming
process? that ?is not really practical? (Yakushiji et
al., 2006). These criticisms are valid, and, as we
829
point out in the next section, they motivate a research
effort to build better languages and tools.
But there is a strong tendency in the NLP lit-
erature to ignore the complex and time-consuming
tasks inherent in solving an extraction problem using
machine learning. These tasks include: defining the
business problem to be solved in strict mathematical
terms; understanding the tradeoffs between different
types of models in the context of the NLP task def-
inition; performing feature engineering based on a
solid working understanding of the chosen model;
and gathering extensive labeled data ? far more
than is needed to measure precision and recall ?
often through clever automation.
All these steps are time-consuming; even highly-
qualified workers with postgraduate degrees rou-
tinely fail to execute them effectively. Not sur-
prisingly, in industry, ML-based systems are often
deemed risky to adopt and difficult to understand
and maintain, largely due to model opaqueness (Fry,
2011; Wagstaff, 2012; Malioutov and Varshney,
2013). The infeasibility of gathering labeled data in
many real-world scenarios further increases the risk
of committing to a ML-based solution.
A measure of the system?s scalability and run-
time efficiency, hardware costs are a function of two
metrics: throughput and memory footprint. These
figures, while extremely important for commercial
vendors, are typically not reported in NLP litera-
ture. Nevertheless, our experience in practice sug-
gests that ML-based approaches are much slower,
and require more memory compared to rule-based
approaches, whose throughput can be in the order
of MB/second/core for complex extraction tasks like
NER (Chiticariu et al, 2010).
The other explanation. Finally, we believe that the
most notable reason behind the academic commu-
nity?s steering away from rule-based IE systems is
the (false) perception of lack of research problems.
The general attitude is one of ?What?s the research
in rule-based IE? Just go ahead and write the rules.?
as indicated by anecdotal evidence and only implic-
itly stated in the literature, where any usage of rules
is significantly underplayed as explained earlier. In
the next section, we strive to debunk this perception.
3 Bridging the Gap
As NLP researchers who also work regularly with
business customers, we have become increasingly
worried about the gap in perception between infor-
mation extraction research and industry. The recent
growth of Big Data analytics has turned IE into big
business (Mendel, 2013). If current trends continue,
the business world will move ahead with unprinci-
pled, ad-hoc solutions to customers? business prob-
lems, while researchers pursue ever more complex
and impractical statistical approaches that become
increasingly irrelevant. Eventually, the gap between
research and practice will become insurmountable,
an outcome in neither community?s best interest.
The academic NLP community needs to stop
treating rule-based IE as a dead-end technology. As
discussed in Section 2, the domination of rule-based
IE systems in the industry is well-justified. Even in
their current form, with ad-hoc solutions built on
techniques from the early 1980?s, rule-based sys-
tems serve the industry needs better than the lat-
est ML techniques. Nonetheless, there is an enor-
mous untapped opportunity for researchers to make
the rule-based approach more principled, effective,
and efficient. In the remainder of this section, we
lay out a research agenda centered around captur-
ing this opportunity. Specifically, taking a systemic
approach to rule-based IE, one can identify a set of
research problems by separating rule development
and deployment. In particular, we believe research
should focus on: (a) data models and rule language,
(b) systems research in rule evaluation and (c) ma-
chine learning research for learning problems in this
richer target language.
Define standard IE rule language and data
model. If research on rule-based IE is to move
forward in a principled way, the community needs
a standard way to express rules. We believe that
the NLP community can replicate the success of
the SQL language in connecting data management
research and practice. SQL has been successful
largely due to: (1) expressivity: the language pro-
vides all primitives required for performing basic
manipulation of structured data, (2) extensibility: the
language can be extended with new features without
fundamental changes to the language, (3) declara-
tivity: the language allows the specification of com-
830
putation logic without describing its control flow,
thus allowing developers to code what the program
should accomplish, rather than how to accomplish it.
An earlier attempt in late 1980?s to formal-
ize a rule language resulted in the Common Pat-
tern Specification Language (CPSL) (Appelt and
Onyshkevych, 1998). While CPSL did not suc-
ceed due to multiple drawbacks, including expres-
sivity limitations, performance limitations, and its
lack of support for core operations such as part of
speech (Chiticariu et al, 2010), CPSL did gain some
traction, e.g., it powers the JAPE language of the
GATE open-source NLP system (Cunningham et al,
2011). Meanwhile, a number of declarative IE lan-
guages developed in the database community, in-
cluding AQL (Chiticariu et al, 2010; Li et al, 2011),
xLog (Shen et al, 2007), and SQL extensions (Wang
et al, 2010; Jain et al, 2009), have shown that for-
malisms of rule-based IE systems are possible, as
exemplified by (Fagin et al, 2013). However, they
largely remain unknown in the NLP community.
We believe now is the right time to establish a
standard IE rule language, drawing from existing
proposals and experience over the past 30 years. To-
wards this goal, IE researchers need to answer the
following questions: What is the right data model to
capture text, annotations over text, and their proper-
ties? Can we establish a standard declarative exten-
sible rule language for processing data in this model
with a clear set of constructs that is sufficiently ex-
pressive to solve most IE tasks encountered so far?
Systems research based on standard IE rule lan-
guage. Standard IE data model and language en-
ables the development of systems implementing the
standard. One may again wonder, ?Where is the re-
search in that?? As in the database community, ini-
tial research should focus on systemic issues such
as data representation and speeding up rule evalua-
tion via automatic performance optimization. Once
baseline systems are established, system-related re-
search would naturally diverge in several directions,
such as extending the language with new primitives
(and corresponding optimizations), and exploring
modern hardware.
ML research based on standard IE rule language.
A standard rule language and corresponding execu-
tion engine enables researchers to use the standard
language as the expressivity of the output model,
and define learning problems for this target lan-
guage, including learning basic primitives such as
regular expressions and dictionaries, or complete
rule sets. (One need not worry about choosing the
language, nor runtime efficiency.) With an expres-
sive rule language, a major challenge is to prevent
the system from generating arbitrarily complex rule
sets, which would be difficult to understand or main-
tain. Some interesting research directions include
devising proper measures for rule complexity, con-
straining the search space such that the learnt rules
closely resemble those written by humans, active
learning techniques to cope with scarcity of labeled
data, and visualization tools to assist rule develop-
ers in exploring and choosing between different au-
tomatically generated rules. Finally, it is conceiv-
able that some problems will not fit in the target
language, and therefore will need alternative solu-
tions. However, the community would have shown
? objectively ? that the problem is not learnable
with the available set of constructs, thus motivating
follow-on research on extending the standard with
new primitives, if possible, or developing novel hy-
brid IE solutions by leveraging the standard IE rule
language together with ML technology.
4 Conclusion
While rule-based IE dominates the commercial
world, it is widely considered obsolete by the
academia. We made a case for the importance
of rule-based approaches to industry practitioners.
Drawing inspiration from the success of SQL and
the database community, we proposed directions
for addressing the disconnect. Specifically, we call
for the standardization of an IE rule language and
outline an ambitious research agenda for NLP re-
searchers who wish to tackle research problems of
wide interest and value in the industry.
Acknowledgments
We would like to thank our colleagues, Howard
Ho, Rajasekar Krishnamurthy, and Shivakumar
Vaithyanathan, as well as the anonymous reviewers
for their thoughtful and constructive comments.
831
References
Douglas E. Appelt and Boyan Onyshkevych. 1998. The
Common Pattern Specification Language. In Proceed-
ings of a workshop held at Baltimore, Maryland: Oc-
tober 13-15, 1998, TIPSTER ?98, pages 23?30.
Martin Atzmueller and Peter Kluegl. 2008. Rule-Based
Information Extraction for Structured Data Acquisi-
tion using TextMarker. In LWA.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li,
Sriram Raghavan, Frederick Reiss, and Shivakumar
Vaithyanathan. 2010. SystemT: An Algebraic Ap-
proach to Declarative Information Extraction. In ACL.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Niraj Aswani, Ian
Roberts, Genevieve Gorrell, Adam Funk, An-
gus Roberts, Danica Damljanovic, Thomas Heitz,
Mark A. Greenwood, Horacio Saggion, Johann
Petrak, Yaoyong Li, and Wim Peters. 2011. Text
Processing with GATE (Version 6), Chapter 8: JAPE:
Regular Expressions over Annotations.
Ronald Fagin, Benny Kimelfeld, Frederick Reiss, and
Stijn Vansummeren. 2013. Spanners: a formal frame-
work for information extraction. In PODS.
Ronen Feldman and Benjamin Rosenfeld. 2006. Boost-
ing Unsupervised Relation Extraction by Using NER.
In EMNLP, pages 473?481.
C. Fry. 2011. Closing the Gap between Analytics and
Action. INFORMS Analytics Mag., 4(6):405.
Seth Grimes. 2011. Text/Content Analyt-
ics 2011: User Perspectives on Solutions.
http://www.medallia.com/resources/item/
text-analytics-market-study/ .
Hong Lei Guo, Li Zhang, and Zhong Su. 2006.
Empirical study on the performance stability of
named entity recognition model across domains.
In EMNLP, pages 509?516.
Alpa Jain, Panagiotis Ipeirotis, and Luis Gravano.
2009. Building query optimizers for information
extraction: the sqout project. SIGMOD Record,
37(4):28?34.
Peter Kluegl, Martin Atzmueller, and Frank Puppe.
2009. TextMarker: A Tool for Rule-Based Infor-
mation Extraction. In UIMA@GSCL Workshop,
pages 233?240.
Vijay Krishnan, Sujatha Das, and Soumen
Chakrabarti. 2005. Enhanced answer type
inference from questions using sequential mod-
els. In HLT, pages 315?322.
Cane Wing-ki Leung, Jing Jiang, Kian Ming A.
Chai, Hai Leong Chieu, and Loo-Nin Teow.
2011. Unsupervised Information Extraction with
Distributional Prior Knowledge. In EMNLP,
pages 814?824.
Yunyao Li, Frederick Reiss, and Laura Chiticariu.
2011. Systemt: A declarative information extrac-
tion system. In ACL.
Dmitry M. Malioutov and Kush R. Varshney. 2013.
Exact rule learning via boolean compressed sens-
ing. In ICML.
Mausam, Michael Schmitz, Stephen Soderland,
Robert Bart, and Oren Etzioni. 2012. Open Lan-
guage Learning for Information Extraction. In
EMNLP-CoNLL, pages 523?534.
Thomas Mendel. 2013. Business
Intelligence and Big Data Trends
2013. http://www.hfsresearch.com/
Business-Intelligence-and-Big-Data-Trends-2013
(accessed March 28th, 2013).
Duangmanee Putthividhya and Junling Hu. 2011.
Bootstrapped Named Entity Recognition for
Product Attribute Extraction. In EMNLP, pages
1557?1567.
Warren Shen, AnHai Doan, Jeffrey F. Naughton, and
Raghu Ramakrishnan. 2007. Declarative Infor-
mation Extraction Using Datalog with Embedded
Extraction Predicates. In VLDB, pages 1033?
1044.
Kiri Wagstaff. 2012. Machine learning that matters.
In ICML.
Daisy Zhe Wang, Eirinaios Michelakis, Michael J.
Franklin, Minos N. Garofalakis, and Joseph M.
Hellerstein. 2010. Probabilistic Declarative In-
formation Extraction. In ICDE.
Akane Yakushiji, Yusuke Miyao, Tomoko Ohta,
Yuka Tateisi, and Jun?ichi Tsujii. 2006. Auto-
matic construction of predicate-argument struc-
ture patterns for biomedical information extrac-
tion. In EMNLP, pages 284?292.
Limin Yao, Aria Haghighi, Sebastian Riedel, and
Andrew McCallum. 2011. Structured Relation
Discovery using Generative Models. In EMNLP,
pages 1456?1466.
Daniel Yuen and Hanns Koehler-Kruener. 2012.
Who?s Who in Text Analytics, September.
832
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 128?137,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
SystemT: An Algebraic Approach to Declarative Information Extraction
Laura Chiticariu Rajasekar Krishnamurthy Yunyao Li
Sriram Raghavan Frederick R. Reiss Shivakumar Vaithyanathan
IBM Research ? Almaden
San Jose, CA, USA
{chiti,sekar,yunyaoli,rsriram,frreiss,vaithyan}@us.ibm.com
Abstract
As information extraction (IE) becomes
more central to enterprise applications,
rule-based IE engines have become in-
creasingly important. In this paper, we
describe SystemT, a rule-based IE sys-
tem whose basic design removes the ex-
pressivity and performance limitations of
current systems based on cascading gram-
mars. SystemT uses a declarative rule
language, AQL, and an optimizer that
generates high-performance algebraic ex-
ecution plans for AQL rules. We com-
pare SystemT?s approach against cascad-
ing grammars, both theoretically and with
a thorough experimental evaluation. Our
results show that SystemT can deliver re-
sult quality comparable to the state-of-the-
art and an order of magnitude higher an-
notation throughput.
1 Introduction
In recent years, enterprises have seen the emer-
gence of important text analytics applications like
compliance and data redaction. This increase,
combined with the inclusion of text into traditional
applications like Business Intelligence, has dra-
matically increased the use of information extrac-
tion (IE) within the enterprise. While the tradi-
tional requirement of extraction quality remains
critical, enterprise applications also demand ef-
ficiency, transparency, customizability and main-
tainability. In recent years, these systemic require-
ments have led to renewed interest in rule-based
IE systems (Doan et al, 2008; SAP, 2010; IBM,
2010; SAS, 2010).
Until recently, rule-based IE systems (Cunning-
ham et al, 2000; Boguraev, 2003; Drozdzynski
et al, 2004) were predominantly based on the
cascading grammar formalism exemplified by the
Common Pattern Specification Language (CPSL)
specification (Appelt and Onyshkevych, 1998). In
CPSL, the input text is viewed as a sequence of an-
notations, and extraction rules are written as pat-
tern/action rules over the lexical features of these
annotations. In a single phase of the grammar, a
set of rules are evaluated in a left-to-right fash-
ion over the input annotations. Multiple grammar
phases are cascaded together, with the evaluation
proceeding in a bottom-up fashion.
As demonstrated by prior work (Grishman and
Sundheim, 1996), grammar-based IE systems can
be effective in many scenarios. However, these
systems suffer from two severe drawbacks. First,
the expressivity of CPSL falls short when used
for complex IE tasks over increasingly pervasive
informal text (emails, blogs, discussion forums
etc.). To address this limitation, grammar-based
IE systems resort to significant amounts of user-
defined code in the rules, combined with pre-
and post-processing stages beyond the scope of
CPSL (Cunningham et al, 2010). Second, the
rigid evaluation order imposed in these systems
has significant performance implications.
Three decades ago, the database community
faced similar expressivity and efficiency chal-
lenges in accessing structured information. The
community addressed these problems by introduc-
ing a relational algebra formalism and an associ-
ated declarative query language SQL. The ground-
breaking work on System R (Chamberlin et al,
1981) demonstrated how the expressivity of SQL
can be efficiently realized in practice by means of
a query optimizer that translates an SQL query into
an optimized query execution plan.
Borrowing ideas from the database community,
we have developed SystemT, a declarative IE sys-
tem based on an algebraic framework, to address
both expressivity and performance issues. In Sys-
temT, extraction rules are expressed in a declar-
ative language called AQL. At compilation time,
128
({First} {Last} ) :full :full.Person
({Caps}  {Last} ) :full :full.Person
({Last} {Token.orth = comma} {Caps | First}) : reverse
:reverse.Person
({First}) : fn  :fn.Person
({Last}) : ln  :ln.Person
({Lookup.majorType = FirstGaz})  : fn  :fn.First
({Lookup.majorType = LastGaz}) : ln  :ln.Last
({Token.orth = upperInitial} | 
{Token.orth = mixedCaps } ) :cw  :cw.Caps
Rule Patterns
50
20
10
10
10
50
50
10
Priority
P2R1
P2R2
P2R3
P2R4
P2R5
P1R1
P1R2
P1R3
RuleId
Input
First
Last
Caps
Token
Output
Person
Input
Lookup
Token
Output
First
Last
Caps
TypesPhase
P2
P1
P2R3        ({Last} {Token.orth = comma} {Caps | First}) : reverse   :reverse.Person
Last followed by Token whose orth attribute has value 
comma followed by Caps or First
Rule part Action part
Create Person
annotation
Bind match 
to variables
Syntax:
Gazetteers containing first names and last names
Figure 1: Cascading grammar for identifying Person names
SystemT translates AQL statements into an al-
gebraic expression called an operator graph that
implements the semantics of the statements. The
SystemT optimizer then picks a fast execution
plan from many logically equivalent plans. Sys-
temT is currently deployed in a multitude of real-
world applications and commercial products1.
We formally demonstrate the superiority of
AQL and SystemT in terms of both expressivity
and efficiency (Section 4). Specifically, we show
that 1) the expressivity of AQL is a strict superset
of CPSL grammars not using external functions
and 2) the search space explored by the SystemT
optimizer includes operator graphs correspond-
ing to efficient finite state transducer implemen-
tations. Finally, we present an extensive experi-
mental evaluation that validates that high-quality
annotators can be developed with SystemT, and
that their runtime performance is an order of mag-
nitude better when compared to annotators devel-
oped with a state-of-the-art grammar-based IE sys-
tem (Section 5).
2 Grammar-based Systems and CPSL
A cascading grammar consists of a sequence of
phases, each of which consists of one or more
rules. Each phase applies its rules from left to
right over an input sequence of annotations and
generates an output sequence of annotations that
the next phase consumes. Most cascading gram-
mar systems today adhere to the CPSL standard.
Fig. 1 shows a sample CPSL grammar that iden-
tifies person names from text in two phases. The
first phase, P1, operates over the results of the tok-
1A trial version is available at
http://www.alphaworks.ibm.com/tech/systemt
Rule skipped 
due to priority 
semantics
CPSL
Phase P1
Last(P1R2) Last(P1R2)
? Mark        Scott     ,            Howard         Smith   ?
First(P1R1) First(P1R1) First(P1R1) Last(P1R2)
CPSL
Phase P2
? Mark        Scott     ,        Howard          Smith    ?
Person(P2R1)
Person (P2R4)
Person(P2R4)
Person (P2R5)
Person(P2R4)
? Mark        Scott     ,            Howard         Smith   ?
First(P1R1) First(P1R1) First(P1R1) Last(P1R2)
JAPE
Phase P1
(Brill) Caps(P1R3) Last(P1R2) Last(P1R2)
Caps(P1R3) Caps(P1R3)
Caps(P1R3)
? Mark        Scott     ,        Howard          Smith    ?
Person(P2R1)
Person (P2R4, P2R5)
JAPE
Phase P2
(Appelt)
Person(P2R1)
Person (P2R2)
Some discarded 
matches omitted
for clarity
? Tomorrow, we will meet Mark Scott, Howard Smith and ?Document d1
Rule fired
Legend
3 persons
identified
2 persons
identified
(a)
(b)
Figure 2: Sample output of CPSL and JAPE
enizer and gazetteer (input types Token and Lookup,
respectively) to identify words that may be part of
a person name. The second phase, P2, identifies
complete names using the results of phase P1.
Applying the above grammar to document d1
(Fig. 2), one would expect that to match ?Mark
Scott? and ?Howard Smith? as Person. However,
as shown in Fig. 2(a), the grammar actually finds
three Person annotations, instead of two. CPSL has
several limitations that lead to such discrepancies:
L1. Lossy sequencing. In a CPSL grammar,
each phase operates on a sequence of annotations
from left to right. If the input annotations to a
phase may overlap with each other, the CPSL en-
gine must drop some of them to create a non-
overlapping sequence. For instance, in phase P1
(Fig. 2(a)), ?Scott? has both a Lookup and a To-
ken annotation. The system has made an arbitrary
choice to retain the Lookup annotation and discard
the Token annotation. Consequently, no Caps anno-
tations are output by phase P1.
L2. Rigid matching priority. CPSL specifies
that, for each input annotation, only one rule can
actually match. When multiple rules match at the
same start position, the following tie-breaker con-
ditions are applied (in order): (a) the rule match-
ing the most annotations in the input stream; (b)
the rule with highest priority; and (c) the rule de-
clared earlier in the grammar. This rigid match-
ing priority can lead to mistakes. For instance,
as illustrated in Fig. 2(a), phase P1 only identi-
fies ?Scott? as a First. Matching priority causes
the grammar to skip the corresponding match for
?Scott? as a Last. Consequently, phase P2 fails to
identify ?Mark Scott? as one single Person.
L3. Limited expressivity in rule patterns. It is
not possible to express rules that compare annota-
tions overlapping with each other. E.g., ?Identify
129
[A-Z]{\w|-}+
DocumentInput Tuple
?
we will meet Mark 
Scott, ?
Output Tuple 2 Span 2Document
Span 1Output Tuple 1 Document
Regex
Caps
Figure 3: Regular Expression Extraction Operator
words that are both capitalized and present in the
FirstGaz gazetteer? or ?Identify Person annotations
that occur within an EmailAddress?.
Extensions to CPSL
In order to address the above limitations, several
extensions to CPSL have been proposed in JAPE,
AFst and XTDL (Cunningham et al, 2000; Bogu-
raev, 2003; Drozdzynski et al, 2004). The exten-
sions are summarized as below, where each solu-
tion Si corresponds to limitation Li.
? S1. Grammar rules are allowed to operate on
graphs of input annotations in JAPE and AFst.
? S2. JAPE introduces more matching regimes
besides the CPSL?s matching priority and thus
allows more flexibility when multiple rules
match at the same starting position.
? S3. The rule part of a pattern has been ex-
panded to allow more expressivity in JAPE,
AFst and XTDL.
Fig. 2(b) illustrates how the above extensions
help in identifying the correct matches ?Mark Scott?
and ?Howard Smith? in JAPE. Phase P1 uses a match-
ing regime (denoted by Brill) that allows multiple
rules to match at the same starting position, and
phase P2 uses CPSL?s matching priority, Appelt.
3 SystemT
SystemT is a declarative IE system based on an
algebraic framework. In SystemT, developers
write rules in a language called AQL. The system
then generates a graph of operators that imple-
ment the semantics of the AQL rules. This decou-
pling allows for greater rule expressivity, because
the rule language is not constrained by the need to
compile to a finite state transducer. Likewise, the
decoupled approach leads to greater flexibility in
choosing an efficient execution strategy, because
many possible operator graphs may exist for the
same AQL annotator.
In the rest of the section, we describe the parts
of SystemT, starting with the algebraic formalism
behind SystemT?s operators.
3.1 Algebraic Foundation of SystemT
SystemT executes IE rules using graphs of op-
erators. The formal definition of these operators
takes the form of an algebra that is similar to the
relational algebra, but with extensions for text pro-
cessing.
The algebra operates over a simple relational
data model with three data types: span, tuple, and
relation. In this data model, a span is a region of
text within a document identified by its ?begin?
and ?end? positions; a tuple is a fixed-size list of
spans. A relation is a multiset of tuples, where ev-
ery tuple in the relation must be of the same size.
Each operator in our algebra implements a single
basic atomic IE operation, producing and consum-
ing sets of tuples.
Fig. 3 illustrates the regular expression ex-
traction operator in the algebra, which per-
forms character-level regular expression match-
ing. Overall, the algebra contains 12 different op-
erators, a full description of which can be found
in (Reiss et al, 2008). The following four oper-
ators are necessary to understand the examples in
this paper:
? The Extract operator (E) performs character-
level operations such as regular expression and
dictionary matching over text, creating a tuple
for each match.
? The Select operator (?) takes as input a set of
tuples and a predicate to apply to the tuples. It
outputs all tuples that satisfy the predicate.
? The Join operator (??) takes as input two sets
of tuples and a predicate to apply to pairs of
tuples from the input sets. It outputs all pairs
of input tuples that satisfy the predicate.
? The consolidate operator (?) takes as input a
set of tuples and the index of a particular col-
umn in those tuples. It removes selected over-
lapping spans from the indicated column, ac-
cording to the specified policy.
3.2 AQL
Extraction rules in SystemT are written in AQL,
a declarative relational language similar in syn-
tax to the database language SQL. We chose SQL
as a basis for our language due to its expres-
sivity and its familiarity. The expressivity of
SQL, which consists of first-order logic predicates
130
Figure 4: Person annotator as AQL query
over sets of tuples, is well-documented and well-
understood (Codd, 1990). As SQL is the pri-
mary interface to most relational database sys-
tems, the language?s syntax and semantics are
common knowledge among enterprise application
programmers. Similar to SQL terminology, we
call a collection of AQL rules an AQL query.
Fig. 4 shows portions of an AQL query. As
can be seen, the basic building block of AQL is
a view: A logical description of a set of tuples in
terms of either the document text (denoted by a
special view called Document) or the contents of
other views. Every SystemT annotator consists
of at least one view. The output view statement in-
dicates that the tuples in a view are part of the final
results of the annotator.
Fig. 4 also illustrates three of the basic con-
structs that can be used to define a view.
? The extract statement specifies basic
character-level extraction primitives to be
applied directly to a tuple.
? The select statement is similar to the SQL
select statement but it contains an additional
consolidate on clause, along with an exten-
sive collection of text-specific predicates.
? The union all statement merges the outputs
of one or more select or extract statements.
To keep rules compact, AQL also provides a
shorthand sequence pattern notation similar to the
syntax of CPSL. For example, the CapsLast
view in Figure 4 could have been written as:
create view CapsLast as
extract pattern <C.name> <L.name>
from Caps C, Last L;
Internally, SystemT translates each of these ex-
tract pattern statements into one or more select
and extract statements.
AQL SystemT
Optimizer
SystemT
Runtime
Compiled
Operator
Graph
Figure 5: The compilation process in SystemT
Figure 6: Execution strategies for the CapsLast rule
in Fig. 4
SystemT has built-in multilingual support in-
cluding tokenization, part of speech and gazetteer
matching for over 20 languages using Language-
Ware (IBM, 2010). Rule developers can utilize
the multilingual support via AQL without hav-
ing to configure or manage any additional re-
sources. In addition, AQL allows user-defined
functions to be used in a restricted context in or-
der to support operations such as validation (e.g.
for extracted credit card numbers), or normaliza-
tion (e.g., compute abbreviations of multi-token
organization candidates that are useful in gener-
ating additional candidates). More details on AQL
can be found in the AQL manual (SystemT, 2010).
3.3 Optimizer and Operator Graph
Grammar-based IE engines place rigid restrictions
on the order in which rules can be executed. Due
to the semantics of the CPSL standard, systems
that implement the standard must use a finite state
transducer that evaluates each level of the cascade
with one or more left to right passes over the entire
token stream.
In contrast, SystemT places no explicit con-
straints on the order of rule evaluation, nor does
it require that intermediate results of an annota-
tor collapse to a fixed-size sequence. As shown in
Fig. 5, the SystemT engine does not execute AQL
directly; instead, the SystemT optimizer compiles
AQL into a graph of operators. By tying a collec-
tion of operators together by their inputs and out-
puts, the system can implement a wide variety of
different execution strategies. Different execution
strategies are associated with different evaluation
costs. The optimizer chooses the execution strat-
egy with the lowest estimated evaluation cost.
131
Fig. 6 presents three possible execution strate-
gies for the CapsLast rule in Fig. 4. If the opti-
mizer estimates that the evaluation cost of Last is
much lower than that of Caps, then it can deter-
mine that Plan C has the lowest evaluation cost
among the three, because Plan C only evaluates
Caps in the ?left? neighborhood for each instance
of Last. More details of our algorithms for enumer-
ating plans can be found in (Reiss et al, 2008).
The optimizer in SystemT chooses the best ex-
ecution plan from a large number of different al-
gebra graphs available to it. Many of these graphs
implement strategies that a transducer could not
express: such as evaluating rules from right to left,
sharing work across different rules, or selectively
skipping rule evaluations. Within this large search
space, there generally exists an execution strategy
that implements the rule semantics far more effi-
ciently than the fastest transducer could. We refer
the reader to (Reiss et al, 2008) for a detailed de-
scription of the types of plan the optimizer consid-
ers, as well as an experimental analysis of the per-
formance benefits of different parts of this search
space.
Several parallel efforts have been made recently
to improve the efficiency of IE tasks by optimiz-
ing low-level feature extraction (Ramakrishnan et
al., 2006; Ramakrishnan et al, 2008; Chandel et
al., 2006) or by reordering operations at a macro-
scopic level (Ipeirotis et al, 2006; Shen et al,
2007; Jain et al, 2009). However, to the best of
our knowledge, SystemT is the only IE system
in which the optimizer generates a full end-to-end
plan, beginning with low-level extraction primi-
tives and ending with the final output tuples.
3.4 Deployment Scenarios
SystemT is designed to be usable in various de-
ployment scenarios. It can be used as a stand-
alone system with its own development and run-
time environment. Furthermore, SystemT ex-
poses a generic Java API that enables the integra-
tion of its runtime environment with other applica-
tions. For example, a specific instantiation of this
API allows SystemT annotators to be seamlessly
embedded in applications using the UIMA analyt-
ics framework (UIMA, 2010).
4 Grammar vs. Algebra
Having described both the traditional cascading
grammar approach and the declarative approach
Figure 7: Supporting Complex Rule Interactions
used in SystemT, we now compare the two in
terms of expressivity and performance.
4.1 Expressivity
In Section 2, we described three expressivity lim-
itations of CPSL grammars: Lossy sequencing,
rigid matching priority, and limited expressivity in
rule patterns. As we noted, cascading grammar
systems extend the CPSL specification in various
ways to provide workarounds for these limitations.
In SystemT, the basic design of the AQL lan-
guage eliminates these three problems without the
need for any special workaround. The key design
difference is that AQL views operate over sets of
tuples, not sequences of tokens. The input or out-
put tuples of a view can contain spans that overlap
in arbitrary ways, so the lossy sequencing prob-
lem never occurs. The annotator will retain these
overlapping spans across any number of views un-
til a view definition explicitly removes the over-
lap. Likewise, the tuples that a given view pro-
duces are in no way constrained by the outputs of
other, unrelated views, so the rigid matching prior-
ity problem never occurs. Finally, the select state-
ment in AQL allows arbitrary predicates over the
cross-product of its input tuple sets, eliminating
the limited expressivity in rule patterns problem.
Beyond eliminating the major limitations of
CPSL grammars, AQL provides a number of other
information extraction operations that even ex-
tended CPSL cannot express without custom code.
Complex rule interactions. Consider an exam-
ple document from the Enron corpus (Minkov et
al., 2005), shown in Fig. 7, which contains a list
of person names. Because the first person in the
list (?Skilling?) is referred to by only a last name,
rule P2R3 in Fig. 1 incorrectly identifies ?Skilling,
Cindy? as a person. Consequently, the output of
phase P2 of the cascading grammar contains sev-
eral mistakes as shown in the figure. This problem
132
went to the Switchfoot concert at the Roxy. It was pretty fun,? The lead singer/guitarist 
was really good, and even though there was another guitarist  (an Asian guy), he ended up 
playing most of the guitar parts, which was really impressive. The biggest surprise though is 
that I actually liked the opening bands. ?I especially liked the first band
Consecutive review snippets are within 25 tokens
At least 4 occurrences of MusicReviewSnippet or GenericReviewSnippet
At least 3 of them should be MusicReviewSnippets
Review ends with one of these.
Start with 
ConcertMention
Complete review is
within 200 tokens
ConcertMention
MusicReviewSnippet
GenericReviewSnippet
Example Rule
Informal Band Review
Figure 8: Extracting informal band reviews from web logs
occurs because CPSL only evaluates rules over
the input sequence in a strict left-to-right fashion.
On the other hand, the AQL query Q1 shown in
the figure applies the following condition: ?Al-
ways discard matches to Rule P2R3 if they overlap
with matches to rules P2R1 or P2R2? (even if the
match to Rule P2R3 starts earlier). Applying this
rule ensures that the person names in the list are
identified correctly. Obtaining the same effect in
grammar-based systems would require the use of
custom code (as recommended by (Cunningham
et al, 2010)).
Counting and Aggregation. Complex extraction
tasks sometimes require operations such as count-
ing and aggregation that go beyond the expressiv-
ity of regular languages, and thus can be expressed
in CPSL only using external functions. One such
task is that of identifying informal concert reviews
embedded within blog entries. Fig. 8 describes, by
example, how these reviews consist of reference
to a live concert followed by several review snip-
pets, some specific to musical performances and
others that are more general review expressions.
An example rule to identify informal reviews is
also shown in the figure. Notice how implement-
ing this rule requires counting the number of Mu-
sicReviewSnippet and GenericReviewSnippet annotations
within a region of text and aggregating this occur-
rence count across the two review types. While
this rule can be written in AQL, it can only be ap-
proximated in CPSL grammars.
Character-Level Regular Expression CPSL
cannot specify character-level regular expressions
that span multiple tokens. In contrast, the extract
regex statement in AQL fully supports these ex-
pressions.
We have described above several cases where
AQL can express concepts that can only be ex-
pressed through external functions in a cascad-
ing grammar. These examples naturally raise the
question of whether similar cases exist where a
cascading grammar can express patterns that can-
not be expressed in AQL.
It turns out that we can make a strong statement
that such examples do not exist. In the absence
of an escape to arbitrary procedural code, AQL is
strictly more expressive than a CPSL grammar. To
state this relationship formally, we first introduce
the following definitions.
We refer to a grammar conforming to the CPSL
specification as a CPSL grammar. When a CPSL
grammar contains no external functions, we refer
to it as a Code-free CPSL grammar. Finally, we
refer to a grammar that conforms to one of the
CPSL, JAPE, AFst and XTDL specifications as an
expanded CPSL grammar.
Ambiguous Grammar Specification An ex-
panded CPSL grammar may be under-specified in
some cases. For example, a single rule contain-
ing the disjunction operator (|) may match a given
region of text in multiple ways. Consider the eval-
uation of Rule P2R3 over the text fragment ?Scott,
Howard? from document d1 (Fig. 1). If ?Howard?
is identified both as Caps and First, then there are
two evaluations for Rule P2R3 over this text frag-
ment. Since the system has to arbitrarily choose
one evaluation, the results of the grammar can be
non-deterministic (as pointed out in (Cunning-
ham et al, 2010)). We refer to a grammar G as
an ambiguous grammar specification for a docu-
ment collection D if the system makes an arbitrary
choice while evaluating G over D.
Definition 1 (UnambigEquiv) A query Q is Un-
ambigEquiv to a cascading grammar G if and only
if for every document collection D, where G is not
an ambiguous grammar specification for D, the
results of the grammar invocation and the query
evaluation are identical.
We now formally compare the expressivity of
AQL and expanded CPSL grammars. The detailed
proof is omitted due to space limitations.
Theorem 1 The class of extraction tasks express-
ible as AQL queries is a strict superset of that ex-
pressible through expanded code-free CPSL gram-
mars. Specifically,
(a) Every expanded code-free CPSL grammar can
be expressed as an UnambigEquiv AQL query.
(b) AQL supports information extraction opera-
tions that cannot be expressed in expanded code-
free CPSL grammars.
133
Proof Outline: (a) A single CPSL grammar can
be expressed in AQL as follows. First, each rule
r in the grammar is translated into a set of AQL
statements. If r does not contain the disjunct (|)
operator, then it is translated into a single AQL
select statement. Otherwise, a set of AQL state-
ments are generated, one for each disjunct opera-
tor in rule r, and the results merged using union
all statements. Then, a union all statement is used
to combine the results of individual rules in the
grammar phase. Finally, the AQL statements for
multiple phases are combined in the same order as
the cascading grammar specification.
The main extensions to CPSL supported by ex-
panded CPSL grammars (listed in Sec. 2) are han-
dled as follows. AQL queries operate on graphs
on annotations just like expanded CPSL gram-
mars. In addition, AQL supports different match-
ing regimes through consolidation operators, span
predicates through selection predicates and co-
references through join operators.
(b) Example operations supported in AQL that
cannot be expressed in expanded code-free CPSL
grammars include (i) character-level regular ex-
pressions spanning multiple tokens, (ii) count-
ing the number of annotations occurring within a
given bounded window and (iii) deleting annota-
tions if they overlap with other annotations start-
ing later in the document. 2
4.2 Performance
For the annotators we test in our experiments
(See Section 5), the SystemT optimizer is able to
choose algebraic plans that are faster than a com-
parable transducer-based implementation. The
question arises as to whether there are other an-
notators for which the traditional transducer ap-
proach is superior. That is, for a given annota-
tor, might there exist a finite state transducer that
is combinatorially faster than any possible algebra
graph? It turns out that this scenario is not possi-
ble, as the theorem below shows.
Definition 2 (Token-Based FST) A token-based
finite state transducer (FST) is a nondeterministic
finite state machine in which state transitions are
triggered by predicates on tokens. A token-based
FST is acyclic if its state graph does not contain
any cycles and has exactly one ?accept? state.
Definition 3 (Thompson?s Algorithm)
Thompson?s algorithm is a common strategy
for evaluating a token-based FST (based on
(Thompson, 1968)). This algorithm processes the
input tokens from left to right, keeping track of the
set of states that are currently active.
Theorem 2 For any acyclic token-based finite
state transducer T , there exists an UnambigEquiv
operator graph G, such that evaluating G has the
same computational complexity as evaluating T
with Thompson?s algorithm starting from each to-
ken position in the input document.
Proof Outline: The proof constructs G by struc-
tural induction over the transducer T . The base
case converts transitions out of the start state into
Extract operators. The inductive case adds a Se-
lect operator to G for each of the remaining state
transitions, with each selection predicate being the
same as the predicate that drives the corresponding
state transition. For each state transition predicate
that T would evaluate when processing a given
document, G performs a constant amount of work
on a single tuple. 2
5 Experimental Evaluation
In this section we present an extensive comparison
study between SystemT and implementations of
expanded CPSL grammar in terms of quality, run-
time performance and resource requirements.
TasksWe chose two tasks for our evaluation:
? NER : named-entity recognition for Person,
Organization, Location, Address, PhoneNumber,
EmailAddress, URL and DateTime.
? BandReview : identify informal reviews in
blogs (Fig. 8).
We chose NER primarily because named-entity
recognition is a well-studied problem and standard
datasets are available for evaluation. For this task
we use GATE and ANNIE for comparison3. We
chose BandReview to conduct performance evalu-
ation for a more complex extraction task.
Datasets. For quality evaluation, we use:
? EnronMeetings (Minkov et al, 2005): collec-
tion of emails with meeting information from
the Enron corpus4 with Person labeled data;
? ACE (NIST, 2005): collection of newswire re-
ports and broadcast news/conversations with
Person, Organization, Location labeled data5.
3To the best of our knowledge, ANNIE (Cunningham et
al., 2002) is the only publicly available NER library imple-
mented in a grammar-based system (JAPE in GATE).
4http://www.cs.cmu.edu/ enron/
5Only entities of type NAM have been considered.
134
Table 1: Datasets for performance evaluation.
Dataset Description of the Content Number of Document size
documents range average
Enronx Emails randomly sampled from the Enron corpus of average size xKB (0.5 < x < 100)2 1000 xKB +/? 10% xKB
WebCrawl Small to medium size web pages representing company news, with HTML tags removed 1931 68b - 388.6KB 8.8KB
FinanceM Medium size financial regulatory filings 100 240KB - 0.9MB 401KB
FinanceL Large size financial regulatory filings 30 1MB - 3.4MB 1.54MB
Table 2: Quality of Person on test datasets.
Precision (%) Recall (%) F1 measure (%)
(Exact/Partial) (Exact/Partial) (Exact/Partial)
EnronMeetings
ANNIE 57.05/76.84 48.59/65.46 52.48/70.69
T-NE 88.41/92.99 82.39/86.65 85.29/89.71
Minkov 81.1/NA 74.9/NA 77.9/NA
ACE
ANNIE 39.41/78.15 30.39/60.27 34.32/68.06
T-NE 93.90/95.82 90.90/92.76 92.38/94.27
Table 1 lists the datasets used for performance
evaluation. The size of FinanceLis purposely
small because GATE takes a significant amount of
time processing large documents (see Sec. 5.2).
Set Up. The experiments were run on a server
with two 2.4 GHz 4-core Intel Xeon CPUs and
64GB of memory. We use GATE 5.1 (build 3431)
and two configurations for ANNIE: 1) the default
configuration, and 2) an optimized configuration
where the Ontotext Japec Transducer6 replaces the
default NE transducer for optimized performance.
We refer to these configurations as ANNIE and
ANNIE-Optimized, respectively.
5.1 Quality Evaluation
The goal of our quality evaluation is two-fold:
to validate that annotators can be built in Sys-
temT with quality comparable to those built in
a grammar-based system; and to ensure a fair
performance comparison between SystemT and
GATE by verifying that the annotators used in the
study are comparable.
Table 2 shows results of our comparison study
for Person annotators. We report the classical
(exact) precision, recall, and F1 measures that
credit only exact matches, and corresponding par-
tial measures that credit partial matches in a fash-
ion similar to (NIST, 2005). As can be seen, T-
NE produced results of significantly higher quality
than ANNIE on both datasets, for the same Person
extraction task. In fact, on EnronMeetings, the F1
measure of T-NE is 7.4% higher than the best pub-
lished result (Minkov et al, 2005). Similar results
6http://www.ontotext.com/gate/japec.html
a) Throughput on Enron
0
100
200
300
400
500
600
700
0 20 40 60 80 100
Average document size (KB)
Th
ro
u
gh
pu
t (
K
B
/s
ec
)
ANNIE
ANNIE-Optimized
T-NE
x
b) Memory Utilization on Enron
0
200
400
600
0 20 40 60 80 100
Average document size (KB)
A
v
g 
H
ea
p 
si
ze
 
(M
B
) ANNIE
ANNIE-Optimized
T-NE
Error bars show
25th and 75th
percentile 
x
Figure 9: Throughput (a) and memory consump-
tion (b) comparisons on Enronx datasets.
can be observed for Organization and Location on
ACE (exact numbers omitted in interest of space).
Clearly, considering the large gap between
ANNIE?s F1 and partial F1 measures on both
datasets, ANNIE?s quality can be improved via
dataset-specific tuning as demonstrated in (May-
nard et al, 2003). However, dataset-specific tun-
ing for ANNIE is beyond the scope of this paper.
Based on the experimental results above and our
previous formal comparison in Sec. 4, we believe
it is reasonable to conclude that annotators can be
built in SystemT of quality at least comparable to
those built in a grammar-based system.
5.2 Performance Evaluation
We now focus our attention on the throughput and
memory behavior of SystemT, and draw a com-
parison with GATE. For this purpose, we have con-
figured both ANNIE and T-NE to identify only the
same eight types of entities listed for NER task.
Throughput. Fig. 9(a) plots the throughput of
the two systems on multiple Enronx datasets with
average document sizes of between 0.5KB and
100KB. For this experiment, both systems ran
with a maximum Java heap size of 1GB.
135
Table 3: Throughput and mean heap size.
ANNIE ANNIE-Optimized T-NE
Dataset ThroughputMemoryThroughput Memory ThroughputMemory
(KB/s) (MB) (KB/s) (MB) (KB/s) (MB)
WebCrawl 23.9 212.6 42.8 201.8 498.9 77.2
FinanceM 18.82 715.1 26.3 601.8 703.5 143.7
FinanceL 19.2 2586.2 21.1 2683.5 954.5 189.6
As shown in Fig. 9(a), even though the through-
put of ANNIE-Optimized (using the optimized trans-
ducer) increases two-fold compared to ANNIE un-
der default configuration, T-NE is between 8 and
24 times faster compared to ANNIE-Optimized. For
both systems, throughput varied with document
size. For T-NE, the relatively low throughput on
very small document sizes (less than 1KB) is due
to fixed overhead in setting up operators to pro-
cess a document. As document size increases, the
overhead becomes less noticeable.
We have observed similar trends on the rest
of the test collections. Table 3 shows that T-
NE is at least an order of magnitude faster than
ANNIE-Optimized across all datasets. In partic-
ular, on FinanceL T-NE?s throughput remains
high, whereas the performance of both ANNIE and
ANNIE-Optimized degraded significantly.
To ascertain whether the difference in perfor-
mance in the two systems is due to low-level com-
ponents such as dictionary evaluation, we per-
formed detailed profiling of the systems. The pro-
filing revealed that 8.2%, 16.2% and respectively
14.2% of the execution time was spent on aver-
age on low-level components in the case of ANNIE,
ANNIE-Optimized and T-NE, respectively, thus lead-
ing us to conclude that the observed differences
are due to SystemT?s efficient use of resources at
a macroscopic level.
Memory utilization. In theory, grammar based
systems can stream tuples through each stage
for minimal memory consumption, whereas Sys-
temT operator graphs may need to materialize in-
termediate results for the full document at certain
points to evaluate the constraints in the original
AQL. The goal of this study is to evaluate whether
this potential problem does occur in practice.
In this experiment we ran both systems with a
maximum heap size of 2GB, and used the Java
garbage collector?s built-in telemetry to measure
the total quantity of live objects in the heap over
time while annotating the different test corpora.
Fig. 9(b) plots the minimum, maximum, and mean
heap sizes with the Enronx datasets. On small doc-
uments of size up to 15KB, memory consumption
is dominated by the fixed size of the data struc-
tures used (e.g., dictionaries, FST/operator graph),
and is comparable for both systems. As docu-
ments get larger, memory consumption increases
for both systems. However, the increase is much
smaller for T-NE compared to that for both AN-
NIE and ANNIE-Optimized. A similar trend can be
observed on the other datasets as shown in Ta-
ble 3. In particular, for FinanceL, both ANNIE and
ANNIE-Optimized required 8GB of Java heap size to
achieve reasonable throughput7 , in contrast to T-
NE which utilized at most 300MB out of the 2GB
of maximum Java heap size allocation.
SystemT requires much less memory than
GATE in general due to its runtime, which monitors
data dependencies between operators and clears
out low-level results when they are no longer
needed. Although a streaming CPSL implemen-
tation is theoretically possible, in practice mecha-
nisms that allow an escape to custom code make it
difficult to decide when an intermediate result will
no longer be used, hence GATE keeps most inter-
mediate data in memory until it is done analyzing
the current document.
The BandReview Task. We conclude by briefly dis-
cussing our experience with the BandReview task
from Fig. 8. We built two versions of this anno-
tator, one in AQL, and the other using expanded
CPSL grammar. The grammar implementation
processed a 4.5GB collection of 1.05 million blogs
in 5.6 hours and output 280 reviews. In contrast,
the SystemT version (85 AQL statements) ex-
tracted 323 reviews in only 10 minutes!
6 Conclusion
In this paper, we described SystemT, a declar-
ative IE system based on an algebraic frame-
work. We presented both formal and empirical
arguments for the benefits of our approach to IE.
Our extensive experimental results show that high-
quality annotators can be built using SystemT,
with an order of magnitude throughput improve-
ment compared to state-of-the-art grammar-based
systems. Going forward, SystemT opens up sev-
eral new areas of research, including implement-
ing better optimization strategies and augmenting
the algebra with additional operators to support
advanced features such as coreference resolution.
7GATE ran out of memory when using less than 5GB of
Java heap size, and thrashed when run with 5GB to 7GB
136
References
Douglas E. Appelt and Boyan Onyshkevych. 1998.
The common pattern specification language. In TIP-
STER workshop.
Branimir Boguraev. 2003. Annotation-based finite
state processing in a large-scale nlp arhitecture. In
RANLP, pages 61?80.
D. D. Chamberlin, A. M. Gilbert, and Robert A. Yost.
1981. A history of System R and SQL/data system.
In vldb.
Amit Chandel, P. C. Nagesh, and Sunita Sarawagi.
2006. Efficient batch top-k search for dictionary-
based entity recognition. In ICDE.
E. F. Codd. 1990. The relational model for database
management: version 2. Addison-Wesley Longman
Publishing Co., Inc., Boston, MA, USA.
H. Cunningham, D. Maynard, and V. Tablan. 2000.
JAPE: a Java Annotation Patterns Engine (Sec-
ond Edition). Research Memorandum CS?00?10,
Department of Computer Science, University of
Sheffield, November.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graphical
development environment for robust NLP tools and
applications. In Proceedings of the 40th Anniver-
sary Meeting of the Association for Computational
Linguistics, pages 168 ? 175.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Marin Dimitrov, Mike
Dowman, Niraj Aswani, Ian Roberts, Yaoyong
Li, and Adam Funk. 2010. Developing language
processing components with gate version 5 (a user
guide).
AnHai Doan, Luis Gravano, Raghu Ramakrishnan, and
Shivakumar Vaithyanathan. 2008. Special issue on
managing information extraction. SIGMOD Record,
37(4).
Witold Drozdzynski, Hans-Ulrich Krieger, Jakub
Piskorski, Ulrich Scha?fer, and Feiyu Xu. 2004.
Shallow processing with unification and typed fea-
ture structures ? foundations and applications.
Ku?nstliche Intelligenz, 1:17?23.
Ralph Grishman and Beth Sundheim. 1996. Message
understanding conference - 6: A brief history. In
COLING, pages 466?471.
IBM. 2010. IBM LanguageWare.
P. G. Ipeirotis, E. Agichtein, P. Jain, and L. Gravano.
2006. To search or to crawl?: towards a query opti-
mizer for text-centric tasks. In SIGMOD.
Alpa Jain, Panagiotis G. Ipeirotis, AnHai Doan, and
Luis Gravano. 2009. Join optimization of informa-
tion extraction output: Quality matters! In ICDE.
Diana Maynard, Kalina Bontcheva, and Hamish Cun-
ningham. 2003. Towards a semantic extraction of
named entities. In Recent Advances in Natural Lan-
guage Processing.
Einat Minkov, Richard C. Wang, and William W. Co-
hen. 2005. Extracting personal names from emails:
Applying named entity recognition to informal text.
In HLT/EMNLP.
NIST. 2005. The ACE evaluation plan.
Ganesh Ramakrishnan, Sreeram Balakrishnan, and
Sachindra Joshi. 2006. Entity annotation based on
inverse index operations. In EMNLP.
Ganesh Ramakrishnan, Sachindra Joshi, Sanjeet Khai-
tan, and Sreeram Balakrishnan. 2008. Optimization
issues in inverted index-based entity annotation. In
InfoScale.
Frederick Reiss, Sriram Raghavan, Rajasekar Kr-
ishnamurthy, Huaiyu Zhu, and Shivakumar
Vaithyanathan. 2008. An algebraic approach to
rule-based information extraction. In ICDE, pages
933?942.
SAP. 2010. Inxight ThingFinder.
SAS. 2010. Text Mining with SAS Text Miner.
Warren Shen, AnHai Doan, Jeffrey F. Naughton, and
Raghu Ramakrishnan. 2007. Declarative informa-
tion extraction using datalog with embedded extrac-
tion predicates. In vldb.
SystemT. 2010. AQL Manual.
http://www.alphaworks.ibm.com/tech/systemt.
Ken Thompson. 1968. Regular expression search al-
gorithm. pages 419?422.
UIMA. 2010. Unstructured Information Management
Architecture.
http://uima.apache.org.
137
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 905?914,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Graph Approach to Spelling Correction in Domain-Centric Search
Zhuowei Bao
University of Pennsylvania
Philadelphia, PA 19104, USA
zhuowei@cis.upenn.edu
Benny Kimelfeld
IBM Research?Almaden
San Jose, CA 95120, USA
kimelfeld@us.ibm.com
Yunyao Li
IBM Research?Almaden
San Jose, CA 95120, USA
yunyaoli@us.ibm.com
Abstract
Spelling correction for keyword-search
queries is challenging in restricted domains
such as personal email (or desktop) search,
due to the scarcity of query logs, and due to
the specialized nature of the domain. For that
task, this paper presents an algorithm that is
based on statistics from the corpus data (rather
than the query log). This algorithm, which
employs a simple graph-based approach, can
incorporate different types of data sources
with different levels of reliability (e.g., email
subject vs. email body), and can handle
complex spelling errors like splitting and
merging of words. An experimental study
shows the superiority of the algorithm over
existing alternatives in the email domain.
1 Introduction
An abundance of applications require spelling cor-
rection, which (at the high level) is the following
task. The user intends to type a chunk q of text,
but types instead the chunk s that contains spelling
errors (which we discuss in detail later), due to un-
careful typing or lack of knowledge of the exact
spelling of q. The goal is to restore q, when given
s. Spelling correction has been extensively studied
in the literature, and we refer the reader to compre-
hensive summaries of prior work (Peterson, 1980;
Kukich, 1992; Jurafsky and Martin, 2000; Mitton,
2010). The focus of this paper is on the special case
where q is a search query, and where s instead of q
is submitted to a search engine (with the goal of re-
trieving documents that match the search query q).
Spelling correction for search queries is important,
because a significant portion of posed queries may
be misspelled (Cucerzan and Brill, 2004). Effective
spelling correction has a major effect on the expe-
rience and effort of the user, who is otherwise re-
quired to ensure the exact spellings of her queries.
Furthermore, it is critical when the exact spelling is
unknown (e.g., person names like Schwarzenegger).
1.1 Spelling Errors
The more common and studied type of spelling error
is word-to-word error: a single word w is misspelled
into another single word w?. The specific spelling er-
rors involved include omission of a character (e.g.,
atachment), inclusion of a redundant character
(e.g., attachement), and replacement of charac-
ters (e.g., attachemnt). The fact that w? is a mis-
spelling of (and should be corrected to) w is denoted
by w? ? w (e.g., atachment ? attachment).
Additional common spelling errors are splitting of
a word, and merging two (or more) words:
? attach ment ? attachment
? emailattachment? email attachment
Part of our experiments, as well as most of our
examples, are from the domain of (personal) email
search. An email from the Enron email collec-
tion (Klimt and Yang, 2004) is shown in Figure 1.
Our running example is the following misspelling of
a search query, involving multiple types of errors.
sadeep kohli excellatach ment ?
sandeep kohli excel attachment (1)
In this example, correction entails fixing sadeep,
splitting excellatach, fixing excell, merging
atach ment, and fixing atachment. Beyond the
complexity of errors, this example also illustrates
other challenges in spelling correction for search.
We need to identify not only that sadeep is mis-
spelled, but also that kohli is correctly spelled.
Just having kohli in a dictionary is not enough.
905
Subject: Follow-Up on Captive Generation
From: sandeep.kohli@enron.com
X-From: Sandeep Kohli
X-To: Stinson Gibner@ECT, Vince J Kaminski@ECT
Vince/Stinson,
Please find below two attachemnts. The Excell spreadsheet
shows some calculations. . . The seond attachement (Word) has
the wordings that I think we can send in to the press. . .
I am availabel on mobile if you have questions o clarifications. . .
Regards,
Sandeep.
Figure 1: Enron email (misspelled words are underlined)
For example, in kohli coupons the user may very
well mean kohls coupons if Sandeep Kohli has
nothing to do with coupons (in contrast to the store
chain Kohl?s). A similar example is the word nail,
which is a legitimate English word, but in the con-
text of email the query nail box is likely to be
a misspelling of mail box (unless nail boxes are
indeed relevant to the user?s email collection). Fi-
nally, while the word kohli is relevant to some
email users (e.g., Kohli?s colleagues), it may have
no meaning at all to other users.
1.2 Domain Knowledge
The common approach to spelling correction uti-
lizes statistical information (Kernighan et al, 1990;
Schierle et al, 2007; Mitton, 2010). As a sim-
ple example, if we want to avoid maintaining a
manually-crafted dictionary to accommodate the
wealth of new terms introduced every day (e.g.,
ipod and ipad), we may decide that atachment
is a misspelling of attachment due to both the
(relative) proximity between the words, and the
fact that attachment is significantly more pop-
ular than atachment. As another example, the
fact that the expression sandeep kohli is fre-
quent in the domain increases our confidence in
sadeep kohli ? sandeep kohli (rather than,
e.g., sadeep kohli ? sudeep kohli). One
can further note that, in email search, the fact that
Sandeep Kohli sent multiple excel attachments in-
creases our confidence in excell ? excel.
A source of statistics widely used in prior work
is the query log (Cucerzan and Brill, 2004; Ahmad
and Kondrak, 2005; Li et al, 2006a; Chen et al,
2007; Sun et al, 2010). However, while query logs
are abundant in the context of Web search, in many
other search applications (e.g. email search, desktop
search, and even small-enterprise search) query logs
are too scarce to provide statistical information that
is sufficient for effective spelling correction. Even
an email provider of a massive scale (such as GMail)
may need to rely on the (possibly tiny) query log of
the single user at hand, due to privacy or security
concerns; moreover, as noted earlier about kohli,
the statistics of one user may be relevant to one user,
while irrelevant to another.
The focus of this paper is on spelling correction
for search applications like the above, where query-
log analysis is impossible or undesirable (with email
search being a prominent example). Our approach
relies mainly on the corpus data (e.g., the collection
of emails of the user at hand) and external, generic
dictionaries (e.g., English). As shown in Figure 1,
the corpus data may very well contain misspelled
words (like query logs do), and such noise is a part of
the challenge. Relying on the corpus has been shown
to be successful in spelling correction for text clean-
ing (Schierle et al, 2007). Nevertheless, as we later
explain, our approach can still incorporate query-log
data as features involved in the correction, as well as
means to refine the parameters.
1.3 Contribution and Outline
As said above, our goal is to devise spelling cor-
rection that relies on the corpus. The corpus often
contains various types of information, with different
levels of reliability (e.g., n-grams from email sub-
jects and sender information, vs. those from email
bodies). The major question is how to effectively
exploit that information while addressing the vari-
ous types of spelling errors such as those discussed
in Section 1.1. The key contribution of this work is
a novel graph-based algorithm, MaxPaths, that han-
dles the different types of errors and incorporates the
corpus data in a uniform (and simple) fashion. We
describe MaxPaths in Section 2. We evaluate the
effectiveness of our algorithm via an experimental
study in Section 3. Finally, we make concluding re-
marks and discuss future directions in Section 4.
2 Spelling-Correction Algorithm
In this section, we describe our algorithm for
spelling correction. Recall that given a search query
906
s of a user who intends to phrase q, the goal is to
find q. Our corpus is essentially a collection D of
unstructured or semistructured documents. For ex-
ample, in email search such a document is an email
with a title, a body, one or more recipients, and so
on. As conventional in spelling correction, we de-
vise a scoring function scoreD(r | s) that estimates
our confidence in r being the correction of s (i.e.,
that r is equal to q). Eventually, we suggest a se-
quence r from a set CD(s) of candidates, such that
scoreD(r | s) is maximal among all the candidates
in CD(s). In this section, we describe our graph-
based approach to finding CD(s) and to determining
scoreD(r | s).
We first give some basic notation. We fix an al-
phabet ? of characters that does not include any
of the conventional whitespace characters. By ??
we denote the set of all the words, namely, fi-
nite sequences over ?. A search query s is a
sequence w1, . . . , wn, where each wi is a word.
For convenience, in our examples we use whites-
pace instead of comma (e.g., sandeep kohli in-
stead of sandeep, kohli). We use the Damerau-
Levenshtein edit distance (as implemented by the
Jazzy tool) as our primary edit distance between two
words r1, r2 ? ??, and we denote this distance by
ed(r1, r2).
2.1 Word-Level Correction
We first handle a restriction of our problem, where
the search query is a single word w (rather than
a general sequence s of words). Moreover, we
consider only candidate suggestions that are words
(rather than sequences of words that account for the
case where w is obtained by merging keywords).
Later, we will use the solution for this restricted
problem as a basic component in our algorithm for
the general problem.
Let UD ? ?? be a finite universal lexicon, which
(conceptually) consists of all the words in the corpus
D. (In practice, one may want add to D words of
auxiliary sources, like English dictionary, and to fil-
ter out noisy words; we did so in the site-search do-
main that is discussed in Section 3.) The set CD(w)
of candidates is defined by
CD(w) def= {w} ? {w? ? UD | ed(w,w?) ? ?} .
for some fixed number ?. Note that CD(w) contains
Table 1: Feature set WFD in email search
Basic Features
ed(w,w?): weighted Damerau-Levenshtein edit distance
ph(w,w?): 1 if w and w? are phonetically equal, 0 otherwise
english(w?): 1 is w? is in English, 0 otherwise
Corpus-Based Features
logfreq(w?)): logarithm of #occurrences of w? in the corpus
Domain-Specific Features
subject(w?): 1 if w? is in some ?Subject? field, 0 otherwise
from(w?): 1 if w? is in some ?From? field, 0 otherwise
xfrom(w?): 1 if w? is in some ?X-From? field, 0 otherwise
w even if w is misspelled; furthermore, CD(w) may
contain other misspelled words (with a small edit
distance to w) that appear in D.
We now define scoreD(w? | w). Here, our cor-
pus D is translated into a set WFD of word features,
where each feature f ? WFD gives a scoring func-
tion scoref (w? | w). The function scoreD(w? | w) is
simply a linear combination of the scoref (w? | w):
scoreD(w? | w) def=
?
f?WFD
af ? scoref (w? | w)
As a concrete example, the features of WFD we used
in the email domain are listed in Table 1; the result-
ing scoref (w? |w) is in the spirit of the noisy channel
model (Kernighan et al, 1990). Note that additional
features could be used, like ones involving the stems
of w and w?, and even query-log statistics (when
available). Rather than manually tuning the param-
eters af , we learned them using the well known
Support Vector Machine, abbreviated SVM (Cortes
and Vapnik, 1995), as also done by Schaback and
Li (2007) for spelling correction. We further discuss
this learning step in Section 3.
We fix a natural number k, and in the sequel we
denote by topD(w) a set of k words w? ? CD(w)
with the highest scoreD(w? | w). If |CD(w)| < k,
then topD(w) is simply CD(w).
2.2 Query-Level Correction: MaxPaths
We now describe our algorithm, MaxPaths, for
spelling correction. The input is a (possibly mis-
spelled) search query s = s1, . . . , sn. As done in
the word-level correction, the algorithm produces a
set CD(s) of suggestions and determines the values
907
Algorithm 1 MaxPaths
Input: a search query s
Output: a set CD(s) of candidate suggestions r,
ranked by scoreD(r | s)
1: Find the strongly plausible tokens
2: Construct the correction graph
3: Find top-k full paths (with the largest weights)
4: Re-rank the paths by word correlation
scoreD(r | s), for all r ? CD(s), in order to rank
CD(s). A high-level overview of MaxPaths is given
in the pseudo-code of Algorithm 1. In the rest of this
section, we will detail each of the four steps in Al-
gorithm 1. The name MaxPaths will become clear
towards the end of this section.
We use the following notation. For a word w =
c1 ? ? ? cm of m characters ci and integers i < j
in {1, . . . ,m + 1}, we denote by w[i,j) the word
ci ? ? ? cj?1. For two words w1, w2 ? ??, the word
w1w2 ? ?? is obtained by concatenating w1 and
w2. Note that for the search query s = s1, . . . , sn
it holds that s1 ? ? ? sn is a single word (in ??). We
denote the word s1 ? ? ? sn by bsc. For example, if
s1 = sadeep and s2 = kohli, then s corresponds
to the query sadeep kohli while bsc is the word
sadeepkohli; furthermore, bsc[1,7) = sadeep.
2.2.1 Plausible Tokens
To support merging and splitting, we first iden-
tify the possible tokens of the given query s. For
example, in excellatach ment we would like to
identify excell and atach ment as tokens, since
those are indeed the tokens that the user has in mind.
Formally, suppose that bsc = c1 ? ? ? cm. A token is
a word bsc[i,j) where 1 ? i < j ? m + 1. To
simplify the presentation, we make the (often false)
assumption that a token bsc[i,j) uniquely identifies
i and j (that is, bsc[i,j) 6= bsc[i?,j?) if i 6= i? or
j 6= j?); in reality, we should define a token as a
triple (bsc[i,j), i, j). In principle, every token bsc[i,j)
could be viewed as a possible word that user meant
to phrase. However, such liberty would require our
algorithm to process a search space that is too large
to manage in reasonable time. Instead, we restrict to
strongly plausible tokens, which we define next.
A token w = bsc[i,j) is plausible if w is a word
of s, or there is a word w? ? CD(w) (as defined in
Section 2.1) such that scoreD(w? | w) > ? for some
fixed number ?. Intuitively, w is plausible if it is an
original token of s, or we have a high confidence in
our word-level suggestion to correct w (note that the
suggested correction for w can be w itself). Recall
that bsc = c1 ? ? ? cm. A tokenization of s is a se-
quence j1, . . . , jl, such that j1 = 1, jl = m+1, and
ji < ji+1 for 1 ? i < l. The tokenization j1, . . . , jl
induces the tokens bsc[j1,j2),. . . ,bsc[jl?1,jl). A tok-
enization is plausible if each of its induced tokens
is plausible. Observe that a plausible token is not
necessarily induced by any plausible tokenization;
in that case, the plausible token is useless to us.
Thus, we define a strongly plausible token, abbre-
viated sp-token, which is a token that is induced by
some plausible tokenization. As a concrete example,
for the query excellatach ment, the sp-tokens in
our implementation include excellatach, ment,
excell, and atachment.
As the first step (line 1 in Algorithm 1), we find
the sp-tokens by employing an efficient (and fairly
straightforward) dynamic-programming algorithm.
2.2.2 Correction Graph
In the next step (line 2 in Algorithm 1), we con-
struct the correction graph, which we denote by
GD(s). The construction is as follows.
We first find the set topD(w) (defined in Sec-
tion 2.1) for each sp-token w. Table 2 shows the sp-
tokens and suggestions thereon in our running exam-
ple. This example shows the actual execution of our
implementation within email search, where s is the
query sadeep kohli excellatach ment; for
clarity of presentation, we omitted a few sp-tokens
and suggested corrections. Observe that some of
the corrections in the table are actually misspelled
words (as those naturally occur in the corpus).
A node of the graph GD(s) is a pair ?w,w??, where
w is an sp-token and w? ? topD(w). Recall our
simplifying assumption that a token bsc[i,j) uniquely
identifies the indices i and j. The graph GD(s) con-
tains a (directed) edge from a node ?w1, w?1? to a
node ?w2, w?2? if w2 immediately follows w1 in bqc;
in other words, GD(s) has an edge from ?w1, w?1?
to ?w2, w?2? whenever there exist indices i, j and k,
such that w1 = bsc[i,j) and w2 = bsc[j,k). Observe
that GD(s) is a directed acyclic graph (DAG).
908
except
excell
excel
excellence
excellent
sandeep
jaideep
kohli
attachement
attachment
attached
sandeep kohli
sent
meet
ment
Figure 2: The graph GD(s)
For example, Figure 2 shows GD(s) for the
query sadeep kohli excellatach ment, with
the sp-tokens w and the sets topD(w) being those of
Table 2. For now, the reader should ignore the node
in the grey box (containing sandeep kohli) and
its incident edges. For simplicity, in this figure we
depict each node ?w,w?? by just mentioning w?; the
word w is in the first row of Table 2, above w?.
2.2.3 Top-k Paths
Let P = ?w1, w?1? ? ? ? ? ? ?wk, w?k? be a path
in GD(s). We say that P is full if ?w1, w?1? has no
incoming edges in GD(s), and ?wk, w?k? has no out-
going edges in GD(s). An easy observation is that,
since we consider only strongly plausible tokens, if
P is full then w1 ? ? ?wk = bsc; in that case, the se-
quence w?1, . . . , w?k is a suggestion for spelling cor-
rection, and we denote it by crc(P ). As an example,
Figure 3 shows two full paths P1 and P2 in the graph
GD(s) of Figure 2. The corrections crc(Pi), for
i = 1, 2, are jaideep kohli excellent ment
and sandeep kohli excel attachement, re-
spectively.
To obtain corrections crc(P ) with high quality,
we produce a set of k full paths with the largest
weights, for some fixed k; we denote this set by
topPathsD(s). The weight of a path P , denoted
weight(P ), is the sum of the weights of all the nodes
and edges in P , and we define the weights of nodes
and edges next. To find these paths, we use a well
known efficient algorithm (Eppstein, 1994).
kohli
kohli
excellent ment
excel attachment
jaideep
sandeep
P1
P2
Figure 3: Full paths in the graph GD(s) of Figure 2
Consider a node u = ?w,w?? of GD(s). In the
construction of GD(s), zero or more merges of (part
of) original tokens have been applied to obtain the
token w; let #merges(w) be that number. Consider
an edge e of GD(s) from a node u1 = ?w1, w?1? to
u2 = ?w2, w?2?. In s, either w1 and w2 belong to
different words (i.e., there is a whitespace between
them) or not; in the former case define #splits(e) =
0, and in the latter #splits(e) = 1. We define:
weight(u) def= scoreD(w? | w) + am ?#merges(w)
weight(e) def= as ?#splits(e)
Note that am and as are negative, as they penalize
for merges and splits, respectively. Again, in our
implementations, we learned am and as by means
of SVM.
Recall that topPathsD(s) is the set of k full paths
(in the graph GD(s)) with the largest weights. From
topPathsD(s) we get the set CD(s) of candidate
suggestions:
CD(s) def= {crc(P ) | P ? topPathsD(s)} .
2.2.4 Word Correlation
To compute scoreD(r|s) for r ? CD(s), we incor-
porate correlation among the words of r. Intuitively,
we would like to reward a candidate with pairs of
words that are likely to co-exist in a query. For
that, we assume a (symmetric) numerical function
crl(w?1, w?2) that estimates the extent to which the
words w?1 and w?2 are correlated. As an example, in
the email domain we would like crl(kohli, excel)
to be high if Kohli sent many emails with excel at-
tachments. Our implementation of crl(w?1, w?2) es-
sentially employs pointwise mutual information that
has also been used in (Schierle et al, 2007), and that
909
Table 2: topD(w) for sp-tokens w
sadeep kohli excellatach ment excell atachment
sandeep kohli excellent ment excel attachmentjaideep excellence sent excell attachedmeet except attachement
compares the number of documents (emails) con-
taining w?1 and w?2 separately and jointly.
Let P ? topPathsD(s) be a path. We de-
note by crl(P ) a function that aggregates the num-
bers crl(w?1, w?2) for nodes ?w1, w?1? and ?w2, w?2?
of P (where ?w1, w?1? and ?w2, w?2? are not nec-
essarily neighbors in P ). Over the email domain,
our crl(P ) is the minimum of the crl(w?1, w?2). We
define scoreD(P ) = weight(P ) + crl(P ). To
improve the performance, in our implementation
we learned again (re-trained) all the parameters in-
volved in scoreD(P ).
Finally, as the top suggestions we take crc(P )
for full paths P with highest scoreD(P ). Note that
crc(P ) is not necessarily injective; that is, there can
be two full paths P1 6= P2 satisfying crc(P1) =
crc(P2). Thus, in effect, scoreD(r | s) is determined
by the best evidence of r; that is,
scoreD(r | s) def= max{scoreD(P ) | crc(P ) = r?
P ? topPathsD(s)} .
Note that our final scoring function essentially views
P as a clique rather than a path. In principle,
we could define GD(s) in a way that we would
extract the maximal cliques directly without find-
ing topPathsD(s) first. However, we chose our
method (finding top paths first, and then re-ranking)
to avoid the inherent computational hardness in-
volved in finding maximal cliques.
2.3 Handling Expressions
We now briefly discuss our handling of frequent n-
grams (expressions). We handle n-grams by intro-
ducing new nodes to the graph GD(s); such a new
node u is a pair ?t, t??, where t is a sequence of
n consecutive sp-tokens and t? is a n-gram. The
weight of such a node u is rewarded for consti-
tuting a frequent or important n-gram. An exam-
ple of such a node is in the grey box of Figure 2,
where sandeep kohli is a bigram. Observe that
sandeep kohli may be deemed an important bi-
gram because it occurs as a sender of an email, and
not necessarily because it is frequent.
An advantage of our approach is avoidance
of over-scoring due to conflicting n-grams. For
example, consider the query textile import
expert, and assume that both textile import
and import export (with an ?o? rather than an
?e?) are frequent bigrams. If the user referred to the
bigram textile import, then expert is likely to
be correct. But if she meant for import export,
then expert is misspelled. However, only one of
these two options can hold true, and we would like
textile import export to be rewarded only
once?for the bigram import export. This is
achieved in our approach, since a full path in GD(s)
may contain either a node for textile import or
a node for import export, but it cannot contain
nodes for both of these bigrams.
Finally, we note that our algorithm is in the spirit
of that of Cucerzan and Brill (2004), with a few in-
herent differences. In essence, a node in the graph
they construct corresponds to what we denote here
as ?w,w?? in the special case where w is an actual
word of the query; that is, no re-tokenization is ap-
plied. They can split a word by comparing it to a bi-
gram. However, it is not clear how they can split into
non-bigrams (without a huge index) and to handle si-
multaneous merging and splitting as in our running
example (1). Furthermore, they translate bigram in-
formation into edge weights, which implies that the
above problem of over-rewarding due to conflicting
bigrams occurs.
3 Experimental Study
Our experimental study aims to investigate the ef-
fectiveness of our approach in various settings, as
we explain next.
3.1 Experimental Setup
We first describe our experimental setup, and specif-
ically the datasets and general methodology.
Datasets. The focus of our experimental study is
on personal email search; later on (Section 3.6),
we will consider (and give experimental results for)
a totally different setting?site search over www.
ibm.com, which is a massive and open domain.
Our dataset (for the email domain) is obtained from
910
the Enron email collection (Bekkerman et al, 2004;
Klimt and Yang, 2004). Specifically, we chose the
three users with the largest number of emails. We re-
fer to the three email collections by the last names of
their owners: Farmer, Kaminski and Kitchen. Each
user mailbox is a separate domain, with a separate
corpus D, that one can search upon. Due to the ab-
sence of real user queries, we constructed our dataset
by conducting a user study, as described next.
For each user, we randomly sampled 50 emails
and divided them into 5 disjoint sets of 10 emails
each. We gave each 10-email set to a unique hu-
man subject that was asked to phrase two search
queries for each email: one for the entire email con-
tent (general query), and the other for the From and
X-From fields (sender query). (Figure 1 shows ex-
amples of the From and X-From fields.) The latter
represents queries posed against a specific field (e.g.,
using ?advanced search?). The participants were not
told about the goal of this study (i.e., spelling correc-
tion), and the collected queries have no spelling er-
rors. For generating spelling errors, we implemented
a typo generator.1 This generator extends an online
typo generator (Seobook, 2010) that produces a vari-
ety of spelling errors, including skipped letter, dou-
bled letter, reversed letter, skipped space (merge),
missed key and inserted key; in addition, our gener-
ator produces inserted space (split). When applied
to a search query, our generator adds random typos
to each word, independently, with a specified prob-
ability p that is 50% by default. For each collected
query (and for each considered value of p) we gener-
ated 5 misspelled queries, and thereby obtained 250
instances of misspelled general queries and 250 in-
stances of misspelled sender queries.
Methodology. We compared the accuracy of
MaxPaths (Section 2) with three alternatives. The
first alternative is the open-source Jazzy, which
is a widely used spelling-correction tool based on
(weighted) edit distance. The second alternative is
the spelling correction provided by Google. We
provided Jazzy with our unigram index (as a dic-
tionary). However, we were not able to do so
with Google, as we used remote access via its Java
API (Google, 2010); hence, the Google tool is un-
1The queries and our typo generator are publicly available
at https://dbappserv.cis.upenn.edu/spell/.
aware of our domain, but is rather based on its
own statistics (from the World Wide Web). The
third alternative is what we call WordWise, which
applies word-level correction (Section 2.1) to each
input query term, independently. More precisely,
WordWise is a simplified version of MaxPaths,
where we forbid splitting and merging of words (i.e.,
only the original tokens are considered), and where
we do not take correlation into account.
Our emphasis is on correcting misspelled queries,
rather than recognizing correctly spelled queries,
due to the role of spelling in a search engine: we
wish to provide the user with the correct query upon
misspelling, but there is no harm in making a sug-
gestion for correctly spelled queries, except for vi-
sual discomfort. Hence, by default accuracy means
the number of properly corrected queries (within
the top-k suggestions) divided by the number of the
misspelled queries. An exception is in Section 3.5,
where we study the accuracy on correct queries.
Since MaxPaths and WordWise involve parame-
ter learning (SVM), the results for them are consis-
tently obtained by performing 5-folder cross valida-
tion over each collection of misspelled queries.
3.2 Fixed Error Probability
Here, we compare MaxPaths to the alternatives
when the error probability p is fixed (0.5). We con-
sider only the Kaminski dataset; the results for the
other two datasets are similar. Figure 4(a) shows the
accuracy, for general queries, of top-k suggestions
for k = 1, k = 3 and k = 10. Note that we can get
only one (top-1) suggestion from Google. As can
be seen, MaxPaths has the highest accuracy in all
cases. Moreover, the advantage of MaxPaths over
the alternatives increases as k increases, which indi-
cates potential for further improving MaxPaths.
Figure 4(b) shows the accuracy of top-k sugges-
tions for sender queries. Overall, the results are sim-
ilar to those of Figure 4(a), except that top-1 of both
WordWise and MaxPaths has a higher accuracy in
sender queries than in general queries. This is due
to the fact that the dictionaries of person names and
email addresses extracted from the X-From and
From fields, respectively, provide strong features
for the scoring function, since a sender query refers
to these two fields. In addition, the accuracy of
MaxPaths is further enhanced by exploiting the cor-
911
0%
20%
40%
60%
80%
100%
Top 1 Top 3 Top 10
Google Jazzy WordWise MaxPaths
(a) General queries (Kaminski)
0%
20%
40%
60%
80%
100%
Top 1 Top 3 Top 10
Google Jazzy WordWise MaxPaths
(b) Sender queries (Kaminski)
0%
25%
50%
75%
100%
0% 20% 40% 60% 80% 100%
Google Jazzy WordWise MaxPaths
Spelling Error Probability
(c) Varying error probability (Kaminski)
Figure 4: Accuracy for Kaminski (misspelled queries)
relation between the first and last name of a person.
3.3 Impact of Error Probability
We now study the impact of the complexity of
spelling errors on our algorithm. For that, we mea-
sure the accuracy while the error probability p varies
from 10% to 90% (with gaps of 20%). The re-
sults are in Figure 4(c). Again, we show the results
only for Kaminski, since we get similar results for
the other two datasets. As expected, in all exam-
ined methods the accuracy decreases as p increases.
Now, not only does MaxPaths outperform the alter-
natives, its decrease (as well as that of WordWise) is
the mildest?13% as p increases from 10% to 90%
(while Google and Jazzy decrease by 23% or more).
We got similar results for the sender queries (and for
each of the three users).
3.4 Adaptiveness of Parameters
Obtaining the labeled data needed for parameter
learning entails a nontrivial manual effort. Ideally,
we would like to learn the parameters of MaxPaths
in one domain, and use them in similar domains.
0%
25%
50%
75%
100%
0% 20% 40% 60% 80% 100%
Google Jazzy MaxPaths* MaxPaths
Spelling Error Probability
(a) General queries (Farmer)
0%
25%
50%
75%
100%
0% 20% 40% 60% 80% 100%
Google Jazzy MaxPaths* MaxPaths
Spelling Error Probability
(b) Sender queries (Farmer)
Figure 5: Accuracy for Farmer (misspelled queries)
More specifically, our desire is to use the parame-
ters learned over one corpus (e.g., the email collec-
tion of one user) on a second corpus (e.g., the email
collection of another user), rather than learning the
parameters again over the second corpus. In this set
of experiments, we examine the feasibility of that
approach. Specifically, we consider the user Farmer
and observe the accuracy of our algorithm with two
sets of parameters: the first, denoted by MaxPaths in
Figures 5(a) and 5(b), is learned within the Farmer
dataset, and the second, denoted by MaxPaths?, is
learned within the Kaminski dataset. Figures 5(a)
and 5(b) show the accuracy of the top-1 suggestion
for general queries and sender queries, respectively,
with varying error probabilities. As can be seen,
these results mean good news?the accuracies of
MaxPaths? and MaxPaths are extremely close (their
curves are barely distinguishable, as in most cases
the difference is smaller than 1%). We repeated this
experiment for Kitchen and Kaminski, and got sim-
ilar results.
3.5 Accuracy for Correct Queries
Next, we study the accuracy on correct queries,
where the task is to recognize the given query as cor-
rect by returning it as the top suggestion. For each
of the three users, we considered the 50 + 50 (gen-
eral + sender) collected queries (having no spelling
errors), and measured the accuracy, which is the
percentage of queries that are equal to the top sug-
912
Table 3: Accuracy for Correct Queries
Dataset Google Jazzy MaxPaths
Kaminski (general) 90% 98% 94%
Kaminski (sender) 94% 98% 94%
Farmer (general) 96% 98% 96%
Farmer (sender) 96% 96% 92%
Kitchen (general) 86% 100% 92%
Kitchen (sender) 94% 100% 98%
gestion. Table 3 shows the results. Since Jazzy is
based on edit distance, it almost always gives the in-
put query as the top suggestion; the misses of Jazzy
are for queries that contain a word that is not the cor-
pus. MaxPaths is fairly close to the upper bound set
by Jazzy. Google (having no access to the domain)
also performs well, partly because it returns the in-
put query if no reasonable suggestion is found.
3.6 Applicability to Large-Scale Site Search
Up to now, our focus has been on email search,
which represents a restricted (closed) domain with
specialized knowledge (e.g., sender names). In this
part, we examine the effectiveness of our algorithm
in a totally different setting?large-scale site search
within www.ibm.com, a domain that is popular on
a world scale. There, the accuracy of Google is very
high, due to this domain?s popularity, scale, and full
accessibility on the Web. We crawled 10 million
documents in that domain to obtain the corpus. We
manually collected 1348 misspelled queries from
the log of search issued against developerWorks
(www.ibm.com/developerworks/) during a
week. To facilitate the manual collection of these
queries, we inspected each query with two or fewer
search results, after applying a random permutation
to those queries. Figure 6 shows the accuracy of
top-k suggestions. Note that the performance of
MaxPaths is very close to that of Google?only 2%
lower for top-1. For k = 3 and k = 10, MaxPaths
outperforms Jazzy and the top-1 of Google (from
which we cannot obtain top-k for k > 1).
3.7 Summary
To conclude, our experiments demonstrate various
important qualities of MaxPaths. First, it outper-
forms its alternatives, in both accuracy (Section 3.2)
and robustness to varying error complexities (Sec-
tion 3.3). Second, the parameters learned in one
domain (e.g., an email user) can be applied to sim-
0%
20%
40%
60%
80%
100%
Top 1 Top 3 Top 10
Google Jazzy WordWise MaxPaths
Figure 6: Accuracy for site search
ilar domains (e.g., other email users) with essen-
tially no loss in performance (Section 3.4). Third,
it is highly accurate in recognition of correct queries
(Section 3.5). Fourth, even when applied to large
(open) domains, it achieves a comparable perfor-
mance to the state-of-the-art Google spelling correc-
tion (Section 3.6). Finally, the higher performance
of MaxPaths on top-3 and top-10 corrections sug-
gests a potential for further improvement of top-1
(which is important since search engines often re-
strict their interfaces to only one suggestion).
4 Conclusions
We presented the algorithm MaxPaths for spelling
correction in domain-centric search. This algo-
rithm relies primarily on corpus statistics and do-
main knowledge (rather than on query logs). It can
handle a variety of spelling errors, and can incor-
porate different levels of spelling reliability among
different parts of the corpus. Our experimental study
demonstrates the superiority of MaxPaths over ex-
isting alternatives in the domain of email search, and
indicates its effectiveness beyond that domain.
In future work, we plan to explore how to utilize
additional domain knowledge to better estimate the
correlation between words. Particularly, from avail-
able auxiliary data (Fagin et al, 2010) and tools like
information extraction (Chiticariu et al, 2010), we
can infer and utilize type information from the cor-
pus (Li et al, 2006b; Zhu et al, 2007). For instance,
if kohli is of type person, and phone is highly cor-
related with person instances, then phone is highly
correlated with kohli even if the two words do not
frequently co-occur. We also plan to explore as-
pects of corpus maintenance in dynamic (constantly
changing) domains.
913
References
F. Ahmad and G. Kondrak. 2005. Learning a spelling
error model from search query logs. In HLT/EMNLP.
R. Bekkerman, A. Mccallum, and G. Huang. 2004. Au-
tomatic categorization of email into folders: Bench-
mark experiments on Enron and Sri Corpora. Techni-
cal report, University of Massachusetts - Amherst.
Q. Chen, M. Li, and M. Zhou. 2007. Improving
query spelling correction using Web search results. In
EMNLP-CoNLL, pages 181?189.
L. Chiticariu, R. Krishnamurthy, Y. Li, S. Raghavan,
F. Reiss, and S. Vaithyanathan. 2010. SystemT: An
algebraic approach to declarative information extrac-
tion. In ACL, pages 128?137.
C. Cortes and V. Vapnik. 1995. Support-vector networks.
Machine Learning, 20(3):273?297.
S. Cucerzan and E. Brill. 2004. Spelling correction as an
iterative process that exploits the collective knowledge
of Web users. In EMNLP, pages 293?300.
D. Eppstein. 1994. Finding the k shortest paths. In
FOCS, pages 154?165.
R. Fagin, B. Kimelfeld, Y. Li, S. Raghavan, and
S. Vaithyanathan. 2010. Understanding queries in a
search database system. In PODS, pages 273?284.
Google. 2010. A Java API for Google spelling check ser-
vice. http://code.google.com/p/google-api-spelling-
java/.
D. Jurafsky and J. H. Martin. 2000. Speech and
Language Processing: An Introduction to Natural
Language Processing, Computational Linguistics, and
Speech Recognition. Prentice Hall PTR.
M. D. Kernighan, K. W. Church, and W. A. Gale. 1990.
A spelling correction program based on a noisy chan-
nel model. In COLING, pages 205?210.
B. Klimt and Y. Yang. 2004. Introducing the Enron cor-
pus. In CEAS.
K. Kukich. 1992. Techniques for automatically correct-
ing words in text. ACM Comput. Surv., 24(4):377?
439.
M. Li, M. Zhu, Y. Zhang, and M. Zhou. 2006a. Explor-
ing distributional similarity based models for query
spelling correction. In ACL.
Y. Li, R. Krishnamurthy, S. Vaithyanathan, and H. V. Ja-
gadish. 2006b. Getting work done on the web: sup-
porting transactional queries. In SIGIR, pages 557?
564.
R. Mitton. 2010. Fifty years of spellchecking. Wring
Systems Research, 2:1?7.
J. L. Peterson. 1980. Computer Programs for Spelling
Correction: An Experiment in Program Design, vol-
ume 96 of Lecture Notes in Computer Science.
Springer.
J. Schaback and F. Li. 2007. Multi-level feature extrac-
tion for spelling correction. In AND, pages 79?86.
M. Schierle, S. Schulz, and M. Ackermann. 2007. From
spelling correction to text cleaning - using context in-
formation. In GfKl, Studies in Classification, Data
Analysis, and Knowledge Organization, pages 397?
404.
Seobook. 2010. Keyword typo generator.
http://tools.seobook.com/spelling/keywords-
typos.cgi.
X. Sun, J. Gao, D. Micol, and C. Quirk. 2010. Learning
phrase-based spelling error models from clickthrough
data. In ACL, pages 266?274.
H. Zhu, S. Raghavan, S. Vaithyanathan, and A. Lo?ser.
2007. Navigating the intranet with high precision. In
WWW, pages 491?500.
914
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 109?114,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
SystemT: A Declarative Information Extraction System
Yunyao Li
IBM Research - Almaden
650 Harry Road
San Jose, CA 95120
yunyaoli@us.ibm.com
Frederick R. Reiss
IBM Research - Almaden
650 Harry Road
San Jose, CA 95120
frreiss@us.ibm.com
Laura Chiticariu
IBM Research - Almaden
650 Harry Road
San Jose, CA 95120
chiti@us.ibm.com
Abstract
Emerging text-intensive enterprise applica-
tions such as social analytics and semantic
search pose new challenges of scalability and
usability to Information Extraction (IE) sys-
tems. This paper presents SystemT, a declar-
ative IE system that addresses these challenges
and has been deployed in a wide range of en-
terprise applications. SystemT facilitates the
development of high quality complex annota-
tors by providing a highly expressive language
and an advanced development environment.
It also includes a cost-based optimizer and a
high-performance, flexible runtime with mini-
mummemory footprint. We present SystemT
as a useful resource that is freely available,
and as an opportunity to promote research in
building scalable and usable IE systems.
1 Introduction
Information extraction (IE) refers to the extraction
of structured information from text documents. In
recent years, text analytics have become the driv-
ing force for many emerging enterprise applications
such as compliance and data redaction. In addition,
the inclusion of text has also been increasingly im-
portant for many traditional enterprise applications
such as business intelligence. Not surprisingly, the
use of information extraction has dramatically in-
creased within the enterprise over the years. While
the traditional requirement of extraction quality re-
mains critical, enterprise applications pose several
two challenges to IE systems:
1.Scalability: Enterprise applications operate
over large volumes of data, often orders of
magnitude larger than classical IE corpora. An
IE system should be able to operate at those
scales without compromising its execution ef-
ficiency or memory consumption.
2.Usability: Building an accurate IE system is
an inherently labor intensive process. There-
fore, the usability of an enterprise IE system in
terms of ease of development and maintenance
is crucial for ensuring healthy product cycle
and timely handling of customer complains.
Traditionally, IE systems have been built from in-
dividual extraction components consisting of rules
or machine learning models. These individual com-
ponents are then connected procedurally in a pro-
gramming language such as C++, Perl or Java. Such
procedural logic towards IE cannot meet the increas-
ing scalability and usability requirements in the en-
terprise (Doan et al, 2006; Chiticariu et al, 2010a).
Three decades ago, the database community faced
similar scalability and expressivity challenges in
accessing structured information. The community
addressed these problems by introducing a rela-
tional algebra formalism and an associated declar-
ative query language SQL. Borrowing ideas from
the database community, several systems (Doan and
others, 2008; Bohannon and others, 2008; Jain et al,
2009; Krishnamurthy et al, 2008; Wang et al, 2010)
have been built in recent years taking an alternative
declarative approach to information extraction. In-
stead of using procedural logic to implement the ex-
traction task, declarative IE systems separate the de-
scription of what to extract from how to extract it,
allowing the IE developer to build complex extrac-
109
Development Environment
Optimizer
Rules(XQL)
ExecutionEngine
SampleDocuments
RuntimeEnvironmentunti environ ent
InputDocumentStream
AnnotatedDocumentStream
Plan(Algebra)
UserInterface
Publish
Figure 1: Overview of SystemT
tion programs without worrying about performance
considerations.
In this demonstration, we showcase one such
declarative IE system called SystemT, designed
to address the scalability and usability challenges.
We illustrate how SystemT, currently deployed in
a multitude of real-world applications and com-
mercial products, can be used to develop and
maintain IE annotators for enterprise applica-
tions. A free version of SystemT is available at
http://www.alphaworks.ibm.com/tech/systemt.
2 Overview of SystemT
Figure 1 depicts the architecture of SystemT. The sys-
tem consists of two major components: the Development
Environment and the Runtime Environment. The Sys-
temT Development Environment supports the iterative
process of constructing and refining rules for information
extraction. The rules are specified in a declarative lan-
guage called AQL (F.Reiss et al, 2008). The Develop-
ment Environment provides facilities for executing rules
over a given corpus of representative documents and vi-
sualizing the results of the execution. Once a developer
is satisfied with the results that her rules produce on these
documents, she can publish her annotator.
Publishing an annotator is a two-step process. First,
given an AQL annotator, there can be many possible
graphs of operators, or execution plans, each of which
faithfully implements the semantics of the annotator.
Some of the execution plans are much more efficient than
others. The SystemT Optimizer explores the space of
the possible execution plans to choose the most efficient
one. This execution plan is then given to the SystemT
Runtime to instantiate the corresponding physical oper-
ators. Once the physical operators are instantiated, the
create view Phone asextract regex /\d{3}-\d{4}/ on D.text as numberfrom Document D;
create view Person asextract dictionary ?firstNames.dict? on D.text as namefrom Document D;
create view PersonPhoneAll asselect CombineSpans(P.name, Ph.number) as matchfrom Person P, Phone Phwhere FollowsTok(P.name, Ph.number, 0, 5);
create view PersonPhone asselect R.name as namefrom PersonPhoneAll Rconsolidate on R.name;
output view PersonPhone; 
Figure 2: An AQL program for a PersonPhone task.
SystemT Runtime feeds one document at a time through
the graph of physical operators and outputs a stream of
annotated documents.
The decoupling of the Development and Runtime en-
vironments is essential for the flexibility of the system. It
facilitates the incorporating of various sophisticated tools
to enable annotator development without sacrificing run-
time performance. Furthermore, the separation permits
the SystemT Runtime to be embedded into larger appli-
cations with minimum memory footprint. Next, we dis-
cuss individual components of SystemT in more details
(Sections 3 ? 6), and summarize our experience with the
system in a variety of enterprise applications (Section 7).
3 The Extraction Language
In SystemT, developers express an information extrac-
tion program using a language called AQL. AQL is a
declarative relational language similar in syntax to the
database language SQL, which was chosen as a basis for
our language due to its expressivity and familiarity. An
AQL program (or an AQL annotator) consists of a set of
AQL rules.
In this section, we describe the AQL language and
its underlying algebraic operators. In Section 4, we ex-
plain how the SystemT optimizer explores a large space
of possible execution plans for an AQL annotator and
chooses one that is most efficient.
3.1 AQL
Figure 2 illustrates a (very) simplistic annotator of rela-
tionships between persons and their phone number. At a
high-level, the annotator identifies person names using a
simple dictionary of first names, and phone numbers us-
ing a regular expression. It then identifies pairs of Person
and Phone annotations, where the latter follows the
110
former within 0 to 5 tokens, and marks the corre-
sponding region of text as a PersonPhoneAll annota-
tion. The final output PersonPhone is constructed by
removing overlapping PersonPhoneAll annotations.
AQL operates over a simple relational data model
with three data types: span, tuple, and view. In this
data model, a span is a region of text within a doc-
ument identified by its ?begin? and ?end? positions,
while a tuple is a list of spans of fixed size. A view
is a set of tuples. As can be seen from Figure 2,
each AQL rule defines a view. As such, a view is the
basic building block in AQL: it consists of a logical
description of a set of tuples in terms of the docu-
ment text, or the content of other views. The input
to the annotator is a special view called Document
containing a single tuple with the document text.
The AQL annotator tags some views as output views,
which specify the annotation types that are the final
results of the annotator.
The example in Figure 2 illustrates two of the
basic constructs of AQL. The extract statement
specifies basic character-level extraction primitives,
such as regular expressions or dictionaries (i.e.,
gazetteers), that are applied directly to the docu-
ment, or a region thereof. The select statement
is similar to the corresponding SQL statement, but
contains an additional consolidate on clause
for resolving overlapping annotations, along with an
extensive collection of text-specific predicates.
To keep rules compact, AQL also allows a short-
hand pattern notation similar to the syntax of the
CPSL grammar standard (Appelt and Onyshkevych,
1998). For example, the PersonPhoneAll view
in Figure 2 can also be expressed as shown below.
Internally, SystemT translates each of these extract
pattern statements into one or more select and ex-
tract statements.
create view PersonPhoneAll as
extract pattern
<P.name> <Token>{0,5} <Ph.number>
from Person P, Phone Ph;
SystemT has built-in multilingual support in-
cluding tokenization, part of speech and gazetteer
matching for over 20 languages using IBM Lan-
guageWare. Annotator developers can utilize the
multilingual support via AQLwithout having to con-
figure or manage any additional resources. In ad-
dition, AQL allows user-defined functions in a re-
firstNames.dict
DocumentInput Tuple
?I?ve seen John and Martin, ?
Output Tuple 2 Span 2Document Span 1Output Tuple 1 Document
Dictionary
Person
(?Anna?, ?John?, ?Martin?, ?)
Figure 3: Dictionary Extraction Operator
stricted context in order to support operations such
as validation or normalization. More details on AQL
can be found in the AQL manual (Chiticariu et al,
2010b).
3.2 Algebraic Operators in SystemT
SystemT executes AQL rules using graphs of op-
erators. These operators are based on an algebraic
formalism that is similar to the relational algebra
formalism, but with extensions for text processing.
Each operator in the algebra implements a single
basic atomic IE operation, producing and consum-
ing sets of tuples (i.e., views).
Fig. 3 illustrates the dictionary extraction operator
in the algebra, which performs character-level dic-
tionary matching. A full description of the 12 differ-
ent operators of the algebra can be found in (F.Reiss
et al, 2008). Three of the operators are listed below.
? The Extract operator (E) performs character-
level operations such as regular expression and
dictionary matching over text, producing one tu-
ple for each match.
? The Select operator (?) takes as input a set of
tuples and a predicate to apply to the tuples, and
outputs all tuples that satisfy the predicate.
? The Join operator (??) takes as input two sets of
tuples and a predicate to apply to pairs of tuples.
It outputs all pairs satisfying the predicate.
Other operators include PartOfSpeech for part-
of-speech detection, Consolidate for removing
overlapping annotations, Block and Group for
grouping together similar annotations occurring
within close proximity to each other, as well as ex-
pressing more general types of aggregation, Sort for
sorting, and Union and Minus for expressing set
union and set difference, respectively.
111
Person Phone
Plan BPlan A Find matches of Person, then discard matches that are not followed by a Phone
? ??dict
Find matches of Person and Phone, then identify pairs that are within 0 to 5 tokens of each other
Plan CFind matches of Phone, then discard matches that are not followed by a Person
?
?
regex
Figure 4: Execution strategies for PersonPhoneAll in
Fig. 2
4 The Optimizer
Grammar-based IE engines such as (Boguraev,
2003; Cunningham et al, 2000) place rigid restric-
tions on the order in which rules can be executed.
Such systems that implement the CPSL standard or
extensions of it must use a finite state transducer to
evaluate each level of the cascade with one or more
left to right passes over the entire input token stream.
In contrast, SystemT uses a declarative approach
based on rules that specify what patterns to extract,
as opposed to how to extract them. In a declarative
IE system such as SystemT the specification of an
annotator is completely separate from its implemen-
tation. In particular, the system does not place ex-
plicit constraints on the order of rule evaluation, nor
does it require that intermediate results of an anno-
tator collapse to a fixed-size sequence.
As shown in Fig. 1, the SystemT engine does
not execute AQL directly; instead, the SystemT
Optimizer compiles AQL into a graph of operators.
Given a collection of AQL views, the optimizer gen-
erates a large number of different operator graphs,
all of which faithfully implement the semantics of
the original views. Even though these graphs always
produce the same results, the execution strategies
that they represent can have very different perfor-
mance characteristics. The optimizer incorporates
a cost model which, given an operator graph, esti-
mates the CPU time required to execute the graph
over an average document in the corpus. This cost
model allows the optimizer to estimate the cost of
each potential execution strategy and to choose the
one with the fastest predicted running time.
Fig. 4 presents three possible execution strategies
for the PersonPhoneAll rule in Fig. 2. If the opti-
mizer estimates that the evaluation cost of Person is
much lower than that of Phone, then it can determine
that Plan B has the lowest evaluation cost among
the three, because Plan B only evaluates Phone in
the ?right? neighborhood for each instance of Per-
son. More details of our algorithms for enumerating
plans can be found in (F.Reiss et al, 2008).
The optimizer in SystemT chooses the best exe-
cution plan from a large number of different algebra
graphs available. Depending on the execution plan
generated by the optimizer, SystemT may evaluate
views out of order, or it may skip evaluating some
views entirely. It may share work among views or
combine multiple equivalent views together. Even
within the context of a single view, the system can
choose among several different execution strategies
without affecting the semantics of the annotator.
This decoupling is possible because of the declar-
ative approach in SystemT, where the AQL rules
specify only what patterns to extract and not how to
extract them. Notice that many of these strategies
cannot be implemented using a transducer. In fact,
we have formally proven that within this large search
space, there generally exists an execution strategy
that implements the rule semantics far more effi-
ciently than the fastest transducer could (Chiticariu
et al, 2010b). This approach also allows for greater
rule expressivity, because the rule language is not
constrained by the need to compile to a finite state
transducer, as in traditional CPSL-based systems.
5 The Runtime
The SystemT Runtime is a compact, small memory
footprint, high-performance Java-based runtime en-
gine designed to be embedded in a larger system.
The runtime engine works in two steps. First, it
instantiates the physical operators in the compiled
operator graph generated by the optimizer. Second,
once the first step has been completed, the runtime
feeds documents through the operator graph one at a
time, producing annotations.
SystemT exposes a generic Java API for the inte-
gration of its runtime environment with other appli-
cations. Furthermore, SystemT provides two spe-
cific instantiations of the Java API: a UIMA API and
a Jaql function that allow the SystemT runtime to
be seamlessly embedded in applications using the
UIMA analytics framework (UIMA, 2010), or de-
ployed in a Hadoop-based environment. The latter
112
allows SystemT to be embedded as a Map job in a
map-reduce framework, thus enabling the system to
scale up and process large volumes of documents in
parallel.
5.1 Memory Consumption
Managing memory consumption is very important
in information extraction systems. Extracting struc-
tured information from unstructured text requires
generating and traversing large in-memory data
structures, and the size of these structures deter-
mines how large a document the system can process
with a given amount of memory.
Conventional rule-based IE systems cannot
garbage-collect their main-memory data structures
because the custom code embedded inside rules can
change these structures in arbitrary ways. As a re-
sult, the memory footprint of the rule engine grows
continuously throughout processing a given docu-
ment.
In SystemT, the AQL view definitions clearly
specify the data dependencies between rules. When
generating an execution plan for an AQL annota-
tor, the optimizer generates information about when
it is safe to discard a given set of intermediate re-
sults. The SystemT Runtime uses this information
to implement garbage collection based on reference-
counting. This garbage collection significantly re-
duces the system?s peak memory consumption, al-
lowing SystemT to handle much larger documents
than conventional IE systems.
6 The Development Environment
The SystemT Development Environment assists a
developer in the iterative process of developing,
testing, debugging and refining AQL rules. Be-
sides standard editor features present in any well-
respected IDE for programming languages such as
syntax highlighting, the Development Environment
also provides facilities for visualizing the results of
executing the rules over a sample document collec-
tion as well as explaining in detail the provenance of
any output annotation as the sequence of rules that
have been applied in generating that output.
7 Evaluation
As discussed in Section 1, our goal in building Sys-
temT was to address the scalability and usability
Application Type Type of Platform
brand management server-side
business insights server-side
client-side mashups client-side
compliance server-side
search (email, web, patent) server-side
security server-side
server-side mashups server-side
Table 1: Types of applications using SystemT
challenges posed by enterprise applications. As
such, our evaluation focuses on these two dimen-
sions.
7.1 Scalability
Table 1 presents a diverse set of enterprise applica-
tions currently using SystemT. SystemT has been
deployed in both client-side applications with strict
memory constraints, as well as on applications on
the cloud, where it can process petabytes of data
in parallel. The focus on scalability in the design
of SystemT is essential for its flexible execution
model. First of all, efficient execution plans are
generated automatically by the SystemT Optimizer
based on sample document collections. This en-
sures that the same annotator can be executed effi-
ciently for different types of document collections.
In fact, our previous experimental study shows that
the execution plan generated by the SystemT opti-
mizer can be 20 times or more faster than a manu-
ally constructed plan (F.Reiss et al, 2008). Further-
more, the Runtime Environment of SystemT results
in compact memory footprint and allows SystemT
to be embedded in applications with strict memory
requirements as small as 10MB.
In our recent study over several document col-
lections of different sizes, we found that for the
same set of extraction tasks, the SystemT through-
put is at least an order of magnitude higher than
that of a state-of-the-art grammar-based IE system,
with much lower memory footprint (Chiticariu et al,
2010b). The high throughput and low memory foot-
print of SystemT allows it to satisfy the scalability
requirement of enterprise applications.
7.2 Usability
Table 2 lists different types of annotators built us-
ing SystemT for a wide range of domains. Most,
113
Domain Sample Annotators Built
blog Sentiment, InformalReview
email ConferenceCall, Signature, Agenda, DrivingDirection, PersonPhone, PersonAddress, PersonEmailAddress
financial Merger, Acquisition, JointVenture, EarningsAnnouncement, AnalystEarningsEstimate, DirectorsOfficers, CorporateActions
generic Person, Location, Organization, PhoneNumber, EmailAddress, URL, Time, Date
healthcare Disease, Drug, ChemicalCompound
web Homepage, Geography, Title, Heading
Table 2: List of Sample Annotators Built Using SystemT for Different Domains
if not all, of these annotators are already deployed
in commercial products. The emphasis on usability
in the design of SystemT has been critical for its
successful deployment in various domains. First of
all, the declarative approach taken by SystemT al-
lows developers to build complex annotators without
worrying about performance. Secondly, the expres-
siveness of the AQL language has greatly eased the
burden of annotator developers when building com-
plex annotators, as complex semantics such as dupli-
cate elimination and aggregation can be expressed in
a concise fashion (Chiticariu et al, 2010b). Finally,
the Development Environment further facilitates an-
notator development, where the clean semantics of
AQL can be exploited to automatically construct ex-
planations of incorrect results to help a developer in
identifying specific parts of the annotator responsi-
ble for a given mistake. SystemT has been suc-
cessfully used by enterprise application developers
in building high quality complex annotators, without
requiring extensive training or background in natural
language processing.
8 Demonstration
This demonstration will present the core function-
alities of SystemT. In particular, we shall demon-
strate the iterative process of building and debug-
ging an annotator in the Development Environment.
We will then showcase the execution plan automati-
cally generated by the Optimizer based on a sample
document collection, and present the output of the
Runtime Environment using the execution plan. In
our demonstration we will first make use of a simple
annotator, as the one shown in Fig. 2, to illustrate
the main constructs of AQL. We will then showcase
the generic state-of-the-art SystemT Named Enti-
ties Annotator Library (Chiticariu et al, 2010c) to
illustrate the quality of annotators that can be built
in our system.
References
D. E. Appelt and B. Onyshkevych. 1998. The common
pattern specification language. In TIPSTER workshop.
B. Boguraev. 2003. Annotation-based finite state pro-
cessing in a large-scale nlp arhitecture. In RANLP.
P. Bohannon et al 2008. Purple SOX Extraction Man-
agement System. SIGMOD Record, 37(4):21?27.
L. Chiticariu, Y. Li, S. Raghavan, and F. Reiss. 2010a.
Enterprise information extraction: Recent develop-
ments and open challenges. In SIGMOD.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li,
Sriram Raghavan, Frederick R. Reiss, and Shivaku-
mar Vaithyanathan. 2010b. Systemt: an algebraic ap-
proach to declarative information extraction. ACL.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Frederick Reiss, and Shivakumar Vaithyanathan.
2010c. Domain adaptation of rule-based annotators
for named-entity recognition tasks. EMNLP.
H. Cunningham, D. Maynard, and V. Tablan. 2000.
JAPE: a Java Annotation Patterns Engine (Second Edi-
tion). Research Memorandum CS?00?10, Department
of Computer Science, University of Sheffield.
A. Doan et al 2008. Information extraction challenges
in managing unstructured data. SIGMOD Record,
37(4):14?20.
A. Doan, R. Ramakrishnan, and S. Vaithyanathan. 2006.
Managing Information Extraction: State of the Art and
Research Directions. In SIGMOD.
F.Reiss, S. Raghavan, R. Krishnamurthy, H. Zhu, and
S. Vaithyanathan. 2008. An algebraic approach to
rule-based information extraction. In ICDE.
A. Jain, P. Ipeirotis, and L. Gravano. 2009. Building
query optimizers for information extraction: the sqout
project. SIGMOD Rec., 37:28?34.
R. Krishnamurthy, Y. Li, S. Raghavan, F. Reiss,
S. Vaithyanathan, and H. Zhu. 2008. SystemT: a sys-
tem for declarative information extraction. SIGMOD
Record, 37(4):7?13.
D. Z. Wang, E. Michelakis, M. J. Franklin, M. Garo-
falakis, and J. M. Hellerstein. 2010. Probabilistic
declarative information extraction. In ICDE.
114
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 109?114,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
WizIE: A Best Practices Guided Development Environment
for Information Extraction
Yunyao Li Laura Chiticariu Huahai Yang Frederick R. Reiss Arnaldo Carreno-fuentes
IBM Research - Almaden
650 Harry Road
San Jose, CA 95120
{yunyaoli,chiti,hyang,frreiss,acarren}@us.ibm.com
Abstract
Information extraction (IE) is becoming a crit-
ical building block in many enterprise appli-
cations. In order to satisfy the increasing text
analytics demands of enterprise applications,
it is crucial to enable developers with general
computer science background to develop high
quality IE extractors. In this demonstration,
we present WizIE, an IE development envi-
ronment intended to reduce the development
life cycle and enable developers with little or
no linguistic background to write high qual-
ity IE rules. WizIE provides an integrated
wizard-like environment that guides IE devel-
opers step-by-step throughout the entire devel-
opment process, based on best practices syn-
thesized from the experience of expert devel-
opers. In addition, WizIE reduces the manual
effort involved in performing key IE develop-
ment tasks by offering automatic result expla-
nation and rule discovery functionality. Pre-
liminary results indicate that WizIE is a step
forward towards enabling extractor develop-
ment for novice IE developers.
1 Introduction
Information Extraction (IE) refers to the problem of
extracting structured information from unstructured
or semi-structured text. It has been well-studied by
the Natural Language Processing research commu-
nity for a long time. In recent years, IE has emerged
as a critical building block in a wide range of enter-
prise applications, including financial risk analysis,
social media analytics and regulatory compliance,
among many others. An important practical chal-
lenge driven by the use of IE in these applications
is usability (Chiticariu et al, 2010c): specifically,
how to enable the ease of development and mainte-
nance of high-quality information extraction rules,
also known as annotators, or extractors.
Developing extractors is a notoriously labor-
intensive and time-consuming process. In order to
ensure highly accurate and reliable results, this task
is traditionally performed by trained linguists with
domain expertise. As a result, extractor develop-
ment is regarded as a major bottleneck in satisfying
the increasing text analytics demands of enterprise
applications. Hence, reducing the extractor devel-
opment life cycle is a critical requirement. Towards
this goal, we have built WizIE, an IE development
environment designed primarily to (1) enable devel-
opers with little or no linguistic background to write
high quality extractors, and (2) reduce the overall
manual effort involved in extractor development.
Previous work on improving the usability of IE
systems has mainly focused on reducing the manual
effort involved in extractor development (Brauer et
al., 2011; Li et al, 2008; Li et al, 2011a; Soder-
land, 1999; Liu et al, 2010). In contrast, the fo-
cus of WizIE is on lowering the extractor develop-
ment entry barrier by means of a wizard-like en-
vironment that guides extractor development based
on best practices drawn from the experience of
trained linguists and expert developers. In doing so,
WizIE also provides natural entry points for differ-
ent tools focused on reducing the effort required for
performing common tasks during IE development.
Underlying our WizIE are a state-of-the-art
IE rule language and corresponding runtime en-
gine (Chiticariu et al, 2010a; Li et al, 2011b). The
runtime engine and WizIE are commercially avail-
109
Profile Extractor  Test  ExtractorDevelop  ExtractorInput Documents Label Text/Clues
Task Analysis Rule Development Performance Tuning Delivery
Export Extractor
Figure 1: Best Practices for Extractor Development
able as part of IBM InfoSphere BigInsights (IBM,
2012).
2 System Overview
The development process for high-quality, high-
performance extractors consists of four phases, as
illustrated in Fig. 1. First, in the Task Analysis
phase, concrete extraction tasks are defined based
on high-level business requirements. For each ex-
traction task, IE rules are developed during the Rule
Development phase. The rules are profiled and fur-
ther fine-tuned in the Performance Tuning phase, to
ensure high runtime performance. Finally, in theDe-
livery phase, the rules are packaged so that they can
be easily embedded in various applications.
WizIE is designed to assist and enable both novice
and experienced developers by providing an intu-
itive wizard-like interface that is informed by the
best practices in extractor development throughout
each of these phases. By doing so, WizIE seeks
to provide the key missing pieces in a conventional
IE development environment (Cunningham et al,
2002; Li et al, 2011b; Soundrarajan et al, 2011),
based on our experience as expert IE developers, as
well as our interactions with novice developers with
general computer science background, but little text
analytics experience, during the development of sev-
eral enterprise applications.
3 The Development Environment
In this section, we present the general functionality
of WizIE in the context of extraction tasks driven
by real business use cases from the media and en-
tertainment domain. We describe WizIE in details
and show how it guides and assists IE developers in
a step-by-step fashion, based on best practices.
3.1 Task Analysis
The high-level business requirement of our run-
ning example is to identify intention to purchase
for movies from online forums. Such information
is of great interest to marketers as it helps pre-
dict future purchases (Howard and Sheth, 1969).
During the first phrase of IE development (Fig. 2),
WizIE guides the rule developer in turning such a
high-level business requirement into concrete ex-
traction tasks by explicitly asking her to select and
manually examine a small number 1 of sample doc-
uments, identify and label snippets of interest in the
sample documents, and capture clues that help to
identify such snippets.
The definition and context of the concrete extrac-
tion tasks are captured by a tree structure called the
extraction plan (e.g. right panel in Fig. 2). Each
leaf node in an extraction plan corresponds to an
atomic extraction task, while the non-leaf nodes de-
note higher-level tasks based on one or more atomic
extraction tasks. For instance, in our running ex-
ample, the business question of identifying intention
of purchase for movies has been converted into the
extraction task of identifying MovieIntent mentions,
which involves two atomic extraction tasks: identi-
fying Movie mentions and Intent mentions.
The extraction plan created, as we will describe
later, plays a key role in the IE development process
in WizIE. Such tight coupling of task analysis with
actual extractor development is a key departure from
conventional IE development environments.
3.2 Rule Development
Once concrete extraction tasks are defined,
WizIE guides the IE developer to write actual rules
based on best practices. Fig. 3(a) shows a screenshot
of the second phase of building an extractor, the
Rule Development phase. The Extraction Task panel
on the left provides information and tips for rule
development, whereas the Extraction Plan panel
on the right guides the actual rule development
for each extraction task. As shown in the figure,
the types of rules associated with each label node
fall into three categories: Basic Features, Can-
1The exact sample size varies by task type.
110
Figure 2: Labeling Snippets and Clues of Interest
didate Generation and Filter&Consolidate. This
categorization is based on best practices for rule
development (Chiticariu et al, 2010b). As such,
the extraction plan groups together the high-level
specification of extraction tasks via examples, and
the actual implementation of those tasks via rules.
The developer creates rules directly in the Rule
Editor, or via the Create Statement wizard, acces-
sible from the Statements node of each label in the
Extraction Plan panel:
The wizard allows the user to select a type for
the new rule, from predefined sets for each of the
three categories. The types of rules exposed in each
category are informed by best practices. For ex-
ample, the Basic Features category includes rules
for defining basic features using regular expressions,
dictionaries or part of speech information, whereas
the Candidate Generation category includes rules for
combining basic features into candidate mentions by
means of operations such as sequence or alternation.
Once the developer provides a name for the new rule
(view) and selects its type, the appropriate rule tem-
plate (such as the one illustrated below) is automat-
ically generated in an appropriate file on disk and
displayed in the editor, for further editing 2.
Once the developer completes an iteration of rule
development, WizIE guides her in testing and refin-
ing the extractor, as shown in Fig. 3(b). The An-
notation Explorer at the bottom of the screen gives
a global view of the extraction results, while other
panels highlight individual results in the context of
the original input documents. The Annotation Ex-
plorer enables filtering and searching results, and
comparing results with those from a previous iter-
ation. WizIE also provides a facility for manually
labeling a document collection with ?ground truth?
annotations, then comparing the extraction results
with the ground truth in order to formally evalu-
ate the quality of the extractor and avoid regressions
during the development process.
An important differentiator of WizIE compared
with conventional IE development environments is
a suite of sophisticated tools for automatic result ex-
planation and rule discovery. We briefly describe
them next.
Provenance Viewer. When the user clicks on an ex-
tracted result, the Provenance Viewer shows a com-
plete explanation of how that result has been pro-
2Details on the rule syntax can be found in (IBM, )
111
AB
Figure 3: Extractor Development: (a) Developing, and (b) Testing.
duced by the extractor, in the form of a graph that
demonstrates the sequence of rules and individual
pieces of text responsible for that result. Such expla-
nations are critical to enable the developer to under-
stand why a false positive is generated by the sys-
tem, and identify problematic rule(s) that could be
refined in order to correct the mistake. An example
explanation for an incorrect MovieIntent mention ?I
just saw Mission Impossible? is shown below.
As can be seen, the MovieIntent mention is gener-
ated by combining a SelfRef (matching first person
pronouns) with a MovieName mention, and in turn,
the latter is obtained by combining several MovieN-
ameCandidate mentions. With this information, the
developer can quickly determine that the SelfRef and
MovieName mentions are correct, but their combina-
tion in MovieIntentCandidate is problematic. She can
then proceed to refine the MovieIntentCandidate rule,
for example, by avoiding any MovieIntentCandidate
mentions containing a past tense verb form such as
saw, since past tense in not usually indicative of in-
tent (Liu et al, 2010).
Pattern Discovery. Negative contextual clues such
as the verb ?saw? above are useful for creating rules
that filter out false positives. Conversely, positive
clues such as the phrase ?will see? are useful for
creating rules that separate ambiguous matches from
high-precision matches. WizIE?s Pattern Discovery
component facilitates automatic discovery of such
clues by mining available sample data for common
patterns in specific contexts (Li et al, 2011a). For
example, when instructed to analyze the context be-
tween SelfRef and MovieName mentions, Pattern Dis-
covery finds a suite of common patterns as shown
in Fig. 4. The developer can analyze these patterns
and choose those suitable for refining the rules. For
example, patterns such as ?have to see? can be seen
as positive clues for intent, whereas phrases such as
?took ... to see? or ?went to see? are negative clues,
and can be used for filtering false positives.
Regular Expression Generator. WizIE also en-
ables the discovery of regular expression patterns.
The Regular Expression Generator takes as input a
112
Figure 4: Pattern Discovery
Figure 5: Regular Expression Generator
set of sample mentions and suggests regular expres-
sions that capture the samples, ranging from more
specific (higher accuracy) to more general expres-
sions (higher coverage). Figure 5 shows two reg-
ular expressions automatically generated based on
mentions of movie ratings, and how the developer is
subsequently assisted in understanding and refining
the generated expression. In our experience, regu-
lar expressions are complex concepts that are diffi-
cult to develop for both expert and novice develop-
ers. Therefore, such a facility to generate expres-
sions based on examples is extremely useful.
3.3 Performance Tuning
Once the developer is satisfied with the quality of the
extractor, WizIE guides her in measuring and tuning
its runtime performance, in preparation for deploy-
ing the extractor in a production environment. The
Profiler observes the execution of the extractor on
a sample input collection over a period of time and
records the percentage of time spent executing each
rule, or performing certain runtime operations. After
the profiling run completes, WizIE displays the top
25 most expensive rules and runtime operations, and
the overall throughput (amount of input data pro-
cessed per unit of time). Based on this information,
the developer can hand-tune the critical parts of the
extractor, rerun the Profiler, and validate an increase
in throughput. She would repeat this process until
satisfied with the extractor?s runtime performance.
3.4 Delivery and Deployment
Once satisfied with both the result quality and
runtime performance, the developer is guided by
WizIE?s Export wizard through the process of ex-
porting the extractor in a compiled executable form.
The generated executable can be embedded in an ap-
plication using a Java API interface. WizIE can also
wrap the executable plan in a pre-packaged applica-
tion that can be run in a map-reduce environment,
then deploy this application on a Hadoop cluster.
4 Evaluation
A preliminary user study was conducted to evalu-
ate the effectiveness of WizIE in enabling novice IE
developers. The study included 14 participants, all
employed at a major technology company. In the
pre-study survey, 10 of the participants reported no
prior experience with IE tasks, two of them have
seen demonstrations of IE systems, and two had
brief involvement in IE development, but no expe-
rience with WizIE. For the question ?According to
your understanding, how easy is it to build IE appli-
cations in general ??, the median rating was 5, on a
113
scale of 1 (very easy) to 7 (very difficult).
The study was conducted during a 2-day training
session. In Day 1, participants were given a thor-
ough introduction to IE, shown example extractors,
and instructed to develop extractors without WizIE.
Towards the end of Day 1, participants were asked
to solve an IE exercise: develop an extractor for
the high-level requirement of identifying mentions
of company revenue by division from the company?s
official press releases. WizIE was introduced to the
participants in Day 2 of the training, and its fea-
tures were demonstrated and explained with exam-
ples. Participants were then asked to complete the
same exercise as in Day 1. Authors of this demon-
stration were present to help participants during the
exercises in both days. At the end of each day, par-
ticipants filled out a survey about their experience.
In Day 1, none of the participants were able to
complete the exercise after 90 minutes. In the sur-
vey, one participant wrote ?I am in sales so it is all
difficult?; another participant indicated that ?I don?t
think I would be able to recreate the example on my
own from scratch?. In Day 2, most participants were
able to complete the exercise in 90 minutes or less
usingWizIE. In fact, two participants created extrac-
tors with accuracy and coverage of over 90%, when
measured against the ground truth. Overall, the par-
ticipants were much more confident about creating
extractors. One participant wrote ?My first impres-
sion is very good?. On the other hand, another par-
ticipant asserted that ?The nature of the task is still
difficult?. They also found that WizIE is useful and
easy to use, and it is easier to build extractors with
the help of WizIE.
In summary, our preliminary results indicate that
WizIE is a step forward towards enabling extractor
development for novice IE developers. In order to
formally evaluate WizIE, we are currently conduct-
ing a formal study of using WizIE to create extrac-
tors for several real business applications.
5 Demonstration
In this demonstration we showcase WizIE?s step-by-
step approach to guide the developer in the iterative
process of IE rule development, from task analysis
to developing, tuning and deploying the extractor
in a production environment. Our demonstration is
centered around the high-level business requirement
of identifying intent to purchase movies from blogs
and forum posts as described in Section 3. We start
by demonstrating the process of developing two rel-
atively simple extractors for identifying MovieIntent
and MovieRating mentions. We then showcase com-
plex state-of-the-art extractors for identifying buzz
and sentiment for the media and entertainment do-
main, to illustrate the quality and runtime perfor-
mance of extractors built with WizIE.
References
F. Brauer, R. Rieger, A. Mocan, and W. M. Barczynski.
2011. Enabling information extraction by inference of
regular expressions from sample entities. In CIKM.
L. Chiticariu, R. Krishnamurthy, Y. Li, S. Raghavan,
F. Reiss, and S. Vaithyanathan. 2010a. SystemT: an
algebraic approach to declarative information extrac-
tion. ACL.
L. Chiticariu, R. Krishnamurthy, Y. Li, F. Reiss, and
S. Vaithyanathan. 2010b. Domain adaptation of rule-
based annotators for named-entity recognition tasks.
EMNLP.
L. Chiticariu, Y. Li, S. Raghavan, and F. Reiss. 2010c.
Enterprise Information Extraction: Recent Develop-
ments and Open Challenges. In SIGMOD (Tutorials).
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. Gate: an architecture for develop-
ment of robust hlt applications. In ACL.
J.A. Howard and J.N. Sheth. 1969. The Theory of Buyer
Behavior. Wiley.
IBM. InfoSphere BigInsights - Annotation Query Lan-
guage (AQL) reference. http://ibm.co/kkzj1i.
IBM. 2012. InfoSphere BigInsights. http://ibm.co/jjbjfa.
Y. Li, R. Krishnamurthy, S. Raghavan, S. Vaithyanathan,
and H. V. Jagadish. 2008. Regular expression learning
for information extraction. In EMNLP.
Y. Li, V. Chu, S. Blohm, H. Zhu, and H. Ho. 2011a. Fa-
cilitating pattern discovery for relation extraction with
semantic-signature-based clustering. In CIKM.
Y. Li, F. Reiss, and L. Chiticariu. 2011b. SystemT: A
Declarative Information Extraction System. In ACL
(Demonstration).
B. Liu, L. Chiticariu, V. Chu, H. V. Jagadish, and F. Reiss.
2010. Automatic Rule Refinement for Information
Extraction. PVLDB, 3(1):588?597.
S. Soderland. 1999. Learning information extrac-
tion rules for semi-structured and free text. Machine
Learning, 34(1-3):233?272, February.
B. R. Soundrarajan, T. Ginter, and S. L. DuVall. 2011.
An interface for rapid natural language processing de-
velopment in UIMA. In ACL (Demonstrations).
114
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1159?1168,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Adaptive Parser-Centric Text Normalization
Congle Zhang?
Dept of Computer Science and Engineering
University of Washington, Seattle, WA 98195, USA
clzhang@cs.washington.edu
Tyler Baldwin Howard Ho Benny Kimelfeld Yunyao Li
IBM Research - Almaden
650 Harry Road, San Jose, CA 95120, USA
{tbaldwi,ctho,kimelfeld,yunyaoli}@us.ibm.com
Abstract
Text normalization is an important first
step towards enabling many Natural Lan-
guage Processing (NLP) tasks over infor-
mal text. While many of these tasks, such
as parsing, perform the best over fully
grammatically correct text, most existing
text normalization approaches narrowly
define the task in the word-to-word sense;
that is, the task is seen as that of mapping
all out-of-vocabulary non-standard words
to their in-vocabulary standard forms. In
this paper, we take a parser-centric view
of normalization that aims to convert raw
informal text into grammatically correct
text. To understand the real effect of nor-
malization on the parser, we tie normal-
ization performance directly to parser per-
formance. Additionally, we design a cus-
tomizable framework to address the often
overlooked concept of domain adaptabil-
ity, and illustrate that the system allows for
transfer to new domains with a minimal
amount of data and effort. Our experimen-
tal study over datasets from three domains
demonstrates that our approach outper-
forms not only the state-of-the-art word-
to-word normalization techniques, but also
manual word-to-word annotations.
1 Introduction
Text normalization is the task of transforming in-
formal writing into its standard form in the lan-
guage. It is an important processing step for a
wide range of Natural Language Processing (NLP)
tasks such as text-to-speech synthesis, speech
recognition, information extraction, parsing, and
machine translation (Sproat et al, 2001).
?This work was conducted at IBM.
The use of normalization in these applications
poses multiple challenges. First, as it is most often
conceptualized, normalization is seen as the task
of mapping all out-of-vocabulary non-standard
word tokens to their in-vocabulary standard forms.
However, the scope of the task can also be seen as
much wider, encompassing whatever actions are
required to convert the raw text into a fully gram-
matical sentence. This broader definition of the
normalization task may include modifying punc-
tuation and capitalization, and adding, removing,
or reordering words. Second, as with other NLP
techniques, normalization approaches are often fo-
cused on one primary domain of interest (e.g.,
Twitter data). Because the style of informal writ-
ing may be different in different data sources,
tailoring an approach towards a particular data
source can improve performance in the desired do-
main. However, this is often done at the cost of
adaptability.
This work introduces a customizable normal-
ization approach designed with domain transfer in
mind. In short, customization is done by provid-
ing the normalizer with replacement generators,
which we define in Section 3. We show that the
introduction of a small set of domain-specific gen-
erators and training data allows our model to out-
perform a set of competitive baselines, including
state-of-the-art word-to-word normalization. Ad-
ditionally, the flexibility of the model also allows it
to attempt to produce fully grammatical sentences,
something not typically handled by word-to-word
normalization approaches.
Another potential problem with state-of-the-art
normalization is the lack of appropriate evaluation
metrics. The normalization task is most frequently
motivated by pointing to the need for clean text
for downstream processing applications, such as
syntactic parsing. However, most studies of nor-
malization give little insight into whether and to
what degree the normalization process improves
1159
the performance of the downstream application.
For instance, it is unclear how performance mea-
sured by the typical normalization evaluation met-
rics of word error rate and BLEU score (Pap-
ineni et al, 2002) translates into performance on
a parsing task, where a well placed punctuation
mark may provide more substantial improvements
than changing a non-standard word form. To ad-
dress this problem, this work introduces an eval-
uation metric that ties normalization performance
directly to the performance of a downstream de-
pendency parser.
The rest of this paper is organized as follows.
In Section 2 we discuss previous approaches to
the normalization problem. Section 3 presents
our normalization framework, including the actual
normalization and learning procedures. Our in-
stantiation of this model is presented in Section 4.
In Section 5 we introduce the parser driven eval-
uation metric, and present experimental results of
our model with respect to several baselines in three
different domains. Finally, we discuss our exper-
imental study in Section 6 and conclude in Sec-
tion 7.
2 Related Work
Sproat et al (2001) took the first major look at
the normalization problem, citing the need for nor-
malized text for downstream applications. Unlike
later works that would primarily focus on specific
noisy data sets, their work is notable for attempt-
ing to develop normalization as a general process
that could be applied to different domains. The re-
cent rise of heavily informal writing styles such as
Twitter and SMS messages set off a new round of
interest in the normalization problem.
Research on SMS and Twitter normalization has
been roughly categorized as drawing inspiration
from three other areas of NLP (Kobus et al, 2008):
machine translation, spell checking, and automatic
speech recognition. The statistical machine trans-
lation (SMT) metaphor was the first proposed to
handle the text normalization problem (Aw et al,
2006). In this mindset, normalizing SMS can be
seen as a translation task from a source language
(informal) to a target language (formal), which can
be undertaken with typical noisy channel based
models. Work by Choudhury et al (2007) adopted
the spell checking metaphor, casting the problem
in terms of character-level, rather than word-level,
edits. They proposed an HMM based model that
takes into account both grapheme and phoneme
information. Kobus et al (2008) undertook a
hybrid approach that pulls inspiration from both
the machine translation and speech recognition
metaphors.
Many other approaches have been examined,
most of which are at least partially reliant on
the above three metaphors. Cook and Steven-
son (2009) perform an unsupervised method,
again based on the noisy channel model. Pen-
nell and Liu (2011) developed a CRF tagger for
deletion-based abbreviation on tweets. Xue et
al. (2011) incorporated orthographic, phonetic,
contextual, and acronym expansion factors to nor-
malize words in both Twitter and SMS. Liu et
al. (2011) modeled the generation process from
dictionary words to non-standard tokens under an
unsupervised sequence labeling framework. Han
and Baldwin (2011) use a classifier to detect ill-
formed words, and then generate correction can-
didates based on morphophonemic similarity. Re-
cent work has looked at the construction of nor-
malization dictionaries (Han et al, 2012) and on
improving coverage by integrating different hu-
man perspectives (Liu et al, 2012).
Although it is almost universally used as a mo-
tivating factor, most normalization work does not
directly focus on improving downstream appli-
cations. While a few notable exceptions high-
light the need for normalization as part of text-
to-speech systems (Beaufort et al, 2010; Pennell
and Liu, 2010), these works do not give any di-
rect insight into how much the normalization pro-
cess actually improves the performance of these
systems. To our knowledge, the work presented
here is the first to clearly link the output of a nor-
malization system to the output of the downstream
application. Similarly, our work is the first to pri-
oritize domain adaptation during the new wave of
text message normalization.
3 Model
In this section we introduce our normalization
framework, which draws inspiration from our pre-
vious work on spelling correction for search (Bao
et al, 2011).
3.1 Replacement Generators
Our input the original, unnormalized text, repre-
sented as a sequence x = x1, x2, . . . , xn of tokens
xi. In this section we will use the following se-
1160
quence as our running example:
x = Ay1 woudent2 of3 see4 ?em5
where space replaces comma for readability, and
each token is subscripted by its position. Given the
input x, we apply a series of replacement genera-
tors, where a replacement generator is a function
that takes x as input and produces a collection of
replacements. Here, a replacement is a statement
of the form ?replace tokens xi, . . . , xj?1 with s.?
More precisely, a replacement is a triple ?i, j, s?,
where 1 ? i ? j ? n + 1 and s is a sequence of
tokens. Note that in the case where i = j, the se-
quence s should be inserted right before xi; and in
the special case where s is empty, we simply delete
xi, . . . , xj?1. For instance, in our running exam-
ple the replacement ?2, 3,would not? replaces
x2 = woudent with would not; ?1, 2,Ay? re-
places x1 with itself (hence, does not change x);
?1, 2, ? (where  is the empty sequence) deletes
x1; ?6, 6,.? inserts a period at the end of the se-
quence.
The provided replacement generators can be ei-
ther generic (cross domain) or domain-specific, al-
lowing for domain customization. In Section 4,
we discuss the replacement generators used in our
empirical study.
3.2 Normalization Graph
Given the input x and the set of replacements pro-
duced by our generators, we associate a unique
Boolean variable Xr with each replacement r. As
expected, Xr being true means that the replace-
ment r takes place in producing the output se-
quence.
Next, we introduce dependencies among vari-
ables. We first discuss the syntactic consistency
of truth assignments. Let r1 = ?i1, j1, s1? and
r2 = ?i2, j2, s2? be two replacements. We say
that r1 and r2 are locally consistent if the inter-
vals [i1, j1) and [i2, j2) are disjoint. Moreover,
we do not allow two insertions to take place at
the same position; therefore, we exclude [i1, j1)
and [i2, j2) from the definition of local consistency
when i1 = j1 = i2 = j2. If r1 and r2 are locally
consistent and j1 = i2, then we say that r2 is a
consistent follower of r1.
A truth assignment ? to our variables Xr is
sound if every two replacements r and r? with
?(Xr) = ?(Xr?) = true are locally consis-
tent. We say that ? is complete if every token
of x is captured by at least one replacement r
with ?(Xr) = true. Finally, we say that ?
is legal if it is sound and complete. The out-
put (normalized sequence) defined by a legal as-
signment ? is, naturally, the concatenation (from
left to right) of the strings s in the replacements
r = ?i, j, s? with ?(Xr) = true. In Fig-
ure 1, for example, if the nodes with a grey
shade are the ones associated with true vari-
ables under ?, then the output defined by ? is
I would not have seen them.
Our variables carry two types of interdependen-
cies. The first is that of syntactic consistency: the
entire assignment is required to be legal. The sec-
ond captures correlation among replacements. For
instance, if we replace of with have in our run-
ning example, then the next see token is more
likely to be replaced with seen. In this work,
dependencies of the second type are restricted to
pairs of variables, where each pair corresponds to
a replacement and a consistent follower thereof.
The above dependencies can be modeled over a
standard undirected graph using Conditional Ran-
dom Fields (Lafferty et al, 2001). However, the
graph would be complex: in order to model lo-
cal consistency, there should be edges between ev-
ery two nodes that violate local consistency. Such
a model renders inference and learning infeasi-
ble. Therefore, we propose a clearer model by a
directed graph, as illustrated in Figure 1 (where
nodes are represented by replacements r instead
of the variables Xr, for readability). To incorpo-
rate correlation among replacements, we introduce
an edge from Xr to Xr? whenever r? is a consis-
tent follower of r. Moreover, we introduce two
dummy nodes, start and end, with an edge from
start to each variable that corresponds to a prefix
of the input sequence x, and an edge from each
variable that corresponds to a suffix of x to end.
The principal advantage of modeling the depen-
dencies in such a directed graph is that now, the le-
gal assignments are in one-to-one correspondence
with the paths from start to end; this is a straight-
forward observation that we do not prove here.
We appeal to the log-linear model formulation
to define the probability of an assignment. The
conditional probability of an assignment ?, given
an input sequence x and the weight vector ? =
??1, . . . , ?k? for our features, is defined as p(? |
1161
?1, 2,I?
end
?2, 4,would not have?
?1, 2,Ay?
?5, 6,them?
?4, 5,seen?
?2, 3,would?
?4, 6,see him?
?3, 4,of?
start
?6, 6, .?
Figure 1: Example of a normalization graph; the
nodes are replacements generated by the replace-
ment generators, and every path from start to end
implies a legal assignment
x,?) = 0 if ? is not legal, and otherwise,
p(? | x,?) = 1Z(x)
?
X?Y ??
exp(
?
j
?j?j(X,Y,x)) .
Here, Z(x) is the partition function, X ? Y ? ?
refers to an edge X ? Y with ?(X) = true and
?(Y ) = true, and ?1(X,Y,x), . . . , ?k(X,Y,x)
are real valued feature functions that are weighted
by ?1, . . . , ?k (the model?s parameters), respec-
tively.
3.3 Inference
When performing inference, we wish to select
the output sequence with the highest probability,
given the input sequence x and the weight vector
? (i.e., MAP inference). Specifically, we want an
assignment ?? = arg max? p(? | x,?).
While exact inference is computationally hard
on general graph models, in our model it boils
down to finding the longest path in a weighted
and acyclic directed graph. Indeed, our directed
graph (illustrated in Figure 1) is acyclic. We as-
sign the real value ?j ?j?j(X,Y,x) to the edge
X ? Y , as the weight. As stated in Section 3.2,
a legal assignment ? corresponds to a path from
start to end; moreover, the sum of the weights on
that path is equal to log p(? | x,?) + logZ(x).
In particular, a longer path corresponds to an as-
signment with greater probability. Therefore, we
can solve the MAP inference within our model by
finding the weighted longest path in the directed
acyclic graph. The algorithm in Figure 2 summa-
rizes the inference procedure to normalize the in-
put sequence x.
Input:
1. A sequence x to normalize;
2. A weight vector ? = ??1, . . . , ?k?.
Generate replacements: Apply all replace-
ment generators to get a set of replacements r,
each r is a triple ?i, j, s?.
Build a normalization graph:
1. For each replacement r, create a node Xr.
2. For each r? and r, create an edge Xr to
Xr? if r? is a consistent follower of r.
3. Create two dummy nodes start and end,
and create edges from start to all prefix
nodes and end to all suffix nodes.
4. For each edge X ? Y , compute the fea-
tures ?j(X,Y,x), and weight the edge by?
j ?j?j(X,Y,x).
MAP Inference: Find a weighted longest path
P from start to end, and return ??, where
??(Xr) = true iff Xr ? P .
Figure 2: Normalization algorithm
3.4 Learning
Our labeled data consists of pairs (xi,ygoldi ),
where xi is an input sequence (to normalize) and
ygoldi is a (manually) normalized sequence. We
obtain a truth assignment ?goldi from each ygoldi
by selecting an assignment ? that minimizes the
edit distance between ygoldi and the normalized
text implied by ?:
?goldi = arg min? DIST(y(?),y
gold
i ) (1)
Here, y(?) denotes the normalized text implied by
?, and DIST is a token-level edit distance. We
apply a simple dynamic-programming algorithm
to compute ?goldi . Finally, the items in our training
data are the pairs (xi, ?goldi ).
Learning over similar models is commonly
done via maximum likelihood estimation:
L(?) = log
?
i
p(?i = ?goldi | xi,?)
Taking the partial derivative gives the following:
?
i
(
?j(?goldi ,xi)? Ep(?i|xi,?)?j(?i,xi)
)
where ?j(?,x) = ?X?Y ?j(X,Y,x), that is,
the sum of values for the jth feature along the
1162
Input:
1. A set {(xi,ygoldi )}
n
i=1 of sequences andtheir gold normalization;
2. Number T of iterations.
Initialization: Initialize each ?j as zero, and
obtain each ?goldi according to (1).
Repeat T times:
1. Infer each ??i from xi using the current ?;
2. ?j ? ?j+?i(?j(?goldi ,xi)??j(??i ,xi))
for all j = 1, . . . , k.
Output: ? = ??1, . . . , ?k?
Figure 3: Learning algorithm
path defined by ?, andEp(?i|xi,?)?j(?i,xi) is the
expected value of that sum (over all legal assign-
ments ?i), assuming the current weight vector.
How to efficiently compute
Ep(?i|xi,?)?j(?i,xi) in our model is un-
clear; naively, it requires enumerating all legal
assignments. We instead opt to use a more
tractable perceptron-style algorithm (Collins,
2002). Instead of computing the expectation,
we simply compute ?j(??i ,xi), where ??i is the
assignment with the highest probability, generated
using the current weight vector. The result is then:
?
i
(
?j(?goldi ,xi)? ?j(??i ,xi)
)
Our learning applies the following two steps it-
eratively. (1) Generate the most probable sequence
within the current weights. (2) Update the weights
by comparing the path generated in the previous
step to the gold standard path. The algorithm in
Figure 3 summarizes the procedure.
4 Instantiation
In this section, we discuss our instantiation of the
model presented in the previous section. In partic-
ular, we describe our replacement generators and
features.
4.1 Replacement Generators
One advantage of our proposed model is that
the reliance on replacement generators allows for
strong flexibility. Each generator can be seen as a
black box, allowing replacements that are created
heuristically, statistically, or by external tools to be
incorporated within the same framework.
Generator From To
leave intact good good
edit distance bac back
lowercase NEED need
capitalize it It
Google spell disspaear disappear
contraction wouldn?t would not
slang language ima I am going to
insert punctuation  .
duplicated punctuation !? !
delete filler lmao 
Table 1: Example replacement generators
To build a set of generic replacement generators
suitable for normalizing a variety of data types, we
collected a set of about 400 Twitter posts as devel-
opment data. Using that data, a series of gener-
ators were created; a sample of them are shown
in Table 1. As shown in the table, these gener-
ators cover a variety of normalization behavior,
from changing non-standard word forms to insert-
ing and deleting tokens.
4.2 Features
Although the proposed framework supports real
valued features, all features in our system are bi-
nary. In total, we used 70 features. Our feature set
pulls information from several different sources:
N-gram: Our n-gram features indicate the fre-
quency of the phrases induced by an edge. These
features are turned into binary ones by bucketing
their log values. For example, on the edge from
?1, 2,I? to ?2, 3,would? such a feature will indi-
cate whether the frequency of I would is over
a threshold. We use the Corpus of Contemporary
English (Davies, 2008 ) to produce our n-gram in-
formation.
Part-of-speech: Part-of-speech information
can be used to produce features that encourage
certain behavior, such as avoiding the deletion of
noun phrases. We generate part-of-speech infor-
mation over the original raw text using a Twit-
ter part-of-speech tagger (Ritter et al, 2011). Of
course, the part-of-speech information obtained
this way is likely to be noisy, and we expect our
learning algorithm to take that into account.
Positional: Information from positions is used
primarily to handle capitalization and punctuation
insertion, for example, by incorporating features
for capitalized words after stop punctuation or the
insertion of stop punctuation at the end of the sen-
tence.
Lineage: Finally, we include binary features
1163
that indicate which generator spawned the replace-
ment.
5 Evaluation
In this section, we present an empirical study of
our framework. The study is done over datasets
from three different domains. The goal is to eval-
uate the framework in two aspects: (1) usefulness
for downstream applications (specifically depen-
dency parsing), and (2) domain adaptability.
5.1 Evaluation Metrics
A few different metrics have been used to evaluate
normalizer performance, including word error rate
and BLEU score. While each metric has its pros
and cons, they all rely on word-to-word matching
and treat each word equally. In this work, we aim
to evaluate the performance of a normalizer based
on how it affects the performance of downstream
applications. We find that the conventional metrics
are not directly applicable, for several reasons. To
begin with, the assumption that words have equal
weights is unlikely to hold. Additionally, these
metrics tend to ignore other important non-word
information such as punctuation or capitalization.
They also cannot take into account other aspects
that may have an impact on downstream perfor-
mance, such as the word reordering as seen in the
example in Figure 4. Therefore, we propose a new
evaluation metric that directly equates normaliza-
tion performance with the performance of a com-
mon downstream application?dependency pars-
ing.
To realize our desired metric, we apply the fol-
lowing procedure. First, we produce gold standard
normalized data by manually normalizing sen-
tences to their full grammatically correct form. In
addition to the word-to-word mapping performed
in typical normalization gold standard generation,
this annotation procedure includes all actions nec-
essary to make the sentence grammatical, such as
word reordering, modifying capitalization, and re-
moving emoticons. We then run an off-the-shelf
dependency parser on the gold standard normal-
ized data to produce our gold standard parses. Al-
though the parser could still produce mistakes on
the grammatical sentences, we feel that this pro-
vides a realistic benchmark for comparison, as it
represents an upper bound on the possible perfor-
mance of the parser, and avoids an expensive sec-
ond round of manual annotation.
Test Gold SVO
I kinda wanna get
ipad NEW
I kind of want to
get a new iPad.
verb(get) verb(want)verb(get)
precisionv = 11
recallv = 12
subj(get,I)
subj(get,wanna)
obj(get,NEW)
subj(want,I)
subj(get,I)
obj(get,iPad)
precisionso = 13
recallso = 13
Figure 4: The subjects, verbs, and objects identi-
fied on example test/gold text, and corresponding
metric scores
To compare the parses produced over automati-
cally normalized data to the gold standard, we look
at the subjects, verbs, and objects (SVO) identi-
fied in each parse. The metric shown in Equa-
tions (2) and (3) below is based on the identified
subjects and objects in those parses. Note that SO
denotes the set of identified subjects and objects
whereas SOgold denotes the set of subjects and
objects identified when parsing the gold-standard
normalization.
precisionso =
|SO ? SOgold|
|SO | (2)
recallso = |SO ? SO
gold|
|SOgold|
(3)
We similarly define precisionv and recallv, where
we compare the set V of identified verbs to V gold
of those found in the gold-standard normalization.
An example is shown in Figure 4.
5.2 Results
To establish the extensibility of our normaliza-
tion system, we present results in three different
domains: Twitter posts, Short Message Service
(SMS) messages, and call-center logs. For Twitter
and SMS messages, we used established datasets
to compare with previous work. As no estab-
lished call-center log dataset exists, we collected
our own. In each case, we ran the proposed system
with two different configurations: one using only
the generic replacement generators presented in
Section 4 (denoted as generic), and one that adds
additional domain-specific generators for the cor-
responding domain (denoted as domain-specific).
All runs use ten-fold cross validation for training
and evaluation. The Stanford parser1 (Marneffe
et al, 2006) was used to produce all dependency
1Version 2.0.4, http://nlp.stanford.edu/
software/lex-parser.shtml
1164
parses. We compare our system to the following
baseline solutions:
w/oN: No normalization is performed.
Google: Output of the Google spell checker.
w2wN: The output of the word-to-word normal-
ization of Han and Baldwin (2011). Not available
for call-center data.
Gw2wN: The manual gold standard word-to-
word normalizations of previous work (Choud-
hury et al, 2007; Han and Baldwin, 2011). Not
available for call-center data.
Our results use the metrics of Section 5.1.
5.2.1 Twitter
To evaluate the performance on Twitter data, we
use the dataset of randomly sampled tweets pro-
duced by (Han and Baldwin, 2011). Because the
gold standard used in this work only provided
word mappings for out-of-vocabulary words and
did not enforce grammaticality, we reannotated the
gold standard data2. Their original gold standard
annotations were kept as a baseline.
To produce Twitter-specific generators, we ex-
amined the Twitter development data collected for
generic generator production (Section 4). These
generators focused on the Twitter-specific notions
of hashtags (#), ats (@), and retweets (RT). For
each case, we implemented generators that al-
lowed for either the initial symbol or the entire to-
ken to be deleted (e.g., @Hertz to Hertz, @Hertz
to ).
The results are given in Table 2. As shown,
the domain-specific generators yielded perfor-
mance significantly above the generic ones and all
baselines. Even without domain-specific genera-
tors, our system outperformed the word-to-word
normalization approaches. Most notably, both
the generic and domain-specific systems outper-
formed the gold standard word-to-word normal-
izations. These results validate the hypothesis that
simple word-to-word normalization is insufficient
if the goal of normalization is to improve depen-
dency parsing; even if a system could produce
perfect word-to-word normalization, it would pro-
duce lower quality parses than those produced by
our approach.
2Our results and the reannotations of the Twitter and SMS
data are available at https://www.cs.washington.
edu/node/9091/
System Verb Subject-ObjectPre Rec F1 Pre Rec F1
w/oN 83.7 68.1 75.1 31.7 38.6 34.8
Google 88.9 78.8 83.5 36.1 46.3 40.6
w2wN 87.5 81.5 84.4 44.5 58.9 50.7
Gw2w 89.8 83.8 86.7 46.9 61.0 53.0
generic 91.7 88.9 90.3 53.6 70.2 60.8
domain specific 95.3 88.7 91.9 72.5 76.3 74.4
Table 2: Performance on Twitter dataset
5.2.2 SMS
To evaluate the performance on SMS data, we use
the Treasure My Text data collected by Choud-
hury et al (2007). As with the Twitter data, the
word-to-word normalizations were reannotated to
enforce grammaticality. As a replacement genera-
tor for SMS-specific substitutions, we used a map-
ping dictionary of SMS abbreviations.3 No further
SMS-specific development data was needed.
Table 3 gives the results on the SMS data. The
SMS dataset proved to be more difficult than the
Twitter dataset, with the overall performance of
every system being lower. While this drop of per-
formance may be a reflection of the difference in
data styles between SMS and Twitter, it is also
likely a product of the collection methodology.
The collection methodology of the Treasure My
Text dataset dictated that every message must have
at least one mistake, which may have resulted in a
dataset that was noisier than average.
Nonetheless, the trends on SMS data mirror
those on Twitter data, with the domain-specific
generators achieving the greatest overall perfor-
mance. However, while the generic setting still
manages to outperform most baselines, it did not
outperform the gold word-to-word normalization.
In fact, the gold word-to-word normalization was
much more competitive on this data, outperform-
ing even the domain-specific system on verbs
alone. This should not be seen as surprising, as
word-to-word normalization is most likely to be
beneficial for cases like this where the proportion
of non-standard tokens is high.
It should be noted that the SMS dataset as avail-
able has had all punctuation removed. While this
may be appropriate for word-to-word normaliza-
tion, this preprocessing may have an effect on the
parse of the sentence. As our system has the abil-
ity to add punctuation but our baseline systems do
not, this has the potential to artificially inflate our
results. To ensure a fair comparison, we manually
3http://www.netlingo.com/acronyms.php
1165
System Verb Subject-ObjectRec Pre F1 Rec Pre F1
w/oN 76.4 48.1 59.0 19.5 21.5 20.4
Google 85.1 61.6 71.5 22.4 26.2 24.1
w2wN 78.5 61.5 68.9 29.9 36.0 32.6
Gw2wN 87.6 76.6 81.8 38.0 50.6 43.4
generic 86.5 77.4 81.7 35.5 47.7 40.7
domain specific 88.1 75.0 81.0 41.0 49.5 44.8
Table 3: Performance on SMS dataset
System Verb Subject-ObjectPre Rec F1 Pre Rec F1
w/oN 98.5 97.1 97.8 69.2 66.1 67.6
Google 99.2 97.9 98.5 70.5 67.3 68.8
generic 98.9 97.4 98.1 71.3 67.9 69.6
domain specific 99.2 97.4 98.3 87.9 83.1 85.4
Table 4: Performance on call-center dataset
added punctuation to a randomly selected small
subset of the SMS data and reran each system.
This experiment suggested that, in contrast to the
hypothesis, adding punctuation actually improved
the results of the proposed system more substan-
tially than that of the baseline systems.
5.2.3 Call-Center
Although Twitter and SMS data are unmistakably
different, there are many similarities between the
two, such as the frequent use of shorthand word
forms that omit letters. The examination of call-
center logs allows us to examine the ability of our
system to perform normalization in more disparate
domains. Our call-center data consists of text-
based responses to questions about a user?s expe-
rience with a call-center (e.g., their overall satis-
faction with the service). We use call-center logs
from a major company, and collect about 150 re-
sponses for use in our evaluation. We collected
an additional small set of data to develop our call-
center-specific generators.
Results on the call-center dataset are in Table 4.
As shown, the raw call-center data was compar-
atively clean, resulting in higher baseline perfor-
mance than in other domains. Unlike on previ-
ous datasets, the use of generic mappings only
provided a small improvement over the baseline.
However, the use of domain-specific generators
once again led to significantly increased perfor-
mance on subjects and objects.
6 Discussion
The results presented in the previous section sug-
gest that domain transfer using the proposed nor-
malization framework is possible with only a
small amount of effort. The relatively modest
set of additional replacement generators included
in each data set alowed the domain-specific ap-
proaches to significantly outperform the generic
approach. In the call-center case, performance im-
provements could be seen by referencing a very
small amount of development data. In the SMS
case, the presence of a domain-specific dictionary
allowed for performance improvements without
the need for any development data at all. It is
likely, though not established, that employing fur-
ther development data would result in further per-
formance improvements. We leave further investi-
gation to future work.
The results in Section 5.2 establish a point that
has often been assumed but, to the best of our
knowledge, has never been explicitly shown: per-
forming normalization is indeed beneficial to de-
pendency parsing on informal text. The parse of
the normalized text was substantially better than
the parse of the original raw text in all domains,
with absolute performance increases ranging from
about 18-25% on subjects and objects. Further-
more, the results suggest that, as hypothesized,
preparing an informal text for a parsing task re-
quires more than simple word-to-word normaliza-
tion. The proposed approach significantly outper-
forms the state-of-the-art word-to-word normal-
ization approach. Perhaps most interestingly, the
proposed approach performs on par with, and in
several cases superior to, gold standard word-to-
word annotations. This result gives strong evi-
dence for the conclusion that parser-targeted nor-
malization requires a broader understanding of the
scope of the normalization task.
While the work presented here gives promis-
ing results, there are still many behaviors found
in informal text that prove challenging. One
such example is the word reordering seen in Fig-
ure 4. Although word reordering could be incor-
porated into the model as a combination of a dele-
tion and an insertion, the model as currently de-
vised cannot easily link these two replacements
to one another. Additionally, instances of re-
ordering proved hard to detect in practice. As
such, no reordering-based replacement generators
were implemented in the presented system. An-
other case that proved difficult was the insertion
of missing tokens. For instance, the informal
sentence ?Day 3 still don?t freaking
1166
feel good!:(? could be formally rendered
as ?It is day 3 and I still do not
feel good!?. Attempts to address missing to-
kens in the model resulted in frequent false pos-
itives. Similarly, punctuation insertion proved to
be challenging, often requiring a deep analysis
of the sentence. For example, contrast the sen-
tence ?I?m watching a movie I don?t
know its name.? which would benefit from
inserted punctuation, with ?I?m watching a
movie I don?t know.?, which would not.
We feel that the work presented here provides a
foundation for future work to more closely exam-
ine these challenges.
7 Conclusions
This work presents a framework for normalization
with an eye towards domain adaptation. The pro-
posed framework builds a statistical model over a
series of replacement generators. By doing so, it
allows a designer to quickly adapt a generic model
to a new domain with the inclusion of a small set of
domain-specific generators. Tests over three dif-
ferent domains suggest that, using this model, only
a small amount of domain-specific data is neces-
sary to tailor an approach towards a new domain.
Additionally, this work introduces a parser-
centric view of normalization, in which the per-
formance of the normalizer is directly tied to the
performance of a downstream dependency parser.
This evaluation metric allows for a deeper under-
standing of how certain normalization actions im-
pact the output of the parser. Using this met-
ric, this work established that, when dependency
parsing is the goal, typical word-to-word normal-
ization approaches are insufficient. By taking a
broader look at the normalization task, the ap-
proach presented here is able to outperform not
only state-of-the-art word-to-word normalization
approaches but also manual word-to-word annota-
tions.
Although the work presented here established
that more than word-to-word normalization was
necessary to produce parser-ready normalizations,
it remains unclear which specific normalization
tasks are most critical to parser performance. We
leave this interesting area of examination to future
work.
Acknowledgments
We thank the anonymous reviewers of ACL for
helpful comments and suggestions. We also thank
Ioana R. Stanoi for her comments on a prelim-
inary version of this work, Daniel S. Weld for
his support, and Alan Ritter, Monojit Choudhury,
Bo Han, and Fei Liu for sharing their tools and
data. The first author is partially supported by the
DARPA Machine Reading Program under AFRL
prime contract numbers FA8750-09-C-0181 and
FA8750-09-C-0179. Any opinions, findings, con-
clusions, or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect the views of DARPA, AFRL, or the
US government. This work is a part of IBM?s Sys-
temT project (Chiticariu et al, 2010).
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for sms text normal-
ization. In ACL, pages 33?40.
Zhuowei Bao, Benny Kimelfeld, and Yunyao Li. 2011.
A graph approach to spelling correction in domain-
centric search. In ACL, pages 905?914.
Richard Beaufort, Sophie Roekhaut, Louise-Ame?lie
Cougnon, and Ce?drick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normal-
izing sms messages. In ACL, pages 770?779.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Sriram Raghavan, Frederick Reiss, and Shivaku-
mar Vaithyanathan. 2010. SystemT: An algebraic
approach to declarative information extraction. In
ACL, pages 128?137.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure
of texting language. IJDAR, 10(3-4):157?174.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In EMNLP,
pages 1?8.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization. In
CALC, pages 71?78.
Mark Davies. 2008-. The corpus of contempo-
rary american english: 450 million words, 1990-
present. Avialable online at: http://corpus.
byu.edu/coca/.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twitter.
In ACL, pages 368?378.
1167
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
for microblogs. In EMNLP-CoNLL, pages 421?432.
Catherine Kobus, Franc?ois Yvon, and Ge?raldine
Damnati. 2008. Normalizing SMS: are two
metaphors better than one? In COLING, pages 441?
448.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In ICML, pages 282?289.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011. Insertion, deletion, or substitution? normal-
izing text messages without pre-categorization nor
supervision. In ACL, pages 71?76.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A
broad-coverage normalization system for social me-
dia language. In ACL, pages 1035?1044.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC-06, pages 449?454.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pages 311?
318.
Deana Pennell and Yang Liu. 2010. Normalization of
text messages for text-to-speech. In ICASSP, pages
4842?4845.
Deana Pennell and Yang Liu. 2011. A character-level
machine translation approach for normalization of
SMS abbreviations. In IJCNLP, pages 974?982.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in Tweets: An ex-
perimental study. In EMNLP, pages 1524?1534.
Richard Sproat, Alan W. Black, Stanley F. Chen,
Shankar Kumar, Mari Ostendorf, and Christopher
Richards. 2001. Normalization of non-standard
words. Computer Speech & Language, 15(3):287?
333.
Zhenzhen Xue, Dawei Yin, and Brian D. Davison.
2011. Normalizing microtext. In Analyzing Micro-
text, volume WS-11-05 of AAAI Workshops.
1168
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 804?809,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Automatic Term Ambiguity Detection
Tyler Baldwin Yunyao Li Bogdan Alexe Ioana R. Stanoi
IBM Research - Almaden
650 Harry Road, San Jose, CA 95120, USA
{tbaldwi,yunyaoli,balexe,irs}@us.ibm.com
Abstract
While the resolution of term ambiguity is
important for information extraction (IE)
systems, the cost of resolving each in-
stance of an entity can be prohibitively
expensive on large datasets. To combat
this, this work looks at ambiguity detec-
tion at the term, rather than the instance,
level. By making a judgment about the
general ambiguity of a term, a system is
able to handle ambiguous and unambigu-
ous cases differently, improving through-
put and quality. To address the term
ambiguity detection problem, we employ
a model that combines data from lan-
guage models, ontologies, and topic mod-
eling. Results over a dataset of entities
from four product domains show that the
proposed approach achieves significantly
above baseline F-measure of 0.96.
1 Introduction
Many words, phrases, and referring expressions
are semantically ambiguous. This phenomenon,
commonly referred to as polysemy, represents a
problem for NLP applications, many of which in-
herently assume a single sense. It can be particu-
larly problematic for information extraction (IE),
as IE systems often wish to extract information
about only one sense of polysemous terms. If
nothing is done to account for this polysemy, fre-
quent mentions of unrelated senses can drastically
harm performance.
Several NLP tasks, such as word sense disam-
biguation, word sense induction, and named en-
tity disambiguation, address this ambiguity prob-
lem to varying degrees. While the goals and initial
data assumptions vary between these tasks, all of
them attempt to map an instance of a term seen
in context to an individual sense. While making
a judgment for every instance may be appropri-
ate for small or medium sized data sets, the cost
of applying these ambiguity resolution procedures
becomes prohibitively expensive on large data sets
of tens to hundreds of million items. To combat
this, this work zooms out to examine the ambigu-
ity problem at a more general level.
To do so, we define an IE-centered ambiguity
detection problem, which ties the notion of am-
biguity to a given topical domain. For instance,
given that the terms Call of Juarez and A New
Beginning can both reference video games, we
would like to discover that only the latter case is
likely to appear frequently in non-video game con-
texts. The goal is to make a binary decision as
to whether, given a term and a domain, we can
expect every instance of that term to reference an
entity in that domain. By doing so, we segregate
ambiguous terms from their unambiguous coun-
terparts. Using this segregation allows ambiguous
and unambiguous instances to be treated differ-
ently while saving the processing time that might
normally be spent attempting to disambiguate in-
dividual instances of unambiguous terms.
Previous approaches to handling word ambigu-
ity employ a variety of disparate methods, vari-
ously relying on structured ontologies, gleaming
insight from general word usage patterns via lan-
guage models, or clustering the contexts in which
words appear. This work employs an ambiguity
detection pipeline that draws inspiration from all
of these methods to achieve high performance.
2 Term Ambiguity Detection (TAD)
A term can be ambiguous in many ways. It may
have non-referential senses in which it shares a
name with a common word or phrase, such as in
the films Brave and 2012. A term may have refer-
ential senses across topical domains, such as The
Girl with the Dragon Tattoo, which may reference
either the book or the film adaptation. Terms may
804
also be ambiguous within a topical domain. For
instance, the term Final Fantasy may refer to the
video game franchise or one of several individual
games within the franchise. In this work we con-
cern ourselves with the first two types of ambigu-
ity, as within topical domain ambiguity tends to
pose a less severe problem for IE systems.
IE systems are often asked to perform extrac-
tion over a dictionary of terms centered around
a single topic. For example, in brand manage-
ment, customers may give a list of product names
and ask for sentiment about each product. With
this use case in mind, we define the term ambigu-
ity detection (TAD) problem as follows: Given a
term and a corresponding topic domain, determine
whether the term uniquely references a member
of that topic domain. That is, given a term such
as Brave and a category such as film, the task is
make a binary decision as to whether all instances
of Brave reference a film by that name.
2.1 Framework
Our TAD framework is a hybrid approach consist-
ing of three modules (Figure 1). The first module
is primarily designed to detect non-referential am-
biguity. This module examines n-gram data from
a large text collection. Data from The Corpus of
Contemporary American English (Davies, 2008 )
was used to build our n-grams.
The rationale behind the n-gram module is
based on the understanding that terms appearing
in non-named entity contexts are likely to be non-
referential, and terms that can be non-referential
are ambiguous. Therefore, detecting terms that
have non-referential usages can also be used to
detect ambiguity. Since we wish for the ambigu-
ity detection determination to be fast, we develop
our method to make this judgment solely on the
n-gram probability, without the need to examine
each individual usage context. To do so, we as-
sume that an all lowercased version of the term is
a reasonable proxy for non-named entity usages in
formal text. After removing stopwords from the
term, we calculate the n-gram probability of the
lower-cased form of the remaining words. If the
probability is above a certain threshold, the term
is labeled as ambiguous. If the term is below the
threshold, it is tentatively labeled as unambiguous
and passed to the next module. To avoid making
judgments of ambiguity based on very infrequent
uses, the ambiguous-unambiguous determination
threshold is empirically determined by minimiz-
ing error over held out data.
The second module employs ontologies to de-
tect across domain ambiguity. Two ontologies
were examined. To further handle the common
phrase case, Wiktionary1 was used as a dictionary.
Terms that have multiple senses in Wiktionary
were labeled as ambiguous. The second ontology
used was Wikipedia disambiguation pages. All
terms that had a disambiguation page were marked
as ambiguous.
The final module attempts to detect both non-
referential and across domain ambiguity by clus-
tering the contexts in which words appear. To do
so, we utilized the popular Latent Dirichlet Allo-
cation (LDA (Blei et al, 2003)) topic modeling
method. LDA represents a document as a distri-
bution of topics, and each topic as a distribution
of words. As our domain of interest is Twitter,
we performed clustering over a large collection of
tweets. For a given term, all tweets that contained
the term were used as a document collection. Fol-
lowing standard procedure, stopwords and infre-
quent words were removed before topic modeling
was performed. Since the clustering mechanism
was designed to make predictions over the already
filtered data of the other modules, it adopts a con-
servative approach to predicting ambiguity. If the
category term (e.g., film) or a synonym from the
WordNet synset does not appear in the 10 most
heavily weighted words for any cluster, the term is
marked as ambiguous.
A term is labeled as ambiguous if any one of
the three modules predicts that it is ambiguous,
but only labeled as unambiguous if all three mod-
ules make this prediction. This design allows each
module to be relatively conservative in predicting
ambiguity, keeping precision of ambiguity predic-
tion high, under the assumption that other modules
will compensate for the corresponding drop in re-
call.
3 Experimental Evaluation
3.1 Data Set
Initial Term Sets We collected a data set of terms
from four topical domains: books, films, video
games, and cameras. Terms for the first three do-
mains are lists of books, films, and video games
respectively from the years 2000-2011 from db-
pedia (Auer et al, 2007), while the initial terms
1http://www.wiktionary.org/
805
Tweet Term Category Judgment
Woke up from a nap to find a beautiful mind on. #win A Beautiful Mind film yes
I Love Tyler Perry ; He Has A Beautiful Mind. A Beautiful Mind film no
I might put it in the top 1. RT @CourtesyFlushMo Splice. Top 5 worst movies ever Splice film yes
Splice is a great, free replacement to iMove for your iPhone, Splice film no
Table 1: Example tweet annotations.
Figure 1: Overview of the ambiguity detection
framework.
for cameras includes all the cameras from the six
most popular brands on flickr2.
Gold Standard A set of 100 terms per domain
were chosen at random from the initial term sets.
Rather than annotating each term directly, am-
biguity was determined by examining actual us-
age. Specifically, for each term, usage examples
were extracted from large amounts of Twitter data.
Tweets for the video game and film categories were
extracted from the TREC Twitter corpus.3 The
less common book and camera cases were ex-
tracted from a subset of all tweets from September
1st-9th, 2012.
For each term, two annotators were given the
term, the corresponding topic domain, and 10 ran-
domly selected tweets containing the term. They
were then asked to make a binary judgment as to
whether the usage of the term in the tweet referred
to an instance of the given category. The degree
of ambiguity is then determined by calculating the
percentage of tweets that did not reference a mem-
ber of the topic domain. Some example judgments
are given in Table 1. If all individual tweet judg-
ments for a term were marked as referring to a
2http://www.flickr.com/cameras/
3http://trec.nist.gov/data/tweets/
Configuration Precision Recall F-measure
Baseline 0.675 1.0 0.806
NG 0.979 0.848 0.909
ON 0.979 0.704 0.819
CL 0.946 0.848 0.895
NG + ON 0.980 0.919 0.948
NG + CL 0.942 0.963 0.952
ON + CL 0.945 0.956 0.950
NG + ON + CL 0.943 0.978 0.960
Table 2: Performance of various framework con-
figurations on the test data.
member of the topic domain, the term was marked
as fully unambiguous within the data examined.
All other cases were considered ambiguous.4
Inter-annotator agreement was high, with raw
agreement of 94% (? = 0.81). Most disagree-
ments on individual tweet judgments had little ef-
fect on the final judgment of a term as ambiguous
or unambiguous, and those that did were resolved
internally.
3.2 Evaluation and Results
Effectiveness To understand the contribution of
the n-gram (NG), ontology (ON), and clustering
(CL) based modules, we ran each separately, as
well as every possible combination. Results are
shown in Table 2, where they are compared to a
majority class (ambiguous) baseline.
As shown, all configurations outperform the
baseline. Of the three individual modules, the n-
gram and clustering methods achieve F-measure
of around 0.9, while the ontology-based module
performs only modestly above baseline. Unsur-
prisingly, the ontology method is affected heav-
ily by its coverage, so its poor performance is pri-
marily attributable to low recall. As noted, many
IE tasks may involve sets of entities that are not
found in common ontologies, limiting the ability
of the ontology-based method alone. Additionally,
ontologies may be apt to list cases of strict ambi-
guity, rather than practical ambiguity. That is, an
ontology may list a term as ambiguous if there are
4The annotated data is available at http:
//researcher.watson.ibm.com/researcher/
view_person_subpage.php?id=4757.
806
several potential named entities it could refer to,
even if the vast majority of references were to only
a single entity.
Combining any two methods produced substan-
tial performance increases over any of the individ-
ual runs. The final system that employed all mod-
ules produced an F-measure of 0.960, a significant
(p < 0.01) absolute increase of 15.4% over the
baseline.
Usefulness To establish that term ambiguity de-
tection is actually helpful for IE, we conducted
a preliminary study by integrating our pipeline
into a commercially available rule-based IE sys-
tem (Chiticariu et al, 2010; Alexe et al, 2012).
The system takes a list of product names as input
and outputs tweets associated with each product.
It utilizes rules that employ more conservative ex-
traction for ambiguous entities.
Experiments were conducted over several mil-
lion tweets using the terms from the video game
and camera domains. When no ambiguity detec-
tion was performed, all terms were treated as un-
ambiguous. The system produced very poor pre-
cision of 0.16 when no ambiguity detection was
used, due to the extraction of irrelevant instances
of ambiguous objects. In contrast, the system pro-
duced precision of 0.96 when ambiguity detection
was employed. However, the inclusion of disam-
biguation did reduce the overall recall; the system
that employed disambiguation returned only about
57% of the true positives returned by the system
that did not employ disambiguation. Although
this reduction in recall is significant, the overall
impact of disambiguation is clearly positive, due
to the stark difference in precision. Nonetheless,
this limited study suggests that there is substantial
room for improvement in the extraction system, al-
though this is out of the scope of the current work.
4 Related Work
Polysemy is a known problem for many NLP-
related applications. Machine translation systems
can suffer, as ambiguity in the source language
may lead to incorrect translations, and unambigu-
ous sentences in one language may become am-
biguous in another (Carpuat and Wu, 2007; Chan
et al, 2007). Ambiguity in queries can also hin-
der the performance of information retrieval sys-
tems (Wang and Agichtein, 2010; Zhong and Ng,
2012).
The ambiguity detection problem is similar to
the well studied problems of named entity dis-
ambiguation (NED) and word sense disambigua-
tion (WSD). However, these tasks assume that
the number of senses a word has is given, essen-
tially assuming that the ambiguity detection prob-
lem has already been solved. This makes these
tasks inapplicable in many IE instances where the
amount of ambiguity is not known ahead of time.
Both named entity and word sense disambigua-
tion are extensively studied, and surveys on each
are available (Nadeau and Sekine, 2007; Navigli,
2009).
Another task that shares similarities with TAD
is word sense induction (WSI). Like NED and
WSD, WSI frames the ambiguity problem as one
of determining the sense of each individual in-
stance, rather than the term as a whole. Unlike
those approaches, the word sense induction task
attempts to both figure out the number of senses a
word has, and what they are. WSI is unsupervised,
relying solely on the information that surrounds
word mentions in the text.
Many different clustering-based WSI methods
have been examined. Pantel and Lin (2002) em-
ploy a clustering by committee method that itera-
tively adds words to clusters based on their sim-
ilarities. Topic model-based methods have been
attempted using variations of Latent Dirichlet Al-
location (Brody and Lapata, 2009) and Hierarchi-
cal Dirichlet Processes (Lau et al, 2012). Sev-
eral graph-based methods have also been exam-
ined (Klapaftis and Manandhar, 2010; Navigli and
Crisafulli, 2010). Although the words that sur-
round the target word are the primary source of
contextual information in most cases, additional
feature sources such as syntax (Van de Cruys,
2008) and semantic relations (Chen and Palmer,
2004) have also been explored.
5 Conclusion
This paper introduced the term ambiguity detec-
tion task, which detects whether a term is am-
biguous relative to a topical domain. Unlike other
ambiguity resolution tasks, the ambiguity detec-
tion problem makes general ambiguity judgments
about terms, rather than resolving individual in-
stances. By doing so, it eliminates the need for
ambiguity resolution on unambiguous objects, al-
lowing for increased throughput of IE systems on
large data sets.
Our solution for the term ambiguity detection
807
task is based on a combined model with three dis-
tinct modules based on n-grams, ontologies, and
clustering. Our initial study suggests that the com-
bination of different modules designed for differ-
ent types of ambiguity used in our solution is ef-
fective in determining whether a term is ambigu-
ous for a given domain. Additionally, an exami-
nation of a typical use case confirms that the pro-
posed solution is likely to be useful in improving
the performance of an IE system that does not em-
ploy any disambiguation.
Although the task as presented here was mo-
tivated with information extraction in mind, it is
possible that term ambiguity detection could be
useful for other tasks. For instance, TAD could
be used to aid word sense induction more gener-
ally, or could be applied as part of other tasks such
as coreference resolution. We leave this avenue of
examination to future work.
Acknowledgments
We would like to thank the anonymous review-
ers of ACL for helpful comments and suggestions.
We also thank Howard Ho and Rajasekar Krishna-
murthy for help with data annotation and Shivaku-
mar Vaithyanathan for his comments on a prelim-
inary version of this work.
References
Bogdan Alexe, Mauricio A. Herna?ndez, Kirsten Hil-
drum, Rajasekar Krishnamurthy, Georgia Koutrika,
Meenakshi Nagarajan, Haggai Roitman, Michal
Shmueli-Scheuer, Ioana Roxana Stanoi, Chitra
Venkatramani, and Rohit Wagle. 2012. Surfacing
time-critical insights from social media. In SIG-
MOD Conference, pages 657?660.
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: a nucleus for a web of open data.
In Proceedings of the 6th international The seman-
tic web and 2nd Asian conference on Asian semantic
web conference, ISWC?07/ASWC?07, pages 722?
735, Berlin, Heidelberg. Springer-Verlag.
David Blei, Andrew Ng, and Micheal I. Jordan. 2003.
Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022, January.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In Proceedings of the 12th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, EACL ?09,
pages 103?111, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61?72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves statisti-
cal machine translation. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 33?40, Prague, Czech Republic,
June. Association for Computational Linguistics.
Jinying Chen and Martha Palmer. 2004. Chinese verb
sense discrimination using an em clustering model
with rich linguistic features. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, ACL ?04, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Sriram Raghavan, Frederick Reiss, and Shivaku-
mar Vaithyanathan. 2010. SystemT: An algebraic
approach to declarative information extraction. In
ACL, pages 128?137.
Mark Davies. 2008-. The corpus of contempo-
rary american english: 450 million words, 1990-
present. Avialable online at: http://corpus.
byu.edu/coca/.
Ioannis P. Klapaftis and Suresh Manandhar. 2010.
Word sense induction & disambiguation using hi-
erarchical random graphs. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?10, pages 745?755,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense in-
duction for novel sense detection. In Proceedings of
the 13th Conference of the European Chapter of the
Association for Computational Linguistics, EACL
?12, pages 591?601, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
David Nadeau and Satoshi Sekine. 2007. A survey
of named entity recognition and classification. Lin-
guisticae Investigationes, 30(1):3?26.
Roberto Navigli and Giuseppe Crisafulli. 2010. Induc-
ing word senses to improve web search result clus-
tering. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?10, pages 116?126, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Roberto Navigli. 2009. Word sense disambiguation:
A survey. ACM Comput. Surv., 41(2):10:1?10:69,
February.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of the eighth
808
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?02, pages
613?619, New York, NY, USA. ACM.
Tim Van de Cruys. 2008. Using three way data for
word sense discrimination. In Proceedings of the
22nd International Conference on Computational
Linguistics - Volume 1, COLING ?08, pages 929?
936, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Yu Wang and Eugene Agichtein. 2010. Query ambigu-
ity revisited: Clickthrough measures for distinguish-
ing informational and ambiguous queries. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 361?364,
Los Angeles, California, June. Association for Com-
putational Linguistics.
Zhi Zhong and Hwee Tou Ng. 2012. Word sense
disambiguation improves information retrieval. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 273?282, Jeju Island, Korea,
July. Association for Computational Linguistics.
809
