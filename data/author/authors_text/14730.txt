Proceedings of the 12th Conference of the European Chapter of the ACL, pages 817?825,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Learning Efficient Parsing
Gertjan van Noord
University of Groningen
G.J.M.van.noord@rug.nl
Abstract
A corpus-based technique is described to
improve the efficiency of wide-coverage
high-accuracy parsers. By keeping track
of the derivation steps which lead to the
best parse for a very large collection of
sentences, the parser learns which parse
steps can be filtered without significant
loss in parsing accuracy, but with an im-
portant increase in parsing efficiency. An
interesting characteristic of our approach
is that it is self-learning, in the sense that
it uses unannotated corpora.
1 Introduction
We consider wide-coverage high-accuracy pars-
ing systems such as Alpino, a parser for Dutch
which contains a grammar based on HPSG and
a maximum entropy disambiguation component
trained on a treebank. Even if such parsing sys-
tems now obtain satisfactory accuracy for a vari-
ety of text types, a drawback concerns the compu-
tational properties of such parsers: they typically
require lots of memory and are often very slow for
longer and very ambiguous sentences.
We present a very simple, fairly general,
corpus-based method to improve upon the prac-
tical efficiency of such parsers. We use the accu-
rate, slow, parser to parse many (unannotated) in-
put sentences. For each sentence, we keep track of
sequences of derivation steps that were required to
find the best parse of that sentence (i.e., the parse
that obtained the best score, highest probability,
according to the parser itself).
Given a large set of successful derivation step
sequences, we experimented with a variety of
simple heuristics to filter unpromising derivation
steps. A heuristic that works remarkably well
simply states that for a new input sentence, the
parser can only consider derivation step sequences
in which any sub-sequence of length N has been
observed at least once in the training data. Exper-
imental results are provided for various heuristics
and amounts of training data.
It is hard to compare fast, accurate, parsers with
slow, slightly more accurate parsers. In section 3
we propose both an on-line and an off-line appli-
cation scenario, introducing a time-out per sen-
tence, which leads to metrics for choosing be-
tween parser variants.
In the experimental part we show that, in an on-
line scenario, the most successful heuristic leads
to a parser that is more accurate than the baseline
system, except for unrealistic time-outs per sen-
tence of more than 15 minutes. Furthermore, we
show that, in an off-line scenario, the most suc-
cessful heuristic leads to a parser that is more than
four times faster than the base-line variant with the
same accuracy.
2 Background: the Alpino parser for
Dutch
The experiments are performed using the Alpino
parser for Dutch. The Alpino system is a linguis-
tically motivated, wide-coverage grammar and
parser for Dutch in the tradition of HPSG. It con-
sists of about 800 grammar rules and a large lexi-
con of over 300,000 lexemes and various rules to
recognize special constructs such as named enti-
ties, temporal expressions, etc. Heuristics have
been implemented to deal with unknown words
and word sequences. Based on the categories as-
signed to words, and the set of grammar rules
compiled from the HPSG grammar, a left-corner
parser finds the set of all parses, and stores this set
compactly in a packed parse forest. In order to se-
lect the best parse from the parse forest, a best-first
search algorithm is applied. The algorithm con-
sults a Maximum Entropy disambiguation model
to judge the quality of (partial) parses.
Although Alpino is not a dependency grammar
817
in the traditional sense, dependency structures are
generated by the lexicon and grammar rules as the
value of a dedicated attribute. The dependency
structures are based on CGN (Corpus Gesproken
Nederlands, Corpus of Spoken Dutch) (Hoekstra
et al, 2003), D-Coi and LASSY (van Noord et al,
2006).
3 Methodology: balancing efficiency and
accuracy
3.1 On-line and off-line parsing scenarios
We focus on the speed of parsing, ignoring other
computational properties such as memory usage.
Problems with respect to parsing are twofold: on
the one hand, parsing simply is too slow for many
input sentences. On the other hand, the rela-
tion between input sentence and expected speed
of parsing is typically unknown. For simple pars-
ing systems based on finite-state, context-free or
mildly context-sensitive grammars, it is possible
to establish an upper-bound of required CPU-time
based on the length of an input sentence. For the
very powerful constraint-based formalisms con-
sidered here, such upper-bounds are not avail-
able. In practice, shorter sentences typically can
be parsed fairly quickly, whereas longer sentences
sometimes can take a very very long time indeed.
As a consequence, measures such as number of
words parsed per minute, or mean parsing time per
sentence are somewhat meaningless. We therefore
introduce two slightly different scenarios which
include a time-out per sentence.
On-line scenario. In some applications, a parser
is applied on-line: an actual user is waiting for the
response of the system, and if the parser required
minutes of CPU-time, the application would not
be successful. In such a scenario, we assume that
it is possible to determine a maximum amount of
CPU-time (a time-out) per sentence, depending on
other factors such as speed of the other system
components, expected patience of users, etc. If
the parser does not finish before the time-out, it is
assumed to have not produced anything. In depen-
dency parsing, the parser produces the empty set
of dependencies in such cases, and hence such an
event has an important negative effect on the ac-
curacy of the system. By studying the relation be-
tween different time-outs and accuracy, it is possi-
ble to choose the most effective parser variant for
a particular application.
Off-line scenario. For other applications, an
off-line parsing scenario might be more appropri-
ate. For instance, if we build a question answering
system for a medical encyclopedia, and we wish to
parse all sentences of that encyclopedia once and
for all, then we are not interested in the amount of
CPU-time the parser spends on a single sentence,
but we want to know how much time it will cost to
parse everything.
In such a scenario, it often still is very useful to
set a time-out for each sentence, but in this case the
time-out can be expected to be (much) higher than
in the on-line scenario. In this scenario, we pro-
pose to study the relation between mean CPU-time
and accuracy ? for various settings of the time-
out parameter. This allows us to determine, for
instance, the mean CPU-time requirements for a
given target accuracy level?
3.2 Accuracy: comparing sets of
dependencies
Let Dip be the number of dependencies produced
by the parser for sentence i, Dig is the number of
dependencies in the treebank parse, and Dio is the
number of correct dependencies produced by the
parser. If no superscript is used, we aggregate over
all sentences of the test set, i.e.,:
Dp =
?
i
Dip Do =
?
i
Dio Dg =
?
i
Dig
We define precision (P = Do/Dp), (R =
Do/Dg) and f-score: 2P ? R/(P + R).
An alternative similarity score is based on the
observation that for a given sentence of n words,
a parser would be expected to return (about) n de-
pendencies. In such cases, we can simply use the
percentage of correct dependencies as a measure
of accuracy. To allow for some discrepancies be-
tween the number of expected and returned depen-
dencies, we divide by the maximum (per sentence)
of both. This leads to the following definition of
named dependency accuracy.
Acc =
Do
?
i max(Dig, Dip)
If time-outs are introduced, the difference be-
tween f-score and accuracy becomes important.
Consider the example in table 1. Here, the parser
produces reasonable results for the first three,
short, sentences, but for the final, long, sentence
no result is produced because of a time-out.
818
i Dio D
i
p D
i
g prec rec f-sc Acc
1 8 10 11 80 73 76 73
2 8 11 10 76 76 76 73
3 8 9 9 80 80 80 77
4 0 0 30 80 40 53 39
Table 1: Hypothetical result of parser on a test set
of four sentences. The columns labeled precision,
recall, f-score and accuracy represent aggregates
over sentences 1 . . . i.
The precision, recall and f-score after the first
three sentences is 80%. After the ? much longer
? fourth sentence, recall drops considerably, but
precision remains the same. As a consequence,
the f-score is quite a bit higher than 40%: it is over
53%. The accuracy score after three sentences is
77%. Including the fourth sentence leads to a drop
in accuracy to 39%.
As this example illustrates, the f-score metric is
less sensitive to parse failures than the accuracy
score. Also, it appears that the accuracy score is
a much better characterization of the success of
this parser: after all, the parser only got 24 cor-
rect dependencies out of 60 expected dependen-
cies. The f-score measure, on the other hand, can
easily be misunderstood to suggest that the parser
does a good job for more than 50%.
4 Learning Efficient Parsing
In this section a method is defined for filtering
derivation step sequences, based on previous expe-
rience of the parser. In a training phase, the parser
is fed with thousands of sentences. For each sen-
tence it finds the best parse, and it stores the rel-
evant sequences of derivation steps, that were re-
quired to find that best parse. After the training
phase, the parser filters those sequences of deriva-
tion steps that are unlikely to be useful. By fil-
tering out unlikely derivation step sequences, effi-
ciency is expected to improve. Since certain parses
now become impossible, a drop in accuracy is ex-
pected as well.
Although the idea of filtering derivation step
sequences based on previous experience is fairly
general, we define the method in more detail with
respect to an actual parsing algorithm: the left-
corner parser along the lines of Matsumoto et al
(1983), Pereira and Shieber (1987, section 6.5)
and van Noord (1997).
4.1 Left-corner parsing
A left-corner parser is a bottom-up parser with
top-down guidance, which is most easily ex-
plained as a non-deterministic search procedure.
A specification of the left-corner algorithm can
be provided in DCG as in figure 2 (Pereira and
Shieber, 1987, section 6.5), where the filter/2
goals should be ignored for the moment. Here,
we assume that dictionary look-up is performed
by the word/3 predicate, with the first argument
a given word, and the second argument its cate-
gory; and that rules are accessible via the predi-
cate rule/3, where the first argument represents
the mother category, and the second argument is
the possibly empty list of daughter categories. The
third argument of both the word/3 and rule/3
predicates are identifiers we need later.
In order to analyze a given sentence as an in-
stance of the top category, we look up the first
word of the string, and show that this lexical cat-
egory is a left-corner of the goal category. To
show that a given category is a left-corner of a
given goal category, a rule is selected. The left-
most daughter node of that rule is identified with
the left-corner. The other daughters of the rule are
parsed recursively. If this succeeds, it remains to
show that the mother node of the rule is a left-
corner of the goal category. The recursion stops
if a left-corner category can be identified with the
goal category.
This simple algorithm is improved and extended
in a variety of ways, as in Matsumoto et al (1983)
and van Noord (1997), to make it efficient and
practical. The extensions include a memoization
of the parse/1 predicate and the construction of a
shared parse forest (a compact representation of
all parses).
4.2 Left-corner splines
For the left-corner parser, the derivation step
sequences that are of interested are left-corner
splines. Such a spline consists of a goal category,
and the rules and lexical entries which were used
in the left-corner, in the order from the top to the
bottom.
A spline consists of a goal category, followed
by a sequence of derivation step names. A deriva-
tion step name is typically a rule identifier, but it
can also be a lexical type, indicating the lexical
category of a word that is the left-corner. A spe-
cial derivation step name is the reserved symbol
819
top
top cat
max xp(np)
np det n
det(de)
de
n
n n rel
noun(de,both,sg)
wijn
rel
rel arg(np)
rel pron(de,no obl)
die
vp
vp vpx
vpx vproj
vp arg v(np)
np pn
pn(sg,PER)
Elvis
vproj
vproj vc
vc v
verb(past(sg),transitive)
dronk
(top,[finish,top_cat,max_xp(np),np_det_n,det(de)]).
(n,[finish,n_n_rel,noun(de,both,sg)]).
(rel,[finish,rel_arg(np),rel_pron(de,no_obl)]).
(vp,[finish,vp_vpx,vpx_vproj,vp_arg_v(np),np_pn,pn(sg,PER),]).
(vproj,[finish,vproj_vc,vc_v,verb(past(sg),transitive)]).
Figure 1: Annotated derivation tree of the sentence
De wijn die Elvis dronk (The wine which Elvis
drank).
finish which is used to indicate that the cur-
rent category is identified with the goal category
(and no further rules are applied). A spline is writ-
ten (g, rn . . . r1) for goal category g and deriva-
tion step names r1 . . . rn. (g, ri . . . r1) is a partial
spline of (g, rn . . . ri . . . r1).
Consider the annotated derivation tree for the
sentence De wijn die Elvis dronk (The wine which
Elvis drank) in figure 1. Boxed leaf nodes con-
tain the lexical category as well as the corre-
sponding word. Boxed non-leaf nodes contain the
goal category (italic) and the rule-name. Non-
boxed non-leaf nodes only list the rule name. The
first left-corner spline consists of the goal cate-
gory top and the identifiers finish, top cat,
max xp(np), np det n, and the lexical type
det(de). All five left-corner splines of the ex-
ample are listed at the bottom of figure 1.
Left-corner splines of best parses of a large set
of sentences constitute the training data for the
parse(Phrase) -->
leaf(SubPhrase,Id),
{ filter(Phrase,[Id]) },
lc(SubPhrase,Phrase,[Id]).
leaf(Cat,Id) -->
[Word], { word(Word,Cat,Id) }.
leaf(Cat,Id) --> { rule(Cat,[],Id) }.
lc(Phrase,Phrase,Spline) -->
{ filter(Phrase,[finish|Spline]) }.
lc(SubPhrase,SuperPhrase,Spline) -->
rule(Phrase,[SubPhrase|Rest],Id),
{ filter(SuperPhrase,[Id|Spline]) },
parse_rest(Rest),
lc(Phrase,SuperPhrase,[Id|Spline]).
Figure 2: DCG Specification of a non-
deterministic left-corner parser, including spline
filtering.
techniques we develop to learn to parse new sen-
tences more efficiently.
4.3 Filtering left-corner splines
The left-corner parser builds left-corner splines
one step at the time. For a given goal, it first se-
lects a potential left-corner, and then continues ap-
plying rules from the bottom to the top until the
left-corner is identified with the goal category. At
every step where the algorithm attempts to extend
a left-corner spline, we now introduce a filter. The
purpose of this filter is to consider only those par-
tial left-corner splines that look promising - based
on the parser?s previous experience on the train-
ing data. The specification of the left-corner parser
given in figure 2 includes calls to this filter.
The purpose of the filter is, that at any time
the parser considers extending a left-corner spline
(g, ri?1 . . . r1) to (g, ri . . . r1), such an extension
only is allowed in promising cases. Obviously,
there are many ways such a filter could be defined.
We identify the following dimensions:
Context size. A filter for (g, ri . . . r1) will typ-
ically ignore at least some of the derivation step
names from the context. We experiment with fil-
ters which take into consideration g, ri, ri?1 (bi-
gram filter); g, ri, ri?1, ri?2 (trigram filter); and
g, ri, ri?1, ri?2, ri?3 (fourgram filter). A further
filter, labeled prefix filter, takes the full history into
account: g, ri . . . r1. The prefix filter thus ensures
that the parser only considers left-corner splines
that are partial splines of splines observed in the
training data.
820
Required evidence. For the various filters, what
kind of evidence from the training data do we re-
quire in order for the filter to accept this particular
derivation step? In initial experiments, we used
relative frequencies. For instance, the trigram fil-
ter would allow any tuple g, ri?2, ri?1, ri for some
constant threshold ? , provided:
C(g, . . . riri?1ri?2 . . .)
C(g, . . . ri?1ri?2 . . .)
> ?
However, we found that filters are more effective
(and require much less space ? see below), which
simply require that every step has been observed
often enough in the training data:
C(g, . . . riri?1ri?2 . . .) > ?
In particular, the case where ? = 0 gave surpris-
ingly good results.
4.4 Comparison with link table
The filter we developed is reminiscent of the link
predicate of (Pereira and Shieber, 1987). An im-
portant difference with the filter developed here
is that the link predicate removes derivation steps
which cannot lead to a successful parse (by an off-
line global analysis of the grammar), whereas we
filter out derivation steps which can lead to a full
parse, but which are not expected to lead to a best
parse. In our implementation, a variant of the link
predicate is used as well.
4.5 Implementation detail
The definition of the filter predicate depends on
our choices with respect to the dimensions identi-
fied above. For instance, if we chose the trigram
filter as our context size, then the training data can
be preprocessed in order to store all goal-trigram-
pairs with frequency above the threshold ? . Dur-
ing parsing, if the filter is given the partial spline
(g, riri?1ri?2 . . .), then a simple table look-up for
the tuple (g, ri?2ri?1ri) is sufficient (this suffices,
because each of the preceding trigrams will have
been checked earlier). In general, the filter pred-
icate needs access to a table containing a pair of
goal category and context, where the context con-
sists of sequences of derivation step names. The
table contains items for those pairs that occurred
with frequency > ? in the training data.
To access such tables efficiently, an obvious
choice is to use a hash table. The additional stor-
age requirements for such a hash table are consid-
erable. For instance, for the prefix filter four years
of newspaper text lead to a table with 941,723 en-
tries - stored as text the data takes 103Mb. To save
space, we experimented with a set-up in which
only the hash keys are stored, but the original in-
formation that the hash key was computed from, is
removed. During parsing, in order to check that a
given tuple is allowable, we compute its hash key,
and check if the hash key is in the table. If so,
the computation continues. The drawback of this
method is, that in the case a hash collision would
have occurred in an ordinary hash table, we now
simply assume that the input tuple was in the ta-
ble. In other words: the filter is potentially too
permissive in such cases. In actual practice, we did
not observe a difference with respect to accuracy
or CPU-time requirements, but the storage costs
dropped considerably.
5 Experimental Results
Some of the experiments have been performed
with the Alpino Treebank. The Alpino Treebank
(van der Beek et al, 2002) consists of manu-
ally verified dependency structures for the cdbl
(newspaper) part of the Eindhoven corpus (den
Boogaart, 1975). The treebank contains 7137 sen-
tences. Average sentence length is about 20 to-
kens.
Some further experiments are performed on the
basis of the D-Coi corpus (van Noord et al, 2006).
From this corpus, we used the manually veri-
fied syntactic annotations of the P-P-H and P-P-
L parts. The P-P-H part consists of over 2200
sentences from the Dutch daily newspaper Trouw
from 2001. Average sentence length is about 16.5
tokens. The P-P-L part contains 1115 sentences
taken from information brochures of Dutch Min-
istries. Average sentence length is about 18.5 to-
kens.
For training data, we used newspaper text from
the TwNC (Twente Newspaper) corpus (Ordelman
et al, 2007). We used Volkskrant 2001, NRC
2000, Algemeen Dagblad 1999. In addition, we
used Volkskrant 1997 newspaper data extracted
from the Volkskrant 1997 CDROM.
5.1 Results on Alpino Treebank
Figure 3 presents results obtained on the Alpino
Treebank. In the graphs, the various filters are
compared with the baseline variant of the parser.
Each of the filters outperforms the default model
for all given time-out values. In fact, the base-
821
1 5 10 50 500
20
40
60
80
timeout (sec)
accu
racy (%
CA)
bigramtrigramfourgramprefixbaseline
5 10 15 20 25
20
40
60
80
mean cputime (sec)
accu
racy (%
CA)
bigramtrigramfourgramprefixbaseline
Figure 3: Accuracy versus time-out (on-line scenario), and accuracy versus mean CPU-time (off-line
scenario) for various time-outs. The graphs compare the default setting of Alpino with the effect of the
various filters based on all available training data. Evaluation on the Alpino treebank.
line parser improves upon the prefix filter only for
unrealistic time-outs larger than fifteen minutes of
CPU-time. The difference in accuracy for a given
time-out value can be considerable: as much as
12% for time-outs around 30 seconds of CPU-
time.
If we focus on mean CPU-time (off-line sce-
nario), differences are even more pronounced.
Without the filter, an accuracy of about 63% is ob-
tained for a mean CPU-time of 6 seconds. The pre-
fix filtering method obtains accuracy of more than
86% for the same mean CPU-time. For that level
of accuracy, the base-line model requires a mean
CPU-time of about 25 seconds. In other words, for
the same level of accuracy, the prefix filter leads to
a parser that is more than four times faster.
5.2 Effect of the amount of training data
In the first two graphs of figure 4 we observe the
effect of the amount of training data. As can be ex-
pected, increasing the amount of data increases the
accuracy, and decreases efficiency (because more
derivation steps have been observed, hence fewer
derivations are filtered out). Generally, models
that take into account larger parts of the history re-
quire more data to obtain good accuracy, but they
are also faster. For each of the variants, adding
more training data after about 40 million words
does not lead to much further improvement; the
little improvement that is observed, is balanced by
a slight increase in parse times too.
It is interesting to note that the accuracy of some
of the filters improves slightly upon the baseline
parser (without any filtering). This can be ex-
plained by the fact that the Alpino parser includes
a best-first beam search to select the best parse
from the parse forest. Apparently, in some cases
the filter throws away candidate parses which
would otherwise confuse this heuristic best search
procedure.
5.3 Experiment with D-Coi data
In this section, we confirm the experimental re-
sults obtained on the Alpino Treebank by perform-
ing similar experiments on the D-Coi data. The
purpose of this confirmation is twofold. On the
one hand, the Alpino Treebank might not be a
reliable test set for the Alpino parser, because it
has been used quite intensively during the devel-
opment of various components of the system. On
the other hand, we might regard the experiments in
the previous section as development experiments
from which we learn the best parameters of the
approach. The real evaluation of the technique is
now performed using only the best method found
on the development set, which is the prefix filter
with ? = 0.
We performed experiments with two parts of the
D-Coi corpus. The first data set, P-P-H, contains
newspaper data, and is therefore comparable both
822
with the Alpino Treebank, and more importantly,
with the training data that we used to develop the
filters. In order to check if the success of the fil-
tering methods requires that training data and test
data need to be taken from similar texts, we also
provide experimental results on a test set consist-
ing of different material: the P-P-L part of the
D-Coi corpus, which contains text extracted from
information brochures published by Dutch Min-
istries.
The third and fourth graphs in figure 4 provide
results obtained on the P-P-H corpus. The in-
creased efficiency of the prefix filter is slightly less
pronounced. This may be due to the smaller mean
sentence length of this data set. Still, the prefix fil-
tering method performs much better for a large va-
riety of time-outs. Only for very high, unrealistic,
time-outs, the baseline parser obtains better accu-
racy. The same general trend is observed in the
P-P-L data-set. From these results we tentatively
conclude that the proposed technique is applicable
across text types and domains.
6 Discussion
One may wonder how the technique introduced in
this paper relates to techniques in which the dis-
ambiguation model is used directly during parsing
to eliminate unlikely partial parses. An example
in the context of wide coverage unification-based
parsing is the beam thresholding technique em-
ployed in the Enju HPSG parser for English (Tsu-
ruoka et al, 2004; Ninomiya et al, 2005).
In a beam-search parser, unlikely partial analy-
ses are constructed, and then - based on the proba-
bility assigned to these partial analyses - removed
from further consideration. One potential advan-
tage of the use of our filters may be, that many of
these partial analyses will not even be constructed
in the first place, and therefore no time is spent on
these alternatives at all.
We have not performed a detailed comparison,
because the statistical model employed in Alpino
contains some features which refer to arbitrary
large parts of a parse. Such non-local features are
not allowed in the Enju approach.
A parsing system may also combine both types
of techniques. In that case there is room for
further experimentation. For instance, during
the learning phase, it may be beneficial to allow
for a wider beam, to obtain more reliable filters.
During testing, the beam can perhaps be smaller
than usual, since the filters already rule out many
of the competing parses.
The idea that corpora can be used to improve
parsing efficiency was an important ingredient of
a technique that was called grammar specializa-
tion. An overview of grammar specialization tech-
niques is given in (Sima?an, 1999). For instance,
Rayner and Carter (1996) use explanation-based
learning to specialize a given general grammar to a
specific domain. They report important efficiency
gains (the parser is about three times faster), cou-
pled with a mild reduction of coverage (5% loss).
In contrast to our approach in which no manual
annotation is required, Rayner and Carter (1996)
report that for each sentence in the training data,
the best parse was selected manually from the set
of parses generated by the parser. For the exper-
iments described in the paper, this constituted an
effort of two and a half person-months. As a con-
sequence, they use only 15.000 training examples
(taken from ATIS, so presumably relatively short
sentences). In our experiments, we used up to 4
million sentences.
A further difference is related to the pruning
strategies. Our pruning strategies are extremely
simple. The cutting criteria employed in grammar
specialization either require carefully manually
tuning, or require more complicated statistical
techniques (Samuelsson, 1994); automatically
derived cutting criteria, however, perform consid-
erably worse.
A possible improvement of our approach con-
sists of predicting whether for a given input sen-
tence the filter should be used, or whether the sen-
tence appears to be ?easy? enough to allow for a
full parse. For instance, one may chose to use
the filter only for sentences of a given minimum
length. Initial experiments indicate that such a
setup may improve somewhat over the results pre-
sented here.
Acknowledgments
This research was carried out in part in the
context of the STEVIN programme which is
funded by the Dutch and Flemish governments
(http://taalunieversum.org/taal/technologie/stevin/).
823
20 40 60 80
85
86
87
88
Million words
Accur
acy (%C
A)
bigramtrigramfourgramprefixno filter
20 40 60 80
0
5
10
15
Million words
Mean
 cputim
e (sec)
bigramtrigramfourgramprefixno filter
1 5 10 50 500
20
40
60
80
timeout (sec)
accu
racy (%
CA)
prefix filterdefault
2 4 6 8 10
20
40
60
80
mean cputime (sec)
accu
racy (%
CA)
prefix filterdefault
1 5 10 50 500
20
40
60
80
timeout (sec)
accu
racy (%
CA)
prefix filterdefault
5 10 15
20
40
60
80
mean cputime (sec)
accu
racy (%
CA)
prefix filterdefault
Figure 4: The first two graphs present accuracy (left) and mean CPU-time (right) as a function of the
amount of training data used. Evaluation on 10% of the Alpino Treebank. The third and fourth graph
present accuracy versus time-out, and accuracy versus mean CPU-time for various time-outs. The graph
compares the baseline system with the parser which uses the prefix filter based on all available training
data. Evaluation on the D-Coi P-P-H 1-109 data-set (newspaper text). The two last graphs are similar,
based on the D-Coi P-P-L data-set (brochures).
824
References
P. C. Uit den Boogaart. 1975. Woordfrequenties
in geschreven en gesproken Nederlands. Oost-
hoek, Scheltema & Holkema, Utrecht. Werkgroep
Frequentie-onderzoek van het Nederlands.
Heleen Hoekstra, Michael Moortgat, Bram Renmans,
Machteld Schouppe, Ineke Schuurman, and Ton
van der Wouden, 2003. CGN Syntactische Anno-
tatie, December.
Y. Matsumoto, H. Tanaka, H. Hirakawa, H. Miyoshi,
and H. Yasukawa. 1983. BUP: a bottom up parser
embedded in Prolog. New Generation Computing,
1(2).
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of beam
thresholding, unification filtering and hybrid pars-
ing. In Proceedings of the International Workshop
on Parsing Technologies (IWPT).
Roeland Ordelman, Franciska de Jong, Arjan van Hes-
sen, and Hendri Hondorp. 2007. Twnc: a mul-
tifaceted Dutch news corpus. ELRA Newsletter,
12(3/4):4?7.
Fernando C. N. Pereira and Stuart M. Shieber. 1987.
Prolog and Natural Language Analysis. Center for
the Study of Language and Information Stanford.
Manny Rayner and David Carter. 1996. Fast pars-
ing using pruning and grammar specialization. In
34th Annual Meeting of the Association for Compu-
tational Linguistics, Santa Cruz.
Christer Samuelsson. 1994. Grammar specialization
through entropy thresholds. In 32th Annual Meet-
ing of the Association for Computational Linguis-
tics, New Mexico. ACL.
Khalil Sima?an. 1999. Learning Efficient Disambigua-
tion. Ph.D. thesis, University of Utrecht.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2004. Towards efficient probabilistic hpsg pars-
ing: integrating semantic and syntactic preference
to guide the parsing. In Beyond Shallow Analyses -
Formalisms and statistical modeling for deep analy-
ses, Hainan China. IJCNLP.
Leonoor van der Beek, Gosse Bouma, Robert Malouf,
and Gertjan van Noord. 2002. The Alpino depen-
dency treebank. In Computational Linguistics in the
Netherlands.
Gertjan van Noord, Ineke Schuurman, and Vincent
Vandeghinste. 2006. Syntactic annotation of large
corpora in STEVIN. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (LREC), Genoa, Italy.
Gertjan van Noord. 1997. An efficient implementation
of the head corner parser. Computational Linguis-
tics, 23(3):425?456. cmp-lg/9701004.
825
Treatment of Epsilon Moves in Subset 
Construction 
Gert jan  van  Noord"  
Rijksuniversiteit Groningen 
The paper discusses the problem of determinizing finite-state automata containing large numbers 
of c-moves. Experiments with finite-state approximations ofnatural anguage grammars often 
give rise to very large automata with a very large number of c-moves. The paper identifies and 
compares a number of subset construction algorithms that treat c-moves. Experiments have been 
performed which indicate that the algorithms differ considerably in practice, both with respect 
to the size of the resulting deterministic automaton, and with respect o practical efficiency. 
Furthermore, the experiments suggest that the average number of ~-moves per state can be used 
to predict which algorithm is likely to be the fastest for a given input automaton. 
1. Introduction 
1.1 Finite-State Language Processing 
An important problem in computational linguistics is posed by the fact that the gram- 
mars typically hypothesized by linguists are unattractive from the point of view of 
computation. For instance, the number of steps required to analyze a sentence of n 
words is n 3 for context-free grammars. For certain linguistically more attractive gram- 
matical formalisms it can be shown that no upper bound to the number of steps 
required to find an analysis can be given. The human language user, however, seems 
to process in linear time; humans understand longer sentences with no noticeable 
delay. This implies that neither context-free grammars nor more powerful grammati- 
cal formalisms are likely models for human language processing. An important issue 
therefore is how the linearity of processing by humans can be accounted for. 
A potential solution to this problem concerns the possibility of approximating 
an underlying general and abstract grammar by techniques of a much simpler sort. 
The idea that a competence grammar might be approximated by finite-state means 
goes back to early work by Chomsky (Chomsky 1963, 1964). There are essentially 
three observations that motivate the view that the processing of natural anguage is 
finite-state: 
1. 
2. 
. 
humans have a finite (small, limited, fixed) amount of memory available 
for language processing 
humans have problems with certain grammatical constructions, uch as 
center-embedding, which are impossible to describe by finite-state means 
(Miller and Chomsky 1963) 
humans process natural anguage very efficiently (in linear time) 
* Alfa-informatica & BCN. E-mail: vannoord@let.rug.nl 
(~ 2000 Association for Computational Linguistics 
Computational Linguistics Volume 26, Number 1 
1.2 Finite-State Approximation and c-Moves 
In experimenting with finite-state approximation techniques for context-free and more 
powerful grammatical formalisms (such as the techniques presented in Black \[1989\], 
Pereira and Wright \[1991, 1997\], Rood \[1996\], Grimley-Evans \[1997\], Nederhof \[1997, 
1998\], and Johnson \[1998\]), we have found that the resulting automata often are ex- 
tremely large. Moreover, the automata contain many e-moves (jumps). And finally, if 
such automata re determinized then the resulting automata re often smaller. It turns 
out that a straightforward implementation f the subset construction determinization 
algorithm performs badly for such inputs. In this paper we consider a number of 
variants of the subset construction algorithm that differ in their treatment of c-moves. 
Although we have observed that finite-state approximation techniques typically 
yield automata with large numbers of c-moves, this is obviously not a necessity. Instead 
of trying to improve upon determinization techniques for such automata, it might be 
more fruitful to try to improve these approximation techniques in such a way that 
more compact automata re produced. 1 However, because research into finite-state 
approximation is still of an exploratory and experimental nature, it can be argued 
that more robust determinization algorithms do still have a role to play: it can be 
expected that approximation techniques are much easier to define and implement if
the resulting automaton is allowed to be nondeterministic and to contain c-moves. 
Note furthermore that even if our primary motivation is in finite-state approxima- 
tion, the problem of determinizing finite-state automata with c-moves may be relevant 
in other areas of language research as well. 
1.3 Subset Construction and c-Moves 
The experiments were performed using the FSA Utilities. The FSA Utilities toolbox 
(van Noord 1997, 1999; Gerdemann and van Noord 1999; van Noord and Gerde- 
mann 1999) is a collection of tools to manipulate regular expressions, finite-state 
automata, and finite-state transducers. Manipulations include determinization, min- 
imization, composition, complementation, i tersection, Kleene closure, etc. Various 
visualization tools are available to browse finite-state automata. The toolbox is imple- 
mented in SICStus Prolog, and is available free of charge under Gnu General Public 
License via anonymous ftp at ftp://ftp.let.rug.nl/pub/vannoord/Fsa/, and via the 
web at http://www.let.rug.nl/~vannoord/Fsa/. At the time of our initial experiments 
with finite-state approximation, an old version of the toolbox was used, which ran 
into memory problems for some of these automata. For this reason, the subset con- 
struction algorithm has been reimplemented, paying special attention to the treatment 
of E-moves. Three variants of the subset construction algorithm are identified, which 
differ in the way c-moves are treated: 
per graph The most obvious and straightforward approach is sequential in the 
following sense: Firstly, an equivalent automaton without c-moves is con- 
structed for the input. To do this, the transitive closure of the graph consist- 
ing of all c-moves is computed. Secondly, the resulting automaton is then 
treated by a subset construction algorithm for c-free automata. Different 
variants of per graph can be identified, depending on the implementation 
of the c-removal step. 
1 Indeed, a later implementation by Nederhof avoids construction f the complete nondetermistic 
automaton byminimizing subautomata before they are embedded into larger subautomata. 
62 
van Noord Epsilon Moves in Subset Construction 
per state For each state that occurs in a subset produced uring subset construc- 
tion, compute the states that are reachable using e-moves. The results of 
this computation can be memorized, or computed for each state in a pre- 
processing step. This is the approach mentioned briefly in Johnson and 
Wood (1997). 2
per subset For each subset Q of states that arises during subset construction, com- 
pute Q~ 2 Q, which extends Q with all states that are reachable from any 
member of Q using e-moves. Such an algorithm is described in Aho, Sethi, 
and Ullman (1986). 
The motivation for this paper is the knowledge gleaned from experience, that the 
first approach turns out to be impractical for automata with very large numbers of 
e-moves. An integration of the subset construction algorithm with the computation of 
e-reachable states performs much better in practice for such automata. 
Section 2 presents a short statement of the problem (how to determinize a given 
finite-state automaton), and a subset construction algorithm that solves this problem in 
the absence of e-moves. Section 3 defines a number of subset construction algorithms 
that differ with respect to the treatment of e-moves. Most aspects of the algorithms are 
not new and have been described elsewhere, and/or were incorporated in previous 
implementations; a comparison of the different algorithms had not been performed 
previously. We provide a comparison with respect to the size of the resulting determin- 
istic automaton (in Section 3) and practical efficiency (in Section 4). Section 4 provides 
experimental results both for randomly generated automata nd for automata gen- 
erated by approximation algorithms. Our implementations of the various algorithms 
are also compared with AT&T's FSM utilities (Mohri, Pereira, and Riley 1998), to es- 
tablish that the experimental differences we find between the algorithms are truly 
caused by differences in the algorithm (as opposed to accidental implementation de- 
tails). 
2. Subset Construction 
2.1 Problem Statement 
Let a finite-state machine M be specified by a tuple (Q, G, 6, S, F) where Q is a finite 
set of states, G is a finite alphabet, and ~ is a function from Q x (G u {?}) --* 2 Q. 
Furthermore, S c_ Q is a set of start states and F _C Q is a set of final states. 3 
Let e-move be the relation {(qi, qj)lqj E ~(qi, e)}. c-reachable is the reflexive and 
transitive closure of e-move. Let e-CLOSURE: 2 Q ~ 2 Q be a function defined as: 
e-CLOSURE(Q') = {qlq' E Q', (q',q) E e-reachable} 
Furthermore, we write e-CLOSURE-I(Q ') for the set {qlq' E Q', (q,q') E e-reachable}. 
2 According to Derick Wood (p. c.), this approach as been implemented in several systems, including 
Howard Johnson's INR system. 
3 Note that a set of start states is required, rather than a single start state. Many operations on automata 
can be defined somewhat more elegantly in this way (including per graph t discussed below). Obviously, 
for deterministic automata this set should be a singleton set. 
63 
Computational Linguistics Volume 26, Number 1 
funct subset_eonstruction( ( Q, ~, ~, S, F) ) 
index_transitions(); Trans := O; Finals := O; States := O; 
Start := epsilon_closure( S) 
add(Start) 
whi le there is an unmarked subset T E States d__qo 
mark(T) 
foreach (a, U) C instructions(T) do 
U := epsilon_closure(U) 
Trans\[T,a\] := {U} 
add(U) 
od 
od 
return (States, ~, Trans, {Start}, Finals) 
end 
proc add(U) Reachable-state-set Maintenance 
if U ~ States 
then add U unmarked to States 
if U M F then Finals := Finals U {U} fi 
fi 
end 
funct instructions(P) Instruction Computation 
return merge(Up~ P transitions(p)) 
end 
funct epsilon_closure( U) 
return U 
end 
variant 1: No c-moves 
Figure 1 
Subset construction algorithm. 
For any given finite-state automaton M = (Q, G, 6, S, F), there is an equivalent de- 
terministic automaton M I = (2 Q, G, 6', {Q0}, FI) ? F ~ is the set of all states in 2 Q containing 
a final state of M, i.e., the set of subsets {Qi E 2Qiq E Qi, q E F}. M'  has a single start 
state Q0, which is the epsilon closure of the start states of M, i.e., Q0 = c-CLOSURE(S). 
Finally, 
6'({ql, q2 . . . . .  qi},a) = E-CLOSURE(6(ql, a) U 6(q2,a) U . . .  U 6(qi, a)) 
An algorithm that computes M / for a given M will only need to take into account 
states in 2 Q that are reachable from the start state Q0. This is the reason that for many 
input automata the algorithm does not need to treat all subsets of states (but note that 
there are automata for which all subsets are relevant, and hence exponential behavior 
cannot be avoided in general). 
Consider the subset construction algorithm in Figure 1. The algorithm maintains 
a set of subsets States. Each subset can be either marked or unmarked (to indicate 
whether the subset has been treated by the algorithm); the set of unmarked sub- 
sets is sometimes referred to as the agenda. The algorithm takes such an unmarked 
subset T and computes all transitions leaving T. This computat ion is per formed by 
the function instructions and is called instruction computation by Johnson and Wood 
(1997). 
64 
van Noord Epsilon Moves in Subset Construction 
The function index_transitions constructs the function transitions: Q --, ~. x 2 Q, which 
returns for a given state p the set of pairs (s, T) representing the transitions leaving p. 
Furthermore, the function merge takes such a set of pairs and merges all pairs with the 
same first element (by taking the union of the corresponding second elements). For 
example: 
merge({(a, {1,2,4}), (b, {2,4}), (a, {3,4}), (b, {5,6})}) 
= {(a, {1,2,3,4}), (b, {2,4,5,6})} 
The procedure add is responsible for "reachable-state-set maintenance," by en- 
suring that target subsets are added to the set of subsets if these subsets were not 
encountered before. Moreover, if such a new subset contains a final state, then this 
subset is added to the set of final states. 
3. Variants for E-Moves 
The algorithm presented in the previous ection does not treat c-moves. In this section, 
possible extensions of the algorithm are identified to treat c-moves. 
3.1 Per Graph 
In the per graph variant, two steps can be identified. In the first step, efree, an equiva- 
lent c-free automaton is constructed. In the second step this c-free automaton is deter- 
minized using the subset construction algorithm. The advantage of this approach is 
that the subset construction algorithm can remain simple because the input automaton 
is c-free. 
An algorithm for efree is described for instance in Hopcroft and Ullman (1979, 26- 
27). The main ingredient of efree is the construction of the function c-CLOSURE, which 
can be computed using a standard transitive closure algorithm for directed graphs: 
this algorithm is applied to the directed graph consisting of all c-moves of M. Such 
an algorithm can be found in several textbooks (see, for instance, Cormen, Leiserson, 
and Rivest \[1990\]). 
For a given finite-state automaton M = (Q, G,6,S,F), efree computes M' = 
(Q, ~, 6', S', F'), where S' = c-CLOSURE(S), F' = c-CLOSURE -1 (F), and 6'(p,a) = 
{qiq' E 6(p', a), p' c c-CLOSURE -1 (p), q E c-CLOSURE(q')}. Instead of using c-CLOSURE 
on both the source and target side of a transition, efree can be optimized in two different 
ways by using c-CLOSURE only on one side: 
efreet: M' = (Q, ~, 6', S',F), where S' = c-CLOSURE(S), and 
6'(p,a) = {qiq' E 6(p,a),q E c-CLOSURE(q')}. 
efreeS: M' = (Q, ~, 6', S,F'), where F' = ?-CLOSURE-I(F), and 
6'(p,a) = {qlq E 6(p',a),p' E c-CLOSURE-I(p)}. 
Although the variants appear very similar, there are some differences. Firstly, efree t 
might introduce states that are not coaccessible: states from which no path exists to a 
final state; in contrast, efree s might introduce states that are not accessible: states from 
which no path exists from the start state. A straightforward modification of both algo- 
rithms is possible to ensure that these states are not present in the output. Thus efree t,c 
65 
Computational Linguistics Volume 26, Number 1 
ca a 
(1) 
(2) (3) 
a 
2) 
a a a 
(4 (5) 
Figure 2 
Illustration of the difference in size between two variants of efree. (1) is the input automaton. 
The result of efree t is given in (2); (3) is the result of erred. (4) and (5) are the result of applying 
the subset construction to the result of efree t and efred, respectively. 
ensures that all states in the resulting automaton are co-accessible; free s,a ensures that 
all states in the resulting automaton are accessible. As a consequence, the size of the 
determinized machine is in general smaller if efree t,c is employed, because states that 
were not co-accessible (in the input) are removed (this is therefore an additional ben- 
efit of efreet,C; the fact that efree s,a removes accessible states has no effect on the size of 
the determinized machine because the subset construction algorithm already ensures 
accessibility anyway). 
Secondly, it turns out that applying eSree t in combination with the subset construc- 
tion algorithm generally produces maller automata than efree s (even if we ignore the 
benefit of ensuring co-accessibility). An example is presented in Figure 2. The differ- 
ences can be quite significant, as illustrated in Figure 3. 
Below we will write per graph x to indicate the nonintegrated algorithm based on 
efree x . 
3.2 Per Subset and Per State 
Next, we discuss two variants (per subset and per state) in which the treatment of c- 
moves is integrated with the subset construction algorithm. We will show later that 
such an integrated approach is in practice often more efficient han the per graph ap- 
proach if there are many C-moves. The per subset and per state approaches are also 
more suitable for a lazy implementation of the subset construction algorithm (in such 
a lazy implementation, subsets are only computed with respect to a given input 
string). 
The per subset and the per state algorithms use a simplified variant of the transitive 
closure algorithm for graphs. Instead of computing the transitive closure of a given 
66 
van Noord Epsilon Moves in Subset Construction 
20000 
18000 
16000 
14000 
12000 
10000 
8000 
Z 
6000 
4000 
2000 
0 
I I I 
efree-source o
ef ree- target  , 
0.2 0.4 0.6 0.8 1 1.2 1.4 
Deterministic Jump Density (mean) 
1.6 1.8 2 
Figure 3 
Difference in sizes of deterministic automata constructed with either efree s or  efree t, for 
randomly generated input automata consisting of 100 states, 15 symbols, and various numbers 
of transitions and jumps (cf. Section 4). Note that all states in the input are co-accessible; the 
difference in size is due solely to the effect illustrated in Figure 2. 
funct closure(T) 
D:=0 
foreach t E T do add t unmarked to D od 
while there is an unmarked state t C D do 
mark(t) 
foreach q E ~5(t, e) do 
if q ~ D then add q unmarked to D fi 
od 
od 
retum D 
end 
Figure 4 
Epsilon closure algorithm. 
graph, this algorithm only computes the closure for a given set of states. Such an 
algorithm is given in Figure 4. 
In both of the two integrated approaches, the subset construction algorithm is ini- 
tialized with an agenda containing a single subset hat is the e-CLOSURE of the set of 
start states of the input; furthermore, the way in which new transitions are computed 
also takes the effect of c-moves into account. Both differences are accounted for by an 
alternative definition of the epsilon_closure function. 
The approach in which the transitive closure is computed for one state at a t ime 
is defined by the following definition of the epsilon_closure function. Note that we 
make sure that the transitive closure computat ion is only performed once for each 
67 
Computational Linguistics Volume 26, Number 1 
input state, by memorizing the closure function; the full computation is memorized 
as well. 4 
funct epsilon_closure( U) 
return memo(Uu~u memo(closure( {u} ) ) ) 
end 
variant 2: per state 
In the case of the per subset approach, the closure algorithm is applied to each 
subset. We also memorize the closure function, in order to ensure that the closure 
computation is performed only once for each subset. This can be useful, since the 
same subset can be generated many times during subset construction. The definition 
simply is: 
funct epsilon_closure( U) 
return memo(closure(U)) 
end 
variant 3: per subset 
The motivation for the per state variant is the insight that in this case the closure 
algorithm is called at most IQ\] times. In contrast, in the per subset approach the transi- 
tive closure algorithm may need to be called 2 IQI times. On the other hand, in the per 
state approach some overhead must be accepted for computing the union of the results 
for each state. Moreover, in practice, the number of subsets is often much smaller than 
21QI. In some cases, the number of reachable subsets is smaller than the number of 
states encountered in those subsets. 
3.3 Implementation 
In order to implement the algorithms efficiently in Prolog, it is important to use ef- 
ficient data structures. In particular, we use an implementation f (non-updatable) 
arrays based on the N+K trees of O'Keefe (1990, 142-145) with N = 95 and K = 32. 
On top of this data structure, a hash array is implemented using the SICStus library 
predicate term_hash/4, which constructs a key for a given term. In such hashes, a 
value in the underlying array is a partial list of key-value pairs; thus collisions are 
resolved by chaining. This provides efficient access in practice, although such ar- 
rays are quite memory-intensive: care must be taken to ensure that the deterministic 
algorithms indeed are implemented without introducing choice-points during run- 
time. 
4. Experiments 
Two sets of experiments have been performed. In the first set of experiments, random 
automata re generated according to a number of criteria based on Leslie (1995). In 
the second set of experiments, results are provided for a number of (much larger) 
automata that surfaced uring actual development work on finite-state approximation 
techniques. 5 
Random Automata. Here, we report on a number of experiments for randomly gener- 
ated automata. Following Leslie (1995), the absolute transition density of an automaton 
4 This is an improvement over the algorithm given in a preliminary version of this paper (van Noord 
1998). 
5 All the automata used in the experiments are freely available from 
http://www.let.rug.nl/-vannoord / Fsa/. 
68 
van Noord Epsilon Moves in Subset Construction 
is defined as the number of transitions divided by the square of the number of states 
multiplied by the number of symbols (i.e., the number of transitions divided by the 
maximum number of "possible" transitions, or, in other words, the probability that a 
possible transition in fact exists). Deterministic transition density is the number of tran- 
sitions divided by the number of states multiplied by the number of symbols (i.e., the 
ratio of the number of transitions and the maximum number of "possible" transitions 
in a deterministic machine). 
In both of these definitions, the number of transitions hould be understood as 
the number of nonduplicate ransitions that do not lead to a sink state. A sink state 
is a state from which there exists no sequence of transitions to a final state. In the 
randomly generated automata, states are accessible and co-accessible by construction; 
sink states and associated transitions are not represented. 
Leslie (1995) shows that deterministic transition density is a reliable measure for 
the difficulty of subset construction. Exponential blow-up can be expected for input 
automata with deterministic transition density of around 2. 6 He concludes (page 66): 
randomly generated automata exhibit the maximum execution time, 
and the maximum number of states, at an approximate deterministic 
density of 2. Most of the area under the curve occurs within 0.5 and 
2.5 deterministic density--this  the area in which subset construction 
is expensive. 
Conjecture. For a given NFA, we can compute the expected num- 
bers of states and transitions in the corresponding DFA, produced by 
subset construction, from the deterministic density of the NFA. In ad- 
dition, this functional relationship gives rise to a Poisson-like curve 
with its peak approximately ata deterministic density of 2. 
A number of automata were generated randomly, according to the number of 
states, symbols, and transitions. For the first experiment, automata were generated 
consisting of 15 symbols, 25 states, and various densities (and no c-moves). The results 
are summarized in Figure 5. CPU-time was measured on a HP 9000/785 machine 
running HP-UX 10.20. Note that our timings do not include the start-up of the Prolog 
engine, nor the time required for garbage collection. 
In order to establish that the differences we obtain later are genuinely due to 
differences in the underlying algorithm, and not due to "accidental" implementation 
details, we have compared our implementation with the determinizer ofAT&T's FSM 
utilities (Mohri, Pereira, and Riley 1998). For automata without e-moves, we establish 
that FSM normally is faster: for automata with very small transition densities, FSM is 
up to four times as fast; for automata with larger densities, the results are similar. 
A new concept called absolute jump density is introduced to specify the number 
of c-moves. It is defined as the number of e-moves divided by the square of the 
number of states (i.e., the probability that an c-move exists for a given pair of states). 
Furthermore, deterministic jump density is the number of e-moves divided by the 
number of states (i.e., the average number of e-moves that leave a given state). In 
order to measure the differences between the three implementations, a number of 
automata have been generated consisting of 15 states and 15 symbols, using various 
6 Leslie uses the terms absolute density and deterministic density. 
69 
Computational Linguistics Volume 26, Number 1 
le+06 
L~ 
o 100000 + 
10000 
1000 
Z 
%- 100 
g 
~ 10 
1 
Figure 5 
+ 
+ 
\[\] 
+ 
\[\] 
~+ o+ 
+ 
\[\] 
+ 
+ 
fsa 
fsm + 
states \[\] 
1 10 
Deterministic Density 
Deterministic transition density versus CPU-time in msec. The input automata have 25 states, 
15 symbols, and no C-moves. fsa represents he CPU-time required by our FSA6 
implementation; fsm represents he CPU-time required by AT&T's FSM library; states 
represents he sum of the number of states of the input and output automata. 
transition densities between 0.01 and 0.3 (for larger densities, the automata tend to 
collapse to an automaton for ~.*). For each of these transition densities, deterministic 
jump densities were chosen in the range 0 to 2.5 (again, for larger values, the automata 
tend to collapse). In Figures 6 to 9, the outcomes of these experiments are summarized 
by listing the average amount of CPU-time required per deterministic jump density 
(for each of the algorithms), using automata with 15, 20, 25, and 100 states, respectively. 
Thus, every dot represents he average for determinizing a number of different input 
automata with various absolute transition densities and the same deterministic jump 
density. 
The striking aspect of these experiments i  that the integrated per subset and per 
state variants are much more efficient for larger deterministic jump densities. The per 
graph t is typically the fastest algorithm of the nonintegrated versions. However, in these 
experiments all states in the input are co-accessible by construction; and moreover, all 
states in the input are final states. Therefore, the advantages of the pergraph t'c algorithm 
could not be observed here. 
The turning point is a deterministic jump density of around 0.8: for smaller densi- 
ties the per graph t is typically slightly faster; for larger densities the per state algorithm 
is much faster. For densities beyond 1.5, the per subset alorithm tends to perform bet- 
ter than the per state algorithm. Interestingly, this generalization is supported by the 
experiments on automata generated by approximation techniques (although the re- 
sults for randomly generated automata re more consistent than the results for "real" 
examples). 
70 
10000 
10000 
i i i i i 
per_graph(t) o 
per_graph(s) , 
per_graph(s,a) \[\] 
per_graph(t,c) x 
per subset -~ 
per_state x 
fsm - 
1000 
-y 
;~ \]oo 
10 ' ' ' ' ' 
0 0.5 1 1.5 2 2.5 
#Jumps/#States 
Figure 6 
Average amount of CPU-time versus jump density for each of the algorithms, and FSM. Input 
automata have 15 states. Absolute transition densities: 0.01-0.3. 
i i t i 4 
per_graph(t) 0 
~ per_graph(s ) ,  
~"~,  per_graph(s,a) \[\] 
\ ~  per_graph(t,c) >< 
~'~x per subset -~ 
"N, \ \  per_state 
%- 
1000 
100 
van Noord Epsilon Moves in Subset Construction 
10 I I I I I 
0 0.5 1 1.5 2 2.5 
#Jumps/#States 
Figure 7 
Average amount of CPU-time versus jump density for each of the algorithms, and FSM. Input 
automata have 20 states. Absolute transition densities: 0.01-0.3. 
71 
Computational Linguistics Volume 26, Number 1 
100000 
10000 
%` 
"~ 1000 
D p~ 
U 
100 
per_graph(t) o 
per graph(s) , 
per_graph(s,a) \[\] 
per_graph(t,c) x 
per_subset -~ 
per_state 
fsm - 
10 i i 1 i i 
0 0.5 1 1.5 2 2.5 
#Jumps/#States 
Figure 8 
Average amount of CPU-time versus deterministic jump density for each of the algorithms, 
and FSM. Input automata have 25 states. Absolute transition densities: 0.01-0.3. 
100000 
%- 
10000 
1000 
i i i i i 
per_graph(t) o 
per__graph(s) , 
per_graph(s,a) \[\] 
per_graph(t,c) ){ 
per_subset 
per_state _* 
100 i i i i i 
0 0.5 1 1.5 2 2.5 
#Jumps/#States 
Figure 9 
Average amount of CPU-time versus deterministic jump density for each of the algorithms, 
and FSM. Input automata have 100 states. Absolute transition densities: 0.001-0.0035. 
72 
van Noord Epsilon Moves in Subset Construction 
Comparison with the FSM Library. We also provide the results for AT&T's FSM library. 
FSM is designed to treat weighted automata for very general weight sets. The initial 
implementation of the library consisted of an on-the-fly computation of the epsilon 
closures combined with determinization. This was abandoned for two reasons: it could 
not be generalized to the case of general weight sets, and it was not outputting the 
intermediate psilon-removed machine (which might be of interest in itself). In the 
current version, c-moves must be removed before determinization is possible. This 
mechanism thus is comparable to our per graph variant. Apparently, FSM employs 
an algorithm equivalent to our per graph s,a. The resulting determinized machines are 
generally larger than the machines produced by our integrated variants and the vari- 
ants that incorporate c-moves on the target side of transitions. The timings below are 
obtained for the pipe 
fsmrmepsilon I fsmdeterminize 
This is somewhat unfair, since this includes the time to write and read the intermediate 
machine. Even so, it is interesting to note that the FSM library is a constant factor faster 
than our per graphS,a; for larger numbers of jumps the per state and per subset variants 
consistently beat the FSM library. 
Experiment: Automata Generated by Approximation Algorithms. The automata used in the 
previous experiments were randomly generated. However, it may well be that in 
practice the automata that are to be treated by the algorithm have typical properties 
not reflected in this test data. For this reason, results are presented for a number of 
automata that were generated using approximation techniques for context-free gram- 
mars; in particular, for automata created by Nederhof, using the technique described 
in Nederhof (1997), and a small number of automata created using the technique 
of Pereira and Wright (1997) (as implemented by Nederhof). We have restricted our 
attention to automata with at least 1,000 states in the input. 
The automata typically contain lots of jumps. Moreover, the number of states of 
the resulting automaton is often smaller than the number of states in the input automa- 
ton. Results are given in Tables I and 2. One of the most striking examples is the ygrim 
automaton consisting of 3,382 states and 9,124 jumps. For this example, the per graph 
implementations ran out of memory (after a long time), whereas the implementation 
of the per subset alorithm produced the determinized automaton (containing only 9 
states) within a single CPU-second. The FSM implementation took much longer for 
this example (whereas for many of the other examples it is faster than our implemen- 
tations). Note that this example has the highest ratio of number of jumps to number 
of states. This confirms the observation that the per subset alorithm performs better 
on inputs with a high deterministic jump density. 
5. Conc lus ion 
We have discussed a number of variants of the subset construction algorithm for deter- 
minizing finite automata containing c-moves. The experiments support he following 
conclusions: 
The integrated variants per subset and per state work much better for 
automata containing a large number of c-moves. The per subset variant 
tends to improve upon the per state algorithm if the number of E-moves 
increases even further. 
73 
Computational Linguistics Volume 26, Number 1 
Table 1 
The automata generated by approximation algorithms. The table lists the number of states, 
transitions, and jumps of the input automaton, and the number of states of the determinized 
machine using the erred, efree t, and the efree t; variants, respectively. 
Input Output 
Id # of States # of Transitions # of Jumps # of States 
per graph s per graph t per graph t; 
per graph s~ per subset 
FSM per state 
g14 1,048 403 1,272 137 137 131 
ovis4.n 1,424 2,210 517 164 133 107 
g13 1,441 1,006 1,272 337 337 329 
rene2 1,800 2,597 96 846 844 844 
ovis9.p 1,868 2,791 2,688 2,478 2,478 1,386 
ygrim 3,382 5,422 9,124 9 9 9 
ygrim.p 48,062 63,704 109,296 702 702 702 
java19 54,369 28,333 51,018 1,971 1,971 1,855 
java16 64,210 43,935 41,305 3,186 3,186 3,078 
zovis3 88,156 78,895 68,093 5,174 5,154 4,182 
zovis2 89,832 80,400 69,377 6,561 6,541 5,309 
Table 2 
Results for automata generated by approximation algorithms. The dashes in the 
table indicate that the corresponding algorithm ran out of memory (after a long 
period of time) for that particular example. 
CPU-time (sec) 
graph t graph t'c graph s graph s~ subset state FSM 
g14 0.4 0.4 0.3 0.3 0.4 0.2 0.1 
ovis4.n 0.9 1.1 0.8 1.0 0.7 0.6 0.6 
g13 0.9 0.8 0.6 0.6 1.2 0.7 0.2 
rene2 0.2 0.3 0.2 0.2 0.2 0.2 0.1 
ovis9.p 36.6 16.0 16.9 17.0 25.2 20.8 . 21.9 
ygrim - 0.9 21.0 512.1 
ygrim.p - 562.1 - 4512.4 
java19 55.5 67.4 52.6 45.0 25.8 19.0 3.8 
java16 30.0 45.8 35.0 29.9 11.3 12.1 3.0 
zovis3 741.1 557.5 407.4 358.4 302.5 325.6 
zovis2 909.2 627.2 496.0 454.4 369.4 392.1 
? We have identified four different variants of the per graph algorithm. In 
our experiments, the per graph t is the algorithm of choice for automata 
containing few c-moves, because it is faster than the other algorithms, 
and because it produces maller automata than the per graph s and per 
graph s,a variants. 
? The per graph t,c variant is an interesting alternative in that it produces the 
smallest results. This variant should be used if the input automaton is
expected to contain many non-co-accessible states. 
74 
van Noord Epsilon Moves in Subset Construction 
Automata produced by finite-state approximation techniques tend to 
contain many c-moves. We found that for these automata the differences 
in speed between the various algorithms can be enormous. The per subset 
and per state algorithms are good candidates for this application. 
We have attempted to characterize the expected efficiency of the various algorithms 
in terms of the number of jumps and the number of states in the input automaton. It
is quite conceivable that other simple properties of the input automaton can be used 
even more effectively for this purpose. One reviewer suggests using the number of 
strongly c-connected components (the strongly connected components of the graph of 
all c-moves) for this purpose. We leave this and other possibilities to a future occasion. 
Acknowledgments 
I am grateful to Mark-Jan Nederhof for 
support, and for providing me with lots of 
(often dreadful) automata generated by his 
finite-state approximation tools. The 
comments of the anonymous FSMNLP and 
CL reviewers were extremely useful. 
References 
Aho, Alfred V., Ravi Sethi, and Jeffrey D. 
Ullman. 1986. Compilers. Principles, 
Techniques and Tools. Addison Wesley. 
Black, Alan W. 1989. Finite state machines 
from feature grammars. In International 
Workshop on Parsing Technologies, pages 
277-285, Pittsburgh, PA. 
Chomsky, Noam. 1963. Formal properties of 
grammars. In R. Duncan Luce, Robert R. 
Bush, and Eugene Galanter, editors, 
Handbook of Mathematical Psychology; 
Volume II. John Wiley, pages 323-418. 
Chomsky, Noam. 1964. On the notion 'rule 
of grammar.' In Jerry E. Fodor and 
Jerrold J. Katz, editors, The Structure of 
Language; Readings in the Philosophy of 
Language. Prentice Hall, pages 119-136. 
Cormen, Thomas H., Charles E. Leiserson, 
and Ronald L. Rivest. 1990. Introduction to 
Algorithms. MIT Press, Cambridge, MA. 
Gerdemann, Dale and Gertjan van Noord. 
1999. Transducers from rewrite rules with 
backreferences. In Ninth Conference ofthe 
European Chapter o/the Association for 
Computational Linguistics, Bergen, Norway. 
Grimley-Evans, Edmund. 1997. 
Approximating context-free grammars 
with a finite-state calculus. In Proceedings 
of the 35th Annual Meeting of the Association 
for Computational Linguistics and 8th 
Conference ofthe European Chapter o/the 
Association for Computational Linguistics, 
pages 452--459, Madrid, Spain. 
Hopcroft, John E. and Jeffrey D. Ullman. 
1979. Introduction to Automata Theory, 
Languages, and Computation. 
Addison-Wesley, Reading, MA. 
Johnson, J. Howard and Derick Wood. 1997. 
Instruction computation i subset 
construction. In Darrell Raymond, Derick 
Wood, and Sheng Yu, editors, Automata 
Implementation. Springer Verlag, pages 
64-71. Lecture Notes in Computer Science 
1260. 
Johnson, Mark. 1998. Finite-state 
approximation of constraint-based 
grammars using left-comer grammar 
transforms. In COLING-ACL '98: 36th 
Annual Meeting of the Association for 
Computational Linguistics and 17th 
International Conference on Computational 
Linguistics. Proceedings ofthe Conference, 
pages 619-623, Montreal, Quebec, 
Canada. 
Leslie, Ted. 1995. Efficient approaches to
subset construction. Master's thesis, 
Computer Science, University of 
Waterloo. 
Miller, George and Noam Chomsky. 1963. 
Finitary models of language users. In 
R. Luce, R. Bush, and E. Galanter, editors, 
Handbook o/Mathematical Psychology. 
Volume 2. John Wiley, pages 419-491. 
Mohri, Mehryar, Fernando C. N. Pereira, 
and Michael Riley. 1998. A rational design 
for a weighted finite-state transducer 
library. In Derick Wood and Sheng Yu, 
editors, Automata Implementation. Lecture 
Notes in Computer Science, Number 1436. 
Springer Verlag, pages 144-158. 
Nederhof, Mark-Jan. 1997. Regular 
approximations of CFLs: A grammatical 
view. In Proceedings ofthe International 
Workshop on Parsing Technologies, 
pages 159-170, Massachusetts Institute of 
Technology. 
Nederhof, Mark-Jan. 1998. Context-free 
parsing through regular approximation. 
In Proceedings ofthe International Workshop 
on Finite-state Methods in Natural Language 
Processing, pages 13-24, Ankara, Turkey. 
75 
Computational Linguistics Volume 26, Number 1 
O'Keefe, Richard A. 1990. The Craft of Prolog. 
MIT Press, Cambridge, MA. 
Pereira, Fernando C. N. and Rebecca N. 
Wright. 1991. Finite-state approximation 
of phrase structure grammars. In 
Proceedings ofthe 29th Annual Meeting, 
pages 246-255, Berkeley. Association for 
Computational Linguistics. 
Pereira, Fernando C. N. and Rebecca N. 
Wright. 1997. Finite-state approximation 
of phrase-structure grammars. In 
Emmanuel Roche and Yves Schabes, 
editors, Finite-State Language Processing. 
MIT Press, Cambridge, MA, pages 
149-173. 
Rood, C. M. 1996. Efficient finite-state 
approximation of context free grammars. 
In A. Kornai, editor, Extended Finite State 
Models of Language, Proceedings of the 
ECAI'96 workshop, pages 58-64, 
Budapest University of Economic 
Sciences, Hungary. 
van Noord, Gertjan. 1997. FSA Utilities: A 
toolbox to manipulate finite-state 
automata. In Darrell Raymond, Derick 
Wood, and Sheng Yu, editors, Automata 
Implementation. Lecture Notes in 
Computer Science, Number 1260. 
Springer Verlag, pages 87-108. 
van Noord, Gertjan. 1998. The treatment of 
epsilon moves in subset construction. In 
Proceedings ofthe International Workshop on 
Finite-state Methods in Natural Language 
Processing, pages 57-68, Ankara, Turkey. 
cmp-lg/9804003. 
van Noord, Gertjan. 1999. FSA6 reference 
manual. The FSA Utilities toolbox is 
available free of charge under Gnu 
General Public License at 
http://www.let.rug.nl/~vannoord/Fsa/. 
van Noord, Gertjan and Dale Gerdemann. 
1999. An extendible regular expression 
compiler for finite-state approaches in
natural anguage processing. In O. Boldt, 
H. Juergensen, and L. Robbins, editors, 
Workshop on Implementing Automata; WIA99 
Pre-Proceedings, pages XW-1-XIV-15, 
Potsdam, Germany. 
76 
Error Mining for Wide-Coverage Grammar Engineering
Gertjan van Noord
Alfa-informatica University of Groningen
POBox 716
9700 AS Groningen
The Netherlands
vannoord@let.rug.nl
Abstract
Parsing systems which rely on hand-coded linguis-
tic descriptions can only perform adequately in as
far as these descriptions are correct and complete.
The paper describes an error mining technique to
discover problems in hand-coded linguistic descrip-
tions for parsing such as grammars and lexicons. By
analysing parse results for very large unannotated
corpora, the technique discovers missing, incorrect
or incomplete linguistic descriptions.
The technique uses the frequency of n-grams of
words for arbitrary values of n. It is shown how a
new combination of suffix arrays and perfect hash
finite automata allows an efficient implementation.
1 Introduction
As we all know, hand-crafted linguistic descriptions
such as wide-coverage grammars and large scale
dictionaries contain mistakes, and are incomplete.
In the context of parsing, people often construct sets
of example sentences that the system should be able
to parse correctly. If a sentence cannot be parsed,
it is a clear sign that something is wrong. This
technique only works in as far as the problems that
might occur have been anticipated. More recently,
tree-banks have become available, and we can apply
the parser to the sentences of the tree-bank and com-
pare the resulting parse trees with the gold standard.
Such techniques are limited, however, because tree-
banks are relatively small. This is a serious prob-
lem, because the distribution of words is Zipfian
(there are very many words that occur very infre-
quently), and the same appears to hold for syntactic
constructions.
In this paper, an error mining technique is de-
scribed which is very effective at automatically dis-
covering systematic mistakes in a parser by using
very large (but unannotated) corpora. The idea is
very simple. We run the parser on a large set of sen-
tences, and then analyze those sentences the parser
cannot parse successfully. Depending on the na-
ture of the parser, we define the notion ?success-
ful parse? in different ways. In the experiments
described here, we use the Alpino wide-coverage
parser for Dutch (Bouma et al, 2001; van der Beek
et al, 2002b). This parser is based on a large con-
structionalist HPSG for Dutch as well as a very large
electronic dictionary (partly derived from CELEX,
Parole, and CGN). The parser is robust in the sense
that it essentially always produces a parse. If a full
parse is not possible for a given sentence, then the
parser returns a (minimal) number of parsed non-
overlapping sentence parts. In the context of the
present paper, a parse is called successful only if the
parser finds an analysis spanning the full sentence.
The basic idea is to compare the frequency of
words and word sequences in sentences that can-
not be parsed successfully with the frequency of the
same words and word sequences in unproblematic
sentences. As we illustrate in section 3, this tech-
nique obtains very good results if it is applied to
large sets of sentences.
To compute the frequency of word sequences of
arbitrary length for very large corpora, we use a new
combination of suffix arrays and perfect hash finite
automata. This implementation is described in sec-
tion 4.
The error mining technique is able to discover
systematic problems which lead to parsing failure.
This includes missing, incomplete and incorrect lex-
ical entries and grammar rules. Problems which
cause the parser to assign complete but incorrect
parses cannot be discovered. Therefore, tree-banks
and hand-crafted sets of example sentences remain
important to discover problems of the latter type.
2 A parsability metric for word sequences
The error mining technique assumes we have avail-
able a large corpus of sentences. Each sentence is a
sequence of words (of course, words might include
tokens such as punctuation marks, etc.). We run
the parser on all sentences, and we note for which
sentences the parser is successful. We define the
parsability of a word R(w) as the ratio of the num-
ber of times the word occurs in a sentence with a
successful parse (C(w|OK)) and the total number
of sentences that this word occurs in (C(w)):
R(w) =
C(w|OK)
C(w)
Thus, if a word only occurs in sentences that can-
not be parsed successfully, the parsability of that
word is 0. On the other hand, if a word only occurs
in sentences with a successful parse, its parsabil-
ity is 1. If we have no reason to believe that a
word is particularly easy or difficult, then we ex-
pect its parsability to be equal to the coverage of the
parser (the proportion of sentences with a successful
parse). If its parsability is (much) lower, then this
indicates that something is wrong. For the experi-
ments described below, the coverage of the parser
lies between 91% and 95%. Yet, for many words
we found parsability values that were much lower
than that, including quite a number of words with
parsability 0. Below we show some typical exam-
ples, and discuss the types of problem that are dis-
covered in this way.
If a word has a parsability of 0, but its frequency
is very low (say 1 or 2) then this might easily be
due to chance. We therefore use a frequency cut-off
(e.g. 5), and we ignore words which occur less often
in sentences without a successful parse.
In many cases, the parsability of a word depends
on its context. For instance, the Dutch word via
is a preposition. Its parsability in a certain exper-
iment was more than 90%. Yet, the parser was
unable to parse sentences with the phrase via via
which is an adverbial expression which means via
some complicated route. For this reason, we gener-
alize the parsability of a word to word sequences
in a straightforward way. We write C(wi . . . wj)
for the number of sentences in which the sequence
wi . . . wj occurs. Furthermore, C(wi . . . wj |OK),
is the number of sentences with a successful parse
which contain the sequence wi . . . wj . The parsabil-
ity of a sequence is defined as:
R(wi . . . wj) =
C(wi . . . wj |OK)
C(wi . . . wj)
If a word sequence wi . . . wj has a low parsabil-
ity, then this might be because it is part of a dif-
ficult phrase. It might also be that part of the se-
quence is the culprit. In order that we focus on
the relevant sequence, we consider a longer se-
quence wh . . . wi . . . wj . . . wk only if its parsabil-
ity is lower than the parsability of each of its sub-
strings:
R(wh . . . wi . . . wj . . . wk) < R(wi . . . wj)
This is computed efficiently by considering the
parsability of sequences in order of length (shorter
sequences before longer ones).
We construct a parsability table, which is a list of
n-grams sorted with respect to parsability. An n-
gram is included in the parsability table, provided:
? its frequency in problematic parses is larger
than the frequency cut-off
? its parsability is lower than the parsability of
all of its sub-strings
The claim in this paper is that a parsability table
provides a wealth of information about systematic
problems in the grammar and lexicon, which is oth-
erwise hard to obtain.
3 Experiments and results
3.1 First experiment
Data. For our experiments, we used the Twente
Nieuws Corpus, version pre-release 0.1.1 This cor-
pus contains among others a large collection of
news articles from various Dutch newspapers in the
period 1994-2001. In addition, we used all news
articles from the Volkskrant 1997 (available on CD-
ROM). In order that this material can be parsed rel-
atively quickly, we discarded all sentences of more
than 20 words. Furthermore, a time-out per sen-
tence of twenty CPU-seconds was enforced. The
Alpino parser normally exploits a part-of-speech tag
filter for efficient parsing (Prins and van Noord,
2003) which was switched off, to ensure that the
results were not influenced by mistakes due to this
filter. In table 1 we list some basic quantitative facts
about this material.
We exploited a cluster of Linux PCs for parsing.
If only a single PC had been available, it would have
taken in the order of 100 CPU days, to construct the
material described in table 1.
These experiments were performed in the autumn
of 2002, with the Alpino parser available then. Be-
low, we report on more recent experiments with the
latest version of the Alpino parser, which has been
improved quite a lot on the basis of the results of the
experiments described here.
Results. For the data described above, we com-
puted the parsability table, using a frequency cut-
off of 5. In figure 1 the frequencies of parsability
scores in the parsability table are presented. From
the figure, it is immediately clear that the relatively
high number of word sequences with a parsability of
(almost) zero cannot be due to chance. Indeed, the
1http://wwwhome.cs.utwente.nl/?druid/
TwNC/TwNC-main.html
newspaper sents coverage %
NRC 1994 582K 91.2
NRC 1995 588K 91.5
Volkskrant 1997 596K 91.6
AD 2000 631K 91.5
PAROOL 2001 529K 91.3
total 2,927K 91.4
Table 1: Overview of corpus material; first experi-
ment (Autumn 2002).
Parsability
F
r
e
q
u
e
n
c
y
0.0 0.2 0.4 0.6 0.8 1.0
0
5
0
0
0
1
5
0
0
0
Figure 1: Histogram of the frequencies of parsabil-
ity scores occurring in parsability table. Frequency
cut-off=5; first experiment (Autumn 2002).
parsability table starts with word sequences which
constitute systematic problems for the parser. In
quite a lot of cases, these word sequences origi-
nate from particular types of newspaper text with
idiosyncratic syntax, such as announcements of new
books, movies, events, television programs etc.; as
well as checkers, bridge and chess diagrams. An-
other category consists of (parts of) English, French
and German phrases.
We also find frequent spelling mistakes such as
de de where only a single de (the definite article)
is expected, and heben for hebben (to have), inden-
tiek for identiek (identical), koninging for koningin
(queen), etc. Other examples include wordt ik (be-
comes I), vindt ik (finds I), vind hij (find he) etc.
We now describe a number of categories of ex-
amples which have been used to improve the parser.
Tokenization. A number of n-grams with low
parsability scores point towards systematic mistakes
during tokenization. Here are a number of exam-
ples:2
2The @ symbol indicates a sentence boundary.
R C n-gram
0.00 1884 @ . @ .
0.00 385 @ ! @ !
0.00 22 ?s advocaat ?s lawyer
0.11 8 H. ?s H. ?s
0.00 98 @ , roept @ , yells
0.00 20 @ , schreeuwt @ , screams
0.00 469 @ , vraagt @ , asks
The first and second n-gram indicate sentences
which start with a full stop or an exclamation mark,
due to a mistake in the tokenizer. The third and
fourth n-grams indicate a problem the tokenizer had
with a sequence of a single capital letter with a dot,
followed by the genitive marker. The grammar as-
sumes that the genitive marking is attached to the
proper name. Such phrases occur frequently in re-
ports on criminals, which are indicated in news pa-
per only with their initials. Another systematic mis-
take is reflected by the last n-grams. In reported
speech such as
(1) Je
You
bent
are
gek!,
crazy!,
roept
yells
Franca.
Franca.
Franca yells: You are crazy!
the tokenizer mistakenly introduced a sentence
boundary between the exclamation mark and the
comma. On the basis of examples such as these,
the tokenizer has been improved.
Mistakes in the lexicon. Another reason an n-
gram receives a low parsability score is a mistake
in the lexicon. The following table lists two typical
examples:
R C n-gram
0.27 18 de kaft the cover
0.30 7 heeft opgetreden has performed
In Dutch, there is a distinction between neuter and
non-neuter common nouns. The definite article de
combines with non-neuter nouns, whereas neuter
nouns select het. The common noun kaft, for exam-
ple, combines with the definite article de. However,
according to the dictionary, it is a neuter common
noun (and thus would be expected to combine only
with the definite article het). Many similar errors
were discovered.
Another syntactic distinction that is listed in the
dictionary is the distinction between verbs which
take the auxiliary hebben (to have) to construct a
perfect tense clause vs. those that take the auxiliary
zijn (to be). Some verbs allow both possibilities.
The last example illustrates an error in the dictio-
nary with respect to this syntactic feature.
Incomplete lexical descriptions. The majority of
problems that the parsability scores indicate reflect
incomplete lexical entries. A number of examples
is provided in the following table:
R C n-gram
0.00 11 begunstigden favoured (N/V)
0.23 10 zich eraan dat self there-on that
0.08 12 aan te klikken on to click
0.08 12 doodzonde dat mortal sin that
0.15 11 zwarts black?s
0.00 16 dupe van victim of
0.00 13 het Turks . the Turkish
The word begunstigden is ambiguous between on
the one hand the past tense of the verb begunstigen
(to favour) and on the other hand the plural nominal-
ization begunstigden (beneficiaries). The dictionary
contained only the first reading.
The sequence zich eraan dat illustrates a missing
valency frame for verbs such as ergeren (to irritate).
In Dutch, verbs which take a prepositional comple-
ment sometimes also allow the object of the prepo-
sitional complement to be realized by a subordinate
(finite or infinite) clause. In that case, the preposi-
tional complement is R-pronominalized. Examples:
(2) a. Hij
He
ergert
is-irritated
zich
self
aan
on
zijn
his
aanwezigheid
presence
He is irritated by his presence
b. Hij
He
ergert
is-irritated
zich
self
er
there
niet
not
aan
on
dat
that
. . .
. . .
He is not irritated by the fact that . . .
The sequence aan te klikken is an example of a
verb-particle combination which is not licensed in
the dictionary. This is a relatively new verb which
is used for click in the context of buttons and hyper-
links.
The sequence doodzonde dat illustrates a syn-
tactic construction where a copula combines with
a predicative complement and a sentential subject,
if that predicative complement is of the appropriate
type. This type is specified in the dictionary, but was
missing in the case of doodzonde. Example:
(3) Het
It
is
is
doodzonde
mortal-sin
dat
that
hij
he
slaapt
sleeps
That he is sleeping is a pity
The word zwarts should have been analyzed as a
genitive noun, as in (typically sentences about chess
or checkers):
(4) Hij
He
keek
looked
naar
at
zwarts
black?s
toren
rook
whereas the dictionary only assigned the inflected
adjectival reading.
The sequence dupe van illustrates an example of
an R-pronominalization of a PP modifier. This is
generally not possible, except for (quite a large)
number of contexts which are determined by the
verb and the object:
(5) a. Hij
He
is
is
de
the
dupe
victim
van
of
jouw
your
vergissing
mistake
He has to suffer for your mistake
b. Hij
He
is
is
daar
there
nu
now
de
the
dupe
victim
van
of
He has to suffer for it
The word Turks can be both an adjective (Turkish)
or a noun the Turkish language. The dictionary con-
tained only the first reading.
Very many other examples of incomplete lexical
entries were found.
Frozen expressions with idiosyncratic syntax.
Dutch has many frozen expressions and idioms with
archaic inflection and/or word order which breaks
the parser. Examples include:
R C n-gram
0.00 13 dan schaadt het then harms it
0.00 13 @ God zij @ God be[I]
0.22 25 God zij God be[I]
0.00 19 Het zij zo It be[I] so
0.45 12 goeden huize good house[I]
0.09 11 berge mountain[I]
0.00 10 hele gedwaald whole[I] dwelled
0.00 14 te weeg
The sequence dan schaadt het is part of the id-
iom Baat het niet, dan schaadt het niet (meaning: it
might be unsure whether something is helpful, but
in any case it won?t do any harm). The sequence
God zij is part of a number of archaic formulas such
as God zij dank (Thank God). In such examples,
the form zij is the (archaic) subjunctive form of the
Dutch verb zijn (to be). The sequence Het zij zo is
another fixed formula (English: So be it), contain-
ing the same subjunctive. The phrase van goeden
huize (of good family) is a frozen expression with
archaic inflection. The word berge exhibits archaic
inflection on the word berg (mountain), which only
occurs in the idiomatic expression de haren rijzen
mij te berge (my hair rises to the mountain) which
expresses a great deal of surprise. The n-gram hele
gedwaald only occurs in the idiom Beter ten halve
gekeerd dan ten hele gedwaald: it is better to turn
halfway, then to go all the way in the wrong direc-
tion. Many other (parts of) idiomatic expressions
were found in the parsability table.
The sequence te weeg only occurs as part of the
phrasal verb te weeg brengen (to cause).
Incomplete grammatical descriptions. Al-
though the technique strictly operates at the level
of words and word sequences, it is capable of
indicating grammatical constructions that are not
treated, or not properly treated, in the grammar.
R C n-gram
0.06 34 Wij Nederlanders We Dutch
0.08 23 Geeft niet Matters not
0.00 15 de alles the everything
0.10 17 Het laten The letting
0.00 10 tenzij . unless .
The sequence Wij Nederlanders constitutes an ex-
ample of a pronoun modified by means of an appo-
sition (not allowed in the grammar) as in
(6) Wij
We
Nederlanders
Dutch
eten
eat
vaak
often
aardappels
potatoes
We, the Dutch, often eat potatoes
The sequence Geeft niet illustrates the syntac-
tic phenomenon of topic-drop (not treated in the
grammar): verb initial sentences in which the topic
(typically the subject) is not spelled out. The se-
quence de alles occurs with present participles (used
as prenominal modifiers) such as overheersende as
in de alles overheersende paniek (literally: the all
dominating panic, i.e., the panic that dominated ev-
erything). The grammar did not allow prenominal
modifiers to select an NP complement. The se-
quence Het laten often occurs in nominalizations
with multiple verbs. These were not treated in the
grammar. Example:
(7) Het
The
laten
letting
zien
see
van
of
problemen
problems
Showing problems
The word sequence tenzij . is due to sentences in
which a subordinate coordinator occurs without a
complement clause:
(8) Gij
Thou
zult
shallt
niet
not
doden,
kill,
tenzij.
unless.
A large number of n-grams also indicate elliptical
structures, not treated in that version of the gram-
mar. Another fairly large source of errors are ir-
regular named entities (Gil y Gil, Osama bin Laden
. . . ).
newspaper # sentences coverage %
NRC 1994 552,833 95.0
Volkskrant 1997 569,314 95,2
AD 2000 662,380 95,7
Trouw 1999 406,339 95,5
Volkskrant 2001 782,645 95,1
Table 2: Overview of corpus material used for the
experiments; second experiment (January 2004).
3.2 Later experiment
Many of the errors and omissions that were found
on the basis of the parsability table have been cor-
rected. As can be seen in table 2, the coverage
obtained by the improved parser increased substan-
tially. In this experiment, we also measured the cov-
erage on additional sets of sentences (all sentences
from the Trouw 1999 and Volkskrant 2001 news-
paper, available in the TwNC corpus). The results
show that coverage is similar on these unseen test-
sets.
Obviously, coverage only indicates how often the
parser found a full parse, but it does not indicate
whether that parse actually was the correct parse.
For this reason, we also closely monitored the per-
formance of the parser on the Alpino tree-bank3
(van der Beek et al, 2002a), both in terms of parsing
accuracy and in terms of average number of parses
per sentence. The average number of parses in-
creased, which is to be expected if the grammar and
lexicon are extended. Accuracy has been steadily
increasing on the Alpino tree-bank. Accuracy is
defined as the proportion of correct named depen-
dency relations of the first parse returned by Alpino.
Alpino employs a maximum entropy disambigua-
tion component; the first parse is the most promising
parse according to this statistical model. The maxi-
mum entropy disambiguation component of Alpino
assigns a score S(x) to each parse x:
S(x) =
?
i
?ifi(x) (1)
where fi(x) is the frequency of a particular feature i
in parse x and ?i is the corresponding weight of that
feature. The probability of a parse x for sentence w
is then defined as follows, where Y (w) are all the
parses of w:
p(x|w) =
exp (S(x))
?
y?Y (w) exp (S(y))
(2)
The disambiguation component is described in de-
tail in Malouf and van Noord (2004).
3http://www.let.rug.nl/?vannoord/trees/
Time (days)
A
c
c
u
r
a
c
y
0 50 100 150 200 250 300 3508
4
.
5
8
5
.
5
8
6
.
5
Figure 2: Development of Accuracy of the Alpino
parser on the Alpino Tree-bank
Figure 2 displays the accuracy from May 2003-
May 2004. During this period many of the prob-
lems described earlier were solved, but other parts
of the system were improved too (in particular, the
disambiguation component was improved consider-
ably). The point of the graph is that apparently the
increase in coverage has not been obtained at the
cost of decreasing accuracy.
4 A note on the implementation
The most demanding part of the implementation
consists of the computation of the frequency of n-
grams. If the corpus is large, or n increases, simple
techniques break down. For example, an approach
in which a hash data-structure is used to maintain
the counts of each n-gram, and which increments
the counts of each n-gram that is encountered, re-
quires excessive amounts of memory for large n
and/or for large corpora. On the other hand, if a
more compact data-structure is used, speed becomes
an issue. Church (1995) shows that suffix arrays
can be used for efficiently computing the frequency
of n-grams, in particular for larger n. If the cor-
pus size increases, the memory required for the suf-
fix array may become problematic. We propose a
new combination of suffix arrays with perfect hash
finite automata, which reduces typical memory re-
quirements by a factor of five, in combination with
a modest increase in processing efficiency.
4.1 Suffix arrays
Suffix arrays (Manber and Myers, 1990; Yamamoto
and Church, 2001) are a simple, but useful data-
structure for various text-processing tasks. A corpus
is a sequence of characters. A suffix array s is an ar-
ray consisting of all suffixes of the corpus, sorted al-
phabetically. For example, if the corpus is the string
abba, the suffix array is ?a,abba,ba,bba?.
Rather than writing out each suffix, we use integers
i to refer to the suffix starting at position i in the
corpus. Thus, in this case the suffix array consists
of the integers ?3, 0, 2, 1?.
It is straightforward to compute the suffix array.
For a corpus of k + 1 characters, we initialize the
suffix array by the integers 0 . . . k. The suffix ar-
ray is sorted, using a specialized comparison rou-
tine which takes integers i and j, and alphabetically
compares the strings starting at i and j in the cor-
pus.4
Once we have the suffix array, it is simple to com-
pute the frequency of n-grams. Suppose we are in-
terested in the frequency of all n-grams for n = 10.
We simply iterate over the elements of the suffix ar-
ray: for each element, we print the first ten words
of the corresponding suffix. This gives us all oc-
currences of all 10-grams in the corpus, sorted al-
phabetically. We now count each 10-gram, e.g. by
piping the result to the Unix uniq -c command.
4.2 Perfect hash finite automata
Suffix arrays can be used more efficiently to com-
pute frequencies of n-grams for larger n, with
the help of an additional data-structure, known as
the perfect hash finite automaton (Lucchiesi and
Kowaltowski, 1993; Roche, 1995; Revuz, 1991).
The perfect hash automaton for an alphabetically
sorted finite set of words w0 . . . wn is a weighted
minimal deterministic finite automaton which maps
wi ? i for each w0?i?n. We call i the word code
of wi. An example is given in figure 3.
Note that perfect hash automata implement an or-
der preserving, minimal perfect hash function. The
function is minimal, in the sense that n keys are
mapped into the range 0 . . . n ? 1, and the function
is order preserving, in the sense that the alphabetic
order of words is reflected in the numeric order of
word codes.
4.3 Suffix arrays with words
In the approach of Church (1995), the corpus is
a sequence of characters (represented by integers
reflecting the alphabetic order). A more space-
efficient approach takes the corpus as a sequence of
words, represented by word codes reflecting the al-
phabetic order.
To compute frequencies of n-grams for larger n,
we first compute the perfect hash finite automaton
for all words which occur in the corpus,5 and map
4The suffix sort algorithm of Peter M. McIlroy and M.
Douglas McIlroy is used, available as http://www.cs.
dartmouth.edu/?doug/ssort.c; This algorithm is ro-
bust against long repeated substrings in the corpus.
5We use an implementation by Jan Daciuk freely avail-
able from http://www.eti.pg.gda.pl/?jandac/
fsa.html.
d::1
c
r::5
s::7
e::1
r
g::1
c k
o
u::2
c
s::1
l
o
t
t
kc
co
Figure 3: Example of a perfect hash finite automa-
ton for the words clock, dock, dog, duck, dust, rock,
rocker, stock. Summing the weights along an ac-
cepting path in the automaton yields the rank of the
word in alphabetic ordering.
the corpus to a sequence of integers, by mapping
each word to its word code. Suffix array construc-
tion then proceeds on the basis of word codes, rather
than character codes.
This approach has several advantages. The rep-
resentation of both the corpus and the suffix array
is more compact. If the average word length is k,
then the corresponding arrays are k times smaller
(but we need some additional space for the perfect
hash automaton). In Dutch, the average word length
k is about 5, and we obtained space savings in that
order.
If the suffix array is shorter, sorting should be
faster too (but we need some additional time to com-
pute the perfect hash automaton). In our experience,
sorting is about twice as fast for word codes.
4.4 Computing parsability table
To compute parsability scores, we assume there are
two corpora cm and ca, where the first is a sub-
corpus of the second. cm contains all sentences
for which parsing was not successful. ca contains
all sentences overall. For both corpora, we com-
pute the frequency of all n-grams for all n; n-grams
with a frequency below a specified frequency cut-
off are ignored. Note that we need not impose an
a priori maximum value for n; since there is a fre-
quency cut-off, for some n there simply aren?t any
sequences which occur more frequently than this
cut-off. The two n-gram frequency files are orga-
nized in such a way that shorter n-grams precede
longer n-grams.
The two frequency files are then combined as
follows. Since the frequency file corresponding to
cm is (much) smaller than the file corresponding
to ca, we read the first file into memory (into a
hash data structure). We then iteratively read an
n-gram frequency from the second file, and com-
pute the parsability of that n-gram. In doing so,
we keep track of the parsability scores assigned to
previous (hence shorter) n-grams, in order to en-
sure that larger n-grams are only reported in case
the parsability scores decrease. The final step con-
sists in sorting all remaining n-grams with respect
to their parsability.
To give an idea of the practicality of the ap-
proach, consider the following data for one of the
experiments described above. For a corpus of
2,927,016 sentences (38,846,604 words, 209Mb),
it takes about 150 seconds to construct the per-
fect hash automaton (mostly sorting). The automa-
ton is about 5Mb in size, to represent 677,488 dis-
tinct words. To compute the suffix array and fre-
quencies of all n-grams (cut-off=5), about 15 min-
utes of CPU-time are required. Maximum runtime
memory requirements are about 400Mb. The re-
sult contains frequencies for 1,641,608 distinct n-
grams. Constructing the parsability scores on the
basis of the n-gram files only takes 10 seconds
CPU-time, resulting in parsability scores for 64,998
n-grams (since there are much fewer n-grams which
actually occur in problematic sentences). The ex-
periment was performed on a Intel Pentium III,
1266MHz machine running Linux. The software is
freely available from http://www.let.rug.
nl/?vannoord/software.html.
5 Discussion
An error mining technique has been presented
which is very helpful in identifying problems in
hand-coded grammars and lexicons for parsing. An
important ingredient of the technique consists of the
computation of the frequency of n-grams of words
for arbitrary values of n. It was shown how a new
combination of suffix arrays and perfect hash fi-
nite automata allows an efficient implementation.
A number of potential improvements can be envi-
sioned.
In the definition of R(w), the absolute frequency
of w is ignored. Yet, if w is very frequent, R(w)
is more reliable than if w is not frequent. There-
fore, as an alternative, we also experimented with
a set-up in which an exact binomial test is applied
to compute a confidence interval for R(w). Results
can then be ordered with respect to the maximum of
these confidence intervals. This procedure seemed
to improve results somewhat, but is computation-
ally much more expensive. For the first experiment
described above, this alternative set-up results in a
parsability table of 42K word tuples, whereas the
original method produces a table of 65K word tu-
ples.
R C n-gram
0.00 8 Beter ten
0.20 12 ten halve
0.15 11 halve gekeerd
0.00 8 gekeerd dan
0.09 10 dan ten hele
0.69 15 dan ten
0.17 10 ten hele
0.00 10 hele gedwaald
0.00 8 gedwaald .
0.20 10 gedwaald
Table 3: Multiple n-grams indicating same error
The parsability table only contains longer n-
grams if these have a lower parsability than the cor-
responding shorter n-grams. Although this heuristic
appears to be useful, it is still possible that a single
problem is reflected multiple times in the parsabil-
ity table. For longer problematic sequences, the
parsability table typically contains partially over-
lapping parts of that sequence. This phenomenon
is illustrated in table 3 for the idiom Beter ten
halve gekeerd dan ten hele gedwaald discussed ear-
lier. This suggests that it would be useful to con-
sider other heuristics to eliminate such redundancy,
perhaps by considering statistical feature selection
methods.
The definition used in this paper to identify a suc-
cessful parse is a rather crude one. Given that gram-
mars of the type assumed here typically assign very
many analyses to a given sentence, it is often the
case that a specific problem in the grammar or lex-
icon rules out the intended parse for a given sen-
tence, but alternative (wrong) parses are still pos-
sible. What appears to be required is a (statistical)
model which is capable of judging the plausibility
of a parse. We investigated whether the maximum
entropy score S(x) (equation 1) can be used to indi-
cate parse plausibility. In this set-up, we considered
a parse successful only if S(x) of the best parse is
above a certain threshold. However, the resulting
parsability table did not appear to indicate problem-
atic word sequences, but rather word sequences typ-
ically found in elliptical sentences were returned.
Apparently, the grammatical rules used for ellip-
sis are heavily punished by the maximum entropy
model in order that these rules are used only if other
rules are not applicable.
Acknowledgments
This research was supported by the PIONIER
project Algorithms for Linguistic Processing funded
by NWO.
References
Gosse Bouma, Gertjan van Noord, and Robert Mal-
ouf. 2001. Wide coverage computational anal-
ysis of Dutch. In W. Daelemans, K. Sima?an,
J. Veenstra, and J. Zavrel, editors, Computational
Linguistics in the Netherlands 2000.
Kenneth Ward Church. 1995. Ngrams. ACL 1995,
MIT Cambridge MA, June 16. ACL Tutorial.
Claudio Lucchiesi and Tomasz Kowaltowski. 1993.
Applications of finite automata representing large
vocabularies. Software Practice and Experience,
23(1):15?30, Jan.
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value
grammars. In Beyond shallow analyses. For-
malisms and statistical modeling for deep anal-
ysis, Sanya City, Hainan, China. IJCNLP-04
Workshop.
Udi Manber and Gene Myers. 1990. Suf-
fix arrays: A new method for on-line string
searching. In Proceedings of the First An-
nual AC-SIAM Symposium on Discrete Algo-
rithms, pages 319?327. http://manber.
com/publications.html.
Robbert Prins and Gertjan van Noord. 2003. Re-
inforcing parser preferences through tagging.
Traitement Automatique des Langues, 44(3):121?
139. in press.
Dominique Revuz. 1991. Dictionnaires et lexiques:
me?thodes et alorithmes. Ph.D. thesis, Institut
Blaise Pascal, Paris, France. LITP 91.44.
Emmanuel Roche. 1995. Finite-state tools for lan-
guage processing. ACL 1995, MIT Cambridge
MA, June 16. ACL Tutorial.
Leonoor van der Beek, Gosse Bouma, Robert Mal-
ouf, and Gertjan van Noord. 2002a. The Alpino
dependency treebank. In Marie?t Theune, Anton
Nijholt, and Hendri Hondorp, editors, Computa-
tional Linguistics in the Netherlands 2001. Se-
lected Papers from the Twelfth CLIN Meeting,
pages 8?22. Rodopi.
Leonoor van der Beek, Gosse Bouma, and Gertjan
van Noord. 2002b. Een brede computationele
grammatica voor het Nederlands. Nederlandse
Taalkunde, 7(4):353?374. in Dutch.
Mikio Yamamoto and Kenneth W. Church. 2001.
Using suffix arrays to compute term frequency
and document frequency for all substrings in a
corpus. Computational Linguistics, 27(1):1?30.
Robust Parsing, Error Mining, Automated Lexical 
Acquisition, and Evaluation 
Gertjan van Noord 
University of Groningen 
vannoord@let.rug.nl
Abstract
In our attempts to construct a wide coverage HPSG parser for Dutch, techniques to improve 
the overall robustness of the parser are required at various steps in the parsing process. 
Straightforward but important aspects include the treatment of unknown words, and the 
treatment of input for which no full parse is available.   
Another important means to improve the parser's performance on unexpected input is the 
ability to learn from your errors. In our methodology we apply the parser to large quantities of 
text (preferably from different types of corpora), and we then apply error mining techniques to 
identify potential errors, and furthermore we apply machine learning techniques to correct 
some of those errors (semi-)automatically, in particular those errors that are due to missing or 
incomplete lexical entries.  
Evaluating the robustness of a parser is notoriously hard. We argue against coverage as a 
meaningful evaluation metric. More generally, we argue against evaluation metrics that do not 
take into account accuracy. We propose to use variance of accuracy across sentences (and 
more generally across corpora) as a measure for robustness.   
1
2
Proceedings of the 10th Conference on Parsing Technologies, pages 1?10,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Using Self-Trained Bilexical Preferences to Improve Disambiguation
Accuracy
Gertjan van Noord
University of Groningen
vannoord@let.rug.nl
Abstract
A method is described to incorporate bilex-
ical preferences between phrase heads, such
as selection restrictions, in a Maximum-
Entropy parser for Dutch. The bilexical
preferences are modelled as association rates
which are determined on the basis of a very
large parsed corpus (about 500M words).
We show that the incorporation of such self-
trained preferences improves parsing accu-
racy significantly.
1 Motivation
In parse selection, the task is to select the correct
syntactic analysis of a given sentence from a set
of parses generated by some other mechanism. On
the basis of correctly labelled examples, supervised
parse selection techniques can be employed to ob-
tain reasonable accuracy. Although parsing has im-
proved enormously over the last few years, even the
most successful parsers make very silly, sometimes
embarassing, mistakes. In our experiments with a
large wide-coverage stochastic attribute-value gram-
mar of Dutch, we noted that the system sometimes
is insensitive to the naturalness of the various lexical
combinations it has to consider. Although parsers
often employ lexical features which are in principle
able to represent preferences with respect to word
combinations, the size of the training data will be
too small to be able to learn the relevance of such
features successfully.
In maximum-entropy parsing, the supervised
parsing technique that we use in our experiments, ar-
bitrary features can be defined which are employed
to characterize different parses. So it is possible to
construct features for any property that is thought
to be important for disambiguation. However, such
features can be useful for disambiguation only in
case the training set contains a sufficient number of
occurrences of these features. This is problematic,
in practice, for features that encode bilexical prefer-
ences such as selection restrictions, because typical
training sets are much too small to estimate the rele-
vance of features representing cooccurrences of two
words. As a simple example consider the ambiguous
Dutch sentence
(1) Melk drinkt de baby niet
Milk drinks the baby not
The standard model of the parser we experimented
with employs a wide variety of features including
syntactic features and lexical features. In particu-
lar, the model also includes features which encode
whether or not the subject or the object is fronted in
a parse. Since subjects, in general, are fronted much
more frequently than objects, the model has learnt
to prefer readings in which the fronted constituent
is analysed as the subject. Although the model also
contains features to distinguish whether e.g. milk
occurs as the subject or the object of drink, the
model has not learnt a preference for either of these
features, since there were no sentences in the train-
ing data that involved both these two words.
To make this point more explicit, we found that in
about 200 sentences of our parsed corpus of 27 mil-
lion sentences milk is the head of the direct object
of the verb drink. Suppose that we would need at
least perhaps 5 to 10 sentences in our training corpus
1
in order to be able to learn the specific preference
between milk and drink. The implication is that
we would need a (manually labeled!) training cor-
pus of approximately 1 million sentences (20 mil-
lion words). In contrast, the disambiguation model
of the Dutch parser we are reporting on in this paper
is trained on a manually labeled corpus of slightly
over 7,000 sentences (145,000 words). It appears
that semi-supervised or un-supervised methods are
required here.
Note that the problem not only occurs for artifi-
cial examples such as (1); here are a few mis-parsed
examples actually encountered in a large parsed cor-
pus:
(2) a. Campari moet u gedronken hebben
Campari must have drunk you
You must have drunk Campari
b. De wijn die Elvis zou hebben gedronken als
hij wijn zou hebben gedronken
The wine Elvis would have drunk if he had
drunk wine
The wine that would have drunk Elvis if he
had drunk wine
c. De paus heeft tweehonderd daklozen te eten
gehad
The pope had twohunderd homeless people
for dinner
In this paper, we describe an alternative approach
in which we employ pointwise mutual informa-
tion association score in the maximum entropy dis-
ambiguation model. Pointwise mutual information
(Fano, 1961) was used to measure strength of selec-
tion restrictions for instance by Church and Hanks
(1990). The association scores used here are esti-
mated using a very large parsed corpus of 500 mil-
lion words (27 million sentences). We show that the
incorporation of this additional knowledge source
improves parsing accuracy. Because the association
scores are estimated on the basis of a large corpus
that is parsed by the parser that we aim to improve
upon, this technique can be described as a somewhat
particular instance of self-training. Self-training has
been investigated for statistical parsing before. Al-
though naively adding self-labeled material to ex-
tend training data is normally not succesfull, there
have been successful variants of self-learning for
parsing as well. For instance, in McClosky et al
(2006) self-learning is used to improve a two-phase
parser reranker, with very good results for the clas-
sical Wall Street Journal parsing task.
Clearly, the idea that selection restrictions ought
to be useful for parsing accuracy is not new. How-
ever, as far as we know this is the first time that au-
tomatically acquired selection restrictions have been
shown to improve parsing accuracy results. Related
research includes Abekawa and Okumura (2006)
and Kawahara and Kurohashi (2006) where statis-
tical information between verbs and case elements
is collected on the basis of large automatically anal-
ysed corpora.
2 Background: Alpino parser
The experiments are performed using the Alpino
parser for Dutch. In this section we briefly describe
the parser, as well as the corpora that we have used
in the experiments described later.
2.1 Grammar and Lexicon
The Alpino system is a linguistically motivated,
wide-coverage grammar and parser for Dutch in the
tradition of HPSG. It consists of over 600 gram-
mar rules and a large lexicon of over 100,000 lex-
emes and various rules to recognize special con-
structs such as named entities, temporal expressions,
etc. The grammar takes a ?constructional? approach,
with rich lexical representations and a large number
of detailed, construction specific rules. Both the lex-
icon and the rule component are organized in a mul-
tiple inheritance hierarchy. Heuristics have been im-
plemented to deal with unknown words and word se-
quences, and ungrammatical or out-of-coverage sen-
tences (which may nevertheless contain fragments
that are analysable). The Alpino system includes a
POS-tagger which greatly reduces lexical ambiguity,
without an observable decrease in parsing accuracy
(Prins, 2005).
2.2 Parser
Based on the categories assigned to words, and
the set of grammar rules compiled from the HPSG
grammar, a left-corner parser finds the set of all
parses, and stores this set compactly in a packed
parse forest. All parses are rooted by an instance
2
of the top category, which is a category that general-
izes over all maximal projections (S, NP, VP, ADVP,
AP, PP and some others). If there is no parse cover-
ing the complete input, the parser finds all parses for
each substring. In such cases, the robustness com-
ponent will then select the best sequence of non-
overlapping parses (i.e., maximal projections) from
this set.
In order to select the best parse from the com-
pact parse forest, a best-first search algorithm is ap-
plied. The algorithm consults a Maximum Entropy
disambiguation model to judge the quality of (par-
tial) parses. Since the disambiguation model in-
cludes inherently non-local features, efficient dy-
namic programming solutions are not directly appli-
cable. Instead, a best-first beam-search algorithm is
employed (van Noord andMalouf, 2005; van Noord,
2006).
2.3 Maximum Entropy disambiguation model
The maximum entropy model is a conditional model
which assigns a probability to a parse t for a given
sentence s. Furthermore, fi(t) are the feature func-
tions which count the occurrence of each feature i in
a parse t. Each feature i has an associated weight ?i.
The score ? of a parse t is defined as the sum of the
weighted feature counts:
?(t) =
?
i
?ifi(t)
If t is a parse of s, the actual conditional proba-
bility is given by the following, where T (s) are all
parses of s:
P (t|s) =
exp(?(t))
?
u?T (s) exp(?(u))
However, note that if we only want to select the
best parse we can ignore the actual probability, and it
suffices to use the score ? to rank competing parses.
The Maximum Entropy model employs a large set
of features. The standard model uses about 42,000
different features. Features describe various prop-
erties of parses. For instance, the model includes
features which signal the application of particular
grammar rules, as well as local configurations of
grammar rules. There are features signalling spe-
cific POS-tags and subcategorization frames. Other
features signal local or non-local occurrences of ex-
traction (WH-movement, relative clauses etc.), the
grammatical role of the extracted element (subject
vs. non-subject etc.), features to represent the dis-
tance of a relative clause and the noun it modifies,
features describing the amount of parallelism be-
tween conjuncts in a coordination, etc. In addition,
there are lexical features which represent the co-
occurrence of two specific words in a specific de-
pendency, and the occurrence of a specific word as a
specific dependent for a given POS-tag. Each parse
is characterized by its feature vector (the counts for
each of the 42,000 features). Once the model is
trained, each feature is associated with its weight ?
(a positive or negative number, typically close to 0).
To find out which parse is the best parse according
to the model, it suffices to multiply the frequency
of each feature with its corresponding weight, and
sum these weighted frequencies. The parse with the
highest sum is the best parse. Formal details of the
disambiguation model are presented in van Noord
and Malouf (2005).
2.4 Dependency structures
Although Alpino is not a dependency grammar in
the traditional sense, dependency structures are gen-
erated by the lexicon and grammar rules as the value
of a dedicated feature dt. The dependency struc-
tures are based on CGN (Corpus Gesproken Ned-
erlands, Corpus of Spoken Dutch) (Hoekstra et al,
2003), D-Coi and LASSY (van Noord et al, 2006).
Such dependency structures are somewhat idiosyn-
cratic, as can be observed in the example in figure 1
for the sentence:
(3) waar en wanneer dronk Elvis wijn?
where and when did Elvis drink wine?
2.5 Evaluation
The output of the parser is evaluated by comparing
the generated dependency structure for a corpus sen-
tence to the gold standard dependency structure in a
treebank. For this comparison, we represent the de-
pendency structure (a directed acyclic graph) as a
set of named dependency relations. The dependency
graph in figure 1 is represented with the following
set of dependencies:
3
?whq
whd
1
conj
cnj
adv
waar0
crd
vg
en1
cnj
adv
wanneer2
body
sv1
mod
1
hd
verb
drink3
su
name
Elvis4
obj1
noun
wijn5
Figure 1: Dependency graph example. Reentrant
nodes are visualized using a bold-face index. Root
forms of head words are explicitly included in sepa-
rate nodes, and different types of head receive a dif-
ferent relation label such as hd, crd (for coordina-
tion), whd (for WH-phrases) etc. In this case, the
WH-phrase is both the whd element of the top-node,
as well as a mod dependent of drink.
crd/cnj(en,waar) crd/cnj(en,wanneer)
whd/body(en,drink) hd/mod(drink,en)
hd/obj1(drink,wijn) hd/su(drink,Elvis)
Comparing these sets, we count the number of de-
pendencies that are identical in the generated parse
and the stored structure, which is expressed tradi-
tionally using f-score (Briscoe et al, 2002). We pre-
fer to express similarity between dependency struc-
tures by concept accuracy:
CA = 1?
?
i D
i
f
max(
?
i Dig,
?
i Dip)
where Dip is the number of dependencies produced
by the parser for sentence i, Dg is the number of
dependencies in the treebank parse, and Df is the
number of incorrect and missing dependencies pro-
duced by the parser.
The standard version of Alpino that we use here
as baseline system is trained on the 145,000 word
Alpino treebank, which contains dependency struc-
tures for the cdbl (newspaper) part of the Eind-
hoven corpus. The parameters for training the model
are the same for the baseline model, as well as the
model that includes the self-trained bilexical prefer-
ences (introduced below). These parameters include
#sentences 100% 30,000,000
#words 500,000,000
#sentences without parse 0.2% 100,000
#sentences with fragments 8% 2,500,000
#single full parse 92% 27,500,000
Table 1: Approximate counts of the number of sen-
tences and words in the parsed corpus. About 0,2%
of the sentences did not get a parse, for computa-
tional reasons (out of memory, or maximum parse
time exceeded).
the Gaussian penalty, thresholds for feature selec-
tion, etc. Details of the training procedure are de-
scribed in van Noord and Malouf (2005).
2.6 Parsed Corpora
Over the course of about a year, Alpino has been
used to parse most of the TwNC-02 (Twente News-
paper Corpus), Dutch Wikipedia, and the Duch part
of Europarl. TwNC consists of Dutch newspaper
texts from 1994 - 2004. We did not use the ma-
terial from Trouw 2001, since part of that mate-
rial is used in the test set used below. We used
the 200 node Beowulf Linux cluster of the High-
Performance Computing center of the University of
Groningen. The dependency structures are stored in
XML. The XML files can be processed and searched
in various ways, for instance, using XPATH, XSLT
and Xquery (Bouma and Kloosterman, 2002). Some
quantitative information of this parsed corpus is
listed in table 1. In the experiments described be-
low, we do not distinguish between full and frag-
ment parses; sentences without a parse are obviously
ignored.
3 Bilexical preferences
3.1 Association Score
The parsed corpora described in the previous sec-
tion have been used in order to compute association
scores between lexical dependencies. The parses
constructed by Alpino are dependency structures. In
such dependency structures, the basic dependencies
are of the form r(w1, w2) where r is a relation such
as subject, object, modifier, prepositional comple-
ment, . . . , and wi are root forms of words.
Bilexical preference between two root forms w1
4
tokens 480,000,000
types 100,000,000
types with frequency ? 20 2,000,000
Table 2: Number of lexical dependencies in parsed
corpora (approximate counts)
bijltje gooi neer 13
duimschroef draai aan 13
peentje zweet 13
traantje pink weg 13
boontje dop 12
centje verdien bij 12
champagne fles ontkurk 12
dorst les 12
Table 3: Pairs involving a direct object relationship
with the highest pointwise mutual information score.
andw2 is computed using an association score based
on pointwise mutual information, as defined by Fano
(1961) and used for a similar purpose in Church and
Hanks (1990), as well as in many other studies in
corpus linguistics. The association score is defined
here as follows:
I(r(w1, w2) = log
f(r(w1, w2))
f(r(w1, ))f( ( , w2))
where f(X) is the relative frequency of X . In the
above formula, the underscore is a place holder for
an arbitrary relation or an arbitrary word. The as-
sociation score I compares the actual relative fre-
quency of w1 and w2 with dependency r, with
the relative frequency we would expect if the
words were independent. For instance, to compute
I(hd/obj1(drink,melk)) we lookup the number
of times drink occurs with a direct object out of all
462,250,644 dependencies (15,713) and the number
of times melk occurs as a dependent (10,172). If we
multiply the two corresponding relative frequencies,
we get the expected relative frequency (0.35) for
hd/obj1(drink,melk), which is about 560 times
as big as the actual frequence, 195. Taking the log
of this gives us the association score (6.33) for this
bi-lexical dependency. Note that pairs that we have
seen fewer than 20 times are ignored. Mutual in-
formation scores are unreliable for low frequencies.
An additional benefit of a frequency threshold is a
manageable size of the resulting data-structures.
The pairs involving a direct object relationship
with the highest scores are listed in table 3. The
biertje small glass of beer 8
borreltje strong alcoholic drink 8
glaasje small glass 8
pilsje small glass of beer 8
pintje small glass of beer 8
pint glass of beer 8
wijntje small glass of wine 8
alcohol alcohol 7
bier beer 7
Table 4: Pairs involving a direct object relationship
with the highest pointwise mutual information score
for the verb drink.
overlangs snijd door 12
welig tier 12
dunnetjes doe over 11
stief moederlijk bedeel 11
on zedelijk betast 11
stierlijk verveel 11
cum laude studeer af 10
hermetisch grendel af 10
ingespannen tuur 10
instemmend knik 10
kostelijk amuseer 10
Table 5: Pairs involving a modifier relationship be-
tween a verb and an adverbial with the highest asso-
ciation score.
highest scoring nouns that occur as the direct object
of drink are listed in table 4.
Selection restrictions are often associated only
with direct objects. We include bilexical association
scores for all types of dependencies. We found that
association scores for other types of dependencies
also captures both collocational preferences as well
as weaker cooccurrence preferences. Some exam-
ples including modifiers are listed in table 5. Such
preferences are useful for disambiguation as well.
Consider the ambiguous Dutch sentence
(4) omdat we lauw bier dronken
because we drank warm beer
because we drank beer warmly
The adjective lauw (cold, lukewarm, warm) can be
used to modify both nouns and verbs; this latter pos-
sibility is exemplified in:
(5) We hebben lauw gereageerd
We reacted indifferently
5
?smain
obj1
conj
cnj
noun
bier0
crd
vg
of1
cnj
noun
wijn2
hd
verb
drink3
su
name
Elvis4
mod
adv
niet5
Figure 2: Dependency structure produced for coor-
dination
3.2 Extending pairs
The CGN dependencies that we work with fail to re-
late pairs of words in certain syntactic constructions
for which it can be reasonably assumed that bilexi-
cal preferences should be useful. We have identified
two such constructions, namely relative clauses and
coordination, and for these constructions we gener-
alize our method, to take such dependencies into ac-
count too.
Consider coordinations such as:
(6) Bier of wijn drinkt Elvis niet
Beer or wine, Elvis does not drink
The dependency structure of the intended analysis
is given in figure 2. The resulting set of dependen-
cies for this example treats the coordinator as the
head of the conjunction:
hd/obj1(drink,of) crd/cnj(of,bier)
crd/cnj(of,wijn) hd/su(drink,elvis)
hd/mod(drink,niet)
So there are no direct dependencies between the verb
and the individual conjuncts. For this reason, we
add additional dependencies r(A,C) for every pair
of dependency r(A,B), crd/cnj(B,C).
Relative clauses are another syntactic phe-
nomenon where we extend the set of dependencies.
For a noun phrase such as:
(7) Wijn die Elvis niet dronk
Wine which Elvis did not drink
there is no direct dependency between wijn and
drink, as can be seen in the dependency structure
?
np
hd
noun
wijn0
mod
rel
rhd
1
pron
die1
body
ssub
obj1
1
su
name
Elvis2
mod
adv
niet3
hd
verb
drink4
Figure 3: Dependency structure produced for rela-
tive clause
given in figure 3. Sets of dependencies are extended
in such cases, to make the relation between the noun
and the role it plays in the relative clause explicit.
3.3 Using association scores as features
The association scores for all dependencies are used
in our maximum entropy disambiguation model as
follows. The technique is reminiscent of the inclu-
sion of auxiliary distributions in stochastic attribute-
value grammar (Johnson and Riezler, 2000).
Recall that a maximum entropy disambiguation
model exploits features. Features are properties of
parses, and we can use such features to describe any
property of parses that we believe is of importance
for disambiguation. For the disambiguation model,
a parse is fully characterized by a vector of feature
counts.
We introduce features z(t, r) for each of the ma-
jor POS labels t (verb, noun, adjective, adverb, . . . )
and each of the dependency relations r. The ?count?
of such a feature is determined by the association
scores for actually occuring dependency pairs. For
example, if in a given parse a given verb v has a
direct object dependent n, then we compute the as-
sociation of this particular pair, and use the resulting
number as the count of that feature. Of course, if
there are multiple dependencies of this type in a sin-
gle parse, the corresponding association scores are
all summed.
To illustrate this technique, consider the depen-
dency structure given earlier in figure 2. For this
6
example, there are four of these new features with a
non-zero count. The counts are given by the corre-
sponding association scores as follows:
z(verb, hd/su) = I(hd/su(drink,elvis))
z(verb, hd/mod) = I(hd/mod(drink,niet))
z(verb, hd/obj1) = I(hd/obj1(drink,of))
+ I(hd/obj1(drink,bier))
+ I(hd/obj1(drink,wijn))
z(conj, crd/cnj) = I(crd/cnj(of,bier))
+ I(crd/cnj(of,wijn))
It is crucial to observe that the new features do not
include any direct reference to actual words. This
means that there will be only a fairly limited number
of new features (depending on the number of tags t
and relations r), and we can expect that these fea-
tures are frequent enough to be able to estimate their
weights in training material of limited size.
Association scores can be negative if two words in
a lexical dependency occur less frequently than one
would expect if the words were independent. How-
ever, since association scores are unreliable for low
frequencies (including, often, frequencies of zero),
and since such negative associations involve low fre-
quencies by their nature, we only take into account
positive association scores.
4 Experiments
We report on two experiments. In the first exper-
iment, we report on the results of tenfold cross-
validation on the Alpino treebank. This is the ma-
terial that is standardly used for training and test-
ing. For each of the sentences of this corpus, the
system produces atmost the first 1000 parses. For
every parse we compute the quality by comparing
its dependency structure with the gold standard de-
pendency structure in the treebank. For training, at-
most 100 parses are selected randomly for each sen-
tence. For (tenfold cross-validated) testing, we use
all available parses for a given sentence. In order to
test the quality of the model, we check for each given
sentence which of its atmost 1000 parses is selected
by the disambiguation model. The quality of that
parse is used in the computation of the accuracy, as
listed in table 6. The column labeled exact measures
the proportion of sentences for which the model se-
lected the best possible parse (there can be multiple
fscore err.red. exact CA
% % % %
baseline 74.02 0.00 16.0 73.48
oracle 91.97 100.00 100.0 91.67
standard 87.41 74.60 52.0 87.02
+self-training 87.91 77.38 54.8 87.51
Table 6: Results with ten-fold cross-validation on
the Eindhoven-cdbl part of the Alpino treebank. In
these experiments, the models are used to select a
parse from a given set of atmost 1000 parses per sen-
tence.
best possible parses). The baseline row reports on
the quality of a disambiguation model which simply
selects the first parse for each sentence. The oracle
row reports on the quality of the best-possible dis-
ambiguation model, which would (by magic) always
select the best possible parse (some parses are out-
side the coverage of the system, and some parses are
generated only after more than 1000 inferior parses).
The error reduction column measures which part of
the disambiguation problem (difference between the
baseline and oracle scores) is solved by the model.1
The results show a small but clear increase in
error reduction, if the standard model (without the
association score features) is compared with a (re-
trained) model that includes the association score
features. The relatively large improvement of the ex-
act score suggests that the bilexical preference fea-
tures are particularly good at choosing between very
good parses.
For the second experiment, we evaluate how well
the resulting model performs in the full system. First
of all, this is the only really convincing evalua-
tion which measures progress for the system as a
whole by virtue of including bilexical preferences.
The second motivation for this experiment is for
methodological reasons: we now test on a truly
unseen test-set. The first experiment can be criti-
1Note that the error reduction numbers presented in the ta-
ble are lower than those presented in van Noord and Malouf
(2005). The reason is, that we report here on experiments in
which parses are generated with a version of Alpino with the
POS-tagger switched on. The POS-tagger already reduces the
number of ambiguities, and in particular solves many of the
?easy? cases. The resulting models, however, are more effec-
tive in practice (where the model also is applied after the POS-
tagger).
7
prec rec fscore CA
% % % %
standard 90.77 90.49 90.63 90.32
+self-training 91.19 90.89 91.01 90.73
Table 7: Results on the WR-P-P-H part of the D-Coi
corpus (2267 sentences from the newspaper Trouw,
from 2001). In these experiments, we report on the
full system. In the full system, the disambiguation
model is used to guide a best-first beam-search pro-
cedure which extracts a parse from the parse forest.
Difference in CA was found to be significant (using
paired T-test on the per sentence CA scores).
cized on methodological grounds as follows. The
Alpino Treebank was used to train the disambigua-
tion model which was used to construct the large
parsed treebank from which we extracted the counts
for the association scores. Those scores might some-
how therefore indirectly reflect certain aspects of the
Alpino Treebank training data. Testing on that data
later (with the inclusion of the association scores) is
therefore not sound.
For this second experiment we used the WR-P-P-
H (newspaper) part of the D-Coi corpus. This part
contains 2256 sentences from the newspaper Trouw
(2001). In table 7 we show the resulting f-score and
CA for a system with and without the inclusion of
the z(t, r) features. The improvement found in the
previous experiment is confirmed.
5 Conclusion and Outlook
One might wonder why self-training works in the
case of selection restrictions, at least in the set-up
described above. One may argue that, in order to
learn that milk is a good object for drink, the parser
has to analyse examples of drink milk in the raw data
correctly. But if the parser is capable of analysing
these examples, why does it need selection restric-
tions? The answer appears to be that the parser
(without selection restrictions) is able to analyse the
large majority of cases correctly. These cases in-
clude the many easy occurrences where no (diffi-
cult) ambiguities arise (case marking, number agree-
ment and other syntactic characteristics often force a
single reading). The easy cases outnumber the mis-
parsed difficult cases, and therefore the selection re-
strictions can be learned. Using these selection re-
strictions as additional features, the parser is then
able to also get the difficult, ambiguous, cases right.
There are various aspects of our method that
need further investigation. First of all, existing
techniques that involve selection restrictions (e.g.,
Resnik (1993)) typically assume classes of nouns,
rather than individual nouns. In future work we
hope to generalize our method to take classes into
account, where the aim is to learn class membership
also on the basis of large parsed corpora.
Another aspect of the technique that needs fur-
ther research involves the use of a threshold in estab-
lishing the association score, and perhaps related to
this issue, the incorporation of negative association
scores (for instance for cases where a large number
of cooccurrences of a pair would be expected but
where in fact none or very few were found).
There are also some more practical issues that
perhaps had a negative impact on our results. First,
the large parsed corpus was collected over a period
of about a year, but during that period, the actual
system was not stable. In particular, due to various
improvements of the dictionary, the root form of
words that was used by the system changed over
time. Since we used root forms in the computation
of the association scores, this could be harmful in
some specific cases. A further practical issue con-
cerns repeated sentences or even full paragraphs.
This happens in typical newspaper material for
instance in the case of short descriptions of movies
that may be repeated weekly for as long as that
movie is playing. Pairs of words that occur in
such repeated sentences receive association scores
that are much too high. The method should be
adapted to take this into account, perhaps simply by
removing duplicated sentences.
Clearly, the idea that selection restrictions ought
to be useful for parsing is not new. However, as far
as we know this is the first time that automatically
acquired selection restrictions have been shown to
improve parsing accuracy results.
Acknowledgements
This research was carried out in part in the
context of the D-Coi and Lassy projects.
The D-Coi and Lassy projects are carried
8
out within the STEVIN programme which is
funded by the Dutch and Flemish governments
(http://taalunieversum.org/taal/technologie/stevin/).
References
Takeshi Abekawa and Manabu Okumura. 2006.
Japanese dependency parsing using co-occurrence in-
formation and a combination of case elements. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 833?840, Sydney, Australia, July. Association
for Computational Linguistics.
Gosse Bouma and Geert Kloosterman. 2002. Query-
ing dependency treebanks in XML. In Proceedings of
the Third international conference on Language Re-
sources and Evaluation (LREC), pages 1686?1691,
Gran Canaria, Spain.
Ted Briscoe, John Carroll, Jonathan Graham, and Ann
Copestake. 2002. Relational evaluation schemes.
In Proceedings of the Beyond PARSEVAL Workshop
at the 3rd International Conference on Language Re-
sources and Evaluation, pages 4?8, Las Palmas, Gran
Canaria.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Robert Mario Fano. 1961. Transmission of Information:
A Statistical Theory of Communications. MIT Press,
Cambridge, MA.
Heleen Hoekstra, Michael Moortgat, Bram Renmans,
Machteld Schouppe, Ineke Schuurman, and Ton
van der Wouden, 2003. CGN Syntactische Annotatie,
December.
Mark Johnson and Stefan Riezler. 2000. Exploiting
auxiliary distributions in stochastic unification-based
grammars. In Proceedings of the first conference on
North American chapter of the Association for Com-
putational Linguistics, pages 154?161, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Daisuke Kawahara and Sadao Kurohashi. 2006. A fully-
lexicalized probabilistic model for japanese syntactic
and case structure analysis. In Proceedings of the main
conference on Human Language Technology Confer-
ence of the North American Chapter of the Association
of Computational Linguistics, pages 176?183, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, Main Conference, pages 152?159, New
York City, USA, June. Association for Computational
Linguistics.
Robbert Prins. 2005. Finite-State Pre-Processing for
Natural Language Analysis. Ph.D. thesis, University
of Groningen.
Philip Stuart Resnik. 1993. Selection and information:
a class-based approach to lexical relationships. Ph.D.
thesis, University of Pennsylvania, Philadelphia, PA,
USA.
Gertjan van Noord and Robert Malouf. 2005.
Wide coverage parsing with stochastic at-
tribute value grammars. Draft available from
http://www.let.rug.nl/?vannoord. A preliminary ver-
sion of this paper was published in the Proceedings
of the IJCNLP workshop Beyond Shallow Analyses,
Hainan China, 2004.
Gertjan van Noord, Ineke Schuurman, and Vincent Van-
deghinste. 2006. Syntactic annotation of large cor-
pora in STEVIN. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC), Genoa, Italy.
Gertjan van Noord. 2006. At Last Parsing Is Now
Operational. In TALN 2006 Verbum Ex Machina,
Actes De La 13e Conference sur Le Traitement Au-
tomatique des Langues naturelles, pages 20?42, Leu-
ven.
Examples
Here we list a number of examples, which suggest
that selection restrictions can also be important for
dependencies, other than direct objects.
High scoring pairs involving a subject relation-
ship with a verb:
alarmbel rinkel
champagnekurk knal
gij echtbreek
haan kraai
kikker kwaak
rups verpop
vonk overspring
zweet parel
belletje rinkel
brievenbus klepper
High scoring pairs involving a modifier relation-
ship with a noun:
9
in vitro fertilisatie
Hubble ruimtetelescoop
zelfrijzend bakmeel
bezittelijk voornaamwoord
ingegroeid teennagel
knapperend haardvuur
levendbarend hagedis
onbevlekt ontvangenis
ongeblust kalk
High scoring pairs involving a predicative com-
plement relationship with a verb:
beetgaar kook
beuk murw
schuimig klop
suf peins
suf pieker
doormidden scheur
ragfijn hak
stuk bijt
au serieux neem
in duigen val
lam leg
High scoring pairs involving an apposition rela-
tionship with a noun:
jongensgroep Boyzone
communicatiesysteem C2000
blindeninstituut De Steffenberg
haptonoom Ted Troost
gebedsgenezeres Greet Hofmans
rally Parijs-Dakar
tovenaar Gandalf
aartsengel Gabriel
keeperstrainer Joep Hiele
basketbalcoach Ton Boot
partizaan Tito
High scoring pairs involving a measure phrase re-
lationship with an adjective:
graadje erger
lichtjaar verwijderd
mijlenver verwijderd
niets liever
eindje verderop
graad warmer
illusie armer
kilogram wegend
onsje minder
maatje te groot
knip waard
10
Proceedings of the 10th Conference on Parsing Technologies, pages 36?38,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
The Impact of Deep Linguistic Processing on Parsing Technology
Timothy Baldwin
University of Melbourne
tim@csse.unimelb.edu.au
Mark Dras
Macquarie University
madras@ics.mq.edu.au
Julia Hockenmaier
University of Pennsylvania
juliahr@cis.upenn.edu
Tracy Holloway King
PARC
thking@parc.com
Gertjan van Noord
University of Groningen
vannoord@let.rug.nl
Abstract
As the organizers of the ACL 2007 Deep
Linguistic Processing workshop (Baldwin et
al., 2007), we were asked to discuss our per-
spectives on the role of current trends in
deep linguistic processing for parsing tech-
nology. We are particularly interested in
the ways in which efficient, broad coverage
parsing systems for linguistically expressive
grammars can be built and integrated into
applications which require richer syntactic
structures than shallow approaches can pro-
vide. This often requires hybrid technolo-
gies which use shallow or statistical methods
for pre- or post-processing, to extend cover-
age, or to disambiguate the output.
1 Introduction
Our talk will provide a view on the relevance of deep
linguistic processing for parsing technologies from
the perspective of the organizers of the ACL 2007
Workshop on Deep Linguistic Processing (Baldwin
et al, 2007). The workshop was conceived with the
broader aim of bringing together the different com-
putational linguistic sub-communities which model
language predominantly by way of theoretical syn-
tax, either in the form of a particular theory (e.g.
CCG, HPSG, LFG, TAG, the Prague School) or a
more general framework which draws on theoretical
and descriptive linguistics. These ?deep linguistic
processing? approaches differ from shallower meth-
ods in that they yield richer, more expressive, struc-
tural representations which capture long-distance
dependencies or the underlying predicate-argument
structure directly.
Aspects of this research have often had their own
separate fora, such as the ACL 2005 workshop on
deep lexical acquisition (Baldwin et al, 2005), as
well as the TAG+ (Kallmeyer and Becker, 2006),
Alpino (van der Beek et al, 2005), ParGram (Butt
et al, 2002) and DELPH-IN (Oepen et al, 2002)
projects and meetings. However, the fundamental
approaches to building a linguistically-founded sys-
tem and many of the techniques used to engineer
efficient systems are common across these projects
and independent of the specific grammar formal-
ism chosen. As such, we felt the need for a com-
mon meeting in which experiences could be shared
among a wider community, similar to the role played
by recent meetings on grammar engineering (Wint-
ner, 2006; Bender and King, 2007).
2 The promise of deep parsing
Deep linguistic processing has traditionally been
concerned with grammar development (for use in
both parsing and generation). However, the linguis-
tic precision and complexity of the grammars meant
that they had to be manually developed and main-
tained, and were computationally expensive to run.
In recent years, machine learning approaches
have fundamentally altered the field of natural lan-
guage processing. The availability of large, manu-
ally annotated, treebanks (which typically take years
of prior linguistic groundwork to produce) enabled
the rapid creation of robust, wide-coverage parsers.
However, the standard evaluation metrics for which
such parsers have been optimized generally ignore
36
much of the rich linguistic information in the orig-
inal treebanks. It is therefore perhaps only natural
that deep processing methods, which often require
substantial amounts of manual labor, have received
considerably less attention during this period.
But even if further work is required for deep
processing techniques to fully mature, we believe
that applications that require natural language under-
standing or inference, among others, will ultimately
need detailed syntactic representations (capturing,
e.g., bounded and unbounded long-range dependen-
cies) from which semantic interpretations can eas-
ily be built. There is already some evidence that
our current deep techniques can, in some cases, out-
perform shallow approaches. There has been work
demonstrating this in question answering, targeted
information extraction and the recent textual entail-
ment recognition task, and perhaps most notably in
machine translation: in this latter field, after a period
of little use of linguistic knowledge, deeper tech-
niques are beginning to lead to better performance,
e.g. by redefining phrases by syntactic ?treelets?
rather than contiguous word sequences, or by explic-
itly including a syntactic component in the probabil-
ity model, or by syntactic preprocessing of the data.
3 Closing the divide
In the past few years, the divide between ?deep?,
rule-based, methods and ?shallow?, statistical, ap-
proaches, has begun to close from both sides. Re-
cent advances in using the same treebanks that have
advanced shallow techniques to extract more expres-
sive grammars or to train statistical disambiguators
for them, and in developing framework-specific tree-
banks, have made it possible to obtain similar cov-
erage, robustness, and disambiguation accuracy for
parsers that use richer structural representations. As
witnessed by many of the papers in our workshop
(Baldwin et al, 2007), a large proportion of current
deep systems have statistical components to them,
e.g., as pre- or post-processing to control ambigu-
ity, as means of acquiring and extending lexical re-
sources, or even use machine learning techniques
to acquire deep grammars automatically. From the
other side of the divide, many of the purely statistical
approaches are using progressively richer linguistic
features and are taking advantage of these more ex-
pressive features to tackle problems that were tradi-
tionally thought to require deep systems, such as the
recovery of traces or semantic roles.
4 The continued need for research on deep
processing
Although statistical techniques are becoming com-
monplace even for systems built around hand-
written grammars, there is still a need for further
linguistic research and manual grammar develop-
ment. For example, supervised machine-learning
approaches rely on large amounts of manually anno-
tated data. Where such data are available, develop-
ers of deep parsers and grammars can exploit them
to determine frequency of certain constructions, to
bootstrap gold standards for their systems, and to
provide training data for the statistical components
of their systems such as parse disambiguators. But
for the majority of the world?s languages, and even
for many languages with large numbers of speakers,
such corpora are unavailable. Under these circum-
stances, manual grammar development is unavoid-
able, and recent progress has allowed the underlying
systems to become increasingly better engineered,
allowing for more rapid development of any given
grammar, as well as for overlay grammars that adapt
to particular domains and applications and for port-
ing of grammars from one language to another.
Despite recent work on (mostly dependency
grammar-based) multilingual parsing, it is still the
case that most research on statistical parsing is done
on English, a fixed word-order language where sim-
ple context-free approximations are often sufficient.
It is unclear whether our current models and al-
gorithms carry over to morphologically richer lan-
guages with more flexible word order, and it is possi-
ble that the more complex structural representations
allowed by expressive formalisms will cease to re-
main a luxury.
Further research is required on all aspects of
deep linguistic processing, including novel linguis-
tic analyses and implementations for different lan-
guages, formal comparisons of different frame-
works, efficient parse and learning algorithms, better
statistical models, innovative uses of existing data
resources, and new evaluation tools and methodolo-
gies. We were fortunate to receive so many high-
37
quality submissions on all of these topics for our
workshop.
5 Conclusion and outlook
Deep linguistic processing brings together a range of
perspectives. It covers current approaches to gram-
mar development and issues of theoretical linguis-
tic and algorithmic properties, as well as the appli-
cation of deep linguistic techniques to large-scale
applications such as question answering and dialog
systems. Having industrial-scale, efficient parsers
and generators opens up new application domains
for natural language processing, as well as inter-
esting new ways in which to approach existing ap-
plications, e.g., by combining statistical and deep
processing techniques in a triage process to pro-
cess massive data quickly and accurately at a fine
level of detail. Notably, several of the papers ad-
dressed the relationship of deep linguistic process-
ing to topical statistical approaches, in particular in
the area of parsing. There is an increasing inter-
est in deep linguistic processing, an interest which
is buoyed by the realization that new, often hybrid,
techniques combined with highly engineered parsers
and generators and state-of-the-art machines opens
the way towards practical, real-world application of
this research. We look forward to further opportu-
nities for the different computational linguistic sub-
communities who took part in this workshop, and
others, to continue to come together in the future.
References
Timothy Baldwin, Anna Korhonen, and Aline Villavicen-
cio, editors. 2005. Proceedings of the ACL-SIGLEX
Workshop on Deep Lexical Acquisition. Ann Arbor,
USA.
Timothy Baldwin, Mark Dras, Julia Hockenmaier,
Tracy Holloway King, and Gertjan van Noord, editors.
2007. Proceedings of the ACL Workshop on Deep Lin-
guistic Processing, Prague, Czech Republic.
Emily Bender and Tracy Holloway King, editors. 2007.
Grammar Engineering Across Frameworks, Stanford
University. CSLI On-line Publications. to appear.
Miriam Butt, Helge Dyvik, T. H. King, Hiroshi Masuichi,
and Christian Rohrer. 2002. The parallel grammar
project. In COLING Workshop on Grammar Engi-
neering and Evaluation, Taipei, Taiwan.
Laura Kallmeyer and Tilman Becker, editors. 2006. Pro-
ceedings of the Eighth International Workshop on Tree
Adjoining Grammar and Related Formalisms (TAG+),
Sydney, Australia.
Stephan Oepen, Dan Flickinger, J. Tsujii, and Hand
Uszkoreit, editors. 2002. Collaborative Language En-
gineering: A Case Study in Efficient Grammar-based
Processing. CSLI Publications.
Leonoor van der Beek, Gosse Bouma, Jan Daciuk, Tanja
Gaustad, Robert Malouf, Mark-Jan Nederhof, Gert-
jan van Noord, Robbert Prins, and Bego na Vil-
lada Moiro?n. 2005. Algorithms for linguistic pro-
cessing. NWO Pionier final report. Technical report,
University of Groningen.
Shuly Wintner. 2006. Large-scale grammar development
and grammar engineering. Research workshop of the
Israel Science Foundation.
38
Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics, pages 33?39,
Athens, Greece, 30 March, 2009. c?2009 Association for Computational Linguistics
Parsed Corpora for Linguistics
Gertjan van Noord
University of Groningen
G.J.M.van.noord@rug.nl
Gosse Bouma
University of Groningen
G.Bouma@rug.nl
Abstract
Knowledge-based parsers are now accu-
rate, fast and robust enough to be used to
obtain syntactic annotations for very large
corpora fully automatically. We argue that
such parsed corpora are an interesting new
resource for linguists. The argument is
illustrated by means of a number of re-
cent results which were established with
the help of parsed corpora.
1 Introduction
Once upon a time, knowledge-based parsers were
slow, inaccurate and fragile. This is no longer
true. In the last decade, enormous improvements
have been achieved in this area. Parsers based on
constraint-based formalisms such as HPSG, LFG,
and CCG are now fast enough for many appli-
cations; they are robust; and they perform much
more accurately than previously by incorporat-
ing, typically, a statistical disambiguation compo-
nent. As a consequence, such parsers now obtain
competitive, if not superior, performance. Zae-
nen (2004), for instance, points out that the (LFG-
based) XLE parser is fast, has a statistical disam-
biguation component, and is robust, and thus al-
lows full parsing to be incorporated in many appli-
cations. Clark and Curran (2007) show that both
accurate and highly efficient parsing is possible
using a CCG.
As a consequence of this development, massive
amounts of parsed sentences now become avail-
able. Such large collections of syntactically an-
notated but not manually verified syntactic analy-
ses are a very useful resource for many purposes.
In this position paper we focus on one purpose:
linguistic analysis. Our claim is, that very large
parsed corpora are an important resource for lin-
guists. Such very large parsed corpora can be
used to search systematically for specific infre-
quent syntactic configurations of interest, and also
to obtain quantitative data about specific syntac-
tic configurations. Although parsed corpora obvi-
ously contain a certain amount of noise, for many
applications the abundant size of these corpora
compensates for this.
In this paper, we illustrate our position by a nu-
mer of recent linguistic studies in which very large
corpora of Dutch have been employed, which
were syntactically annotated by the freely avail-
able Alpino parser (Bouma et al, 2001; van No-
ord, 2006).
The Alpino system incorporates a linguistically
motivated, wide-coverage grammar for Dutch in
the tradition of HPSG. It consists of over 800
grammar rules and a large lexicon of over 300,000
lexemes (including very many person names, geo-
graphical names, and organization names) and var-
ious rules to recognize special constructs such as
named entities, temporal expressions, etc. Since
we use Alpino to parse large amounts of data, it
is crucial that the parser is capable to treat sen-
tences with unknown words. A large set of heuris-
tics have been implemented carefully to deal with
unknown words and word sequences.
Based on the categories assigned to words, and
the set of grammar rules compiled from the HPSG
grammar, a left-corner parser finds the set of all
parses, and stores this set compactly in a packed
parse forest. All parses are rooted by an instance
of the top category, which is a category that gen-
eralizes over all maximal projections (S, NP, VP,
ADVP, AP, PP and some others). If there is no
parse covering the complete input, the parser finds
all parses for each substring. In such cases, the
robustness component will then select the best se-
quence of non-overlapping parses (i.e., maximal
projections) from this set.
In order to select the best parse from the parse
forest, a best-first search algorithm is applied. The
algorithm consults a Maximum Entropy disam-
biguation model to judge the quality of (partial)
33
parses. The disambiguation model is trained on
the manually verified Alpino treebank (about 7100
sentences from newspaper texts).
Although Alpino is not a dependency grammar
in the traditional sense, dependency structures are
generated by the lexicon and grammar rules as
the value of a dedicated feature. The dependency
structures are based on CGN (Corpus Gesproken
Nederlands, Corpus of Spoken Dutch) (Hoekstra
et al, 2003), D-Coi and LASSY (van Noord et al,
2006).
Dependency structures are stored in XML. Ad-
vantages of the use of XML include the avail-
ability of general purpose search and visualiza-
tion software. For instance, we exploit XPATH
(standard XML query language) to search in large
sets of dependency structures, and Xquery to ex-
tract information from such large sets of depen-
dency structures (Bouma and Kloosterman, 2002;
Bouma and Kloosterman, 2007).
2 Extraposition of comparative objects
out of topic
The first illustration of our thesis that parsed cor-
pora provide an interesting new resource for lin-
guists, constitutes more of an anecdote than a sys-
tematic study. We include the example, presented
earlier in van Noord (2009), because it is fairly
easy to explain, and because it was how we be-
came aware ourselves of the potential of parsed
corpora for the purpose of linguistics.
In van der Beek et al (2002), the grammar un-
derlying the Alpino parser is presented in some de-
tail. As an example of how the various specific
rules of the grammar interact with the more gen-
eral principles, the analysis of comparatives and
the interaction with generic principles for (right-
ward) extraposition is illustrated. In short, com-
paratives such as comparative adjectives and the
adverb anders as in the following example (1)
license corresponding comparative phrases (such
as phrases headed by dan (than)) by means of a
feature which percolates according to the extrapo-
sition principle. The analysis is illustrated in fig-
ure 1.
(1) . . . niks
. . . nothing
anders
else
doen
do
dan
than
almaar
continuously
ruw
raw
materiaal
material
verzamelen
collect
do nothing else but collect raw material (cdbl-
7)
Figure 2: Dependency structure for Lager was de
koers dan gisteren
An anonymous reviewer criticized the anal-
ysis, because the extraposition principle would
also allow the rightward extraction of comparative
phrases licensed by comparatives in topic position.
The extraposition principle would have to allow
for this in the light of examples such as
(2) De
The
vraag
question
is
is
gerechtvaardigd
justified
waarom
why
de
the
regering
government
niets
nothing
doet
does
The question is justified why the goverment
does not act
However, the reviewer claimed that comparative
phrases cannot be extraposed out of topic, as ex-
amples such as the following indicate:
(3) ?Lager
Lower
was
was
de
the
koers
rate
dan
than
gisteren
yesterday
The rate never was lower than yesterday
Since the Alpino grammar allows such cases, it
is possible to investigate if genuine examples of
this type occur in parsed corpora. In order to un-
derstand how we can specify a search query for
such cases, it is instructive to consider the de-
pendency structure assigned to such examples in
figure 2. As can be observed in the dependency
graph, the left-right order of nodes does not rep-
resent the left-right ordering in the sentence. The
word-order of words and phrases is indicated with
XML attributes begin and end (not shown in fig-
ure 2) which indicate for each node the begin and
end position in the sentence respectively.
The following XPATH query enumerates all ex-
34
vproj[extra ? ??
vproj[extra ?ompp[dan???
np[extra ?ompp[dan???
n
niks
adj-s[extra ?ompp[dan???
anders
v
doen
ompp[dan?
omp[dan?
dan
sbar
almaar ... verzamelen
Figure 1: Derivation of extraposed comparative object
amples of extraposition of comparative phrases
out of topic. We can then inspect the resulting list
to check whether the examples are genuine.
//node[
@cat="smain"
and
node[
node[@rel="obcomp"]/@end
>
../node[@rel="hd"]/@begin
]/@begin = @begin
]
The query can be read as: find root sentences
in which there is a daughter node, which itself has
a daughter node with relation label obcomp (the
label used for comparative complements). The
daughter node should begin at the same position
as the root sentence. Finally, the end position of
the obcomp node must be larger than the end po-
sition of the head of the root sentence (i.e. the finite
verb).
In addition to many mis-parsed sentences, we
found quite a few genuine cases. A mis-parse
can for instance occur if a sentence contains two
potential licensers for the comparative phrase, as
in the following example in which verder can be
wrongly analysed as a comparative adjective.
(4) Verder
Further
wil
want
ik
I
dat
that
mijn
my
backhand
backhand
even
just-as
goed
good
wordt
becomes
als
as
mijn
my
forehand
forehand
Furthermore, I want my backhand to become
as good as my forehand
More interestingly for the present discussion are
the examples which were parsed correctly. Not
only do we find such examples, but informants
agree that nothing is wrong with such cases. Some
examples are listed in figure 3. It is striking that
many examples involve the comparative adjectives
liever and eerder. Also, the list involves exam-
ples where adverbials such as zo, zozeer, zoveel are
related with an extraposed subordinate sentence
headed by dat which according to the annotation
guidelines are also treated as comparative comple-
ments.
The examples show that at least in some cases,
the possibility of extraposition of comparative
complements out of topic must be allowed; we hy-
pothesize that the acceptability of such cases is not
a binary decision, but rather a preference which
depends on the choice of comparative on the one
hand, and the heaviness of the comparative com-
plement on the other hand.
For the purpose of this paper, we hope to have
illustrated how parsed corpora can be helpful to
find new empirical evidence for fairly complicated
and suble linguistic issues. Note that for a con-
struction of this type, manually verified treebanks
are much too small. We estimated that it takes
about 5 million words to find a single, good, ex-
ample. It appears unrealistic to assume that tree-
banks of the required order of magnitude of tens
of millions of words will become available soon.
3 Frequency versus Complexity
Our second illustration is of a different nature, and
taken from a study related to agrammatic Broca?s
aphasia.
In Bastiaanse et al (to appear), potential causes
are discussed of the problems that patients suffer-
ing from agrammatic Broca?s aphasia encounter.
The Derived Order Problem Hypothesis (Basti-
aanse and van Zonneveld, 2005) assumes that the
linguistic representations of agrammatic patients
are intact, but due to processing disorders, some
representations are harder to retrieve than oth-
35
(5) Liever
Rather
betaalden
paid
werkgevers
employers
een
a
(
(
hoge
high
)
)
verzekeringspremie
insurance-fee
,
,
dan
than
opgescheept
left
te
to
zitten
be
met
with
niet
not
volwaardig
fully
functionerende
functioning
medewerkers
employees
Rather, employers pay a high insurance fee, than be left with not fully functioning employees (Alge-
meen Dagblad, January 15, 1999)
(6) Beter
Better
is
is
het
it
te
to
zorgen
ensure
dat
that
ziekenhuizen
hospitals
hun
their
verplichtingen
obligations
volgens
according-to
de
the
huidige
current
BOPZ
BOPZ
gaan
start
nakomen
meet
,
,
dan
than
de
the
rechten
rights
van
of
pati???12 ten
patients
nog
yet
verder
further
aan
PART
te
to
tasten
violate
It is better to ensure that hospitals start to meet their obligations according to the current BOZP,
than to violate rights of patients even further (Algemeen Dagblad, August 18, 2001)
(7) Dus
So
wat
what
anders
else
konden
could
de
the
LPF?ers
LPF-representatives
de
the
afgelopen
last
week
week
dan
than
zich
self
stil
quiet
houden
keep
?
?
What else could the LPF-representatives do last week , than keep quiet? (Volkskrant June 1, 2002)
(8) Sneller
Faster
kennen
know
ze
they
hun
their
tafels
tables
van
of
vermenigvuldiging
multiplication
dan
than
de
the
handelingen
acts
van
of
de
the
groet
greeting
They know the tables of multiplication faster than the acts of greeting (De Morgen March 27, 2006)
Figure 3: Some genuine examples of extraposition of comparative objects from topic. The examples are
identified automatically using an XPATH query applied to a large parsed corpus.
ers, due to differences in linguistic complexity.
This hypothesis thus assumes that agrammatic pa-
tients have difficulty with constructions of higher
linguistic complexity. An alternative hypothesis
states, that agrammatic patients have more diffi-
culty with linguistic constructions of lower fre-
quency.
In order to compare the two hypotheses, Bas-
tiaanse et al perform three corpus studies. In
three earlier experimental studies it was found that
agrammatic patients have more difficulty with (a)
finite verbs in verb-second position versus finite
verbs in verb-final position; (b) scrambled direct
objects versus non-scrambled direct objects; and
(c) transitive verbs used as unaccusative versus
transitive verbs used as transitive.
The three pairs of constructions are illustrated
as follows.
(9) a. de
the
jongen
boy
die
who
een
a
boek
book
leest
reads
the boy who reads a book
b. de
the
jongen
boy
leest
reads
een
a
boek
book
the boy reads a book
(10) a. dit
this
is
is
de
the
jongen
boy
die
who
vandaag
today
het
the
boek
book
leest
reads
this is the boy who reads the book today
b. dit
this
is
is
de
the
jongen
boy
die
who
het
the
boek
book
vandaag
today
leest
reads
this is the boy who reads the book today
(11) a. de
the
jongen
boy
breekt
breaks
het
the
glas
glass
the boy breaks the glass
b. het
the
glas
glass
breekt
breaks
the glass breaks
In each of the three cases, corpus data is used
to estimate the frequency of both syntactic con-
figurations. Two corpora were used: the manu-
ally verified syntactically annotated CGN corpus
(spoken language, approx. 1M words), and the the
automatically parsed TwNC corpus (Ordelman et
al., 2007) (the newspapers up to 2001, a parsed
corpus of 300 million words). For the first two
experiments, manual inspection revealed that the
parsed corpus material was of high enough quality
to be used directly. Furthermore, the relevant con-
structions are highly frequent, and thus even rela-
tively small corpora (such as the syntactically an-
36
notated part of CGN) provide sufficient data. For
the third experiment (unaccusative versus transi-
tive usage of verbs), an additional layer of manual
verification was used, and furthermore, as the sub-
categorization frequencies of individual verbs are
estimated, the full TwNC was searched in order to
obtain reasonably reliable estimates.
The outcome of the three experiments was the
same in each case: frequency information cannot
explain the difficulty encountered by agrammatic
patients. Verb-second is more frequent than verb-
final word order for lexical verbs and transitive
lexical verbs (the verbs used in the experiments
were all transitive). Finite verbs occur slightly
more often in verb-second position than in verb-
final position, but the difference is quite small.
Scrambled word order is more frequent than the
basic word order. The difference between the two
corpora (CGN and TwNC) is quite small in both
cases. Figure 4 gives an overview of the number
of occurrences of the transitive and unaccusative
use of the verbs used in the experiments in the
full TwNC. The data suggest that the relative fre-
quency of unaccusative depends strongly on the
verb, but that it is not in general the case that the
unaccusative use is less frequent than the transitive
use.
The three ?difficult? constructions used in the
experiments with aphasia patients are by no means
infrequent in Dutch. The authors conclude that the
hypothesis that processing difficulties are corre-
lated with higher linguistic complexity cannot be
falsified by an appeal to frequency.
What is interesting for the purposes of the cur-
rent paper, is that parsed corpora are used to es-
timate frequencies of syntactic constructions, and
that these are used to support claims about the role
of linguistic complexity in processing difficulties
of aphasia patients. Also note that figure 4 shows
that even in a large (300M word) corpus, the num-
ber of occurrences of a specific verb used with a
specific valency frame can be quite small. Thus,
it is unlikely that reliable frequency estimates can
be obtained for these cases from manually verified
treebanks.
Roland et al (2007) report on closely related
work for English. In particular, they give fre-
quency counts for a range of syntactic construc-
tions in English, and subcategorization frequen-
cies for specific verbs. They demonstrate that
these frequencies are highly dependent on corpus
and genre in a number of cases. They use their data
to verify claims in the psycholinguistic literature
about the processing of subject vs. object clefts,
relative clauses and sentential complements.
4 The distribution of zelf and zichzelf
As a further example of the use of parsed corpora
to further linguistic insights, we consider a recent
study (Bouma and Spenader, 2009) of the distribu-
tion of weak and strong reflexive objects in Dutch.
If a verb is used reflexively in Dutch, two forms
of the reflexive pronoun are available. This is il-
lustrated for the third person form in the examples
below.
(12) Brouwers
Brouwers
schaamt
shames
zich/?zichzelf
self1/self2
voor
for
zijn
his
schrijverschap.
writing
Brouwers is ashamed of his writing
(13) Duitsland
Germany
volgt
follows
?zich/zichzelf
self1/self2
niet
not
op
PART
als
as
Europees
European
kampioen.
Champion
Germany does not succeed itself as Euro-
pean champion
(14) Wie
Who
zich/zichzelf
self1/self2
niet
not
juist
properly
introduceert,
introduces,
valt
is
af.
out
Everyone who does not introduce himself
properly, is out.
The choice between zich and zichzelf depends on
the verb. Generally three groups of verbs are
distinguished. Inherent reflexives are claimed to
never occur with a non-reflexive argument, and as
a reflexive argument are claimed to use zich exclu-
sively, (12). Non-reflexive verbs seldom, if ever
occur with a reflexive argument. If they do how-
ever, they can only take zichzelf as a reflexive ar-
gument (13). Accidental reflexives can be used
with both zich and zichzelf, (14). Accidental re-
flexive verbs vary widely as to the frequency with
which they occur with both arguments. Bouma
and Spenader (2009) set out to explain this dis-
tribution.
The influential theory of Reinhart and Reuland
(1993) explains the distribution as the surface real-
ization of two different ways of reflexive coding.
An accidental reflexive that can be realized with
37
verb unacc trans
# % # %
luiden to ring/sound 269 26.6 743 73.4
scheuren to rip 332 28.8 819 71.2
breken to break 1969 31.2 4341 68.8
verbrand to burn 479 43.5 623 56.5
oplossen to (dis)solve 296 59.2 204 40.8
draaien to turn 2709 59.4 1852 40.6
smelten to melt 723 71.4 290 28.6
rollen to roll 3500 93.5 244 6.5
verdrinken to drown 1397 94.6 80 5.4
stuiteren to bounce 334 97.9 7 2.1
Figure 4: Estimated number of occurrences in TwNC of unaccusative and transitive uses of Dutch verbs
which may undergo the causative alternation
both zich and zichzelf is actually ambiguous be-
tween an inherent reflexive and an accidental re-
flexive (which always is realized with zichzelf).
An alternative approach is that of Haspelmath
(2004), Smits et al (2007), and Hendriks et al
(2008), who have claimed that the distribution of
weak vs. strong reflexive object pronouns corre-
lates with the proportion of events described by
the verb that are self-directed vs. other-directed.
In the course of this investigation, a first inter-
esting observation is, that many inherently reflex-
ive verbs, which are claimed not to occur with
zichzelf, actually often do combine with this pro-
noun. Here are a number of examples (simplified
for expository purposes):
(15) Nederland
Netherlands
moet
must
stoppen
stop
zichzelf
self2
op
on
de
the
borst
chest
te
to
slaan
beat
The Netherlands must stop beating itself on
the chest
(16) Hunze
Hunze
wil
want
zichzelf
self2
niet
not
al
all
te
too
zeer
much
op
on
de
the
borst
chest
kloppen
knock
Hunze doesn?t want to knock itself on the
chest too much
(17) Ze
They
verloren
lost
zichzelf
self2
soms
sometimes
in
in
het
tactical
gegoochel
variants
met alerlei tactische varianten
They sometimes lost themselves in tactical
variants
With regards to the main hypothesis of their
study, (Bouma and Spenader, 2009) use linear re-
gression to determine the correlation between re-
flexive use of a (non-inherently reflexive) verb and
the relative preference for a weak or strong re-
flexive pronoun. Frequency counts are collected
from the parsed TwNC corpus (almost 500 mil-
lion words). They limit the analysis to verbs that
occur at least 10 times with a reflexive meaning
and at least 50 times in total, distinguishing uses
by subcategorization frames. The statistical analy-
sis shows a significant correlation, which accounts
for 30% of the variance of the ratio of nonreflexive
over reflexive uses.
5 Conclusion
Knowledge-based parsers are now accurate, fast
and robust enough to be used to obtain syntactic
annotations for very large corpora fully automati-
cally. We argued that such parsed corpora are an
interesting new resource for linguists. The argu-
ment is illustrated by means of a number of re-
cent results which were established with the help
of huge parsed corpora.
Huge parsed corpora are especially crucial (1)
to obtain evidence concerning infrequent syntac-
tic configurations, and (2) to obtain more reliable
quantitative data about particular syntactic config-
urations.
Acknowledgments
This research was carried out in part in the
context of the STEVIN programme which is
funded by the Dutch and Flemish governments
38
(http://taalunieversum.org/taal/technologie/stevin/).
References
Roelien Bastiaanse and Ron van Zonneveld. 2005.
Sentence production with verbs of alternating tran-
sitivity in agrammatic Broca?s aphasia. Journal of
Neurolinguistics, 18(1):57?66, January.
Roelien Bastiaanse, Gosse Bouma, and Wendy Post.
to appear. Frequency and linguistic complexity in
agrammatic speech production. Brain and Lan-
guage.
Gosse Bouma and Geert Kloosterman. 2002. Query-
ing dependency treebanks in XML. In Proceedings
of the Third international conference on Language
Resources and Evaluation (LREC), pages 1686?
1691, Gran Canaria, Spain.
Gosse Bouma and Geert Kloosterman. 2007. Mining
syntactically annotated corpora using XQuery. In
Proceedings of the Linguistic Annotation Workshop,
Prague, June. ACL.
Gosse Bouma and Jennifer Spenader. 2009. The distri-
bution of weak and strong object reflexives in Dutch.
In Frank van Eynde, Anette Frank, Koenraad De
Smedt, and Gertjan van Noord, editors, Proceed-
ings of the Seventh International Workshop on Tree-
banks and Linguistic Theories (TLT 7), number 12
in LOT Occasional Series, pages 103?114, Utrecht,
The Netherlands. Netherlands Graduate School of
Linguistics.
Gosse Bouma, Gertjan van Noord, and Robert Mal-
ouf. 2001. Wide coverage computational analysis
of Dutch. InW. Daelemans, K. Sima?an, J. Veenstra,
and J. Zavrel, editors, Computational Linguistics in
the Netherlands 2000.
S. Clark and J.R. Curran. 2007. Wide-Coverage Effi-
cient Statistical Parsing with CCG and Log-Linear
Models. Computational Linguistics, 33(4):493?
552.
Martin Haspelmath. 2004. A frequentist explanation
of some universals of reflexive marking. Draft of a
paper presented at the Workshop on Reciprocals and
Reflexives, Berlin.
Petra Hendriks, Jennifer Spenader, and Erik-Jan Smits.
2008. Frequency-based constraints on reflexive
forms in Dutch. In Proceedings of the 5th Interna-
tional Workshop on Constraints and Language Pro-
cessing, pages 33?47, Roskilde, Denmark.
Heleen Hoekstra, Michael Moortgat, Bram Renmans,
Machteld Schouppe, Ineke Schuurman, and Ton
van der Wouden, 2003. CGN Syntactische Anno-
tatie, December.
Roeland Ordelman, Franciska de Jong, Arjan van Hes-
sen, and Hendri Hondorp. 2007. TwNC: a mul-
tifaceted Dutch news corpus. ELRA Newsletter,
12(3/4):4?7.
Tanya Reinhart and Eric Reuland. 1993. Reflexivity.
Linguistic Inquiry, 24:656?720.
Douglas Roland, Frederic Dick, and Jeffrey L. El-
man. 2007. Frequency of basic english grammatical
structures: A corpus analysis. Journal of Memory
and Language, 57(3):348?379, October.
Erik-Jan Smits, Petra Hendriks, and Jennifer Spenader.
2007. Using very large parsed corpora and judge-
ment data to classify verb reflexivity. In Antonio
Branco, editor, Anaphora: Analysis, Algorithms and
Applications, pages 77?93, Berlin. Springer.
Leonoor van der Beek, Gosse Bouma, and Gertjan van
Noord. 2002. Een brede computationele grammat-
ica voor het Nederlands. Nederlandse Taalkunde,
7(4):353?374.
Gertjan van Noord, Ineke Schuurman, and Vincent
Vandeghinste. 2006. Syntactic annotation of large
corpora in STEVIN. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (LREC), Genoa, Italy.
Gertjan van Noord. 2006. At Last Parsing Is Now
Operational. In TALN 2006 Verbum Ex Machina,
Actes De La 13e Conference sur Le Traitement
Automatique des Langues naturelles, pages 20?42,
Leuven.
Gertjan van Noord. 2009. Huge parsed corpora in
Lassy. In Frank van Eynde, Anette Frank, Koen-
raad De Smedt, and Gertjan van Noord, editors, Pro-
ceedings of the Seventh International Workshop on
Treebanks and Linguistic Theories (TLT 7), num-
ber 12 in LOT Occasional Series, pages 115?126,
Utrecht, The Netherlands. Netherlands Graduate
School of Linguistics.
Annie Zaenen. 2004. but full parsing is impossible.
ELSNEWS, 13(2):9?10.
39
Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 71?79,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
A generalized method for iterative error mining in parsing results
Danie?l de Kok
University of Groningen
d.j.a.de.kok@rug.nl
Jianqiang Ma
University of Groningen
j.ma@student.rug.nl
Gertjan van Noord
University of Groningen
g.j.m.van.noord@rug.nl
Abstract
Error mining is a useful technique for
identifying forms that cause incomplete
parses of sentences. We extend the iter-
ative method of Sagot and de la Clerg-
erie (2006) to treat n-grams of an arbi-
trary length. An inherent problem of in-
corporating longer n-grams is data sparse-
ness. Our new method takes sparseness
into account, producing n-grams that are
as long as necessary to identify problem-
atic forms, but not longer.
Not every cause for parsing errors can be
captured effectively by looking at word
n-grams. We report on an algorithm for
building more general patterns for min-
ing, consisting of words and part of speech
tags.
It is not easy to evaluate the various er-
ror mining techniques. We propose a new
evaluation metric which will enable us to
compare different error miners.
1 Introduction
In the past decade wide-coverage grammars and
parsers have been developed for various lan-
guages, such as the Alpino parser and grammar
(Bouma et al, 2001) for Dutch and the English
Resource Grammar (Copestake and Flickinger,
2000). Such grammars account for a large num-
ber of grammatical and lexical phenomena, and
achieve high accuracies. Still, they are usually
tailored to general domain texts and fail to reach
the same accuracy for domain-specific texts, due
to missing lexicon entries, fixed expressions, and
grammatical constructs. When parsing new texts
there are usually two types of parsing errors:
? The parser returns an incorrect parse. While
the parser may have constructed the correct
parse, the disambiguation model chose an in-
correct parse.
? The parser can not find an analysis that spans
the full sentence. If that sentence is allowed
in the language, the grammar or lexicon is in-
complete.
While the first type of errors can be alleviated
by improving the disambiguation model, the sec-
ond type of problems requires extension of the
grammar or lexicon. Finding incomplete descrip-
tions by hand can become a tedious task once a
grammar has wide coverage. Error mining tech-
niques aim to find problematic words or n-grams
automatically, allowing the grammar developer to
focus on frequent and highly suspicious forms
first.
2 Previous work
In the past, two major error mining techniques
have been developed by Van Noord (2004) and
Sagot and de la Clergerie (2006). In this paper we
propose a generalized error miner that combines
the strengths of these methods. Both methods fol-
low the same basic principle: first, a large (unan-
notated) corpus is parsed. After parsing, the sen-
tences can be split up in a list of parsable and a list
of unparsable sentences. Words or n-grams that
occur in the list of unparsable sentences, but that
do not occur in the list of parsable sentences have
a high suspicion of being the cause of the parsing
error.
2.1 Suspicion as a ratio
Van Noord (2004) defines the suspicion of a word
as a ratio:
S(w) =
C(w|error)
C(w)
(1)
where C(w) is the number of occurrences of
word w in all sentences, and C(w|error) is the
71
number of occurrences of w in unparsable sen-
tences. Of course, it is often useful to look at n-
grams as well. For instance, Van Noord (2004)
gives an example where the word via had a low
suspicion after parsing a corpus with the Dutch
Alpino parser, while the Dutch expression via via
(via a complex route) was unparsable.
To account for such phenomena, the notion of
suspicion is extended to n-grams:
S(wi..wj) =
C(wi..wj |error)
C(wi..wj)
(2)
Where a longer sequence wh...wi...wj ...wk is
only considered if its suspicion is higher than each
of its substrings:
S(wh...wi...wj ...wk) > S(wi...wj) (3)
While this method works well for forms that are
unambiguously suspicious, it also gives forms that
just happened to occur often in unparsable sen-
tences by ?bad luck? a high suspicion. If the occur-
rences in unparsable sentences were accompanied
by unambiguously suspicious forms, there is even
more reason to believe that the form is not prob-
lematic. However, in such cases this error mining
method will still assign a high suspicion to such
forms.
2.2 Iterative error mining
The error mining method described by Sagot and
de la Clergerie (2006) alleviates the problem of
?accidentally suspicious? forms. It does so by
taking the following characteristics of suspicious
forms into account:
? If a form occurs within parsable sentences, it
becomes less likely that the form is the cause
of a parsing error.
? The suspicion of a form should depend on the
suspicions of other forms in the unparsable
sentences in which it occurs.
? A form observed in a shorter sentence is ini-
tially more suspicious than a form observed
in a longer sentence.
To be able to handle the suspicion of a form
within its context, this method introduces the no-
tion of observation suspicion, which is the suspi-
cion of a form within a given sentence. The suspi-
cion of a form, outside the context of a sentence,
is then defined to be the average of all observation
suspicions:
Sf =
1
|Of |
?
oi,j?Of
Si,j (4)
HereOf is the set of all observations of the form
f , oi,j is the jth form of the ith sentence, and Si,j
is the observation suspicion of oi,j . The observa-
tion suspicions themselves are dependent on the
form suspicions, making the method an iterative
process. The suspicion of an observation is the
suspicion of its form, normalized by suspicions of
other forms occurring within the same sentence:
S(n+1)i,j = error(si)
S(n+1)F (oi,j)
?
1?j?|Si| S
(n+1)
F (oi,j)
(5)
Here error(si) is the sentence error rate, which
is normally set to 0 for parsable sentences and 1
for unparsable sentences. SF (oi,j) is the suspicion
of the form of observation oi,j .
To accommodate the iterative process, we will
have to redefine the form suspicion to be depen-
dent on the observation suspicions of the previous
cycle:
S(n+1)f =
1
|Of |
?
oi,j?Of
S(n)i,j (6)
Since there is a recursive dependence between
the suspicions and the observation suspicions,
starting and stopping conditions need to be defined
for this cyclic process. The observation suspicions
are initialized by uniformly distributing suspicion
over observed forms within a sentence:
S(0)i,j =
error(si)
|Si|
(7)
The mining is stopped when the process reaches
a fixed point where suspicions have stabilized.
This method solves the ?suspicion by accident?
problem of ratio-based error mining. However, the
authors of the paper have only used this method to
mine on unigrams and bigrams. They note that
they have tried mining with longer n-grams, but
encountered data sparseness problems. Their pa-
per does not describe criteria to determine when to
use unigrams and when to use bigrams to represent
forms within a sentence.
3 N-gram expansion
3.1 Inclusion of n-grams
While the iterative miner described by Sagot and
de la Clergerie (2006) only mines on unigrams and
72
bigrams, our prior experience with the miner de-
scribed by Van Noord (2004) has shown that in-
cluding longer n-grams in the mining process can
capture many additional phenomena. To give one
example: the words de (the), eerste (first), and
beste (best) had very low suspicions during er-
ror mining, while the trigram eerste de beste had
a very high suspicion. This trigram occurred in
the expression de eerste de beste (the first you can
find). While the individual words within this ex-
pression were described by the lexicon, this multi-
word expression was not.
3.2 Suspicion sharing
It may seem to be attractive to include all n-grams
within a sentence in the mining process. However,
this is problematic due to suspicion sharing. For
instance, consider the trigram w1, w2, w3 in which
w2 is the cause of a parsing error. In this case,
the bigrams w1, w2 and w2, w3 will become sus-
picious, as well as the trigram w1, w2, w3. Since
there will be multiple very suspicious forms within
the same sentence the unigramw2 will have no op-
portunity to manifest itself.
A more practical consideration is that the num-
ber of forms within a sentence grows at such a rate
(n + (n ? 1)... + 1) that error mining becomes
unfeasible for large corpora, both in time and in
space.
3.3 Expansion method
To avoid suspicion sharing we have devised a
method for adding and expanding n-grams when
it is deemed useful. This method iterates through
a sentence of unigrams, and expands unigrams to
longer n-grams when there is evidence that it is
useful. This expansion step is a preprocessor to
the iterative miner, that uses the same iterative al-
gorithm as described by Sagot and De la Clergerie.
Within this preprocessor, suspicion is defined in
the same manner as in Van Noord (2004), as a ra-
tio of occurrences in unparsable sentences and the
total number of occurrences.
The motivation behind this method is that there
can be two expansion scenarios. When we have
the bigram w1, w2, either one of the unigrams can
be problematic or the bigram w1, w2. In the for-
mer case, the bigram w1, w2 will also inherit the
high suspicion of the problematic unigram. In the
latter case, the bigram will have a higher suspicion
than both of its unigrams. Consequently, we want
to expand the unigram w1 to the bigram w1, w2 if
the bigram is more suspicious than both of its un-
igrams. If w1, w2 is equally suspicious as one of
its unigrams, it is not useful to expand to a bigram
since we want to isolate the cause of the parsing
error as much as possible.
The same methodology is followed when we
expand to longer n-grams. Expansion of w1, w2
to the trigram w1, w2, w3 will only be permitted
if w1, w2, w3 is more suspicious than its bigrams.
Since the suspicion of w3 aggregates to w2, w3,
we account for both w3 and w2, w3 in this com-
parison.
The general algorithm is that the expansion to
an n-gram i..j is allowed when S(i..j) > S(i..j?
1) and S(i..j) > S(i + 1..j). This gives us a sen-
tence that is represented by the n-grams n0..nx,
n1..ny, ... n|si|?1..n|si|?1.
3.4 Data sparseness
While initial experiments with the expansion al-
gorithm provided promising results, the expansion
algorithm was too eager. This eagerness is caused
by data sparseness. Since longer n-grams occur
less frequently, the suspicion of an n-gram oc-
curring in unparsable sentences goes up with the
length of the n-gram until it reaches its maximum
value. The expansion conditions do not take this
effect into account.
To counter this problem, we have introduced an
expansion factor. This factor depends on the fre-
quency of an n-gram within unparsable sentences
and asymptotically approaches one for higher fre-
quencies. As a result more burden of proof
is inflicted upon the expansion: the longer n-
gram either needs to be relatively frequent, or it
needs to be much more suspicious than its (n-1)-
grams. The expansion conditions are changed to
S(i..j) > S(i..j ? 1) ? extFactor and S(i..j) >
S(i + 1..j) ? extFactor, where
extFactor = 1 + e??|Of,unparsable| (8)
In our experiments ? = 1.0 proved to be a good
setting.
3.5 Pattern expansion
Previous work on error mining was primarily fo-
cused on the extraction of interesting word n-
grams. However, it could also prove useful to al-
low for patterns consisting of other information
than words, such as part of speech tags or lemmas.
We have done preliminary work on the integra-
tion of part of speech tags during the n-gram ex-
73
pansion. We use the same methodology as word-
based n-gram expansion, however we also con-
sider expansion with a part of speech tag.
Since we are interested in building patterns that
are as general as possible, we expand the pat-
tern with a part of speech tag if that creates a
more suspicious pattern. Expansion with a word
is attempted if expansion with a part of speech
tag is unsuccessful. E.g., if we attempt to ex-
pand the word bigram w1w2, we first try the tag
expansion w1w2t3. This expansion is allowed
when S(w1, w2, t3) > S(w1, w2) ? extFactor
and S(w1, w2, t3) > S(w2, t3) ? extFactor. If
the expansion is not allowed, then expansion to
S(w1, w2, w3) is attempted. As a result, mixed
patterns emerge that are as general as possible.
4 Implementation
4.1 Compact representation of data
To be able to mine large corpora some precau-
tions need to be made. During the n-gram expan-
sion stage, we need quick access to the frequen-
cies of arbitrary length n-grams. Additionally, all
unparsable sentences have to be kept in memory,
since we have to traverse them for n-gram expan-
sion. Ordinary methods for storing n-gram fre-
quencies (such as hash tables) and data will not
suffice for large corpora.
As Van Noord (2004) we used perfect hashing
to restrict memory use, since hash codes are gen-
erally shorter than the average token length. Addi-
tionally, comparisons of numbers are much faster
than comparisons of strings, which speeds up the
n-gram expansion step considerably.
During the n-gram expansion step the miner
calculates ratio-based suspicions of n-grams us-
ing frequencies of an n-gram in parsable and un-
parsable sentences. The n-gram can potentially
have the length of a whole sentence, so it is not
practical to store n-gram ratios in a hash table.
Instead, we compute a suffix array (Manber and
Myers, 1990) for the parsable and unparsable sen-
tences1. A suffix array is an array that contains in-
dices pointing to sequences in the data array, that
are ordered by suffix.
We use suffix arrays differently than Van No-
ord (2004), because our expansion algorithm re-
quires the parsable and unparsable frequencies of
the (n-1)-grams, and the second (n-1)-gram is not
1We use the suffix sorting algorithm by Peter M. McIlroy
and M. Douglas McIlroy.
(necessarily) adjacent to the n-gram in the suffix
array. As such, we require random access to fre-
quencies of n-grams occurring in the corpus. We
can compute the frequency of any n-gram by look-
ing up its upper and lower bounds in the suffix ar-
ray2, where the difference is the frequency.
4.2 Determining ratios for pattern expansion
While suffix arrays provide a compact and rela-
tively fast data structure for looking up n-gram fre-
quencies, they are not usable for pattern expansion
(see section 3.5). Since we need to look up fre-
quencies of every possible combination of repre-
sentations that are used, we would have to create
dl suffix arrays to be (theoretically) able to look
up pattern frequencies with the same time com-
plexity, where d is the number of dimensions and
l is the corpus length.
For this reason, we use a different method for
calculating pattern frequencies. First, we build a
hash table for each type of information that can
be used in patterns. A hash table contains an in-
stance of such information as a key (e.g. a specific
word or part of speech tag) and a set of corpus in-
dices where the instance occurred in the corpus as
the value associated with that key. Now we can
look up the frequency of a sequence i..j by calcu-
lating the set intersection of the indices of j and
the indices found for the sequence i..j ? 1, after
incrementing the indices of i..j ? 1 by one.
The complexity of calculating frequencies fol-
lowing this method is linear, since the set of in-
dices for a given instance can be retrieved with
a O(1) time complexity, while both increment-
ing the set indices and set intersection can be per-
formed in O(n) time. However, n can be very
large: for instance, the start of sentence marker
forms a substantial part of the corpus and is looked
up once for every sentence. In our implementation
we limit the time spent on such patterns by caching
very frequent bigrams in a hash table.
4.3 Removing low-suspicion forms
Since normally only one form within a sentence
will be responsible for a parsing error, many forms
will have almost no suspicion at all. However, dur-
ing the mining process, their suspicions will be
recalculated during every cycle. Mining can be
sped up considerably by removing forms that have
a negligible suspicion.
2Since the suffix array is sorted, finding the upper and
lower bounds is a binary search in O(log n) time.
74
If we do not drop forms, mining of the Dutch
Wikipedia corpus described in section 5.3, with
n-gram expansion and the extension factor en-
abled, resulted in 4.8 million forms with 13.4 mil-
lion form observations in unparsable sentences. If
we mine the same material and drop forms with
a suspicion below 0.001 there were 3.5 million
forms and 4.0 million form observations within
unparsable sentences left at the end of the iterative
mining process.
5 Evaluation
5.1 Methodology
In previous articles, error mining methods have
primarily been evaluated manually. Both Van No-
ord (2004) and Sagot and de la Clergerie (2006)
make a qualitative analysis of highly suspicious
forms. But once one starts experimenting with var-
ious extensions, such as n-gram expansion and ex-
pansion factor functions, it is difficult to qualify
changes through small-scale qualitative analysis.
To be able to evaluate changes to the error
miner, we have supplemented qualitative analysis
with a automatic quantitative evaluation method.
Since error miners are used by grammar engineers
to correct a grammar or lexicon by hand, the eval-
uation metric should model this use case:
? We are interested in seeing problematic forms
that account for errors in a large number of
unparsable sentences first.
? We are only interested in forms that actually
caused the parsing errors. Analysis of forms
that do not, or do not accurately pinpoint ori-
gin of the parsing errors costs a lot of time.
These requirements map respectively to the re-
call and precision metrics from information re-
trieval:
P =
|{Sunparsable} ? {Sretrieved}|
|{Sretrieved}|
(9)
R =
|{Sunparsable} ? {Sretrieved}|
|{Sunparsable}|
(10)
Consequently, we can also calculate the f-score
(van Rijsbergen, 1979):
F ? score =
(1 + ?2) ? (P ? R)
(?2 ? P + R)
(11)
The f-score is often used with ? = 1.0 to give
as much weight to precision as recall. In evalu-
ating error mining, this can permit cheating. For
instance, consider an error mining that recalls the
start of sentence marker as the first problematic
form. Such a strategy would instantly give a re-
call of 1.0, and if the coverage of a parser for a
corpus is relatively low, a relatively good initial f-
score will be obtained. Since error mining is often
used in situations where coverage is still low, we
give more bias to precision by using ? = 0.5.
We hope to provide more evidence in the future
that this evaluation method indeed correlates with
human evaluation. But in our experience it has the
required characteristics for the evaluation of error
mining. For instance, it is resistant to recalling
of different or overlapping n-grams from the same
sentences, or recalling n-grams that occur often in
both parsable and unparsable sentences.
5.2 Scoring methods
After error mining, we can extract a list of forms
and suspicions, and order the forms by their sus-
picion. But normally we are not only interested in
forms that are the most suspicious, but forms that
are suspicious and frequent. Sagot and de la Clerg-
erie (2006) have proposed three scoring methods
that can be used to rank forms:
? Concentrating on suspicions: Mf = Sf
? Concentrating on most frequent potential er-
rors: Mf = Sf |Of |
? Balancing between these possibilities: Mf =
Sf ? ln|Of |
For our experiments, we have replaced the ob-
servation frequencies of the form (|Of |) by the
frequency of observations within unparsable sen-
tences (|{Of,unparsable}|). This avoids assigning a
high score to very frequent unsuspicious forms.
5.3 Material
In our experiments we have used two corpora that
were parsed with the wide-coverage Alpino parser
and grammar for Dutch:
? Quantitative evaluation was performed on the
Dutch Wikipedia of August 20083. This cor-
pus consists of 7 million sentences (109 mil-
lion words). For 8.4% of the sentences no full
analysis could be found.
3http://ilps.science.uva.nl/WikiXML/
75
? A qualitative evaluation of the extensions was
performed on the Flemish Mediargus news-
paper corpus (up to May 31, 2007)4. This
corpus consists of 67 million sentences (1.1
billion words). For 9.2% of the sentences no
full analysis could be found.
Flemish is a variation of Dutch written and spo-
ken in Belgium, with a grammar and lexicon that
deviates slightly from standard Dutch. Previously,
the Alpino grammar and lexicon was never specif-
ically modified for parsing Flemish.
6 Results
6.1 Iterative error mining
We have evaluated the different mining methods
with the three scoring functions discussed in sec-
tion 5.2. In the results presented in this section we
only list the results with the scoring function that
performed best for a given error mining method
(section 6.3 provides an overview of the best scor-
ing functions for different mining methods).
Our first interest was if, and how much itera-
tive error mining outperforms error mining with
suspicion as a ratio. To test this, we compared
the method described by Van Noord (2004) and
the iterative error miner of Sagot and de la Clerg-
erie (2006). For the iterative error miner we eval-
uated both on unigrams, and on unigrams and bi-
grams where all unigrams and bigrams are used
(without further selection). Figure 6.1 shows the
f-scores for these miners after N retrieved forms.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0  2000  4000  6000  8000  10000
F 0.5-
Scor
e
N
ratioiter.unigramsiter.uni.bigrams
Figure 1: F-scores after retrieving N forms for
ratio-based mining, iterative mining on unigrams
and iterative mining on uni- and bigrams.
4http://www.mediargus.be/
The unigram iterative miner outperforms the
ratio-based miner during the retrieval of the first
8000 forms. The f-score graph of the iterative
miner on unigrams flattens after retrieving about
4000 forms. At that point unigrams are not spe-
cific enough anymore to pinpoint more sophisti-
cated problems. The iterative miner on uni- and bi-
grams performs better than the ratio-based miner,
even beyond 8000 forms. More importantly, the
curves of the iterative miners are steeper. This is
relevant if we consider that a grammar engineer
will only look at a few thousands of forms. For
instance, the ratio-based miner achieves an f-score
of 0.4 after retrieving 8448 forms, while the iter-
ative miner on uni- and bigrams attains the same
f-score after retrieving 5134 forms.
6.2 N-gram expansion
In our second experiment we have compared the
performance of iterative mining on uni- and bi-
grams with an iterative miner using the n-gram
expansion algorithm described in section 3. Fig-
ure 6.2 shows the result of n-gram expansion com-
pared to mining just uni- and bigrams. Both the
results for expansion with and without use of the
expansion factor are shown.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0  2000  4000  6000  8000  10000
F 0.5-
Scor
e
N
iter.uni.bigramiter.expansioniter.expansion.ef
Figure 2: F-scores after retrieving N forms for it-
erative mining on uni- and bigrams, and iterative
mining using n-gram expansion with and without
using an expansion factor.
We can see that the expansion to longer n-grams
gives worse results than mining on uni- and bi-
grams when data sparseness is not accounted for.
The expansion stage will select forms that may be
accurate, but that are more specific than needed.
As such, the recall per retrieved form is lower on
76
average, as can be seen in figure 6.2. But if sparse-
ness is taken into account through the use of the
expansion factor, we achieve higher f-scores than
mining on uni- and bigrams up to the retrieval of
circa five thousand forms. Since a user of an error
mining tool will probably only look at the first few
thousands of forms, this is a welcome improve-
ment.
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0.16
 0  2000  4000  6000  8000  10000
Reca
ll
N
iter.uni.bigramiter.expansioniter.expansion.ef
Figure 3: Recall after retrieving N forms for it-
erative mining on uni- and bigrams, and iterative
mining using n-gram expansion with and without
using an expansion factor.
Among the longer n-grams in the mining results
for the Mediargus corpus, we found many Flemish
idiomatic expressions that were not described in
the Alpino lexicon. For example:
? had er (AMOUNT) voor veil [had
(AMOUNT) for sale]
? (om de muren) van op te lopen [to get terribly
annoyed by]
? Ik durf zeggen dat [I dare to say that]
? op punt stellen [to fix/correct something]
? de daver op het lijf [shocked]
? (op) de tippen (van zijn tenen) [being very
careful]
? ben fier dat [am proud of]
? Nog voor halfweg [still before halfway]
? (om duimen en vingers) van af te likken [de-
licious]
Since these expressions are longer than bi-
grams, they cannot be captured properly without
using n-gram expansion. We also found longer
n-grams describing valid Dutch phrases that were
not described by the grammar or lexicon.
? Het stond in de sterren geschreven dat [It was
written in the stars that]
? zowat de helft van de [about half of the]
? er zo goed als zeker van dat [almost sure of]
? laat ons hopen dat het/dit lukt [let us hope that
it/this works]
6.3 Scoring methods
The miners that use n-gram expansion perform
best with the Mf = Sf |Of | function, while the
other miners perform best with the Mf = Sf ?
ln|Of | function. This is not surprising ? the it-
erative miners that do not use n-gram expansion
can not make very specific forms and give rela-
tively high scores to forms that happen to occur in
unparsable sentences (since some forms in a sen-
tence will have to take blame, if no specific sus-
picious form is found). If such forms also hap-
pen to be frequent, they may be ranked higher
than some more suspicious infrequent forms. In
the case of the ratio-based miner, there are many
forms that are ?suspicious by accident? which may
become highly ranked when they are more fre-
quent than very suspicious, but infrequent forms.
Since the miners with n-gram expansion can find
specific suspicious forms and shift blame to them,
there is less chance of accidentally ranking a form
to highly by directly including the frequency of
observations of that form within unparsable sen-
tences in the scoring function.
6.4 Pattern expansion
We have done some preliminary experiments with
pattern expansion, allowing for patterns consisting
of words and part of speech tags. For this exper-
iment we trained a Hidden Markov Model part of
speech tagger on 90% of the Dutch Eindhoven cor-
pus using a small tag set. We then extracted 50000
unparsable and about 495000 parsable sentences
from the Flemish Mediargus corpus. The pattern
expansion preprocessor was then used to find in-
teresting patterns.
We give two patterns that were extracted to give
an impression how patterns can be useful. A fre-
quent pattern was doorheen N (through followed
77
by a (proper) noun). In Flemish a sentence such
as We reden met de auto doorheen Frankrijk (lit-
eral: We drove with the car through France) is al-
lowed, while in standard Dutch the particle heen
is separated from the preposition door. Conse-
quently, the same sentence in standard Dutch is We
reden met de auto door Frankrijk heen. Mining
on word n-grams provided hints for this difference
in Flemish through forms such as doorheen Krot-
tegem, doorheen Engeland, doorheen Hawai, and
doorheen Middelkerke, but the pattern provides a
more general description with a higher frequency.
Another pattern that was found is wegens Prep
Adj (because of followed by a preposition and
an adjective). This pattern captures prepositional
modifiers where wegens is the head, and the fol-
lowing words within the constituent form an ar-
gument, such as in the sentence Dat idee werd
snel opgeborgen wegens te duur (literal: That idea
became soon archived because of too expensive).
This pattern provided a more general description
of forms such as wegens te breed (because it is
too wide), wegens te deprimerend (because it is
too depressing), wegens niet rendabel (because it
is not profitable), and wegens te ondraaglijk (be-
cause it is too unbearable).
While instances of both patterns were found us-
ing the word n-gram based miner, patterns consol-
idate different instances. For example, there were
120 forms with a high suspicion containing the
word wegens. If such a form is corrected, the other
examples may still need to be checked to see if a
solution to the parsing problem is comprehensive.
The pattern gives a more general description of the
problem, and as such, most of these 120 forms can
be represented by the pattern wegens Prep Adj.
Since we are still optimizing the pattern ex-
pander to scale to large corpora, we have not per-
formed an automatic evaluation using the Dutch
Wikipedia yet.
7 Conclusions
We combined iterative error mining with expan-
sion of forms to n-grams of an arbitrary length,
that are long enough to capture interesting phe-
nomena, but not longer. We dealt with the prob-
lem of data sparseness by introducing an expan-
sion factor that softens when the expanded form is
very frequent.
In addition to the generalization of iterative er-
ror mining, we introduced a method for automatic
evaluation. This allows us to test modifications to
the error miner without going through the tedious
task of ranking and judging the results manually.
Using this automatic evaluation method, we
have shown that iterative error mining improves
upon ratio-based error mining. As expected,
adding bigrams improves performance. Allowing
expansion beyond bigrams can lead to data sparse-
ness problems, but if we correct for data sparse-
ness the performance of the miner improves over
mining on just unigrams and bigrams.
We have also described preliminary work on
a preprocessor that allows for more general pat-
terns that incorporate additional information, such
as part of speech tags and lemmas. We hope to
optimize and improve pattern-based mining in the
future and evaluate it automatically on larger cor-
pora.
The error mining methods described in this pa-
per are generic, and can be used for any grammar
or parser, as long as the sentences within the cor-
pus can be divided in a list of parsable and un-
parsable sentences. The error miner is freely avail-
able5, and is optimized to work on large corpora.
The source distribution includes a graphical user
interface for browsing mining results, showing the
associated sentences, and removing forms when
they have been corrected in the grammar or lex-
icon.
References
Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2001. Alpino: Wide-coverage Computational Anal-
ysis of Dutch. In Computational Linguistics in The
Netherlands 2000.
Ann Copestake and Dan Flickinger. 2000. An
open source grammar development environment and
broad-coverage English grammar using HPSG. In
Proceedings of LREC 2000, pages 591?600.
Udi Manber and Gene Myers. 1990. Suffix arrays: a
new method for on-line string searches. In SODA
?90: Proceedings of the first annual ACM-SIAM
symposium on Discrete algorithms, pages 319?327.
Society for Industrial and Applied Mathematics.
Beno??t Sagot and E?ric de la Clergerie. 2006. Error
mining in parsing results. In ACL-44: Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 329?336, Morristown, NJ, USA. Association
for Computational Linguistics.
5http://www.let.rug.nl/?dekok/errormining/
78
Gertjan Van Noord. 2004. Error mining for wide-
coverage grammar engineering. In ACL ?04: Pro-
ceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, page 446, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
C. J. van Rijsbergen. 1979. Information retrieval. But-
terworths, London, 2 edition.
79
Coling 2010: Poster Volume, pages 153?161,
Beijing, August 2010
Acquisition of Unknown Word Paradigms for Large-Scale Grammars
Kostadin Cholakov
University of Groningen
The Netherlands
k.cholakov@rug.nl
Gertjan van Noord
University of Groningen
The Netherlands
g.j.m.van.noord@rug.nl
Abstract
Unknown words are a major issue for
large-scale grammars of natural language.
We propose a machine learning based al-
gorithm for acquiring lexical entries for
all forms in the paradigm of a given un-
known word. The main advantages of our
method are the usage of word paradigms
to obtain valuable morphological knowl-
edge, the consideration of different con-
texts which the unknown word and all
members of its paradigm occur in and
the employment of a full-blown syntactic
parser and the grammar we want to im-
prove to analyse these contexts and pro-
vide elaborate syntactic constraints. We
test our algorithm on a large-scale gram-
mar of Dutch and show that its application
leads to an improved parsing accuracy.
1 Introduction
In this paper, we present an efficient machine
learning based method for automated lexical ac-
quisition (LA) which improves the performance
of large-scale computational grammars on real-
life tasks.
Our approach has three main advantages which
distinguish it from other methods applied to the
same task. First, it enables the acquisition of the
whole paradigm of a given unknown word while
other approaches are only concerned with the par-
ticular word form encountered in the data sub-
ject to LA. Second, we analyse different contexts
which the unknown word occurs in. Third, the
analysis of these contexts is provided by a full-
blown syntactic parser and the grammar we aim
to improve which gives the grammar the opportu-
nity to participate directly in the LA process.
Our method achieves an F-measure of 84.6%
on unknown words in experiments with the wide-
coverage Alpino grammar (van Noord, 2006) of
Dutch. The integration of this method in the
parser leads to a 4.2% error reduction in terms of
labelled dependencies.
To predict a lexical entry for a given unknown
word, we take into account two factors? its mor-
phology and the syntactic constraints imposed by
its context. As for the former, the acquisition of
the whole paradigm provides us with a valuable
source of morphological information. If we were
to deal with only one form of the unknown word,
this information would not be accessible.
Further, looking at different contexts of the un-
known word gives us the possibility to work with
linguistically diverse data and to incorporate more
syntactic information into the LA process. Cases
where this is particularly important include mor-
phologically ambiguous words and verbs which
subcategorize for various types of syntactic argu-
ments. We also consider contexts of the other
members of the paradigm of the unknown word
in order to increase the amount of linguistic data
our method has access to.
Finally, the usage of a full-blown syntactic
parser and the grammar we want to acquire lex-
ical entries for has two advantages. First, LA
can benefit from the high-quality analyses such
a parser produces and the elaborate syntactic in-
formation they provide. Second, this information
comes directly from the grammar, thus allowing
the LA process to make predictions based on what
the grammar considers to be best suited for it.
153
The remainder of the paper is organised as fol-
lows. Section 2 describes the basic steps in our
LA algorithm. Section 3 presents initial exper-
iments conducted with Alpino and shows that
the main problems our LA method encounters
are the acquisition of morphologically ambigu-
ous words, the learning of the proper subcate-
gorization frames for verbs and the acquisition
of particular types of adjectives. In Section 4
we make extensive use of the paradigms of the
unknown words to develop specific solutions for
these problems. Section 5 describes experiments
with our LA method applied to a set of real un-
known words. Section 6 provides a comparison
between our approach and work previously done
on LA. This section also discusses the application
of our method to other systems and languages.
2 Basic Algorithm
The Alpino wide-coverage dependency parser is
based on a large stochastic attribute value gram-
mar. The grammar takes a ?constructional? ap-
proach, with rich lexical representations stored in
the lexicon and a large number of detailed, con-
struction specific rules (about 800). Currently, the
lexicon contains about 100K lexical entries and
a list of about 200K named entities. Each word
is assigned one or more lexical types. For ex-
ample, the verb amuseert (to amuse) is assigned
two lexical types? verb(hebben,sg3,intransitive)
and verb(hebben,sg3,transitive)? because it can
be used either transitively or intransitively. The
other type features indicate that it is a present third
person singular verb and it forms perfect tense
with the auxiliary verb hebben.
The goal of our LA method is to assign the cor-
rect lexical type(s) to a given unknown word. The
method takes into account only open-class lexical
types: nouns, adjectives and verbs, under the as-
sumption that the grammar is already able to han-
dle all closed-class cases. We call the types con-
sidered by our method universal types. The adjec-
tives can be used as adverbs in Dutch and thus, we
do not consider the latter to be an open class.
We employ a ME-based classifier which, for
some unknown word, takes various morphological
and syntactic features as input and outputs lexical
types. The probability of a lexical type t, given an
unknown word and its context c is:
(1) p(t|c) = exp(
?
i ?ifi(t,c))?
t??T exp(
?
i ?ifi(t?,c))
where fi(t, c) may encode arbitrary characteris-
tics of the context and < ?1,?2, ... > can be eval-
uated by maximising the pseudo-likelihood on a
training corpus (Malouf, 2002).
Table 1 shows the features for the noun in-
spraakprocedures (consultation procedures). Row
(i) contains 4 separate features derived from the
prefix of the word and 4 other suffix features are
given in row (ii). The two features in rows (iii)
and (iv) indicate whether the word starts with a
particle and if it contains a hyphen, respectively.
Another source of morphological features is the
paradigm of the unknown word which provides
information that is otherwise inaccessible. For ex-
ample, in Dutch, neuter nouns always take the het
definite article while all other noun forms are used
with the de article. Since the article is distinguish-
able only in the singular noun form, the correct
article of a word, assigned a plural noun type, can
be determined if we know its singular form.
We adopt the method presented in Cholakov
and van Noord (2009) where a finite state mor-
phology is applied to generate the paradigm(s) of
a given word. The morphology does not have ac-
cess to any additional linguistic information and
thus, it generates all possible paradigms allowed
by the word structure. Then, the number of
search hits Yahoo returns for each form in a given
paradigm is combined with some simple heuris-
tics to determine the correct paradigm(s).
However, we make some modifications to this
method because it deals only with regular mor-
phological phenomena. Though all typical irreg-
ularities are included in the Alpino lexicon, there
are cases of irregular verbs composed with parti-
cles which are not listed there. One such example
is the irregular verb meevliegen (to fly with some-
one) for which no paradigm would be generated.
To avoid this, we use a list of common parti-
cles to strip off any particle from a given unknown
word. Once we have removed a particle, we check
if what is left from the word is listed in the lexicon
as a verb (e.g. vliegen in the case of meevliegen).
If so, we extract all members of its paradigm from
154
Features
i) i, in, ins, insp
ii) s, es, res, ures
iii) particle yes #in this case in
iv) hyphen no
v) noun?de,pl?
vi) noun(de,count,pl), tmp noun(de,count,sg)
vii) noun(de), noun(count), noun(pl), tmp noun(de)
tmp noun(count), tmp noun(sg)
Table 1: Features for inspraakprocedures
the lexicon and use them to build the paradigm of
the unknown word. All forms are validated by us-
ing the same web-based heuristics as in the origi-
nal model of Cholakov and van Noord (2009).
A single paradigm is generated for in-
spraakprocedures indicating that this word is a
plural de noun. This information is explicitly used
as a feature in the classifier which is shown in row
(v) of Table 1.
Next, we obtain syntactic features for in-
spraakprocedures by extracting a number of sen-
tences which it occurs in from large corpora or
Internet. These sentences are parsed with a differ-
ent ?mode? of Alpino where this word is assigned
all universal types, i.e. it is treated as being maxi-
mally ambiguous. For each sentence only the best
parse is preserved. Then, the lexical type that has
been assigned to inspraakprocedures in this parse
is stored. During parsing, Alpino?s POS tagger
(Prins and van Noord, 2001) keeps filtering im-
plausible type combinations. For example, if a de-
terminer occurs before the unknown word, all verb
types are typically not taken into consideration.
This heavily reduces the computational overload
and makes parsing with universal types computa-
tionally feasible. When all sentences have been
parsed, a list can be drawn up with the types that
have been used and their frequency:
(2) noun(de,count,pl) 78
tmp noun(de,count,sg) 7
tmp noun(het,count,pl) 6
proper name(pl,?PER?) 5
proper name(pl,?ORG?) 3
verb(hebben,pl,vp) 1
The lexical types assigned to inspraakprocedures
in at least 80% of the parses are used as features
in the classifier. These are the two features in row
(vi) of Table 1. Further, as illustrated in row (vii),
each attribute of the considered types is also taken
as a separate feature. By doing this, we let the
grammar decide which lexical type is best suited
for a given unknown word. This is a new and ef-
fective way to include the syntactic constraints of
the context in the LA process.
However, for the parsing method to work prop-
erly, the disambiguation model of the parser needs
to be adapted. The model heavily relies on the
lexicon and it has learnt preferences how to parse
certain phrases. For example, it has learnt a pref-
erence to parse prepositional phrases as verb com-
plements, if the verb includes such a subcatego-
rization frame. This is problematic when parsing
with universal types. If the unknown word is a
verb and it occurs together with a PP, it would al-
ways get analysed as a verb which subcategorizes
for a PP.
To avoid this, the disambiguation model is re-
trained on a specific set of sentences meant to
make it more robust to input containing many un-
known words. We have selected words with low
frequency in large corpora and removed them tem-
porarily from the Alpino lexicon. Less frequent
words are typically not listed in the lexicon and
the selected words are meant to simulate their be-
haviour. Then, all sentences from the Alpino tree-
bank which contain these words are extracted and
used to retrain the disambiguation model.
3 Initial Experiments and Evaluation
To evaluate the performance of the classifier, we
conduct an experiment with a target type inven-
tory of 611 universal types. A type is considered
universal only if it is assigned to at least 15 dis-
tinct words occurring in large Dutch newspaper
corpora (?16M sentences) automatically parsed
with Alpino.
In order to train the classifier, 2000 words are
temporarily removed from the Alpino lexicon.
The same is done for another 500 words which
are used as a test set. All words have between
50 and 100 occurrences in the corpora. This se-
lection is again meant to simulate the behaviour
of unknown words. Experiments with a minimum
lower than 50 occurrences have shown that this is
a reasonable threshold to filter out typos, words
written together, etc.
155
The classifier yields a probability score for each
predicted type. Since a given unknown word can
have more than one correct type, we want to pre-
dict multiple types. However, the least frequent
types, accounting together for less than 5% of
probability mass, are discarded.
We evaluate the results in terms of precision
and recall. Precision indicates how many types
found by the method are correct and recall indi-
cates how many of the lexical types of a given
word are actually found. The presented results are
the average precision and recall for the 500 test
words.
Additionally, there are three baseline methods:
? Naive? each unknown word is assigned
the most frequent type in the lexicon:
noun(de,count,sg)
? POS tagger? the unknown word is given the
type most frequently assigned by the Alpino
POS tagger in the parsing stage
? Alpino? the unknown word is assigned the
most frequently used type in the parsing
stage
The overall results are given in Table 2. Table 3
shows the results for each POS in our model.
Model Precision(%) Recall(%) F-measure(%)
Naive 19.60 18.77 19.17
POS tagger 30 26.21 27.98
Alpino 44.60 37.59 40.80
Our model 86.59 78.62 82.41
Table 2: Overall experiment results
POS Precision(%) Recall(%) F-measure(%)
Nouns 93.83 88.61 91.15
Adjectives 75.50 73.12 74.29
Verbs 77.32 55.37 64.53
Table 3: Detailed results for our model
Our LA method clearly improves upon the
baselines. However, as we see in Table 3, adjec-
tives and especially verbs remain difficult to pre-
dict.
The problems with the former are due to the fact
that Alpino employs a rather complicated adjec-
tive system. The classifier has difficulties distin-
guishing between 3 kinds of adjectives: i) adjec-
tives which can attach to and modify verbs and
verbal phrases (VPs) (3-a), ii) adjectives which
can attach to verbs and VPs but modify one of
the complements of the verb, typically the sub-
ject (3-b) and iii) adjectives which cannot attach
to verbs and VPs (3-c).
(3) a. De
DET
hardloper
runner
loopt
walks
mooi.
nice
?The runner runs nicely = The runner has a
good running technique?
b. Hij
he
loopt
walks
dronken
drunk
naar
to
huis.
home
?He walks home drunk = He is walking home
while being drunk?
c. *Hij
he
loopt
walks
nederlandstalig.
Dutch speaking
?He walks Dutch speaking.?
Each of these is marked by a special attribute in
the lexical type definitions? adv, padv and non-
adv, respectively. Since all three of them are seen
in ?typical? adjectival contexts where they modify
nouns, it is hard for the classifier to make a distinc-
tion. The predictions appear to be arbitrary and
there are many cases where the unknown word is
classified both as a nonadv and an adv adjective. It
is even more difficult to distinguish between padv
and adv adjectives since this is a solely semantic
distinction.
The main issue with verbs is the prediction of
the correct subcategorization frame. The classifier
tends to predict mostly transitive and intransitive
verb types. As a result, it either fails to capture in-
frequent frames which decreases the recall or, in
cases where it is very uncertain what to predict, it
assigns a lot of types that differ only in the subcat
frame, thus damaging the precision. For example,
onderschrijf (?to agree with?) has 2 correct sub-
cat frames but receives 8 predictions which differ
only in the subcat features.
One last issue is the prediction, in some rare
cases, of types of the wrong POS for morpholog-
ically ambiguous words. In most of these cases
adjectives are wrongly assigned a past partici-
ple type but also some nouns receive verb pre-
dictions. For instance, OESO-landen (?countries
of the OESO organisation?) has one correct noun
type but because landen is also the Dutch verb for
?to land? the classifier wrongly assigns a verb type
as well.
156
4 Improving LA
4.1 POS Correction
Since the vast majority of wrong POS predictions
has to do with the assignment of incorrect verb
types, we decided to explicitly use the generated
verb paradigms as a filtering mechanism. For each
word which is assigned a verb type, we check if
there is a verb paradigm generated for it. If not, all
verb types predicted for the word are discarded.
In very rare cases a word is assigned only verb
types and therefore, it ends up with no predictions.
For such words, we examine the ranked list of pre-
dicted types yielded by the classifier and the word
receives the non-verb lexical type with the high-
est probability score. If this type happens to be
an adjective one, we first check whether there is
an adjective paradigm generated for the word in
question. If not, the word gets the noun type with
the highest probability score.
The same procedure is also applied to all words
which are assigned an adjective type. However,
it is not used for words predicted to be nouns be-
cause the classifier is already very good at predict-
ing nouns. Further, the generated noun paradigms
are not reliable enough to be a filtering mechanism
because there are mass nouns with no plural forms
and thus with no paradigms generated.
Another modification we make to the classifier
output has to do with the fact that past participles
(psp) in Dutch can also be used as adjectives. This
systematic ambiguity, however, is not treated as
such in Alpino. Each psp should also have a sep-
arate adjective lexical entry but this is not always
the case. That is why, in some cases, the classifier
fails to capture the adjective type of a given psp.
To account for it, all words predicted to be past
participles but not adjectives are assigned two ad-
ditional adjective types? one with the nonadv and
one with the adv feature. For reasons explained
later on, a type with the padv feature is not added.
After the application of these techniques, all
cases of words wrongly predicted to be verbs or
adjectives have been eliminated.
4.2 Guessing Subcategorization Frames
Our next step is to guess the correct subcatego-
rization feature for verbs. Learning the proper
subcat frame is well studied (Brent, 1993; Man-
ning, 1993; Briscoe and Caroll, 1997; Kinyon and
Prolo, 2002; O?Donovan et al, 2005). Most of
the work follows the ?classical? Briscoe and Caroll
(1997) approach where the verb and the subcate-
gorized complements are extracted from the out-
put analyses of a probabilistic parser and stored as
syntactic patterns. Further, some statistical tech-
niques are applied to select the most probable
frames out of the proposed syntactic patterns.
Following the observations made in Korho-
nen et al (2000), Lapata (1999) and Messiant
(2008), we employ a maximum likelihood es-
timate (MLE) from observed relative frequen-
cies with an empirical threshold to filter out low
probability frames. For each word predicted to
be a verb, we look up the verb types assigned
to it during the parsing with universal types.
Then, the MLE for each subcat frame is deter-
mined and only frames with MLE of 0.2 and
above are considered. For example, jammert
(to moan.3SG.PRES) is assigned a single type?
verb(hebben,sg3,intransitive). However, the cor-
rect subcat features for it are intransitive and sbar.
Here is the list of all verb types assigned to jam-
mert during the parsing with universal types:
(4) verb(hebben,sg3,intransitive) 48
verb(hebben,sg3,transitive) 15
verb(hebben,past(sg),np sbar) 3
verb(hebben,past(sg),tr sbar) 3
verb(zijn,sg3,intransitive) 2
verb(hebben,past(sg),ld pp) 2
verb(hebben,sg3,sbar) 1
The MLE for the intransitive subcat feature is 0.68
and for the transitive one? 0.2. All previously pre-
dicted verb types are discarded and each consid-
ered subcat frame is used to create a new lexi-
cal type. That is how jammert gets two types at
the end? the correct verb(hebben,sg3,intransitive)
and the incorrect verb(hebben,sg3,transitive). The
sbar frame is wrongly discarded.
To avoid such cases, the generated word
paradigms are used to increase the number of con-
texts observed for a given verb. Up to 200 sen-
tences are extracted for each form in the paradigm
of a given word predicted to be a verb. These sen-
tences are again parsed with the universal types
and then, the MLE for each subcat frame is recal-
157
culated.
We evaluated the performance of our MLE-
based method on the 116 test words predicted to
be verbs. We extracted the subcat features from
their type definitions in the Alpino lexicon to cre-
ate a gold standard of subcat frames. Addition-
ally, we developed two baseline methods: i) all
frames assigned during parsing are considered and
ii) each verb is taken to be both transitive and in-
transitive. Since most verbs have both or one of
these frames, the purpose of the second baseline is
to see if there is a simpler solution to the problem
of finding the correct subcat frame. The results
are given in Table 4.
Model Precision(%) Recall(%) F-measure(%)
all frames 16.76 94.34 28.46
tr./intr. 62.29 69.17 65.55
our model 85.82 67.28 75.43
Table 4: Subcat frames guessing results
Our method significantly outperforms both
baselines. It is able to correctly identify the transi-
tive and/or the intransitive frames. Since they are
the most frequent ones in the test data, this boosts
up the precision. However, the method is also able
to capture other, less frequent subcat frames. For
example, after parsing the additional sentences for
jammert, the sbar frame had enough occurrences
to get above the threshold. The MLE for the tran-
sitive one, on the other hand, fell below 0.2 and it
was correctly discarded.
4.3 Guessing Adjective Types
We follow a similar approach for finding the cor-
rect adjective type. It should be noted that the
distinction among nonadv, adv and padv does
not exist for every adjective form. Most ad-
jectives in Dutch get an -e suffix when used
attributively? de mooie/mooiere/mooiste jongen
(the nice/nicer/nicest boy). Since these inflected
forms can only occur before nouns, the distinction
we are dealing with is not relevant for them. Thus
we are only interested in the noninflected base,
comparative and superlative adjective forms.
One of the possible output formats of Alpino
is dependency triples. Here is the output for the
sentence in (3-a):
(5) verb:loop|hd/su|noun:hardloper
noun:hardloper|hd/det|det:de
verb:loop|hd/mod|adj:mooi
verb:loop|?/?|punct:.
Each line is a single dependency triple. The line
contains three fields separated by the ?|? character.
The first field contains the root of the head word
and its POS, the second field indicates the type of
the dependency relation and the third one contains
the root of the dependent word and its POS. The
third line in (5) shows that the adjective mooi is a
modifier of the head, in this case the verb loopt.
Such a dependency relation indicates that this ad-
jective can modify a verb and therefore, it belongs
to the adv type.
As already mentioned, padv adjectives cannot
be distinguished from the ones of the adv kind.
That is why, if the classifier has decided to assign
a padv type to a given unknown word, we discard
all other adjective types assigned to it (if any) and
do not apply the technique described below to this
word.
For each of the 59 words assigned an non-
inflected adjective type after the POS correction
stage, we extract up to 200 sentences for all non-
inflected forms in its paradigm. These sentences
are parsed with Alpino and the universal types and
the output is dependency triples. All triples where
the unknown word occurs as a dependent word in
a head modifier dependency (hd/mod, as shown in
(5)) and its POS is adjective are extracted from the
parse output. We calculate the MLE of the cases
where the head word is a verb, i.e. where the un-
known word modifies a verb. If the MLE is 0.05
or larger, the word is assigned an adv lexical type.
For example, the classifier correctly identifies
the word doortimmerd (solid) as being of the ad-
jective(no e(nonadv)) type but it also predicts the
adjective(no e(adv))1 type for it. Since we have
not found enough sentences where this word mod-
ifies a verb, the latter type is correctly discarded.
Our technique produced correct results for 53 out
of the 59 adjectives processed.
1The no e type attribute denotes a noninflected base ad-
jective form.
158
4.4 Improved Results and Discussion
Table 5 presents the results obtained after apply-
ing the improvement techniques described in this
section to the output of the classifier (the ?Model
2? rows). For comparison, we also give the re-
sults from Table 3 again (the ?Model 1? rows).
The numbers for the nouns happen to remain un-
changed and that is why they are not shown in Ta-
ble 5.
POS Models Prec.(%) Rec.(%) F-meas.(%)
Adj Model 1 75.50 73.12 74.29Model 2 85.16 80.16 82.58
Verbs Model 1 77.32 55.37 64.53Model 2 80.56 56.24 66.24
Overall Model 1 86.59 78.62 82.41Model 2 89.08 80.52 84.58
Table 5: Improved results
The automatic addition of adjective types for
past participles improved significantly the recall
for adjectives and our method for choosing be-
tween adv and nonadv types caused a 10% in-
crease in precision.
However, these procedures also revealed some
incomplete lexical entries in Alpino. For example,
there are two past participles not listed as adjec-
tives in the lexicon though they should be. Thus
when our method correctly assigned them adjec-
tive types, it got punished since these types were
not in the gold standard.
We see in Table 5 that the increase in precision
for the verbs is small and recall remains practi-
cally unchanged. The unimproved recall shows
that we have not gained much from the subcat
frame heuristics. Even when the number of the
observed sentences was increased, less frequent
frames often remained unrecognisable from the
noise in the parsed data. This could be seen as
a proof that in the vast majority of cases verbs
are used transitively and/or intransitively. Since
the MLE method we employ proved to be good at
recognising these two frames and differentiating
between them, we have decided to continue using
it.
The overall F-score improved by only 2% be-
cause the modified verb and adjective predictions
are less than 30% of the total predictions made by
the classifier.
5 Experiment with Real Unknown
Words
To investigate whether the proposed LA method
is also beneficial for the parser, we observe how
parsing accuracy changes when the method is em-
ployed. Accuracy in Alpino is measured in terms
of labelled dependencies.
We have conducted an experiment with a test
set of 300 sentences which contain 188 real un-
known words. The sentences have been randomly
selected from the manually annotated LASSY
corpus (van Noord, 2009) which contains text
from various domains. The average sentence
length is 26.54 tokens.
The results are given in Table 6. The standard
Alpino model uses its guesser to assign types to
the unknown words. Model 1 employs the trained
ME-based classifier to predict lexical entries for
the unknown words offline and then uses them
during parsing. Model 2 uses lexical entries modi-
fied by applying the methods described in Section
4 to the output of the classifier (Model 1).
Model Accuracy (%) msec/sentence
Alpino 88.77 8658
Model 1 89.06 8772
Model 2 89.24 8906
Table 6: Results with real unknown words
Our LA system as a whole shows an error re-
duction rate of more than 4% with parse times re-
maining similar to those of the standard Alpino
version. It should also be noted that though much
of the unknown words are generally nouns, we see
from the results that it makes sense to also employ
the methods for improving the predictions for the
other POS types. A wrong verb or even adjec-
tive prediction can cause much more damage to
the analysis than a wrong noun one.
These results illustrate that the integration of
our method in the parser can improve its perfor-
mance on real-life data.
6 Discussion
6.1 Comparison to Previous Work
The performance of the LA method we presented
in this paper can be compared to the performance
159
of a number of other approaches previously ap-
plied to the same task.
Baldwin (2005) uses a set of binary classifiers
to learn lexical entries for a large-scale gram-
mar of English (ERG; (Copestake and Flickinger,
2000)). The main disadvantage of the method is
that it uses information obtained from secondary
language resources? POS taggers, chunkers, etc.
Therefore, the grammar takes no part in the LA
process and the method acquires lexical entries
based on incomplete linguistic information pro-
vided by the various resources. The highest F-
measure (about 65%) is achieved by using fea-
tures from a chunker but it is still 20% lower than
the results we report here. Further, no evalua-
tion is done on how the method affects the per-
formance of the ERG when the grammar is used
for parsing.
Zhang and Kordoni (2006) and Cholakov et
al. (2008), on the other hand, include features
from the grammar in a maximum entropy (ME)
classifier to predict new lexical entries for the
ERG and a large German grammar (GG; (Crys-
mann, 2003)), respectively. The development data
for this method consist of linguistically annotated
sentences from treebanks and the grammar fea-
tures used in the classifier are derived from this
annotation. However, when the method is applied
to open-text unannotated data, the grammar fea-
tures are replaced with POS tags. Therefore, the
grammar is no longer directly involved in the LA
process which affects the quality of the predic-
tions. Evaluation on sentences containing real un-
known words shows improvement of the coverage
for the GG when LA is employed but the accuracy
decreases by 2%. Such evaluation has not been
done for the ERG. The results on the development
data are not comparable with ours because evalu-
ation is done only in terms of precision while we
are also able to measure recall.
Statistical LA has previously been applied to
Alpino as well (van de Cruys, 2006). However,
his method employs less morphosyntactic features
in comparison to our approach and does not make
use of word paradigms. Further, though experi-
ments on development data are performed on a
smaller scale, the results in terms of F-measure are
10% lower than those reported in our case study.
Experiments with real unknown words have not
been performed.
Other, non-statistical LA methods also exist.
Cussens and Pulman (2000) describe a symbolic
approach which employs inductive logic program-
ming and Barg and Walther (1998) and Fouvry
(2003) follow a unification-based approach. How-
ever, the generated lexical entries might be both
too general or too specific and it is doubtful if
these methods can be used on a large scale. They
have not been applied to broad-coverage gram-
mars and no evaluation is provided.
6.2 Application to Other Systems and
Languages
We stress the fact that the experiments with
Alpino represent only a case study. The proposed
LA method can be applied to other computational
grammars and languages providing that the fol-
lowing conditions are fulfilled.
First, words have to be mapped onto some fi-
nite set of labels of which a subset of open-class
(universal) labels has to be selected. This subset
represents the labels which the ME-based classi-
fier can predict for unknown words. Second, a
(large) corpus has to be available, so that various
sentences in which a given unknown word occurs
can be extracted. This is crucial for obtaining dif-
ferent contexts in which this word is found.
Next, we need a parser to analyse the extracted
sentences which allows for the syntactic con-
straints imposed by these contexts to be included
in the prediction process.
Finally, as for the paradigm generation, the idea
of combining a finite state morphology and web
heuristics is general enough to be implemented
for different languages. It is also important to
note that the classifier allows for arbitrary com-
binations of features and therefore, a researcher is
free to include any (language-specific) features he
or she considers useful for performing LA.
We have already started investigating the appli-
cability of our LA method to large-scale gram-
mars of German and French and the initial experi-
ments and results we have obtained are promising.
160
References
Baldwin, Tim. 2005. Bootstrapping deep lexical re-
sources: Resources for courses. In Proceedings of
the ACL-SIGLEX 2005 Workshop on Deep Lexical
Acquisition, Ann Arbor, USA.
Barg, Petra and Markus Walther. 1998. Processing un-
known words in HPSG. In Proceedings of the 36th
Conference of the ACL, Montreal, Quebec, Canada.
Brent, Michael R. 1993. From grammar to lexicon:
unsupervised learning of lexical syntax. Computa-
tional Linguistics, 19(2):243?262.
Briscoe, Ted and John Caroll. 1997. Automatic ex-
traction of subcategorization from corpora. In Pro-
ceedings of the 5th ACL Conference on Applied Nat-
ural Language Processing, Washington, DC.
Cholakov, Kostadin and Gertjan van Noord. 2009.
Combining finite state and corpus-based techniques
for unknown word prediction. In Proceedings of the
7th Recent Advances in Natural Language Process-
ing (RANLP) conference, Borovets, Bulgaria.
Cholakov, Kostadin, Valia Kordoni, and Yi Zhang.
2008. Towards domain-independent deep linguistic
processing: Ensuring portability and re-usability of
lexicalised grammars. In Proceedings of COLING
2008 Workshop on Grammar Engineering Across
Frameworks (GEAF08), Manchester, UK.
Copestake, Ann and Dan Flickinger. 2000. An
open-source grammar development environment
and broad-coverage English grammar using HPSG.
In Proceedings of the 2nd International Confer-
ence on Language Resource and Evaluation (LREC
2000), Athens, Greece.
Crysmann, Berthold. 2003. On the efficient imple-
mentation of German verb placement in HPSG. In
Proceedings of RANLP 2003, Borovets, Bulgaria.
Cussens, James and Stephen Pulman. 2000. Incor-
porating linguistic constraints into inductive logic
programming. In Proceedings of the Fourth Con-
ference on Computational Natural Language Learn-
ing.
Fouvry, Frederik. 2003. Lexicon acquisition with a
large-coverage unification-based grammar. In Com-
panion to the 10th Conference of EACL, pages 87?
90, Budapest, Hungary.
Kinyon, Alexandra and Carlos A Prolo. 2002. Iden-
tifying verb arguments and their syntactic function
in the Penn Treebank. In Proceedings of the 3rd In-
ternational Conference on Language Resource and
Evaluation (LREC 2002), Las Palmas de Gran Ca-
naria, Spain.
Korhonen, Anna, Genevieve Gorell, and Diana Mc-
Carthy. 2000. Statistical filtering and subcatego-
rization frame acquisition. In Proceedings of the
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Cor-
pora, Hong Kong, China.
Lapata, Mirella. 1999. Acquiring lexical generaliza-
tions from corpora. A case study for diathesis alter-
nations. In Proceedings of the 37th Annual Meeting
of ACL, Maryland, USA.
Malouf, Robert. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the 6th conference on Natural Language
Learning (CoNLL-2002), pages 49?55, Taipei, Tai-
wan.
Manning, Christopher. 1993. Automatic acquisition
of a large subcategorization dictionary from cor-
pora. In Proceedings of the 31st Annual Meeting
of ACL, Columbus, OH.
Messiant, Cedric. 2008. A subcategorization acquisi-
tion system for French verbs. In Proceedings of the
ACL 2008 Student Research Workshop, Columbus,
OH.
O?Donovan, Ruth, Michael Burke, Aoife Cahill, Josef
van Genabith, and Andy Way. 2005. Large-scale
induction and evaluation of lexical resources from
the Penn-II and Penn-III Treebanks. Computational
Linguistics, 31(3):329?365.
Prins, Robbert and Gertjan van Noord. 2001. Un-
supervised POS-tagging improves parcing accuracy
and parsing efficiency. In Proceedings of IWPT,
Beijing, China.
van de Cruys, Tim. 2006. Automatically extending the
lexicon for parsing. In Huitnik, Janneje and Sophia
Katrenko, editors, Proceedings of the Eleventh ESS-
LLI Student Session, pages 180?189.
van Noord, Gertjan. 2006. At last parsing is now oper-
ational. In Proceedings of TALN, Leuven, Belgium.
van Noord, Gertjan. 2009. Huge parsed corpora in
LASSY. In Proceedings of the Seventh Interna-
tional Workshop on Treebanks and Linguistic The-
ories (TLT 7), Groningen, The Netherlands.
Zhang, Yi and Valia Kordoni. 2006. Automated deep
lexical acquisition for robust open text processing.
In Proceedings of the Fifth International Confer-
ence on Language Resourses and Evaluation (LREC
2006), Genoa, Italy.
161
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1382?1391, Dublin, Ireland, August 23-29 2014.
From neighborhood to parenthood: the advantages of dependency
representation over bigrams in Brown clustering
Simon
?
Suster
University of Groningen
Netherlands
s.suster@rug.nl
Gertjan van Noord
University of Groningen
Netherlands
g.j.m.van.noord@rug.nl
Abstract
We present an effective modification of the popular Brown et al. 1992 word clustering algorithm,
using a dependency language model. By leveraging syntax-based context, resulting clusters are
better when evaluated against a wordnet for Dutch. The improvements are stable across parameters
such as number of clusters, minimum frequency and granularity. Further refinement is possible
through dependency relation selection. Our approach achieves a desired clustering quality with
less data, resulting in a decrease in cluster creation times.
1 Introduction
Semi-supervised approaches have been successful in various areas of natural language processing. Among
a plethora of clustering techniques, Brown clustering (Brown et al., 1992) is popular for its conceptual
simplicity, available implementations (Liang, 2005; Stolcke, 2002), and because the resulting word
clusters can be helpful for several tasks. Clusters are used as syntactic and semantic generalizations of
words, requiring fewer model parameters.
Brown clustering (section 2) groups words based on shared context. However, only immediately
adjacent words are taken into account as recognized e.g. by Koo et al. (2008), Sagae and Gordon (2009),
and Grave et al. (2013). For example, even though verbs constitute an informative context for object nouns,
they are rarely considered in Brown clustering, unlike in dependency-based clustering. The difference
between the contexts can be illustrated with the following example:
The method repeatedly samples the data
bigram contexts
dependency contexts
The bigram context thus fails to capture the relation between the object data and the predicate samples, as
well as the one between the subject method and the predicate. Furthermore, the dependency representation
rightly ignores some of the less informative contexts coming from immediately adjacent words. For
example, there is no relation between the predicate samples and the article the to the right.
It might be preferable therefore to induce word clusters based on the dependency relations in which
the words occur. In section 3, we present how this relates to Brown clustering, and we modify the code
by Percy Liang, so that dependency clustering can be used. We evaluate clusters in a wordnet-based
similarity experiment. Dependency clustering yields superior clusters for Dutch across different settings
of parameters such as number of clusters, frequency threshold and level of granularity. Selecting specific
dependency relation labels and using data obtained from them as input to clustering further improves the
clustering quality. The proposed adaptation of Brown clustering does not change the complexity of the
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1382
algorithm, and?although we assume that syntactically parsed text is available?it requires much less data
for a desired level of clustering quality.
2 The Brown clustering algorithm
Brown clustering (Brown et al., 1992) is an agglomerative algorithm that induces a hierarchical clustering
of words. It takes a tokenized corpus and groups words into k clusters identified by bit strings, representing
paths in the induced binary tree in which the leaves are word clusters. Prefixes of the paths can be used to
achieve clusters of coarser granularity (Sun et al., 2011; Turian et al., 2010). The obtained clusters contain
words that are semantically related, or are paradigmatic or orthographic variants.
1
The algorithm starts by putting k most frequent words into distinct clusters. Then, the k+1
th
most
frequent word is assigned to a new cluster, and two among the resulting k+1 clusters are merged, i.e. the
pair that maximizes the average mutual information of the current clustering. This process is repeated
until all words have been merged. The resulting k clusters are then merged to build the binary tree. The
version of the algorithm optimized for speed runs in O(k
2
|V|), with |V| the vocabulary size.
Brown clustering has been used extensively in supervised NLP tasks such as parsing (Koo et al., 2008;
Candito and Crabb?e, 2009; Haffari et al., 2011), named-entity recognition (NER) and chunking (Turian
et al., 2010), sentiment analysis (Popat et al., 2013), relation extraction (Plank and Moschitti, 2013),
unsupervised semantic role labeling (Titov and Klementiev, 2012), question answering (Momtazi et al.,
2010), POS tagging (Owoputi et al., 2013) and speech recognition with recursive neural networks (Shi et
al., 2013). Recently, multilingual clustering has also been proposed (T?ackstr?om et al., 2012; Faruqui and
Dyer, 2013).
Among the most frequently recognized limitations (cf. Koo et al. (2008); Chrupala (2011)) are a) the
hard nature of the clustering, b) relatively long running time
2
and c) insensitivity to wider context. Our
method attempts to overcome the final disadvantage. As it requires less data, it also reduces the running
time.
Leveraging syntactic context for word representations has been explored, among others, in Lin (1998)
on distributional thesauri; Haffari et al. (2011) on combining Brown clusters and word groupings from split
non-terminals; Sagae and Gordon (2009) on using unlexicalized syntactic context in hierarchical clustering;
Van de Cruys (2010) and Pad?o and Lapata (2007) on comparison of window- and syntactic-based word
space models; and Boyd-Graber and Blei (2008) on syntactic topic models.
The work closest to ours is that of Grave et al. (2013). The authors show that clusters obtained from
dependency trees outperform standard Brown clustering when used as features in super-sense tagging
and NER. Their focus is on a generalization of Brown clustering with Hidden Markov models (extending
Markov chains to trees), allowing the creation of soft clusters.
3
Learning and inference are done with
online expectation-maximization and belief propagation.
Whereas Grave et al. focus on new learning methods for clustering with HMMs on dependency trees,
we take an in-depth look at parameters and choices that are standardly considered using the (Brown et
al., 1992) algorithm. We show that the advantage of dependency clustering can be observed throughout
different parametrizations of cluster capacity, granularity level, frequency thresholding and other criteria
(section 6), and that the advantage is roughly constant for varying amounts of input data. Finally, we
provide new insight in the advantage of selective dependency clustering, in which the data obtained only
from specific dependency relations lead to better clusters. Our approach constitutes a straightforward
extension of Brown clustering, and only required a simple modification of the Brown clustering code.
1
We are using the term semantic relatedness in its broadest possible scope. Words or clusters are semantically related when
they have any kind of semantic relation: synonymy, meronymy, antonymy, hypernymy etc. (Turney and Pantel, 2010).
2
Although coarser clustering (k<1000) can mean more practical running times, as the clustering depends quadratically on k.
3
This approach allows to capture homonymy/polysemy, with the idea that when a word representation is needed, it can
be obtained in a context-sensitive way (Huang et al., 2011; Nepal and Yates, 2014). This is certainly an important advantage
over Brown clustering in which the mapping between a word and a cluster is deterministic; however, it comes with its own
disadvantages: creating context-sensitive representations requires (potentially) costly inference; furthermore, HMM-based
clustering does not build nor lends itself easily to a hierarchy, which is often exploited during feature creation in supervised
learning to control cluster granularity (see the end of section 5.2)
1383
3 Extension of the Brown clustering
The bigram language model underlying Brown clustering takes the probability of a sentence as the
product of probabilities of words based on immediately preceding words. In contrast, we replace this
by a dependency language model (DLM), which defines the probability of a sentence over dependency
trees (Shen et al., 2008). This probability can be factorized in different ways (Chen et al., 2012; Charniak,
2001; Popel and Mare?cek, 2010), but the common idea is that a word is conditioned on some history,
where the link between the two is a dependency. In practice, the history can include the immediate parent
of the word, which can be either a lexical head or the artificial root node, as well as siblings between the
child and the parent. Our take on DLM is similar to Charniak (2001) and Popel and Mare?cek (2010): the
probability of a word is conditioned simply on its parent. This is the same view as taken by Grave et
al. (2013).
The Brown clustering objective is to find such a deterministic clustering function C mapping each word
from the vocabulary V to one of K clusters that maximizes the likelihood of the data. The likelihood of a
sequence of word tokens, w = ?w
i
?
m
i=1
, with each w
i
? V , factors as
L(w; C) =
m
?
i=1
p(C(w
i
)|C(w
i?1
))p(w
i
|C(w
i
)), (3.1)
where C(w
0
) is a special start-of-sequence symbol. As shown by Brown et al. (1992), by taking the
negative logarithm and using the ML estimates, the equation 3.1 is decomposed to the negative entropy of
the sequence w and mutual information between adjacent clusters. Since the entropy is independent of
the clustering function, the objective amounts to finding such C that maximizes the mutual information.
For dependency clustering, we change the cluster transition probability so that conditioning is on the
cluster of the parent of the word at position i, instead of on the cluster of the previous word:
L
?
(w; C) =
m
?
i=1
p(C(w
i
)|C(w
pi(i)
))p(w
i
|C(w
i
)), (3.2)
where i ranges over all children in a tree and pi is a function from the children to their unique parents
(which include the special root of the tree). Calculation of the mutual information changes only to
the extent that count tables no longer represent adjacency relationship (bigrams) between words but
parenthood (child?parent relation).
4 Evaluation task
We evaluate our word clusters by following the method of Van de Cruys (2010) for evaluating vector space
models. The method is based on a wordnet for Dutch and assumes that two semantically related words
also occur close to each other in the wordnet hierarchy.
4
We use Cornetto (Vossen et al., 2013), which
includes more than 92,000 form-POS pairs described in terms of lexical units, synsets and other criteria.
For calculating similarity scores, we treat Cornetto as a digraph, with nodes constituting synsets and arcs
constituting hypernymic relations, and adopt the Lin similarity measure (Lin, 1998)
5
in combination with
the ontological variant of Information Content
6
.
Evaluation is guided by a list of 10,000 most frequent words from SoNaR, a 500M-word reference
corpus for Dutch.
7
Every word is compared to other words in the same cluster, and the average similarity
for all comparisons is taken as the final score. The described method is well suited for measuring
intracluster quality, yet useful information about word similarity is available also by looking at neighboring
4
For English, several semantic similarity datasets are available (such as WordSimilarity-353 (Finkelstein et al., 2001)), some
of which can identify the type of relatedness captured. We are not aware of such datasets for Dutch.
5
Which is a function of the IC of the least common subsumer of two synsets and the IC of individual synsets. The score
ranges between 0 and 1.
6
Which is the negative logarithm of (|L|+ 1)
?1
((|L
s
|/|S
s
|) + 1), where L are the leaves of the hierarchy, L
s
are the leaves
reachable from a synset s, and S
s
are the subsumers of s (S?anchez et al., 2011).
7
http://lands.let.ru.nl/projects/SoNaR
1384
clusters in the binary tree. This intercluster quality, according to which clusters that are close in the
binary tree are more similar than clusters that are far apart, can be captured indirectly by evaluating using
different bit substrings. In this way, when a substring is used, two or more semantically related, but
isolated clusters are merged, which should result in a drop in clustering quality (semantic relatedness
tends to ?dissolve? when merging).
For both standard and dependency Brown clustering, the same set of sentences is used. From SoNaR,
we sampled sentences amounting to roughly 46M words, which is comparable to the count for English
datasets of Koo et al. (2008) and Turian et al. (2010). The sentence length was restricted to five or more
words to exclude noisy text. Corpus annotation was removed.
For dependency clustering, the dataset was lemmatized and parsed with the Alpino parser (Van Noord,
2006), an HPSG parser with a maxent disambiguation component, achieving labeled dependency accuracy
of around 90.5 for Dutch.
8
The parsing accuracy is likely to be lower on our dataset, but we expect this
effect to be small since Alpino has been shown to be relatively insensitive to domain shifts compared
to some entirely data-driven parsers (Plank and van Noord, 2010). For default clustering, we only use
first-order dependencies produced by the parser. The bilexical counts (head and dependent regardless of
the relation label) serve as input for dependency clustering.
5 Experiments and Results
The main parameter for word clustering is the number of clusters k, which we set to either 1000 or 3200,
9
except when measuring clustering capacity, for which smaller values of k are used. Additionally, we limit
the minimum frequency of words in clustering to three, unless stated otherwise. The vocabulary size for
k=1000 clustering with applied frequency threshold is around 237,000. We use a paired t-test to check for
statistical significance of observed differences in means.
5.1 Cluster examples
In Table 1, we show both the versatility of dependency clusters by dividing the examples in five groups
(A?E), and the similarity of clusters within group. The longer the common bit substring between clusters,
the closer they are in the hierarchy. Group A includes words describing professions or people?s roles
and functions. Group B lists personal pronouns, including reflexive pronouns (B2), where substantial
differentiation exists with many singleton clusters. Clusters are capable of grouping orthographic variants
(D1; email and e-mail) and diminutives (sms DIM, corresponding to Dutch smsje). Because first and last
names are extremely common in our corpus, clustering creates fine-grained distinctions between these
(C). C1 groups names of presidents, whereas C2 and C3 distinguish between feminine and masculine
names. Measurable concepts are included in E.
5.2 Cluster quality
Table 2 presents the general quality of standard and dependency clustering. The results for 1000 and 3200
clusters (in the latter we use a higher frequency threshold for faster computation) show that we obtain
a higher similarity score for 3200 clusters compared to 1000, and a more marked difference between
standard and dependency clustering in the case of k=3200 (?=0.019). We also looked at how many
words from the frequency list were evaluated successfully. The recall depends on the success of mapping
between words and synsets as well as the success of finding the word in one of the clusters. The latter
factor influences the recall to a much lesser degree, as almost all words are found in the clustering.
For 3200 clusters with the minimum frequency set to fifty, approximately 5000 words are successfully
evaluated, whereas for 1000 clusters, this number is around 7000.
10
These numbers are not affected by
the type of clustering (standard or dependency).
8
Strictly speaking, the output of lemmatization is root forms. We perform this preprocessing step to increase the number of
times that a word is successfully matched in the wordnet hierarchy and evaluated.
9
Which are standardly encountered throughout the literature. For k above 3200, the algorithm falls short of practicality on
current hardware assuming a single-core implementation.
10
The difference between the figures occurs because of a different frequency threshold.
1385
Group Cluster id Most frequent words Left
A1 001010001011100
aannemer,
contractor,
huis arts,
family doctor,
bakker,
baker,
notaris,
lawyer,
apotheker,
pharmacist,
makelaar
estate agent
+57
A2 001010001011011
analist,
analyst,
criticus,
reviewer,
waarnemer,
observer,
kenner,
expert,
commentator,
commentator,
mens recht organisatie
human rights organization
+8
A3 0010100010111110
ondernemer,
entrepreneur,
zakenman,
businessman,
bedrijf leider,
manager,
zelfstandige,
self-employed,
koopman,
merchant,
starter
starter
+18
B1 011101111011110
mij
me
0
B2 01110111101110
zichzelf,
him/herself,
mezelf,
myself,
jezelf,
yourself,
onszelf,
ourselves,
mijzelf,
myself,
uzelf
yourself
0
B3 01110111101101
hem
him
0
B4 01110111101100
hen
them
0
C1 00110010010
Bush,
Bush,
Obama,
Obama,
Clinton,
Clinton,
Poetin,
Putin,
Chirac,
Chirac,
Sarkozy
Sarkozy
+95
C2 0011000111010
Sarah, Kim, Nathalie, Justine, Kirsten, Tia, Eline
+12
C3 0011000111011
David, Jimmy, Benjamin, Samuel, Tommy, Sean
+98
D1 001011100010101
email, mail, sms, sms DIM, e-mail, mail DIM
+13
D2 001011100010100
telefoon,
telephone,
satelliet,
satellite,
telefonie,
telephony,
telefoon lijn,
telephone line,
Explorer,
Explorer,
muziek speler,
music player,
iTunes
iTunes
+7
E 001000010110101
inkomen,
income,
energie verbruik,
energy consumption,
minimum loon,
minimum wage,
cholesterol,
cholesterol,
opleidingsniveau,
level of education,
IQ,
IQ,
alcohol gehalte
alcohol content
+32
Table 1: Example dependency clusters obtained from a run with number of clusters set to 3200 and
minimum frequency to 50. The underlined part of the bit string indicates the longest common substring
within one group. English translation of the Dutch original is given in italics and is left out when clear
from the original. Column Left indicates the remaining number of (less frequent) words in the cluster.
k Brown DepBrown ?
1000 0.191 0.196 +.005*
3200 0.279 0.298 +.019**
Table 2: Lin similarity scores for standard Brown clustering and dependency Brown clustering (DepBrown),
with k the number of clusters. ?=DepBrown ? Brown. Frequency threshold of 50 is used for clustering
with k = 3200. *: statistically significant with p < 0.05, **: statistically significant with p < 0.001.
Results for four different clustering parametrizations are shown in Table 3. One way of controlling the
granularity is to choose the number of output clusters k. As shown in the table under CAP (?capacity?),
dependency clustering achieves a better quality regardless of the choice of k, and in general, choosing
a smaller k decreases quality, which is compatible with the observations of Turian et al. (2010) in their
chunking experiments.
An effect similar to that of controlling capacity can be achieved by making use of the fact that the
induced structure is a hierarchy.
11
By choosing a path prefix length that is shorter than the maximum
length, we control the cluster granularity (denoted in the table as PREF-*). For different tasks, different
path prefixes might be appropriate (Sun et al., 2011; Koo et al., 2008; Miller et al., 2004). For example,
one might prefer coarser distinctions (i.e. shorter bit strings) in parsing, while finer granularity might be
necessary to obtain effective representations of proper names in NER. We ran the experiment with prefix
length ranging from one to eighteen, and show a selection of four settings in the table. Across the board,
dependency clustering yields better results than standard clustering. Naturally, with shorter prefixes the
quality decreases, which is explained by increasing word population in the clusters, with more and more
11
The parameter k needs to be chosen before clustering, whereas the hierarchical structure can be exploited during feature
preparation based on already existing clusters.
1386
Setting k min Brown DepBrown ?
CAP
200 10 0.148 0.157 +.009
400 10 0.169 0.175 +.006
600 10 0.182 0.191 +.009
800 10 0.191 0.205 +.014
PREF-16 1000 10 0.2 0.215 +.015
PREF-12 1000 10 0.187 0.202 +.015
PREF-8 1000 10 0.159 0.168 +.009
PREF-4 1000 10 0.114 0.127 +.013
FREQ
1000 5 0.196 0.204 +.008
1000 10 0.202 0.216 +.014
1000 20 0.206 0.221 +.015
1000 30 0.209 0.224 +.015
1000 50 0.216 0.227 +.011
NOUNS 1000 3 0.272 0.279 +.007
Table 3: Lin similarity scores for standard Brown clustering and dependency Brown clustering (DepBrown),
with k the number of clusters, min the minimum frequency of words. CAP: varying k, fixed min; FREQ:
varying min, fixed k; NOUNS: evaluating only nouns, PREF-n: size of bit-string prefix, ?=DepBrown ?
Brown. All the results reported for DepBrown are significantly different from Brown with p < 0.001.
distant (both hierarchically and semantically) clusters being merged.
By inspecting individual clusters, we observe that frequent words in a cluster exhibit clear semantic
relatedness, but that rare words are often semantically quite unrelated.
12
This is confirmed by our results
in which the quality of the clustering improves approximately logarithmically with frequency threshold
increasing (FREQ). The margin between standard and dependency clustering is also increasing as we
increase the threshold. In practice, Brown clusters appear to be equally useful with a high frequency
threshold (Owoputi et al., 2013) as without thresholding (Koo et al., 2008; Turian et al., 2010).
We also investigate the quality of nouns only, to facilitate the comparison to Van de Cruys (2010). We
observe a considerable gain in quality when only nouns are used compared to using all parts of speech
? the Lin score is increased by 0.08. In the noun-only evaluation, dependency clustering achieves a
higher score (0.279) than standard clustering (0.272). Van de Cruys (2010) shows that syntactic vector
space models outperform window-based models, which is confirmed by our finding for word clustering
as well. In his work, syntactic vector space models yield a 0.04 advantage in Lin score, whereas our
dependency clusters achieve a less marked advantage, reaching up to 0.019 in Lin score. A possible
explanation for this difference is that in his evaluation an average over only five most similar nouns is
taken, whereas we impose no such restriction. We would like to point out that our work does not aim to
compare and discuss the merits of clustering and vector space models as possible techniques for obtaining
word representations, but rather to provide a comprehensive comparison of standard Brown clustering and
its dependency extension.
5.3 Learning curves
Figure 5.2 shows the amount of data needed to achieve a certain quality of clustering. For clustering on
ten thousand sentences the similarity score is around 0.14, with a higher score for standard clustering.
For each subsequent addition of data, dependency clustering outperforms standard clustering. In order to
achieve the highest score attained by standard clustering (0.19), resulting from clustering on 2.4 million
sentences (41 million words), dependency clustering requires only slightly more than 500 thousand
sentences (8.5 million words). This observation is advantageous especially because less data means
12
Although cf. Turian et al. (2010) who show that Brown clustering has a superior representation for rare words than neural
word embeddings in their experiment.
1387
? = 1.9M
l
l
l
l
l
l l
l l l ll
l l
0.14
0.16
0.18
0.20
10K 50K 100K 500K 1M ALL(2.7M)
Number of sentences
Av
er
ag
e 
qu
ali
ty 
(Lin
)
l
Standard Brown
DepBrown (this paper)
Figure 1: Learning curves for standard and dependency Brown clustering with 1000 clusters and a
frequency threshold of 3. Dashed line displays the difference in amount of data needed for DepBrown to
achieve the best quality of Brown. Using all, 2.7 million sentences from the corpus (ALL) corresponds to
46 million words.
shorter running time for clustering as the number of word types is reduced.
5.4 Refinement of dependency clusters
Our dependency clustering described in the previous sections operates on words appearing in all depen-
dency relations. We now investigate whether selecting only a particular dependency relation?i.e. using
as the input both parent and child words from that dependency relation?leads to clusters with higher
semantic relatedness. Each relation can be characterized as either a first- or a second-order relation.
13
A second-order relation is between two words with an intervening preposition, e.g. between a verb and
a noun of a directional complement introduced by a preposition, such as in the Dutch ?eten achter pc?
(?eating at the computer?).
14
We ran clustering for each of the forty-five dependency relations separately
and measured the quality of each resulting clustering. The cumulative baseline that does not distinguish
between dependency relations is given as ALL for first-order relations in Table 4. This is the same result
as reported on the first line in Table 2. The addition of second-order dependencies does not change the
clustering quality of the baseline (0.196) but increases the number of types.
In the upper part of Table 4, we list six relations leading to clustering quality above the baseline.
13
The experiments in previous sections included only first-order relations.
14
The preposition should be seen only as an implicit link between two words and is not included in the input data for clustering.
For the example fragment only ?eating? and ?computer? constitute the data instance actually used by the algorithm.
1388
Type Ord-1 Ord-2 DepBrown Population
OBJ2  0.238 1,622
LD  0.233 2,419
PC  0.211 21,157
LD  0.208 12,149
OBJ1  0.203 108,037
SU  0.199 79,844
ALL  0.196 495,479
ALL   0.196 559,908
SU+OBJ1  0.202 156,645
Table 4: Lin similarity scores for dependency Brown clustering (DepBrown) per type of dependency rela-
tion. Ord-1: first-order relation; Ord-2: second-order relation (with intervening preposition); Population:
number of word types in the clustering.
Two conclusions can be drawn from the results on these relations. First, some dependency relations
contribute better context that leads to increased semantic relatedness compared to clustering without
relation selection. Second, both first- and second-order relations appear among the relations outperforming
the baseline. The highest score from the top six relations is achieved by taking words exclusively from the
second-order secondary object (OBJ2) relation. However, relatively few word types are included in the
clusters. The same is true for the first-order directional complements (LD). Of course, clustering with
only one of these relations would have quite limited applicability if used in a supervised NLP task due
to the low number of word types. However, the main point we want to make here is that these relations
yield semantically superior clusters and demonstrate that syntactic functions truly merit further attention
in learning semantic clusters using syntax. The remaining four among the top six relations are more
frequent relations, and lead to clusterings with higher number of word types. These are the second-order
prepositional complement (PC) and directional complement (LD) relations, and the first-order direct
object (OBJ1) and subject (SU) relations. Finally, the setting SU+OBJ1 joins words obtained from subject
and direct object relations, and achieves a quality that falls between the values obtained for the two
relations separately, yet still increases the number of word types.
6 Conclusion and future work
We have presented a detailed study on a simple extension of Brown clustering with a dependency language
model. In the first part, we have consolidated the advantage of dependency clustering over standard
Brown clustering in a series of experiments, including cluster capacity, granularity level, frequency
thresholding, amount of data and other. In the second part, we put forward the idea of selective clustering
using data obtained only from specific dependency relations. Several relations lead to a clustering with
improved intracluster similarity. We make the code as well as the induced clusters freely available at
https://github.com/rug-compling/dep-brown-cluster.
Our findings from the selective clustering warrant the development of more complex models capable of
including syntactic functions for obtaining semantic clusters. We reserve this work for the future. We find
it interesting to apply dependency Brown clustering to languages of different families and compare it in
this setting to the standard Brown clustering. The future work further includes a study of the effect of
dependency clusters in downstream tasks. Another important point is the effect of parser accuracy on the
quality of obtained clusters.
Acknowledgments
Thanks to C?a?gr? C??oltekin, Gregory Mills, Olga Yeroshina and the anonymous reviewers for valuable
suggestions, and to Percy Liang for implementation-related comments.
1389
References
Jordan Boyd-Graber and David M. Blei. 2008. Syntactic topic models. In NIPS.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18(4):467?479.
Marie Candito and Beno??t Crabb?e. 2009. Improving generative statistical parsing with semi-supervised word
clustering. In IWPT.
Eugene Charniak. 2001. Immediate-head parsing for language models. In ACL.
Wenliang Chen, Min Zhang, and Haizhou Li. 2012. Utilizing dependency language models for graph-based
dependency parsing models. In ACL.
Grzegorz Chrupala. 2011. Efficient induction of probabilistic word classes with LDA. In IJCNLP.
Manaal Faruqui and Chris Dyer. 2013. An information theoretic approach to bilingual word clustering. In ACL.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.
2001. Placing search in context: The concept revisited. In WWW.
Edouard Grave, Guillaume Obozinski, and Francis Bach. 2013. Hidden Markov tree models for semantic class
induction. In CoNLL.
Gholamreza Haffari, Marzieh Razavi, and Anoop Sarkar. 2011. An ensemble model that combines syntactic and
semantic clustering for discriminative dependency parsing. In ACL.
Fei Huang, Alexander Yates, Arun Ahuja, and Doug Downey. 2011. Language models as representations for
weakly-supervised nlp tasks. In CoNLL.
Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple Semi-supervised Dependency Parsing. In Pro-
ceedings of ACL-08: HLT, pages 595?603, Columbus, Ohio.
Percy Liang. 2005. Semi-supervised learning for natural language. Master?s thesis, Massachusetts Institute of
Technology.
Dekang Lin. 1998. An information-theoretic definition of similarity. In ICML.
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative
training. In HLT-NAACL.
Saeedeh Momtazi, Sanjeev Khudanpur, and Dietrich Klakow. 2010. A comparative study of word co-occurrence
for term clustering in language model-based sentence retrieval. In ACL-HLT.
Anjan Nepal and Alexander Yates. 2014. Factorial Hidden Markov models for learning representations of natural
language. In ICLR.
Gertjan Van Noord. 2006. At Last Parsing Is Now Operational. In TALN.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversational text with word clusters. In HLT-NAACL.
Sebastian Pad?o and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computa-
tional Linguistics, 33:161?199.
Barbara Plank and Alessandro Moschitti. 2013. Embedding semantic similarity in tree kernels for domain adapta-
tion of relation extraction. In ACL.
Barbara Plank and Gertjan van Noord. 2010. Grammar-driven versus data-driven: Which parsing system is more
affected by domain shifts? In NLPLING Workshop.
Kashyap Popat, Balamurali A.R, Pushpak Bhattacharyya, and Gholamreza Haffari. 2013. The haves and the
have-nots: Leveraging unlabelled corpora for sentiment analysis. In ACL.
Martin Popel and David Mare?cek. 2010. Perplexity of n-gram and dependency language models. In TSD.
Kenji Sagae and Andrew S. Gordon. 2009. Clustering words by syntactic similarity improves dependency parsing
of predicate-argument structures. In IWPT.
1390
David S?anchez, Montserrat Batet, and David Isern. 2011. Ontology-based information content computation.
Knowledge-Based Systems, 24(2):297?303.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm
with a target dependency language model. In ACL.
Yongzhe Shi, Wei-Qiang Zhang, Jia Liu, and Michael Johnson. 2013. Rnn language model with word clustering
and class-based output layer. EURASIP Journal on Audio, Speech, and Music Processing, (1).
Andreas Stolcke. 2002. SRILM?an extensible language modeling toolkit. In ICSLP.
Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011. Semi-supervised relation extraction with large-scale word
clustering. In HLT-ACL.
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszkoreit. 2012. Cross-lingual word clusters for direct transfer of
linguistic structure. In HLT-NAACL.
Ivan Titov and Alexandre Klementiev. 2012. A Bayesian approach to unsupervised semantic role induction. In
EACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In ACL.
Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
Tim van de Cruys. 2010. Mining for Meaning: The Extraction of Lexico-semantic Knowledge from Text. Ph.D.
thesis, University of Groningen.
Piek Vossen, Isa Maks, Roxanne Segers, Hennie van der Vliet, Marie-Francine Moens, Katja Hofmann, Erik
Tjong Kim Sang, and Maarten de Rijke, editors, 2013. Cornetto: A Combinatorial Lexical Semantic Database
for Dutch. Springer.
1391
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 902?912,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Using Unknown Word Techniques To Learn Known Words
Kostadin Cholakov
University of Groningen
The Netherlands
k.cholakov@rug.nl
Gertjan van Noord
University of Groningen
The Netherlands
g.j.m.van.noord@rug.nl
Abstract
Unknown words are a hindrance to the perfor-
mance of hand-crafted computational gram-
mars of natural language. However, words
with incomplete and incorrect lexical entries
pose an even bigger problem because they can
be the cause of a parsing failure despite being
listed in the lexicon of the grammar. Such lex-
ical entries are hard to detect and even harder
to correct.
We employ an error miner to pinpoint words
with problematic lexical entries. An auto-
mated lexical acquisition technique is then
used to learn new entries for those words
which allows the grammar to parse previously
uncovered sentences successfully.
We test our method on a large-scale grammar
of Dutch and a set of sentences for which this
grammar fails to produce a parse. The appli-
cation of the method enables the grammar to
cover 83.76% of those sentences with an ac-
curacy of 86.15%.
1 Introduction
In this paper, we present an automated two-phase
method for treating incomplete or incorrect lexical
entries in the lexicons of large-scale computational
grammars. The performance of our approach is
tested in a case study with the wide-coverage Alpino
grammar (van Noord, 2006) of Dutch. When ap-
plied to real test sentences previously not covered
by Alpino, the method causes a parsing coverage of
83.76% and the accuracy of the delivered analyses
is 86.15%.
The main advantage of our approach is the suc-
cessful combination of efficient error mining and
lexical acquisition techniques. In the first phase, er-
ror mining pinpoints words which are listed in the
lexicon of a given grammar but which nevertheless
often lead to a parsing failure. This indicates that the
current lexical entry for such a word is either wrong
or incomplete and that one or more correct entries
for this word are missing from the lexicon. Our idea
is to treat the word as if it was unknown and, in the
second phase, to employ lexical acquisition (LA) to
learn the missing correct entries.
In the case study presented here, we employ the
iterative error miner of de Kok et al (2009). Since
it has to be run on a large parsed corpus, we have
parsed the Flemish Mediargus corpus (?1.5 billion
words) with Alpino. The reason for this choice is
the relatively large lexical difference between stan-
dard Dutch and Flemish. This increases the chance
to encounter words which are used in Flemish in a
way not handled by Alpino yet.
For example, the word afwater (to drain) is listed
as a first person singular present verb in the Alpino
lexicon. However, the error miner identifies this
word as the reason for the parsing failure of 9 sen-
tences. A manual examination reveals that the word
is used as a neuter noun in these cases? het afwater
(the drainage). Since there is no noun entry in the
lexicon, Alpino was not able to produce full-span
analyses.
After the error miner identifies afwater as a prob-
lematic word, we employ our machine learning
based LA method presented in Cholakov and van
Noord (2010) to learn new entries for this word.
This method has already been successfully applied
to the task of learning lexical entries for unknown
words and, as the error miner, it can be used ?out of
the box?. LA correctly predicts a neuter noun en-
902
try for afwater and the addition of this entry to the
lexicon enables Alpino to cover the 9 problematic
sentences from the Mediargus corpus.
It should be noted that since our approach cannot
differentiate between incomplete and incorrect en-
tries, no entry in the lexicon is modified. We simply
add the lexical entries which, according to the LA
method, are most suitable for a given problematic
word and assume that, if these entries are correct,
the grammar should be able to cover previously un-
parsable sentences in which the word occurs.
The remainder of the paper is organised as fol-
lows. Section 2 describes the error miner. Section
3 presents the Alpino grammar and parser and the
LA technique we employ. Section 4 describes an
experiment where error mining is performed on the
Mediargus corpus and then, LA is applied to learn
new lexical entries for problematic words. Section
5 discusses the effect which the addition of the new
entries to the lexicon has on the parsing coverage
and accuracy. Section 6 provides a comparison be-
tween our approach and previous work similar in na-
ture. This section also discusses the application of
our method to other systems and languages as well
as some ideas for future research.
2 Error Mining
The error miner of de Kok et al (2009) combines the
strengths of the error mining methods of van Noord
(2004) and Sagot and de la Clergerie (2006). The
idea behind these methods is that grammar errors
lead to the parsing failure of some grammatical sen-
tences. By running the grammar over a large corpus,
the corpus can be split into two subsets? the set of
sentences which received a full-span parse and the
set of sentences failed to parse. Words or n-grams
which occur in the latter set have a suspicion of be-
ing the cause of parsing failures.
van Noord (2004) defines the suspicion of a word
sequence as:
(1) S(wi...wj) =
C(wi...wj |error)
C(wi...wj)
where C(wi...wj) is the number of sentences
which the sequence wi...wj occurs in and
C(wi...wj |error) is the number of occurrences of
the sequence in unparsable sentences.
While this method performs well in identifying
words and n-grams that are unambiguously suspi-
cious, it also assigns incorrectly a high suspicion
to forms which happen to occur often in unparsable
sentences by ?bad luck?. The iterative error mining
algorithm of Sagot and de la Clergerie (2006) tackles
this problem by taking the following into account:
? If a form occurs within parsable sentences, it
becomes less likely for it to be the cause of a
parsing failure.
? The suspicion of a form depends on the suspi-
cions of the other forms in the unparsable sen-
tences it occurs in.
? A form observed in a shorter sentence is ini-
tially more suspicious than a form observed in
a longer one.
However, because of data sparseness problems, this
method is only able to handle unigrams and bigrams.
Another potential problem is the absence of criteria
to determine when to use unigrams and when bi-
grams to represent forms within a given sentence.
Consider the trigram w1, w2, w3 where w2 is the
cause of a parsing failure. In this case, the whole
trigram as well as the bigrams w1, w2 and w2, w3
will become suspicious which would prevent the un-
igram w2 from ?manifesting? itself.
To avoid this problem, de Kok et al (2009) uses
a preprocessor to the iterative miner of Sagot and
de la Clergerie (2006) which iterates through a sen-
tence of unigrams and expands unigrams to longer
n-grams when there is evidence that this is useful. A
unigram w1 is expanded to a bigram w1, w2 if this
bigram is more suspicious than both of its unigrams.
The general algorithm is that the expansion to an n-
gram i...j is allowed when the following two condi-
tions are fulfilled:
(2) S(i...j) > S(i...j ? 1) ? expFactor
S(i...j) > S(i + 1...j) ? expFactor
Within the preprocessor, suspicion is defined as
shown in (1) and the expFactor is a parameter spe-
cially designed to deal with data sparseness.
As the error mining technique of de Kok et al
(2009) successfully overcomes the problems which
903
the other error mining methods we discussed en-
counter, we have chosen to employ this technique
in our experiment.
3 Automated Lexical Acquisition
3.1 The Alpino Grammar and Parser
Since we employ Alpino for the purposes of our case
study, it is convenient to explain the LA method we
have chosen to use in the context of this system.
The Alpino wide-coverage parser is based on a
large stochastic attribute value grammar. The gram-
mar takes a ?constructional? approach, with rich
lexical representations stored in the lexicon and a
large number of detailed, construction specific rules
(about 800).
Currently, the lexicon contains over 100K lexical
entries and a list of about 200K named entities. Each
word is assigned one or more lexical types. For
example, the verb amuseert (to amuse) is assigned
two lexical types? verb(hebben,sg3,intransitive) and
verb(hebben,sg3,transitive)? because it can be used
either transitively or intransitively. The other type
features indicate that it is a present third person sin-
gular verb and it forms perfect tense with the auxil-
iary verb hebben.
3.2 Learning Algorithm
The goal of the LA method we describe Cholakov
and van Noord (2010) is to assign correct lexical
type(s) to a given unknown word.
It takes into account only open-class lexical types:
nouns, adjectives and verbs. The types considered in
the learning process are called universal types1.
For a given word, a maximum entropy (ME)
based classifier takes various morphological and
syntactic features as input and outputs a ranked list
of lexical types. The probability of a lexical type t,
given an unknown word and its context c is:
(3) p(t|c) =
exp(
?
i
?ifi(t,c))?
t??T
exp(
?
i
?ifi(t?,c))
where fi(t, c) may encode arbitrary characteristics
of the context and < ?1,?2, ... > is a weighting
parameter which maximises the entropy and can be
1The adjectives can be used as adverbs in Dutch and thus,
the latter are not considered to be an open class.
Features
i) a, af, afw, afwa
ii) r, er, ter, ater
iii) particle yes #in this case af
iv) hyphen no
v) noun?het,sg?, verb?sg1?
vi) noun(het,count,sg), noun(de,count,pl)
vii) noun(het), noun(count), noun(sg), noun(de)
noun(pl)
Table 1: Features for afwater
evaluated by maximising the pseudo-likelihood on a
training corpus (Malouf, 2002).
Table 1 shows the features for afwater, the word
we discussed in Section 1. Row (i) contains 4 sepa-
rate features derived from the prefix of the word and
4 other suffix features are given in row (ii). The two
features in rows (iii) and (iv) indicate whether the
word starts with a particle and if it contains a hy-
phen, respectively.
Further, the method we describe in Cholakov
and van Noord (2009) is applied to generate the
paradigm(s) of each word in question. This method
uses a finite state morphology to generate possible
paradigm(s) for a given word. The morphology does
not have access to any additional linguistic infor-
mation and thus, it generates all possible paradigms
allowed by the word orthography. Then, the num-
ber of search hits Yahoo returns for each form in
a given paradigm is combined with some simple
heuristics to determine the correct paradigm(s). The
web search heuristics are also able to determine the
correct definite article (de or het) for words with
noun paradigms.
One verb and one noun paradigm are generated
for afwater. In these paradigms, afwater is listed as
a first person singular present verb form and a sin-
gular het noun form, respectively. This information
is explicitly used as features in the classifier which
is shown in row (v) of Table 1.
Next, syntactic features for afwater are obtained
by extracting a number of sentences which it oc-
curs in from large corpora or Internet. These sen-
tences are parsed with a different ?mode? of Alpino
where this word is assigned all universal types, i.e. it
is treated as being maximally ambiguous. For each
sentence only the parse which is considered to be the
best by the Alpino statistical disambiguation model
904
is preserved. Then, the lexical type that has been
assigned to afwater in this parse is stored. During
parsing, Alpino?s POS tagger (Prins and van Noord,
2001) keeps filtering implausible type combinations.
For example, if a determiner occurs before the un-
known word, all verb types are typically not taken
into consideration. This heavily reduces the compu-
tational overload and makes parsing with universal
types computationally feasible.
When all sentences have been parsed, a list can
be drawn up with the types that have been used and
their frequency:
(4) noun(het,count,sg) 54
noun(de,count,pl) 7
tmp noun(het,count,sg) 4
adjective(no e(adv)) 4
proper name(sg,?ORG?) 1
The lexical types assigned to afwater in at least 80%
of the parses are used as features in the classifier.
These are the two features in row (vi) of Table 1.
Further, as illustrated in row (vii), each attribute of
the considered types is also taken as a separate fea-
ture.
After the classifier predicts lexical types for each
word, these predictions are subject to two additional
steps of processing. In the first one, the generated
word paradigms are explicitly used as a filtering
mechanism. When a word is assigned a verb or an
adjective type by the classifier but there is no verb or
adjective paradigm generated for it, all verb or ad-
jective predictions for this word are discarded.
The output of this ?filtering? is further processed
in the second step which deals with the correct
prediction of subcategorization frames for verbs.
Following the observations made in Korhonen et
al. (2000), Lapata (1999) and Messiant (2008),
Cholakov and van Noord (2010) employ a maximum
likelihood estimate (MLE) from observed relative
frequencies with an empirical threshold to filter out
low probability frames.
Since some frames could be very infrequent and
the MLE method may not capture them, the gener-
ated word paradigms are used to increase the num-
ber of contexts observed for a given verb. Addi-
tional sentences are extracted for each form in the
paradigm of a given word predicted to be a verb.
These sentences are again parsed with the universal
types. Then we look up the assigned universal verb
types, calculate the MLE for each subcategorization
frame and filter out frames with MLE below some
empirical threshold.
4 Learning New Lexical Entries
Before we start with the description of the exper-
iment, it is important to note that Alpino is very
robust? essentially, it always produces a parse. If
there is no analysis spanning the whole sentence,
the parser finds all parses for each substring and re-
turns what it considers to be the best sequence of
non-overlapping parses. However, in the context of
this experiment, a sentence will be considered suc-
cessfully parsed only if it receives a full-span anal-
ysis. For the sake of clarity, from now on we shall
use the terms coverage and cover only with regard
to such sentences. The term parsing failure shall re-
fer to a sentence for which Alpino fails to produce a
full-span analysis.
4.1 Error Mining on Mediargus
The first step in our experiment is to perform er-
ror mining on the Mediargus corpus. The corpus
consists of texts from Flemish newspapers from the
period between 1998 and 2007. It contains about
1.5 billion words (?78M sentences). The corpus
has been parsed with Alpino and the parsing results
are fed into the error miner of de Kok et al (2009).
The parser has not produced a full-span analysis for
7.28% of the sentences (?5.7M sentences).
When finished, the error miner stores the results
in a data base containing potentially problematic n-
grams. Each n-gram is linked to its suspicion score
and the sentences which it occurs in and which were
not covered by Alpino.
Before proceeding with LA, however, we should
identify the n-grams which are indicative for a prob-
lem in the lexicon. The first step in this direction
is to extract all unigrams from the data base which
have a suspicion equal to or greater than 0.7 together
with the uncovered sentences they occur in. This
resulted in a list containing 4179 unique unigrams.
Further, we select from this list only those unigrams
which have lexical entries in the Alpino lexicon and
occur in more than 5 sentences with no full-span
905
parse. Sometimes, the error miner might be wrong
about the exact word which causes the parsing fail-
ure for a given sentence. The 5 sentences empiri-
cal threshold is meant to guarantee that the selected
words are systematically causing problems for the
parser.
The result of this selection is 36 unigrams (words)
which occur in a total of 388 uncovered sentences?
an average of 10.78 sentences per word. The small
number of selected words is due to the fact that
most of the problematic 4179 unigrams represent to-
kenization errors (two or more words written as one)
and spelling mistakes which, naturally, are not listed
in the Alpino lexicon. Very few of the 4179 uni-
grams are actual unknown words. Table 2 shows
some of the problematic unigrams and their suspi-
cions.
opVorig 0.898989
GentHoewel 0.89759
Nieuwpoortl 0.897414
SportTijdens 0.897016
DirvenDe 0.896428
mistrap 0.896038
Dwoeurp 0.896013
passerde 0.89568
doorHugo 0.893901
goedkmaken 0.892407
ManneN 0.891539
toegnag 0.891523
Table 2: Problematic unigrams and their suspicions
It can be seen immediately that most of the uni-
grams presented in the table are tokenization errors.
There are also some typos. The unigram passerde
should be written as passeerde, the past singular
verb form of the verb ?to pass? and toegnag is the
misspelled noun toegang (access). The only prob-
lematic unigram with a lexical entry in the Alpino
lexicon is mistrap (misstep, to misstep).
Although the experiment setup yields a small test
set, we employ it because the words in this set repre-
sent ?clear-cut? cases. This allows us to demonstrate
better the effect of our technique.
4.2 Applying Lexical Acquisition
Our assumption is that incomplete or incorrect lex-
ical entries prevented the production of full-span
parses for the 388 sentences in which the 36 prob-
lematic words pinpointed by the error miner oc-
cur. That is why, in the second step of the exper-
iment, these words are temporarily removed from
the Alpino lexicon, i.e. they are treated as unknown
words, and we employ the LA method presented in
the previous section to learn offline new lexical en-
tries for them.
The setup for the learning process is exactly the
same as in Cholakov and van Noord (2010). The set
of universal types consists of 611 types and the ME-
based classifier has been trained on the same set of
2000 words as in Cholakov and van Noord (2010).
Those types predicted by the classifier which ac-
count together for less than 5% of probability mass
are discarded.
In order to increase the number of observed con-
texts for a given word when parsing with the univer-
sal types, up to 100 additional sentences in which the
word occurs are extracted from Internet. However,
when predicting new lexical entries for this word,
we want to take into account only sentences where
it causes a parsing failure. It is in such sentences
where a new lexical entry can be learnt through LA.
For example, the LA method would be able to pre-
dict a noun entry for afwater if it focuses only on
contexts where it has a noun reading, i.e. on sen-
tences not covered by Alpino.
That is why, the sentences we extracted from In-
ternet are first parsed with the standard Alpino con-
figuration. When averaging over the 36 sentence
sets, it turns out that Alpino has been able to cover
10.05% of the sentences. Although we cannot be
sure that the 36 words are the cause of a parsing
failure in each of the uncovered sentences, this low
coverage indicates once more that Alpino has sys-
tematic problems with sentences containing these
words.
Then, the uncovered sentences from Internet to-
gether with the 388 problematic sentences from the
Mediargus corpus are parsed with Alpino and the
universal types. For example, the list of univer-
sal types assigned to afwater in (4) contains mostly
noun types, i.e. the kind of types which are currently
not in the lexicon for this word and which we want
to learn.
The result of the LA process is the prediction of
a total of 102 lexical types, or 2.83 types per word.
This high number is due to the fact that 25 words
receive verb predictions. Since a verb can have vari-
906
ous subcategorization frames, there is one type as-
signed for each frame. For example, inscheppen
(to spoon in(to)) receives 3 types which differ only
in the subcategorization frame? verb(hebben,inf,tr.),
verb(hebben,inf,intr.) and verb(hebben,inf,np np).
However, the infinitive in Dutch is also the
form for plural present and inscheppen correctly
receives 3 more predictions? verb(hebben,pl,tr.),
verb(hebben,pl,intr.) and verb(hebben,pl,np np).
Let us examine the most frequent types of lexicon
errors for the 36 problematic words by looking at
the current Alpino lexical entries for some of these
words and the predictions they receive from the LA
method. The original Alpino entries for 19 of the
25 words predicted to be verbs are a product of a
specific lexical rule in the grammar. Consider the
following sentences:
(5) a. Ik
I
schep
spoon
de
the
soep
soup
in
in
de
the
kom.
bowl
?I spoon the soup into the bowl.?
b. dat
that
ik
I
de
the
soep
soup
de
the
kom
bowl
in
in
schep
spoon
?that I spoon the soup into the bowl?
c. dat
that
ik
I
de
the
soep
soup
de
the
kom
bowl
inschep
in spoon
?that I spoon the soup into the bowl?
We see in (5-b) that the preposition in is used as a
postposition in the relative clause. However, in such
cases, there is linguistic evidence that in behaves as
a separate verb particle. That is why, as shown in
(5-c), people sometimes write in and the verb to-
gether when they occur next to each other in the sen-
tence. To account for this, Alpino employs a special
lexical rule. This rule assigns a certain type of sub-
categorization frame to verbs like inscheppen where
a postposition can be interpreted as a separable par-
ticle. That subcategorization frame requires a noun
phrase (?the soup? in (5-c)) and a locative NP (?the
bowl? in (5-c)).
However, in some cases, the entries generated by
this lexical rule cannot account for other possible us-
ages of the verbs in question. For example,
(6) U
you
moet
must
deze
this
zelf
yourself
inscheppen.
spoon in.INF
?You have to spoon this in yourself.?
Alpino fails to parse this sentence because inschep-
pen is used without a locative NP. Now, when the
LA method has predicted a transitive verb type for
inscheppen, the parser should be able to cover the
sentence. Other such examples from our data in-
clude wegwist (to erase.3PER.SG), onderligt (to lie
under.3PER.SG), etc.
Further, there are 10 words, including afwater,
which represent cases of nominalisation currently
not accounted for in the Alpino lexicon. The
LA process correctly predicts noun types for these
words. This should enable the parser to cover sen-
tences like:
(7) Die
this
moet
must
een
a
deel
part
van
from
het
the
afwater
drainage
vervoeren.
transport/move
?This has to move a part of the drainage.?
where afwater is used as a noun.
There are also 3 words which correctly receive
adjective predictions. Currently, their lexical en-
tries are incomplete because they are assigned only
past participle types in the lexicon. However, past
participles in Dutch can also act as adjectives. For
historical reasons, this systematic ambiguity is not
treated as such in Alpino. Each participle should
also have a separate adjective lexical entry but, as
we see, this is not always the case.
5 Results
After LA is finished, we restore the original lexical
entries for the 36 words but, additionally, each word
is also assigned the types which have been predicted
for it by the LA method. The 388 problematic sen-
tences from the Mediargus corpus are then re-parsed
with Alpino. We are interested in observing:
1. how many sentences receive a full-span analy-
sis
2. how the parsing accuracy of Alpino changes
Table 3 shows that when the Alpino lexicon is ex-
tended with the lexical entries we learnt through LA,
the parser is able to cover nearly 84% of the sen-
tences, including the ones given in (6) and (7). Since
there is no suitable baseline which this result can
be compared to, we developed an additional model
which indicates what is likely to be the maximum
coverage that Alpino can achieve for those sentences
by adding new lexical entries only.
907
In this second model, for each of the 36 words, we
add to the lexicon all types which were successfully
used for the respective word during the parsing with
universal types. In this way, Alpino is free to choose
from all types it has considered suitable for a given
word, i.e. the parser is not limited by the outcome
of the LA process but rather by the overall quality of
the grammar.
The ?universal types? model performs better than
ours? it achieves 87.9% coverage. Still, the perfor-
mance of our model is close to this result, i.e. close
to what we consider to be the maximal possible cov-
erage of Alpino for these 388 sentences when only
LA is used.
Model Coverage (%)
Our model (Alpino + LA) 83.76
Universal types 87.89
Table 3: Coverage results for the re-parsed 388 problem-
atic sentences
Some of the sentences which cannot be covered
by both models are actually not proper sentences
but fragments which were wrongly identified as sen-
tences during tokenization. Many other cases in-
clude sentences like:
(8) Een
a
gele
yellow
frommel
crease
papier,
paper
Arabische
Arabic
lettertekens.
characters
?A yellow paper crease, Arabic characters.?
which is probably the caption of a photo or an illus-
tration. However, because of the absence of a verb,
Alpino splits the analysis into two parts? the part be-
fore the comma and the part after the comma.
Here is a more interesting case:
(9) Als
when
we
we
ons
us
naar
to
de
the
buffettafel
buffet
begeven,
proceed
mistrap
misstep
ik
I
me.
myself
?When we proceed to the buffet I misstep.?
The LA method does not predict a reflexive verb
type for mistrap which prevents the production of
a full-span analysis because Alpino cannot connect
the reflexive pronoun me to mistrap. In this case,
however, the universal type model outperforms ours.
A reflexive verb type is among the universal types
and thus, Alpino is able to use that type to deliver a
full-span parse. We should note though, that LA cor-
rectly predicts a noun type for mistrap which enables
Alpino to parse successfully the other 14 sentences
which this word occurs in.
Let us now look at the correctness of the deliv-
ered parses. To estimate the accuracy of the parser,
we have randomly selected 100 sentences out of the
388 sentences in the test set and we have manually
annotated them in order to create a gold standard for
evaluation.
Accuracy in Alpino is measured in terms of de-
pendency relations. The accuracy for sentences
which are not assigned a full-span analysis but a se-
quence of non-overlapping parses can still be larger
than zero because, within these parses, some cor-
rect dependency relations could have been produced.
That is why, though the coverage of Alpino for the
selected 100 sentences is zero, we can still obtain
a number for accuracy and use it as a baseline for
comparison. Clearly, this baseline is expected to per-
form worse than both our model and the universal
types one since those are able to cover most of the
sentences and thus, they are likely to produce more
correct dependency relations. However, it gives us
an idea how much extra quality is gained when cov-
erage improves.
The accuracy results for the 100 annotated sen-
tences are given in Table 4. The average sentence
length is 18.9 tokens.
Model Accuracy (%) msec/sentence
Alpino 63.35 803
Our model 86.15 718
Universal types 85.12 721
Table 4: Accuracy results for the 100 annotated sentences
Our model achieves the highest accuracy without
increasing the parse times. Further, the baseline has
a much lower result which shows that coverage is
not gained on the expense of accuracy.
Our model and the universal types one achieve the
same accuracy for most of the sentences. However,
the universal types model has an important disad-
vantage which, in some cases, leads to the produc-
tion of wrong dependency relations. The model pre-
dicts a large number of lexical types which, in turn,
leads to large lexical ambiguity. This lexical am-
biguity increases the number of possible analyses
Alpino chooses from, thus making it harder for the
908
parser to produce the correct analysis. Let us con-
sider the following example where a sentence is cov-
ered by both models but the universal types model
has lower accuracy:
(10) Dat
that
wij
we
het
it
rechttrokken,
straighten.PAST.PL.
pleit
plead
voor
for
onze
our
huidige
current
conditie.
condition
?It pleads for our condition that we straightened it.?
Here, het is the object of the verb rechttrokken.
However, although there are transitive verb types
among the universal types assigned to rechttrokken,
Alpino chooses to use a verb type which subcate-
gorizes for a measure NP. This causes for het to be
analysed not as an object but as a measure comple-
ment, i.e. the produced dependency relation is incor-
rect.
The LA method, on the other hand, is much more
restrictive but its predictions are also much more ac-
curate. Since it considers sentences containing other
forms of the paradigm of rechttrokken when predict-
ing subcategorization frames, the LA method cor-
rectly assigns only one transitive and one intransitive
verb type to this word. This allows Alpino to recog-
nize het as the object of the verb and to produce the
correct dependency relation.
The few cases where the universal types model
outperforms ours include sentences like the one
given in (9) where the application of our model
could not enable Alpino to assign a full-span analy-
sis. Sometimes, the LA method is too restrictive and
does not output some of the correct types. These
types, on the other hand, could be provided by the
universal types model and could enable Alpino to
cover a given sentence and thus, to produce more
correct dependency relations. Allowing for the LA
method to predict more types, however, has proven
to be a bad solution because, due to the increased
lexical ambiguity, this leads to lower parsing accu-
racy.
6 Discussion
6.1 Comparison to Previous Work
The performance of the technique we presented in
this paper can be compared to the performance of a
number of other approaches applied to similar tasks.
Zhang et al (2006) and Villavicencio et al (2007)
use error mining to semi-automatically detect En-
glish multiword expressions (MWEs). Then, they
employ LA to learn proper lexical entries for these
MWEs and add them to the lexicon of a large-scale
HPSG grammar of English (ERG; (Copestake and
Flickinger, 2000)). This increases parsing coverage
by 15% to 22.7% for a test set of 674 sentences
containing MWEs and parsed with the PET parser
(Callmeier, 2000). In both studies, however, the
combination of error mining and LA is applied to
a very specific task whereas our method is a general
one.
Nicolas et al (2008) employ a semi-automatic
method to improve a large-scale morphosyntactic
lexicon of French (Lefff ; (Sagot et al, 2006)).
The lexicon is used in two grammars? the FRMG
(Thomasset and de la Clergerie, 2005), a hybrid Tree
Adjoining/Tree Insertion Grammar, and the SxLFG-
FR LFG grammar (Boullier and Sagot, 2006). The
first step in this approach is also the application of an
error miner (Sagot and de la Clergerie, 2006) which
uses a parsed newspaper corpus (about 4.3M words)
to pinpoint problematic unigrams.
The crucial difference with our method is in the
second step. Nicolas et al (2008) assign underspec-
ified lexical entries to a given problematic unigram
to allow the grammar to parse the uncovered sen-
tences associated with this unigram. Then, these en-
tries are ranked based on the number of successful
parses they have been used in.
The use of underspecification, however, causes
large ambiguity and severe parse overgeneration
(observed also in Fouvry (2003)). As a consequence
of that, the ranked list of lexical entries for each un-
igram is manually validated to filter out the wrong
entries. The employment of LA in our approach, on
the other hand, makes it fully automatic. The rank-
ing of the predictions is done by the classifier and
the predicted entries are good enough to improve the
parsing coverage and accuracy without any manual
work involved. Generally, recent studies (Baldwin,
2005; Zhang and Kordoni, 2006; Cholakov et al,
2008; Cholakov and van Noord, 2010) have clearly
shown that when it comes to learning new lexical
entries, elaborate LA techniques perform better and
are more suitable for large-scale grammars than un-
909
derspecification2.
Further, the naive ranking system used in Nicolas
et al (2008) puts a correctly generated entry for an
infrequent usage of a given word (e.g., a verb with
a rare subcat frame) in the bottom of the ranked list
because of the low number of sentences in which
this entry is used. The LA method we employ is
more sensitive to rare usages of words because it
considers occurrences of the word in question out-
side the parsed corpus (very important if the corpus
is domain-specific) and it also takes into account all
forms in the paradigm(s) of the word. This increases
the chances of a rare usage of this word to ?manifest?
itself.
Nicolas et al (2008) uses the lexical entries which
remain after the manual validation to re-parse the
newspaper corpus. 254 words (mostly verbs) are
corrected and the parse coverage increases by 3.4%
and 1.7% for the FRMG and the SxLFG, respec-
tively. However, the authors do not mention how
many of the original uncovered sentences they are
able to cover and therefore, we cannot compare our
coverage result. Nothing is said about the parsing
accuracy. Even with manually validated lexical en-
tries, it is still possible for the grammar to produce
full-span but wrong analyses.
6.2 Application to Other Systems and
Languages
It is important to note that this paper should be
viewed as a case study where we illustrate the re-
sults of the application of what we believe to be a
good algorithm for dealing with incomplete or in-
correct lexical entries? namely, the combination of
error mining and LA. However, our method is gen-
eral enough to be applied to other large-scale gram-
mars and languages.
The error mining is directly usable as soon as
there is a large parsed corpus available. The LA
technique we employed is also quite general pro-
vided that certain requirements are fulfilled. First,
words have to be mapped onto some finite set of la-
bels of which a subset of open-class (universal) la-
bels has to be selected. This subset represents the
labels which can be predicted for unknown words.
2In Nicolas et al (2008) the authors also admit that an elab-
orate LA technique will produce better results.
Second, we need a parser to analyse sentences
in which a given unknown word occurs. Finally,
the ME-based classifier allows for arbitrary com-
binations of features and therefore, any (language-
specific) features considered useful can be included.
As for the paradigm generation method, the idea of
combining a finite state morphology and web heuris-
tics is general enough to be implemented for differ-
ent languages.
We have already started investigating the applica-
bility of our method to the FRMG and a large-scale
grammar of German and the initial experiment and
results we have obtained are promising.
6.3 Future Research
Currently, our algorithm handles only unigrams
(words). However, it would be useful to extend it,
so it can work with longer n-grams. For example,
a given word could have some reading which is not
yet handled in the lexicon only within a particular
bi- or trigram.
Consider the bigram ?schampte af ? which has
been identified as problematic by the error miner.
It represents the particle verb ?afschampte? (to
glance.PAST.SG). Although the lexicon contains a
verb entry for ?schampte?, there is no entry handling
the case when this verb combines with the particle
?af ?. Another example is the bigram ?de slachtoffer?
(the victim). In standard Dutch, the noun ?slachtof-
fer? goes with the ?het? definite article which is
marked in its lexical entry. However, in Flemish it is
used with the ?de? article.
Our method is currently not able to capture these
two cases since they can be identified as problem-
atic on bigram level and not when only unigrams are
considered.
Further, the definition of what the error miner
considers to be a successful parse is a rather crude
one. As we saw, even if the grammar is able to pro-
duce a full-span analysis for a given sentence, this
analysis could still not be the correct one. There-
fore, it is possible that a word could have a prob-
lematic lexical entry even if it only occurs in sen-
tences which are assigned a full-span parse. Cur-
rently, such a word will not be identified as prob-
lematic by the error miner. That is why, some (sta-
tistical) model which is capable of judging the plau-
sibility of a parse should be developed and incorpo-
910
rated in the calculation of the suspicions during error
mining.
References
Tim Baldwin. 2005. Bootstrapping deep lexical re-
sources: Resources for courses. In Proceedings of the
ACL-SIGLEX 2005 Workshop on Deep Lexical Acqui-
sition, Ann Arbor, USA.
Pierre Boullier and Beno??t Sagot. 2006. Efficient parsing
of large corpora with a deep LFG parser. In Proceed-
ings of LREC?06, Genoa, Italy.
Ulrich Callmeier. 2000. PET? a platform for experimen-
tation with efficient HPSG processing techniques. In
Journal of Natural Language Engineering, volume 6,
pages 99?107. Cambridge University Press.
Kostadin Cholakov and Gertjan van Noord. 2009. Com-
bining finite state and corpus-based techniques for
unknown word prediction. In Proceedings of the
7th Recent Advances in Natural Language Processing
(RANLP) conference, Borovets, Bulgaria.
Kostadin Cholakov and Gertjan van Noord. 2010. Ac-
quisition of unknown word paradigms for large-scale
grammars. In Proceedings of the 23rd International
Conference on Computational Linguistics (COLING-
2010), Beijing, China.
Kostadin Cholakov, Valia Kordoni, and Yi Zhang. 2008.
Towards domain-independent deep linguistic process-
ing: Ensuring portability and re-usability of lexicalised
grammars. In Proceedings of COLING 2008 Work-
shop on Grammar Engineering Across Frameworks
(GEAF08), Manchester, UK.
Ann Copestake and Dan Flickinger. 2000. An open-
source grammar development environment and broad-
coverage English grammar using HPSG. In Pro-
ceedings of the 2nd International Conference on Lan-
guage Resource and Evaluation (LREC 2000), Athens,
Greece.
Danie?l de Kok, Jianqiang Ma, and Gertjan van Noord.
2009. A generalized method for iterative error mining
in parsing results. In Proceedigns of the 2009 Work-
shop on Grammar Engineering Across Frameworks,
ACL-IJCNLP 2009, pages 71?79, Singapore.
Frederik Fouvry. 2003. Lexicon acquisition with a large-
coverage unification-based grammar. In Companion
to the 10th Conference of EACL, pages 87?90, Bu-
dapest, Hungary.
Anna Korhonen, Genevieve Gorell, and Diana McCarthy.
2000. Statistical filtering and subcategorization frame
acquisition. In Proceedings of the Joint SIGDAT Con-
ference on Empirical Methods in Natural Language
Processing and Very Large Corpora, Hong Kong,
China.
Mirella Lapata. 1999. Acquiring lexical generalizations
from corpora. A case study for diathesis alternations.
In Proceedings of the 37th Annual Meeting of ACL,
Maryland, USA.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proceed-
ings of the 6th conference on Natural Language Learn-
ing (CoNLL-2002), pages 49?55, Taipei, Taiwan.
Cedric Messiant. 2008. A subcategorization acquisition
system for French verbs. In Proceedings of the ACL
2008 Student Research Workshop, Columbus, OH.
Lionel Nicolas, Beno??t Sagot, Miguel Molinero, Jacques
Farre?, and Eric de la Clergerie. 2008. Computer aided
correction and extension of a syntactic wide-coverage
lexicon. In Proceedings of the 22nd International
Conference on Computational Linguistics (COLING-
2008), pages 633?640, Manchester, UK.
Robbert Prins and Gertjan van Noord. 2001. Unsu-
pervised POS-tagging improves parcing accuracy and
parsing efficiency. In Proceedings of IWPT, Beijing,
China.
Beno??t Sagot and Eric de la Clergerie. 2006. Error min-
ing in parsing results. In Proceedings of the 44th Meet-
ing of the Association for Computational Linguistics
(ACL?06), pages 329?336, Morristown, NJ, USA.
Beno??t Sagot, Lionel Cle?ment, Eric de la Clergerie, and
Pierre Boullier. 2006. The Lefff 2 syntactic lexicon
for French. In Proceedings of LREC?06, Genoa, Italy.
Franc?ois Thomasset and Eric de la Clergerie. 2005.
Comment obtenir plus des me?etagrammaires. In Pro-
ceedings of TALN?05, Dourdan, France.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL?04), pages 446?453, Barcelona,
Spain.
Gertjan van Noord. 2006. At last parsing is now opera-
tional. In Proceedings of TALN, Leuven, Belgium.
Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation and
evaluation of automatically acquired multiword ex-
pressions for grammar engineering. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 1034?1043,
Prague, Czech Republic.
Yi Zhang and Valia Kordoni. 2006. Automated deep
lexical acquisition for robust open text processing. In
Proceedings of the Fifth International Conference on
Language Resourses and Evaluation (LREC 2006),
Genoa, Italy.
Yi Zhang, Valia Kordoni, Aline Villavicencio, and Marco
Idiart. 2006. Automated multiword expression pre-
diction for grammar engineering. In Proceedings of
911
the ACL Workshop on Multiword Expressions: Identi-
fying and Exploiting Underlying Properties, pages 36?
44, Sydney, Australia.
912
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1566?1576,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Effective Measures of Domain Similarity for Parsing
Barbara Plank
University of Groningen
The Netherlands
b.plank@rug.nl
Gertjan van Noord
University of Groningen
The Netherlands
G.J.M.van.Noord@rug.nl
Abstract
It is well known that parsing accuracy suf-
fers when a model is applied to out-of-domain
data. It is also known that the most benefi-
cial data to parse a given domain is data that
matches the domain (Sekine, 1997; Gildea,
2001). Hence, an important task is to select
appropriate domains. However, most previ-
ous work on domain adaptation relied on the
implicit assumption that domains are some-
how given. As more and more data becomes
available, automatic ways to select data that is
beneficial for a new (unknown) target domain
are becoming attractive. This paper evaluates
various ways to automatically acquire related
training data for a given test set. The results
show that an unsupervised technique based on
topic models is effective ? it outperforms ran-
dom data selection on both languages exam-
ined, English and Dutch. Moreover, the tech-
nique works better than manually assigned la-
bels gathered from meta-data that is available
for English.
1 Introduction and Motivation
Previous research on domain adaptation has focused
on the task of adapting a system trained on one do-
main, say newspaper text, to a particular new do-
main, say biomedical data. Usually, some amount
of (labeled or unlabeled) data from the new domain
was given ? which has been determined by a human.
However, with the growth of the web, more and
more data is becoming available, where each doc-
ument ?is potentially its own domain? (McClosky
et al, 2010). It is not straightforward to determine
which data or model (in case we have several source
domain models) will perform best on a new (un-
known) target domain. Therefore, an important is-
sue that arises is how to measure domain similar-
ity, i.e. whether we can find a simple yet effective
method to determine which model or data is most
beneficial for an arbitrary piece of new text. More-
over, if we had such a measure, a related question is
whether it can tell us something more about what is
actually meant by ?domain?. So far, it was mostly
arbitrarily used to refer to some kind of coherent
unit (related to topic, style or genre), e.g.: newspa-
per text, biomedical abstracts, questions, fiction.
Most previous work on domain adaptation, for in-
stance Hara et al (2005), McClosky et al (2006),
Blitzer et al (2006), Daume? III (2007), sidestepped
this problem of automatic domain selection and
adaptation. For parsing, to our knowledge only one
recent study has started to examine this issue (Mc-
Closky et al, 2010) ? we will discuss their approach
in Section 2. Rather, an implicit assumption of all of
these studies is that domains are given, i.e. that they
are represented by the respective corpora. Thus, a
corpus has been considered a homogeneous unit. As
more data is becoming available, it is unlikely that
domains will be ?given?. Moreover, a given corpus
might not always be as homogeneous as originally
thought (Webber, 2009; Lippincott et al, 2010). For
instance, recent work has shown that the well-known
Penn Treebank (PT) Wall Street Journal (WSJ) ac-
tually contains a variety of genres, including letters,
wit and short verse (Webber, 2009).
In this study we take a different approach. Rather
than viewing a given corpus as a monolithic entity,
1566
we break it down to the article-level and disregard
corpora boundaries. Given the resulting set of doc-
uments (articles), we evaluate various ways to au-
tomatically acquire related training data for a given
test set, to find answers to the following questions:
? Given a pool of data (a collection of articles
from unknown domains) and a test article, is
there a way to automatically select data that is
relevant for the new domain? If so:
? Which similarity measure is good for parsing?
? How does it compare to human-annotated data?
? Is the measure also useful for other languages
and/or tasks?
To this end, we evaluate measures of domain sim-
ilarity and feature representations and their impact
on dependency parsing accuracy. Given a collection
of annotated articles, and a new article that we want
to parse, we want to select the most similar articles
to train the best parser for that new article.
In the following, we will first compare automatic
measures to human-annotated labels by examining
parsing performance within subdomains of the Penn
Treebank WSJ. Then, we extend the experiments to
the domain adaptation scenario. Experiments were
performed on two languages: English and Dutch.
The empirical results show that a simple measure
based on topic distributions is effective for both lan-
guages and works well also for Part-of-Speech tag-
ging. As the approach is based on plain surface-
level information (words) and it finds related data in
a completely unsupervised fashion, it can be easily
applied to other tasks or languages for which anno-
tated (or automatically annotated) data is available.
2 Related Work
The work most related to ours is McClosky et al
(2010). They try to find the best combination of
source models to parse data from a new domain,
which is related to Plank and Sima?an (2008). In
the latter, unlabeled data was used to create sev-
eral parsers by weighting trees in the WSJ accord-
ing to their similarity to the subdomain. McClosky
et al (2010) coined the term multiple source domain
adaptation. Inspired by work on parsing accuracy
prediction (Ravi et al, 2008), they train a linear re-
gression model to predict the best (linear interpola-
tion) of source domain models. Similar to us, Mc-
Closky et al (2010) regard a target domain as mix-
ture of source domains, but they focus on phrase-
structure parsing. Furthermore, our approach differs
from theirs in two respects: we do not treat source
corpora as one entity and try to mix models, but
rather consider articles as base units and try to find
subsets of related articles (the most similar articles);
moreover, instead of creating a supervised model (in
their case to predict parsing accuracy), our approach
is ?simplistic?: we apply measures of domain simi-
larity directly (in an unsupervised fashion), without
the necessity to train a supervised model.
Two other related studies are (Lippincott et al,
2010; Van Asch and Daelemans, 2010). Van Asch
and Daelemans (2010) explore a measure of domain
difference (Renyi divergence) between pairs of do-
mains and its correlation to Part-of-Speech tagging
accuracy. Their empirical results show a linear cor-
relation between the measure and the performance
loss. Their goal is different, but related: rather than
finding related data for a new domain, they want to
estimate the loss in accuracy of a PoS tagger when
applied to a new domain. We will briefly discuss
results obtained with the Renyi divergence in Sec-
tion 5.1. Lippincott et al (2010) examine subdomain
variation in biomedicine corpora and propose aware-
ness of NLP tools to such variation. However, they
did not yet evaluate the effect on a practical task,
thus our study is somewhat complementary to theirs.
The issue of data selection has recently been ex-
amined for Language Modeling (Moore and Lewis,
2010). A subset of the available data is automati-
cally selected as training data for a Language Model
based on a scoring mechanism that compares cross-
entropy scores. Their approach considerably outper-
formed random selection and two previous proposed
approaches both based on perplexity scoring.1
3 Measures of Domain Similarity
3.1 Measuring Similarity Automatically
Feature Representations A similarity function
may be defined over any set of events that are con-
1We tested data selection by perplexity scoring, but found
the Language Models too small to be useful in our setting.
1567
sidered to be relevant for the task at hand. For
parsing, these might be words, characters, n-grams
(of words or characters), Part-of-Speech (PoS) tags,
bilexical dependencies, syntactic rules, etc. How-
ever, to obtain more abstract types such as PoS tags
or dependency relations, one would first need to
gather respective labels. The necessary tools for this
are again trained on particular corpora, and will suf-
fer from domain shifts, rendering labels noisy.
Therefore, we want to gauge the effect of the sim-
plest representation possible: plain surface charac-
teristics (unlabeled text). This has the advantage
that we do not need to rely on additional supervised
tools; moreover, it is interesting to know how far we
can get with this level of information only.
We examine the following feature representa-
tions: relative frequencies of words, relative fre-
quencies of character tetragrams, and topic mod-
els. Our motivation was as follows. Relative fre-
quencies of words are a simple and effective rep-
resentation used e.g. in text classification (Manning
and Schu?tze, 1999), while character n-grams have
proven successful in genre classification (Wu et al,
2010). Topic models (Blei et al, 2003; Steyvers
and Griffiths, 2007) can be considered an advanced
model over word distributions: every article is repre-
sented by a topic distribution, which in turn is a dis-
tribution over words. Similarity between documents
can be measured by comparing topic distributions.
Similarity Functions There are many possible
similarity (or distance) functions. They fall broadly
into two categories: probabilistically-motivated and
geometrically-motivated functions. The similarity
functions examined in this study will be described
in the following.
The Kullback-Leibler (KL) divergence D(q||r) is
a classical measure of ?distance?2 between two prob-
ability distributions, and is defined as: D(q||r) =
?
y q(y) log
q(y)
r(y) . It is a non-negative, additive,
asymmetric measure, and 0 iff the two distributions
are identical. However, the KL-divergence is unde-
fined if there exists an event y such that q(y) > 0
but r(y) = 0, which is a property that ?makes it
unsuitable for distributions derived via maximum-
likelihood estimates? (Lee, 2001).
2It is not a proper distance metric since it is asymmetric.
One option to overcome this limitation is to apply
smoothing techniques to gather non-zero estimates
for all y. The alternative, examined in this paper, is
to consider an approximation to the KL divergence,
such as the Jensen-Shannon (JS) divergence (Lin,
1991) and the skew divergence (Lee, 2001).
The Jensen-Shannon divergence, which is sym-
metric, computes the KL-divergence between q, r,
and the average between the two. We use the JS
divergence as defined in Lee (2001): JS(q, r) =
1
2 [D(q||avg(q, r)) + D(r||avg(q, r))]. The asym-
metric skew divergence s?, proposed by Lee (2001),
mixes one distribution with the other by a degree de-
fined by ? ? [0, 1): s?(q, r, ?) = D(q||?r + (1 ?
?)q). As ? approaches 1, the skew divergence ap-
proximates the KL-divergence.
An alternative way to measure similarity is to
consider the distributions as vectors and apply
geometrically-motivated distance functions. This
family of similarity functions includes the cosine
cos(q, r) = q(y) ? r(y)/||q(y)||||r(y)||, euclidean
euc(q, r) =
??
y(q(y)? r(y))
2 and variational
(also known as L1 or Manhattan) distance function,
defined as var(q, r) =
?
y |q(y)? r(y)|.
3.2 Human-annotated data
In contrast to the automatic measures devised in the
previous section, we might have access to human an-
notated data. That is, use label information such as
topic or genre to define the set of similar articles.
Genre For the Penn Treebank (PT) Wall Street
Journal (WSJ) section, more specifically, the subset
available in the Penn Discourse Treebank, there ex-
ists a partition of the data by genre (Webber, 2009).
Every article is assigned one of the following genre
labels: news, letters, highlights, essays, errata, wit
and short verse, quarterly progress reports, notable
and quotable. This classification has been made on
the basis of meta-data (Webber, 2009). It is well-
known that there is no meta-data directly associated
with the individual WSJ files in the Penn Treebank.
However, meta-data can be obtained by looking at
the articles in the ACL/DCI corpus (LDC99T42),
and a mapping file that aligns document numbers of
DCI (DOCNO) to WSJ keys (Webber, 2009). An
example document is given in Figure 1. The meta-
data field HL contains headlines, SO source info, and
1568
the IN field includes topic markers.
<DOC><DOCNO> 891102-0186. </DOCNO>
<WSJKEY> wsj_0008 </WSJKEY>
<AN> 891102-0186. </AN>
<HL> U.S. Savings Bonds Sales
@ Suspended by Debt Limit </HL>
<DD> 11/02/89 </DD>
<SO> WALL STREET JOURNAL (J) </SO>
<IN> FINANCIAL, ACCOUNTING, LEASING (FIN)
BOND MARKET NEWS (BON) </IN>
<GV> TREASURY DEPARTMENT (TRE) </GV>
<DATELINE> WASHINGTON </DATELINE>
<TXT>
<p><s>
The federal government suspended sales of U.S.
savings bonds because Congress hasn?t lifted
the ceiling on government debt.</s></p> [...]
Figure 1: Example of ACL/DCI article. We have aug-
mented it with the WSJ filename (WSJKEY).
Topic On the basis of the same meta-data, we
devised a classification of the Penn Treebank WSJ
by topic. That is, while the genre division has been
mostly made on the basis of headlines, we use the
information of the IN field. Every article is assigned
one, more than one or none of a predefined set of
keywords. While their origin remains unclear,3
these keywords seem to come from a controlled
vocabulary. There are 76 distinct topic markers.
The three most frequent keywords are: TENDER
OFFERS, MERGERS, ACQUISITIONS (TNM),
EARNINGS (ERN), STOCK MARKET, OFFERINGS
(STK). This reflects the fact that a lot of arti-
cles come from the financial domain. But the
corpus also contains articles from more distant do-
mains, like MARKETING, ADVERTISING (MKT),
COMPUTERS AND INFORMATION TECHNOLOGY
(CPR), HEALTH CARE PROVIDERS, MEDICINE,
DENTISTRY (HEA), PETROLEUM (PET).
4 Experimental Setup
4.1 Tools & Evaluation
The parsing system used in this study is the MST
parser (McDonald et al, 2005), a state-of-the-art
data-driven graph-based dependency parser. It is
3It is not known what IN stands for, as also stated in Mark
Liberman?s notes in the readme of the ACL/DCI corpus. How-
ever, a reviewer suggested that IN might stand for ?index terms?
which seems plausible.
a system that can be trained on a variety of lan-
guages given training data in CoNLL format (Buch-
holz and Marsi, 2006). Additionally, the parser im-
plements both projective and non-projective pars-
ing algorithms. The projective algorithm is used for
the experiments on English, while the non-projective
variant is used for Dutch. We train the parser using
default settings. MST takes PoS-tagged data as in-
put; we use gold-standard tags in the experiments.
We estimate topic models using Latent Dirichlet
Allocation (Blei et al, 2003) implemented in the
MALLET4 toolkit. Like Lippincott et al (2010),
we set the number of topics to 100, and otherwise
use standard settings (no further optimization). We
experimented with the removal of stopwords, but
found no deteriorating effect while keeping them.
Thus, all experiments are carried out on data where
stopwords were not removed.
We implemented the similarity measures pre-
sented in Section 3.1. For skew divergence, that re-
quires parameter ?, we set ? = .99 (close to KL
divergence) since that has shown previously to work
best (Lee, 2001). Additionally, we evaluate the ap-
proach on English PoS tagging using two different
taggers: MXPOST, the MaxEnt tagger of Ratna-
parkhi5 and Citar,6 a trigram HMM tagger.
In all experiments, parsing performance is mea-
sured as Labeled Attachment Score (LAS), the per-
centage of tokens with correct dependency edge and
label. To compute LAS, we use the CoNLL 2007
evaluation script7 with punctuation tokens excluded
from scoring (as was the default setting in CoNLL
2006). PoS tagging accuracy is measured as the per-
centage of correctly labeled words out of all words.
Statistical significance is determined by Approxi-
mate Randomization Test (Noreen, 1989; Yeh, 2000)
with 10,000 iterations.
4.2 Data
English - WSJ For English, we use the portion of
the Penn Treebank Wall Street Journal (WSJ) that
has been made available in the CoNLL 2008 shared
4http://mallet.cs.umass.edu/
5ftp://ftp.cis.upenn.edu/pub/adwait/jmx/
6Citar has been implemented by Danie?l de Kok and is avail-
able at: https://github.com/danieldk/citar
7http://nextens.uvt.nl/depparse-wiki/
1569
task. This data has been automatically converted8
into dependency structure, and contains three files:
the training set (sections 02-21), development set
(section 24) and test set (section 23).
Since we use articles as basic units, we actually
split the data to get back original article boundaries.9
This led to a total of 2,034 articles (1 million words).
Further statistics on the datasets are given in Ta-
ble 1. In the first set of experiments on WSJ subdo-
mains, we consider articles from section 23 and 24
that contain at least 50 sentences as test sets (target
domains). This amounted to 22 test articles.
EN: WSJ WSJ+G+B Dutch
articles 2,034 3,776 51,454
sentences 43,117 77,422 1,663,032
words 1,051,997 1,784,543 20,953,850
Table 1: Overview of the datasets for English and Dutch.
To test whether we have a reasonable system,
we performed a sanity check and trained the MST
parser on the training section (02-21). The result
on the standard test set (section 23) is identical to
previously reported results (excluding punctuation
tokens: LAS 87.50, Unlabeled Attachment Score
(UAS) 90.75; with punctuation tokens: LAS 87.07,
UAS 89.95). The latter has been reported in (Sur-
deanu and Manning, 2010).
English - Genia (G) & Brown (B) For the Do-
main Adaptation experiments, we added 1,552 ar-
ticles from the GENIA10 treebank (biomedical ab-
stracts from Medline) and 190 files from the Brown
corpus to the pool of data. We converted the data
to CoNLL format with the LTH converter (Johans-
son and Nugues, 2007). The size of the test files is,
respectively: Genia 1,360 sentences with an aver-
age number of 26.20 words per sentence; the Brown
test set is the same as used in the CoNLL 2008
shared task and contains 426 sentences with a mean
of 16.80 words.
8Using the LTH converter: http://nlp.cs.lth.se/
software/treebank_converter/
9This was a non-trivial task, as we actually noticed that some
sentences have been omitted from the CoNLL 2008 shared task.
10We use the GENIA distribution in Penn Treebank for-
mat available at http://bllip.cs.brown.edu/download/
genia1.0-division-rel1.tar.gz
5 Experiments on English
5.1 Experiments within the WSJ
In the first set of experiments, we focus on the WSJ
and evaluate the similarity functions to gather re-
lated data for a given test article. We have 22 WSJ
articles as test set, sampled from sections 23 and
24. Regarding feature representations, we examined
three possibilities: relative frequencies of words, rel-
ative frequencies of character tetragrams (both un-
smoothed) and document topic distributions.
In the following, we only discuss representations
based on words or topic models as we found charac-
ter tetragrams less stable; they performed sometimes
like their word-based counterparts but other times,
considerably worse.
Results of Similarity Measures Table 2 com-
pares the effect of the different ways to select re-
lated data in comparison to the random baseline for
increasing amounts of training data. The table gives
the average over 22 test articles (rather than show-
ing individual tables for the 22 articles). We select
articles up to various thresholds that specify the to-
tal number of sentences selected in each round (e.g.
0.3k, 1.2k, etc.).11 In more detail, Table 2 shows the
result of applying various similarity functions (intro-
duced in Section 3.1) over the two different feature
representations (w: words; tm: topic model) for in-
creasing amounts of data. We additionally provide
results of using the Renyi divergence.12
Clearly, as more and more data is selected, the
differences become smaller, because we are close
to the data limit. However, for all data points less
than 38k (97%), selection by jensen-shannon, varia-
tional and cosine similarity outperform random data
selection significantly for both types of feature rep-
resentations (words and topic model). For selection
by topic models, this additionally holds for the eu-
clidean measure.
From the various measures we can see that se-
lection by jensen-shannon divergence and varia-
tional distance perform best, followed by cosine
similarity, skew divergence, euclidean and renyi.
11Rather than choosing k articles, as article length may differ.
12The Renyi divergence (Re?nyi, 1961), also used by Van
Asch and Daelemans (2010), is defined as D?(q, r) = 1/(??
1) log(
?
q?r1??).
1570
1% 3% 25% 49% 97%
(0.3k) (1.2k) (9.6k) (19.2k) (38k)
random 70.61 77.21 82.98 84.48 85.51
w-js 74.07? 79.41? 83.98? 84.94? 85.68
w-var 74.07? 79.60? 83.82? 84.94? 85.45
w-skw 74.20? 78.95? 83.68? 84.60 85.55
w-cos 73.77? 79.30? 83.87? 84.96? 85.59
w-euc 73.85? 78.90? 83.52? 84.68 85.57
w-ryi 73.41? 78.31 83.76? 84.46 85.46
tm-js 74.23? 79.49? 84.04? 85.01? 85.45
tm-var 74.29? 79.59? 83.93? 84.94? 85.43
tm-skw 74.13? 79.42? 84.13? 84.82 85.73
tm-cos 74.04? 79.27? 84.14? 84.99? 85.42
tm-euc 74.27? 79.53? 83.93? 85.15? 85.62
tm-ryi 71.26 78.64? 83.79? 84.85 85.58
Table 2: Comparison of similarity measures based
on words (w) and topic model (tm): parsing accu-
racy for increasing amounts of training data as average
over 22 WSJ articles (js=jensen-shannon; cos=cosine;
skw=skew; var=variational; euc=euclidean; ryi=renyi).
Best score (per representation) underlined, best overall
score bold; ? indicates significantly better (p < 0.05)
than random.
Renyi divergence does not perform as well as other
probabilistically-motivated functions. Regarding
feature representations, the representation based on
topic models works slightly better than the respec-
tive word-based measure (cf. Table 2) and often
achieves the overall best score (boldface).
Overall, the differences in accuracy between the
various similarity measures are small; but interest-
ingly, the overlap between them is not that large.
Table 3 and Table 4 show the overlap (in terms of
proportion of identically selected articles) between
pairs of similarity measures. As shown in Table 3,
for all measures there is only a small overlap with
the random baseline (around 10%-14%). Despite
similar performance, topic model selection has inter-
estingly no substantial overlap with any other word-
based similarity measures: their overlap is at most
41.6%. Moreover, Table 4 compares the overlap of
the various similarity functions within a certain fea-
ture representation (here x stands for either topic
model ? left value ? or words ? right value). The
table shows that there is quite some overlap be-
tween jensen-shannon, variational and skew diver-
gence on one side, and cosine and euclidean on
the other side, i.e. between probabilistically- and
geometrically-motivated functions. Variational has
a higher overlap with the probabilistic functions. In-
terestingly, the ?peaks? in Table 4 (underlined, i.e.
the highest pair-wise overlaps) are the same for the
different feature representations.
In the following we analyze selection by topic
model and words, as they are relatively different
from each other, despite similar performance. For
the word-based model, we use jensen-shannon as
similarity function, as it turned out to be the best
measure. For topic model, we use the simpler vari-
ational metric. However, very similar results were
achieved using jensen-shannon. Cosine and eu-
clidean did not perform as well.
ran w-js w-var w-skw w-cos w-euc
ran ? 10.3 10.4 10.0 10.4 10.2
tm-js 12.1 41.6 39.6 36.0 29.3 28.6
tm-var 12.3 40.8 39.3 34.9 29.3 28.5
tm-skw 11.8 40.9 39.7 36.8 30.0 30.1
tm-cos 14.0 31.7 30.7 27.3 24.1 23.2
tm-euc 14.6 27.5 27.2 23.4 22.6 22.1
Table 3: Average overlap (in %) of similarity measure:
random selection (ran) vs. measures based on words (w)
and topic model (tm).
x=tm/w x-js x-var x-skw x-cos x-euc
tm/w-var 76/74 ? 60/63 55/48 49/47
tm/w-skw 69/72 60/63 ? 48/41 42/42
tm/w-cos 57/42 55/48 48/41 ? 62/71
tm/w-euc 47/41 49/47 42/42 62/71 ?
Table 4: Average overlap (in %) for different feature
representations x as tm/w, where tm=topic model and
w=words. Highest pair-wise overlap is underlined.
Automatic Measure vs. Human labels The next
question is how these automatic measures compare
to human-annotated data. We compare word-based
and topic model selection (by using jensen-shannon
and variational, respectively) to selection based on
human-given labels: genre and topic. For genre, we
randomly select larger amounts of training data for
a given test article from the same genre. For topic,
the approach is similar, but as an article might have
1571
several topic markers (keywords in the IN field), we
rank articles by proportion of overlapping keywords.
l
l
l
l
l
l
0 5000 10000 15000 20000
76
78
80
82
84
86
Average
number of sentences
Accu
racy
l randomwords?jstopic model?vargenretopic (IN fields)
Figure 2: Comparison of automatic measures (words us-
ing jensen-shannon and topic model using variational)
with human-annotated labels (genre/topic). Automatic
measures outperform human labels (p < 0.05).
Figure 2 shows that human-labels do actually not
perform better than the automatic measures. Both
are close to random selection. Moreover, the line
of selection by topic marker (IN fields) stops early
? we believe the reason for this is that the IN fields
are too fine-grained, which limits the number of ar-
ticles that are considered relevant for a given test
article. However, manually aggregating articles on
similar topics did not improve topic-based selection
either. We conclude that the automatic selection
techniques perform significantly better than human-
annotated data, at least within the WSJ domain con-
sidered here.
5.2 Domain Adaptation Results
Until now, we compared similarity measures by re-
stricting ourselves to articles from the WSJ. In this
section, we extend the experiments to the domain
adaptation scenario. We augment the pool of WSJ
articles with articles coming from two other corpora:
Genia and Brown. We want to gauge the effective-
ness of the domain similarity measures in the multi-
domain setting, where articles are selected from the
pool of data without knowing their identity (which
corpus the articles came from).
The test sets are the standard evaluation sets from
the three corpora: the standard WSJ (section 23)
and Brown test set from CoNLL 2008 (they contain
2,399 and 426 sentences, respectively) and the Ge-
nia test set (1,370 sentences). As a reference, we
give results of models trained on the respective cor-
pora (per-corpus models; i.e. if we consider corpora
boundaries and train a model on the respective do-
main ? this model is ?supervised? in the sense that it
knows from which corpus the test article came from)
as well as a baseline model trained on all data, i.e.
the union of all three corpora (wsj+genia+brown),
which is a standard baseline in domain adapta-
tion (Daume? III, 2007; McClosky et al, 2010).
WSJ Brown Genia
(38k) (28k) (19k)
random 86.58 73.81 83.77
per-corpus 87.50 81.55 86.63
union 87.05 79.12 81.57
topic model (var) 87.11? 81.76? 86.77?
words (js) 86.30 81.47? 86.44?
Table 5: Domain Adaptation Results on English (signifi-
cantly better: ? than random; ? than random and union).
The learning curves are shown in Figure 3, the
scores for a specific amount of data are given in
Table 5. The performance of the reference mod-
els (per-corpus and union in Table 5) are indicated
in Figure 3 with horizontal lines: the dashed line
represents the per-corpus performance (?supervised?
model); the solid line shows the performance of the
union baseline trained on all available data (77k sen-
tences). For the former, the vertical dashed lines in-
dicate the amount of data the model was trained on
(e.g. 23k sentences for Brown).
Simply taking all available data has a deteriorat-
ing effect: on all three test sets, the performance of
the union model is below the presumably best per-
formance of a model trained on the respective corpus
(per-corpus model).
The empirical results show that automatic data se-
lection by topic model outperforms random selec-
tion on all three test sets and the union baseline in
two out of three cases. More specifically, selection
by topic model outperforms random selection sig-
nificantly on all three test sets and all points in the
graph (p < 0.001). Selection by the word-based
measure (words-js) achieves a significant improve-
1572
ll
l
l
l l
0 10000 20000 30000 400008
0
82
84
86
88
wsj23all
number of sentences
Accu
racy
l
l
l
l
l l
l
0 10000 20000 30000 40000
70
75
80
brown
number of sentences
Accu
racy
l
l
l
l
l l
l
0 10000 20000 30000 40000
76
78
80
82
84
86
88
genia
number of sentences
Accu
racy
l randomwords?jstopic model?varper?corpus modelunion (wsj+genia+brown)
Figure 3: Domain Adaptation Results for English Parsing with Increasing Amounts of Training Data. The vertical line
represents the amount of data the per-corpus model is trained on.
ment over the random baseline on two out of the
three test sets ? it falls below the random baseline on
the WSJ test set. Thus, selection by topic model per-
forms best ? it achieves better performance than the
union baseline with comparatively little data (Genia:
4k; Brown: 19k ? in comparison: union has 77k).
Moreover, it comes very close to the supervised per-
corpus model performance13 with a similar amount
of data (cf. vertical dashed line). This is a very good
result, given that the technique disregards the origin
of the articles and just uses plain words as informa-
tion. It automatically finds data that is beneficial for
an unknown target domain.
So far we examined domain similarity measures
for parsing, and concluded that selection by topic
model performs best, closely followed by word-
based selection using the jensen-shannon diver-
gence. The question that remains is whether the
measure is more widely applicable: How does it per-
form on another language and task?
PoS tagging We perform similar Domain Adap-
tation experiments on WSJ, Genia and Brown for
PoS tagging. We use two taggers (HMM and Max-
Ent) and the same three test articles as before. The
results are shown in Figure 4 (it depicts the aver-
age over the three test sets, WSJ, Genia, Brown, for
space reasons). The left figure shows the perfor-
mance of the HMM tagger; on the right is the Max-
Ent tagger. The graphs show that automatic train-
ing data selection outperforms random data selec-
13On Genia and Brown (cf. Table 5) there is no significant
difference between topic model and per-corpus model.
tion, and again topic model selection performs best,
closely followed by words-js. This confirms previ-
ous findings and shows that the domain similarity
measures are effective also for this task.
l
l
l
l l
l l l
0 10000 20000 30000 400000.9
00.
920
.94
0.96
0.98
Average HMM tagger
number of sentences
Accura
cy
l randomwords?jstopic model?var
l
l
l
l l
l l
0 10000 20000 30000 400000.9
00.
920
.94
0.96
0.98
Average MXPOST tagger
number of sentences
Accura
cy
l randomwords?jstopic model?var
Figure 4: PoS tagging results, average over 3 test sets.
6 Experiments on Dutch
For Dutch, we evaluate the approach on a bigger and
more varied dataset. It contains in total over 50k ar-
ticles and 20 million words (cf. Table 1). In con-
trast to the English data, only a small portion of the
dataset is manually annotated: 281 articles.14
Since we want to evaluate the performance of
different similarity measures, we want to keep the
influence of noise as low as possible. Therefore,
we annotated the remaining articles with a parsing
system that is more accurate (Plank and van No-
ord, 2010), the Alpino parser (van Noord, 2006).
Note that using a more accurate parsing system to
train another parser has recently also been proposed
by Petrov et al (2010) as uptraining. Alpino is a
14http://www.let.rug.nl/vannoord/Lassy/
1573
parser tailored to Dutch, that has been developed
over the last ten years, and reaches an accuracy level
of 90% on general newspaper text. It uses a condi-
tional MaxEnt model as parse selection component.
Details of the parser are given in (van Noord, 2006).
l
l
l l
l
l
l
0 5000 10000 15000 20000 25000 30000
74
76
78
80
82
84
86
Average
number of sentences
Accu
racy
l randomtopic model?varwords?js
Figure 5: Result on Dutch; average over 30 articles.
Data and Results The Dutch dataset contains
articles from a variety of sources: Wikipedia15,
EMEA16 (documents from the European Medicines
Agency) and the Dutch parallel corpus17 (DPC), that
covers a variety of subdomains. The Dutch arti-
cles were parsed with Alpino and automatically con-
verted to CoNLL format with the treebank conver-
sion software from CoNLL 2006, where PoS tags
have been replaced with more fine-grained Alpino
tags as that had a positive effect on MST. The 281
annotated articles come from all three sources. As
with English, we consider as test set articles with
at least 50 sentences, from which 30 are randomly
sampled.
The results on Dutch are shown in Figure 5. Do-
main similarity measures clearly outperform random
data selection also in this setting with another lan-
guage and a considerably larger pool of data (20 mil-
lion words; 51k articles).
7 Discussion
In this paper we have shown the effectiveness of a
simple technique that considers only plain words as
domain selection measure for two tasks, dependency
15http://ilps.science.uva.nl/WikiXML/
16http://urd.let.rug.nl/tiedeman/OPUS/EMEA.php
17http://www.kuleuven-kortrijk.be/DPC
parsing and PoS tagging. Interestingly, human-
annotated labels did not perform better than the au-
tomatic measures. The best technique is based on
topic models, and compares document topic distri-
butions estimated by LDA (Blei et al, 2003) using
the variational metric (very similar results were ob-
tained using jensen-shannon). Topic model selec-
tion significantly outperforms random data selection
on both examined languages, English and Dutch,
and has a positive effect on PoS tagging. More-
over, it outperformed a standard Domain Adapta-
tion baseline (union) on two out of three test sets.
Topic model is closely followed by the word-based
measure using jensen-shannon divergence. By ex-
amining the overlap between word-based and topic
model-based techniques, we found that despite sim-
ilar performance their overlap is rather small. Given
these results and the fact that no optimization has
been done for the topic model itself, results are en-
couraging: there might be an even better measure
that exploits the information from both techniques.
So far, we tested a simple combination of the two by
selecting half of the articles by a measure based on
words and the other half by a measure based on topic
models (by testing different metrics). However, this
simple combination technique did not improve re-
sults yet ? topic model alone still performed best.
Overall, plain surface characteristics seem to
carry important information of what kind of data is
relevant for a given domain. Undoubtedly, parsing
accuracy will be influenced by more factors than lex-
ical information. Nevertheless, as we have seen, lex-
ical differences constitute an important factor.
Applying divergence measures over syntactic pat-
terns, adding additional articles to the pool of
data (by uptraining (Petrov et al, 2010), selftrain-
ing (McClosky et al, 2006) or active learning (Hwa,
2004)), gauging the effect of weighting instances
according to their similarity to the test data (Jiang
and Zhai, 2007; Plank and Sima?an, 2008), as well
as analyzing differences between gathered data are
venues for further research.
Acknowledgments
The authors would like to thank Bonnie Webber and
the three anonymous reviewers for their valuable
comments on earlier drafts of this paper.
1574
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain Adaptation with Structural Correspon-
dence Learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, Sydney, Australia.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing. In
Proceedings of the 10th Conference on Computational
Natural Language Learning (CoNLL-X), pages 149?
164, New York City.
Hal Daume? III. 2007. Frustratingly Easy Domain Adap-
tation. In Proceedings of the 45th Meeting of the Asso-
ciation for Computational Linguistics, Prague, Czech
Republic.
Daniel Gildea. 2001. Corpus Variation and Parser Per-
formance. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Processing,
Pittsburgh, PA.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Adapting a Probabilistic Disambiguation Model
of an HPSG Parser to a New Domain. In Robert Dale,
Kam-Fai Wong, Jian Su, and Oi Yee Kwong, editors,
Natural Language Processing IJCNLP 2005, volume
3651 of Lecture Notes in Computer Science, pages
199?210. Springer Berlin / Heidelberg.
Rebecca Hwa. 2004. Sample Selection for Statistical
Parsing. Compututational Linguistics, 30:253?276,
September.
Jing Jiang and ChengXiang Zhai. 2007. Instance
Weighting for Domain Adaptation in NLP. In Pro-
ceedings of the 45th Meeting of the Association for
Computational Linguistics, pages 264?271, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Richard Johansson and Pierre Nugues. 2007. Extended
Constituent-to-dependency Conversion for English. In
Proceedings of NODALIDA, Tartu, Estonia.
Lillian Lee. 2001. On the Effectiveness of the Skew Di-
vergence for Statistical Language Analysis. In In Ar-
tificial Intelligence and Statistics 2001, pages 65?72,
Key West, Florida.
J. Lin. 1991. Divergence measures based on the Shannon
entropy. Information Theory, IEEE Transactions on,
37(1):145 ?151, January.
Tom Lippincott, Diarmuid O? Se?aghdha, Lin Sun, and
Anna Korhonen. 2010. Exploring variation across
biomedical subdomains. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 689?697, Beijing, China, August.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press, Cambridge Mass.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective Self-Training for Parsing. In Pro-
ceedings of Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 152?159, Brooklyn,
New York. Association for Computational Linguistics.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic Domain Adaptation for Parsing. In
Proceedings of Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 28?36, Los An-
geles, California, June. Association for Computational
Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective Dependency Parsing
using Spanning Tree Algorithms. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 523?530, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
Robert C. Moore and William Lewis. 2010. Intelligent
Selection of Language Model Training Data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July. Association
for Computational Linguistics.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for Accurate Deter-
ministic Question Parsing. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 705?713, Cambridge, MA,
October. Association for Computational Linguistics.
Barbara Plank and Khalil Sima?an. 2008. Subdomain
Sensitive Statistical Parsing using Raw Corpora. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation, Marrakech, Mo-
rocco, May.
Barbara Plank and Gertjan van Noord. 2010. Grammar-
Driven versus Data-Driven: Which Parsing System Is
More Affected by Domain Shifts? In Proceedings of
the 2010 Workshop on NLP and Linguistics: Finding
the Common Ground, pages 25?33, Uppsala, Sweden,
July. Association for Computational Linguistics.
Sujith Ravi, Kevin Knight, and Radu Soricut. 2008. Au-
tomatic Prediction of Parser Accuracy. In EMNLP
?08: Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 887?
1575
896, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
A. Re?nyi. 1961. On measures of information and en-
tropy. In Proceedings of the 4th Berkeley Sympo-
sium on Mathematics, Statistics and Probability, pages
547?561, Berkeley.
Satoshi Sekine. 1997. The Domain Dependence of Pars-
ing. In In Proceedings of the Fifth Conference on
Applied Natural Language Processing, pages 96?102,
Washington D.C.
Mark Steyvers and Tom Griffiths, 2007. Probabilistic
Topic Models. Lawrence Erlbaum Associates.
Mihai Surdeanu and Christopher D. Manning. 2010. En-
semble Models for Dependency Parsing: Cheap and
Good? In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
649?652, Los Angeles, California, June. Association
for Computational Linguistics.
Vincent Van Asch and Walter Daelemans. 2010. Us-
ing Domain Similarity for Performance Estimation. In
Proceedings of the 2010 Workshop on Domain Adap-
tation for Natural Language Processing, pages 31?36,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Gertjan van Noord. 2006. At Last Parsing Is Now Oper-
ational. In TALN 2006 Verbum Ex Machina, Actes De
La 13e Conference sur Le Traitement Automatique des
Langues naturelles, pages 20?42, Leuven.
Bonnie Webber. 2009. Genre distinctions for Discourse
in the Penn TreeBank. In Proceedings of the 47th
Meeting of the Association for Computational Linguis-
tics, pages 674?682, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
Zhili Wu, Katja Markert, and Serge Sharoff. 2010. Fine-
Grained Genre Classification Using Structural Learn-
ing Algorithms. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 749?759, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th conference on Computational linguistics,
pages 947?953, Morristown, NJ, USA. Association for
Computational Linguistics.
1576
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 194?199,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Reversible Stochastic Attribute-Value Grammars
Danie?l de Kok
University of Groningen
d.j.a.de.kok@rug.nl
Barbara Plank
University of Groningen
b.plank@rug.nl
Gertjan van Noord
University of Groningen
g.j.m.van.noord@rug.nl
Abstract
An attractive property of attribute-value gram-
mars is their reversibility. Attribute-value
grammars are usually coupled with sepa-
rate statistical components for parse selection
and fluency ranking. We propose reversible
stochastic attribute-value grammars, in which
a single statistical model is employed both for
parse selection and fluency ranking.
1 Introduction
Reversible grammars were introduced as early as
1975 by Martin Kay (1975). In the eighties, the
popularity of attribute-value grammars (AVG) was
in part motivated by their inherent reversible na-
ture. Later, AVG were enriched with a statistical
component (Abney, 1997): stochastic AVG (SAVG).
Training a SAVG is feasible if a stochastic model
is assumed which is conditioned on the input sen-
tences (Johnson et al, 1999). Various parsers based
on this approach now exist for various languages
(Toutanova et al, 2002; Riezler et al, 2002; van
Noord and Malouf, 2005; Miyao and Tsujii, 2005;
Clark and Curran, 2004; Forst, 2007). SAVG can be
applied for generation to select the most fluent real-
ization from the set of possible realizations (Velldal
et al, 2004). In this case, the stochastic model is
conditioned on the input logical forms. Such gener-
ators exist for various languages as well (Velldal and
Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et
al., 2007; de Kok and van Noord, 2010).
If an AVG is applied both to parsing and gen-
eration, two distinct stochastic components are re-
quired, one for parsing, and one for generation. To
some extent this is reasonable, because some fea-
tures are only relevant in a certain direction. For
instance, features that represent aspects of the sur-
face word order are important for generation, but ir-
relevant for parsing. Similarly, features which de-
scribe aspects of the logical form are important for
parsing, but irrelevant for generation. Yet, there are
also many features that are relevant in both direc-
tions. For instance, for Dutch, a very effective fea-
ture signals a direct object NP in fronted position in
main clauses. If a main clause is parsed which starts
with a NP, the disambiguation component will fa-
vor a subject reading of that NP. In generation, the
fluency component will favor subject fronting over
object fronting. Clearly, such shared preferences are
not accidental.
In this paper we propose reversible SAVG in
which a single stochastic component is applied both
in parsing and generation. We provide experimen-
tal evidence that such reversible SAVG achieve sim-
ilar performance as their directional counterparts.
A single, reversible model is to be preferred over
two distinct models because it explains why pref-
erences in a disambiguation component and a flu-
ency component, such as the preference for subject
fronting over object fronting, are shared. A single,
reversible model is furthermore of practical inter-
est for its simplicity, compactness, and maintainabil-
ity. As an important additional advantage, reversible
models are applicable for tasks which combine as-
pects of parsing and generation, such as word-graph
parsing and paraphrasing. In situations where only a
small amount of training data is available for parsing
or generation, cross-pollination improves the perfor-
194
mance of a model. If preferences are shared between
parsing and generation, it follows that a generator
could benefit from parsing data and vice versa. We
present experimental results indicating that in such a
bootstrap scenario a reversible model achieves better
performance.
2 Reversible SAVG
As Abney (1997) shows, we cannot use relatively
simple techniques such as relative frequencies to
obtain a model for estimating derivation probabili-
ties in attribute-value grammars. As an alternative,
he proposes a maximum entropy model, where the
probability of a derivation d is defined as:
p(d) =
1
Z
exp
?
i
?ifi(d) (1)
fi(d) is the frequency of feature fi in derivation
d. A weight ?i is associated with each feature fi.
In (1), Z is a normalizer which is defined as fol-
lows, where ? is the set of derivations defined by
the grammar:
Z =
?
d???
exp
?
i
?ifi(d
?) (2)
Training this model requires access to all derivations
? allowed by the grammar, which makes it hard to
implement the model in practice.
Johnson et al (1999) alleviate this problem by
proposing a model which conditions on the input
sentence s: p(d|s). Since the number of derivations
for a given sentence s is usually finite, the calcula-
tion of the normalizer is much more practical. Con-
versely, in generation the model is conditioned on
the input logical form l, p(d|l) (Velldal et al, 2004).
In such directional stochastic attribute-value gram-
mars, the probability of a derivation d given an input
x (a sentence or a logical form) is defined as:
p(d|x) =
1
Z(x)
exp
?
i
?ifi(x, d) (3)
with Z(x) as (?(x) are all derivations for input x):
Z(x) =
?
d???(x)
exp
?
i
?ifi(x, d
?) (4)
Consequently, the constraint put on feature values
during training only refers to derivations with the
same input. If X is the set of inputs (for parsing,
all sentences in the treebank; for generation, all log-
ical forms), then we have:
Ep(fi)? Ep?(fi) = 0 ? (5)
?
x?X
?
d??(x)
p?(x)p(d|x)fi(x, d)? p?(x, d)fi(x, d) = 0
Here we assume a uniform distribution for p?(x).
Let j(d) be a function which returns 0 if the deriva-
tion d is inconsistent with the treebank, and 1 in case
the derivation is correct. p?(x, d) is now defined in
such a way that it is 0 for incorrect derivations, and
uniform for correct derivations for a given input:
p?(x, d) = p?(x)
j(d)
?d???(x)j(d?)
(6)
Directional SAVG make parsing and generation
practically feasible, but require separate models for
parse disambiguation and fluency ranking.
Since parsing and generation both create deriva-
tions that are in agreement with the constraints im-
plied by the input, a single model can accompany
the attribute-value grammar. Such a model estimates
the probability of a derivation d given a set of con-
straints c, p(d|c). We use conditional maximum en-
tropy models to estimate p(d|c):
p(d|c) =
1
Z(c)
exp
?
i
?ifi(c, d) (7)
Z(c) =
?
d???(c)
exp
?
i
?ifi(c, d
?) (8)
We derive a reversible model by training on data
for parse disambiguation and fluency ranking simul-
taneously. In contrast to directional models, we im-
pose the two constraints per feature given in figure 1:
one on the feature value with respect to the sentences
S in the parse disambiguation treebank and the other
on the feature value with respect to logical forms L
in the fluency ranking treebank. As a result of the
constraints on training defined in figure 1, the fea-
ture weights in the reversible model distinguish, at
the same time, good parses from bad parses as well
as good realizations from bad realizations.
3 Experimental setup and evaluation
To evaluate reversible SAVG, we conduct experi-
ments in the context of the Alpino system for Dutch.
195
?s?S
?
d??(s)
p?(s)p(d|c = s)fi(s, d)? p?(c = s, d)fi(s, d) = 0
?
l?L
?
d??(l)
p?(l)p(d|c = l)fi(l, d)? p?(c = l, d)fi(l, d) = 0
Figure 1: Constraints imposed on feature values for training reversible models p(d|c).
Alpino provides a wide-coverage grammar, lexicon
and parser (van Noord, 2006). Recently, a sentence
realizer has been added that uses the same grammar
and lexicon (de Kok and van Noord, 2010).
In the experiments, the cdbl part of the Alpino
Treebank (van der Beek et al, 2002) is used as train-
ing data (7,154 sentences). The WR-P-P-H part
(2,267 sentences) of the LASSY corpus (van Noord
et al, 2010), which consists of text from the Trouw
2001 newspaper, is used for testing.
3.1 Features
The features that we use in the experiment are the
same features which are available in the Alpino
parser and generator. In the following section, these
features are described in some detail.
Word adjacency. Two word adjacency features
are used as auxiliary distributions (Johnson and Rie-
zler, 2000). The first feature is the probability of the
sentence according to a word trigram model. The
second feature is the probability of the sentence ac-
cording to a tag trigram model that uses the part-
of-speech tags assigned by the Alpino system. In
both models, linear interpolation smoothing for un-
known trigrams, and Laplacian smoothing for un-
known words and tags is applied. The trigram mod-
els have been trained on the Twente Nieuws Corpus
corpus (approximately 110 million words), exclud-
ing the Trouw 2001 corpus. In conventional pars-
ing tasks, the value of the word trigram model is the
same for all derivations of a given input sentence.
Lexical frames. Lexical analysis is applied dur-
ing parsing to find all possible subcategorization
frames for the tokens in the input sentence. Since
some frames occur more frequently in good parses
than others, we use feature templates that record the
frames that were used in a parse. An example of
such a feature is: ??to play? serves as an intransi-
tive verb?. We also use an auxiliary distribution of
word and frame combinations that was trained on
a large corpus of automatically annotated sentences
(436 million words). The values of lexical frame
features are constant for all derivations in sentence
realization, unless the frame is not specified in the
logical form.
Dependency relations. There are also feature
templates which describe aspects of the dependency
structure. For each dependency, three types of de-
pendency features are extracted. Examples of such
features are ?a pronoun is used as the subject of
a verb?, ?the pronoun ?she? is used as the sub-
ject of a verb?, ?the noun ?beer? is used as the
object of the verb ?drink??. In addition, features
are used which implement auxiliary distributions
for selectional preferences, as described in Van No-
ord (2007). In conventional realization tasks, the
values of these features are constant for all deriva-
tions for a given input representation.
Syntactic features. Syntactic features include fea-
tures which record the application of each grammar
rule, as well as features which record the application
of a rule in the context of another rule. An exam-
ple of the latter is ?rule 167 is used to construct the
second daughter of a derivation constructed by rule
233?. In addition, there are features describing more
complex syntactic patterns such as: fronting of sub-
jects and other noun phrases, orderings in the middle
field, long-distance dependencies, and parallelism of
conjuncts in coordination.
3.2 Parse disambiguation
Earlier we assumed that a treebank is a set of cor-
rect derivations. In practice, however, a treebank
only contains an abstraction of such derivations (in
196
our case sentences with corresponding dependency
structures), thus abstracting away from syntactic de-
tails needed in a parse disambiguation model. As in
Osborne (2000), the derivations for the parse disam-
biguation model are created by parsing the training
corpus. In the current setting, up to at most 3000
derivations are created for every sentence. These
derivations are then compared to the gold standard
dependency structure to judge the quality of the
parses. For a given sentence, the parses with the
highest concept accuracy (van Noord, 2006) are con-
sidered correct, the rest is treated as incorrect.
3.3 Fluency ranking
For fluency ranking we also need access to full
derivations. To ensure that the system is able to
generate from the dependency structures in the tree-
bank, we parse the corresponding sentence, and se-
lect the parse with the dependency structure that
corresponds most closely to the dependency struc-
ture in the treebank. The resulting dependency
structures are fed into the Alpino chart generator
to construct derivations for each dependency struc-
ture. The derivations for which the corresponding
sentences are closest to the original sentence in the
treebank are marked correct. Due to a limit on gen-
eration time, some longer sentences and correspond-
ing dependency structures were excluded from the
data. As a result, the average sentence length was
15.7 tokens, with a maximum of 26 tokens. To com-
pare a realization to the correct sentence, we use the
General Text Matcher (GTM) method (Melamed et
al., 2003; Cahill, 2009).
3.4 Training the models
Models are trained by taking an informative sam-
ple of ?(c) for each c in the training data (Osborne,
2000). This sample consists of at most 100 ran-
domly selected derivations. Frequency-based fea-
ture selection is applied (Ratnaparkhi, 1999). A fea-
ture f partitions ?(c), if there are derivations d and
d? in ?(c) such that f(c, d) 6= f(c, d?). A feature is
used if it partitions the informative sample of ?(c)
for at least two c. Table 1 lists the resulting charac-
teristics of the training data for each model.
We estimate the parameters of the conditional
Features Inputs Derivations
Generation 1727 3688 141808
Parse 25299 7133 376420
Reversible 25578 10811 518228
Table 1: Size of the training data for each model
maximum entropy models using TinyEst,1 with a
Gaussian (`2) prior distribution (? = 0, ?2 = 1000)
to reduce overfitting (Chen and Rosenfeld, 1999).
4 Results
4.1 Parse disambiguation
Table 2 shows the results for parse disambiguation.
The table also provides lower and upper bounds: the
baseline model selects an arbitrary parse per sen-
tence; the oracle chooses the best available parse.
Figure 2 shows the learning curves for the direc-
tional parsing model and the reversible model.
Model CA (%) f-score (%)
Baseline 75.88 76.28
Oracle 94.86 95.09
Parse model 90.93 91.28
Reversible 90.87 91.21
Table 2: Concept Accuracy scores and f-scores in terms
of named dependency relations for the parsing-specific
model versus the reversible model.
The results show that the general, reversible,
model comes very close to the accuracy obtained
by the dedicated, parsing specific, model. Indeed,
the tiny difference is not statistically significant. We
compute statistical significance using the Approxi-
mate Randomization Test (Noreen, 1989).
4.2 Fluency ranking
Table 3 compares the reversible model with a di-
rectional fluency ranking model. Figure 3 shows
the learning curves for the directional generation
model and the reversible model. The reversible
model achieves similar performance as the direc-
tional model (the difference is not significant).
To show that a reversible model can actually profit
from mutually shared features, we report on an ex-
periment where only a small amount of generation
1http://github.com/danieldk/tinyest
197
0.0 0.1 0.2 0.3 0.4 0.57
67
88
08
28
48
68
89
0
Proportion parse training data
CA (%
)
parse modelreversible model
Figure 2: Learning curve for directional and reversible
models for parsing. The reversible model uses all training
data for generation.
Model GTM
Random 55.72
Oracle 86.63
Fluency 71.82
Reversible 71.69
Table 3: General Text Matcher scores for fluency ranking
using various models.
training data is available. In this experiment, we
manually annotated 234 dependency structures from
the cdbl part of the Alpino Treebank, by adding cor-
rect realizations. In many instances, there is more
than one fluent realization. We then used this data to
train a directional fluency ranking model and a re-
versible model. The results for this experiment are
shown in Table 4. Since the reversible model outper-
forms the directional model we conclude that indeed
fluency ranking benefits from parse disambiguation
data.
Model GTM
Fluency 70.54
Reversible 71.20
Table 4: Fluency ranking using a small amount of anno-
tated fluency ranking training data (difference is signifi-
cant at p < 0.05).
0.0 0.1 0.2 0.3 0.4 0.5
60
65
70
Proportion generation training data
GTM
 scor
e
generation modelreversible model
Figure 3: Learning curves for directional and reversible
models for generation. The reversible models uses all
training data for parsing.
5 Conclusion
We proposed reversible SAVG as an alternative to
directional SAVG, based on the observation that
syntactic preferences are shared between parse dis-
ambiguation and fluency ranking. This framework
is not purely of theoretical interest, since the exper-
iments show that reversible models achieve accura-
cies that are similar to those of directional models.
Moreover, we showed that a fluency ranking model
trained on a small data set can be improved by com-
plementing it with parse disambiguation data.
The integration of knowledge from parse disam-
biguation and fluency ranking could be beneficial for
tasks which combine aspects of parsing and genera-
tion, such as word-graph parsing or paraphrasing.
198
References
Steven Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4):597?618.
Aoife Cahill, Martin Forst, and Christian Rohrer. 2007.
Stochastic realisation ranking for a free word order
language. In ENLG ?07: Proceedings of the Eleventh
European Workshop on Natural Language Genera-
tion, pages 17?24, Morristown, NJ, USA.
Aoife Cahill. 2009. Correlating human and automatic
evaluation of a german surface realiser. In Proceed-
ings of the ACL-IJCNLP 2009 Conference - Short Pa-
pers, pages 97?100.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical report, Carnegie Mellon University, Pittsburg.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proceed-
ings of the 42nd Annual Meeting of the ACL, pages
103?110, Morristown, NJ, USA.
Danie?l de Kok and Gertjan van Noord. 2010. A sentence
generator for Dutch. In Proceedings of the 20th Com-
putational Linguistics in the Netherlands conference
(CLIN).
Martin Forst. 2007. Filling statistics with linguistics:
property design for the disambiguation of german lfg
parses. In DeepLP ?07: Proceedings of the Workshop
on Deep Linguistic Processing, pages 17?24, Morris-
town, NJ, USA.
Mark Johnson and Stefan Riezler. 2000. Exploiting
auxiliary distributions in stochastic unification-based
grammars. In Proceedings of the 1st Meeting of the
NAACL, pages 154?161, Seattle, Washington.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceedings of the
37th Annual Meeting of the ACL.
Martin Kay. 1975. Syntactic processing and functional
sentence perspective. In TINLAP ?75: Proceedings of
the 1975 workshop on Theoretical issues in natural
language processing, pages 12?15, Morristown, NJ,
USA.
I. Dan Melamed, Ryan Green, and Joseph Turian. 2003.
Precision and recall of machine translation. In HLT-
NAACL.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage hpsg pars-
ing. In Proceedings of the 43rd Annual Meeting of the
ACL, pages 83?90, Morristown, NJ, USA.
Hiroko Nakanishi and Yusuke Miyao. 2005. Probabilis-
tic models for disambiguation of an hpsg-based chart
generator. In Proceedings of the 9th International
Workshop on Parsing Technologies (IWPT), pages 93?
102.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience.
Miles Osborne. 2000. Estimation of stochastic attribute-
value grammars using an informative sample. In Pro-
ceedings of the 18th conference on Computational lin-
guistics (COLING), pages 586?592.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1):151?175.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell III, and Mark John-
son. 2002. Parsing the wall street journal using a
lexical-functional grammar and discriminative estima-
tion techniques. In Proceedings of the 40th Annual
Meeting of the ACL, pages 271?278, Morristown, NJ,
USA.
Kristina Toutanova, Christopher D. Manning, Stuart M.
Shieber, Dan Flickinger, and Stephan Oepen. 2002.
Parse disambiguation for a rich hpsg grammar. In
First Workshop on Treebanks and Linguistic Theories
(TLT), pages 253?263, Sozopol.
Leonoor van der Beek, Gosse Bouma, Robert Malouf,
and Gertjan van Noord. 2002. The Alpino depen-
dency treebank. In Computational Linguistics in the
Netherlands (CLIN).
Gertjan van Noord and Robert Malouf. 2005. Wide
coverage parsing with stochastic attribute value gram-
mars. Draft available from the authors. A preliminary
version of this paper was published in the Proceedings
of the IJCNLP workshop Beyond Shallow Analyses,
Hainan China, 2004.
Gertjan van Noord, Ineke Schuurman, and Gosse Bouma.
2010. Lassy syntactische annotatie, revision 19053.
Gertjan van Noord. 2006. At Last Parsing Is Now
Operational. In TALN 2006 Verbum Ex Machina,
Actes De La 13e Conference sur Le Traitement Au-
tomatique des Langues naturelles, pages 20?42, Leu-
ven.
Gertjan van Noord. 2007. Using self-trained bilexical
preferences to improve disambiguation accuracy. In
Proceedings of the International Workshop on Parsing
Technology (IWPT), ACL 2007 Workshop, pages 1?
10, Prague.
Erik Velldal and Stephan Oepen. 2006. Statistical rank-
ing in tactical generation. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 517?525, Sydney,
Australia, July. ACL.
Erik Velldal, Stephan Oepen, and Dan Flickinger. 2004.
Paraphrasing treebanks for stochastic realization rank-
ing. In Proceedings of the 3rd Workshop on Treebanks
and Linguistic Theories (TLT), pages 149?160.
199
Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 9?16
Manchester, August 2008
Exploring an Auxiliary Distribution based approach to
Domain Adaptation of a Syntactic Disambiguation Model
Barbara Plank
University of Groningen
The Netherlands
B.Plank@rug.nl
Gertjan van Noord
University of Groningen
The Netherlands
G.J.M.van.Noord@rug.nl
Abstract
We investigate auxiliary distribu-
tions (Johnson and Riezler, 2000) for
domain adaptation of a supervised parsing
system of Dutch. To overcome the limited
target domain training data, we exploit an
original and larger out-of-domain model
as auxiliary distribution. However, our
empirical results exhibit that the auxiliary
distribution does not help: even when very
little target training data is available the
incorporation of the out-of-domain model
does not contribute to parsing accuracy on
the target domain; instead, better results
are achieved either without adaptation or
by simple model combination.
1 Introduction
Modern statistical parsers are trained on large an-
notated corpora (treebanks) and their parameters
are estimated to reflect properties of the training
data. Therefore, a disambiguation component will
be successful as long as the treebank it was trained
on is representative for the input the model gets.
However, as soon as the model is applied to an-
other domain, or text genre (Lease et al, 2006),
accuracy degrades considerably. For example, the
performance of a parser trained on the Wall Street
Journal (newspaper text) significantly drops when
evaluated on the more varied Brown (fiction/non-
fiction) corpus (Gildea, 2001).
A simple solution to improve performance on
a new domain is to construct a parser specifically
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
for that domain. However, this amounts to hand-
labeling a considerable amount of training data
which is clearly very expensive and leads to an un-
satisfactory solution. In alternative, techniques for
domain adaptation, also known as parser adap-
tation (McClosky et al, 2006) or genre porta-
bility (Lease et al, 2006), try to leverage ei-
ther a small amount of already existing annotated
data (Hara et al, 2005) or unlabeled data (Mc-
Closky et al, 2006) of one domain to parse data
from a different domain. In this study we examine
an approach that assumes a limited amount of al-
ready annotated in-domain data.
We explore auxiliary distributions (Johnson and
Riezler, 2000) for domain adaptation, originally
suggested for the incorporation of lexical selec-
tional preferences into a parsing system. We gauge
the effect of exploiting a more general, out-of-
domain model for parser adaptation to overcome
the limited amount of in-domain training data. The
approach is examined on two application domains,
question answering and spoken data.
For the empirical trials, we use Alpino (van No-
ord and Malouf, 2005; van Noord, 2006), a ro-
bust computational analyzer for Dutch. Alpino
employs a discriminative approach to parse selec-
tion that bases its decision on a Maximum Entropy
(MaxEnt) model. Section 2 introduces the MaxEnt
framework. Section 3 describes our approach of
exploring auxiliary distributions for domain adap-
tation. In section 4 the experimental design and
empirical results are presented and discussed.
2 Background: MaxEnt Models
Maximum Entropy (MaxEnt) models are widely
used in Natural Language Processing (Berger et
al., 1996; Ratnaparkhi, 1997; Abney, 1997). In
this framework, a disambiguation model is speci-
9
fied by a set of feature functions describing prop-
erties of the data, together with their associated
weights. The weights are learned during the train-
ing procedure so that their estimated value deter-
mines the contribution of each feature. In the task
of parsing, features appearing in correct parses are
given increasing weight, while features in incorrect
parses are given decreasing weight. Once a model
is trained, it can be applied to parse selection that
chooses the parse with the highest sum of feature
weights.
During the training procedure, the weights vec-
tor is estimated to best fit the training data. In
more detail, given m features with their corre-
sponding empirical expectation E
p?
[f
j
] and a de-
fault model q
0
, we seek a model p that has mini-
mum Kullback-Leibler (KL) divergence from the
default model q
0
, subject to the expected-value
constraints: E
p
[f
j
] = E
p?
[f
j
], where j ? 1, ...,m.
In MaxEnt estimation, the default model q
0
is
often only implicit (Velldal and Oepen, 2005) and
not stated in the model equation, since the model
is assumed to be uniform (e.g. the constant func-
tion 1
?(s)
for sentence s, where ?(s) is the set of
parse trees associated with s). Thus, we seek the
model with minimum KL divergence from the uni-
form distribution, which means we search model
p with maximum entropy (uncertainty) subject to
given constraints (Abney, 1997).
In alternative, if q
0
is not uniform then p is
called a minimum divergence model (according
to (Berger and Printz, 1998)). In the statistical
parsing literature, the default model q
0
that can
be used to incorporate prior knowledge is also re-
ferred to as base model (Berger and Printz, 1998),
default or reference distribution (Hara et al, 2005;
Johnson et al, 1999; Velldal and Oepen, 2005).
The solution to the estimation problem of find-
ing distribution p, that satisfies the expected-
value constraints and minimally diverges from
q
0
, has been shown to take a specific parametric
form (Berger and Printz, 1998):
p
?
(?, s) =
1
Z
?
q
0
exp
P
m
j=1
?
j
f
j
(?) (1)
with m feature functions, s being the input sen-
tence, ? a corresponding parse tree, and Z
?
the
normalization equation:
Z
?
=
?
?
?
??
q
0
exp
P
m
j=1
?
j
f
j
(?
?
) (2)
Since the sum in equation 2 ranges over all pos-
sible parse trees ?? ? ? admitted by the gram-
mar, calculating the normalization constant ren-
ders the estimation process expensive or even in-
tractable (Johnson et al, 1999). To tackle this
problem, Johnson et al (1999) redefine the esti-
mation procedure by considering the conditional
rather than the joint probability.
P
?
(?|s) =
1
Z
?
q
0
exp
P
m
j=1
?
j
f
j
(?) (3)
with Z
?
as in equation 2, but instead, summing
over ?? ? ?(s), where ?(s) is the set of parse
trees associated with sentence s. Thus, the proba-
bility of a parse tree is estimated by summing only
over the possible parses of a specific sentence.
Still, calculating ?(s) is computationally very
expensive (Osborne, 2000), because the number of
parses is in the worst case exponential with respect
to sentence length. Therefore, Osborne (2000) pro-
poses a solution based on informative samples. He
shows that is suffices to train on an informative
subset of available training data to accurately es-
timate the model parameters. Alpino implements
the Osborne-style approach to Maximum Entropy
parsing. The standard version of the Alpino parser
is trained on the Alpino newspaper Treebank (van
Noord, 2006).
3 Exploring auxiliary distributions for
domain adaptation
3.1 Auxiliary distributions
Auxiliary distributions (Johnson and Riezler,
2000) offer the possibility to incorporate informa-
tion from additional sources into a MaxEnt Model.
In more detail, auxiliary distributions are inte-
grated by considering the logarithm of the proba-
bility given by an auxiliary distribution as an addi-
tional, real-valued feature. More formally, given k
auxiliary distributions Q
i
(?), then k new auxiliary
features f
m+1
, ..., f
m+k
are added such that
f
m+i
(?) = logQ
i
(?) (4)
where Q
i
(?) do not need to be proper probability
distributions, however they must strictly be posi-
tive ?? ? ? (Johnson and Riezler, 2000).
The auxiliary distributions resemble a reference
distribution, but instead of considering a single
reference distribution they have the advantage
that several auxiliary distributions can be inte-
grated and weighted against each other. John-
10
son establishes the following equivalence between
the two (Johnson and Riezler, 2000; Velldal and
Oepen, 2005):
Q(?) =
k
?
i=1
Q
i
(?)
?
m+i (5)
where Q(?) is the reference distribution and
Q
i
(?) is an auxiliary distribution. Hence, the con-
tribution of each auxiliary distribution is regulated
through the estimated feature weight. In general,
a model that includes k auxiliary features as given
in equation (4) takes the following form (Johnson
and Riezler, 2000):
P
?
(?|s) =
?
k
i=1
Q
i
(?)
?
m+i
Z
?
exp
P
m
j=1
?
j
f
j
(?) (6)
Due to the equivalence relation in equation (5)
we can restate the equation to explicitly show that
auxiliary distributions are additional features1.
P
?
(?|s)
=
Q
k
i=1
[exp
f
m+i(?)
]
?
m+i
Z
?
exp
P
m
j=1
?
j
f
j
(?)
(7)
=
1
Z
?
k
Y
i=1
exp
f
m+i(?)
??
m+i
exp
P
m
j=1
?
j
f
j
(?) (8)
=
1
Z
?
exp
P
k
i=1
f
m+i(?)
??
m+i
exp
P
m
j=1
?
j
f
j
(?)
(9)
=
1
Z
?
exp
P
m+k
j=1
?
j
f
j
(?)
with f
j
(?) = logQ(?) for m < j ? (m + k)
(10)
3.2 Auxiliary distributions for adaptation
While (Johnson and Riezler, 2000; van Noord,
2007) focus on incorporating several auxiliary dis-
tributions for lexical selectional preferences, in
this study we explore auxiliary distributions for do-
main adaptation.
We exploit the information of the more gen-
eral model, estimated from a larger, out-of-domain
treebank, for parsing data from a particular tar-
get domain, where only a small amount of train-
ing data is available. A related study is Hara
et al (2005). While they also assume a limited
amount of in-domain training data, their approach
1Note that the step from equation (6) to (7) holds by re-
stating equation (4) as Q
i
(?) = exp
f
m+i
(?)
differs from ours in that they incorporate an origi-
nal model as a reference distribution, and their es-
timation procedure is based on parse forests (Hara
et al, 2005; van Noord, 2006), rather than infor-
mative samples. In this study, we want to gauge
the effect of auxiliary distributions, which have the
advantage that the contribution of the additional
source is regulated.
More specifically, we extend the target model
to include (besides the original integer-valued fea-
tures) one additional real-valued feature (k=1)2.
Its value is defined to be the negative logarithm
of the conditional probability given by OUT , the
original, out-of-domain, Alpino model. Hence, the
general model is ?merged? into a single auxiliary
feature:
f
m+1
= ?logP
OUT
(?|s) (11)
The parameter of the new feature is estimated us-
ing the same estimation procedure as for the re-
maining model parameters. Intuitively, our auxil-
iary feature models dispreferences of the general
model for certain parse trees. When the Alpino
model assigns a high probability to a parse candi-
date, the auxiliary feature value will be small, close
to zero. In contrast, a low probability parse tree in
the general model gets a higher feature value. To-
gether with the estimated feature weight expected
to be negative, this has the effect that a low prob-
ability parse in the Alpino model will reduce the
probability of a parse in the target domain.
3.3 Model combination
In this section we sketch an alternative approach
where we keep only two features under the Max-
Ent framework: one is the log probability assigned
by the out-domain model, the other the log proba-
bility assigned by the in-domain model:
f
1
= ?logP
OUT
(?|s), f
2
= ?logP
IN
(?|s)
The contribution of each feature is again scaled
through the estimated feature weights ?
1
, ?
2
.
We can see this as a simple instantiation of model
combination. In alternative, data combination is
a domain adaptation method where IN and OUT-
domain data is simply concatenated and a new
model trained on the union of data. A potential and
well known disadvantage of data combination is
that the usually larger amount of out-domain data
2Or alternatively, k ? 1 (see section 4.3.1).
11
?overwhelms? the small amount of in-domain data.
Instead, Model combination interpolates the two
models in a linear fashion by scaling their contri-
bution. Note that if we skip the parameter esti-
mation step and simply assign the two parameters
equal values (equal weights), the method reduces
to P
OUT
(?|s) ? P
IN
(?|s), i.e. just multiplying
the respective model probabilities.
4 Experiments and Results
4.1 Experimental design
The general model is trained on the Alpino Tree-
bank (van Noord, 2006) (newspaper text; approx-
imately 7,000 sentences). For the domain-specific
corpora, in the first set of experiments (section 4.3)
we consider the Alpino CLEF Treebank (ques-
tions; approximately 1,800 sentences). In the sec-
ond part (section 4.4) we evaluate the approach
on the Spoken Dutch corpus (Oostdijk, 2000)
(CGN, ?Corpus Gesproken Nederlands?; spoken
data; size varies, ranging from 17 to 1,193 sen-
tences). The CGN corpus contains a variety of
components/subdomains to account for the various
dimensions of language use (Oostdijk, 2000).
4.2 Evaluation metric
The output of the parser is evaluated by comparing
the generated dependency structure for a corpus
sentence to the gold standard dependency structure
in a treebank. For this comparison, we represent
the dependency structure (a directed acyclic graph)
as a set of named dependency relations. To com-
pare such sets of dependency relations, we count
the number of dependencies that are identical in
the generated parse and the stored structure, which
is expressed traditionally using precision, recall
and f-score (Briscoe et al, 2002).
Let Di
p
be the number of dependencies produced
by the parser for sentence i, Di
g
is the number of
dependencies in the treebank parse, and Di
o
is the
number of correct dependencies produced by the
parser. If no superscript is used, we aggregate over
all sentences of the test set, i.e.,:
D
p
=
?
i
D
i
p
D
o
=
?
i
D
i
o
D
g
=
?
i
D
i
g
Precision is the total number of correct dependen-
cies returned by the parser, divided by the over-
all number of dependencies returned by the parser
(precision = D
o
/D
p
); recall is the number of
correct system dependencies divided by the total
number of dependencies in the treebank (recall =
D
o
/D
g
). As usual, precision and recall can be
combined in a single f-score metric.
An alternative similarity score for dependency
structures is based on the observation that for a
given sentence of n words, a parser would be ex-
pected to return n dependencies. In such cases,
we can simply use the percentage of correct de-
pendencies as a measure of accuracy. Such a la-
beled dependency accuracy is used, for instance,
in the CoNLL shared task on dependency parsing
(?labeled attachment score?).
Our evaluation metric is a variant of labeled
dependency accuracy, in which we do allow for
some discrepancy between the number of returned
dependencies. Such a discrepancy can occur,
for instance, because in the syntactic annotations
of Alpino (inherited from the CGN) words can
sometimes be dependent on more than a single
head (called ?secondary edges? in CGN). A fur-
ther cause is parsing failure, in which case a parser
might not produce any dependencies. We argue
elsewhere (van Noord, In preparation) that a metric
based on f-score can be misleading in such cases.
The resulting metric is called concept accuracy, in,
for instance, Boros et al (1996).3
CA = Do?
i
max(D
i
g
,D
i
p
)
The concept accuracy metric can be characterized
as the mean of a per-sentence minimum of recall
and precision. The resulting CA score therefore
is typically slightly lower than the corresponding
f-score, and, for the purposes of this paper, equiv-
alent to labeled dependency accuracy.
4.3 Experiments with the QA data
In the first set of experiments we focus on the
Question Answering (QA) domain (CLEF corpus).
Besides evaluating our auxiliary based approach
(section 3), we conduct separate baseline experi-
ments:
? In-domain (CLEF): train on CLEF (baseline)
? Out-domain (Alpino): train on Alpino
? Data Combination (CLEF+Alpino): train a model on
the combination of data, CLEF ? Alpino
3In previous publications and implementations defini-
tions were sometimes used that are equivalent to: CA =
D
o
max(D
g
,D
p
)
which is slightly different; in practice the dif-
ferences can be ignored.
12
Dataset In-dom. Out-dom. Data Combination Aux.distribution Model Combination
size (#sents) CLEF Alpino CLEF+Alpino CLEF+Alpino aux CLEF aux+Alpino aux equal weights
CLEF 2003 (446) 97.01 94.02 97.21 97.01 97.14 97.46
CLEF 2004 (700) 96.60 89.88 95.14 96.60 97.12 97.23
CLEF 2005 (200) 97.65 87.98 93.62 97.72 97.99 98.19
CLEF 2006 (200) 97.06 88.92 95.16 97.06 97.00 96.45
CLEF 2007 (200) 96.20 92.48 97.30 96.33 96.33 96.46
Table 1: Results on the CLEF test data; underlined scores indicate results > in-domain baseline (CLEF)
? Auxiliary distribution (CLEF+Alpino aux): adding
the original Alpino model as auxiliary feature to CLEF
? Model Combination: keep only two features
P
OUT
(?|s) and P
IN
(?|s). Two variants: i) estimate
the parameters ?
1
, ?
2
(CLEF aux+Alpino aux); ii)
give them equal values, i.e. ?
1
=?
2
=?1 (equal weights)
We assess the performance of all of these mod-
els on the CLEF data by using 5-fold cross-
validation. The results are given in table 1.
The CLEF model performs significantly better
than the out-of-domain (Alpino) model, despite of
the smaller size of the in-domain training data.
In contrast, the simple data combination results
in a model (CLEF+Alpino) whose performance is
somewhere in between. It is able to contribute in
some cases to disambiguate questions, while lead-
ing to wrong decisions in other cases.
However, for our auxiliary based approach
(CLEF+Alpino aux) with its regulated contribu-
tion of the general model, the results show that
adding the feature does not help. On most datasets
the same performance was achieved as by the in-
domain model, while on only two datasets (CLEF
2005, 2007) the use of the auxiliary feature results
in an insignificant improvement.
In contrast, simple model combination works
surprisingly well. On two datasets (CLEF 2004
and 2005) this simple technique reaches a sub-
stantial improvement over all other models. On
only one dataset (CLEF 2006) it falls slightly off
the in-domain baseline, but still considerably out-
performs data combination. This is true for both
model combination methods, with estimated and
equal weights. In general, the results show that
model combination usually outperforms data com-
bination (with the exception of one dataset, CLEF
2007), where, interestingly, the simplest model
combination (equal weights) often performs best.
Contrary to expectations, the auxiliary based ap-
proach performs poorly and could often not even
come close to the results obtained by simple model
combination. In the following we will explore pos-
sible reasons for this result.
Examining possible causes One possible point
of failure could be that the auxiliary feature was
simply ignored. If the estimated weight would be
close to zero the feature would indeed not con-
tribute to the disambiguation task. Therefore, we
examined the estimated weights for that feature.
From that analysis we saw that, compared to the
other features, the auxiliary feature got a weight
relatively far from zero. It got on average a weight
of ?0.0905 in our datasets and as such is among
the most influential weights, suggesting it to be im-
portant for disambiguation.
Another question that needs to be asked, how-
ever, is whether the feature is modeling properly
the original Alpino model. For this sanity check,
we create a model that contains only the single
auxiliary feature and no other features. The fea-
ture?s weight is set to a constant negative value4.
The resulting model?s performance is assessed on
the complete CLEF data. The results (0% column
in table 3) show that the auxiliary feature is indeed
properly modeling the general Alpino model, as
the two result in identical performance.
4.3.1 Feature template class models
In the experiments so far the general model was
?packed? into a single feature value. To check
whether the feature alone is too weak, we exam-
ine the inclusion of several auxiliary distributions
(k > 1). Each auxiliary feature we add represents
a ?submodel? corresponding to an actual feature
template class used in the original model. The fea-
ture?s value is the negative log-probability as de-
fined in equation 11, where OUT corresponds to
the respective Alpino submodel.
The current Disambiguation Model of Alpino
uses the 21 feature templates (van Noord and Mal-
ouf, 2005). Out of this given feature templates,
we create two models that vary in the number of
classes used. In the first model (?5 class?), we cre-
ate five (k = 5) auxiliary distributions correspond-
ing to five clusters of feature templates. They are
4Alternatively, we may estimate its weight, but as it does
not have competing features we are safe to assume it constant.
13
defined manually and correspond to submodels for
Part-of-Speech, dependencies, grammar rule ap-
plications, bilexical preferences and the remaining
Alpino features. In the second model (?21 class?),
we simply take every single feature template as its
own cluster (k = 21).
We test the two models and compare them to
our baseline. The results of this experiment are
given in table 2. We see that both the 5 class and
the 21 class model do not achieve any considerable
improvement over the baseline (CLEF), nor over
the single auxiliary model (CLEF+Alpino aux).
Dataset (#sents) 5class 21class CLEF+Alpino aux CLEF
CLEF2003 (446) 97.01 97.04 97.01 97.01
CLEF2004 (700) 96.57 96.60 96.60 96.60
CLEF2005 (200) 97.72 97.72 97.72 97.65
CLEF2006 (200) 97.06 97.06 97.06 97.06
CLEF2007 (200) 96.20 96.27 96.33 96.20
Table 2: Results on CLEF including several auxil-
iary features corresponding to Alpino submodels
4.3.2 Varying amount of training data
Our expectation is that the auxiliary feature is at
least helpful in the case very little in-domain train-
ing data is available. Therefore, we evaluate the
approach with smaller amounts of training data.
We sample (without replacement) a specific
amount of training instances from the original QA
data files and train models on the reduced train-
ing data. The resulting models are tested with and
without the additional feature as well as model
combination on the complete data set by using
cross validation. Table 3 reports the results of these
experiments for models trained on a proportion of
up to 10% CLEF data. Figure 1 illustrates the over-
all change in performance.
Obviously, an increasing amount of in-domain
training data improves the accuracy of the models.
However, for our auxiliary feature, the results in
table 3 show that the models with and without the
auxiliary feature result in an overall almost iden-
tical performance (thus in figure 1 we depict only
one of the lines). Hence, the inclusion of the aux-
iliary feature does not help in this case either. The
models achieve similar performance even indepen-
dently of the available amount of in-domain train-
ing data.
Thus, even on models trained on very little in-
domain training data (e.g. 1% CLEF training data)
the auxiliary based approach does not work. It
even hurts performance, i.e. depending on the spe-
cific dataset, the inclusion of the auxiliary feature
 86
 88
 90
 92
 94
 96
 98
 0  10  20  30  40  50  60
CA
% training data
Varying amount of training data (CLEF 2004)
Aux.distr. (CLEF+Alp_aux)
Out-dom (Alpino)
Mod.Comb. (CLEF_aux+Alpino_aux)
Figure 1: Amount of in-domain training data ver-
sus concept accuracy (Similar figures result from
the other CLEF datasets) - note that we depict only
aux.distr. as its performance is nearly indistin-
guishable from the in-domain (CLEF) baseline
results in a model whose performance lies even be-
low the original Alpino model accuracy, for up to a
certain percentage of training data (varying on the
dataset from 1% up to 10%).
In contrast, simple model combination is much
more beneficial. It is able to outperform almost
constantly the in-domain baseline (CLEF) and
our auxiliary based approach (CLEF+Alpino aux).
Furthermore, in contrast to the auxiliary based ap-
proach, model combination never falls below the
out-of-domain (Alpino) baseline, not even in the
case a tiny amount of training data is available.
This is true for both model combinations (esti-
mated versus equal weights).
We would have expected the auxiliary feature to
be useful at least when very little in-domain train-
ing data is available. However, the empirical re-
sults reveal the contrary5. We believe the reason
for this drop in performance is the amount of avail-
able in-domain training data and the corresponding
scaling of the auxiliary feature?s weight. When
little training data is available, the weight cannot
be estimated reliably and hence is not contributing
enough compared to the other features (exempli-
fied in the drop of performance from 0% to 1%
5As suspected by a reviewer, the (non-auxiliary) features
may overwhelm the single auxiliary feature, such that possi-
ble improvements by increasing the feature space on such a
small scale might be invisible. We believe this is not the case.
Other studies have shown that including just a few features
might indeed help (Johnson and Riezler, 2000; van Noord,
2007). (e.g., the former just added 3 features).
14
0% 1% 5% 10%
Dataset no aux = Alp. no aux +aux m.c. eq.w. no aux +aux m.c. eq.w. no aux +aux m.c. eq.w.
CLEF2003 94.02 94.02 91.93 91.93 95.59 93.65 93.83 93.83 95.74 95.17 94.80 94.77 95.72 95.72
CLEF2004 89.88 89.88 86.59 86.59 90.97 91.06 93.62 93.62 93.42 92.95 94.79 94.82 96.26 95.85
CLEF2005 87.98 87.98 87.34 87.41 91.35 89.15 95.90 95.90 97.92 97.52 96.31 96.37 98.19 97.25
CLEF2006 88.92 88.92 89.64 89.64 92.16 91.17 92.77 92.77 94.98 94.55 95.04 95.04 95.04 95.47
CLEF2007 92.48 92.48 91.07 91.13 95.44 93.32 94.60 94.60 95.63 95.69 94.21 94.21 95.95 95.43
Table 3: Results on the CLEF data with varying amount of training data
training data in table 3). In such cases it is more
beneficial to just apply the original Alpino model
or the simple model combination technique.
4.4 Experiments with CGN
One might argue that the question domain is
rather ?easy?, given the already high baseline per-
formance and the fact that few hand-annotated
questions are enough to obtain a reasonable
model. Therefore, we examine our approach on
CGN (Oostdijk, 2000).
The empirical results of testing using cross-
validation within a subset of CGN subdomains
are given in table 4. The baseline accuracies
are much lower on this more heterogeneous, spo-
ken, data, leaving more room for potential im-
provements over the in-domain model. How-
ever, the results show that the auxiliary based ap-
proach does not work on the CGN subdomains ei-
ther. The approach is not able to improve even on
datasets where very little training data is available
(e.g. comp-l), thus confirming our previous find-
ing. Moreover, in some cases the auxiliary fea-
ture rather, although only slightly, degrades perfor-
mance (indicated in italic in table 4) and performs
worse than the counterpart model without the ad-
ditional feature.
Depending on the different characteristics of
data/domain and its size, the best model adapta-
tion method varies on CGN. On some subdomains
simple model combination performs best, while on
others it is more beneficial to just apply the origi-
nal, out-of-domain Alpino model.
To conclude, model combination achieves in most
cases a modest improvement, while we have
shown empirically that our domain adaptation
method based on auxiliary distributions performs
just similar to a model trained on in-domain data.
5 Conclusions
We examined auxiliary distributions (Johnson and
Riezler, 2000) for domain adaptation. While
the auxiliary approach has been successfully ap-
plied to lexical selectional preferences (Johnson
and Riezler, 2000; van Noord, 2007), our empir-
ical results show that integrating a more general
into a domain-specific model through the auxil-
iary feature approach does not help. The auxil-
iary approach needs training data to estimate the
weight(s) of the auxiliary feature(s). When little
training data is available, the weight cannot be es-
timated appropriately and hence is not contributing
enough compared to the other features. This re-
sult was confirmed on both examined domains. We
conclude that the auxiliary feature approach is not
appropriate for integrating information of a more
general model to leverage limited in-domain data.
Better results were achieved either without adapta-
tion or by simple model combination.
Future work will consist in investigating other pos-
sibilities for parser adaptation, especially semi-
supervised domain adaptation, where no labeled
in-domain data is available.
References
Abney, Steven P. 1997. Stochastic attribute-value grammars.
Computational Linguistics, 23:597?618.
Berger, A. and H. Printz. 1998. A comparison of criteria
for maximum entropy / minimum divergence feature selec-
tion. In In Proceedings of the 3nd Conference on Empir-
ical Methods in Natural Language Processing (EMNLP),
pages 97?106, Granada, Spain.
Berger, Adam, Stephen Della Pietra, and Vincent Della Pietra.
1996. A maximum entropy approach to natural language
processing. Computational Linguistics, 22(1):39?72.
Boros, M., W. Eckert, F. Gallwitz, G. Go?rz, G. Hanrieder, and
H. Niemann. 1996. Towards understanding spontaneous
speech: Word accuracy vs. concept accuracy. In Pro-
ceedings of the Fourth International Conference on Spoken
Language Processing (ICSLP 96), Philadelphia.
Briscoe, Ted, John Carroll, Jonathan Graham, and Ann
Copestake. 2002. Relational evaluation schemes. In Pro-
ceedings of the Beyond PARSEVAL Workshop at the 3rd In-
ternational Conference on Language Resources and Eval-
uation, pages 4?8, Las Palmas, Gran Canaria.
Gildea, Daniel. 2001. Corpus variation and parser perfor-
mance. In Proceedings of the 2001 Conference on Empir-
ical Methods in Natural Language Processing (EMNLP).
Hara, Tadayoshi, Miyao Yusuke, and Jun?ichi Tsujii. 2005.
Adapting a probabilistic disambiguation model of an hpsg
15
comp-a (1,193) - Spontaneous conversations (?face-to-face?) comp-b (525) - Interviews with teachers of Dutch
DataSet no aux + aux Alpino Mod.Comb. Mod.Comb. Dataset no aux + aux Alpino Mod.Comb. Mod.Comb
eq.weights eq.weights
fn000250 63.20 63.28 62.90 63.91 63.99 fn000081 66.20 66.39 66.45 67.26 66.85
fn000252 64.74 64.74 64.06 64.87 64.96 fn000089 62.41 62.41 63.88 64.35 64.01
fn000254 66.03 66.00 65.78 66.39 66.44 fn000086 62.60 62.76 63.17 63.59 63.77
comp-l (116) - Commentaries/columns/reviews (broadcast) comp-m (267) - Ceremonious speeches/sermons
DataSet no aux + aux Alpino Mod.Comb. Model.Comb. Dataset no aux + aux Alpino Mod.Comb. Mod.Comb
eq.weights eq.weights
fn000002 67.63 67.63 77.30 76.96 72.40 fn000271 59.25 59.25 63.78 64.94 61.76
fn000017 64.51 64.33 66.42 66.30 65.74 fn000298 70.33 70.19 74.55 74.83 72.70
fn000021 61.54 61.54 64.30 64.10 63.24 fn000781 72.26 72.37 73.55 73.55 73.04
Table 4: Excerpt of results on various CGN subdomains (# of sentences in parenthesis).
parser to a new domain. In Proceedings of the Interna-
tional Joint Conference on Natural Language Processing.
Johnson, Mark and Stefan Riezler. 2000. Exploiting auxiliary
distributions in stochastic unification-based grammars. In
Proceedings of the first conference on North American
chapter of the Association for Computational Linguistics,
pages 154?161, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Johnson, Mark, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceedings of the 37th
Annual Meeting of the ACL.
Lease, Matthew, Eugene Charniak, Mark Johnson, and David
McClosky. 2006. A look at parsing and its applications.
In Proceedings of the Twenty-First National Conference on
Artificial Intelligence (AAAI-06), Boston, Massachusetts,
16?20 July.
McClosky, David, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference of
the NAACL, Main Conference, pages 152?159, New York
City, USA, June. Association for Computational Linguis-
tics.
Oostdijk, Nelleke. 2000. The Spoken Dutch Corpus:
Overview and first evaluation. In Proceedings of Sec-
ond International Conference on Language Resources and
Evaluation (LREC), pages 887?894.
Osborne, Miles. 2000. Estimation of stochastic attribute-
value grammars using an informative sample. In Proceed-
ings of the Eighteenth International Conference on Com-
putational Linguistics (COLING 2000).
Ratnaparkhi, A. 1997. A simple introduction to maximum
entropy models for natural language processing. Technical
report, Institute for Research in Cognitive Science, Univer-
sity of Pennsylvania.
van Noord, Gertjan and Robert Malouf. 2005. Wide coverage
parsing with stochastic attribute value grammars. Draft
available from http://www.let.rug.nl/?vannoord. A prelim-
inary version of this paper was published in the Proceed-
ings of the IJCNLP workshop Beyond Shallow Analyses,
Hainan China, 2004.
van Noord, Gertjan. 2006. At Last Parsing Is Now
Operational. In TALN 2006 Verbum Ex Machina, Actes
De La 13e Conference sur Le Traitement Automatique des
Langues naturelles, pages 20?42, Leuven.
van Noord, Gertjan. 2007. Using self-trained bilexical
preferences to improve disambiguation accuracy. In Pro-
ceedings of the Tenth International Conference on Parsing
Technologies. IWPT 2007, Prague., pages 1?10, Prague.
van Noord, Gertjan. In preparation. Learning efficient pars-
ing.
Velldal, E. and S. Oepen. 2005. Maximum entropy mod-
els for realization ranking. In Proceedings of MT-Summit,
Phuket, Thailand.
16
Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 25?33,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Grammar-driven versus Data-driven: Which Parsing System is More
Affected by Domain Shifts?
Barbara Plank
University of Groningen
The Netherlands
b.plank@rug.nl
Gertjan van Noord
University of Groningen
The Netherlands
G.J.M.van.Noord@rug.nl
Abstract
In the past decade several parsing systems
for natural language have emerged, which
use different methods and formalisms. For
instance, systems that employ a hand-
crafted grammar and a statistical disam-
biguation component versus purely sta-
tistical data-driven systems. What they
have in common is the lack of portabil-
ity to new domains: their performance
might decrease substantially as the dis-
tance between test and training domain in-
creases. Yet, to which degree do they suf-
fer from this problem, i.e. which kind of
parsing system is more affected by domain
shifts? Intuitively, grammar-driven sys-
tems should be less affected by domain
changes. To investigate this hypothesis,
an empirical investigation on Dutch is car-
ried out. The performance variation of
a grammar-driven versus two data-driven
systems across domains is evaluated, and a
simple measure to quantify domain sensi-
tivity proposed. This will give an estimate
of which parsing system is more affected
by domain shifts, and thus more in need
for adaptation techniques.
1 Introduction
Most modern Natural Language Processing (NLP)
systems are subject to the wellknown problem of
lack of portability to new domains: there is a sub-
stantial drop in their performance when the sys-
tem gets input from another text domain (Gildea,
2001). This is the problem of domain adapta-
tion. Although the problem exists ever since the
emergence of supervised Machine Learning, it has
started to get attention only in recent years.
Studies on supervised domain adaptation
(where there are limited amounts of annotated
resources in the new domain) have shown that
straightforward baselines (e.g. models based on
source only, target only, or the union of the data)
achieve a relatively high performance level and are
?surprisingly difficult to beat? (Daume? III, 2007).
In contrast, semi-supervised adaptation (i.e. no
annotated resources in the new domain) is a much
more realistic situation but is clearly also consid-
erably more difficult. Current studies on semi-
supervised approaches show very mixed results.
Dredze et al (2007) report on ?frustrating? re-
sults on the CoNLL 2007 semi-supervised adap-
tation task for dependency parsing, i.e. ?no team
was able to improve target domain performance
substantially over a state-of-the-art baseline?. On
the other hand, there have been positive results as
well. For instance, McClosky et al (2006) im-
proved a statistical parser by self-training. Struc-
tural Correspondence Learning (Blitzer et al,
2006) was effective for PoS tagging and Sentiment
Analysis (Blitzer et al, 2006; Blitzer et al, 2007),
while only modest gains were obtained for struc-
tured output tasks like parsing.
For parsing, most previous work on do-
main adaptation has focused on data-driven sys-
tems (Gildea, 2001; McClosky et al, 2006;
Dredze et al, 2007), i.e. systems employing (con-
stituent or dependency based) treebank gram-
mars. Only few studies examined the adaptation of
grammar-based systems (Hara et al, 2005; Plank
and van Noord, 2008), i.e. systems employing
a hand-crafted grammar with a statistical disam-
biguation component. This may be motivated by
the fact that potential gains for this task are inher-
ently bound by the grammar. Yet, domain adap-
tation poses a challenge for both kinds of pars-
ing systems. But to what extent do these differ-
ent kinds of systems suffer from the problem? We
test the hypothesis that grammar-driven systems
are less affected by domain changes. We empir-
ically investigate this in a case-study on Dutch.
25
2 Related work
Most previous work has focused on a single pars-
ing system in isolation (Gildea, 2001; Hara et
al., 2005; McClosky et al, 2006). However,
there is an observable trend towards combining
different parsing systems to exploit complemen-
tary strengths. For instance, Nivre and McDon-
ald (2008) combine two data-driven systems to im-
prove dependency accuracy. Similarly, two studies
successfully combined grammar-based and data-
driven systems: Sagae et al (2007) incorporate
data-driven dependencies as soft-constraint in a
HPSG-based system for parsing the Wallstreet
Journal. In the same spirit (but the other di-
rection), Zhang and Wang (2009) use a deep-
grammar based backbone to improve data-driven
parsing accuracy. They incorporate features from
the grammar-based backbone into the data-driven
system to achieve better generalization across do-
mains. This is the work most closest to ours.
However, which kind of system (hand-crafted
versus purely statistical) is more affected by the
domain, and thus more sensitive to domain shifts?
To the best of our knowledge, no study has yet ad-
dressed this issue. We thus assess the performance
variation of three dependency parsing systems for
Dutch across domains, and propose a simple mea-
sure to quantify domain sensitivity.
3 Parsing Systems
The parsing systems used in this study are: a
grammar-based system for Dutch (Alpino) and
two data-driven systems (MST and Malt), all de-
scribed next.
(1) Alpino is a parser for Dutch which has
been developed over the last ten years, on the ba-
sis of a domain-specific HPSG-grammar that was
used in the OVIS spoken dialogue system. The
OVIS parser was shown to out-perform a statisti-
cal (DOP) parser, in a contrastive formal evalua-
tion (van Zanten et al, 1999). In the ten years af-
ter this evaluation, the system has developed into a
generic parser for Dutch. Alpino consists of more
than 800 grammar rules in the tradition of HPSG,
and a large hand-crafted lexicon. It produces de-
pendency structures as ouput, where more than a
single head per token is allowed. For words that
are not in the lexicon, the system applies a large
variety of unknown word heuristics (van Noord,
2006), which deal with number-like expressions,
compounds, proper names, etc. Coverage of the
grammar and lexicon has been extended over the
years by paying careful attention to the results of
parsing large corpora, by means of error mining
techniques (van Noord, 2004; de Kok et al, 2009).
Lexical ambiguity is reduced by means of a
POS-tagger, described in (Prins and van No-
ord, 2003). This POS-tagger is trained on large
amounts of parser output, and removes unlikely
lexical categories. Some amount of lexical am-
biguity remains. A left-corner parser constructs
a parse-forest for an input sentence. Based on
large amounts of parsed data, the parser considers
only promising parse step sequences, by filtering
out sequences of parse steps which were not pre-
viously used to construct a best parse for a given
sentence. The parse step filter improves efficiency
considerably (van Noord, 2009).
A best-first beam-search algorithm retrieves the
best parse(s) from that forest by consulting a Max-
imum Entropy disambiguation component. Fea-
tures for the disambiguation component include
non-local features. For instance, there are features
that can be used to learn a preference for local ex-
traction over long-distance extraction, and a pref-
erence for subject fronting rather than direct ob-
ject fronting, and a preference for certain types of
orderings in the ?mittelfeld? of a Dutch sentence.
The various features that we use for disambigua-
tion, as well as the best-first algorithm is described
in (van Noord, 2006). The model now also con-
tains features which implement selection restric-
tions, trained on the basis of large parsed corpora
(van Noord, 2007). The maximum entropy dis-
ambiguation component is trained on the Alpino
treebank, described below.
To illustrate the role of the disambiguation com-
ponent, we provide some results for the first 536
sentences of one of the folds of the training data
(of course, the model used in this experiment is
trained on the remaining folds of training data).
In this setup, the POS-tagger and parse step filter
already filter out many, presumably bad, parses.
This table indicates that a very large amount of
parses can be constructed for some sentences. Fur-
thermore, the maximum entropy disambiguation
component does a good job in selecting good
parses from those. Accuracy is given here in terms
of f-score of named dependencies.
sents parses oracle arbitrary model
536 45011 95.74 76.56 89.39
(2) MST Parser (McDonald et al, 2005) is a
26
data-driven graph-based dependency parser. The
system couples a minimum spanning tree search
procedure with a separate second stage classifier
to label the dependency edges.
(3) MALT Parser (Nivre et al, 2007) is a data-
driven transition-based dependency parser. Malt
parser uses SVMs to learn a classifier that predicts
the next parsing action. Instances represent parser
configurations and the label to predict determines
the next parser action.
Both data-driven parsers (MST and Malt) are
thus not specific for the Dutch Language, however,
they can be trained on a variety of languages given
that the training corpus complies with the column-
based format introduced in the 2006 CoNLL
shared task (Buchholz and Marsi, 2006). Ad-
ditionally, both parsers implement projective and
non-projective parsing algorithms, where the latter
will be used in our experiments on the relatively
free word order language Dutch. Despite that, we
train the data-driven parsers using their default set-
tings (e.g. first order features for MST, SVM with
polynomial kernel for Malt).
4 Datasets and experimental setup
The source domain on which all parsers are trained
is cdb, the Alpino Treebank (van Noord, 2006).
For our cross-domain evaluation, we consider
Wikipedia and DPC (Dutch Parallel Corpus) as
target data. All datasets are described next.
Source: Cdb The cdb (Alpino Treebank) con-
sists of 140,000 words (7,136 sentences) from the
Eindhoven corpus (newspaper text). It is a col-
lection of text fragments from 6 Dutch newspa-
pers. The collection has been annotated accord-
ing to the guidelines of CGN (Oostdijk, 2000) and
stored in XML format. It is the standard treebank
used to train the disambiguation component of the
Alpino parser. Note that cdb is a subset of the
training corpus used in the CoNLL 2006 shared
task (Buchholz and Marsi, 2006). The CoNLL
training data additionally contained a mix of non-
newspaper text,1 which we exclude here on pur-
pose to keep a clean baseline.
Target: Wikipedia and DPC We use the
Wikipedia and DPC subpart of the LASSY cor-
1Namely, a large amount of questions (from CLEF,
roughly 4k sentences) and hand-crafted sentences used dur-
ing the development of the grammar (1.5k).
Wikipedia Example articles #a #w ASL
LOC (location) Belgium, Antwerp (city) 31 25259 11.5
KUN (arts) Tervuren school 11 17073 17.1
POL (politics) Belgium elections 2003 16 15107 15.4
SPO (sports) Kim Clijsters 9 9713 11.1
HIS (history) History of Belgium 3 8396 17.9
BUS (business) Belgium Labour Federation 9 4440 11.0
NOB (nobility) Albert II 6 4179 15.1
COM (comics) Suske and Wiske 3 4000 10.5
MUS (music) Sandra Kim, Urbanus 3 1296 14.6
HOL (holidays) Flemish Community Day 4 524 12.2
Total 95 89987 13.4
DPC Description/Example #a #words ASL
Science medicine, oeanography 69 60787 19.2
Institutions political speeches 21 28646 16.1
Communication ICT/Internet 29 26640 17.5
Welfare state pensions 22 20198 17.9
Culture darwinism 11 16237 20.5
Economy inflation 9 14722 18.5
Education education in Flancers 2 11980 16.3
Home affairs presentation (Brussel) 1 9340 17.3
Foreign affairs European Union 7 9007 24.2
Environment threats/nature 6 8534 20.4
Finance banks (education banker) 6 6127 22.3
Leisure various (drugscandal) 2 2843 20.3
Consumption toys from China 1 1310 22.6
Total 186 216371 18.5
Table 1: Overview Wikipedia and DPC corpus (#a
articles, #w words, ASL average sentence length)
pus2 as target domains. These corpora contain sev-
eral domains, e.g. sports, locations, science. On
overview of the corpora is given in Table 1. Note
that both consist of hand-corrected data labeled by
Alpino, thus all domains employ the same anno-
tation scheme. This might introduce a slight bias
towards Alpino, however it has the advantage that
all domains employ the same annotation scheme ?
which was the major source of error in the CoNLL
task on domain adaptation (Dredze et al, 2007).
CoNLL2006 This is the testfile for Dutch that
was used in the CoNLL 2006 shared task on multi-
lingual dependency parsing. The file consists
of 386 sentences from an institutional brochure
(about youth healthcare). We use this file to check
our data-driven models against state-of-the-art.
Alpino to CoNLL format In order to train the
MST and Malt parser and evaluate it on the var-
ious Wikipedia and DPC articles, we needed to
convert the Alpino Treebank format into the tab-
ular CoNLL format. To this end, we adapted the
treebank conversion software developed by Erwin
Marsi for the CoNLL 2006 shared task on multi-
lingual dependency parsing. Instead of using the
PoS tagger and tagset used in the shared task (to
which we did not have access to), we replaced the
PoS tags with more fine-grained tags obtained by
2LASSY (Large Scale Syntactic Annotation of written
Dutch), ongoing project. Corpus version 17905, obtained
from http://www.let.rug.nl/vannoord/Lassy/corpus/
27
parsing the data with the Alpino parser.3 At testing
time, the data-driven parsers are given PoS tagged
input, while Alpino gets plain sentences.
Evaluation In all experiments, unless otherwise
specified, performance is measured as Labeled
Attachment Score (LAS), the percentage of to-
kens with the correct dependency edge and label.
To compute LAS, we use the CoNLL 2007 eval-
uation script4 with punctuation tokens excluded
from scoring (as was the default setting in CoNLL
2006). We thus evaluate all parsers using the same
evaluation metric. Note that the standard metric
for Alpino would be a variant of LAS, which al-
lows for a discrepancy between expected and re-
turned dependencies. Such a discrepancy can oc-
cur, for instance, because the syntactic annotation
of Alpino allows words to be dependent on more
than a single head (?secondary edges?) (van No-
ord, 2006). However, such edges are ignored in
the CoNLL format; just a single head per token
is allowed. Furthermore, there is another simpli-
fication. As the Dutch tagger used in the CoNLL
2006 shared task did not have the concept of multi-
words, the organizers chose to treat them as a sin-
gle token (Buchholz and Marsi, 2006). We here
follow the CoNLL 2006 task setup. To determine
whether results are significant, we us the Approx-
imate Randomization Test (see Yeh (2000)) with
1000 random shuffles.
5 Domain sensitivity
The problem of domain dependence poses a chal-
lenge for both kinds of parsing systems, data-
driven and grammar-driven. However, to what ex-
tent? Which kind of parsing system is more af-
fected by domain shifts? We may rephrase our
question as: Which parsing system is more robust
to different input texts? To answer this question,
we will examine the robustness of the different
parsing systems in terms of variation of accuracy
on a variety of domains.
A measure of domain sensitivity Given a pars-
ing system (p) trained on some source domain
and evaluated on a set of N target domains, the
most intuitive measure would be to simply calcu-
3As discussed later (Section 6, cf. Table 2), using Alpino
tags actually improves the performance of the data-driven
parsers. We could perform this check as we recently got ac-
cess to the tagger and tagset used in the CoNLL shared task
(Mbt with wotan tagset; thanks to Erwin Marsi).
4
http://nextens.uvt.nl/depparse-wiki/SoftwarePage
late mean (?) and standard deviation (sd) of the
performance on the target domains:
LASip = accuracy of parser p on target domain i
?targetp =
?N
i=1 LAS
i
p
N
, sdtargetp =
?
?N
i=1(LAS
i
p ? ?
target
p )2
N ? 1
However, standard deviation is highly influenced
by outliers. Furthermore, this measure does not
take the source domain performance (baseline)
into consideration nor the size of the target domain
itself. We thus propose to measure the domain
sensitivity of a system, i.e. its average domain
variation (adv), as weighted average difference
from the baseline (source) mean, where weights
represents the size of the various domains:
adv =
?N
i=1w
i ??ip
?N
i=1wi
, with
?ip = LAS
i
p?LAS
baseline
p and w
i =
size(wi)
?N
i=1 size(wi)
In more detail, we measure average domain
variation (adv) relative to the baseline (source do-
main) performance by considering non-squared
differences from the out-of-domain mean and
weigh it by domain size. The adv measure can
thus take on positive or negative values. Intu-
itively, it will indicate the average weighted gain
or loss in performance, relative to the source do-
main. As alternative, we may want to just cal-
culate a straight, unweighted average: uadv =
?N
i=1 ?
i
p/N . However, this assumes that domains
have a representative size, and a threshold might
be needed to disregard domains that are presum-
ably too small.
We will use adv in the empirical result section
to evaluate the domain sensitivity of the parsers,
where sizewill be measured in terms of number of
words. We additionally provide values for the un-
weighted version using domains with at least 4000
words (cf. Table 1).
6 Empirical results
First of all, we performed several sanity checks.
We trained the MST parser on the entire original
CoNLL training data as well as the cdb subpart
only, and evaluated it on the original CoNLL test
data. As shown in Table 2 (row 1-2) the accura-
cies of both models falls slightly below state-of-
the-art performance (row 5), most probably due to
the fact that we used standard parsing settings (e.g.
28
no second-order features for MST). More impor-
tantly, there was basically no difference in perfor-
mance when trained on the entire data or cdb only.
Model LAS UAS
MST (original CoNLL) 78.35 82.89
MST (original CoNLL, cdb subpart) 78.37 82.71
MST (cdb retagged with Alpino) 82.14 85.51
Malt (cdb retagged with Alpino) 80.64 82.66
MST (Nivre and McDonald, 2008) 79.19 83.6
Malt (Nivre and McDonald, 2008) 78.59 n/a
MST (cdb retagged with Mbt) 78.73 82.66
Malt (cdb retagged with Mbt) 75.34 78.29
Table 2: Performance of data-driven parsers ver-
sus state-of-the-art on the CoNLL 2006 testset (in
Labeled/Unlabeled Attachment Score).
We then trained the MST and Malt parser on
the cdb corpus converted into the retagged CoNLL
format, and tested on CoNLL 2006 test data (also
retagged with Alpino). As seen in Table 2, by
using Alpino tags the performance level signifi-
cantly improves (with p < 0.002 using Approx-
imate Randomization Test with 1000 iterations).
This increase in performance can be attributed to
two sources: (a) improvements in the Alpino tree-
bank itself over the course of the years, and (b) the
more fine-grained PoS tagset obtained by parsing
the data with the deep grammar. To examine the
contribution of each source, we trained an addi-
tional MST model on the cdb data but tagged with
the same tagger as in the CoNLL shared task (Mbt,
cf. Table 2 last row): the results show that the
major source of improvement actually comes from
using the more fine-grained Alpino tags (78.73?
82.14 = +3.41 LAS), rather than the changes in
the treebank (78.37 ? 78.73 = +0.36 LAS).
Thus, despite the rather limited training data and
use of standard training settings, we are in line
with, and actually above, current results of data-
driven parsing for Dutch.
Baselines To establish our baselines, we per-
form 5-fold cross validation for each parser on the
source domain (cdb corpus, newspaper text). The
baselines for each parser are given in Table 3. The
grammar-driven parser Alpino achieves a baseline
that is significantly higher (90.75% LAS) com-
pared to the baselines of the data-driven systems
(around 80-83% LAS).
Cross-domain results As our goal is to assess
performance variation across domains, we evalu-
ate each parser on the Wikipedia and DPC corpora
Model Alpino MST Malt
Baseline (LAS) 90.76 83.63 79.95
Baseline (UAS) 92.47 88.12 83.31
Table 3: Baseline (5-fold cross-validation). All
differences are significant at p < 0.001.
that cover a variety of domains (described in Ta-
ble 1). Figure 1 and Figure 2 summarizes the re-
sults for each corpus, respectively. In more detail,
the figures depict for each parser the baseline per-
formance as given in Table 3 (straight lines) and
the performance on every domain (bars). Note that
domains are ordered by size (number of words), so
that the largest domains appear as bars on the left.
Similar graphs come up if we replace labeled at-
tachment score with its unlabeled variant.
Figure 1 depicts parser performance on the
Wikipedia domains with respect to the source
domain baseline. The figure indicates that the
grammar-driven parser does not suffer much from
domain shifts. Its performance falls even above
baseline for several Wikipedia domains. In con-
trast, the MST parser suffers the most from the
domain changes; on most domains a substantial
performance drop can be observed. The transition-
based parser scores on average significantly lower
than the graph-based counterpart and Alpino, but
seems to be less affected by the domain shifts.
We can summarize this findings by our pro-
posed average domain variation measure (un-
weighted scores are given in the Figure): On av-
erage (over all Wikipedia domains), Alpino suf-
fers the least (adv = +0.81), followed by Malt
(+0.59) and MST (?2.2), which on average loses
2.2 absolute LAS. Thus, the graph-based data-
driven dependency parser MST suffers the most.
We evaluate the parsers also on the more var-
ied DPC corpus. It contains a broader set of do-
mains, amongst others science texts (medical texts
from the European Medicines Agency as well as
texts about oceanography) and articles with more
technical vocabulary (Communication, i.e. Inter-
net/ICT texts). The results are depicted in Fig-
ure 2. Both Malt (adv = 0.4) and Alpino (adv =
0.22) achieve on average a gain over the baseline,
with this time Malt being slightly less domain af-
fected than Alpino (most probably because Malt
scores above average on the more influential/larger
domains). Nevertheless, Alpino?s performance
level is significantly higher compared to both data-
driven counterparts. The graph-based data-driven
29
La
be
led
 At
tac
hm
en
t S
cor
e (L
AS)
Alpino
adv= 0.81 (+/? 3.7 )
uadv (>4k)= 2 (+/? 2.1 )
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
LO
C
KU
N
PO
L
SP
O HIS BU
S
NO
B
CO
M
MU
S
HO
L
LO
C
KU
N
PO
L
SP
O HIS BU
S
NO
B
CO
M
MU
S
HO
L
MST
adv = ?2.2 (+/? 9 )
uadv (>4k)= ?1.8 (+/? 4 )
LO
C
KU
N
PO
L
SP
O HIS BU
S
NO
B
CO
M
MU
S
HO
L
Malt
adv = 0.59 (+/? 9.4 )
uadv(>4k)= 1.3 (+/? 3 )
Alpino
MST
Malt
Figure 1: Performance on Wikipedia domains with respect to the source baseline (newspaper text) in-
cluding average domain variation (adv) score and its unweighted alternative (uadv). Domains are ordered
by size (largest on left). Full-colored bars indicate domains where performance lies below the baseline.
parser MST is the most domain-sensitive parser
also on DPC (adv = ?0.27).
In contrast, if we would take only the deviation
on the target domains into consideration (with-
out considering the baseline, cf. Section 5), we
would get a completely opposite ranking on DPC,
where the Malt parser would actually be consid-
ered the most domain-sensitive (here higher sd
means higher sensitivity): Malt (sd = 1.20), MST
(sd = 1.14), Alpino (sd = 1.05). However, by
looking at Figure 2, intuitively, MST suffers more
from the domain shifts than Malt, as most bars lie
below the baseline. Moreover, the standard devia-
tion measure neither gives a sense of whether the
parser on average suffers a loss or gain over the
new domains, nor incorporates the information of
domain size. We thus believe our proposed aver-
age domain variation is a better suited measure.
To check whether the differences in perfor-
mance variation are statistically significant, we
performed an Approximate Randomization Test
over the performance differences (deltas) on the
23 domains (DPC and Wikipedia). The results
show that the difference between Alpino and MST
is significant. The same goes for the difference
between MST and Malt. Thus Alpino is signifi-
cantly more robust than MST. However, the dif-
ference between Alpino and Malt is not signif-
icant. These findings hold for differences mea-
sured in both labeled and unlabeled attachments
scores. Furthermore, all differences in absolute
performance across domains are significant.
To summarize, our empirical evaluation shows
that the grammar-driven system Alpino is rather
robust across domains. It is the best perform-
ing system and it is significantly more robust than
MST. In constrast, the transition-based parser Malt
scores the lowest across all domains, but its vari-
ation turned out not to be different from Alpino.
Over all domains, MST is the most domain-
sensitive parser.
30
La
be
led
 A
tta
ch
me
nt 
Sc
ore
 (LA
S)
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
Sc
ien
ce
Ins
titu
tio
ns
Co
mm
un
ica
tio
n
W
elf
are
_s
tat
e
Cu
ltu
re
Ec
on
om
y
Ed
uc
ati
on
Ho
me
_a
ffa
irs
Fo
rei
gn
_a
ffa
irs
En
vir
on
me
nt
Fin
an
ce
Le
isu
re
Co
ns
um
pti
on
Alpino
adv = 0.22 (+/? 0.823 )
uadv (>4k)= 0.4 (+/? 0.8 )
Sc
ien
ce
Ins
titu
tio
ns
Co
mm
un
ica
tio
n
W
elf
are
_s
tat
e
Cu
ltu
re
Ec
on
om
y
Ed
uc
ati
on
Ho
me
_a
ffa
irs
Fo
rei
gn
_a
ffa
irs
En
vir
on
me
nt
Fin
an
ce
Le
isu
re
Co
ns
um
pti
on
MST
adv = ?0.27 (+/? 0.56 )
uadv (>4k)= ?0.21 (+/? 1 )
Sc
ien
ce
Ins
titu
tio
ns
Co
mm
un
ica
tio
n
W
elf
are
_s
tat
e
Cu
ltu
re
Ec
on
om
y
Ed
uc
ati
on
Ho
me
_a
ffa
irs
Fo
rei
gn
_a
ffa
irs
En
vir
on
me
nt
Fin
an
ce
Le
isu
re
Co
ns
um
pti
on
Malt
adv = 0.4 (+/? 0.54 )
uadv (>4k)= 0.41 (+/? 0.9 )
Alpino
MST
Malt
Figure 2: Performance on DPC domains with respect to the source baseline (newspaper text).
Excursion: Lexical information Both kinds
of parsing systems rely on lexical information
(words/stems) when learning their parsing (or
parse disambiguation) model. However, how
much influence does lexical information have?
To examine this issue, we retrain all parsing sys-
tems by excluding lexical information. As all pars-
ing systems rely on a feature-based representa-
tion, we remove all feature templates that include
words and thus train models on a reduced fea-
ture space (original versus reduced space: Alpino
24k/7k features; MST 14M/1.9M features; Malt
17/13 templates). The result of evaluating the
unlexicaled models on Wikipedia are shown in
Figure 3. Clearly, performance drops for for all
parsers in all domains. However, for the data-
driven parsers to a much higher degree. For in-
stance, MST loses on average 11 absolute points
in performance (adv = ?11) and scores below
baseline on all Wikipedia domains. In contrast,
the grammar-driven parser Alpino suffers far less,
still scores above baseline on some domains.5 The
Malt parser lies somewhere in between, also suf-
fers from the missing lexical information, but to a
lesser degree than the graph-based parser MST.
7 Conclusions and Future work
We examined a grammar-based system cou-
pled with a statistical disambiguation component
(Alpino) and two data-driven statistical parsing
systems (MST and Malt) for dependency parsing
of Dutch. By looking at the performance variation
across a large variety of domains, we addressed
the question of how sensitive the parsing systems
are to the text domain. This, to gauge which kind
5Note that the parser has still access to its lexicon here;
for now we removed lexicalized features from the trainable
part of Alpino, the statistical disambiguation component.
31
La
bel
ed 
Att
ach
me
nt S
cor
e (L
AS)
Alpino
adv= ?0.63 (+/? 3.6 )
uadv (>4k)= 0.1 (+/? 2 )
66
68
70
72
74
76
78
80
82
84
86
88
90
92
94
96
LO
C
KU
N
PO
L
SP
O HIS BU
S
NO
B
CO
M
MU
S
HO
L
LO
C
KU
N
PO
L
SP
O HIS BU
S
NO
B
CO
M
MU
S
HO
L
MST
adv = ?11 (+/? 11 )
uadv (>4k)= ?11 (+/? 2.1 )
LO
C
KU
N
PO
L
SP
O HIS BU
S
NO
B
CO
M
MU
S
HO
L
Malt
adv = ?4.9 (+/? 9 )
uadv (>4k)= ?4.8 (+/? 3 )
Alpino
MST
Malt
Figure 3: Performance of unlexical parsers on Wikipedia domains with respect to the source baseline.
of system (data-driven versus grammar-driven) is
more affected by domain shifts, and thus more in
need for adaptation techniques. We also proposed
a simple measure to quantify domain sensitivity.
The results show that the grammar-based sys-
tem Alpino is the best performing system, and it
is robust across domains. In contrast, MST, the
graph-based approach to data-driven parsing is the
most domain-sensitive parser. The results for Malt
indicate that its variation across domains is lim-
ited, but this parser is outperformed by both other
systems on all domains. In general, data-driven
systems heavily rely on the training data to esti-
mate their models. This becomes apparent when
we exclude lexical information from the train-
ing process, which results in a substantial perfor-
mance drop for the data-driven systems, MST and
Malt. The grammar-driven model was more robust
against the missing lexical information. Grammar-
driven systems try to encode domain independent
linguistic knowledge, but usually suffer from cov-
erage problems. The Alpino parser successfully
implements a set of unknown word heuristics and
a partial parsing strategy (in case no full parse can
be found) to overcome this problem. This makes
the system rather robust across domains, and, as
shown in this study, significantly more robust than
MST. This is not to say that domain dependence
does not consitute a problem for grammar-driven
parsers at all. As also noted by Zhang and Wang
(2009), the disambiguation component and lexi-
cal coverage of grammar-based systems are still
domain-dependent. Thus, domain dependence is a
problem for both types of parsing systems, though,
as shown in this study, to a lesser extent for the
grammar-based system Alpino. Of course, these
results are specific for Dutch; however, it?s a first
step. As the proposed methods are indepedent of
language and parsing system, they can be applied
to another system or language.
In future, we would like to (a) perform an error
analysis (e.g. why for some domains the parsers
outperform their baseline; what are typical in-
domain and out-domain errors), (a) examine why
there is such a difference in performance variation
between Malt and MST, and (c) investigate what
part(s) of the Alpino parser are responsible for the
differences with the data-driven parsers.
32
References
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Conference on Empirical Meth-
ods in Natural Language Processing, Sydney.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL, Prague, Czech Republic.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
In Proc. of CoNLL, pages 149?164.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In ACL, Prague, Czech Republic.
Danie?l de Kok, Jianqiang Ma, and Gertjan van Noord.
2009. A generalized method for iterative error min-
ing in parsing results. In Proceedings of the 2009
Workshop on Grammar Engineering Across Frame-
works (GEAF 2009), pages 71?79, Suntec, Singa-
pore, August.
Mark Dredze, John Blitzer, Pratha Pratim Taluk-
dar, Kuzman Ganchev, Joao Graca, and Fernando
Pereira. 2007. Frustratingly hard domain adaptation
for parsing. In Proceedings of the CoNLL Shared
Task Session, Prague, Czech Republic.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the 2001 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Tadayoshi Hara, Miyao Yusuke, and Jun?ichi Tsu-
jii. 2005. Adapting a probabilistic disambiguation
model of an hpsg parser to a new domain. In Pro-
ceedings of the International Joint Conference on
Natural Language Processing.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152?159, New York City.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523?530.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL-08: HLT, pages
950?958, Columbus, Ohio, June.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13:95?135.
Nelleke Oostdijk. 2000. The Spoken Dutch Corpus:
Overview and first evaluation. In Proceedings of
LREC, pages 887?894.
Barbara Plank and Gertjan van Noord. 2008. Ex-
ploring an auxiliary distribution based approach to
domain adaptation of a syntactic disambiguation
model. In Proceedings of the Workshop on Cross-
Framework and Cross-Domain Parser Evaluation
(PE), Manchester, August.
Robbert Prins and Gertjan van Noord. 2003. Reinforc-
ing parser preferences through tagging. Traitement
Automatique des Langues, 44(3):121?139.
Kenji Sagae, Yusuke Miyao, and Jun?ichi Tsujii. 2007.
Hpsg parsing with shallow dependency constraints.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 624?
631, Prague, Czech Republic, June.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In ACL2004,
Barcelona. ACL.
Gertjan van Noord. 2006. At Last Parsing Is Now
Operational. In TALN 2006 Verbum Ex Machina,
Actes De La 13e Conference sur Le Traitement
Automatique des Langues naturelles, pages 20?42,
Leuven.
Gertjan van Noord. 2007. Using self-trained bilexical
preferences to improve disambiguation accuracy. In
Proceedings of the International Workshop on Pars-
ing Technology (IWPT), ACL 2007 Workshop, pages
1?10, Prague. ACL.
Gertjan van Noord. 2009. Learning efficient parsing.
In EACL 2009, The 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 817?825, Athens, Greece.
Gert Veldhuijzen van Zanten, Gosse Bouma, Khalil
Sima?an, Gertjan van Noord, and Remko Bonnema.
1999. Evaluation of the NLP components of the
OVIS2 spoken dialogue system. In Frank van
Eynde, Ineke Schuurman, and Ness Schelkens, ed-
itors, Computational Linguistics in the Netherlands
1998, pages 213?229. Rodopi Amsterdam.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In ACL,
pages 947?953, Morristown, NJ, USA.
Yi Zhang and Rui Wang. 2009. Cross-domain depen-
dency parsing using a deep linguistic grammar. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 378?386, Suntec, Singapore,
August.
33
