Proceedings of the 12th Conference of the European Chapter of the ACL, pages 246?254,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Company-Oriented Extractive Summarization of Financial News?
Katja Filippova?, Mihai Surdeanu?, Massimiliano Ciaramita?, Hugo Zaragoza?
?EML Research gGmbH ?Yahoo! Research
Schloss-Wolfsbrunnenweg 33 Avinguda Diagonal 177
69118 Heidelberg, Germany 08018 Barcelona, Spain
filippova@eml-research.de,{mihais,massi,hugoz}@yahoo-inc.com
Abstract
The paper presents a multi-document sum-
marization system which builds company-
specific summaries from a collection of fi-
nancial news such that the extracted sen-
tences contain novel and relevant infor-
mation about the corresponding organiza-
tion. The user?s familiarity with the com-
pany?s profile is assumed. The goal of
such summaries is to provide information
useful for the short-term trading of the cor-
responding company, i.e., to facilitate the
inference from news to stock price move-
ment in the next day. We introduce a
novel query (i.e., company name) expan-
sion method and a simple unsupervized al-
gorithm for sentence ranking. The sys-
tem shows promising results in compari-
son with a competitive baseline.
1 Introduction
Automatic text summarization has been a field of
active research in recent years. While most meth-
ods are extractive, the implementation details dif-
fer considerably depending on the goals of a sum-
marization system. Indeed, the intended use of the
summaries may help significantly to adapt a par-
ticular summarization approach to a specific task
whereas the broadly defined goal of preserving rel-
evant, although generic, information may turn out
to be of little use.
In this paper we present a system whose goal is
to extract sentences from a collection of financial
?This work was done during the first author?s internship
at Yahoo! Research. Mihai Surdeanu is currently affiliated
with Stanford University (mihais@stanford.edu).
Massimiliano Ciaramita is currently at Google
(massi@google.com).
news to inform about important events concern-
ing companies, e.g., to support trading (i.e., buy or
sell) the corresponding symbol on the next day, or
managing a portfolio. For example, a company?s
announcement of surpassing its earnings? estimate
is likely to have a positive short-term effect on its
stock price, whereas an announcement of job cuts
is likely to have the reverse effect. We demonstrate
how existing methods can be extended to achieve
precisely this goal.
In a way, the described task can be classified
as query-oriented multi-document summarization
because we are mainly interested in information
related to the company and its sector. However,
there are also important differences between the
two tasks.
? The name of the company is not a query,
e.g., as it is specified in the context of the
DUC competitions1, and requires an exten-
sion. Initially, a query consists exclusively
of the ?symbol?, i.e., the abbreviation of the
name of a company as it is listed on the stock
market. For example, WPO is the abbrevia-
tion used on the stock market to refer to The
Washington Post?a large media and educa-
tion company. Such symbols are rarely en-
countered in the news and cannot be used to
find all the related information.
? The summary has to provide novel informa-
tion related to the company and should avoid
general facts about it which the user is sup-
posed to know. This point makes the task
related to update summarization where one
has to provide the user with new information
1http://duc.nist.gov; since 2008 TAC: http:
//www.nist.gov/tac.
246
given some background knowledge2. In our
case, general facts about the company are as-
sumed to be known by the user. Given WPO,
we want to distinguish between The Wash-
ington Post is owned by The Washington Post
Company, a diversified education and media
company and The Post recently went through
its third round of job cuts and reported an
11% decline in print advertising revenues for
its first quarter, the former being an example
of background information whereas the lat-
ter is what we would like to appear in the
summary. Thus, the similarity to the query
alone is not the decisive parameter in com-
puting sentence relevance.
? While the summaries must be specific for a
given organization, important but general fi-
nancial events that drive the overall market
must be included in the summary. For exam-
ple, the recent subprime mortgage crisis af-
fected the entire economy regardless of the
sector.
Our system proceeds in the three steps illus-
trated in Figure 1. First, the company symbol is
expanded with terms relevant for the company, ei-
ther directly ? e.g., iPod is directly related to Apple
Inc. ? or indirectly ? i.e., using information about
the industry or sector the company operates in. We
detail our symbol expansion algorithm in Section
3. Second, this information is used to rank sen-
tences based on their relatedness to the expanded
query and their overall importance (Section 4). Fi-
nally, the most relevant sentences are re-ranked
based on the degree of novelty they carry (Section
5).
The paper makes the following contributions.
First, we present a new query expansion tech-
nique which is useful in the context of company-
dependent news summarization as it helps identify
sentences important to the company. Second, we
introduce a simple and efficient method for sen-
tence ranking which foregrounds novel informa-
tion of interest. Our system performs well in terms
of the ROUGE score (Lin & Hovy, 2003) com-
pared with a competitive baseline (Section 6).
2 Data
The data we work with is a collection of financial
news consolidated and distributed by Yahoo! Fi-
2See the DUC 2007 and 2008 update tracks.
nance3 from various sources4. Each story is la-
beled as being relevant for a company ? i.e., it
appears in the company?s RSS feed ? if the story
mentions either the company itself or the sector the
company belongs to. Altogether the corpus con-
tains 88,974 news articles from a period of about
5 months (148 days). Some articles are labeled
as being relevant for several companies. The total
number of (company name, news collection) pairs
is 46,444.
The corpus is cleaned of HTML tags, embed-
ded graphics and unrelated information (e.g., ads,
frames) with a set of manually devised rules. The
filtering is not perfect but removes most of the
noise. Each article is passed through a language
processing pipeline (described in (Atserias et al,
2008)). Sentence boundaries are identified by
means of simple heuristics. The text is tokenized
according to Penn TreeBank style and each to-
ken lemmatized using Wordnet?s morphological
functions. Part of speech tags and named entities
(LOC, PER, ORG, MISC) are identified by means
of a publicly available named-entity tagger5 (Cia-
ramita & Altun, 2006, SuperSense). Apart from
that, all sentences which are shorter than 5 tokens
and contain neither nouns nor verbs are sorted out.
We apply the latter filter as we are interested in
textual information only. Numeric information
contained, e.g., in tables can be easily and more
reliably obtained from the indices tables available
online.
3 Query Expansion
In company-oriented summarization query expan-
sion is crucial because, by default, our query con-
tains only the symbol, that is the abbreviation of
the name of the company. Unfortunately, exist-
ing query expansion techniques which utilize such
knowledge sources as WordNet or Wikipedia are
not useful for symbol expansion. WordNet does
not include organizations in any systematic way.
Wikipedia covers many companies but it is unclear
how it can be used for expansion.
3http://finance.yahoo.com
4http://biz.yahoo.com, http://www.
seekingalpha.com, http://www.marketwatch.
com, http://www.reuters.com, http://www.
fool.com, http://www.thestreet.com, http:
//online.wsj.com, http://www.forbes.com,
http://www.cnbc.com, http://us.ft.com,
http://www.minyanville.com
5http://sourceforge.net/projects/
supersensetag
247
Expansion
Query
Expanded
Query
Relatedness
to Query
Filtering
Relevant
Sentences
Ranking
Novelty
Company
Profile
Yahoo! Finance
Symbol
Summary
News
Figure 1: System architecture
Intuitively, a good expansion method should
provide us with a list of products, or properties,
of the company, the field it operates in, the typi-
cal customers, etc. Such information is normally
found on the profile page of a company at Yahoo!
Finance6. There, so called ?business summaries?
provide succinct and financially relevant informa-
tion about the company. Thus, we use business
summaries as follows. For every company sym-
bol in our collection, we download its business
summary, split it into tokens, remove all words
but nouns and verbs which we then lemmatize.
Since words like company are fairly uninforma-
tive in the context of our task, we do not want to
include them in the expanded query. To filter out
such words, we compute the company-dependent
TF*IDF score for every word on the collection of
all business summaries:
score(w) = tfw,c ? log
?
N
cfw
?
(1)
where c is the business summary of a company,
tfw,c is the frequency of w in c, N is the total
number of business summaries we have, cfw is
the number of summaries that contain w. This
formula penalizes words occurring in most sum-
maries (e.g., company, produce, offer, operate,
found, headquarter, management). At the mo-
ment of running the experiments, N was about
3,000, slightly less than the total number of sym-
6http://finance.yahoo.com/q/pr?s=AAPL
where the trading symbol of any company can be used
instead of AAPL.
bols because some companies do not have a busi-
ness summary on Yahoo! Finance. It is impor-
tant to point out that companies without a business
summary are usually small and are seldom men-
tioned in news articles: for example, these compa-
nies had relevant news articles in only 5% of the
days monitored in this work.
Table 1 gives the ten high scoring words for
three companies (Apple Inc. ? the computer and
software manufacture, Delta Air Lines ? the air-
line, and DaVita ? dyalisis services). Table 1
shows that this approach succeeds in expanding
the symbol with terms directly related to the com-
pany, e.g., ipod for Apple, but also with more gen-
eral information like the industry or the company
operates in, e.g., software and computer for Apple.
All words whose TF*IDF score is above a certain
threshold ? are included in the expanded query (?
was tuned to a value of 5.0 on the development
set).
4 Relatedness to Query
Once the expanded query is generated, it can be
used for sentence ranking. We chose the system of
Otterbacher et al (2005) as a a starting point for
our approach and also as a competitive baseline
because it has been successfully tested in a simi-
lar setting?it has been applied to multi-document
query-focused summarization of news documents.
Given a graph G = (S,E), where S is the set
of all sentences from all input documents, and E is
the set of edges representing normalized sentence
similarities, Otterbacher et al (2005) rank all sen-
248
AAPL DAL DVA
apple air dialysis
music flight davita
mac delta esrd
software lines kidney
ipod schedule inpatient
computer destination outpatient
peripheral passenger patient
movie cargo hospital
player atlanta disease
desktop fleet service
Table 1: Top 10 scoring words for three companies
tence nodes based on the inter-sentence relations
as well as the relevance to the query q. Sentence
ranks are found iteratively over the set of graph
nodes with the following formula:
r(s, q) = ?
rel(s|q)
P
t?S rel(t|q)
+(1??)
X
t?S
sim(s, t)
P
v?S sim(v, t)
r(t, q) (2)
The first term represents the importance of a sen-
tence defined in respect to the query, whereas the
second term infers the importance of the sentence
from its relation to other sentences in the collec-
tion. ? ? (0, 1) determines the relative importance
of the two terms and is found empirically. Another
parameter whose value is determined experimen-
tally is the sentence similarity threshold ? , which
determines the inclusion of a sentence in G. Ot-
terbacher et al (2005) report 0.2 and 0.95 to be
the optimal values for ? and ? respectively. These
values turned out to produce the best results also
on our development set and were used in all our
experiments. Similarity between sentences is de-
fined as the cosine of their vector representations:
sim(s, t) =
P
w?s?t weight(w)
2
q
P
w?s weight(w)2 ?
q
P
w?t weight(w)2
(3)
weight(w) = tfw,sidfw,S (4)
idfw,S = log
( |S| + 1
0.5 + sfw
)
(5)
where tfw,s is the frequency of w in sentence s,
|S| is the total number of sentences in the docu-
ments from which sentences are to be extracted,
and sfw is the number of sentences which contain
the word w (all words in the documents as well
as in the query are stemmed and stopwords are re-
moved from them). Relevance to the query is de-
fined in Equation (6) which has been previously
used for sentence retrieval (Allan et al, 2003):
rel(s|q) =
X
w?q
log(tfw,s + 1) ? log(tfw,q + 1) ? idfw,S (6)
where tfw,x stands for the number of times w ap-
pears in x, be it a sentence (s) or the query (q). If
a sentence shares no words other than stopwords
with the query, the relevance becomes zero. Note
that without the relevance to the query part Equa-
tion 2 takes only inter-sentence similarity into ac-
count and computes the weighted PageRank (Brin
& Page, 1998).
In defining the relevance to the query, in Equa-
tion (6), words which do not appear in too many
sentences in the document collection weigh more.
Indeed, if a word from the query is contained in
many sentences, it should not count much. But it
is also true that not all words from the query are
equally important. As it has been mentioned in
Section 3, words like product or offer appear in
many business summaries and are equally related
to any company. To penalize such words, when
computing the relevance to the query, we multiply
the relevance score of a given word w with the in-
verted document frequency of w on the corpus of
business summaries Q ? idfw,Q:
idfw,Q = log
( |Q|
qfw
)
(7)
We also replace tfw,s with the indicator function
s(w) since it has been reported to be more ad-
equate for sentences, in particular for sentence
alignment (Nelken & Shieber, 2006):
s(w) =
{
1 if s contains w
0 otherwise
(8)
Thus, the modified formula we use to compute
sentence ranks is as follows:
rel(s|q) =
X
w?q
s(w) ? log(tfw,q + 1) ? idfw,S ? idfw,Q (9)
We call these two ranking algorithms that use
the formula in (2) OTTERBACHER and QUERY
WEIGHTS, the difference being the way the rel-
evance to the query is computed: (6) or (9). We
use the OTTERBACHER algorithm as a baseline in
the experiments reported in Section 6.
249
5 Novelty Bias
Apart from being related to the query, a good sum-
mary should provide the user with novel infor-
mation. According to Equation (2), if there are,
say, two sentences which are highly similar to the
query and which share some words, they are likely
to get a very high score. Experimenting with the
development set, we observed that sentences about
the company, such as e.g., DaVita, Inc. is a lead-
ing provider of kidney care in the United States,
providing dialysis services and education for pa-
tients with chronic kidney failure and end stage re-
nal disease, are ranked high although they do not
contribute new information. However, a non-zero
similarity to the query is indeed a good filter of the
information related to the company and to its sec-
tor and can be used as a prerequisite of a sentence
to be included in the summary. These observations
motivate our proposal for a ranking method which
aims at providing relevant and novel information
at the same time.
Here, we explore two alternative approaches to
add the novelty bias to the system:
? The first approach bypasses the relatedness
to query step introduced in Section 4 com-
pletely. Instead, this method merges the dis-
covery of query relatedness and novelty into
a single algorithm, which uses a sentence
graph that contains edges only between sen-
tences related to the query, (i.e., sentences for
which rel(s|q) > 0). All edges connecting
sentences which are unrelated to the query
are skipped in this graph. In this way we limit
the novelty ranking process to a subset of sen-
tences related to the query.
? The second approach models the problem
in a re-ranking architecture: we take the
top ranked sentences after the relatedness-to-
query filtering component (Section 4) and re-
rank them using the novelty formula intro-
duced below.
The main difference between the two approaches
is that the former uses relatedness-to-query and
novelty information but ignores the overall impor-
tance of a sentence as given by the PageRank al-
gorithm in Section 4, while the latter combines all
these aspects ?i.e., importance of sentences, relat-
edness to query, and novelty? using the re-ranking
architecture.
To amend the problem of general information
ranked inappropriately high, we modify the word-
weighting formula (4) so that it implements a nov-
elty bias, thus becoming dependent on the query.
A straightforward way to define the novelty weight
of a word would be to draw a line between the
?known? words, i.e., words appearing in the busi-
ness summary, and the rest. In this approach all
the words from the business summary are equally
related to the company and get the weight of 0:
weight(w) =
{
0 if Q contains w
tfw,sidfw,S otherwise
(10)
We call this weighting scheme SIMPLE. As
an alternative, we also introduce a more elab-
orate weighting procedure which incorporates
the relatedness-to-query (or rather distance from
query) in the word weight formula. Intuitively, the
more related to the query a word is (e.g., DaVita,
the name of the company), the more familiar to the
user it is and the smaller its novelty contribution
is. If a word does not appear in the query at all, its
weight becomes equal to the usual tfw,sidfw,S :
weight(w) =
 
1 ? tfw,q ? idfw,QP
wi?q
tfwi,q ? idfwi,Q
!
? tfw,sidfw,S (11)
The overall novelty ranking formula is based
on the query-dependent PageRank introduced in
Equation (2). However, since we already incorpo-
rate the relatedness to the query in these two set-
tings, we focus only on related sentences and thus
may drop the relatedness to the query part from
(2):
r?(s, q) = ? + (1 ? ?)
?
t?S
sim(s, t, q)
?
u?S sim(t, u, q)
(12)
We set ? to the same value as in OTTERBACHER.
We deliberately set the sentence similarity thresh-
old ? to a very low value (0.05) to prevent the
graph from becoming exceedingly bushy. Note
that this novelty-ranking formula can be equally
applied in both scenarios introduced at the begin-
ning of this section. In the first scenario, S stands
for the set of nodes in the graph that contains only
sentences related to the query. In the second sce-
nario, S contains the highest ranking sentences
detected by the relatedness-to-query component
(Section 4).
250
5.1 Redundancy Filter
Some sentences are repeated several times in the
collection. Such repetitions, which should be
avoided in the summary, can be filtered out ei-
ther before or after the sentence ranking. We ap-
ply a simple repetition check when incrementally
adding ranked sentences to the summary. If a sen-
tence to be added is almost identical to the one
already included in the summary, we skip it. Iden-
tity check is done by counting the percentage of
non-stop word lemmas in common between two
sentences. 95% is taken as the threshold.
We do not filter repetitions before the rank-
ing has taken place because often such repetitions
carry important and relevant information. The re-
dundancy filter is applied to all the systems de-
scribed as they are equally prone to include repe-
titions.
6 Evaluation
We randomly selected 23 company stock names,
and constructed a document collection for each
containing all the news provided in the Yahoo! Fi-
nance news feed for that company in a period of
two days (the time period was chosen randomly).
The average length of a news collection is about
600 tokens. When selecting the company names,
we took care of not picking those which have only
a few news articles for that period of time. This
resulted into 9.4 news articles per collection on av-
erage. From each of these, three human annotators
independently selected up to ten sentences. All an-
notators had average to good understanding of the
financial domain. The annotators were asked to
choose the sentences which could best help them
decide whether to buy, sell or retain stock for the
company the following day and present them in
the order of decreasing importance. The anno-
tators compared their summaries of the first four
collections and clarified the procedure before pro-
ceeding with the other ones. These four collec-
tions were then later used as a development set.
All summaries ? manually as well as automat-
ically generated ? were cut to the first 250 words
which made the summaries 10 words shorter on
average. We evaluated the performance automat-
ically in terms of ROUGE-2 (Lin & Hovy, 2003)
using the parameters and following the methodol-
ogy from the DUC events. The results are pre-
sented in Table 2. We also report the 95% confi-
dence intervals in brackets. As in DUC, we used
METHOD ROUGE-2
Otterbacher 0.255 (0.226 - 0.285)
Query Weights 0.289 (0.254 - 0.324)
Novelty Bias (simple) 0.315 (0.287 - 0.342)
Novelty Bias 0.302 (0.277 - 0.329)
Manual 0.472 (0.415 - 0.531)
Table 2: Results of the four extraction methods
and human annotators
jackknife for each (query, summary) pair and com-
puted a macro-average to make human and au-
tomatic results comparable (Dang, 2005). The
scores computed on summaries produced by hu-
mans are given in the bottom line (MANUAL) and
serve as upper bound and also as an indicator for
the inter-annotator agreement.
6.1 Discussion
From Table 2 follows that the modifications we
applied to the baseline are sensible and indeed
bring an improvement. QUERY WEIGHTS per-
forms better than OTTERBACHER and is in turn
outperformed by the algorithms biased to novel in-
formation (the two NOVELTY systems). The over-
lap between the confidence intervals of the base-
line and the simple version of the novelty algo-
rithm is minimal (0.002).
It is remarkable that the achieved improvement
is due to a more balanced relatedness to the query
ranking (9), as well as to the novelty bias re-
ranking. The fact that the simpler novelty weight-
ing formula (10) produced better results than the
more elaborated one (11) requires a deeper anal-
ysis and a larger test set to explain the difference.
Our conjecture so far is that the SIMPLE approach
allows for a better combination of both novelty
and relatedness to query. Since the more complex
novelty ranking formula penalizes terms related
to the query (Equation (11)), it favors a scenario
where novelty is boosted in detriment of related-
ness to query, which is not always realistic.
It is important to note that, compared with the
baseline, we did not do any parameter tuning for
? and the inter-sentence similarity threshold. The
improvement between the system of Otterbacher
et al (2005) and our best model is statistically
significant.
251
6.2 System Combination
Recall from Section 5 that the motivation for pro-
moting novel information came from the fact that
sentences with background information about the
company obtained very high scores: they were re-
lated but not novel. The sentences ranked by OT-
TERBACHER or QUERY WEIGHTS required a re-
ranking to include related and novel sentences in
the summary. We checked whether novelty re-
ranking brings an improvement if added on top
of a system which does not have a novelty bias
(baseline or QUERY WEIGHTS) and compared it
with the setting where we simply limit the novelty
ranking to all the sentences related to the query
(NOVELTY SIMPLE and NOVELTY). In the simi-
larity graph, we left only edges between the first
30 sentences from the ranked list produced by
one of the two algorithms described in Section 4
(OTTERBACHER or QUERY WEIGHTS). Then we
ranked the sentences biased to novel information
the same way as described in Section 5. The re-
sults are presented in Table 3. What we evalu-
ate here is whether a combination of two methods
performs better than the simple heuristics of dis-
carding edges between sentences unrelated to the
query.
METHOD ROUGE-2
Otterbacher + Novelty simple 0.280 (0.254 - 0.306)
Otterbacher + Novelty 0.273 (0.245 - 0.301)
Query Weights + Novelty simple 0.275 (0.247 - 0.302)
Query Weights + Novelty 0.265 (0.242 - 0.289)
Table 3: Results of the combinations of the four
methods
From the four possible combinations, there is
an improvement over the baseline only (0.255 vs.
0.280 resp. 0.273). None of the combinations per-
forms better than the simple novelty bias algo-
rithm on a subset of edges. This experiment sug-
gests that, at least in the scenario investigated here
(short-term monitoring of publicly-traded compa-
nies), novelty is more important than relatedness
to query. Hence, the simple novelty bias algo-
rithm, which emphasizes novelty and incorporates
relatedness to query only through a loose con-
straint (rel(s|q) > 0) performs better than com-
plex models, which are more constrained by the
relatedness to query.
7 Related Work
Summarization has been extensively investigated
in recent years and to date there exists a multi-
tude of very different systems. Here, we review
those that come closest to ours in respect to the
task and that concern extractive multi-document
query-oriented summarization. We also mention
some work on using textual news data for stock
indices prediction which we are aware of.
Stock market prediction: Wu?thrich et al
(1998) were among the first who introduced an au-
tomatic stock indices prediction system which re-
lies on textual information only. The system gen-
erates weighted rules each of which returns the
probability of the stock going up, down or remain-
ing steady. The only information used in the rules
is the presence or absence of certain keyphrases
provided by a human expert who ?judged them
to be influential factors potentially moving stock
markets?. In this approach, training data is re-
quired to measure the usefulness of the keyphrases
for each of the three classes. More recently, Ler-
man et al (2008) introduced a forecasting system
for prediction markets that combines news anal-
ysis with a price trend analysis model. This ap-
proach was shown to be successful for the fore-
casting of public opinion about political candi-
dates in such prediction markets. Our approach
can be seen as a complement to both these ap-
proaches, necessary especially for financial mar-
kets where the news typically cover many events,
only some related to the company of interest.
Unsupervized summarization systems extract
sentences whose relevance can be inferred from
the inter-sentence relations in the document col-
lection. In (Radev et al, 2000), the centroid of
the collection, i.e., the words with the highest
TF*IDF, is considered and the sentences which
contain more words from the centroid are ex-
tracted. Mihalcea & Tarau (2004) explore sev-
eral methods developed for ranking documents
in information retrieval for the single-document
summarization task. Similarly, Erkan & Radev
(2004) apply in-degree and PageRank to build a
summary from a collection of related documents.
They show that their method, called LexRank,
achieves good results. In (Otterbacher et al, 2005;
Erkan, 2006) the ranking function of LexRank is
extended to become applicable to query-focused
summarization. The rank of a sentence is deter-
mined not just by its relation to other sentences in
252
the document collection but also by its relevance
to the query. Relevance to the query is defined as
the word-based similarity between query and sen-
tence.
Query expansion has been used for improv-
ing information retrieval (IR) or question answer-
ing (QA) systems with mixed results. One of the
problems is that the queries are expanded word
by word, ignoring the context and as a result the
extensions often become inadequate7. However,
Riezler et al (2007) take the entire query into ac-
count when adding new words by utilizing tech-
niques used in statistical machine translation.
Query expansion for summarization has not yet
been explored as extensively as in IR or QA.
Nastase (2008) uses Wikipedia and WordNet for
query expansion and proposes that a concept can
be expanded by adding the text of all hyper-
links from the first paragraph of the Wikipedia
article about this concept. The automatic eval-
uation demonstrates that extracting relevant con-
cepts from Wikipedia leads to better performance
compared with WordNet: both expansion systems
outperform the no-expansion version in terms of
the ROUGE score. Although this method proved
helpful on the DUC data, it seems less appropriate
for expanding company names. For small compa-
nies there are short articles with only a few links;
the first paragraphs of the articles about larger
companies often include interesting rather than
relevant information. For example, the text pre-
ceding the contents box in the article about Apple
Inc. (AAPL) states that ?Fortune magazine named
Apple the most admired company in the United
States?8. The link to the article about the For-
tune magazine can be hardly considered relevant
for the expansion of AAPL. Wikipedia category
information, which has been successfully used in
some NLP tasks (Ponzetto & Strube, 2006, inter
alia), is too general and does not help discriminate
between two companies from the same sector.
Our work suggests that query expansion is
needed for summarization in the financial domain.
In addition to previous work, we also show that an-
other key factor for success in this task is detecting
and modeling the novelty of the target content.
7E.g., see the proceedings of TREC 9, TREC 10: http:
//trec.nist.gov.
8Checked on September 17, 2008.
8 Conclusions
In this paper we presented a multi-document
company-oriented summarization algorithm
which extracts sentences that are both relevant for
the given organization and novel to the user. The
system is expected to be useful in the context of
stock market monitoring and forecasting, that is,
to help the trader predict the move of the stock
price for the given company. We presented a
novel query expansion method which works par-
ticularly well in the context of company-oriented
summarization. Our sentence ranking method is
unsupervized and requires little parameter tuning.
An automatic evaluation against a competitive
baseline showed supportive results, indicating that
the ranking algorithm is able to select relevant
sentences and promote novel information at the
same time.
In the future, we plan to experiment with po-
sitional features which have proven useful for
generic summarization. We also plan to test the
system extrinsically. For example, it would be of
interest to see if a classifier may predict the move
of stock prices based on a set of features extracted
from company-oriented summaries.
Acknowledgments: We would like to thank the
anonymous reviewers for their helpful feedback.
References
Allan, James, Courtney Wade & Alvaro Bolivar
(2003). Retrieval and novelty detection at the
sentence level. In Proceedings of the 26th An-
nual International ACM SIGIR Conference on
Research and Development in Information Re-
trieval Toronto, On., Canada, 28 July ? 1 Au-
gust 2003, pp. 314?321.
Atserias, Jordi, Hugo Zaragoza, Massimiliano
Ciaramita & Giuseppe Attardi (2008). Se-
mantically annotated snapshot of the English
Wikipedia. In Proceedings of the 6th Interna-
tional Conference on Language Resources and
Evaluation, Marrakech, Morocco, 26 May ? 1
June 2008.
Brin, Sergey & Lawrence Page (1998). The
anatomy of a large-scale hypertextual web
search engine. Computer Networks and ISDN
Systems, 30(1?7):107?117.
Ciaramita, Massimiliano & Yasemin Altun
(2006). Broad-coverage sense disambiguation
253
and information extraction with a supersense
sequence tagger. In Proceedings of the 2006
Conference on Empirical Methods in Natural
Language Processing, Sydney, Australia,
22?23 July 2006, pp. 594?602.
Dang, Hoa Trang (2005). Overview of DUC
2005. In Proceedings of the 2005 Document
Understanding Conference held at the Human
Language Technology Conference and Confer-
ence on Empirical Methods in Natural Lan-
guage Processing, Vancouver, B.C., Canada, 9?
10 October 2005.
Erkan, Gu?nes? (2006). Using biased random walks
for focused summarization. In Proceedings
of the 2006 Document Understanding Confer-
ence held at the Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,,
New York, N.Y., 8?9 June 2006.
Erkan, Gu?nes? & Dragomir R. Radev (2004).
LexRank: Graph-based lexical centrality as
salience in text summarization. Journal of Arti-
ficial Intelligence Research, 22:457?479.
Lerman, Kevin, Ari Gilder, Mark Dredze & Fer-
nando Pereira (2008). Reading the markets:
Forecasting public opinion of political candi-
dates by news analysis. In Proceedings of
the 22st International Conference on Computa-
tional Linguistics, Manchester, UK, 18?22 Au-
gust 2008, pp. 473?480.
Lin, Chin-Yew & Eduard H. Hovy (2003). Au-
tomatic evaluation of summaries using N-gram
co-occurrence statistics. In Proceedings of the
Human Language Technology Conference of the
North American Chapter of the Association for
Computational Linguistics, Edmonton, Alberta,
Canada, 27 May ?1 June 2003, pp. 150?157.
Mihalcea, Rada & Paul Tarau (2004). Textrank:
Bringing order into texts. In Proceedings of the
2004 Conference on Empirical Methods in Nat-
ural Language Processing, Barcelona, Spain,
25?26 July 2004, pp. 404?411.
Nastase, Vivi (2008). Topic-driven multi-
document summarization with encyclopedic
knowledge and activation spreading. In Pro-
ceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Hon-
olulu, Hawaii, 25?27 October 2008. To appear.
Nelken, Rani & Stuart M. Shieber (2006). To-
wards robust context-sensitive sentence align-
ment for monolingual corpora. In Proceedings
of the 11th Conference of the European Chapter
of the Association for Computational Linguis-
tics, Trento, Italy, 3?7 April 2006, pp. 161?168.
Otterbacher, Jahna, Gu?nes? Erkan & Dragomir
Radev (2005). Using random walks for
question-focused sentence retrieval. In Pro-
ceedings of the Human Language Technology
Conference and the 2005 Conference on Empir-
ical Methods in Natural Language Processing,
Vancouver, B.C., Canada, 6?8 October 2005,
pp. 915?922.
Ponzetto, Simone Paolo & Michael Strube (2006).
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Pro-
ceedings of the Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,
New York, N.Y., 4?9 June 2006, pp. 192?199.
Radev, Dragomir R., Hongyan Jing & Malgorzata
Budzikowska (2000). Centroid-based summa-
rization of mutliple documents: Sentence ex-
traction, utility-based evaluation, and user stud-
ies. In Proceedings of the Workshop on Au-
tomatic Summarization at ANLP/NAACL 2000,
Seattle, Wash., 30 April 2000, pp. 21?30.
Riezler, Stefan, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal & Yi Liu (2007).
Statistical machine translation for query expan-
sion in answer retrieval. In Proceedings of
the 45th Annual Meeting of the Association for
Computational Linguistics, Prague, Czech Re-
public, 23?30 June 2007, pp. 464?471.
Wu?thrich, B, D. Permunetilleke, S. Leung, V. Cho,
J. Zhang & W. Lam (1998). Daily prediction of
major stock indices from textual WWW data. In
In Proceedings of the 4th International Confer-
ence on Knowledge Discovery and Data Mining
- KDD-98, pp. 364?368.
254
Learning to Rank Answers to Non-Factoid
Questions fromWeb Collections
Mihai Surdeanu?
Stanford University
Massimiliano Ciaramita??
Google Inc.
Hugo Zaragoza?
Yahoo! Research
This work investigates the use of linguistically motivated features to improve search, in par-
ticular for ranking answers to non-factoid questions. We show that it is possible to exploit
existing large collections of question?answer pairs (from online social Question Answering sites)
to extract such features and train ranking models which combine them effectively. We investigate
a wide range of feature types, some exploiting natural language processing such as coarse word
sense disambiguation, named-entity identification, syntactic parsing, and semantic role label-
ing. Our experiments demonstrate that linguistic features, in combination, yield considerable
improvements in accuracy. Depending on the system settings we measure relative improvements
of 14% to 21% in Mean Reciprocal Rank and Precision@1, providing one of the most compelling
evidence to date that complex linguistic features such as word senses and semantic roles can have
a significant impact on large-scale information retrieval tasks.
1. Introduction
The problem of Question Answering (QA) has received considerable attention in the
past few years. Nevertheless, most of the work has focused on the task of factoid
QA, where questions match short answers, usually in the form of named or numerical
entities. Thanks to international evaluations organized by conferences such as the Text
REtrieval Conference (TREC) and the Cross Language Evaluation Forum (CLEF) Work-
shop, annotated corpora of questions and answers have become available for several
languages, which has facilitated the development of robust machine learning models
for the task.1
? Stanford University, 353 Serra Mall, Stanford, CA 94305?9010. E-mail: mihais@stanford.edu.
?? Google Inc., Brandschenkestrasse 110, CH?8002 Zu?rich, Switzerland. E-mail: massi@google.com.
? Yahoo! Research, Avinguda Diagonal 177, 8th Floor, 08018 Barcelona, Spain.
E-mail: hugoz@yahoo-inc.com.
1 TREC: http://trec.nist.gov; CLEF: http://www.clef-campaign.org.
The primary part of this work was carried out while all authors were working at Yahoo! Research.
Submission received: 1 April 2010; revised submission received: 11 September 2010; accepted for publication:
23 November 2010.
? 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 2
Table 1
Sample content from Yahoo! Answers.
High Q: How do you quiet a squeaky door?
Quality A: Spray WD-40 directly onto the hinges of the door. Open and close the door
several times. Remove hinges if the door still squeaks. Remove any rust,
dirt or loose paint. Apply WD-40 to removed hinges. Put the hinges back,
open and close door several times again.
High Q: How does a helicopter fly?
Quality A: A helicopter gets its power from rotors or blades. So as the rotors turn,
air flows more quickly over the tops of the blades than it does below.
This creates enough lift for flight.
Low Q: How to extract html tags from an html documents with c++?
Quality A: very carefully
The situation is different once one moves beyond the task of factoid QA. Com-
paratively little research has focused on QA models for non-factoid questions such
as causation, manner, or reason questions. Because virtually no training data is avail-
able for this problem, most automated systems train either on small hand-annotated
corpora built in-house (Higashinaka and Isozaki 2008) or on question?answer pairs
harvested from Frequently Asked Questions (FAQ) lists or similar resources (Soricut
and Brill 2006; Riezler et al 2007; Agichtein et al 2008). None of these situations is
ideal: The cost of building the training corpus in the former setup is high; in the latter
scenario the data tend to be domain-specific, hence unsuitable for the learning of open-
domain models, and for drawing general conclusions about the underlying scientific
problems.
On the other hand, recent years have seen an explosion of user-generated content
(or social media). Of particular interest in our context are community-driven question-
answering sites, such as Yahoo! Answers, where users answer questions posed by other
users and best answers are selected manually either by the asker or by all the partici-
pants in the thread.2 The data generated by these sites have significant advantages over
other Web resources: (a) they have a high growth rate and they are already abundant;
(b) they cover a large number of topics, hence they offer a better approximation of open-
domain content; and (c) they are available for many languages. Community QA sites,
similar to FAQs, provide a large number of question?answer pairs. Nevertheless, these
data have a significant drawback: they have high variance of quality (i.e., questions
and answers range from very informative to completely irrelevant or even abusive).
Table 1 shows some examples of both high and low quality content from the Yahoo!
Answers site.
In this article we investigate two important aspects of non-factoid QA:
1. Is it possible to learn an answer-ranking model for non-factoid questions, in a
completely automated manner, using data available in on-line social QA sites?
This is an interesting question because a positive answer indicates that a
plethora of training data are readily available to researchers and system
2 http://answers.yahoo.com.
352
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
developers working on natural language processing, information retrieval,
and machine learning.
2. Which features and models are more useful in this context, that is, ample but
noisy data? For example: Are similarity models as effective as models that
learn question-to-answer transformations? Does syntactic and semantic
information help?
Social QA sites are the ideal vehicle to investigate such questions. Questions posted
on these sites typically have a correct answer that is selected manually by users. As-
suming that all the other candidate answers are incorrect (we discuss this assumption
in Section 5), it is trivial to automatically organize these data into a format ready for
discriminative learning, namely, the pair question?correct answer generates one posi-
tive example and all other answers for the same question are used to generate negative
examples. This allows one to use the collection in a completely automated manner to
learn answer ranking models.
The contributions of our investigation are the following:
1. We introduce and evaluate many linguistic features for answer re-ranking.
Although several of these features have been introduced in previous work,
some are novel in the QA context, for example, syntactic dependencies
and semantic role dependencies with words generalized to semantic tags.
Most importantly, to the best of our knowledge this is the first work that
combines all these features into a single framework. This allows us to
investigate their comparative performance in a formal setting.
2. We propose a simple yet powerful representation for complex linguistic
features, that is, we model syntactic and semantic information as bags of
syntactic dependencies or semantic role dependencies and build similarity
and translation models over these representations. To address sparsity,
we incorporate a back-off approach by adding additional models where
lexical elements in these structures are generalized to semantic tags.
These models are not only simple to build, but, as our experiments
indicate, they perform at least as well as complex, dedicated models such
as tree kernels.
3. We are the first to evaluate the impact of such linguistic features in a
large-scale setting that uses real-world noisy data. The impact on QA of
some of the features we propose has been evaluated before, but these
experiments were either on editorialized data enhanced with gold
semantic structures (e.g., the Wall Street Journal corpus with semantic
roles from PropBank [Bilotti et al 2007]), or on very few questions
(e.g., 413 questions from TREC 12 [Cui et al 2005]). On the other hand,
we evaluate on over 25,000 questions, and each question has up to
100 candidate answers from Yahoo! Answers. All our data are processed
with off-the-shelf natural language (NL) processors.
The article is organized as follows. We describe our approach, including all the
features explored for answer modeling, in Section 2. We introduce the corpus used in
our empirical analysis in Section 3. We detail our experiments and analyze the results
353
Computational Linguistics Volume 37, Number 2
in Section 4. Section 5 discusses current shortcomings of our system and proposes
solutions. We overview related work in Section 6 and conclude the article in Section 7.
2. Approach
Figure 1 illustrates our QA architecture. The processing flow is the following. First, the
answer retrieval component extracts a set of candidate answers A for a question Q
from a large collection of answers, C, provided by a community-generated question-
answering site. The retrieval component uses a state-of-the-art information retrieval
(IR) model to extract A given Q. The second component, answer ranking, assigns to
each answer Ai ? A a score that represents the likelihood that Ai is a correct answer
for Q, and ranks all answers in descending order of these scores. In our experiments,
the collection C contains all answers previously selected by users of a social QA site
as best answers for non-factoid questions of a certain type (e.g., ?How to? questions).
The entire collection of questions, Q, is split into a training set and two held-out sets:
a development one used for parameter tuning, and a testing one used for the formal
evaluation.
Our architecture follows closely the architectures proposed in the TREC QA track
(see, e.g., Voorhees 2001). For efficiency reasons, most participating systems split the
answer extraction phase into a retrieval phase that selected likely answer snippets
using shallow techniques, followed by a (usually expensive) answer ranking phase
that processes only the candidates proposed by the retrieval component. Due to this
separation, such architectures can scale to collections of any size. We discuss in Section 6
how related work has improved this architecture further?for example, by adding
query expansion terms from the translation models back to answer retrieval (Riezler
et al 2007).
The focus of this work, however, is on the re-ranking model implemented in the
answer ranking component. We call this model FMIX?from feature mix?because the
proposed scoring function is a linear combination of four different classes of features
Figure 1
Architecture of our QA framework.
354
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
(detailed in Section 2.2). To accommodate and combine all these feature classes, our
QA approach combines three types of machine learning methodologies (as highlighted
in Figure 1): the answer retrieval component uses unsupervised IR models, the an-
swer ranking is implemented using discriminative learning, and finally, some of the
ranking features are produced by question-to-answer translation models, which use
class-conditional generative learning. To our knowledge, this combined approach is
novel in the context of QA. In the remainder of the article, we will use the FMIX
function to answer the research objectives outlined in the Introduction. To answer the
first research objective we will compare the quality of the rankings provided by this
component against the rankings generated by the IRmodel used for answer retrieval. To
answer the second research objective we will analyze the contribution of the proposed
feature set to this function.
We make some simplifying assumptions in this study. First, we will consider only
manner questions, and in particular only ?How to? questions. This makes the corpus
more homogeneous and more focused on truly informational questions (as opposed
to social questions such as ?Why don?t girls like me??, or opinion questions such as
?Who will win the next election??, both of which are very frequent in Yahoo! Answers).
Second, we concentrate on the task of answer-re-ranking, and ignore all other modules
needed in a complete on-line social QA system. For example, we ignore the problem
of matching questions to questions, very useful when retrieving answers in a FAQ or
a QA collection (Jeon, Croft, and Lee 2005), and we ignore all ?social? features such as
the authority of users (Jeon et al 2006; Agichtein et al 2008). Instead, we concentrate on
matching answers and on the different textual features. Hence, the document collection
used in our experiments contains only answers, without the corresponding questions
answered. Furthermore, we concentrate on the re-ranking phase and we do not explore
techniques to improve the recall of the initial retrieval phase (by methods of query
expansion, for example). Such aspects are complementary to our work, and can be
investigated separately.
2.1 Representations of Content
One of our main interests in using very large data sets was to show that complex lin-
guistic features can improve rankingmodels if they are correctly combinedwith simpler
features, in particular using discriminative learning methods on a particular task. For
this reason we explore several forms of textual representation going beyond the bag
of words. In particular, we generate our features over four different representations
of text:
Words (W): This is the traditional IR view where the text is seen as a bag of words.
n-grams (N): The text is represented as a bag of word n-grams, where n ranges from two
up to a given length (we discuss structure parameters in the following).
Dependencies (D): The text is converted to a bag of syntactic dependency chains. We
extract syntactic dependencies in the style of the CoNLL-2007 shared task using the
syntactic processor described in Section 3.3 From the tree of syntactic dependencies we
extract all the paths up to a given length following modifier-to-head links. The top part
3 http://depparse.uvt.nl/depparse-wiki/SharedTaskWebsite.
355
Computational Linguistics Volume 37, Number 2
Figure 2
Sample syntactic dependencies and semantic tags.
Figure 3
Sample semantic proposition.
of Figure 2 shows a sample corpus sentence with the actual syntactic dependencies ex-
tracted by our syntactic processor. The figure indicates that this representation captures
important syntactic relations, such as subject?verb (e.g., helicopter
SBJ??? gets) or object-
verb (e.g., power
OBJ??? gets).
Semantic Roles (R): The text is represented as a bag of predicate?argument rela-
tions extracted using the semantic parser described in Section 3. The parser follows the
PropBank notations (Palmer, Gildea, and Kingsbury 2005), that is, it assigns semantic
argument labels to nodes in a constituent-based syntactic tree. Figure 3 shows an exam-
ple. The figure shows that the semantic proposition corresponding to the predicate gets
includes A helicopter as the Arg0 argument (Arg0 stands for agent), its power as the Arg1
argument (or patient), and from rotors or blades as Arg2 (or instrument). Semantic roles
have the advantage that they extract meaning beyond syntactic representations (e.g., a
syntactic subject may be either an agent or a patient in the actual proposition). We con-
vert the semantic propositions detected by our parser into semantic dependencies using
the same approach as Surdeanu et al (2008), that is, we create a semantic dependency
between each predicate and the syntactic head of every one of its arguments. These
dependencies are labeled with the label of the corresponding argument. For example,
the semantic dependency that includes the Arg0 argument in Figure 3 is represented as
gets
Arg0
??? helicopter. If the syntactic constituent corresponding to a semantic argument is
356
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
a prepositional phrase (PP), we convert it to a bigram that includes the preposition and
the head word of the attached phrase. For example, the tuple for Arg2 in the example is
represented as gets
Arg2
??? from-rotors.
In all representations we remove structures where either one of the elements is a
stop word and convert the remaining words to their WordNet lemmas.4
The structures we propose are highly configurable. In this research, we investigate
this issue along three dimensions:
Degree of lexicalization:We reduce the sparsity of the proposed structures by replacing
the lexical elements with semantic tags which might provide better generalization. In
this article we use two sets of tags, the first consisting of coarse WordNet senses, or
supersenses (WNSS) (Ciaramita and Johnson 2003), and the second of named-entity
labels extracted from the Wall Street Journal corpus. We present in detail the tag sets
and the processors used to extract them in Section 3. For an overview, we show a sample
annotated sentence in the bottom part of Figure 2.
Labels of relations: Both dependency and predicate?argument relations can be labeled
or unlabeled (e.g., gets
Arg0
??? helicopter versus gets? helicopter). We make this distinction
in our experiments for two reasons: (a) removing relation labels reduces the model
sparsity because fewer elements are created, and (b) performing relation recognition
without classification is simpler than performing the two tasks, so the corresponding
NL processors might be more robust in the unlabeled-relation setup.
Structure size: This parameter controls the size of the generated structures, namely,
number of words in n-grams or dependency chains, or number of elements in the
predicate?argument tuples. Nevertheless, in our experiments we did not see any im-
provements from structure sizes larger than two. In the experiments reported in this
article, all the structures considered are of size two, that is, we use bigrams, dependency
chains of two elements, and tuples of one predicate and one semantic argument.
2.2 Features
We explore a rich set of features inspired by several state-of-the-art QA systems
(Harabagiu et al 2000; Magnini et al 2002; Cui et al 2005; Soricut and Brill 2006; Bilotti
et al 2007; Ko, Mitamura, and Nyberg 2007). To the best of our knowledge this is the
first work that: (a) adapts all these features for non-factoid answer ranking, (b) combines
them in a single scoring model, and (c) performs an empirical evaluation of the different
feature families and their combinations.
For clarity, we group the features into four sets: features that model the similarity
between questions and answers (FG1), features that encode question-to-answer trans-
formations using a translation model (FG2), features that measure keyword density and
frequency (FG3), and features that measure the correlation between question?answer
pairs and other collections (FG4). Wherever applicable, we explore different syntactic
and semantic representations of the textual content, as introduced previously. We next
explain in detail each of these feature groups.
4 http://wordnet.princeton.edu.
357
Computational Linguistics Volume 37, Number 2
FG1: Similarity Features. We measure the similarity between a question Q and an
answer A using the length-normalized BM25 formula (Robertson and Walker 1997),
which computes the score of the answer A as follows:
BM25(A) =
|Q|
?
i=0
(k1 + 1)tf
A
i (k3 + 1)tf
Q
i
(K+ tf Ai )(k3 + tf
Q
i )
log(idfi) (1)
where tf Ai and tf
Q
i are the frequencies of the question term i in A and Q, and idfi is
the inverse document frequency of term i in the answer collection. K is the length-
normalization factor:
K = k1((1? b)+ b|A|/avg len)
where avg len is the average answer length in the collection. For all the constants in the
formula (b, k1, and k3) we use values reported optimal for other IR collections (b = 0.75,
k1 = 1.2, and k3 = 1, 000).
We chose this similarity formula because, of all the IR models we tried, it provided
the best ranking at the output of the answer retrieval component. For completeness
we also include in the feature set the value of the tf ? idf similarity measure. For both
formulas we use the implementations available in the Terrier IR platform with the
default parameters.5
To understand the contribution of our syntactic and semantic processors we com-
pute the similarity features for different representations of the question and answer
content, ranging from bag of words to semantic roles. We detail these representations in
Section 2.1.
FG2: Translation Features. Berger et al (2000) showed that similarity-based models
are doomed to perform poorly for QA because they fail to ?bridge the lexical chasm?
between questions and answers. One way to address this problem is to learn question-
to-answer transformations using a translation model (Berger et al 2000; Echihabi and
Marcu 2003; Soricut and Brill 2006; Riezler et al 2007). In our model, we incorporate this
approach by adding the probability that the question Q is a translation of the answer
A, P(Q|A), as a feature. This probability is computed using IBM?s Model 1 (Brown et al
1993):
P(Q|A) =
?
q?Q
P(q|A) (2)
P(q|A) = (1? ?)Pml(q|A)+ ?Pml(q|C) (3)
Pml(q|A) =
?
a?A
(T(q|a)Pml(a|A)) (4)
where the probability that the question term q is generated from answer A, P(q|A),
is smoothed using the prior probability that the term q is generated from the entire
collection of answers C, Pml(q|C). ? is the smoothing parameter. Pml(q|C) is computed
5 http://ir.dcs.gla.ac.uk/terrier.
358
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
using the maximum likelihood estimator. To mitigate sparsity, we set Pml(q|C) to a small
value for out-of-vocabulary words.6 Pml(q|A) is computed as the sum of the probabilities
that the question term q is a translation of an answer term a, T(q|a), weighted by the
probability that a is generated fromA. The translation table for T(q|a) is computed using
the EM algorithm implemented in the GIZA++ toolkit.7
Translation models have one important limitation when used for retrieval tasks:
They do not guarantee that the probability of translating a word to itself, that is, T(w|w),
is high (Murdock and Croft 2005). This is a problem for QA, where word overlap
between question and answer is a good indicator of relevance (Moldovan et al 1999).
We address this limitation with a simple algorithm: we set T(w|w) = 0.5 and re-scale
the other T(w?|w) probabilities for all other words w? in the vocabulary to sum to 0.5, to
guarantee that
?
w? T(w
?|w) = 1. This has the desired effect that T(w|w) becomes larger
than any other T(w?|w). Our initial experiments proved empirically that this is essential
for good performance.
As prior work indicates, tuning the smoothing parameter ? is also crucial for the
performance of translation models, especially in the context of QA (Xue, Jeon, and
Croft 2008). We tuned the ? parameter independently for each of the translation models
introduced as follows: (a) for a smaller subset of the development corpus introduced
in Section 3 (1,500 questions) we retrieved candidate answers using our best retrieval
model (BM25); (b) we implemented a simple re-ranking model using as the only feature
the translation model probability; and (c) we explored a large range of values for ?
and selected the one that maximizes the mean reciprocal rank (MRR) of the re-ranking
model. This process selected a wide range of values for the ? parameter for the different
translation models (e.g., 0.09 for the translation model over labeled syntactic depen-
dencies, and 0.43 for the translation model over labeled semantic role dependencies).
Similarly to the previous feature group, we add translation-based features for the
different text representations detailed in Section 2.1. By moving beyond the bag-of-
words representation we hope to learn relevant transformations of structures, for ex-
ample, from the squeaky? door dependency to spray?WD-40 in the Table 1 example.
FG3: Density and Frequency Features. These features measure the density and fre-
quency of question terms in the answer text. Variants of these features were used
previously for either answer or passage ranking in factoid QA (Moldovan et al 1999;
Harabagiu et al 2000). Tao and Zhai (2007) evaluate a series of proximity-based mea-
sures in the context of information retrieval.
Same word sequence: Computes the number of non-stop question words that are
recognized in the same order in the answer.
Answer span: The largest distance (in words) between two non-stop question words in
the answer. We compute multiple variants of this feature, where we count: (a) the total
number of non-stop words in the span, or (b) the number of non-stop nouns.
Informativeness: Number of non-stop nouns, verbs, and adjectives in the answer text
that do not appear in the question.
6 We used 1E-9 for the experiments in this article.
7 http://www.fjoch.com/GIZA++.html.
359
Computational Linguistics Volume 37, Number 2
Same sentencematch:Number of non-stop question termsmatched in a single sentence
in the answer. This feature is added both unnormalized and normalized by the question
length.
Overall match: Number of non-stop question terms matched in the complete answer.
All these features are computed as raw counts and as normalized counts (dividing
the count by the question length, or by the answer length in the case of Answer span).
The last two features (Same sentence match and Overall match) are computed for all
text representations introduced, including syntactic and semantic dependencies (see
Section 2.1).
Note that counting the number of matched syntactic dependencies is essentially
a simplified tree kernel for QA (e.g., see Moschitti et al 2007) matching only trees of
depth 2. We also include in this feature group the following tree-kernel features.
Tree kernels: Tomodel larger syntactic structures that are shared between questions and
answers we compute the tree kernel values between all question and answer sentences.
We implemented a dependency-tree kernel based on the convolution kernels proposed
by Collins and Duffy (2001). We add as features the largest value measured between
any two individual sentences, as well as the average of all computed kernel values for
a given question and answer. We compute tree kernels for both labeled and unlabeled
dependencies, and for both lexicalized trees and for trees where words are generalized
to their predicted WNSS or named-entity tags (when available).
FG4: Web Correlation Features. Previous work has shown that the redundancy of a
large collection (e.g., the Web) can be used for answer validation (Brill et al 2001;
Magnini et al 2002). In the same spirit, we add features that measure the correlation
between question?answer pairs and large external collections:
Web correlation:Wemeasure the correlation between the question?answer pair and the
Web using the Corrected Conditional Probability (CCP) formula of Magnini et al (2002):
CCP(Q,A) = hits(Q+ A)/(hits(Q) hits(A)2/3) (5)
where hits returns the number of page hits from a search engine. The hits procedure
constructs a Boolean query from the given set of terms, represented as a conjunction of
all the corresponding keywords. For example, for the second question in Table 1, hits(Q)
uses the Boolean query: helicopter AND fly.
It is notable that this formula is designed for Web-based QA, that is, the conditional
probability is adjusted with 1/hits(A)2/3 to reduce the number of cases when snippets
containing high-frequency words are marked as relevant answers. This formula was
shown to perform best for the task of QA (Magnini et al 2002). Nevertheless, this
formula was designed for factoid QA, where both the question and the exact answer
have a small number of terms. This is no longer true for non-factoid QA. In this context
it is likely that the number of hits returned forQ, A, orQ+ A is zero given the large size
of the typical question and answer. To address this issue, wemodified the hits procedure
to include a simple iterative query relaxation algorithm:
1. Assign keyword priorities using a set of heuristics inspired by
Moldovan et al (1999). The complete priority detection algorithm
is listed in Table 2.
360
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
Table 2
Keyword priority heuristics.
Step Keyword type Priority
(a) Non-stop keywords within quotes 8
(b) Non-stop keywords tagged as proper nouns 7
(c) Contiguous sequences of 2+ adjectives as nouns 6
(d) Contiguous sequences of 2+ nouns 5
(e) Adjectives not assigned in step (c) 4
(f) Nouns not assigned in steps (c) or (d) 3
(g) Verbs and adverbs 2
(h) Non-stop keywords not assigned in the previous steps 1
2. Fetch the number of page hits using the current query.
3. If the number of hits is larger than zero, stop; otherwise discard the set of
keywords with the smallest priority in the current query and repeat from
step 2.
Query-log correlation: As in Ciaramita, Murdock, and Plachouras (2008), we also com-
pute the correlation between question?answer pairs from a search-engine query-log
corpus of more than 7.5 million queries, which shares roughly the same time stamp
with the community-generated question?answer corpus. Using the query-log correla-
tion between two snippets of text was shown to improve performance for contextual
advertising, that is, linking a user?s query to the description of an ad (Ciaramita,
Murdock, and Plachouras 2008). In this work, we adapt this idea to the task of QA.
However, because it is not clear which correlation metric performs best in this context,
we compute both the Pointwise Mutual Information (PMI) and chi square (?2) associ-
ation measures between each question?answer word pair in the query-log corpus. The
largest and the average values are included as features, as well as the number of QA
word pairs which appear in the top 10, 5, and 1 percentile of the PMI and ?2 word pair
rankings.
We replicate all features that can be computed for different content representations
using every independent representation and parameter combination introduced in
Section 2.1. For example, we compute similarity scores (FG1) for 16 different repre-
sentations of question/answer content, produced by different parametrizations of the
four different generic representations (W, N, D, R). One important exception to this
strategy are the translation-model features (FG2). Because our translation models aim
to learn both lexical and structural transformations between questions and answers,
it is important to allow structural variations in the question/answer representations.
In this article, we implement a simple and robust approximation for this purpose: For
translation models we concatenate all instances of structured representations (N, D, R)
with the corresponding bag-of-words representation (W). This allows the translation
models to learn some combined lexical and structural transformation (e.g., from the
dependency squeaky? door dependency to the tokenWD-40). All in all, replicating our
features for all the different content representations yields 137 actual features to be used
for learning.
361
Computational Linguistics Volume 37, Number 2
2.3 Ranking Models
Our approach is agnostic with respect to the actual learning model. To emphasize this,
we experimented with two learning algorithms. First, we implemented a variant of the
ranking Perceptron proposed by Shen and Joshi (2005). In this framework the ranking
problem is reduced to a binary classification problem. The general idea is to exploit the
pairwise preferences induced from the data by training on pairs of patterns, rather than
independently on each pattern. Given a weight vector ?, the score for a pattern x (a
candidate answer) is given by the inner product between the pattern and the weight
vector:
f?(x) = ?x,?? (6)
However, the error function depends on pairwise scores. In training, for each pair
(xi, xj) ? A, the score f?(xi ? xj) is computed; note that if f is an inner product f?(xi ?
xj) = f?(xi)? f?(xj). In this framework one can define suitable margin functions that
take into account different levels of relevance; for example, Shen and Joshi (2005)
propose g(i, j) = ( 1i ?
1
j ), where i and j are the rank positions of xi and xj. Because in
our case there are only two relevance levels we use a simpler sign function yi,j, which
is negative if i > j and positive otherwise; yi,j is then scaled by a positive rate ? found
empirically on the development data. In the presence of numbers of possible rank levels
appropriate margin functions can be defined. During training, if f?(xi ? xj) ? yi,j?, an
update is performed as follows:
?t+1 = ?t + (xi ? xj)yi,j? (7)
We notice, in passing, that variants of the perceptron including margins have been
investigated before; for example, in the context of uneven class distributions (see Li et al
2002). It is interesting to notice that such variants have been found to be competitive
with SVMs in terms of performance, while being more efficient (Li et al 2002; Surdeanu
and Ciaramita 2007). The comparative evaluation from our experiments are consistent
with these findings. For regularization purposes, we use as a final model the average of
all Perceptron models posited during training (Freund and Schapire 1999).
We also experimented with SVM-rank (Joachims 2006), which is an instance of
structural SVM?a family of Support Vector Machine algorithms that model structured
outputs (Tsochantaridis et al 2004)?specifically tailored for ranking problems.8 SVM-
rank optimizes the area under a ROC curve. The ROC curve is determined by the true
positive rate vs. the false positive rate for varying values of the prediction threshold,
thus providing a metric closely related to Mean Average Precision (MAP).
3. The Corpus
The corpus is extracted from a sample of the U.S. Yahoo! Answers questions and
answers. We focus on the subset of advice or ?how to? questions due to their fre-
quency, quality, and importance in social communities. Nevertheless, our approach
8 http://www.cs.cornell.edu/People/tj/svm light/svm rank.html.
362
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
is independent of the question type. To construct our corpus, we implemented the
following successive filtering steps:
Step 1: From the full corpus we keep only questions that match the regular
expression:
how (to|do|did|does|can|would|could|should)
and have an answer selected as best either by the asker or by the
participants in the thread. The outcome of this step is a set of
364,419 question?answer pairs.
Step 2: From this corpus we remove the questions and answers of dubious
quality. We implement this filter with a simple heuristic by keeping
only questions and answers that have at least four words each, out
of which at least one is a noun and at least one is a verb. The
rationale for this step is that genuine answers to ?how to? questions
should have a minimal amount of structure, approximated by the
heuristic. This step filters out questions like How to be excellent? and
answers such as I don?t know. The outcome of this step forms our
answer collection C. C contains 142,627 question?answer pairs.
This corpus is freely available through the Yahoo! Webscope
program.9
Arguably, all these filters could be improved. For example, the first step can be
replaced by a question classifier (Li and Roth 2006). Similarly, the second step can be
implemented with a statistical classifier that ranks the quality of the content using
both the textual and non-textual information available in the database (Jeon et al 2006;
Agichtein et al 2008). We plan to further investigate these issues, which are not the
main object of this work.
The data was processed as follows. The text was split at the sentence level, token-
ized and POS tagged, in the style of the Wall Street Journal Penn TreeBank (Marcus,
Santorini, and Marcinkiewicz 1993). Each word was morphologically simplified using
the morphological functions of the WordNet library. Sentences were annotated with
WNSS categories, using the tagger of Ciaramita and Altun (2006), which annotates
text with a 46-label tagset.10 These tags, defined by WordNet lexicographers, provide
a broad semantic categorization for nouns and verbs and include labels for nouns such
as food, animal, body, and feeling, and for verbs labels such as communication, contact,
and possession. We chose to annotate the data with this tagset because it is less biased
towards a specific domain or set of semantic categories than, for example, a named-
entity tagger. Using the same tagger as before we also annotated the text with a named-
entity tagger trained on the BBNWall Street Journal (WSJ) Entity Corpus which defines
105 categories for entities, nominal concepts, and numerical types.11 See Figure 2 for a
sample sentence annotated with these tags.
Next, we parsed all sentences with the dependency parser of Attardi et al (2007).12
We chose this parser because it is fast and it performed very well in the domain adap-
tation shared task of CoNLL 2007. Finally, we extracted semantic propositions using
9 You can request the corpus by email at research-data-requests@yahoo-inc.com. More information
about this corpus can be found at: http://www.yr-bcn.es/MannerYahooAnswers.
10 http://sourceforge.net/projects/supersensetag.
11 LDC catalog number LDC2005T33.
12 http://sourceforge.net/projects/desr.
363
Computational Linguistics Volume 37, Number 2
the SwiRL semantic parser of Surdeanu et al (2007).13 SwiRL starts by syntactically
analyzing the text using a constituent-based full parser (Charniak 2000) followed by a
semantic layer, which extracts PropBank-style semantic roles for all verbal predicates in
each sentence.
It is important to realize that the output of all mentioned processing steps is noisy
and contains plenty of mistakes, because the data have huge variability in terms of
quality, style, genres, domains, and so forth. In terms of processing speed, both the
semantic tagger of Ciaramita and Altun and the Attardi et al parser process 100+
sentences/second. The SwiRL system is significantly slower: On average, it parses less
than two sentences per second. However, recent research showed that this latter task
can be significantly sped up without loss of accuracy (Ciaramita et al 2008).
We used 60% of the questions for training, 20% for development, and 20% for test-
ing. Our ranking model was tuned strictly on the development set for feature selection
(described later) and the ? parameter of the translation models. The candidate answer
set for a given question is composed of one positive example, that is, its corresponding
best answer, and as negative examples all the other answers retrieved in the top N by
the retrieval component.
4. Experiments
We used several measures to evaluate our models. Recall that we are using an initial
retrieval engine to select a pool of N answer candidates (Figure 1), which are then re-
ranked. This couples the performance of the initial retrieval engine and the re-rankers.
We tried to de-couple them in our performance measures, as follows. We note that if
the initial retrieval engine does not rank the correct answer in the pool of top N results,
it is impossible for any re-ranker to do well. We therefore follow the approach of Ko
et al (2007) and define performance measures only with respect to the subset of pools
which contain the correct answer for a given N.
This complicates slightly the typical notions of recall and precision. Let us callQ the
set of all queries in the collection and QN the subset of queries for which the retrieved
answer pool of size N contains the correct answer. We will then use the following
performance measure definitions:
Retrieval Recall@N: The usual recall definition:
|QN|
|Q| . This is equal for all re-rankers.
Re-ranking Precision@1: Average Precision@1 over the QN set, where the Precision@1
of a query is defined as 1 if the correct answer is re-ranked into the first position,
0 otherwise.
Re-ranking MRR: MRR over theQN set, where the reciprocal rank is the inverse of the
rank of the correct answer.
Note that as N gets larger, QN grows in size, increasing the Retrieval Recall@N but
also increasing the difficulty of the task for the re-ranker, and therefore decreasing Re-
ranking Precision@1 and Re-ranking MRR.
During training of the FMIX re-ranker, the presentation of the training instances is
randomized, which defines a randomized training protocol producing different models
with each permutation of the data. We exploit this property to estimate the variance on
the experimental results by reporting the average performance of 10 different models,
together with an estimate of the standard deviation.
13 http://swirl-parser.sourceforge.net.
364
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
Table 3
Re-ranking evaluation for Perceptron and SVM-rank. Improvement indicates relative
improvement over the baseline.
N = 15 N = 25 N = 50 N = 100
Retrieval Recall@N 29.04% 32.81% 38.09% 43.42%
Re-ranking Precision@1
Baseline 41.48 36.74 31.66 27.75
FMIX (Perceptron) 49.87?0.03 44.48?0.03 38.53?0.11 33.72?0.05
FMIX (SVM-rank) 49.48 44.10 38.18 33.52
Improvement (Perceptron) +20.22% +21.06% +21.69% +21.51%
Improvement (SVM-rank) +19.28% +20.03% +20.59% +20.79%
Re-ranking MRR
Baseline 56.12 50.31 43.74 38.53
FMIX (Perceptron) 64.16?0.01 58.20?0.01 51.19?0.07 45.29?0.05
FMIX (SVM-rank) 63.81 57.89 50.93 45.12
Improvement (Perceptron) +14.32% +15.68% +17.03% +17.54%
Improvement (SVM-rank) +13.70% +15.06% +16.43% +17.10%
The initial retrieval engine used to select the pool of candidate answers is the BM25
score as described earlier. This is also our baseline re-ranker. We will compare this to the
FMIX re-ranker using all features or using subsets of features.
4.1 Overall Results
Table 3 and Figure 4 show the results obtained using FMIX and the baseline for in-
creasing values of N. We report results for Perceptron and SVM-rank using the optimal
feature set for each (we discuss feature selection in the next sub-section).
Looking at the first column in Table 3 we see that a good bag-of-words representa-
tion alone (BM25 in this case) can achieve 41.5% Precision@1 (for the 29.0% of queries for
Figure 4
Re-ranking evaluation; precision-recall curve.
365
Computational Linguistics Volume 37, Number 2
which the retrieval engine can find an answer in the top N = 15 results). These baseline
results are interesting because they indicate that the problem is not hopelessly hard, but
it is far from trivial. In principle, we see much room for improvement over bag-of-words
methods. Indeed, the FMIX re-ranker greatly improves over the baseline. For example,
the FMIX approach using Perceptron yields a Precision@1 of 49.9%, a 20.2% relative
increase.
SettingN to a higher valuewe see recall increase at the expense of precision. Because
recall depends only on the retrieval engine and not on the re-ranker, what we are
interested in is the relative performance of our re-rankers for increasing numbers of
N. For example, setting N = 100 we observe that the BM25 re-ranker baseline obtains
27.7% Precision@1 (for the 43.4% of queries for which the best answer is found in the
top N = 100). For this same subset, the FMIX re-ranker using Perceptron obtains 33.7%
Precision@1, a 21.5% relative improvement over the baseline model.
The FMIX system yields a consistent and significant improvement for all values
of N, regardless of the type of learning algorithm used. As expected, as N grows the
precision of both re-rankers decreases, but the relative improvement holds or increases.
This can be seen most clearly in Figure 4 where re-ranking Precision and MRR are
plotted against retrieval Recall. Recalling that the FMIX model was trained only once,
using pools of N = 15, we can note that the training framework is stable at increasing
sizes of N.
Table 3 and Figure 4 show that the two FMIX variants (Perceptron and SVM-rank)
yield scores that are close (e.g., Precision@1 scores are within 0.5% of each other). We
hypothesize that the small difference between the two different learning models is
caused by our greedy tuning procedures (described in the next section), which converge
to slightly different solutions due to the different learning algorithms. Most importantly,
the fact that we obtain analogous results with two different learningmodels underscores
the robustness of our approach and of our feature set.
These overall results provide strong evidence that: (a) readily available and scalable
NLP technology can be used to improve lexical matching and translation models for
retrieval and QA tasks, (b) we can use publicly available online QA collections to
investigate features for answer ranking without the need for costly human evaluation,
and (c) we can exploit large and noisy on-line QA collections to improve the accuracy of
answer ranking systems. In the remainder of this section we analyze the performance
of the different features.
4.2 Contribution of Feature Groups
In order to gain some insights about the effectiveness of the different features groups,
we carried out a greedy feature selection procedure. We implemented similar processes
for Perceptron and SVM-rank, to guarantee that our conclusions are not biased by a
particular learning model.
4.2.1 Perceptron. We initialized the feature selection process with a single feature that
replicates the baseline model (BM25 applied to the bag-of-words [W] representation).
Then the algorithm incrementally adds to the feature set the single feature that provides
the highest MRR improvement in the development partition. The process stops when
no features yield any improvement. Note that this is only a heuristic process, and needs
to be interpreted with care. For example, if two features were extremely correlated, the
algorithm would choose one at random and discard the other. Therefore, if a feature is
366
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
missing from the selection process it means that it is either useless, or strongly correlated
with other features in the list.
Table 4 summarizes the outcome of this feature selection process. Where applicable,
we show within parentheses the text representation for the corresponding feature: W
for words, N for n-grams, D for syntactic dependencies, and R for semantic roles. We
use subscripts to indicate if the corresponding representation is fully lexicalized (no
subscript), or its elements are replaced by WordNet supersenses (WNSS) or named-
entity tags (WSJ). Where applicable, we use the l superscript to indicate if the cor-
responding structures are labeled. No superscript indicates unlabeled structures. For
example, DWNSS stands for unlabeled syntactic dependencies where the participating
tokens are replaced by their WordNet supersense; RlWSJ stands for semantic tuples of
predicates and labeled arguments with the words replaced with the corresponding WSJ
named-entity tags.
The table shows that, although the features selected span all the four feature groups
introduced, the lion?s share is taken by the translation features (FG2): 75% of the MRR
improvement is achieved by these features. The frequency/density features (FG3) are
responsible for approximately 16% of the improvement. The rest is due to the query-log
correlation features (FG4). This indicates that, even though translation models are the
most useful, it is worth exploring approaches that combine several strategies for answer
ranking.
As we noted before, many features may be missing from this list simply because
they are strongly correlated with others. For example most similarity features (FG1) are
correlated with BM25(W); for this reason the selection process does not choose a FG1
feature until iteration 9. On the other hand, some features do not provide a useful signal
Table 4
Summary of the model selection process using Perceptron.
Iteration Feature Set Group MRR P@1 (%)
0 BM25(W) FG1 56.09 41.14
1 + translation(R) FG2 61.18 46.33
2 + translation(N) FG2 62.49 47.97
3 + overall match(DWNSS) FG3 63.07 48.93
4 + translation(W) FG2 63.27 49.12
5 + query-log avg(PMI) FG4 63.57 49.56
6 + overall match(W) FG3 63.72 49.74
7 + overall match(W), normalized by Q size FG3 63.82 49.89
8 + same word sequence, normalized by Q size FG3 63.90 49.94
9 + BM25(N) FG1 63.98 50.00
10 + informativeness: verb count FG3 64.16 49.97
11 + query-log max(PMI) FG4 64.37 50.28
12 + same sentence match(W) FG3 64.42 50.40
13 + overall match(NWSJ) FG3 64.49 50.51
14 + query-log max(?2) FG4 64.56 50.59
15 + same word sequence FG3 64.66 50.72
16 + BM25(RWSJ) FG1 64.68 50.78
17 + translation(RlWSJ) FG2 64.71 50.75
18 + answer span, normalized by A size FG3 64.76 50.80
19 + query-log top10(?2) FG4 64.89 51.06
20 + tree kernel(DWSJ) FG3 64.93 51.07
21 + translation(RWNSS) FG2 64.95 51.16
367
Computational Linguistics Volume 37, Number 2
at all. A notable example in this class is theWeb-based CCP feature, which was designed
originally for factoid answer validation and does not adapt well to our problem. To test
this, we learned a model with BM25 and the Web-based CCP feature only, and this
model did not improve over the baseline model at all. We hypothesize that because the
length of non-factoid answers is typically significantly larger than in the factoid QA
task, we have to discard a large part of the query when computing hits(Q+ A) to reach
non-zero counts. This means that the final hit counts, hence the CCP value, are generally
uncorrelated with the original (Q,A) tuple.
One interesting observation is that two out of the first three features chosen by
our model selection process use information from the NLP processors. The first feature
selected is the translation probability computed between the R representation (unla-
beled semantic roles) of the question and the answer. This feature alone accounts for
57% of the measured MRR improvement. This is noteworthy: Semantic roles have been
shown to improve factoid QA, but to the best of our knowledge this is the first result
demonstrating that semantic roles can improve ad hoc retrieval (on a large set of non-
factoid open-domain questions). We also find noteworthy that the third feature chosen
measures the number of unlabeled syntactic dependencies with words replaced by their
WNSS labels that are matched in the answer. Overall, the features that use the output of
NL processors account for 68% of the improvement produced by our model over the IR
baseline. These results provide empirical evidence that natural language analysis (e.g.,
coarse word sense disambiguation, syntactic parsing, and semantic role labeling) has a
positive contribution to non-factoid QA, even in broad-coverage noisy settings based
on Web data. To our knowledge, this had not been shown before.
Finally, we note that tree kernels provide minimal improvement: A tree kernel
feature is selected only in iteration 20 and the MRR improvement is only 0.04 points.
One conjecture is that, due to the sparsity and the noise of the data, matching trees of
depth higher than 2 is highly uncommon. Hence matching immediate dependencies
is a valid approximation of kernels in this setup. Another possible explanation is that
because the syntactic trees produced by the parser contain several mistakes, the tree
kernel, which considers matches between an exponential number of candidate sub-
trees, might be particularly unreliable on noisy data.
4.2.2 SVM-rank. For SVM-rank we employed a tuning procedure similar to the one used
for the Perceptron that implements both feature selection and tuning of the regularizer
parameter C. We started with the baseline feature alone and greedily added one feature
at a time. In each iteration we added the feature that provided the best improvement.
The procedure continues to evaluate all available features, until no improvement is
observed. For this step we set the regularizer parameter to 1.0, a value which provided
a good tradeoff between accuracy and speed as evaluated in an initial experiment.
The selection procedure generated 12 additional features. At this point, using only the
selected features, we fine-tuned the regularization parameter C across a wide spectrum
of possible values. This can be useful because in SVM-rank the interpretation of C is
slightly different than in standard SVM, specifically Csvm = Crank/m, where m is the
number of queries, or questions in our case. Therefore, an optimal value can depend
crucially on the target data. The final value selected by this search procedure was equal
to 290, although performance is relatively stable with values between 1 and 100,000. As
a final optimization step, we continued the feature selection routine, starting from the
13 features already chosen and C = 290. This last step selected six additional features.
A further attempt at fine-tuning the C parameter did not provide any improvements.
368
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
This process is summarized in Table 5, using the same notations as Table 4. Al-
though the features selected by SVM-rank are slightly different than the ones chosen
by the Perceptron, the conclusions drawn are the same as before: Features generated
by NL processors provide a significant boost on top of the IR model. Similarly to the
Perceptron, the first feature chosen by the selection procedure is a translation probabil-
ity computed over semantic role dependencies (labeled, unlike the Perceptron, which
prefers unlabeled dependencies). This feature alone accounts for 33.3% of the measured
MRR improvement. This further enforces our observation that semantic roles improve
retrieval performance for complex tasks such as our non-factoid QA exercise. All in all,
13 out of the 18 selected features, responsible for 70% of the total MRR improvement,
use information from the NL processors.
4.3 Contribution of Natural Language Structures
One of the conclusions of the previous analysis is that features based on natural lan-
guage processing are important for the problem of QA. This observation deserves a
more detailed analysis. Table 6 shows the performance of our first three feature groups
when they are applied to each of the content representations and incremental combina-
tions of representations. In this table, for simplicity we merge features from labeled
and unlabeled representations. For example, R indicates that features are extracted
from both labeled (Rl) and unlabeled (R) semantic role representations. The g subscript
indicates that the lexical terms in the corresponding representation are separately gener-
alized toWNSS andWSJ labels. For example,Dgmerges features generated fromDWNSS,
Table 5
Summary of the model selection process using SVM-rank.
Iteration Feature Set Group MRR P@1 (%)
0 BM25(W) FG1 56.09 41.12
1 + translation(Rl) FG2 59.02 43.99
2 + answer span FG3 60.31 45.05
3 + translation(W) FG2 61.16 46.13
4 + translation(R) FG2 61.65 46.77
5 + overall match(D) FG3 62.85 48.57
6 + translation(RlWSJ) FG2 63.05 48.78
7 + translation(NWSJ) FG2 63.23 48.88
8 + translation(DlWSJ) FG2 63.47 49.21
9 + query-log max(?2) FG4 63.64 49.35
10 + translation(D) FG2 63.77 49.53
11 + translation(N) FG2 63.85 49.66
12 + overall match(NWSJ) FG3 64.03 49.93
+ C fine tuning 64.49 50.43
13 + BM25(DWSJ) FG1 64.49 50.43
14 + BM25(N) FG1 64.74 50.71
15 + tf ? idf(DWNSS) FG1 64.74 50.60
16 + answer span in nouns FG3 64.74 50.60
17 + tf ? idf(Rl) FG1 64.84 50.89
18 + translation(Dl) FG2 64.88 50.91
369
Computational Linguistics Volume 37, Number 2
Table 6
Contribution of natural language structures in each feature group. Scores are MRR changes of
the Perceptron on the development set over the baseline model (FG1 with W), for N = 15. The
best scores for each feature group (i.e., column in the table), are marked in bold.
FG1 FG2 FG3
W ? +4.18 ?6.80
N ?13.97 +2.49 ?13.63
Ng ?18.65 +3.63 ?15.57
D ?15.15 +1.48 ?15.39
Dg ?19.31 +3.41 ?18.18
R ?27.61 +0.33 ?27.82
Rg ?28.29 +3.46 ?26.74
W +N +1.46 +5.20 ?4.36
W +N +Ng +1.51 +5.33 ?4.31
W +N +Ng +D +1.56 +5.78 ?4.31
W +N +Ng +D +Dg +1.56 +5.85 ?4.21
W +N +Ng +D +Dg + R +1.58 +6.12 ?4.28
W +N +Ng +D +Dg + R + Rg +1.65 +6.29 ?4.28
DWSJ, D
l
WNSS, and D
l
WSJ. For each cell in the table, we use only the features from the
corresponding feature group and representation to avoid the correlation with features
from other groups. We generate each best model using the same feature selection
process described above.
The top part of the table indicates that all individual representations perform worse
than the bag-of-words representation (W) in every feature group. The differences range
from less than one MRR point (e.g., FG2[Rg] versus FG2[W]), to over 28 MRR points
(e.g., FG1[Rg] versus FG1[W]). Such a large difference is justified by the fact that for
feature groups FG1 and FG3 we compute feature values using only the corresponding
structures (e.g., only semantic roles), which could be very sparse. For example, there
are questions in our corpus where our SRL system does not detect any semantic propo-
sition. Because translation models merge all structured representations with the bag-
of-word representation, they do not suffer from this sparsity problem. Furthermore, on
their own, FG3 features are significantly less powerful than FG1 or FG2 features. This
explains why models using FG3 features fail to improve over the baseline. Regardless
of these differences, the analysis indicates that in our noisy setting the bag-of-words
representation outperforms any individual structured representation.
However, the bottom part of the table tells a more interesting story: The second
part of our analysis indicates that structured representations provide complementary
information to the bag-of-words representation. Even the combination of bag of words
with the simplest n-gram structures (W + N) always outperforms the bag-of-words
representation alone. But the best results are always obtained when the combination
includes more natural language structures. The improvements are relatively small, but
remarkable (e.g., see FG2) if we take into account the significant scale and settings of the
evaluation. The improvements yielded by natural language structures are statistically
significant for all feature groups. This observation correlates well with the analysis
shown in Tables 4 and 5, which shows that features using semantic (R) and syntactic
(D) representations contribute the most on top of the IR model (BM25(W)).
370
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
5. Error Analysis and Discussion
Similar to most re-ranking systems, our system improves the answer quality for some
questions while decreasing it for others. Table 7 lists the percentage of questions from
our test set that are improved (i.e., the correct answer is ranked higher after re-ranking),
worsened (i.e., the correct answer is ranked lower), and unchanged (i.e., the position of
the correct answer does not change after re-ranking). The table indicates that, regard-
less of the number of candidate answers for re-ranking (N), the number of improved
questions is approximately twice the number of worsened questions. This explains the
consistent improvements in P@1 and MRR measured for various values of N. As N
increases, the number of questions that are improved also grows, which is an expected
consequence of having more candidate answers to re-rank. However, the percentage
of improved questions grows at a slightly lower rate than the percentage of worsened
questions. This indicates that choosing the ideal number of candidate answers to re-
rank requires a trade-off: On the one hand, having more candidate answers increases
the probability of capturing the correct answer in the set; on the other hand, it also
increases the probability of choosing an incorrect answer due to the larger number
of additional candidates. For our problem, it seems that re-ranking using values of N
much larger than 100 would not yield significant benefits over smaller values of N.
This analysis is consistent with the experiments reported in Table 3 where we did not
measure significant growth in P@1 or MRR for N larger than 50.
Although Table 7 gives the big picture of the behavior of our system, it is important
to look at actual questions that are improved or worsened by the re-ranking model in
order to understand the strengths and weaknesses of our system. Table 8 lists some
representative questions where the re-ranking model brings the correct answer to the
top position. For every question we list: (a) the correct answer and its position as given
by the baseline IR Model (?Baseline?) and the re-ranking model (?Re-ranking?); and (b)
the answer that was ranked by the baseline model in the first position and its position
after re-ranking.
Generally, Table 8 indicates that our model performs considerably better than the
bag-of-words IRmodel. For example, we boost the rank of answers that share structures
with the question: for example, the cook ? grouse syntactico-semantic dependency for
the second sample question or make ? call and see ? number for the third example.
Modeling structures is important especially for questions with minimal context, that is,
short length and common terms, like the third sample question. Due to the structure-
based translation models and/or the generalizations to supersenses or named-entity
labels, our model can match structures even when they are not identical. For example,
Table 7
Percentage of questions in the test set that are improved/worsened/unchanged after re-ranking.
This experiment used the Perceptron model.
Better (%) Worse (%) Unchanged (%)
N = 10 33.98 16.81 49.21
N = 15 36.76 18.56 44.68
N = 25 39.64 20.68 39.68
N = 50 42.95 23.30 33.75
N = 100 45.18 25.28 29.54
371
Computational Linguistics Volume 37, Number 2
Table 8
Examples of questions improved by our re-ranking model. URLs were replaced with <URL> in
answer texts. Some non-relevant text was replaced with <...> to save space. The remaining text
maintains the original capitalization and spelling. Non-stop question terms are emphasized in
the answers.
How would you rank the top 5 NFL teams? Do your rankings depend on the outcome
of the Colts Vs. Pats Game?
Baseline Re-ranking Correct? Answer Text
2 1 yes Ok. How can you think the Chargers are better than the Colts, Ravens, and
the Broncos? As for the Rankings, this is what I got: 1. Colts (they actually beat
a good team) 2. Bears (for now, they are going to lose to the Giants) 3. Patriots
(the winner of Cotls vs Patriots should be the next #1) 4. Broncos (They lost
on a field goal) 5. Ravens (all about the D)
1 2 no Basically in sport rankings you will look at the how they perform their
wins against their loss or their stats. Ranking is the process of positioning
individuals, groups or businesses on a ordinal scale in relation to others.
A list arranged in this way is said to be in rank order. Some examples: In
many sports, individuals or teams are given rankings, generally by the sport?s
governing body. In football (soccer), national teams are ranked in the FIFA
World Rankings. In snooker, players are ranked using the Snooker world
rankings. In ice hockey, national teams are ranked in the IIHF World Ranking.
In golf, the top male golfers are ranked using the Official World Golf Rankings
how can i cook grouse quick with normal household spices w/o going out to buy stuff?
Baseline Re-ranking Correct? Answer Text
10 1 yes I?ve never cooked a grouse, but poultry is poultry... You could salt it and pepper
it, put some flour on a plate and roll it in the flour to coat it lightly, then heat
a few tablespoons of olive oil in a skillet and pan-fry it. (If you have no olive
oil, use a little vegetable oil plus a pat of butter ? the oil is to keep the butter
from burning.) Squeeze a few drops of lemon juice over it if you want. Or:
Skip the flour. Salt and pepper the grouse. Pan-fry it in a little olive oil. When
it?s looking close to ready, pour in a little white wine which will sizzle and
reduce down to a sauce. If you?ve got some fresh or dried herbs (rosemary,
thyme, parsley) you could sprinkle a pinch of that in.
1 2 no Well, a grouse is a prey animal. If there was a decline in the population
of grouse, then the animals that usually prey on the grouse - coyotes, owls,
etc - would probably start eating other prey animals, like the pheasants and
squirrels.
how did I do for make a call and that the other dont see my number?
Baseline Re-ranking Correct? Answer Text
2 1 yes to make a call so that the other person cant see the number... dial *67 and wait
for the three beeps.. then dial the number
1 2 no Oneday out of the blue call her. If u dont have her number, when u see her ask
her if she wanted to go out oneday then get her number. When u talk on the
phone get to know her. But dont ask her out too soon because she may not
feel the same way. After a couple of days or weeks taking to her let her know
how u felt about her since the first time u met her.
372
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
Table 8
(continued)
how can i find a veterinary college with dorms?
Baseline Re-ranking Correct? Answer Text
14 1 yes <...> I would say not to look for a specific school of veterinarianmedicine but
rather find a creditable University that offers a degree such as Pre-Vet. Then
from there you can attend graduate school to finish up to become a doctor in
that field. Most major universities will have this degree along with dorms. In
my sources you can see that this is just one of many major universities that
offer Pre-vet medicine.
1 7 no Hi there... here?s an instructional video by Cornell University Feline Health
Center - College of VeterinaryMedicine on how to pill cats: <URL>
how to handle commission splits with partners in Real estate?
Baseline Re-ranking Correct? Answer Text
5 1 yes My company splits the commissions all evenly. However many various
agents/brokers are involved (or even think they are involved), it gets split
further. Keeps everyone happy. No one complains that someone gets ?more?.
1 3 no You will find information regarding obtaining a real estate license in Okla-
homa at the Oklahoma Real Estate Commission?s website (<URL>) Good luck!
for the fourth question, find? college can bematched to look? school if the structures are
generalized toWordNet supersenses. Translation models are crucial to fetching answers
rich in terms related to question concepts. For example, for the first question, our model
boosts the position of the correct answer due to the large numbers of concepts that
are related to NFL, Colts, and Pats: Ravens, Broncos, Patriots, and so forth. In the second
example, our model ranks on the first position the answer containing many concepts
related to cook: salt, pepper, flour, tablespoons, oil, skillet, and so on. In the last example,
our model is capable of associating the bigram real estate to agent and broker. Without
these associationsmany answers are lost to false positives provided by the bag-of-words
similarity models. For example, in the first and last examples in the table, the answers
selected by the baseline model contain more matches of the questions terms than the
correct answers extracted by our model.
All in all, this analysis proves that non-factoid QA is a complex problem where
many phenomena must be addressed. The key for success does not seem to be a unique
model, but rather a combination of approaches each capable of addressing different
facets of the problem. Our model makes a step forward towards this goal, mainly
through concept expansion and the exploration of syntactico-semantic structures. Nev-
ertheless, our model is not perfect. To understand where FMIX fails we performed
a manual error analysis on 50 questions where FMIX performs worse than the IR
baseline and we identified seven error classes. Table 9 lists the distribution of these error
classes and Table 10 lists sample questions and answers from each class. Note that the
percentage values listed in Table 9 sum up to more than 100% because the error classes
are not exclusive. We now detail each of these error classes.
373
Computational Linguistics Volume 37, Number 2
Table 9
Distribution of error classes in questions where FMIX (Perceptron) performs worse.
COMPLEX INFERENCE 38%
ELLIPSIS 36%
ALSO GOOD 18%
REDIRECTION 10%
ANSWER QUALITY 4%
SPELLING 2%
CLARIFICATION 2%
COMPLEX INFERENCE: This is the most common class of errors (38%). Questions in this
class could theoretically be answered by an automated system but such a system would
require complex reasoning mechanisms, large amounts of world knowledge, and dis-
course understanding. For example, to answer the first question in Table 10, a system
would have to understand that confronting or being supportive are forms of dealing with
a person. To answer the second question, the system would have to know that creating
a CD at what resolution you need supersedes making a low resolution CD. Our approach
captures some simple inference rules through translationmodels but fails to understand
complex implications such as these.
ELLIPSIS: This class of errors is not necessarily a fault of our approach but is rather
caused by the problem setting. Because in a social QA site each answer responds to a
specific question, discourse ellipsis (i.e., omitting the context set by the question in the
answer text) is common. This makes some answers (e.g., the third answer in Table 10)
ambiguous, hence hard to retrieve automatically. This affects 36% of the questions
analyzed.
ALSO GOOD: It is a common phenomenon in Yahoo! Answers that a question is asked
several times by different users, possibly in a slightly different formulation. To enable
our large scale automatic evaluation, we considered an answer as correct only if it was
chosen as the ?best answer? for the corresponding question. So in our setting, ?best
answers? from equivalent questions are marked as incorrect. This causes 18% of the
?errors? of the re-ranking model. One example is the fourth question in Table 10, where
the answer selected by our re-ranking model is obviously also correct. It is important
to note that at testing time we do not have access to the questions that generated the
candidate answers for the current test question, that is, the system does not know
which questions are answered by the answers in the ALSO GOOD section of Table 10.
So the answers in the ALSO GOOD category are not selected based on the similarity of
the corresponding queries, but rather, based on better semantic matching between test
question and candidate answer.
REDIRECTION: Some answers (10% of the questions analyzed) do not directly answer a
question but rather redirect the user to relevant URLs (see the fifth question in Table 10).
Because we do not extract the text behind URLs in the answer content, such questions
are virtually impossible to answer using our approach.
ANSWER QUALITY: For a small number of the questions analyzed (4%) the choice of ?best
answer? is dubious (see the sixth example in Table 10). This is to be expected in a
social QA site, where the selection of best answers is not guaranteed to be optimal.
Nevertheless, the relatively small number of such cases is unlikely to influence the
quality of the evaluation.
374
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
Table 10
Examples of questions in each error class. The corresponding error class is listed on the left side
of the question text. We list the answer ranked at the top position by FMIX only where relevant
(e.g., the ALSO GOOD category). URLs were replaced with <URL> in answer texts. Some
non-relevant text was replaced with <...> to save space. The remaining text maintains the
original capitalization and spelling.
COMPLEX INFERENCE how to deal with a person in denial with M.P.D.?
Baseline Re-ranking Correct? Answer Text
1 6 yes First, i would find out if MPD has been diagnosed by a pro-
fessional. In current terminology, MPD is considered a part
of Dissociative Personality Disorder. In any case, it would be
up to the professionals to help this person because you could
cause further problems by confronting this person with what
you think the problem is. If this person is a family member,
you could ask for a consultation with the psychiatric profes-
sional who is treating him/her. Please, please, just do you
best to be supportive without being confrontational since that
might make things even worse for that person.
COMPLEX INFERENCE How do I make a low resolution CD of 100 phtos that were shot at
8 megapixels?
Baseline Re-ranking Correct? Answer Text
1 4 yes you can use picasa2 from google: <URL> is free. In picasa
you can do ?GiftCD? create a CD with you picture at what
resolution you need (including original size)
ELLIPSIS How do mineral ions affect the biology and survival of a pond organism?
Baseline Re-ranking Correct? Answer Text
1 3 yes Some mineral ions are fertilizer and will increase vegetative
growth while others are poisons.
ALSO GOOD How to learn the British accent?
Baseline Re-ranking Correct? Answer Text
4 5 yes Get a dictionary where there is a pronunciation guide which
gives the pronunciation in British English. Watch british
movies and imitate what you can. Then just practice, practice
practice. But before you go about learning accents, slangs or
dialects, make sure you brush up on your basic grammar.
<...>
3 1 no You can do one of two things: first, go to a local bookstore, like
Barnes and Noble. They sell cd?s with different accents from
around the world, accompanied by a book that phonetically
spells the words. This is designed for actors/actresses who
need to learn different accents. Also, go rent a bunch of british
movies, or watch british television. Continually pause and
repeat common phrases and words.
375
Computational Linguistics Volume 37, Number 2
Table 10
(continued)
REDIRECTION How can I build an easy lean-to shed out of scrap wood and skids?
Baseline Re-ranking Correct? Answer Text
6 15 yes the pallet shed... <URL> building a wood shed from pallets...
<URL> good ideas from those who?ve been there...<URL> pics. of
the shed... <URL> nice pics. <URL> taking pallets apart... and other
tips... <URL> <...>
ANSWER QUALITY How do make a Naruto AMV? Can you show me how? I need the
website or program and the exact directions.?
Baseline Re-ranking Correct? Answer Text
2 94 yes i?m not an expert. but i sure do like Naruto. i?ll wait for answers
too
SPELLING how does aliquid expansion boiler thrrmosstat work?
Baseline Re-ranking Correct? Answer Text
2 4 yes the liquid expands inside the thermostat when the liquid reaches
the shutoff temp or pressure it will shut off the boiler preventing
boiler explosions
CLARIFICATION how could you combine your styles and personalities effectively to
produce the best paper?
Baseline Re-ranking Correct? Answer Text
29 1 yes Your question is not clear. Are you asking about writing styles?
it also depends on what kind of paper you are writing? Your
question cannot be answered without more info.
SPELLING: Two percent (2%) of the error cases analyzed are caused by spelling errors
(e.g., the seventh example in Table 10). Because these errors are relatively infrequent,
they are not captured by our translation models, and our current system does not
include any other form of spelling correction.
CLARIFICATION: Another 2% of the questions inspected manually had answers that
pointed to errors or ambiguities in the question text rather than responding to the given
question (see the last example in Table 10). These answers are essentially correct but
they require different techniques to be extracted: Our assumption is that questions are
always correct and sufficient for answer extraction.
6. Related Work
There is a considerable amount of previous work in several related areas. First, we will
discuss related work with respect to the features and models used in this research; most
of this work is to be found in the factoid QA community, where the most sophisticated
376
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
QA selection and re-ranking algorithms have been developed. We then review existing
work in non-factoid QA; we will see that in this area there is much less work, and the
emphasis has been so far in query re-writing and scalability using relatively simple
features andmodels. Finally wewill discuss relatedwork in the area of community-built
(social) QA sites. Although we do not exploit the social aspect of our QA collection, this
is complementary to our work and would be a natural extension. Table 11 summarizes
aspects of the different approaches discussed in this section, highlighting the differences
and similarities with our current work.
Our work borrows ideas from many of the papers mentioned in this section, es-
pecially for feature development; indeed our work includes matching features as well
as translation and retrieval models, and operates at the lexical level, the parse tree
Table 11
Comparison of some of the characteristics of the related work cited. Task: Document Retrieval
(DRet), Answer Extraction (Ex) or Answer Re-ranking or Selection (Sel).Queries: factoid (Fact)
or non-factoid (NonFact). Features: lexical (L), n-grams (Ngr), collocations (Coll), paraphrases
(Para), POS, syntactic dependency tree (DT), syntactic constituent tree (CT), named entities (NE),
WordNet Relations (WNR), WordNet supersenses (WNSS), semantic role labeling (SRL), causal
relations (CR), query classes (QC), query-log co-ocurrences (QLCoOcc).Models: bag-of-words
scoring (BOW), tree matching (TreeMatch), linear (LM), log-linear (LLM), statistical learning
(kernel) (SL), probabilistic grammar (PG), statistical machine translation (SMT), query likelihood
language model (QLLM).Development and Evaluation: data sizes used, expressed as number
of queries/number of query?answer pairs (i.e., sum of all candidate answers per question).
Data: type of source used for feature construction, training and/or evaluation. Question marks
are place holders for information not available or not applicable in the corresponding work.
Publication Task Queries Features Models Devel Eval Data
Agichtein et al DRet NonFact L, Ngr, Coll BOW, LM ?/10K 50/100 WWW
(2001)
Echihabi and Sel Fact, L, Ngr, Coll, SMT 4.6K/100K 1K/300K? TREC, KM,
Marcu (2003) NonFact DT, NE, WWW
WN
Higashinaka and Sel NonFact L, WN, SRL, SL 1K/500K 1K/500K WHYQA
Isozaki (2008) (Why) CR
Punyakanok et al Sel Fact L, POS, DT, TreeMatch ?/400 TREC13
(2004) NE, QC
Riezler et al DRet NonFact L, Ngr, Para SMT 10M/10M 60/1.2K WWW, FAQ
(2007)
Soricut and Brill DRet, NonFact L, Ngr, Coll BOW, SMT 1M/? 100/? WWW, FAQ
(2006) Sel,
Ex
Verberne et al Sel NonFact CT, WN, BOW, LLM same as eval 186/28K Webclopedia,
(2010) (Why) Para Wikipedia
Wang et al (2007) Sel Fact L, POS, DT, LLM, PG 100/1.7K 200/1.7K TREC13
NE, WNR,
Hyb
Xue et al (2008) DRet, NonFact, L, Coll SMT, QLLM 1M/1M 50/? SocQA,
Sel Fact TREC9
This work Sel NonFact L, Ngr, POS, TreeMatch, 112K/1.6M 28K/up to SocQA,
(How) DT, SRL, BOW, SMT, 2.8M QLog
NE, WN, SL
WNSS,
Hyb,
QLCoOcc
377
Computational Linguistics Volume 37, Number 2
level, as well as the level of semantic roles, named entities, and lexical semantic classes.
However, to the best of our knowledge no previous work in QA has evaluated the use
of so many types of features concurrently, nor has it built so many combinations of these
features at different levels. Furthermore, we employ unsupervised methods, generative
methods, and supervised learning methods. This is made possible by the choice of the
task and the data collection, another novelty of our work which should enable future
research in complex linguistic features for QA and ranking.
Factoid QA. Within the statistical machine translation community there has been
much research on the issue of automatically learning transformations (at the lexical,
syntactical, and semantical level). Some of this work has been applied to automated
QA systems, mostly for factoid questions. For example, Echihabi and Marcu (2003)
presented a noisy-channel approach (IBM model 4) adapted for the task of QA. The
features used included lexical and parse-tree elements as well as some named entities
(such as dates). They use a dozen heuristic rules to heavily reduce the feature space and
choose a single representation mode for each of the tokens in the queries (for example:
?terms overlapping with the question are preserved as surface text?) and learn language
models on the resulting representation. We extend Echihabi and Marcu by considering
deeper semantic representations (such as SRL andWNSS), but instead of using selection
heuristics we learn models from each of the full representations (as well as from some
hybrid representations) and then combine them using discriminant learning techniques.
Punyakanok, Roth, and Yih (2004) attempted a more comprehensive use of the
parse tree information, computing a similarity score between question and answer
parse trees (using a distance function based on approximate tree matching algorithms).
This is an unsupervised approach, which is interesting especially when coupled with
appropriate distances. Shen and Joshi (2005) extend this idea with a supervised learning
approach, training dependency tree kernels to compute the similarity. In our work we
also used this type of feature, although we show that, in our context, features based on
dependency tree kernels are subsumed by simpler features that measure the overlap
of binary dependencies. Another alternative is proposed by Cui et al (2005), where
significant words are aligned and similarity measures (based on mutual information of
correlations) are then computed on the resulting dependency paths. Shen and Klakow
(2006) extend this using a dynamic time warping algorithm to improve the alignment
for approximate question phrase mapping, and learn a Maximum Entropy model to
combine the obtained scores for re-ranking. Wang, Smith, andMitamura (2007) propose
to use a probabilistic quasi-synchronous grammar to learn the syntactic transformations
between questions and answers. We extend the work of Cui et al by considering paths
within and across different representations beyond dependency trees, although we do
not investigate the issue of alignment specifically?instead we use standard statistical
translation models for this.
Non-factoid QA. The previous works dealt with the problem of selection, that is,
finding the single sentence that correctly answers the question out of a set of candidate
documents. A related problem in QA is that of retrieval: selecting potentially relevant
documents or sentences prior to the selection phase. This problem is closer to gene-
ral document retrieval and it is therefore easier to generalize to the non-factoid domain.
Retrieval algorithms tend to be much simpler than selection algorithms, however, in
part due to the need for speed, but also because there has been little previous evidence
that complex algorithms or deeper linguistic analysis helps at this stage, especially in
the context of non-factoid questions.
378
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
Previous work addressed the task by learning transformations between questions
and answers and using them to improve retrieval. All these works use only lexical
features. For example, Agichtein et al (2001) learned lexical transformations (from the
original question to a set of Web search queries, from ?what is a? to ?the term?, ?stands
for?, etc.) which are likely to retrieve good candidate documents in commercial Web
search engines; they applied this successfully to large-scale factoid and non-factoid QA
tasks. Murdock and Croft (2005) study the problem of candidate sentence retrieval for
QA and show that a lexical translation model can be exploited to improve factoid QA.
Xue, Jeon, and Croft (2008) show that a linear interpolation of translation models and
a query likelihood language model outperforms each individual model for a QA task
that is independent of the question type. In the same space, Riezler et al (2007) develop
SMT-based query expansionmethods and use them for retrieval from FAQpages. In our
work we did not address the issue of query expansion and re-writing directly: While
our re-ranking approach is limited to the recall of the retrieval model, these methods of
query transformation could be used in a complementary manner to improve the recall.
Even more interesting would be to couple the two approaches in an efficient manner;
this remains as future work.
There has also been some work in the problem of selection for non-factoid ques-
tions. Girju (2003) extracts non-factoid answers by searching for certain semantic struc-
tures (e.g., causation relations as answers to causation questions). We generalized this
methodology (in the form of semantic roles) and evaluated it systematically. Soricut
and Brill (2006) develop a statistical model by extracting (in an unsupervised manner)
QA pairs from one million FAQs obtained from the Web. They show how different
statistical models may be used for the problems of ranking, selection, and extraction
of non-factoid QAs on the Web; due to the scale of their problem they only consider lex-
ical n-grams and collocations, however. More recent work has showed that structured
retrieval improves answer ranking for factoid questions: Bilotti et al (2007) showed that
matching predicate?argument frames constructed from the question and the expected
answer types improves answer ranking. Cui et al (2005) learned transformations of
dependency paths from questions to answers to improve passage ranking. All these
approaches use similarity models at their core because they require the matching of
the lexical elements in the search structures, however. On the other hand, our approach
allows the learning of full transformations from question structures to answer structures
using translation models applied to different text representations.
The closest work to ours is that of Higashinaka and Isozaki (2008) and Verberne
et al (2010), both on Why questions. Higashinaka et al consider a wide range of
semantic features by exploiting WordNet and gazetteers, semantic role labeling, and
extracted causal relations. Verberne et al exploit syntactic information from constituent
trees, WordNet synonymy sets and relatedness measures, and paraphrases. As in our
models, both these works combine these features using discriminative learning tech-
niques and apply the learned models to re-rank answers to non-factoid questions (Why
type questions). Their features, however, are based on counting matches or events
defined heuristically. We have extended this approach in several ways. First, we use a
much larger feature set that includes correlation and transformation-based features and
five different content representations. Second, we use generative (translation) models
to learn transformation functions before they are combined by the discriminant learner.
Finally, we carry out training and evaluation at a much larger scale.
Content from community-built question?answer sites can be retrieved by searching
for similar questions already answered (Jeon, Croft, and Lee 2005) and ranked using
meta-data information like answerer authority (Jeon et al 2006; Agichtein et al 2008).
379
Computational Linguistics Volume 37, Number 2
Here we show that the answer text can be successfully used to improve answer ranking
quality. Our method is complementary to the earlier approaches. It is likely that an
optimal retrieval engine from social media would combine all three methodologies.
Moreover, our approach might have applications outside of social media (e.g., for open-
domainWeb-based QA), because the rankingmodel built is based only on open-domain
knowledge and the analysis of textual content.
7. Conclusions
In this work we describe an answer ranking system for non-factoid questions built
using a large community-generated question?answer collection. We show that the best
ranking performance is obtained when several strategies are combined into a single
model. We obtain the best results when similarity models are aggregated with features
that model question-to-answer transformations, frequency and density of content, and
correlation of QA pairs with external collections. Although the features that model
question-to-answer transformations provide the most benefits, we show that the com-
bination is crucial for improvement. Further, we show that complex linguistic features,
most notably semantic role dependencies and semantic labels derived from WordNet
senses, yield a statistically significant performance increase on top of the traditional
bag-of-words and n-gram representations. We obtain these results using only off-the-
shelf NL processors that were not adapted in any way for our task. As a side effect, our
experiments prove that we can effectively exploit large amounts of availableWeb data to
do research on NLP for non-factoid QA systems, without any annotation or evaluation
cost. This provides an excellent framework for large-scale experimentation with various
models that otherwise might be hard to understand or evaluate.
As implications of our work, we expect the outcome of our investigation to help
several applications, such as retrieval from social media and open-domain QA on the
Web. On social media, for example, our system should be combined with a component
that searches for similar questions already answered; the output of this ensemble can
possibly be filtered further by a content-quality module that explores ?social? features
such as the authority of users, and so on. Although we do not experiment on Wikipedia
or news sites in this work, one can view our data as a ?worse-case scenario,? given its
ungrammaticality and annotation quality. It seems reasonable to expect that training our
model on cleaner data (e.g., fromWikipedia or news), would yield even better results.
This work can be extended in several directions. First, answers that were not se-
lected as best, but were marked as good by a minority of voters, could be incorporated
in the training data, possibly introducing a graded notion of relevance. This wouldmake
the learning problemmore interesting andwould also provide valuable insights into the
possible pitfalls of user-annotated data. It is not clear if more data, but of questionable
quality, is beneficial. Another interesting problem concerns the adaptation of the re-
ranking model trained on social media to collections from other genres and/or domains
(news, blogs, etc.). To our knowledge, this domain adaptation problem for QA has not
been investigated yet.
References
Agichtein, Eugene, Carlos Castillo, Debora
Donato, Aristides Gionis, and Gilad Mishne.
2008. Finding high-quality content in social
media, with an application to community-
based question answering. In Proceedings of
the Web Search and Data Mining Conference
(WSDM), pages 183?194, Stanford, CA.
Agichtein, Eugene, Steve Lawrence, and Luis
Gravano. 2001. Learning search engine
specific query transformations for question
answering. In Proceedings of the World
380
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
Wide Web Conference, pages 169?178,
Hong Kong.
Attardi, Giuseppe, Felice Dell?Orletta, Maria
Simi, Atanas Chanev, and Massimiliano
Ciaramita. 2007. Multilingual dependency
parsing and domain adaptation using
DeSR. In Proceedings of the Shared Task
of the Conference on Computational
Natural Language Learning (CoNLL),
pages 1112?1118, Prague.
Berger, Adam, Rich Caruana, David Cohn,
Dayne Freytag, and Vibhu Mittal. 2000.
Bridging the lexical chasm: Statistical
approaches to answer finding. In
Proceedings of the 23rd Annual International
ACM SIGIR Conference on Research &
Development on Information Retrieval,
pages 192?199, Athens, Greece.
Bilotti, Matthew W., Paul Ogilvie, Jamie
Callan, and Eric Nyberg. 2007. Structured
retrieval for question answering. In
Proceedings of the 30th Annual International
ACM SIGIR Conference on Research &
Development on Information Retrieval,
pages 351?358, Amsterdam.
Brill, Eric, Jimmy Lin, Michele Banko,
Susan Dumais, and Andrew Ng. 2001.
Data-intensive question answering.
In Proceedings of the Text REtrieval
Conference (TREC), pages 393?400,
Gaithersburg, MD, USA.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Charniak, Eugene. 2000. A maximum-
entropy-inspired parser. In Proceedings of
the North American Chapter of the Association
for Computational Linguistics (NAACL),
pages 132?139, Seattle, WA.
Ciaramita, Massimiliano and Yasemin
Altun. 2006. Broad coverage sense
disambiguation and information
extraction with a supersense sequence
tagger. In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 594?602,
Sidney.
Ciaramita, Massimiliano, Giuseppe Attardi,
Felice Dell?Orletta, and Mihai Surdeanu.
2008. Desrl: A linear-time semantic role
labeling system. In Proceedings of the Shared
Task of the 12th Conference on Computational
Natural Language Learning (CoNLL-2008),
pages 258?262, Manchester.
Ciaramita, Massimiliano and Mark Johnson.
2003. Supersense tagging of unknown
nouns in WordNet. In Proceedings of the
2003 Conference on Empirical Methods in
Natural Language Processing,
pages 168?175, Sapporo.
Ciaramita, Massimiliano, Vanessa Murdock,
and Vassilis Plachouras. 2008. Semantic
associations for contextual advertising.
Journal of Electronic Commerce
Research?Special Issue on Online
Advertising and Sponsored Search, 9(1):1?15.
Collins, Michael and Nigel Duffy. 2001.
Convolution kernels for natural language.
In Proceedings of the Neural Information
Processing Systems Conference (NIPS),
pages 625?632, Vancouver, Canada.
Cui, Hang, Renxu Sun, Keya Li, Min-Yen
Kan, and Tat-Seng Chua. 2005. Question
answering passage retrieval using
dependency relations. In Proceedings of the
28th Annual International ACM SIGIR
Conference on Research & Development in
Information Retrieval, pages 400?407,
Salvador.
Echihabi, Abdessamad and Daniel Marcu.
2003. A noisy-channel approach to
question answering. In Proceedings of the
41st Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 16?23, Sapporo.
Freund, Yoav and Robert E. Schapire. 1999.
Large margin classification using the
perceptron algorithm.Machine Learning,
37:277?296.
Girju, Roxana. 2003. Automatic detection of
causal relations for question answering. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics
(ACL), Workshop on Multilingual
Summarization and Question Answering,
pages 76?83, Sapporo.
Harabagiu, Sanda, Dan Moldovan, Marius
Pasca, Rada Mihalcea, Mihai Surdeanu,
Razvan Bunescu, Roxana Girju, Vasile Rus,
and Paul Morarescu. 2000. Falcon:
Boosting knowledge for answer engines.
In Proceedings of the Text REtrieval
Conference (TREC), pages 479?487,
Gaithersburg, MD.
Higashinaka, Ryuichiro and Hideki Isozaki.
2008. Corpus-based question answering
for why-questions. In Proceedings of the
Third International Joint Conference on
Natural Language Processing (IJCNLP),
pages 418?425, Hyderabad.
Jeon, Jiwoon, W. Bruce Croft, and
Joon Hoo Lee. 2005. Finding similar
questions in large question and answer
archives. In Proceedings of the ACM
Conference on Information and Knowledge
381
Computational Linguistics Volume 37, Number 2
Management (CIKM), pages 84?90,
Bremen.
Jeon, Jiwoon, W. Bruce Croft, Joon Hoo Lee,
and Soyeon Park. 2006. A framework to
predict the quality of answers with
non-textual features. In Proceedings of the
29th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 228?235,
Seattle, WA.
Joachims, Thorsten. 2006. Training linear
svms in linear time. In KDD ?06:
Proceedings of the 12th ACM SIGKDD
International Conference on Knowledge
Discovery and Data Mining, pages 217?226,
New York, NY.
Ko, Jeongwoo, Teruko Mitamura, and Eric
Nyberg. 2007. Language-independent
probabilistic answer ranking for question
answering. In Proceedings of the 45th
Annual Meeting of the Association for
Computational Linguistics, pages 784?791,
Prague.
Li, Xin and Dan Roth. 2006. Learning
question classifiers: The role of semantic
information. Natural Language Engineering,
12:229?249.
Li, Yaoyong, Hugo Zaragoza, Ralf Herbrich,
John Shawe-Taylor, and Jaz S. Kandola.
2002. The perceptron algorithm with
uneven margins. In Proceedings of the
Nineteenth International Conference on
Machine Learning, pages 379?386,
Sidney.
Magnini, Bernardo, Matteo Negri, Roberto
Prevete, and Hristo Tanev. 2002.
Comparing statistical and content-based
techniques for answer validation on the
web. In Proceedings of the VIII Convegno
AI*IA, Siena, Italy.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Moldovan, Dan, Sanda Harabagiu, Marius
Pasca, Rada Mihalcea, Richard Goodrum,
Roxana Girju, and Vasile Rus. 1999.
Lasso?a tool for surfing the answer net. In
Proceedings of the Text REtrieval Conference
(TREC), pages 175?183, Gaithersburg, MD.
Moschitti, Alessandro, Silvia Quarteroni,
Roberto Basili, and Suresh Manandhar.
2007. Exploiting syntactic and shallow
semantic kernels for question/answer
classification. In Proceedings of the 45th
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 776?783, Prague.
Murdock, Vanessa and W. Bruce Croft. 2005.
A translation model for sentence retrieval.
In Proceedings of the Conference on Human
Language Technology and Empirical Methods
in Natural Language Processing,
pages 684?691, Vancouver.
Palmer, Martha, Daniel Gildea, and Paul
Kingsbury. 2005. The proposition bank: An
annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106
Punyakanok, Vasin, Dan Roth, and Wen-tau
Yih. 2004. Mapping dependencies trees:
An application to question answering.
Proceedings of AI&Math 2004, pages 1?10,
Fort Lauderdale, FL.
Riezler, Stefan, Alexander Vasserman,
Ioannis Tsochantaridis, Vibhu Mittal,
and Yi Liu. 2007. Statistical machine
translation for query expansion in
answer retrieval. In Proceedings of the
45th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 464?471, Prague.
Robertson, Stephen and Stephen G. Walker.
1997. On relevance weights with little
relevance information. In Proceedings of
the Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 16?24,
New York, NY.
Shen, Dan and Dietrich Klakow. 2006.
Exploring correlation of dependency
relation paths for answer extraction. In
Proceedings of the 21st International
Conference on Computational Linguistics and
the 44th Annual Meeting of the Association for
Computational Linguistics, pages 889?896,
Sydney.
Shen, Libin and Aravind K. Joshi. 2005.
Ranking and reranking with perceptron.
Machine Learning. Special Issue on Learning
in Speech and Language Technologies,
60(1):73?96.
Soricut, Radu and Eric Brill. 2006. Automatic
question answering using the Web:
Beyond the factoid. Journal of Information
Retrieval?Special Issue on Web Information
Retrieval, 9(2):191?206.
Surdeanu, Mihai and Massimiliano
Ciaramita. 2007. Robust information
extraction with perceptrons. In Proceedings
of the NIST 2007 Automatic Content
Extraction Workshop (ACE07), College Park,
MD. Available at: http://www.surdeanu.
name/mihai/papers/ace07a.pdf.
Surdeanu, Mihai, Richard Johansson, Adam
Meyers, Lluis Marquez, and Joakim Nivre.
2008. The CoNLL-2008 shared task on joint
parsing of syntactic and semantic
382
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
dependencies. In Proceedings of the
Conference on Computational Natural
Language Learning (CoNLL), pages 159?177,
Manchester.
Surdeanu, Mihai, Lluis Marquez, Xavier
Carreras, and Pere R. Comas. 2007.
Combination strategies for semantic role
labeling. Journal of Artificial Intelligence
Research, 29:105?151.
Tao, Tao and ChengXiang Zhai. 2007. An
exploration of proximity measures in
information retrieval. In Proceedings of the
30th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 259?302,
Amsterdam.
Tsochantaridis, Ioannis, Thomas Hofmann,
Thorsten Joachims, and Yasemin Altun.
2004. Support vector machine learning for
interdependent and structured output
spaces. In ICML ?04: Proceedings of the
Twenty-First International Conference on
Machine Learning, pages 104?111,
New York, NY.
Verberne, Suzan, Lou Boves, Nelleke
Oostdijk, and Peter-Arno Coppen. 2010.
What is not in the bag of words for
why-qa? Computational Linguistics,
36(2):229?245.
Voorhees, Ellen M. 2001. Overview of the
TREC-9 question answering track. In
Proceedings of the Text REtrieval Conference
(TREC) TREC-9 Proceedings, pages 1?15,
Gaithersburg, MD.
Wang, Mengqiu, Noah A. Smith, and Teruko
Mitamura. 2007. What is the Jeopardy
model? A quasi-synchronous grammar for
QA. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural
Language Processing and Computational
Natural Language Learning, pages 22?32,
Prague.
Xue, Xiaobing, Jiwoon Jeon, and W. Bruce
Croft. 2008. Retrieval models for question
and answer archives. In Proceedings of the
Annual ACM SIGIR Conference on Research
and Development in Information Retrieval,
pages 475?482, Singapore.
383

Proceedings of ACL-08: HLT, pages 719?727,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learning to Rank Answers on Large Online QA Collections
Mihai Surdeanu, Massimiliano Ciaramita, Hugo Zaragoza
Barcelona Media Innovation Center, Yahoo! Research Barcelona
mihai.surdeanu@barcelonamedia.org, {massi,hugo}@yahoo-inc.com
Abstract
This work describes an answer ranking engine
for non-factoid questions built using a large
online community-generated question-answer
collection (Yahoo! Answers). We show how
such collections may be used to effectively
set up large supervised learning experiments.
Furthermore we investigate a wide range of
feature types, some exploiting NLP proces-
sors, and demonstrate that using them in com-
bination leads to considerable improvements
in accuracy.
1 Introduction
The problem of Question Answering (QA) has re-
ceived considerable attention in the past few years.
Nevertheless, most of the work has focused on the
task of factoid QA, where questions match short an-
swers, usually in the form of named or numerical en-
tities. Thanks to international evaluations organized
by conferences such as the Text REtrieval Confer-
ence (TREC)1 or the Cross Language Evaluation Fo-
rum (CLEF) Workshop2, annotated corpora of ques-
tions and answers have become available for several
languages, which has facilitated the development of
robust machine learning models for the task.
The situation is different once one moves beyond
the task of factoid QA. Comparatively little research
has focused on QA models for non-factoid ques-
tions such as causation, manner, or reason questions.
Because virtually no training data is available for
this problem, most automated systems train either
1http://trec.nist.gov
2http://www.clef-campaign.org
Q: How do you quiet a squeaky door?
A: Spray WD-40 directly onto the hinges
of the door. Open and close the door
several times. Remove hinges if the
door still squeaks. Remove any rust,
dirt or loose paint. Apply WD-40 to
High removed hinges. Put the hinges back,
Quality open and close door several times again.
Q: How to extract html tags from an html
Low documents with c++?
Quality A: very carefully
Table 1: Sample content from Yahoo! Answers.
on small hand-annotated corpora built in house (Hi-
gashinaka and Isozaki, 2008) or on question-answer
pairs harvested from Frequently Asked Questions
(FAQ) lists or similar resources (Soricut and Brill,
2006). None of these situations is ideal: the cost
of building the training corpus in the former setup
is high; in the latter scenario the data tends to be
domain-specific, hence unsuitable for the learning of
open-domain models.
On the other hand, recent years have seen an ex-
plosion of user-generated content (or social media).
Of particular interest in our context are community-
driven question-answering sites, such as Yahoo! An-
swers3, where users answer questions posed by other
users and best answers are selected manually either
by the asker or by all the participants in the thread.
The data generated by these sites has significant ad-
vantages over other web resources: (a) it has a high
growth rate and it is already abundant; (b) it cov-
ers a large number of topics, hence it offers a better
3http://answers.yahoo.com
719
approximation of open-domain content; and (c) it is
available for many languages. Community QA sites,
similar to FAQs, provide large number of question-
answer pairs. Nevertheless, this data has a signifi-
cant drawback: it has high variance of quality, i.e.,
answers range from very informative to completely
irrelevant or even abusive. Table 1 shows some ex-
amples of both high and low quality content.
In this paper we address the problem of answer
ranking for non-factoid questions from social media
content. Our research objectives focus on answering
the following two questions:
1. Is it possible to learn an answer ranking model
for complex questions from such noisy data?
This is an interesting question because a posi-
tive answer indicates that a plethora of training
data is readily available to QA researchers and
system developers.
2. Which features are most useful in this sce-
nario? Are similarity models as effective as
models that learn question-to-answer transfor-
mations? Does syntactic and semantic infor-
mation help? For generality, we focus only on
textual features extracted from the answer text
and we ignore all meta data information that is
not generally available.
Notice that we concentrate on one component of a
possible social-media QA system. In addition to
answer ranking, a complete system would have to
search for similar questions already answered (Jeon
et al, 2005), and rank content quality using ?social?
features such as the authority of users (Jeon et al,
2006; Agichtein et al, 2008). This is not the focus of
our work: here we investigate the problem of learn-
ing an answer ranking model capable of dealing with
complex questions, using a large number of, possi-
ble noisy, question-answer pairs. By focusing exclu-
sively on textual content we increase the portability
of our approach to other collections where ?social?
features might not available, e.g., Web search.
The paper is organized as follows. We describe
our approach, including all the features explored for
answer modeling, in Section 2. We introduce the
corpus used in our empirical analysis in Section 3.
We detail our experiments and analyze the results in
Section 4. We overview related work in Section 5
and conclude the paper in Section 6.
AnswerCollection
Answers
Translation
Features
Web Correlation
FeaturesFeatures
Similarity
Answer
RankingQ AnswerRetrieval
(unsupervised)
(discriminative learning)
(class?conditional learning)
Features
Density/Frequency
Figure 1: System architecture.
2 Approach
The architecture of the QA system analyzed in the
paper, summarized in Figure 1, follows that of the
most successful TREC systems. The first com-
ponent, answer retrieval, extracts a set of candi-
date answers A for question Q from a large col-
lection of answers, C, provided by a community-
generated question-answering site. The retrieval
component uses a state-of-the-art information re-
trieval (IR) model to extract A given Q. Since
our focus is on exploring the usability of the an-
swer content, we do not perform retrieval by find-
ing similar questions already answered (Jeon et al,
2005), i.e., our answer collection C contains only
the site?s answers without the corresponding ques-
tions answered.
The second component, answer ranking, assigns
to each answer Ai ? A a score that represents
the likelihood that Ai is a correct answer for Q,
and ranks all answers in descending order of these
scores. The scoring function is a linear combina-
tion of four different classes of features (detailed in
Section 2.2). This function is the focus of the pa-
per. To answer our first research objective we will
compare the quality of the rankings provided by this
component against the rankings generated by the IR
model used for answer retrieval. To answer the sec-
ond research objective we will analyze the contri-
bution of the proposed feature set to this function.
Again, since our interest is in investigating the util-
ity of the answer textual content, we use only infor-
mation extracted from the answer text when learn-
ing the scoring function. We do not use any meta
information (e.g., answerer credibility, click counts,
etc.) (Agichtein et al, 2008; Jeon et al, 2006).
Our QA approach combines three types of ma-
chine learning methodologies (as highlighted in Fig-
ure 1): the answer retrieval component uses un-
720
supervised IR models, the answer ranking is im-
plemented using discriminative learning, and fi-
nally, some of the ranking features are produced
by question-to-answer translation models, which use
class-conditional learning.
2.1 Ranking Model
Learning with user-generated content can involve
arbitrarily large amounts of data. For this reason
we choose as a ranking algorithm the Perceptron
which is both accurate and efficient and can be
trained with online protocols. Specifically, we im-
plement the ranking Perceptron proposed by Shen
and Joshi (2005), which reduces the ranking prob-
lem to a binary classification problem. The general
intuition is to exploit the pairwise preferences in-
duced from the data by training on pairs of patterns,
rather than independently on each pattern. Given a
weight vector ?, the score for a pattern x (a candi-
date answer) is simply the inner product between the
pattern and the weight vector:
f?(x) = ?x, ?? (1)
However, the error function depends on pairwise
scores. In training, for each pair (xi,xj) ? A,
the score f?(xi ? xj) is computed; note that if f
is an inner product f?(xi?xj) = f?(xi)? f?(xj).
Given a margin function g(i, j) and a positive rate ? ,
if f?(xi ? xj) ? g(i, j)? , an update is performed:
?t+1 = ?t + (xi ? xj)?g(i, j) (2)
By default we use g(i, j) = (1i ? 1j ), as a mar-
gin function, as suggested in (Shen and Joshi, 2005),
and find ? empirically on development data. Given
that there are only two possible ranks in our set-
ting, this function only generates two possible val-
ues. For regularization purposes, we use as a final
model the average of all Perceptron models posited
during training (Freund and Schapire, 1999).
2.2 Features
In the scoring model we explore a rich set of features
inspired by several state-of-the-art QA systems. We
investigate how such features can be adapted and
combined for non-factoid answer ranking, and per-
form a comparative feature analysis using a signif-
icant amount of real-world data. For clarity, we
group the features into four sets: features that model
the similarity between questions and answers (FG1),
features that encode question-to-answer transfor-
mations using a translation model (FG2), features
that measure keyword density and frequency (FG3),
and features that measure the correlation between
question-answer pairs and other collections (FG4).
Wherever applicable, we explore different syntactic
and semantic representations of the textual content,
e.g., extracting the dependency-based representation
of the text or generalizing words to their WordNet
supersenses (WNSS) (Ciaramita and Altun, 2006).
We detail each of these feature groups next.
FG1: Similarity Features
We measure the similarity between a question
Q and an answer A using the length-normalized
BM25 formula (Robertson and Walker, 1997). We
chose this similarity formula because, out of all the
IR models we tried, it provided the best ranking at
the output of the answer retrieval component. For
completeness we also include in the feature set the
value of the tf ?idf similarity measure. For both for-
mulas we use the implementations available in the
Terrier IR platform4 with the default parameters.
To understand the contribution of our syntactic
and semantic processors we compute the above sim-
ilarity features for five different representations of
the question and answer content:
Words (W) - this is the traditional IR view where the
text is seen as a bag of words.
Dependencies (D) - the text is represented as a bag
of binary syntactic dependencies. The relative syn-
tactic processor is detailed in Section 3. Dependen-
cies are fully lexicalized but unlabeled and we cur-
rently extract dependency paths of length 1, i.e., di-
rect head-modifier relations (this setup achieved the
best performance).
Generalized dependencies (Dg) - same as above, but
the words in dependencies are generalized to their
WNSS, if detected.
Bigrams (B) - the text is represented as a bag of bi-
grams (larger n-grams did not help). We added this
view for a fair analysis of the above syntactic views.
Generalized bigrams (Bg) - same as above, but the
words are generalized to their WNSS.
4http://ir.dcs.gla.ac.uk/terrier
721
In all these representations we skip stop words
and normalize all words to their WordNet lemmas.
FG2: Translation Features
Berger et al (2000) showed that similarity-based
models are doomed to perform poorly for QA be-
cause they fail to ?bridge the lexical chasm? be-
tween questions and answers. One way to address
this problem is to learn question-to-answer trans-
formations using a translation model (Berger et al,
2000; Echihabi and Marcu, 2003; Soricut and Brill,
2006; Riezler et al, 2007). In our model, we in-
corporate this approach by adding the probability
that the question Q is a translation of the answer A,
P (Q|A), as a feature. This probability is computed
using IBM?s Model 1 (Brown et al, 1993):
P (Q|A) =
?
q?Q
P (q|A) (3)
P (q|A) = (1? ?)Pml(q|A) + ?Pml(q|C) (4)
Pml(q|A) =
?
a?A
(T (q|a)Pml(a|A)) (5)
where the probability that the question term q is
generated from answer A, P (q|A), is smoothed us-
ing the prior probability that the term q is gen-
erated from the entire collection of answers C,
Pml(q|C). ? is the smoothing parameter. Pml(q|C)
is computed using the maximum likelihood estima-
tor. Pml(q|A) is computed as the sum of the proba-
bilities that the question term q is a translation of an
answer term a, T (q|a), weighted by the probability
that a is generated from A. The translation table for
T (q|a) is computed using the EM-based algorithm
implemented in the GIZA++ toolkit5.
Similarly with the previous feature group, we
add translation-based features for the five differ-
ent text representations introduced above. By
moving beyond the bag-of-word representation we
hope to learn relevant transformations of structures,
e.g., from the ?squeaky? ? ?door? dependency to
?spray? ? ?WD-40? in the Table 1 example.
FG3: Density and Frequency Features
These features measure the density and frequency
of question terms in the answer text. Variants of
these features were used previously for either an-
swer or passage ranking in factoid QA (Moldovan
et al, 1999; Harabagiu et al, 2000).
5http://www.fjoch.com/GIZA++.html
Same word sequence - computes the number of non-
stop question words that are recognized in the same
order in the answer.
Answer span - the largest distance (in words) be-
tween two non-stop question words in the answer.
Same sentence match - number of non-stop question
terms matched in a single sentence in the answer.
Overall match - number of non-stop question terms
matched in the complete answer.
These last two features are computed also for the
other four text representations previously introduced
(B, Bg, D, and Dg). Counting the number of
matched dependencies is essentially a simplified
tree kernel for QA (e.g., see (Moschitti et al,
2007)) matching only trees of depth 2. Experiments
with full dependency tree kernels based on several
variants of the convolution kernels of Collins and
Duffy (2001) did not yield improvements. We con-
jecture that the mistakes of the syntactic parser may
be amplified in tree kernels, which consider an ex-
ponential number of sub-trees.
Informativeness - we model the amount of informa-
tion contained in the answer by counting the num-
ber of non-stop nouns, verbs, and adjectives in the
answer text that do not appear in the question.
FG4: Web Correlation Features
Previous work has shown that the redundancy of
a large collection (e.g., the web) can be used for an-
swer validation (Brill et al, 2001; Magnini et al,
2002). In the same spirit, we add features that mea-
sure the correlation between question-answer pairs
and large external collections:
Web correlation - we measure the correlation be-
tween the question-answer pair and the web using
the Corrected Conditional Probability (CCP) for-
mula of Magnini et al (2002): CCP (Q,A) =
hits(Q + A)/(hits(Q) hits(A)2/3) where hits re-
turns the number of page hits from a search engine.
When a query returns zero hits we iteratively relax it
by dropping the keyword with the smallest priority.
Keyword priorities are assigned using the heuristics
of Moldovan et al (1999).
Query-log correlation - as in (Ciaramita et al, 2008)
we also compute the correlation between question-
answer pairs and a search-engine query-log cor-
pus of more than 7.5 million queries, which shares
722
roughly the same time stamp with the community-
generated question-answer corpus. We compute the
Pointwise Mutual Information (PMI) and Chi square
(?2) association measures between each question-
answer word pair in the query-log corpus. The
largest and the average values are included as fea-
tures, as well as the number of QA word pairs which
appear in the top 10, 5, and 1 percentile of the PMI
and ?2 word pair rankings.
3 The Corpus
The corpus is extracted from a sample of the U.S.
Yahoo! Answers logs. In this paper we focus on
the subset of advice or ?how to? questions due to
their frequency and importance in social communi-
ties.6 To construct our corpus, we implemented the
following successive filtering steps:
Step 1: from the full corpus we keep only questions
that match the regular expression:
how (to|do|did|does|can|would|could|should)
and have an answer selected as best either by
the asker or by the participants in the thread.
The outcome of this step is a set of 364,419
question-answer pairs.
Step 2: from the above corpus we remove the questions
and answers of obvious low quality. We im-
plement this filter with a simple heuristic by
keeping only questions and answers that have
at least 4 words each, out of which at least 1 is
a noun and at least 1 is a verb. This step filters
out questions like ?How to be excellent?? and
answers such as ?I don?t know?. The outcome
of this step forms our answer collection C. C
contains 142,627 question-answer pairs.7.
Arguably, all these filters could be improved. For
example, the first step can be replaced by a question
classifier (Li and Roth, 2005). Similarly, the second
step can be implemented with a statistical classifier
that ranks the quality of the content using both the
textual and non-textual information available in the
database (Jeon et al, 2006; Agichtein et al, 2008).
We plan to further investigate these issues which are
not the main object of this work.
6Nevertheless, the approach proposed here is independent
of the question type. We will explore answer ranking for other
non-factoid question types in future work.
7The data will be available through the Yahoo! Webscope
program (research-data-requests@yahoo-inc.com).
The data was processed as follows. The text was
split at the sentence level, tokenized and PoS tagged,
in the style of the Wall Street Journal Penn Tree-
Bank (Marcus et al, 1993). Each word was morpho-
logically simplified using the morphological func-
tions of the WordNet library8. Sentences were an-
notated with WNSS categories, using the tagger of
Ciaramita and Altun (2006)9, which annotates text
with a 46-label tagset. These tags, defined by Word-
Net lexicographers, provide a broad semantic cat-
egorization for nouns and verbs and include labels
for nouns such as food, animal, body and feeling,
and for verbs labels such as communication, con-
tact, and possession. Next, we parsed all sentences
with the dependency parser of Attardi et al (2007)10.
It is important to realize that the output of all men-
tioned processing steps is noisy and contains plenty
of mistakes, since the data has huge variability in
terms of quality, style, genres, domains etc., and do-
main adaptation for the NLP tasks involved is still
an open problem (Dredze et al, 2007).
We used 60% of the questions for training, 20%
for development, and 20% for test. The candidate
answer set for a given question is composed by one
positive example, i.e., its corresponding best answer,
and as negative examples all the other answers re-
trieved in the top N by the retrieval component.
4 Experiments
We evaluate our results using two measures: mean
Precision at rank=1 (P@1) ? i.e., the percentage of
questions with the correct answer on the first posi-
tion ? and Mean Reciprocal Rank (MRR) ? i.e., the
score of a question is 1/k, where k is the position
of the correct answer. We use as baseline the output
of our answer retrieval component (Figure 1). This
component uses the BM25 criterion, the highest per-
forming IR model in our experiments.
Table 2 lists the results obtained using this base-
line and our best model (?Ranking? in the table) on
the testing partition. Since we are interested in the
performance of the ranking model, we evaluate on
the subset of questions where the correct answer is
retrieved by answer retrieval in the top N answers
(similar to Ko et al (2007)). In the table we report
8http://wordnet.princeton.edu
9sourceforge.net/projects/supersensetag
10http://sourceforge.net/projects/desr
723
MRR P@1
N = 10 N = 15 N = 25 N = 50 N = 10 N = 15 N = 25 N = 50
recall@N 26.25% 29.04% 32.81% 38.09% 26.25% 29.04% 32.81% 38.09%
Baseline 61.33 56.12 50.31 43.74 45.94 41.48 36.74 31.66
Ranking 68.72?0.01 63.84?0.01 57.76?0.07 50.72?0.01 54.22?0.01 49.59?0.03 43.98?0.09 37.99?0.01
Improvement +12.04% +13.75% +14.80% +15.95% +18.02% +19.55% +19.70% +19.99%
Table 2: Overall results for the test partition.
results for several N values. For completeness, we
show the percentage of questions that match this cri-
terion in the ?recall@N? row.
Our ranking model was tuned strictly on the de-
velopment set (i.e., feature selection and parame-
ters of the translation models). During training, the
presentation of the training instances is randomized,
which generates a randomized ranking algorithm.
We exploit this property to estimate the variance in
the results produced by each model and report the
average result over 10 trials together with an esti-
mate of the standard deviation.
The baseline result shows that, for N = 15,
BM25 alone can retrieve in first rank 41% of the
correct answers, and MRR tells us that the correct
answer is often found within the first three answers
(this is not so surprising if we remember that in this
configuration only questions with the correct answer
in the first 15 were kept for the experiment). The
baseline results are interesting because they indicate
that the problem is not hopelessly hard, but it is far
from trivial. In principle, we see much room for im-
provement over bag-of-word methods.
Next we see that learning a weighted combina-
tion of features yields consistently marked improve-
ments: for example, for N = 15, the best model
yields a 19% relative improvement in P@1 and 14%
in MRR. More importantly, the results indicate that
the model learned is stable: even though for the
model analyzed in Table 2 we used N = 15 in train-
ing, we measure approximately the same relative im-
provement as N increases during evaluation.
These results provide robust evidence that: (a) we
can use publicly available online QA collections to
investigate features for answer ranking without the
need for costly human evaluation, (b) we can exploit
large and noisy online QA collections to improve the
accuracy of answer ranking systems and (c) readily
available and scalable NLP technology can be used
Iter. Feature Set MRR P@1
0 BM25(W) 56.06 41.12%
1 + translation(Bg) 61.13 46.24%
2 + overall match(D) 62.50 48.34%
3 + translation(W) 63.00 49.08%
4 + query-log avg(?2) 63.50 49.63%
5 + answer span
normalized by A size 63.71 49.84%
6 + query-log max(PMI) 63.87 50.09%
7 + same word sequence 63.99 50.23%
8 + translation(B) 64.03 50.30%
9 + tfidf(W) 64.08 50.42%
10 + same sentence match(W) 64.10 50.42%
11 + informativeness:
verb count 64.18 50.36%
12 + tfidf(B) 64.22 50.36%
13 + same word sequence
normalized by Q size 64.33 50.54%
14 + query-log max(?2) 64.46 50.66%
15 + same sentence match(W)
normalized by Q size 64.55 50.78%
16 + query-log avg(PMI) 64.60 50.88%
17 + overall match(W) 64.65 50.91%
Table 3: Summary of the model selection process.
to improve lexical matching and translation models.
In the remaining of this section we analyze the per-
formance of the different features.
Table 3 summarizes the outcome of our automatic
greedy feature selection process on the development
set. Where applicable, we show within parentheses
the text representation for the corresponding feature.
The process is initialized with a single feature that
replicates the baseline model (BM25 applied to the
bag-of-words (W) representation). The algorithm
incrementally adds to the feature set the feature that
provides the highest MRR improvement in the de-
velopment partition. The process stops when no fea-
tures yield any improvement. The table shows that,
while the features selected span all the four feature
groups introduced, the lion?s share is taken by the
translation features: approximately 60% of the MRR
724
W B Bg D Dg W + W + W + B + W + B + Bg
B B + Bg Bg + D D + Dg
FG1 (Similarity) 0 +1.06 -2.01 +0.84 -1.75 +1.06 +1.06 +1.06 +1.06
FG2 (Translation) +4.95 +4.73 +5.06 +4.63 +4.66 +5.80 +6.01 +6.36 +6.36
FG3 (Frequency) +2.24 +2.33 +2.39 +2.27 +2.41 +3.56 +3.56 +3.62 +3.62
Table 4: Contribution of NLP processors. Scores are MRR improvements on the development set.
improvement is achieved by these features. The fre-
quency/density features are responsible for approx-
imately 23% of the improvement. The rest is due
to the query-log correlation features. This indicates
that, even though translation models are the most
useful, it is worth exploring approaches that com-
bine several strategies for answer ranking.
Note that if some features do not appear in Table 3
it does not necessarily mean that they are useless.
In some cases such features are highly correlated
with features previously selected, which already ex-
ploited their signal. For example, most similarity
features (FG1) are correlated. Because BM25(W)
is part of the baseline model, the selection process
chooses another FG1 feature only much later (iter-
ation 9) when the model is significantly changed.
On the other hand, some features do not provide a
useful signal at all. A notable example in this class
is the web-based CCP feature, which was designed
originally for factoid answer validation and does not
adapt well to our problem. Because the length of
non-factoid answers is typically significantly larger
than in the factoid QA task, we have to discard a
large part of the query when computing hits(Q+A)
to reach non-zero counts. This means that the final
hit counts, hence the CCP value, are generally un-
correlated with the original (Q,A) tuple.
One interesting observation is that the first two
features chosen by our model selection process use
information from the NLP processors. The first cho-
sen feature is the translation probability computed
between the Bg question and answer representations
(bigrams with words generalized to their WNSS
tags). The second feature selected measures the
number of syntactic dependencies from the question
that are matched in the answer. These results pro-
vide empirical evidence that coarse semantic disam-
biguation and syntactic parsing have a positive con-
tribution to non-factoid QA, even in broad-coverage
noisy settings based on Web data.
The above observation deserves a more detailed
analysis. Table 4 shows the performance of our first
three feature groups when they are applied to each
of the five text representations or incremental com-
binations of representations. For each model cor-
responding to a table cell we use only the features
from the corresponding feature group and represen-
tation to avoid the correlation with features from
other groups. We generate each best model using
the same feature selection process described above.
The left part of Table 4 shows that, generally, the
models using representations that include the output
of our NLP processors (Bg, D and Dg) improve over
the baseline (FG1 and W).11 However, comparable
improvements can be obtained with the simpler bi-
gram representation (B). This indicates that, in terms
of individual contributions, our NLP processors can
be approximated with simpler n-gram models in this
task. Hence, is it fair to say that syntactic and se-
mantic analysis is useful for such Web QA tasks?
While the above analysis seems to suggest a neg-
ative answer, the right-hand side of Table 4 tells a
more interesting story. It shows that the NLP anal-
ysis provides complementary information to the n-
gram-based models. The best models for the FG2
and FG3 feature groups are obtained when combin-
ing the n-gram representations with the representa-
tions that use the output of the NLP processors (W +
B + Bg + D). The improvements are relatively small,
but remarkable (e.g., see FG2) if we take into ac-
count the significant scale of the evaluation. This
observation correlates well with the analysis shown
in Table 3, which shows that features using semantic
(Bg) and syntactic (D) representations contribute the
most on top of the IR model (BM25(W)).
11The exception to this rule are the models FG1(Bg) and
FG1(Dg). This is caused by the fact that the BM25 formula
is less forgiving with errors of the NLP processors (due to the
high idf scores assigned to bigrams and dependencies), and the
WNSS tagger is the least robust component in our pipeline.
725
5 Related Work
Content from community-built question-answer
sites can be retrieved by searching for similar ques-
tions already answered (Jeon et al, 2005) and
ranked using meta-data information like answerer
authority (Jeon et al, 2006; Agichtein et al, 2008).
Here we show that the answer text can be success-
fully used to improve answer ranking quality. Our
method is complementary to the above approaches.
In fact, it is likely that an optimal retrieval engine
from social media should combine all these three
methodologies. Moreover, our approach might have
applications outside of social media (e.g., for open-
domain web-based QA), because the ranking model
built is based only on open-domain knowledge and
the analysis of textual content.
In the QA literature, answer ranking for non-
factoid questions has typically been performed by
learning question-to-answer transformations, either
using translation models (Berger et al, 2000; Sori-
cut and Brill, 2006) or by exploiting the redundancy
of the Web (Agichtein et al, 2001). Girju (2003) ex-
tracts non-factoid answers by searching for certain
semantic structures, e.g., causation relations as an-
swers to causation questions. In this paper we com-
bine several methodologies, including the above,
into a single model. This approach allowed us to per-
form a systematic feature analysis on a large-scale
real-world corpus and a comprehensive feature set.
Recent work has showed that structured retrieval
improves answer ranking for factoid questions:
Bilotti et al (2007) showed that matching predicate-
argument frames constructed from the question and
the expected answer types improves answer ranking.
Cui et al (2005) learned transformations of depen-
dency paths from questions to answers to improve
passage ranking. However, both approaches use
similarity models at their core because they require
the matching of the lexical elements in the search
structures. On the other hand, our approach al-
lows the learning of full transformations from ques-
tion structures to answer structures using translation
models applied to different text representations.
Our answer ranking framework is closest in spirit
to the system of Ko et al (2007) or Higashinaka et
al. (2008). However, the former was applied only
to factoid QA and both are limited to similarity, re-
dundancy and gazetteer-based features. Our model
uses a larger feature set that includes correlation and
transformation-based features and five different con-
tent representations. Our evaluation is also carried
out on a larger scale. Our work is also related to that
of Riezler et al (2007) where SMT-based query ex-
pansion methods are used on data from FAQ pages.
6 Conclusions
In this work we described an answer ranking en-
gine for non-factoid questions built using a large
community-generated question-answer collection.
On one hand, this study shows that we can effec-
tively exploit large amounts of available Web data
to do research on NLP for non-factoid QA systems,
without any annotation or evaluation cost. This pro-
vides an excellent framework for large-scale experi-
mentation with various models that otherwise might
be hard to understand or evaluate. On the other hand,
we expect the outcome of this process to help sev-
eral applications, such as open-domain QA on the
Web and retrieval from social media. For example,
on the Web our ranking system could be combined
with a passage retrieval system to form a QA system
for complex questions. On social media, our system
should be combined with a component that searches
for similar questions already answered; this output
can possibly be filtered further by a content-quality
module that explores ?social? features such as the
authority of users, etc.
We show that the best ranking performance
is obtained when several strategies are combined
into a single model. We obtain the best results
when similarity models are aggregated with features
that model question-to-answer transformations, fre-
quency and density of content, and correlation of
QA pairs with external collections. While the fea-
tures that model question-to-answer transformations
provide most benefits, we show that the combination
is crucial for improvement.
Lastly, we show that syntactic dependency pars-
ing and coarse semantic disambiguation yield a
small, yet statistically significant performance in-
crease on top of the traditional bag-of-words and
n-gram representation. We obtain these results us-
ing only off-the-shelf NLP processors that were not
adapted in any way for our task.
726
References
G. Attardi, F. Dell?Orletta, M. Simi, A. Chanev and
M. Ciaramita. 2007. Multilingual Dependency Pars-
ing and Domain Adaptation using DeSR. Proc. of
CoNLL Shared Task Session of EMNLP-CoNLL 2007.
E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G.
Mishne. 2008. Finding High-Quality Content in So-
cial Media, with an Application to Community-based
Question Answering. Proc. of WSDM.
E. Agichtein, S. Lawrence, and L. Gravano. 2001.
Learning Search Engine Specific Query Transforma-
tions for Question Answering. Proc. of WWW.
A. Berger, R. Caruana, D. Cohn, D. Freytag, and V. Mit-
tal. 2000. Bridging the Lexical Chasm: Statistical
Approaches to Answer Finding. Proc. of SIGIR.
M. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. 2007.
Structured Retrieval for Question Answering. Proc. of
SIGIR.
E. Brill, J. Lin, M. Banko, S. Dumais, and A. Ng. 2001.
Data-Intensive Question Answering. Proc. of TREC.
P. Brown, S. Della Pietra, V. Della Pietra, R. Mercer.
1993. The Mathematics of Statistical Machine Trans-
lation: Parameter Estimation. Computational Linguis-
tics, 19(2).
M. Ciaramita and Y. Altun. 2006. Broad Coverage Sense
Disambiguation and Information Extraction with a Su-
persense Sequence Tagger. Proc. of EMNLP.
M. Ciaramita, V. Murdock and V. Plachouras. 2008. Se-
mantic Associations for Contextual Advertising. 2008.
Journal of Electronic Commerce Research - Special Is-
sue on Online Advertising and Sponsored Search, 9(1),
pp.1-15.
M. Collins and N. Duffy. 2001. Convolution Kernels for
Natural Language. Proc. of NIPS 2001.
H. Cui, R. Sun, K. Li, M. Kan, and T. Chua. 2005. Ques-
tion Answering Passage Retrieval Using Dependency
Relations. Proc. of SIGIR.
M. Dredze, J. Blitzer, P. Pratim Talukdar, K. Ganchev,
J. Graca, and F. Pereira. 2007. Frustratingly Hard
Domain Adaptation for Parsing. In Proc. of EMNLP-
CoNLL 2007 Shared Task.
A. Echihabi and D. Marcu. 2003. A Noisy-Channel Ap-
proach to Question Answering. Proc. of ACL.
Y. Freund and R.E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37, pp. 277-296.
R. Girju. 2003. Automatic Detection of Causal Relations
for Question Answering. Proc. of ACL, Workshop on
Multilingual Summarization and Question Answering.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdeanu, R. Bunescu, R. Girju, V. Rus, and P.
Morarescu. 2000. Falcon: Boosting Knowledge for
Answer Engines. Proc. of TREC.
R. Higashinaka and H. Isozaki. 2008. Corpus-based
Question Answering for why-Questions. Proc. of IJC-
NLP.
J. Jeon, W. B. Croft, and J. H. Lee. 2005. Finding Simi-
lar Questions in Large Question and Answer Archives.
Proc. of CIKM.
J. Jeon, W. B. Croft, J. H. Lee, and S. Park. 2006. A
Framework to Predict the Quality of Answers with
Non-Textual Features. Proc. of SIGIR.
J. Ko, T. Mitamura, and E. Nyberg. 2007. Language-
independent Probabilistic Answer Ranking. for Ques-
tion Answering. Proc. of ACL.
X. Li and D. Roth. 2005. Learning Question Classifiers:
The Role of Semantic Information. Natural Language
Engineering.
B. Magnini, M. Negri, R. Prevete, and H. Tanev. 2002.
Comparing Statistical and Content-Based Techniques
for Answer Validation on the Web. Proc. of the VIII
Convegno AI*IA.
M.P. Marcus, B. Santorini and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn TreeBank. Computational Linguis-
tics, 19(2), pp. 313-330.
D. Moldovan, S. Harabagiu, M. Pasca, R. Mihalcea, R.
Goodrum, R. Girju, and V. Rus. 1999. LASSO - A
Tool for Surfing the Answer Net. Proc. of TREC.
A. Moschitti, S. Quarteroni, R. Basili and S. Manand-
har. 2007. Exploiting Syntactic and Shallow Semantic
Kernels for Question/Answer Classification. Proc. of
ACL.
S. Robertson and S. Walker. 1997. On relevance Weights
with Little Relevance Information. Proc. of SIGIR.
R. Soricut and E. Brill. 2006. Automatic Question An-
swering Using the Web: Beyond the Factoid. Journal
of Information Retrieval - Special Issue on Web Infor-
mation Retrieval, 9(2).
L. Shen and A. Joshi. 2005. Ranking and Reranking
with Perceptron, Machine Learning. Special Issue on
Learning in Speech and Language Technologies, 60(1-
3), pp. 73-96.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal
and Y. Liu. 2007. Statistical Machine Translation
for Query Expansion in Answer Retrieval. In Proc.
of ACL.
727
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 291?298, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Learning What to Talk About in Descriptive Games
Hugo Zaragoza
Microsoft Research
Cambridge, United Kingdom
hugoz@microsoft.com
Chi-Ho Li
University of Sussex
Brighton, United Kingdom
C.H.Li@sussex.ac.uk
Abstract
Text generation requires a planning mod-
ule to select an object of discourse and its
properties. This is specially hard in de-
scriptive games, where a computer agent
tries to describe some aspects of a game
world. We propose to formalize this prob-
lem as a Markov Decision Process, in
which an optimal message policy can be
defined and learned through simulation.
Furthermore, we propose back-off poli-
cies as a novel and effective technique to
fight state dimensionality explosion in this
framework.
1 Introduction
Traditionally, text generation systems are decom-
posed into three modules: the application module
which manages the high-level task representation
(state information, actions, goals, etc.), the text plan-
ning module which chooses messages based on the
state of the application module, and the sentence
generation module which transforms messages into
sentences. The planning module greatly depends
on the characteristics of both the application and
the generation modules, solving issues in domain
modelling, discourse and sentence planning, and to
some degree lexical and feature selection (Cole et
al., 1997). In this paper we concentrate on one
of the most basic tasks that text planning needs to
solve: selecting the message content, or more sim-
ply, choosing what to talk about.
Work on text-generation often assumes that an
object or topic has been already chosen for discus-
sion. This is reasonable for many applications, but
in some cases choosing what to talk about can be
harder than choosing how to. This is the case in the
type of text generation applications that we are in-
terested in: generating descriptive messages in com-
puter games. In a modern computer game at any
given moment there may be an enormous number
of object properties that can be described, each with
varying importance and consequences. The outcome
of the game depends not only on the skill of the
player, but also on the quality of the descriptive mes-
sages produced. We refer to such situations as de-
scriptive games.
Our goal is to develop a strategy to choose the
most interesting descriptive messages that a particu-
lar talker may communicate to a particular listener,
given their context (i.e. their knowledge of the world
and of each-other). We refer to this as message plan-
ning.
Developing a general framework for planning is
very difficult because of the strong coupling be-
tween the planning and application modules. We
propose to frame message planning as a Markov De-
cision Process (MDP) which encodes the environ-
ment, the information available to the talker and lis-
tener, the consequences of their communicative and
non-communicative acts, and the constraints of the
text generation module. Furthermore we propose to
use Reinforcement Learning (RL) to learn the op-
timal message policy. We demonstrate the overall
principle (Section 2) and then develop in more de-
tail a computer game setting (Section 3).
291
One of the main weaknesses of RL is the problem
of state dimensionality explosion. This problem is
specially acute in message planning, since in typical
situations there can be hundreds of thousands of po-
tential messages. At the same time, the domain is
highly structured. We propose to exploit this struc-
ture using a form of the back-off smoothing princi-
ple on the state space (Section 4).
1.1 Related Work
Our problem setting can be seen as a generalisation
of the content selection problem in the generation of
referring expressions in NLG. In the standard set-
ting of this problem (see for example (van Deemter
and Krahmer, to appear)) an algorithm needs to se-
lect the distinguishing description of an object in a
scene. This description can be seen as a subset of
scene properties which i) uniquely identifies a given
target object, and ii) is optimal in some sense (min-
imal, psychologically plausible, etc.) van Deemter
and Krahmer show that most content selection algo-
rithms can be described as different cost functions
over a particular graph representation of the scene.
Minimising the cost of a subgraph leads to a distin-
guishing description.
Some aspects of our work generalise that of con-
tent selection: i) we consider the target object is un-
known, ii) we consider scenes (i.e. world states) that
are dynamic (i.e. they change over time) and reac-
tive (i.e. utterances change the world), and iii) we
consider listeners that have partial knowledge of the
scene. This has important consequences. For exam-
ple, the cost of a description cannot be directly eval-
uated; instead, we must play the game, that is, gener-
ate utterances and observe the rewards obtained over
time. Also identical word-states may lead to differ-
ent optimal messages, depending on the listener?s
partial knowledge. Other aspects of our work are
very simplistic compared to current work in con-
tent selection, for example with respect to the use
of negation and of properties that are boolean, rel-
ative or graded (van Deemter and Krahmer, to ap-
pear). We hope to incorporate these ideas into our
work soon.
Probabilistic dialogue policies have been previ-
ously proposed for spoken dialogue systems (SDS)
(see for example (Singh et al, 2002; Williams et
al., 2005) and references therein). However, work in
SDS focus mainly on coping with the noise and un-
certainty resulting from speech recognition and sen-
tence parsing. In this context MDPs are used to infer
features and plan communicative strategies (modal-
ity, confusion, initiative, etc.) In our work we do not
need to deal with uncertainty or parsing; our main
concern is in the selection of the message content.
In this sense our work is closer to (Henderson et al,
2005), where RL is used to train a SDS with very
many states encoding message content.
Finally, with respect to the state-explosion prob-
lem in RL, related work can be found in the areas of
multi-task learning and robot motion planning (Diet-
terich, 2000, and references therein). In these works
the main concern is identifying the features that are
relevant to specific sub-tasks, so that robots may
learn multiple loosely-coupled tasks without incur-
ring state-explosion. (Henderson et al, 2005) also
addresses this problem in the context of SDS and
proposes a semi-supervised solution. Our approach
is related to these works, but it is different in that
we assume that the feature structure is known in ad-
vance and has a very particular form amenable to a
form of back-off regularisation.
2 Message planning
Let us consider an environment comprising a world
with some objects and some agents, and some dy-
namics that govern their interaction. Agents can ob-
serve and memorize certain things about the world,
can carry out actions and communicate with other
agents. As they do so, they are rewarded or pun-
ished by the environment (e.g. if they find food, if
the complete some goal, if they run out of energy,
etc.)
The agents? actions are governed by a policy. We
will consider separately the physical action policy
(?), which decides which physical action to take
given the state of the agent, and the message action
policy (?), which decides when to communicate, to
whom, and what about. Our main concern in this
paper will be to learn an optimal ?. Before we de-
fine this goal more precisely, we will introduce some
notation.
A property is a set of attribute-value pairs. An
object is a set of properties, with (at least) attributes
Type and Location. A domain is a set of objects. Fur-
292
thermore, we say that s? is a sub-domain of s if s? can
be obtained by deleting property?value pairs from
s (while enforcing the condition that remaining ob-
jects must have Type and Location). Sub(s) is the set
containing s, all sub-domains of s, and the empty
domain ?.
A world state can be represented as a domain,
noted s
W
. Any partial view of the world state can
also be represented as a domain s ? Sub(s
W
). Sim-
ilarly the content of any descriptive message about
the world, noted m, can be represented as a partial
view of it. An agent is the tuple:
A :=
(
s
A
, ?
A
, {?
AA
?
, s
AA
?}
A
? 6=A
)
? s
A
? Sub(s
W
): knowledge that A has about
the state of the world.
? s
AA
? ? Sub(s
A
? s?
A
): knowledge that A
has about the knowledge that A? has about the
world.
? ?
a
:= P (c|s
A
) is the action policy of A, and c
is a physical action.
? ?
AA
? := P (m ? M(s
A
)|s
A
, s
AA
?) is the mes-
sage policy of A for sending messages to A?,
and M(s
A
) are all valid messages at state s
A
(discussed in Section 2.3).
When an agent A decides to send a message to A?,
it can use its knowledge of A? to choose messages
effectively. For example, A will prefer to describe
things that it knows A? does not know (i.e. not in
s
AA
?). This is the reason why the message policy
?
A
depends on both s
A
and s
AA
? . After a message is
sent (i.e. realised and uttered) the agent?s will update
their knowledge states s
A
? , s
A
?
A
and s
AA
? .
The question that we address in this paper is that
of learning an optimal message policy ?
AA
? .
2.1 Talker?s Markov Decision Process
We are going to formalize this problem as a stan-
dard Markov Decision Process (MDP). In general a
MDP (Sutton and Barto, 1998) is defined over some
set of states S := {s
i
}
i=1..K
and actions associated
to every state, A(s
i
) := {a
ij
}
j=1..N
i
. The envi-
ronment is governed by the state transition function
Pa
ss
?
:= P (s?|s, a). A policy determines the likeli-
hood of actions at a given state: ?(s) := P (a|s). At
each state transition a reward is generated from the
reward function Ra
ss
?
:= E{r|s, s?, a}.
MDPs allow us to define and find optimal poli-
cies which maximise the expected reward. Classical
MDPs assume that the different functions introduced
above are known and have some tractable analyti-
cal form. Reinforcement Learning (RL) in as ex-
tension of MDPs in which the environment function
Pa
ss
?
is unknown or complex, and so the optimal pol-
icy needs to be learned online by directly interacting
with the environment. There exist a number of algo-
rithms to solve a RL problem, such as Q-Learning
or SARSA (Sutton and Barto, 1998).
We can use a MDP to describe a full descrip-
tive game, in which several agents interact with the
world and communicate with each-other. To do so
we would need to consider composite states con-
taining s
W
, {s
A
}
A
, and
{
{s
AA
?}
A
? 6=A
}
A
. Simi-
larly, we need to consider composite policies con-
taining {?
A
}
A
and
{
(?
AA
?)
A
? 6=A
}
A
. Finally, we
would consider the many constrains in this model;
for example: only physical actions affect the state
of the world, only message actions affect believes,
and only believe states can affect the choice of the
agent?s actions.
MDPs provide us with a principled way to deal
with these elements and their relationships. How-
ever, dealing with the most general case results in
models that are very cumbersome and which hide
the conceptual simplicity of our approach. For this
reason, we will limit ourselves in this paper to one
of the simplest communication cases of interest: a
single all-knowing talker, and a single listener com-
pletely observed by the talker. We will discuss later
how this can be generalized.
2.2 The Talking God Setting
In the simplest case, an all-knowing agent A
0
sits in
the background, without taking any physical actions,
and uses its message policy (?
01
) to send messages
to a listener agent A
1
. The listener agent cannot talk
back, but can interact with the environment using
its physical action policy ?
1
. Rewards obtained by
A
1
are shared by both agents. We refer to this set-
ting as the talking God setting. Examples of such
situations are common in games, for example when
a computer character talks to its (computer) team-
293
w w?
s1 s?1
m0
a1
s s?
a
r
Figure 1: Talking God MDP.
mates, or when a mother-ship with full information
of the ground sends a small simple robot to do a task.
Another example would be that of a teacher talking
to a learner, except that the teacher may not have full
information of the learners head!
Since the talker is all-knowing, it follows that
s
0
= s
W
and s
01
= s
1
. Furthermore, since the
talker does not take physical actions, ?
0
does not
need to be defined. Similarly, since the listener does
not talk we do not need to define ?
10
or s
10
. This
case is depicted in Figure 1 as a graphical model.
By grouping states and actions (dotted lines) we can
see that this is can be modelled as a standard MDP.
If all the probability distributions are known analyt-
ically, or if they can be sampled, optimal physical
and message policies can be learnt (thick arrows).
Several generalizations of this model are possible.
A straight forward generalization is to consider more
than one listener agent. We can then choose to learn
a single policy for all, or individual policies for each
agent.
A second way to generalize the setting is to make
the listeners mind only partially observable to the
talker. In this case the talker continues to know the
entire world (s
0
= s
W
), but does not know ex-
actly what the listener knows (s
01
6= s
0
). This is
more realistic in situations in which the listener can-
not talk back to the talker, or in which the talkers
mind is not observable. However, to model this we
need a partially observable MDP (POMDP). Solv-
ing POMDPS is much harder than solving MDPs,
but there have been models proposed for dialogue
management (Williams et al, 2005).
In the more general case, the talker would have
partial knowledge of the world and of the listener,
and would itself act. In that case all agents are equal
and can communicate as they evolve in the envi-
ronment. The other agents minds are not directly
observable, but we obtain information about them
from their actions and their messages. This can all
be in principle modelled by POMDPs in a straight-
forward manner, although solving these models is
more involved. We are currently working towards
doing so.
Finally, we note that all the above cases have
dealt with worlds in which objects are static (i.e.
information does not become obsolete), agents do
not gain or communicate erroneous information, and
communication itself is non-ambiguous and loss-
less. This is a realistic scenario for text generation,
and for communication between computer agents in
games, but it is far removed from the spoken dia-
logue setting.
2.3 Generation Module and Valid Messages
Generating descriptive sentences of domains can be
done in a number of ways, from template to feature-
based systems (Cole et al, 1997). Our framework
does not depend on a particular choice of generation
module, and so we do not need to discuss this mod-
ule. However, our message policy is not decoupled
of the generation module; indeed, it would not make
sense to develop a planning module which plans
messages that cannot be realised! In our framework,
the generation module is seen simply as a fixed and
known filter over all possible the messages.
We formalize this by representing an agent?s gen-
eration module as a function ?
A
(m) mapping a mes-
sage m to a NL sentence, or to ? if the module can-
not fully realise m. The set of available messages
to an agent A in state s
A
is therefore: M(s
A
) :=
{m |m ? Sub(s
A
) , ?
A
(m) 6= ?}.
3 A Simple Game Example
In this section we will use a simple computer game
to demonstrate how the proposed framework can be
used to learn message policies.
The game evolves in a grid-world. A mother-
ship sends a scout, which will try to move from its
294
Figure 2: Example of a Simple Game Board.
starting position (top left corner) to a target (bot-
tom right). There are two types of objects on the
board, Type := {bomb, tree}, with a property Size :=
{big, small} in addition of Location. If a scout at-
tempts to move into a big tree, the move is blocked;
small trees have no effect. If a scout moves into
a bomb the scout is destroyed and a new one is
created at the starting position. Before every step
the mother-ship may send a message to the scout.
Then the scout moves one step (horizontal or ver-
tical) towards the target choosing the shortest path
which avoids hazards known by the scout (the A*
algorithm is used for this). Initially scouts have no
knowledge of the objects in the world; they gain this
knowledge by stepping into objects or by receiving
information from the mother-ship.
This is an instance of the talking god model dis-
cussed previously. The scout is the listener agent
(A
1
), and the mother-ship the talker (A
0
). The
scouts action policy ?
1
is fixed (as described above),
but we need to learn the message policy ?
01
.
Rewards are associated with the results of phys-
ical actions: a high positive reward (1000) is as-
signed to reaching the destination, a large negative
reward (-100) to stepping in a bomb, a medium neg-
ative reward (-10) to being blocked by a big tree, a
small negative reward to every step (-1). Further-
more, sending a message has a small negative re-
ward proportional to the number of attributes men-
tioned in the message (-2 per attribute, to discourage
the talker from sending useless information). The
message ? is given zero cost; this is done in order to
200 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
250
300
350
400
450
500
550
600
650
Training Cycles
R
ew
ar
d
optimal properties
all properties
TYPE only
Figure 3: Simple Game Learning Results
State Best Action Learnt
(and possible sentence realisation)
{ TREE-BIG-LEFT } ?
-SILENCE-
{ BOMB-BIG-FRONT } BOMB-FRONT
There is a bomb in front of you
{ TREE-SMALL-LEFT, TREE-BIG-RIGHT
TREE-BIG-RIGHT } There is a big tree to your right
{ BOMB-BIG-FRONT,
BOMB-SMALL-LEFT, TREE-BIG-RIGHT
TREE-BIG-RIGHT, There is a big tree to your right
TREE-SMALL-BACK }
Table 1: Examples of learnt actions.
learn when not to talk.
Learning is done as follows. We designed five
maps of 11 ? 11 cells, each with approximately 15
bombs and 20 trees of varying sizes placed in strate-
gic locations to make the scouts task difficult (one
of these maps is depicted in Figure 2; an A* path
without any knowledge and one with full knowl-
edge of the board are shown as dotted and dashed ar-
rows respectively). A training epoch consists of ran-
domly drawing one of these maps and running a sin-
gle game until completion. The SARSA algorithm
is used to learn the message policy, with  = 0.1
and ? = 0.9. The states s
W
and s
1
are encoded
to represent the location of objects surrounding the
scout, relative to its direction (i.e. objects directly in
front of the agent always receive the same location
value). To speed up training, we only consider the 8
cells adjacent to the agent.
Figure 3 shows the results of these experiments.
For comparison, we note that completing the game
295
with a uniformly random talking policy results in an
average reward of less than ?3000 meaning that on
average more than 30 scouts die before the target is
reached. The dashed line indicates the reward ob-
tained during training for a policy which does not
use the size attribute, but only type and location.
This policy effectively learns that both bombs and
trees in front of the agent are to be communicated,
resulting in an average reward of approximately 400,
and reducing the average number of deaths to less
than 2. The solid line represents the results obtained
by a policy that is forced to use all attributes. De-
spite the increase in communication cost, this pol-
icy can distinguish between small and large trees,
and so it increases the overall reward two-fold. Fi-
nally, the dotted line represents the results obtained
by a policy that can choose whether to use or not the
size attribute. This policy proves to be even more
effective than the previous one; this means that it
has learnt to use the size attribute only when it is
necessary. Some optimal (state,action) pairs learnt
for this policy are shown in Table 1. The first three
show correctly learnt optimal actions. The last is an
example of a wrongly learnt action, due to the state
being rare.
These are encouraging results, since they demon-
strate in practice how optimal policies may be learnt
for message planning. However, it should be clear
form this example that, as we increase the number
of types, attributes and values, this approach will be-
come unfeasible. This is discussed in the next sec-
tion.
4 Back-Off Policies
One of the main problems when using RL in prac-
tical settings (and, more generally, using MDPs) is
the exponential growth of the state space, and con-
sequently of the learning time required. In our case,
if there are M attributes, and each attribute p
i
has
N(p
i
) values, then there are S =
?
M
i=1
N(p
i
) pos-
sible sub-domains, and up to 2S states in the state
space. This exponential growth, unless addressed,
will render MDP learning unfeasible.
NL domains are usually rich with structure, some
of it which is known a priori. This is the case in
text generation of descriptions for computer games,
where we have many sources of information about
the objects of discourse (i.e. world ontology, dy-
namics, etc.) We propose to tackle the problem of
state dimensionality explosion by using this struc-
ture explicitly in the design of hierarchical policies.
We do so by borrowing the back-off smoothing
idea from language models. This idea can be stated
as: train a set of probability models, ordered by their
specificity, and make predictions using the most spe-
cific model possible, but only if there is enough
training data to support its prediction; otherwise,
back-off to the next less-specific model available.
Formally, let us assume that for every state
s we can construct a sequence of K embedded
partial representations of increasing complexity,
(s
[1]
, . . . , s
[k]
, . . . , s
[K]
). Let us denote ??
[k]
a se-
quence of policies operating at each of the partial
representation levels respectively, and let each of
these policies have a confidence measurement c
k
(s)
indicating the quality of the prediction at each state.
Since k indicates increasingly complex, we require
that c
k
(s) ? c
k
?(s) if k < k?. Then, the most spe-
cific policy we can use at state s can be written as:
k
?
s
:= arg max
k
{k ? sign (c
k
(s) ? ?)} (1)
A back-off policy can be implemented by choosing,
at every state s the most specific policy available:
?(s) = ??
[k
?
s
]
(s
[k
?
s
]
) (2)
We can use a standard off-policy learning algo-
rithm (such as Q-learning or SARSA) to learn all the
policies simultaneously. At every step, we draw an
action using (2) and update all policies with the ob-
tained reward1. Initially, the learning will be driven
by high-level (simple) policies. More complex poli-
cies will kick-in progressively for those states that
are encountered more often.
In order to implement back-off policies for our
setting, we need to define a confidence function c
k
.
A simple confidence measure is the number of times
the state s
[k]
has been previously encountered. This
measure grows on average very quickly for small k
states and slowly for high k states. Nevertheless, re-
occurring similar states will have high visit counts
1An alternative view of back-off policies is to consider that a
single complete policy is being learnt, but that actions are being
drawn from regularised versions of this policy, where the regu-
larisation is a back-off model on the features. We show this in
Appendix I
296
0 1000 2000 3000 4000 5000
500
550
600
650
700
750
800
850
training epochs
A
ve
ra
ge
 T
ot
al
 R
ew
ar
d 
(1
00
 R
un
s)
Full State, without noise objects
Full State, with 40 noise objects
Simple State, without noise objects
Simple State, with 40 noise objects
Back?Off with 40 noise objects
Figure 4: Back-Off Policy Simulation Results.
for all k values. This is exactly the kind of behav-
iour we require.
Furthermore, we need to choose a set of repre-
sentations of increasing complexity. For example,
in the case of n-gram models it is natural to choose
as representations sequences of preceding words of
increasing size. There are many choices open to us
in our application domain. A natural choice is to or-
der attribute types by their importance to the task.
For example, at the simplest level of representation
objects can be represented only by their type, at a
second level by the type and colour, and at a third
level by all the attributes. This same technique could
be used to exploit ontologies and other sources of
knowledge. Another way to create levels of repre-
sentation of increasing detail is to consider different
perceptual windows. For example, at the simplest
level the agent can consider only objects directly in
front of it, since these are generally the most im-
portant when navigating. At a second level we may
consider also what is to the left and right of us, and
finally consider all surrounding cells. This could be
pursued even further by considering regions of in-
creasing size.
4.1 Simulation Results
We present here a series of experiments based on
the previous game setting, but further simplified to
pinpoint the effect of dimensionality explosion, and
how back-off policies can be used to mitigate it.
We modify the simple game of Section 3 as fol-
lows. First, we add a new object type, stone, and a
new property Colour := {red, green}. We let al trees
be green and big and all bombs red and small, and
furthermore we fix their location (i.e. we use one
map instead of five). Finally we change the world
behaviour so that an agent that steps into a bomb re-
ceives the negative reward but does not die, it contin-
ues until it reaches the target. All these changes are
done to reduce the variability of our learning base-
line.
At every game we generate 40 stones of random
location, size and colour. Stepping on stones has no
physical effect to the scout and it generates the same
reward as moving into an empty cell, but this is un-
known to the talker and will need to be learnt. These
stones are used as noise objects, which increase the
size of the state space. When there are no noise ob-
jects, the number of possible states is 38 ? 6.5K
(the actual number of states will be much smaller
since there is a single maze). Noise objects can take
2 ? 2 = 4 possible forms, so the total number of
states with noise objects is (3 + 4)8 ? 6M . Even
with such a simplistic example we can see how dras-
tic the state dimensionality problem is. Despite the
fact that the noise objects do not affect the reward
structure of our simple game, reinforcement learn-
ing will be drastically slowed down by them.
Simulation results2 are shown in Figure 4. First
let us look at the results obtained using the full state
representation used in Section 3 (noted Full State).
Solid and dotted lines represent runs obtained with
and without noise objects. First note that learning
without noise objects (dotted circles) occurs mostly
within the first few epochs and settles after 250
epochs. When noise objects are added (solid cir-
cles) learning greatly slows down, taking over 5K
epochs. This is a typical illustration of the effect that
the number of states has on the speed of learning.
An obvious way to limit the number of states is
to eliminate features. For comparison, we learned
a simple representation policy with states encod-
ing only the type of the object directly in front of
the agent, ignoring its colour and all other locations
(noted Simple State). Without noise, the performance
(dotted triangles) is only slightly worse than that of
the original policy. However, when noise objects
2Every 200 training epochs we run 100 validation epochs
with  = 0. Only the average validation rewards are plotted.
297
are added (solid triangles) the training is no longer
slowed down. In fact, with noise objects this policy
outperforms the original policy up to epoch 1000:
the performance lost in the representation is made
up by the speed of learning.
We set up a back-off policy with K = 3 as fol-
lows. We use the Simple representation at k = 1,
plus a second level of representation where we rep-
resent the colour as well as the type of the object in
front of the agent, and finally the Full representation
as the third level. As the c
k
function we use state
visit counts as discussed above and we set ? = 10.
Before reaching the full policy (level 3), this policy
should progressively learn to avoid bombs and trees
directly in front (level 1), then (level 2) not avoid
small trees directly in front. We plot the perfor-
mance of this back-off policy (stars) in Figure 4. We
see that it attains very quickly the performance of
the simple policy (in less than 200 epochs), but the
continues to increase in performance settling within
500 epochs with a performance superior to that of
the full state representation, and very close to that of
the policies operating in the noiseless world.
Despite the small scale of this study, our results
clearly suggest that back-off policies can be used
effectively to control state dimensionality explosion
when we have strong prior knowledge of the struc-
ture of the state space. Furthermore (and this may be
very important in real applications such as game de-
velopment) we find that back-off policies produce a
natural to feel to the errors incurred while learning,
since policies develop progressively in their com-
plexity.
5 Conclusion
We have developed a formalism to learn interac-
tively the most informative message content given
the state of the listener and the world. We formalised
this problem as a MDP and shown how RL may be
used to learn message policies even when the envi-
ronment dynamics are unknown. Finally, we have
shown the importance of tackling the problem of
state dimensionality explosion, and we have pro-
posed one method to do so which exploits explicit
a priori ontological knowledge of the task.
References
R. Cole, J. Mariani, H. Uszkoreit, A. Zaenen, and V. Zue.
1997. Survey of the State of the Art in Human Lan-
guage Technology. Cambridge University Press.
T. G. Dietterich. 2000. Hierarchical reinforcement learn-
ing with the MAXQ value function decomposition.
Journal of Artificial Intelligence Research, 13:227?
303.
J. Henderson, O. Lemon, and K. Georgila. 2005. Hybrid
reinforcement/supervised learning for dialogue poli-
cies from communicator data. In 4th IJCAI Workshop
on Knowledge and Reasoning in Practical Dialogue
Systems.
S. Singh, D. Litmanand, M. Kearns, and M. Walker.
2002. Optimizing dialogue management with re-
inforcement learning: Experiments with the njfun
system. Journal of Artificial Intelligence Research,
16:105?133.
R. S. Sutton and A. G. Barto. 1998. Reinforcement
Learning. MIT Press.
K. van Deemter and E. Krahmer. (to appear). Graphs and
booleans. In Computing Meaning, volume 3 of Stud-
ies in Linguistics and Philosophy. Kluwer Academic
Publishers.
J. D. Williams, P. Poupart, and S. Young. 2005. Fac-
tored partially observable markov decision processes
for dialogue management. In 4th IJCAI Workshop on
Knowledge and Reasoning in Practical Dialogue Sys-
tems.
6 Appendix I
We show here that the expected reward for a partial
policy ?
k
after an action a, noted Q?k(s, a), can be
obtained from the expected reward of the full pol-
icy Q?(s, a) and the conditional state probabilities
P (s|s
[k]
). We may use this to compute the expected
risk of any partial policy R?k(s) from the full policy.
Let T
k
(s) :=
{
s
? ? S | s?
[k]
= s
[k]
}
be the sub-
set of full states which map to the same value of s.
Given a state distribution P (s) we can define distri-
butions over partial states:
P (s
[k]
, s
[j]
) =
?
s
??T
k
(s)?T
j
(s)
P (s?) . (3)
Since
?
s
??T
k
(s)
P (s?|s
[k]
) = 1, we have
P (A|s
[k]
) =
?
s
??T
k
(s)
P (A|s?)P (s?|s
[k]
), and so:
Q
?
k(s, a) =
?
s
??T
k
(s)
P (s?|s
[k]
)Q?(s?, a) . (4)
298
