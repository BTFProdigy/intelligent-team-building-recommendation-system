Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 155?158,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
First Steps towards Multi-Engine Machine Translation
Andreas Eisele
Computational Linguistics
Saarland University P.O.Box 151150
D-66041 Saarbru?cken, Germany
eisele@coli.uni-saarland.de
Abstract
We motivate our contribution to the shared
MT task as a first step towards an inte-
grated architecture that combines advan-
tages of statistical and knowledge-based
approaches. Translations were generated
using the Pharaoh decoder with tables de-
rived from the provided alignments for all
four languages, and for three of them us-
ing web-based and locally installed com-
mercial systems. We then applied statis-
tical and heuristic algorithms to select the
most promising translation out of each set
of candidates obtained from a source sen-
tence. Results and possible refinements
are discussed.
1 Motivation and Long-term Perspective
?The problem of robust, efficient and reliable
speech-to-speech translation can only be cracked
by the combined muscle of deep and shallow pro-
cessing approaches.? (Wahlster, 2001) Although this
statement has been coined in the context of VerbMo-
bil, aiming at translation for direct communication,
it appears also realistic for many other translation
scenarios, where demands on robustness, coverage,
or adaptability on the input side and quality on the
output side go beyond today?s technological possi-
bilities. The increasing availability of MT engines
and the need for better quality has motivated con-
siderable efforts to combine multiple engines into
one ?super-engine? that is hopefully better than any
of its ingredients, an idea pionieered in (Frederking
and Nirenburg, 1994). So far, the larger group of
related publications has focused on the task of se-
lecting, from a set of translation candidates obtained
from different engines, one translation that looks
most promising (Tidhar and Ku?ssner, 2000; Akiba et
al., 2001; Callison-Burch and Flournoy, 2001; Ak-
iba et al, 2002; Nomoto, 2004). But also the more
challenging problem of decomposing the candidates
and re-assembling from the pieces a new sentence,
hopefully better than any of the given inputs, has
recently gained considerable attention (Rayner and
Carter, 1997; Hogan and Frederking, 1998; Banga-
lore et al, 2001; Jayaraman and Lavie, 2005).
Although statistical MT approaches currently
come out as winners in most comparative evalua-
tions, it is clear that the achievable quality of meth-
ods relying purely on lookup of fixed phrases will be
limited by the simple fact that for any given combi-
nation of topic, application scenario, language pair,
and text style there will never be sufficient amounts
of pre-existing translations to satisfy the needs of
purely data-driven approaches.
Rule-based approaches can exploit the effort that
goes into single entries in their knowledge reposi-
tories in a broader way, as these entries can be un-
folded, via rule applications, into large numbers of
possible usages. However, this increased generality
comes at significant costs for the acquisition of the
required knowledge, which needs to be encoded by
specialists in formalisms requiring extensive train-
ing to be used. In order to push the limits of today?s
MT technology, integrative approaches will have to
be developed that combine the relative advantages of
155
both paradigms and use them to compensate for their
disadvantages. In particular, it should be possible
to turn single instances of words and constructions
found in training data into internal representations
that allow them to be used in more general ways.
In a first step towards the development of inte-
grated solutions, we need to investigate the relative
strengths and weaknesses of competing systems on
the level of the target text, i.e. find out which sen-
tences and which constructions are rendered well
by which type of engine. In a second step, such
an analysis will then make it possible to take the
outcomes of various engines apart and re-assemble
from the building blocks new translations that avoid
errors made by the individual engines, i.e. to find in-
tegrated solutions that improve over the best of the
candidates they have been built from. Once this can
be done, the third and final step will involve feed
back of corrections into the individual systems, such
that differences between system behaviour can trig-
ger (potentially after manual resolution of unclear
cases) system updates and mutual learning.
In the long term, one would hope to achieve a
setup where a group of MT engines can converge
to a committee that typically disagrees only in truly
difficult cases. In such a committee, remaining dis-
sent between the members would be a symptom of
unresolved ambiguity, that would warrant the cost
of manual intervention by the fact that the system as
a whole can actually learn from the additional ev-
idence. We expect this setup to be particularly ef-
fective when existing MT engines have to be ported
to new application domains. Here, a rule-based en-
gine would be able to profit from its more generic
knowledge during the early stages of the transition
and could teach unseen correspondences of known
words and phrases to the SMT engine, whereas the
SMT system would bring in its abilities to apply
known phrase pairs in novel contexts and quickly
learn new vocabulary from examples.
2 Collecting Translation Candidates
2.1 Setting up Statistical MT
In the general picture laid out in the preceding sec-
tion, statistical MT plays an important role for sev-
eral reasons. On one hand, the construction of a rel-
atively well-performing phrase-based SMT system
from a given set of parallel corpora is no more overly
difficult, especially if ? as in the case in this shared
task ? word alignments and a decoder are provided.
Furthermore, once the second task in our chain will
have been surmounted, it will be relatively easy to
feed back building blocks of improved translations
into the phrase table, which constitutes the central
resource of the SMT system Therefore, SMT facili-
tates experiments aiming at dynamic and interactive
adaptation, the results of which should then also be
applicable to MT engines that represent knowledge
in a more condensed form.
In order to collect material for testing these ideas,
we constructed phrase tables for all four languages,
following roughly the procedure given in (Koehn,
2004) but deviating in one detail related to the treat-
ment of unaligned words at the beginning or end of
the phrases1. We used the Pharaoh decoder as de-
scribed on http://www.statmt.org/wpt05/mt-shared-
task/ after normalization of all tables to lower case.
2.2 Using Commercial Engines
As our main interest is in the integration of statis-
tical and rule-based MT, we tried to collect results
from ?conventional? MT systems that had more or
less uniform characteristics across the languages in-
volved. We could not find MT engines supporting all
four source languages, and therefore decided to drop
Finnish for this part of the experiment. We sent the
texts of the other three languages through several in-
carnations of Systran-based MT Web-services2 and
through an installation of Lernout & Hauspie Power
Translator Pro, Version 6.43.3
1We used slightly more restrictive conditions that resulted in
a 5.76% reduction of phrase table size
2The results were incomplete and different, but sufficiently
close to each other so that it did not seem worthwhile to explore
the differences systematically. Instead we ranked the services
according to errors in an informal comparison and took for each
sentence the first available translation in this order.
3After having collected or computed all translations, we ob-
served that in the case of French, both systems were quite sen-
sitive to the fact that the apostrophes were formatted as separate
tokens in the source texts (l ? homme instead of l?homme). We
therefore modified and retranslated the French texts, but did not
explore possible effects of similar transformations in the other
languages.
156
3 Heuristic Selection
3.1 Approach
We implemented two different ways to select, out
of a set of alternative translations of a given sen-
tence, one that looks most promising. The first ap-
proach is purely heuristic and is limited to the case
where more than two candidates are given. For each
candidate, we collect a set of features, consisting of
words and word n-grams (n ? {2, 3, 4}). Each of
these features is weighted by the number of can-
didates it appears in, and the candidate with the
largest feature weight per word is taken. This can
be seen as the similarity of each of the candidate
to a prototypical version composed as a weighted
mixture of the collection, or as being remotely re-
lated to a sentence-specific language model derived
from the candidates. The heuristic measure was used
to select ?favorite? from each group of competing
translations obtained from the same source sentence,
yielding a fourth set of translations for the sentences
given in DE, FR, and ES.
A particularity of the shared task is the fact that
the source sentences of the development and test sets
form a parallel corpus. Therefore, we can not only
integrate multiple translations of the same source
sentence into a hopefully better version, but we can
merge the translations of corresponding parts from
different source languages into a target form that
combines their advantages. This approach, called
triangulation in (Kay, 1997), can be motivated by
the fact that most cases of translation for dissemi-
nation involve multiple target languages; hence one
can assume that, except for the very first of them,
renderings in multiple languages exist and can be
used as input to the next step4. See also (Och and
Ney, 2001) for some related empirical evidence. In
order to obtain a first impression of the potential of
triangulation in the domain of parliament debates,
we applied the selection heuristics to a set of four
translations, one from Finnish, the other three the
result of the selections mentioned above.
3.2 Results and Discussion
The BLEU scores (Papineni et al, 2002) for 10 di-
rect translations and 4 sets of heuristic selections
4Admittedly, in typical instances of such chains, English
would appear earlier.
Source MT BLEU
Language Engine score
DE Pharaoh 20.48
L & H 13.97
Systran 14.92
heuristic selection 16.01
statistical selection 20.55
FR Pharaoh 26.29
L & H 17.82
Systran 20.29
heuristic selection 21.44
statistical selection 26.49
ES Pharaoh 26.69
L & H 17.28
Systran 17.38
heuristic selection 19.16
statistical selection 26.74
FI Pharaoh 16.76
all heuristic selection 22.83
statistical selection 25.80
Table 1: BLEU scores of various MT engines and
combinations
thereof are given in Table 1. These results show
that in each group of translations for a given source
language, the statistical engine came out best. Fur-
thermore, our heuristic approach for the selection
of the best among a small set of candidate transla-
tions did not result in an increase of the measured
BLEU score, but typically gave a score that was
only slightly better than the second best of the in-
gredients. This somewhat disappointing result can
be explained in two ways. Apparently, the selection
heuristic does not give effective estimates of trans-
lation quality for the candidates. Furthermore, the
granularity on which the choices have to bee made
is too coarse, i.e. the pieces for which the symbolic
engines do produce better translations than the SMT
engine are accompanied by too many bad choices so
that the net effect is negative.
4 Statistical Selection
The other score we used was based on probabilities
as computed by the trigram language model for En-
glish provided by the organizers of the task, in a
representation compatible with the SRI LM toolkit
157
(Stolcke, 2002). However, a correct implementa-
tion for obtaining these estimates was not available
in time, so the selections generated from the statis-
tical language model could not be used for official
submissions, but were generated and evaluated af-
ter the closing date. The results, also displayed in
Table 1, show that this approach can lead to slight
improvements of the BLEU score, which however
turn out not to be statistically sigificant in then sense
of (Zhang et al, 2004).
5 Next Steps
When we started the experiments reported here, the
hope was to find relatively simple methods to select
the best among a small set of candidate translations
and to achieve significant improvements of a hybrid
architecture over a purely statistical approach. Al-
though we could indeed measure certain improve-
ments, these are not yet big enough for a conclu-
sive ?proof of concept?. We have started a refine-
ment of our approach that can not only pick the best
among translations of complete sentences, but also
judge the quality of the building blocks from which
the translations are composed. First informal results
look very promising. Once we can replace single
phrases that appear in one translation by better alter-
natives taken from a competing candidate, chances
are good that a significant increase of the overall
translation quality can be achieved.
6 Acknowledgements
This work has been funded by the Deutsche
Forschungsgemeinschaft. We want to thank two
anonymous reviewers for numerous pointers to rel-
evant literature, Bogdan Sacaleanu for his help with
the collection of translations from on-line MT en-
gines, as well as the organizers of the shared task for
making these interesting experiments possible.
References
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita.
2001. Using multiple edit distances to automatically
rank machine translation output. In Proceedings of MT
Summit VIII, Santiago de Compostela, Spain.
Yasuhiro Akiba, Taro Watanabe, and Eiichiro Sumita.
2002. Using language and translation models to se-
lect the best among outputs from multiple mt systems.
In COLING.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In ASRU, Italy.
Chris Callison-Burch and Raymond S. Flournoy. 2001.
A program for automatically selecting the best output
from multiple machine translation engines. In Proc. of
MT Summit VIII, Santiago de Compostela, Spain.
Robert E. Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In ANLP, pages 95?100.
Christopher Hogan and Robert E. Frederking. 1998. An
evaluation of the multi-engine mt architecture. In Pro-
ceedings of AMTA, pages 113?123.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proc. of EAMT, Budapest, Hungary.
Martin Kay. 1997. The proper place of men and ma-
chines in language translation. Machine Translation,
12:3?23. First appeared as a Xerox PARC working
paper in 1980.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In AMTA, pages 115?124.
Tadashi Nomoto. 2004. Multi-engine machine transla-
tion with voted language model. In Proc. of ACL.
Franz-Josef Och and Hermann Ney. 2001. Statistical
multi-source translation. In Proceedings of MT Sum-
mit VIII, Santiago de Compostela, Spain, September.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL,
pages 311?318.
Manny Rayner and David M. Carter. 1997. Hybrid lan-
guage processing in the spoken language translator. In
Proc. ICASSP ?97, pages 107?110, Munich, Germany.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken Lan-
guage Processing.
Dan Tidhar and Uwe Ku?ssner. 2000. Learning to select
a good translation. In COLING, pages 843?849.
Wolfgang Wahlster. 2001. Robust translation of spon-
taneous speech: A multi-engine approach. In IJCAI,
pages 1484?1493. Invited Talk.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. In-
terpreting BLEU/NIST scores: How much improve-
ment do we need to have a better system? In Proceed-
ings of LREC, Lisbon, Portugal.
158
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 70?74,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Translation Combination using Factored Word Substitution
Christian Federmann1, Silke Theison2, Andreas Eisele1,2, Hans Uszkoreit1,2,
Yu Chen2, Michael Jellinghaus2, Sabine Hunsicker2
1: Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz GmbH, Saarbru?cken, Germany
2: Universita?t des Saarlandes, Saarbru?cken, Germany
{cfedermann,eisele,uszkoreit}@dfki.de, {sith,yuchen,micha,sabineh}@coli.uni-sb.de
Abstract
We present a word substitution approach
to combine the output of different machine
translation systems. Using part of speech
information, candidate words are deter-
mined among possible translation options,
which in turn are estimated through a pre-
computed word alignment. Automatic
substitution is guided by several decision
factors, including part of speech, local
context, and language model probabili-
ties. The combination of these factors
is defined after careful manual analysis
of their respective impact. The approach
is tested for the language pair German-
English, however the general technique it-
self is language independent.
1 Introduction
Despite remarkable progress in machine transla-
tion (MT) in the last decade, automatic translation
is still far away from satisfactory quality. Even the
most advanced MT technology as summarized by
(Lopez, 2008), including the best statistical, rule-
based and example-based systems, produces out-
put rife with errors. Those systems may employ
different algorithms or vary in the linguistic re-
sources they use which in turn leads to different
characteristic errors.
Besides continued research on improving MT
techniques, one line of research is dedicated to bet-
ter exploitation of existing methods for the com-
bination of their respective advantages (Macherey
and Och, 2007; Rosti et al, 2007a).
Current approaches for system combination in-
volve post-editing methods (Dugast et al, 2007;
Theison, 2007), re-ranking strategies, or shal-
low phrase substitution. The combination pro-
cedure applied for this pape tries to optimize
word-level translations within a ?trusted? sentence
frame selected due to the high quality of its syntac-
tic structure. The underlying idea of the approach
is the improvement of a given (original) translation
through the exploitation of additional translations
of the same text. This can be seen as a simplified
version of (Rosti et al, 2007b).
Considering our submission from the shared
translation task as the ?trusted? frame, we add
translations from four additional MT systems that
have been chosen based on their performance in
terms of automatic evaluation metrics. In total, the
combination system performs 1,691 substitutions,
i.e., an average of 0.67 substitutions per sentence.
2 Architecture
Our system combination approach computes a
combined translation from a given set of machine
translations. Below, we present a short overview
by describing the different steps in the derivation
of a combined translation.
Compute POS tags for translations. We apply
part-of-speech (POS) tagging to prepare the
selection of possible substitution candidates.
For the determination of POS tags we use the
Stuttgart TreeTagger (Schmid, 1994).
Create word alignment. The alignment between
source text and translations is needed to
identify translation options within the differ-
ent systems? translations. Word alignment
is computed using the GIZA++ toolkit (Och
and Ney, 2003), only one-to-one word align-
ments are employed.
Select substitution candidates. For the shared
task, we decide to substitute nouns, verbs
and adjectives based on the available POS
tags. Initially, any such source word is con-
sidered as a possible substitution candidate.
As we do not want to require substitution can-
70
didates to have exactly the same POS tag as
the source, we use groups of ?similar? tags.
Compute decision factors for candidates. We
define several decision factors to enable an
automatic ranking of translation options.
Details on these can be found in section 4.
Evaluate the decision factors and substitute.
Using the available decision factors we
compute the best translation and substitute.
The general combination approach is language
independent as it only requires a (statistical) POS
tagger and GIZA++ to compute the word align-
ments. More advanced linguistic resources are not
required. The addition of lexical resources to im-
prove the extracted word alignments has been con-
sidered, however the idea was then dropped as we
did not expect any short-term improvements.
3 System selection
Our system combination engine takes any given
number of translations and enables us to compute
a combined translation out of these. One of the
given system translations is chosen to provide the
?sentence skeleton?, i.e. the global structure of the
translation, thus representing the reference system.
All other systems can only contribute single words
for substitution to the combined translation, hence
serve as substitution sources.
3.1 Reference system
Following our research on hybrid translation try-
ing to combine the strengths of rule-based MT
with the virtues of statistical MT, we choose our
own (usaar) submission from the shared task to
provide the sentence frame for our combination
system. As this translation is based upon a rule-
based MT system, we expect the overall sentence
structure to be of a sufficiently high quality.
3.2 Substitution sources
For the implementation of our combination sys-
tem, we need resources of potential substitution
candidates. As sources for possible substitution,
we thus include the translation results of the fol-
lowing four systems:
? Google (google)1
1The Google submission was translated by the Google
MT production system offered within the Google Language
Tools as opposed to the qualitatively superior Google MT
research system.
? University of Karlsruhe (uka)
? University of Maryland (umd)
? University of Stuttgart (stuttgart)
The decision to select the output of these par-
ticular MT systems is based on their performance
in terms of different automatic evaluation metrics
obtained with the IQMT Framework by (Gime?nez
and Amigo?, 2006). This includes BLEU, BLEU1,
TER, NIST, METEOR, RG, MT06, and WMT08.
The results, listing only the three best systems per
metric, are given in table 1.
metric best three systems
BLEU1 google uka systran
0.599 0.593 0.582
BLEU google uka umd
0.232 0.231 0.223
TER umd rwth.c3 uka
0.350 0.335 0.332
NIST google umd uka
6.353 6.302 6.270
METEOR google uka stuttgart
0.558 0.555 0.548
RG umd uka google
0.527 0.525 0.520
MT06 umd google stuttgart
0.415 0.413 0.410
WMT08 stuttgart rbmt3 google
0.344 0.341 0.336
Table 1: Automatic evaluation results.
On grounds of these results we anticipate the
four above named translation engines to perform
best when being combined with our hybrid ma-
chine translation system. We restrict the substi-
tution sources to the four potentially best systems
in order to omit bad substitutions and to reduce
the computational complexity of the substitution
problem. It is possible to choose any other num-
ber of substitution sources.
4 Substitution
As mentioned above, we consider nouns, verbs
and adjectives as possible substitution candidates.
In order to allow for automatic decision making
amongst several translation options we define a set
of factors, detailed in the following. Furthermore,
we present some examples in order to illustrate the
use of the factors within the decision process.
71
4.1 Decision factors
The set of factors underlying the decision proce-
dure consists of the following:
A: Matching POS. This Boolean factor checks
whether the target word POS tag matches the
source word?s POS category. The factor com-
pares the source text to the reference trans-
lation as we want to preserve the sentential
structure of the latter.
B: Majority vote. For this factor, we compute
an ordered list of the different translation op-
tions, sorted by decreasing frequency. A con-
sensus between several systems may help to
identify the best translation.
Both the reference system and the Google
submission receive a +1 bonus, as they ap-
peared to offer better candidates in more
cases within the small data sample of our
manual analysis.
C: POS context. Further filtering is applied de-
termining the words? POS context. This is
especially important as we do not want to de-
grade the sentence structure maintained by
the translation output of the reference system.
In order to optimize this factor, we conduct
trials with the single word, the ?1 left, and
the +1 right context. To reduce complex-
ity, we shorten POS tags to a single character,
e.g. NN ? N or NPS ? N .
D: Language Model. We use an English lan-
guage model to score the different translation
options. As the combination system only re-
places single words within a bi-gram context,
we employ the bi-gram portion of the English
Gigaword language model.
The language model had been estimated us-
ing the SRILM toolkit (Stolcke, 2002).
4.2 Factor configurations
To determine the best possible combination of our
different factors, we define four potential factor
configurations and evaluate them manually on a
small set of sentences. The configurations differ
in the consideration of the POS context for factor
C (strict including ?1 left context versus relaxed
including no context) and in the usage of factor A
Matching POS (+A). Table 2 shows the settings of
factors A and C for the different configurations.
configuration Matching POS POS context
strict disabled ?1 left
strict+A enabled ?1 left
relaxed disabled single word
relaxed+A enabled single word
Table 2: Factor configurations for combination.
Our manual evaluation of the respective substi-
tution decisions taken by different factor combi-
nation is suggestive of the ?relaxed+A? configura-
tion to produce the best combination result. Thus,
this configuration is utilized to produce sound
combined translations for the complete data set.
4.3 Factored substitution
Having determined the configuration of the dif-
ferent factors, we compute those for the complete
data set, in order to apply the final substitution step
which will create the combined translation.
The factored substitution algorithm chooses
among the different translation options in the fol-
lowing way:
(a) Matching POS? If factor A is activated for
the current factor configuration (+A), sub-
stitution of the given translation options can
only be possible if the factor evaluates to
True. Otherwise the substitution candidate is
skipped.
(b) Majority vote winner? If the majority vote
yields a unique winner, this translation option
is taken as the final translation.
Using the +1 bonuses for both the reference
system and the Google submission we intro-
duce a slight bias that was motivated by man-
ual evaluation of the different systems? trans-
lation results.
(c) Language model. If several majority vote
winners can be determined, the one with the
best language model score is chosen.
Due to the nature of real numbers this step
always chooses a winning translation option
and thus the termination of the substitution
algorithm is well-defined.
Please note that, while factors A, B, and D are
explicitly used within the substitution algorithm,
factor C POS context is implicitly used only when
computing the possible translation options for a
given substitution candidate.
72
configuration substitutions ratio
strict 1,690 5.714%
strict+A 1,347 4.554%
relaxed 2,228 7.532%
relaxed+A 1,691 5.717%
Table 3: Substitutions for 29,579 candidates.
Interestingly we are able to obtain best results
without considering the ?1 left POS context, i.e.
only checking the POS tag of the single word
translation option for factor C.
4.4 Combination results
We compute system combinations for each of the
four factor configurations defined above. Table
3 displays how many substitutions are conducted
within each of these configurations.
The following examples illustrate the perfor-
mance of the substitution algorithm used to pro-
duce the combined translations.
?Einbruch?: the reference translation for ?Ein-
bruch? is ?collapse?, the substitution sources
propose ?slump? and ?drop?, but also ?col-
lapse?, all three, considering the context,
forming good translations. The majority vote
rules out the suggestions different to the ref-
erence translation due to the fact that 2 more
systems recommend ?collapse? as the correct
translation.
?Ru?ckgang?: the reference system translates this
word as ?drop? while all of the substitution
sources choose ?decline? as the correct trans-
lation. Since factor A evaluates to True, i.e.
the POS tags are of the same nature, ?de-
cline? is clearly selected as the best transla-
tion by factor B Majority vote and thus re-
places ?drop? in the final combined transla-
tion result.
?Tagesgescha?fte?: our reference system trans-
lates ?Tagesgescha?fte? with ?requirements?,
while two of the substitution systems indi-
cate ?business? to be a better translation. Due
to the +1 bonus for our reference translation
a tie between the two possible translations
emerges, leaving the decision to the language
model score, which is higher for ?business?.
4.5 Evaluation results
Table 4 shows the results of the manual evaluation
campaign carried out as part of the WMT09 shared
task. Randomly chosen sentences are presented
to the annotator, who then has to put them into
relative order. Note that each annotator is shown a
random subset of the sentences to be evaluated.
system relative rank data points
google -2.74 174
uka -3.00 217
umd -3.03 170
stuttgart -2.89 163
usaar -2.78 186
usaar-combo -2.91 164
Table 4: Relative ranking results from the WMT09
manual evalution campaign.
Interestingly, our combined system is not able
to outperform the baseline, i.e., additional data
did not improve translation results. However the
evaluation is rather intransparent since it does not
allow for a strict comparison between sentences.
5 Conclusion
Within the system described in this paper, we ap-
proach a hybrid translation technique combining
the output of different MT systems. Substituting
particular words within a well-structured transla-
tion frame equips us with considerably enhanced
translation output. We obtain promising results
providing substantiated proof that our approach is
going in the right direction.
Further steps in the future will include machine
learning methods to optimize the factor selection.
This was, due to limited amount of time and data,
not feasible thus far. We will also investigate the
potential of phrase-based substitution taking into
account multi-word alignments instead of just sin-
gle word mappings. Additionally, we would like
to continue work on the integration of lexical re-
sources to post-correct the word alignments ob-
tained by GIZA++ as this will directly improve the
overall system performance.
Acknowledgments
This work was supported by the EuroMatrix
project (IST-034291) which is funded by the
European Community under the Sixth Frame-
work Programme for Research and Technological
Development.
73
References
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on SYSTRAN?s rule-based
translation system. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
220?223, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Jesu?s Gime?nez and Enrique Amigo?. 2006. IQMT: A
framework for automatic machine translation eval-
uation. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC?06).
Adam Lopez. 2008. Statistical machine translation.
ACM Computing Surveys, 40(3):1?49.
Wolfgang Macherey and Franz J. Och. 2007. An em-
pirical study on computing consensus translations
from multiple machine translation systems. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 986?995, Prague, Czech Republic,
June. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007a. Combining outputs from multiple
machine translation systems. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 228?235, Rochester, New York, April.
Association for Computational Linguistics.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007b. Improved word-level system
combination for machine translation. In Proceed-
ings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 312?319,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, September.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In the 7th International
Conference on Spoken Language Processing (IC-
SLP) 2002, Denver, Colorado.
Silke Theison. 2007. Optimizing rule-based machine
translation output with the help of statistical meth-
ods. Master?s thesis, Saarland University, Computa-
tional Linguistics department.
74
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 128?136,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Intersecting multilingual data for faster and better statistical translations
Yu Chen1,2, Martin Kay1,3, Andreas Eisele1,2
1: Universita?t des Saarlandes, Saarbru?cken, Germany
2: Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz GmbH, Saarbru?cken, Germany
3: Stanford University, CA, USA
{yuchen,kay,eisele}@coli.uni-saarland.de
Abstract
In current phrase-based SMT systems, more
training data is generally better than less.
However, a larger data set eventually intro-
duces a larger model that enlarges the search
space for the translation problem, and con-
sequently requires more time and more re-
sources to translate. We argue redundant in-
formation in a SMT system may not only de-
lay the computations but also affect the qual-
ity of the outputs. This paper proposes an ap-
proach to reduce the model size by filtering
out the less probable entries based on com-
patible data in an intermediate language, a
novel use of triangulation, without sacrificing
the translation quality. Comprehensive exper-
iments were conducted on standard data sets.
We achieved significant quality improvements
(up to 2.3 BLEU points) while translating with
reduced models. In addition, we demon-
strate a straightforward combination method
for more progressive filtering. The reduction
of the model size can be up to 94% with the
translation quality being preserved.
1 Introduction
Statistical machine translation (SMT) applies ma-
chine learning techniques to a bilingual corpus to
produce a translation system entirely automatically.
Such a scheme has many potential advantages over
earlier systems which relied on carefully crafted
rules. The most obvious is that it at dramatically
reduces cost in human labor and it is able to reach
many critical translation rules that are easily over-
looked by human being.
SMT systems generally assemble translations by
selecting phrases from a large candidate set. Un-
supervised learning often introduces a considerable
amount of noise into this set as a result of which the
selection process becomes more longer and less ef-
fective. This paper provides one approach to these
problems.
Various filtering techniques, such as (Johnson et
al., 2007) and (Chen et al, 2008), have been ap-
plied to eliminate a large portion of the translation
rules that were judged unlikely to be of value for
the current translation. However, these approaches
were only able to improve the translation quality
slightly. In this paper, we describe a triangulation
approach (Kay, 1997) that incorporates multilingual
data to improve system efficiency and translation
quality at the same time. Most of the previous tri-
angulation approaches (Kumar et al, 2007; Cohn
and Lapata, 2007; Filali and Bilmes, 2005; Simard,
1999; Och and Ney, 2001) add information obtained
from a third language. In other words, they work
with the union of the data from the different lan-
guages. In contrast, we work with the intersection of
information acquired through a third language. The
hope is that the intersection will be more precise and
more compact than the union, so that a better result
will be obtained more efficiently.
2 Noise in a phrase-based SMT system
The phrases in a translation model are extracted
heuristically from a word alignment between the
parallel texts in two languages using machine learn-
ing techniques. The translation model feature values
are stored in the form of a so-called phrase-table,
128
while the distortion model is in the reordering-table.
As we have said models built in this way tend to con-
tain a contains a considerable amount of noise. The
phrase-table entries are far less reliable than the lex-
icons and grammar rules handcrafted for rule-based
systems.
The main source of noise in the phrase table is
errors from the word alignment process. For exam-
ple, many function words occur so frequently that
they are incorrectly mapped to translations of many
function words in the other language to which they
are, in fact, unrelated. On the other hand, many
words remain unaligned on account of their very low
frequency. Another source noise comes from the
phrase extraction algorithm itself. The unaligned
words are usually attached to aligned sequences In
order to achieve longer phrase pairs.
The final selection of entries from the phrase ta-
ble is based not only on the values assigned to them
there, but also to values coming from the language
and reordering models, so that entries that receive an
initially high value may end up not being preferred.
(1) Sie
they
lieben
love
ihre
their
Kinder
children
nicht.
not
They don?t love their children.
The frequently occurring German negative ?nicht?
in (1). is sometimes difficult for SMT systems
to translate into English because it may appear in
many positions of a sentence. For instance, it oc-
curs at the end of the sentence in (1). The phrase
pairs ?ihre kinder nicht ? their children are not?
and ?ihre kinder nicht ? their children? are both
likely also to appear in the phrase table and the for-
mer has greater estimated probability. However, the
language model would preferred the latter in this ex-
ample because the sentence ?They love their children
are not.? is unlikely to be attested. Accordingly,
SMT system may therefore produce the misleading
translation in (2).
(2) They love their children.
The system would not produce translations with the
opposite meanings if the noisy entries like ?ihre
kinder nicht ? their children? were excluded from
the translation candidates. Eliminating the noise
should help to improve the system?s performance,
for both efficiency and translation quality.
3 Triangulated filtering
While direct translation and pivot translation
through a bridge language presumably introduce
noise, in substantially similar amounts, there is no
reason to expect the noise in the two systems to cor-
relate strongly. In fact, the noise from such differ-
ent sources, tends to be quite distinct, whereas the
more useful information is often retained. This en-
courages us to hope that information gathered from
various sources will be more reliable overall.
Our plan is to ameliorate the noise problem by
constructing a smaller phrase-table by taking the
intersection of a number of sources. We reason that a
target phrase is will appear as a candidate translation
of a given source phrase, only if it also appears as a
candidate translation for some word or phrase in the
bridge language mapping to the source phrase. We
refer to this triangulation approach as triangulated
phrase-table filtering.
TargetTextSourceText
ModelFiltered
ParallelCorpus
Extraction
Alignment,Phrase
SMTDecoder
TranslationModel
LanguageModelMonolingualCorpus
CountingSmoothing
Filtering
ModelTarget?Bridge
ModelSource?Bridge
Figure 1: Triangulated filtering in SMT systems
Figure 1 illustrates our triangulation approach.
Two bridge models are first constructed: one from
the source language to the bridge language, and an-
other from the target language to the bridge lan-
guage. Then, we use these two models to filter the
original source-target model. For each phrase pair
in the original table, we try to find a common link
in these bridge models to connect both phrases. If
such links do not exist, we remove the entry from
the table. The probability values in the table remain
129
unchanged. The reduced table can be used in place
of the original one in the SMT system.
There are various forms of links that can be used
as our evidence for the filtering process. One obvi-
ous form is complete phrases in the bridge language,
which means, for each phrase pair in the model to
be filtered, we should look for a third phrase in the
bridge language that can relate the two phrases in the
pair.
This approach to filtering examines each phrase
pair presented in the phrase-table one by one. For
each phrase pair, we collect the corresponding trans-
lations using the models for translation into a third
language. If both phrases can be mapped to some
phrases in the bridge language, but to different ones,
we should remove it from the model. It is also possi-
ble that neither of the phrases appear in correspond-
ing bridge models. In this case, we consider the
bridge models insufficient for making the filtering
decision and prefer to keep the pair in the table.
The way a decoder constructs translation hypothe-
ses is directly related to the weights for different
model features in a SMT system, which are usually
optimized for a given set of models with minimum
error rate training (MERT) (Och, 2003) to achieve
better translation performance. In other words, the
weights obtained for a model do not necessarily ap-
ply to another model. Since the triangulated filter-
ing method removes a part of the model, it is impor-
tant to readjust the feature weights for the reduced
phrase-table.
4 Experimental design
All the text data used in our experiments are
from Release v3 of ?European Parliament Proceed-
ings Parallel Corpus 1996-2006? (Europarl) cor-
pus (Koehn, 2005). We mainly investigated trans-
lations from Spanish to English. There are enough
structural differences in these two language to in-
troduce some noise in the phrase table. French,
Portuguese, Danish, German and Finnish were used
as bridge languages. Portuguese is very similar to
Spanish and French somewhat less so. Finnish is un-
related and fairly different typologically with Danish
and German occupying the middle ground. In addi-
tion, we also present briefly the results on German-
English translations with Dutch, Spanish and Danish
as bridges.
For the Spanish-English pair, three translation
models were constructed over the same parallel cor-
pora. We acquired comparable data sets by draw-
ing several subsets from the same corpus according
to various maximal sentence lengths. The subsets
Tokens
Model Sentences Spanish English
EP-20 410,487 5,220,142 5,181,452
EP-40 964,687 20,820,067 20,229,833
EP-50 1,100,813 26,731,269 25,867,370
Europarl 1,304,116 37,870,751 36,429,274
Table 1: Europarl subsets for building the Spanish-
English SMT system
we used in the experiments are presented by ?EP-
20?, ?EP-40? and ?EP-50?, in which the numbers
indicate the maximal sentence length in respective
Europarl subsets. Table 1 lists the characteristics
of the Spanish-English subsets. Although the max-
imal sentence length in these sets is far less than
that of the whole corpus (880 tokens), EP-50 al-
ready includes nearly 85% of Spanish-English sen-
tence pairs from Europarl.
The translations models, both the models to be
filtered and the bridge models, were generated
from compatible Europarl subsets using the Moses
toolkit (Koehn et al, 2007) with the most basic con-
figurations. The feature weights for the Spanish-
English translation models were optimized over a
development set of 500 sentences using MERT to
maximize BLEU (Papineni et al, 2001).
The triangulated filtering algorithm was applied
to each combination of a translation model and a
third language. The reordering models were also
filtered according to the phrase-table. Only those
phrase pairs that appeared in the phrase-table re-
mained in the reordering table. We rerun the MERT
process solely based on the remaining entries in the
filtered tables. Each table is used to translate a set of
2,000 sentences of test data (from the shared task of
the third Workshop on Statistical Machine Transla-
tion, 2008 1). Both the test set and the development
data set have been excluded from the training data.
We evaluated the proposed phrase-table filtering
1For details, see
http://www.statmt.org/wmt08/shared-task.html
130
method mainly from two points of view: the effi-
ciency of systems with filtered tables and the quality
of output translations produced by the systems.
5 Results
5.1 System efficiency
Often the question of machine translation is not only
how to produce a good translation, but also how
to produce it quickly. To evaluate the system ef-
ficiency, we measured both storage space and time
consumption. For recording the computation time,
we run an identical of installation of the decoder
with different models and then measure the average
execution time for the given translation task.
In Table 2, we give the number of entries in each
phrase table (N ), and the physical file size of the
phrase table (SPT ) and the reordering table (SRT )
(without any compression or binarization), Tl, the
time for the program to load phrase tables and Tt the
time to translate the complete test set. We also high-
lighted the largest and the smallest reduction from
each group.
All filtered models showed significant reductions
in size. The greatest reduction of model sizes, taking
both phrase-table and reordering table into account,
is nearly 11 gigabytes for filtering the largest model
(EP-50) with a Finnish bridge, which leads to the
maximal time saving of 939 seconds, or almost 16
minutes, for translating two thousand sentences.
The reduction rates from two larger models are
very close to each other whereas the filtered table
scaled down the most significantly on the smallest
model (EP-20), which was in fact constructed over a
much smaller subset of Europarl corpus, consisting
of less than half of the sentences pairs in the other
two Europarl subsets. Compared to the larger Eu-
roparl subsets, the small data set is expected to pro-
duce more errors through training as there is much
less relevant data for the machine learning algorithm
to correctly extract useful information from. Conse-
quently, there are more noisy entries in this small
model, and therefore more entries to be removed. In
addition, the filtering is done by exact matching of
complete phrases, which presumably happens much
less frequently even for correctly paired phrase pairs
in the very limited data supplied by the smallest
training set. For the same reason, the distinction be-
tween different bridge languages was less clear for
this small model.
Due to hardware limitation, we are not able to
fit the unfiltered phrase tables completely into the
memory. Every table was filtered based on the given
input so only a small portion of each table was
loaded into memory. This may diminish the differ-
ence between the original and the filtered table to a
certain degree. The relative time consumptionnev-
ertheless agrees with the reduction in size: phrase
tables from the smallest model showed the most re-
duction for both loading the models and processing
the translations.
For loading time, we count the time it takes to
start and to load the bilingual phrase-tables plus re-
ordering tables and the monolingual language model
into the memory. The majority of the loading time
for the smallest model, even before filtering, has
been used for loading language models and other
start-up processes, could not be reduced as much as
the reduction on table size.
5.2 Translation quality
Bridge EP-20 EP-40 EP-50
? 26.62 31.43 31.68
pt 28.40 32.90 33.93
fr 28.28 32.69 33.47
da 28.48 32.47 33.88
de 28.05 32.65 33.13
fi 28.02 31.91 33.04
Table 3: BLEU scores of translations using filtered phrase
tables
Efficiency aside, a translation system should be
able to produce useful translation. It is important
to verify that the filtering approach does not affect
the translation quality of the system. Table 3 show
the BLEU scores of each translation acquired in the
experiments.
Between translation models of different sizes,
there are obvious performance gaps. Different
bridge languages can cause different effects on per-
formance. However, the translation qualities from
a single model are fairly close to each other. We
therefore take it that the effect of the triangulation
approach is rather robust across translation models
of different sizes.
131
Time Table Size
Model+Bridge Tl (s) Tt (s) N SPT (byte) SRT (byte)
EP-20+ ? 55 3529 7,599,271 953M 717M
EP-20+ pt 53 2826 1,712,508 (22.54%) 198M 149M
EP-20+ fr 48 2702 1,536,056 (20.21%) 172M 131M
EP-20+ da 52 2786 1,659,067 (21.83%) 186M 141M
EP-20+ de 43 2732 1,260,524 (16.59%) 132M 101M
EP-20+ fi 47 2670 1,331,323 (17.52%) 147M 111M
EP-40+ ? 65 3673 19,199,807 2.5G 1.9G
EP-40+ pt 50 3091 8,378,517 (43.64%) 1.1G 1.8G
EP-40+ fr 46 3129 8,599,708 (44.79%) 1.1G 741M
EP-40+ da 42 3050 6,716,304 (34.98%) 842M 568M
EP-40+ de 46 3069 6,113,769 (31.84%) 725M 492M
EP-40+ fi 40 2889 4,473,483 (23.30%) 533M 353M
EP-50+ ? 140 4130 54,382,715 7.1G 5.4G
EP-50+ pt 78 3410 13,225,654 (24.32%) 1.6G 1.3G
EP-50+ fr 97 3616 24,057,849 (44.24%) 3.0G 2.3G
EP-50+ da 81 3418 12,547,839 (23.07%) 1.5G 1.2G
EP-50+ de 95 3488 15,938,151 (29.31%) 1.9G 1.5G
EP-50+ fi 71 3191 7,691,904 (17.75%) 895M 677M
Table 2: System efficiency: time consumption and phrase-table size
It is obvious that the best systems are usually
NOT from the filtered tables that preserved the most
entries from the original. All the filtered models
showed some improvement in quality with updated
model weights. Mostly around 1.5 BLEU points, the
increases ranged from 0.36 to 2.25. Table 4 gives a
set of translations from the experiments. The unfil-
tered baseline system inserted the negative by mis-
take while all the filtered systems are able to avoid
this. It indicates that there are indeed noisy entries
affecting translation quality in the original table. We
were able to achieve better translations by eliminat-
ing noisy entries.
The filtering methods indeed tend to remove en-
tries composed of long phrases. Table 5 lists the
average length of phrases in several models. Both
source phrases and target phrases are taken into ac-
count. The best models have shortest phrases on av-
erage. Discarding such entries seems to be neces-
sary. This is consistent with the findings in (Koehn,
2003) that phrases longer than three words improve
performance little for training corpora of up to 20
million words.
Quality gains appeared to converge in the results
across different bridge languages while the original
models became larger. Translations generated us-
ing large models filtered with different bridge lan-
Bridge EP-20 EP-40 EP-50
? 3.776 4.242 4.335
pt 3.195 3.943 3.740
fr 3.003 3.809 3.947
da 3.005 3.74 3.453
de 2.535 3.501 3.617
fi 2.893 3.521 3.262
Table 5: Average phrase length
guages are less diverse. Meanwhile, the degradation
is less for a larger model. It is reasonable to expect
improvements for extremely large models with arbi-
trary bridge languages. For relatively small models,
the selection of bridge languages would be critical
for the effect of our approach.
5.3 Language clustering
To further understand how the triangulated filter-
ing approach worked and why it could work as it
did, we examined a randomly selected phrase table
fragment through the experiments. The segment in-
cluded 10 potential English translations of the same
Spanish word ?fabricantes?, the plural form of the
word ?fabricante? (manufacturer).
Table 6 shows the filtering results on a randomly
selected segment from the original ?EP-40? model,
including 10 English translations of the same source
132
source As??, se van modificando poco a poco los principios habituales del Estado de derecho por influencia de una
concepcin extremista de la lucha con tra las discriminaciones..
ref thus , the usual principles of the rule of law are being gradually altered under the influence of an extremist
approach to combating discrimination.
baseline we are not changing the usual principles of the rule of law from the influence of an extremist approach in
the fight against discrimination.
pt so , are gradually changing normal principles of the rule of law by influence of an extremist conception of
the fight against discrimination.
fr so , we are gradually changing the usual principles of the rule of law by influence of an extremist conception
of the fight against discrimination.
da so , are gradually changing the usual principles of the rule of law by influence of an extremist conception
of the fight against discrimination.
de thus , we are gradually altering the usual principles of the rule of law by influence of an extremist concep-
tion of the fight against discrimination.
fi so , are gradually changing normal principles of the rule of law by influence of an extremist conception of
the fight against discrimination.
Table 4: Examples
fabricantes pt fr da de fi
a manufacturer X X X X 4
battalions X X X 3
car manufacturers have 0
car manufacturers X X X X X 5
makers X X X 3
manufacturer X X X X X 5
manufacturers X X X X X 5
producers are X X X 3
producers need 0
producers X X X X X 5
Table 6: Phrase-table entries before and after filtering a
model with different bridges
word ?fabricantes?. X indicates that the corre-
sponding English phrase remained in the table after
triangulated filtering with the corresponding bridge
language. We also counted the number of tables that
included each phrase pair.
Regardless of the bridge language, the triangu-
lated filtering approach had removed those entries
that are clearly noise. Meanwhile, entries which
are surely correct were always preserved in the fil-
tered tables. The results of using different bridge
languages turned out to be consistent on these ex-
treme cases. The 5 filtering processes agreed on six
out of ten pairs.
As for the other 4 pairs, the decisions were differ-
ent using different bridge languages. The remaining
entries were always different when the bridge was
changed. None of the languages led to the identi-
cal eliminations. None of the cases excludes all er-
rors. Apparently, the selection of bridge languages
had immediate effects on the filtering results.
 31
 31.2
 31.4
 31.6
 31.8
 32
 32.2
 32.4
 32.6
 32.8
 33
 4  6  8  10  12  14  16  18  20
BL
EU
 (%
)
Phrase-table Entries (Mil.)
Portugese
French
Danish
German
Finnish
Baseline
Figure 2: Clustering of bridge languages
We compared two factors of these filtered tables:
their sizes and the corresponding BLEU scores. Fig-
ure 2 shows interesting signs of language similar-
ity/dissimilarity. There are apparently two groups
of languages having extremely close performance,
which happen to fall in two language groups: Ger-
manic (German and Danish) and Romance (French
and Portuguese). The Romance group was as-
sociated with larger filtered tables that produced
slightly better translations. The filtered tables cre-
ated with Germanic bridge languages contained ap-
133
proximately 2 million entries less than Romance
groups. The translation quality difference between
these two groups was within 1 point of BLEU.
Observed from this figure, it seems that the trans-
lation quality was connected to the similarity be-
tween the bridge language and the source language.
The closer the bridge is to the source language, the
better translations it may produce. For instance, Por-
tuguese led to a filtered table that produced the best
translations. On the other hand, the more different
the bridge languages compared to the source, the
larger portion of the phrase-table the filtering algo-
rithm will remove. The table filtered with German
was the smallest in the four cases.
Finnish, a language that is unrelated to others, was
associated with distinctive results. The size of the
table filtered with Finnish is only 23% of the orig-
inal, almost half of the table generated with Por-
tuguese. Finnish has extremely rich morphology,
hence a great many word-forms, which would make
exact matching in bridge models less likely to hap-
pen. Many more phrase pairs in the original table
were removed for this reason even though some of
these entries were beneficial for translations. Even
though the improvement on translation quality due
to the Finnish bridge was less significant than the
others, it is clear that triangulated filtering retained
the useful information from the original model.
5.4 Further filtering
The filtering decision with a bridge language on a
particular phrase pair is fixed: either to keep the en-
try or to discard it. It is difficult to adjust the system
to work differently. However, as the triangulated fil-
tering procedure does not consider probability distri-
butions in the models, it is possible to further filter
the tables according to the probabilities.
The phrase pairs are associated with values com-
puted from the given set of feature weights and
sorted, so that we can remove any portions of the
remain entries based on the values. Each generated
table is used to translate the test set again. Fig-
ure 3 shows BLEU scores of the translation out-
puts produced with tables derived from the ?EP-50?
model with respect to their sizes. We also included
the curve of probability-based filtering alone as the
baseline.
The difference between filtered tables at the same
 24
 26
 28
 30
 32
 0  10  20  30  40  50
BL
EU
 (%
)
Phrase-table Entries (Mil.)
BaselinePortugeseFrenchDanishGermanFinnish
Figure 3: Combining probability-based filtering
size can be over 6 BLEU points, which is a re-
markable advantage for the triangulated filtering ap-
proach always producing better translations. The
curves of the triangulated filtered models are clearly
much steeper than that of the naive pruned ones.
Data in these filtered models are more compact than
the original model before any filtering. The triangu-
lated filtered phrase-tables contain more useful in-
formation than a normal phrase-table of the same
size. The curves representing the triangulated filter-
ing performance are always on the left of the original
curves.
We are able to use less than 6% of the original
phrase table (40% of the table filtered with Finnish)
to obtain translations with the same quality as the
original. The extreme case, using only 1.4% of the
original table, leads to a reasonable BLEU score, in-
dicating that most of the output sentences should
still be understandable. In this case, the overall size
of the phrase table and the reordering table was less
than 100 megabytes, potentially feasible for mobile
devices, whereas the original models took nearly
12.5 gigabytes of disk space.
5.5 Different source language
Bridge EP-40 EP-50
? 5.1G 26.92 6.5G 27.23
Dutch 562M 27.11 1.3G 28.14
Spanish 3.0G 27.28 3.6G 28.09
Danish 505M 28.04 780M 28.21
Table 7: Filtered German-English systems (Size and
BLEU)
134
In addition to Spanish-English translation, we
also conducted experiments on German-English
translation. The results, shown in Table 7, appear
consistent with the results of Spanish-English trans-
lation. Translations in most cases have performance
close to the original unfiltered models, whereas the
reduction in phrase-table size ranged from 40% to
85%. Meanwhile, translation speed has been in-
creased up to 17%.
Due to German?s rich morphology, the unfil-
tered German-English models contain many more
entries than the Spanish-English ones constructed
from similar data sets. Unlike the Spanish-English
models, the difference between ?EP-40? and ?EP-
50? was not significant. Neither was the difference
between the impacts of the filtering in terms of trans-
lation quality. In addition, German and English are
so dissimilar that none of the three bridge languages
we chose turned out to be significantly superior.
6 Conclusions
We highlighted one problem of the state-of-the-art
SMT systems that was generally neglected: the
noise in the translation models. Accordingly, we
proposed triangulated filtering methods to deal with
this problem. We used data in a third language as ev-
idence to locate the less probable items in the trans-
lation models so as to obtain the intersection of in-
formation extracted from multilingual data. Only
the occurrences of complete phrases were taken into
account. The probability distributions of the phrases
have not been considered so far.
Although the approach was fairly naive, our ex-
periments showed it to be effective. The approaches
were applied to SMT systems built with the Moses
toolkit. The translation quality was improved at least
1 BLEU for all 15 cases (filtering 3 different models
with 5 bridge languages). The improvement can be
as much as 2.25 BLEU. It is also clear that the best
translations were not linked to the largest translation
models. We also sketched a simple extension to the
triangulated filtering approach to further reduce the
model size, which allows us to generate reasonable
results with only 1.4% of the entries from the origi-
nal table.
The results varied for different bridge languages
as well as different models. For translation from
Spanish to English, Finnish, the most distinctive
bridge language, appeared to be a more effective
intermediate language which could remove more
phrase pair entries while still improving the transla-
tion quality. Portuguese, the most close to the source
language, always leads to a filtered model that pro-
duces the best translations. The selection of bridge
languages has more obvious impact on the perfor-
mance of our approach when the size of the model
to filter was larger.
The work gave one instance of the general ap-
proach described in Section 3. There are several
potential directions for continuing this work. The
most straightforward one is to use our approaches
with more different languages, such as Chinese and
Arabic, and incompatible corpora, for example, dif-
ferent segments of Europarl. The main focus of such
experiments should be verifying the conclusions we
had in this paper.
Acknowledgments
This work was supported by European Community
through the EuroMatrix project funded under the
Sixth Framework Programme and the EuroMatrix
Plus project funded under the Seventh Framework
Programme for Research and Technological Devel-
opment.
References
Yu Chen, Andreas Eisele, and Martin Kay. 2008. Im-
proving Statistical Machine Translation Efficiency by
Triangulation. In the 6th International Conference
on Language Resources and Evaluation (LREC ?08),
May.
Trevor Cohn and Mirella Lapata. 2007. Machine
Translation by Triangulation: Making Effective Use
of Multi-Parallel Corpora. In the 45th Annual Meet-
ing of the Association for Computational Linguistics,
Prague, Czech, June.
Karim Filali and Jeff Bilmes. 2005. Leveraging Multi-
ple Languages to Improve Statistical MT Word Align-
ments. In IEEE Automatic Speech Recognition and
Understanding (ASRU), Cancun, Mexico, November.
J. Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving Translation Qual-
ity by Discarding Most of the Phrasetable. In the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
135
Language Learning (EMNLP-CoNLL), Prague, Czech
Republic, June.
Martin Kay. 1997. The proper place of men and ma-
chines in language translation. Machine Translation,
12(1-2):3?23.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In the 45th
Annual Meeting of the Association for Computational
Linguistics (ACL), Prague, Czech Republic, June.
Philipp Koehn. 2003. Noun Phrase Translation. Ph.D.
thesis, University of Southern California.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In MT Summit 2005.
Shankar Kumar, Franz Josef Och, and Wolfgang
Macherey. 2007. Improving word alignment with
bridge languages. In the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 42?50, Prague, Czech.
Franz Josef Och and Hermann Ney. 2001. Statistical
multi-source translation. In MT Summit VIII, Santiago
de Compostela, Spain.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160?167, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic eval-
uation of machine translation. In the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 311?318, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Michel Simard. 1999. Text-translation alignment: Three
languages are better than two. In EMNLP/VLC-99,
College Park, MD, June.
136
Proceedings of the Second Workshop on Statistical Machine Translation, pages 193?196,
Prague, June 2007. c?2007 Association for Computational Linguistics
Multi-Engine Machine Translation
with an Open-Source Decoder for Statistical Machine Translation
Yu Chen1, Andreas Eisele1,2, Christian Federmann2,
Eva Hasler3, Michael Jellinghaus1, Silke Theison1
(authors listed in alphabetical order)
1: Saarland University, Saarbru?cken, Germany
2: DFKI GmbH, Saarbru?cken, Germany
3: University of Cologne, Germany
Abstract
We describe an architecture that allows
to combine statistical machine translation
(SMT) with rule-based machine translation
(RBMT) in a multi-engine setup. We use a
variant of standard SMT technology to align
translations from one or more RBMT sys-
tems with the source text. We incorporate
phrases extracted from these alignments into
the phrase table of the SMT system and use
the open-source decoder Moses to find good
combinations of phrases from SMT training
data with the phrases derived from RBMT.
First experiments based on this hybrid archi-
tecture achieve promising results.
1 Introduction
Recent work on statistical machine translation has
led to significant progress in coverage and quality of
translation technology, but so far, most of this work
focuses on translation into English, where relatively
simple morphological structure and abundance of
monolingual training data helped to compensate for
the relative lack of linguistic sophistication of the
underlying models. As SMT systems are trained on
massive amounts of data, they are typically quite
good at capturing implicit knowledge contained in
co-occurrence statistics, which can serve as a shal-
low replacement for the world knowledge that would
be required for the resolution of ambiguities and the
insertion of information that happens to be missing
in the source text but is required to generate well-
formed text in the target language.
Already before, decades of work went into the im-
plementation of MT systems (typically rule-based)
for frequently used language pairs1, and these sys-
tems quite often contain a wealth of linguistic
knowledge about the languages involved, such as
fairly complete mechanisms for morphological and
syntactic analysis and generation, as well as a large
number of bilingual lexical entries spanning many
application domains.
It is an interesting challenge to combine the differ-
ent types of knowledge into integrated systems that
could then exploit both explicit linguistic knowledge
contained in the rules of one or several conventional
MT system(s) and implicit knowledge that can be
extracted from large amounts of text.
The recently started EuroMatrix2 project will ex-
plore this integration of rule-based and statistical
knowledge sources, and one of the approaches to
be investigated is the combination of existing rule-
based MT systems into a multi-engine architecture.
The work described in this paper is one of the
first incarnations of such a multi-engine architec-
ture within the project, and a careful analysis of the
results will guide us in the choice of further steps
within the project.
2 Architectures for multi-engine MT
Combinations of MT systems into multi-engine ar-
chitectures have a long tradition, starting perhaps
with (Frederking and Nirenburg, 1994). Multi-
engine systems can be roughly divided into simple
1See (Hutchins et al, 2006) for a list of commercial MT
systems
2See http://www.euromatrix.net
193
Figure 1: Architecture for multi-engine MT driven
by a SMT decoder
architectures that try to select the best output from a
number of systems, but leave the individual hypothe-
ses as is (Tidhar and Ku?ssner, 2000; Akiba et al,
2001; Callison-Burch and Flournoy, 2001; Akiba et
al., 2002; Nomoto, 2004; Eisele, 2005) and more so-
phisticated setups that try to recombine the best parts
from multiple hypotheses into a new utterance that
can be better than the best of the given candidates,
as described in (Rayner and Carter, 1997; Hogan and
Frederking, 1998; Bangalore et al, 2001; Jayaraman
and Lavie, 2005; Matusov et al, 2006; Rosti et al,
2007).
Recombining multiple MT results requires find-
ing the correspondences between alternative render-
ings of a source-language expression proposed by
different MT systems. This is generally not straight-
forward, as different word order and errors in the
output can make it hard to identify the alignment.
Still, we assume that a good way to combine the var-
ious MT outcomes will need to involve word align-
ment between the MT output and the given source
text, and hence a specialized module for word align-
ment is a central component of our setup.
Additionally, a recombination system needs a way
to pick the best combination of alternative building
blocks; and when judging the quality of a particu-
lar configuration, both the plausibility of the build-
ing blocks as such and their relation to the context
need to be taken into account. The required opti-
mization process is very similar to the search in a
SMT decoder that looks for naturally sounding com-
binations of highly probable partial translations. In-
stead of implementing a special-purpose search pro-
cedure from scratch, we transform the information
contained in the MT output into a form that is suit-
able as input for an existing SMT decoder. This has
the additional advantage that resources used in stan-
dard phrase-based SMT can be flexibly combined
with the material extracted from the rule-based MT
results; the optimal combination can essentially be
reduced to the task of finding good relative weights
for the various phrase table entries.
A sketch of the overall architecture is given in
Fig. 1, where the blue (light) parts represent the
modules and data sets used in purely statistical MT,
and the red (dark) parts are the additional modules
and data sets derived from the rule-based engines. It
should be noted that this is by far not the only way
to combine systems. In particular, as this proposed
setup gives the last word to the SMT decoder, we
risk that linguistically well-formed constructs from
one of the rule-based engines will be deteriorated in
the final decoding step. Alternative architectures are
under exploration and will be described elsewhere.
3 MT systems and other knowledge
sources
For the experiments, we used a set of six rule-based
MT engines that are partly available via web inter-
faces and partly installed locally. The web based
systems are provided by Google (based on Systran
for the relevant language pairs), SDL, and ProMT
which all deliver significantly different output. Lo-
cally installed systems are OpenLogos, Lucy (a re-
cent offspring of METAL), and translate pro by lin-
genio (only for German? English). In addition to
these engines, we also used the scripts included in
the Moses toolkit (Koehn et al, 2006)3 to generate
phrase tables from the training data. We enhanced
the phrase tables with information on whether a
given pair of phrases can also be derived via a third,
intermediate language. We assume that this can be
useful to distinguish different degrees of reliability,
but due to lack of time for fine-tuning we could not
yet show that it indeed helps in increasing the overall
quality of the output.
3see http://www.statmt.org/moses/
194
4 Implementation Details
4.1 Alignment of MT output
The input text and the output text of the MT systems
was aligned by means of GIZA++ (Och and Ney,
2003), a tool with which statistical models for align-
ment of parallel texts can be trained. Since training
new models on merely short texts does not yield very
accurate results, we applied a method where text can
be aligned based on existing models that have been
trained on the Europarl Corpus (Koehn, 2005) be-
forehand. This was achieved by using a modified
version of GIZA++ that is able to load given mod-
els.
The modified version of GIZA++ is embedded
into a client-server setup. The user can send two
corresponding files to the server, and specify two
models for both translation directions from which
alignments should be generated. After generating
alignments in both directions (by running GIZA++
twice), the system also delivers a combination of
these alignments which then serves as input to the
following steps described below.
4.2 Phrase tables from MT output
We then concatenated the phrase tables from the
SMT baseline system and the phrase tables obtained
from the rule-based MT systems and augmented
them by additional columns, one for each system
used. With this additional information it is clear
which of the MT systems a phrase pair stems from,
enabling us to assign relative weights to the con-
tributions of the different systems. The optimal
weights for the different columns can then be as-
signed with the help of minimum error rate training
(Och, 2003).
5 Results
We compared the hybrid system to a purely statis-
tical baseline system as well as two rule-based sys-
tems. The only differences between the baseline sys-
tem and our hybrid system are the phrase table ? the
hybrid system includes more lexical entries than the
baseline ? and the weights obtained from minimum
error rate training.
For a statistical system, lexical coverage becomes
an obstacle ? especially when the bilingual lexical
entries are trained on documents from different do-
mains. However, due to the distinct mechanisms
used to generate these entries, rule-based systems
and statistical systems usually differ in coverage.
Our system managed to utilize lexical entries from
various sources by integrating the phrase tables de-
rived from rule-based systems into the phrase table
trained on a large parallel corpus. Table 1 shows
Systems Token #
Ref. 2091 (4.21%)
R-I 3886 (7.02%)
R-II 3508 (6.30%)
SMT 3976 (7.91%)
Hybrid 2425 (5.59%)
Table 1: Untranslated tokens (excl. numbers and
punctuations) in output for news commentary task
(de-en) from different systems
a rough estimation of the number of untranslated
words in the respective output of different systems.
The estimation was done by counting ?words? (i.e.
tokens excluding numbers and punctuations) that ap-
pear in both the source document and the outputs.
Note that, as we are investigating translations from
German to English, where the languages share a lot
of vocabulary, e.g. named entities such as ?USA?,
there are around 4.21% of words that should stay the
same throughout the translation process. In the hy-
brid system, 5.59% of the words remain unchanged,
which is is the lowest percentage among all systems.
Our baseline system (SMT in Table 1), not compris-
ing additional phrase tables, was the one to produce
the highest number of such untranslated words.
Baseline Hybrid
test 18.07 21.39
nc-test 21.17 22.86
Table 2: Performance comparison (BLEU scores)
between baseline and hybrid systems, on in-domain
(test) and out-of-domain (nc-test) test data
Higher lexical coverage leads to better perfor-
mance as can be seen in Table 2, which compares
BLEU scores of the baseline and hybrid systems,
both measured on in-domain and out-of-domain test
data. Due to time constraints these numbers reflect
195
results from using a single RBMT system (Lucy);
using more systems would potentially further im-
prove results.
6 Outlook
Due to lack of time for fine-tuning the parameters
and technical difficulties in the last days before de-
livery, the results submitted for the shared task do
not yet show the full potential of our architecture.
The architecture described here places a strong
emphasis on the statistical models and can be seen
as a variant of SMT where lexical information from
rule-based engines is used to increase lexical cover-
age. We are currently also exploring setups where
statistical alignments are fed into a rule-based sys-
tem, which has the advantage that well-formed syn-
tactic structures generated via linguistic rules can-
not be broken apart by the SMT components. But
as rule-based systems typically lack mechanisms for
ruling out implausible results, they cannot easily
cope with errors that creep into the lexicon due to
misalignments and similar problems.
7 Acknowledgements
This research has been supported by the European
Commission in the FP6-IST project EuroMatrix. We
also want to thank Teresa Herrmann for helping us
with the Lucy system.
References
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita.
2001. Using multiple edit distances to automatically
rank machine translation output. In Proceedings of MT
Summit VIII, Santiago de Compostela, Spain.
Yasuhiro Akiba, Taro Watanabe, and Eiichiro Sumita.
2002. Using language and translation models to select
the best among outputs from multiple mt systems. In
COLING.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In ASRU, Italy.
Chris Callison-Burch and Raymond S. Flournoy. 2001.
A program for automatically selecting the best output
from multiple machine translation engines. In Proc. of
MT Summit VIII, Santiago de Compostela, Spain.
Andreas Eisele. 2005. First steps towards multi-engine
machine translation. In Proceedings of the ACL Work-
shop on Building and Using Parallel Texts, June.
Robert E. Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In ANLP, pages 95?100.
Christopher Hogan and Robert E. Frederking. 1998. An
evaluation of the multi-engine MT architecture. In
Proceedings of AMTA, pages 113?123.
John Hutchins, Walter Hartmann, and Etsuo Ito. 2006.
IAMT compendium of translation software. Twelfth
Edition, January.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proc. of EAMT, Budapest, Hungary.
P. Koehn, M. Federico, W. Shen, N. Bertoldi, O. Bo-
jar, C. Callison-Burch, B. Cowan, C. Dyer, H. Hoang,
R. Zens, A. Constantin, C. C. Moran, and E. Herbst.
2006. Open source toolkit for statistical machine trans-
lation: Factored translation models and confusion net-
work decoding. Final Report of the 2006 JHU Summer
Workshop.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of the MT
Summit.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In In Proc. EACL, pages 33?40.
Tadashi Nomoto. 2004. Multi-engine machine translation
with voted language model. In Proc. of ACL.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of ACL,
Sapporo, Japan, July.
Manny Rayner and David M. Carter. 1997. Hybrid lan-
guage processing in the spoken language translator. In
Proc. ICASSP ?97, pages 107?110, Munich, Germany.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie J. Dorr.
2007. Combining translations from multiple machine
translation systems. In Proceedings of the Conference
on Human Language Technology and North American
chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT-NAACL?2007), pages 228?
235, Rochester, NY, April 22-27.
Dan Tidhar and Uwe Ku?ssner. 2000. Learning to select a
good translation. In COLING, pages 843?849.
196
Proceedings of the Third Workshop on Statistical Machine Translation, pages 179?182,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Using Moses to Integrate Multiple Rule-Based Machine Translation Engines
into a Hybrid System
Andreas Eisele1,2, Christian Federmann2, Herve? Saint-Amand1,
Michael Jellinghaus1, Teresa Herrmann1, Yu Chen1
1: Saarland University, Saarbru?cken, Germany
2: DFKI GmbH, Saarbru?cken, Germany
Abstract
Based on an architecture that allows to com-
bine statistical machine translation (SMT)
with rule-based machine translation (RBMT)
in a multi-engine setup, we present new results
that show that this type of system combination
can actually increase the lexical coverage of
the resulting hybrid system, at least as far as
this can be measured via BLEU score.
1 Introduction
(Chen et al, 2007) describes an architecture that
allows to combine statistical machine translation
(SMT) with one or multiple rule-based machine
translation (RBMT) systems in a multi-engine setup.
It uses a variant of standard SMT technology to align
translations from one or more RBMT systems with
the source text and incorporated phrases extracted
from these alignments into the phrase table of the
SMT system. Using this approach it is possible to
employ a vanilla installation of the open-source de-
coder Moses1 (Koehn et al, 2007) to find good com-
binations of phrases from SMT training data with
the phrases derived from RBMT. A similar method
was presented in (Rosti et al, 2007).
This setup provides an elegant solution to the
fairly complex task of integrating multiple MT re-
sults that may differ in word order using only stan-
dard software modules, in particular GIZA++ (Och
and Ney, 2003) for the identification of building
blocks and Moses for the recombination, but the
authors were not able to observe improvements in
1see http://www.statmt.org/moses/
terms of BLEU score. A closer investigation re-
vealed that the experiments had suffered from a cou-
ple of technical difficulties, such as mismatches in
character encodings generated by different MT en-
gines and similar problems. This motivated us to
re-do these experiments in a somewhat more sys-
tematic way for this year?s shared translation task,
paying the required attention to all the technical de-
tails and also to try it out on more language pairs.
2 System Architecture
For conducting the translations, we use a multi-
engine MT approach based on a ?vanilla? Moses
SMT system with a modified phrase table as a cen-
tral element. This modification is performed by aug-
menting the standard phrase table with entries ob-
tained from translating the data with several rule-
based MT systems. The resulting phrase table thus
combines statistically gathered phrase pairs with
phrase pairs generated by linguistic rules.
Basing its decision about the final translation on
the obtained ?combined? phrase table, the SMT de-
coder searches for the best translation by recombin-
ing the building blocks that have been contributed by
the different RBMT systems and the original SMT
system trained on Europarl data.
A sketch of the overall architecture is given in
Fig. 1, where the lighter parts represent the mod-
ules and data sets used in purely statistical MT,
and the darker parts are the additional modules and
data sets derived from the rule-based engines. The
last word in the proposed setup is thus given to the
SMT decoder, which can recombine (and potentially
also tear apart) linguistically well-formed constructs
179
ModelLanguagePhrasetable
Combined
Alignment,PhraseExtraction
DecoderSMT
Rule?basedMT engines
ParallelCorpus
SourceText
TargetText
MonolingualCorpus
Hypotheses
CountingSmoothing
Figure 1: Hybrid architecture of the system
from the rule-based engines? output.
2.1 The Combined Phrase Table
The combined phrase table is built from the orig-
inal Moses phrase table and separate phrase tables
for each of the RBMT systems that are used in our
setup. Since the original phrase table is created
during the training process of the Moses decoder
with the Europarl bilingual corpus as training ma-
terial, it comprises general knowledge about typical
constructions and vocabulary from the Europarl do-
main. Therefore, a standard Moses SMT system is,
in principle, well adapted for input from this do-
main. However, it will have problems in dealing
with vocabulary and structures that did not occur in
the training data. The additional phrase tables are
generated separately for each RBMT system from
the source text and its translation by the respective
system. By using a combined phrase table that in-
cludes the original Moses phrase table as well as the
phrase tables from the RBMT systems, the hybrid
system can both handle a wider range of syntactic
constructions and exploit knowledge that the RBMT
systems possess about the particular vocabulary of
the source text.
3 Implementation
3.1 MT Systems and Knowledge Sources
Apart from the Moses SMT system, we used a
set of six rule-based MT engines that are partly
available via web interfaces and partly installed lo-
cally. The web interfaces are provided by Al-
tavista Babelfish (based on Systran), SDL, ProMT
and Lucy (a recent offspring of METAL). All of
them deliver significantly different output trans-
lations. Locally installed systems are OpenLo-
gos (for German?English, English?Spanish and
English?French) and translatePro by lingenio (for
German?English). The language model for our pri-
mary setup is based on the Europarl corpus whereas
the English Gigaword corpus served as training data
for a contrastive setup that was created for the trans-
lation direction German?English only.
3.2 Alignment of RBMT output
As already mentioned above, the construction of the
RBMT system specific phrase tables is a major part
of the overall system architecture. Such an RBMT
phrase table is generated from a bilingual corpus
consisting of the input text and its translation by
the respective RBMT system. Because this corpus
has the mere size of the text to be translated, it usu-
ally is not big enough to ensure the statistical meth-
ods for phrase table building of the Moses system to
work. Therefore, we create the alignments between
the RBMT input and output with help of another tool
(Theison, 2007) that is based on knowledge learned
in a previously conducted training phase with an ap-
propriately bigger corpus. On the basis of the align-
ments created in this manner, the Moses training
script provides a phrase table that consists of the
source text vocabulary. These steps are carried out
for each one of the six RBMT systems leading to
six source text specific phrase tables which are then
combined with the original Moses phrase table.
3.3 Combination of Phrase Tables
The combination process basically consists of the
concatenation of the Moses phrase table and the pre-
viously created RBMT phrase tables with one mi-
nor adjustment: The phrase table resulting from this
combination now also features additional columns
indicating which system each phrase table entry
originated from. For each new source text, the
RBMT phrase tables have to be created from scratch
and incorporated into a new combined phrase table.
3.4 Tuning
The typical process for creating an SMT system with
the Moses toolkit includes a tuning step in which
180
Europarl NewsCommentary
de-en en-de fr-en en-fr es-en en-es de-en en-de fr-en en-fr es-en en-es
SMT 22.81 19.78 24.18 21.62 31.68 24.46 14.24 9.75 11.60 12.24 17.27 14.48
Hybrid 27.85 20.75 28.12 28.82 33.15 32.31 17.36 13.57 17.66 20.71 22.16 22.55
RBMT1? 13.34 11.09 ?? 17.19 ?? 18.63 14.90 12.34 ?? 15.11 ?? 17.13
RBMT2 16.19 12.06 ?? ?? ?? ?? 16.66 13.64 ?? ?? ?? ??
RBMT3 16.32 10.88 18.18 20.38 19.32 20.89 16.88 12.53 17.20 18.82 19.00 19.98
RBMT4 15.58 12.09 19.00 22.20 18.99 21.69 17.41 13.93 17.73 20.85 19.14 21.70
RBMT5 15.58 9.54 21.36 12.98 18.47 20.59 15.99 11.05 18.65 19.49 20.50 20.02
RBMT6 13.96 9.44 17.16 18.91 18.01 19.18 15.08 10.41 16.86 17.82 18.70 19.97
Table 1: Performance of baseline SMT system, our system and RBMT systems (BLEU scores)
the system searches for the best weight configura-
tion for the columns in the phrase table while given
a development set to be translated, and correspond-
ing reference translations. In our hybrid setup, it is
equally essential to conduct tuning since the com-
bined phrase table we use contains 7 more columns
than the original Moses phrase table. All these
columns are given the same default weight initially
and thus still need be to be tuned to more meaning-
ful values. From this year?s Europarl development
data the first 200 sentences of each of the data sets
dev2006, test2006, test2007 and devtest2006 were
concatenated to build our development set. This set
of 800 sentences was used for Minimum Error Rate
Training (Och, 2003) to tune the weights of our sys-
tem with respect to BLEU score.
4 Results
In order to be able to evaluate our hybrid approaches
in contrast to stand-alone rule-based approaches, we
also calculated BLEU scores for the translations
conducted by the RBMT systems used in the hy-
brid setup. Our hybrid system is compared to a SMT
baseline and all the 6 RBMT systems that we used.
Table 1 shows the evaluation of all the systems in
terms of BLEU score (Papineni et al, 2002) with the
best score highlighted. The empty cells in the table
indicate the language pairs which are not available
in the corresponding systems2. The SMT system is
the one upon which we build the hybrid system. Ac-
cording to the scores, the hybrid system produces
better results than the baseline SMT system in all
2The identities of respective RBMT systems are not revealed
in this paper. RBMT1 is evaluated on the partial results pro-
duced due to some technical problems.
cases. The difference between our system and the
baseline is more significant for out-of-domain tests,
where gaps in the lexicon tend to be more severe.
Figure 2 illustrates an example of how the hy-
brid system differs from the baseline SMT system
and how it benefits from the RBMT systems. The
example lists the English translations of the same
German sentence (from News Commentary test set)
from different systems involved in our experiment.
Neither the word ?Pentecost? nor its German trans-
lation ?Pfingsten? has appeared in the training cor-
pus. Therefore, the SMT baseline system cannot
translate the word and chooses to leave the word
as it is whereas all the RBMT systems translate the
word correctly. The hybrid system appears to have
the corresponding lexicon gap covered by the ex-
tra entries produced by the RBMT systems. On the
other side, these additional entries may not always
be helpful. The errors in RBMT outputs can be sig-
nificant noise that destroys the correct information
in the SMT system. In the example translation pro-
duced by the hybrid system, there is a comma miss-
ing after ?in addition?, which appears to be frequent
in the RBMT outputs.
5 Outlook
The results reported in this paper are still somewhat
preliminary in the sense that many possible (includ-
ing some desirable) variants of the setup could not
be tried out due to lack of time. In particular, we
think that the full power of our approach on out-
of-domain test data can only be exploited with the
help of large language models trained on out-of-
domain text, but could not yet try this systematically.
Furthermore, the presence of multiple instances of
181
Source Daru?ber hinaus gibt es je zwei Feiertage zu Ostern, Pfingsten, und Weihnachten.
Reference In addition, Easter, Pentecost, and Christmas are each two-day holidays.
Moses In addition, there are two holidays, pfingsten to Easter, and Christmas.
Hybrid In addition there are the two holidays to Easter, Pentecost and Christmas.
RBMT1 Furthermore there are two holidays to Easter, Pentecost and Christmas .
RBMT2 Furthermore there are two holidays each at Easter, Pentecost and Christmas.
RBMT3 In addition there are each two holidays to Easters, Whitsun, and Christmas.
RBMT4 In addition, there is two holidays to Easter, Pentecost, and Christmas.
RBMT5 Beyond that there are ever two holidays to Easter, Whitsuntide, and Christmas.
RBMT6 In addition it gives two holidays apiece to easter, Pentecost, and Christmas.
Figure 2: German-English translation examples
the same phrase pair (with different weight) in the
combined phrase table causes the decoder to gen-
erate many instances of identical results in differ-
ent ways, which increases computational effort and
significantly decreases the number of distinct cases
that are considered during MERT. We suspect that a
modification of our scheme that avoids this problem
will be able to achieve better results, but experiments
in this direction are still ongoing.
The approach presented here combines the
strengths of multiple systems and is different from
recent work on post-correction of RBMT output as
presented in (Simard et al, 2007; Dugast et al,
2007), which focuses on the improvement of a sin-
gle RBMT system by correcting typical errors via
SMT techniques. These ideas are independent and a
suitable combination of them could give rise to even
better results.
Acknowledgments
This work was supported by the EuroMatrix project
funded by the European Commission (6th Frame-
work Programme). We thank Martin Kay, Hans
Uszkoreit, and Silke Theison for interesting discus-
sions and practical help, and two anonymous re-
viewers for hints to improve the paper.
References
Yu Chen, Andreas Eisele, Christian Federmann, Eva
Hasler, Michael Jellinghaus, and Silke Theison. 2007.
Multi-engine machine translation with an open-source
SMT decoder. In Proceedings of WMT07, pages 193?
196, Prague, Czech Republic, June. Association for
Computational Linguistics.
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on SYSTRAN?s rule-based
translation system. In Proceedings of WMT07, pages
220?223, Prague, Czech Republic, June. Association
for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL Demo and Poster Sessions, pages 177?180, Jun.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, Mar.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of ACL,
Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proceedings of ACL.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie J. Dorr.
2007. Combining translations from multiple machine
translation systems. In Proceedings of the Conference
on Human Language Technology and North American
chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT-NAACL?2007), pages 228?
235, Rochester, NY, April 22-27.
Michel Simard, Nicola Ueffing, Pierre Isabelle, and
Roland Kuhn. 2007. Rule-based translation with
statistical phrase-based post-editing. In Proceedings
of WMT07, pages 203?206, Prague, Czech Republic,
June. Association for Computational Linguistics.
Silke Theison. 2007. Optimizing rule-based machine
translation output with the help of statistical methods.
Diploma thesis, Saarland University.
182
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 42?46,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Combining Multi-Engine Translations with Moses
Yu Chen1, Michael Jellinghaus1, Andreas Eisele1,2,Yi Zhang1,2,
Sabine Hunsicker1, Silke Theison1, Christian Federmann2, Hans Uszkoreit1,2
1: Universita?t des Saarlandes, Saarbru?cken, Germany
2: Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz GmbH, Saarbru?cken, Germany
{yuchen,micha,yzhang,sabineh,sith}@coli.uni-saarland.de
{eisele,cfedermann,uszkoreit}@dfki.de
Abstract
We present a simple method for generating
translations with the Moses toolkit (Koehn
et al, 2007) from existing hypotheses pro-
duced by other translation engines. As
the structures underlying these translation
engines are not known, an evaluation-
based strategy is applied to select sys-
tems for combination. The experiments
show promising improvements in terms of
BLEU.
1 Introduction
With the wealth of machine translation systems
available nowadays (many of them online and
for free), it makes increasing sense to investigate
clever ways of combining them. Obviously, the
main objective lies in finding out how to integrate
the respective advantages of different approaches:
Statistical machine translation (SMT) and rule-
based machine translation (RBMT) systems of-
ten have complementary characteristics. Previous
work on building hybrid systems includes, among
others, approaches using reranking, regeneration
with an SMT decoder (Eisele et al, 2008; Chen
et al, 2007), and confusion networks (Matusov et
al., 2006; Rosti et al, 2007; He et al, 2008).
The approach by (Eisele et al, 2008) aimed
specifically at filling lexical gaps in an SMT sys-
tem with information from a number of RBMT
systems. The output of the RBMT engines was
word-aligned with the input, yielding a total of
seven phrase tables which where simply concate-
nated to expand the phrase table constructed from
the training corpus. This approach differs from the
confusion network approaches mainly in that the
final hypotheses do not necessarily follow any of
the input translations as the skeleton. On the other
hand, it emphasizes that the additional translations
should be produced by RBMT systems with lexi-
cons that cannot be learned from the data.
The present work continues on the same track
as the paper mentioned above but implements a
number of important changes, most prominently
a relaxation of the restrictions on the number and
type of input systems. These differences are de-
scribed in more detail in Section 2. Section 3 ex-
plains the implementation of our system and Sec-
tion 4 its application in a number of experiments.
Finally, Section 5 concludes this paper with a sum-
mary and some thoughts on future work.
2 Integrating Multiple Systems of
Unknown Type and Quality
When comparing (Eisele et al, 2008) to the
present work, our proposal is more general in a
way that the requirement for knowledge about the
systems is minimum. The types and the identities
of the participated systems are assumed unknown.
Accordingly, we are not able to restrict ourselves
to a certain class of systems as (Eisele et al, 2008)
did. We rely on a standard phrase-based SMT
framework to extract the valuable pieces from the
system outputs. These extracted segments are also
used to improve an existing SMT system that we
have access to.
While (Eisele et al, 2008) included translations
from all of a fixed number of RBMT systems
and added one feature to the translation model for
each system, integrating all given system outputs
in this way in our case could expand the search
space tremendously. Meanwhile, we cannot rely
on the assumption that all candidate systems ac-
tually have the potential to improve our baseline.
This implies the need for a first step of system se-
lection where the best candidate systems are iden-
tified and a limited number of them is chosen to be
included in the combination. Our approach would
not work without a small set of tuning data being
available so that we can evaluate the systems for
later selection and adjust the weights of our sys-
tems. Such tuning data is included in this year?s
42
task.
In this paper, we use the Moses decoder to con-
struct translations from the given system outputs.
We mainly propose two slightly different ways:
One is to construct translation models solely from
the given translations and the other is to extend
an existing translation model with these additional
translations.
3 Implementation
Despite the fact that the output of current MT sys-
tems is usually not comparable in quality to hu-
man translations, the machine-generated transla-
tions are nevertheless ?parallel? to the input so
that it is straightforward to construct a translation
model from data of this kind. This is the spirit
behind our method for combining multiple trans-
lations.
3.1 Direct combination
Clearly, for the same source sentence, we expect
to have different translations from different trans-
lation systems, just like we would expect from hu-
man translators. Also, every system may have its
own advantages. We break these translations into
smaller units and hope to be able to select the best
ones and form them into a better translation.
One single translation of a few thousand sen-
tences is normally inadequate for building a re-
liable general-purpose SMT system (data sparse-
ness problem). However, in the system combina-
tion task, this is no longer an issue as the system
only needs to translate sentences within the data
set.
When more translation engines are available,
the size of this set becomes larger. Hence,
we collect translations from all available systems
and pair them with the corresponding input text,
thus forming a medium-sized ?hypothesis? cor-
pus. Our system starts processing this corpus
with a standard phrase-based SMT setup, using the
Moses toolkit (Koehn et al, 2007).
The hypothesis corpus is first tokenized and
lowercased. Then, we run GIZA++ (Och and
Ney, 2003) on the corpus to obtain word align-
ments in both directions. The phrases are extracted
from the intersection of the alignments with the
?grow? heuristics. In addition, we also generate
a reordering model with the default configuration
as included in the Moses toolkit. This ?hypothe-
sis? translation model can already be used by the
Moses decoder together with a language model to
perform translations over the corresponding sen-
tence set.
3.2 Integration into existing SMT system
Sometimes, the goal of system combination is not
only to produce a translation but also to improve
one of the systems. In this paper, we aim at incor-
porating the additional system outputs to improve
an out-of-domain SMT system trained on the Eu-
roparl corpus (Koehn, 2005). Our hope is that the
additional translation hypotheses could bring in
new phrases or, more generally, new information
that was not contained in the Europarl model. In
order to facilitate comparisons, we use in-domain
LMs for all setups.
We investigate two alternative ways of integrat-
ing the additional phrases into the existing SMT
system: One is to take the hypothesis translation
model described in Section 3.1, the other is to
construct system-specific models constructed with
only translations from one system at a time.
Although the Moses decoder is able to work
with two phrase tables at once (Koehn and
Schroeder, 2007), it is difficult to use this method
when there is more than one additional model.
The method requires tuning on at least six more
features, which expands the search space for the
translation task unnecessarily. We instead inte-
grate the translation models from multiple sources
by extending the phrase table. In contrast to the
prior approach presented in (Chen et al, 2007) and
(Eisele et al, 2008) which concatenates the phrase
tables and adds new features as system markers,
our extension method avoids duplicate entries in
the final combined table.
Given a set of hypothesis translation models
(derived from an arbitrary number of system out-
puts) and an original large translation model to be
improved, we first sort the models by quality (see
Section 3.3), always assigning the highest priority
to the original model. The additional phrase tables
are appended to the large model in sorted order
such that only phrase pairs that were never seen
before are included. Lastly, we add new features
(in the form of additional columns in the phrase ta-
ble) to the translation model to indicate each pair?s
origin.
3.3 System evaluation
Since both the system translations and the ref-
erence translations are available for the tuning
43
set, we first compare each output to the reference
translation using BLEU (Papineni et al, 2001)
and METEOR (Banerjee and Lavie, 2005) and a
combined scoring scheme provided by the ULC
toolkit (Gimenez and Marquez, 2008). In our ex-
periments, we selected a subset of 5 systems for
the combination, in most cases, based on BLEU.
On the other hand, some systems may be de-
signed in a way that they deliver interesting unique
translation segments. Therefore, we also measure
the similarity among system outputs as shown in
Table 2 in a given collection by calculating aver-
age similarity scores across every pair of outputs.
de-en fr-en es-en en-de en-fr en-es
Num. 20 23 28 15 16 9
Median 19.87 26.55 22.50 13.78 24.76 23.70
Range 16.37 17.06 9.74 4.75 11.05 13.94
Top 5 de-en fr-en es-en en-de en-fr en-es
Median 22.26 27.93 26.43 15.21 26.62 26.61
Range 4.31 4.76 5.71 1.71 0.68 5.56
Table 1: Statistics of system outputs? BLEU scores
The range of BLEU scores cannot indicate the
similarity of the systems. The direction with the
most systems submitted is Spanish-English but
their respective performances are very close to
each other. As for the selected subset, the English-
French systems have the most similar performance
in terms of BLEU scores. The French-English
translations have the largest range in BLEU but the
similarity in this group is not the lowest.
de-en fr-en es-en en-de en-fr en-es
All 34.09 46.48 61.83 31.74 44.95 38.11
Selected 36.65 56.16 56.06 33.92 52.78 57.25
Table 2: Similarity of the system outputs
Ideally, we should select systems with highest
quality scores and lowest similarity scores. For
German-English, we selected the three with the
highest METEOR scores and another two with
high METEOR scores but low similarity scores to
the first three. For the other language directions,
we chose five systems from different institutions
with the highest scores.
3.4 Language models
We use a standard n-gram language model for
each target language using the monolingual train-
ing data provided in the translation task. These
LMs are thus specific to the same domain as the
input texts. Moreover, we also generate ?hypoth-
esis? LMs solely based on the given system out-
puts, that is, LMs that model how the candidate
systems convey information in the target language.
These LMs do not require any additional training
data. Therefore, we do not require any training
data other than the given system outputs by using
the ?hypothesis? language model and the ?hypoth-
esis? translation model.
3.5 Tuning
After building the models, it is essential to tune
the SMT system to optimize the feature weights.
We use Minimal Error Rate Training (Och, 2003)
to maximize BLEU on the complete development
data. Unlike the standard tuning procedure, we do
not tune the final system directly. Instead, we ob-
tain the weights using models built from the tuning
portion of the system outputs.
For each combination variant, we first train
models on the provided outputs corresponding to
the tuning set. This system, called the tuning sys-
tem, is also tuned on the tuning set. The initial
weights of any additional features not included in
the standard setting are set to 0. We then adapt the
weights to the system built with translations cor-
responding to the test set. The procedure and the
settings for building this system must be identical
to that of the tuning system.
4 Experiments
The purpose of this exercise is to understand the
nature of the system combination task in prac-
tice. Therefore, we restrict ourselves to the train-
ing data and system translations provided by the
shared task. The types of the systems that pro-
duced the translations are assumed to be unknown.
We report results for six translation directions be-
tween four languages.
4.1 Data and baseline
We build an SMT system from release v4 of the
Europarl corpus (Koehn, 2005), following a stan-
dard routine using the Moses toolkit. The sys-
tem also includes 5-gram language models trained
on in-domain corpora of the respective target lan-
guages using SRILM (Stolcke, 2002).
The systems in this paper, including the base-
line, are all tuned on the same 501-sentence tuning
set. Note also that the provided n-best outputs are
excluded in our experiments.
44
4.2 Results
The experiments include three different setups for
direct system combination, involving only hypoth-
esis translation models. System S0, the baseline
for this group, uses a hypothesis translation model
built with all available system translations and a
hypothesis LM (also from the machine-generated
outputs). S1 differs from S0 in that the LM in S1 is
generated from a large news corpus. S2 consists of
translation models built with only the five selected
systems. The BLEU scores of these systems are
shown in Table 3.
de-en fr-en es-en en-de en-fr en-es
Top 1 21.16 30.91 28.54 14.96 26.55 27.84
Mean 17.29 23.78 21.39 12.76 22.96 21.43
S0 20.46 27.50 23.35 13.95 27.29 25.59
S1 21.76 28.05 25.49 15.16 27.70 26.09
S2 21.71 24.98 27.26 15.62 24.28 25.22
Table 3: BLEU scores of direct system combina-
tion
When all outputs are included, the combined
system can always produce translations better than
most of the systems. When only a hypothesis LM
is used, the BLEU scores are always higher than
the average BLEU scores of the outputs. It even
outperforms the top system for English-French.
This simple setup (S0) is certainly a feasible so-
lution when no additional data is available and no
system evaluation is possible. This approach ap-
pears to be more effective on typically difficult
language pairs that involve German.
As for the systems with normal language mod-
els, neither of the systems ensure better transla-
tions. The translation quality is not completely
determined by the number of included translations
and their quality. On the other hand, the output
set with higher diversity (Table 2) usually leads
to better combination results. This observation is
consistent with the results from the system inte-
gration experiments shown in Table 4.
de-en fr-en es-en en-de en-fr en-es
Bas 19.13 25.07 24.55 13.59 23.67 23.67
Med 17.99 24.56 20.70 13.19 24.19 22.12
All 21.40 28.00 27.75 15.21 27.20 26.41
Top5 21.70 26.01 28.53 15.52 27.87 27.92
Table 4: BLEU scores of integrated SMT systems
(Bas: Baseline, Med: Median)
There are two variants in our experiments on
system integration. All in Table 4 represents the
system that integrates the complete hypothesis
translation model with the Europarl model, while
Top 5 refers to the system that incorporates the five
system-specific models separately. Both setups re-
sult in an improvement over the baseline Europarl-
based SMT system. BLEU scores increase by up
to 4.25 points. The integrated SMT system some-
times produces translations better than the best
system (7 out of 12 cases).
5 Conclusion
This work uses the Moses toolkit to combine
translations from multiple engines in a simple way.
The experiments on six translation directions show
interesting results: The final translations are al-
ways better than the majority of the given systems,
while the combination performs better than the
best system in half the cases. A similar approach
was applied to improve an existing SMT system
which was built in a domain different from the test
task. We achieved improvements in all cases.
There are many possible future directions to
continue this work. As we have shown, the qual-
ity of the combined system is more related to the
diversity of the involved systems than to the num-
ber of the systems or their quality. Hand-picked
systems lead to better combinations than those se-
lected by BLEU scores. It would be interesting
to develop a more comprehensive system selection
strategy.
Acknowledgments
This work was supported by the EuroMatrix
project (IST-034291) which is funded by the
European Community under the Sixth Frame-
work Programme for Research and Technological
Development.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Yu Chen, Andreas Eisele, Christian Federmann, Eva
Hasler, Michael Jellinghaus, and Silke Theison.
2007. Multi-engine machine translation with an
open-source SMT decoder. In Proceedings of
45
WMT07, pages 193?196, Prague, Czech Republic,
June. Association for Computational Linguistics.
Andreas Eisele, Christian Federmann, Herve? Saint-
Amand, Michael Jellinghaus, Teresa Herrmann, and
Yu Chen. 2008. Using Moses to integrate mul-
tiple rule-based machine translation engines into a
hybrid system. In Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 179?
182, Columbus, Ohio, June. Association for Compu-
tational Linguistics.
Jesus Gimenez and Lluis Marquez. 2008. A smor-
gasbord of features for automatic MT evaluation.
In Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 195?198, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-HMM-
based hypothesis alignment for combining outputs
from machine translation systems. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 98?107, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224?227,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbs. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of Annual meet-
ing of the Association for Computation Linguis-
tics (acl), demonstration session, pages 177?180,
Prague, Czech, June.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit 2005.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multi-
ple machine translation systems using enhanced hy-
potheses alignment. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 33?40, Trento, Italy, April.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160?
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 311?318, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Antti-Veikko I. Rosti, Spyridon Matsoukas, and
Richard M. Schwartz. 2007. Improved word-level
system combination for machine translation. In
ACL.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In the 7th International
Conference on Spoken Language Processing (IC-
SLP) 2002, Denver, Colorado.
46
Towards a Road Map on Human Language Technology: 
Natural Language Processing 
 
Editors: Andreas Eisele, Dorothea Ziegler-Eisele 
Version 2 (March 2002) 
Abstract 
This document summarizes contributions and discussions from two workshops that took place 
in November 2000 and July 2001.  It presents some visions of NLP-related applications that 
may become reality within ten years from now. It investigates the technological requirements 
that must be met in order to make these visions realistic and sketches milestones that may 
help to measure our progress towards these goals. 
1. Introduction  
Scope of this Document 
One of the items on ELSNET's agenda for the period 2000-2002 is to develop views on and 
visions of the longer-term future of the field of language and speech technologies and 
neighboring areas, also called ELSNET's Road Map for Human Language Technologies. As a 
first step in this process, ELSNET's Research Task group is organizing a series of 
brainstorming workshop with a number of prominent researchers and developers from our 
community.  The first one of these workshops took place in November 2000 under the general 
motto ?How will language and speech technology be used in the information world of 2010? 
Research challenges and infrastructure needs for the next ten years".  The second one was co-
organized in July 2001 by ELSNET and MITRE as part of ACL-2001 and had the somewhat 
more specific orientation on ?Human Language Technology and Knowledge Management 
(HLT-KM)?. This workshop brought together more than 40 researchers from industry and 
academia and covered a considerable range of topics related to KM and HLT in general. 
This paper aims at summarizing and organizing material from both workshops, but 
concentrates on applications and technologies that involve NLP, i.e. the processing of written 
natural language, as speech-related technologies and new models of interactivity have already 
been covered in documents presented around the first workshop. In the discussion of question 
answering and summarization, vision papers and roadmaps compiled by researchers in the US 
and published by NIST have been taken as an additional source of inspiration.   
The Growing Need for Human Language Technology 
Natural language is the prime vehicle in which information is encoded, by which it is 
accessed and through which it is disseminated.  With the explosion in the quantity of on-line 
 
text and multimedia information in recent years there is a pressing demand for technologies 
that facilitate the access to and exploitation of the knowledge contained in these documents.  
Advances in human language technology will offer nearly universal access to on-line 
information and services for more and more people, with or without skills to use computers. 
These technologies will play a key role in the age of information and are cited as key 
capabilities for competitive advantage in global enterprises.  
Extraction of knowledge from multiple sources and languages (books, periodicals, newscasts, 
satellite images, etc.) and the fusion into a single, coherent textual representation requires not 
only an understanding of the informational content of each of these documents, the removal 
of redundancies and resolution of contradictions.  Also, models of the user are required, the 
prior knowledge that can be assumed, the level of abstraction and the style that is appropriate 
to produce output that is suitable for a given purpose. 
More advanced knowledge management (KM) applications will be able to draw inferences 
and to present the conclusions to the user in condensed form, but let the user ask for 
explanations of the internal reasoning. In order to find solutions for problems beyond a static 
pool of knowledge, we need systems that are able to identify experts, who have solved similar 
problems. Again, advanced NLP capabilities will be required to appraise the aptitude of 
candidates from documents authored by them or describing prior performance. 
But also outside of KM, sophisticated applications of NLP will emerge over the next years 
and decades and find their way into our daily lives. The range of possibilities is almost 
unlimited. An important group of applications is related to electronic commerce, i.e. new 
methods to establish and maintain contact between companies and their customers. Via 
mobile phones, e-mail, animated web-based interfaces, or innovative multi-channel interfaces, 
people will want to make use of all kinds of services related to buying and selling goods, 
home-banking, booking of journeys, and the like. Also in the area of electronic learning a 
considerable growth is expected within the coming years.   
Multilinguality 
Whereas English is still the predominant language on the WWW, the fraction of non-English 
Web pages and sites is steadily increasing. Contrasting earlier apprehensions, the future will 
probably present ample opportunities for giving value to different languages and cultures. 
However, the possibility to collect information from disparate, multilingual sources also 
provides considerable challenges for the human user of these sources and for any kind of NLP 
technology that will be employed.  
One of the major challenges is lexical complexity. There will be about 200 different 
languages on the web and thus about 40.000 potential language pairs for translation.  Clearly, 
it will not be possible to build bilingual dictionaries that are comprehensive both in the 
number of language pairs and in the coverage of application domains.  Instead, multilingual 
vocabularies need to provide mappings into language independent knowledge organization 
structures, i.e. common systems of concepts linked by semantic relations.  However, the 
definition of such an ?interlingua? will be difficult in cases in which languages make 
distinctions of different granularity. 
Research Trends and Challenges 
The field of human language technology covers a broad range of activities with the goal of 
enabling people to communicate with machines using natural communication skills.  
 
Although NLP can help to facilitate knowledge management, it requires a large amount of 
specialized knowledge by itself.  This knowledge may be encoded in complex systems of 
linguistic rules and descriptions, such as grammars and lexicons, which are written in 
dedicated grammar formalisms and typically require many person-years of development 
effort.  The rules and entries in such descriptions interact in complex ways, and adaptation of 
such a sophisticated system to a new text style or application domain is a task that requires a 
considerable amount of specialized manpower. 
One way to cope with the difficulties in the acquisition of linguistic knowledge was to restrict 
attention to shallower tasks, such as looking for syntactic ?chunks? instead of a full syntactic 
analysis. Whereas this has proven rather successful for some applications, it obviously 
severely limits the depth to which the meaning of a document or utterance is taken into 
account. 
Another approach was to shift attention towards models of linguistic performance (what 
occurs in practice, instead of what is principally possible) and to use statistical or machine 
learning methods to acquire the necessary parameters from corpora of annotated examples. 
These data-driven approaches offer the possibility to express and exploit gradual distinctions, 
which is quite important in practice. They are not only easier to scale and adapt to new 
domains, their algorithms are also inherently robust, i.e. they can deal, to a certain extent, 
gracefully with errors in the input. 
Statistical parsers, trained on suitable tree banks, now achieve more than 90% precision and 
recall in the recognition of syntactic constituents in unseen sentences from English financial 
newspaper text. 
However, a lot of work remains to be done, and it is not obvious how the success of corpus-
driven approaches can be enlarged along many dimensions simultaneously.  One challenge is 
that analysis methods need to work for many languages, application domains and text types, 
whereas the manual annotation of large corpora of all relevant types will not be economically 
feasible.  Another challenge is that, other than syntax, many additional levels of analysis will 
be required, such as the identification of word sense, the reference of expressions, structure of 
argumentation and of documents, and the pragmatic role of utterances.  Often, the theoretical 
foundation that is required before the annotation of corpora can begin is still lacking. 
One could say that for corpus-driven approaches the issue of scalability of the required 
resources shows up again, albeit in a somewhat different disguise.  Hence, research in NLP 
will have to address this issue seriously, and find answers to the question how better tools and 
learning methods can reduce the effort of manual annotation, how annotated corpora of a 
slightly different type could best be re-used, how data-driven acquisition processes can 
exploit and extend existing lexicons and grammars, and finally how analysis levels for which 
the theoretical basis is still under development could be advanced in a data-driven way. 
Structure of this Document 
The remainder of this document is structured as follows.  In Chapter 2 we describe a number 
of prototypical applications and scenarios in which NLP will play a crucial role.  Whereas 
each of these scenarios is discussed mainly from a user?s perspective, we also give 
indications, which technological requirements must be met to make various levels of 
sophistication of these applications possible.  In Chapter 3, the technologies that have been 
mentioned earlier are discussed in more detail, and we try to indicate which levels of 
functionality may be expected within the timeframe of this study.  These building blocks are 
 
then put into a tentative chronological order, which is displayed in Chapter 4.  Finally, 
Chapter 5 gives some general recommendations about beneficial measures concerning the 
infrastructure for the relevant research. 
2. Applications of NLP 
Recent developments in natural language processing have made it clear that formerly 
independent technologies can be harnessed together to an increasing degree in order to form 
sophisticated and powerful information delivery vehicles. Information retrieval engines, text 
summarizers, question answering and other dialog systems, and language translators provide 
complementary functionalities which can be combined to serve a variety of users, ranging 
from the casual user asking questions of the web to a sophisticated, professional knowledge 
worker. 
Though one cannot strictly separate the following applications from each other, because one 
can act as a part of another, we try to dissect the large field of existing and future applications 
in the hope of making the field as a whole more transparent.  
Information Retrieval (IR) 
What is called information retrieval today is actually but a foretaste of what it should be.  
Current systems neither understand the information need of the user, nor the content of the 
documents in their repositories. Instead of meaningful replies, they just return a ranked, and 
often very long list of documents that are somehow related to the given query, which is 
typically very short.  A better name for this restricted functionality would be text retrieval. 
Information retrieval systems must understand a query, retrieve relevant information, and 
present the results. Retrieved information may consist of a long document, multiple 
documents of the same topic, etc and good systems should present the most important 
material in a clear and coherent manner. 
Current information retrieval techniques either rely on an encoding process using a certain 
perspective or classification scheme to describe a given item, or perform a superficial full-text 
analysis, searching for user-specific words. Neither case guarantees content matching. 
The ability to leverage advances in input processing (especially natural language query 
processing) together with advances in content-based access to multimedia artifacts (e.g., text, 
audio, imagery, video) promises to enhance the richness and breadth of accessible material 
while at the same time improving retrieval precision and recall and thus reducing the search 
time. Dealing with noisy, large scale, and multimedia data from sources as diverse as radio, 
television, documents, web pages, and human conversations (e.g., chat sessions and speech 
transcriptions) will offer challenges. 
One important part of IR would be multi-document summarization that can turn a large set of 
input documents into several different short summaries, which can then be sorted by topics or 
otherwise put into a coherent order.  
 
Summarization 
Summarization will enable knowledge workers access to larger amounts of material with less 
required reading time. The goal of automatic text summarization is to take a partially 
structured source text, extract information content from it and present the most important 
content in a condensed form in a manner sensitive to the needs of the user and task. 
Scalability to large collections and the generation of user-tailored or purpose-tailored 
summaries are active areas of research. 
The summarization can either be an extract consisting entirely of material copied from the 
input, or an abstract containing material not present in the input, such as subject categories, 
paraphrases of content, etc. 
For extraction shallower approaches are possible, as frequently the sentences may be 
extracted out of context. The transformation here involves selecting salient units and 
synthesizing them with the necessary smoothing (adjusting references, rearranging the 
text?). Training by using large corpora is possible. 
Abstracts need a deeper level of analysis, the synthesis involves natural language generation 
and some coding for a domain is required. 
Depending on their function, three types of abstracts can be distinguished: An indicative 
abstract provides a reference function for selecting documents for more in-depth reading. An 
informative abstract covers all the salient information in the source at some level of detail and 
evaluative abstracts express the abstractor?s views on the quality of the work of the author. 
Characteristics for the summarization are the reduction of the information content 
(compression rate), the fidelity to the source, the relevance to the user?s interest, and the well-
formedness regarding both to syntactic and discourse level. Extracts need to avoid gaps, 
dangling anaphora, ravaged tables and lists, abstracts need to produce grammatical, plausible 
output. 
Some current applications of summarization are: 
1. Multimedia news summaries: watch the news and tell what happened while I was 
away 
2. Physicians? aids: summarize and compare the recommended treatments for this patient 
3. Meeting summarization: find out what happened at that teleconference I missed 
4. Search engine hits: summarize the information in hit lists retrieved by search engines 
5. Intelligence gathering: create a 500-word biography of Osama bin Laden 
6. Hand-held devices: create a screen-sized summary of a book 
7. Aids for the Handicapped: compact the text and read it out for a blind person 
Though there are already promising approaches towards mastering all types of summaries, 
there are still obstacles to overcome such as the need for robust methods for the recognition of 
semantic relations, speech acts, and rhetorical structure. 
 
Question Answering  (QA) 
The straightest way to get access to the gigantic volume of knowledge around us is probably 
asking questions by communicating with other persons, computers or machines. 
An important new class of systems will move us from our current form of search on the web 
(type in keywords to retrieve documents) to a more direct form of asking questions in natural 
language, which are then directly responded to with an extracted or generated answer. 
Currently it is rather straightforward to get an answer to ?what questions? (what is the capital 
of China, what are the opening hours of the hermitage etc.), whereas ?why questions? (why 
did the new market fail) are normally not answered by an information retrieval query, unless 
the answer happens to be present in the information database, or can be inferred afterwards by 
the user from the answers she gets. 
In the next decade time has come to find answers to why questions from information systems 
by letting the systems make the appropriate inferences. This requires very sophisticated 
automatic reasoning methods, based on systematic extraction of information from texts, 
storing the information in a systematized way, which lends itself to reasoning and inference 
rules that will be able to draw the proper conclusions from the knowledge stored in the 
information database. 
We can subdivide the long-term goal of building powerful, multipurpose information 
management systems for QA in simpler subtasks that can be attacked in parallel at varying 
levels of sophistication, over shorter time frames.  
Clearly there is not a single, archetypical user of a Q&A system. In fact there is a full 
spectrum of questions, starting with simple factual questions, which could be answered in a 
single short phrase found in a single document (e.g. ?Where is the Taj Mahal??). Next, 
questions like ?What do we know about Company xyz??, where the answer cannot be found 
in a single document but will require retrieving multiple documents, locating portions of 
answers in them and combining them into a single response. This kind of question might be 
addressed by decomposing it into a series of single focus questions. 
Finally there are very complex questions, with broad scope, using judgment terms and 
needing deep knowledge of the user?s context to be answered. Imagine someone is watching a 
television newscast, becomes interested in a person, who appears to be acting as an advisor to 
the country?s Prime Minister. And now the person wants to know things like: ?Who is this 
individual. What is his background? What do we know about the political relationship of this 
person and the Prime Minister and/or the ruling party??. The future systems that can deal with 
this type of questions must manage the search in multiple sources in multiple 
media/languages, the fusion of information, resolution of conflicting data, multiple 
alternatives, adding interpretation, drawing conclusions.  
In order to realize this goal, research must deal with question analysis, response discovery and 
generation from heterogeneous sources, which may include structured and unstructured 
language data of all media types, multiple languages, multiple styles, formats and also image 
data i.e. document images, photography and video. 
To the extent to which NLP research will learn to master the challenges of source selection, 
source segmentation, extraction, and semantic integration across heterogeneous sources of 
unstructured and semi-structured data, NLP technology will help us to reduce the time, 
 
memory, and attention required to sift through many returned web pages from a traditional 
search by providing direct answers to questions. 
Semantic Web 
The standardization committee for the WWW (called W3C) expects around a billion web 
users by 2002 and an even higher number of available documents. However, this success and 
exponential grow makes it increasingly difficult to find, to access, to present, and to maintain 
the information of use to a wide variety of users. 
The semantic web will bring structure to the meaningful content of Web pages, creating an 
environment where software agents roaming from page to page can readily carry out 
sophisticated tasks for users. 
The semantic web is not a separate web but an extension of the current one, in which 
information is given well-defined meaning better enabling computers and people to work in 
cooperation. With the help of ontologies large amounts of text can be semantically annotated 
and classified. 
Currently pages on the web use representations rooted in format languages such as HTML or 
SGML. The information content, however, is mainly presented by natural language. Thus, 
there is a wide gap between the information available for tools that try to address the 
problems above and the information kept in human readable form. 
The semantic web will provide intelligent access to heterogeneous and distributed information 
enabling software agents to mediate between the user needs and the available information 
sources. 
The first steps in weaving the semantic web into the structure of the existing web are already 
under way. In the near future, these developments will usher in significant new functionality 
as machines become much better able to process and ?understand? the data that they merely 
display at present. 
What is required: creation of a machine understandable semantics for some or all of the 
information presented in the WWW i.e.  
? Developing languages for expressing machine understandable meta-information for 
documents, in the line of RDF, DAML, and similar proposals. 
? Developing terminologies (i.e., name spaces or ontologies) using these languages and 
making them available on the web. 
? Integrating and translating different terminologies 
? Developing tools that use such languages and terminologies to provide support in 
finding, accessing, presenting and maintaining information sources. 
Developing such languages, ontologies and tools is a wide-ranging problem that touches on 
the research areas of a broad variety of research communities.  
Creation of the relevant tools will require a better knowledge of what the users want to know 
from websites, i.e. these developments need to be based on a user-centered process view. 
 
Another crucial issue will be: ?Who is going to populate the semantic web??  The semantic 
markup that is required by automated software agents needs to be very easy to create and 
supporting tools need to be provided, otherwise this wonderful idea will not have significant 
impact for a long time. Advanced NLP technology that can ?guess? the correct semantic 
annotation and propose suitable markup semi-automatically will enable conformance to the 
needs of software agents with minimal manual effort. 
Dialogue Systems  
No matter if people want to buy something, find or use a service or just need information, 
dialog systems promise user-friendly and effective ways to achieve these goals, even for first 
time users.  
Despite the apparent resemblance to QA systems, there are several specific problems to be 
solved concerning dialogue modality and structure. Input to a dialog system might be via 
keypad, voice, pointing device, combinations thereof, or other channels, so all errors and 
incompleteness of spontaneous natural language will show up. In contrast to QA systems, 
there will be mixed initiatives of speaker and system and the scope is much wider if we take 
into account that the focus during natural dialogue may often change.  Also, the utterance 
made during a dialog can only be correctly interpreted based on the dialog context and the 
mutual knowledge that has been accumulated before it was made. 
In future we require systems that can support natural, mixed initiative human computer 
interaction that deals robustly with context shift, interruptions, feedback and shift of locus or 
control.  
Open research challenges include the ability to tailor flow and control of interactions and 
facilitate interactions including error detection and correction tailored to individual physical, 
perceptual and cognitive differences.  
Motivational and engaging life-like agents offer promising opportunities for innovation. 
Agent/user modeling: Computers can construct models of user beliefs, goals and plans as well 
as models of users? individual and collective skills by processing materials such as documents 
or user interactions/conversations. While raising important privacy issues, modeling users or 
groups of users unobtrusively from public materials or conversations can enable a range of 
important knowledge management capabilities  
tracking of user characteristic skills and goals enhances interaction as well as discovery of 
experts by other users or agents 
A central problem for the development of dialogue systems is the fact that contemporary 
linguistics is still struggling to achieve a genuine integration of semantics and pragmatics. A 
satisfactory analysis of dialogue requires in general both semantic representation i.e. 
representation of the content of what the different participants are saying and pragmatic 
information, i.e. what kinds of speech acts they are performing (are they asking a question, 
making a proposal?)  
Analysis of a dialog needs to explain the purpose behind the utterances it consists of. 
Determining the semantic representation of an utterance and its pragmatic features must in 
general proceed in tandem. A dialogue system identifying the relevant semantic and 
pragmatic information will thus have to be based on a theory in which semantics and 
 
pragmatics are both developed with the formal precision that is a prerequisite for 
implementation and suitably attuned to each other and intertwined. 
Applications in Electronic Commerce 
New technological possibilities can quickly impact the interaction between companies and 
their customers. One example are dialog systems that allow customers to obtain personal 
advises or services. For reasons indicated above, these systems are difficult to build, but once 
this investment has been done, they can be operated at low cost for the company. 
Another example, which may be even sooner to come, is the creation of systems that support 
processing of emails sent by customers.  According to business analyses, e-mail has already 
now become one of the most common forms of customer communication. For numerous 
businesses that are not well-prepared, this has transformed e-mail into a severe pain point, 
giving rise to the pressing need to adopt e-mail response management systems. 
Obviously, NLP technologies that are able to extract the salient facts from email messages 
can constitute a central part of these systems.  Due to the potential complexity of the queries 
and additional problems like ungrammatical input and spelling errors, the correct 
interpretation of arbitrary messages is far from easy.  However, there are several factors that 
alleviate the situation: Messages that are too difficult for automatic processing can be routed 
to human agents. In cases in which doubts about the correctness of generated responses 
persist, these responses can always be checked by manual inspection. Historical data about 
email exchange with customers can be used to bootstrap the models that are required for the 
system. Depending on the business, a significant fraction of the emails may be amenable to 
NLP, including requests for information material, business reports, certificates, statements of 
account, scheduling requests, conference registrations etc. 
e-Learning 
Using modern technology to facilitate learning is one of the most promising application 
domains of NLP.  Good QA systems that are able to give answers to the point, or 
summarization systems that can adapt to the user?s prior knowledge and present important 
additions in a way that is easy to understand could immediately take the place of a good 
teacher, which an unlimited supply of time and patience.  One technology is ripe to build 
these tools, using them for e-learning will one of the biggest opportunities to our knowledge 
society. 
However, as the European society evolves more and more into multilingualism, it is natural to 
ask how NLP can help to make language learning easier and more effective. We can imagine 
systems to help train children to write and to speak a foreign language. There will be 
combinations of multi-modal aids for the handicapped. A child will write a sentence and the 
system will correct it and tutor him about the problems. A child will read a text aloud and the 
system will monitor which words are not right and why and will analyze where the 
pronunciation problems are. Later the system would suggest some pronunciation exercises in 
the particular problem. 
Systems that are able to guess the intention of a speaker from the speaker?s utterances in a 
flexible and intelligent way will offer a plethora of possibilities for e-learning. As similar 
capabilities are required for dialog systems in general, there will be significant synergy effects 
between these fields of research. 
 
Translation 
The idea of machine translation (MT) has been one of the driving forces in the early days of 
NLP.  However, even after more than 50 years of effort, current systems still produce output 
of limited quality, which is suitable for assimilation of foreign-language documents, but not 
for the production of publishable material. But even if the old dreams did not come true, MT 
will play an increasing role in the multilingual world. 
Last year, for the first time, English constituted less than half the material on the web. Some 
predict that Chinese will be the primary language of the web by 2007. Given that information 
on the web will increasingly appear in foreign languages and not all users will be fluent in 
those languages, there will be a need to gist or skim content for relevance assessment and/or 
provide high quality translation for deeper understanding. Some forms of translation for 
information access is already today available in the web at no cost. The increasing demand for 
these services will give a push to improve their quality and the providers will find ways to 
increase vocabularies and translation quality semi-automatically from terminological 
resources, bilingual corpora and similar sources.  Also the need for interactive systems that 
can give rough translations of chat sessions in real time will create interesting challenges. 
Clearly, any systematic collection of lexical and terminological information in the form of 
domain-specific ontologies will help to build better MT systems for these domains.  
Conversely, the construction of ontologies can be facilitated by automatic alignment of 
existing translations, as this will naturally lead to a clustering of the vocabulary along the 
relevant semantic distinctions. 
These developments will also have an impact on improved systems for high-quality 
translation for the dissemination of documents. Chances are that hybrid combinations of 
symbolic and stochastic translation engines, able to learn relevant terminology from 
translation memories will eventually achieve a level of performance that will make them 
useful for the professional translator. Combined with multi-modal workbenches where voice 
input, keyboard and mouse interaction will make the composition of the target text as 
convenient as possible, these new technologies may help at least in some easier domains, 
where so far the effort of the human translator is dominated by low-level activities such as 
entering the text, adjusting the formatting, copying names and numbers, which are clearly 
amenable to partial automation.  
3. Technologies for NLP 
This chapter contains a more detailed discussion of some of the technologies that are required 
for the applications mentioned in the last chapter. Most of the material is organized along 
traditional fields of research in NLP, describing technologies that already exist, but must be 
further developed to achieve the ambitious goals.  Some technologies cannot be assigned to 
one specific level, because they serve a more generic purpose, such as the extraction of 
relevant knowledge from text corpora.  
Low-Level Processing 
Most systems that analyse natural language text typically start by segmenting the text into 
meaningful tokens.  Sometimes, the exact spelling of these tokens needs to be brought into a 
 
canonical form, so that it can match with a lexical entry. Both processes can be based on 
matching the input against regular expressions, for which efficient algorithms exist. Whereas 
this task looks straightforward from the distance, there are actually some subtle details that 
need to be considered.  Quite often, a decision whether a word should be split at a special 
character or whether a dot ends a sentence or is part of the preceding word depends on the 
vocabulary of the domain and on layout conventions used in this document, so that general 
rules cannot be defined.  Documents that need to be analyzed may contain markup from text 
processors, which needs to be stripped or interpreted in a suitable way. The knowledge 
required in these preliminary stages of processing can already be quite specific, so that a 
manual creation of suitable rule systems is not economically feasible. 
Current research on the automatic tokenization and normalization of texts therefore 
concentrates on the question how the knowledge required by these methods can automatically 
be derived from examples, using techniques statistical or machine learning approaches. 
Another difficulty is the treatment of noise in the input.  Output of speech recognition systems 
often contains recognition errors at rather high rates. Utterances entered interactively or 
printed documents that have undergone OCR have similar problems.  Unfortunately, the 
distortion of even a single character can mess up the linguistic analysis of the complete input.  
But of course, we expect NLP systems to deal gracefully and intelligently with small 
distortions and errors in the input. 
To make systems more robust against noisy input, probabilistic techniques for the restoration 
of distorted signals,  which have shown to be quite effective in speech recognition, need to be 
adapted and generalized to new applications.  However, training simple-minded statistical 
models on massive amounts of data will often not be feasible.  By now, statistical language 
models that incorporate grammatical knowledge are able to give slight improvements over n-
gram approaches, and it seems plausible to expect that future improvements of these will be 
easier to use in specific situation where training data is scarce. Large vocabularies, many 
types of distortions, and the need to use fine-grained contextual knowledge for improved 
predictive models constitute significant research challenges.  Most likely, there will be some 
synergy between language models used in speech and similar models that will be developed 
for low-level processing and correction of written ill-formed input. 
Once the segmentation into basic units has been performed, the next step is to identify 
suitable lexical entries for each token and, in cases where more than one entry applies, to 
determine which one is most appropriate in the given context. This process is called part-of-
speech disambiguation or POS tagging and is usually done with statistical models or machine-
learning approaches trained on manually tagged data.  Current technology achieves rather 
high accuracy on newspaper text, but again, performance suffers significantly when a model 
trained on a certain set of data is applied to text from a different domain. As the output of the 
POS tagger is typically used as input to subsequent modules, tagging errors may hamper the 
correct analysis of much more than the affected word. Research on high-quality POS tagging 
will face problems that are similar to those of language modelling: It requires detailed 
information about a large number of rare words that may be quite specific to the given domain 
and application, which is difficult to construct, no matter which road to lexical acquisition is 
taken.  Any effort that will support the construction, distribution, sharing and re-use of large, 
domain-specific lexical resources will doubtlessly also help to improve the accuracy of POS 
tagging on text from these domains. 
 
The next step in the analysis of text is to identify groups of words that belong together and 
refer to one semantic entity. Often, these phrases contain names, and for many practical 
applications, it is important to classify these expressions according to the type of entity they 
denote (Person, City, Company, etc.).  Depending on the application, the classification may 
be more or less fine-grained. Again, it is obvious that improved lexical knowledge will help to 
improve the performance of named entity recognition. But we cannot in all cases rely on a 
lexical resource to cover the relevant entities.  A text may discuss the opening of a new 
company, which will therefore not be contained in the lexicon. To handle such cases 
intelligently, we need mechanisms that can exploit contextual clues for the correct 
classification of unknown entities and we need effective mechanisms that propagate 
information about new entities into the lexical repositories, so that the system as a whole 
learns from the texts it sees, similar to the way a human reader would do.  
Syntactic Analysis 
The goal of syntactic analysis is to break down given textual units, typically sentences, into 
smaller constituents, to assign categorical labels to them, and to identify the grammatical 
relations that hold between the various parts. 
In most applications of language technology the encoded linguistic knowledge, i.e. the 
grammar, is separated from the processing components. The grammar consists of a lexicon, 
and rules that syntactically and semantically combine words and phrases into larger phrases 
and sentences. 
Several language technology products on the market today employ annotated phrase-structure 
grammars, grammars with several hundreds or thousands of rules describing different phrase 
types. Each of these rules is annotated by features and sometimes also by expressions in a 
programming language. 
The resulting systems might be sufficiently efficient for some applications but they lack the 
speed of processing needed for interactive systems, such as applications involving spoken 
input, or systems that have to process large volumes of texts, as in machine translation. 
In current research, a certain polarization has taken place. Very simple grammar models are 
employed, e.g. different kinds of finite-state grammars that support highly efficient 
processing. Some approaches do away with grammars altogether and use statistical methods 
to find basic linguistic patterns. Other than speed, these shallow and statistically trained 
approaches have advantages in terms of robustness, and they also implicitly perform 
disambiguation, i.e. when more than one analysis is possible, they make a decision for one 
reading (which of course may be the wrong one). 
On the other end of the scale, we find a variety of powerful linguistically sophisticated 
representation formalisms that facilitate grammar engineering. These systems are typically set 
up in a way that all logically possible readings are computed, which increases the clarity (no 
magic heuristics hidden in procedures), but also slows down the processing.  Despite their 
nice theoretical properties it has so far been difficult to adapt these systems to the needs of 
real-world applications, where speed, robustness, and partial correctness in typical cases are 
more urgent than theoretical faithfulness and depth of analysis. 
How will this situation evolve? The two approaches will continue to compete for potential 
applications, and the current advantage for shallow approaches will diminish as more 
ambitious applications get within reach, and as languages are used that require richer analysis.  
 
This will give incentives for shallow approaches to struggle for higher accuracy and more 
detailed analyses, whereas the deep processing will be forced to find workable solutions for 
the problems with speed and robustness.  In the ideal case, more fine-grained forms of 
integration will be found, i.e. hybrid systems that will keep the advantages of both worlds as 
far as possible. 
The simplest integration will just use shallow analysis as a fallback mechanism when deep 
analysis fails. In this case, results from both approaches need to be translated into one 
common representation, and the development of such a ?common denominator? will be a 
significant challenge.  To achieve an even more fine-grained cooperation between both 
approaches, deep analysis may be equipped with the ability to locally fall back to more 
superficial processing, driven by the need to deal with a specific problem in the input. Vice 
versa, the results of shallow analysis might be combined into a more detailed structure 
incrementally, based on rules from a deep grammar.  Also analyses of corpus data obtained 
with shallow tools can be mined for linguistic knowledge that is then fed into resources used 
by a deep parser, and vice versa. 
Research challenges will be how to find syntactic parsers that are at the same time fast, 
robust, deliver a detailed analysis that is correct with high probability and that are easily to 
adapt to special domains. 
Semantic Analysis 
The goal of semantic analysis is to assign meanings to utterances, which is an essential 
precondition for most applications of NLP.  However, what level of abstraction is required in 
this phase depends on the difficulty of the task.  Extraction of answers to simple factual 
questions from a given text will require less depth in analysis than the summarization of a 
lengthy treatise in few paragraphs. 
We can dissect the task of semantic analysis into several subtasks, depending on the linguistic 
level where it takes place.  Most important are the semantic tagging of ambiguous words and 
phrases, and the resolution of referring expressions. 
The disambiguation of word senses needs to identify the meaning that should be assigned to a 
given word. The hardest part of this task is to define the set of meanings that should be 
considered in this task, i.e. to select the appropriate granularity for the conceptualization.  The 
emergence of standardized, large-scale ontological resources will help to solve this part of the 
task, as the concepts that appear in such ontologies are a natural choice for the meanings of 
single words or simple phrases. Additionally, multilingual corpora that are aligned on the 
level of words and phrases can serve as an approximation to sense-tagged corpora, so draft 
ontologies and models for sense disambiguation can be extracted from these. 
Considerable efforts in defining useful evaluation metrics for sense disambiguation are 
pursued in the ongoing SENSEVAL activities.  So far, the methods used by the participants of 
SENSEVAL are mostly based on simple statistical classification using features extracted from 
the context of word occurrences. To the extent to which robust, high quality systems for 
syntactic analysis will appear, this will also help to obtain improved accuracy in the semantic 
disambiguation. 
The resolution of referring expression such as pronouns or definite noun phrases is the ability 
to identify their target, which may be expressions that appear prior in the text, abstractions of 
material that appeared earlier, or entities that exist independently from the text in existing 
 
background knowledge. Seen in a more general way, the task is to cull out objects and events 
from multimedia sources (text, audio, video). An example challenge includes extracting 
entities within media and correlating those across media. For example this might include 
extracting names or locations from written/spoken sources and correlating those with 
associated images. Whereas commercial products exist to extract named entities from text 
with precision and recall in the ninetieth percentile, domain independent event extractors 
work at best in the fiftieth percentile and performance degrades further with noisy, corrupted, 
or idiosyncratic data. 
Therefore work on the resolution of referring expression and the identification of entities in 
text and multimedia documents remains important fields of activity for the future. 
Discourse and Dialogue 
Extracting the knowledge contained in documents and understanding and generating natural 
dialog behavior requires more than the resolution of local semantic ambiguities.  Intelligent 
analysis needs to consider the global argumentative structure of documents and discourse, and 
dialogs need to be analyzed for pragmatic content. 
Computational work in discourse has focused on two different types of discourse: extended 
texts and dialogues, both spoken and written, yet there is a clear overlap between these two: 
dialogues contain text-like sequences spoken by a single individual and texts may contain 
dialogues. But application opportunities and needs are different. Work on text is of direct 
relevance to document analysis and retrieval applications, whereas work on dialogue is of 
import for human-computer interfaces regardless of the modality of interaction. Both are 
divisible into segments (discourse segments and phrases) with the meaning of the segments 
being more than the meaning of the individual parts. 
The main focus of the research is the interpretation beyond sentence boundaries, the 
intentional and informational approach. 
According to the informational approaches, the coherence of discourse follows from semantic 
relationships between the information conveyed by successive utterances. As a result, the 
major computational tools used here are inference and abduction on representations of the 
propositional content of utterances. 
According to the intentional approaches the coherence of discourse derives from the 
intentions of speakers and writers and understanding depends on recognition of those 
intentions. 
One difficulty is to build models of human-machine-dialog when initially only examples of 
human-human interaction exist, which may not be relevant.  Bootstrapping suitable models 
will therefore require Wizard-of-Oz studies with simulated systems. 
Natural Language Generation 
In many of the applications mentioned above, systems need to produce high-quality natural 
language text from computer-internal representations of information. Natural language 
generation can be decomposed into the tasks of text planning, sentence planning and surface 
realization. Text planners select from a knowledge pool which information to include in the 
output and out of this create a text structure to ensure coherence. On a more local scale, 
sentence planners organize the content of each sentence, massaging and ordering its parts.  
 
Surface realizers convert sentence-sized chunks of representation into grammatically correct 
sentences. 
Generator processes can be classified into points on a range of sophistication and expressive 
power, starting with inflexible canned methods and ending with maximally flexible feature 
combination methods. It is safe to say that at the present time one can fairly easily build a 
single-purpose generator for any specific application, or with some difficulty adapt an 
existing sentence generator to the application, with acceptable results. However, one cannot 
yet build a general-purpose sentence generator or a non-toy text planner. Several significant 
problems remain without sufficiently general solutions: 
? Lexical selection is one of the most difficult problems in generation. At its simplest 
this question involves selecting the most appropriate single word for a given unit of 
input. However as soon as the semantic model approaches a realistic size and as soon 
as the lexicon is large enough to permit alternative locutions, the problem becomes 
very complex. The decision depends on what has already been said, what is 
referentially available from context, what is most salient, what stylistic effect the 
speaker wishes to produce and so on. What is required: development of theories about 
and implementations of lexical selection algorithms, for reference to objects, events 
states, etc., and tested with large lexical. 
? Discourse structure (see also there) So far, no text planner exists that can reliably plan 
texts of several paragraphs in general. What is required: Theories of the structural 
nature of discourse, of the development of theme and focus in discourse, and of 
coherence and cohesion; libraries of discourse relations, communicative goals and text 
plans: implemented representational paradigms for characterizing stereotypical texts 
such as reports and business letters; implemented text planners that are tested in 
realistic non-toy domains. 
? Sentence planning: Even assuming the text planning problem is solved, a number of 
tasks remain before well-structured multi-sentence text can be generated: These tasks, 
required for planning the structure and content of each sentence, include: pronoun 
specification, theme signaling, focus signaling, content aggregation to remove 
unnecessary redundancies, the ordering of prepositional phrases, adjectives, etc. What 
is required: Theories of pronoun use, theme and focus selection and signaling, and 
content aggregation; implemented sentence planners with rules that perform these 
operations; testing in realistic domains. 
? Domain modeling: a significant shortcoming in generation research is the lack of 
large, well-motivated application domain models, or even the absence of clear 
principles by which to build such models. A traditional problem with generators is that 
the inputs are frequently hand-crafted, or are built by some other system that uses 
representation elements from a fairly small hand-crafted domain model, making the 
generator?s inputs already highly oriented toward the final language desired?.What is 
required: Implemented large-size (over 10.000 concepts) domain models that are 
useful both for some non-linguistic application and for generation; criteria for 
evaluating the internal consistency of such models; theories on and practical 
experience in the linking of generators to such models: lexicon of commensurate size. 
 
Probably the problem least addressed in generator systems today is the one that will take the 
longest to solve. This is the problem of guiding the generation process through its choices 
when multiple options exist to handle any given input. 
The generator user has to specify not only the semantic content of the desired text, but also its 
pragmatic ? interpersonal and situational ? effects. Very little research has been performed on 
this question beyond a handful of small-scale pilot studies. What is required: Classifications 
of the types of reader characteristics and goals, the types of author goals, and the interpersonal 
and situational aspects that affect the form and content of language; theories of how these 
aspects affect the generation process; implemented rules and/or planning systems that guide 
generator systems? choices; criteria for evaluating appropriateness of general text in specified 
communicative situations. 
Effective presentations require the appropriate selection of content, allocation to media, and 
fine grained coordination and realization in time and space. Discovery and presentation of 
knowledge may require mixed media (e.g., text, graphics, video, speech and non-speech 
audio) and mixed mode (e.g., linguistic, visual, auditory) displays tailored to the user and 
context. This might include tailoring content and form to the specific physical, perceptual, or 
cognitive characteristics of the user. It might lead to new visualization and browsing 
paradigms for massive multimedia and multilingual repositories that reduce cognitive load or 
task time, increase analytic depth and breadth, or simply increase user satisfaction. A grand 
challenge is the automated generation of coordinated speech, natural language, gesture, 
animation, non-speech audio, generation, possibly delivered via interactive, animated lifelike 
agents. Preliminary experiments suggest that, independent of task performance, agents may 
simply be more engaging/motivating to younger and/or less experienced users. 
Ontologies 
Large-scale ontologies are becoming an essential component of many applications including 
standard search (such as Yahoo and Lycos), e-commerce (such as Amazon and eBay), 
configuration (such as Dell and PC-Order), and government intelligence (such as DARPA?s 
High Performance Knowledge Base program).  As discussed in the preceding paragraphs, 
ontologies will constitute a major source of knowledge needed for several levels of NLP. 
Ontologies are increasingly seen as an important vehicle for describing the semantic content 
of web-based information sources and they are becoming so large that it is not uncommon for 
distributed teams of people to be in charge of the ontology development, design, population, 
and maintenance. 
Ontologies define a vocabulary for researchers who need to share common understanding of 
the structure of information in a domain. It includes machine-interpretable definitions of basic 
concepts in the domain and relations among them. The principal reasons to use an ontology in 
machine translation (MT) and other language technologies are to enable source language 
analyzers and target language generators to share knowledge, to store semantic constraints 
and to resolve semantic ambiguities by making inferences using the concept network of the 
ontology. An ontology contains only language independent information and many other 
semantic relations as well as taxonomic relations. 
Though the utility of domain ontologies is now widely acknowledged in the IT (Information 
Technology) community, several barriers must be overcome before ontologies become 
practical and useful tools. One important achievement would be to reduce the time and cost of 
identifying and manually entering several thousand concept descriptions by developing 
 
automatic ontology construction.  Another important task is to find arrangements that make 
development and sharing of ontologies commercially attractive.  
Some challenges for ontology research:  
Work on ontologies needs to provide generally applicable top-ontologies that cover most 
important core concepts that will be needed for many domains.  Extensions to new domains 
could then start by enriching these top-ontologies in a specific direction, reducing the initial 
effort for creating new ontologies, for merging independently developed extensions, and for 
rapid customisation of existing ontologies. 
This requires that ontology-creators are willing to share parts of their work and find suitable 
processes to organize cooperation.  It also requires the development of standards for the 
languages in which ontologies are specified and can be interchanged (e.g. along the lines of 
the OIL proposal).  Here, the challenge is to find suitable compromises between expressive 
power and depth on one hand and ease of use on the other hand.  Ideally, one specification 
language should be able to cover the whole spectrum up to advanced knowledge 
representation as used in the CYC project.  
Incremental improvement of ontologies needs to be facilitated by specialized tools for easy 
visualization and modification. These tools (and the representations they work on) need to be 
domain-independent and suited even for casual users, and their design needs to be based on a 
user-centred process view.   
It must be easy to plug in ontologies into various NLP-based tools such as tools for 
information extraction, organization and annotation of document collections (semantic Web), 
environments for terminology management and controlled language. This will permit to audit 
the contained knowledge in manifold ways, and will allow for rapid quality improvement. 
What is required: tools that support broad ranges of users in (1) merging of ontological terms 
from varied sources, (2) diagnosis of coverage and correctness of ontologies, and (3) 
maintaining ontologies over time.  
Lexicons 
Lexical knowledge ? knowledge about individual words in the language ? is essential for all 
types of natural language processing. Developers of machine translation systems, which from 
the beginning have involved large vocabularies, have long recognized the lexicon as a critical 
(and perhaps the critical) system resource. As researchers and developers in other areas of 
natural language processing move from toy systems to systems which process real texts over 
broad subject domains, larger and richer lexicons will be needed and the task of lexicon 
design and development will become a more central aspect of any project. 
A basic lexicon will typically include information about morphology and on the syntactic 
level, the complement structures of each word or word sense. A more complex lexicon may 
also include semantic information, such as a classification hierarchy and selectional patterns 
or case frames stated in terms of this hierarchy. For machine translation, the lexicon will also 
have to record correspondences between lexical items in the source and target language; for 
speech understanding and generation, it will have to include information about the 
pronunciation of individual words. For this purpose the overall lexicon architecture and the 
representation formalism used to encode the data are important issues. 
 
No matter if we want to build an ontology or a lexicon, in general for this kind of high-quality 
semantic knowledge base, manual processing is indispensable. Traditionally computer 
lexicons have been built by hand specifically for the purpose of language analysis and 
generation. However, the needs for larger lexicons are now leading to efforts for the 
development of common lexical representations and co-operative lexicon development. 
The area is ripe ? at least for some levels of linguistic description ? for reaching in the short 
term a consensus on common lexical specifications. We must expand the experiences with the 
sorts of semantic knowledge that could be effectively used by multiple systems. We must also 
recognize the importance of the rapidly growing stock of machine-readable text as a resource 
for lexical research. The major areas of potential results in the immediate future seem to lie in 
the combination of lexicon and corpus work. There?s a growing interest from many groups in 
topics such as sense tagging or sense disambiguation on very large text corpora, where lexical 
tools and data provide a first input to the systems and are in turn enhanced with the 
information acquired and extracted from corpus analysis. 
Machine Learning 
As mentioned above, the acquisition of knowledge continues to impose on of the biggest 
difficulties to the application of NLP technologies. This holds both for linguistic knowledge 
(grammars lexicons) and for world knowledge (ontologies, facts).  In order to make 
extensions of NLP to new domains possible, the acquisition process needs to be supported by 
algorithms that can exploit existing textual material and extract knowledge of various types 
from it. 
Approaches to these methods can be found in various fields of research, such as statistical 
language models, bilingual alignment, grammar induction, statistical parsing, statistical 
classification technology, Bayesian networks and other ML methods used in artificial 
intelligence research, data mining techniques etc. 
Due to the specific nature of lexical information, it is important to pick or develop methods 
that scale to large vocabularies and large sets of features and that can exploit multiple sources 
of evidence in a good way.  Also, the methods need to be able to use a rich set of existing 
background knowledge, so that no effort is wasted in re-discovering what was already known. 
It is important to have methods that can use richly annotated training data, but do not require 
that large datasets have to be annotated in this way.  Instead, methods should be able to draw 
a maximum of advantage from raw data without annotation using unsupervised learning 
approaches.  Also, it will be important to guide the effort of human annotation so that time is 
spent in the most efficient way, using active learning methods.  Tools and processes for 
managing annotation projects (including assessment of quality levels) need to be developed 
and shared on a broad basis. 
Whenever possible, one should try to use models that contain explicit linguistic 
representations (ideally organized along different strata) so that partial reuse of models and 
rapid adaptation to slightly different is facilitated. 
 
4. Milestones 
Some relevant items not included in Bernsen 2000. 
 
Basic technologies 
Short term  
- accurate syntactic analysis for well-formed input from specific domains 
- simple methods for minimizing annotation effort during domain adaptation 
- ML algorithms that combine active and unsupervised learning for optimal exploitation 
of data 
- generally applicable annotation schemes for semantic markup of text 
- standards for encoding and exchange of ontological resources emerge 
- top-level ontologies generally available 
- tools for semi-automatic construction and population of ontologies from text 
- tools for simple semantic enrichment of Web pages 
- approaches to markup of discourse structure and pragmatics 
Medium term 
- improved methods for minimizing annotation effort during domain adaptation 
- tools for adaptation of syntactic analysis to specific application with minimal human 
effort 
- accurate syntactic analysis for slightly ill-formed input for restricted domains 
- improved syntactic analysis of input with uncertainties (word lattices) 
- machine learning methods that exploit and extend existing knowledge sources 
- sufficiently accurate semantic analysis of free text from restricted domains 
- generic schemes for the annotation of pragmatic content 
- schemes for annotation of discourse and document structure 
- generally usable ontologies exist for many domains 
- NL generation verbalizes information extracted/deduced from multiple sources for QA 
- Agent/user models for dialogs of moderate complexity 
 
Long term 
- accurate syntactic analysis for ill-formed input from multiple domains 
- sufficiently accurate semantic analysis of free text from multiple domains 
- recognition of pragmatic content in text and dialog 
- NL generation produces stylistically adequate and well-structured text 
Systems 
Short term  
- QA systems are able to answer simple factual questions  
- Summarization system produce well-formed extracts from short documents 
- automated e-mail response systems deliver high-quality replies in easy cases 
- MT for information assimilation 
Medium term 
- QA systems that deduce answers from information in multiple sources  
- Summarization systems are able to merge multiple documents 
- Summarization systems are able to deliver different types of summaries 
- Integration of translation memories with MT enables fast domain-adaptation  
- Mixed-initiative dialogue systems for services and e-commerce 
Long term 
- Translator?s workbenches based on TM, MT, and multi-modal input facilities 
- QA systems that are able to explain their reasoning 
5. Recommendations for NLP research in Europe 
1. Build and make publicly available at low cost large-scale multilingual lexical 
resources, with broad coverage, generic enough to be reusable in different application 
frameworks  
2. To turn special attention to the development of better ontologies which are reusable 
across domains in order to encode static world knowledge 
3. Creation of large common accessible multilingual corpora of syntactical and 
semantically annotated data annotated also beyond sentence boundaries  
 
4. Encourage development of statistical and machine-learning methods that facilitate 
bootstrapping of linguistic resources 
5. Common standards will improve the effectiveness of people?s cooperation, the 
identification of the requirements for the system specification, the inter-operability 
among systems and the possibility of re-using and sharing system components. 
6. Integration of language processing into the rest of cognitive science, artificial 
intelligence and computer science e.g. some ambitious projects centered on NL but 
combining various techniques and different areas of AI. New type of projects: Very 
different for scale, ambition and timeframe 
7. Establishment of centers of excellence as focus points for projects for a period of five 
to ten years. 
8. Encourage systematic evaluations (but how ?) 
6. References 
? Berners-Lee, T. (2001) The Semantic Web, Scientific American (5/2001) 
? Bernsen , N.O. (2000) Speech-Related Technologies. Where will the field go in 10 
years? roadmap workshop, Katwijk 
? Burger, J. e.a. (2000) Issues, Tasks and Program Structures to Roadmap Research in 
Question & Answering, Memo National Institute of Standards and Technology, 
Gaithersburg 
? Carbonell, J. e.a. (2000) Vision Statement to Guide Research in Q&A and Text 
Summarization, Memo National Institute of Standards and Technology, Gaithersburg 
? Cole,R.A. (Ed.). (1997) Survey of the State of the Art in Human Language 
Technology Cambridge University Press, Cambridge 
? Declerck, Th., Wittenburg, P., Cunningham, H. (2001) The Automatic Generation of 
Formal Annotations in a Multimedia Indexing and Searching Environment, ACL 
Workshop, Toulouse 
? Delannoy, J.-F. (2001) What are the points? What are the stances? Decanting for 
question-driven retrieval and executive summarization, ACL Meeting, Toulouse 
? Fensel, D. Hendler, J., Lieberman, H. ,Wahlster, W. (2000) Dagstuhl-Seminar: 
Semantics for the WWW, Dagstuhl, Germany 
? Grishman, R. and Calzolari, N. Lexicons in Survey of the State of the Art in Human 
Language Technology, Cambridge University Press, Cambridge 
? Grosz, B. (1997) Discourse and Dialogue in Survey of the State of the Art in Human 
Language Technology, Cambridge University Press, Cambridge 
 
 ? Heisterkamp, P., (2000) Speech Technology in the year 2010, roadmap workshop, 
Katwijk 
? Hirschman, L. and Thompson, H.S. (1997) Evaluation in Survey of the State of the 
Art in Human Language Technology, Cambridge University Press, Cambridge 
? Hovy, E., (1997) Language Generation in Survey of the State of the Art in Human 
Language Technology, Cambridge University Press, Cambridge 
? Kang, S.-J. and Lee, J.-H. (2001) Semi-Automatic Practical Ontology Construction by 
using Thesaurus, Computational Dictionaries, and Large Corpora, ACL workshop 
Toulouse 
? Kay, M. (1997) Machine Translation: The Disappointing Past and Present. In: Survey 
of the State of the Art in Human Language Technology, Cambridge University Press, 
Cambridge 
? Kay, M. (1997) Multilinguality. In: Survey of the State of the Art in Human Language 
Technology, Cambridge University Press, Cambridge 
? Knight, K. (2001) Language Modeling for Good Generation, Workshop on Language 
Modeling and Information Retrieval, Pittsburgh 
? Krauwer, St., (2000) Going from ?what? to ?why? across language barriers in the 
unified distributed information space. Roadmap workshop, Katwijk 
? Maybury, M.T. and Mani, I., (2001) Automatic Summarization, ACL Meeting 
Toulouse 
? Maybury, M.T., (2001) Human Language Technologies for Knowledge Management: 
Challenges and Opportunities, ACL Meeting, Toulouse 
? Pardo, J.M., (2000) How will language and speech technology be used in the 
information world of 2010? Research challenges & Infrastructure needs for the next 
ten years. Report on the Roadmap Workshop, Katwijk aan Zee 
? Staab, St., (2001) Knowledge Portals, ACL Meeting, Toulouse 
? Stock, O. (2000) Processing Natural Language from 2000 to 2010, roadmap 
workshop, Katwijk 
? Velardi, P. and Missikoff, M. and Basili, R. (2001) Identification of relevant terms to 
support the construction of Domain Ontologies, ACL workshop Toulouse 
? Uszkoreit; H.  (2001) Crosslingual Language Technologies for Knowledge Creation 
and Knowledge Sharing, Toulouse 
? Zaenen, A. and Uszkoreit, H. (1997) Language Analysis and Understanding. In: 
Survey of the State of the Art in Human Language Technology, Cambridge University 
Press, Cambridge 
 
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 77?81,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Further Experiments with Shallow Hybrid MT Systems
Christian Federmann1, Andreas Eisele1, Hans Uszkoreit1,2,
Yu Chen1, Sabine Hunsicker1, Jia Xu1
1: Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz GmbH, Saarbru?cken, Germany
2: Universita?t des Saarlandes, Saarbru?cken, Germany
{cfedermann,eisele,uszkoreit,yuchen,sabine.hunsicker,jia.xu}@dfki.de
Abstract
We describe our hybrid machine trans-
lation system which has been developed
for and used in the WMT10 shared task.
We compute translations from a rule-
based MT system and combine the re-
sulting translation ?templates? with par-
tial phrases from a state-of-the-art phrase-
based, statistical MT engine. Phrase sub-
stitution is guided by several decision
factors, a continuation of previous work
within our group. For the shared task,
we have computed translations for six lan-
guage pairs including English, German,
French and Spanish. Our experiments
have shown that our shallow substitu-
tion approach can effectively improve the
translation result from the RBMT system;
however it has also become clear that a
deeper integration is needed to further im-
prove translation quality.
1 Introduction
In recent years the quality of machine translation
(MT) output has improved greatly, although each
paradigm suffers from its own particular kind of
errors: statistical machine translation (SMT) of-
ten shows poor syntax, while rule-based engines
(RBMT) experience a lack in vocabulary. Hybrid
systems try to avoid these typical errors by com-
bining techniques from both paradigms in a most
useful manner.
In this paper we present the improved version of
the hybrid system we developed last year?s shared
task (Federmann et al, 2009). We take the out-
put from an RBMT engine as basis for our hybrid
translations and substitute noun phrases by trans-
lations from an SMT engine. Even though a gen-
eral increase in quality could be observed, our sys-
tem introduced errors of its own during the substi-
tution process. In an internal error analysis, these
degradations were classified as follows:
- the translation by the SMT engine is incorrect
- the structure degrades through substitution
(because of e.g. capitalization errors, double
prepositions, etc.)
- the phrase substitution goes astray (caused by
alignment problems, etc.)
Errors of the first class cannot be corrected, as
we have no way of knowing when the translation
by the SMT engine is incorrect. The other two
classes could be eliminated, however, by introduc-
ing additional steps for pre- and post-processing
as well as improving the hybrid algorithm itself.
Our current error analysis based on the results of
this year?s shared task does not show these types
of errors anymore.
Additionally, we extended our coverage to also
include the language pairs English?French and
English?Spanish in both directions as well as
English?German, compared to last year?s initial
experiments for German?English only. We were
able to achieve an increase in translation quality
for this language set, which shows that the substi-
tution method works for different language config-
urations.
2 Architecture
Our hybrid translation system takes translation
output from a) the Lucy RBMT system (Alonso
and Thurmair, 2003) and b) a Moses-based SMT
system (Koehn et al, 2007). We then identify
noun phrases inside the rule-based translation and
compute the most likely correspondences in the
statistical translation output. For these, we apply a
factored substitution method that decides whether
the original RBMT phrase should be kept or rather
be replaced by the Moses phrase. As this shallow
substitution process may introduce problems at
77
phrase boundaries, we afterwards perform several
post-processing steps to cleanup and finalize the
hybrid translation result. A schematic overview
of our hybrid system and its main components is
given in figure 1.
Figure 1: Schematic overview of the hybrid MT
system architecture.
2.1 Input to the Hybrid System
Lucy RBMT System We obtain the translation
as well as linguistic structures from the RBMT
system. An internal evaluation has shown that
these structures are usually of a high quality which
supports our initial decision to consider the RBMT
output as an appropriate ?template? for our hybrid
translation approach. The Lucy translation output
can include additional markup that allows to iden-
tify unknown words or other, local phenomena.
The Lucy system is a transfer-based MT system
that performs translation in three phases, namely
analysis, transfer, and generation. Intermediate
tree structures for each of the translation phases
can be extracted from the Lucy system to guide
the hybrid system. Sadly, only the 1-best path
through these three phases is given, so no alterna-
tive translation possibilities can be extracted from
the given data; a fact that clearly limits the poten-
tial for more deeply integrated hybrid translation
approaches. Nevertheless, the availability of the
1-best trees already allows to improve the transla-
tion quality of the RBMT system as we will show
in this paper.
Moses SMT System We used a state-of-the-art
Moses SMT system to create statistical phrase-
based translations of our input text. Moses has
been modified so that it returns the translation re-
sults together with the bidirectional word align-
ments between the source texts and the transla-
tions. Again, we make use of markup which helps
to identify unknown words as these will later guide
the factored substitution method. Both of the
translation models and the language models within
our SMT systems were only trained with lower-
cased and tokenized Europarl training data. The
system used sets of feature weights determined us-
ing data sets also from Europarl (test2008). In
addition, we used LDC gigaword corpus to train
large scale n-gram language models to be used in
our hybrid system. We tokenized the source texts
using the standard tokenizers available from the
shared task website. The SMT translations are re-
cased before being fed into the hybrid system to-
gether with the word alignment information.The
hybrid system can easily be adapted to support
other statistical translation engines. If the align-
ment information is not available, a suitable align-
ment tool would be necessary to compute it as the
alignment is a key requirement for the hybrid sys-
tem.
2.2 Aligning RBMT and SMT Output
We compute alignment in several components of
the hybrid system, namely:
source-text-to-tree: we first find an alignment
between the source text and the correspond-
ing analysis tree(s). As Lucy tends to sub-
divide large sentences into several smaller
units, it sometimes becomes necessary to
align more than one tree structure to a given
source sentence.
analysis-transfer-generation: for each of the
analysis trees, we re-construct the path from
its tree nodes, via the transfer tree, and their
corresponding generation tree nodes.
tree-to-target-text: similarly to the first align-
ment process, we find a mapping between
generation tree nodes and the actual transla-
tion output of the RBMT system.
source-text-to-tokenized: as the Lucy RBMT
system works on non-tokenized input text
and our Moses system takes tokenized input,
78
we need to align the source text to its tok-
enized form.
Given the aforementioned alignments, we can then
correlate phrases from the rule-based translation
with their counterparts from the statistical trans-
lation, both on source or target side. As our
hybrid approach relies on the identification of
such phrase pairs, the computation of the different
alignments is critical to obtain good combination
performance.
Please note that all these tree-based alignments
can be computed with a very high accuracy. How-
ever, due to the nature of statistical word align-
ment, the same does not hold for the alignment
obtained from the Moses system. If the alignment
process has produced erroneous phrase tables, it is
very likely that Lucy phrases and their ?aligned?
SMT matches simply will not fit. Or put the other
way round: the better the underlying SMT word
alignment, the greater the potential of the hybrid
substitution approach.
2.3 Factored Substitution
Given the results of the alignment process, we can
then identify ?interesting? phrases for substitution.
Following our experimental setup from last year?s
shared task, we again decided to focus on noun
phrases as these seem to be best-suited for in-place
swapping of phrases. Our initial assumption is that
SMT phrases are better on a lexical level, hence
we aim to replace Lucy?s noun phrases by their
Moses counterparts.
Still, we want to perform the substitution in a
controlled manner in order to avoid problems or
non-matching insertions. For this, we have (man-
ually) derived a set of factors that are checked for
each of the phrase pairs that are processed. The
factors are described briefly below:
identical? simply checks whether two candidate
phrases are identical.
too complex? a Lucy phrase is ?too complex?
to substitute if it contains more than 2
embedded noun phrases.
many-to-one? this factor checks if a Lucy phrase
containing more than one word is mapped to
a Moses phrase with only one token.
contains pronoun? checks if the Lucy phrase
contains a pronoun.
contains verb? checks if the Lucy phrase con-
tains a verb.
unknown? checks whether one of the phrases is
marked as ?unknown?.
length mismatch computes the number of words
for both phrases and checks if the absolute
difference is too large.
language model computes language model
scores for both phrases and checks which is
more likely according to the LM.
All of these factors have been designed and ad-
justed during an internal development phase using
data from previous shared tasks.
2.4 Post-processing Steps
After the hybrid translation has been computed,
we perform several post-processing steps to clean
up and finalize the result:
cleanup first, we perform basic cleanup opera-
tions such as whitespace normalization, cap-
italizing the first word in each sentence, etc.
multi-words then, we take care of proper han-
dling of multi-word expressions. Using the
tree structures from the RBMT system we
eliminate superfluous whitespace and join
multi-words, even if they were separated in
the SMT phrase.
prepositions finally, we give prepositions a spe-
cial treatment. Experience from last year?s
shared task had shown that things like double
prepositions contributed to a large extent to
the amount of avoidable errors. We tried to
circumvent this class of error by identifying
the correct prepositions; erroneous preposi-
tions are removed.
3 Hybrid Translation Analysis
We evaluated the intermediate outputs using
BLEU (Papineni et al, 2001) against human refer-
ences as in table 3. The BLEU score is calculated
in lower case after the text tokenization. The trans-
lation systems compared are Moses, Lucy, Google
and our hybrid system with different configura-
tions:
Hybrid: we use the language model with case
information and substitute some NPs in Lucy
outputs by Moses outputs.
Hybrid LLM: same as Hybrid but we use a
larger language model.
79
Table 1: Intermediate results of BLEU[%] scores for WMT10 shared task.
System de?en en?de fr?en en?fr es?en en?es
Moses 18.32 12.66 22.26 20.06 24.28 24.72
Lucy 16.85 12.38 18.49 17.61 21.09 20.85
Google 25.64 18.51 28.53 28.70 32.77 32.20
Hybrid 17.29 13.05 18.92 19.58 22.53 23.55
Hybrid LLM 17.37 13.73 18.93 19.76 22.61 23.66
Hybrid SG 17.43 14.40 19.67 20.55 24.37 24.99
Hybrid NCLM 17.38 14.42 19.56 20.55 24.41 24.92
Hybrid SG: same as Hybrid but the NP substitu-
tions are based on Google output instead of
Moses translations.
Hybrid NCLM: same as Hybrid but we use the
language model without case information.
We participated in the translation evaluation in
six language pairs: German to English (de?en),
English to German (en?de), French to English
(fr?en), English to French (en?fr), Spanish to
English (es?en) and English to Spanish (en?es).
As shown in table 3, the Moses translation sys-
tem achieves better results overall than the Lucy
system does. Google?s system outperforms other
systems in all language pairs. The hybrid transla-
tion as described in section 2 improves the Lucy
translation quality with a BLEU score up to 2.7%
absolutely.
As we apply a larger language model or a lan-
guage model without case information, the trans-
lation performance can be improved further. One
major problem in the hybrid translation is that the
Moses outputs are still not good enough to replace
the Lucy outputs, therefore we experimented on
a hybrid translation of Google and Lucy systems
and substitute some unrelaible NP translations by
the Google?s translations. The results in the line
of ?Hybrid SG? shows that the hybrid translation
quality can be enhanced if the translation system
where we select substitutions is better.
4 Internal Evaluation of Results
In the analysis of the remaining issues, the fol-
lowing main sources of problems can be distin-
guished:
- Lucy?s output contains structural errors that
cannot be fixed by the chosen approach.
- Lucy results contain errors that could have
been corrected by alternative expressions
from SMT, but the constraints in our system
were too restrictive to let that happen.
- The SMT engine we use generates subopti-
mal results that find their way into the hybrid
result.
- SMT results that are good are incorporated
into the hybrid results in a wrong way.
We have inspected a part of the results and classi-
fied the problems according to these criteria. As
this work is still ongoing, it is too early to report
numerical results for the relative frequencies of the
different causes of the error. However, we can
already see that three of these four cases appear
frequently enough to justify further attention. We
observed several cases in which the parser in the
Lucy system was confused by unknown expres-
sions and delivered results that could have been
significantly improved by a more robust parsing
approach. We also encountered several cases in
which an expression from SMT was used although
the original Lucy output would have been better.
Also we still observe problems finding to correct
correspondences between Lucy output and SMT
output, which leads to situations where material is
inserted in the wrong place, which can lead to the
loss of content words in the output.
5 Conclusion and Outlook
In our contribution to the shared task we have ap-
plied the hybrid architecture from (Federmann et
al., 2009) to six language pairs. We have identi-
fied and fixed many of the problems we had ob-
served last year, and we think that, in addition to
the increased coverage in laguage pairs, the overall
quality has been significantly increased.
However, in the last section we characterized
three main sources of problems that will require
further attention. We will address these problems
in the near future in the following way:
80
1. We will investigate in more detail the align-
ment issue that leads to occasional loss of
content words, and we expect that a careful
inspection and correction of the code will in
all likelihood give us a good remedy.
2. The problem of picking expressions from the
SMT output that appear more probable to the
language model although they are inferior to
the original expression from the RBMT sys-
tem is more difficult to fix. We will try to find
better thresholds and biases that can at least
reduce the number of cases in which this type
of degradation happen.
3. Finally, we will also address the robustness
issue that leads to suboptimal structures from
the RBMT engine caused by parsing failures.
Our close collaboration with Lucy enables us to
address these issues in a very effective way via the
inspection and classification of intermediate struc-
tures and, if these structures indicate parsing prob-
lems, the generation of variants of the input sen-
tence that facilitate correct parsing.
Acknowledgments
This work was supported by the EuroMatrixPlus
project (IST-231720) which is funded by the Eu-
ropean Commission under the Seventh Framework
Programme. The authors want to thank Michael
Jellinghaus and Bastian Simon for help with the
inspection of intermediate results and classifica-
tion of errors.
References
Juan A. Alonso and Gregor Thurmair. 2003. The Com-
prendium Translator system. In Proc. of the Ninth
MT Summit.
Christian Federmann, Silke Theison, Andreas Eisele,
Hans Uszkoreit, Yu Chen, Michael Jellinghaus, and
Sabine Hunsicker. 2009. Translation combination
using factored word substitution. In Proceedings of
the Fourth Workshop on Statistical Machine Transla
tion, pages 70?74, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of ACL Demo and Poster Sessions, pages 177?
180, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. IBM Research Report
RC22176(W0109-022), IBM.
81
