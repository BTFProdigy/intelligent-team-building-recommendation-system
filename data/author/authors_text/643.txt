Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 745?754,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Predicting Success in Machine Translation
Alexandra Birch Miles Osborne Philipp Koehn
a.c.birch-mayne@sms.ed.ac.uk miles@inf.ed.ac.uk pkoehn@inf.ed.ac.uk
School of Informatics
University of Edinburgh
10 Crichton Street
Edinburgh, EH8 9AB, UK
Abstract
The performance of machine translation sys-
tems varies greatly depending on the source
and target languages involved. Determining
the contribution of different characteristics of
language pairs on system performance is key
to knowing what aspects of machine transla-
tion to improve and which are irrelevant. This
paper investigates the effect of different ex-
planatory variables on the performance of a
phrase-based system for 110 European lan-
guage pairs. We show that three factors are
strong predictors of performance in isolation:
the amount of reordering, the morphological
complexity of the target language and the his-
torical relatedness of the two languages. To-
gether, these factors contribute 75% to the
variability of the performance of the system.
1 Introduction
Statistical machine translation (SMT) has improved
over the last decade of intensive research, but for
some language pairs, translation quality is still low.
Certain systematic differences between languages
can be used to predict this. Many researchers have
speculated on the reasons whymachine translation is
hard. However, there has never been, to our knowl-
edge, an analysis of what the actual contribution of
different aspects of language pairs is to translation
performance. This understanding of where the diffi-
culties lie will allow researchers to know where to
most gainfully direct their efforts to improving the
current models of machine translation.
Many of the challenges of SMT were first out-
lined by Brown et al (1993). The original IBM
Models were broken down into separate translation
and distortion models, recognizing the importance
of word order differences in modeling translation.
Brown et al also highlighted the importance of mod-
eling morphology, both for reducing sparse counts
and improving parameter estimation and for the cor-
rect production of translated forms.We see these two
factors, reordering and morphology, as fundamental
to the quality of machine translation output, and we
would like to quantify their impact on system per-
formance.
It is not sufficient, however, to analyze the mor-
phological complexity of the source and target lan-
guages. It is also very important to know how sim-
ilar the morphology is between the two languages,
as two languages which are morphologically com-
plex in very similar ways, could be relatively easy
to translate. Therefore, we also include a measure of
the family relatedness of languages in our analysis.
The impact of these factors on translation is mea-
sured by using linear regression models. We perform
the analysis with data from 110 different language
pairs drawn from the Europarl project (Koehn,
2005). This contains parallel data for the 11 official
language pairs of the European Union, providing a
rich variety of different language characteristics for
our experiments. Many research papers report re-
sults on only one or two languages pairs. By analyz-
ing so many language pairs, we are able to provide
a much wider perspective on the challenges facing
machine translation. This analysis is important as it
provides very strong motivation for further research.
The findings of this paper are as follows: (1) each
of the main effects, reordering, target language com-
plexity and language relatedness, is a highly signif-
icant predictor of translation performance, (2) indi-
vidually these effects account for just over a third of
745
the variation of the BLEU score, (3) taken together,
they account for 75% of the variation of the BLEU
score, (4) when removing Finnish results as out-
liers, reordering explains the most variation, and fi-
nally (4) the morphological complexity of the source
language is uncorrelated with performance, which
suggests that any difficulties that arise with sparse
counts are insignificant under the experimental con-
ditions outlined in this paper.
2 Europarl
In order to analyze the influence of different lan-
guage pair characteristics on translation perfor-
mance, we need access to a large variety of compa-
rable parallel corpora. A good data source for this is
the Europarl Corpus (Koehn, 2005). It is a collection
of the proceedings of the European Parliament, dat-
ing back to 1996. Version 3 of the corpus consists of
up to 44 million words for each of the 11 official lan-
guages of the European Union: Danish (da), German
(de), Greek (el), English (en), Spanish (es), Finnish
(fi), French (fr), Italian (it), Dutch (nl), Portuguese
(pt), and Swedish (sv).
In trying to determine the effect of properties of
the languages involved in translation performance,
it is very important that other variables be kept con-
stant. Using Europarl, the size of the training data
for the different language pairs is very similar, and
there are no domain differences as all sentences are
roughly trained on translations of the same data.
3 Morphological Complexity
The morphological complexity of the language pairs
involved in translation is widely recognized as one
of the factors influencing translation performance.
However, most statistical translation systems treat
different inflected forms of the same lemma as com-
pletely independent of one another. This can result in
sparse statistics and poorly estimated models. Fur-
thermore, different variations of the lemma may re-
sult in crucial differences in meaning that affect the
quality of the translation.
Work on improving MT systems? treatment of
morphology has focussed on either reducing word
forms to lemmas to reduce sparsity (Goldwater
and McClosky, 2005; Talbot and Osborne, 2006)
or including morphological information in decod-
Language
Av. 
Voc
abu
lary
 Siz
e
en fr it es pt el nl sv da de fi
?
?
?
?
?
100k
200k
300k
400k
500k
Figure 1. Average vocabulary size for each language.
ing (Dyer, 2007).
Although there is a significant amount of research
into improving the treatment of morphology, in this
paper we aim to discover the effect that different lev-
els of morphology have on translation. We measure
the amount of morphological complexity that exists
in both languages and then relate this to translation
performance.
Some languages seem to be intuitively more com-
plex than others, for instance Finnish appears more
complex than English. There is, however, no obvi-
ous way of measuring this complexity. One method
of measuring complexity is by choosing a number
of hand-picked, intuitive properties called complex-
ity indicators (Bickel and Nichols, 2005) and then
to count their occurrences. Examples of morpholog-
ical complexity indicators could be the number of in-
flectional categories or morpheme types in a typical
sentence. This method suffers from the major draw-
back of finding a principled way of choosing which
of the many possible linguistic properties should be
included in the list of indicators.
A simple alternative employed by Koehn (2005)
is to use vocabulary size as a measure of morpho-
logical complexity. Vocabulary size is strongly in-
fluenced by the number of word forms affected by
number, case, tense etc. and it is also affected by the
number of agglutinations in the language. The com-
plexity of the morphology of languages can there-
fore be approached by looking at vocabulary size.
746
Figure 1 shows the vocabulary size for all rele-
vant languages. Each language pair has a slightly
different parallel corpus, and so the size of the vo-
cabularies for each language needs to be averaged.
You can see that the size of the Finnish vocabulary is
about six times larger (510,632 words) than the En-
glish vocabulary size (88,880 words). The reason for
the large vocabulary size is that Finnish is character-
ized by a rich inflectional morphology, and it is typo-
logically classified as an agglutinative-fusional lan-
guage. As a result, words are often polymorphemic,
and become remarkably long.
4 Language Relatedness
The morphological complexity of each language in
isolation could be misleading. Large differences in
morphology between two languages could be more
relevant to translation performance than a complex
morphology that is very similar in both languages.
Languages which are closely related could share
morphological forms which might be captured rea-
sonably well in translation models. We include a
measure of language relatedness in our analyses to
take this into account.
Comparative linguistics is a field of linguistics
which aims to determine the historical relatedness
of languages. Lexicostatistics, developed by Morris
Swadesh in the 1950s (Swadesh, 1955), is an ap-
proach to comparative linguistics that is appropriate
for our purposes because it results in a quantitative
measure of relatedness by comparing lists of lexical
cognates.
The lexicostatistic percentages are extracted as
follows. First, a list of universal culture-free mean-
ings are generated. Words are then collected for
these meanings for each language under consider-
ation. Lists for particular purposes have been gen-
erated. For example, we use the data from Dyen et
al. (1992) who developed a list of 200 meanings for
84 Indo-European languages. Cognacy decisions are
then made by a trained linguist. For each pair of lists
the cognacy of a form can be positive, negative or in-
determinate. Finally, the lexicostatistic percentage is
calculated. This percentage is related to the propor-
tion of meanings for a particular language pair that
are cognates, i.e. relative to the total without inde-
terminacy. Factors such as borrowing, tradition and
Language ?animal? ?black?
French animal noir
Italian animale nero
Spanish animal negro
English animal black
German tier schwarz
Swedish djur svart
Danish dyr sort
Dutch dier zwart
Table 1. An example from the (Dyen et al, 1992) cognate
list.
taboo can skew the results.
A portion of the Dyen et al (1992) data set is
shown in Table 1 as an example. From this data a
trained linguist would calculate the relatedness of
French, Italian and Spanish as 100% because their
words for ?animal? and ?black? are cognates. The
Romance languages share one cognate with English,
?animal? but not ?black?, which means that the lex-
icostatistic percentage here would be 50%, and no
cognates with the rest of the languages, 0%.
We use the Dyen lexicostatistic percentages as our
measure of language relatedness or similarity for all
bidirectional language pairs except for Finnish, for
which there is not data. Finnish is a Finno-Ugric
language and is not part of the Indo-European lan-
guage family and is therefore not included in the
Dyen results. We were not able to recreate the con-
ditions of this study to generate the data for Finnish
- expert linguists with knowledge of all the lan-
guages would be required. Excluding Finnish would
have been a shame as it is an interesting language
to look at, however we took care to confirm which
effects found in this paper still held when exclud-
ing Finnish. Not being part of the Indo-European
languages means that its historical similarity with
our other languages is very low. For example, En-
glish would be more closely related to Hindu than to
Finnish. We therefore assume that Finnish has zero
similarity with the other languages in the set.
Figure 2 shows the symmetric matrix of language
relatedness, where the width of the square is pro-
portional to the value of relatedness. Finnish is the
language which is least related to the other lan-
guages and has a relatedness score of 0%. Spanish-
Portuguese is the most related language pair with a
747
it sv en el pt da es fr nl fi de
it
sv
en
el
pt
da
es
fr
nl
fi
de
= 0.17 = 0.35 = 0.52 = 0.7 = 0.87
Figure 2. Language relatedness - the width of the squares
indicates the lexicostatical relatedness.
score of 0.87%.
A measure of family relatedness should improve
our understanding of the relationship between mor-
phological complexity and translation performance.
5 Reordering
Reordering refers to differences in word order that
occur in a parallel corpus and the amount of reorder-
ing affects the performance of a machine translation
system. In order to determine how much it affects
performance, we first need to measure it.
5.1 Extracting Reorderings
Reordering is largely driven by syntactic differences
between languages and can involve complex rear-
rangements between nodes in synchronous trees.
Modeling reordering exactly would require a syn-
chronous tree-substitution grammar. This represen-
tation would be sparse and heterogeneous, limiting
its usefulness as a basis for analysis. We make an
important simplifying assumption in order for the
detection and extraction of reordering data to be
tractable and useful. We assume that reordering is
a binary process occurring between two blocks that
are adjacent in the source. This is similar to the
ITG constraint (Wu, 1997), however our reorder-
ings are not dependent on a synchronous grammar
or a derivation which covers the sentences. There are
also similarities with the Human-Targeted Transla-
tion Edit Rate metric (HTER) (Snover et al, 2006)
which attempts to find the minimum number of hu-
man edits to correct a hypothesis, and admits mov-
ing blocks of words, however our algorithm is auto-
matic and does not consider inserts or deletes.
Before describing the extraction of reorderings we
need to define some concepts. We define a block A
as consisting of a source span, As, which contains
the positions from Asmin to Asmax and is aligned to
a set of target words. The minimum and maximum
positions (Atmin and Atmax) of the aligned target
words mark the block?s target span, At.
A reordering r consists of the two blocks rA and
rB , which are adjacent in the source and where the
relative order of the blocks in the source is reversed
in the target. More formally:
rAs < rBs , rAt > rBt , rAsmax = rBsmin ? 1
A consistent block means that betweenAtmin and
Atmax there are no target word positions aligned
to source words outside of the block?s source span
As. A reordering is consistent if the block projected
from rAsmin to rBsmax is consistent.
The following algorithm detects reorderings and
determines the dimensions of the blocks involved.
We step through all the source words, and if a word
is reordered in the target with respect to the previ-
ous source word, then a reordering is said to have
occurred. These two words are initially defined as
the blocks A and B. Then the algorithm attempts
to grow block A from this point towards the source
starting position, while the target span ofA is greater
than that of block B, and the new block A is consis-
tent. Finally it attempts to grow block B towards the
source end position, while the target span of B is
less than that of A and the new reordering is incon-
sistent.
See Figure 3 for an example of a sentence pair
with two reorderings. Initially a reordering is de-
tected between the Chinese words aligned to ?from?
and ?late?. The block A is grown from ?late? to in-
clude the whole phrase pair ?late last night?. Then
the block B is grown from ?from? to include ?Bei-
jing? and stops because the reordering is then con-
sistent. The next reordering is detected between ?ar-
rived in? and ?Beijing?. We can see that block A at-
tempts to grow as large a block as possible and block
748
Figure 3. A sentence pair from the test corpus, with its
alignment. Two reorderings are shown with two different
dash styles.
B attempts to grow the smallest block possible. The
reorderings thus extracted would be comparable to
those of a right-branching ITG with inversions. This
allows for syntactically plausible embedded reorder-
ings. This algorithm has the worst case complexity
of O(n
2
2 ) when the words in the target occur in the
reverse order to the words in the source.
5.2 Measuring Reordering
Our reordering extraction technique allows us to an-
alyze reorderings in corpora according to the dis-
tribution of reordering widths. In order to facilitate
the comparison of different corpora, we combine
statistics about individual reorderings into a sen-
tence level metric which is then averaged over a cor-
pus.
RQuantity =
?
r?R |rAs | + |rBs |
I
where R is the set of reorderings for a sentence, I
is the source sentence length, A and B are the two
blocks involved in the reordering, and |rAs | is the
size or span of block A on the source side. RQuan-
tity is thus the sum of the spans of all the reordering
blocks on the source side, normalized by the length
of the source sentence.
RQuantity
Europarl, auto align 0.620
WMT06 test, auto align 0.647
WMT06 test, manual align 0.668
Table 2. The reordering quantity for the different reorder-
ing corpora for DE-EN.
5.3 Automatic Alignments
Reorderings extracted from manually aligned data
can be reliably assumed to be correct. The only
exception to this is that embedded reorderings are
always right branching and these might contradict
syntactic structure. In this paper, however, we use
alignments that are automatically extracted from the
training corpus using GIZA++. Automatic align-
ments could give very different reordering results.
In order to justify using reordering data extracted
from automatic alignments, we must show that they
are similar enough to gold standard alignments to be
useful as a measure of reordering.
5.3.1 Experimental Design
We select the German-English language pair be-
cause it has a reasonably high level of reordering. A
manually aligned German-English corpus was pro-
vided by Chris Callison-Burch and consists of the
first 220 sentences of test data from the 2006 ACL
Workshop on Machine Translation (WMT06) test
set. This test set is from a held out portion of the
Europarl corpus.
The automatic alignments were extracted by ap-
pending the manually aligned sentences on to the
respective Europarl v3 corpora and aligning them
using GIZA++ (Och and Ney, 2003) and the grow-
final-diag algorithm (Koehn et al, 2003).
5.3.2 Results
In order to use automatic alignments to extract re-
ordering statistics, we need to show that reorderings
from automatic alignments are comparable to those
from manual alignments.
We first look at global reordering statistics and
then we look in more detail at the reordering dis-
tribution of the corpora. Table 2 shows the amount
of reordering in the WMT06 test corpora, with both
manual and automatic alignments, and in the auto-
matically aligned Europarl DE-EN parallel corpus.
749
5 10 15 20
0.0
0.2
0.4
0.6
0.8
1.0
Reordering Width
Av. 
Reo
rder
ings
 per 
Sen
tenc
e
ACL Test ManualACL Test AutomaticEuromatrix
Figure 4. Average number of reorderings per sentence
mapped against the total width of the reorderings for DE-
EN.
We can see that all three corpora show a similar
amount of reordering.
Figure 4 shows that the distribution of reorder-
ings between the three corpora is also very similar.
These results provide evidence to support our use of
automatic reorderings in lieu of manually annotated
alignments. Firstly, they show that our WMT06 test
corpus is very similar to the Europarl data, which
means that any conclusions that we reach using the
WMT06 test corpus will be valid for the Europarl
data. Secondly, they show that the reordering behav-
ior of this corpus is very similar when looking at
automatic vs. manual alignments.
Although differences between the reorderings de-
tected in the manually and automatically aligned
German-English corpora are minor, there we accept
that there could be a language pair whose real re-
ordering amount is very different to the expected
amount given by the automatic alignments. A par-
ticular language pair could have alignments that are
very unsuited to the stochastic assumptions of the
IBM or HMM alignment models. However, manu-
ally aligning 110 language pairs is impractical.
5.4 Amount of reordering for the matrix
Extracting the amount of reordering for each of the
110 language pairs in the matrix required a sam-
pling approach. We randomly extracted a subset of
2000 sentences from each of the parallel training
corpora. From this subset we then extracted the av-
Sou
rce 
Lan
gua
ges
it sv en el pt da es fr nl fi de
it
sv
en
el
pt
da
es
fr
nl
fi
de
= 0.13 = 0.25 = 0.38 = 0.51 = 0.64
Target Languages
Figure 5. Reordering amount - the width of the squares
indicates the amount of reordering or RQuantity.
erage RQuantity.
In Figure 5 the amount of reordering for each
of the language pairs is proportional to the width
of the relevant square. Note that the matrix is not
quite symmetrical - reordering results differ de-
pending on which language is chosen to measure
the reordering span. The lowest reordering scores
are generally for languages in the same language
group (like Portuguese-Spanish, 0.20, and Danish-
Swedish, 0.24) and the highest for languages from
different groups (like German-French, 0.64, and
Finnish-Spanish, 0.61).
5.5 Language similarity and reordering
In this paper we use linear regression models to de-
termine the correlation and significance of various
explanatory variables with the dependent variable,
the BLEU score. Ideally the explanatory variables
involved should be independent of each other, how-
ever the amount of reordering in a parallel corpus
could easily be influenced by family relatedness. We
investigate the correlation between these variables.
Figure 6 shows the plot of the reordering amount
against language similarity. The regression is highly
significant and has an R2 of 0.2347. This means that
reordering is correlated with language similarity and
that 23% of reordering can be explained by language
similarity.
750
ll
l
ll l lll l
l
l
ll
l l
l
l
l
ll
l ll
l
l
l
l
l
l
l
l l
l
ll
l
l
l
l
l
l
l
ll
l
l
l
lll
l
l
ll l
l
l
l l
ll
l ll lll ll lll l
ll l ll ll l
0.2 0.3 0.4 0.5 0.60
.0
0.2
0.4
0.6
0.8
Reordering Amount
Lang
uage
 Sim
ilarit
y
Figure 6. Reordering compared to language similarity
with regression.
6 Experimental Design
We used the phrase-based model Moses (Koehn et
al., 2007) for the experiments with all the standard
settings, including a lexicalized reordering model,
and a 5-gram language model. Tests were run on
the ACL WSMT 2008 test set (Callison-Burch et al,
2008).
6.1 Evaluation of Translation Performance
We use the BLEU score (Papineni et al, 2002) to
evaluate our systems. While the role of BLEU in
machine translation evaluation is a much discussed
topic, it is generally assumed to be a adequate metric
for comparing systems of the same type.
Figure 7 shows the BLEU score results for the ma-
trix. Comparing this figure to Figure 5 there seems
to be a clear negative correlation between reordering
amount and translation performance.
6.2 Regression Analysis
We perform multiple linear regression analyses us-
ing measures of morphological complexity, lan-
guage relatedness and reordering amount as our in-
dependent variables. The dependent variable is the
translation performance metric, the BLEU score.
We then use a t-test to determine whether the co-
efficients for the independent variables are reliably
different from zero. We also test how well the model
explains the data using an R2 test. The two-tailed
significance levels of coefficients and R2 are also
Sou
rce 
Lan
gua
ges
it sv en el pt da es fr nl fi de
it
sv
en
el
pt
da
es
fr
nl
fi
de
= 0.08 = 0.16 = 0.24 = 0.32 = 0.4
Target Languages
Figure 7. System performance - the width of the squares
indicates the system performance in terms of the BLEU
score.
Explanatory Variable Coefficient
Target Vocab. Size -3.885 ***
Language Similarity 3.274 ***
Reordering Amount -1.883 ***
Target Vocab. Size2 1.017 ***
Language Similarity2 -1.858 **
Interaction: Reord/Sim -1.4536 ***
Table 3. The impact of the various explanatory features
on the BLEU score via their coefficients in the minimal ad-
equate model.
given where * means p < 0.05, ** means p < 0.01,
and *** means p < 0.001.
7 Results
7.1 Combined Model
The first question we are interested in answering is
which factors contribute most and how they interact.
We fit a multiple regression model to the data. The
source vocabulary size has no significant effect on
the outcome. All explanatory variable vectors were
normalized to be more comparable.
In Table 3 we can see the relative contribution of
the different features to the model. Source vocabu-
lary size did not contribute significantly to the ex-
planatory power of this multiple regression model
and was therefore not included. The fraction of the
variance explained by the model, or its goodness of
fit, the R2, is 0.750 which means that 75% of the
751
variation in BLEU can be explained by these three
factors. The interaction of reordering amount and
language relatedness is the product of the values of
these two features, and in itself it is an important ex-
planatory feature.
To make sure that our regression is valid, we need
to consider the special case of Finnish. Data points
where Finnish is the target language are outliers.
Finnish has the lowest language similarity with all
other languages, and the largest vocabulary size. It
also has very high amounts of reordering, and the
lowest BLEU scores when it is the target language.
The multiple regression of Table 3 where Finnish as
the source and target language is excluded, shows
that all the effects are still very significant, with the
model?s R2 dropping only slightly to 0.68.
The coefficients of the variables in the multiple
regression model have only limited usefulness as a
measure of the impact of the explanatory variables
in the model. One important factor to consider is that
if the explanatory variables are highly correlated,
then the values of the coefficients are unstable. The
model could attribute more importance to one or the
other variable without changing the overall fit of the
model. This is the problem of multicollinearity. Our
explanatory variables are all correlated, but a large
amount of this correlation can be explained by look-
ing at language pairs with Finnish as the target lan-
guage. Excluding these data points, only language
relatedness and reordering amount are still corre-
lated, see Section 5.5 for more details.
7.2 Contribution in isolation
In order to establish the relative contribution of vari-
ables, we isolate their impact on the BLEU score by
modeling them in separate linear regression models.
Figure 8 shows a simple regression model over
the plot of BLEU scores against target vocabulary
size. This figure shows groups of data points with the
same target language in almost vertical lines. Each
language pair has a separate parallel training corpus,
but the target vocabulary size for one language will
be very similar in all of them. The variance in BLEU
amongst the group with the same target language is
then largely explained by the other factors, similarity
and reordering.
Figure 9 shows a simple regression model over the
plot of BLEU scores against source vocabulary size.
l
l
l
l
ll
l
ll
ll
l
l
l
l
l
ll
l
l
l
l
l
l
l
lll
l
ll
l
l
l
l
ll
l
lll
l
l
l
l
l
l
ll
l l
l
l
ll
l
l
l
l
l
l
ll
l
l
l
lll l
l
l
l
l lll
llllll
l
l
l
l
l
l
l
l
l
lll
0.15
0.20
0.25
0.30
0.35
0.40
Target Vocabulary Size
BLE
U
| | | | |100k 200k 300k 400k 500k
Figure 8. BLEU score of experiments compared to target
vocabulary size showing regression
This regression model shows that in isolation source
vocabulary size is significant (p< 0.05), but that this
is due to the distorting effect of Finnish. Excluding
results that include Finnish, there is no longer any
significant correlation with BLEU. The source mor-
phology might be significant for models trained on
smaller data sets, where model parameters are more
sensitive to sparse counts.
Figure 10 shows the simple regression model over
the plot of BLEU scores against the amount of re-
ordering. This graph shows that with more reorder-
ing, the performance of the translation model re-
duces. Data points with low levels of reordering and
high BLEU scores tend to be language pairs where
both languages are Romance languages. High BLEU
scores with high levels of reordering tend to have
German as the source language and a Romance lan-
guage as the target.
Figure 11 shows the simple regression model over
the plot of BLEU scores against the amount of lan-
guage relatedness. The left hand line of points are
the results involving Finnish. The vertical group of
points just to the right, are results where Greek
is involved. The next set of points are the results
where the translation is between Germanic and Ro-
mance languages. The final cloud to the right are re-
sults where languages are in the same family, either
within the Romance or the Germanic languages.
Table 4 shows the amount of the variance of
BLEU explained by the different models. As these
752
l
l
l
l
ll
l
ll
ll
l
l
l
l
l
ll
l
l
l
l
l
l
l
lll
l
l
l
l
l
l
l
l
l ll
l
l
l
l
l
l
ll
l
l
ll
lll
l
lll
lll
l ll
l
l
l
ll ll
l
l
l
l
l
ll
l ll ll
ll
l
l
l
l
l
l
llll
0.15
0.20
0.25
0.30
0.35
0.40
Source Vocabulary Size
BLE
U
| | | | |100k 200k 300k 400k 500k
l
l
l
FinnishOther
Figure 9. BLEU score of experiments compared to source
vocabulary size highlighting the Finnish source vocabu-
lary data points. The regression includes Finnish in the
model.
Explanatory Variable R2
Target Vocab. Size 0.388 ***
Reordering Amount 0.384 ***
Language Similarity 0.366 ***
Source Vocab. Size 0.045 *
Excluding Finnish
Target Vocab. Size 0.219 ***
Reordering Amount 0.332 ***
Language Similarity 0.188 ***
Source Vocab. Size 0.007
Table 4. Goodness of fit of different simple linear regres-
sion models which use just one explanatory variable. The
significance level represents the level of probability that
the regression is appropriate. The second set of results
excludes Finnish in the source and target language.
are simple regression models, with just one explana-
tory variable, multicolinearity is avoided. This table
shows that each of the main effects explains about a
third of the variance of BLEU, which means that they
can be considered to be of equal importance. When
Finnish examples are removed, only reordering re-
tains its power, and target vocabulary and language
similarity reduce in importance and source vocabu-
lary size no longer correlates with performance.
8 Conclusion
We have broken down the relative impact of the
characteristics of different language pairs on trans-
l
l
l
l
l l
l
ll
ll
l
l
l
l
l
l l
l
l
l
l
l
l
l
l l l
l
l l
l
l
l
l
ll
l
l l l
l
l
l
l
l
l
l
ll
ll
l
ll
ll
l
l ll
ll l
ll
l
l ll
l
l
l
ll l l
l
l
l
ll
l
l l
l lll ll
ll
l
l
l
l
l
l
l
ll l
0.2 0.3 0.4 0.5 0.6
0.15
0.20
0.25
0.30
0.35
0.40
Reordering Amount
BLE
U
Figure 10. BLEU score of experiments compared to
amount of reordering.
l
l
l
l
ll
l
ll
ll
l
l
l
l
l
ll
l
l
l
l
l
l
l
lll
l
ll
l
l
l
l
ll
ll
l
l
l
l
l
l
ll
l
l
l
l
lll
ll
l l
l
ll ll
l
lll
l
l
l
l
l
ll
lll
l
l
l
l
l
l
llll
0.0 0.2 0.4 0.6 0.8
0.15
0.20
0.25
0.30
0.35
0.40
Language Similarity
BLE
U
Figure 11. BLEU score of experiments compared to lan-
guage relatedness.
lation performance. The analysis done is able to ac-
count for a large percentage (75%) of the variabil-
ity of the performance of the system, which shows
that we have captured the core challenges for the
phrase-based model. We have shown that their im-
pact is about the same, with reordering and target
vocabulary size each contributing about 0.38%.
These conclusions are only strictly relevant to the
model for which this analysis has been performed,
the phrase-based model. However, we suspect that
the conclusions would be similar for most statisti-
cal machine translation models because of their de-
pendence on automatic alignments. This will be the
topic of future work.
753
References
Balthasar Bickel and Johanna Nichols, 2005. The World
Atlas of Language Structures, chapter Inflectional syn-
thesis of the verb. Oxford University Press.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 70?106, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
Isidore Dyen, Joseph Kruskal, and Paul Black. 1992. An
indoeuropean classification, a lexicostatistical experi-
ment. Transactions of the American Philosophical So-
ciety, 82(5).
Chris Dyer. 2007. The ?noisier channel?: Transla-
tion from morphologically complex languages. In
Proceedings on the Workshop on Statistical Machine
Translation, Prague, Czech Republic.
Sharon Goldwater and David McClosky. 2005. Im-
proving statistical MT through morphological analy-
sis. In Proceedings of Empirical Methods in Natural
Language Processing.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings of the
Human Language Technology and North American As-
sociation for Computational Linguistics Conference,
pages 127?133, Edmonton, Canada. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the Association for Computational Linguistics Com-
panion Demo and Poster Sessions, pages 177?180,
Prague, Czech Republic. Association for Computa-
tional Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT-
Summit.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):9?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic evalu-
ation of machine translation. In Proceedings of the As-
sociation for Computational Linguistics, pages 311?
318, Philadelphia, USA.
M Snover, B Dorr, R Schwartz, L Micciulla, and
J Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In AMTA.
Morris Swadesh. 1955. Lexicostatistic dating of prehis-
toric ethnic contacts. In Proceedings American Philo-
sophical Society, volume 96, pages 452?463.
David Talbot and Miles Osborne. 2006. Modelling lex-
ical redundancy for machine translation. In Proceed-
ings of the Association of Computational Linguistics,
Sydney, Australia.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
754
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177?180,
Prague, June 2007. c?2007 Association for Computational Linguistics 
Moses: Open Source Toolkit for Statistical Machine Translation 
Philipp Koehn 
Hieu Hoang  
Alexandra Birch 
Chris Callison-Burch 
University of Edin-
burgh1 
Marcello Federico 
Nicola Bertoldi 
ITC-irst2 
Brooke Cowan 
Wade Shen 
Christine Moran 
MIT3 
Richard Zens 
RWTH Aachen4 
Chris Dyer 
University of Maryland5 
 
Ond?ej Bojar 
Charles University6 
Alexandra Constantin 
Williams College7 
Evan Herbst 
Cornell8 
1 pkoehn@inf.ed.ac.uk, {h.hoang, A.C.Birch-Mayne}@sms.ed.ac.uk, callison-burch@ed.ac.uk. 
2{federico, bertoldi}@itc.it. 3 brooke@csail.mit.edu, swade@ll.mit.edu, weezer@mit.edu. 4 
zens@i6.informatik.rwth-aachen.de. 5 redpony@umd.edu. 6 bojar@ufal.ms.mff.cuni.cz. 7 
07aec_2@williams.edu. 8 evh4@cornell.edu 
 
Abstract 
We describe an open-source toolkit for sta-
tistical machine translation whose novel 
contributions are (a) support for linguisti-
cally motivated factors, (b) confusion net-
work decoding, and (c) efficient data for-
mats for translation models and language 
models. In addition to the SMT decoder, 
the toolkit also includes a wide variety of 
tools for training, tuning and applying the 
system to many translation tasks.  
1 Motivation 
Phrase-based statistical machine translation 
(Koehn et al 2003) has emerged as the dominant 
paradigm in machine translation research. How-
ever, until now, most work in this field has been 
carried out on proprietary and in-house research 
systems. This lack of openness has created a high 
barrier to entry for researchers as many of the 
components required have had to be duplicated. 
This has also hindered effective comparisons of the 
different elements of the systems. 
By providing a free and complete toolkit, we 
hope that this will stimulate the development of the 
field. For this system to be adopted by the commu-
nity, it must demonstrate performance that is com-
parable to the best available systems. Moses has 
shown that it achieves results comparable to the 
most competitive and widely used statistical ma-
chine translation systems in translation quality and 
run-time (Shen et al 2006). It features all the ca-
pabilities of the closed sourced Pharaoh decoder 
(Koehn 2004). 
Apart from providing an open-source toolkit 
for SMT, a further motivation for Moses is to ex-
tend phrase-based translation with factors and con-
fusion network decoding. 
The current phrase-based approach to statisti-
cal machine translation is limited to the mapping of 
small text chunks without any explicit use of lin-
guistic information, be it morphological, syntactic, 
or semantic. These additional sources of informa-
tion have been shown to be valuable when inte-
grated into pre-processing or post-processing steps. 
Moses also integrates confusion network de-
coding, which allows the translation of ambiguous 
input. This enables, for instance, the tighter inte-
gration of speech recognition and machine transla-
tion. Instead of passing along the one-best output 
of the recognizer, a network of different word 
choices may be examined by the machine transla-
tion system. 
Efficient data structures in Moses for the 
memory-intensive translation model and language 
model allow the exploitation of much larger data 
resources with limited hardware. 
177
 2 Toolkit 
The toolkit is a complete out-of-the-box trans-
lation system for academic research. It consists of 
all the components needed to preprocess data, train 
the language models and the translation models. It 
also contains tools for tuning these models using 
minimum error rate training (Och 2003) and evalu-
ating the resulting translations using the BLEU 
score (Papineni et al 2002).  
Moses uses standard external tools for some of 
the tasks to avoid duplication, such as GIZA++ 
(Och and Ney 2003) for word alignments and 
SRILM for language modeling.  Also, since these 
tasks are often CPU intensive, the toolkit has been 
designed to work with Sun Grid Engine parallel 
environment to increase throughput.  
In order to unify the experimental stages, a 
utility has been developed to run repeatable ex-
periments. This uses the tools contained in Moses 
and requires minimal changes to set up and cus-
tomize. 
The toolkit has been hosted and developed un-
der sourceforge.net since inception. Moses has an 
active research community and has reached over 
1000 downloads as of 1st March 2007.  
The main online presence is at  
http://www.statmt.org/moses/ 
where many sources of information about the 
project can be found. Moses was the subject of this 
year?s Johns Hopkins University Workshop on 
Machine Translation (Koehn et al 2006). 
The decoder is the core component of Moses. 
To minimize the learning curve for many research-
ers, the decoder was developed as a drop-in re-
placement for Pharaoh, the popular phrase-based 
decoder. 
In order for the toolkit to be adopted by the 
community, and to make it easy for others to con-
tribute to the project, we kept to the following 
principles when developing the decoder: 
? Accessibility 
? Easy to Maintain 
? Flexibility 
? Easy for distributed team development 
? Portability 
It was developed in C++ for efficiency and fol-
lowed modular, object-oriented design. 
3 Factored Translation Model 
Non-factored SMT typically deals only with 
the surface form of words and has one phrase table, 
as shown in Figure 1. 
i am buying you a green cat
using phrase dictionary:
i
 am buying
you
a
green
cat
je
ach?te
vous
un
vert
chat
a une
je vous ach?te un chat vert
Translate:
 
In factored translation models, the surface 
forms may be augmented with different factors, 
such as POS tags or lemma. This creates a factored 
representation of each word, Figure 2.  
1 1 1 / sing /
                                 
je vous achet un chat
PRO PRO VB ART NN
je vous acheter un chat
st st st present masc masc
? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?
1 1 / 1 sing sing
i buy you a cat
PRO VB PRO ART NN
i tobuy you a cat
st st present st
? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?  
 
Mapping of source phrases to target phrases 
may be decomposed into several steps. Decompo-
sition of the decoding process into various steps 
means that different factors can be modeled sepa-
rately. Modeling factors in isolation allows for 
flexibility in their application. It can also increase 
accuracy and reduce sparsity by minimizing the 
number dependencies for each step. 
For example, we can decompose translating 
from surface forms to surface forms and lemma, as 
shown in Figure 3. 
Figure 2. Factored translation 
Figure 1. Non-factored translation 
178
  
Figure 3. Example of graph of decoding steps 
By allowing the graph to be user definable, we 
can experiment to find the optimum configuration 
for a given language pair and available data.  
The factors on the source sentence are consid-
ered fixed, therefore, there is no decoding step 
which create source factors from other source fac-
tors. However, Moses can have ambiguous input in 
the form of confusion networks. This input type 
has been used successfully for speech to text 
translation (Shen et al 2006). 
Every factor on the target language can have its 
own language model. Since many factors, like 
lemmas and POS tags, are less sparse than surface 
forms, it is possible to create a higher order lan-
guage models for these factors. This may encour-
age more syntactically correct output. In Figure 3 
we apply two language models, indicated by the 
shaded arrows, one over the words and another 
over the lemmas. Moses is also able to integrate 
factored language models, such as those described 
in (Bilmes and Kirchhoff 2003) and (Axelrod 
2006). 
4 Confusion Network Decoding 
Machine translation input currently takes the 
form of simple sequences of words. However, 
there are increasing demands to integrate machine 
translation technology into larger information 
processing systems with upstream NLP/speech 
processing tools (such as named entity recognizers, 
speech recognizers, morphological analyzers, etc.). 
These upstream processes tend to generate multiple, 
erroneous hypotheses with varying confidence. 
Current MT systems are designed to process only 
one input hypothesis, making them vulnerable to 
errors in the input.  
In experiments with confusion networks, we 
have focused so far on the speech translation case, 
where the input is generated by a speech recog-
nizer. Namely, our goal is to improve performance 
of spoken language translation by better integrating 
speech recognition and machine translation models. 
Translation from speech input is considered more 
difficult than translation from text for several rea-
sons. Spoken language has many styles and genres, 
such as, formal read speech, unplanned speeches, 
interviews, spontaneous conversations; it produces 
less controlled language, presenting more relaxed 
syntax and spontaneous speech phenomena. Fi-
nally, translation of spoken language is prone to 
speech recognition errors, which can possibly cor-
rupt the syntax and the meaning of the input. 
There is also empirical evidence that better 
translations can be obtained from transcriptions of 
the speech recognizer which resulted in lower 
scores. This suggests that improvements can be 
achieved by applying machine translation on a 
large set of transcription hypotheses generated by 
the speech recognizers and by combining scores of 
acoustic models, language models, and translation 
models. 
Recently, approaches have been proposed for 
improving translation quality through the process-
ing of multiple input hypotheses. We have imple-
mented in Moses confusion network decoding as 
discussed in (Bertoldi and Federico 2005), and de-
veloped a simpler translation model and a more 
efficient implementation of the search algorithm. 
Remarkably, the confusion network decoder re-
sulted in an extension of the standard text decoder. 
5 Efficient Data Structures for Transla-
tion Model and Language Models 
With the availability of ever-increasing 
amounts of training data, it has become a challenge 
for machine translation systems to cope with the 
resulting strain on computational resources. Instead 
of simply buying larger machines with, say, 12 GB 
of main memory, the implementation of more effi-
cient data structures in Moses makes it possible to 
exploit larger data resources with limited hardware 
infrastructure. 
A phrase translation table easily takes up giga-
bytes of disk space, but for the translation of a sin-
gle sentence only a tiny fraction of this table is 
needed. Moses implements an efficient representa-
tion of the phrase translation table. Its key proper-
ties are a prefix tree structure for source words and 
on demand loading, i.e. only the fraction of the 
phrase table that is needed to translate a sentence is 
loaded into the working memory of the decoder. 
179
 For the Chinese-English NIST  task, the mem-
ory requirement of the phrase table is reduced from 
1.7 gigabytes to less than 20 mega bytes, with no 
loss in translation quality and speed (Zens and Ney 
2007). 
The other large data resource for statistical ma-
chine translation is the language model. Almost 
unlimited text resources can be collected from the 
Internet and used as training data for language 
modeling. This results in language models that are 
too large to easily fit into memory. 
The Moses system implements a data structure 
for language models that is more efficient than the 
canonical SRILM (Stolcke 2002) implementation 
used in most systems. The language model on disk 
is also converted into this binary format, resulting 
in a minimal loading time during start-up of the 
decoder.  
An even more compact representation of the 
language model is the result of the quantization of 
the word prediction and back-off probabilities of 
the language model. Instead of representing these 
probabilities with 4 byte or 8 byte floats, they are 
sorted into bins, resulting in (typically) 256 bins 
which can be referenced with a single 1 byte index. 
This quantized language model, albeit being less 
accurate, has only minimal impact on translation 
performance (Federico and Bertoldi 2006). 
6 Conclusion and Future Work 
This paper has presented a suite of open-source 
tools which we believe will be of value to the MT 
research community. 
We have also described a new SMT decoder 
which can incorporate some linguistic features in a 
consistent and flexible framework. This new direc-
tion in research opens up many possibilities and 
issues that require further research and experimen-
tation. Initial results show the potential benefit of 
factors for statistical machine translation, (Koehn 
et al 2006) and (Koehn and Hoang 2007). 
References 
Axelrod, Amittai. "Factored Language Model for Sta-
tistical Machine Translation." MRes Thesis. 
Edinburgh University, 2006. 
Bertoldi, Nicola, and Marcello Federico. "A New De-
coder for Spoken Language Translation Based 
on Confusion Networks." Automatic Speech 
Recognition and Understanding Workshop 
(ASRU), 2005. 
Bilmes, Jeff A, and Katrin Kirchhoff. "Factored Lan-
guage Models and Generalized Parallel Back-
off." HLT/NACCL, 2003. 
Koehn, Philipp. "Pharaoh: A Beam Search Decoder for 
Phrase-Based Statistical Machine Translation 
Models." AMTA, 2004. 
Koehn, Philipp, Marcello Federico, Wade Shen, Nicola 
Bertoldi, Ondrej Bojar, Chris Callison-Burch, 
Brooke Cowan, Chris Dyer, Hieu Hoang, 
Richard Zens, Alexandra Constantin, Christine 
Corbett Moran, and Evan Herbst. "Open 
Source Toolkit for Statistical Machine Transla-
tion". Report of the 2006 Summer Workshop at 
Johns Hopkins University, 2006. 
Koehn, Philipp, and Hieu Hoang. "Factored Translation 
Models." EMNLP, 2007. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
"Statistical Phrase-Based Translation." 
HLT/NAACL, 2003. 
Och, Franz Josef. "Minimum Error Rate Training for 
Statistical Machine Translation." ACL, 2003. 
Och, Franz Josef, and Hermann Ney. "A Systematic 
Comparison of Various Statistical Alignment 
Models." Computational Linguistics 29.1 
(2003): 19-51. 
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. "BLEU: A Method for Automatic 
Evaluation of Machine Translation." ACL, 
2002. 
Shen, Wade, Richard Zens, Nicola Bertoldi, and 
Marcello Federico. "The JHU Workshop 2006 
Iwslt System." International Workshop on Spo-
ken Language Translation, 2006. 
Stolcke, Andreas. "SRILM an Extensible Language 
Modeling Toolkit." Intl. Conf. on Spoken Lan-
guage Processing, 2002. 
Zens, Richard, and Hermann Ney. "Efficient Phrase-
Table Representation for Machine Translation 
with Applications to Online MT and Speech 
Recognition." HLT/NAACL, 2007. 
180
Proceedings of the Workshop on Statistical Machine Translation, pages 154?157,
New York City, June 2006. c?2006 Association for Computational Linguistics
Constraining the Phrase-Based, Joint Probability Statistical Translation
Model
Alexandra Birch Chris Callison-Burch Miles Osborne Philipp Koehn
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW, UK
a.c.birch-mayne@sms.ed.ac.uk
Abstract
The joint probability model proposed by
Marcu and Wong (2002) provides a strong
probabilistic framework for phrase-based
statistical machine translation (SMT). The
model?s usefulness is, however, limited by
the computational complexity of estimat-
ing parameters at the phrase level. We
present the first model to use word align-
ments for constraining the space of phrasal
alignments searched during Expectation
Maximization (EM) training. Constrain-
ing the joint model improves performance,
showing results that are very close to state-
of-the-art phrase-based models. It also al-
lows it to scale up to larger corpora and
therefore be more widely applicable.
1 Introduction
Machine translation is a hard problem because of
the highly complex, irregular and diverse nature
of natural languages. It is impossible to accurately
model all the linguistic rules that shape the trans-
lation process, and therefore a principled approach
uses statistical methods to make optimal decisions
given incomplete data.
The original IBM Models (Brown et al, 1993)
learn word-to-word alignment probabilities which
makes it computationally feasible to estimate
model parameters from large amounts of train-
ing data. Phrase-based SMT models, such as the
alignment template model (Och, 2003), improve
on word-based models because phrases provide
local context which leads to better lexical choice
and more reliable local reordering. However, most
phrase-based models extract their phrase pairs
from previously word-aligned corpora using ad-
hoc heuristics. These models perform no search
for optimal phrasal alignments. Even though this
is an efficient strategy, it is a departure from the
rigorous statistical framework of the IBM Models.
Marcu and Wong (2002) proposed the joint
probability model which directly estimates the
phrase translation probabilities from the corpus in
a theoretically governed way. This model neither
relies on potentially sub-optimal word alignments
nor on heuristics for phrase extraction. Instead, it
searches the phrasal alignment space, simultane-
ously learning translation lexicons for both words
and phrases. The joint model has been shown to
outperform standard models on restricted data sets
such as the small data track for Chinese-English in
the 2004 NIST MT Evaluation (Przybocki, 2004).
However, considering all possible phrases and
all their possible alignments vastly increases the
computational complexity of the joint model when
compared to its word-based counterpart. In this
paper, we propose a method of constraining the
search space of the joint model to areas where
most of the unpromising phrasal alignments are
eliminated and yet as many potentially useful
alignments as possible are still explored. The
joint model is constrained to phrasal alignments
which do not contradict a set high confidence word
alignments for each sentence. These high con-
fidence alignments could incorporate information
from both statistical and linguistic sources. In this
paper we use the points of high confidence from
the intersection of the bi-directional Viterbi word
alignments to constrain the model, increasing per-
formance and decreasing complexity.
154
2 Translation Models
2.1 Standard Phrase-based Model
Most phrase-based translation models (Och, 2003;
Koehn et al, 2003; Vogel et al, 2003) rely on
a pre-existing set of word-based alignments from
which they induce their parameters. In this project
we use the model described by Koehn et al (2003)
which extracts its phrase alignments from a corpus
that has been word aligned. From now on we re-
fer to this phrase-based translation model as the
standard model. The standard model decomposes
the foreign input sentence F into a sequence of
I phrases f1, . . . , f I . Each foreign phrase fi is
translated to an English phrase ei using the prob-
ability distribution ?(f i|ei). English phrases may
be reordered using a relative distortion probability.
This model performs no search for optimal
phrase pairs. Instead, it extracts phrase pairs
(f i, ei) in the following manner. First, it uses the
IBM Models to learn the most likely word-level
Viterbi alignments for English to Foreign and For-
eign to English. It then uses a heuristic to recon-
cile the two alignments, starting from the points
of high confidence in the intersection of the two
Viterbi alignments and growing towards the points
in the union. Points from the union are selected if
they are adjacent to points from the intersection
and their words are previously unaligned.
Phrases are then extracted by selecting phrase
pairs which are ?consistent? with the symmetrized
alignment, which means that all words within the
source language phrase are only aligned to the
words of the target language phrase and vice versa.
Finally the phrase translation probability distribu-
tion is estimated using the relative frequencies of
the extracted phrase pairs.
This approach to phrase extraction means that
phrasal alignments are locked into the sym-
metrized alignment. This is problematic because
the symmetrization process will grow an align-
ment based on arbitrary decisions about adjacent
words and because word alignments inadequately
represent the real dependencies between transla-
tions.
2.2 Joint Probability Model
The joint model (Marcu and Wong, 2002), does
not rely on a pre-existing set of word-level align-
ments. Like the IBM Models, it uses EM to align
and estimate the probabilities for sub-sentential
units in a parallel corpus. Unlike the IBM Mod-
els, it does not constrain the alignments to being
single words.
The joint model creates phrases from words and
commonly occurring sequences of words. A con-
cept, ci, is defined as a pair of aligned phrases
< ei, f i >. A set of concepts which completely
covers the sentence pair is denoted by C. Phrases
are restricted to being sequences of words which
occur above a certain frequency in the corpus.
Commonly occurring phrases are more likely to
lead to the creation of useful phrase pairs, and
without this restriction the search space would be
much larger.
The probability of a sentence and its translation
is the sum of all possible alignments C, each of
which is defined as the product of the probability
of all individual concepts:
p(F,E) =
?
C?C
?
<ei,f i>?C
p(< ei, f i >) (1)
The model is trained by initializing the trans-
lation table using Stirling numbers of the second
kind to efficiently estimate p(< ei, f i >) by cal-
culating the proportion of alignments which con-
tain p(< ei, f i >) compared to the total number
of alignments in the sentence (Marcu and Wong,
2002). EM is then performed by first discovering
an initial phrasal alignments using a greedy algo-
rithm similar to the competitive linking algorithm
(Melamed, 1997). The highest probability phrase
pairs are iteratively selected until all phrases are
are linked. Then hill-climbing is performed by
searching once for each iteration for all merges,
splits, moves and swaps that improve the proba-
bility of the initial phrasal alignment. Fractional
counts are collected for all alignments visited.
Training the IBM models is computationally
challenging, but the joint model is much more de-
manding. Considering all possible segmentations
of phrases and all their possible alignments vastly
increases the number of possible alignments that
can be formed between two sentences. This num-
ber is exponential with relation to the length of the
shorter sentence.
3 Constraining the Joint Model
The joint model requires a strategy for restricting
the search for phrasal alignments to areas of the
alignment space which contain most of the proba-
bility mass. We propose a method which examines
155
phrase pairs that are consistent with a set of high
confidence word alignments defined for the sen-
tence. The set of alignments are taken from the in-
tersection of the bi-directional Viterbi alignments.
This strategy for extracting phrase pairs is simi-
lar to that of the standard phrase-based model and
the definition of ?consistent? is the same. How-
ever, the constrained joint model does not lock
the search into a heuristically derived symmetrized
alignment. Joint model phrases must also occur
above a certain frequency in the corpus to be con-
sidered.
The constraints on the model are binding during
the initialization phase of training. During EM,
inconsistent phrase pairs are given a small, non-
zero probability and are thus not considered un-
less unaligned words remain after linking together
high probability phrase pairs. All words must be
aligned, there is no NULL alignment like in the
IBM models.
By using the IBM Models to constrain the joint
model, we are searching areas in the phrasal align-
ment space where both models overlap. We com-
bine the advantage of prior knowledge about likely
word alignments with the ability to perform a
probabilistic search around them.
4 Experiments
All data and software used was from the NAACL
2006 Statistical Machine Translation workshop
unless otherwise indicated.
4.1 Constraints
The unconstrained joint model becomes in-
tractable with very small amounts of training data.
On a machine with 2 Gb of memory, we were
only able to train 10,000 sentences of the German-
English Europarl corpora. Beyond this, pruning is
required to keep the model in memory during EM.
Table 1 shows that the application of the word con-
straints considerably reduces the size of the space
of phrasal alignments that is searched. It also im-
proves the BLEU score of the model, by guiding it
to explore the more promising areas of the search
space.
4.2 Scalability
Even though the constrained joint model reduces
complexity, pruning is still needed in order to scale
up to larger corpora. After the initialization phase
of the training, all phrase pairs with counts less
Unconstrained Constrained
No. Concepts 6,178k 1,457k
BLEU 19.93 22.13
Time(min) 299 169
Table 1. The impact of constraining the joint model
trained on 10,000 sentences of the German-English
Europarl corpora and tested with the Europarl test set
used in Koehn et al (2003)
than 10 million times that of the phrase pair with
the highest count, are pruned from the phrase ta-
ble. The model is also parallelized in order to
speed up training.
The translation models are included within a
log-linear model (Och and Ney, 2002) which al-
lows a weighted combination of features func-
tions. For the comparison of the basic systems
in Table 2 only three features were used for both
the joint and the standard model: p(e|f), p(f |e)
and the language model, and they were given equal
weights.
The results in Table 2 show that the joint model
is capable of training on large data sets, with a
reasonable performance compared to the standard
model. However, here it seems that the standard
model has a slight advantage. This is almost cer-
tainly related to the fact that the joint model results
in a much smaller phrase table. Pruning eliminates
many phrase pairs, but further investigations indi-
cate that this has little impact on BLEU scores.
BLEU Size
Joint Model 25.49 2.28
Standard Model 26.15 19.04
Table 2. Basic system comparisons: BLEU scores
and model size in millions of phrase pairs for Spanish-
English
The results in Table 3 compare the joint and the
standard model with more features. Apart from
including all Pharaoh?s default features, we use
two new features for both the standard and joint
models: a 5-gram language model and a lexical-
ized reordering model as described in Koehn et al
(2005). The weights of the feature functions, or
model components, are set by minimum error rate
training provided by David Chiang from the Uni-
versity of Maryland.
On smaller data sets (Koehn et al, 2003) the
joint model shows performance comparable to the
standard model, however the joint model does
not reach the level of performance of the stan-
156
EN-ES ES-EN
Joint
3-gram, dl4 20.51 26.64
5-gram, dl6 26.34 27.17
+ lex. reordering 26.82 27.80
Standard Model
5-gram, dl6
+ lex. reordering 31.18 31.86
Table 3. Bleu scores for the joint model and the stan-
dard model showing the effect of the 5-gram language
model, distortion length of 6 (dl) and the addition of
lexical reordering for the English-Spanish and Spanish-
English tasks.
dard model for this larger data set. This could
be due to the fact that the joint model results in
a much smaller phrase table. During EM only
phrase pairs that occur in an alignment visited dur-
ing hill-climbing are retained. Only a very small
proportion of the alignment space can be searched
and this reduces the chances of finding optimum
parameters. The small number of alignments vis-
ited would lead to data sparseness and over-fitting.
Another factor could be efficiency trade-offs like
the fast but not optimal competitive linking search
for phrasal alignments.
4.3 German-English submission
We also submitted a German-English system using
the standard approach to phrase extraction. The
purpose of this submission was to validate the syn-
tactic reordering method that we previously pro-
posed (Collins et al, 2005). We parse the Ger-
man training and test corpus and reorder it accord-
ing to a set of manually devised rules. Then, we
use our phrase-based system with standard phrase-
extraction, lexicalized reordering, lexical scoring,
5-gram LM, and the Pharaoh decoder.
On the development test set, the syntactic re-
ordering improved performance from 26.86 to
27.70. The best submission in last year?s shared
task achieved a score of 24.77 on this set.
5 Conclusion
We presented the first attempt at creating a system-
atic framework which uses word alignment con-
straints to guide phrase-based EM training. This
shows competitive results, to within 0.66 BLEU
points for the basic systems, suggesting that a
rigorous probabilistic framework is preferable to
heuristics for extracting phrase pairs and their
probabilities.
By introducing constraints to the alignment
space we can reduce the complexity of the joint
model and increase its performance, allowing it to
train on larger corpora and making the model more
widely applicable.
For the future, the joint model would benefit
from lexical weighting like that used in the stan-
dard model (Koehn et al, 2003). Using IBM
Model 1 to extract a lexical alignment weight for
each phrase pair would decrease the impact of data
sparseness, and other kinds smoothing techniques
will be investigated. Better search algorithms for
Viterbi phrasal alignments during EM would in-
crease the number and quality of model parame-
ters.
This work was supported in part under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-C-0022.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005.
Clause restructuring for statistical machine translation. In
Proceedings of ACL.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT/NAACL, pages 127?133.
Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne,
and Chris Callison-Burch. 2005. Edinburgh system de-
scription. In IWSLT Speech Translation Evaluation.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine translation.
In Proceedings of EMNLP.
Dan Melamed. 1997. A word-to-word model of translational
equivalence. In Proceedings of ACL.
Franz Josef Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical ma-
chine translation. In ACL.
Franz Josef Och. 2003. Statistical Machine Translation:
From Single-Word Models to Alignment Templates. Ph.D.
thesis, RWTH Aachen Department of Computer Science,
Aachen, Germany.
Mark Przybocki. 2004. NIST 2004 machine translation eval-
uation results. Confidential e-mail to workshop partici-
pants, May.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Tribble,
Ashish Venugopal, Bing Zhao, and Alex Waibel. 2003.
The CMU statistical machine translation system. In Ma-
chine Translation Summit.
157
Proceedings of the Second Workshop on Statistical Machine Translation, pages 9?16,
Prague, June 2007. c?2007 Association for Computational Linguistics
CCG Supertags in Factored Statistical Machine Translation
Alexandra Birch Miles Osborne Philipp Koehn
a.c.birch-mayne@sms.ed.ac.uk miles@inf.ed.ac.uk pkoehn@inf.ed.ac.uk
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW, UK
Abstract
Combinatorial Categorial Grammar (CCG)
supertags present phrase-based machine
translation with an opportunity to access
rich syntactic information at a word level.
The challenge is incorporating this informa-
tion into the translation process. Factored
translation models allow the inclusion of su-
pertags as a factor in the source or target lan-
guage. We show that this results in an im-
provement in the quality of translation and
that the value of syntactic supertags in flat
structured phrase-based models is largely
due to better local reorderings.
1 Introduction
In large-scale machine translation evaluations,
phrase-based models generally outperform syntax-
based models1. Phrase-based models are effective
because they capture the lexical dependencies be-
tween languages. However, these models, which
are equivalent to finite-state machines (Kumar and
Byrne, 2003), are unable to model long range word
order differences. Phrase-based models also lack the
ability to incorporate the generalisations implicit in
syntactic knowledge and they do not respect linguis-
tic phrase boundaries. This makes it difficult to im-
prove reordering in phrase-based models.
Syntax-based models can overcome some of the
problems associated with phrase-based models be-
cause they are able to capture the long range struc-
tural mappings that occur in translation. Recently
1www.nist.gov/speech/tests/mt/mt06eval official results.html
there have been a few syntax-based models that
show performance comparable to the phrase-based
models (Chiang, 2005; Marcu et al, 2006). How-
ever, reliably learning powerful rules from parallel
data is very difficult and prone to problems with
sparsity and noise in the data. These models also
suffer from a large search space when decoding with
an integrated language model, which can lead to
search errors (Chiang, 2005).
In this paper we investigate the idea of incorporat-
ing syntax into phrase-based models, thereby lever-
aging the strengths of both the phrase-based models
and syntactic structures. This is done using CCG
supertags, which provide a rich source of syntactic
information. CCG contains most of the structure of
the grammar in the lexicon, which makes it possi-
ble to introduce CCG supertags as a factor in a fac-
tored translation model (Koehn et al, 2006). Fac-
tored models allow words to be vectors of features:
one factor could be the surface form and other fac-
tors could contain linguistic information.
Factored models allow for the easy inclusion of
supertags in different ways. The first approach is to
generate CCG supertags as a factor in the target and
then apply an n-gram model over them, increasing
the probability of more frequently seen sequences
of supertags. This is a simple way of including syn-
tactic information in a phrase-based model, and has
also been suggested by Hassan et al (2007). For
both Arabic-English (Hassan et al, 2007) and our
experiments in Dutch-English, n-gram models over
CCG supertags improve the quality of translation.
By preferring more likely sequences of supertags,
it is conceivable that the output of the decoder is
9
more grammatical. However, its not clear exactly
how syntactic information can benefit a flat struc-
tured model: the constraints contained within su-
pertags are not enforced and relationships between
supertags are not linear. We perform experiments to
explore the nature and limits of the contribution of
supertags, using different orders of n-gram models,
reordering models and focussed manual evaluation.
It seems that the benefit of using n-gram supertag
sequence models is largely from improving reorder-
ing, as much of the gain is eroded by using a lexi-
calised reordering model. This is supported by the
manual evaluation which shows a 44% improvement
in reordering Dutch-English verb final sentences.
The second and novel way we use supertags is
to direct the translation process. Supertags on the
source sentence allows the decoder to make deci-
sions based on the structure of the input. The sub-
categorisation of a verb, for instance, might help se-
lect the correct translation. Using multiple depen-
dencies on factors in the source, we need a strat-
egy for dealing with sparse data. We propose using
a logarithmic opinion pool (Smith et al, 2005) to
combine the more specific models (which depend on
both words and supertags) with more general mod-
els (which only depends on words). This paper is the
first to suggest this approach for combining multiple
information sources in machine translation.
Although the addition of supertags to phrase-
based translation does show some improvement,
their overall impact is limited. Sequence models
over supertags clearly result in some improvements
in local reordering but syntactic information con-
tains long distance dependencies which are simply
not utilised in phrase-based models.
2 Factored Models
Inspired by work on factored language models,
Koehn et al (2006) extend phrase-based models to
incorporate multiple levels of linguistic knowledge
as factors. Phrase-based models are limited to se-
quences of words as their units with no access to
additional linguistic knowledge. Factors allow for
richer translation models, for example, the gender or
tense of a word can be expressed. Factors also allow
the model to generalise, for example, the lemma of a
word could be used to generalise to unseen inflected
forms.
The factored translation model combines features
in a log-linear fashion (Och, 2003). The most likely
target sentence t? is calculated using the decision rule
in Equation 1:
t? = argmax
t
{
M?
m=1
?mhm(s
Fs
1 , t
Ft
1 )
}
(1)
t? ?
M?
m=1
?mhm(s
Fs
1 , t
Ft
1 ) (2)
where M is the number of features, hm(s
Fs
1 , t
Ft
1 )
are the feature functions over the factors, and ? are
the weights which combine the features which are
optimised using minimum error rate training (Venu-
gopal and Vogel, 2005). Each function depends on a
vector sFs1 of source factors and a vector t
Ft
1 of tar-
get factors. An example of a factored model used in
upcoming experiments is:
t? ?
M?
m=1
?mhm(sw, twc) (3)
where sw means the model depends on (s)ource
(w)ords, and twc means the model generates (t)arget
(w)ords and (c)cg supertags. The model is shown
graphically in Figure 1.
WordWord
CCG
SOURCE TARGET
Figure 1. Factored translation with source words deter-
mining target words and CCG supertags
For our experiments we used the following fea-
tures: the translation probabilities Pr(sFs1 |t
Ft
1 ) and
Pr(tFt1 |s
Fs
1 ), the lexical weights (Koehn et al, 2003)
lex(sFs1 |t
Ft
1 ) and lex(t
Ft
1 |s
Fs
1 ), and a phrase penalty
e, which allows the model to learn a preference for
longer or shorter phrases. Added to these features
10
is the word penalty e?1 which allows the model to
learn a preference for longer or shorter sentences,
the distortion model d that prefers monotone word
order, and the language model probability Pr(t).
All these features are logged when combined in the
log-linear model in order to retain the impact of very
unlikely translations or sequences.
One of the strengths of the factored model is it
allows for n-gram distributions over factors on the
target. We call these distributions sequence models.
By analogy with language models, for example, we
can construct a bigram sequence model as follows:
p(f1, f2, . . . fn) = p(f1)
n?
i=2
p(fi|f(i?1))
where f is a factor (eg. CCG supertags) and n is
the length of the string. Sequence models over POS
tags or supertags are smaller than language models
because they have restricted lexicons. Higher or-
der, more powerful sequence models can therefore
be used.
Applying multiple factors in the source can lead to
sparse data problems. One solution is to break down
the translation into smaller steps and translate each
factor separately like in the following model where
source words are translated separately to the source
supertags:
t? ?
M?
m=1
?mhm(sw, tw) +
N?
n=1
?nhn(sc, tw)
However, in many cases multiple dependencies
are desirable. For instance translating CCG su-
pertags independently of words could introduce er-
rors. Multiple dependencies require some form of
backing off to simpler models in order to cover the
cases where, for instance, the word has been seen in
training, but not with that particular supertag. Dif-
ferent backoff paths are possible, and it would be
interesting but prohibitively slow to apply a strat-
egy similar to generalised parallel backoff (Bilmes
and Kirchhoff, 2003) which is used in factored lan-
guage models. Backoff in factored language mod-
els is made more difficult because there is no ob-
vious backoff path. This is compounded for fac-
tored phrase-based translation models where one has
to consider backoff in terms of factors and n-gram
lengths in both source and target languages. Fur-
thermore, the surface form of a word is probably the
most valuable factor and so its contribution must al-
ways be taken into account. We therefore did not use
backoff and chose to use a log-linear combination of
features and models instead.
Our solution is to extract two translation models:
t? ?
M?
m=1
?mhm(swc, tw) +
N?
n=1
?nhn(sw, tw) (4)
One model consists of more specific features m
and would return log probabilities, for example
log2Pr(tw|swc), if the particular word and supertag
had been seen before in training. Otherwise it re-
turns ?C, a negative constant emulating log2(0).
The other model consist of more general features
n and always returns log probabilities, for example
log2Pr(tw|sw).
3 CCG and Supertags
CCGs have syntactically rich lexicons and a small
set of combinatory operators which assemble the
parse-trees. Each word in the sentence is assigned a
category from the lexicon. A category may either be
atomic (S, NP etc.) or complex (S\S, (S\NP)/NP
etc.). Complex categories have the general form
?/? or ?\? where ? and ? are themselves cate-
gories. An example of a CCG parse is given:
Peter eats apples
NP (S\NP)/NP NP
>
S\NP
<
S
where the derivation proceeds as follows: ?eats?
is combined with ?apples? under the operation of
forward application. ?eats? can be thought of as a
function that takes a NP to the right and returns a
S\NP. Similarly the phrase ?eats apples? can be
thought of as a function which takes a noun phrase
NP to the left and returns a sentence S. This opera-
tion is called backward application.
A sentence together with its CCG categories al-
ready contains most of the information present in a
full parse. Because these categories are lexicalised,
11
they can easily be included into factored phrase-
based translation. CCG supertags are categories that
have been provided by a supertagger. Supertags
were introduced by Bangalore (1999) as a way of in-
creasing parsing efficiency by reducing the number
of structures assigned to each word. Clark (2002)
developed a suppertagger for CCG which uses a
conditional maximum entropy model to estimate the
probability of words being assigned particular cat-
egories. Here is an example of a sentence that has
been supertagged in the training corpus:
We all agree on that .
NP NP\NP (S[dcl]\NP)/PP PP/NP NP .
The verb ?agree? has been assigned a complex su-
pertag (S[dcl]\NP)/PP which determines the type
and direction of its arguments. This information can
be used to improve the quality of translation.
4 Experiments
The first set of experiments explores the effect of
CCG supertags on the target, translating from Dutch
into English. The last experiment shows the effect
of CCG supertags on the source, translating from
German into English. These language pairs present
a considerable reordering challenge. For example,
Dutch and German have SOVword order in subordi-
nate clauses. This means that the verb often appears
at the end of the clause, far from the position of the
English verb.
4.1 Experimental Setup
The experiments were run using Moses2, an open
source factored statistical machine translation sys-
tem. The SRILM language modelling toolkit (Stol-
cke, 2002) was used with modified Kneser-Ney dis-
counting and interpolation. The CCG supertag-
ger (Clark, 2002; Clark and Curran, 2004) was pro-
vided with the C&C Language Processing Tools3.
The supertagger was trained on the CCGBank in
English (Hockenmaier and Steedman, 2005) and in
German (Hockenmaier, 2006).
The Dutch-English parallel training data comes
from the Europarl corpus (Koehn, 2005) and ex-
cludes the proceedings from the last quarter of 2000.
2see http://www.statmt.org/moses/
3see http://svn.ask.it.usyd.edu.au/trac/candc/wiki
This consists of 855,677 sentences with a maximum
of 50 words per sentence. 500 sentences of tuning
data and the 2000 sentences of test data are taken
from the ACLWorkshop on Building and Using Par-
allel Texts4.
The German-English experiments use data from
the NAACL 2006 Workshop on Statistical Machine
Translation5. The data consists of 751,088 sentences
of training data, 500 sentences of tuning data and
3064 sentences of test data. The English and Ger-
man training sets were POS tagged and supertagged
before lowercasing. The language models and the
sequence models were trained on the Europarl train-
ing data. Where not otherwise specified, the POS
tag and supertag sequence models are 5-gram mod-
els and the language model is a 3-gram model.
4.2 Sequence Models Over Supertags
Our first Dutch-English experiment seeks to estab-
lish what effect sequence models have on machine
translation. We show that supertags improve trans-
lation quality. Together with Shen et al (2006) it is
one of the first results to confirm the potential of the
factored model.
Model BLEU
sw, tw 23.97
sw, twp 24.11
sw, twc 24.42
sw, twpc 24.43
Table 1. The effect of sequence models on Dutch-English
BLEU score. Factors are (w)ords, (p)os tags, (c)cg su-
pertags on the source s or the target t
Table 1 shows that sequence models over CCG su-
pertags in the target (model sw, twc) improves over
the baseline (model sw, tw) which has no supertags.
Supertag sequence models also outperform models
which apply POS tag sequence models (sw, twp)
and, interestingly do just as well as models which
apply both POS tag and supertag sequence mod-
els (sw, twps). Supertags are more informative than
POS tags as they contain the syntactic context of a
word.
These experiments were run with the distortion
limit set to 6. This means that at most 6 words in
4see http://www.statmt.org/wpt05/
5see http://www.statmt.org/wpt06/
12
the source sentence can be skipped. We tried setting
the distortion limit to 15 to see if allowing longer
distance reorderings with CCG supertag sequence
models could further improve performance, however
it resulted in a decrease in performance to a BLEU
score of 23.84.
4.3 Manual Analysis
The BLEU score improvement in Table 1 does not
explain how the supertag sequence models affect the
translation process. As suggested by Callison-Burch
et al(2006) we perform a focussed manual analysis
of the output to see what changes have occurred.
From the test set, we randomly selected 100
sentences which required reordering of verbs: the
Dutch sentences ended with a verb which had to be
moved forward in the English translation. We record
whether or not the verb was correctly translated and
whether it was reordered to the correct position in
the target sentence.
Model Translated Reordered
sw, tw 81 36
sw, twc 87 52
Table 2. Analysis of % correct translation and reordering
of verbs for Dutch-English translation
In Table 2 we can see that the addition of the CCG
supertag sequence model improved both the transla-
tion of the verbs and their reordering. However, the
improvement is much more pronounced for reorder-
ing. The difference in the reordering results is signif-
icant at p < 0.05 using the ?2 significance test. This
shows that the syntactic information in the CCG su-
pertags is used by the model to prefer better word
order for the target sentence.
In Figure 2 we can see two examples of Dutch-
English translations that have improved with the ap-
plication of CCG supertag sequence models. In the
first example the verb ?heeft? occurs at the end of the
source sentence. The baseline model (sw, tw) does
not manage to translate ?heeft?. The model with the
CCG supertag sequence model (sw, twc) translates it
correctly as ?has? and reorders it correctly 4 places
to the left. The second example also shows the se-
quence model correctly translating the Dutch verb at
the end of the sentence ?nodig?. One can see that it
is still not entirely grammatical.
The improvements in reordering shown here are
reorderings over a relatively short distance, two or
three positions. This is well within the 5-gram order
of the CCG supertag sequence model and we there-
fore consider this to be local reordering.
4.4 Order of the Sequence Model
The CCG supertags describe the syntactic context
of the word they are attached to. Therefore they
have an influence that is greater in scope than sur-
face words or POS tags. Increasing the order of
the CCG supertag sequence model should also in-
crease the ability to perform longer distance reorder-
ing. However, at some point the reliability of the
predictions of the sequence models is impaired due
to sparse counts.
Model None 1gram 3gram 5gram 7gram
sw, twc 24.18 23.96 24.19 24.42 24.32
sw, twpc 24.34 23.86 24.09 24.43 24.14
Table 3. BLUE scores for Dutch-English models which
apply CCG supertag sequence models of varying orders
In Table 3 we can see that the optimal order for
the CCG supertag sequence models is 5.
4.5 Language Model vs. Supertags
The language model makes a great contribution to
the correct order of the words in the target sentence.
In this experiment we investigate whether by using a
stronger language model the contribution of the se-
quence model will no longer be relevant. The rel-
ative contribution of the language mode and differ-
ent sequence models is investigated for different lan-
guage model n-gram lengths.
Model None 1gram 3gram 5gram 7gram
sw, tw - 21.22 23.97 24.05 24.13
sw, twp 21.87 21.83 24.11 24.25 24.06
sw, twc 21.75 21.70 24.42 24.67 24.60
sw, twpc 21.99 22.07 24.43 24.48 24.42
Table 4. BLEU scores for Dutch-English models which use
language models of increasing n-gram length. Column
None does not apply any language model. Model sw, tw
does not apply any sequence models, and model sw, twpc
applies both POS tag and supertag sequence models.
In Table 4 we can see that if no language model
is present(None), the system benefits slightly from
13
source:hij kan toch niet beweren dat hij daar geen exacte informatie over heeft !
reference: how can he say he does not have any precise information ?
sw, tw:he cannot say that he is not an exact information about .
sw, twc: he cannot say that he has no precise information on this !
source: wij moeten hun verwachtingen niet beschamen . meer dan ooit hebben al die landen thans onze bijstand nodig
reference: we must not disappoint them in their expectations , and now more than ever these countries need our help
sw, tw:we must not fail to their expectations , more than ever to have all these countries now our assistance necessary
sw, twc: we must not fail to their expectations , more than ever , those countries now need our assistance
Figure 2. Examples where the CCG supertag sequence model improves Dutch-English translation
having access to all the other sequence models.
However, the language model contribution is very
strong and in isolation contributes more to transla-
tion performance than any other sequence model.
Even with a high order language model, applying
the CCG supertag sequence model still seems to im-
prove performance. This means that even if we use
a more powerful language model, the structural in-
formation contained in the supertags continues to be
beneficial.
4.6 Lexicalised Reordering vs. Supertags
In this experiment we investigate using a stronger
reordering model to see how it compares to the con-
tribution that CCG supertag sequence models make.
Moses implements the lexicalised reordering model
described by Tillman (2004), which learns whether
phrases prefer monotone, inverse or disjoint orienta-
tions with regard to adjacent phrases. We apply this
reordering models to the following experiments.
Model None Lex. Reord.
sw, tw 23.97 24.72
sw, twc 24.42 24.78
Table 5. Dutch-English models with and without a lexi-
calised reordering model.
In Table 5 we can see that lexicalised reorder-
ing improves translation performance for both mod-
els. However, the improvement that was seen us-
ing CCG supertags without lexicalised reordering,
almost disappears when using a stronger reordering
model. This suggests that CCG supertags? contribu-
tion is similar to that of a reordering model. The lex-
icalised reordering model only learns the orientation
of a phrase with relation to its adjacent phrase, so its
influence is very limited in range. If it can replace
CCG supertags, it suggests that supertags? influence
is also within a local range.
4.7 CCG Supertags on Source
Sequence models over supertags improve the perfor-
mance of phrase-based machine translation. How-
ever, this is a limited way of leveraging the rich syn-
tactic information available in the CCG categories.
We explore the potential of letting supertags direct
translation by including them as a factor on the
source. This is similar to syntax-directed translation
originally proposed for compiling (Aho and Ullman,
1969), and also used in machine translation (Quirk et
al., 2005; Huang et al, 2006). Information about the
source words? syntactic function and subcategori-
sation can directly influence the hypotheses being
searched in decoding. These experiments were per-
formed on the German to English translation task,
in contrast to the Dutch to English results given in
previous experiments.
We use a model which combines more specific
dependencies on source words and source CCG su-
pertags, with a more general model which only has
dependancies on the source word, see Equation 4.
We explore two different ways of balancing the sta-
tistical evidence from these multiple sources. The
first way to combine the general and specific sources
of information is by considering features from both
models as part of one large log-linear model. How-
ever, by including more and less informative fea-
tures in one model, we may transfer too much ex-
planatory power to the more specific features. To
overcome this problem, Smith et al (2006) demon-
strated that using ensembles of separately trained
models and combining them in a logarithmic opin-
ion pool (LOP) leads to better parameter values.
This approach was used as the second way in which
14
we combined our models. An ensemble of log-linear
models was combined using a multiplicative con-
stant ? which we train manually using held out data.
t? ?
M?
m=1
?mhm(swc, tw) + ?
(
N?
n=1
?nhn(sw, tw)
)
Typically, the two models would need to be nor-
malised before being combined, but here the multi-
plicative constant fulfils this ro?le by balancing their
separate contributions. This is the first work sug-
gesting the application of LOPs to decoding in ma-
chine translation. In the future more sophisticated
translation models and ensembles of models will
need methods such as LOPs in order to balance sta-
tistical evidence from multiple sources.
Model BLEU
sw, tw 23.30
swc, tw 19.73
single 23.29
LOP 23.46
Table 6. German-English: CCG supertags are used as a
factor on the source. The simple models are combined in
two ways: either as a single log-linear model or as a LOP
of log-linear models
Table 6 shows that the simple, general model
(model sw, tw) performs considerably better than
the simple specific model, where there are multi-
ple dependencies on both words and CCG supertags
(model swc, tw). This is because there are words in
the test sentence that have been seen before but not
with the CCG supertag. Statistical evidence from
multiple sources must be combined. The first way
to combine them is to join them in one single log-
linear model, which is trained over many features.
This makes finding good weights difficult as the in-
fluence of the general model is greater, and its dif-
ficult for the more specific model to discover good
weights. The second method for combining the in-
formation is to use the weights from the separately
trained simple models and then combine them in a
LOP. Held out data is used to set the multiplicative
constant needed to balance the contribution of the
two models. We can see that this second approach is
more successful and this suggests that it is important
to carefully consider the best ways of combining dif-
ferent sources of information when using ensembles
of models. However, the results of this experiment
are not very conclusive. There is no uncertainty in
the source sentence and the value of modelling it us-
ing CCG supertags is still to be demonstrated.
5 Conclusion
The factored translation model allows for the inclu-
sion of valuable sources of information in many dif-
ferent ways. We have shown that the syntactically
rich CCG supertags do improve the translation pro-
cess and we investigate the best way of including
them in the factored model. Using CCG supertags
over the target shows the most improvement, espe-
cially when using targeted manual evaluation. How-
ever, this effect seems to be largely due to improved
local reordering. Reordering improvements can per-
haps be more reliably made using better reordering
models or larger, more powerful language models.
A further consideration is that supertags will always
be limited to the few languages for which there are
treebanks.
Syntactic information represents embedded
structures which are naturally incorporated into
grammar-based models. The ability of a flat struc-
tured model to leverage this information seems to be
limited. CCG supertags? ability to guide translation
would be enhanced if the constraints encoded in
the tags were to be enforced using combinatory
operators.
6 Acknowledgements
We thank Hieu Hoang for assistance with Moses, Ju-
lia Hockenmaier for access to CCGbank lexicons in
German and English, and Stephen Clark and James
Curran for providing the supertagger. This work was
supported in part under the GALE program of the
Defense Advanced Research Projects Agency, Con-
tract No. HR0011-06-C-0022 and in part under the
EuroMatrix project funded by the European Com-
mission (6th Framework Programme).
15
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Properties of syn-
tax directed translations. Journal of Computer and System
Sciences, 3(3):319?334.
Srinivas Bangalore and Aravind Joshi. 1999. Supertagging:
An approach to almost parsing. Computational Linguistics,
25(2):237?265.
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored language
models and generalized parallel backoff. In Proceedings of
the North American Association for Computational Linguis-
tics Conference, Edmonton, Canada.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
2006. Re-evaluating the role of Bleu in machine transla-
tion research. In Proceedings of the European Chapter of
the Association for Computational Linguistics, Trento, Italy.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of the Asso-
ciation for Computational Linguistics, pages 263?270, Ann
Arbor, Michigan.
Stephen Clark and James R. Curran. 2004. Parsing the wsj
using ccg and log-linear models. In Proceedings of the
Association for Computational Linguistics, pages 103?110,
Barcelona, Spain.
Stephen Clark. 2002. Supertagging for combinatory categorial
grammar. In Proceedings of the International Workshop on
Tree Adjoining Grammars, pages 19?24, Venice, Italy.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation. In
Proceedings of the Association for Computational Linguis-
tics, Prague, Czech Republic. (to appear).
Julia Hockenmaier and Mark Steedman. 2005. Ccgbank man-
ual. Technical Report MS-CIS-05-09, Department of Com-
puter and Information Science, University of Pennsylvania.
Julia Hockenmaier. 2006. Creating a ccgbank and a wide-
coverage ccg lexicon for german. In Proceedings of the In-
ternational Conference on Computational Linguistics and of
the Association for Computational Linguistics, Sydney, Aus-
tralia.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. A
syntax-directed translator with extended domain of locality.
In Proceedings of the Workshop on Computationally Hard
Problems and Joint Inference in Speech and Language Pro-
cessing, pages 1?8, New York City, New York. Association
for Computational Linguistics.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of the Human
Language Technology and North American Association for
Computational Linguistics Conference, pages 127?133, Ed-
monton, Canada. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Chris Callison-Burch, Marcello
Federico, Nicola Bertoldi, Richard Zens, Chris Dyer, Brooke
Cowan, Wade Shen, Christine Moran, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2006. Open source toolkit
for statistical machine translation. In Summer Workshop on
Language Engineering, John Hopkins University Center for
Language and Speech Processing.
Philipp Koehn. 2005. Europarl: A parallel corpus for statistical
machine translation. In MT Summit.
Shankar Kumar and William Byrne. 2003. A weighted finite
state transducer implementation of the alignment template
model for statistical machine translation. In Proceedings of
the Human Language Technology and North American As-
sociation for Computational Linguistics Conference, pages
63?70, Edmonton, Canada.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical machine translation with
syntactified target language phrases. In Proceedings of the
Conference on Empirical Methods in Natural Language Pro-
cessing, pages 44?52, Sydney, Australia.
Franz Josef Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the Associ-
ation for Computational Linguistics, pages 160?167, Sap-
poro, Japan.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed phrasal
SMT. In Proceedings of the Association for Computational
Linguistics, pages 271?279, Ann Arbor, Michigan.
Wade Shen, Richard Zens, Nicola Bertoldi, and Marcello Fed-
erico. 2006. The JHU workshop 2006 IWSLT system. In
Proceedings of the International Workshop on Spoken Lan-
guage Translation (IWSLT), pages 59?63, Kyoto, Japan.
Andrew Smith and Miles Osborne. 2006. Using gazetteers in
discriminative information extraction. In The Conference on
Natural Language Learning, New York City, USA.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005. Loga-
rithmic opinion pools for conditional random fields. In Pro-
ceedings of the Association for Computational Linguistics,
pages 18?25, Ann Arbor, Michigan.
Andreas Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proceedings of Spoken Language Process-
ing, pages 901?904.
Christoph Tillman. 2004. A unigram orientation model for
statistical machine translation. In Proceedings of the Hu-
man Language Technology and North American Association
for Computational Linguistics Conference, pages 101?104,
Boston, USA. Association for Computational Linguistics.
Ashish Venugopal and Stephan Vogel. 2005. Considerations
in MCE and MMI training for statistical machine transla-
tion. In Proceedings of the European Association for Ma-
chine Translation, Budapest, Hungary.
16
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 197?205,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
A Quantitative Analysis of Reordering Phenomena
Alexandra Birch Phil Blunsom Miles Osborne
a.c.birch-mayne@sms.ed.ac.uk pblunsom@inf.ed.ac.uk miles@inf.ed.ac.uk
University of Edinburgh
10 Crichton Street
Edinburgh, EH8 9AB, UK
Abstract
Reordering is a serious challenge in sta-
tistical machine translation. We propose
a method for analysing syntactic reorder-
ing in parallel corpora and apply it to un-
derstanding the differences in the perfor-
mance of SMT systems. Results at recent
large-scale evaluation campaigns show
that synchronous grammar-based statisti-
cal machine translation models produce
superior results for language pairs such as
Chinese to English. However, for language
pairs such as Arabic to English, phrase-
based approaches continue to be competi-
tive. Until now, our understanding of these
results has been limited to differences in
BLEU scores. Our analysis shows that cur-
rent state-of-the-art systems fail to capture
the majority of reorderings found in real
data.
1 Introduction
Reordering is a major challenge in statistical ma-
chine translation. Reordering involves permuting
the relative word order from source sentence to
translation in order to account for systematic dif-
ferences between languages. Correct word order is
important not only for the fluency of output, it also
affects word choice and the overall quality of the
translations.
In this paper we present an automatic method
for characterising syntactic reordering found in a
parallel corpus. This approach allows us to analyse
reorderings quantitatively, based on their number
and span, and qualitatively, based on their relation-
ship to the parse tree of one sentence. The methods
we introduce are generally applicable, only requir-
ing an aligned parallel corpus with a parse over the
source or the target side, and can be extended to
allow for more than one reference sentence and
derivations on both source and target sentences.
Using this method, we are able to compare the re-
ordering capabilities of two important translation
systems: a phrase-based model and a hierarchical
model.
Phrase-based models (Och and Ney, 2004;
Koehn et al, 2003) have been a major paradigm
in statistical machine translation in the last few
years, showing state-of-the-art performance for
many language pairs. They search all possible re-
orderings within a restricted window, and their
output is guided by the language model and a
lexicalised reordering model (Och et al, 2004),
both of which are local in scope. However, the
lack of structure in phrase-based models makes it
very difficult to model long distance movement of
words between languages.
Synchronous grammar models can encode
structural mappings between languages which al-
low complex, long distance reordering. Some
grammar-based models such as the hierarchical
model (Chiang, 2005) and the syntactified target
language phrases model (Marcu et al, 2006) have
shown better performance than phrase-based mod-
els on certain language pairs.
To date our understanding of the variation in re-
ordering performance between phrase-based and
synchronous grammar models has been limited to
relative BLEU scores. However, Callison-Burch et
al. (2006) showed that BLEU score alone is insuffi-
cient for comparing reordering as it only measures
a partial ordering on n-grams. There has been little
direct research on empirically evaluating reorder-
ing.
We evaluate the reordering characteristics of
these two paradigms on Chinese-English and
Arabic-English translation. Our main findings are
as follows: (1) Chinese-English parallel sentences
exhibit many medium and long-range reorderings,
but less short range ones than Arabic-English, (2)
phrase-based models account for short-range re-
orderings better than hierarchical models do, (3)
197
by contrast, hierarchical models clearly outper-
form phrase-based models when there is signif-
icant medium-range reordering, and (4) none of
these systems adequately deal with longer range
reordering.
Our analysis provides a deeper understand-
ing of why hierarchical models demonstrate bet-
ter performance for Chinese-English translation,
and also why phrase-based approaches do well at
Arabic-English.
We begin by reviewing related work in Sec-
tion 2. Section 3 describes our method for ex-
tracting and measuring reorderings in aligned and
parsed parallel corpora. We apply our techniques
to human aligned parallel treebank sentences in
Section 4, and to machine translation outputs in
Section 5.We summarise our findings in Section 6.
2 Related Work
There are few empirical studies of reordering be-
haviour in the statistical machine translation lit-
erature. Fox (2002) showed that many common
reorderings fall outside the scope of synchronous
grammars that only allow the reordering of child
nodes. This study was performed manually and
did not compare different language pairs or trans-
lation paradigms. There are some comparative
studies of the reordering restrictions that can be
imposed on the phrase-based or grammar-based
models (Zens and Ney, 2003; Wellington et al,
2006), however these do not look at the reordering
performance of the systems. Chiang et al (2005)
proposed a more fine-grained method of compar-
ing the output of two translation systems by us-
ing the frequency of POS sequences in the output.
This method is a first step towards a better under-
standing of comparative reordering performance,
but neglects the question of what kind of reorder-
ing is occurring in corpora and in translation out-
put.
Zollmann et al (2008) performed an empiri-
cal comparison of the BLEU score performance
of hierarchical models with phrase-based models.
They tried to ascertain which is the stronger model
under different reordering scenarios by varying
distortion limits the strength of language models.
They show that the hierarchical models do slightly
better for Chinese-English systems, but worse for
Arabic-English. However, there was no analysis of
the reorderings existing in their parallel corpora,
or on what kinds of reorderings were produced in
their output. We perform a focused evaluation of
these issues.
Birch et al (2008) proposed a method for ex-
tracting reorderings from aligned parallel sen-
tences.We extend this method in order to constrain
the reorderings to a derivation over the source sen-
tence where possible.
3 Measuring Reordering
Reordering is largely driven by syntactic differ-
ences between languages and can involve complex
rearrangements between nodes in synchronous
trees. Modeling reordering exactly would be
sparse and heterogeneous and thus we make an
important simplifying assumption in order for the
detection and extraction of reordering data to be
tractable and useful. We assume that reordering
is a binary process occurring between two blocks
that are adjacent in the source. We extend the
methods proposed by Birch et al (2008) to iden-
tify and measure reordering. Modeling reordering
as the inversion in order of two adjacent blocks is
similar to the approach taken by the Inverse Trans-
duction Model (ITG) (Wu, 1997), except that here
we are not limited to a binary tree. We also detect
and include non-syntactic reorderings as they con-
stitute a significant proportion of the reorderings.
Birch et al (2008) defined the extraction pro-
cess for a sentence pair that has been word aligned.
This method is simple, efficient and applicable to
all aligned sentence pairs. However, if we have ac-
cess to the syntax tree, we can more accurately
determine the groupings of embedded reorder-
ings, and we can also access interesting informa-
tion about the reordering such as the type of con-
stituents that get reordered. Figure 1 shows the
advantage of using syntax to guide the extraction
process. Embedded reorderings that are extracted
without syntax assume a right branching structure.
Reorderings that are extracted using the syntac-
tic extraction algorithm reflect the correct sentence
structure. We thus extend the algorithm to extract-
ing syntactic reorderings. We require that syntac-
tic reorderings consist of blocks of whole sibling
nodes in a syntactic tree over the source sentence.
In Figure 2 we can see a sentence pair with an
alignment and a parse tree over the source. We per-
form a depth first recursion through the tree, ex-
tracting the reorderings that occur between whole
sibling nodes. Initially a reordering is detected be-
tween the leaf nodes P and NN. The block growing
algorithm described in Birch et al (2008) is then
used to grow block A to include NT and NN, and
block B to include P and NR. The source and tar-
get spans of these nodes do not overlap the spans
198
Figure 1. An aligned sentence pair which shows two
different sets of reorderings for the case without and
with a syntax tree.
of any other nodes, and so the reordering is ac-
cepted. The same happens for the higher level re-
ordering where block A covers NP-TMP and PP-
DIR, and block B covers the VP. In cases where
the spans do overlap spans of nodes that are not
siblings, these reorderings are then extracted us-
ing the algorithm described in Birch et al (2008)
without constraining them to the parse tree. These
non-syntactic reorderings constitute about 10% of
the total reorderings and they are a particular chal-
lenge to models which can only handle isomorphic
structures.
RQuantity
The reordering extraction technique allows us to
analyse reorderings in corpora according to the
distribution of reordering widths and syntactic
types. In order to facilitate the comparison of dif-
ferent corpora, we combine statistics about in-
dividual reorderings into a sentence level metric
which is then averaged over a corpus. This met-
ric is defined using reordering widths over the tar-
get side to allow experiments with multiple lan-
guage pairs to be comparable when the common
language is the target.
We use the average RQuantity (Birch et al,
2008) as our measure of the amount of reordering
in a parallel corpus. It is defined as follows:
RQuantity =
?
r?R |rAt | + |rBt |
I
where R is the set of reorderings for a sentence,
I is the target sentence length, A and B are the
two blocks involved in the reordering, and |rAs |
is the size or span of block A on the target side.
RQuantity is thus the sum of the spans of all the
reordering blocks on the target side, normalised
$ %
$
%
Figure 2. A sentence pair from the test corpus, with its
alignment and parse tree. Two reorderings are shown
with two different dash styles.
by the length of the target sentence. The minimum
RQuantity for a sentence would be 0. The max-
imum RQuantity occurs where the order of the
sentence is completely inverted and the RQuantity
is
?I
i=2 i. See, for example, Figure 1 where the
RQuantity is 94 .
4 Analysis of Reordering in Parallel
Corpora
Characterising the reordering present in different
human generated parallel corpora is crucial to un-
derstanding the kinds of reordering wemust model
in our translations. We first need to extract reorder-
ings for which we need alignments and deriva-
tions. We could use automatically generated an-
notations, however these contain errors and could
be biased towards the models which created them.
The GALE project has provided gold standard
word alignments for Arabic-English (AR-EN) and
Chinese-English (CH-EN) sentences.1 A subset of
these sentences come from the Arabic and Chi-
nese treebanks, which provide gold standard parse
trees. The subsets of parallel data for which we
have both alignments and parse trees consist of
1see LDC corpus LDC2006E93 version GALE-Y1Q4
199
ll
l l
l l
l l l
l
l
0.0
0.2
0.4
0.6
0.8
1.0
Sentence Length Bin
RQu
antit
y
0?9 20?29 40?49 60?69 80?89 >=100
l CH.EN.RQuantityAR.EN.RQuantity
Figure 3. Sentence level measures of RQuantity for the
CH-EN and AR-EN corpora for different English sen-
tence lengths.
l l
l
l
l
l
l
l
l
l
0
500
1000
1500
2000
2500
Reordering Width
Num
ber 
of R
eord
ering
s
2 3 4 5 6 7?8 9?10 16?20
l CH?ENAR?EN
Figure 4. Comparison of reorderings of different widths
for the CH-EN and AR-EN corpora.
3,380 CH-EN sentences and 4,337 AR-EN sen-
tences.
Figure 3 shows that the different corpora have
very different reordering characteristics. The CH-
EN corpus displays about three times the amount
of reordering (RQuantity) than the AR-EN cor-
pus. For CH-EN, the RQuantity increases with
sentence length and for AR-EN, it remains con-
stant. This seems to indicate that for longer CH-
EN sentences there are larger reorderings, but this
is not the case for AR-EN. RQuantity is low for
very short sentences, which indicates that these
sentences are not representative of the reordering
characteristics of a corpus. The measures seem
to stabilise for sentences with lengths of over 20
words.
The average amount of reordering is interesting,
but it is also important to look at the distribution
of reorderings involved. Figure 4 shows the re-
orderings in the CH-EN and AR-EN corpora bro-
l
l
l
l
l
l
l
l
l
l0
5
10
15
20
25
30
Widths of Reorderings
% N
umb
er o
f Re
orde
rings
 for W
idth
2 3 4 5 6 7?8 9?10 16?20
l NPDNPCPNP.PN
Figure 5. The four most common syntactic types being
reordered forward in target plotted as % of total syntac-
tic reorderings against reordering width (CH-EN).
ken down by the total width of the source span
of the reorderings. The figure clearly shows how
different the two language pairs are in terms of
reordering widths. Compared to the CH-EN lan-
guage pair, the distribution of reorderings in AR-
EN has many more reorderings over short dis-
tances, but many fewer medium or long distance
reorderings. We define short, medium or long dis-
tance reorderings to mean that they have a reorder-
ing of width of between 2 to 4 words, 5 to 8 and
more than 8 words respectively.
Syntactic reorderings can reveal very rich
language-specific reordering behaviour. Figure 5
is an example of the kinds of data that can be used
to improve reordering models. In this graph we se-
lected the four syntactic types that were involved
in the largest number of reorderings. They cov-
ered the block that was moved forward in the tar-
get (block A). We can see that different syntactic
types display quite different behaviour at different
reordering widths and this could be important to
model.
Having now characterised the space of reorder-
ing actually found in parallel data, we now turn
to the question of how well our translation models
account for them. As both the translation models
investigated in this work do not use syntax, in the
following sections we focus on non-syntactic anal-
ysis.
5 Evaluating Reordering in Translation
We are interested in knowing how current trans-
lation models perform specifically with regard to
reordering. To evaluate this, we compare the re-
orderings in the parallel corpora with the reorder-
ings that exist in the translated sentences. We com-
200
None Low Medium High
Average RQuantity
CH-EN 0 0.39 0.82 1.51
AR-EN 0 0.10 0.25 0.57
Number of Sentences
CH-EN 105 367 367 367
AR-EN 293 379 379 379
Table 1. The RQuantity and the number of sentences
for each reordering test set.
pare two state-of-the-art models: the phrase-based
system Moses (Koehn et al, 2007) (with lexi-
calised reordering), and the hierarchical model Hi-
ero (Chiang, 2007). We use default settings for
both models: a distortion limit of seven for Moses,
and a maximum source span limit of 10 words for
Hiero. We trained both models on subsets of the
NIST 2008 data sets, consisting mainly of news
data, totalling 547,420 CH-EN and 1,069,658 AR-
EN sentence pairs. We used a trigram language
model on the entire English side (211M words)
of the NIST 2008 Chinese-English training cor-
pus. Minimum error rate training was performed
on the 2002 NIST test for CH-EN, and the 2004
NIST test set for AR-EN.
5.1 Reordering Test Corpus
In order to determine what effect reordering has
on translation, we extract a test corpus with spe-
cific reordering characteristics from the manually
aligned and parsed sentences described in Sec-
tion 4. To minimise the impact of sentence length,
we select sentences with target lengths from 20 to
39 words inclusive. In this range RQuantity is sta-
ble. From these sentences we first remove those
with no detected reorderings, and we then divide
up the remaining sentences into three sets of equal
sizes based on the RQuantity of each sentence. We
label these test sets: ?none?, ?low?, ?medium? and
?high?.
All test sentences have only one reference En-
glish sentence. MT evaluations using one refer-
ence cannot make strong claims about any partic-
ular test sentence, but are still valid when used to
compare large numbers of hypotheses.
Table 1 and Figure 6 show the reordering char-
acteristics of the test sets. As expected, we see
more reordering for Chinese-English than for Ara-
bic to English.
It is important to note that although we might
name a set ?low? or ?high?, this is only relative
to the other groups for the same language pair.
The ?high? AR-EN set, has a lower RQuantity
than the ?medium? CH-EN set. Figure 6 shows
0
50
100
150
200
250
Widths of Reorderings
Num
ber 
of R
eord
ering
s
2 3 4 5 6 7?8 9?10 16?20
LowMediumHigh
Figure 6. Number of reorderings in the CH-EN test set
plotted against the total width of the reorderings.
none low med high all
MOSESHIERO
14
16
18
20
22
Figure 7. BLEU scores for the different CH-EN reorder-
ing test sets and the combination of all the groups for
the two translation models.The 95% confidence levels
as measured by bootstrap resampling are shown for
each bar.
that the CH-EN reorderings in the higher RQuan-
tity groups have more and longer reorderings. The
AR-EN sets show similar differences in reordering
behaviour.
5.2 Performance on Test Sets
In this section we compare the translation output
for the phrase-based and the hierarchical system
for different reordering scenarios. We use the test
sets created in Section 5.1 to explicitly isolate the
effect reordering has on the performance of two
translation systems.
Figure 7 and Figure 8 show the BLEU score
results of the phrase-based model and the hierar-
chical model on the different reordering test sets.
The 95% confidence intervals as calculated by
bootstrap resampling (Koehn, 2004) are shown for
each of the results. We can see that the models
show quite different behaviour for the different
test sets and for the different language pairs. This
demonstrates that reordering greatly influences the
201
none low med high all
MOSESHIERO
16
18
20
22
24
26
Figure 8. BLEU scores for the different AR-EN reorder-
ing test sets and the combination of all the groups for
the two translation models. The 95% confidence lev-
els as measured by bootstrap resampling are shown for
each bar.
BLEU score performance of the systems.
In Figure 7 we see that the hierarchical model
performs considerably better than Moses on the
?medium? CH-EN set, although the confidence
interval for these results overlap somewhat. This
supports the claim that Hiero is better able to cap-
ture longer distance reorderings than Moses.
Hiero performs significantly worse than Moses
on the ?none? and ?low? sets for CH-EN, and
for all the AR-EN sets, other than ?none?. All
these sets have a relatively low amount of reorder-
ing, and in particular a low number of medium
and long distance reorderings. The phrase-based
model could be performing better because it
searches all possible permutations within a certain
window whereas the hierarchical model will only
permit reorderings for which there is lexical evi-
dence in the training corpus. Within a small win-
dow, this exhaustive search could discover the best
reorderings, but within a bigger window, the more
constrained search of the hierarchical model pro-
duces better results. It is interesting that Hiero is
not always the best choice for translation perfor-
mance, and depending on the amount of reorder-
ing and the distribution of reorderings, the simpler
phrase-based approach is better.
The fact that both models show equally poor
performance on the ?high? RQuantity test set sug-
gests that the hierarchical model has no advantage
over the phrase-based model when the reorder-
ings are long enough and frequent enough. Nei-
ther Moses nor Hiero can perform long distance
reorderings, due to the local constraints placed on
their search which allows performance to be lin-
ear with respect to sentence length. Increasing the
window in which these models are able to perform
reorderings does not necessarily improve perfor-
l
l l
l l
l l l0
20
40
60
80
100
120
140
Widths of Reorderings
Num
ber 
of R
eord
ering
s
2 3 4 5 6 7 8 >8
l NoneLowMediumHigh
Figure 9. Reorderings in the CH-EN MOSES transla-
tion of the reordering test set, plotted against the total
width of the reorderings.
mance, due to the number of hypotheses the mod-
els must discriminate amongst.
The performance of both systems on the ?high?
test set could be much worse than the BLEU score
would suggest. A long distance reordering that has
been missed, would only be penalised by BLEU
once at the join of the two blocks, even though it
might have a serious impact on the comprehension
of the translation. This flaw seriously limits the
conclusions that we can draw from BLEU score,
and motivates analysing translations specifically
for reordering as we do in this paper.
Reorderings in Translation
At best, BLEU can only partially reflect the re-
ordering performance of the systems. We therefore
perform an analysis of the distribution of reorder-
ings that are present in the systems? outputs, in or-
der to compare them with each other and with the
source-reference distribution.
For each hypothesis translation, we record
which source words and phrase pairs or rules were
used to produce which target words. From this we
create an alignment matrix from which reorder-
ings are extracted in the same manner as previ-
ously done for the manually aligned corpora.
Figure 9 shows the distribution of reorderings
that occur between the source sentence and the
translations from the phrase-based model. This
graph is interesting when compared with Figure 6,
which shows the reorderings that exist in the orig-
inal reference sentence pair. The two distribu-
tions are quite different. Firstly, as the models use
phrases which are treated as blocks, reorderings
which occur within a phrase are not recorded. This
reduces the number of shorter distance reorder-
ings in the distribution in Figure 6, as mainly short
202
l
l
l l
l
l
l l0
10
20
30
40
50
Widths of Reorderings
Num
ber 
of R
eord
ering
s
2 3 4 5 6 7 8 >8
l NoneLowMediumHigh
Figure 10. Reorderings in the CH-EN Hiero translation
of the reordering test set, plotted against the total width
of the reorderings.
phrases pairs are used in the hypothesis. However,
even taking reorderings within phrase pairs into
account, there are many fewer reorderings in the
translations than in the references, and there are
no long distance reorderings.
It is interesting that the phrase-based model is
able to capture the fact that reordering increases
with the RQuantity of the test set. Looking at the
equivalent data for the AR-EN language pair, a
similar pattern emerges: there are many fewer re-
orderings in the translations than in the references.
Figure 10 shows the reorderings from the output
of the hierarchical model. The results are very dif-
ferent to both the phrase-based model output (Fig-
ure 9) and to the original reference reordering dis-
tribution (Figure 6). There are fewer reorderings
here than even in the phrase-based output. How-
ever, the Hiero output has a slightly higher BLEU
score than the Moses output. The number of re-
orderings is clearly not the whole story. Part of the
reason why the output seems to have few reorder-
ings and yet scores well, is that the output of hier-
archical models does not lend itself to the analysis
that we have performed successfully on the ref-
erence or phrase-based translation sentence pairs.
This is because the output has a large number of
non-contiguous phrases which prevent the extrac-
tion of reorderings from within their span. Only
4.6% of phrase-based words were blocked off due
to non-contiguous phrases but 47.5% of the hier-
archical words were. This problem can be amelio-
rated with the detection and unaligning of words
which are obviously dependent on other words in
the non-contiguous phrase.
Even taking blocked off phrases into account,
however, the number of reorderings in the hierar-
l l
l
l
l
l
l
l
l0
100
200
300
400
500
600
Reordering Width
Num
ber 
of R
eord
ering
s
2 3 4 5 6 7?8 9?10 16?20
l Test.SetPhrase.BasedHierarchical
Figure 11. Number of reorderings in the original CH-
EN test set, compared to the reorderings retained by
the phrase-based and hierarchical models. The data is
shown relative to the length of the total source width of
the reordering.
chical output is still low, especially for the medium
and long distance reorderings, as compared to the
reference sentences. The hierarchical model?s re-
ordering behaviour is very different to human re-
ordering. Even if human translations are freer and
contain more reordering than is strictly necessary,
many important reorderings are surely being lost.
Targeted Automatic Evaluation
Comparing distributions of reorderings is inter-
esting, but it cannot approach the question of how
many reorderings the system performed correctly.
In this section we identify individual reorderings
in the source and reference sentences and detect
whether or not they have been reproduced in the
translation.
Each reordering in the original test set is ex-
tracted. Then the source-translation alignment is
inspected to determine whether the blocks in-
volved in the original reorderings are in the reverse
order in the translation. If so, we say that these re-
orderings have been retained from the reference to
the translation.
If a reordering has been translated by one phrase
pair, we assume that the reordering has been re-
tained, because the reordering could exist inside
the phrase. If the segmentation is slightly differ-
ent, but a reordering of the correct size occurred at
the right place, it is also considered to be retained.
Figure 11 shows that the hierarchical model
retains more reorderings of all widths than the
phrase-based system. Both systems retain few re-
orderings, with the phrase-based model missing
almost all the medium distance reorderings, and
both models failing on all the long distance re-
203
Correct Incorrect NA
Retained 61 4 10
Not Retained 32 31 12
Table 2. Correlation between retaining reordering and it
being correct - for humans and for system
orderings. This is possibly the most direct evi-
dence of reordering performance so far, and again
shows how Hiero has a slight advantage over the
phrase-based systemwith regard to reordering per-
formance.
Targeted Manual Analysis
The relationship between targeted evaluation
and the correct reordering of the translation still
needs to be established. The translation system can
compensate for not retaining a reordering by us-
ing different lexical items. To judge the relevance
of the targeted evaluation we need to perform a
manual evaluation. We present evaluators with the
reference and the translation sentences. We mark
the target ranges of the blocks that are involved
in the particular reordering we are analysing, and
ask the evaluator if the reordering in the translation
is correct, incorrect or not applicable. The not ap-
plicable case is chosen when the translated words
are so different from the reference that their order-
ing is irrelevant. There were three evaluators who
each judged 25 CH-EN reorderings which were re-
tained and 25 CH-EN reorderings which were not
retained by the Moses translation model.
The results in Table 2 show that the retained
reorderings are generally judged to be correct. If
the reordering is not retained, then the evaluators
divided their judgements evenly between the re-
ordering being correct or incorrect. It seems that
the fact that a reordering is not retained does in-
dicate that its ordering is more likely to be incor-
rect. We used Fleiss? Kappa to measure the cor-
relation between annotators. It expresses the ex-
tent to which the amount of agreement between
raters is greater than what would be expected if
all raters made their judgements randomly. In this
case Fleiss? kappa is 0.357 which is considered to
be a fair correlation.
6 Conclusion
In this paper we have introduced a general and
extensible automatic method for the quantitative
analyse of syntactic reordering phenomena in par-
allel corpora.
We have applied our method to a systematic
analysis of reordering both in the training corpus,
and in the output, of two state-of-the-art transla-
tion models. We show that the hierarchical model
performs better than the phrase-based model in sit-
uations where there are many medium distance re-
orderings. In addition, we find that the choice of
translation model must be guided by the type of re-
orderings in the language pair, as the phrase-based
model outperforms the hierarchical model when
there is a predominance of short distance reorder-
ings. However, neither model is able to capture the
reordering behaviour of the reference corpora ad-
equately. These result indicate that there is still
much research to be done if statistical machine
translation systems are to capture the full range of
reordering phenomena present in translation.
References
Alexandra Birch, Miles Osborne, and Philipp Koehn. 2008.
Predicting success in machine translation. In Proceedings
of the Empirical Methods in Natural Language Process-
ing.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
2006. Re-evaluating the role of Bleu in machine trans-
lation research. In Proceedings of the European Chapter
of the Association for Computational Linguistics, Trento,
Italy.
David Chiang, Adam Lopez, Nitin Madnani, Christof Monz,
Philip Resnik, and Michael Subotin. 2005. The Hiero
machine translation system: Extensions, evaluation, and
analysis. In Proceedings of the Human Language Tech-
nology Conference and Conference on Empirical Methods
in Natural Language Processing, pages 779?786, Vancou-
ver, Canada.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of the As-
sociation for Computational Linguistics, pages 263?270,
Ann Arbor, Michigan.
David Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics (to appear), 33(2).
Heidi J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages 304?
311, Philadelphia, USA.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings of the
Human Language Technology and North American Asso-
ciation for Computational Linguistics Conference, pages
127?133, Edmonton, Canada. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the Association for Computational Linguistics Companion
Demo and Poster Sessions, pages 177?180, Prague, Czech
Republic. Association for Computational Linguistics.
204
Philipp Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages 388?
395, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical machine translation with
syntactified target language phrases. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing, pages 44?52, Sydney, Australia.
Franz Josef Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation. Com-
putational Linguistics, 30(4):417?450.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop
Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Li-
bin Shen, David Smith, Katherine Eng, Viren Jain, Zhen
Jin, and Dragomir Radev. 2004. A smorgasbord of fea-
tures for statistical machine translation. In Proceedings of
Human Language Technology Conference and Conference
on Empirical Methods in Natural Language Processing,
pages 161?168, Boston, USA. Association for Computa-
tional Linguistics.
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the complex-
ity of translational equivalence. In Proceedings of the In-
ternational Conference on Computational Linguistics and
of the Association for Computational Linguistics, pages
977?984, Sydney, Australia.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Computa-
tional Linguistics, 23(3):377?403.
Richard Zens and Hermann Ney. 2003. A comparative study
on reordering constraints in statistical machine translation.
In Proceedings of the Association for Computational Lin-
guistics, pages 144?151, Sapporo, Japan.
Andreas Zollmann, Ashish Venugopal, Franz Och, and Jay
Ponte. 2008. A systematic comparison of phrase-based,
hierarchical and syntax-augmented statistical mt. In Pro-
ceedings of International Conference On Computational
Linguistics.
205
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 857?868,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Soft Dependency Constraints for Reordering
in Hierarchical Phrase-Based Translation
Yang Gao, Philipp Koehn and Alexandra Birch
School of Informatics
University of Edinburgh
Edinburgh, UK, EH8 9AB
yanggao1119@gmail.com, pkoehn@inf.ed.ac.uk, a.birch@ed.ac.uk
Abstract
Long-distance reordering remains one of the
biggest challenges facing machine translation.
We derive soft constraints from the source de-
pendency parsing to directly address the re-
ordering problem for the hierarchical phrase-
based model. Our approach significantly im-
proves Chinese?English machine translation
on a large-scale task by 0.84 BLEU points
on average. Moreover, when we switch the
tuning function from BLEU to the LRscore
which promotes reordering, we observe total
improvements of 1.21 BLEU, 1.30 LRscore
and 3.36 TER over the baseline. On aver-
age our approach improves reordering preci-
sion and recall by 6.9 and 0.3 absolute points,
respectively, and is found to be especially ef-
fective for long-distance reodering.
1 Introduction
Reordering, especially movement over longer dis-
tances, continues to be a hard problem in statistical
machine translation. It motivates much of the re-
cent work on tree-based translation models, such as
the hierarchical phrase-based model (Chiang, 2007)
which extends the phrase-based model (Koehn et al,
2003) by allowing the so-called hierarchical phrases
containing subphrases.
The hierarchical phrase-based model captures the
recursiveness of language without relying on syntac-
tic annotation, and promises better reordering than
the phrase-based model. However, Birch et al
(2009) find that although the hierarchical phrase-
based model outperforms the phrase-based model in
terms of medium-range reordering, it does equally
poorly in long-distance reordering due to constraints
to guarantee efficiency.
Syntax-based models that use phrase structure
constituent labels as non-terminals in their transfer
rules, exemplified by that of Galley et al (2004),
produce smarter and syntactically motivated re-
ordering. However, when working with off-the-shelf
tools for parsing and alignment, this approach may
impose harsh limits on rule extraction and requires
serious efforts of optimization (Wang et al, 2010).
An alternative approach is to augment the general
hierarchical phrase-based model with soft syntactic
constraints. Here, we derive three word-based, com-
plementary constraints from the source dependency
parsing, including:
? A dependency orientation feature, trained with
maximum entropy on the word-aligned par-
allel data, which directly models the head-
dependent orientation for source words;
? An integer-valued cohesion penalty that com-
plements the dependency orientation feature,
and fires when a word is not translated with its
head. It measures derivation well-formedness
and is used to indirectly help reordering;
? An auxiliary unaligned penalty feature that mit-
igates search error given the other two features.
We achieve significant improvements in terms of
the overall translation quality and reordering behav-
ior. To our knowledge we are the first to use the
source dependency parsing to target the reordering
problem for hierarchical phrase-based MT.
857
??   ?   ?  ??      ?    ??   ?   ??   ??    ??  .pobj dobj
prep
 Australia  is          with  North Korea    have       dipl. rels.  that      few           countries    one of    .     Aozhou    shi        yu     Beihan             you        bangjiao    de        shaoshu    guojia         zhiyi      .    
 Australia is one of the few countries that have diplomatic relations with North Korea. 
top
pobj
cpm nummodnn
attrpunct
rcmod
 
Figure 1: Example dependency parsing generated by the Stanford Parser. The Chinese source sentence and its English
translation come from (Chiang, 2007).
2 Three Soft Dependency Constraints
Our features are based on the source dependency
parsing, as shown in Figure 1. The basic unit of de-
pendency parsing is a triple consisting of the depen-
dent word, the head word and the dependency rela-
tion that connects them. For example, in Figure 1,
an arrow labelled prep goes from the word yu (En-
glish with) to the word you (English have), showing
that yu is a prepositional modifier of you.
We use the Stanford Parser1 to generate depen-
dency parsing, which automatically extracts de-
pendency relations from phrase structure parsing
(de Marneffe et al, 2006).
2.1 Dependency Orientation
Based on the assumption that constituents generally
move as a whole (Quirk et al, 2005), we decompose
the sentence reordering probability into the reorder-
ing probability for each aligned source word with re-
spect to its head, excluding the root word at the top
of the dependency hierarchy which does not have a
head word. Similarly, Hayashi et al (2010) also take
a word-based reordering approach for HPBMT, but
they model all possible pairwise orientation from
the source side as a general linear ordering prob-
lem (Tromble and Eisner, 2009).
To be more specific, we have a maximum entropy
orientation classifier that predicts the probability of
a source word being translated in a monotone or re-
versed manner with respect to its head. For example,
1http://nlp.stanford.edu/software/lex-parser.shtml
(a) (b)
pob poj pod pob poj pod
rob rob
roj roj
rod rod
roe roe
iheadidep idepihead
Figure 2: Word alignments to illustrate orientation clas-
sification. In (a), monotone (M); in (b), reversed (R).
given the alignment in Figure 2(a), with the align-
ment points (idep, jdep) for the source dependent
word and (ihead, jhead) for the source head word,
we define two orientation classes as:
c =
{
R if (jdep ? jhead)(idep ? ihead) < 0
M otherwise
(1)
When a source head or dependent word is aligned
to multiple target words, as shown in Figure 2(b),
we always take the first target word for orientation
classification.
The orientation classifier is trained on the large
word-aligned parallel corpus. Various features can
potentially be used, based on the source and target
context as well as syntactic and semantic analysis.
The orientation probability is evaluated in the fol-
lowing log-linear equation, where f is the source
context, d is the source dependency parsing, e? is
the target context produced so far, a? is the align-
ment produced so far and c is the orientation class:
858
Word p(M) p(R)
Aozhou 0.81 0.19
shi NA NA
yu 0.45 0.55
Beihan 0.88 0.12
you 0.12 0.88
bangjiao 0.83 0.17
de 0.58 0.42
shaoshu 0.30 0.70
guojia 0.19 0.81
zhiyi 0.85 0.15
. 1.00 0.00
Table 1: The dependency orientation probabilities for
words of the Figure 1 sentence, in both monotone and
reversed cases.
p(c|f, d, e?, a?) =
exp(
?N
n=1 ?nhn(f, d, e?, a?, c))?
c??{M,R} exp(
?N
n=1 ?nhn(f, d, e?, a?, c?))
(2)
Currently, we only use two kinds of features: (1)
the concatenation of the source dependent word with
the dependency relation and (2) the concatenation of
the source head word with the dependency relation.
So for the word yu (English with) in Figure 1, we
extract these features for orientation classification:
prep DEP yu and prep HEAD you.
We define the dependency orientation feature
score for a translation hypothesis as the sum of the
log orientation probabilities for each source word.
This score is used as one feature in the log-linear
formulation of the hierarchical phrase-based model.
Table 1 shows the dependency orientation proba-
bilities for all words in the Figure 1 sentence. Most
interestingly, the orientation probabilities for you
(English have) strongly support global reordering of
one of the few countries with the relative clause that
have diplomatic relations with North Korea. We find
that it is a general trend for long-distance reordering
to gain stronger support, since it is often correlated
with prominent reordering patterns (such as relative
clause and preposition) as well as lexical evidences
(such as ?... zhiyi? (English ?one of ...?)) for which
the reversed orientation takes up the majority of the
training cases.
Consider the following rules (both terminals and
nonterminals are coindexed):
X ? (yu1 Beihan2 you3 bangjiao4,
have3 dipl.4 rels.4 with1 North2 Korea2)
(3)
X ? (yu1 Beihan2 you3 bangjiao4,
with1 North2 Korea2 have3 dipl.4 rels.4)
(4)
According to Table 1, the hypothesis that applies
Rule 3 receives a probability of 0.55 for yu getting
reversed with its head you, as well as 0.88 and 0.83
for translating Beihan and bangjiao in a monotone
manner with respect to their heads. Rule 4 is associ-
ated with probabilities 0.45, 0.88 and 0.83 for mono-
tone translation of yu, Beihan and bangjiao. Thus
our dependency orientation feature is able to trace
the difference in ordering the PP with North Korea
(as underlined) and the VP have dipl. rels. down to
the orientation of the preposition yu (English with)
with respect to its head you (English have), and pro-
mote Rule 3 which has the right word order.
The word you (English have) cannot be scored
in Rules 3 or 4, since its head word zhiyi (English
one of) is not covered. In this case, we say that
the word you is unresolved. We carry an unre-
solved word along in the derivation process until we
reach a terminator hypothesis which translates the
head word. Then the resulting dependency orien-
tation score is added to the terminator hypothesis.
This means that the dependency orientation feature
is ?stateless?, i.e., hypotheses that cover the same
source span with the same orientation information
will receive the same feature score, regardless of the
derivation history. Therefore, Derivation 5 in the fol-
lowing will have the same dependency orientation
score as Derivation (Rule) 3, and Derivation 6 will
score the same as Derivation (Rule) 4.
5.1 X ? (yu1, with1)
5.2 X ? (Beihan1, North1 Korea1)
5.3 X ? (X1 X2, X1 X2)
5.4 X ? (X1 you2 bangjiao3,
have2 dipl.3 rels.3 X1)
(5)
859
6.1 X ? (Beihan1 you2, North1 Korea1 has2)
6.2 X ? (X1 bangjiao2, X1 dipl.2 rels.2)
6.3 X ? (yu1 X2, with1 X2)
(6)
2.2 Cohesion Penalty
When the dependency orientation for a word is
temporarily unavailable (?unresolved?), a cohesion
penalty fires. Cohesion penalty counts the total oc-
currences of unresolved words for a translation hy-
pothesis, which involve newly encountered unre-
solved words as well as old unresolved words car-
ried on from the derivation history. Therefore, the
cohesion penalty is ?stateful?, i.e., an unresolved
word is repeatedly penalized until it gets resolved.
Under this definition, the most cohesive derivation
translates the entire sentence with one rule, where
every word is locally resolved. The least cohe-
sive derivation translates each word individually and
glues word translations together. Consulting Fig-
ure 1, the cohesion penalty in Derivation 5 is 4, since
the word yu (English with) is unresolved twice (in
5.1 and 5.3), and both Beihan (English North Ko-
rea) and you (English have) are unresolved once (in
5.2 and 5.4, respectively); the cohesion penalty in
Derivation 6 is 5: 2 from Beihan (English North
Korea) (in 6.1 and 6.2) and 3 from you (English
have). As a result, Derivation 5 gets promoted,
which echoes with human intuition since Deriva-
tion 5 translates syntactic constituents. To sum
up, our cohesion penalty provides an integer-valued
measure of derivation well-formedness in the hierar-
chical phrase-based MT. Same as dependency orien-
tation, the cohesion penalty is not applicable to the
root word of the sentence.
We propose the cohesion penalty in order to fur-
ther improve reordering, especially in long-distance
cases, since a well-formed derivation at an earlier
stage makes it more likely to explore hierarchical
rules that perform more reliable reordering. In this
respect, the cohesion penalty can be seen as an aid
to the glue rule penalty and as an alternative to
constituency-based constraints.
Specifically, the glue rule penalty (Chiang, 2007)
promotes hierarchical rules. Hierarchical rules
whose lexical evidence helps resolve words locally
will also be favored by our cohesion penalty feature.
However, ignorant of the syntactic structure, the
glue rule penalty may penalize a reasonably cohe-
sive derivation such as Derivation 5 and at the same
time promote a less cohesive hierarchical transla-
tion, such as Derivation 6.
Compared with constituency constraints based on
the phrase structure, our cohesion penalty derived
from the binary dependency parsing has two differ-
ent characteristics.
First, our cohesion penalty is by nature more tol-
erant to some meaningful noncontituent translations.
For example, constituency constraints in (Chiang,
2005; Marton and Resnik, 2008; Chiang et al, 2009)
would penalize Rule 7 below which is useful for
German?English translation (Koehn et al, 2003),
and Rule 8 which can be applied to the Figure 1
sentence. Fuzzy constituency constraints can solve
this problem with a combination of product cate-
gories and slash categories (Chiang, 2010). Yet
our cohesion penalty by nature admits these trans-
lations as cohesive (with no extra cost from es and
Aozhou since both are locally resolved). Admittedly,
our current implementation of the cohesion penalty
is blind to some other meaningful nonconstituent
collocations, such as neighbouring siblings of a
common uncovered head (regulated as the ?floating
structure? in (Shen et al, 2008)). A concrete exam-
ple is Rule 9 which is useful for the Figure 1 sen-
tence. To address this problem, another feature can
be defined in the same manner to capture how each
head word is translated with its children.
X ? (es1 gibt2, there1 is2) (7)
X ? (Aozhou1 shi2, Australia1 is2) (8)
X ? (shaoshu1 guojia2, few1 countries2) (9)
Second, our cohesion penalty can be by na-
ture more discriminative. Compared with the
constituency constraints, the cohesion penalty is
integer-valued, and can be made sensitive to the
depth of each word in the dependency hierarchy (see
Section 2.4). Inspired by (Marton and Resnik, 2008;
Chiang et al, 2009), the cohesion penalty could
also be made sensitive to the dependency relation
of each word. However, this drastically increases
the number of features and requires a tuning algo-
rithm which scales better to high-dimensional model
spaces, such as MIRA (Watanabe et al, 2007; Chi-
ang et al, 2008).
860
pobj
 
?
pobj
dobjprep
 Aus t top
cpm
nummod nn
attr punct
rcmod
?
?? .
?
??
?
??
?? ??
??
 Aus r 
aliwh t 
aliwh r 
aliwh N 
aliwh o 
aliwh K 
Figure 3: Using 2 bins for the dependency parse tree of
the Figure 1 sentence.
2.3 Unaligned Penalty
The dependency orientation and cohesion penalty
cannot be applied to unaligned source words. This
may lead to search error, such as dropping (i.e., un-
aligning) key content words that are important for
lexical translation and reordering. The problem is
mitigated by an unaligned penalty applicable to all
words in the dependency hierarchy.
2.4 Grouping Words into Bins
Having defined dependency orientation, cohesion
penalty and unaligned penalty, we section the source
dependency tree uniformly by depth, group words at
different depths into bins and only add the feature
scores of a word into its respective bin. In this way
one feature is split into several sub-features and each
can be trained discriminatively by MERT.
There are two motivations for binning. The pri-
mary motivation is to distinguish long-distance re-
ordering which is still problematic for the hiero-
style model, since local reorderings generally op-
erate at low levels of the tree while high tree lev-
els tend to take more care of long-distance reorder-
ing. Parsing accuracy is another concern, yet its
impact on feature performance is intricate and our
MaxEnt-trained dependency orientation feature also
buffers against odd parsing. Using bins, we simply
let the tuning process decide how much to trust fea-
ture scores coming from different levels of parsing.
We experiment with 1, 2 and 3 bins. An example
of binning for the Figure 1 sentence can be found in
Figure 3. With 2 bins (hereafter ?bin-2?), words at
Depth 1 and 2 are grouped into Bin 1, and words at
Depth 3, 4, 5 are grouped into Bin 2. As a simple
approach, binning does not take into account how
the tree levels spread out.
3 Experiments
3.1 General Settings
We used a parallel training corpus with 2.1 mil-
lion Chinese?English sentence pairs, aligned by
GIZA++. The Chinese side was parsed by the Stan-
ford Parser. Then we extracted 33.8 million exam-
ples from the parsed Chinese side to discriminatively
train 1.1 million features (using the MegaM soft-
ware2) for dependency orientation classification.
We trained three 5-gram language models with
modified Kneser-Ney smoothing (Kneser and Ney,
1995): one on the English half of the parallel cor-
pus, one on the Xinhua part of the Gigaword corpus,
one on the AFP part, and interpolated them for best
fit to the tuning set (Schwenk and Koehn, 2008).
We used NIST MT06 evaluation data (1664 lines)
as our tuning set, and tested on NIST MT02 (878
lines), MT05 (1082 lines) and MT08 (1357 lines).
Our baseline system was the Moses implemen-
tation of the hierarchical phrase-based model with
standard settings (Hoang et al, 2009). When only
1 bin was used, 3 additional features were added to
the baseline, one each from the soft dependency con-
straints. When we used 2 or 3 bins, the additional
feature counts doubled or tripled. We preserved ter-
minal alignment alongside nonterminal alignment
during the rule extraction and output word align-
ments together with translated strings. Since the fea-
tures we currently define are based entirely on the
source side, we used preprocessing to speed up de-
coding of our feature-augmented model. All experi-
ments were tuned with MERT (Och, 2003).
3.2 Using BLEU as the Tuning Metric
As a standard practice, we first used BLEU (Pap-
ineni et al, 2002) as the objective function for tun-
ing. Table 2 shows the results of the baseline model
as well as our complete feature-augmented model
with different bin numbers. With the ?bin-2? setting,
we get substantial improvement of up to 1.03 BLEU
points (on MT02 data), and 0.84 BLEU points on
average. Using more than one bin (i.e., differentiat-
ing tree depths) is generally beneficial, although the
2http://www.umiacs.umd.edu/?hal/megam/index.html
861
Setting BLEU / LRscore / TERMT02 MT05 MT08 Average
baseline 34.01 / 41.85 / 68.93 32.23 / 40.50 / 68.15 28.09 / 37.17 / 66.82 31.44 / 39.84 / 67.97
bin-2 35.04 / 43.07 / 65.58 33.18 / 41.62 / 65.59 28.63 / 38.12 / 65.36 32.28 / 40.94 / 65.51
baseline-lr 34.23 / 42.06 / 68.08 32.28 / 40.61 / 67.61 27.99 / 37.27 / 66.98 31.50 / 39.98 / 67.56
bin-2-lr 35.42 / 43.25 / 64.82 33.44 / 41.80 / 64.88 29.10 / 38.38 / 64.14 32.65 / 41.14 / 64.61
Table 4: Results for the baseline model and the complete feature-augmented model with 2 bins (?bin-2?), using BLEU
and LRscore (?-lr?) as the tuning function. The BLEU scores of ?bin-2? and ?bin-2-lr? are significantly better than
baseline (p < 0.05), computed by paired bootstrap resampling (Koehn, 2004).
Setting BLEUMT02 MT05 MT08 Average
baseline 34.01 32.23 28.09 31.44
bin-1 34.20 32.13 28.41 31.58(+.14)
bin-2 35.04 33.18 28.63 32.28(+.84)
bin-3 34.35 32.79 28.37 31.84(+.40)
Table 2: Results of the baseline model as well as our
complete feature-augmented model with 1, 2 and 3 bins.
BLEU is the tuning function.
Setting BLEUMT02 MT05 MT08 Average
baseline 34.01 32.23 28.09 31.44
dep 34.26 32.58 28.07 31.64(+.20)
dep+coP 34.47 32.81 28.61 31.96(+.52)
dep+coP+unP 35.04 33.18 28.63 32.28(+.84)
Table 3: Contributions of the three soft dependency con-
straints, with the ?bin-2? setting
problem of overfitting sets in when we use 3 bins
(with slightly higher tuning BLEU, not shown here).
We also studied the effect of adding features in-
crementally onto the baseline with the ?bin-2? set-
ting, as shown in Table 3. On average, all three fea-
tures seem to have similar contributions.
3.3 Using LRscore as the Tuning Metric
Since our features are proposed to address the re-
ordering problem and BLEU is not sensitive enough
to reordering (especially in long-distance cases), we
have also tried tuning with a metric that highlights
reordering, i.e., the LRscore (Birch and Osborne,
2010). LRscore is a linear interpolation of a lexi-
cal metric and a reordering metric. We interpolated
BLEU (as the lexical metric) with the Kendall?s
tau permutation distance (as the reordering metric).
The Kendall?s tau permutation distance measures the
relative word order difference between the transla-
tion output and the reference(s) and is particularly
sensitive to long-distance reordering. Testing re-
sults in terms of BLEU, LRscore and TER (Snover
et al, 2006) are shown in Table 4. Tuned with
the LRscore, our feature-augmented model achieves
further average improvements (compare ?bin-2? and
?bin-2-lr?) of 0.20 LRscore as well as 0.37 BLEU
and 0.90 TER. Note that while the BLEU increase
can largely be seen as a projection of the LRscore
increase back into its lexical component, the consis-
tent TER drop confirms that our improvement is not
metric-specific3. Altogether the final improvement
is 1.21 BLEU, 1.30 LRscore and 3.36 TER on aver-
age over the baseline.
However, an important question is how our fea-
tures affect short, medium and long-distance re-
orderings. In the next section, we conduct quanti-
tative analysis on reordering precision and recall, as
well as qualitative analysis on translation examples.
4 Analysis
4.1 Precision and Recall of Reordering
The key to obtaining precision and recall for reorder-
ing is to investigate whether reorderings in the refer-
ences are reproduced in the translations. We calcu-
late precision as the number of reproduced reorder-
ings divided by the total number of reorderings in
the translation, and recall as the number of repro-
duced reorderings divided by the number of reorder-
3One of our reviewers points out that according to the in-
ductive learning theory, it is counter-intuitive to improve on
BLEU and TER if we optimize by the LRscore. Yet we do
observe some other papers reporting increased TER or other
metric scores when BLEU is used for tuning (Carpuat and Wu,
2007; Shen et al, 2008), suggesting that MT evaluation might
be too complicated to be characterized just with inductive learn-
ing. Similar results based on extensive experiments can also be
found in (Birch and Osborne, 2011).
862
Setting MT02 MT05 MT08 Average
baseline 37.0 35.3 35.6 36.0
bin-2 42.7 40.8 38.7 40.7 (+4.7)
baseline-lr 37.3 35.0 34.2 35.5 (-0.5)
bin-2-lr 44.1 42.0 42.5 42.9 (+6.9)
Table 5: Overall precision for the test sets.
Setting MT02 MT05 MT08 Average
baseline 37.5 36.2 33.2 35.6
bin-2 36.8 35.9 31.8 34.8 (-0.8)
baseline-lr 37.0 35.6 32.2 34.9 (-0.7)
bin-2-lr 37.7 36.7 33.2 35.9 (+0.3)
Table 6: Overall recall for the test sets.
ings in the reference. Then we average the precision
and recall over all four reference translations.
Details of measuring reproduced reordering can
be found in Birch et al (2008). An important dif-
ference in this work is in handling many-to-one and
one-to-many alignments, as we only retain the first
word alignment for any source or target word which
has multiple alignments. This is consistent with our
treatment in dependency orientation classification,
and results in more reorderings being extracted.
From Table 5 we can see that our features im-
prove precision by an average of 4.7 absolute points
when BLEU is used for tuning (?bin-2?). Switch-
ing from BLEU to the LRscore (?bin-2-lr?), we gain
2.2 points more and have a total improvement of 6.9
absolute points on average. This is a novel and im-
portant finding as we directly show that the quality
of reordering has been improved.
From Table 6, we observe a small but consistent
increase in recall with the ?bin-2-lr? setting, averag-
ing 0.3 absolute points. However, the drop of recall
with the ?bin-2? setting (by an average 0.8 points
from the baseline) is unexpected. It seems that when
applying our features alone, we are trading a small
drop in recall for a large gain in precision.
In Figure 4 we break down the precision and re-
call statistics in MT08 by the reordering width on
the source side. We find that our features con-
sistently help precision over all word ranges, with
more substantial improvement in the medium and
long word ranges. When recall is concerned, our
model does not help for short ranges of up to Width
4, but improves consistently for longer distance re-
2 3 4 5 6 7 8 9 10 11 12 13 14 15
baselinebin?2bin?2?lr
Reordering Widths
Prec
ision
0.0
0.1
0.2
0.3
0.4
0.5
0.6
2 3 4 5 6 7 8 9 10 11 12 13 14 15
baselinebin?2bin?2?lr
Reordering Widths
Reca
ll
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Figure 4: Precision and recall breakdown for the source-
side reordering width 2-15 for the NIST MT08 dataset.
orderings. Once again, it seems that the feature-
augmented model is able to benefit from tuning with
a metric that is more sensitive to reordering, as the
performance of ?bin-2-lr? is the best in all reorder-
ing statistics.
4.2 Translation Examples
We observe a number of outputs with improved
word order and more cohesive derivation, as the one
in Figure 5. The baseline translation is fragmented
and requires more glue rule applications. Specifi-
cally, it fails to translate the boxed area as a whole
into ?the relations between the palestinian national
authority (pna) and the european union (eu)?. The
key dependency orientation that controls the global
reordering is between the prepositional modifier dui
(English to) and its head word, the verb gandao (En-
glish feel). The baseline system translates dui (En-
glish to) as ?of the? and misorders the sentence. In
contrast, the feature-augmented model ?bin-2? cap-
863
 AustraliwshhhhhhhhNhhhoKeishhhhhhvAs
ethtwdhiepdhtspdhNhhhoKKeihhhhhtr .stwhhhhfchhhhhhhhhhhhKdt.dda
 AhhhhhhhnApdalhhhzwsysea
BebditsaseahgetbjhoAtwrmst 
e hhhhpsazAhhhhAeabshhhyslrA vdhhhhhlAea s
!fhhhhmdbetsrai
????    , ??? ? ?  ??  ??  ?? ?   ??    ?? ?  ?? ??  ??     . 
leaverhhpea shhhhhhhhhhj
"ddbhhhhhhhhietsi"eutsrahhj
pobp
baseline: [at the same time , abbas] [of the] [palestinian [national authority ( pna )]] [and] [is satisfied
with the [relations] [between [the european union ( eu )]]] [.]
bin-2: [at the same time , abbas] [expressed satisfaction with [the relations between the [palestinian
[national authority ( pna ) [and the european union ( eu )]]]] .]
Figure 5: Example translations from the NIST MT08 set, output by the baseline model and ?bin-2? model. The ?-lr?
version outputs are quite similar and not shown here. Translation outputs are in lower case.
tures the boxed area as a whole and uses Rule 10 to
perform the right global reordering.
X ? (dui1 X2 gandao3 manyi4 .5 ,
expressed3 satisfaction4 with1 X2 .5 ) (10)
5 Related Work
In recent years, there has been a growing body of re-
search on using dependency for statistical machine
translation. Some directly encodes dependency in
the translation model (Ding and Palmer, 2005; Quirk
et al, 2005; Xiong et al, 2007; Shen et al, 2008; Mi
and Liu, 2010), while others use dependency as a
soft constraint (Cherry, 2008; Bach et al, 2009a,b;
Chang et al, 2009). Among them, Shen et al (2008)
report that just filtering the phrase table by the so-
called well-formed target dependency structure does
not help, yet adding a target dependency language
model improves performance significantly. Our in-
tuitive interpretation is that the target dependency
language model capitalizes on two characteristics of
the dependency structure: it is based on words and it
directly connects head and child. Therefore, the tar-
get dependency language model makes good use of
the dependency representation as well as the target
side training data.
We follow the second line of research, and derive
three word-based soft constraints from the source
dependency parsing. Note that although we reuse
the word ?cohesion? to name one of the constraints,
our work is different from (Cherry, 2008; Bach
et al, 2009a,b) which have successfully defined an-
other cohesion constraint from the source depen-
dency structure, with the aim of improving reorder-
ing in phrase-based MT.
To take a glance, Cherry (2008) and Bach et al
(2009b) define cohesion as translating a source de-
pendency subtree contiguously into the target side
without interruption (span or subtree overlapping),
following Fox (2002). This span-based cohesion
constraint has a different criterion from our word-
based cohesion penalty and often leads to opposite
conclusions. Bach et al (2009a) also use cohesion to
correlate with the lexicalized reordering model (Till-
man, 2004; Koehn et al, 2005), whereas we define
an orthogonal dependency orientation feature to ex-
plicitly model head-dependent reordering.
The fundamental difference, however, is rooted
in the translation model. Their span-based cohe-
sion constraint is implemented as an ?interruption
check? to encourage finishing a subtree before trans-
lating something else. This check is very effective
for phrase-based decoding which searches over an
entire space within the distortion limit in order to
advance a hypothesis. In fact, it constrains reorder-
ing for the phrase-based model, as Cherry finds that
the cohesion constraint is used ?primarily to prevent
distortion? and to provide ?an intelligent estimate as
to when source order must be respected? (Cherry,
2008). However, since the hierarchical phrase-
based model already conducts principled reorder-
ing search with rules through the more constrained
chart-decoding, ill-formed derivations exhibit them-
selves more often as nonconstituent translation than
interrupted translation as defined in (Cherry, 2008;
Bach et al, 2009a,b) (They do have a non-empty in-
tersection, but neither subsumes the other). There-
864
fore, our cohesion penalty is better suited for the hi-
erarchical phrase-based model.
To discourage nonconstituent translation, Chiang
(2005) has proposed a constituency feature to exam-
ine whether a source rule span matches the source
constituent as defined by phrase structure parsing.
Finer-grained constituency constraints significantly
improve hierarchical phrase-based MT when ap-
plied on the source side (Marton and Resnik, 2008;
Chiang et al, 2009), or on the target side in a
more tolerant fashion (Zollmann and Venugopal,
2006). Using both source and target syntax, but
relaxing on rule extraction and substitution enables
HPBMT to produce more well-formed and syntac-
tically richer derivations (Chiang, 2010). Softening
constituency matching with latent syntactic distribu-
tions proves to be helpful (Huang et al, 2010). Com-
pared to constituency-based approaches, our cohe-
sion penalty based on the dependency structure nat-
urally supports constituent translations as well as
some nonconstituent translations, if not all of them
(as discussed in Section 2.2).
Our dependency orientation feature is similar to
the order model within dependency treelet trans-
lation (Quirk et al, 2005). Yet instead of a
head-relative position number for each modifier
word, we simply predict the head-dependent ori-
entation which is either monotone or reversed.
Our coarser-grained approach is more robust from
a machine learning perspective, yet still captures
prominent and long-distance reordering patterns ob-
served in Chinese?English (Wang et al, 2007),
German?English (Collins et al, 2005), Japanese?
English (Katz-Brown and Collins, 2008) and trans-
lation from English to a group of SOV lan-
guages (Xu et al, 2009). Not committed to spe-
cific language pairs, we learn orientation classifi-
cation from the word-aligned parallel data through
maximum entropy training as Zens and Ney (2006)
and Chang et al (2009) for phrase-based translation
and Xiong et al (2006) for the BTG model (Wu,
1996). While Chang et al (2009) also make use
of source dependency, their orientation classifica-
tion concerns two subsequent phrase pairs in the left-
to-right phrase-based decoding (as apposed to each
dependent word and its head) and is therefore less
linguistically-motivated.
6 Conclusion
We have derived three novel features from the source
dependency structure for hierarchical phrase-based
MT. They work as a whole to capitalize on two char-
acteristics of the dependency representation: it is di-
rectly based on words and it directly connects head
and child. The effectiveness of our approach has
been demonstrated by a final average improvement
of 1.21 BLEU, 1.30 LRscore and 3.36 TER. On av-
erage we improve reordering precision and recall by
6.9 and 0.3 absolute points, respectively, over the
baseline. Moreover, our approach is found to be es-
pecially effective for long-distance reodering.
As mentioned in Section 2.2, the cohesion penalty
can be extended to also account for how a head
word is translated with its children so that we are
not biased towards one form of cohesive noncon-
stituent translation. All our features can be made
sensitive to the dependency relations or even words.
This fine-grainedness is especially desirable when
we want to reward words for being unaligned or un-
resolved, such as punctuations and function words
in certain context. Word alignment quality is crucial
for the performance of our features as well as the
LRscore which uses word alignment to compute the
permutation distance. As an alternative to GIZA++,
we would like to experiment with syntactically in-
formed aligners that better handle function words
which often exhibit high alignment ambiguity due
to low cross-lingual correspondence.
Finally, since our soft dependency constraints
promote reordering without increasing model com-
plexity, further gains can be achieved when combin-
ing our approach with orthogonal studies to improve
the quantity and quality of hierarchical (reordering)
rules, such as relaxing hierarchical rule extraction
constraints (Setiawan and Resnik, 2010) and selec-
tively lexicalizing rules with function words (Seti-
awan et al, 2009).
Acknowledgments
We would like to thank Miles Osborne, Adam
Lopez, Barry Haddow, Hieu Hoang, Philip Williams
and Michael Auli in the Edinburgh SMT group
as well as Kevin Knight, David Chiang and An-
drew Dai for inspiring discussions. We appreci-
ate Pichuan Chang, Huihsin Tseng, Richard Zens,
Matthew Snover and Nguyen Bach for helping us
865
understand their brilliant work. Many thanks to the
anonymous reviewers for their insightful comments
and suggestions. This work was supported in part
by the EuroMatrixPlus project funded by the Euro-
pean Commission (7th Framework Programme) and
in part under the GALE program of the Defense
Advanced Research Projects Agency, Contract No.
HR0011-06-C-0022.
References
Bach, N., Gao, Q., and Vogel, S. (2009a). Source-
side dependency tree reordering models with sub-
tree movements and constraints. In Proceedings of
the Twelfth Machine Translation Summit (MTSummit-
XII), Ottawa, Canada. International Association for
Machine Translation.
Bach, N., Vogel, S., and Cherry, C. (2009b). Cohesive
constraints in a beam search phrase-based decoder. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Short Papers, pages 1?4, Boul-
der, Colorado.
Birch, A., Blunsom, P., and Osborne, M. (2009). A quan-
titative analysis of reordering phenomena. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, pages 197?205, Athens, Greece.
Birch, A. and Osborne, M. (2010). LRscore for evaluat-
ing lexical and reordering quality in MT. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 327?332, Upp-
sala, Sweden.
Birch, A. and Osborne, M. (2011). Reordering metrics
for mt. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Techologies, pages 1027?1035, Port-
land, Oregon, USA.
Birch, A., Osborne, M., and Koehn, P. (2008). Predict-
ing success in machine translation. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 745?754, Honolulu,
Hawaii.
Carpuat, M. and Wu, D. (2007). Improving statistical
machine translation using word sense disambiguation.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 61?72, Prague, Czech Republic.
Chang, P.-C., Tseng, H., Jurafsky, D., andManning, C. D.
(2009). Discriminative reordering with Chinese gram-
matical relations features. In Proceedings of the Third
Workshop on Syntax and Structure in Statistical Trans-
lation (SSST-3) at NAACL HLT 2009, pages 51?59,
Boulder, Colorado.
Cherry, C. (2008). Cohesive phrase-based decoding for
statistical machine translation. In Proceedings of ACL-
08: HLT, pages 72?80, Columbus, Ohio.
Chiang, D. (2005). A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, ACL ?05, pages 263?270, Strouds-
burg, PA, USA.
Chiang, D. (2007). Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
Chiang, D. (2010). Learning to translate with source and
target syntax. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 1443?1452, Uppsala, Sweden.
Chiang, D., Knight, K., and Wang, W. (2009). 11,001
new features for statistical machine translation. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
218?226, Boulder, Colorado.
Chiang, D., Marton, Y., and Resnik, P. (2008). Online
large-margin training of syntactic and structural trans-
lation features. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Process-
ing, pages 224?233, Honolulu, Hawaii.
Collins, M., Koehn, P., and Kucerova, I. (2005). Clause
restructuring for statistical machine translation. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
531?540, Ann Arbor, Michigan.
de Marneffe, M.-C., MacCartney, B., and Manning, C. D.
(2006). Generating typed dependency parses from
phrase structure parses. In Proceedings of LREC-06.
Ding, Y. and Palmer, M. (2005). Machine translation
using probabilistic synchronous dependency insertion
grammars. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 541?548, Ann Arbor, Michigan.
Fox, H. (2002). Phrasal cohesion and statistical machine
translation. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing,
pages 304?3111.
Galley, M., Hopkins, M., Knight, K., and Marcu, D.
(2004). What?s in a translation rule? In HLT-NAACL
2004: Main Proceedings, pages 273?280, Boston,
Massachusetts, USA.
866
Hayashi, K., Tsukada, H., Sudoh, K., Duh, K., and Ya-
mamoto, S. (2010). Hierarchical phrase-based ma-
chine translation with word-based reordering model.
In Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010), pages
439?446, Beijing, China.
Hoang, H., Koehn, P., and Lopez, A. (2009). A unified
framework for phrase-based, hierarchical, and syntax-
based statistical machine translation. In Proceedings
of the International Workshop on Spoken Language
Translation, pages 152?159, Tokyo, Japan.
Huang, Z., Cmejrek, M., and Zhou, B. (2010). Soft syn-
tactic constraints for hierarchical phrase-based trans-
lation using latent syntactic distributions. In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 138?147, Cam-
bridge, MA.
Katz-Brown, J. and Collins, M. (2008). Syntactic reorder-
ing in preprocessing for japanese-to-english transla-
tion: Mit system description for ntcir-7 patent transla-
tion task. In Proceedings of NTCIR-7 Workshop Meet-
ing, Tokyo, Japan.
Kneser, R. and Ney, H. (1995). Improved backing-off
for m-gram language modeling. In Proceedings of the
IEEE International Conference on Acoustics, Speech,
and Signal Processing, pages 181?184.
Koehn, P. (2004). Statistical significance tests for ma-
chine translation evaluation. In Lin, D. and Wu, D.,
editors, Proceedings of EMNLP 2004, pages 388?395,
Barcelona, Spain. Association for Computational Lin-
guistics.
Koehn, P., Axelrod, A., Birch, A., Callison-burch, C., Os-
borne, M., and Talbot, D. (2005). Edinburgh system
description for the 2005 iwslt speech translation eval-
uation. In Proceedings of IWSLT2005.
Koehn, P., Och, F. J., and Marcu, D. (2003). Statisti-
cal phrase based translation. In Proceedings of the
Joint Conference on Human Language Technologies
and the Annual Meeting of the North American Chap-
ter of the Association of Computational Linguistics
(HLT-NAACL).
Marton, Y. and Resnik, P. (2008). Soft syntactic con-
straints for hierarchical phrased-based translation. In
Proceedings of ACL-08: HLT, pages 1003?1011,
Columbus, Ohio.
Mi, H. and Liu, Q. (2010). Constituency to dependency
translation with forests. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1433?1442, Uppsala, Sweden.
Och, F. J. (2003). Minimum error rate training for statis-
tical machine translation. In Proceedings of the 41st
Annual Meeting of the Association of Computational
Linguistics (ACL).
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). Bleu: a method for automatic evaluation of
machine translation. In Proceedings of 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 311?318, Philadelphia, Pennsylvania,
USA. Association for Computational Linguistics.
Quirk, C., Menezes, A., and Cherry, C. (2005). De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 271?279, Ann Arbor, Michigan.
Schwenk, H. and Koehn, P. (2008). Large and diverse
language models for statistical machine translation. In
Proceedings of International Joint Conference on Nat-
ural Language Processing.
Setiawan, H., Kan, M. Y., Li, H., and Resnik, P. (2009).
Topological ordering of function words in hierarchical
phrase-based translation. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 324?332, Sun-
tec, Singapore.
Setiawan, H. and Resnik, P. (2010). Generalizing hierar-
chical phrase-based translation using rules with adja-
cent nonterminals. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 349?352, Los Angeles, California.
Shen, L., Xu, J., and Weischedel, R. (2008). A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Pro-
ceedings of ACL-08: HLT, pages 577?585, Columbus,
Ohio.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L., and
Makhoul, J. (2006). A study of translation edit rate
with targeted human annotation. In Proceedings of As-
sociation for Machine Translation in the Americas.
Tillman, C. (2004). A unigram orientation model for
statistical machine translation. In HLT-NAACL 2004:
Short Papers, pages 101?104, Boston, Massachusetts,
USA.
Tromble, R. and Eisner, J. (2009). Learning linear order-
ing problems for better translation. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 1007?1016, Singapore.
Wang, C., Collins, M., and Koehn, P. (2007). Chinese
syntactic reordering for statistical machine translation.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
867
Computational Natural Language Learning (EMNLP-
CoNLL), pages 737?745, Prague, Czech Republic.
Wang, W., May, J., Knight, K., and Marcu, D. (2010).
Re-structuring, re-labeling, and re-aligning for syntax-
based machine translation. Computational Linguistics,
36(2).
Watanabe, T., Suzuki, J., Tsukada, H., and Isozaki, H.
(2007). Online large-margin training for statistical ma-
chine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 764?773,
Prague, Czech Republic.
Wu, D. (1996). A polynomial-time algorithm for statis-
tical machine translation. In Proceedings of the 34th
Annual Meeting of the Association for Computational
Linguistics, pages 152?158, Santa Cruz, California,
USA.
Xiong, D., Liu, Q., and Lin, S. (2006). Maximum en-
tropy based phrase reordering model for statistical ma-
chine translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics, pages 521?528, Sydney, Australia.
Xiong, D., Liu, Q., and Lin, S. (2007). A dependency
treelet string correspondence model for statistical ma-
chine translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 40?47,
Prague, Czech Republic.
Xu, P., Kang, J., Ringgaard, M., and Och, F. (2009). Us-
ing a dependency parser to improve smt for subject-
object-verb languages. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 245?253, Boulder,
Colorado.
Zens, R. and Ney, H. (2006). Discriminative reordering
models for statistical machine translation. In Proceed-
ings on the Workshop on Statistical Machine Transla-
tion, pages 55?63, New York City.
Zollmann, A. and Venugopal, A. (2006). Syntax aug-
mented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation, pages 138?141, New York City.
868
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 126?134,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Generalizing a Strongly Lexicalized Parser using Unlabeled Data
Tejaswini Deoskar
1
, Christos Christodoulopoulos
2
, Alexandra Birch
1
, Mark Steedman
1
1
School of Informatics, University of Edinburgh, Edinburgh, EH8 9AB
2
University of Illinois, Urbana-Champaign, Urbana, IL 61801
{tdeoskar,abmayne,steedman}@inf.ed.ac.uk, christod@illinois.edu
Abstract
Statistical parsers trained on labeled data
suffer from sparsity, both grammatical and
lexical. For parsers based on strongly
lexicalized grammar formalisms (such as
CCG, which has complex lexical cate-
gories but simple combinatory rules), the
problem of sparsity can be isolated to
the lexicon. In this paper, we show that
semi-supervised Viterbi-EM can be used
to extend the lexicon of a generative CCG
parser. By learning complex lexical entries
for low-frequency and unseen words from
unlabeled data, we obtain improvements
over our supervised model for both in-
domain (WSJ) and out-of-domain (ques-
tions and Wikipedia) data. Our learnt
lexicons when used with a discriminative
parser such as C&C also significantly im-
prove its performance on unseen words.
1 Introduction
An important open problem in natural language
parsing is to generalize supervised parsers, which
are trained on hand-labeled data, using unlabeled
data. The problem arises because further hand-
labeled data in the amounts necessary to signif-
icantly improve supervised parsers are very un-
likely to be made available. Generalization is also
necessary in order to achieve good performance on
parsing in textual domains other than the domain
of the available labeled data. For example, parsers
trained on Wall Street Journal (WSJ) data suffer a
fall in accuracy on other domains (Gildea, 2001).
In this paper, we use self-training to generalize
the lexicon of a Combinatory Categorial Gram-
mar (CCG) (Steedman, 2000) parser. CCG is a
strongly lexicalized formalism, in which every
word is associated with a syntactic category (sim-
ilar to an elementary syntactic structure) indicat-
ing its subcategorization potential. Lexical en-
tries are fine-grained and expressive, and contain
a large amount of language-specific grammatical
information. For parsers based on strongly lexical-
ized formalisms, the problem of grammar general-
ization can be cast largely as a problem of lexical
extension.
The present paper focuses on learning lexi-
cal categories for words that are unseen or low-
frequency in labeled data, from unlabeled data.
Since lexical categories in a strongly lexicalized
formalism are complex, fine-grained (and far more
numerous than simple part-of-speech tags), they
are relatively sparse in labeled data. Despite per-
forming at state-of-the-art levels, a major source
of error made by CCG parsers is related to unseen
and low-frequency words (Hockenmaier, 2003;
Clark and Curran, 2007; Thomforde and Steed-
man, 2011). The unseen words for which we learn
categories are surprisingly commonplace words of
English; examples are conquered, apprehended,
subdivided, scoring, denotes, hunted, obsessed,
residing, migrated (Wikipedia). Correctly learn-
ing to parse the predicate-argument structures as-
sociated with such words (expressed as lexical cat-
egories in the case of CCG), is important for open-
domain parsing, not only for CCG but indeed for
any parser.
We show that a simple self-training method,
Viterbi-EM (Neal and Hinton, 1998) when used
to enhance the lexicon of a strongly-lexicalized
parser can be an effective strategy for self-training
and domain-adaptation. Our learnt lexicons im-
prove on the lexical category accuracy of two su-
pervised CCG parsers (Hockenmaier (2003) and
the Clark and Curran (2007) parser, C&C) on
within-domain (WSJ) and out-of-domain test sets
(a question corpus and a Wikipedia corpus).
In most prior work, when EM was initialized
based on labeled data, its performance did not im-
prove over the supervised model (Merialdo, 1994;
126
Charniak, 1993). We found that in order for per-
formance to improve, unlabeled data should be
used only for parameters which are not well cov-
ered by the labeled data, while those that are well
covered should remain fixed.
In an additional contribution, we compare two
strategies for treating unseen words (a smoothing-
based, and a part-of-speech back-off method) and
find that a smoothing-based strategy for treat-
ing unseen words is more effective for semi-
supervised learning than part-of-speech back-off.
2 Combinatory Categorial Grammar
Combinatory Categorial Grammar (CCG) (Steed-
man, 2000) is a strongly lexicalized grammar
formalism, in which the lexicon contains all
language-specific grammatical information. The
lexical entry of a word consists of a syntactic cat-
egory which expresses the subcategorization po-
tential of the word, and a semantic interpretation
which defines the compositional semantics (Lewis
and Steedman, 2013). A small number of combi-
natory rules are used to combine constituents, and
it is straightforward to map syntactic categories to
a logical form for semantic interpretation.
For statistical CCG parsers, the lexicon is learnt
from labeled data, and is subject to sparsity due
to the fine-grained nature of the categories. Fig-
ure 1 illustrates this with a simple CCG deriva-
tion. In this sentence, bake is used as a ditransi-
tive verb and is assigned the ditransitive category
S\NP/NP/NP . This category defines the verb syn-
tactically as mapping three NP arguments to a sen-
tence S , and semantically as a ternary relation be-
tween its three arguments, thus providing a com-
plete analysis of the sentence.
[
NNP
John ] [
VBD
baked ] [
NNP
Mary] [
DT
a ] [
NN
cake]
NP S\NP/NP/NP NP NP/N N
> >
S\NP/NP NP
>
S\NP
<
S
?John baked Mary a cake?
Figure 1: Example CCG derivation
For a CCG parser to obtain the correct deriva-
tion above, its lexicon must include the ditransitive
category S\NP/NP/NP for the verb bake. It is not
sufficient to have simply seen the verb in another
context (say a transitive context like ?John baked a
cake?, which is a more common context). This is
in contrast to standard treebank parsers where the
verbal category is simply VBD (past tense verb)
and a ditransitive analysis of the sentence is not
ruled out as a result of the lexical category.
In addition to sparsity related to open-class
words like verbs as in the above example, there are
also missing categories in labeled data for closed-
class words like question words, due to the small
number of questions in the Penn Treebank. In gen-
eral, lexical sparsity for a statistical CCG parser
can be broken down into three types: (i) where a
word is unseen in training data but is present in
test data, (ii) where a word is seen in the train-
ing data but not with the category type required
in the test data (but the category type is seen with
other words) and (iii) where a word bears a cate-
gory type required in the test data but the category
type is completely unseen in the training data.
In this paper, we deal with the first two kinds.
The third kind is more prevalent when the size
of labeled data is comparatively small (although,
even in the case of the English WSJ CCG tree-
bank, there are several attested category types that
are entirely missing from the lexicon, Clark et al.,
2004). We make the assumption here that all cat-
egory types in the language have been seen in the
labeled data. In principle new category types may
be introduced independently without affecting our
semi-supervised process (for instance, manually,
or via a method that predicts new category types
from those seen in labeled data).
3 Related Work
Previous attempts at harnessing unlabeled data to
improve supervised CCG models using methods
like self-training or co-training have been unsat-
isfactory (Steedman et al., 2003, 43-44). Steed-
man et al. (2003) experimented with self-training
a generative CCG parser, and co-training a genera-
tive parser with an HMM-based supertagger. Co-
training (but not self-training) improved the results
of the parser when the seed labeled data was small.
When the seed data was large (the full treebank),
i.e., the supervised baseline was high, co-training
and self-training both failed to improve the parser.
More recently, Honnibal et al. (2009) improved
the performance of the C&C parser on a domain-
adaptation task (adaptation to Wikipedia text) us-
ing self-training. Instead of self-training the pars-
ing model, they re-train the supertagging model,
which in turn affects parsing accuracy. They
obtained an improvement of 1.09% (dependency
127
score) on supertagger accuracy on Wikipedia (al-
though performance on WSJ text dropped) but did
not attempt to re-train the parsing model.
An orthogonal approach for extending a CCG
lexicon using unlabeled data is that of Thomforde
and Steedman (2011), in which a CCG category for
an unknown word is derived from partial parses
of sentences with just that one word unknown.
The method is capable of inducing unseen cate-
gories types (the third kind of sparsity mentioned
in ?2.1), but due to algorithmic and efficiency is-
sues, it did not achieve the broad-coverage needed
for grammar generalisation of a high-end parser. It
is more relevant for low-resource languages which
do not have substantial labeled data and category
type discovery is important.
Some notable positive results for non-CCG
parsers are McClosky et al. (2006) who use a
parser-reranker combination. Koo et al. (2008)
and Suzuki et al. (2009) use unsupervised word-
clusters as features in a dependency parser to get
lexical dependencies. This has some notional sim-
ilarity to categories, since, like categories, clus-
ters are less fine-grained than words but more fine-
grained than POS-tags.
4 Supervised Parser
The CCG parser used in this paper is a re-
implementation of the generative parser of Hock-
enmaier and Steedman (2002) and Hockenmaier
(2003)
1
, except for the treatment of unseen and
low-frequency words.
We use a model (the LexCat model in Hock-
enmaier (2003)) that conditions the generation of
constituents in the parse tree on the lexical cate-
gory of the head word of the constituent, but not on
the head word itself. While fully-lexicalized mod-
els that condition on words (and thus model word-
to-word dependencies) are more accurate than un-
lexicalized ones like the LexCat model, we use
an unlexicalized model
2
for two reasons: first,
1
These generative models are similar to the Collins? head-
based models (Collins, 1997), where for every node, a head is
generated first, and then a sister conditioned on the head. De-
tails of the models are in Hockenmaier and Steedman (2002)
and Hockenmaier 2003:pg 166.
2
A terminological clarification: unlexicalized here refers
to the model, in the sense that head-word information is
not used for rule-expansion. The formalism itself (CCG)
is referred to as strongly-lexicalized, as used in the title of
the paper. Formalisms like CCG and LTAG are consid-
ered strongly-lexicalized since linguistic knowledge (func-
tions mapping words to syntactic structures/semantic inter-
pretations) is included in the lexicon.
our lexicon smoothing procedure (described in the
next section) introduces new words and new cat-
egories for words into the lexicon. Lexical cate-
gories are added to the lexicon for seen and un-
seen words, but no new category types are intro-
duced. Since the LexCat model conditions rule ex-
pansions on lexical categories, but not on words, it
is still able to produce parses for sentences with
new words. In contrast, a fully lexicalized model
would need all components of the grammar to be
smoothed, a task that is far from trivial due to the
resulting explosion in grammar size (and one that
we leave for future work).
Second, although lexicalized models perform
better on in-domain WSJ data (the LexCat model
has an accuracy of 87.9% on Section 23, as op-
posed to 91.03% for the head-lexicalized model
in Hockenmaier (2003) and 91.9% for the C&C
parser), our parser is more accurate on a question
corpus, with a lexical category accuracy of 82.3%,
as opposed to 71.6% and 78.6% for the C&C and
Hockenmaier (2003) respectively.
4.1 Handling rare and unseen words
Existing CCG parsers (Hockenmaier (2003) and
Clark and Curran (2007)) back-off rare and unseen
words to their POS tag. The POS-backoff strategy
is essentially a pipeline approach, where words
are first tagged with coarse tags (POS tags) and
finer tags (CCG categories) are later assigned, by
the parser (Hockenmaier, 2003) or the supertag-
ger (Clark and Curran, 2007). As POS-taggers
are much more accurate than parsers, this strat-
egy has given good performance in general for
CCG parsers, but it has the disadvantage that POS-
tagging errors are propagated. The parser can
never recover from a tagging error, a problem that
is serious for words in the Zipfian tail, where these
words might also be unseen for the POS tagger
and hence more likely to be tagged incorrectly.
This issue is in fact more generally relevant than
for CCG parsers alone?the dependence of parsers
on POS-taggers was cited as one of the problems
in domain-adaptation of parsers in the NAACL-
2012 shared task on parsing the web (Petrov and
McDonald, 2012). Lease and Charniak (2005)
obtained an improvement in the accuracy of the
Charniak (2000) parser on a biomedical domain
simply by training a new POS tagger model.
In the following section, we describe an alter-
native smoothing-based approach to handling un-
128
seen and rare words. This method is less sen-
sitive to POS tagging errors, as described below.
In this approach, in a pre-processing step prior
to parsing, categories are introduced into the lex-
icon for unseen and rare words from the data to
be parsed. Some probability mass is taken from
seen words/categories and given to unseen word
and category pairs. Thus, at parse time, no word is
unseen for the parser.
4.1.1 Smoothing
In our approach, we introduce lexical entries for
words from the unlabeled corpus that are unseen
in the labeled data, and also add categories to ex-
isting entries for rarely seen words. The most gen-
eral case of this would be to assign all known cat-
egories to a word. However, doing this reduces
the lexical category accuracy.
3
A second option,
chosen here, is to limit the number of categories
assigned to the word by using some information
about the word (for instance, its part-of-speech).
Based on the part-of-speech of an unseen word in
the unlabeled or test corpus, we add an entry to the
lexicon of the word with the top n categories that
have been seen with that part-of-speech in the la-
beled data. Each new entry of (w, cat), where w
is a word and cat is a CCG category, is associated
with a count c(w, cat), obtained as described be-
low. Once all (w, cat) entries are added to the lex-
icon along with their counts, a probability model
P (w|cat) is calculated over the entire lexicon.
Our smoothing method is based on a method
used in Deoskar (2008) for smoothing a PCFG
lexicon. Eq. 1 and 2 apply it to CCG entries for
unseen and rare words. In the first step, an out-
of-the-box POS tagger is used to tag the unlabeled
or test corpus (we use the C&C tagger). Counts
of words and POS-tags c
corpus
(w, T ) are obtained
from the tagged corpus. For the CCG lexicon, we
ultimately need a count for a word w and a CCG
category cat. To get this count, we split the count
of a word and POS-tag amongst all categories seen
with that tag in the supervised data in the same
ratio as the ratio of the categories in the super-
vised data. In Eq. 1, this ratio is c
tb
(cat
T
)/c
tb
(T )
where c
tb
(cat
T
) is the treebank count of a cate-
gory cat
T
seen with a POS-tag T , and c
tb
(T ) is the
marginal count of the tag T in the treebank. This
3
For instance, we find that assigning all categories to un-
seen verbs gives a lexical category accuracy of 52.25 %, as
opposed to an accuracy of 65.4% by using top 15 categories,
which gave us the best results, as reported later in Table 3.
ratio makes a more frequent category type more
likely than a rarer one for an unseen word. For ex-
ample, for unseen verbs, it would make the transi-
tive category more likely than a ditransitive one
(since transitives are more frequent than ditran-
sitives). There is an underlying assumption here
that relative frequencies of categories and POS-
tags in the labeled data are maintained in the un-
labeled data, which in fact can be thought of as
a prior while estimating from unlabeled data (De-
oskar et al., 2012).
c
corpus
(w, cat) =
c
tb
(cat
T
)
c
tb
(T )
? c
corpus
(w, T ) (1)
Additionally, for seen but low-frequency words,
we make use of the existing entry in the lexicon.
Thus in a second step, we interpolate the count
c
corpus
(w, cat) of a word and category with the
supervised count of the same c
tb
(w, cat) (if it ex-
ists) to give the final smoothed count of a word and
category c
smooth
(w, cat) (Eq. 2).
c
smooth
(w, cat) = ? ? c
tb
(w, cat) +
(1? ?) ? c
corpus
(w, cat)
(2)
When this smoothed lexicon is used with a
parser, POS-backoff is not necessary since all
needed words are now in the lexicon. Lexical en-
tries for words in the parse are determined not by
the POS-tag from a tagger, but directly by the pars-
ing model, thus making the parse less susceptible
to tagging errors.
5 Semi-supervised Learning
We use Viterbi-EM (Neal and Hinton, 1998) as
the self-training method. Viterbi-EM is an alter-
native to EM where instead of using the model
parameters to find a true posterior from unlabeled
data, a posterior based on the single maximum-
probability (Viterbi) parse is used. Viterbi-EM
has been used in various NLP tasks before and
often performs better than classic EM (Cohen
and Smith, 2010; Goldwater and Johnson, 2005;
Spitkovsky et al., 2010). In practice, a given pars-
ing model is used to obtain Viterbi parses of un-
labeled sentences. The Viterbi parses are then
treated as training data for a new model. This pro-
cess is iterated until convergence.
Since we are interested in learning the lexi-
con, we only consider lexical counts from Viterbi
parses of the unlabeled sentences. Other parame-
ters of the model are held at their supervised val-
ues. We conducted some experiments where we
129
self-trained all components of the parsing model,
which is the usual case of self-training. We ob-
tained negative results similar to Steedman et al.
(2003), where self-training reduced the perfor-
mance of the parsing model. We do not report
them here. Thus, using unlabeled data only to es-
timate parameters that are badly estimated from
labeled data (lexical entries in CCG, due to lexi-
cal sparsity) results in improvements, in contrast
to prior work with semi-supervised EM.
As is common in semi-supervised settings, we
treated the count of each lexical event as the
weighted count of that event in the labeled data
(treebank)
4
and the count from the Viterbi-parses
of unlabeled data. Here we follow Bacchiani et al.
(2006) and McClosky et al. (2006) who show that
count merging is more effective than model inter-
polation.
We placed an additional constraint on the con-
tribution that the unlabeled data makes to the semi-
supervised model?we only use counts (from un-
labeled data) of lexical events that are rarely
seen/unseen in the labeled data. Our reasoning
was that many lexical entries are estimated accu-
rately from the treebank (for example, those re-
lated to function words and other high-frequency
words) and estimation from unlabeled data might
hurt them. We thus had a cut-off frequency (of
words in labeled data) above which we did not
allow the unlabeled counts to affect the semi-
supervised model. In practise, our experiments
turned out to be fairly insensitive to the value of
this parameter, on evaluations over rare or un-
seen verbs. However, overall accuracy would drop
slightly if this cut-off was increased. We experi-
mented with cut-offs of 5, 10 and 15, and found
that the most conservative value (of 5) gave the
best results on in-domain WSJ experiments, and a
higher value of 10 gave the best results for out-of-
domain experiments.
We also conducted some limited experiments
with classical semi-supervised EM, with similar
settings of weighting labeled counts, and using un-
labeled counts only for rare/unseen events. Since
it is a much more computationally expensive pro-
cedure, and most of the results did not come close
to the results of Viterbi-EM, we did not pursue it.
4
The labeled count is weighted in order to scale up the la-
beled data which is usually smaller in size than the unlabeled
data, to avoid swamping the labeled counts with much larger
unlabeled counts.
5.1 Data
Labeled: Sec. 02-21 of CCGbank (Hockenmaier
and Steedman, 2007). In one experiment, we used
Sec. 02-21 minus 1575 sentences that were held
out to simulate test data containing unseen verbs?
see ?6.2 for details.
Unlabeled: For in-domain experiments, we used
sentences from the unlabeled WSJ portion of the
ACL/DCI corpus (LDC93T1, 1993), and the WSJ
portion of the ANC corpus (Reppen et al., 2005),
limited to sentences containing 20 words or less,
creating datasets of approximately 10, 20 and 40
million words each. Additionally, we have a
dataset of 140 million words ? 40M WSJ words
plus an additional 100M from the New York
Times.
For domain-adaptation experiments, we use
two different datasets. The first one consists
of question-sentences ? 1328 unlabeled ques-
tions, obtained by removing the manual annota-
tion of the question corpus from Rimell and Clark
(2008). The second out-of-domain dataset con-
sists of Wikipedia data, approximately 40 million
words in size, with sentence length < 20 words.
5.2 Experimental setup
We ran our semi-supervised method using our
parser with a smoothed lexicon (from ?4.1.1) as
the initial model, on unlabeled data of different
sizes/domains. For comparison, we also ran ex-
periments using a POS-backed off parser (the orig-
inal Hockenmaier and Steedman (2002) LexCat
model) as the initial model. Viterbi-EM converged
at 4-5 iterations. We then parsed various test sets
using the semi-supervised lexicons thus obtained.
In all experiments, the labeled data was scaled to
match the size of the unlabeled data. Thus, the
scaling factor of labeled data was 10 for unlabeled
data of 10M words, 20 for 20M words, etc.
5.3 Evaluation
We focused our evaluations on unseen and low-
frequency verbs, since verbs are the most impor-
tant open-class lexical entries and the most am-
biguous to learn from unlabeled data (approx. 600
categories, versus 150 for nouns). We report lexi-
cal category accuracy in parses produced using our
semi-supervised lexicon, since it is a direct mea-
sure of the effect of the lexicon.
5
We discuss four
5
Dependency recovery accuracy is also used to evaluate
performance of CCG parsers and is correlated with lexical
130
All words All Verbs Unseen
Verbs
SUP 87.76 78.10 52.54
SEMISUP 88.14 78.46 **57.28
SUP
bkoff
87.91 76.08 54.14
SEMISUP
bkoff
87.79 75.68 54.60
Table 1: Lexical category accuracy on TEST-4SEC
**: p < 0.004, McNemar test
experiments below. The first two are on in-domain
(WSJ) data. The last two are on out-of-domain
data ? a question corpus and a Wikipedia corpus.
6 Results
6.1 In-domain: WSJ unseen verbs
Our first testset consists of a concatenation of 4
sections of CCGbank (01, 22, 24, 23), a total of
7417 sentences, to form a testset called TEST-
4SEC. We use all these sections in order to get
a reasonable token count of unseen verbs, which
was not possible with Sec. 23 alone.
Table 1 shows the performance of the smoothed
supervised model (SUP) and the semi-supervised
model (SEMISUP) on this testset. There is a sig-
nificant improvement in performance on unseen
verbs, showing that the semi-supervised model
learns good entries for unseen verbs over and
above the smoothed entry in the supervised lexi-
con. This results in an improvement in the over-
all lexical category accuracy of the parser on all
words, and all verbs.
We also performed semi-supervised training us-
ing a supervised model that treated unseen words
with a POS-backoff strategy SUP
bkoff
. We used
the same settings of cut-off and the same scal-
ing of labeled counts as before. The supervised
backed-off model performs somewhat better than
the supervised smoothed model. However, it did
not improve as much as the smoothed one from
unlabeled data. Additionally, the overall accuracy
of SEMISUP
bkoff
fell below the supervised level,
in contrast to the smoothed model, where overall
numbers improved. This could indicate that the
accuracy of a POS tagger on unseen words, es-
pecially verbs, may be an important bottleneck in
semi-supervised learning.
Low-frequency verbs We also obtain improve-
ments on verbs that are seen but with a low fre-
quency in the labeled data (Table 2). We divided
category accuracy, but a dependency evaluation is more rele-
vant when comparing performance with parsers in other for-
malisms and does not have much utility here.
Freq. Bin 1-5 6-10 11-20
SUP 64.13 75.19 77.6
SEMISUP 66.72 76.21 79.8
Table 2: Seen but rare verbs, TEST-4SEC
verbs occurring in TEST-4SEC into different bins
according to their occurrence frequency in the la-
beled data (bins of frequency 1-5, 6-10 and 11-20).
Semi-supervised training improves over the super-
vised baseline for all bins of low-frequency verbs.
Note that our cut-off frequency for using unlabeled
data is 5, but there are improvements in the 6-10
and 11-20 bins as well, suggesting that learning
better categories for rare words (below the cut-off)
impacts the accuracy of words above the cut-off as
well, by affecting the rest of the parse positively.
6.2 In-domain : heldout unseen verbs
The previous section showed significant improve-
ment in learning categories for verbs that are un-
seen in the training sections of CCGbank. How-
ever, these verbs are in the Zipfian tail, and for this
reason have fairly low occurrence frequencies in
the unlabeled corpus. In order to estimate whether
our method will give further improvements in the
lexical categories for these verbs, we would need
unlabeled data of a much larger size. We there-
fore designed an experimental scenario in which
we would be able to get high counts of unseen
verbs from a similar size of unlabeled data. We
first made a list of N verbs from the treebank and
then extracted all sentences containing them (ei-
ther as verbs or otherwise) from CCGbank training
sections. These sentences form a testset of 1575
sentences, called TEST-HOV (for held out verbs).
The verbs in the list were chosen based on occur-
rence frequency f in the treebank, choosing all
verbs that occurred with a frequency of f = 11.
This number gave us a large enough set and a
good type/token ratio to reliably evaluate and ana-
lyze our semi-supervised models?112 verb types,
with 1115 token occurrences
6
. Since these verbs
are actually mid-frequency verbs in the supervised
data, they have a correspondingly large occurrence
frequency in the unlabeled data, occurring much
more often than true unseen verbs. Thus, the un-
labeled data size is effectively magnified?as far
as these verbs are concerned, the unlabeled data is
approximately 11 times larger than it actually is.
Table 3 shows lexical category accuracy on
6
Selecting a different but close value of f such as f = 10
or f = 12 would have also served this purpose.
131
All Words All Verbs Unseen
Verbs
SUP 87.26 74.55 65.49
SEMISUP 87.78 75.30 *** 70.43
SUP
bkoff
87.58 73.06 67.25
SEMISUP
bkoff
87.52 72.89 68.05
Table 3: Lexical category accuracy in TEST-HOV.
***p<0.0001, McNemar test
55
60
65
70
0 10 20 40 140
Size of Unlabelled Data (in millions of words)
Lexic
al Ca
tegor
y Acc
uracy
 for U
nsee
n Ver
bs
Test:HOVTest:4Sec
Figure 2: Increasing accuracy on unseen verbs
with increasing amounts of unlabeled data.
this testset. The baseline accuracy of the parser
on these verbs is much higher than that on the
truly unseen verbs.
7
The semi-supervised model
(SEMISUP) improves over the supervised model
SUP very significantly on these unseen verbs. We
also see an overall improvement on all verbs (seen
and unseen) in the test data, and in the over-
all lexical category accuracy as well. Again, the
backed-off model does not improve as much as
the smoothed model, and moreover, overall per-
formance falls below the supervised level.
Figure 2 shows the effect of different sizes of
unlabeled data on accuracy of unseen verbs for
the two testsets TEST-HOV and TEST-4SEC . Im-
provements are monotonic with increasing unla-
beled data sizes, up to 40M words. The additional
100M words of NYT also improve the models but
to a lesser degree, possibly due to the difference in
domain. The graphs indicate that the method will
lead to more improvements as more unlabeled data
(especially WSJ data) is added.
7
This could be because verbs in the Zipfian tail have more
idiosyncratic subcategorization patterns than mid-frequency
verbs, and thus are harder for a parser. Another reason is that
they may have been seen as nouns or other parts of speech,
leading to greater ambiguity in their case.
QUESTIONS WIKIPEDIA
All wh All Unseen
words words words words
SUP 82.36 61.77 84.31 79.5
SEMISUP *83.21 63.22 *85.6 80.25
Table 4: Out-of-domain: Questions and
Wikipedia, *p<0.05, McNemar test
6.2.1 Out-of-Domain
Questions The question corpus is not strictly a
different domain (since questions form a differ-
ent kind of construction rather than a different do-
main), but it is an interesting case of adaptation
for several reasons: WSJ parsers perform poorly
on questions due to the small number of questions
in the Penn Treebank/CCGbank. Secondly, unsu-
pervised adaptation to questions has not been at-
tempted before for CCG (Rimell and Clark (2008)
did supervised adaptation of their supertagger).
The supervised model SUP already performs
at state-of-the-art on this corpus, on both overall
scores and on wh(question)-words alone. C&C
and Hockenmaier (2003) get 71.6 and 78.6% over-
all accuracies respectively, and only 33.6 and 50.7
on wh-words alone. To our original unlabeled
WSJ data (40M words), we add 1328 unlabeled
question-sentences from Rimell and Clark, 2008,
scaled by ten, so that each is counted ten times. We
then evaluated on a testset containing questions
(500 question sentences, from Rimell and Clark
(2008)). The overall lexical category accuracy on
this testset improves significantly as a result of the
semi-supervised learning (Table 4). The accuracy
on the question words alone (who, what, where,
when, which, how, whose, whom) also improves
numerically, but by a small amount (the number
of tokens that improve are only 7). This could be
an effect of the small size of the testset (500 sen-
tences, i.e. 500 wh-words).
Wikipedia We obtain statistically significant im-
provements in overall scores over a testset consist-
ing of Wikipedia sentences hand-annotated with
CCG categories (from Honnibal et al. (2009)) (Ta-
ble 4). We also obtained improvements in lexical
category accuracy on unseen words, and on un-
seen verbs alone (not shown), but could not prove
significance. This testset contains only 200 sen-
tences, and counts for unseen words are too small
for significance tests, although there are numeric
improvements. However, the overall improvement
is statistically significantly, showing that adapting
the lexicon alone is effective for a new domain.
132
6.3 Using semi-supervised lexicons with the
C&C parser
To show that the learnt lexical entries may be use-
ful to parsers other than our own, we incorpo-
rate our semi-supervised lexical entries into the
C&C parser to see if it benefits performance. We
do this in a naive manner, as a proof of concept,
making no attempt to optimize the performance
of the C&C parser (since we do not have access
to its internal workings). We take all entries of
unseen words from our best semi-supervised lex-
icon (word, category and count) and add them to
the dictionary of the C&C supertagger (tagdict).
The C&C is a discriminative, lexicalized model
that is more accurate than an unlexicalized model.
Even so, the lexical entries that we learn improve
the C&C parsers performance over and above its
back-off strategy for unseen words. Table 5 shows
the results on WSJ data TEST-4SEC and TEST-
HOV. There were numeric improvements on the
TEST-4SEC test set as shown in Table 5
8
. We ob-
tain significance on the TEST-HOV testset which
has a larger number of tokens of unseen verbs and
entries that were learnt from effectively larger un-
labeled data. We tested two cases: when these
verbs were seen for the POS tagger used to tag
the test data, and when they were unseen for the
POS tagger, and found statistically significant im-
provement for the case when the verbs were un-
seen for the POS tagger
9
, indicating sensitivity to
POS-tagger errors.
6.4 Entropy and KL-divergence
We also evaluated the quality of the semi-
supervised lexical entries by measuring the over-
all entropy and the average Kullback-Leibler (KL)
divergence of the learnt entries of unseen verbs
from entries in the gold testset. The gold entry
for each verb from the TEST-HOV testset was ob-
tained from the heldout gold treebank trees. Su-
pervised (smoothed) and semi-supervised entries
were obtained from the respective lexicons. These
metrics use the conditional probability of a cate-
gory given a word, which is not a factor in the
generative model (which considers probabilities of
8
There were also improvements on the question and
Wikipedia testsets (not shown) (8 and 6 tokens each) but the
size of these testsets is too small for significance.
9
Note that for this testset TEST-HOV, the numbers are the
supertagger?s accuracy, and not the parser?s. We were only
able to retrain the supertagger on training data with TEST-
HOV sentences heldout, but could not retrain the parser, de-
spite consultation with the authors.
TEST-4SEC TEST-HOV
POS-seen POS-unseen
(590) (1134) (1134)
C&C 62.03 (366) 76.71 (870) 72.39 (821)
C&C
(enhanced) 63.89 (377) 77.34 (877) *73.98 (839)
Table 5: TEST-4SEC: Lexical category accuracy of
C&C parser on unseen verbs. Numbers in brackets
are the number of tokens.*p<0.05, McNemar test
words given categories), but provide a good mea-
sure of how close the learnt lexicons are to the gold
lexicon. We find that the average KL divergence
reduces from 2.17 for the baseline supervised en-
tries to 1.40 for the semi-supervised entries. The
overall entropy for unseen verb distributions also
goes down from 2.23 (supervised) to 1.37 (semi-
supervised), showing that semi-supervised distri-
butions are more peaked, and bringing them closer
to the true entropy of the gold distribution (0.93).
7 Conclusions
We have shown that it is possible to learn CCG lex-
ical entries for unseen and low-frequency words
from unlabeled data. When restricted to learning
only lexical entries, Viterbi-EM improved the per-
formance of the supervised parser (both in-domain
and out-of-domain). Updating all parameters of
the parsing model resulted in a decrease in the ac-
curacy of the parser. We showed that the entries
we learnt with an unlexicalized model were accu-
rate enough to also be useful to a highly-accurate
lexicalized parser. It is likely that a lexicalized
parser will provide even better lexical entries. The
lexical entries continued to improve with increas-
ing size of unlabeled data. For the out-of-domain
testsets, we obtained statistically significant over-
all improvements, but we were hampered by the
small sizes of the testsets in evaluating unseen/wh
words.
In future work, we would like to add unseen but
predicted category types to the initial lexicon using
an independent method, and then apply the same
semi-supervised learning to words of these types.
Acknowledgements
We thank Mike Lewis, Shay Cohen and the three
anonymous EACL reviewers for helpful com-
ments. This work was supported by the ERC Ad-
vanced Fellowship 249520 GRAMPLUS.
133
References
Michiel Bacchiani, Michael Riley, Brian Roark, and Richard
Sproat. 2006. MAP adaptation of stochastic grammars.
Computer Speech and Language, 20(1):41?68.
Eugene Charniak. 1993. Statistical Language Learning. MIT
Press.
Stephen Clark and James R. Curran. 2007. Wide-Coverage
Efficient Statistical Parsing with CCG and Log-Linear
Models. Computational Linguistics, 33(4):493?552.
Stephen Clark, Mark Steedman, and James Curran. 2004.
Object-extraction and question-parsing using CCG. In
Proceedings of EMNLP 2004.
Shay Cohen and Noah Smith. 2010. Viterbi Training for
PCFGs: Hardness Results and Competitiveness of Uni-
form Initialization. In Proceedings of ACL 2010.
Michael Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings of the 35th ACL.
Tejaswini Deoskar. 2008. Re-estimation of Lexical Param-
eters for Treebank PCFGs. In Proceedings of COLING
2008.
Tejaswini Deoskar, Markos Mylonakis, and Khalil Sima?an.
2012. Learning Structural Dependencies of Words in the
Zipfian Tail. Journal of Logic and Computation.
Daniel Gildea. 2001. Corpus Variation and Parser Perfor-
mance. In Proceedings of EMNLP 2001.
Sharon Goldwater and Mark Johnson. 2005. Bias in learning
syllable structure. In Proceedings of CoNLL05.
Julia Hockenmaier. 2003. Data and Models for Statistical
Parsing with Combinatory Categorial Grammar. Ph.D.
thesis, School of Informatics, University of Edinburgh.
Julia Hockenmaier and Mark Steedman. 2002. Generative
Models for Statistical Parsing with Combinatory Catego-
rial Grammar. In ACL40.
Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A
Corpus of CCG Derivations and Dependency Structures
Extracted from the Penn Treebank. Computational Lin-
guistics, 33:355?396.
Matthew Honnibal, Joel Nothman, and James R. Curran.
2009. Evaluating a Statistial CCG Parser on Wikipedia.
In Proceedings of the 2009 Workshop on the People?s Web
Meets NLP, ACL-IJCNLP.
Terry Koo, Xavier Carreras, and Michael Collins. 2008. Sim-
ple Semi-supervised Dependency Parsing. In Proceedings
of ACL-08: HLT , pages 595?603. Association for Com-
putational Linguistics, Columbus, Ohio.
LDC93T1. 1993. LDC93T1. Linguistic Data Consortium,
Philadelphia.
Matthew Lease and Eugene Charniak. 2005. Parsing Biomed-
ical Literature. In R. Dale, K.-F. Wong, J. Su, and
O. Kwong, eds., Proceedings of the 2nd International
Joint Conference on Natural Language Processing (IJC-
NLP?05), vol. 3651 of Lecture Notes in Computer Science,
pages 58 ? 69. Springer-Verlag, Jeju Island, Korea.
Mike Lewis and Mark Steedman. 2013. Combined Distribu-
tional and Logical Semantics. Transactions of the Associ-
ation for Computational Linguistics.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective Self-Training for Parsing. In Proceedings
of HLT-NAACL 2006.
Bernard Merialdo. 1994. Tagging English Text with a Prob-
abilistic Model. Computational Linguistics, 20(2):155?
171.
Radford M. Neal and Geoffrey E. Hinton. 1998. A view of
the EM algorithm that justifies incremental, sparse, and
other variants. In Learning and Graphical Models, pages
355 ? 368. Kluwer Academic Publishers.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In First Work-
shop on Syntactic Analysis of Non-Canonical Language
(SANCL) Workshop at NAACL 2012.
Randi Reppen, Nancy Ide, and Keith Suderman. 2005.
LDC2005T35, American National Corpus (ANC) Second
Release. Linguistic Data Consortium, Philadelphia.
Laura Rimell and Stephen Clark. 2008. Adapting a
Lexicalized-Grammar Parser to Contrasting Domains. In
Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP-08).
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky, and
Christopher D. Manning. 2010. Viterbi Training Improves
Unsupervised Dependency Parsing. In Proceedings of
CoNLL-2010.
Mark Steedman. 2000. The Syntactic Process. MIT
Press/Bradford Books.
Mark Steedman, Steven Baker, Jeremiah Crim, Stephen
Clark, Julia Hockenmaier, Rebecca Hwa, Miles Osbornn,
Paul Ruhlen, and Anoop Sarkar. 2003. Semi-Supervised
Training for Statistical Parsing. Tech. rep., CLSP WS-02.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael
Collins. 2009. An Empirical Study of Semi-supervised
Structured Conditional Models for Dependency Parsing.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 551?
560. Association for Computational Linguistics, Singa-
pore.
Emily Thomforde and Mark Steedman. 2011. Semi-
supervised CCG Lexicon Extension. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing, Edinburgh UK.
134
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1027?1035,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Reordering Metrics for MT
Alexandra Birch Miles Osborne
a.birch@ed.ac.uk miles@inf.ed.ac.uk
University of Edinburgh
10 Crichton Street
Edinburgh, EH8 9AB, UK
Abstract
One of the major challenges facing statistical
machine translation is how to model differ-
ences in word order between languages. Al-
though a great deal of research has focussed
on this problem, progress is hampered by the
lack of reliable metrics. Most current metrics
are based on matching lexical items in the
translation and the reference, and their abil-
ity to measure the quality of word order has
not been demonstrated. This paper presents
a novel metric, the LRscore, which explic-
itly measures the quality of word order by
using permutation distance metrics. We show
that the metric is more consistent with human
judgements than other metrics, including the
BLEU score. We also show that the LRscore
can successfully be used as the objective func-
tion when training translation model parame-
ters. Training with the LRscore leads to output
which is preferred by humans. Moreover, the
translations incur no penalty in terms of BLEU
scores.
1 Introduction
Research in machine translation has focused broadly
on two main goals, improving word choice and im-
proving word order in translation output. Current
machine translation metrics rely upon indirect meth-
ods for measuring the quality of the word order, and
their ability to capture the quality of word order is
poor (Birch et al, 2010).
There are currently two main approaches to eval-
uating reordering. The first is exemplified by the
BLEU score (Papineni et al, 2002), which counts
the number of matching n-grams between the refer-
ence and the hypothesis. Word order is captured by
the proportion of longer n-grams which match. This
method does not consider the position of match-
ing words, and only captures ordering differences
if there is an exact match between the words in the
translation and the reference. Another approach is
taken by two other commonly used metrics, ME-
TEOR (Banerjee and Lavie, 2005) and TER (Snover
et al, 2006). They both search for an alignment be-
tween the translation and the reference, and from
this they calculate a penalty based on the number
of differences in order between the two sentences.
When block moves are allowed the search space is
very large, and matching stems and synonyms in-
troduces errors. Importantly, none of these metrics
capture the distance by which words are out of order.
Also, they conflate reordering performance with the
quality of the lexical items in the translation, making
it difficult to tease apart the impact of changes. More
sophisticated metrics, such as the RTE metric (Pado?
et al, 2009), use higher level syntactic or semantic
analysis to determine the grammaticality of the out-
put. These approaches require annotation and can be
very slow to run. For most research, shallow metrics
are more appropriate.
We introduce a novel shallow metric, the Lexical
Reordering Score (LRscore), which explicitly mea-
sures the quality of word order in machine trans-
lations and interpolates it with a lexical metric.
This results in a simple, decomposable metric which
makes it easy for researchers to pinpoint the effect
of their changes. In this paper we show that the
LRscore is more consistent with human judgements
1027
than other metrics for five out of eight different lan-
guage pairs. We also apply the LRscore during Mini-
mum Error Rate Training (MERT) to see whether in-
formation on reordering allows the translation model
to produce better reorderings. We show that hu-
mans prefer the output of systems trained with the
LRscore 52.5% as compared to 43.9% when train-
ing with the BLEU score. Furthermore, training with
the LRscore does not result in lower BLEU scores.
The rest of the paper proceeds as follows. Sec-
tion 2 describes the reordering and lexical metrics
that are used and how they are combined. Section 3
presents the experiments on consistency with human
judgements and describes how to train the language
independent parameter of the LRscore. Section 4 re-
ports the results of the experiments on MERT. Fi-
nally we discuss related work and conclude.
2 The LRscore
In this section we present the LRscore which mea-
sures reordering using permutation distance metrics.
These reordering metrics have been demonstrated to
correlate strongly with human judgements of word
order quality (Birch et al, 2010). The LRscore com-
bines the reordering metrics with lexical metrics to
provide a complete metric for evaluating machine
translations.
2.1 Reordering metrics
The relative ordering of words in the source and tar-
get sentences is encoded in alignments. We can in-
terpret algnments as permutations which allows us
to apply research into metrics for ordered encodings
to measuring and evaluating reorderings. We use dis-
tance metrics over permutations to evaluate reorder-
ing performance. Figure 1 shows three permutations.
Each position represents a source word and each
value indicates the relative positions of the aligned
target words. In Figure 1 (a) represents the identity
permutation, which would result from a monotone
alignment, (b) represents a small reordering consist-
ing of two words whose orders are inverted, and (c)
represents a large reordering where the two halves
of the sentence are inverted in the target.
A translation can potentially have many valid
word orderings. However, we can be reasonably cer-
tain that the ordering of the reference sentence must
be acceptable. We therefore compare the ordering
(a) (1 2 3 4 5 6 7 8 9 10)
(b) (1 2 3 4 ?6 ?5 ?7 8 9 10)
(c) (6 7 8 9 10 ?1 2 3 4 5)
Figure 1. Three permutations: (a) monotone (b) with a
small reordering and (b) with a large reordering. Bullet
points highlight non-sequential neighbours.
of a translation with that of the reference sentence.
Where multiple references exist, we select the clos-
est, i.e. the one that gives the best score. The un-
derlying assumption is that most reasonable word
orderings should be fairly similar to the reference,
which is a necessary assumption for all automatic
machine translation metrics.
Permutations encode one-one relations, whereas
alignments contain null alignments and one-many,
many-one and many-many relations. We make some
simplifying assumptions to allow us to work with
permutations. Source words aligned to null are as-
signed the target word position immediately after
the target word position of the previous source word.
Where multiple source words are aligned to the same
target word or phrase, a many-to-one relation, the
target ordering is assumed to be monotone. When
one source word is aligned to multiple target words,
a one-to-many relation, the source word is assumed
to be aligned to the first target word. These simplifi-
cations are chosen so as to reduce the alignment to a
bijective relationship without introducing any extra-
neous reorderings, i.e. they encode a basic monotone
ordering assumption.
We choose permutation distance metrics which
are sensitive to the number of words that are out
of order, as humans are assumed to be sensitive to
the number of words that are out of order in a sen-
tence. The two permutations we refer to, pi and ?,
are the source-reference permutation and the source-
translation permutation. The metrics are normalised
so that 0 means that the permutations are completely
inverted, and 1 means that they are identical. We re-
port these scores as percentages.
2.1.1 Hamming Distance
The Hamming distance (Hamming, 1950) mea-
sures the number of disagreements between two per-
mutations. It is defined as follows:
dh(pi, ?) = 1?
?n
i=1 xi
n
, xi =
{
0 if pi(i) = ?(i)
1 otherwise
1028
Eg. BLEU METEOR TER dh dk
(a) 100.0 100.0 100.0 100.0 100.0
(b) 61.8 86.9 90.0 80.0 85.1
(c) 81.3 92.6 90.0 0.0 25.5
Table 1. Metric scores for examples in Figure 1 which are
calculated by comparing the permutations to the identity.
All metrics are adjusted so that 100 is the best score and
0 the worst.
where n is the length of the permutation. The
Hamming distance is the simplest permutation dis-
tance metric and is useful as a baseline. It has no
concept of the relative ordering of words.
2.1.2 Kendall?s Tau Distance
Kendall?s tau distance is the minimum number
of transpositions of two adjacent symbols necessary
to transform one permutation into another (Kendall,
1938). It represents the percentage of pairs of ele-
ments which share the same order between two per-
mutations. It is defined as follows:
dk(pi, ?) = 1?
?
?n
i=1
?n
j=1 zij
Z
where zij =
{
1 if pi(i) < pi(j) and ?(i) > ?(j)
0 otherwise
Z =
(n2 ? n)
2
Kendalls tau seems particularly appropriate for
measuring word order differences as the relative or-
dering words is taken into account. However, most
human and machine ordering differences are much
closer to monotone than to inverted. The range of
values of Kendall?s tau is therefore too narrow and
close to 1. For this reason we take the square root
of the standard metric. This adjusted dk is also
more correlated with human judgements of reorder-
ing quality (Birch et al, 2010).
We use the example in Figure 1 to highlight the
problem with current MT metrics, and to demon-
strate how the permutation distance metrics are cal-
culated. In Table 1 we present the metric results for
the example permutations. The metrics are calcu-
lated by comparing the permutation string with the
monotone permutation. (a) receives the best score
for all metrics as it is compared to itself. BLEU
and METEOR fail to recognise that (b) represents a
small reordering and (c) a large reordering and they
assign a lower score to (b). The reason for this is that
they are sensitive to breaks in order, but not to the
actual word order differences. BLEU matches more
n-grams for (c) and consequently assigns it a higher
score. METEOR counts the number of blocks that
the translation is broken into, in order to align it with
the source. (b) is aligned using four blocks, whereas
(c) is aligned using only two blocks. TER counts the
number of edits, allowing for block shifts, and ap-
plies one block shift for each example, resulting in
an equal score for (b) and (c). Both the Hamming
distance dh and the Kendall?s tau distance dk cor-
rectly assign (c) a worse score than (b). Note that
for (c), the Hamming distance was not able to re-
ward the permutation for the correct relative order-
ing of words within the two large blocks and gave
(c) a score of 0, whereas Kendall?s tau takes relative
ordering into account.
Wong and Kit (2009) also suggest a metric which
combines a word choice and a word order compo-
nent. They propose a type of F-measure which uses
a matching function M to calculate precision and
recall. M combines the number of matched words,
weighted by their tfidf importance, with their posi-
tion difference score, and finally subtracting a score
for unmatched words. Including unmatched words
in the M function undermines the interpretation of
the supposed F-measure. The reordering component
is the average difference of absolute and relative
word positions which has no clear meaning. This
score is not intuitive or easily decomposable and it is
more similar to METEOR, with synonym and stem
functionality mixed with a reordering penalty, than
to our metric.
2.2 Combined Metric
The LRscore consists of a reordering distance met-
ric which is linearly interpolated with a lexical score
to form a complete machine translation evaluation
metric. The metric is decomposable because the in-
dividual lexical and reordering components can be
looked at individually. The following formula de-
scribes how to calculate the LRscore:
LRscore = ?R+ (1? ?)L (1)
The metric contains only one parameter, ?, which
balances the contribution of the reordering metric,
R, and the lexical metric, L. Here we use BLEU as
1029
the lexical metric. R is the average permutation dis-
tance metric adjusted by the brevity penalty and it is
calculated as follows:
R =
?
s?S dsBPs
|S|
(2)
Where S is a set of test sentences, ds is the reorder-
ing distance for a sentence and BP is the brevity
penalty.
The brevity penalty is calculated as:
BP =
{
1 if t > r
e1?r/t if t ? r
(3)
where t is the length of the translation, and r is the
closest reference length. If the reference sentence is
slightly longer than the translation, then the brevity
penalty will be a fraction somewhat smaller than
1. This has the effect of penalising translations that
are shorter than the reference. The brevity penalty
within the reordering component is necessary as the
distance-based metric would provide the same score
for a one word translation as it would for a longer
monotone translation. R is combined with a system
level lexical score.
In this paper we apply the BLEU score as the lex-
ical metric, as it is well known and it measures lexi-
cal precision at different n-gram lengths. We experi-
ment with the full BLEU score and the 1-gram BLEU
score, BLEU1, which is purely a measure of the pre-
cision of the word choice. The 4-gram BLEU score
includes some measure of the local reordering suc-
cess in the precision of the longer n-grams. BLEU
is an important baseline, and improving on it by in-
cluding more reordering information is an interest-
ing result. The lexical component of the system can
be any meaningful metric for a particular target lan-
guage. If a researcher was interested in morpholog-
ically rich languages, for example, METEOR could
be used. We use the LRscore to return sentence level
scores as well system level scores, and when doing
so the smoothed BLEU (Lin and Och, 2004) is used.
3 Consistency with Human Judgements
Automatic metrics must be validated by compar-
ing their scores with human judgements. We train
the metric parameter to optimise consistency with
human preference judgements across different lan-
guage pairs and then we show that the LRscore is
more consistent with humans than other commonly
used metrics.
3.1 Experimental Design
Human judgement of rank has been chosen as the of-
ficial determinant of translation quality for the 2009
Workshop on Machine Translation (Callison-Burch
et al, 2009). We used human ranking data from this
workshop to evaluate the LRscore. This consisted
of German, French, Spanish and Czech translation
systems that were run both into and out of English.
In total there were 52,265 pairwise rank judgements
collected.
Our reordering metric relies upon word align-
ments that are generated between the source and the
reference sentences, and the source and the trans-
lated sentences. In an ideal scenario, the transla-
tion system outputs the alignments and the refer-
ence set can be selected to have gold standard hu-
man alignments. However, the data that we use to
evaluate metrics does not have any gold standard
alignments and we must train automatic alignment
models to generate them. We used version two of
the Berkeley alignment model (Liang et al, 2006),
with the posterior threshold set at 0.5. Our Spanish-,
French- and German-English alignment models are
trained using Europarl version 5 (Koehn, 2005). The
Czech-English alignment model is trained on sec-
tions 0-2 of the Czech-English Parallel Corpus, ver-
sion 0.9 (Bojar and Zabokrtsky, 2009).
The metric scores are calculated for the test set
from the 2009 workshop on machine translation. It
consists of 2525 sentences in English, French, Ger-
man, Spanish and Czech. These sentences have been
translated by different machine translation systems
and the output submitted to the workshop. The sys-
tem output along with human evaluations can be
downloaded from the web1.
The BLEU score has five parameters, one for each
n-gram, and one for the brevity penalty. These pa-
rameters are set to a default uniform value of one.
METEOR has 3 parameters which have been trained
for human judgements of rank (Lavie and Agarwal,
2008). METEOR version 0.7 was used. The other
baseline metric used was TER version 0.7.25. We
adapt TER by subtracting it from one, so that all
1http://www.statmt.org/wmt09/results.html
1030
metric increases mean an improvement in the trans-
lation. The TER metric has five parameters which
have not been trained.
Using rank judgements, we do not have absolute
scores and so we cannot compare translations across
different sentences and extract correlation statistics.
We therefore use the method adopted in the 2009
workshop on machine translation (Callison-Burch et
al., 2009). We ascertained how consistent the auto-
matic metrics were with the human judgements by
calculating consistency in the following manner. We
take each pairwise comparison of translation output
for single sentences by a particular judge, and we
recorded whether or not the metrics were consistent
with the human rank. I.e. we counted cases where
both the metric and the human judge agreed that one
system is better than another. We divided this by the
total number of pairwise comparisons to get a per-
centage. We excluded pairs which the human anno-
tators ranked as ties.
de-en es-en fr-en cz-en
dk 73.9 80.5 80.4 81.1
Table 2. The average Kendall?s tau reordering distance
between the test and reference sentences. 100 means
monotone thus de-en has the most reordering.
We present a novel method for setting the
LRscore parameter. Using multiple language pairs,
we train the parameter according to the amount of
reordering seen in each test set. The advantage of
this approach is that researchers do not need to train
the parameter for new language pairs or test do-
mains. They can simply calculate the amount of re-
ordering in the test set and adjust the parameter ac-
cordingly. The amount of reordering is calculated
as the Kendall?s tau distance between the source
and the reference sentences as compared to dummy
monotone sentences. The amount of reordering for
the test sentences is reported in Table 2. German-
English shows more reordering than other language
pairs as it has a lower dk score of 73.9. The language
independent parameter (?) is adjusted by applying
the reordering amount (dk) as an exponent. ? is al-
lowed to takes values of between 0 and 1. This works
in a similar way to the brevity penalty. With more re-
ordering, the dk becomes smaller which leads to an
increase in the final value of ?. ? represents the per-
centage contribution of the reordering component in
the LRscore:
? = ?dk (4)
The language independent parameter ? is trained
once, over multiple language pairs. This procedure
optimises the average of the consistency results
across the different language pairs. We use greedy
hillclimbing in order to find the optimal setting. As
hillclimbing can end up in a local minima, we per-
form 20 random restarts, and retaining only the pa-
rameter value with the best consistency result.
3.2 Results
Table 3 reports the optimal consistency of the
LRscore and baseline metrics with human judge-
ments for each language pair. The LRscore vari-
ations are named as follows: LR refers to the
LRscore, ?H? refers to the Hamming distance and
?K? to Kendall?s tau distance. ?B1? and ?B4? refer
to the smoothed BLEU score with the 1-gram and
the complete scores. Table 3 shows that the LRscore
is more consistent with human judgement for 5 out
of the 8 language pairs. This is an important result
which shows that combining lexical and reordering
information makes for a stronger metric than the
baseline metrics which do not have a strong reorder-
ing component.
METEOR is the most consistent for the Czech-
English and English-Czech language pairs, which
have the least amount of reordering. METEOR lags
behind for the language pairs with the most reorder-
ing, the German-English and English-German pairs.
Here LR-KB4 is the best metric, which shows that
metrics which are sensitive to the distance words are
out of order are more appropriate for situations with
a reasonable amount of reordering.
4 Optimising Translation Models
Automatic metrics are useful for evaluation, but they
are essential for training model parameters. In this
section we apply the LRscore as the objective func-
tion in MERT training (Och, 2003). MERT min-
imises translation errors according to some auto-
matic evaluation metric while searching for the best
parameter settings over the N-best output. A MERT
trained model is likely to exhibit the properties that
1031
Metric de-en es-en fr-en cz-en en-de en-es en-fr en-cz ave
METEOR 58.6 58.3 58.3 59.4 52.6 55.7 61.2 55.6 57.5
TER 53.2 50.1 52.6 47.5 48.6 49.6 58.3 45.8 50.7
BLEU1 56.1 57.0 56.7 52.5 52.1 54.2 62.3 53.3 55.6
BLEU 58.7 55.5 57.7 57.2 54.1 56.7 63.7 53.1 57.1
LR-HB1 59.7 60.0 58.6 53.2 54.6 55.6 63.7 54.5 57.5
LR-HB4 60.4 57.3 58.7 57.2 54.8 57.3 63.3 53.8 57.9
LR-KB1 60.4 59.7 58.0 54.0 54.1 54.7 63.4 54.9 57.5
LR-KB4 61.0 57.2 58.5 58.6 54.8 56.8 63.1 55.0 58.7
Table 3. The percentage consistency between human judgements of rank and metrics. The LRscore variations (LR-*)
are optimised for average consistency across language pair (shown in right hand column). The bold numbers represent
the best consistency score per language pair.
the metric rewards, but will be blind to aspects of
translation quality that are not directly captured by
the metric. We apply the LRscore in order to im-
prove the reordering performance of a phrase-based
translation model.
4.1 Experimental Design
We hypothesise that the LRscore is a good metric
for training translation models. We test this by eval-
uating the output of the models, first with automatic
metrics, and then by using human evaluation. We
choose to run the experiment with Chinese-English
as this language pair has a large amount of medium
and long distance reorderings.
4.1.1 Training Setup
The experiments are carried out with Chinese-
English data from GALE. We use the official test
set of the 2006 NIST evaluation (1994 sentences).
For the development test set, we used the evalu-
ation set from the GALE 2008 evaluation (2010
sentences). Both development set and test set have
four references. The phrase table was built from
1.727M parallel sentences from the GALE Y2 train-
ing data. The phrase-based translation model called
MOSES was used, with all the default settings. We
extracted phrases as in (Koehn et al, 2003) by run-
ning GIZA++ in both directions and merging align-
ments with the grow-diag-final heuristic. We used
the Moses translation toolkit, including a lexicalised
reordering model. The SRILM language modelling
toolkit (Stolcke, 2002) was used with interpolated
Kneser-Ney discounting. There are three separate 3-
gram language models trained on the English side
of parallel corpus, the AFP part of the Gigaword
corpus, and the Xinhua part of the Gigaword cor-
LR-HB1 LR-HB4 LR-KB1 LR-KB4
26.40 07.19 43.33 26.23
Table 4. The parameter setting representing the % impact
of the reordering component for the different versions of
the LRscore metric.
pus. A 4 or 5-gram language model would have
led to higher scores for all objective functions, but
would not have changed the findings in this paper.
We used the MERT code available in the MOSES
repository (Bertoldi et al, 2009).
The reordering metrics require alignments which
were created using the Berkeley word alignment
package version 1.1 (Liang et al, 2006), with the
posterior probability to being 0.5.
We first extracted the LRscore Kendall?s tau dis-
tance from the monotone for the Chinese-English
test set and this value was 66.1%. This is far more re-
ordering than the other language pairs shown in Ta-
ble 2. We then calculated the optimal parameter set-
ting, using the reordering amount as a power expo-
nent. Table 4 shows the parameter settings we used
in the following experiments. The optimal amount of
reordering for LR-HB4 is low, but the results show
it still makes an important contribution.
4.1.2 Human Evaluation Setup
Human judgements of translation quality are nec-
essary to determine whether humans prefer sen-
tences from models trained with the BLEU score
or with the LRscore. There have been some recent
studies which have used the online micro-market,
Amazons Mechanical Turk, to collect human anno-
tations (Snow et al, 2008; Callison-Burch, 2009).
While some of the data generated is very noisy, in-
valid responses are largely due to a small number
of workers (Kittur et al, 2008). We use Mechanical
1032
Turk and we improve annotation quality by collect-
ing multiple judgements, and eliminating workers
who do not achieve a certain level of performance
on gold standard questions.
We randomly selected a subset of sentences from
the test set. We use 60 sentences each for compar-
ing training with BLEU to training with LR-HB4
and with LR-KB4. These sentences were between
15 and 30 words long. Shorter sentences tend to have
uninteresting differences, and longer sentences may
have many conflicting differences.
Workers were presented with a reference sen-
tence and two translations which were randomly
ordered. They were told to compare the transla-
tions and select their preferred translation or ?Don?t
Know?. Workers were screened to guarantee reason-
able judgement quality. 20 sentence pairs were ran-
domly selected from the 120 test units and anno-
tated as gold standard questions. Workers who got
less than 60% of these gold questions correct were
disqualified and their judgements discarded.
After disagreeing with a gold annotation, a worker
is presented with the gold answer and an expla-
nation. This guides the worker on how to perform
the task and motivates them to be more accurate.
We used the Crowdflower2 interface to Mechanical
Turk, which implements the gold functionality.
Even though experts can disagree on preference
judgements, gold standard labels are necessary to
weed out the poor standard workers. There were 21
trusted workers who achieved an average accuracy
of 91% on the gold. There were 96 untrusted work-
ers who averaged 29% accuracy on the gold. Their
judgements were discarded. Three judgements were
collected from the trusted workers for each of the
120 test sentences.
4.2 Results
4.2.1 Automatic Evaluation of MERT
In this experiment we demonstrate that the re-
ordering metrics can be used as learning criterion in
minimum error rate training to improve parameter
estimation for machine translation.
Table 5 reports the average of three runs of MERT
training with different objective functions. The lexi-
cal metric BLEU is used as an objective function in
2http://www.crowdflower.com
MetricsPPPPObj.Func. BLEU LR-HB4 LR-KB4 TER MET.
BLEU 31.1 32.1 41.0 60.7 55.5
LRHB4 31.1 32.2 41.3 60.6 55.7
LRKB4 31.0 32.2 41.2 61.0 55.8
Table 5. Average results of three different MERT runs for
different objective functions.
isolation, and also as part of the LRscore together
with the Hamming distance and Kendall?s tau dis-
tance. We test with these metrics, and we also report
the TER and METEOR scores for comparison.
The first thing we note in Table 5 is that we would
expect the highest scores when training with the
same metric as that used for evaluation as MERT
maximises the objective function on the develop-
ment data set. Here, however, when testing with
BLEU, we see that training with BLEU and with
LR-HB4 leads to equally high BLEU scores. The
reordering component is more discerning than the
BLEU score. It reliably increases as the word order
approaches that of the reference, whereas BLEU can
reports the same score for a large number of different
alternatives. This might make the reordering metric
easier to optimise, leading to the joint best scores
at test time. This is an important result, as it shows
that by training with the LRscore objective function,
BLEU scores do not decrease, which is desirable as
BLEU scores are usually reported in the field.
The LRscore also results in better scores when
evaluated with itself and the other two baseline met-
rics, TER and METEOR. Reordering and the lexi-
cal metrics are orthogonal information sources, and
this shows that combining them results in better per-
forming systems. BLEU has shown to be a strong
baseline metric to use as an objective function (Cer
et al, 2010), and so the LRscore performance in Ta-
ble 5 is a good result.
Examining the weights that result from the dif-
ferent MERT runs, the only notable difference is
that the weight of the distortion cost is considerably
lower with the LRscore. This shows more trust in
the quality of reorderings. Although it is interesting
to look at the model weights, any final conclusion on
the impact of the metrics on training must depend on
human evaluation of translation quality.
1033
Type Sentence
Reference silicon valley is still a rich area in the united states. the average salary in the area was us
$62,400 a year, which was 64% higher than the american average.
LR-KB4 silicon valley is still an affluent area of the united states, the regional labor with an average
annual salary of 6.24 million us dollars, higher than the average level of 60 per cent.
BLEU silicon valley is still in the united states in the region in an affluent area of the workforce,
the average annual salary of 6.24 million us dollars, higher than the average level of 60 per
cent
Table 7. A reference sentence is compared with output from models trained with BLEU and with the LR-KB4 lrscore.
Prefer LR Prefer BLEU Don?t Know
LR-KB4 96 79 5
LR-HB4 93 79 8
Total 189 (52.5%) 158 (43.9%) 13
Table 6. The number of the times human judges preferred
the output of systems trained either with the LRscore or
with the BLEU score, or were unable to choose.
4.2.2 Human Evaluation
We collect human preference judgements for out-
put from systems trained using the BLEU score and
the LRscore in order to determine whether training
with the LRscore leads to genuine improvements in
translation quality. Table 6 shows the number of the
times humans preferred the LRscore or the BLEU
score output, or when they did not know. We can see
that humans have a greater preference for the out-
put for systems trained with the LRscore, which is
preferred 52.5% of the time, compared to the BLEU
score, which was only preferred 43.9% of the time.
The sign test can be used to determine whether
this difference is significant. Our null hypothesis
is that the probability of a human preferring the
LRscore trained output is the same as that of prefer-
ring the BLEU trained output. The one-tailed alter-
native hypothesis is that humans prefer the LRscore
output. If the null hypothesis is true, then there is
only a probability of 0.048 that 189 out of 347
(189 + 158) people will select the LRscore output.
We therefore discard the null hypothesis and the hu-
man preference for the output of the LRscore trained
system is significant to the 95% level.
In order to judge how reliable our judgements are
we calculate the inter-annotator agreement. This is
given by the Kappa coefficient (K), which balances
agreement with expected agreement. The Kappa co-
efficient is 0.464 which is considered to be a moder-
ate level of agreement.
In analysis of the results, we found that output
from the system trained with the LRscore tend to
produce sentences with better structure. In Table 7
we see a typical example. The word order of the
sentence trained with BLEU is mangled, whereas
the LR-KB4 model outputs a clear translation which
more closely matches the reference. It also garners
higher reordering and BLEU scores.
We expect that more substantial gains can be
made in the future by using models which have more
powerful reordering capabilities. A richer set of re-
ordering features, and a model capable of longer
distance reordering would better leverage metrics
which reward good word orderings.
5 Conclusion
We introduced the LRscore which combines a lexi-
cal and a reordering metric. The main motivation for
this metric is the fact that it measures the reorder-
ing quality of MT output by using permutation dis-
tance metrics. It is a simple, decomposable metric
which interpolates the reordering component with
a lexical component, the BLEU score. This paper
demonstrates that the LRscore metric is more con-
sistent with human preference judgements of ma-
chine translation quality than other machine trans-
lation metrics. We also show that when training a
phrase-based translation model with the LRscore as
the objective function, the model retains its perfor-
mance as measured by the baseline metrics. Cru-
cially, however, optimisation using the LRscore im-
proves subjective evaluation. Ultimately, the avail-
ability of a metric which reliably measures reorder-
ing performance should accelerate progress towards
developing more powerful reordering models.
1034
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for MT evaluation with improved
correlation with human judgments. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization.
Nicola Bertoldi, Barry Haddow, and Jean-Baptiste Fouet.
2009. Improved Minimum Error Rate Training in
Moses. The Prague Bulletin of Mathematical Linguis-
tics, 91:7?16.
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2010. Metrics for MT Evaluation: Evaluating Re-
ordering. Machine Translation, 24(1):15?26.
Ondrej Bojar and Zdenek Zabokrtsky. 2009. CzEng0.9:
Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics, 92:63?
84.
Chris Callison-Burch, Philipp Koehn, Christof Monz, and
Josh Schroeder. 2009. Findings of the 2009 Workshop
on Statistical Machine Translation. In Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, pages 1?28, Athens, Greece, March. Association
for Computational Linguistics.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using Amazon?s
Mechanical Turk. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 286?295, Singapore, August. Associa-
tion for Computational Linguistics.
Daniel Cer, Christopher D. Manning, and Daniel Juraf-
sky. 2010. The best lexical metric for phrase-based
statistical MT system optimization. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 555?563, Los An-
geles, California, June.
Richard Hamming. 1950. Error detecting and er-
ror correcting codes. Bell System Technical Journal,
26(2):147?160.
Maurice Kendall. 1938. A new measure of rank correla-
tion. Biometrika, 30:81?89.
A. Kittur, E. H. Chi, and B. Suh. 2008. Crowdsourcing
user studies with Mechanical Turk. In Proceeding of
the twenty-sixth annual SIGCHI conference on Human
factors in computing systems, pages 453?456. ACM.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Sta-
tistical Phrase-Based translation. In Proceedings of
the Human Language Technology and North Ameri-
can Association for Computational Linguistics Con-
ference, pages 127?133, Edmonton, Canada. Associ-
ation for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT-
Summit.
Alon Lavie and Abhaya Agarwal. 2008. Meteor,
m-BLEU and m-TER: Evaluation metrics for high-
correlation with human rankings of machine transla-
tion output. In Proceedings of the Workshop on Sta-
tistical Machine Translation at the Meeting of the As-
sociation for Computational Linguistics (ACL-2008),
pages 115?118.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 104?111, New York City, USA,
June. Association for Computational Linguistics.
Chin-Yew Lin and Franz Och. 2004. ORANGE: a
method for evaluating automatic evaluation metrics for
machine translation. In Proceedings of the Conference
on Computational Linguistics, pages 501?507.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the As-
sociation for Computational Linguistics, pages 160?
167, Sapporo, Japan.
Sebastian Pado?, Daniel Cer, Michel Galley, Dan Jurafsky,
and Christopher D. Manning. 2009. Measuring ma-
chine translation quality as semantic equivalence: A
metric based on entailment features. Machine Trans-
lation, pages 181?193.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic evalu-
ation of machine translation. In Proceedings of the As-
sociation for Computational Linguistics, pages 311?
318, Philadelphia, USA.
Matthew Snover, Bonnie Dorr, R. Schwartz, L. Micciulla,
and J. Makhoul. 2006. A study of translation edit
rate with targeted human annotation. In Proceedings
of Association for Machine Translation in the Ameri-
cas, pages 223?231.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: Evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 254?263. Association for Computational Lin-
guistics.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of Spoken Language
Processing, pages 901?904.
Billy Wong and Chunyu Kit. 2009. ATEC: automatic
evaluation of machine translation via word choice and
word order. Machine Translation, 23(2-3):141?155.
1035
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 327?332,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
LRscore for Evaluating Lexical and Reordering Quality in MT
Alexandra Birch
University of Edinburgh
United Kingdom
a.c.birch-mayne@s0454866.ed.ac.uk
Miles Osborne
University of Edinburgh
United Kingdom
miles@inf.ed.ac.uk
Abstract
The ability to measure the quality of word
order in translations is an important goal
for research in machine translation. Cur-
rent machine translation metrics do not
adequately measure the reordering perfor-
mance of translation systems. We present
a novel metric, the LRscore, which di-
rectly measures reordering success. The
reordering component is balanced by a
lexical metric. Capturing the two most im-
portant elements of translation success in
a simple combined metric with only one
parameter results in an intuitive, shallow,
language independent metric.
1 Introduction
The main purpose of MT evaluation is to de-
termine ?to what extent the makers of a system
have succeeded in mimicking the human transla-
tor? (Krauwer, 1993). But machine translation
has no ?ground truth? as there are many possi-
ble correct translations. It is impossible to judge
whether a translation is incorrect or simply un-
known and it is even harder to judge the the degree
to which it is incorrect. Even so, automatic met-
rics are necessary. It is nearly impossible to collect
enough human judgments for evaluating incre-
mental improvements in research systems, or for
tuning statistical machine translation system pa-
rameters. Automatic metrics are also much faster
and cheaper than human evaluation and they pro-
duce reproducible results.
Machine translation research relies heavily
upon automatic metrics to evaluate the perfor-
mance of models. However, current metrics rely
upon indirect methods for measuring the quality
of the word order, and their ability to capture re-
ordering performance has been demonstrated to be
poor (Birch et al, 2010). There are two main ap-
proaches to capturing reordering. The first way
to measure the quality of word order is to count
the number of matching n-grams between the ref-
erence and the hypothesis. This is the approach
taken by the BLEU score (Papineni et al, 2002).
This method discounts any n-gram which is not
identical to a reference n-gram, and also does not
consider the relative position of the strings. They
can be anywhere in the sentence. Another com-
mon approach is typified by METEOR (Banerjee
and Lavie, 2005) and TER (Snover et al, 2006).
They calculate an ordering penalty for a hypoth-
esis based on the minimum number of chunks the
translation needs to be broken into in order to align
it to the reference. The disadvantage of the second
approach is that aligning sentences with very dif-
ferent words can be inaccurate. Also there is no
notion of how far these blocks are out of order.
More sophisticated metrics, such as the RTE met-
ric (Pado? et al, 2009), use higher level syntactic or
even semantic analysis to determine the quality of
the translation. These approaches are useful, but
can be very slow, require annotation, they are lan-
guage dependent and their parameters are hard to
train. For most research work shallow metrics are
more appropriate.
Apart from failing to capture reordering perfor-
mance, another common criticism of most cur-
rent automatic MT metrics is that a particular
score value reported does not give insights into
quality (Przybocki et al, 2009). This is because
there is no intrinsic significance of a difference
in scores. Ideally, the scores that the metrics re-
port would be meaningful and stand on their own.
However, the most one can say is that higher is
better for accuracy metrics and lower is better for
error metrics.
We present a novel metric, the LRscore, which
explicitly measures the quality of word order in
machine translations. It then combines the re-
ordering metric with a metric measuring lexical
success. This results in a comprehensive met-
327
ric which measures the two most fundamental as-
pects of translation. We argue that the LRscore
is intuitive and meaningful because it is a simple,
decomposable metric with only one parameter to
train.
The LRscore has many of the properties that are
deemed to be desirable in a recent metric eval-
uation campaign (Przybocki et al, 2009). The
LRscore is language independent. The reorder-
ing component relies on abstract alignments and
word positions and not on words at all. The lex-
ical component of the system can be any mean-
ingful metric for a particular target language. In
our experiments we use 1-gram BLEU and 4-gram
BLEU, however, if a researcher was interested in
morphologically rich languages, a different met-
ric which scores partially correct words might be
more appropriate. The LRscore is a shallow met-
ric, which means that it is reasonably fast to run.
This is important in order to be useful for train-
ing of the translation model parameters. A final
advantage is that the LRscore is a sentence level
metric. This means that human judgments can be
directly compared to system scores and helps re-
searchers to understand what changes they are see-
ing between systems.
In this paper we start by describing the reorder-
ing metrics and then we present the LRscore. Fi-
nally we discuss related work and conclude.
2 Reordering Metrics
The relative ordering of words in the source and
target sentences is encoded in alignments. We
can interpret algnments as permutations. This
allows us to apply research into metrics for or-
dered encodings to our primary tasks of measur-
ing and evaluating reorderings. A word alignment
over a sentence pair allows us to transcribe the
source word positions in the order of the aligned
target words. Permutations have already been
used to describe reorderings (Eisner and Tromble,
2006), primarily to develop a reordering model
which uses ordering costs to score possible per-
mutations. Here we use permutations to evaluate
reordering performance based on the methods pre-
sented in (Birch et al, 2010).
The ordering of the words in the target sentence
can be seen as a permutation of the words in the
source sentence. The source sentence s of length
N consists of the word positions s0 ? ? ? si ? ? ? sN .
Using an alignment function where a source word
at position i is mapped to a target word at position
j with the function a : i ? j, we can reorder the
source word positions to reflect the order of the
words in the target. This gives us a permutation.
A permutation is a bijective function from a set
of natural numbers 1, 2, ? ? ? , N to itself. We will
name our permutations pi and ?. The ith symbol
of a permutation pi will be denoted as pi(i), and
the inverse of the permutation pi?1 is defined so
that if pi(i) = j then pi?1(j) = i. The identity, or
monotone, permutation id is the permutation for
which id(i) = i for all i. Table 1 shows the per-
mutations associated with the example alignments
in Figure 1. The permutations are calculated by
iterating over the source words, and recording the
ordering of the aligned target words.
Permutations encode one-one relations,
whereas alignments contain null alignments and
one-many, many-one and many-many relations.
For now, we make some simplifying assumptions
to allow us to work with permutations. Source
words aligned to null (a(i) ? null) are assigned
the target word position immediately after the
target word position of the previous source word
(pi(i) = pi(i ? 1) + 1). Where multiple source
words are aligned to the same target word or
phrase, a many-to-one relation, the target ordering
is assumed to be monotone. When one source
word is aligned to multiple target words, a one-to-
many relation, the source word is assumed to be
aligned to the first target word.
A translation can potentially have many valid
word orderings. However, we can be reason-
ably certain that the ordering of reference sentence
must be acceptable. We therefore compare the or-
dering of a translation with that of the reference
sentence. The underlying assumption is that most
reasonable word orderings should be fairly similar
to the reference. The assumption that the reference
is somehow similar to the translation is necessary
for all automatic machine translation metrics. We
propose using permutation distance metrics to per-
form the comparison.
There are many different ways of measuring
distance between two permutations, with different
solutions originating in different domains (statis-
tics, computer science, molecular biology, . . . ).
Real numbered data leads to measures such as Eu-
clidean distance, binary data to measures such as
Hamming distance. But for ordered sets, there
are many different options, and the best one de-
328
t1
t2
t3
t4
t5
t6
t7
t8
t9
t10
s1 s2 s3 s4 s5 s6 s7 s8 s9 s1
0
(a)
t1
t2
t3
t4
t6
t5
t7
t8
t9
t10
s1 s2 s3 s4 s5 s6 s7 s8 s9 s1
0
(b)
t6
t7
t8
t9
t10
t1
t2
t3
t4
t5
s1 s2 s3 s4 s5 s6 s7 s8 s9 s1
0
(c)
t10
t1
t2
t3
t4
t5
t6
t7
t8
t9
s1 s2 s3 s4 s5 s6 s7 s8 s9 s1
0
(d)
Figure 1: Synthetic examples: a translation and three reference scenarios. (a) is a monotone translation,
(b) is a reference with one short distance word order difference, (c) is a reference where the order of the
two halves has been swapped, and (d) is a reference with a long distance reordering of the first target
word.
pends on the task at hand. We choose a few
metrics which are widely used, efficient to calcu-
late and capture certain properties of the reorder-
ing. In particular, they are sensitive to the num-
ber of words that are out of order. Three of the
metrics, Kendall?s tau, Spearman?s rho and Spear-
man?s footrule distances also take into account the
distance between positions in the reference and
translation sentences, or the size of the reordering.
An obvious disadvantage of this approach is the
fact that we need alignments, either between the
source and the reference, and the source and the
translation, or directly between the reference and
the translation. If accuracy is paramount, the test
set could include manual alignments and the sys-
tems could directly output the source-translation
alignments. Outputting the alignment informa-
tion should require a trivial change to the decoder.
Alignments can also be automatically generated
using the alignment model that aligns the training
data.
Distance metrics increase as the quality of trans-
lation decreases. We invert the scale of the dis-
(a) (1 2 3 4 5 6 7 8 9 10)
(b) (1 2 3 4 ?6 ?5 ?7 8 9 10)
(c) (6 7 8 9 10 ?1 2 3 4 5)
(d) (2 3 4 5 6 7 8 9 10 ?1)
Table 1: Permutations extracted from the sentence
pairs shown in Figure 1: (a) is a monotone permu-
tation and (b), (c) and (d) are permutations with
different amounts of disorder, where bullet points
highlight non-sequential neighbors.
tance metrics in order to easily compare them with
other metrics where increases in the metrics mean
increases in translation quality. All permutation
distance metrics are thus subtracted from 1. Note
that the two permutations we refer to pi and ? are
relative to the source sentence, and not to the ref-
erence: the source-reference permutation is com-
pared to the source-translation permutation.
2.1 Hamming Distance
The Hamming distance (Hamming, 1950) mea-
sures the number of disagreements between two
329
permutations. The Hamming distance for permu-
tations was proposed by (Ronald, 1998) and is also
known as the exact match distance. It is defined
as follows:
dH(pi, ?) = 1?
?n
i=1 xi
n
where xi =
{
0 if pi(i) = ?(i)
1 otherwise
Where pi, ? are the two permutations and the
normalization constant Z is n, the length of the
permutation. We are interested in the Hamming
distance for its ability to capture the amount of ab-
solute disorder that exists between two permuta-
tions. The Hamming distance is widely utilized in
coding theory to measure the discrepancy between
two binary sequences.
2.2 Kendall?s Tau Distance
Kendall?s tau distance is the minimum number
of transpositions of two adjacent symbols nec-
essary to transform one permutation into an-
other (Kendall, 1938; Kendall and Gibbons,
1990). This is sometimes known as the swap dis-
tance or the inversion distance and can be inter-
preted as a function of the probability of observing
concordant and discordant pairs (Kerridge, 1975).
It is defined as follows:
d? (pi, ?) = 1?
?n
i=1
?n
j=1 zij
Z
where zij =
{
1 if pi(i) < pi(j) and ?(i) > ?(j)
0 otherwise
Z =
(n2 ? n)
2
The Kendall?s tau metric is possibly the most in-
teresting for measuring reordering as it is sensitive
to all relative orderings. It consequently measures
not only how many reordering there are but also
the distance that words are reordered.
In statistics, Spearman?s rho and Kendall?s tau
are widely used non-parametric measures of as-
sociation for two rankings. In natural language
processing research, Kendall?s tau has been used
as a means of estimating the distance between
a system-generated and a human-generated gold-
standard order for the sentence ordering task (La-
pata, 2003). Kendall?s tau has also been used
in machine translation as a cost function in a re-
ordering model (Eisner and Tromble, 2006) and
an MT metric called ROUGE-S (Lin and Och,
2004) is similar to a Kendall?s tau metric on lexical
items. ROUGE-S is an F-measure of ordered pairs
of words in the translation. As far as we know,
Kendall?s tau has not been used as a reordering
metric before.
3 LRscore
The goal of much machine translation research is
either to improve the quality of the words used in
the output, or their ordering. We use the reordering
metrics and combine them with a measurement of
lexical performance to produce a comprehensive
metric, the LRscore. The LRscore is a linear in-
terpolation of a reordering metric with the BLEU
score. If we use the 1-gram BLEU score, BLEU1,
then the LRscore relies purely upon the reorder-
ing metric for all word ordering evaluation. We
also use the 4-gram BLEU score, BLEU4, as it is
an important baseline and the values it reports are
very familiar to machine translation researchers.
BLEU4 also contains a notion of word ordering
based on longer matching n-grams. However, it
is aware only of very local orderings. It does not
measure the magnitude of the orderings like the
reordering metrics do, and it is dependent on ex-
act lexical overlap which does not affect the re-
ordering metric. The two components are there-
fore largely orthogonal and there is a benefit in
combining them. Both the BLEU score and the
reordering distance metric apply a brevity penalty
to account for translations of different lengths.
The formula for calculating the LRscore is as
follows:
LRscore = ? ?R+ (1? ?)BLEU
Where the reordering metricR is calculated as fol-
lows:
R = d ?BP
Where we either take the Hamming distance dH
or the Kendall?s tau distance d? as the reordering
distance d and then we apply the brevity penalty
BP . The brevity penalty is calculated as:
BP =
{
1 if t > r
e1?r/t if t ? r
where t is the length of the translation, and r is
the closest reference length. R is calculated at the
sentence level, and the scores are averaged over a
test set. This average is then combined with the
330
system level lexical score. The Lexical metric is
the BLEU score which sums the log precision of
n-grams. In our paper we set the n-gram length to
either be one or four.
The only parameter in the metric ? balances the
contribution of reordering and the lexical compo-
nents. There is no analytic solution for optimizing
this parameter, and we use greedy hillclimbing in
order to find the optimal setting. We optimize the
sentence level correlation of the metric to human
judgments of accuracy as provided by the WMT
2010 shared task. As hillclimbing can end up in a
local minima, we perform 20 random restarts, and
retaining only the parameter value with the best
consistency result. Random-restart hill climbing is
a surprisingly effective algorithm in many cases. It
turns out that it is often better to spend CPU time
exploring the space, rather than carefully optimiz-
ing from an initial condition.
The brevity penalty applies to both the reorder-
ing metric and the BLEU score. We do not set
a parameter to regulate the impact of the brevity
penalty, as we want to retain BLEU scores that are
comparable with BLEU scores computed in pub-
lished research. And as we do not regulate the
brevity penalty in the BLEU score, we do not wish
to do so for the reordering metric either. It there-
fore impacts on both the reordering and the lexical
components equally.
4 Correlation with Human Judgments
It has been common to use seven-point fluency
and adequacy scores as the main human evalua-
tion task. These scores are intended to be absolute
scores and comparable across sentences. Seven-
point fluency and adequacy judgements are quite
unreliable at a sentence level and so it seems du-
bious that they would be reliable across sentences.
However, having absolute scores does have the ad-
vantage of making it easy to calculate the correla-
tion coefficients of the metric with human judge-
ments. Using rank judgements, we do not have
absolute scores and thus we cannot compare trans-
lations across different sentences.
We therefore take the method adopted in the
2009 workshop on machine translation (Callison-
Burch et al, 2009). We ascertained how consis-
tent the automatic metrics were with the human
judgements by calculating consistency in the fol-
lowing manner. We take each pairwise compari-
son of translation output for single sentences by a
Metric de-en es-en fr-en cz-en
BLEU4 58.72 55.48 57.71 57.24
LR-HB1 60.37 60.55 58.59 53.70
LR-HB4 60.49 58.88 58.80 57.74
LR-KB1 60.67 58.54 58.46 54.20
LR-KB4 61.07 59.86 58.59 58.92
Table 2: The percentage consistency between hu-
man judgements of rank and metrics. The LRscore
variations (LR-*) are optimised for consistency for
each language pair.
particular judge, and we recorded whether or not
the metrics were consistent with the human rank.
Ie. we counted cases where both the metric and the
human judged agree that one system is better than
another. We divided this by the total umber of pair-
wise comparisons to get a percentage. There were
many ties in the human data, but metrics rarely
give the same score to two different translations.
We therefore excluded pairs that the human anno-
tators ranked as ties. The human ranking data and
the system outputs from the 2009 Workshop on
Machine Translation (Callison-Burch et al, 2009)
have been used to evaluate the LRscore.
We optimise the sentence level consistency of
the metric. As hillclimbing can end up in a local
minima, we perform 20 random restarts, and re-
taining only the parameter value with the best con-
sistency result. Random-restart hill climbing is a
surprisingly effective algorithm in many cases. It
turns out that it is often better to spend CPU time
exploring the space, rather than carefully optimis-
ing from an initial condition.
Table 2 reports the optimal consistency of the
LRscore and baseline metrics with human judge-
ments for each language pair. The table also
reports the individual component results. The
LRscore variations are named as follows: LR
refers to the LRscore, ?H? refers to the Hamming
distance and ?K? to Kendall?s tau distance. ?B1?
and ?B4? refer to the smoothed BLEU score with
the 1-gram and 4-gram scores. The LRscore is the
metric which is most consistent with human judge-
ment. This is an important result which shows
that combining lexical and reordering information
makes for a stronger metric.
5 Related Work
(Wong and Kit, 2009) also suggest a metric which
combines a word choice and a word order com-
331
ponent. They propose a type of F-measure which
uses a matching function M to calculate precision
and recall. M combines the number of matched
words, weighted by their tfidf importance, with
their position difference score, and finally sub-
tracting a score for unmatched words. Includ-
ing unmatched words in the in M function un-
dermines the interpretation of the supposed F-
measure. The reordering component is the average
difference of absolute and relative word positions
which has no clear meaning. This score is not intu-
itive or easily decomposable and it is more similar
to METEOR, with synonym and stem functional-
ity mixed with a reordering penalty, than to our
metric.
6 Conclusion
We propose the LRscore which combines a lexi-
cal and a reordering metric. This results in a met-
ric which is both meaningful and accurately mea-
sures the word order performance of the transla-
tion model.
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for MT evaluation with improved
correlation with human judgments. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization.
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2010. Metrics for MT Evaluation: Evaluating Re-
ordering. Machine Translation (to appear).
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
Jason Eisner and Roy W. Tromble. 2006. Local search
with very large-scale neighborhoods for optimal
permutations in machine translation. In Proceed-
ings of the HLT-NAACL Workshop on Computation-
ally Hard Problems and Joint Inference in Speech
and Language Processing, pages 57?75, New York,
June.
Richard Hamming. 1950. Error detecting and error
correcting codes. Bell System Technical Journal,
26(2):147?160.
M. Kendall and J. Dickinson Gibbons. 1990. Rank
Correlation Methods. Oxford University Press,
New York.
Maurice Kendall. 1938. A new measure of rank corre-
lation. Biometrika, 30:81?89.
D Kerridge. 1975. The interpretation of rank correla-
tions. Applied Statistics, 2:257?258.
S. Krauwer. 1993. Evaluation of MT systems: a pro-
grammatic view. Machine Translation, 8(1):59?66.
Mirella Lapata. 2003. Probabilistic text structur-
ing: Experiments with sentence ordering. Compu-
tational Linguistics, 29(2):263?317.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic evaluation of machine translation quality us-
ing longest common subsequence and skip-bigram
statistics. In Proceedings of the 42nd Meeting
of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 605?612, Barcelona,
Spain, July.
Sebastian Pado?, Daniel Cer, Michel Galley, Dan Juraf-
sky, and Christopher D. Manning. 2009. Measur-
ing machine translation quality as semantic equiva-
lence: A metric based on entailment features. Ma-
chine Translation.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the Association for Computational Linguistics,
pages 311?318, Philadelphia, USA.
Mark Przybocki, Kay Peterson, Se?bastien Bronsart,
and Gregory Sanders. 2009. The nist 2008 metrics
for machine translation challengeoverview, method-
ology, metrics, and results. Machine Translation.
S Ronald. 1998. More distance functions for order-
based encodings. In the IEEE Conference on Evolu-
tionary Computation, pages 558?563.
Matthew Snover, Bonnie Dorr, R Schwartz, L Micci-
ulla, and J Makhoul. 2006. A study of translation
edit rate with targeted human annotation. In AMTA.
B. Wong and C. Kit. 2009. ATEC: automatic eval-
uation of machine translation via word choice and
word order. Machine Translation, pages 1?15.
332
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 52?61,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The Feasibility of HMEANT as a Human MT Evaluation Metric
Alexandra Birch Barry Haddow Ulrich Germann
a.birch@ed.ac.uk bhaddow@inf.ed.ac.uk ugermann@inf.ed.ac.uk
Maria Nadejde Christian Buck Philipp Koehn
maria.nadejde@gmail.com cbuck@lantis.de pkoehn@inf.ed.ac.uk
University of Edinburgh
10 Crichton Street
Edinburgh, EH8 9AB, UK
Abstract
There has been a recent surge of interest in
semantic machine translation, which stan-
dard automatic metrics struggle to evalu-
ate. A family of measures called MEANT
has been proposed which uses semantic
role labels (SRL) to overcome this prob-
lem. The human variant, HMEANT, has
largely been evaluated using correlation
with human contrastive evaluations, the
standard human evaluation metric for the
WMT shared tasks. In this paper we claim
that for a human metric to be useful, it
needs to be evaluated on intrinsic proper-
ties. It needs to be reliable; it needs to
work across different language pairs; and
it needs to be lightweight. Most impor-
tantly, however, a human metric must be
discerning. We conclude that HMEANT
is a step in the right direction, but has
some serious flaws. The reliance on verbs
as heads of frames, and the assumption
that annotators need minimal guidelines
are particularly problematic.
1 Introduction
Human evaluation is essential in machine transla-
tion (MT) research because it is the ultimate way
to judge system quality. Furthermore, human eval-
uation is used to evaluate automatic metrics which
are necessary for tuning system parameters. Un-
fortunately, there is no clear consensus on which
evaluation strategy is best. Humans have been
asked to judge if translations are correct, to grade
them and to rank them. But it is often very difficult
to decide how good a translation is, when there are
so many possible ways of translating a sentence.
Another problem is that different types of evalua-
tion might be useful for different purposes. If the
MT is going to be the basis of a human transla-
tor?s work-flow, then post-editing effort seems like
a natural fit. However, for people using MT for
gisting, what we really want is some measure of
how much meaning has been retained.
We clearly need a metric which tries to answer
the question, how much of the meaning does the
translation capture. In this paper, we explore the
use of human evaluation metrics which attempt
to capture the extent of this meaning retention.
In particular, we consider HMEANT (Lo and Wu,
2011a), a metric that uses semantic role labels
to measure how much of the ?who, why, when,
where? has been preserved. For HMEANT evalua-
tion, annotators are instructed to identify verbs as
heads of semantic frames. Then they attach role
fillers to the heads and finally they align heads
and role fillers in the candidate translation with
those in a reference translation. In a series of pa-
pers, Lo and Wu (2010, 2011b,a, 2012) explored a
number of questions, evaluating HMEANT by us-
ing correlation statistics to compare it to judge-
ments of human adequacy and contrastive evalu-
ations. Given the drawbacks of those evaluation
measures, which we discuss in Sec. 2, they could
just as well have been evaluating the human ade-
quacy and contrastive judgements using HMEANT.
Human evaluation metrics need to be judged on
other intrinsic qualities, which we describe below.
The aim of this paper is to evaluate the effec-
tiveness of HMEANT, with the goal of using it to
judge the relative merits of different MT systems,
for example in the shared task of the Workshop on
Machine Translation.
In order to be useful, an MT evaluation metric
must be reliable, be language independent, have
discriminatory power, and be efficient. We address
each of these criteria as follows:
52
Reliability We produce extensive IAA (Inter-
annotator agreement) for HMEANT, breaking it
down into the different stages of annotation. Our
experimental results show that whilst the IAA for
HMEANT is acceptable at the individual stages of
the annotation, the compounding effect of dis-
agreement at each stage of the pipeline greatly re-
duces the effective overall IAA ? to 0.44 on role
alignment for German, and, only slightly better,
0.59 for English. This raises doubts about the reli-
ability of HMEANT in its current form.
Discriminatory Power We consider output of
three types of MT system (Phrase-based, Syntax-
based and Rule-based) to attempt to gain insight
into the different types of semantic information
preserved by the different systems. The Syntax-
based system seems to have a slight edge overall,
but since IAA is so low, this result has to be taken
with a grain of salt.
Language Independence We apply HMEANT
to both English and German translation outputs,
showing that the guidelines can be adapted to the
new language.
Efficiency Whilst HMEANT evaluation will
never be as fast as, for example, the contrastive
judgements used for the WMT shared task,
it is still reasonably efficient considering the
fine-grained nature of the evaluation. On average,
annotators evaluated about 10 sentences per hour.
2 Related Work
Even though the idea that machine translation re-
quires a semantic representation of the translated
content is as old as the idea of computer-based
translation itself (Weaver, 1955), it has not been
until recently that people have begun to combine
statistical models with semantic representations.
Jones et al (2012), for example, represent mean-
ing as directed acyclic graphs and map these to
PropBank (Palmer et al, 2005) style dependen-
cies. To evaluate such approaches properly, we
need evaluation metrics that capture the accuracy
of the translation.
Current automatic metrics of machine trans-
lation, such as BLEU (Papineni et al, 2002),
METEOR (Lavie and Denkowski, 2009) and
TER (Snover et al, 2009b), which have greatly
accelerated progress in MT research, rely on shal-
low surface properties of the translations, and
only indirectly capture whether or not the trans-
lation preserves the meaning. This has meant that
potentially more sophisticated translation models
are pitted against the flatter phrase-based mod-
els, based on metrics which cannot reflect their
strengths. Callison-Burch et al (2011) provide ev-
idence that automatic metrics are inconsistent with
human judgements when comparing rule-based
against statistical machine translation systems.
Automatic evaluation metrics are evaluated and
calibrated based on their correlation with human
judgements. However, after more than 60 years
of research into machine translation, there is still
no consensus on how to evaluate machine transla-
tion based on human judgements. (Hutchins and
Somers, 1992; Przybocki et al, 2009).
One obvious approach is to ask annotators to
rate translation candidates on a numerical scale.
Under the DARPA TIDES program, the Linguistic
Data Consortium (2002) developed an evaluation
scheme that relies on two five-point scales repre-
senting fluency and adequacy. This was also the
human evaluation scheme used in the annual MT
competitions sponsored by NIST (2005).
In an analysis of human evaluation results for
the WMT ?07 workshop, however, Callison-Burch
et al (2007) found high correlation between flu-
ency and adequacy scores assigned by individual
annotators, suggesting that human annotators are
not able to separate these two evaluation dimen-
sions easily. Furthermore these absolute scores
show low inter-annotator agreement. Instead of
giving absolute quality assessments, annotators
appeared to be using their ratings to rank trans-
lation candidates according to their overall prefer-
ence for one over the other.
In line with these findings, Callison-Burch et al
(2007) proposed to let annotators rank translation
candidates directly, without asking them to assign
an absolute quality assessment to each candidate.
This type of human evaluation has been performed
in the last six Workshops on Statistical Machine
Translation.
Although it is useful to have a score or a rank
for a particular sentence, especially for evaluat-
ing automatic metrics, these ratings are necessar-
ily a simplification of the real differences between
translations. Translations can contain a large num-
ber of different types of errors of varying severity.
Even if we put aside difficulties with selecting one
preferred sentence, ranking judgements are diffi-
cult to generalise. Humans are shown five transla-
tions at a time, and there is a high cognitive cost to
ranking these at once. Furthermore, these repre-
53
sent a subset of the competing systems, and these
rankings must be combined with other annotators
judgements on five other system outputs to com-
pute an overall ranking. The methodology for in-
terpreting the contrastive evaluations has been the
subject of much recent debate in the community
(Bojar et al, 2011; Lopez, 2012).
There has been some effort to overcome these
problems. HTER (Snover et al, 2009a) is a met-
ric which counts the number of edits needed by a
human to convert the machine translation so as to
convey the same meaning as the reference. This
type of evaluation is of some use when one is us-
ing MT to aid human translation (although the re-
lationship between number of edits and actual ef-
fort is not straightforward (Koponen, 2012)), but
it is not so helpful when one?s task is gisting. The
number of edits need not correlate with the sever-
ity of the semantic differences between the two
sentences. The loss of a negative, for instance, is
only one edit away from the original, but the se-
mantics change completely.
Alternatively, HyTER (Dreyer and Marcu,
2012) is an annotation tool which allows a user
to create an exponential number of correct trans-
lations for a given sentence. These references are
then efficiently exploited to compare with machine
translation output. The authors argue that the cur-
rent metrics fail simply because they have access
to sets of reference translations which are simply
too small. However, the fact is that even if one
does have access to large numbers of translations,
it is very difficult to determine whether the refer-
ence correctly captures the essential semantic con-
tent of the references.
The idea of using semantic role labels to evalu-
ate machine translation is not new. Gime?nez and
Ma`rquez (2007) proposed using automatically as-
signed semantic role labels as a feature in a com-
bined MT metric. The main difference between
this application of semantic roles and MEANT is
that arguments for specific verbs are taken into ac-
count, instead of just applying the subset agent,
patient and benefactor. This idea would probably
help human annotators to handle sentences with
passives, copulas and other constructions which
do not easily match the most basic arguments. On
the other hand, verb specific arguments are lan-
guage dependent.
Bojar and Wu (2012), applying HMEANT to
English-to-Czech MT output, identified a number
of problems with HMEANT, and suggested a vari-
ety of improvements. In some respects, this work
is very similar, except that our goal is to evaluate
HMEANT along a range of intrinsic properties, to
determine how useful the metric really is to evalu-
ation campaigns such as the workshop on machine
translation.
3 Evaluation with HMEANT
3.1 Annotation Procedure
The goal of the HMEANT metric is to capture es-
sential semantic content, but still be simple and
fast. There are two stages to the annotation, the
first of which is semantic role labelling (SRL).
Here the annotator is directed to select the actions,
or frame heads, by marking all the verbs in the sen-
tence except for auxilliaries and modals. The roles
(or slot fillers) within the frame are then marked
and each is linked with a unique action. Each role
is given a type from an inventory of 11 (Table 1),
and an action with its collection of corresponding
roles is known as a frame. In the role annotation
the idea is to get the annotator to recognise who
did what to who, when, where and why in both the
references and the MT outputs.
who what whom when where
agent patient benefactive temporal locative
why how
purpose degree, manner, modal, negation, other
Table 1: Semantic roles
The second stage in the annotation is alignment,
where the annotators match elements of the SRL
annotation in the reference with that in the MT
output. The annotators link both actions and roles,
and these alignments can be matched as ?Correct?
or ?Partial? matches, depending on how well the
action or role is translated. The guidelines for the
annotators are deliberately minimalistic, with the
argument being that non-experts can get started
quickly. Lo and Wu (2011a) claim that unskilled
annotators can be trained within 15 minutes.
In all such human evaluation, there is a trade-
off between simplicity and accuracy. Clearly when
evaluating bad machine translation output, we do
not want to label too much. However, sometimes
having so little choice of semantic roles can lead
to confusion and slow down the annotator when
more complicated examples do not fit the scheme.
Therefore, common exceptions need to be handled
either in the roles provided, or in the annotator
guidelines.
54
3.2 Calculation of Score
The overall HMEANT score for MT evaluation
is computed as the f-score from the counts of
matches of frames and their role fillers between
the reference and the MT output. Unmatched
frames are excluded from the calculation together
with all their corresponding roles.
In recognition that preservation of some types
of semantic relations may be more important than
others for a human to understand a sentence, one
may want to weight them differently in the com-
putation of the HMEANT score. Lo and Wu (2012)
train weights for each role filler type to optimise
correlation with human adequacy judgements. As
an unsupervised alternative, they suggest weight-
ing roles according to their frequency as approxi-
mation to their importance.
Since the main focus of the current paper is the
annotation of the actions, roles and alignments that
HMEANT depends on, we do not explore such dif-
ferent weight-setting schemes, but set the weights
uniformly, with the exception of a partial align-
ment, which is given a weight of 0.5. HMEANT is
thus defined as follows:
Fi = # correct or partially correct fillers
for PRED i in MT
MTi = total # fillers for PRED i in MT
REFi = total # fillers for PRED i in REF
P =
?
matched i
Fi
MTi
R =
?
matched i
Fi
REFi
Ptotal =
Pcorrect + 0.5Ppartial
total # predicates in MT
Rtotal =
Pcorrect + 0.5Ppartial
total # predicates in REF
HMEANT = 2 ? Ptotal ?RtotalPtotal +Rtotal
3.3 Automating HMEANT
One of the main directions taken by the authors of
HMEANT is in creating a fully automated version
of the metric (MEANT) in (Lo et al, 2012). The
metric combines shallow semantic parsing with a
simple maximum weighted bipartite matching al-
gorithm for aligning semantic frames. They use
approximate matching schemes (Cosine and Jac-
card similarity) for matching roles, with the lat-
ter producing better alignments (Tumuluru et al,
2012). They demonstrate that MEANT corre-
lates with human adequacy judgements better than
other commonly used automatic metrics. In this
paper we focus on human evaluation, as it is es-
sential for building better automatic metrics, and
therefore a more fundamental problem.
4 Experimental Setup
4.1 Systems and Data Sets
We performed HMEANT evaluation on three
systems selected from 2013 WMT evaluation1.
The systems we selected were uedin-wmt13,
uedin-syntax and rbmt-3, which were cho-
sen to provide us with a high performing phrase-
based system, a high performing syntax-based
system and the top performing rule-based system,
respectively. The cased BLEU scores of the three
systems are shown in Table 2.
System Type de-en en-de
uedin-wmt13 Phrase 26.6 20.1
uedin-syntax Syntax 26.3 19.4
rbmt-3 Rule 18.8 16.5
Table 2: Cased BLEU on the full newstest2013
test set for the systems used in this study
We randomly selected sentences from the en-de
and de-en newstest2013 tasks, and extracted
the corresponding references and system outputs
for these sentences. For the en-de task, 75% of our
selected sentences were selected from the section
of newstest2013 that was originally in Ger-
man, with the other 25% from the section that was
originally in English. The sentence selection for
the de-en task was performed in a similar man-
ner. For presentation to the annotators, the sen-
tences were split into segments of 12. We found
that with practice, annotators could complete one
of these segments in around 100-120 minutes. In
total, with close to 70 hours of annotator effort,
we evaluated 142 sentences of German, and 72
sentences of English. The annotation for each
sentence includes 1 reference, 3 system outputs,
and their corresponding alignments. Apart from 5
singly-annotated German sentences, and 1 singly-
annotated English sentence, all sentences were an-
notated by exactly 2 annotators.
1www.statmt.org/wmt13
55
4.2 Annotation
The annotation for English was performed by 3
different annotators (E1, E2 and E3), and the Ger-
man annotation by 2 annotators (D1 and D2).
All the English annotators were machine transla-
tion researchers, with E1 and E2 both native En-
glish speakers whereas E3 is not a native speaker,
but lives and works in an English-speaking coun-
try. The two German annotators were both native
speakers of German, with no background in com-
putational linguistics, although D2 is a teacher of
German as a second language and has had linguis-
tic training.
The HMEANT evaluation task was carried out
following the framework described in Lo and Wu
(2011a) and Bojar and Wu (2012). For each sen-
tence in the evaluation set, the annotators were first
asked to mark the semantic frames and roles (i.e.,
slot fillers within the frame) in a human reference
translation of the respective sentence. They were
then presented with the output of several machine
translation systems for the same source sentence,
one system at a time, with the reference transla-
tion and its annotations visible in the left half of
the screen (cf. Fig. 1). For each system, the an-
notators were asked to annotate semantic frames
and slot fillers in the translation first, and then
align them with frame heads and slot fillers in
the human reference translation. Annotations and
alignment were performed with Edi-HMEANT2,
a web-based annotation tool for HMEANT that
we developed on the basis of Yawat (Germann,
2008). The tool allows the alignment of slots from
different semantic frames, and the alignment of
slots of different types; however, such alignments
are not considered in the computation of the final
HMEANT score.
The annotation guidelines were essentially
those used in Bojar and Wu (2012), with some ad-
ditional English examples, and a complete set of
German examples. For ease of comparison with
prior work, we used the same set of semantic role
labels as Bojar and Wu (2012), shown in Table 1.
Given the restriction that the head of a frame can
consist of only one word, a convention was made
that all other verbs attached to the main verb such
as modals, auxiliaries or separable particles for
German verbs, would be labelled as modal. This
was the only change we made to the HMEANT
2Edi-HMEANT is part of the Edinburgh
Multi-text Annotation and Alignment Tool Suite
(http://www.statmt.org/edimtaats).
scheme.
5 Results and Discussion
5.1 Inter-Annotator Agreement
We first measured IAA on role identification, as
in Lo and Wu (2011a), except that we use exact
match on word spans as opposed to the approx-
imate match employed in that reference. Whilst
exact match is a harsher measure, penalising dis-
agreements related to punctuation and articles, us-
ing any sort of approximate match would mean
having to deal with N:M matches. IAA is defined
as follows:
IAA = 2 ? P ?RP +R
Where P is defined as the number of labels (ei-
ther heads, roles, or alignments) that match be-
tween annotators, divided by the total number of
labels given by annotator 1. And R is defined the
same way for annotator 2. This is similar to an
F-measure (f1), where we consider one of the an-
notators as the gold standard. The IAA for role
identification is shown in Table 3.
Reference Hypothesis
Lang. matches f1 matches f1
de 865 0.846 2091 0.737
en 461 0.759 1199 0.749
Table 3: IAA for role identification. This is calcu-
lated by considering exact endpoint matches on all
spans (predicates and arguments).
The agreements in Table 3 are not too differ-
ent from those reported in earlier work. We note
that the IAA for the German annotators drops for
the MT system outputs, but this may be because
the English annotators (as MT researchers) are less
bothered by bad MT output than their counterparts
working on the German texts.
Next we looked at the IAA on role classifica-
tion, the other IAA figure provided by Lo and Wu
(2011a). We only considered roles where both an-
notators had marked the same span in the same
frame, with the frame being identified by its ac-
tion. The IAA for role classification is shown in
Table 4.
Again, we show similar levels of IAA to those
reported in (Lo and Wu, 2011a). Examining the
disagreements in more detail, we produced counts
of the most common role type disagreements, by
56
Figure 1: Example of a sentence pair annotated with Edi-HMEANT. The reference translation is on
the left, the machine translation output on the right. Head and slot fillers for each semantic frame are
marked by selecting spans in the text and automatically listed in tables below the respective sentences.
Frames and slot fillers are aligned by clicking on table cells. The alignments of the semantic frames are
highlighted: green (grey in black and white version) for exact match and grey (light grey) for partial
match.
Reference Hypothesis
Lang. matches f1 matches f1
de 425 0.717 1050 0.769
en 245 0.825 634 0.826
Table 4: IAA for role classification. We only con-
sider cases where annotators had marked the same
span in the same frame.
Role 1 Role 2 Count
Agent Experiencer-Patient 110
Degree-Extent Modal 92
Beneficiary Experiencer-Patient 45
Experiencer-Patient Manner 26
Manner Other 25
Table 5: Most common role type disagreements,
for German
language. We show the top 5 disagreements in Ta-
bles 5 and 6. Essentially these show that the most
common role types provide the most confusions.
In order to shed more light on the role type dis-
agreements, we examined a random sample of 10
of the English annotations where the annotators
had disagreed about ?Agent? versus ?Experiencer-
Patient?. In 7 of these cases, there was a definite
correct answer, according to the annotation guide-
lines. Of the other 3, there were 2 cases of poor
MT output making the semantic interpretation dif-
ficult, and one case of existential ?there?. Of the 7
cases where one annotator appears in error, 3 were
passive, 1 was a copula, and 1 involved the verb
Role 1 Role 2 Count
Agent Experiencer-Patient 44
Manner Other 22
Degree-Extent Temporal 12
Degree-Extent Other 12
Beneficiary Experiencer-Patient 11
Table 6: Most common role type disagreements,
for English
?receive?. For the other 2 there was no clear rea-
son for the error. From this small sample, we sug-
gest that passive constructions are still difficult to
annotate semantically.
The last of elements of the semantic frames to
be considered for IAA are the actions, i.e. the
frame heads or predicates. In this case identifying
a match was straightforward as actions are identi-
fied by a single token. The IAA for action identi-
fication is shown in Table 7.
Reference Hypothesis
Lang. matches f1 matches f1
de 238 0.937 592 0.826
en 126 0.818 362 0.868
Table 7: IAA for action identification.
We see fairly high IAA for actions, which seems
encouraging, but given the importance of actions
in HMEANT, we probably need the scores to be
higher. Most of the problems with the identifica-
tion of actions centre around multiple-verb con-
structions and participles.
We now turn our attention to the second stage
of the annotation process where the annotators
marked alignments between slots and roles. These
provide the relevant statistics for the calculation of
the HMEANT score so it is important that they are
annotated reliably.
Firstly, we consider the alignment of actions. In
this case, we use pipelined statistics, in that if one
annotator marks actions in the reference and hy-
pothesis, then aligns them, whilst the other anno-
tator does not mark the corresponding actions, we
still count this as an action alignment mismatch.
This creates a harsher measure on action align-
ment, but gives a better idea of the overall relia-
bility of the annotation task. In Table 8 we show
the IAA (as F1) on action alignments. Comparing
Tables 8 and 7 we see that, for English at least, the
57
Lang. matches f1
de 300 0.655
en 275 0.769
Table 8: IAA for action alignment, collapsing par-
tial and full alignment
agreement on action alignment is not much lower
than that on action identification, indicating that if
annotators agree on the actions then they generally
agree on how they align. For German, however,
the IAA on action alignment is a bit lower, ap-
parently because one of the annotators was much
stricter about which actions they aligned.
In order to calculate the IAA on role align-
ments, we only consider those alignments that
connect two roles in aligned frames, of the same
type, since these are the only role alignments that
count for computing the HMEANT score. This
means that if one of the annotators does not align
the frames, then all the contained role alignments
are counted as mismatches. We do not consider
the spans when calculating the agreement on role
alignments, meaning that if one annotator has an
alignment between roles of type T in frame F ,
and the other annotator also aligns the same types
of roles in the same frame, then they are consid-
ered as a match. This is done because it is only the
counts of alignments that are relevant for HMEANT
scoring. The IAA on the role alignments is quite
Lang. matches f1
de 448 0.442
en 506 0.596
Table 9: IAA for role alignment.
low, dipping below 0.5 for German. This is mainly
because of the pipelining effect, where annota-
tion disagreements at each stage are compounded.
Since the final HMEANT score is computed essen-
tially by counting role alignments, this level of
IAA causes problems for this score calculation.
We computed HMEANT and BLEU scores for the
hypotheses annotated by each annotator pair. The
HMEANT scores were calculated as described in
Section 3.2. The two metrics are calculated for
each sentence (we apply +1 smoothing for BLEU),
then averaged across all sentences. Table 10 shows
the scores organised by annotator pair and sys-
tem type. The agreement in the overall scores is
not good, but really just reflects the compounded
Annotator System BLEU HMEANT HMEANT
Pair (Annot. 1) (Annot. 2)
Phrase 0.310 0.626 (2) 0.672 (3)
E1, E2 Syntax 0.291 0.635 (1) 0.730 (1)
Rule 0.252 0.578 (3) 0.673 (2)
Phrase 0.378 0.569 (1) 0.602 (3)
E1, E3 Syntax 0.376 0.553 (2) 0.627 (2)
Rule 0.320 0.546 (3) 0.646 (1)
Phrase 0.360 0.669 (2) 0.696 (3)
E2, E3 Syntax 0.362 0.751 (1) 0.739 (1)
Rule 0.308 0.624 (3) 0.716 (2)
Phrase 0.296 0.327 (1) 0.631 (3)
D1, D2 Syntax 0.321 0.312 (2) 0.707 (1)
Rule 0.242 0.274 (3) 0.648 (2)
Table 10: Scores assigned by each annotator pair.
The numbers in brackets after the HMEANT scores
show the relative ranking assigned by each anno-
tator.
agreement problems in the role alignments (Table
9). In no case do the annotators choose a consis-
tent ranking of the 3 systems, and in 2 of the 4 an-
notator pairs, the annotators disagree about which
is the top performing system.
5.2 Overall Scores
In this section we report the overall HMEANT
scores of the three systems whose output we an-
notated. Our main focus on this paper was on the
annotation task, so we do not wish to emphasise
the scoring, but it is nevertheless an important end-
product of the HMEANT annotation process. The
overall scores (HMEANT and +1 smoothed sen-
tence BLEU, averaged across sentences and anno-
tators) are given in Table 11.
Language System BLEU HMEANT
Phrase 0.351 0.634
en Syntax 0.344 0.667
Rule 0.295 0.625
Phrase 0.294 0.482
de Syntax 0.302 0.517
Rule 0.242 0.464
Table 11: Comparison of mean HMEANT and
(smoothed sentence) BLEU for the three systems.
From the table we can observe that, whilst
BLEU shows similar scores for the phrase-based
and syntax-based systems, with lower scores for
the rule-based system, HMEANT shows the syntax-
based system as being ahead, with the other two
showing similar performance. We would caution
against reading too much into this, considering the
relatively small number of sentences annotated,
58
and the issues with IAA exposed in the previous
section, but it is an encouraging results for syntax-
based MT.
5.3 Discussion
Machine translation research needs a reliable
method for evaluating and comparing different
machine translation systems. The performance of
HMEANT as shown in the previous section is dis-
appointing. The fact that the final role IAA, in Ta-
ble 9, is 0.442 for German and 0.596 for English,
demonstrates that there are fundamental problems
with the scheme. One of the areas of greatest con-
fusion is between what seems like one of the eas-
iest role types to distinguish: agent and patient.
Here is an example of a passive where one anno-
tator has marked ?tea? wrongly as agent, and the
other annotator correctly labelled it as patient:
Reference: In the kitchen, tea is prepared for
the guests
ACTION prepared
LOCATIVE In the kitchen
AGENT / PATIENT tea
MODAL is
BENEFICIARY for the guests
We would argue that the most important change
to HMEANT must be in creating more comprehen-
sive annotation guidelines, with examples of diffi-
cult cases. Bojar and Wu (2012) listed a number of
problems and improvements to HMEANT, which
we largely agree with. We list the most important
limitations of HMEANT that we have encountered:
? Single Word Heads Verbal predicates often
consist of multiple words, which can be split.
For example: ?Take him up on his offer?.
? Heads being limited to verbs The semantics
of verbs can often be carried by an equivalent
noun and should be allowed by HMEANT. For
example ?My father broke down and cried .?,
the verb ?cried? is correctly paraphrased in
?My father collapsed in tears .?
? Copular Verbs These do not fit in to the lim-
ited list of role types. For example forcing
this sentence ?The story is plausible?, to have
and agent and patient is confusing.
? Prepositional Phrases attaching to a noun
These can greatly affect the semantics of a
sentence, but HMEANT has no way of captur-
ing this.
? Semantics not on head This frequently oc-
curs with light verbs, for example ?Bouson
did the review of the paper? is equivalent to
?Bouson reviewed the paper?.
? Hierarchy of frames There are often frames
which are embedded in other frames, for ex-
ample in reported speech. It is not clear
whether errors at the lowest level should be
marked wrong just at that point, or whether
they should be marked wrong all the way up
the semantic tree. For example: ?Arafat said
?Isreal suffocates such a hope in the germ? ?.
The frame headed by ?said? is largely cor-
rect, but the reported speech is not. The pa-
tient role of the verb ?said? could be aligned
as correct, as the error is already captured in
relation to the verb ?suffocates?.
? No discourse markers These are impor-
tant for capturing the relationships between
frames and should be labelled.
6 Conclusion
HMEANT represents an attempt to create a human
evaluation for machine translation which directly
measures the semantic content preserved by the
MT. It partly succeeds. However we have cast
doubt on the claim that HMEANT can be reliably
annotated with minimal annotator training and
guidelines. In the most extensive study of inter-
annotator agreement yet performed for HMEANT,
across two language pairs, we have shown that the
disagreements between annotators make it diffi-
cult to reliably compare different MT systems with
HMEANT scores.
Furthermore, the fact that HMEANT is restricted
to annotating purely verbal predicates results in
some important disadvantages. Ideally we need a
more general definition of a frame, not restricted
to purely verbal predicates, and we would like
to be able to link frames. We should explore
the feasibility of a semantic framework which at-
tempts to overcome reliance on syntactic proper-
ties such as Universal Conceptual Cognitive An-
notation (Abend and Rappoport, 2013).
7 Acknowledgements
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement 287658 (EU BRIDGE).
59
References
Abend, Omri and Ari Rappoport. 2013. ?Univer-
sal Conceptual Cognitive Annotation (UCCA).?
Proceedings of ACL.
Bojar, Ondrej, Milos? Ercegovc?evic?, Martin Popel,
and Omar Zaidan. 2011. ?A Grain of Salt for the
WMT Manual Evaluation.? Proceedings of the
Sixth Workshop on Statistical Machine Transla-
tion, 1?11. Edinburgh, Scotland.
Bojar, Ondrej and Dekai Wu. 2012. ?Towards a
Predicate-Argument Evaluation for MT.? Pro-
ceedings of SSST, 30?38.
Callison-Burch, Chris, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
2007. ?(Meta-) evaluation of machine trans-
lation.? Proceedings of the Second Workshop
on Statistical Machine Translation, 136?158.
Prague, Czech Republic.
Callison-Burch, Chris, Philipp Koehn, Christof
Monz, and Omar F Zaidan. 2011. ?Findings of
the 2011 workshop on statistical machine trans-
lation.? Proceedings of the Sixth Workshop on
Statistical Machine Translation, 22?64.
Dreyer, Markus and Daniel Marcu. 2012. ?Hyter:
Meaning-equivalent semantics for translation
evaluation.? Proceedings of the 2012 Con-
ference of the North American Chapter of
the Association for Computational Linguis-
tics: Human Language Technologies, 162?171.
Montre?al, Canada.
Germann, Ulrich. 2008. ?Yawat: Yet Another
Word Alignment Tool.? Proceedings of the
ACL-08: HLT Demo Session, 20?23. Colum-
bus, Ohio.
Gime?nez, Jesu?s and Llu??s Ma`rquez. 2007. ?Lin-
guistic features for automatic evaluation of het-
erogenous mt systems.? Proceedings of the Sec-
ond Workshop on Statistical Machine Transla-
tion, StatMT ?07, 256?264. Stroudsburg, PA,
USA.
Hutchins, W. J. and H. L. Somers. 1992. An intro-
duction to machine translation. Academic Press
New York.
Jones, Bevan, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
?Semantics-based machine translation with hy-
peredge replacement grammars.? Proceedings
of COLING.
Koponen, Maarit. 2012. ?Comparing human per-
ceptions of post-editing effort with post-editing
operations.? Proceedings of the Seventh Work-
shop on Statistical Machine Translation, 181?
190. Montre?al, Canada.
Lavie, Alon and Michael Denkowski. 2009. ?The
METEOR metric for automatic evaluation of
machine translation.? Machine Translation.
Linguistic Data Consortium. 2002. ?Lin-
guistic data annotation specification: As-
sessment of fluency and adequacy in
Chinese-English translation.? http:
//projects.ldc.upenn.edu/TIDES/
Translation/TranAssessSpec.pdf.
Lo, Chi-kiu, Anand Karthik Tumuluru, and Dekai
Wu. 2012. ?Fully automatic semantic MT eval-
uation.? Proceedings of WMT, 243?252.
Lo, Chi-kiu and Dekai Wu. 2010. ?Evaluating
machine translation utility via semantic role la-
bels.? Proceedings of LREC, 2873?2877.
Lo, Chi-kiu and Dekai Wu. 2011a. ?MEANT : An
inexpensive , high-accuracy , semi-automatic
metric for evaluating translation utility via se-
mantic frames.? Proceedings of ACL, 220?229.
Lo, Chi-kiu and Dekai Wu. 2011b. ?Structured vs.
flat semantic role representations for machine
translation evaluation.? Proceedings of SSST,
10?20.
Lo, Chi-kiu and Dekai Wu. 2012. ?Unsupervised
vs. supervised weight estimation for semantic
MT evaluation metrics.? Proceedings of SSST,
49?56.
Lopez, Adam. 2012. ?Putting human assessments
of machine translation systems in order.? Pro-
ceedings of WMT, 1?9.
NIST. 2005. ?The 2005 NIST machine
translation evaluation plan (MT-05).?
http://www.itl.nist.gov/iad/
mig/tests/mt/2005/doc/mt05_
evalplan.v1.1.pdf.
Palmer, Martha, Daniel Gildea, and Paul Kings-
bury. 2005. ?The proposition bank: An anno-
tated corpus of semantic roles.? Computational
Linguistics, 31(1):71?106.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. ?BLEU: a method for au-
tomatic evaluation of machine translation.? Pro-
ceedings of the Association for Computational
Linguistics, 311?318. Philadelphia, USA.
Przybocki, Mark, Kay Peterson, Se?bastien Bron-
sart, and Gregory Sanders. 2009. ?The NIST
60
2008 metrics for machine translation challen-
geoverview, methodology, metrics, and results.?
Machine Translation, 23(2):71?103.
Snover, Matthew, Nitin Madnani, Bonnie Dorr,
and Richard Schwartz. 2009a. ?Fluency, ad-
equacy, or HTER? exploring different human
judgments with a tunable MT metric.? Proceed-
ings of the Workshop on Statistical Machine
Translation at the Meeting of the European
Chapter of the Association for Computational
Linguistics (EACL-2009). Athens, Greece.
Snover, Matthew, Nitin Madnani, Bonnie Dorr,
and Richard Schwartz. 2009b. ?TER-plus:
paraphrase, semantic, and alignment enhance-
ments to translation edit rate.? Machine Trans-
lation.
Tumuluru, Anand Karthik, Chi-kiu Lo, and Dekai
Wu. 2012. ?Accuracy and robustness in measur-
ing the lexical similarity of semantic role fillers
for automatic semantic MT evaluation.? Pro-
ceedings of PACLIC, 574?581.
Weaver, Warren. 1955. ?Translation.? William N.
Locke and Andrew D. Booth (eds.), Machine
Translation of Languages; Fourteen Essays,
15?23. Cambridge, MA: MIT Press. Reprint of
a memorandum written in 1949.
61
