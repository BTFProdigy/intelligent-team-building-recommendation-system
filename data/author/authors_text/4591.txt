Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 65?68,
Paris, October 2009. c?2009 Association for Computational Linguistics
Deductive Parsing with Interaction Grammars
Joseph Le Roux
NCLT, School of Computing,
Dublin City University
jleroux@computing.dcu.ie
Abstract
We present a parsing algorithm for In-
teraction Grammars using the deductive
parsing framework. This approach brings
new perspectives to this problem, depart-
ing from previous methods which rely on
constraint-solving techniques.
1 Introduction
An Interaction Grammar (IG) (Guillaume and Per-
rier, 2008) is a lexicalized grammatical formal-
ism that primarily focuses on valency, explicitly
expressed using polarities decorating syntagms.
These polarities and the use of underspecified
structures naturally lead parsing to be viewed as
a constraint-solving problem ? for example (Bon-
fante et al) reduce the parsing problem to a graph-
rewriting problem in (2003) .
However, in this article we depart from this ap-
proach and present an algorithm close to (Earley,
1970) for context-free grammars. We introduce
this algorithm using the standard framework of de-
ductive parsing (Shieber et al, 1995).
This article is organised as follows: we first
present IGs (section 2), then we describe the algo-
rithm (section 3). Finally we discuss some techni-
cal points and conclude (sections 4 and 5).
2 Interaction Grammars
We briefly introduce IGs as in (Guillaume and Per-
rier, 2008)1. However, we omit polarized feature
structures, for the sake of exposition.
2.1 Polarized Tree Descriptions
The structures associated with words by the lexi-
con are Polarized Tree Descriptions (PTDs). They
represent fragments of parse trees. The nodes of
these structures are labelled with a category and a
1This paper also discusses the linguistic motivations be-
hind IGs.
polarity. IGs use 4 polarities, P = {?,?,=,?},
namely positive, negative, neutral and virtual.
A multiset of polarities is superposable2 if it
contains at most one? and at most one?.
A multiset of polarities is saturated if it contains
either (1) one?, one? and any number of? and
=, or (2) zero?, zero?, any number of? and at
least one =.
The two previous definitions can be extended to
nodes: a multiset of nodes is saturated (resp. su-
perposable) if all the elements have the same cat-
egory and if the induced multiset of polarities is
saturated (resp. superposable).
A PTD is a DAG with four binary relations: the
immediate dominance >, the general dominance
>?, the immediate precedence ? and the general
precedence ?+. A valid PTD is a PTD where (1)
> and >? define a tree structure3 , (2) ? and ?+
are restricted to couples of nodes having the same
ancestor by >, and (3) one leaf is the anchor. In
the rest of this paper, all PTDs will be valid.
We now introduce some notations : if n >?
m, we say that m is constrained by n and for a
set of nodes N , we define Nz = {N |?M ?
N ,MzN} wherez is a binary relation.
2.2 Grammars
An IG is a tuple G = {?,C, S,P, phon}, where ?
is the terminal alphabet, C the non-terminal alpha-
bet, S ? C the initial symbol, P is a set of PTDs
with node labels in C? P, and phon is a function
from anchors in P to ?.
The structure obtained from parsing is a syntac-
tic tree, a totally ordered tree in which all nodes
are labelled with a non-terminal. We call lab(A)
the label of node A. If a leaf L is labelled with a
terminal, this terminal is denoted word(L).
2This name comes from the superposition introduced in
previous presentations of IGs.
3For readers familiar with D-Tree Grammars (Rambow et
al., 1995), > adds an i-edge while >? adds a d-edge.
65
We will write M  N if the node M is
the mother of N and N  [N1, . . . , Nk] if the
N is the mother of the ordered list of nodes
[N1, . . . , Nk]. The order between siblings can also
be expressed using the relation ??: M ?? N
means that N is the immediate successor of M .
??+ is the transitive closure of ?? and ? the
reflexive transitive closure of.
We define the phonological projection PP
of a node as : PP (M) = [t] if M 
[] and word(M) = t, or PP (M) =
[PP (N1) . . . PP (Nk)] if M  [N1, . . . , Nk]
A syntactic tree T is a model for a multisetD of
PTDs if there exists a total function I from nodes
in D (ND) to nodes in T (NT ). I must respect
the following conditions, where variables M,N
range over ND and A,B over NT :
1. I?1(A) is saturated and non-empty.
2. if M > N then I(M) I(N)
3. if M >? N then I(M)? I(N)
4. if M ? N then I(M) ?? I(N)
5. if M ?+ N then I(M) ??+ I(N)
6. if A B then there exists M ? I?1(A) and
N ? I?1(B) such that M > N
7. lab(A) = lab(M) for all M ? I?1(A)
8. if phon(M) = w then PP (I(M)) = [w]
Given an IG G = {?,C, S,P, phon} and a sen-
tence s = w1, . . . , wn in ??, a syntactic tree T is
a parse tree for s if there exists a multiset of PTDs
D from P such that the root node R of T is la-
belled with S and PP (R) = [w1, . . . , wn]. The
language generated by G is the set of strings in ??
for which there is a parse tree.
3 Parsing Algorithm
We use the deductive parsing framework (Shieber
et al, 1995). A state of the parser is encoded as an
item, created by applying deductive rules. Our al-
gorithm resembles the Earley algorithm for CFGs
and uses the same rules : prediction, scanning and
completion.
3.1 Items
Items [A(H,N,F ) ? ? ? ?, i, j, (O,U,D)] con-
sist of a dotted rule, 2 position indexes and a 3-
tuple of sets of constrained nodes.
The dotted rule A(H,N,F ) ? ? ? ? means
that there exists a node A in the parse tree with
antecedentsH ?N ?F . Elements of the sequence
? are also nodes of the parse tree. For the sequence
?, the elements have the form Bk(Hk) where Bk
is a node of the parse tree and Hk is a subset of its
antecedents, the predicted antecedents.
This item asserts that a syntactic tree can be par-
tially built from the input grammar and sentence,
that contains A  [A1 . . . AkB1 . . . Bl] and that
PP (A1) ? ? ? ? ? PP (Ak) = [mi+1 . . .mj ].
The proper use of constrained nodes is managed
by O, U and D:
? Nodes in D are available in prediction to find
antecedents for new parse tree nodes.
? Nodes in O must be used in a sub-parse. To
use an item as a completer, O must be empty.
? U contains constrained nodes that have been
used in a prediction, and for which the con-
straining nodes have not been completed yet.
Moreover, we will use 3 additional symbols: >
as the left-hand side of the axiom item which can
be seen as a dummy root, and  or  that mark
items for which prediction is not terminated.
We will need sequences of antecedents that re-
spect the order relations of an IG. Given a set of
nodes N , we define the set of all these orderings:
ord(N ) = {[N1 . . .Nk]|
(Ni)1?i?k is a partition of N?
1 ? i ? k,Ni is superposable ?
if n1, n2 ? N and n1 ? n2 then
?1 ? j < k s.t. n1 ? Nj and n2 ? Nj+1?
if n1, n2 ? N and n1 ?+ n2 then
?1 ? i < j ? k s.t. n1 ? Ni and n2 ? Nj}
3.2 Deductive Rules
In this section, we assume an input sentence s =
w1, . . . , wn and a IG G = {?,C, S,P, phon}.
Axiom This rule creates the first item. It pre-
pares the prediction of a node of category S start-
ing at position 0 without constrained nodes.
[> ? ?S(?), 0, 0, (?, ?, ?)]ax
66
Prediction This rule initializes a sub-parse. We
divide it in three in order to introduce the different
constraints one at a time.
[A(H,N,F )? ? ? C(HC)?, i, j, (O,U,D)]
[C(HC , ?, ?)? , j, j, (?, U,D ?O)] p1
In this first step, we initialize a new sub-parse
at the current position j where C will be the pre-
dicted node that we want to find antecedents for. If
some antecedentsHC have already been predicted
we use them. The nodes in O, that must be used
in one of the sub-parse of A, become available as
possible antecedents for C.
[C(HC , ?, ?)? , j, j, (?, U1, D1)]
[C(HC , NC , ?)? , j, j, (?, U2, D2)]p2
?
?????
?????
HC ?NC 6= ?
HC ?NC is superposable
NC ? D1 ? roots(P)
D2 = D1 ?NC
U2 = U1 ? (D1 ?NC)
In this second step, new antecedents for C are
added from the set NC , chosen among available
nodes in D1 and root nodes from the PTDs of
the grammar. The 3 node sets are then updated.
Constrained nodes that have been chosen as an-
tecedents for C are not available anymore and are
added to the set of used constrained nodes.
[C(HC , NC , ?)? , j, j, (?, U,D)]
[C(HC , NC , FC)? ??, j, j, (O,U,D)]p3
?
?????
?????
HC ?NC ? FC is saturated
? ? ord((HC ?NC ? FC)>)
FC = ?iQi, Q0 ? (HC ?NC)>? , Qi+1 ? Q>?i
O = (HC ?NC ? FC)>? ? FC
no anchor node in HC ?NC ? FC
In this last step of prediction, we can choose
new antecedents for C among nodes constrained
by antecedents already chosen in the previous
steps in order to saturate them. This choice is re-
cursive : each added antecedent triggers the pos-
sibility of choosing the nodes it constrains. The
second part of this step consists of predicting the
shape of the tree. We need to order and superpose
the daughter nodes of the antecedents in such a
way that ordering relations in PTDs are respected:
an element of ord((HC ?NC ? FC)>) is chosen.
Finally, the nodes that must be used in a sub-
parse are the ones that are constrained by an-
tecedents of C and not antecedents themselves.
Scan This is the rule that checks predictions
against the input string. It is similar to the previ-
ous rule, but one (and only one) of the antecedents
must be an anchor.
[C(HC , NC , ?)? , j, j, (?, U,D)]
[C(HC , NC , FC)? ?, j, j + 1, (?, U,D)]s
?
???????
???????
HC ?NC ? FC is saturated
(HC ?NC ? FC)> = ?
FC = ?iQi, Q0 ? (HC ?NC)>? , Qi+1 ? Q>?i(HC ?NC ? FC)>? ? FC = ?
one anchor a in HC ?NC ? FC
phon(a) = wj+1
If the expected terminal is read on the input
string, parsing can proceed. Note that antecedents
for C should not constrain nodes that are not an-
tecedents of C themselves.
Completion This rule extends a parse by com-
bining it with a complete sub-parse.
[A(H,N,F )? ? ? C(Hc)?, i, j, (O1, U1, D1)]
[C(HC , NC , FC)? ??, j, k, (?, U2, D2)]
[A(H,N,F )? ?C ? ?, i, k, (O3, U3, D3)]
c
?
???????
???????
NC ? D1 ?O1 ? P
D2 ? (D1 ?O1)?NC
U1 ? U2
O3 = O1 ? U2
D3 = D1 ? U2
U3 = U2 ?O1
We have to make sure that the second hypothe-
sis is a sub-parse for the first : (1) the set of avail-
able nodes in the sub-parse must be a subset of
the available nodes for current parse, (2) the set of
used nodes in the main parse must be a subset of
the used nodes in the sub-parse and (3) used nodes
constrained by the first hypothesis disappear.
Goal Parsing is successful if the following item
is created : [> ? S?, 0, n, (?, ?, ?)].
4 Discussion
4.1 Consistency and completeness
An item [A(H,N,F )? ???, i, j, (O,U,D)] as-
serts the following invariants :
67
? A and the elements ?l of ? are models for
saturated sets of nodes. Conditions 1, 7 and 3
(reflexive case) of a model are respected.
? Elements ?k of ? are superposable. Then we
have ?k ? (A?1)> (conditions 2 and 6).
? the sequence ?? is compatible with the order
relations from the PTDs (conditions 4 and 5).
? PP (?1) ? . . . ? PP (?l) = [wi+1 ? ? ?wj ]
? a node N in U is a constrained node in re-
lation >? with a node such that condition 3
holds.
These invariants can be checked by induction
on rules. Hence, such an item asserts there exists a
function J from the nodes of a subset of the PTDs
of an IG to a syntactic tree with its root labelled
by S and phonological projection w1 . . . wj . This
function has the same properties as the function I
for models but conditions 2 to 5 only apply if both
nodes are in the domain of J . The parsing process
extends the domain until (1) all the nodes of each
PTD selected are used and (2) the input string has
been read completely. Then J defines a syntactic
tree which is a parse tree.
4.2 Sources of non-determinism
The parsing problem in IGs is a NP-hard prob-
lem (Bonfante et al, 2003). Our presentation lets
us see several sources of non-determinism.
In p2, new antecedents are chosen among avail-
able nodes and root nodes of PTDs from the in-
put grammar. There is an exponential number of
such choices. However, IGs are lexicalized : only
PTDs associated with a word in the sentence will
be used and efficient lexical filters have been de-
veloped for IGs (Bonfante et al, 2006) that drasti-
cally decrease the number of PTDs to consider.
In p3 and s, constrained nodes can be chosen as
antecedents (nodes in FC). There is again an ex-
ponential number of such choices. But in existing
IGs, nodes have at most one successor by >? and
there is no chain of nodes in relation by >?. Con-
sequently, |FC | can be bounded by |HC ?NC |.
In p3, daughters must be partitioned. Instead of
building all these partitions in p3 and generating
many useless items, one can think of a lazy ap-
proach like the one proposed by (Nederhof et al,
2003) for pomset-CFGS.
It can be noticed that the completion rule, while
having the most positional indexes, is not a partic-
ular source of non-determinism.
5 Conclusion
We presented a parsing algorithm for IGs. Al-
though we used a simplified version without polar-
ized feature structures, adding a unification mech-
anism shouldn?t be an issue. The novelty of
this presentation is the use of deductive parsing
for a formalism developed in the model-theoretic
framework (Pullum and Scholz, 2001).
This change of perspective provides new in-
sights on the causes of non-determinism. It is
a first step to a precise complexity study of the
problem. In the future, it will be interesting to
search for algorithmical approximations to im-
prove efficiency. Another way to overcome NP-
hardness is to restrict superpositions, as in (k-)TT-
MCTAGs (Kallmeyer and Parmentier, 2008).
References
G. Bonfante, B. Guillaume, and G. Perrier. 2003.
Analyse syntaxique e?lectrostatique. Traitement Au-
tomatique des Langues, 44(3).
G. Bonfante, J. Le Roux, and G. Perrier. 2006. Lexi-
cal disambiguation with polarities and automata. In
Proceedings of CIAA.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94?102.
B. Guillaume and G. Perrier. 2008. Interaction Gram-
mars. Research Report RR-6621, INRIA.
L. Kallmeyer and Y. Parmentier. 2008. On the relation
between TT-MCTAG and RCG. In Proceedings of
LATA.
M.J. Nederhof, G. Satta, and S. Shieber. 2003. Par-
tially ordered multiset context-free grammars and
ID/LP parsing. In Proceedings of IWPT.
G. Pullum and B. Scholz. 2001. On the distinction be-
tween model-theoretic and generative-enumerative
syntactic frameworks. In Proccedings of LACL.
O. Rambow, K. Vijay-Shanker, and D. Weir. 1995. D-
tree grammars. In Proceedings of ACL.
S. Shieber, Y. Schabes, and F. Pereira. 1995. Principles
and implementation of deductive parsing. Journal of
Logic Programming, 24(1?2):3?36.
68
Coling 2008: Companion volume ? Posters and Demonstrations, pages 153?156
Manchester, August 2008
A Toolchain for Grammarians
Bruno Guillaume
LORIA
INRIA Nancy Grand-Est
Bruno.Guillaume@loria.fr
Joseph Le Roux
LORIA
Nancy Universit?e
Joseph.Leroux@loria.fr
Jonathan Marchand
LORIA
Nancy Universit?e
Jonathan.Marchand@loria.fr
Guy Perrier
LORIA
Nancy Universit?e
Guy.Perrier@loria.fr
Kar
?
en Fort
LORIA
INRIA Nancy Grand-Est
Karen.Fort@loria.fr
Jennifer Planul
LORIA
Nancy Universit?e
Jennifer.Planul@loria.fr
Abstract
We present a chain of tools used by gram-
marians and computer scientists to develop
grammatical and lexical resources from
linguistic knowledge, for various natural
languages. The developed resources are
intended to be used in Natural Language
Processing (NLP) systems.
1 Introduction
We put ourselves from the point of view of re-
searchers who aim at developing formal grammars
and lexicons for NLP systems, starting from lin-
guistic knowledge. Grammars have to represent all
common linguistic phenomena and lexicons have
to include the most frequent words with their most
frequent uses. As everyone knows, building such
resources is a very complex and time consuming
task.
When one wants to formalize linguistic knowl-
edge, a crucial question arises: which mathemat-
ical framework to choose? Currently, there is no
agreement on the choice of a formalism in the sci-
entific community. Each of the most popular for-
malisms has its own advantages and drawbacks. A
good formalism must have three properties, hard to
conciliate: it must be sufficiently expressive to rep-
resent linguistic generalizations, easily readable by
linguists and computationally tractable. Guided
by those principles, we advocate a recent formal-
ism, Interaction Grammars (IGs) (Perrier, 2003),
the goal of which is to synthesize two key ideas,
expressed in two kinds of formalisms up to now:
using the resource sensitivity of natural languages
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
as a principle of syntactic composition, which is
a characteristic feature of Categorial Grammars
(CG) (Retor?e, 2000), and viewing grammars as
constraint systems, which is a feature of unifica-
tion grammars such as LFG (Bresnan, 2001) or
HPSG (Pollard and Sag, 1994).
Researchers who develop large lexicons and
grammars from linguistic knowledge are con-
fronted to the contradiction between the necessity
to choose a specific grammatical framework and
the cost of developing resources for this frame-
work. One of the most advanced systems de-
voted to such a task is LKB (Copestake, 2001).
LKB allows grammars and lexicons to be devel-
oped for different languages, but only inside the
HPSG framework, or at most a typed feature struc-
ture framework. Therefore, all produced resources
are hardly re-usable for other frameworks. Our
goal is to design a toolchain that is as much as pos-
sible re-usable for other frameworks than IG.
Our toolchain follows the following architecture
(see Figure 1):
? First, for building grammars, we use XMG
(Section 3.1) which translates the source
grammar into an object grammar.
? IGs that we have developed with XMG are
all lexicalized. Therefore, the object grammar
has to be anchored in a lexicon (Section 3.2)
in order to produce the anchored grammar.
? Then, when analyzing a sentence, we start
with a lexical disambiguation module (Sec-
tion 3.3).
? The resulting lexical selections, presented in
the compact form of an automaton, are finally
sent to the LEOPAR parser (Section 3.4).
153
LEOPAR
source grammar
XMG
object grammar lexicons
anchoring
input sentence
lexical disambiguation
output parse trees
anchored grammar
automaton
parsing
Figure 1: Toolchain architecture
2 Interaction Grammars
IGs (Perrier, 2003) are a grammatical formalism
based on the notion of polarity. Polarities express
the resource sensitivity of natural languages by
modeling the distinction between saturated and un-
saturated syntactic structures. Syntactic composi-
tion is represented as a chemical reaction guided
by the saturation of polarities. In a more precise
way, syntactic structures are underspecified trees
equipped with polarities expressing their satura-
tion state. They are superposed under the con-
trol of polarities in order to saturate them. In
CG, Tree Adjoining Grammars (TAGs) and De-
pendency Grammars, syntactic composition can
also be viewed as a mechanism for saturating po-
larities, but this mechanism is less expressive be-
cause node merging is localized at specific places
(root nodes, substitution nodes, foot nodes, ad-
junction nodes . . .). In IGs, tree superposition is
a more flexible way of realizing syntactic compo-
sition. Therefore, it can express sophisticated con-
straints on the environment in which a polarity has
to be saturated. From this angle, IGs are related
to Unification Grammars, such as HPSG, because
tree superposition is a kind of unification, but with
an important difference: polarities play an essen-
tial role in the control of unification.
3 Description of the Toolchain
3.1 The XMG Grammar Compiler
The first piece of software in our toolchain is
XMG
1
(Duchier et al, 2004), a tool used to de-
velop grammars. XMG addresses the issue of de-
signing wide-coverage grammars: it is based on a
distinction between source grammar, written by a
human, and object grammar, used in NLP systems.
XMG provides a high level language for writing
source grammars and a compiler which translates
those grammars into operational object grammars.
XMG is particularly adapted to develop lexical-
ized grammars. In those grammars, parsing a sen-
tence amounts to combining syntactical items at-
tached to words. In order to have an accurate lan-
guage model, it may be necessary to attach a huge
number of syntactical items to some words (verbs
and coordination words, in particular) that describe
the various usages of those words. In this context,
a grammar is a collection of items representing
syntactical behaviors. Those items, although dif-
ferent from each other, often share substructures
(for instance, almost all verbs have a substruc-
ture for subject verb agreement). That is to say,
if a linguist wants to change the way subject-verb
agreement is modeled, (s)he would have to mod-
ify all the items containing that substructure. This
is why designing and maintaining strongly lexical-
ized grammars is a difficult task.
The idea behind the so-called metagrammati-
cal approach is to write only substructures (called
fragments) and then add rules that describe the
combinations (expressed with conjunctions, dis-
junctions and unifications) of those fragments to
obtain complete items.
Fragments may contain syntactic, morpho-
syntactic and semantic pieces of information. An
object grammar is a set of structures containing
syntactic and semantic information, that can be an-
chored using morpho-syntactic information stored
in the interface of the structure (see Section 3.2).
During development and debugging stages, por-
tions of the grammar can be evaluated indepen-
dently. The grammar can be split into various mod-
ules that can be shared amongst grammars. Fi-
nally, graphical tools let the users explore the in-
heritance hierarchy and the partial structures be-
fore complete evaluation.
1
XMG is freely available under the CeCILL license at
http://sourcesup.cru.fr/xmg
154
XMG is also used to develop TAGs (Crabb?e,
2005) and it can be easily extended to other gram-
matical frameworks based on tree representations.
3.2 Anchoring the Object Grammar with a
Lexicon
The tool described in the previous section builds
the set of elementary trees of the grammar. The
toolchain includes a generic anchoring mechanism
which allows to use formalism independent lin-
guistic data for the lexicon part.
Each structure produced by XMG comes with
an interface (a two-level feature structure) which
describes morphological and syntactical con-
straints used to select words from the lexicon. Du-
ally, in the lexicon, each inflected form of the natu-
ral language is described by a set of two-level fea-
ture structures that contain morphological and syn-
tactical information.
If the interface of an unanchored tree unifies
with some feature structure associated with w in
the lexicon, then an anchored tree is produced for
the word w.
The toolchain also contains a modularized lexi-
con manager which aims at easing the integration
of external and formalism independent resources.
The lexicon manager provides several levels of lin-
guistic description to factorize redundant data. It
also contains a flexible compilation mechanism to
improve anchoring efficiency and to ease lexicon
debugging.
3.3 Lexical Disambiguation
Neutralization of polarities is the key mechanism
in the parsing process as it is used to control syn-
tactic composition. This principle can also be used
to filter lexical selections. For a input sentence, a
lexical selection is a choice of an elementary tree
from the anchored grammar for each word of the
sentence.
Indeed, the number of possible lexical selec-
tions may present an exponential complexity in
the length of the sentence. A way of filter-
ing them consists in abstracting some information
from the initial formalism F to a new formalism
F
abs
. Then, parsing in F
abs
allows to eliminate
wrong lexical selections at a minimal cost (Boul-
lier, 2003). (Bonfante et al, 2004) shows that po-
larities allow original methods of abstraction.
Following this idea, the lexical disambiguation
module checks the global neutrality of every lex-
ical selection for each polarized feature: a set of
trees bearing negative and positive polarities can
only be reduced to a neutral tree if the sum of the
negative polarities for each feature equals the sum
of its positive polarities.
Counting the sum of positive and negative fea-
tures can be done in a compact way by using an au-
tomaton. This automaton structure allows to share
all paths that have the same global polarity bal-
ance (Bonfante et al, 2004).
3.4 The LEOPAR Parser
The next piece of software in our toolchain is a
parser based on the IGs formalism
2
. In addition
to a command line interface, the parser provides
an intuitive graphical user interface. Parsing can
be highly customized in both modes. Besides, the
processed data can be viewed at each stage of the
analysis via the interface so one can easily check
the behavior of the grammar and the lexicons in
the parsing process.
The parsing can also be done manually: one first
chooses a lexical selection of the sentence given
by the lexer and then proceeds to the analysis by
neutralizing nodes from the selection. This way,
the syntactic composition can be controlled by the
user.
4 Results
Our toolchain has been used first to produce a large
coverage French IG. Most of the usual syntactical
constructions of French are covered. Some non
trivial constructions covered by the grammar are,
for instance: coordination, negation (in French,
negation is expressed with two words with com-
plex placement rules), long distance dependencies
(with island constraints). The object grammar con-
tains 2,074 syntactic structures which are produced
by 455 classes in the source grammar.
The French grammar has been tested on the
French TSNLP (Test Suite for the Natural Lan-
guage Processing) (Lehmann et al, 1996); this test
suite contains around 1,300 grammatical sentences
and 1,600 ungrammatical ones. The fact that our
grammar is based on linguistic knowledge ensures
a good coverage and greatly limits overgeneration:
88% of the grammatical sentences are correctly
parsed and 85% of the ungrammatical sentences
are rejected by our grammar.
2
LEOPAR is freely available under the CeCILL license at
http://www.loria.fr/equipes/calligramme/
leopar
155
A few months ago, we started to build an En-
glish IG. The modularity of the toolchain was an
advantage to build this grammar by abstracting the
initial grammar and then specifying the abstract
kernel for English. The English TSNLP has been
used to test the new grammar: 85% of the gram-
matical sentences are correctly parsed and 84%
of the ungrammatical sentences are rejected. It is
worth noting that those scores are obtained with a
grammar that is still being developed .
5 Future work
The toolchain we have presented here aims at pro-
ducing grammars and lexicons with large cover-
age from linguistic knowledge. This justifies the
choice of discarding statistical methods in the first
stage of the toolchain development: in the two
steps of lexical disambiguation and parsing, we
want to keep all possible solutions without dis-
carding even the less probable ones. Now, in a
next future, we have the ambition of using the
toolchain for parsing large raw corpora in differ-
ent languages.
For French, we have a large grammar and a large
lexicon, which are essential for such a task. The in-
troduction of statistics in the two modules of lexi-
cal disambiguation and parsing will contribute to
computational efficiency. Moreover, we have to
enrich our parsing strategies with robustness. We
also ambition to integrate semantics into grammars
and lexicons.
Our experience with English is a first step to take
multi-linguality into account. The crucial point
is to make our grammars evolve towards an even
more multi-lingual architecture with an abstract
kernel, common to different languages, and differ-
ent specifications of this kernel for different lan-
guages, thus following the approach of the Gram-
matical Framework (Ranta, 2004).
Finally, to make the toolchain evolve towards
multi-formalism, it is first necessary to extend
XMG for more genericity; there is no fundamental
obstacle to this task. Many widespread formalisms
can then benefit from our original methods of lex-
ical disambiguation and parsing, based on polari-
ties. (Kahane, 2006) presents the polarization of
several formalisms and (Kow, 2007) shows that
this way is promising.
References
Bonfante, G., B. Guillaume, and G. Perrier. 2004. Po-
larization and abstraction of grammatical formalisms
as methods for lexical disambiguation. In CoL-
ing?2004, 2004, pages 303?309, Geneva, Switzer-
land.
Boullier, P. 2003. Supertagging: A non-statistical
parsing-based approach. In IWPT 03, pages 55?65,
Nancy, France.
Bresnan, J. 2001. Lexical-Functional Syntax. Black-
well Publishers, Oxford.
Copestake, A. 2001. Implementing Typed Feature
Structure Grammars. CSLI Publications.
Crabb?e, B. 2005. Repr?esentation informatique de
grammaires fortement lexicalis?ees : application `a la
grammaire d?arbres adjoints. Phd thesis, Universit?e
Nancy 2.
Duchier, D., J. Le Roux, and Y. Parmentier. 2004.
The metagrammar compiler : A NLP Application
with a Multi-paradigm Architecture. In Second
International Mozart/Oz Conference - MOZ 2004,
Charleroi, Belgium.
Kahane, S. 2006. Polarized unification grammars.
In 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 137?144,
Sydney, Australia.
Kow, E. 2007. Surface realisation: ambiguity and de-
terminism. Phd thesis, Universit?e Nancy 2.
Lehmann, S., S. Oepen, S. Regnier-Pros, K. Netter,
V. Lux, J. Klein, K. Falkedal, F. Fouvry, D. Estival,
E. Dauphin, H. Compagnion, J. Baur, L. Balkan, and
D. Arnold. 1996. TSNLP ? Test Suites for Natu-
ral Language Processing. In CoLing 1996, Kopen-
hagen.
Perrier, G. 2003. Les grammaires d?interaction. Ha-
bilitation thesis, Universit?e Nancy 2.
Pollard, C.J. and I.A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Ranta, A. 2004. Grammatical Framework: A Type-
Theoretical Grammar Formalism. Journal of Func-
tional Programming, 14(2):145?189.
Retor?e, C. 2000. The Logic of Categorial Grammars.
ESSLI?2000, Birmingham.
156
XMG - An expressive formalism for describing tree-based grammars
Yannick Parmentier
INRIA / LORIA
Universite? Henri Poincare?
615, Rue du Jardin Botanique
54 600 Villers-Les-Nancy
France
parmenti@loria.fr
Joseph Le Roux
LORIA
Institut National
Polytechnique de Lorraine
615, Rue du Jardin Botanique
54 600 Villers-Les-Nancy
France
leroux@loria.fr
Beno??t Crabbe?
HCRC / ICCS
University of Edinburgh
2 Buccleuch Place
EH8 9LW,
Edinburgh, Scotland
bcrabbe@inf.ed.ac.uk
Abstract
In this paper1 we introduce eXtensible
MetaGrammar, a system that facilitates
the development of tree based grammars.
This system includes both (1) a formal lan-
guage adapted to the description of lin-
guistic information and (2) a compiler for
this language. It applies techniques of
logic programming (e.g. Warren?s Ab-
stract Machine), thus providing an effi-
cient and theoretically motivated frame-
work for the processing of linguistic meta-
descriptions.
1 Introduction
It is well known that grammar engineering is a
complex task and that factorizing grammar in-
formation is crucial for the rapid development,
the maintenance and the debugging of large scale
grammars. While much work has been deployed
into producing such factorizing environments for
standard unification grammars, less attention has
been paid to the issue of developing such environ-
ments for ?tree based grammars? that is, grammars
like Tree Adjoining Grammars (TAG) or Tree De-
scription Grammars where the basic unit of infor-
mation is a tree rather than a category encoded in
a feature structure.
For these grammars, two trends have emerged
to automatize tree-based grammar production:
systems based on lexical rules (see (Becker,
2000)) and systems based on combination of
classes (also called metagrammar systems, see
(Candito, 1999), (Gaiffe et al, 2002)).
1We are grateful to Claire Gardent for useful comments
on this work. This work is partially supported by an INRIA
grant.
In this paper, we present a metagrammar system
for tree-based grammars which differs from com-
parable existing approaches both linguistically and
computationally.
Linguistically, the formalism we introduce is
both expressive and extensible. In particularly, we
show that it supports the description and factor-
ization both of trees and of tree descriptions; that
it allows the synchronized description of several
linguistic dimensions (e.g., syntax and semantics)
and that it includes a sophisticated treatment of
the interaction between inheritance and identifier
naming.
Computationally, the production of a grammar
from a metagrammar is handled using power-
ful and well-understood logic programming tech-
niques. A metagrammar is viewed as an extended
definite clause grammar and compiled using a vir-
tual machine closely resembling the Warren?s Ab-
stract Machine. The generation of the trees satisfy-
ing a given tree description is furthermore handled
using a tree description solver.
The paper is structured as follows. We begin
(section 2) by introducing the linguistic formal-
ism used for describing and factorizing tree based
grammars. We then sketch the logic program-
ming techniques used by the metagrammar com-
piler (section 3). Section 4 presents some evalu-
ation results concerning the use of the system for
implementing different types of tree based gram-
mars. Section 5 concludes with pointers for fur-
ther research and improvements.
2 Linguistic formalism
As mentioned above, the XMG system produces a
grammar from a linguistic meta-description called
a metagrammar. This description is specified us-
ing the XMG metagrammar formalism which sup-
103
ports three main features:
1. the reuse of tree fragments
2. the specialization of fragments via in-
heritance
3. the combination of fragments by
means of conjunctions and disjunctions
These features reflect the idea that a metagrammar
should allow the description of two main axes: (i)
the specification of elementary pieces of informa-
tion (fragments), and (ii) the combination of these
to represent alternative syntactic structures.
Describing syntax In a tree-based metagram-
mar, the basic informational units to be handled
are tree fragments. In the XMG formalism, these
units are put into classes. A class associates a
name with a content. At the syntactic level, a con-
tent is a tree description2 . The tree descriptions
supported by the XMG formalism are defined by
the following tree description language:
Description ::= x ? y | x ?+ y | x ?? y |
x ? y | x ?+ y | x ?? y |
x[f :E] (1)
where x, y represent node variables, ? immediate
dominance (x is directly above y),?+ strict dom-
inance (x is above y), ?? large dominance (x is
above or equal to y), ? is immediate precedence,
?+ strict precedence, and ?? large precedence3 .
x[f :E] constrains feature f with associated ex-
pression E on node x (a feature can for instance
refer to the syntactic category of the node)4.
Tree fragments can furthermore be combined
using conjunction and/or disjunction. These
two operators allow the metagrammar designer to
achieve a high degree of factorization. Moreover
the XMG system also supports inheritance be-
tween classes, thus offering more flexibility and
structure sharing by allowing one to reuse and
specialize classes.
Identifiers? scope When describing a broad-
coverage grammar, dealing with identifiers scope
is a non-trivial issue.
In previous approaches to metagrammar com-
pilation ((Candito, 1999), (Gaiffe et al, 2002)),
2As we shall later see, a content can in fact be multi-
dimensional and integrate for instance both semantic and syn-
tax/semantics interface information.
3We call strict the transitive closure of a relation and large
the reflexive and transitive one.
4E is an expression, so it can be a feature structure: that?s
how top and bottom are encoded in TAG.
node identifiers had global scope. When design-
ing broad-coverage metagrammars however, such
a strategy quickly reduces modularity and com-
plexifies grammar maintenance. To start with, the
grammar writer must remember each node name
and its interpretation and in a large coverage gram-
mar the number of these node names amounts to
several hundreds. Further it is easy to use twice
the same name erroneously or on the contrary, to
mistype a name identifier, in both cases introduc-
ing errors in the metagrammar
In XMG, identifiers are local to a class and can
thus be reused freely. Global and semi-global (i.e.,
global to a subbranch in the inheritance hierar-
chy) naming is also supported however through a
system of import / export inspired from Object
Oriented Programming. When defining a class as
being a sub-class of another one, the XMG user
can specify which are the viewable identifiers (i.e.
which identifiers have been exported in the super-
class).
Extension to semantics The XMG formalism
further supports the integration in the grammar of
semantic information. More generally, the lan-
guage manages dimensions of descriptions so that
the content of a class can consists of several ele-
ments belonging to different dimensions. Each di-
mension is then processed differently according to
the output that is expected (trees, set of predicates,
etc).
Currently, XMG includes a semantic represen-
tation language based on Flat Semantics (see (Gar-
dent and Kallmeyer, 2003)):
Description ::= `:p(E1, ..., En) |
?`:p(E1, ..., En) | Ei  Ej (2)
where `:p(E1, ..., En) represents the predicate p
with parameters E1, .., En, and labeled `. ? is the
logical negation, and Ei  Ej is the scope be-
tween Ei and Ej (used to deal with quantifiers).
Thus, one can write classes whose content con-
sists of tree description and/or of semantic formu-
las. The XMG formalism furthermore supports the
sharing of identifiers across dimension hence al-
lowing for a straightforward encoding of the syn-
tax/semantics interface (see figure 1).
3 Compiling a MetaGrammar into a
Grammar
We now focus on the compilation process and on
the constraint logic programming techniques we
104
Figure 1: Tree with syntax/semantics interface
draw upon.
As we have seen, an XMG metagrammar con-
sists of classes that are combined. Provided these
classes can be referred to by means of names, we
can view a class as a Clause associating a name
with a content or Goal to borrow vocabulary from
Logic Programming. In XMG, this Goal will be
either a tree Description, a semantic Description,
a Name (class call) or a combination of classes
(conjunction or disjunction). Finally, the valua-
tion of a specific class can be seen as being trig-
gered by a query.
Clause ::= Name ? Goal (3)
Goal ::= Description | Name
| Goal ? Goal | Goal ?Goal (4)
Query ::= Name (5)
In other words, we view our metagrammar lan-
guage as a specific kind of Logic Program namely,
a Definite Clause Grammar (or DCG). In this
DCG, the terminal symbols are descriptions.
To extend the approach to the representation of
semantic information as introduced in 2, clause (4)
is modified as follows:
Goal ::= Dimension+=Description |
Name |
Goal ? Goal | Goal ? Goal
Note that, with this modification, the XMG lan-
guage no longer correspond to a Definite Clause
Grammar but to an Extended Definite Clause
Grammar (see (Van Roy, 1990)) where the sym-
bol += represents the accumulation of information
for each dimension.
Virtual Machine The evaluation of a query is
done by a specific Virtual Machine inspired by
the Warren?s Abstract Machine (see (Ait-Kaci,
1991)). First, it computes the derivations con-
tained in the description, i.e. in the Extended Def-
inite Clause Grammar, and secondly it performs
unification of non standard data-types (nodes,
node features for TAG). Eventually it produces
as an output a description, more precisely one de-
scription per dimension (syntax, semantics).
In the case of TAG, the virtual machine produces
a tree description. We still need to solve this de-
scription in order to obtain trees (i.e. the items of
the resulting grammar).
Constraint-based tree description solver The
tree description solver we use is inspired by
(Duchier and Niehren, 2000). The idea is to:
1. associate to each node x in the description an
integer,
2. then refer to x by means of the tuple
(Eqx,Upx,Downx,Leftx,Rightx) where Eqx
(respectively Upx, Downx, Leftx, Rightx) de-
notes the set of nodes in the description which
are equal, (respectively above, below, left, and
right) of x (see picture 2). Note that these sets
are set of integers.
Eq
Up
Down
Left
Right
Figure 2: node regions
The operations supported by the XMG language
(i.e. dominance, precedence, etc) are then con-
verted into constraints on these sets. For instance,
let us consider 2 nodes x and y of the description.
Assuming we associate x with the integer i and
y with j, we can translate the dominance relation
x ? y the following way5:
N i ? N j ? [N iEqUp ? N
j
Up ?
N iDown ? N
j
EqDown ?
N iLeft ? N
j
Left ?
N iRight ? N
j
Right]
This means that if x dominates y, then in a model,
(1) the set of integers representing nodes that are
equal or above x is included in the set of inte-
gers representing nodes that are strictly above y,
5N iEqUp corresponds to the disjoint union of N iEq and
N iUp, similarly for N jEqDown with N
i
Eq and N iDown.
105
(2) the dual holds, i.e. the set of integers repre-
senting nodes that are below x contains the set of
integers representing nodes that are equal or be-
low y, (3) the set of integers representing nodes
that are on the left of x is included in the set of
integers representing those on the left of y, and (4)
symmetrically for the nodes on the right6.
Parameterized constraint solver To recap 3
from a grammar-designer?s point of view, a
queried class needs not define complete trees but
rather a set of tree descriptions. The solver is then
called to generate all the matching valid minimal
trees from those descriptions. This feature pro-
vides the users with a way to concentrate on what
is relevant in the grammar, thus taking advantage
of underspecification, and to delegate the tiresome
work to the solver.
Actually, the solver can be parameterized to per-
form various checks or constraints on the tree de-
scriptions besides tree-shaping them. These pa-
rameters are called principles in the XMG termi-
nology. Some are specific to a target formalism
(e.g. TAG trees must have at most one foot node)
while others are independent. The most interesting
one is a resources/needs mechanism for node uni-
fication called color principle, see (Crabb?e and
Duchier, 2004).
At the end of this tree description solving pro-
cess we obtain the trees of the grammar. Note that
the use of constraint programming techniques to
solve tree descriptions allows us to compute gram-
mars faster than the previous approaches (see sec-
tion 4).
4 Evaluation
The XMG system has been successfully used by
linguists to develop a core TAG for French contain-
ing more than 6.000 trees. This grammar has been
evaluated on the TSNLP test-suite, with a cover-
age rate of 75 % (see (Crabb?e, 2005)). The meta-
grammar used to produce that grammar consists of
290 classes and is compiled by the XMG system
in about 16 minutes with a Pentium 4, 2.6 GHz
and 1 GB of RAM.7
XMG has also been used to produce a core
size Interaction Grammar for French (see (Perrier,
2003)).
6See (Duchier and Niehren, 2000) for details .
7Because this metagrammar is highly unspecifi ed, con-
straint solving takes about 12 min. Of course, subsets of the
grammar may be rebuilt separately.
Finally, XMG is currently used to develop a
TAG that includes a semantic dimension along the
line described in (Gardent and Kallmeyer, 2003).
5 Conclusion and Future Work
We have presented a system, XMG8, for produc-
ing broad-coverage grammars, system that offers
an expressive description language along with an
efficient compiler taking advantages from logic
and constraint programming techniques.
Besides, we aim at extending XMG to a generic
tool. That is to say, we now would like to obtain
a compiler which would propose a library of lan-
guages (each associated with a specific process-
ing) that the user would load dynamically accord-
ing to his/her target formalism (not only tree-based
formalisms, but others such as HPSG or LFG).
References
H. Ait-Kaci. 1991. Warren?s abstract machine: A tu-
torial reconstruction. In Proc. of the Eighth Interna-
tional Conference of Logic Programming.
T. Becker. 2000. Patterns in metarules. In A. Abeille
and O. Rambow, editors, Tree Adjoining Grammars:
formal, computational and linguistic aspects. CSLI
publications, Stanford.
M.H. Candito. 1999. Repre?sentation modulaire
et parame?trable de grammaires e?lectroniques lex-
icalise?es : application au franc?ais et a` l?italien.
Ph.D. thesis, Universit?e Paris 7.
B. Crabb?e and D. Duchier. 2004. Metagrammar redux.
In CSLP 2004, Copenhagen.
B. Crabb?e. 2005. Repr?esentation informatique de
grammaires fortement lexicalise?es : Application a`
la grammaire d?arbres adjoints. Ph.D. thesis, Uni-
versit?e Nancy 2.
D. Duchier and J. Niehren. 2000. Dominance
constraints with set operators. In Proceedings of
CL2000.
B. Gaiffe, B. Crabb?e, and A. Roussanaly. 2002. A new
metagrammar compiler. In Proceedings of TAG+6.
C. Gardent and L. Kallmeyer. 2003. Semantic con-
struction in ftag. In Proceedings of EACL?03.
Guy Perrier. 2003. Les grammaires d?interaction.
HDR en informatique, Universit?e Nancy 2.
P. Van Roy. 1990. Extended dcg notation: A tool for
applicative programming in prolog. Technical re-
port, Technical Report UCB/CSD 90/583, Computer
Science Division, UC Berkeley.
8XMG is freely available at http://sourcesup.
cru.fr/xmg .
106
Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 9?16,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A constraint driven metagrammar
Joseph Le Roux
LORIA
Institut National
Polytechnique de Lorraine
615, Rue du Jardin Botanique
54 600 Villers-Le`s-Nancy
France
leroux@loria.fr
Beno??t Crabbe?
HCRC / ICCS
University of Edinburgh
2 Buccleuch Place
EH8 9LW,
Edinburgh, Scotland
bcrabbe@inf.ed.ac.uk
Yannick Parmentier
INRIA / LORIA
Universite? Henri Poincare?
615, Rue du Jardin Botanique
54 600 Villers-Le`s-Nancy
France
parmenti@loria.fr
Abstract
We present an operational framework al-
lowing to express a large scale Tree Ad-
joining Grammar (TAG) by using higher
level operational constraints on tree de-
scriptions. These constraints first meant
to guarantee the well formedness of the
grammatical units may also be viewed as
a way to put model theoretic syntax at
work through an efficient offline grammat-
ical compilation process. Our strategy pre-
serves TAG formal properties, hence en-
sures a reasonable processing efficiency.
1 Introduction
This paper is concerned with the semi-automatic
grammar development of real-scale grammars.
For natural language syntax, lexicalised TAGs are
made of thousands of trees, carrying an extreme
structural redundancy. Their development and
their maintenance is known to be cumbersome as
the size of the grammar raises significantly.
To counter the lack of generalisations inher-
ent to strong lexicalisation, various proposals for
semi-automatic grammar development have been
carried out: lexical rules or meta-rules (Becker,
2000) and metagrammars: (Candito, 1999; Gaiffe
et al, 2002; Xia, 2001). The aim of these frame-
works is twofold: expressing general facts about
the grammar of a language and factorising the in-
formation to avoid redundancy.
The metagrammar path adopts a different per-
spective from the lexical rule based grammar de-
velopment: instead of describing how a derived
tree is different from a canonical one, grammati-
cal description mainly consists of combining frag-
mentary tree descriptions or building blocks.
The paper is structured as follows. We start
in section 2 by providing motivations and back-
ground information on the framework we are us-
ing. Section 3 shows that the metagrammar frame-
work may be viewed as an offline system allowing
to express high level well-formedness constraints
on elementary grammatical structures while pre-
serving TAG computational and formal proper-
ties. Section 4 shows how to implement effi-
ciently this constraint-based approach with logic
programming techniques and finally section 5 pro-
vides an idea of the performance of the imple-
mented system.
2 eXtensible MetaGrammar (XMG)
By opposition to other metagrammatical frame-
works, XMG (Duchier et al, 2004) uses an expres-
sive though simple language, enabling a mono-
tonic description of a real scale grammar. Mono-
tonicity is important because it means that the or-
der of application of the different operations does
not matter. This is the major drawback of lexical-
rule systems. Moreover, (Crabb e?, 2005b) shows
that it is sufficiently expressive to implement con-
veniently a core TAG for French.
XMG allows the grammar writer to manipulate
tree descriptions through a control language. The
intuition behind is that a metagrammatical lan-
guage needs to provide means to describe syn-
tactic information along two methodological axis
(Crabb e?, 2005b): structure sharing and alterna-
tives. Structure sharing is the axis dedicated to
express factorisation in the grammar, whereas al-
ternatives allow to express regular alternation re-
lationships such as alternatives between the rep-
resentation of a canonical nominal subject and its
interrogative representation, or between an active
9
and a passive verb form1.
Building on this intuition the XMG language al-
lows the user to name partial tree descriptions
within classes. The name of the class can be ma-
nipulated afterwards. For instance the following
tree descriptions on the right of the arrow are as-
sociated with the names stated on the left of the
arrow2:
(1) a. CanonicalSubject ?
S
N? V
b. RelativisedSubject ?
N
N* S
N? V
c. VerbalForm ?
S
V
Naming is the main device that allows the gram-
mar writer to express and to take advantage of the
structure sharing axis mentioned above. Indeed
class names can be reused in other descriptions.
Thus names can also be used to describe alterna-
tives. To express, in our simplified example, that a
Subject is an abstract way to name a Relativised-
Subject or a CanonicalSubject, we use a choice op-
erator (?) as illustrated below:
(2) Subject ? CanonicalSubject
? RelativisedSubject
Disjunction (non-deterministic choice) is the de-
vice provided by the language to express the
methodological axis of alternatives.
Finally, names can be given to class combina-
tions. To express the composition of two tree de-
scriptions in the language, we use the ? operator.
1The passive is a semi-regular alternation, many transi-
tive verbs do not passivise. Our system presupposes a classi-
cal architecture for the computational representation of Tree
Adjoining Grammars such as XTAG, where means to ex-
press such exceptions during the anchoring process are well-
known. In what follows, we therefore consider only tree tem-
plates (or tree schematas) as our working units. Finally the
trees depicted in this paper take their inspiration from the
grammar described by (Abeill e?, 2002).
2To represent the tree descriptions mentioned in this pa-
per, we use a graphical notation. Immediate dominance is de-
picted with a straight line and precedence follows the graphi-
cal order. Note that nodes are decorated with their labels only,
ignoring the names of the variables denoting them. Note also
that we use only the reflexive transitive closure of precedence
between sibling nodes and it is explicitly stated with the sym-
bol ??.
Thus we can say that an IntransitiveVerb is made
by the composition of a Subject and a VerbalForm
as follows:
(3) IntransitiveVerb ? Subject ? VerbalForm
Given these 3 primitives, the control language
is naturally interpreted as a context free grammar
whose terminals are tree descriptions and where
our composition plays the role of concatenation.
This abstract grammar or metagrammar is further
restricted to be non recursive in order to ensure
that the generated TAG is finite.
Provided the axiom IntransitiveVerb, an inter-
preter for this language generates non determinis-
tically all the sentences of the grammar3 underly-
ing a grammatical description. Thus in our current
example the two sentences generated are those de-
picted on the left hand side of the arrows in Figure
1. On the right hand side of the arrow is depicted
the result of the composition of the tree descrip-
tions.
It remains to make clear what is actually this
composition. The grammatical classes may con-
tain information on tree descriptions and/or ex-
press composition of descriptions stated in other
classes. Tree descriptions take their inspiration
from the logic described in (Rogers and Vijay-
Shanker, 1994). Its syntax is the following:
Description ::= x ? y | x ?? y |
x ? y | x ?? y |
x[f :E]
where x, y are node variables, ? the dominance
relation, ? the precedence relation, ? denoting the
reflexive transitive closure of a relation. The last
line associates x with a feature f whose value is
the result of evaluating expression E.
Tree descriptions are interpreted as finite linear
ordered trees being the minimal models of the de-
scription.
Using tree descriptions, the above mentioned
operation of tree ?composition? breaks down to a
conjunction of formulas where variables of each
conjunct are in first approximation renamed to
avoid name collisions. Renaming is a crucial dif-
ference with previous approaches to metagrammar
(Candito, 1999; Xia, 2001) where the user had to
manage explicitly a ?global namespace?. Here a
specific attention is given to namespace manage-
ment, because this was a bottleneck for real scale
3Understood as compositions of tree fragments.
10
SN? V
Le garc?on. . .
The boy. . .
?
S
V
dort
sleeps
?
S
N? V
Le garc?on dort
The boy who sleeps
N
N* S
N? V
(Le garc?on) qui. . .
(The boy) who. . .
?
S
V
dort
sleeps
?
N
N* S
N? V
Le garc?on qui dort
The boy who sleeps
Figure 1: Interpretation of a grammatical description
grammar design. More precisely each class has
its own namespace of identifiers and namespace
merging can be triggered when a class combina-
tion occurs. This merging relies on a fine-grained
import/export mechanism.
In addition to conjunction and disjunction, XMG
is augmented with syntactic sugar to offer some
of the features other metagrammatical formalisms
propose. For instance, inheritance of classes is not
built-in in the core language but is realised through
conjunction and namespace import. Of course,
this restricts users to monotonic inheritance (spe-
cialisation) but it seems to be sufficient for most
linguists.
3 Constraining admissible structures
XMG has been tested against the development of a
large scale French Grammar (Crabb e?, 2005a). To
ease practical grammatical development we have
added several augmentations to the common tree
description language presented so far in order to
further restrict the class of admissible structures
generated by the metagrammar.
Further constraining the structures generated by
a grammar is a common practice in computational
linguistics. For instance a Lexical Functional
Grammar (Bresnan and Kaplan, 1982) further re-
stricts the structures generated by the grammar by
means of a functional uniqueness and a functional
completeness principles. These constraints further
restricts the class of admissible structures gener-
ated by an LFG grammar to verify valency condi-
tions.
For TAG and in a theoretical context, (Frank,
2002) states a set of such well formedness prin-
ciples that contribute to formulate a TAG theory
within a minimalist framework. In what remains
we describe operational constraints of this kind
that further restrict the admissibility of the struc-
ture generated by the metagrammar. By contrast
with the principles stated by (Frank, 2002), we
do not make any theoretical claim, instead we
are stating operational constraints that have been
found useful in practical grammar development.
However as already noted by (Frank, 2002) and
by opposition to an LFG framework where con-
straints apply to the syntactic structure of a sen-
tence as a whole, we formulate here constraints on
the well-formedness of TAG elementary trees. In
other words these constraints apply to units that
define themselves their own global domain of lo-
cality. In this case, it means that we can safely
ignore locality issues while formulating our con-
straints. This is theoretically weaker than formu-
lating constraints on the whole sentential structure
but this framework allows us to generate common
TAG units, preserving the formal and computa-
tional properties of TAG.
We formulate this constraint driven framework
by specifying conditions on model admissibility.
Methodologically the constraints used in the de-
velopment of the French TAG can be classified
in four categories: formal constraints, operational
constraints, language dependent constraints and
theoretical principles.
First the formal constraints are those constrain-
ing the trees generated by the model builder to
be regular TAG trees. These constraints require
the trees to be linear ordered trees with appropri-
ate decorations : each node has a category label,
leaf nodes are either terminal, foot or substitution,
there is at most one foot node, the category of the
foot note is identical to that of the root node, each
tree has at least one leaf node which is an anchor.
11
It is worth noting here that using a different set
of formal constraints may change the target for-
malism. Indeed XMG provides a different set of
formal constraints (not detailed here) that allow to
generate elementary units for another formalism,
namely Interaction Grammars.
The second kind of constraint is a single op-
erational constraint dubbed the colouration con-
straint. We found it convenient in the course
of grammar development. It consists of associ-
ating colour-based polarities to the nodes to en-
sure a proper combination of the fragmentary
tree descriptions stated within classes. Since in
our framework descriptions stated in two different
classes are renamed before being conjoined, given
a formula being the conjunction of the two follow-
ing tree descriptions :
(4)
X
W Z
X
Z Y
both the following trees are valid models of that
formula:
(5) (a)
X
W Z Y (b)
X
W Z Z Y
In the context of grammar development, however,
only (a) is regarded as a desired model. To rule out
(b) (Candito, 1999; Xia, 2001) use a naming con-
vention that can be viewed as follows4: they assign
a name to every node of the tree description. Both
further constrain model admissibility by enforcing
the identity of the interpretation of two variables
associated to the same name. Thus the description
stated in their systems can be exemplified as fol-
lows:
(6)
Xa
Wb Zc
Xa
Zc Yd
Though solving the initial formal problem, this de-
sign choice creates two additional complications:
(1) it constrains the grammar writer to manually
manage a global naming, entailing obvious prob-
lems as the size of the grammatical description
grows and (2) it prevents the user to reuse sev-
eral times the same class in a composition. This
case is a real issue in the context of grammati-
cal development since a grammar writer willing
to describe a ditransitive context with two prepo-
sitional phrases cannot reuse two times a fragment
4They actually use a different formal representation that
does not affect the present discussion.
describing such a PP since the naming constraint
will identify them.
To solve these problems we use a colouration
constraint. This constraint associates unary prop-
erties, colours, to every node of the descriptions.
A colour is taken among the set red(?R), black(?B ),
white (?W). A valid model is a model in which ev-
ery node is coloured either in red or black. Two
variables in the description interpreted by the same
node have their colours merged following the table
given in Figure 2.
?B ?R ?W ?
?B ? ? ?B ?
?R ? ? ? ?
?W ?B ? ?W ?
? ? ? ? ?
Figure 2: Colour identification rules.
The table indicates the resulting colour after
a merge. The ? symbol indicates that this two
colours cannot be merged and hence two nodes la-
belled with these colours cannot be merged. Note
that the table is designed to ensure that merging is
not a procedural operation.
The idea behind colouration is that of saturat-
ing the tree description. The colour white repre-
sents the non saturation or the need of a node to
be combined with a resource, represented by the
colour black. Black nodes need not necessarily
be combined with other nodes. Red is the colour
used to label nodes that cannot be merged with
any other node. A sample tree description with
coloured node is as follows:
(7)
X?B
W?R Z?B
X?W
Z?W Y?R
Colours contribute to rule out the (b) case and re-
move the grammar writer the burden of managing
manually a ?global namespace?.
The third category of constraints are language
dependent constraints. In the case of French, such
constraints are clitic ordering, islands constraints,
etc. We illustrate these constraints with clitic or-
dering in French. In French clitics are non tonic
particles with two specific properties already iden-
tified by (Perlmutter, 1970): first they appear in
front of the verb in a fixed order according to their
rank (8a-8b) and second two different clitics in
front of the verb cannot have the same rank (8c).
For instance the clitics le, la have the rank 3 and
lui the rank 4.
12
SN? V??+ ?
V?
Cl?3 V?+ ?
V?
Cl?4 V?+ ?
S
V?
V ?
S
N? V?
Cl?3 Cl?4 V
S
N? V?
Cl?4 Cl?3 V
Figure 3: Clitic ordering
(8) a. Jean le3 lui4 donne
John gives it to him
b. *Jean lui4 le3 donne
*John gives to him it
c. *Jean le3 la3 donne
*John gives it it
In the French grammar of (Crabb e?, 2005a) trees
with clitics are generated with the fragments illus-
trated on the left of the arrow in Figure 35. As
illustrated on the right of the arrow, the composi-
tion may generate ill-formed trees. To rule them
out we formulate a clitic ordering constraint. Each
variable labelled with a clitic category is also la-
belled with a property, an integer representing its
rank. The constraint stipulates that sibling nodes
labelled with a rank have to be linearly ordered ac-
cording to the order defined over integers.
Overall language dependent constraints handle
cases where the information independently spec-
ified in different fragments may interact. These
interactions are a counterpart in a metagrammar to
the interactions between independently described
lexical rules in a lexical rule based system. As-
suming independent lexical rules moving canoni-
cal arguments (NP or PP) to their clitic position,
lexical rules fall short for capturing the relative or-
dering among clitics6 .
A fourth category of constraints, not imple-
mented in our system so far are obviously the lan-
guage independent principles defining the theory
underlying the grammar. Such constraints could
involve for instance a Principle of Predicate Argu-
ment Coocurrency (PPAC) or even the set of min-
imalist principles described by (Frank, 2002).
4 Efficient implementation
We describe now the implementation of our meta-
grammatical framework. In particular, we will fo-
5Colours are omitted.
6This observation was already made by (Perlmutter, 1970)
in a generative grammar framework where clitics where as-
sumed to be moved by transformations.
cus on the implementation of the constraints dis-
cussed above within XMG.
As mentioned above, a metagrammar corre-
sponds to a reduced description of the grammar.
In our case, this description consists of tree frag-
ments combined either conjunctively or disjunc-
tively. These combinations are expressed using
a language close to the Definite Clause Grammar
formalism (Pereira and Warren, 1980), except that
partial tree descriptions are used as terminal sym-
bols. In this context, a metagrammar can be re-
duced to a logic program whose execution will
lead to the computation of the trees of the gram-
mar.
To perform this execution, a compiler for our
metagrammatical language has been implemented.
This compilation is a 3-step process as shown in
Figure 4.
First, the metagrammar is compiled into in-
structions for a specific virtual machine inspired
by the Warren?s Abstract Machine (Ait-Kaci,
1991). These instructions correspond to the un-
folding of the relations7 contained in the tree de-
scriptions of the metagrammar.
Then, the virtual machine performs unifications
of structures meant to refer to corresponding in-
formation within fragments (e.g. two nodes, two
feature structures ...). Note that the XMG?s virtual
machine uses the structure sharing technique for
memory management, i.e. data are represented by
a pair pattern ? environment in which to interpret
it. The consequences are that (a) we save mem-
ory when compiling the metagrammar, and (b) we
have to perform pointer dereferencing during uni-
fication. Even if the latter is time-consuming, it
remains more efficient than structure copying as
we have to possibly deal with a certain amount of
tree descriptions.
Eventually, as a result of this instruction pro-
cessing by the virtual machine, we obtain poten-
7These relations are either dominance or precedence be-
tween node variables, or their reflexive transitive closure, or
the labelling of node variable with feature structures.
13
STEP1
(translation of concrete syntax)
INTO INSTRUCTIONS
CONCRETE SYNTAX
METAGRAMMATICAL
COMPILATION OF 
TREE DESCRIPTION SOLVING
STEP3
(unification of data structures)
STEP2
A SPECIFIC VIRTUAL MACHINE
INSTRUCTIONS BY
EXECUTION OF THE 
INPUT: MetaGrammar
Total tree descriptions OUTPUT: TAGCompiled partial tree descriptions
Figure 4: Metagrammar compilation.
tially total tree descriptions, that have to be solved
in order to produce the expected TAG.
Now, we will introduce XMG?s tree description
solver and show that it is naturally designed to pro-
cess efficiently the higher level constraints men-
tioned above. In particular, we will see that the
description solver has been designed to be easily
extended with additional parametric admissibility
constraints.
4.1 Tree descriptions solving
To find the minimal models corresponding to the
total tree descriptions obtained by accumulating
fragmentary tree descriptions, we use a tree de-
scription solver. This solver has been developed in
the Constraint Programming paradigm using the
constraint satisfaction approach of (Duchier and
Niehren, 2000). The idea is to translate relations
between node variables into constraints over sets
of integers.
Basically, we refer to a node of the input de-
scription in terms of the nodes being equals,
above, below, or on its side (see Figure 5). More
precisely, we associate each node of the descrip-
tion with an integer, then our reference to a node
corresponds to a tuple containing sets of nodes (i.e.
sets of integers).
As a first approximation, let us imagine that we
refer to a node x in a model by means of a 5-tuple
N ix = (Eq, Up, Down, Left, Right) where i is an in-
teger associated with x and Eq (respectively Up,
Down, Left, Right) denotes the set of nodes8 in the
description which are equal, (respectively above,
below, left, and right) of x.
Then we can convert the relations between
nodes of our description language into constraints
on sets of integer.
8I.e. integers.
Eq
Up
Down
Left
Right
Figure 5: Node representation.
For instance, if we consider 2 nodes x and y of
the description. Assuming we associate x with the
integer i and y with j, we can translate the domi-
nance relation x ? y the following way9:
N ix? N jy?
[N ix.EqUp ? N jy.Up?N ix.Down ? N jy.EqDown
?N ix.Left ? N jy.Left?N ix.Right ? N
j
y.Right]
This means that if the node10 x strictly dominates
y in the input description, then (i) the set of nodes
that are above or equal x in a valid model is in-
cluded in the set of those that are strictly above y
and (ii) the dual holds for the nodes that are above
and (iii) the set of nodes that are on the left of y is
included in the set of those that are on the left of x
and (iv) similarly for the right part.
Once the constraints framework is settled, we
can search for the solutions to our problem, i.e.
the variable assignments for each of the sets of in-
tegers used to refer to the nodes of the input de-
scription. This search is performed by associating
with each pair of nodes (x, y) of the input descrip-
tion a choice variable denoting the mutually ex-
clusive relations11 between these two nodes. Then
9N ix.EqUp corresponds to the disjoint union of N ix.Eq and
N ix.Up, similarly for N jx.EqDown with N
i
x.Eq and N ix.Down.
10One should read the node denoted by the variable x.
11Either x equals y, x dominates y, y dominates x, x pre-
cedes y or y precedes x.
14
we use a search strategy to explore the consistent
assignments to these choices variables (and the as-
sociated assignments for sets of integers referring
to nodes)12 . Note that the strategy used in XMG
is a first-fail strategy which leads to very good re-
sults (see section 5 below). The implementation
of this solver has been done using the constraint
programming support of the Mozart Programming
System (The Oz-Mozart Board, 2005).
4.2 Extension to higher-level constraints
solving
An important feature of our approach is that this
system of constraints over integer sets can be
extended so that we not only ensure tree well-
formedness of the outputted trees, but also the re-
spect of linguistic properties such as the unique-
ness of clitics in French, etc.
The idea is that if we extend adequately our
node representation, we can find additional con-
straints that reflects the syntactic constraints we
want to express.
Clitic uniqueness For instance, let us consider
the clitic uniqueness constraint introduced above.
We want to express the fact that in a valid model
?, there is only one node having a given property
p (i.e. a parameter of the constraint, here the cat-
egory clitic13). This can be done by introducing,
for each node x of the description, a boolean vari-
able px indicating whether the node denoting x in
the model has this property or not. Then, if we call
V?p the set of integers referring to nodes having the
property p in a model, we have:
px ? (N ix.Eq ? V?p ) 6= ?
Finally, if we represent the true value with the in-
teger 1 and false with 0, we can sum the px for
each x in the model. When this sum gets greater
than 1, we can consider that we are not building a
valid model.
Colouration constraint Another example of the
constraints introduced in section 3 is coloura-
tion. Colouration represents operational con-
straints whose effect is to control tree fragment
combination. The idea is to label nodes with a
colour between red, black and white. Then, during
12More information about the use of such choice variables
is given in (Duchier, 1999)
13In fact, the uniqueness concerns the rank of the clitics,
see (Crabb e?, 2005b), ?9.6.3.
description solving, nodes are identified according
to the rules given previously (see Figure 2).
That is, red nodes are not identified with any
other node, white nodes can be identified with a
black one. Black nodes are not identified with
each other. A valid model in this context is a satu-
rated tree, i.e. where nodes are either black (possi-
bly resulting from identifications) or red. In other
words, for every node in the model, there is at most
one red or black node with which it has been iden-
tified. The implementation of such a constraint
is done the following way. First, the tuples rep-
resenting nodes are extended by adding a integer
field RB referring to the red or black node with
which the node has been identified. Then, con-
sidering the following sets of integers: VR, VB,
VW respectively containing the integers referring
to red, black and white nodes in the input descrip-
tion, the following constraints hold:
x ? VR ? N ix.RB = i ? N ix.Eq = {i} (a)
x ? VB ? N ix.RB = i (b)
x ? VW ? N ix.RB ? V?B (c)
where V?B represents the black nodes in a model,
i.e. V?B = V? ? VB. (a) expresses the fact that for
red nodes, N ix.RB is the integer i associated with
x itself, and N ix.Eq is a set only containing i. (b)
means that for black nodes, we have that N ix.RB is
also the integer i denoting x itself, but we cannot
say anything about N ix.Eq. Eventually (c) means
that whites nodes have to be identified with a black
one.
Thus, we have seen that Constraint Program-
ming offers an efficient and relatively natural way
of representing syntactic constraints, as ?all? that
has to be done is to find an adequate node repre-
sentation in terms of sets of nodes, then declare the
constraints associated with these sets, and finally
use a search strategy to compute the solutions.
5 Some features
There are two points worth considering here: (i)
the usability of the formalism to describe a real
scale grammar with a high factorisation, and (ii)
the efficiency of the implementation in terms of
time and memory use.
Concerning the first point, XMG has been used
successfully to compute a TAG having more than
6,000 trees from a description containing 293
15
classes14 . Moreover, this description has been de-
signed relatively quickly as the description lan-
guage is intuitive as advocated in (Crabb e?, 2005a).
Concerning the efficiency of the system, the
compilation of this TAG with more than 6,000 trees
takes about 15 min with a P4 processor 2.6 GHz
and 1 GB RAM. Note that compared with the
compilation time of previous approaches (Candito,
1999; Gaiffe et al, 2002) (with the latter, a TAG of
3,000 trees was compiled in about an hour), these
results are quite encouraging.
Eventually, XMG is released under the terms of
the GPL-like CeCILL license15 and can be freely
downloaded at http://sourcesup.cru.fr/xmg.
6 Conclusion
Unlike previous approaches, the description lan-
guage implemented by XMG is fully declara-
tive, hence allowing to reuse efficient techniques
borrowed to Logic Programming. The system
has been used successfully to produce core TAG
(Crabb e?, 2005b) and Interaction Grammar (Per-
rier, 2003) for French along with a core French
TAG augmented with semantics (Gardent, 2006).
This paper shows that the metagrammar can be
used to put model theoretic syntax at work while
preserving reasonably efficient processing proper-
ties. The strategy used here builds on constraining
offline a TAG whose units are elementary trees The
other option is to formulate constraints applied
on-line, in the course of parsing, applying on the
whole syntactic structure. In a dependency frame-
work, XDG followed this path (Debusmann et al,
2004), however it remains unknown to us whether
this approach remains computationally tractable
for parsing with real scale grammars.
References
A. Abeill e?. 2002. Une grammaire e?lectronique du franais.
CNRS Editions, Paris.
H. Ait-Kaci. 1991. Warren?s abstract machine: A tuto-
rial reconstruction. In K. Furukawa, editor, Proc. of the
Eighth International Conference of Logic Programming.
MIT Press, Cambridge, MA.
T. Becker. 2000. Patterns in metarules. In A. Abeille and
O. Rambow, editors, Tree Adjoining Grammars: formal,
computational and linguistic aspects. CSLI publications,
Stanford.
14I.e. tree fragments or conjunction / disjunction of frag-
ments
15More information about this license at http://www.
cecill.info/index.en.html.
Joan Bresnan and Ronal M. Kaplan. 1982. The Mental Rep-
resentation of Grammatical Relations. The MIT Press,
Cambridge MA.
M.H. Candito. 1999. Repre?sentation modulaire et
parame?trable de grammaires e?lectroniques lexicalise?es :
application au franc? ais et a` l?italien. Ph.D. thesis, Uni-
versit e? Paris 7.
B. Crabb e?. 2005a. Grammatical development with XMG.
Proceedings of the Fifth International Conference on Log-
ical Aspects of Computational Linguistics (LACL05).
B. Crabb e?. 2005b. Repre?sentation informatique de gram-
maires fortement lexicalise?es : Application a` la gram-
maire d?arbres adjoints. Ph.D. thesis, Universit e? Nancy
2.
R. Debusmann, D. Duchier, and G.-J. M. Kruijff. 2004. Ex-
tensible dependency grammar: A new methodology. In
Proceedings of the COLING 2004 Workshop on Recent
Advances in Dependency Grammar, Geneva/SUI.
D. Duchier and J. Niehren. 2000. Dominance constraints
with set operators. In Proceedings of CL2000, volume
1861 of Lecture Notes in Computer Science, pages 326?
341. Springer.
D. Duchier, J. Le Roux, and Y. Parmentier. 2004. The Meta-
grammar Compiler: An NLP Application with a Multi-
paradigm Architecture. In 2nd International Mozart/Oz
Conference (MOZ?2004), Charleroi.
D. Duchier. 1999. Set constraints in computational linguis-
tics - solving tree descriptions. In Workshop on Declara-
tive Programming with Sets (DPS?99), Paris, pp. 91 - 98.
Robert Frank. 2002. Phrase Structure Composition and Syn-
tactic Dependencies. MIT Press, Boston.
B. Gaiffe, B. Crabb e?, and A. Roussanaly. 2002. A new meta-
grammar compiler. In Proceedings of TAG+6, Venice.
C. Gardent. 2006. Int e?gration d?une dimension s e?mantique
dans les grammaires d?arbres adjoints. In Actes de La
13e`me e?dition de la confe?rence sur le TALN (TALN 2006).
F. Pereira and D. Warren. 1980. Definite clause grammars
for language analysis ?a survey of the formalism and a
comparison to augmented transition networks. Artificial
Intelligence, 13:231?278.
David Perlmutter. 1970. Surface structure constraints in syn-
tax. Linguistic Inquiry, 1:187?255.
Guy Perrier. 2003. Les grammaires d?interaction. HDR en
informatique, Universit e? Nancy 2.
J. Rogers and K. Vijay-Shanker. 1994. Obtaining trees from
their descriptions: An application to tree-adjoining gram-
mars. Computational Intelligence, 10:401?421.
The Oz-Mozart Board. 2005. The Oz-Mozart Programming
System. http://www.mozart-oz.org.
Fei Xia. 2001. Automatic Grammar Generation from two
Different Perspectives. Ph.D. thesis, University of Penn-
sylvania.
16
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1875?1885, Dublin, Ireland, August 23-29 2014.
Syntactic Parsing and Compound Recognition via Dual Decomposition:
Application to French
Joseph Le Roux
1
and Matthieu Constant
2
and Antoine Rozenknop
1
(1) LIPN, Universit? Paris 13 ? Sorbonne Paris Cit?, CNRS
(2) LIGM, Universit? Paris Est, CNRS
leroux@univ-paris13.fr, mconstan@univ-mlv.fr, antoine.rozenknop@lipn.univ-paris13.fr
Abstract
In this paper we show how the task of syntactic parsing of non-segmented texts, including com-
pound recognition, can be represented as constraints between phrase-structure parsers and CRF
sequence labellers. In order to build a joint system we use dual decomposition, a way to com-
bine several elementary systems which has proven successful in various NLP tasks. We evaluate
this proposition on the French SPMRL corpus. This method compares favorably with pipeline
architectures and improves state-of-the-art results.
1 Introduction
Dual decomposition (DD), which can be used as a method to combine several elementary systems, has
already been successfully applied to many NLP tasks, in particular syntactic parsing, see (Rush et al.,
2010; Koo et al., 2010) inter alia. Intuitively, the principle can be described quite simply: at decoding
time, the combined systems seek for a consensus on common subtasks, in general the prediction of some
parts of the overall structure, via an iterative process imposing penalties where the systems disagree. If
the systems converge to a solution, it is formally guaranteed to be optimal. Besides, this approach is
quite flexible and easy to implement. One can add or remove elementary systems without rebuilding
the architecture from the ground up. Moreover, the statistical models for the subsystems can often be
estimated independently at training time.
In this paper we show how syntactic parsing of unsegmented texts, integrating compound recognition,
can be represented by constraints between phrase-structure parsers and sequence labellers, either for
compound recognition or part-of-speech (POS) tagging, and solved using DD. We compare this approach
experimentally with pipeline architectures: our system demonstrates state-of-the-art performance. While
this paper focuses on French, the approach is generic and can be applied to any treebank with compound
information, and more generally to tasks combining segmentation and parsing.
This paper is structured as follows. First, we describe the data we use to build our elementary systems.
Second, we present related work in compound recognition, in particular for French, and the type of
information one is able to incorporate in tag sets. Third, we show how CRF-based sequence labellers with
these different tag sets can be combined using DD to obtain an efficient decoding algorithm. Fourth, we
extend our method to add phrase-structure parsers in the combination. Finally, we empirically evaluate
these systems and compare them with pipeline architectures.
2 Data
We use the phrase-structure treebank released for the SPMRL 2013 shared task (Seddah et al., 2013).
This corresponds to a new version of the French Treebank (Abeill? et al., 2003). One of the key dif-
ferences between French data and other treebanks of the shared task is the annotation of compounds.
Compounds are sequences of words with a certain degree of semantic non-compositionality. They form
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1875
a single lexical unit to which one can assign a single POS. In the SPRML corpus 15% of the tokens be-
long to a compound, or 12.7% if we omit numerals: the training, development and test sets respectively
comprise 23658, 2120 and 4049 compounds.
In the treebank, compounds are annotated as subtrees whose roots are labelled with the POS of the
compounds with a + suffix. Each leaf under a compound is the daughter of its own POS tag, which is in
turn the daughter of the root of the compound. For example, the tree in Figure 1 contains a subtree with
the compound adverb pour l?instant (so far) whose category ADV+ dominates the preposition pour, the
determiner l?, and the noun instant.
SENT
PONCT
.
VN
VP
bloqu?e
ADV
compl?tement
V
est
NP-SUJ
NC
situation
DET
la
PONCT
,
ADV+
NC
instant
DET
l?
P
Pour
Figure 1: Syntatic annotation in the SPRML FTB: So far, the situation has been completely blocked.
The sequence labellers used in the experiments are able to exploit external lexical resources that will
help coping with data missing from the training corpus. These resources are dictionaries, consisting of
triplets (flexed form, lemma, POS tag), where form and lemmamay be compound or simple.
Several such dictionaries exist for French. We use:
? DELA (Courtois et al., 1997) contains a million entries, among which 110,000 are compounds;
? Lefff (Sagot, 2010) contains 500,000 entries, among which 25,000 are compounds;
? Prolex (Piton et al., 1999) is a toponym dictionary with approximately 100,000 entries.
The described resources are additional to the SPMRL shared task data (Seddah et al., 2013), but were
also used in (Constant et al., 2013a) for the shared task.
3 Compound Recognition
3.1 Related Work
The compound recognition traditionally relies on 2 types of information: lexical and syntactic clues. A
strong lexical association between the tokens of a compound can be detected using a compound dictio-
nary or by measuring a degree of relatedness, which can be learnt on a corpus. Some recent approaches
use sequence labellers. The linear chain CRF model (Lafferty et al., 2001) is widely used, see for ex-
ample (Vincze et al., 2011; Constant and Tellier, 2012). It has proven to be a very adequate model: it
is flexible enough to incorporate information from labelled data and external resources (POS taggers,
compound lexicons or named entity recognisers).
The compound recognition may also be directly performed by syntactic parsers learnt from corpora
where compounds are marked, such as the one we use in this paper
1
(Arun and Keller, 2005; Green et al.,
2011; Green et al., 2013; Constant et al., 2013b), but these results are contradictory. Green et al. (2011)
experimentally show that a lexicalised model is better than an unlexicalised one. On the other hand,
Constant et al. (2013b) show that, using products of PCFG-LA (Petrov, 2010), unlexicalised models can
be more accurate. They obtain performance on a par with a linear chain CRF system without external
information. But such information is difficult to incorporate directly in a PCFG-LA model. Constant et
al. (2012) resort to a reranker to add arbitrary features in the parse selection process, but their system
showed inferior performance compared with a CRF model with access to the same external information.
1
Such an approach has been used already for joint named entity recognition and parsing based on CRF (Finkel and Manning,
2009).
1876
3.2 Annotation schemes
Compound recognition can be seen as a segmentation task which consists in assigning to each token a
label with segmentation information. We use label B if the token is the beginning of a word (single or
compound), and label I if the token is inside a compound, but not in initial position. This lexical seg-
mentation can be enriched with additional information, for example POS tags of compounds or tokens in
compounds, and gives a variety of tag sets. This leads us to define 5 simple tag sets for our problem, each
with very simple information, that will be combined in the next section. These tag sets are exemplified
on a sentence with the compound vin rouge (red wine).
1. (basic) recognition with two labels (B and I)
Luc/B aime/B le/B vin/B rouge/I (Luc likes red wine)
2. (partial) recognition with compound POS tags: [BI]-POS for tokens in compounds; B for others
Luc/B aime/B le/B vin/B-NC+ rouge/I-NC+
3. (partial-internal) recognition with token POS tags in compounds
Luc/B aime/B le/B vin/B-NC rouge/I-ADJ
4. (complete) recognition with POS tags for all tokens; in compounds use compound POS tags
Luc/B-NPP aime/B-V le/B-DET vin/B-NC+ rouge/I-NC+
5. (complete-internal) recognition with POS tags for all tokens; in compounds use token POS tags
Luc/B-NPP aime/B-V le/B-DET vin/B-NC rouge/I-ADJ
4 Dual decomposition for compound recognition using CRFs
4.1 A maximisation problem
4.1.1 CRF
A conditional random field (Lafferty et al., 2001), or CRF, is a tuple c = (?,L
c
, w
c
, {f
c
p
}
p
) which
defines the conditional probability of a sequence of labels y ? L
?
c
given a sequence of words of the same
length x ? ?
?
as a logistic regression of the form:
P
c
(y|x) =
exp
(
?
p?P(x)
w
c
? f
c
p
(x, y
p
)
)
Z(w
c
, x)
, where (1)
? w
c
? R
d
is a d-dimensional weight vector, where d is the number of features of the system,
? Z is the partition function
? P(x) is a set of places, in our case the set of unigram and bigram decompositions of sequences
of words. A place p is of the form [i]
x
for unigrams and [i, i + 1]
x
for bigrams. We omit x when
context is unambiguous.
? y
p
is the restriction of y to the place p, and we will write y
i
for y
[i]
and y
i
y
i+1
for y
[i,i+1]
? f
c
p
is the feature function for the place p that projects (x, y
p
) on R
d
.
Our goal is to find the best labelling, i.e. the one that maximises the conditional probability given a
sequence of tokens. One can observe that this labelling also maximises the numerator of Equation 1, as
Z(w
c
, x) does not depend on y. We therefore write:
y?
c
= argmax
y
?
c
(x, y) = argmax
y
?
p?P(x)
w
c
? f
c
p
(x, y
p
) (2)
1877
4.1.2 Viterbi Algorithm for CRFs
Since our combination of CRF systems relies on the Viterbi algorithm, we review it briefly. For a given
input sentence x = x
1
. . . x
n
, we represent the problem of finding the best labelling with a CRF c as a
best path algorithm in a directed acyclic graph G
c
= (V, E) built from a set of nodes V and a set of edges
E . Nodes are pairs (x
i
, l) where x
i
is an input token and l is an admissible label for x
i
.
2
Edges connect
nodes of the form (x
i
, l) to nodes of the form (x
i+1
, l
?
) and the weights of these arcs are given by c. In
order to find the weight of the best path in this graph, that corresponds to the score of the best labelling,
we use Algorithm 1.
3
One can remark that the score of a node decomposes as a score s
1
, computed from
a vector of unigram features, written f
c
[i]
(x, l), and a score s
2
computed from a vector of bigram features,
written f
c
[i?1,i]
(x, l
?
, l).
4
The Viterbi algorithm has a time complexity linear in the length of the sentence
and quadratic in the number of labels of the CRF.
Algorithm 1 Viterbi Algorithm for CRFs with unigram and bigram features
1: Viterbi(G
c
, w
c
, {f
c
p
}
p
,?
BI
,?
IB
):
2: for all node v do
3: ?[v] = ??
4: end for
5: ?[ (<s>,START)] = w ? f
p
0
(x, START)
6: for all non initial node v = (x
i
, l) in topological order do
7: s
1
? w
c
? f
c
[i]
(x, l)
8: s
2
? ??
9: for all incoming edge v
?
= (x
i?1
, l
?
)? v do
10: t? ?[v
?
] + w
c
? f
c
[i?1,i]
(x, l
?
, l)
11: t? t? b(l
?
)i(l) ? ?
BI
[i]? i(l
?
)b(l) ? ?
IB
[i] ? only for DD: we ignore this line otherwise
12: if t > s
2
then
13: s
2
= t
14: end if
15: end for
16: ?[v]? s
1
+ s
2
17: end for
18: return the best scoring path ?[ (</s>,STOP) ]
4.2 Dual decomposition for CRF combinations
In Section 3.2 we described several annotation schemes that lead to different CRF models. These
schemes give the same lexical segmentation information but they use more or less rich part-of-speech
tag sets. It is not clear a priori if the richness of the tag set has a beneficial effect over segmentation
prediction: there is a compromise between linguistic informativeness and data sparseness. Instead of
trying to find the best annotation scheme, we propose a consensus-based joint system between several
CRF-based sequence labellers for the task of text segmentation relying on dual decomposition (Rush
et al., 2010). This system maximises the sum of the scores of combined CRFs, while enforcing global
consistency between systems in terms of constraints over the admissible solutions. These constraints are
specifically realised as reparametrisations of the elementary CRFs until a consensus is reached. Since
we deal with several annotation schemes, we will use predicates to abstract from them:
? b(l) is true if l starts with B;
? i(l) is true if l starts with I;
? bi(i, y) is true if b(y
i?1
) and i(y
i
) are true;
? ib(i, y) is true if i(y
i?1
) and b(y
i
) are true.
For a labelling y, we define 2 boolean vectors that indicate where the compounds begin and end:
2
We also include two additional nodes: an initial state (<s>,START) and a final state (</s>, STOP).
3
Algorithm 1 calculates the score and backpointers must be added to retrieve the corresponding path.
4
This algorithm takes as input 2 vectors that will be used for DD and will be explained in ? 4.2. They can be ignored now.
1878
? D(y), such that D(y)[i] = 1 if bi(i, y), and 0 otherwise;
? F (y), such that F (y)[i] = 1 if ib(i, y), and 0 otherwise.
As we want to combine CRFs, the solution of our system will be a tuple of label sequences with the
same compound segmentation. For an input sequence x, this new maximisation problem is:
(P ) : find max
(y
1
,...,y
n
)
n
?
c=1
?
c
(y
c
) =
n
?
c=1
?
p?P(x)
w
c
? f
c
p
(x, y
c
p
) (3)
s.t. ?u
1
, u
2
?c ? J1, nK, D(y
c
) = u
1
, F (y
c
) = u
2
(4)
Objective (3) indicates that we seek for a tuple for which the sum of the scores of its elements is
maximal. Constraints (4) imply that the compound frontiers ? transitions B to I and I to B ? must be
the same for each element of the tuple. There are several ways to tackle this problem. The first one
is by swapping the sum signs in (3) and noticing that the problem could then be represented by a joint
system relying on dynamic programming ? a CRF for which labels would be elements of the product
L = L
1
? ? ? ? ? L
n
? and for which it is straightforward to define a weight vector and feature functions.
We can therefore reuse the Viterbi algorithm but the complexity is quadratic in the size of L, which is
impractical
5
.
In any case, this approach would be inadequate for inclusion of parsers, and we therefore rely on
lagrangian relaxation. We modify the objective by introducing Lagrange multipliers, two real vectors
?
BI
c
and ?
IB
c
indexed by bigram places
6
for each CRF c of the combination. We obtain a new problem
with the same solution as the previous one, since constraints (4) are garanteed to be satisfied at optimality:
max
(y
1
,...,y
n
,u
1
,u
2
)
min
(?
BI
,?
IB
)
n
?
c=1
?
c
(y
c
) ?
n
?
c=1
[
(D(y
c
) ? u
1
) ? ?
BI
c
+ (F (y
c
) ? u
2
) ? ?
IB
c
]
(5)
The next step is dualisation, which gives an upper bound of our problem:
min
(?
BI
,?
IB
)
max
(y
1
,...,y
n
,u
1
,u
2
)
n
?
c=1
?
c
(y
c
)?
n
?
c=1
D(y
c
) ??
BI
c
+u
1
n
?
c=1
?
BI
c
?
n
?
c=1
F (y
c
) ??
IB
c
+u
2
n
?
c=1
?
IB
c
(6)
We then remark that
?
n
c=1
?
BI
c
and
?
n
c=1
?
IB
c
must be zeros at optimum (if the problem is feasible).
7
It is convenient to convert this remark to hard constraints in order to remove any reference to vectors u
i
? and therefore to the coupling contraints ? from the objective. We obtain the constrained problem with
the same optimal solution :
(Du) : find min
(?
BI
,?
IB
)
n
?
c=1
max
y
c
[
?
c
(y
c
) ? D(y
c
) ? ?
BI
c
? F (y
c
) ? ?
IB
c
]
(7)
s.t.
n
?
c=1
?
BI
c
= 0 and
n
?
c=1
?
IB
c
= 0 (8)
In order to solve (Du) we use the projected subgradient descent method that has already been used
in many problems, for example MRF decoding (Komodakis et al., 2007). For the problem at hand, this
method gives Algorithm 2. This iterative algorithm consists in reparametrising the elementary CRFs of
the system, by modifying the weights associated with the bigram features in places that correspond to
compound frontiers, penalising them on CRFs that diverge from the average solution. This is performed
5
One could object that some combinations are forbidden. It remains that the number of labels still grows exponentially.
6
Bigram places are identified by their second position.
7
Otherwise the sum expressions would be unbounded and their maximum is +? for an appropriate value of u
i
.
1879
by amending the vectors ?
BI
c
and ?
IB
c
that are updated at each iteration proportionally to the difference
between the feature vectors for c and the average values of these vectors. Hence the farther a solution
is from the consensus, the more penalised it gets at the next iteration. This algorithm stops when the
updates are null for all CRFs: in this case the consensus is reached.
Algorithm 2 Best segmentation with combined CRF system via subgradient descent
Require: n CRF c = (?,L
c
, w
c
, {f
c
p
}
p
), an input sentence x, a maximum number of iterations ? , stepsizes {?
t
}
0?t??
1: for all CRF c, bigram end position i, bigram label pair (l,m) do
2: ?
BI
c
[i, l,m]
(0)
= 0; ?
IB
c
[i, l,m]
(0)
= 0
3: end for
4: for t = 0? ? do
5: for all CRF c do
6: y
c
(t)
= V iterbi(G
c
, w
c
, f
c
,?
BI
(t)
c
,?
IB
(t)
c
)
7: end for
8: for all CRF c do
9: ?
BI
(t)
c
? ?
t
(
D
(
y
c
(t)
)
?
?
1?d?n
D
(
y
d
(t)
)
n
)
; ?
IB
(t)
c
? ?
t
(
F
(
y
c
(t)
)
?
?
1?d?n
F
(
y
d
(t)
)
n
)
10: ?
BI
(t+1)
c
? ?
BI
(t)
c
+ ?
BI
(t)
c
; ?
IB
(t+1)
c
? ?
IB
(t)
c
+ ?
IB
(t)
c
11: end for
12: if ?
BI
(t)
c
= 0 and ?
IB
(t)
c
= 0 for all c then
13: Exit loop
14: end if
15: end for
16: return (y
1
(t)
, ? ? ? , y
n
(t)
)
We set the maximum number of iteration ? to 1000. For the step size, we use a common heuristic:
?
t
=
1
1+k
where k is the number of times that (Du) has increased between two successive iterations.
4.3 Experimental results for CRF combinations
We modified the wapiti software (Lavergne et al., 2010) with Algorithm 2. Table 1 reports segmen-
tation results on the development set with the different tag sets, the best DD combination, and the best
voting system.
8
Tag Set CRF / combination Recall Precision F-score
partial-internal 79.59 85.49 82.44
partial 78.98 85.57 82.14
basic 79.74 84.65 82.12
complete 79.69 83.10 81.36
complete-internal 79.03 82.66 80.80
MWE basic complete partial-internal 80.82 86.07 83.36
vote (basic complete partial-internal) 80.49 85.46 82.90
Table 1: Segmentation scores of CRF systems (dev)
System F-score (all) F-score (compounds)
complete 94.29 78.32
MWE 94.59 80.00
Table 2: Segmentation + POS tagging (dev)
We see that the best system is a combination of 3 CRFs (tag sets basic, complete and partial-internal)
with DD, that we call MWE in the remaining of the paper. The subgradient descent converges on all
instances in 2.14 iterations on average. The DD combination is better than the voting system.
We can also evaluate the POS tagging accuracy of the system for systems including the complete
tag set. We compare the results of the complete CRF with the MWE combination on Table 2. The second
column gives the F-score of the complete task, segmentation and POS tagging. The third column restricts
the evaluation to compounds. Again, the MWE combination outperforms the single system.
8
Each system has one vote and in case of a draw, we pick the best system?s decision.
1880
In some preliminary experiments, the weights of the CRF systems were based on unigram features
mainly ? i.e. those described in (Constant et al., 2012). As our CRFs are constrained on transitions from
B to I and I to B, penalising systems resulted in modifying (low) bigram weights and had only a minor
effect on the predictions and consequently the projected gradient algorithm was slow to converge. We
therefore added bigram templates for some selected unigram templates, so that our system can converge
in a reasonable time. Adding these bigram features resulted in slower elementary CRFs. On average the
enriched CRFs were 1.8 times slower that their preliminary counterparts.
5 Dual Decomposition to combine parsers and sequence labellers
We now present an extension of the previous method to incorporate phrase-structure parsers in the com-
bination. Our approach relies on the following requirement for the systems to agree: if the parser predicts
a compound between positions i and j, then the CRFs must predict compound frontiers at the same po-
sitions. In this definition, like in previous CRF combinations, only the positions are taken into account,
not the grammatical categories. From a parse tree a, we define two feature vectors:
? D(a), such that D(a)[i] = 1 if a contains a subtree for a compound starting at position i ? 1
? F (a), such that F (a)[i] = 1 if a contains a subtree for a compound ending at position i ? 1
In other words, D(a)[i] indicates whether the CRFs should label position i ? 1 with B and position i
with I, while F (a)[i] indicates whether the CRFs should label position i ? 1 with I and position i with
B. See Figure 2 for an example.
NP
ADJ
vol?e
B
NC+
ADJ
bleue
I
NC
carte
B
DET
une
B
Figure 2: Parser and CRF alignments (A stolen credit card)
5.1 Parsing with probabilistic context-free grammars
We follow the type of reasoning we used in ? 4.2. With a PCFG g, we can define the score of a parse for
an input sentence x as the logarithm of the probability assigned to this parse by g. Finding the best parse
takes a form analogous to the one in Equation 2, and we can write the CKY algorithm as a best path
algorithm with penalties on nodes, as we did for the Viterbi algorithm previously. This is closely related
to the PCFG combinations of (Le Roux et al., 2013). We introduce penalties through two real vectors
?
BI
and ?
IB
indexed by compound positions. The modified CKY is presented in Algorithm 3
9
where
the parse forest F is assumed to be already available and we note w the vector of rule log-probabilities.
5.2 System combination
As in ? 4.2, our problem amounts to finding a tuple that now consists of a parse tree and several labellings.
All systems must agree on compound frontiers. Our objective is:
(P
?
) : find max
(a,y
1
,...,y
n
)
?
p
(a) + ?
n
?
c=1
?
c
(y
c
) (9)
s.t. ?u
1
, u
2
?c ? J1, nK, D(y
c
) = u
1
, F (y
c
) = u
2
, D(a) = u
1
, F (a) = u
2
(10)
9
Without loss of generality, only binary rules are taken into account.
1881
Algorithm 3 CKY with node penalties for compound start/end positions
1: CKY(F , w,?
BI
,?
IB
):
2: for all node v in the forest F do
3: ?[v] = ??
4: end for
5: for all leaf node x do
6: ?[x] = 0
7: end for
8: for all non-terminal node (A, i, j) in topological order do
9: for all incoming hyperedge u = (B, i, k)(C, k + 1, j)? (A, i, j) do
10: s? ?[(B, i, k)] + ?[(C, k + 1, j)] + w
A?BC
? w
A?BC
is the score for rule A? BC
11: if A is a compound label then
12: s? s? ?
BI
[i]? ?
IB
[j + 1]
13: end if
14: if s > ?[(A, i, j)] then
15: ?[(A, i, j)]? s
16: end if
17: end for
18: end for
19: return hyperpath with score ?[(ROOT, 1, n)]
We use ? to set the relative weights of the CRFs and the PCFG. It will be tuned on the develop-
ment set. We then reuse the same procedure as before: lagrangian relaxation, dualisation, and projected
subgradient descent. Algorithm 4 presents the function we derive from these operations.
Algorithm 4 Find the best segmentation with a PCFG and CRFs
Require: a PCFG parser p, n CRFs, an input sentence x, a bound ?
1: set Lagrange multipliers (penalty vectors) to zero
2: for t = 0? ? do
3: for all CRF c do
4: y
c
(t)
? V iterbi(G
c
, w
c
, f
c
,?
BI
(t)
c
,?
IB
(t)
c
)
5: end for
6: a
(t)
? CKY (F , w,?
BI
(t)
p
,?
IB
(t)
p
)
7: for all CRF c and parser p do
8: Update penalty vectors proportionally to the difference between corresponding solution and average solution
9: end for
10: if update is null for all c and p then
11: Exit loop
12: end if
13: end for
14: return (a
(t)
, y
1
(t)
, ? ? ? , y
n
(t)
)
Algorithm 4 follows the method used in ? 4 and simply adds the PCFG parser as another subsystem.
This method can then be extended further: for instance, we can add a POS tagger (Rush et al., 2010) or
multiple PCFG parsers (Le Roux et al., 2013). Due to lack of space, we omit the presentation of these
systems, but we experiment with them in the following section.
6 Experiments
For this series of experiments, we used wapiti as in ? 4.3 and the LORG PCFG-LA parser in the
configuration presented in (Le Roux et al., 2013) that we modified by implementing Algorithm 4. This
parser already implements a combination of parsers based on DD, a very competitive baseline.
For parse evaluation, we used the EVALB tool, modified by the SPMRL 2013 organisers, in order
to compare our results with the shared task participants. We evaluated several configurations: (i) the
LORG parser alone, a combination of 4 PCFG-LA parsers as in (Le Roux et al., 2013), (ii) a pipeline
of POS, a CRF-based POS tagger, and LORG, (iii) joint LORG and POS, using DD as in (Rush et al.,
2010), (iv) joint LORG and MWE (our best CRF combination for compound segmentation) using DD, and
(v) joint LORG, POS et MWE using DD. We also compare these architectures with 2 additional pipelines,
in which we first run MWE and then merge compounds as single tokens. The converted sentences are
then sent to a version of LORG learnt on this type of corpus. After parsing, compounds are unmerged,
1882
replaced with the corresponding subtree. In one of these two architectures, we add a POS tagger.
The evaluations for the parsing task of all these configurations are summarised in Table 3. The best
system is the DD joint system combining the POS tagger, the parser and the compound recognisers.
System Recall Precision Fscore EX Tag
LORG 82.01 82.37 82.19 18.06 97.35
pipeline POS? LORG 82.36 82.59 82.47 19.22 97.73
DD POS + LORG 82.48 82.73 82.61 19.19 97.84
DD MWE + LORG 82.91 83.07 82.99 19.19 97.41
DD POS + MWE + LORG 83.38 83.42 83.40 20.73 97.85
pipeline MWE MERGE? LORG? UNMERGE 82.56 82.63 82.59 18.79 97.39
pipeline MWE MERGE/POS? LORG? UNMERGE 82.73 82.64 82.69 20.02 97.57
Table 3: Parse evaluation on dev set (recall, precision and F-score, exactness and POS tagging).
Table 4 shows evaluation results of our best system and comparisons with baseline or alternative
configurations on the SPMRL 2013 test set.
Parsing The DD method performs better than our baseline, and better than the best system in the
SPMRL 2013 shared task (Bj?rkelund et al., 2013). This system is a pipeline consisting of a
morpho-syntactic tagger with a very rich and informative tag set, a product of PCFG-LAs, and
a parse reranker. Although this approach is quite different from ours, we believe our system is more
accurate overall because our method is more resilient to an error from one of its components.
Compound recognition and labelling For the task of recognition alone, where only the frontiers are
evaluated, the DD combinations of CRFs performs better than the best single CRF which itself
performs better than the parser alone, but the complete architecture is again the best system. If we
also evaluate compound POS tags, we get similar results. The DD combination is always beneficial.
System Recall Precision Fscore EX Tag
LORG 82.79 83.06 82.92 22.00 97.39
(Bj?rkelund et al., 2013) ? ? 82.86 ? ?
DD POS + MWE + LORG 83.74 83.85 83.80 23.81 97.87
compound recognition LORG 78.03 78.63 78.49 ? ?
compound recognition best single CRF (partial-internal) 78.27 82.84 80.49 ? ?
compound recognition MWE 79.68 83.50 81.54 ? ?
compound recognition DD POS + MWE + LORG 80.76 84.19 82.44 ? ?
compound recognition + POS tagging LORG 75.43 75.71 75.57 ? ?
compound recognition + POS tagging MWE 76.49 80.10 78.28 ? ?
compound recognition + POS tagging DD POS + MWE + LORG 77.92 81.23 79.54 ? ?
Table 4: Evaluation on SPMRL 2013 test set: parsing (first 3 lines), and compound recognition.
7 Conclusion
We have presented an original architecture for the joint task of syntactic parsing and compound recogni-
tion. We first introduced a combination of recognisers based on linear chain CRFs, and a second system
that adds in a phrase-structure parser. Our experimental prototype improves state-of-the-art on the French
SPMRL corpus.
In order to derive decoding algorithms for these joint systems, we used dual decomposition. This
approach, leading to simple and efficient algorithms, can be extended further to incorporate additional
components. As opposed to pipeline approaches, a component prediction can be corrected if its solution
is too far from the general consensus. As opposed to joint systems relying on pure dynamic programming
to build a complex single system, the search space does not grow exponentially, so we can avoid using
pruning heuristics such as beam search. The price to pay is an iterative algorithm.
Finally, this work paves the way towards component-based NLP software systems that perform com-
plex processing based on consensus between components, as opposed to previous pipelined approaches.
1883
Acknowledgements
We would like to thank Nadi Tomeh and Davide Buscaldi for their feedback on an early draft of this
paper, as well as the three anonymous reviewers for their helpful comments. This work is supported by
a public grant overseen by the French National Research Agency (ANR) as part of the Investissements
d?Avenir program (ANR-10-LABX-0083).
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel. 2003. Building a treebank for French. In Anne Abeill?,
editor, Treebanks. Kluwer, Dordrecht.
Abhishek Arun and Frank Keller. 2005. Lexicalization in crosslinguistic probabilistic parsing: The case of French.
In Proceedings of the Annual Meeting of the Association For Computational Linguistics (ACL?05), pages 306?
313.
Anders Bj?rkelund, Rich?rd Farkas, Thomas M?ller, and Wolfgang Seeker. 2013. (re) ranking meets morphosyn-
tax: State-of-the-art results from the spmrl 2013 shared task. In Proceedings of the 4th Workshop on Statistical
Parsing of Morphologically Rich Languages: Shared Task.
Matthieu Constant and Isabelle Tellier. 2012. Evaluating the impact of external lexical resources into a CRF-based
multiword segmenter and part-of-speech tagger. In Proceedings of the 8th conference on Language Resources
and Evaluation.
Matthieu Constant, Anthony Sigogne, and Patrick Watrin. 2012. Discriminative strategies to integrate multi-
word expression recognition and parsing. In Proceedings of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL?12), pages 204?212.
Matthieu Constant, Marie Candito, and Djam? Seddah. 2013a. The LIGM-Alpage Architecture for the SPMRL
2013 Shared Task: Multiword Expression Analysis and Dependency Parsing. In Proceedings of the 4th Work-
shop on Statistical Parsing of Morphologically Rich Languages: Shared Task, Seattle, WA.
Matthieu Constant, Joseph Le Roux, and Anthony Sigogne. 2013b. Combining compound recognition and PCFG-
LA parsing with word lattices and conditional random fields. ACM Transaction in Speech and Language Pro-
cessing, 10(3).
Blandine Courtois, Myl?ne Garrigues, Gaston Gross, Maurice Gross, Ren? Jung, Michel Mathieu-Colas, Anne
Monceaux, Anne Poncet-Montange, Max Silberztein, and Robert Viv?s. 1997. Dictionnaire ?lectronique
DELAC : les mots compos?s binaires. Technical Report 56, University Paris 7, LADL.
Jenny Rose Finkel and Christopher D. Manning. 2009. Joint parsing and named entity recognition. In Proceed-
ings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the
Association for Computational Linguistics, NAACL ?09, pages 326?334, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Spence Green, Marie-Catherine de Marneffe, John Bauer, and Christopher D. Manning. 2011. Multiword expres-
sion identification with tree substitution grammars: A parsing tour de force with french. In Proceedings of the
conference on Empirical Method for Natural Language Processing (EMNLP?11), pages 725?735.
Spence Green, Marie-Catherine de Marneffe, and Christopher D Manning. 2013. Parsing models for identifying
multiword expressions. Computational Linguistics, 39(1):195?227.
Nikos Komodakis, Nikos Paragios, and Georgios Tziritas. 2007. MRF optimization via dual decomposition:
Message-passing revisited. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on,
pages 1?8. IEEE.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition
for parsing with non-projective head automata. In Proceedings of the Conference on Empirical Methods in
Natural Language Processing.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on
Machine Learning.
1884
Thomas Lavergne, Olivier Capp?, and Fran?ois Yvon. 2010. Practical very large scale CRFs. In Proceedings the
48th Annual Meeting of the Association for Computational Linguistics (ACL?10), pages 504?513.
Joseph Le Roux, Antoine Rozenknop, and Jennifer Foster. 2013. Combining PCFG-LA models with dual de-
composition: A case study with function labels and binarization. In Association for Computational Linguistics,
editor, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, October.
Slav Petrov. 2010. Products of random latent variable grammars. In Proceedings of the Annual Conference of the
North American Chapter of the Association for Computational Linguistics - Human Language Technologies,
pages 19?27.
Odile Piton, Denis Maurel, and Claude Belleil. 1999. The Prolex Data Base : Toponyms and gentiles for
NLP. In Proceedings of the Third International Workshop on Applications of Natural Language to Data Bases
(NLDB?99), pages 233?237.
Alexander Rush, David Sontag, Michael Collins, and Tommi Jaakola. 2010. On dual decomposition and linear
programming relaxations for natural language processing. In ACL, editor, Proceedings of the Conference on
Empirical Methods in Natural Language Processing. Association for Computational Linguistics.
Beno?t Sagot. 2010. The lefff, a freely available, accurate and large-coverage lexicon for french. In Proceedings
of the 7th International Conference on Language Resources and Evaluation.
Djam? Seddah, Reut Tsarfaty, Sandra K
?
?ubler, Marie Candito, Jinho Choi, Rich?rd Farkas, Jennifer Foster, Iakes
Goenaga, Koldo Gojenola, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolfgang Maier,
Joakim Nivre, Adam Przepiorkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika Vincze, Marcin
Woli?nski, Alina Wr?blewska, and Eric Villemonte de la Cl?rgerie. 2013. Overview of the spmrl 2013 shared
task: A cross-framework evaluation of parsing morphologically rich languages. In Proceedings of the 4th
Workshop on Statistical Parsing of Morphologically Rich Languages, Seattle, WA.
Veronica Vincze, Istv?n Nagy, and G?bor Berend. 2011. Multiword expressions and named entities in the wiki50
corpus. In Proceedings of the conference on Recent Advances in Natural Language Processing (RANLP?11),
pages 289?295.
1885
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1158?1169,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Combining PCFG-LA Models with Dual Decomposition: A Case Study with
Function Labels and Binarization
Joseph Le Roux?, Antoine Rozenknop?, Jennifer Foster?
? Universite? Paris 13, Sorbonne Paris Cite?, LIPN, F-93430, Villetaneuse, France
? NCLT/CNGL, School of Computing, Dublin City University, Dublin 9, Ireland
joseph.leroux@lipn.fr antoine.rozenknop@lipn.fr jfoster@computing.dcu.ie
Abstract
It has recently been shown that different NLP
models can be effectively combined using
dual decomposition. In this paper we demon-
strate that PCFG-LA parsing models are suit-
able for combination in this way. We exper-
iment with the different models which result
from alternative methods of extracting a gram-
mar from a treebank (retaining or discarding
function labels, left binarization versus right
binarization) and achieve a labeled Parseval
F-score of 92.4 on Wall Street Journal Sec-
tion 23 ? this represents an absolute improve-
ment of 0.7 and an error reduction rate of 7%
over a strong PCFG-LA product-model base-
line. Although we experiment only with bina-
rization and function labels in this study, there
is much scope for applying this approach to
other grammar extraction strategies.
1 Introduction
Because of the large amount of possibly contra-
dictory information contained in a treebank, learn-
ing a phrase-structure-based parser implies making
several choices regarding the prevalent annotations
which have to be kept ? or discarded ? in order to
guide the learning algorithm. These choices, which
include whether to keep function labels and empty
nodes, how to binarize the trees and whether to alter
the granularity of the tagset, are often motivated em-
pirically by parsing performance rather than by the
different aspects of the language they may be able to
capture.
Recently Rush et al (2010), Martins et al (2011)
and Koo et al (2010) have shown that Dual De-
composition or Lagrangian Relaxation is an elegant
S
fedcb
(a) Original example
S
?S?
?S?
?S?
fe
d
c
b
(b) Left Binarized example
S
f?S?
e?S?
d?S?
cb
(c) Right Binarized example
Figure 1: Binarization with markovization
framework for combining different types of NLP
tasks or for building parsers from simple slave pro-
cesses that only check partial well-formedness. Here
we propose to follow this idea, but with a different
objective. We want to mix different parsers trained
on different versions of a treebank each of which
makes some annotation choices in order to learn
more specific or richer information. We will use
state-of-the-art unlexicalized probabilistic context-
free grammars with latent annotations (PCFG-LA)
in order to compare our approach with a strong base-
line of high-quality parses. Dual Decomposition is
used to mix several systems (between two and four)
that may in turn be combinations of grammars, here
products of PCFG-LAs (Petrov, 2010). The systems
being combined make different choices with regard
to i) function labels and ii) grammar binarization.
Common sense would suggest that information in
the form of function labels ? syntactic labels such as
SBJ and PRD and semantic labels such as TMP and
LOC ? might help in obtaining a fine-grained anal-
ysis. On the other hand, the independence hypothe-
1158
sis on which CFGs rely and on which most popular
parsers are based may be too strong to learn the de-
pendencies between functions across the parse trees.
Also, the number of parameters increases with the
use of function labels and this can affect the learn-
ing process.
At first glance, binarization need not be an is-
sue, as CFGs admit a binarized form recognizing
exactly the same language. But binarization can be
associated with horizontal markovization and in this
case the recognized language will differ. Further-
more this can impose an unwanted emphasis on what
frontier information is more relevant to learning (be-
ginning or end of constituents). In the toy exam-
ple of Figure 1, the original grammar consisting of a
unique rule extracted from one tree only recognizes
the string bcdef, while the grammar learned from
the left binarized and markovized tree recognizes
(among others) bcdef and bdcef and the gram-
mar learned from the right binarized and markovized
tree recognizes (among others) bcdef and bcedf.
We find that i) retaining the function labels in non-
terminal categories loses its negative impact on pars-
ing as the number of grammars increases in PCFG-
LA product models, ii) the function labels them-
selves can be recovered with near state-of-the-art-
accuracy, iii) combining grammars with and without
function labels using dual decomposition is bene-
ficial, iv) combining left and right-binarized gram-
mars using dual decomposition also leads to bet-
ter trees and, v) our best results (a Parseval la-
beled F-score of 92.4, a Stanford labeled attach-
ment score (LAS) of 93.0 and a penn2malt unla-
beled attachment score (UAS) of 94.3 on Section 23
of the Wall Street Journal) are obtained by combin-
ing three grammars which encode different function
label/binarization decisions.
The paper is organized as follows. ? 2 reviews
related work. ? 3 presents approximate PCFG-LA
parsers as linear models, while ? 4 shows how we
can use dual decomposition to derive an algorithm
for combining these models. Experimental results
are presented and discussed in ? 5.
2 Related Work
Parser Model Combination It is well known that
improved parsing performance can be achieved by
leveraging the alternative perspectives provided by
several parsing models rather than relying on just
one. Examples are parser co-training (Steedman
et al, 2003; Sagae and Tsujii, 2007), voting over
phrase structure constituents or dependency arcs
(Henderson and Brill, 1999; Sagae and Tsujii, 2007;
Surdeanu and Manning, 2010), dependency pars-
ing stacking (Nivre and McDonald, 2008), product
model PCFG-LA parsing (Petrov, 2010), using dual
decomposition to combine dependency and phrase
structure models (Rush et al, 2010) or several non-
projective dependency parsing models (Koo et al,
2010; Martins et al, 2011), and using expecta-
tion propagation, a related approach to dual decom-
position, to combine lexicalized, unlexicalized and
PCFG-LA models (Hall and Klein, 2012). In this
last example, the models must factor in the same
way: in other words, the grammars must use the
same binarization scheme. In our study, we employ
PCFG-LA product models with dual decomposition,
and we relax the constraints on factorization, as we
require only a loose coupling of the models.
Function Label Parsing Although function labels
have been available in the Penn Treebank (PTB) for
almost twenty years (Marcus et al, 1994), they have
been to a large extent overlooked in English parsing
research ? most studies that report parsing results
on Section 23 of the Wall Street Journal (WSJ) use
parsing models that are trained on a version of the
WSJ trees where the function labels have been re-
moved. Notable exceptions are Merlo and Musillo
(2005) and Gabbard et al (2006) who each trained
a parsing model on a version of the PTB with func-
tion labels intact. Gabbard et al (2006) found that
parsing accuracy was not affected by keeping the
function labels. There have also been attempts to
use machine learning to recover the function labels
post-parsing (Blaheta and Charniak, 2000; Chrupala
et al, 2007). We recover function labels as part of
the parsing process, and use dual decomposition to
combine parsing models with and without function
labels. We are not aware of any other work that
leverages the benefits of both types of models.
Grammar Binarization Matsuzaki et al (2005)
compare binarization strategies for PCFG-LA pars-
ing, and conclude that the differences between them
have a minor effect on parsing accuracy as the num-
1159
ber of latent annotations increases beyond two. Hall
and Klein (2012) are forced to use head binarization
when combining their lexicalized and unlexicalized
parsers. Dual decomposition allows us to combine
models with different binarization schemes.
3 Approximation of PCFG-LAs as Linear
Models
In this section, we explain how we can use PCFG-
LAs to devise linear models suitable for the dual de-
composition framework.
3.1 PCFG-LA
Let us recall that PCFG-LAs are defined as tuples
G = (N , T ,H,RH, S, p) where:
? N is a set of observed non-terminals, among
which S is the distinguished initial symbol,
? T is a set of terminals (words),
? H is a set of latent annotations or hidden states,
? RH is a set of annotated rules, of the form
a[h1] ? b[h2] c[h3] for internal rules1 and
a[h1] ? w for lexical rules. Here a, b, c ? N
are non-terminals, w ? T is a terminal and
h1, h2, h3 ? H are latent annotations. Follow-
ing Cohen et al (2012) we also define the set of
skeletal rules R, in other words, rules without
hidden states, of the form a? b c or a? w.
? p : RH ? R?0 defines the probabilities asso-
ciated with rules conditioned on their left-hand
side. Like Petrov and Klein (2007), we impose
that the initial symbol S has only one latent an-
notation. In other words, among rules with S
on the left-hand side, only those of the form
S[0]? ? are inRH.
With such a grammar G we can define probabil-
ities over trees in the following way. We will con-
sider two types of trees, annotated trees and skeletal
trees. An annotated tree is a sequence of rules from
RH, while a skeletal tree is a sequence of skeletal
rules from R. An annotated tree TH is obtained by
left-most derivation from S[0]. Its probability is:
1For brevity and without loss of generality, we omit unary
and n-ary rules, as PCFG-LA admit a Chomsky normal form.
p(TH) =
?
r?TH
p(r) (1)
We define a projection ? from annotated trees to
skeletal trees. ?(TH) is a tree T isomorphic to TH
with the same terminal and non-terminal symbols la-
beling nodes, without hidden states. The probability
of a skeletal tree T is a sum of the probabilities of
all annotated trees that admit T as their projection.
p(T ) =
?
TH???1(T )
?
r?TH
p(r) (2)
PCFG-LA parsing amounts to, given a sequence
of words, finding the most probable skeletal tree
with this sequence as its yield according to a gram-
mar G:
T ? = arg max
T
?
TH???1(T )
?
r?TH
p(r) (3)
Because of this alternation of sum and products,
the parsing problem is intractable. Moreover, the
PCFG-LAs do not belong to the family of linear
models that are assumed in the Lagrangian frame-
work of (Rush and Collins, 2012). We now turn to
approximations for the parsing problem in order to
address both issues.
3.2 Variational Inference and MaxRule
Variational inference is a common technique to ap-
proximate a probability distribution p with a cruder
one q, as close as possible to the original one,
by minimizing the Kullback-Liebler divergence be-
tween the two ? see for instance (Smith, 2011),
chapter 5 for an introduction. Matsuzaki et al
(2005) showed that one can easily find such a cruder
distribution for PCFG-LAs and demonstrated exper-
imentally that this approximation gives good results.
More precisely, they find a PCFG that only rec-
ognizes the input sentence where the probabilities
q(rs) of the rules are set according to their marginal
probabilities in the original PCFG-LA parse forest.
The parameters rs are skeletal rules with span infor-
mation. Distribution q is defined in Figure 2.
Other approximations are possible. In particu-
lar, Petrov and Klein (2007) found that normaliz-
ing by the forest probability (in other words the in-
side probability of the root node) give better exper-
1160
score(a? b c, i, j, k) =
?
x,y,z?H
P i,kout
(
a[x]
)
? p
(
a[x]? b[y] c[z]
)
? P i,jin
(
b[y]
)
? P j,kin
(
c[z]
)
norm(a? b c, i, j, k) =
?
x?H
P i,kin
(
a[x]
)
? P i,kout
(
a[x]
)
score(a? w, i) =
?
x?H
P i,iout
(
a[x]
)
? p
(
a[x]? w
)
norm(a? w, i) =
?
x?H
P i,iin
(
a[x]
)
? P i,iout
(
a[x]
)
q(rs) =
[
score(rs)
norm(rs)
(Variational Inference)
]
or
[
score(rs)
P 0,nin (S[0])
(MaxRule-Product)
]
Figure 2: Variational Inference for PCFG-LA. Pin and Pout denote inside and outside probabilities.
imental results although its interpretation as varia-
tional inference is still unclear. This approximation
is called MaxRule-Product and amounts to replacing
the norm function (see Figure 2).
In both cases, the probability of a skeletal tree
now becomes a simple product of parameters asso-
ciated with anchored skeletal rules. For our purpose,
the consequence is twofold:
1. The parsing problem becomes tractable by ap-
plying standard PCFG algorithms relying on
dynamic programming (CKY for example).
2. Equivalent to probability, a score ? can be de-
fined as the logarithm of the probability. The
parsing problem becomes2:
T ? = arg max
T
?
rs?T
q(rs)
= arg max
T
?
rs?T
log q(rs)
= arg max
T
?
rs?F
wrs ? 1{rs ? T}
= arg max
T
?(T )
Thus, from a PCFG-LA we are able to de-
fine a linear model whose parameters are the log-
probabilities of the rules in distribution q.
2We denote the parse forest of a sentence by F and the char-
acteristic function of a set by 1.
3.3 Products of PCFG-LAs
Although PCFG-LA training is beyond the scope
of this paper, it is worthwhile mentioning that the
most common way to learn their parameters relies
on Expectation-Maximization which is not guaran-
teed to find the optimal estimation. Fortunately, this
can be partly overcome by combining grammars that
only differ on the initial parameterization of the EM
algorithm. The probability of a skeletal tree is the
product of the probabilities assigned by each single
grammar Gi.
T ? = arg max
T
n?
i=1
qGi(T ) (4)
Since grammars only differ by their numerical pa-
rameters (i.e. skeletal rules are the same), inference
can be efficiently implemented using dynamic pro-
gramming (Petrov, 2010).
Scoring with n such grammars now becomes:
T ? = arg max
T
n?
i=1
?
r?T
log qGi(r) (5)
= arg max
T
?
r?T
n?
i=1
log qGi(r) (6)
The distributions qGi still have to be computed in-
dependently ? and possibly in parallel ? but the final
decoding can be performed jointly. This is still a
linear model for PCFG-LA parsing, but restricted to
grammars that share the same skeletal rules.
1161
4 Dual Decomposition
In this section, we show how we derive an algorithm
to work out the best parse according to a set of n
grammars that do not share the exact same skele-
tal rules. As such, the grammars? product cannot
be easily conducted inside the parser to produce and
score a same and unique best tree, and we now con-
sider a c(ompound)-parse as a tuple (T1 . . . Tn) of
n compatible trees. Each grammar Gi is responsi-
ble for scoring tree Ti, and we seek to obtain the
c-parse that maximizes the sum of the scores of its
different trees. For a c-parse to be consistent, we
have to precisely define the parts on which the trees
must agree to be compatible with each other, so that
we can model these as agreement constraints.
4.1 Compound Parse Consistency
Let us suppose we have a set of phrase-structure
parsers trained on different versions of the same
treebank. Hence, some elements in the charts will
either be the same or can be mapped to each other
provided an equivalence relation and we define con-
sensus between parsers on these elements.
When the grammar is not functionally annotated,
phrase-structure trees can be decomposed into a set
of anchored (syntactical) categories Xs, asserting
that a category X is in the tree at position3 s. Thus,
such a tree T can be described by means of a boolean
vector z(T ) indexed by anchored labels Xs, where
z(T )Xs = 1 if Xs is in T and 0 otherwise.
We will differentiate the set of natural non-
terminals that occur in the treebanks from the set
of artificial non-terminals that do not occur in the
treebank and are the results of a binarization with
markovization. As these artificial non-terminals dis-
appear after reversing binarization in solution trees,
they do not play any role in the consensus between
parsers, and we only consider natural non-terminals
in the set of anchored labels.
When the grammar is functionally annotated,
each label X? in a tree is a pair (X,F ), where X
is a syntactical category and F is a function label.
In this case, in order to manage the consensus with
3The anchor s of a label is composed of the span (i, j), de-
noting that the label covers terminals of the input sentence from
index i to index j. In case the grammar contains unary non-
lexical rules, the anchor also discriminates the different posi-
tions in a sequence of unary rules.
non-functional grammars, we decompose such a tree
into two sets: a set of anchored categories Xs and a
set of anchored function labels Fs. Thus, a tree T
can be described by means of two boolean vectors:
? z(T ) indexed by anchored categories Xs,
z(T )Xs = 1 if there exists a function label F
so that (X,F )s is in T , and 0 otherwise;
? ?(T ) indexed by anchored function labels Fs,
?(T )Fs = 1 if there exists a category X so that
(X,F )s is in T , and 0 otherwise.
In the present work, a compound parse (T1 . . . Tn)
is said to be consistent iff every tree shares the same
set of anchored categories, i.e. iff:
?(i, j) ? J1, nK2, z(Ti) = z(Tj)
4.2 Combining Parsers through Dual
Decomposition
Like previous applications, we base our reasoning
on the assumption that computing the optimal score
with each grammar Gi can be efficiently calculated,
which is the case for approximate PCFG-LA pars-
ing. We follow the presentation of the decomposi-
tion from (Martins et al, 2011) to explain how we
can combine several PCFG-LA parsers together.
For a sentence s, we want to obtain the best con-
sistent compound parse from a set of n parsers:
(P ) : find arg max
(T1...Tn)?C
n?
p=1
?p(Tp) (7)
s.t. ?(i, j) ? J1, nK2, z(Ti) = z(Tj) (8)
where C = F1(s) ? ... ? Fn(s) is the product of
parse forests F i(s), and F i(s) is the set of trees in
grammar Gi whose yields are the input sentence s.
Solving this problem with an exact algorithm is
intractable. While artificial nodes could be inferred
using a traditional parsing algorithm based on dy-
namic programming (i.e. CKY), the natural nodes
require a coupling of the parsers? items to enforce
the fact that natural daughter nodes must be identical
(or equivalent) with the same spans for all parsers.
Since the debinarization of markovized rules enables
the creation of arbitrarily long n-ary rules, in the
worst case the number of natural daughters to check
is exponential in the size of the span to infer. Even if
1162
we bound the length of debinarized rules, the prob-
lem is hardly tractable.
As this problem is intractable, even for approxi-
mate PCFG-LA parsing, we apply the iterate method
presented in (Komodakis et al, 2007) for MRFs,
also applied for joint tasks in NLP such as combined
parsing and POS tagging in (Rush et al, 2010).
First, we introduce a witness vector u in order to
simplify constraints in (8). Problem (P ) can then be
written in an equivalent form :
(P ) : find oP = max
(T1...Tn)?C
n?
i=1
?i(Ti) (9)
s.t. ?i ? J1, nK, z(Ti) = u (10)
Next, we proceed to a Lagrangian decomposition.
This decomposition is a two-step process:
Step 1 (Relaxation): the coupling constraints (10)
are removed by introducing a vector of Lagrange
multipliers ?i = (?i,Xs)Xs for each parser i, in-
dexed by anchored categories Xs, and writing the
equivalent problem:
(RP ) : oRP = max
u, T1...n
min
?
f(u, T1...n,?)
where:
f(u, T1...n,?) =
?
i
?i(Ti) +
?
i
(z(Ti)? u) ? ?i
Intuitively, we can see the equivalence of (RP )
and (P ) with the following reasoning:
? whenever all constraints (10) are met, the sec-
ond sum in f is nullified and f(u, T1...n,?) =?
i ?i(Ti), which is a finite value and precisely
the objective function maximized in (P );
? if there is at least one (i,X, s) such that
z(Ti)Xs 6= uXs , then the value of
?
i(z(Ti) ?
u) ? ?i can be made arbitrarily small by
an appropriate choice of ?i,Xs ; in this case,
min? f(u, T1...n,?) = ??. Thus, (RP ) can
not reach its maximum at a point where con-
straints (10) are not satisfied.
Step 2 (dualization): the dual problem (LP ) is ob-
tained by permuting max and min in (RP ):
(LP1) : oLP = min
?
max
u, T1...n
f(u, T1...n,?)
Finally, u can be removed from (LP1) by adding
the constraint:
?
i ?i = 0. As a matter of fact,
one can see that if this constraint is not matched,
maxu,T1...n f(u, T1...n,?) = +? and (LP1) can
not reach its minimum on such a point. We can now
find the maximum of f by maxing each Ti indepen-
dently of each other. The dual problem becomes:
(LP ) : oLP = min
?
n?
i=1
max
Ti?Fi
(?i(Ti) + z(Ti) ? ?i)
s.t.
?
i
?i = 0
Minimization in (LP ) can be solved iteratively
using the projected subgradient method. Finding a
subgradient amounts to computing the optimal so-
lution (Rush and Collins, 2012) for each of the n
subproblems (the slave problems in the terminol-
ogy of (Martins et al, 2011) and (Komodakis et al,
2007)) which can be done efficiently, by incorpo-
rating the calculation of the penalties in the parsing
algorithm, and in parallel. Until the agreement con-
straints are met (or a maximal number of iterations
? ), the Lagrangian multipliers are updated according
to the deviations from the average solutions (i.e. up-
dates are zeros for a natural span if the parsers agree
on it). This leads to Algorithm 1.
It should be noted that the DP charts are built and
pruned during the first iteration only (t = 0); fur-
ther iterations do not require recreating the DP chart,
which is memory intensive and time consuming, nor
recomputing the approximate distribution for varia-
tional inference. As DP on the pruned charts is a fast
process, the bottleneck of the algorithm still is in the
first calculation of slave solutions.
The stepsize sequence (?t)0?t must be diminish-
ing and non-summable, that is to say: ?t, ?t ? 0,
limt?? ?t = 0 and
??
t=0 ?t = ?. In practice, we
set ?t = 11+c(t) where c(t) is the number of times the
objective function oP has increased since iterations
began.
Solving (P): it is easy to see that oLP is an up-
per bound of oP , but we do not necessarily have
1163
Algorithm 1 Find best compound parse with con-
straints on natural spans
Require: n parsers {pi}1?i?n
for all i, syntactical category X , anchor s do
?(0)i,Xs = 0
end for
for t = 0? ? do
for all parsers pi do
T (t)i ? arg maxT?Fi
(
?i(T ) + z(T ) ? ?
(t)
i
)
end for
for all parsers pi do
?(t)i ? ?t
(
z
(
T (t)i
)
?
?
1?j?n z
(
T (t)j
)
n
)
?(t+1)i ? ?
(t)
i + ?
(t)
i
end for
if ?(t)i = 0 for all i then
Exit loop
end if
end for
return (T (?)1 , ? ? ? , T
(?)
n )
strong duality (i.e. oLP = oP ) due to the facts that
parse forests are discrete sets. Furthermore, they get
pruned independently of each other. Thus, the algo-
rithm is not guaranteed to find a t such that z(T (t)i )
is the same for every parser i. However ? see (Koo
et al, 2010) ? if it does reach such a state, then we
have the guarantee of having found an exact solution
of the primal problem (P ). We show in the experi-
ments that this occurs very frequently.
5 Experiments
5.1 Experimental Setup
We perform our experiments on the WSJ sections of
the PTB with the usual split: sections 2 to 21 for
training, section 23 for testing, and we run bench-
marks on section 22. evalb is used for evaluation.
We use the LORG parser modified with Algo-
rithm 1. 4 All grammars are trained using 6
split/merge EM cycles. For the handling of unknown
words, we removed all words occurring once in the
training set and replaced them by their morpholog-
ical signature (Attia et al, 2010). Grammars for
products are obtained by training with 16 random
seeds for each setting. We use the approximate al-
4The LORG parser is available at https://github.
com/CNGLdlab/LORG-Release and the modification at
https://github.com/jihelhere/LORG-Release/
tree/functional_c11.
gorithm MaxRule-Product (Petrov and Klein, 2007).
The basic settings are a combination of the two
following parameters:
left or right binarization: we conjecture that this
affects the quality of the parsers by impacting the
recognition of left and right constituent frontiers.
We set vertical markovization to 1 (no parent anno-
tation) and horizontal markovization to 0 (we drop
all left/right annotations).
with or without functional annotations: in par-
ticular when non-terminals are annotated with mul-
tiple functions, all are kept.
5.2 Products of Grammars
We first evaluate each setting on its own before com-
bining them. We test the 4 different settings on the
development set, using a single grammar or a prod-
uct of n grammars. Results are reported on Figure 3.
We can see that right binarization performs better
than left binarization. Contrary to the results of Gab-
bard et al (2006), function labels are detrimental for
parsing performance for one grammar only. How-
ever, they do not penalize performance when using
the product model with 8 grammars or more.
n
F
1 2 4 8 16
89
90
91
92
93
Func Right
No Func Right
No Func Left
Func Left
Figure 3: F1 for products of n grammars on the dev. set
EM is not guaranteed to find the optimal model
and the problem is made harder by the increased
number of parameters. Product models effectively
alleviate this curse of dimensionality by letting some
models compensate for the errors made by others.
On the other hand, as differences between left
and right binarization settings remain over all prod-
uct sizes, right binarization seems more useful on
its own. The first part of Table 1 gives F-score and
1164
Exact Match results of the product models with 16
grammars on the development set.
5.3 Combinations with Dual Decomposition
We now turn to a series of experiments combining
product models of 16 grammars. In all these experi-
ments, we set the maximum number of iterations in
Algorithm 1 to 1000. The system then returns the
first element of the c-parse. We first try to combine
two settings in four different combinations:
DD Right Bin the two right-binarized systems ?
with and without functions ? the system returns
the function-labeled parse;
DD Left Bin the two left-binarized systems ? with
and without functions ? the system returns the
function-labeled parse;
DD Func the two systems with functions ? left and
right binarization ? the system returns the right-
binarized parse;
DD No Func the two systems without functions ?
left and right binarization ? the system returns
the right-binarized parse;
Results are in the second part of Table 1. Un-
surprisingly, the best configuration is the one com-
bining the two best product systems (with right bi-
narization) but all combined systems perform better
than their single components.
Setting F EX
No Func Right 92.26 42.97
No Func Left 91.92 42.91
Func Right 92.37 43.35
Func Left 91.95 43.15
DD Right Bin 92.71 44.44
DD Left Bin 92.23 43.97
DD Func 92.51 44.79
DD No Func 92.52 44.08
DD3 92.86 45.03
DD4 92.82 45.14
Table 1: Parse evaluation on development set.
We also combine 3 and 4 parsers to see if combin-
ing the above DD Right Bin setting with informa-
tion that could improve the recognition of beginning
of constituents can be helpful. We have 2 settings:
DD3 The 2 right-binarized parsers combined with
the left binarized parser without functions,
DD4 The 4 parsers together.
In both cases the system returns the right-
binarized function annotated parse. The results are
shown in the last part of Table 1. These 2 new con-
figurations give similar F-scores, better than all 2-
parser configurations.
We conclude from these results that left-
binarization and right-binarization capture different
linguistic aspects, even in the case of heavy horizon-
tal markovization, and that the method we propose
enables a practical integration of these models.
Table 2 shows for each setting how often the sys-
tems agree before 1000 iterations of Algorithm 1.
As one might expect, the more diverse the systems
are, the lower the rate of agreement.
Setting Rate
DD Right Bin 99.24
DD Left Bin 99.12
DD Func 98.53
DD No Func 99.12
DD3 96.18
DD4 94.53
Table 2: Rate of certificates of optimality on the dev set.
5.4 Evaluation of Function Labeling
We also evaluate the quality of the function labels.
We compare the results obtained directly from the
parser output with results obtained with Funtag, a
state-of-the-art functional tagger that is applied on
parser output, using a gold model trained on sections
02 to 21 of the WSJ (Chrupala et al, 2007).
Setting SYSTEM FUN FUNTAG
No Func Right ? 90.41
No Func Left ? 90.26
Func Right 89.61 90.37
Func Left 89.29 90.40
DD Right Bin 89.50 90.38
DD Left Bin 89.11 90.31
DD Func 89.54 90.49
DD No Func ? 90.36
DD3 89.48 90.42
DD4 89.57 90.45
Table 3: Function labeling F1 on development set.
The results are shown in Table 3. First, we can
see that the parser output is always outperformed by
Funtag. This is expected from a context-free parser
1165
that has a limited domain of locality with strong in-
dependence constraints, compared to a voted-SVM
classifier that can rely on arbitrarily rich features.
Second, the quality of the Funtag prediction seems
to be influenced by the fact that parser already han-
dle functions and by the accuracy of the parser (Par-
seval F-score). This is because we use a model
trained on the gold reference and so the closer the
parser output is from the reference, the better the
prediction. On the other hand, this is not the case
with parser predicted functions, where the best sys-
tem is the right-binarized product model with func-
tions, with very similar performance obtained by the
combinations consisting of 2 function parsers, set-
tings DD Func and DD4. This tends to indicate
that the constraints we have set to define consisten-
cies in c-parses, focusing on syntactical categories,
do not help in retrieving better function labels. This
suggests some possible further improvements where
parsers with functional annotations should be forced
to agree on these too.
5.5 Evaluation of Dependencies
Setting Stanford LTH p2m
LAS UAS LAS UAS UAS
Func Right 92.18 94.32 89.51 93.92 94.2
No Func Right 92.03 94.47 65.31 92.22 94.2
Func Left 91.86 94.06 89.28 93.75 93.9
No Func Left 91.83 94.29 65.33 92.18 94.1
DD Right Bin 92.56 94.60 89.81 94.17 94.5
DD Left Bin 92.01 94.38 89.62 94.05 94.2
DD Func 92.19 94.36 89.67 94.06 94.2
DD No Func 92.19 94.57 65.44 92.37 94.3
DD3 92.77 94.79 90.04 94.33 94.5
DD4 92.59 94.62 89.95 94.24 94.4
Table 4: Dependency accuracies on the dev set
Dependency-based evaluation of phrase structure
parser output has been used in recent years to pro-
vide a more rounded view on parser performance
and to compare with direct dependency parsers (Cer
et al, 2010; Petrov et al, 2010; Nivre et al, 2010;
Foster et al, 2011; Petrov and McDonald, 2012).
We evaluate our various parsing models on their
ability to recover three types of dependencies: basic
Stanford dependencies (de Marneffe and Manning,
2008)5, LTH dependencies (Johansson and Nugues,
5We used the latest version at the time of writing, i.e. 3.20.
2007)6 and penn2malt dependencies.7 The latter
are a simpler version of the LTH dependencies but
are still used when reporting unlabeled attachment
scores for dependency parsing.
The results, shown in Table 4, mirror the con-
stituency evaluation results in that the dual decom-
position results tend to outperform the basic product
model results, and combining three or four gram-
mars using dual decomposition yields the highest
scores. The differences between the Func and No
Func results highlight an important difference be-
tween the Stanford and LTH dependency schemes.
The tool used to produce Stanford dependencies has
been designed to work with phrase structure trees
that do not contain function labels. In contrast, the
LTH tool makes use of function label information
in phrase structure trees. Thus, their availability re-
sults in only a moderate improvement in LAS for the
Stanford dependencies and a very striking improve-
ment for the LTH dependencies. By retaining func-
tion labels during parsing, we have shown that LTH
dependencies can be recovered with a high level of
accuracy without having to resort to a post-parsing
function labeling step.
5.6 Test Set Results
We now evaluate our various systems on the test set
(the first half of Table 5) and compare these results
with state-of-the-art systems (the second half of Ta-
ble 5). We present parser accuracy results, measured
using Parseval F-score and penn2malt UAS, and, for
our systems, function label accuracy for labels pro-
duced during parsing and after parsing using Funtag.
We also carried out statistical significance testing8
on the F-score differences between our various sys-
tems on the development and test sets. The results
6nlp.cs.lth.se/software/treebank_converter. It
is recommended that LTH is used with the version of the Penn
Treebank which contains the more detailed NP bracketing pro-
vided by Vadas and Curran (2007). However, to facilitate com-
parison with other parsers and dependency schemes, we did not
use it in our experiments. We ran the converter with the right-
Branching=false option to indicate that we are using the version
without extra noun phrase bracketing.
7stp.lingfil.uu.se/?nivre/research/Penn2Malt.
The English head-finding rules of Yamada and Mat-
sumoto (2003), supplied on the website, are employed.
8We used Dan Bikel?s compare.pl script which uses
stratified shuffling to compute significance. We consider a p
value < 0.05 to indicate a statistically significant difference.
1166
Setting F UAS Fun Funtag
Func Right 91.73 93.9 91.02 91.88
No Func Right 91.76 93.8 ? 91.80
Func Left 91.45 93.7 90.41 91.80
No Func Left 91.57 93.7 ? 91.74
DD Right Bin 92.16 94.1 90.85 91.86
DD Left Bin 91.89 93.9 90.10 91.85
DD Func 92.23 94.1 91.02 91.91
DD No Func 92.09 94.0 ? 91.86
DD3 92.45 94.3 90.86 91.98
DD4 92.44 94.3 90.97 92.04
(Shindo et al, 2012) 92.4
(Zhang et al, 2009) 92.3
(Petrov, 2010) 91.8
(Huang, 2008) 91.7
(Bohnet and Nivre, 2012) 93.7
Table 5: Test Set Results: Parseval F-score, penn2malt
UAS, Function Label Accuracy and Funtag Function La-
bel Accuracy
are shown in Table 6.
Comparison Dev Test
Func Right vs. No Func Right 7 7
Func Left vs. No Func Left 7 7
Func Right vs. Func Left X 7
No Func Right vs. No Func Left 7 7
DD Right Bin vs. Func Right X X
DD Right Bin vs. No Func Right X X
DD Left Bin vs. Func Left X X
DD Left Bin vs. No Func Left X X
DD Right Bin vs DD Left Bin X X
DD Func vs. Func Right 7 X
DD Func vs. Func Left X X
DD No Func vs. No Func Right X X
DD No Func vs. No Func Left X X
DD Func vs. DD No Func 7 7
DD3 vs. DD Right Bin 7 X
DD3 vs. No Func Left X X
DD3 vs. DD Func X X
DD4 vs. DD. Right Bin 7 X
DD4 vs. DD. Left Bin X X
DD4 vs. DD Func X X
DD4 vs. DD3 7 7
Table 6: Statistical Significance Testing
We measured the performance of DD4 on the test
set. It is approximately 3 times slower than the
slowest product model (left binarization with func-
tion labels) and 7 slower than the fastest one (right
binarization without function labels). This system
performs on average 85.5 iterations of the DD al-
gorithm. If we exclude the non-converging cases
(5.1% of the cases), this drops to 39.4.
Finally we compare our results with systems
trained and evaluated on the PTB, see the lower half
of Table 5. Our product models are not different
from those presented in (Petrov, 2010) and it is not
surprising to see that the F-scores are similar. More
interestingly our DD4 setting improves on these re-
sults and compares favorably with systems relying
on richer syntactic information, such as the discrim-
inative parser of (Huang, 2008) that makes use of
non-local features to score trees and the TSG parser
of (Shindo et al, 2012) that can take into account
larger tree fragments: this would indicate that by
combining our parsers we extend the domain of lo-
cality, horizontally with binarization schemes and
vertically with function labels. Our system also per-
forms better than the combination system presented
in (Zhang et al, 2009) that only relies on material
from the PTB9 but a more detailed comparison is
difficult: this system does not use products of la-
tent models and more generally their method is or-
thogonal to ours. We also include for comparison
state-of-the-art dependency parsing results (Bohnet
and Nivre, 2012).
6 Conclusion
We presented an algorithm and a set of experiments
showing that grammar extraction strategies can be
combined in an elegant way and give state-of-the-art
results when applied to high-quality phrase-based
parsers. As well as repeating these experiments for
languages which rely more on function annotation,
we also plan to apply our method to other types of
annotations, e.g. more linguistically motivated bina-
rization strategies or ? of particular interest to us ?
annotation of empty elements.
Acknowledgments
We are grateful to the reviewers for their helpful
comments. We also thank Joachim Wagner for pro-
viding feedback on an early version of the paper.
This work has been partially funded by the Labex
EFL (ANR/CGI).
9Their other system relying on the self-trained version of the
BLLIP parser achieves 92.6 F1.
1167
References
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the First Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010).
Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In Proceedings of the 1st
Annual Meeting of the North American chapter of the
ACL.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 1455?1465.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing to
Stanford Dependencies: Trade-offs between speed and
accuracy. In Proceedings of LREC.
Grzegorz Chrupala, Nicolas Stroppa, Josef van Genabith,
and Georgiana Dinu. 2007. Better training for func-
tion labeling. In Proceedings of the 2007 Conference
on Recent Advances in Natural Language Processing
(RANLP).
Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2012. Spectral learning of
latent-variable PCFGs. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics (ACL?12).
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In Proceedings of the COLING Workshop
on Cross-Framework and Cross-Domain Parser Eval-
uation.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Joakim Nivre, Deirdre Hogan, and
Josef van Genabith. 2011. From news to comment:
Resources and benchmarks for parsing the language
of web 2.0. In Proceedings of IJCNLP.
Ryan Gabbard, Mitchell Marcus, and Seth Kulick. 2006.
Fully parsing the penn treebank. In Proceedings of the
Human Language Technology Conference of the North
American Chapter of the ACL, pages 184?191.
David Hall and Dan Klein. 2012. Training factored
PCFGs with expectation propagation. In Proceedings
of the 2012 Conference on Empirical Methods in Nat-
ural Language Processing, pages 649?652.
John C. Henderson and Eric Brill. 1999. Exploiting
diversity in natural language processing: Combining
parsers. In Proceedings of the 1999 Conference on
Empirical Methods in Natural Language Processing,
pages 187?194.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek,
and Mare Koit, editors, Proceedings of NODALIDA
2007, pages 105?112.
Nikos Komodakis, Nikos Paragios, and Georgios Tziri-
tas. 2007. MRF optimization via dual decomposition:
Message-passing revisited. In Computer Vision, 2007.
ICCV 2007. IEEE 11th International Conference on,
pages 1?8. IEEE.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decompo-
sition for parsing with non-projective head automata.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The penn tree-
bank: Annotating predicate argument structure. In
Proceedings of the 1994 ARPA Speech and Natural
Language Workshop, pages 114?119.
Andre? FT Martins, Noah A Smith, Pedro MQ Aguiar,
and Ma?rio AT Figueiredo. 2011. Dual decomposition
with many overlapping components. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 238?249.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
75?82.
Paola Merlo and Gabriele Musillo. 2005. Accu-
rate function parsing. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 620?627.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950?958.
Joakim Nivre, Laura Rimell, Ryan Mc Donald, and Car-
los Go?mez-Rodr??guez. 2010. Evaluation of depen-
dency parsers on unbounded dependencies. In Pro-
ceedings of COLING.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
the conference on Human Language Technologies and
the conference of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL?07).
1168
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Working
Notes of the SANCL Workshop (NAACL-HLT).
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate deter-
ministic question parsing. In Proceedings of EMNLP.
Slav Petrov. 2010. Products of random latent variable
grammars. In Proceedings of the conference on Hu-
man Language Technologies and the conference of the
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL?10), pages 19?27.
Alexander Rush and Michael Collins. 2012. A tutorial
on dual decomposition and lagrangian relaxation for
inference in natural language processing. Journal of
Artificial Intelligence Research, 45:305?362.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 1?11.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL shared task
session of EMNLP-CoNLL, pages 1044?1050.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined tree
substitution grammars for syntactic parsing. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers-Volume
1, pages 440?448.
Noah A. Smith. 2011. Linguistic Structure Predic-
tion. Synthesis Lectures on Human Language Tech-
nologies. Morgan and Claypool, May.
Mark Steedman, Miles Osbourne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Boot-
strapping statistical parsers from small datasets. In
Proceedings of EACL, pages 759?763.
Mihai Surdeanu and Christopher D. Manning. 2010. En-
semble models for dependency parsing: Cheap and
good? In Proceedings of the conference on Hu-
man Language Technologies and the conference of the
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL?10), pages 649?
652.
David Vadas and James R. Curran. 2007. Adding noun
phrase structure to the penn treebank. In Proceedings
of ACL, pages 240?247.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT, pages 195?206.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1552?1560.
1169
XMG: eXtensible MetaGrammar
Beno??t Crabbe??
INRIA - Universite? Paris 7
Denys Duchier??
LIFO - Universite? d?Orle?ans
Claire Gardent?
CNRS - LORIA, Nancy
Joseph Le Roux?
LIPN - Universite? Paris Nord
Yannick Parmentier?
LIFO - Universite? d?Orle?ans
In this article, we introduce eXtensible MetaGrammar (XMG), a framework for specifying
tree-based grammars such as Feature-Based Lexicalized Tree-Adjoining Grammars (FB-LTAG)
and Interaction Grammars (IG). We argue that XMG displays three features that facilitate
both grammar writing and a fast prototyping of tree-based grammars. Firstly, XMG is fully
declarative. For instance, it permits a declarative treatment of diathesis that markedly departs
from the procedural lexical rules often used to specify tree-based grammars. Secondly, the XMG
language has a high notational expressivity in that it supports multiple linguistic dimensions,
inheritance, and a sophisticated treatment of identifiers. Thirdly, XMG is extensible in that its
computational architecture facilitates the extension to other linguistic formalisms. We explain
how this architecture naturally supports the design of three linguistic formalisms, namely,
FB-LTAG, IG, and Multi-Component Tree-Adjoining Grammar (MC-TAG). We further show
how it permits a straightforward integration of additional mechanisms such as linguistic and
formal principles. To further illustrate the declarativity, notational expressivity, and extensibility
of XMG, we describe the methodology used to specify an FB-LTAG for French augmented with a
? UFR de Linguistique, Universite? Paris Diderot-Paris 7, Case 7003, 2, F-75205 Paris Cedex 13, France.
E-mail: bcrabbe@linguist.jussieu.fr.
?? Laboratoire d?Informatique Fondamentale d?Orle?ans, Ba?timent IIIA, Rue Le?onard de Vinci, B.P. 6759,
F-45067 Orle?ans Cedex 2, France. E-mail: denys.duchier@univ-orleans.fr.
? Laboratoire LORIA - CNRS, Projet Synalp, Ba?timent B, BP 239, Campus Scientifique, F-54506
Vand?uvre-Le`s-Nancy Cedex, France. E-mail: gardent@loria.fr.
? Laboratoire d?Informatique de Paris Nord, UMR CNRS 7030, Institut Galile?e - Universite? Paris-Nord, 99,
avenue Jean-Baptiste Cle?ment, F-93430 Villetaneuse, E-mail: leroux@univ-paris13.fr.
? Laboratoire d?Informatique Fondamentale d?Orle?ans, Ba?timent IIIA, Rue Le?onard de Vinci, B.P. 6759,
F-45067 Orle?ans Cedex 2, France. E-mail: yannick.parmentier@univ-orleans.fr.
Submission received: 27 March 2009; revised version received: 2 July 2012; accepted for publication:
11 August 2012.
doi:10.1162/COLI a 00144
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
unification-based compositional semantics. This illustrates both how XMG facilitates the
modeling of the tree fragment hierarchies required to specify tree-based grammars and of a
syntax/semantics interface between semantic representations and syntactic trees. Finally, we
briefly report on several grammars for French, English, and German that were implemented
using XMG and compare XMG with other existing grammar specification frameworks for
tree-based grammars.
1. Introduction
In the late 1980s and early 1990s, many grammar engineering environments were
developed to support the specification of large computational grammars for natural
language. One may, for instance, cite XLE (Kaplan and Newman 1997) for specifying
Lexical-Functional Grammars (LFG), LKB (Copestake and Flickinger 2000) for speci-
fying Head-driven Phrase Structure Grammars (HPSG), and DOTCCG (Baldridge
et al 2007) for specifying Combinatory Categorial Grammars (CCG). Concretely, such
environments usually rely on (i) a formal language used to describe a target com-
putational grammar, and (ii) a processor for this language, which aims at generating
the actual described grammar (and potentially at checking it, e.g., by feeding it to
a parser).
Although these environments were tailored for specific grammar formalisms, they
share a number of features. Firstly, they are expressive enough to characterize subsets
of natural language. Following Shieber (1984), we call this feature weak completeness.
Secondly, they are notationally expressive enough to relatively easily formalize important
theoretical notions. Thirdly, they are rigorous, that is, the semantics of their underlying
language is well defined and understood. Additionally, for an environment to be useful
in practice, it should be simple to use (by a linguist), and make it possible to detect errors
in the described target grammar.
If we consider a particular type of computational grammar, namely, tree-based
grammars?that is, grammars where the basic units are trees (or tree descriptions) of
arbitrary depth, such as Tree-Adjoining Grammar (TAG; Joshi, Levy, and Takahashi
1975), D-Tree Grammar (DTG; Rambow, Vijay-Shanker, and Weir 1995), Tree Description
Grammars (TDG; Kallmeyer 1999) or Interaction Grammars (IG; Perrier 2000)?
environments sharing all of the listed features are lacking. As we shall see in Section 7
of this article, there have been some proposals for grammar engineering environments
for tree-based grammar (e.g., Candito 1996; Xia, Palmer, and Vijay-Shanker 1999,
but these lack notational expressivity. This is partly due to the fact that tree-based
formalisms offer an extended domain of locality where one can encode constraints
between remote syntactic constituents. If one wants to define such constraints while
giving a modular and incremental specification of the grammar, one needs a high level
of notational expressivity, as we shall see throughout the article (and especially in
Section 4).
In this article, we present XMG (eXtensible MetaGrammar), a framework for
specifying tree-based grammars. Focusing mostly on Feature-Based Lexicalized Tree-
Adjoining Grammars (FB-LTAG) (but using Interaction Grammars [IG] and Multi-
Component Tree-Adjoining Grammars [MC-TAG] to illustrate flexibility), we argue that
XMG departs from other existing computational frameworks for designing tree-based
grammars in three main ways:
 First, XMG is a declarative language. In other words, grammaticality is
defined in an order-independent fashion by a set of well-formedness
592
Crabbe? et al XMG: eXtensible MetaGrammar
constraints rather than by procedures. In particular, XMG permits a
fully declarative treatment of diathesis that markedly departs from the
procedural rules (called meta-rules or lexical rules) previously used to
specify tree-based grammars.
 Second, XMG is notationally expressive. The XMG language supports full
disjunction and conjunction of grammatical units, a modular treatment
of multiple linguistic dimensions, multiple inheritance of units, and a
sophisticated treatment of identifiers. We illustrate XMG?s notational
expressivity by showing (i) how it facilitates the modeling of the tree
fragment hierarchies required to specify tree-based grammars and (ii) how
it permits a natural modeling of the syntax/semantics interface between
semantic representations and syntactic trees as can be used in FB-LTAG.
 Third, XMG is extensible in that its computational architecture facilitates
(i) the integration of an arbitrary number of linguistic dimensions (syntax,
semantics, etc.), (ii) the modeling of different grammar formalisms
(FB-LTAG, MC-TAG, IG), and (iii) the specification of general linguistic
principles (e.g., clitic ordering in French).
The article is structured as follows. Section 2 starts by giving a brief introduction
to FB-LTAG, the grammar formalism we used to illustrate most of XMG?s features. The
next three sections then go on to discuss and illustrate XMG?s three main features?
namely, declarativity, notational expressivity, and flexibility. In Section 3, we focus
on declarativity and show how XMG?s generalized disjunction permits a declarative
encoding of diathesis. We then contrast the XMG approach with the procedural methods
previously resorted to for specifying FB-LTAG. Section 4 addresses notational expressiv-
ity. We present the syntax of XMG and show how the sophisticated identifier handling
it supports or permits a natural treatment (i) of identifiers in tree based hierarchies
and (ii) of the unification-based syntax/semantics interface often used in FB-LTAG. In
Section 5, we concentrate on extensibility. We first describe the operational semantics
of XMG and the architecture of the XMG compiler. We then show how these facilitate
the adaptation of the basic XMG language to (i) different grammar formalisms (IG,
MC-TAG, FB-LTAG), (ii) the integration of specific linguistic principles such as clitic
ordering constraints, and (iii) the specification of an arbitrary number of linguistic
dimensions. In Section 6, we illustrate the usage of XMG by presenting an XMG
specification for the verbal fragment of a large scale FB-LTAG for French augmented
with a unification-based semantics. We also briefly describe the various other tree-
based grammars implemented using XMG. Section 7 discusses the limitations of other
approaches to the formal specification of tree-based grammars, and Section 8 concludes
with pointers for further research.
2. Tree-Adjoining Grammar
A Tree-Adjoining Grammar (TAG) consists of a set of auxiliary or initial elementary
trees and of two tree composition operations, namely, substitution and adjunction.
Initial trees are trees whose leaves are either substitution nodes (marked with ?) or
terminal symbols (words). Auxiliary trees are distinguished by a foot node (marked
with ) whose category must be the same as that of the root node. Substitution inserts a
tree onto a substitution node of some other tree and adjunction inserts an auxiliary tree
593
Computational Linguistics Volume 39, Number 3
N
Marie
Mary
V
V
a
has
V
S
N? V
vu
seen
N?
N
Jean
John
??
S
N
Marie
Mary
V
V
a
has
V
vu
seen
N
Jean
John
Figure 1
Sample derivation of Marie a vu Jean ?Mary has seen John? in a TAG.
into a tree. Figure 1 shows a toy TAG generating the sentence Marie a vu Jean ?Mary has
seen John? and sketches its derivation.1
Among existing variants of TAG, one commonly used in practice is Lexical-
ized FB-LTAG (Vijay-Shanker and Joshi 1988). A lexicalized TAG is such that each
elementary tree has at least one leaf labeled with a lexical item (word), whereas in
an FB-LTAG, tree nodes are additionally decorated with two feature structures (called
top and bottom). These feature structures are unified during derivation as follows. On
substitution, the top features of the substitution node are unified with the top features of
the root node of the tree being substituted in. On adjunction, the top features of the root
of the auxiliary tree are unified with the top features of the node where adjunction takes
place; and the bottom features of the foot node of the auxiliary tree are unified with the
bottom features of the node where adjunction takes place. At the end of a derivation,
the top and bottom feature structures of all nodes in the derived tree are unified.
Implementation of Tree-Adjoining Grammars. Most existing implementations of TAGs fol-
low the three-layer architecture adopted for the XTAG grammar (XTAG Research Group
2001), a feature-based lexicalized TAG for English. Thus the grammar consists of (i) a
set of so-called tree schemas (i.e., elementary trees having a leaf node labeled with a
 referring to where to anchor lexical items2), (ii) a morphological lexicon associating
words with lemmas, and (iii) a syntactic lexicon associating lemmas with tree schemas
(these are gathered into families according to syntactic properties, such as the sub-
categorization frame for verbs). Figure 2 shows some of the tree schemas associated
with transitive verbs in the XTAG grammar. The tree corresponds (a) to a declarative
sentence, (b) to a WH-question on the subject, (c) to a passive clause with a BY-agent,
and (d) to a passive clause with a WH-object. As can be seen, each tree schema contains
an anchor node (marked with ). During parsing this anchor node can be replaced by
any word morphologically related to a lemma listed in the syntactic lexicon as anchor-
ing the transitive tree family.
This concept of tree family allows us to share structural information (tree schemas)
between words having common syntactic properties (e.g., sub-categorization frames).
There still remains a large redundancy within the grammar because many elementary
tree schemas share common subtrees (large coverage TAGs usually consist of hun-
dreds, sometimes thousands, of tree schemas). An important issue when specifying
1 The elementary trees displayed in this article conform to Abeille? (2002), that is, we reject the use of a VP
constituent in French.
2 As mentioned earlier, we describe lexicalized TAG, thus every tree schema has to contain at least one
anchor (node labeled ).
594
Crabbe? et al XMG: eXtensible MetaGrammar
(a) (b)
Sr
NP0 ? VP
V NP1 ?
Sq
NP0 ?
[
wh +
]
[]
Sr
NPNA

VP
V NP1 ?
(c) (d)
Sr [][
mode 3
]
NP1 ? VP
[
mode 3
]
?
?
passive 1
mode 2
?
?
V
?
?
passive 1 +
mode 2 ppart
?
?
[]
PP
P
by
NP0 ?
Sq
NP1 ?
[
wh +
]
[]
Sr [][
mode 3
]
NPNA

VP
[
mode 3
]
?
?
passive 1
mode 2
?
?
V
?
?
passive 1 +
mode 2 ppart
?
?
[]
PP
P
by
NP0 ?
Figure 2
Some tree schemas for English transitive verbs.
such grammars is thus structure sharing. Being able to share structural information is
necessary not only for a faster grammar development, but also for an easier grammar
maintenance (modifications to be applied to the tree schemas would be restricted to
shared structures). In the next section, we will see how XMG declarativity can be
efficiently used to factorize TAGs. In addition, Section 4 will show how XMG notational
expressivity facilitates the specification of another commonly used tree sharing device,
namely, inheritance hierarchies of tree fragments.
Extending TAG with a Unification-Based Semantics. To extend FB-LTAG with a compo-
sitional semantics, Gardent and Kallmeyer (2003) propose to associate each elementary
tree with a flat semantic representation. For instance, in Figure 3, the trees3 for John, runs,
and often are associated with the semantics l0:name(j,john), l1:run(e,s), and l2:often(x),
respectively. Importantly, the arguments of semantic functors are represented by uni-
fication variables which occur both in the semantic representation of this functor and
on some nodes of the associated syntactic tree. Thus in Figure 3, the semantic index s
occurring in the semantic representation of runs also occurs on the subject substitution
node of the associated elementary tree. The value of semantic arguments is then deter-
mined by the unifications resulting from adjunction and substitution. For instance, the
semantic index s in the tree for runs is unified during substitution with the semantic
index j labeling the root node of the tree for John. As a result, the semantics of John often
runs is {l0:name(j,john), l1:run(e,j), l2:often(e)}.
Gardent and Kallmeyer?s (2003) proposal was applied to various semantic phe-
nomena (Kallmeyer and Romero 2004a, 2004b, 2008). Its implementation, however,
3 Cx/Cx abbreviate a node with category C and a top/bottom feature structure including the feature-value
pair { index : x}.
595
Computational Linguistics Volume 39, Number 3
NPj
John
l0:name(j,john)
Sg
NP?s VPgf
Vfe
runs
l1:run(e,s)
VPx
often VP*x
l2:often(x)
? l0:name(j,john), l1:run(e,j), l2:often(e)
Figure 3
A toy lexicalized FTAG with unification-based semantics (l0, l1, l2, e, and j are constants and
s, f, g, x are unification variables).
relies on having a computational framework that associates syntactic trees with flat
semantic formulae while allowing for shared variables between trees and formulae. In
the following sections, we will show how XMG notational expressivity makes it pos-
sible to specify an FB-LTAG equipped with a unification-based semantics.
3. Declarativity
In this section, we show how a phenomenon which is often handled in a procedural
way by existing approaches can be provided with a declarative specification in XMG.
Concretely, we show how XMG supports a declarative account of diathesis that avoids
the drawbacks of lexical rules (e.g., information erasing). We start by presenting the
lexical rule approach. We then contrast it with the XMG account.
3.1 Capturing Diathesis Using Lexical Rules
Following Flickinger (1987), redundancy among grammatical descriptions is often han-
dled using two devices: an inheritance hierarchy and a set of lexical rules. Whereas
the inheritance hierarchy permits us to encode the sharing of common substructures,
lexical rules (sometimes called meta-rules) permit us to capture relationships between
trees by deriving new trees from already specified ones. For instance, passive trees will
be derived from active ones.
Although Flickinger?s (1987) approach was developed for HPSGs, several similar
approaches have been put forward for FB-LTAG (Vijay-Shanker and Schabes 1992;
Becker 1993; Evans, Gazdar, and Weir 1995; XTAG Research Group 2001). One important
drawback of these approaches, however, is that they are procedural in that the order in
which lexical rules apply matters. For instance, consider again the set of trees given
in Figure 2. In the meta-rule representation scheme adopted by Becker (1993), the base
tree (a) would be specified in the inheritance hierarchy grouping all base trees, and
the derived trees (b, c, d) would be generated by applying one or more meta-rules on
this base tree. Figure 4 sketches these meta-rules. The left-hand side of the meta-rule
is a matching pattern replaced with the right-hand side of the meta-rule in the newly
generated tree. Symbol ??? denotes a meta-variable whose matching subtree in the input
is substituted in place of the variable in the output tree. Given these, the tree family in
Figure 2 is generated as follows: (b) and (c) are generated by application to the base
tree (a) of the Wh-Subject and Passive meta-rules, respectively. Further, (d) is generated
by applying first, the Wh-Subject meta-rule and second, the Passive meta-rule to the
base tree.
596
Crabbe? et al XMG: eXtensible MetaGrammar
Passive meta-rule Wh-Subject meta-rule
Sr
?1 NP? VP
V ?2 NP?
? Sr [][
mode 3
]
?2 NP? VP
[
mode 3
]
?
?
mode 2
passive 1
?
?
V
?
?
passive 1 +
mode 2 ppart
?
?
[]
PP
P
by
?1 NP?
Sr
?2NP? ? ?1
? Sq
?2NP? ?
[
wh +
]
[]
Sr
NP?NA

?1
Figure 4
Simplified meta-rules for passive and wh-subject extraction.
More generally a meta-rule is a procedural device that, given a tree instance,
generates a new tree instance by adding, suppressing (hence possibly substituting)
information in grammatical units. Prolo (2002) defines a set of meta-rules that can
be used to specify a large FB-LTAG for English. Given an ordered set of meta-rules,
however, there is no guarantee that the trees they derive are linguistically appropriate
and that the derivation process terminates. Thus, to ensure termination and consistency,
Prolo needs to additionally provide rule ordering schemes (expressed as automata).
3.2 XMG: Capturing Diathesis Using Disjunction
XMG provides an alternative account for describing tree sets such as that of Figure 2
without lexical rules and without the related ordering constraints. In essence, the
approach consists of enumerating trees by combining tree fragments using conjunction
and disjunction.
More specifically, the tree set given in Figure 2 can be generated by combining
some of the tree fragments sketched in Figure 5 using the following conjunctions and
disjunctions:4
Subject ? CanonicalSubject ? Wh-NP-Subject (1)
ActiveTransitiveVerb ? Subject ? ActiveVerb ? CanonicalObject (2)
PassiveTransitiveVerb ? Subject ? PassiveVerb ? CanonicalByObject (3)
TransitiveVerb ? ActiveTransitiveVerb ? PassiveTransitiveVerb (4)
The first clause (Subject) groups together two subtrees representing the possi-
ble realizations of a subject (canonical and wh). The next two clauses define a tree
set for active and passive transitive verbs, respectively. The last clause defines the
TransitiveVerb family as a disjunction of the two verb forms (passive or active). In sum,
the TransitiveVerb clause defines the tree set sketched in Figure 2 as a disjunction of
conjunctions of tree fragments.
One of the issues of meta-rules reported by Prolo (2002) is the handling of feature
equations. For a number of cases (including subject relativization in passive trees),
4 For now, let us consider that the tree fragments are combined in order to produce minimal trees by
merging nodes whose categories (and features) unify. In the next section, we will see how to precisely
control node identification using either node variables or node constraints.
597
Computational Linguistics Volume 39, Number 3
Canonical Subject ? Wh-NP-Subject ? Canonical Object ? Wh-NP-Object ?
Sr
NP? VP
Sq
NP?
[
wh +
]
[]
Sr
NPNA

VP
VP
V NP?
Sq
NP?
[
wh +
]
[]
Sr
VP NPNA

Canonical By Object ? Wh By Object ? Active Verb ? Passive Verb ?
VP
V PP
P
by
NP?
VP
PP
P
by
NP?
V
Sr
VP
V
Sr[]
[
mode 3
]
VP
[
mode 3
]
?
?
passive 1
mode 2
?
?
V
?
?
passive 1 +
mode 2 ppart
?
?
[]
Figure 5
Tree fragments.
ad hoc meta-rules are needed, for a unified tree transformation cannot be defined. In
a declarative approach such as the one here, dealing with feature equations can be
done relatively easily. Let us imagine that we now want to extend the trees of Figure 2
with feature equations for subject?number agreement. We can for instance do so by
defining the following tree fragment (the dashed line indicates that the VP node can be
a descendant, not only a daughter, of the S node):5
SubjAgreement ? S
NP?
[
num 1
]
[
num 1
]
VP
[
num 1
]
[
num 1
]
Then we extend the definition of Subject as follows:
Subject ? SubjAgreement ? ( CanonicalSubject ? Wh-NP-Subject ) (5)
If we want to get further with the description of transitive verbs, for instance by
taking into account wh-objects and by-objects, this can be done as follows. We first
define the elementary fragments Wh-NP-Object and Wh-By-Object (see Figure 5), and
then define the following additional combinations:6
ActiveTransitiveVerb ? CanonicalSubject ? ActiveVerb ? Wh-Np-Object (6)
PassiveTransitiveVerb ? CanonicalSubject ? PassiveVerb ? Wh-By-Object (7)
5 Note that in XMG, it is not mandatory to define any tree structure inside SubjAgreement. We could define
independent NP and VP nodes, and associate them with variables, say n1 and n2. n1 and n2 would then
be exported and reused directly in the classes CanonicalSubject and Wh-NP-Subject, respectively.
6 Note that these clauses only consider canonical subjects to avoid having both a Wh-subject and a
Wh-object. This is not entirely satisfactory, as we would prefer to define a single abstraction over objects
(as was done for subjects) and use it wherever possible. There would then be another mechanism to
capture this exception and cause the invalid combination to fail (that is, the resulting tree description not
to have any model). Such a mechanism exists in XMG, and is called linguistic principle (see Section 5).
598
Crabbe? et al XMG: eXtensible MetaGrammar
Evans, Gazdar, and Weir (1995) argue for the necessity of using lexical rules for
grammatical description based on two arguments: (i) morphology is irregular and has
to be handled by a non-monotonic device and (ii) erasing rules such as the agentless
passive (John eats an apple / An apple is eaten ) are needed to erase an argument from
the canonical base tree. Neither of these arguments holds here, however: The first
argument because we describe tree schema hence lexical and morphological issues are
ruled out; the second because agentless passive and, more generally, argument erasing
constructions can simply be defined by an additional clause such as:
AgentlessPassiveTransitiveVerb ? Subject ? PassiveVerb (8)
To summarize, using a declarative language to specify a tree-based grammar offers
an adequate level of control on the structures being described while avoiding having
to deal with ordering and termination issues. It facilitates grammar design and mainte-
nance, by providing an abstract view on grammar trees, uniquely made of monotonic
(no information removal) combinations of tree fragments.
4. Notational Expressivity
We now focus on notational expressivity and show how XMG supports a direct
encoding of (i) distinct linguistic dimensions (here syntax, semantics and the syntax/
semantics interface) and (ii) the various types of coreferences7 that arise in the devel-
opment of tree-based grammars.
The syntax of the XMG language can be formally defined as follows.
Class ::= NameC1,...,Ckx1,...,xn ? Content (9)
Content ::= ?SYN, SEM, DYN? | Name | Content ? Content | Content ? Content
(10)
SYN ::=
n1 ? n2 | n1 ?+ n2 | n1 ?? n2 | n1 ? n2 | n1 ?+ n2 | n1 ?? n2 |
n1[f1 : v1,..., fk : vk] | n1(c1 : cv1,..., cl : cvl) | n1 = n2 | x = Ci.y |
n1 (c1 : cv1,..., cl : cvl) [f1 : v1,..., fk : vk] | SYN ? SYN
(11)
SEM ::= li : p(E1,...,En) | li ? hj | SEM ? SEM (12)
DYN ::= ? f1 : v1,...,fn : vn ? (13)
Here and in what follows, we use the following notational conventions. Ci denote
variables over class names; xi, x, and y are variables ranging over tree nodes or feature
values; ni refer to node variables; f, fi are features and v, vi and feature values (constants
or variables); li, hj, p, and Ei are variables over semantic labels, semantic holes, predi-
cates, and predicate arguments in flat semantic formulae, respectively.8 [ ] are used to
associate a node variable with some feature constraint. ( ) are used to associate a node
variable with some property constraint (e.g., node colors, see Section 5). ci and cvi denote
7 By coreference, we mean the sharing of information between distinct elementary fragments of the
grammar specification.
8 See Gardent and Kallmeyer (2003) for a detailed introduction to flat semantics.
599
Computational Linguistics Volume 39, Number 3
a property constraint and a property constraint value, respectively. Ci.y denotes the y
variable declared in class Ci and = is unification; ? and ? denote linear precedence and
immediate dominance relations between nodes. Finally, +, ? represent the transitive and
transitive-reflexive closure of a relation, respectively.
The first two clauses of the formal definition here specify XMG classes and how they
combine. The next three clauses define the languages supported for describing three lin-
guistic dimensions, namely, syntax (SYN), semantics (SEM), and the syntax/semantics
interface (called DYN for dynamic interface). We now discuss each of these in more
detail starting bottom?up with the three linguistic dimensions and ending with the
control language that permits us to combine basic linguistic units into bigger ones.
SYN. The XMG formalism for syntax (copied here for convenience) is a tree description
logic similar to that proposed by Vijay-Shanker and Schabes (1992) and Rogers and
Vijay-Shanker (1994) to describe tree-based grammars.
SYN ::= n1 ? n2 | n1 ?+ n2 | n1 ?? n2 | n1 ? n2 | n1 ?+ n2 | n1 ?? n2 |
n1[f1 : v1,..., fk : vk] | n1(c1 : cv1,..., cl : cvl) | n1 = n2 | x = Ci.y |
n1 (c1 : cv1,..., cl : cvl) [f1 : v1,..., fk : vk] | SYN ? SYN
It includes tree node variables, feature names, feature values, and feature variables.
Tree node variables can be related by equality (node identification), precedence (imme-
diate or non-immediate), and dominance (immediate or non-immediate). Tree nodes
can also be labeled with feature structures of depth 2, that is, sets of feature/value
pairs where feature values are either variables, constants (e.g., syntactic category), or
non-recursive feature structure (e.g., top and bottom feature structures).
Here is a graphical illustration of how tree logic formulae can be used to describe
tree fragments: The depicted tree fragment is a model satisfying the given formula.
n1 ? n2 ? n1 ? n3 ? n2 ? n3
? n1[cat : S] ? n2(mark : subst) [cat : NP] ? n3[cat : VP]
S
NP? VP
One distinguishing feature of the XMG tree language is the introduction of node
constraints (n1(c : cv)) that generalize Muskens and Krahmer?s (1998) use of positive
and negative node markings. Concretely, node constraints are attribute-value matri-
ces, which contain information to be used when solving tree descriptions to produce
grammar trees. In other words, node constraints are used to further restrict the set
of models satisfying a tree description. As an example of node constraint, consider
node annotations in FB-LTAG (foot node, substitution node, null-adjunction, etc.). Such
annotations can be used as node constraints to allow the description solver to apply
well-formedness constraints (e.g., there is at most one foot node).
Another interesting feature of XMG concerns the inclusion of the dot operator,
which permits us to identify variables across classes in cases where name sharing cannot
be resorted to. When a variable y is declared in a class C, the latter being instantiated
within a class D, y can be accessed from D by C.y (the identifier y still being available
in D?s namespace).
600
Crabbe? et al XMG: eXtensible MetaGrammar
SEM. The semantic dimension supports a direct encoding of the flat semantic formulae
used by Gardent and Kallmeyer (2003):
SEM ::= li : p(E1,...,En) | li ? hj | SEM ? SEM
where li : p(E1,..., En) represents a predicate p with label li and arguments E1,..., En and
li ? hj is a scope constraint between label li and scope hj. Expressions (predicate argu-
ments Ei) can refer to semantic holes, constants (atomic values), or unification variables
(written x, y hereafter).
For instance, the following flat semantic formula can be used to underspecify the
meaning of the sentence ?Every dog chases a cat?:
l0 : ?(x, h1, h2) ? l1 ? h1 ? l1 : Dog(x) ? l2 ? h2 ? l2 : Chase(x, y)
? l3 : ?(y, h3, h4) ? l4 ? h3 ? l4 : Cat(y) ? l2 ? h4
(14)
This formula denotes the following two first-order logic formulae, thereby describing
the two possibles readings of this sentence.9
l0 : ?(x, l1, l3) ? l1 : Dog(x) ? l2 : Chase(x, y) ? l3 : ?(y, l4, l2) ? l4 : Cat(y) (15)
l0 : ?(x, l1, l2) ? l1 : Dog(x) ? l2 : Chase(x, y) ? l3 : ?(y, l4, l0) ? l4 : Cat(y) (16)
DYN. The DYN dimension generalizes Kinyon?s hypertag (Kinyon 2000) which is
unified whenever two tree fragments are combined. Similarly, in XMG the DYN
dimension is a feature structure that is unified whenever two XMG classes are com-
bined through inheritance or through conjunction (see the discussion on XMG control
language, subsequently).
For instance, the following constraints ensure a coreference between the index I
occurring in the syntactic dimension and the argument X occurring in the semantic
dimension (indexsubject and arg1 are feature names, and E, I, X, and V local unification
variables).
C1 ? Node [idx : I] ? ?indexsubject : I? (17)
C2 ? L : P(E) ? L : Theta1(E, X) ? ?arg1 : X? (18)
SubjectArg1 ? C1 ? C2 ? ?indexsubject : V, arg1 : V? (19)
More generally, the DYN dimension permits us to unify nodes and feature values
that belong to distinct classes and dimensions, and are thus often not related within
the inheritance hierarchy. As we shall see in Section 6, the DYN dimension permits
a modular account of the syntax/semantics interface in which linking constraints can
be stipulated separately and reused to specify the various diatheses.
In other words, the DYN feature structure allows us to extend the scope of some
specific variables so that they can be unified with variables (or values) introduced
in some other classes of the metagrammar. This concept of scope extension can be
compared with that of hook in Copestake, Lascarides, and Flickinger (2001).
9 For more details on the interpretation of flat semantics and on its association with a grammar of natural
language, see Gardent (2008).
601
Computational Linguistics Volume 39, Number 3
Control language. The linguistic units (named Content here) defined by the linguist can
be abstracted and combined as follows:
Class ::= NameC1,...,Ckx1,...,xn ? Content
Content ::= ?SYN, SEM, DYN? | Name | Content ? Content | Content ? Content
The first clause states that the linguistic information encoded in Content is abstracted in
a class named Name and that this class inherits classes C1,..., Ck and exports variables
x1,..., xn. That is, XMG allows for abstraction, inheritance, and variable exports. By
default, variables (referring to nodes and feature values) are local to a class. Export
statements extend the scope of a variable to all sub-classes, however. An exported
variable can also be accessed from outside its class in case of class instantiation (using
the dot operator introduced earlier in this section). The second clause states that an
XMG class consists of a syntactic, a semantic, and a dynamic description (each of them
possibly empty), and that XMG classes can be combined by conjunction and disjunc-
tion and reused through class instantiation. The notation ?SYN, SEM, DYN? represents
simultaneous contributions (possibly empty) to all three dimensions.10
The XMG control language differs from other frameworks used to specify tree-
based grammars (Vijay-Shanker and Schabes 1992; Xia et al 1998; Candito 1999)
in two main ways. First, it supports generalized conjunctions and disjunctions of
classes. As shown in Section 3, this permits us, inter alia, a declarative treatment of
diathesis.
Second, it allows for both local and exported variables. As mentioned in Section 3, a
common way to share structure within a tree-based grammar is to define an inheritance
hierarchy of either tree fragments (Evans, Gazdar, and Weir 1995) or tree descriptions
(Vijay-Shanker and Schabes 1992; Candito 1996; Xia 2001). When considering an FB-
LTAG augmented with unification semantics, the hierarchy will additionally contain
semantic representations and/or tuples made of tree fragments and semantic represen-
tations. In all cases, the question arises of how to handle identifiers across classes and,
more specifically, how to share them.
In Candito?s (1996) approach, tree nodes are referred to using constants so that
multiple occurrences of the same node constant refer to the same node. As pointed out
in Gardent and Parmentier (2006), global names have several non-trivial shortcomings.
First, they complicate grammar writing in that the grammar writer must remember the
names used and their intended interpretation. Second, they fail to support multiple uses
of the same class within one class. For instance, in French, some verbs sub-categorize
for two prepositional phrases (PP). A natural way of deriving the tree for such verbs
would be to combine a verbal tree fragment with two instances of a PP fragment. If,
however, the nodes in the PP fragment are labeled with global names, then the two
occurrences of these nodes will be identified thereby blocking the production of the
appropriate tree.11
A less restrictive treatment of identifiers is proposed by Vijay-Shanker and Schabes
(1992), where each tree description can be associated with a set of declared node
variables and subsets of these node variables can be referred to by descriptions in the
10 Although formally precise, this notation can be cumbersome. In the interest of legibility we adopt
throughout the convention that SYN stands for ?SYN, , ?, SEM for ? , SEM, ?, and DYN for ? , , DYN?.
11 An analogous situation may arise in English with ditransitive verbs requiring two direct objects.
602
Crabbe? et al XMG: eXtensible MetaGrammar
hierarchy that inherit from the description in which these node variables were declared.
For instance, if entity A in the hierarchy declares such a special node variable X and B
inherits from A, then X can be referred to in B using the notation A.X.12
XMG generalizes Vijay-Shanker and Schabes?s (1992) approach by integrating an
export mechanism that can be used to extend the scope of a given identifier (node
or feature value variable) to classes that inherit from the exporting class. Thus if
class B inherits from class A and class A exports variable X, then X is visible in B
and its reuse forces identity. If B inherits from several classes and two (or more) of
these inherited classes export the same variable name X, then X is not directly visible
from B. It can be accessed though using the dot operator. First A is identified with a
local variable (e.g., T = A), then T.X can be used to refer to the variable X exported
by A.
To summarize, XMG allows for local variables to be exported to sub-classes as well
as for prefixed variables?that is, variables that are prefixed (using the dot operator)
with a reference to the class in which they are declared. In this way, the pitfalls in-
troduced by global names are avoided while providing enough expressivity to handle
variable coreference (via the definition of variable namespaces). Section 6 will further
illustrate the use of the various coreference devices made available by XMG showing
how they concretely facilitate grammar writing.
Let us finally illustrate variable handling with XMG in the example of Figure 2.
Recall that we define the trees of Figure 2 as the conjunctions and disjunctions of some
tree fragments of Figure 5, such as:
Subject ? SubjAgreement ? ( CanonicalSubject ? Wh-NP-Subject ) (20)
CanonicalSubject can be defined as a tree description formula as follows (only variables
n2 and n3 are exported):
CanonicalSubjectn2,n3 ?
n1 ? n2 ? n1[cat : S] ? n2(mark : subst) [cat : NP]?
n1 ? n3 ? n3[cat : VP] ? n2 ? n3
(21)
The class Wh-NP-Subject is defined accordingly (i.e., by means of a slightly more
complex tree description formula using the n2 and n3 variable identifiers to refer to
the nodes involved in subject agreement). The class SubjAgreement is defined slightly
differently (we do not impose any tree relation between the node concerned with
number agreement):
SubjAgreementn1,n2 ?
n1 [[top : [num : x]] [bot : [num : x]]]?
n2 [[top : [num : x]] [bot : [num : x]]]
(22)
12 In fact, the notation used by Vijay-Shanker and Schabes (1992) is attr:X with attr an attribute variable
ranging over a finite set of attributes, to indicate special node variables that scope outside their class; and
attr(A) to refer to such variables from outside the entity in which they were declared. We use a different
notation here to enforce consistency with the XMG notation.
603
Computational Linguistics Volume 39, Number 3
We can then explicitly control the way the fragments combine as follows:
Subject ?
C1 = SubjAgreementn1,n2 ?
C2 = ( CanonicalSubjectn2,n3 ? Wh-NP-Subjectn2,n3 ) ?
C1.n1 = C2.n2 ? C1.n2 = C2.n3
(23)
In this example, we see how to constrain, via variable export and unification, some
given syntactic nodes to be labeled with feature structures defined somewhere else in
the metagrammar. We use XMG?s flexible management of variable scope to deal with
node coreference. Compared with previous approaches on metagrammars such as those
of Candito (1996), Xia (2001), having the possibility of handling neither only global nor
only local variables, offers a high level of expressivity along with a precise control on
the structures being described.
5. Extensibility
A third distinguishing feature of XMG is extensibility. XMG is extensible in that
(i) dimensions can be added and (ii) each dimension can be associated with its own
interpreter. In order to support an arbitrary number of dimensions, XMG relies on a
device permitting the accumulation of an arbitrary number of types of literals, namely,
Extensible Definite Clause Grammar (EDCG) (Van Roy 1990). Once literals are accumu-
lated according to their type (i.e., each type of literals is accumulated separately), they
can be fed to dedicated interpreters. Because each of these sets of literals represents
formulas of a description language, these interpreters are solvers whose role is to
compute models satisfying the accumulated formulas.
Via this concept of separated dimensions, XMG allows us (i) to describe different
levels of language (not only syntax, but also semantics and potentially morphology,13
etc.), and (ii) to define linguistic principles (well-formedness constraints to be applied on
the structures being described). These principles depend either on the dimension (e.g.,
scope constraints in flat semantics), the target formalism (e.g. cooccurrence predicate-
arguments in FB-LTAG), or the natural language (e.g., clitic ordering in Romance lan-
guages) being described.
In what follows, we start by showing how XMG handles dimensions independently
from each other introducing EDCG (Section 5.1). We then summarize the architecture
of the XMG system (Section 5.2). We finally show how different solvers can be used
to implement various constraints on each of these dimensions (Section 5.3). In partic-
ular, we discuss three kinds of extensions implemented in XMG: extension to several
grammar formalisms, integration of explicit linguistic generalizations, and inclusion of
color-based node marking to facilitate grammar writing.
5.1 XMG: Accumulating and Interpreting an Arbitrary Number of Descriptions
Accumulating (tree) descriptions. First, let us notice that XMG is nothing other than a logic
language a` la Prolog (Duchier, Parmentier, and Petitjean 2012). More precisely, an XMG
13 Recently, XMG has been used to describe the morphology of verbs in Ikota, a Bantu language spoken in
Gabon (Duchier, Parmentier, and Petitjean 2012).
604
Crabbe? et al XMG: eXtensible MetaGrammar
specification is a collection of Horn clauses, which contribute a declarative description
of what a computational tree grammar is.
Logic Program XMG Metagrammar
Clause ::= Head ? Body
Body ::= Fact | Head |
Body ? Body |
Body ? Body
Query ::= Head
Class ::= Name ? Content
Content ::= Description | Name |
Content ? Content |
Content ? Content
Axiom ::= Name
Recall that the descriptions handled by XMG are in fact tuples of the form
?SYN, SEM, DYN?. An XMG class can thus describe, in a non-exclusive way, any of these
three levels of description. If one wants to add another level of description (i.e., another
dimension), one needs to extend the arity of this tuple. Before discussing this, let us first
see how such tuples are processed by XMG.
As mentioned earlier, XMG?s control language is comparable to Horn clauses.
A common way to represent Horn clauses is by using Definite Clause Grammar
(DCG) (Pereira and Warren 1980). Concretely, a DCG is a rewriting system (namely, a
context-free grammar), where the symbols of the rewriting rules are equipped with
pairs of unification variables (these are usually called difference list or accumulator)
(Blackburn, Bos, and Striegnitz 2006, page 100). As an illustration, consider the follow-
ing toy example.
s --> np,vp. np --> det,n.
vp --> v,np. vp --> v.
det --> [the]. det --> [a].
n --> [cat]. n --> [mouse].
v --> [eats].
The string language described by this DCG can be obtained by submitting the query
s(X,[]) where X is a unification variable to be bound with lists of facts (these being the
sentences belonging to the string language). As we can easily see, this language contains
the sentences ?a cat eats,? ?the cat eats,? ?a mouse eats,? ?the mouse eats,? ?a cat eats a
mouse,? ?a mouse eats a cat,? and so on.
Similarly, we can represent XMG classes as DCG clauses. For instance, the combina-
tions of syntactic fragments given in relations (1)?(4) can be rewritten as DCG clauses
as follows:
subject --> canonicalSubject.
subject --> whNpSubject.
activeTransitiveVerb --> subject, activeVerb, canonicalObject.
passiveTransitiveVerb --> subject, passiveVerb, canonicalByObject.
transitiveVerb --> activeTransitiveVerb.
transitiveVerb --> passiveTransitiveVerb.
Disjunctions (e.g., the subject specification) translate to multiple clauses with iden-
tical heads and conjunctions (e.g., activeTransitiveVerb) to a clause body.
In our case, the terminal symbols of the underlying DCG are not just facts, but
tuples of descriptions. In other words, the DCG clause whose head is canonicalSubject
is associated with a tuple of the following form (the dots have to be replaced with
605
Computational Linguistics Volume 39, Number 3
adequate descriptions, these can contain unification variables, whose scope is by default
local to the clause):
canonicalSubject --> [desc(syn(...),sem(...),dyn(...))].
In order to allow for an extension of XMG to an arbitrary number of dimensions,
instead of compiling XMG classes into a DCG whose accumulator stores tuples with
a fixed arity, these classes are compiled into an EDCG (Van Roy 1990). EDCG are DCG
with multiple accumulators. In XMG, each dimension is thus allocated a dedicated
accumulator in the underlying EDCG.
Note that although the content of the various dimensions is accumulated separately,
dimensions may nevertheless share information either via local unification variables
(if the XMG class defines several dimensions locally), via exported unification vari-
ables (in case of class instantiation or inheritance), or via the shared unification variables
supported by the DYN dimension.
At the end of the EDCG execution, we obtain, for each axiom of the metagrammar
(i.e., for each class name to be valuated), a list of description formulas per accumulator.
These lists are grouped together into a tuple of lists of the following form (N is the
number of dimensions, and consequently of accumulators):
desc(accu1(L1),accu2(L2), ... ,accuN(LN))
Each element (i.e., list Li) of such a tuple is a complete description of a given dimension,
where shared variables have been unified (via unification with backtracking).
Solving (tree) descriptions. As illustrated earlier, interpreting XMG?s control language in
terms of an EDCG yields tuples whose arity is the number of dimensions defined by
the linguist, that is, triples of the form ?SYN, SEM, DYN? if syntax, semantics, and the
dynamic interface are described.
For each dimension D, XMG includes a constraint solver SD that computes the set of
minimal models MD = SD(dD) satisfying the description (dD) of that dimension. In other
words, each dimension is interpreted separately by a specific solver. For instance, the
syntactic dimension is handled by a tree description solver that produces, for a given
tree description, the set of trees satisfying that description, whereas the solver for the
semantic dimension simply outputs the flat semantic representation (list of semantic
literals) built by the EDCG through accumulation.
Note that, although solvers are distinct, the models computed in each dimension
may nonetheless be coupled through shared variables. In that case, these variables can
constrain the models computed by the respective solvers. For instance, shared variables
can be used for the syntactic tree description solver to be parametrized by some value
coming from the semantic input description. Note that the output of the solving process
is a Cartesian product of the sets of minimal models of each solver. As a consequence,
the worst case complexity of metagrammar compilation is that of the various solvers
associated with relevant dimensions.
In addition to having separate solvers for each dimension, the constraint-solving
approach used in XMG permits us to modularize a given solver by combining different
principles. Each such principle enforces specific constraints on the models satisfying
the description of a given dimension. For instance, for the syntactic dimension of an
FB-LTAG, a set of principles is used to enforce that the structures produced by the
compiler are trees, and that these conform to the FB-LTAG formalism (e.g., there is no
tree having two foot nodes).
606
Crabbe? et al XMG: eXtensible MetaGrammar
5.2 Architecture
The XMG compiler14 consists of the following three modules:
 A compiler that parses XMG?s concrete syntax and compiles XMG classes
into clauses of an EDCG.
 A virtual machine (VM), which interprets EDCG. This VM performs
the accumulation of dimensions along with scope management and
identifiers resolution. This VM is basically a unification engine equipped
with backtracking, and which is extended to support EDCG. Although its
architecture is inspired by the Warren Abstract Machine (A??t-Kaci 1991),
it uses structure-sharing to represent and unify prolog terms, and, given
a query on a class, processes the conjunctions, disjunctions, inheritance,
and export statements related to that class to produce its full definition,
namely, a tree description for the SYN dimension, a flat semantic formula
for the SEM dimension, and a feature structure for the DYN dimension.
 A constraint-solving phase that produces for each dimension the minimal
models satisfying the input description as unfolded by the preceding
two steps.
As already mentioned, the first part is extensible in that new linguistic dimensions
can be added by specifying additional dedicated accumulators to the underlying EDCG.
The second part is a unification engine that interprets EDCG while performing both term
unification and polarized unification (i.e., unification of polarized feature structures, as
defined by Perrier [2000], and discussed in Section 5.3.1). This extended unification is
the reason why XMG does not merely recourse to an existing Prolog engine to process
EDCG, but relies on a specific VM instead.
The third part is completely modular in that various constraint solvers can be
plugged in depending on the requirements set by the dimensions used, and the chosen
grammatical framework. For instance, the SYN dimension is solved in terms of tree
models, and the SEM dimension is solved in terms of underspecified flat semantic
formulae (i.e., the input semantics remains untouched modulo the unification of its
shared variables).
Importantly, these additional solvers can be ?turned on/off? (via a primitive of the
XMG language) so that, for instance, the same processor can be used to compile an
XMG specification for an FB-LTAG using linguistic principles such as those defined in
the next section (i.e., clitic ordering principle) or not.
5.3 Three Extensions of XMG
We now show (i) how the modular architecture of the XMG compiler permits us
to specify grammars for several tree-based linguistic formalisms; (ii) how it can be
extended to enforce language specific constraints on the syntactic trees; and (iii) how
additional formal constraints (namely node marking) can be integrated to simplify node
identifications (and consequently grammar writing).
14 The XMG compiler is open source software released under the terms of the CeCILL GPL-compliant
licence. See http://sourcesup.renater.fr/xmg.
607
Computational Linguistics Volume 39, Number 3
Eq
Up
Down
Left
Right
Figure 6
Partition of the nodes of tree models.
5.3.1 TAG, MC-TAG, and IG: Producing Trees, Tree Sets, or Tree Descriptions. XMG in-
tegrates a generic tree solver that computes minimal tree models from tree descrip-
tion logic formulae built on the language SYN introduced in Section 4. This solver
integrates the dominance solving technique proposed by Duchier and Niehren (2000)
and can be summarized as follows. A minimal tree model is described in terms of
the relative positions of its nodes. For each node n in a minimal tree model T, the
set of all the nodes of T can be partitioned in five subsets, depending on their po-
sition relative to n. Hence, for each node variable n appearing in a tree description,
it is first associated with an integer (called node id). We then define the five sets
of node ids (i.e., sets of integers) Downn, Upn, Leftn, Rightn, and Eqn referring to the
ids of the nodes located below, above, on the left, on the right, or identified with n,
respectively (see Figure 6). Note that we require that these sets are a partition of all
node ids.
Using this set-based representation of a model, we translate each node relation
from the input formula (built on the tree description language introduced in Section 4)
into constraints on the sets of node ids that must hold in a valid model. For instance,
the sub-formula n1 ?+ n2, which states that node n1 strictly precedes node n2, is
translated into:
n1 ?+ n2 ? EqDownn1 ? Leftn2 ? EqDownn2 ? Rightn1?
Rightn2 ? Rightn1 ? Leftn1 ? Leftn2
(24)
where15 EqDownx = Eqx unionmulti Downx for x ? {n1, n2}. In other words, in a valid minimal
tree model, the set of nodes below or equal to n1 is included in the set of nodes (strictly)
on the left of n2, the set of nodes below or equal to n2 is included in the set of nodes
(strictly) on the right of n1, the set of nodes on the right of n2 is included in the set of
nodes on the right of n1, and finally the set of nodes on the left of n1 is included in the
set of nodes on the left of n2.
Once all input relations are translated into set constraints, the solver uses standard
Constraint Satisfaction techniques (e.g., a first-fail exploration of the search tree) to find a
set of consistent partitions. Finally, the nodes of the models are obtained by considering
nodes with distinct Eqn.
15 unionmulti represents disjoint union.
608
Crabbe? et al XMG: eXtensible MetaGrammar
FB-LTAG trees. To support the specification of FB-LTAG trees, the XMG compiler extends
the generic tree solver described here with a set of constraints ensuring that the trees are
well-formed TAG trees. In effect, these constraints require the trees to be linear ordered
trees with appropriate decorations. Each node must be labeled with a syntactic category.
Leaf nodes are either terminal, foot, or substitution nodes. There is at most one foot
node per tree and the category of the foot node must be identical to that of the root
node. Finally, each tree must have at least one leaf node that is an anchor.
MCTAG tree sets. Where FB-LTAG consists of trees, MC-TAG (Weir 1988) consists of sets
of trees. To support the specification of MC-TAG, the sole extension needed concerns
node variables that are not dominated by any other node variable in the tree description.
Whereas for FB-LTAG, these are taken to denote either the same root node or nodes that
are connected to some other node (i.e., uniqueness of the root), for MC-TAG they can
be treated as distinct nodes, thereby allowing for models that are sets of trees rather
than trees (Parmentier et al 2007). In other words, the only modification brought to the
tree description solver is that, in MC-TAG mode, it does not enforce the uniqueness of
a root node in a model.
IG polarized tree descriptions. IG (Perrier 2000) consist of tree descriptions whose node
variables are labeled with polarized feature structures. A polarized feature structure is
a set of polarized feature triples (f, p, v) where f and v are standard features and feature
values, respectively, and p is a polarity value in {?,?,=,?}. Polarities are used to
guide parsing in that a valid derivation structure must neutralize polarities.
To support an XMG encoding of IG, two extensions are introduced, namely, (i) the
ability to output tree descriptions rather than trees, and (ii) the ability to write polarized
feature structures. The first extension is trivially realized by specifying a description
solver that ensures that any output description has at least one tree model. For the
second point, the SYN language is extended to define polarized feature structures and
the unification engine to support unification of polarized features (for instance, a ?
feature will unify with a neutral (=) feature to yield a ? polarized feature value triple).
5.3.2 Adding Specific Linguistic Constraints: The Case of Clitics. XMG can be extended
to support specific constraints on tree descriptions (e.g., constraints on node linear
order), which make it possible to describe linguistic-dependent phenomena, such as,
for instance, clitic ordering in French, at a meta-level (i.e., within the metagrammar).
According to Perlmutter (1970), clitics are subject to two hard constraints. First,
they appear in front of the verb in a fixed order according to their rank (Exam-
ples 25a and 25b).16 Second, two different clitics in front of the verb cannot have the
same rank (Example 25c).
(25) a. Jean le3 lui4 donne.
?John gives it to him.?
b. *Jean lui4 le3 donne.
*?John gives to him it.?
c. *Jean le3 la3 donne.
*?John gives it it.?
16 In (Examples 25a?c), the numbers on the clitics indicate their rank.
609
Computational Linguistics Volume 39, Number 3
S
N? ?+ V?
?
V?
Cl?3 ?+ V
?
V?
Cl?4 ?+ V
?
S
V?
V
?
S
N? V?
Cl?3 Cl?4 V
S
N? V?
Cl?4 Cl?3 V
Figure 7
Clitic ordering in French.
To support a direct encoding of Perlmutter?s observation, XMG includes both a
node uniqueness principle and a node ordering principle. The latter allows us to label
nodes with some property (let us call it rank) whose value is an integer (for instance,
one can define a node as n1(rank : 2)[cat : Cl]). When solving tree descriptions, XMG
further requires that in a valid tree model, (i) there are no two nodes with the same
rank and (ii) sibling nodes labeled with a rank are linearly ordered according to their
rank.
Accordingly, in the French grammar of Crabbe? (2005), each node labeled with a clitic
category is also labeled with a numerical node property representing its rank.17 XMG
ordering principle then ensures that the ill-formed tree crossed out in Figure 7 is not
produced. Note that in Figure 7, every type of clitic is defined locally (i.e., in a separate
class), and that the interactions between these local definitions are handled by XMG
using this rank principle, to produce only one valid description (pictured to the right of
the arrow).
That is, XMG ordering constraints permit a simple, declarative encoding of the
interaction between clitics. This again contrasts with systems based on lexical rules. As
noted by Perlmutter (1970), if clitics are assumed to be moved by transformations, then
the order in which lexical rules apply this movement must be specified.
To implement the uniqueness principle, one needs to express the fact that in a valid
model ?, there is only one node having a given property p (i.e., a parameter of the
constraint, here the value of the rank node property). This can be done by introducing,
for each node n of the description, a Boolean variable pn indicating whether the node
denoting n in the model has this property or not (i.e., are there two nodes of identical
rank?). Then, if we call V?p the set of integers referring to nodes having the property p in
a model, we have: pn ? (Eqn ? V
?
p ) = ?. Finally, if we represent pn being true with 1 and
pn being false with 0,18 and we sum pn for each n in the model, we have that in a valid
model this sum is strictly lower than 2:
?
n?? pn < 2.
To implement the ordering principle, one needs to express the fact that in a valid
model ?, two sibling nodes n1 and n2 having a given property p of type integer and
of values p1 and p2, respectively, are such that the linear precedence between these
nodes conform to the natural order between p1 and p2. This can be done by first
introducing, for each pair of nodes n, m of the description, a Boolean variable bn,m
indicating whether they have the same ancestors: bn,m ? (Upn ? Upm) = (Upn ? Upm).
For each pair of nodes that do so, we check whether they both have the property p,
17 Recall that node properties are features whose values are used by the tree description solver in order to
restrict the set of valid models. These properties may not appear in the trees produced from the input
metagrammar. For instance, the rank property is not part of the FB-LTAG formalism, and thus does not
appear in the FB-LTAG elementary trees produced by XMG.
18 These integer representations are usually called reified constraints.
610
Crabbe? et al XMG: eXtensible MetaGrammar
and if this is the case, we add to the input description a strict precedence constraint on
these nodes according to their respective values of the property p:19
bn,m ? (pn < pm) ? n ?+ m (26)
bn,m ? (pm < pn) ? m ?+ n (27)
5.3.3 Adding Color Constraints to Facilitate Grammar Writing. To further ease grammar
development, XMG supports a node coloring mechanism that permits nameless node
identification (Crabbe? and Duchier 2004), reminiscent of the polarity-based node iden-
tification first proposed by Muskens and Krahmer (1998) and later used by Duchier
and Thater (1999) and Perrier (2000). Such a mechanism offers an alternative to explicit
node identification using equations between node variables. The idea is to label node
variables with a color property, whose value (either red, black, or white) can trigger
node identifications.
This mechanism is another parameter of the tree solver. When in use, the valid
tree models must satisfy some color constraints, namely, they must only have red or
black nodes (no remaining white nodes; these have to be identified with some black
nodes). As shown in the following table, node identification must observe the following
constraints: A white node must be identified with a black node; a red node cannot be
identified with any other node; and a black node may be identified with one or more
white nodes.20
?B ?R ?W ?
?B ? ? ?B ?
?R ? ? ? ?
?W ?B ? ?W ?
? ? ? ? ?
We now briefly describe how the constraint solver sketched in Section 5.3.1 was
extended to support colors. As mentioned previously, in valid models all white nodes
are identified with a black node (at most one black node per white node). Consequently,
there is a bijection from the red and black nodes of the tree description to the nodes of
the model. In order to take this bijection into account, we add a node variable RBn to
the five sets already associated with a node variable n from Section 5.1. RBn denotes
either n if n is a black or red node, or the black node identified with n if n is a white
node. Note that all the node variables must be colored: the set of node variables in a
tree description can then be partitioned into three sets: Red, Black, and White. Basically,
we know that, for all nodes n, RBn ? Eqn (this is what the bijection is about). Again
we translate color information into constraints on node sets (these constraints help the
generic tree solver by reducing the ambiguity for the Eqn sets):
n ? Red ? (n = RBn) ? (Eqn = {n}) (28)
n ? Black ? (n = RBn) ? (Eqn\{n} ? White) (29)
n ? White ? (RBn ? Black) ? (Eqn ? Black = {RBn}) (30)
19 In fact, rather than adding strict precedence constraints to the tree description, we directly add to the
solver their equivalent set constraints on Eq, Up, Left, Right, Down, introduced earlier.
20 In other words, node colors can be seen as information on node saturation.
611
Computational Linguistics Volume 39, Number 3
Node coloring offers an alternative to complex namespace management. The main
advantage of this particular identification mechanism is its economy: Not only is there
no longer any need to remember node identifiers, there is in fact no need to choose a
name for node variables.
It is worth stressing that the XMG node identification process is reduced to a
constraint-solving problem and so it is not a sequential process. Thus the criticisms
leveled by Cohen-Sygal and Wintner (2007, 2009) against non-associative constraints
on node unification do not apply.
Briefly, in their work, Cohen-Sygal and Wintner (2007, 2009) showed that any
polarity-based tree description formalism is not associative. In other words, when
describing trees in terms of combinations of polarized structures, the order in which
the structures are combined matters (i.e., the output structures depend on the combi-
nation order). This feature makes such formalisms not appropriate for a modular and
collaborative grammar engineering, such as that of Cohen-Sygal and Wintner (2011) for
Unification Grammar.
In the XMG case, when using node colors, the tree description solver does not
rely on any specific fragment combination order. It computes all possible combination
orders. In this context, the grammar designer cannot think in terms of sequences of node
identifications. This would lead to tree overgeneration.
Again, it is important to remember that tree solving computes any valid tree model,
independently of any specific sequence of node identifications (all valid node identifica-
tions are computed). In this context, non-associativity of color-based node identification
is not an issue, but rather a feature, as it allows for a compact description of a large
number of node identifications (and thus of tree structures).
6. Writing Grammars with XMG
In this section, we first provide a detailed example showing how XMG can be used to
specify the verbal trees of a large FB-LTAG for French extended with unification-based
semantics. We then give a brief description of several large- and middle-scale grammars
that were implemented using XMG.
6.1 SEMTAG: A large FB-LTAG for French Covering Syntax and Semantics
We now outline the XMG specification for the verbal trees of SEMTAG, a large FB-LTAG
for French. This specification further illustrates how the various features of XMG (e.g.,
combined use of disjunction and conjunction, node colors) permit us to specify compact
and declarative grammar descriptions. We first discuss the syntactic dimension (SYN).
We then go on to show how the semantic dimension (SEM) and the syntax/semantic
interface (DYN) are specified.
6.1.1 The Syntactic Dimension. The methodology used to implement the verbal fragment
of SEMTAG can be summarized as follows. First, tree fragments are defined that rep-
resent either a possible realization of a verb argument or a possible realization of the
verb. The verbal elementary TAG trees of SEMTAG are then defined by appropriately
combining these tree fragments.
To maximize structure sharing, we work with four levels of abstraction. First, basic
tree fragments describing verb or verb argument realizations are defined. Second, gram-
matical functions are defined as disjunctions of argument realizations. Third, verbal
diathesis alternatives are defined as conjunctions of verb realizations and grammatical
612
Crabbe? et al XMG: eXtensible MetaGrammar
CanonSubj ?
S?W
N??R V?W CanonObj ?
S?W
V?W N??R
CanonIndirObj ?
S?W
V?W PP?R
P?R
a`?R
N??R
CanonByObj ?
S?W
V?W PP?R
P?R
par?R
N??R
RelatSubj ?
N?R
N?R S?W
N??R V?W WhObj ?
S?R
N??R S?W
V?W
WhByObj ?
S?R
PP?R
P?R
par?R
N??R
S?W
WhIndirObj ?
S?R
PP?R
P?R
a`?R
N??R
S?W
ActiveVerbForm?
S?B
V?B PassiveVerbForm?
S?B
V?B
V??B V?B
Figure 8
Elementary tree fragments used as building blocks of the grammar (nodes are colored to control
their identification when blocks are combined).
functions. Fourth, diathesis alternatives are gathered into tree families. In the next
paragraphs, we explain each of these levels in more detail.
Tree fragments. Tree fragments are the basic building blocks used to define SEMTAG.
These are the units that are shared and reused in the definition of many elementary
trees. For instance, the fragment for a canonical subject will be used by all FB-LTAG
elementary trees involving a canonical subject.
As mentioned earlier, to specify the verbal elementary trees of SEMTAG, we begin
by defining tree fragments which describe the possible syntactic realizations of the verb
arguments and of the verb itself. Figure 8 provides some illustrative examples of these
fragments. Here and in the following, we omit the feature structures decorating the trees
to facilitate reading.21
To further factorize information and facilitate grammar maintenance, the basic tree
fragments are organized in an inheritance hierarchy.22 Figure 9 shows a partial view of
21 See Crabbe? (2005) for a complete description of SEMTAG tree fragments, including feature structures.
22 Recall from Section 4 that inheritance is used to share namespaces. Thus, (node or feature) variables
introduced in a given class C can be directly reused in the sub-classes of C.
613
Computational Linguistics Volume 39, Number 3
VerbalArgument
CanonSubj CanonCompl
CanonObj CanPP
CanonIndirObj CanonByObj
Wh
WhObj WhPP
WhIndirObj WhByObj
RelatSubj
Figure 9
Organization of elementary fragments in an inheritance hierarchy.
this hierarchy illustrating how the tree fragments for argument realization depicted in
Figure 8 are organized to maximize the sharing of common information. The hierarchy
classifies the verbal arguments depicted in Figure 8 into four categories:
1. The canonical subject is a noun realized in front of the verb.
2. Canonical complements occur after the verb. The canonical object is a
noun phrase whereas prepositional complements are introduced by
specific prepositions, namely, a` for the canonical indirect object and
par for the canonical by object.
3. Wh-arguments (or questioned arguments) occur in front of a sentence
headed by a verb. A Wh-object is an extracted noun whereas questioned
prepositional objects are extracted prepositional phrases that are
introduced by a specific preposition.
4. Finally, the relativized subject is a relative pronoun realized in front
of the sentence. Extracted subjects in French cannot be realized at an
unbounded distance from the predicate.
Syntactic functions. The second level of abstraction uses syntactic function names such
as Subject and Object to group together alternative ways in which a given syntactic
function can be realized. For instance, if we make the simplifying assumption that the
possible argument realizations are limited to those given in Figure 8, the Subject, Object,
ByObject, and IndirectObject classes would be defined as follows.23
Subject ? CanonSubj ? RelatSubj (31)
Object ? CanonObj ? WhObj (32)
ByObject ? CanonByObj ? WhByObj (33)
IndirectObject ? CanonIndirObj ? WhIndirObj (34)
That is, we define the Subject class as an abstraction for talking about the set of tree
fragments that represent the possible realizations of a subject argument?namely, in
23 Note that, when these abstractions will be combined to describe for instance transitive verbs, the
combination of WhObj with WhByObj will be ruled out by using a uniqueness principle such as
introduced in Section 5.
614
Crabbe? et al XMG: eXtensible MetaGrammar
our restricted example, canonical and relativized subject. Thus, the simplified Subject
class defined in Equation (31) characterizes contexts such as the following:
(35) a. Jean mange. (canonical subject)
?John eats.?
b. Le garc?on qui mange (relativized subject)
?The boy who eats?
Similarly, the IndirectObject class abstracts over the realization of an argument intro-
duced by the preposition a` to the right of the verb (CanonIndirObj) or realized in
extracted position (possibly realized at an unbounded distance from the predicate) as
illustrated by the following examples:
(36) a. Jean parle a` Marie. (canonical indirect object)
?John talks to Mary.?
b. A` qui Jean parle-t-il ? (wh indirect object)
?To whom is John talking ??
c. A` qui Pierre croit-il que Jean parle ? (wh indirect object)
?To whom Peter thinks that John talks ??
This way of grouping tree fragments is reminiscent of the informal classification of
French syntactic functions presented by Iordanskaja and Mel?c?uk (2009) whereby each
syntactic function is associated with a set of possible syntactic constructions.
Diathesis alternations. In this third level, we take advantage of the abstractions defined
in the previous level to represent diathesis alternations. Again, we are interested here
in describing alternatives. Diathesis alternations are those alternations of mapping
between arguments and syntactic functions such as for instance the active/passive
alternation. In a diathesis alternation, the actual form of the verb constrains the way
predicate arguments are realized in syntax. Thus, in the following example, it is con-
sidered that both Examples (37a) and (37b) are alternative realizations of a predicate
argument structure such as send(John, a letter).
(37) a. Jean envoie une lettre.
?John sends a letter.?
b. Une lettre est envoye?e par Jean.
?A letter is sent by John.?
The active/passive diathesis alternation captures the fact that if the verb is in the
active form, its two arguments are realized by a subject and an object whereas if the
verb is in the passive form, then the arguments consist of a subject and a by-object.
TransitiveDiathesis ? (Subject ? ActiveVerbForm ? Object)
? (Subject ? PassiveVerbForm ? ByObject)
(38)
Finally a traditional case of ?erasing,?24 such as the agentless passive (or passive
without agent) can be expressed in our language by adding an additional alternative
24 It is often argued that a language of grammatical representation must be equipped with an ?erasing
device? like lexical rules because of phenomena such as the passive without agent. In this framework it
turns out that this kind of device is not needed because we do not grant any special status to base trees.
615
Computational Linguistics Volume 39, Number 3
where the by-object or agentive complement is not expressed. Thus Equation (39) is an
augmentation of (38) where we have added the agentless passive alternative (indicated
in boldface).
TransitiveDiathesis ? (Subject ? ActiveVerbForm ? Object)
? (Subject ? PassiveVerbForm ? ByObject)
? (Subject ? PassiveVerbForm)
(39)
This methodology can be further augmented to implement an actual linking in the
manner of Bresnan and Zaenen (1990). For the so-called erasing cases, one can map the
?erased? predicative argument to an empty realization in syntax. We refer the reader to
Crabbe? (2005) for further details.
Tree families. Finally, tree families are defined?that is, sets of trees capturing alternative
realizations of a given verb type (i.e., sub-categorization frame). Continuing with the
simplified example presented so far, we can for instance define the tree family for
verbs taking a nominal subject, a nominal object, and an indirect nominal object (i.e.,
ditransitive verbs) as follows:
DitransitiveFamily ? TransitiveDiathesis ? IndirectObject (40)
The trees generated for such a family will, among others, handle the following
contexts:25
(41) a. Jean offre des fleurs a` Marie.
?John offers flowers to Mary.?
b. A` quelle fille Jean offre-t-il des fleurs ?
?To which girl does John offer flowers ??
c. Le garc?on qui offre des fleurs a` Marie.
?The boy who offers flowers to Mary.?
d. Quelles fleurs le garc?on offre-t-il a` Marie ?
?Which flowers does the boy offer to Mary ??
e. Les fleurs sont offertes par Jean a` Marie.
?The flowers are offered by John to Mary.?
f. Par quel garc?on les fleurs sont-elles offertes a` Marie ?
?By which boy are the flowers offered to Mary ??
It is straightforward to extend the grammar with new families. Thus, for instance,
Equation (42) shows how to define the transitive family (for verbs taking a nominal
subject and a nominal object) and Equation (43), the intransitive one (alternatives of a
verb sub-categorizing for a nominal subject).
TransitiveFamily ? TransitiveDiathesis (42)
IntransitiveFamily ? Subject ? ActiveVerbForm (43)
25 Note that number and gender agreements are dealt with using coreferences between features labeling
syntactic nodes, see Crabbe? (2005).
616
Crabbe? et al XMG: eXtensible MetaGrammar
Similarly, tree families for non-verbal predicates (adjectives, nouns) can be defined
using the abstraction over grammatical functions defined for verbs. For instance, the ex-
amples in (44a?44b) can be captured using the adjectival trees defined in Equations (46)
and (47), respectively, where Subject extends the definition of subject given above with a
Wh-subject, PredAdj combines a subject tree fragment with a tree fragment describing a
predicative adjective, and PredAdjAObj extends a PredAdj tree fragment with a canonical
a`-object.
(44) a. Jean est attentif. Qui est attentif ? L?homme qui est attentif
?John is mindful. Who is mindful ? The man who is mindful?
b. Jean est attentif a` Marie. Qui est attentif a` Marie ? L?homme qui est attentif a`
Marie
?John is mindful of Mary. Who is mindful of Mary ? The man who is mindful
of Mary?
Subject ? CanonSubj ? RelatSubj ? WhSubj (45)
PredAdj ? Subject ? AdjectivalForm (46)
PredAdjAObj ? PredAdj ? CanonAObj (47)
6.1.2 The Semantic Dimension and the Syntax/Semantic Interface. We now show how to
extend the XMG specification presented in the previous section to integrate a
unification-based compositional semantics. Three main changes need to be carried out:
1. Each elementary tree must be associated with a semantic formula. This is
done using the SEM dimension.
2. The nodes of elementary trees must be labeled with the appropriate
semantic indices. This involves introducing the correct attribute-value pair
in the correct feature structure (top or bottom) on the appropriate node.
3. Syntax and semantics need to be synchronized?that is, variable sharing
between semantic formulae and tree indices need to be enforced. To this
end we use the DYN dimension.
Informing the semantic dimension. To associate each elementary tree with a formula rep-
resenting the meaning of the words potentially anchoring that tree, we use the SEM
dimension to specify a semantic schema. For instance, the TransitiveFamily class defined
in Equation (42) for verbs taking two nominal arguments is extended as follows:
TransitiveFamily ? TransitiveDiathesis ? BinaryRel (48)
where TransitiveDiathesis is the XMG class defined in Equation (39) to describe the set of
trees associated with transitive verbs and BinaryRel the class describing the following
semantic schema:
L : P(E) ? L : Theta1(E, X) ? L : Theta2(E, Y) (49)
In this semantic schema, P, Theta1, and Theta2 are unification variables that become
ground when the tree is anchored with a specific word. For instance, P, Theta1, and
Theta2 are instantiated to eat, agent, and patient, respectively, when the anchor is ate (these
617
Computational Linguistics Volume 39, Number 3
pieces of information?predicate, thematic roles?are associated with lemmas, located
in the syntactic lexicon, and unified with adequate semantic variables via anchoring
equations). Further, X, Y, E, L are unification variables representing semantic arguments.
As illustrated in Figure 3, these become ground during (or after) derivation as a side
effect of the substitutions and adjunctions taking place when trees are combined. It
is worth noting that by combining semantic schemas with diathesis classes, one such
specification assigns the specified semantic schema to many trees, namely, all the trees
described by the corresponding diathesis class. In this way, the assignment of semantic
formulae to trees is relatively economical. Indeed in SEMTAG, roughly 6,000 trees are
assigned a semantic schema using a total of 75 schema calls.
Co-indexing trees and formulae indices. Assuming that tree nodes are appropriately deco-
rated with semantic indices by the specification scheme described in the next paragraph,
we now show how to enforce the correct mapping between syntactic and semantic
arguments. This is done in two steps.
First, we define a set of interface constraints of the form ?indexF : V, argi : V? which
are used to enforce the identification of the semantic index (indexF) labeling a given tree
node with grammatical function F (e.g., F := subject) with the index (argi) representing
the i-th argument in a semantic schema. For instance, the following constraints ensure
a subject/arg1 mapping, that is, a coreference between the index labeling a subject node
and the index representing the first argument of a semantic schema:
C1 ? Node [idx : I] ? ?indexsubject : I?
C2 ? L : P(E) ? L : Theta1(E, X) ? ?arg1 : X?
SubjectArg1 ? C1 ? C2 ? ?indexsubject : V, arg1 : V?
(50)
Given such interface constraints, we refine the diathesis definitions so as to ensure the
correct bindings. For instance, the specification in Equation (38) is modified to:
TransitiveDiathesis ? TransitiveActive ? TransitivePassive
TransitiveActive ? (SubjectArg1 ? ObjectArg2?
Subject ? ActiveVerbForm ? Object)
(51)
and the passive diathesis is specified as:
TransitivePassive ? (SubjectArg2 ? ByObjectArg1?
Subject ? PassiveVerbForm ? ByObject)
(52)
Labeling tree nodes with semantic indices. This scheme relies on the assumption that tree
nodes are appropriately labeled with semantic indices (e.g., the subject node must be
labeled with a semantic index) and that these indices are appropriately named (arg1
must denote the parameter representing the first argument of a binary relation and
indexsubject the value of the index feature on a subject node). As suggested by Gardent
(2007), a complete semantic labeling of a TAG with the semantic features necessary
618
Crabbe? et al XMG: eXtensible MetaGrammar
to enrich this TAG with the unification-based compositional semantics sketched in the
previous section can be obtained by applying the following labeling principles:26
Argument labeling: In trees associated with semantic functors, each argument node
is labeled with a semantic index27 named after the grammatical function of the
argument node (e.g., indexsubject for a subject node).
Controller/Controllee: In trees associated with control verbs, the semantic index of the
controller is identified with the value of the controlled index occurring on the
sentential argument node.
Anchor projection: The anchor node projects its index up to its maximal projection.
Foot projection: A foot node projects its index up to the root.28
As we shall now see, XMG permits a fairly direct encoding of these principles.
The Argument Labeling principle states that, in the tree associated with a syntactic
functor (e.g., a verb), each node representing a syntactic argument (e.g., the subject
node) should be labeled with a semantic index named after the grammatical function of
that node (e.g., indexsubject).29
To specify this labeling, we define for each grammatical function Function ?
{Subject, Object, ByObject, IndirectObject, . . . }, a semantic class FunctionSem which as-
sociates with an (exported) node variable called FunctionNode the feature value pair
[index : I] and a DYN constraint of the form ?indexFunction : I?. For instance, the class
SubjectSem associates the node SubjectNode with the feature value pair [index : I] and
the DYN constraint ?indexsubject : I?.
SubjectSem ? SubjectNode [index : I] ? ?indexsubject : I? (53)
Additionally, in the tree fragments describing the possible realizations of the grammat-
ical functions, the (exported) variable denoting the argument node is systematically
named ArgNode.
Finally, we modify the specification of the realizations of the grammatical functions
to import the appropriate semantic class and identify ArgNode and FunctionNode. For
instance, the Subject specification given above is changed to:
Subject ? SubjectSem ? ArgNode = SubjectNode ?
(CanonSubj ? RelatSubj ? WhSubj)
(54)
26 The principles required to handle quantification are omitted. We refer the reader to Gardent (2007) for a
more extensive presentation of how semantics is implemented using XMG.
27 For simplicity, we only mention indices. To be complete, however, labels should also be used.
28 The foot projection principle only applies to foot nodes that are not argument nodes (i.e., to modifiee
nodes).
29 In other words, this argument labeling principle defines an explicit and normalized reference to any
realization of a semantic argument. Following FB-LTAG predicate?argument co-occurrence principle
(Abeille?, Candito, and Kinyon 1999), we know that any elementary tree includes a leaf node for each
realized semantic argument of its anchor. This principle thus holds in any FB-LTAG. Its implementation,
however, is closely related to the architecture of the metagrammar; here we benefit from the fact that
verbal arguments are described in dedicated classes to reach a high degree of factorization.
619
Computational Linguistics Volume 39, Number 3
E3
E2
E2
E1
E2
E1
E1
E
E1
E
E1
E
? E? ? E? ? E?
Depth 3 Depth 2 Depth 1
SE2E1
VPE1E
?VE?
ActiveVerbForm
Figure 10
Anchor/Foot projection.
As a result, all ArgNode nodes in the tree descriptions associated with a subject realiza-
tion are labeled with an index feature I whose global name is indexsubject.
Value sharing between the semantic index of the controller (e.g., the subject of
the control verb) and that of the controllee (e.g., the empty subject of the infinitival
complement) is enforced using linking constraints between the semantic index labeling
the controller node and that labeling the sentential argument node of the control verb.
Control verb definitions then import the appropriate (object or subject control) linking
constraint.
The anchor (respectively, foot) projection principle stipulates the projection of
semantic indices from the anchor (respectively, foot) node up to the maximal projection
(respectively, root). Concretely, this means that the top and bottom features of the nodes
located on this path between the anchor (respectively, foot) and the maximal projection
(respectively, root) all include an index feature whose value is shared between adjacent
nodes (see variables Ei in Figure 10).30 Once the top and bottom structures are unified,
so are the semantic indices along this path (modulo expected adjunctions realized on
the projection).
To implement these principles, we define a set of anchor projection classes
{Depth1, Depth2, Depth3} as illustrated in Figure 10. We then ?glue? these projection
skeletons onto the relevant syntactic trees by importing the skeletons in the syntactic
tree description and explicitly identifying the anchor node of the semantic projection
classes with the anchor or foot node of these syntactic tree descriptions. Because the
models must be trees, the nodes dominating the anchor node of the projection class
will deterministically be identified with those dominating the anchor or foot node of
the trees being combined with. For instance, for verbs, the class specifying the verbal
spine (e.g., ActiveVerbForm, see Figure 10) equates the anchor node of the verbal spine
with that of the projection skeleton. As a result, the verb projects its index up to
the root.
6.1.3 Some Figures About SEMTAG. As mentioned previously, SEMTAG is a large FB-LTAG
for French equipped with semantics (Gardent 2008); it extends the purely syntactic
FTAG of Crabbe? (2005) with a unification based compositional semantics as described
by Gardent and Kallmeyer (2003).31 The syntactic FTAG in essence implements Abeille??s
(2002) proposal for an FB-LTAG-based modeling of French syntax. FTAG contains
around 6,000 elementary trees built from 293 XMG classes and covers some 40 basic
30 For sake of brevity, we write E2E1 for [bot : [index : E1] top : [index : E2]]. ? ? refers to the anchor / foot.31 FTAG and SEMTAG are freely available under the terms of the GPL-compliant CeCILL license, the former
at https://sourcesup.renater.fr/scm/viewvc.php/trunk/METAGRAMMARS/FrenchTAG/?root=xmg, and
the latter on request.
620
Crabbe? et al XMG: eXtensible MetaGrammar
verbal sub-categorization frames. For each of these frames, FTAG defines a set of
argument alternations (active, passive, middle, neuter, reflexivization, impersonal,
passive impersonal) and of argument realizations (cliticization, extraction, omission,
permutations, etc.) possible for this frame. Predicative (adjectival, nominal, and
prepositional) and light verb constructions are also covered as well as some common
sub-categorizing noun and adjective constructions. Basic descriptions are provided for
the remaining constructions namely, adverbs, determiners, and prepositions.
FTAG and SEMTAG were both evaluated on the Test Suite for Natural Language Pro-
cessing (TSNLP) (Lehmann et al 1996), using a lexicon designed specifically on the test
suite, hence reducing lexical ambiguity (Crabbe? 2005; Parmentier 2007). This test suite
focuses on difficult syntactical phenomena, providing grammatical and ungrammatical
sentences. These competence grammars accept 76% of the grammatical items, reject 83%
of the ungrammatical items, and have an average ambiguity of 1.64 parses per sentence.
To give an idea of the compilation time, under architectures made of a 2-Ghz processor
with 1 Gb of RAM, it takes XMG 10 minutes to compile the whole SEMTAG (recall that
there is no semantic description solving, hence the compilation times between FTAG
and SEMTAG do not differ).32
Note that SEMTAG can be used for assigning semantic representations to sentences
when combined with an FB-LTAG parser and a semantic construction module as de-
scribed by Gardent and Parmentier (2005, 2007).33 Conversely, it can be used to verbalize
the meaning denoted by a given semantic representation when coupled with the GenI
surface realizer described by Gardent and Kow (2007).
6.2 Other Grammars Designed with XMG
XMG has been used mainly to design FB-LTAG and IG for French or English. More
recently, it has also been used to design a FB-LTAG for Vietnamese and a TreeTuple
MC-TAG for German. We now briefly describe each of these resources.
SemXTAG. The English grammar, SEMXTAG (Alahverdzhieva 2008), reimplements the
FB-LTAG developed for English at the University of Pennsylvania (XTAG Research
Group 2001) and extends it with a unification-based semantics. It contains 1,017 trees
and covers the syntactic fragment of XTAG, namely, auxiliaries, copula, raising and
small clause constructions, topicalization, relative clauses, infinitives, gerunds, pas-
sives, adjuncts, ditransitives (and datives), ergatives, it-clefts, wh-clefts, PRO con-
structions, noun?noun modification, extraposition, determiner sequences, genitives,
negation, noun?verb contractions, sentential adjuncts, imperatives, and resultatives.
The grammar was tested on a handbuilt test-suite of 998 sentences illustrating the
various syntactic constructions meant to be covered by the grammar. All sentences in
the test suite can be parsed using the grammar.
FrenchIG. The extended XMG framework was used to design a core IG for French
consisting of 2,059 tree descriptions compiled out of 448 classes (Perrier 2007). The
resulting grammar is lexicalized, and its coverage was evaluated using the previously
mentioned TSNLP. The French IG accepts 88% of the grammatical sentences and rejects
32 As a comparison, about one hour was needed by Candito?s (1999) compiler to produce a French FB-LTAG
containing about 1,000 tree schemas.
33 As an alternative way to parse FB-LTAG grammars equipped with flat semantics such as those produced
by XMG, one can use the Tu?bingen Linguistic Parsing Architecture (TuLiPA) (Kallmeyer et al 2010).
621
Computational Linguistics Volume 39, Number 3
85% of the ungrammatical sentences, although the current version of the French IG
does not yet cover all the syntactic phenomena presented in the test suite (for example,
causative and superlative constructions).
Vietnamese TAG. The XMG language was used by Le Hong, N?Guyen, and Roussanaly
(2008) to produce a core FB-LTAG for Vietnamese. Their work is rather a proof of con-
cept than a large-scale implementation. They focused on Vietnamese?s categorization
frames, and were able to produce a TAG covering the following frames: intransitive
(tree family N0V), transitive with a nominal complement (N0VN1), transitive with a
clausal complement (N0VS1), transitive with modal complement (N0V0V1), ditransi-
tive (N0VN1N2), ditransitive with a preposition (N0VN1ON2), ditransitive with a ver-
bal complement (N0V0N1V1), ditransitive with an adjectival complement (N0VN1A),
movement verbs with a nominal complement (N0V0V1N1), movement verbs with an
adjectival complement (N0V0AV1), and movement ditransitive (N0V0N1V1N2).
GerTT. Another XMG-based grammar corresponds to the German MC-TAG of
Kallmeyer et al (2008). This grammar, called GerTT, is in fact an MC-TAG with
Tree Tuples (Lichte 2007). This variant of MCTAG has been designed to model free
word order phenomena. This is done by imposing node sharing constraints on MCTAG
derivations (Kallmeyer 2005). GerTT covers phenomena such as scrambling, coherent
constructions, relative clauses, embedded questions, copula verbs, complementized
sentences, verbs with various sub-categorization frames, nouns, prepositions, determin-
ers, adjectives, and partly includes semantics. It is made of 103 tree tuples, compiled
from 109 classes.
7. Related Work
We now compare XMG with existing environments for designing tree-based grammars
and briefly report on the grammars designed with these systems.
7.1 Environments for Designing Tree-Based Grammars
Candito?s Metagrammar Compiler. The concept of metagrammar was introduced by
Candito (1996). In her paper, Candito presented a compiler for abstract specifications
of FB-LTAG trees (the so-called metagrammars). Such specifications are based on three
dimensions, each of them being encoded in a separate inheritance hierarchy of linguistic
descriptions. Dimension 1 describes canonical sub-categorization frames (e.g., transitive),
the Dimension 2 describes redistributions of syntactic functions (e.g., active to passive),
and Dimension 3 the tree descriptions corresponding to the realizations of the syntactic
functions defined in Dimension 2. This three-dimensional metagrammatical description
is then processed by a compiler to compute FB-LTAG tree schemas. In essence, these
tree schemas are produced by associating a canonical sub-categorization frame (Dimen-
sion 1) with a compatible redistribution schema (Dimension 2), and with exactly one
function realization (Dimension 3) for each function required by the sub-categorization
frame.
Candito?s (1996, 1999) approach improves on previous proposals by Vijay-Shanker
and Schabes (1992) and Evans, Gazdar, and Weir (1995) in that it provides a linguistically
principled basis for structuring the inheritance hierarchy. As shown in Section 6.1,
622
Crabbe? et al XMG: eXtensible MetaGrammar
the XMG definition of SEMTAG uses similar principles. Candito?s approach differs,
however, from the XMG account in several important ways:
 Much of the linguistic knowledge used to determine which classes to
combine is hard-coded in the compiler (unlike in XMG, there is no explicit
control on class combinations). In other words, there is no clear separation
between the linguistic knowledge needed to specify a high-level FB-LTAG
description and the algorithm used to compile an actual FB-LTAG from
this description. This makes grammar extension and maintenance by
linguists extremely difficult.
 As in Vijay-Shanker and Schabes (1992) Evans, Gazdar, and Weir (1995),
the linguistic description is non-monotonic in that some erasing classes
are used to remove information introduced by other dimensions
(e.g., agentless passive).
 The approach fails to provide an easy means to state exceptions. These
are usually encoded in the compiling algorithm.
 The tree description language used to specify classes in Dimension 3
relies on global node variables. Thus, two variables with identical names
introduced in different classes are expected to refer to the same tree node.
As argued in Section 4, this makes it hard to design large-scale
metagrammars.
The LexOrg system. An approach similar to Candito?s was presented by Xia et al
(1998), Xia (2001), and Xia, Palmer, and Vijay-Shanker (2005, 2010). As in Candito?s
approach, a TAG abstract specification relies on a three-dimensional description made
of, namely, sub-categorization frames, blocks, and lexical redistribution rules. To com-
pile this specification into a TAG, the system selects a canonical sub-categorization
frame, and applies some lexical redistribution rules to derive new frames and finally
select blocks corresponding to the resulting frames. These blocks contain tree descrip-
tions using the logic of Rogers and Vijay-Shanker (1994).
LexOrg suffers from similar limitations as Candito?s compiler. Much of the lin-
guistic knowledge is embedded in the compiling algorithm, making it difficult for
linguists to extend the grammar description and to handle exceptions. Unlike in Can-
dito?s framework, the tree description language uses local node variables and lets the
tree description solver determine node identifications. Although this avoids having to
memorize node names, this requires that the descriptions be constrained enough to
impose the required node identifications and prevent the unwanted ones. In practice,
this again complicates grammar writing. In contrast, XMG provides an intermediate
solution which, by combining local variables with export declarations, avoids having to
memorize too many node variable names (only those local to the relevant sub-hierarchy
need memorizing) while allowing for explicit node identification.
The Metagrammar Compiler of Gaiffe, Crabbe?, and Roussanaly. Gaiffe, Crabbe?, and
Roussanaly (2002) proposed a compiler for FB-LTAG that aims to remedy both the lack
of a clear separation between linguistic information and compilation algorithm, and
the lack of explicit control on the class combinations prevalent in Candito (1996), Xia
et al (1998), and Xia (2001). In their approach, the linguistic specification consists of
a single inheritance hierarchy of classes, each class containing a tree description. The
623
Computational Linguistics Volume 39, Number 3
description logic used is similar to Candito?s. That is, global node names are used. To
trigger class combinations, classes are labeled with two types of information: needs and
resources. The compiler selects all final classes of the hierarchy, performs all possible
combinations, and only keeps those combinations that neutralize the stated needs
and resources. The tree descriptions contained in these neutral combinations are then
solved to produce the expected trees.
Although this approach implements a clear separation between linguistic informa-
tion and compilation algorithm, the fully automatic derivation of FB-LTAG trees from
the inheritance hierarchy makes it difficult in practice to control overgeneration. In
contrast, XMG?s explicit definitions of class combinations by conjunction, disjunction,
and inheritance makes it easier to control the tree set that will be generated by the
compiler from the grammar specification. Additionally, the issues raised by global
variables remain (no way to instantiate twice a given class, and cumbersome definition
of variables in large metagrammars).
The MGCOMP System. More recently, Villemonte de la Clergerie (2005, 2010) proposed a
compiler for FB-LTAG that aims at preserving a high degree of factorization in both the
abstract grammar specification and the grammar which is compiled from it. Thus, the
MGCOMP system does not compute FB-LTAG elementary trees, but factorized trees.
In MGCOMP, like in Gaiffe, Crabbe?, and Roussanaly?s (2002) approach, a meta-
grammar consists of a single hierarchy of classes. The classes are labeled with needs and
resources, and final classes of the hierarchy are combined to compute tree descriptions.
The main differences with Gaiffe, Crabbe?, and Roussanaly (2002), lies in the fact that
(i) a description can include new factorizing operators, such as repetition (Kleene-star
operator), shuffling (interleaving of nodes), optionality, and disjunctions; and (ii) it offers
namespaces to specify the scope of variables. MGCOMP?s extended tree descriptions
are not completely solved by the compiler. Rather, it compiles underspecified trees (also
called factorized trees). With this approach, a large grammar is much smaller in terms of
number of grammatical structures than a classical FB-LTAG. As a result, the grammars it
compiles are only compatible with the DyALog parsing environment (Villemonte de La
Clergerie 2005). And, because the linguist designs factorized trees and not actual TAG
trees, debugging the metagrammar becomes harder.
7.2 Resources Built Using Candito, Xia, and De La Clergerie?s Systems
Candito?s system has been used by Candito (1999) herself to design a core FB-LTAG
for French and Italian, and later by Barrier (2006) to design a FB-LTAG for adjectives
in French. Xia?s system (LexOrg) has been used to semi-automatically generate XTAG
(Xia 2001). De La Clergerie?s system (MGCOMP) has been used to design a grammar
for French named FRMG (FRench MetaGrammar) (Villemonte de la Clergerie 2010).
FRMG makes use of MGCOMP?s factorizing operators (e.g., shuffling operator), thus
producing not sensu stricto a FB-LTAG, but a factorized FB-LTAG. FRMG is freely
available, contains 207 factorized trees (having optional branches, etc.) built from 279
metagrammatical classes, and covers 95% of the TSNLP.
8. Conclusion
In this article, we presented the eXtensible MetaGrammar framework and argued that,
contrary to other existing grammar writing environments for tree-based grammar,
624
Crabbe? et al XMG: eXtensible MetaGrammar
XMG is declarative, extensible, and notationally expressive. We believe that these fea-
tures make XMG particularly appropriate for a fast prototyping of the kind of deep
tree-based grammars that are used in applications requiring high precision in gram-
mar modeling (e.g., language teaching, man/machine dialogue systems, data-to-text
generation).
The XMG language is documented on-line, and its compiler is open source soft-
ware, freely available under the terms of the GPL-compliant CeCILL license.34 Many
grammars designed with XMG (FB-LTAG and IG for French and English, TT-MCTAG
for German) are also open-source and available on-line.35
Future research will focus on extensibility. So far, XMG has been used to design tree-
based grammars for different languages. We plan to extend XMG to handle other types
of formalisms36 such as dependency grammars, and to support dimensions other than
syntax and semantics such as for instance, phonology or morphology. As mentioned
here, XMG offers a modular architecture, making it possible to extend it relatively easily.
Nonetheless, in its current state, such extensions imply modifying XMG?s code. We are
exploring new extensions of the formalism, which would allow the linguist to dynam-
ically define her/his metagrammar formalism (e.g., which principles or descriptions to
use) depending on the target formalism.
Another interesting question concerns cross-language grammar engineering. So far,
the metagrammar allows for dealing with structural redundancy. As pointed out by
Kinyon et al (2006), a metagrammar can be used to capture generalizations across
languages and is surely worth further investigating.
Finally, we plan to extend XMG with features borrowed from Integrated De-
velopment Environments (IDE) for programming languages. Designing a grammar
is, in some respect, similar to programming an application. Grammar environments
should benefit from the same tools as those used for the development of applications
(incremental compilation, debugger, etc.).
Acknowledgments
We are grateful to the three anonymous
reviewers for their valuable comments.
Any remaining errors are ours.
References
Abeille?, A. 2002. Une grammaire e?lectronique
du franc?ais. CNRS Editions.
Abeille?, A., M. Candito, and A. Kinyon. 1999.
Ftag: current status and parsing scheme.
In Proceedings of Vextal ?99, pages 283?292,
Venice.
A??t-Kaci, Hassan. 1991. Warren?s Abstract
Machine: A Tutorial Reconstruction. MIT
Press, Cambridge, MA.
Alahverdzhieva, Katya. 2008. XTAG using
XMG. Masters thesis, Nancy Universite?.
Baldridge, Jason, Sudipta Chatterjee,
Alexis Palmer, and Ben Wing. 2007.
DotCCG and VisCCG: Wiki and
programming paradigms for improved
grammar engineering with OpenCCG.
In Tracy Holloway King and Emily M.
Bender, editors, Proceedings of the Grammar
Engineering Across Framework Workshop
(GEAF 07). CSLI, Stanford, CA, pages 5?25.
Barrier, Se?bastien. 2006. Une me?tagrammaire
pour les noms pre?dicatifs du franc?ais :
de?veloppement et expe?rimentations pour les
grammaires TAG. Ph.D. thesis, Universite?
Paris 7.
Becker, Tilman. 1993. HyTAG: A New Type
of Tree Adjoining Grammars for Hybrid
Syntactic Representation of Free Word Order
Language. Ph.D. thesis, Universita?t des
Saarlandes.
34 See https://sourcesup.renater.fr/xmg.
35 The French TAG and French and English IG are available on XMG?s website, and the German TreeTuple
MC-TAG is available at http://www.sfs.uni-tuebingen.de/emmy/res.html.
36 Preliminary work on cross-framework grammar engineering has been realized by Cle?ment and Kinyon
(2003), who used Gaiffe et al?s compiler to produce both a TAG and a LFG from a given metagrammar.
625
Computational Linguistics Volume 39, Number 3
Blackburn, Patrick, Johan Bos, and Kristina
Striegnitz. 2006. Learn Prolog Now!,
volume 7 of Texts in Computing. College
Publications, London.
Bresnan, Joan and Annie Zaenen. 1990. Deep
unaccusitivity in LFG. In K. Dziwirek,
P. Farell, and E. Mejias-Bikandi, editors,
Grammatical Relations: A Cross-Theoretical
Perspective. CSLI publications, Stanford,
CA, pages 45?57.
Candito, Marie. 1996. A principle-based
hierarchical representation of LTAGs.
In Proceedings of the 16th International
Conference on Computational Linguistics
(COLING?96), pages 194?199, Copenhagen.
Candito, Marie. 1999. Repre?sentation modulaire
et parame?trable de grammaires e?lectroniques
lexicalise?es : application au franc?ais et a`
l?italien. Ph.D. thesis, Universite? Paris 7.
Cle?ment, Lionel and Alexandra Kinyon.
2003. Generating parallel multilingual
lfg-tag grammars from a metagrammar.
In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics,
pages 184?191, Sapporo.
Cohen-Sygal, Yael and Shuly Wintner. 2007.
The Non-Associativity of Polarized
Tree-Based Grammars. In Proceedings of the
Eighth International Conference on Intelligent
Text Processing and Computational Linguistics
(CICLing-2007), pages 208?217, Mexico City.
Cohen-Sygal, Yael and Shuly Wintner. 2009.
Associative grammar combination operators
for tree-based grammars. Journal of Logic,
Language and Information, 18(3):293?316.
Cohen-Sygal, Yael and Shuly Wintner. 2011.
Towards modular development of typed
unification grammars. Computational
Linguistics, 37(1):29?74.
Copestake, Ann and Dan Flickinger. 2000.
An open-source grammar development
environment and broad-coverage English
grammar using HPSG. In Proceedings of the
Second Conference on Language Resources and
Evaluation (LREC-2000), Athens.
Copestake, Ann, Alex Lascarides, and Dan
Flickinger. 2001. An algebra for semantic
construction in constraint-based
grammars. In Proceedings of 39th Annual
Meeting of the Association for Computational
Linguistics, pages 140?147, Toulouse.
Crabbe?, Benoit. 2005. Repre?sentation
informatique de grammaires fortement
lexicalise?es : Application a` la grammaire
d?arbres adjoints. Ph.D. thesis, Universite?
Nancy 2.
Crabbe?, Beno??t and Denys Duchier. 2004.
Metagrammar redux. In Proceedings of the
Workshop on Constraint Solving for Language
Processing (CSLP 2004), pages 32?47,
Copenhagen.
Duchier, Denys, Brunelle Magnana Ekoukou,
Yannick Parmentier, Simon Petitjean, and
Emmanuel Schang. 2012. Describing
morphologically-rich languages using
metagrammars: A look at verbs in Ikota.
In Workshop on ?Language Technology for
Normalisation of Less-resourced Languages,?
8th SALTMIL Workshop on Minority
Languages and 4th Workshop on African
Language Technology, International
Conference on Language Resources and
Evaluation, LREC 2012, pages 55?60,
Istanbul.
Duchier, Denys and Joachim Niehren. 2000.
Dominance constraints with set operators.
In John W. Lloyd, Vero?nica Dahl, Ulrich
Furbach, Manfred Kerber, Kung-Kiu Lau,
Catuscia Palamidessi, Lu??s Moniz Pereira,
Yehoshua Sagiv, and Peter J. Stuckey,
editors, Proceedings of the First International
Conference on Computational Logic,
volume 1861 of Lecture Notes in Computer
Science. Springer, Berlin, pages 326?341.
Duchier, Denys, Yannick Parmentier, and
Simon Petitjean. 2012. Metagrammars as
logic programs. In International Conference
on Logical Aspects of Computational
Linguistics (LACL 2012). Proceedings of
the Demo Session, pages 1?4, Nantes.
Duchier, Denys and Stefan Thater. 1999.
Parsing with tree descriptions: A
constraint-based approach. In Proceedings
of the Sixth International Workshop on
Natural Language Understanding and Logic
Programming (NLULP?99), pages 17?32,
Las Cruces, NM.
Evans, Roger, Gerald Gazdar, and David
Weir. 1995. Encoding lexicalized tree
adjoining grammars with a nonmonotonic
inheritance hierarchy. In Proceedings of the
33rd Annual Meeting of the Association for
Computational Linguistics, pages 77?84,
Cambridge, MA.
Flickinger, Daniel. 1987. Lexical Rules in the
Hierarchical Lexicon. Ph.D. thesis, Stanford
University.
Gaiffe, Bertrand, Beno??t Crabbe?, and Azim
Roussanaly. 2002. A new metagrammar
compiler. In Proceedings of the Sixth
International Workshop on Tree Adjoining
Grammars and Related Frameworks (TAG+6),
pages 101?108, Venice.
Gardent, Claire. 2007. Tree adjoining
grammar, semantic calculi and labelling
invariants. In Proceedings of the International
Workshop on Computational Semantics
(IWCS), Tilburg.
626
Crabbe? et al XMG: eXtensible MetaGrammar
Gardent, Claire. 2008. Integrating a
unification-based semantics in a large
scale lexicalised tree adjoininig grammar
for French. In Proceedings of the 22nd
International Conference on Computational
Linguistics (COLING?08), pages 249?256,
Manchester.
Gardent, Claire and Laura Kallmeyer. 2003.
Semantic construction in feature-based
tree adjoining grammar. In Proceedings of
the 10th Conference of the European Chapter of
the Association for Computational Linguistics,
pages 123?130, Budapest.
Gardent, Claire and Eric Kow. 2007. A
symbolic approach to near-deterministic
surface realisation using tree adjoining
grammar. In 45th Annual Meeting of the
Association for Computational Linguistics,
pages 328?335, Prague.
Gardent, Claire and Yannick Parmentier.
2005. Large scale semantic construction for
tree adjoining grammars. In Proceedings
of the Fifth International Conference
on Logical Aspects of Computational
Linguistics (LACL?05), pages 131?146,
Bordeaux.
Gardent, Claire and Yannick Parmentier.
2006. Coreference Handling in XMG.
In Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics (COLING/ACL
2006) Main Conference Poster Sessions,
pages 247?254, Sydney.
Gardent, Claire and Yannick Parmentier.
2007. SemTAG: A platform for specifying
tree adjoining grammars and performing
TAG-based semantic construction. In
Proceedings of the 45th Annual Meeting
of the Association for Computational
Linguistics Companion Volume Proceedings
of the Demo and Poster Sessions,
pages 13?16, Prague.
Iordanskaja, Lidija and Igor Mel?c?uk, 2009.
Establishing an inventory of surface?
syntactic relations: valence-controlled
surface-dependents of the verb in French.
In A. Polgue`re and I. A. Mel?duk, editors,
Dependency in Linguistic Description.
John Benjamins, Amsterdam,
pages 151?234.
Joshi, Aravind K., Leon S. Levy, and Masako
Takahashi. 1975. Tree adjunct grammars.
Journal of Computer and System Sciences,
10(1):136?163.
Kallmeyer, Laura. 1999. Tree Description
Grammars and Underspecified
Representations. Ph.D. thesis,
Universita?t Tu?bingen.
Kallmeyer, Laura. 2005. Tree-local
multicomponent tree-adjoining grammars
with shared nodes. Computational
Linguistics, 31(2):187?226.
Kallmeyer, Laura, Timm Lichte, Wolfgang
Maier, Yannick Parmentier, and Johannes
Dellert. 2008. Developing a TT-MCTAG for
German with an RCG-based parser. In
Proceedings of the Sixth Language Resources
and Evaluation Conference (LREC),
pages 782?789, Marrakech.
Kallmeyer, Laura, Wolfgang Maier, Yannick
Parmentier, and Johannes Dellert. 2010.
TuLiPA?Parsing extensions of TAG with
range concatenation grammars. Bulletin of
the Polish Academy of Sciences: Technical
Sciences, 58(3):377?392.
Kallmeyer, Laura and Maribel Romero.
2004a. LTAG semantics for questions.
In Proceedings of 7th International Workshop
on Tree-Adjoining Grammar and Related
Formalisms (TAG+7), pages 186?193,
Vancouver.
Kallmeyer, Laura and Maribel Romero.
2004b. LTAG semantics with semantic
unification. In Proceedings of 7th
International Workshop on Tree-Adjoining
Grammar and Related Formalisms (TAG+7),
page 155?162, Vancouver.
Kallmeyer, Laura and Maribel Romero.
2008. Scope and situation binding in
LTAG using semantic unification.
Research on Language and Computation,
6(1):3?52.
Kaplan, Ronald and Paula Newman. 1997.
Lexical resource reconciliation in the Xerox
linguistic environment. In Proceedings
of the ACL Workshop on Computational
Environments for Grammar Development
and Linguistic Engineering, pages 54?61,
Madrid.
Kinyon, Alexandra. 2000. Hypertags.
In Proceedings of the 18th International
Conference on Computational Linguistics
(COLING?00), pages 446?452, Saarbru?cken.
Kinyon, Alexandra, Owen Rambow, Tatjana
Scheffler, SinWon Yoon, and Aravind K.
Joshi. 2006. The metagrammar goes
multilingual: A cross-linguistic look at
the v2-phenomenon. In Proceedings of
the Eighth International Workshop on Tree
Adjoining Grammar and Related Formalisms,
pages 17?24, Sydney.
Le Hong, Phuong, Thi-Min-Huyen
N?Guyen, and Azim Roussanaly. 2008.
A metagrammar for Vietnamese. In
Proceedings of the 9th International Workshop
on Tree-Adjoining Grammar and Related
Formalisms (TAG+9), Tu?bingen.
627
Computational Linguistics Volume 39, Number 3
Lehmann, Sabine, Stephan Oepen, Sylvie
Regnier-Prost, Klaus Netter, Veronika Lux,
Judith Klein, Kirsten Falkedal, Frederik
Fouvry, Dominique Estival, Eva Dauphin,
Herve? Compagnion, Judith Baur, Lorna
Balkan, and Doug Arnold. 1996. TSNLP?
Test suites for natural language processing.
In Proceedings of the 16th International
Conference on Computational Linguistics
(COLING?96), pages 711?716, Copenhagen.
Lichte, Timm. 2007. An MCTAG with tuples
for coherent constructions in German.
In Proceedings of the 12th Conference on
Formal Grammar (FG 2007), 12 pages,
Dublin.
Muskens, Reinhard and Emiel Krahmer.
1998. Description theory, LTAGs and
Underspecified Semantics. In Fourth
International Workshop on Tree Adjoining
Grammars and Related Frameworks,
pages 112?115, Philadelphia, PA.
Parmentier, Yannick. 2007. SemTAG: une
plate-forme pour le calcul se?mantique a` partir
de Grammaires d?Arbres Adjoints. Ph.D.
thesis, Universite? Henri Poincare? - Nancy.
Parmentier, Yannick, Laura Kallmeyer, Timm
Lichte, and Wolfgang Maier. 2007. XMG:
eXtending MetaGrammars to MCTAG. In
Proceedings of the Workshop on High-Level
Syntactic Formalisms, 14th Conference on
Natural Language Processing (TALN?2007),
pages 473?482, Toulouse.
Pereira, Fernando and David Warren. 1980.
Definite clause grammars for language
analysis?A survey of the formalism
and a comparison to augmented
transition networks. Artificial
Intelligence, 13:231?278.
Perlmutter, David. 1970. Surface structure
constraints in syntax. Linguistic Inquiry,
1:187?255.
Perrier, Guy. 2000. Interaction grammars.
In Proceedings of the 18th International
Conference on Computational Linguistics
(COLING 2000), pages 600?606,
Saarbru?cken.
Perrier, Guy. 2007. A French interaction
grammar. In Proceedings of the 6th
Conference on Recent Advances in Natural
Language Processing (RANLP 2007),
pages 463?467, Borovets.
Prolo, Carlos A. 2002. Generating the
XTAG English grammar using metarules.
In Proceedings of the 19th International
Conference on Computational Linguistics
(COLING?2002), pages 814?820, Taipei.
Rambow, Owen, K. Vijay-Shanker, and
David Weir. 1995. D-tree grammars.
In Proceedings of the 33th Meeting of the
Association for Computational Linguistics,
pages 151?158, Cambridge, MA.
Rogers, James and K. Vijay-Shanker. 1994.
Obtaining trees from their descriptions:
An application to tree-adjoining
grammars. Computational Intelligence,
10:401?421.
Shieber, Stuart M. 1984. The design of a
computer language for linguistic
information. In Proceedings of the Tenth
International Conference on Computational
Linguistics, pages 362?366, Stanford, CA.
Van Roy, Peter. 1990. Extended DCG
notation: A tool for applicative
programming in prolog. Technical
Report UCB/CSD 90/583, University
of California, Berkeley.
Vijay-Shanker, K. and Aravind K. Joshi.
1988. Feature structures based tree
adjoining grammars. In Proceedings
of the 12th Conference on Computational
Linguistics (COLING?88), pages 714?719,
Budapest.
Vijay-Shanker, K. and Yves Schabes. 1992.
Structure sharing in lexicalized tree
adjoining grammars. In Proceedings
of the 14th International Conference on
Computational Linguistics (COLING?92),
pages 205?212, Nantes.
Villemonte de La Clergerie, E?ric. 2005.
DyALog: a tabular logic programming
based environment for NLP. In Proceedings
of 2nd International Workshop on Constraint
Solving and Language Processing (CSLP?05),
pages 18?33, Barcelona.
Villemonte de la Clergerie, E?ric. 2010.
Building factorized TAGs with
meta-grammars. In Proceedings of
the 10th International Workshop on
Tree-Adjoining Grammar and Related
Formalisms (TAG+10), pages 111?118,
New Haven, CT.
Weir, David J. 1988. Characterizing Mildly
Context-Sensitive Grammar Formalisms.
Ph.D. thesis, University of Pennsylvania.
Xia, Fei. 2001. Automatic Grammar Generation
from Two Different Perspectives. Ph.D. thesis,
University of Pennsylvania.
Xia, Fei, Martha Palmer, and K. Vijay-
Shanker. 1999. Toward semi-automating
grammar development. In Proceedings of
the 5th Natural Language Processing Pacific
Rim Symposium (NLPRS-99), pages 96?101,
Beijing.
Xia, Fei, Martha Palmer, and K. Vijay-
Shanker. 2005. Automatically generating
tree adjoining grammars from abstract
specifications. Journal of Computational
Intelligence, 21(3):246?287.
628
Crabbe? et al XMG: eXtensible MetaGrammar
Xia, Fei, Martha Palmer, and
K. Vijay-Shanker. 2010. Developing
tree-adjoining grammars with lexical
descriptions. In Srinivas Bangalore and
Aravind Joshi, editors, Supertagging:
Using Complex Lexical Descriptions in
Natural Language Processing. MIT Press,
Cambridge, MA, pages 73?110.
Xia, Fei, Martha Palmer, K. Vijay-Shanker,
and Joseph Rosenzweig. 1998. Consistent
grammar development using partial-tree
descriptions for LTAGs. In Proceedings
of the 4th International Workshop on
Tree Adjoining Grammar and Related
Formalisms (TAG+ 1998), pages 180?183,
Philadelphia, PA.
XTAG Research Group. 2001. A lexicalized
tree adjoining grammar for English.
Technical Report IRCS-01-03, IRCS,
University of Pennsylvania.
629

Proceedings of the ACL-HLT 2011 System Demonstrations, pages 86?91,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
MACAON
An NLP Tool Suite for Processing Word Lattices
Alexis Nasr Fre?de?ric Be?chet Jean-Franc?ois Rey Beno??t Favre Joseph Le Roux?
Laboratoire d?Informatique Fondamentale de Marseille- CNRS - UMR 6166
Universite? Aix-Marseille
(alexis.nasr,frederic.bechet,jean-francois.rey,benoit.favre,joseph.le.roux)
@lif.univ-mrs.fr
Abstract
MACAON is a tool suite for standard NLP tasks
developed for French. MACAON has been de-
signed to process both human-produced text
and highly ambiguous word-lattices produced
by NLP tools. MACAON is made of several na-
tive modules for common tasks such as a tok-
enization, a part-of-speech tagging or syntac-
tic parsing, all communicating with each other
through XML files . In addition, exchange pro-
tocols with external tools are easily definable.
MACAON is a fast, modular and open tool, dis-
tributed under GNU Public License.
1 Introduction
The automatic processing of textual data generated
by NLP software, resulting from Machine Transla-
tion, Automatic Speech Recognition or Automatic
Text Summarization, raises new challenges for lan-
guage processing tools. Unlike native texts (texts
produced by humans), this new kind of texts is the
result of imperfect processors and they are made
of several hypotheses, usually weighted with con-
fidence measures. Automatic text production sys-
tems can produce these weighted hypotheses as n-
best lists, word lattices, or confusion networks. It is
crucial for this space of ambiguous solutions to be
kept for later processing since the ambiguities of the
lower levels can sometimes be resolved during high-
level processing stages. It is therefore important to
be able to represent this ambiguity.
?This work has been funded by the French Agence Nationale
pour la Recherche, through the projects SEQUOIA (ANR-08-
EMER-013) and DECODA (2009-CORD-005-01)
MACAON is a suite of tools developped to pro-
cess ambiguous input and extend inference of in-
put modules within a global scope. It con-
sists in several modules that perform classical
NLP tasks (tokenization, word recognition, part-of-
speech tagging, lemmatization, morphological anal-
ysis, partial or full parsing) on either native text
or word lattices. MACAON is distributed under
GNU public licence and can be downloaded from
http://www.macaon.lif.univ-mrs.fr/.
From a general point of view, a MACAON module
can be seen as an annotation device1 which adds a
new level of annotation to its input that generally de-
pends on annotations from preceding modules. The
modules communicate through XML files that allow
the representation different layers of annotation as
well as ambiguities at each layer. Moreover, the ini-
tial XML structuring of the processed files (logical
structuring of a document, information from the Au-
tomatic Speech Recognition module . . . ) remains
untouched by the processing stages.
As already mentioned, one of the main charac-
teristics of MACAON is the ability for each module
to accept ambiguous inputs and produce ambiguous
outputs, in such a way that ambiguities can be re-
solved at a later stage of processing. The compact
representation of ambiguous structures is at the heart
of the MACAON exchange format, described in sec-
tion 2. Furthermore every module can weight the
solutions it produces. such weights can be used to
rank solutions or limit their number for later stages
1Annotation must be taken here in a general sense which in-
cludes tagging, segmentation or the construction of more com-
plex objets as syntagmatic or dependencies trees.
86
of processing.
Several processing tools suites alread exist for
French among which SXPIPE (Sagot and Boullier,
2008), OUTILEX (Blanc et al, 2006), NOOJ2 or UNI-
TEX3. A general comparison of MACAON with these
tools is beyond the scope of this paper. Let us just
mention that MACAON shares with most of them the
use of finite state machines as core data represen-
tation. Some modules are implemented as standard
operations on finite state machines.
MACAON can also be compared to the numerous
development frameworks for developping process-
ing tools, such as GATE4, FREELING5, ELLOGON6
or LINGPIPE7 that are usually limited to the process-
ing of native texts.
The MACAON exchange format shares a cer-
tain number of features with linguistic annotation
scheme standards such as the Text Encoding Initia-
tive8, XCES9, or EAGLES10. They all aim at defining
standards for various types of corpus annotations.
The main difference between MACAON and these
approaches is that MACAON defines an exchange for-
mat between NLP modules and not an annotation
format. More precisely, this format is dedicated to
the compact representation of ambiguity: some in-
formation represented in the exchange format are
to be interpreted by MACAON modules and would
not be part of an annotation format. Moreover,
the MACAON exchange format was defined from the
bottom up, originating from the authors? need to use
several existing tools and adapt their input/output
formats in order for them to be compatible. This is in
contrast with a top down approach which is usually
chosen when specifying a standard. Still, MACAON
shares several characteristics with the LAF (Ide and
Romary, 2004) which aims at defining high level
standards for exchanging linguistic data.
2www.nooj4nlp.net/pages/nooj.html
3www-igm.univ-mlv.fr/?unitex
4gate.ac.uk
5garraf.epsevg.upc.es/freeling
6www.ellogon.org
7alias-i.com/lingpipe
8www.tei-c.org/P5
9www.xml-ces.org
10www.ilc.cnr.it/eagles/home.html
2 The MACAON exchange format
The MACAON exchange format is based on four con-
cepts: segment, attribute, annotation level and seg-
mentation.
A segment refers to a segment of the text or
speech signal that is to be processed, as a sentence,
a clause, a syntactic constituent, a lexical unit, a
named entity . . . A segment can be equipped with at-
tributes that describe some of its aspects. A syntac-
tic constituent, for example, will define the attribute
type which specifies its syntactic type (Noun Phrase,
Verb Phrase . . . ). A segment is made of one or more
smaller segments.
A sequence of segments covering a whole sen-
tence for written text, or a spoken utterance for oral
data, is called a segmentation. Such a sequence can
be weighted.
An annotation level groups together segments of
a same type, as well as segmentations defined on
these segments. Four levels are currently defined:
pre-lexical, lexical, morpho-syntactic and syntactic.
Two relations are defined on segments: the prece-
dence relation that organises linearly segments of a
given level into segmentations and the dominance
relation that describes how a segment is decomposed
in smaller segments either of the same level or of a
lower level.
We have represented in figure 2, a schematic rep-
resentation of the analysis of the reconstructed out-
put a speech recognizer would produce on the in-
put time flies like an arrow11. Three annotation lev-
els have been represented, lexical, morpho-syntactic
and syntactic. Each level is represented by a finite-
state automaton which models the precedence rela-
tion defined over the segments of this level. Seg-
ment time, for example, precedes segment flies. The
segments are implicitly represented by the labels of
the automaton?s arcs. This label should be seen as
a reference to a more complex objet, the actual seg-
ment. The dominance relations are represented with
dashed lines that link segments of different levels.
Segment time, for example, is dominated by seg-
ment NN of the morpho-syntactic level.
This example illustrates the different ambiguity
cases and the way they are represented.
11For readability reasons, we have used an English example,
MACAON, as mentioned above, currently exists for French.
87
thyme
time
flies like
liken
an arrow
a row
JJ IN
VB
DT NN
DT NN
VB
NN
NN
VBZ
VB VB
VP
VP
NP
NP
VP
NP
VP
VP
PP
NP
NP
Figure 1: Three annotation levels for a sample sentence.
Plain lines represent annotation hypotheses within a level
while dashed lines represent links between levels. Trian-
gles with the tip up are ?and? nodes and triangles with
the tip down are ?or? nodes. For instance, in the part-of-
speech layer, The first NN can either refer to ?time? or
?thyme?. In the chunking layer, segments that span mul-
tiple part-of-speech tags are linked to them through ?and?
nodes.
The most immediate ambiguity phenomenon is
the segmentation ambiguity: several segmentations
are possible at every level. This ambiguity is rep-
resented in a compact way through the factoring of
segments that participate in different segmentations,
by way of a finite state automaton.
The second ambiguity phenomenon is the dom-
inance ambiguity, where a segment can be decom-
posed in several ways into lower level segments.
Such a case appears in the preceding example, where
the NN segment appearing in one of the outgoing
transition of the initial state of the morpho-syntactic
level dominates both thyme and time segments of the
lexical level. The triangle with the tip down is an
?or? node, modeling the fact that NN corresponds to
time or thyme.
Triangles with the tip up are ?and? nodes. They
model the fact that the PP segment of the syntac-
tic level dominates segments IN, DT and NN of the
morpho-syntactic level.
2.1 XML representation
The MACAON exchange format is implemented in
XML. A segment is represented with the XML tag
<segment> which has four mandatory attributes:
? type indicates the type of the segment, four dif-
ferent types are currently defined: atome (pre-
lexical unit usually referred to as token in en-
glish), ulex (lexical unit), cat (part of speech)
and chunk (a non recursive syntactic unit).
? id associates to a segment a unique identifier in
the document, in order to be able to reference
it.
? start and end define the span of the segment.
These two attributes are numerical and repre-
sent either the index of the first and last char-
acter of the segment in the text string or the
beginning and ending time of the segment in
a speech signal.
A segment can define other attributes that can be
useful for a given description level. We often find
the stype attribute that defines subtypes of a given
type.
The dominance relation is represented through the
use of the <sequence> tag. The domination of the
three segments IN, DT and NN by a PP segment,
mentionned above is represented below, where p1,
p2 and p3 are respectively the ids of segments IN,
DT and NN.
<segment type="chunk" stype="PP" id="c1">
<sequence>
<elt segref="p1"/>
<elt segref="p2"/>
<elt segref="p3"/>
</sequence>
</segment>
The ambiguous case, described above where seg-
ment NN dominates segments time or thyme is rep-
resented below as a disjunction of sequences inside
a segment. The disjunction itself is not represented
as an XML tag. l1 and l2 are respectively the ids
of segments time and thyme.
<segment type="cat" stype="NN" id="c1">
<sequence>
<elt segref="l1" w="-3.37"/>
</sequence>
<sequence>
<elt segref="l2" w="-4.53"/>
</sequence>
</segment>
88
The dominance relation can be weighted, by way
of the attribute w. Such a weight represents in the
preceding example the conditional log-probability
of a lexical unit given a part of speech, as in a hidden
Markov model.
The precedence relation (i.e. the organization
of segments in segmentations), is represented as a
weighted finite state automaton. Automata are rep-
resented as a start state, accept states and a list of
transitions between states, as in the following exam-
ple that corresponds to the lexical level of our exam-
ple.
<fsm n="9">
<start n="0"/>
<accept n="6"/>
<ltrans>
<trans o="0" d="1" i="l1" w="-7.23"/>
<trans o="0" d="1" i="l2" w="-9.00"/>
<trans o="1" d="2" i="l3" w="-3.78"/>
<trans o="2" d="3" i="l4" w="-7.37"/>
<trans o="3" d="4" i="l5" w="-3.73"/>
<trans o="2" d="4" i="l6" w="-6.67"/>
<trans o="4" d="5" i="l7" w="-4.56"/>
<trans o="5" d="6" i="l8" w="-2.63"/>
<trans o="4" d="6" i="l9" w="-7.63"/>
</ltrans>
</fsm>
The <trans/> tag represents a transition, its
o,d,i and w features are respectively the origin, and
destination states, its label (the id of a segment) and
a weight.
An annotation level is represented by the
<section> tag which regroups two tags, the
<segments> tag that contains the different segment
tags defined at this annotation level and the <fsm>
tag that represents all the segmentations of this level.
3 The MACAON architecture
Three aspects have guided the architecture of
MACAON: openness, modularity, and speed. Open-
ness has been achieved by the definition of an ex-
change format which has been made as general as
possible, in such a way that mapping can be de-
fined from and to third party modules as ASR, MT
systems or parsers. Modularity has been achieved
by the definition of independent modules that com-
municate with each other through XML files using
standard UNIX pipes. A module can therefore be re-
placed easily. Speed has been obtained using effi-
cient algorithms and a representation especially de-
signed to load linguistic data and models in a fast
way.
MACAON is composed of libraries and compo-
nents. Libraries contain either linguistic data, mod-
els or API functions. Two kinds of components are
presented, the MACAON core components and third
party components for which mappings to and from
the MACAON exchange format have been defined.
3.1 Libraries
The main MACAON library is macaon common.
It defines a simple interface to the MACAON ex-
change format and functions to load XML MACAON
files into memory using efficient data structures.
Other libraries macaon lex, macaon code and
macaon tagger lib represent the lexicon, the
morphological data base and the tagger models in
memory.
MACAON only relies on two third-party libraries,
which are gfsm12, a finite state machine library and
libxml, an XML library13.
3.2 The MACAON core components
A brief description of several standard components
developed in the MACAON framework is given be-
low. They all comply with the exchange format de-
scribed above and add a <macaon stamp> to the
XML file that indicates the name of the component,
the date and the component version number, and rec-
ognizes a set of standard options.
maca select is a pre-processing component: it adds
a macaon tag under the target tags specified by
the user to the input XML file. The follow-
ing components will only process the document
parts enclosed in macaon tags.
maca segmenter segments a text into sentences by
examining the context of punctuation with a
regular grammar given as a finite state automa-
ton. It is disabled for automatic speech tran-
scriptions which do not typically include punc-
tuation signs and come with their own segmen-
tation.
12ling.uni-potsdam.de/?moocow/projects/
gfsm/
13xmlsoft.org
89
maca tokenizer tokenizes a sentence into pre-
lexical units. It is also based on regular gram-
mars that recognize simple tokens as well as a
predefined set of special tokens, such as time
expressions, numerical expressions, urls. . . .
maca lexer allows to regroup pre-lexical units into
lexical units. It is based on the lefff French lex-
icon (Sagot et al, 2006) which contains around
500,000 forms. It implements a dynamic pro-
gramming algorithm that builds all the possible
grouping of pre-lexical units into lexical units.
maca tagger associates to every lexical unit one or
more part-of-speech labels. It is based on a
trigram Hidden Markov Model trained on the
French Treebank (Abeille? et al, 2003). The es-
timation of the HMM parameters has been re-
alized by the SRILM toolkit (Stolcke, 2002).
maca anamorph produces the morphological anal-
ysis of lexical units associated to a part of
speech. The morphological information come
from the lefff lexicon.
maca chunker gathers sequences of part-of-speech
tags in non recursive syntactic units. This com-
ponent implements a cascade of finite state
transducers, as proposed by Abney (1996). It
adds some features to the initial Abney pro-
posal, like the possibility to define the head of
a chunk.
maca conv is a set of converters from and to the
MACAON exchange format. htk2macaon
and fsm2macaon convert word lattices from
the HTK format (Young, 1994) and ATT
FSM format (Mohri et al, 2000) to the
MACAON exchange format. macaon2txt and
txt2macaon convert from and to plain text
files. macaon2lorg and lorg2macaon
convert to and from the format of the LORG
parser (see section 3.3).
maca view is a graphical interface that allows to in-
spect MACAON XML files and run the compo-
nents.
3.3 Third party components
MACAON is an open architecture and provides a rich
exchange format which makes possible the repre-
sentation of many NLP tools input and output in the
MACAON format. MACAON has been interfaced with
the SPEERAL Automatic Speech Recognition Sys-
tem (Nocera et al, 2006). The word lattices pro-
duced by SPEERAL can be converted to pre-lexical
MACAON automata.
MACAON does not provide any native module for
parsing yet but it can be interfaced with any already
existing parser. For the purpose of this demonstra-
tion we have chosen the LORG parser developed at
NCLT, Dublin14. This parser is based on PCFGs
with latent annotations (Petrov et al, 2006), a for-
malism that showed state-of-the-art parsing accu-
racy for a wide range of languages. In addition it of-
fers a sophisticated handling of unknown words re-
lying on automatically learned morphological clues,
especially for French (Attia et al, 2010). Moreover,
this parser accepts input that can be tokenized, pos-
tagged or pre-bracketed. This possibility allows for
different settings when interfacing it with MACAON.
4 Applications
MACAON has been used in several projects, two of
which are briefly described here, the DEFINIENS
project and the LUNA project.
DEFINIENS (Barque et al, 2010) is a project that
aims at structuring the definitions of a large coverage
French lexicon, the Tre?sor de la langue franc?aise.
The lexicographic definitions have been processed
by MACAON in order to decompose the definitions
into complex semantico-syntactic units. The data
processed is therefore native text that possesses a
rich XML structure that has to be preserved during
processing.
LUNA15 is a European project that aims at extract-
ing information from oral data about hotel booking.
The word lattices produced by an ASR system have
been processed by MACAON up to a partial syntactic
level from which frames are built. More details can
be found in (Be?chet and Nasr, 2009). The key aspect
of the use of MACAON for the LUNA project is the
ability to perform the linguistic analyses on the mul-
tiple hypotheses produced by the ASR system. It is
therefore possible, for a given syntactic analysis, to
14www.computing.dcu.ie/?lorg. This software
should be freely available for academic research by the time
of the conference.
15www.ist-luna.eu
90
Figure 2: Screenshot of the MACAON visualization inter-
face (for French models). It allows to input a text and see
the n-best results of the annotation.
find all the word sequences that are compatible with
this analysis.
Figure 2 shows the interface that can be used to
see the output of the pipeline.
5 Conclusion
In this paper we have presented MACAON, an NLP
tool suite which allows to process native text as well
as several hypotheses automatically produced by an
ASR or an MT system. Several evolutions are cur-
rently under development, such as a named entity
recognizer component and an interface with a de-
pendency parser.
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.
2003. Building a treebank for french. In Anne
Abeille?, editor, Treebanks. Kluwer, Dordrecht.
Steven Abney. 1996. Partial parsing via finite-state cas-
cades. In Workshop on Robust Parsing, 8th European
Summer School in Logic, Language and Information,
Prague, Czech Republic, pages 8?15.
M. Attia, J. Foster, D. Hogan, J. Le Roux, L. Tounsi, and
J. van Genabith. 2010. Handling Unknown Words in
Statistical Latent-Variable Parsing Models for Arabic,
English and French. In Proceedings of SPMRL.
Lucie Barque, Alexis Nasr, and Alain Polgue`re. 2010.
From the definitions of the tre?sor de la langue franc?aise
to a semantic database of the french language. In EU-
RALEX 2010, Leeuwarden, Pays Bas.
Fre?de?ric Be?chet and Alexis Nasr. 2009. Robust depen-
dency parsing for spoken language understanding of
spontaneous speech. In Interspeech, Brighton, United
Kingdom.
Olivier Blanc, Matthieu Constant, and Eric Laporte.
2006. Outilex, plate-forme logicielle de traitement de
textes e?crits. In TALN 2006, Leuven.
Nancy Ide and Laurent Romary. 2004. International
standard for a linguistic annotation framework. Nat-
ural language engineering, 10(3-4):211?225.
M. Mohri, F. Pereira, and M. Riley. 2000. The design
principles of a weighted finite-state transducer library.
Theoretical Computer Science, 231(1):17?32.
P. Nocera, G. Linares, D. Massonie?, and L. Lefort. 2006.
Phoneme lattice based A* search algorithm for speech
recognition. In Text, Speech and Dialogue, pages 83?
111. Springer.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In ACL.
Beno??t Sagot and Pierre Boullier. 2008. Sxpipe 2:
architecture pour le traitement pre?syntaxique de cor-
pus bruts. Traitement Automatique des Langues,
49(2):155?188.
Beno??t Sagot, Lionel Cle?ment, Eric Villemonte de la
Clergerie, and Pierre Boullier. 2006. The lefff 2 Syn-
tactic Lexicon for French: Architecture, Acquisition,
Use. In International Conference on Language Re-
sources and Evaluation, Genoa.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In International Conference on Spo-
ken Language Processing, Denver, Colorado.
S.J. Young. 1994. The HTK Hidden Markov Model
Toolkit: Design and Philosophy. Entropic Cambridge
Research Laboratory, Ltd, 2:2?44.
91
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777?785,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Semi-supervised Dependency Parsing using Lexical Affinities
Seyed Abolghasem Mirroshandel?,? Alexis Nasr? Joseph Le Roux
?Laboratoire d?Informatique Fondamentale de Marseille- CNRS - UMR 7279
Universite? Aix-Marseille, Marseille, France
LIPN, Universite? Paris Nord & CNRS,Villetaneuse, France
?Computer Engineering Department, Sharif university of Technology, Tehran, Iran
(ghasem.mirroshandel@lif.univ-mrs.fr, alexis.nasr@lif.univ-mrs.fr,
leroux@univ-paris13.fr)
Abstract
Treebanks are not large enough to reliably
model precise lexical phenomena. This de-
ficiency provokes attachment errors in the
parsers trained on such data. We propose
in this paper to compute lexical affinities,
on large corpora, for specific lexico-syntactic
configurations that are hard to disambiguate
and introduce the new information in a parser.
Experiments on the French Treebank showed
a relative decrease of the error rate of 7.1% La-
beled Accuracy Score yielding the best pars-
ing results on this treebank.
1 Introduction
Probabilistic parsers are usually trained on treebanks
composed of few thousands sentences. While this
amount of data seems reasonable for learning syn-
tactic phenomena and, to some extent, very frequent
lexical phenomena involving closed parts of speech
(POS), it proves inadequate when modeling lexical
dependencies between open POS, such as nouns,
verbs and adjectives. This fact was first recognized
by (Bikel, 2004) who showed that bilexical depen-
dencies were barely used in Michael Collins? parser.
The work reported in this paper aims at a better
modeling of such phenomena by using a raw corpus
that is several orders of magnitude larger than the
treebank used for training the parser. The raw cor-
pus is first parsed and the computed lexical affinities
between lemmas, in specific lexico-syntactic config-
urations, are then injected back in the parser. Two
outcomes are expected from this procedure, the first
is, as mentioned above, a better modeling of bilexi-
cal dependencies and the second is a method to adapt
a parser to new domains.
The paper is organized as follows. Section 2 re-
views some work on the same topic and highlights
their differences with ours. In section 3, we describe
the parser that we use in our experiments and give
a detailed description of the frequent attachment er-
rors. Section 4 describes how lexical affinities be-
tween lemmas are calculated and their impact is then
evaluated with respect to the attachment errors made
by the parser. Section 5 describes three ways to in-
tegrate the lexical affinities in the parser and reports
the results obtained with the three methods.
2 Previous Work
Coping with lexical sparsity of treebanks using raw
corpora has been an active direction of research for
many years.
One simple and effective way to tackle this prob-
lem is to put together words that share, in a large
raw corpus, similar linear contexts, into word clus-
ters. The word occurrences of the training treebank
are then replaced by their cluster identifier and a new
parser is trained on the transformed treebank. Us-
ing such techniques (Koo et al, 2008) report signi-
ficative improvement on the Penn Treebank (Marcus
et al, 1993) and so do (Candito and Seddah, 2010;
Candito and Crabbe?, 2009) on the French Treebank
(Abeille? et al, 2003).
Another series of papers (Volk, 2001; Nakov
and Hearst, 2005; Pitler et al, 2010; Zhou et al,
2011) directly model word co-occurrences. Co-
occurrences of pairs of words are first collected in a
777
raw corpus or internet n-grams. Based on the counts
produced, lexical affinity scores are computed. The
detection of pairs of words co-occurrences is gen-
erally very simple, it is either based on the direct
adjacency of the words in the string or their co-
occurrence in a window of a few words. (Bansal
and Klein, 2011; Nakov and Hearst, 2005) rely on
the same sort of techniques but use more sophisti-
cated patterns, based on simple paraphrase rules, for
identifying co-occurrences.
Our work departs from those approaches by the
fact that we do not extract the lexical information
directly on a raw corpus, but we first parse it and
then extract the co-occurrences on the parse trees,
based on some predetermined lexico-syntactic pat-
terns. The first reason for this choice is that the lin-
guistic phenomena that we are interested in, such as
as PP attachment, coordination, verb subject and ob-
ject can range over long distances, beyond what is
generally taken into account when working on lim-
ited windows. The second reason for this choice was
to show that the performances that the NLP commu-
nity has reached on parsing, combined with the use
of confidence measures allow to use parsers to ex-
tract accurate lexico-syntactic information, beyond
what can be found in limited annotated corpora.
Our work can also be compared with self train-
ing approaches to parsing (McClosky et al, 2006;
Suzuki et al, 2009; Steedman et al, 2003; Sagae
and Tsujii, 2007) where a parser is first trained on
a treebank and then used to parse a large raw cor-
pus. The parses produced are then added to the ini-
tial treebank and a new parser is trained. The main
difference between these approaches and ours is that
we do not directly add the output of the parser to the
training corpus, but extract precise lexical informa-
tion that is then re-injected in the parser. In the self
training approach, (Chen et al, 2009) is quite close
to our work: instead of adding new parses to the tree-
bank, the occurrence of simple interesting subtrees
are detected in the parses and introduced as new fea-
tures in the parser.
The way we introduce lexical affinity measures in
the parser, in 5.1, shares some ideas with (Anguiano
and Candito, 2011), who modify some attachments
in the parser output, based on lexical information.
The main difference is that we only take attachments
that appear in an n-best parse list into account, while
they consider the first best parse and compute all po-
tential alternative attachments, that may not actually
occur in the n-best forests.
3 The Parser
The parser used in this work is the second order
graph based parser (McDonald et al, 2005; Ku?bler
et al, 2009) implementation of (Bohnet, 2010). The
parser was trained on the French Treebank (Abeille?
et al, 2003) which was transformed into dependency
trees by (Candito et al, 2009). The size of the tree-
bank and its decomposition into train, development
and test sets is represented in table 1.
nb of sentences nb of words
FTB TRAIN 9 881 278 083
FTB DEV 1 239 36 508
FTB TEST 1 235 36 340
Table 1: Size and decomposition of the French Treebank
The part of speech tagging was performed with
the MELT tagger (Denis and Sagot, 2010) and lem-
matized with the MACAON tool suite (Nasr et al,
2011). The parser gave state of the art results for
parsing of French, reported in table 2.
pred. POS tags gold POS tags
punct no punct punct no punct
LAS 88.02 90.24 88.88 91.12
UAS 90.02 92.50 90.71 93.20
Table 2: Labeled and unlabeled accuracy score for auto-
matically predicted and gold POS tags with and without
taking into account punctuation on FTB TEST.
Figure 1 shows the distribution of the 100 most
common error types made by the parser. In this
figure, x axis shows the error types and y axis
shows the error ratio of the related error type
( number of errors of the specific typetotal number of errors ). We define an error
type by the POS tag of the governor and the POS
tag of the dependent. The figure presents a typical
Zipfian distribution with a low number of frequent
error types and a large number of unfrequent error
types. The shape of the curve shows that concen-
trating on some specific frequent errors in order to
increase the parser accuracy is a good strategy.
778
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 10  20  30  40  50  60  70  80  90  100
er
ro
r 
ra
ti
o
Error Type
Figure 1: Distribution of the types of errors
Table 3 gives a finer description of the most com-
mon types of error made by the parser. Here we
define more precise patterns for errors, where some
lexical values are specified (for prepositions) and, in
some cases, the nature of the dependency is taken
into account. Every line of the table corresponds to
one type of error. The first column describes the
error type. The second column indicates the fre-
quency of this type of dependency in the corpus. The
third one displays the accuracy for this type of de-
pendency (the number of dependencies of this type
correctly analyzed by the parser divided by the to-
tal number of dependencies of this type). The fourth
column shows the contribution of the errors made on
this type of dependency to the global error rate. The
last column associates a name with some of the error
types that will prove useful in the remainder of the
paper to refer to the error type.
Table 3 shows two different kinds of errors that
impact the global error rate. The first one concerns
very common dependencies that have a high accu-
racy but, due to their frequency, hurt the global er-
ror rate of the parser. The second one concerns low
frequency, low accuracy dependency types. Lines 2
and 3, respectively attachment of the preposition a` to
a verb and the subject dependency illustrate such a
contrast. They both impact the total error rate in the
same way (2.53% of the errors). But the first one
is a low frequency low accuracy type (respectively
0.88% and 69.11%) while the second is a high fre-
quency high accuracy type (respectively 3.43% and
93.03%). We will see in 4.2.2 that our method be-
haves quite differently on these two types of error.
dependency freq. acc. contrib. name
N?N 1.50 72.23 2.91
V? a` 0.88 69.11 2.53 VaN
V?suj? N 3.43 93.03 2.53 SBJ
N? CC 0.77 69.78 2.05 NcN
N? de 3.70 92.07 2.05 NdeN
V? de 0.66 74.68 1.62 VdeN
V?obj? N 2.74 90.43 1.60 OBJ
V? en 0.66 81.20 1.24
V? pour 0.46 67.78 1.10
N? ADJ 6.18 96.60 0.96 ADJ
N? a` 0.29 70.64 0.72 NaN
N? pour 0.12 38.64 0.67
N? en 0.15 47.69 0.57
Table 3: The 13 most common error types
4 Creating the Lexical Resource
The lexical resource is a collection of tuples
?C, g, d, s? where C is a lexico-syntactic configu-
ration, g is a lemma, called the governor of the
configuration, d is another lemma called the depen-
dent and s is a numerical value between 0 and 1,
called the lexical affinity score, which accounts for
the strength of the association between g and d in
the context C. For example the tuple ?(V, g)
obj
?
(N, d), eat , oyster , 0.23? defines a simple configu-
ration (V, g)
obj
? (N, d) that is an object depen-
dency between verb g and noun d. When replac-
ing variables g and d in C respectively with eat
and oyster , we obtain the fully specified lexico syn-
tactic pattern(V, eat)
obj
? (N, oyster), that we call
an instantiated configuration. The numerical value
0.23 accounts for how much eat and oyster like
to co-occur in the verb-object configuration. Con-
figurations can be of arbitrary complexity but they
have to be generic enough in order to occur fre-
quently in a corpus yet be specific enough to model
a precise lexico syntactic phenomenon. The context
(?, g)
?
? (?, d), for example is very generic but does
not model a precise linguistic phenomenon, as selec-
tional preferences of a verb, for example. Moreover,
configurations need to be error-prone. In the per-
spective of increasing a parser performances, there
is no point in computing lexical affinity scores be-
tween words that appear in a configuration for which
779
the parser never makes mistakes.
The creation of the lexical resource is a three stage
process. The first step is the definition of configura-
tions, the second one is the collection of raw counts
from the machine parsed corpora and the third one is
the computation of lexical affinities based on the raw
counts. The three steps are described in the follow-
ing subsection while the evaluation of the created
resource is reported in subsection 4.2.
4.1 Computing Lexical Affinities
A set of 9 configurations have been defined. Their
selection is a manual process based on the analysis
of the errors made by the parser, described in sec-
tion 3, as well as on the linguistic phenomena they
model. The list of the 9 configurations is described
in Table 4. As one can see on this table, configu-
rations are usually simple, made up of one or two
dependencies. Linguistically, configurations OBJ
and SBJ concern subject and object attachments,
configuration ADJ is related to attachments of ad-
jectives to nouns and configurations NdeN, VdeN,
VaN, and NaN indicate prepositional attachments.
We have restricted ourselves here to two common
French prepositions a` and de. Configurations NcN
and VcV deal respectively with noun and verb coor-
dination.
Name Description
OBJ (V, g)
obj
? (N, d)
SBJ (V, g)
subj
? (N, d)
ADJ (N, g) ? ADJ
NdeN (N, g) ? (P, de)? (N, d)
VdeN (V, g) ? (P, de)? (N, d)
NaN (N, g) ? (P, a`)? (N, d)
VaN (V, g) ? (P, a`)? (N, d)
NcN (N, g) ? (CC, ?)? (N, d)
VcV (V, g) ? (CC, ?)? (V, d)
Table 4: List of the 9 configurations.
The computation of the number of occurrences of
an instantiated configuration in the corpus is quite
straightforward, it consists in traversing the depen-
dency trees produced by the parser and detect the
occurrences of this configuration.
At the end of the counts collection, we have gath-
CORPUS Sent. nb. Tokens nb.
AFP 1 024 797 31 486 618
EST REP 1 103 630 19 635 985
WIKI 1 592 035 33 821 460
TOTAL 3 720 462 84 944 063
Table 5: sizes of the corpora used to gather lexical counts
ered for every lemma l its number of occurrences as
governor (resp. dependent) of configurationC in the
corpus, noted C(C, l, ?) (resp. C(C, ?, l)), as well as
the number of occurrences of configuration C with
lemma lg as a governor and lemma ld as a depen-
dent, noted C(C, lg, ld). We are now in a position
to compute the score s(C, lg, ld). This score should
reflect the tendency of lg and ld to appear together
in configuration C. It should be maximal if when-
ever lg occurs as the governor of configuration C,
the dependent position is occupied by ld and, sym-
metrically, if whenever ld occurs as the dependent of
configuration C, the governor position is occupied
by lg. A function that conforms such a behavior is
the following:
s(C, lg, ld) =
1
2
(
C(C, lg, ld)
C(C, lg, ?)
+
C(C, lg, ld)
C(C, ?, ld)
)
it takes its values between 0 (lg and ld never
co-occur) and 1 (g and d always co-occur). This
function is close to pointwise mutual information
(Church and Hanks, 1990) but takes its values be-
tween 0 and 1.
4.2 Evaluation
Lexical affinities were computed on three corpora of
slightly different genres. The first one, is a collection
of news report of the French press agency Agence
France Presse, the second is a collection of news-
paper articles from a local French newspaper : l?Est
Re?publicain. The third one is a collection of articles
from the French Wikipedia. The size of the different
corpora are detailed in table 5. The corpus was first
POS tagged, lemmatized and parsed in order to get
the 50 best parses for every sentence. Then the lexi-
cal resource was built, based on the 9 configurations
described in table 4.
The lexical resource has been evaluated on
FTB DEV with respect to two measures: coverage
780
and correction rate, described in the next two sec-
tions.
4.2.1 Coverage
Coverage measures the instantiated configura-
tions present in the evaluation corpus that are in the
resource. The results are presented in table 6. Every
line represents a configuration, the second column
indicates the number of different instantiations of
this configuration in the evaluation corpus, the third
one indicates the number of instantiated configura-
tions that were actually found in the lexical resource
and the fourth column shows the coverage for this
configuration, which is the ratio third column over
the second. Last column represents the coverage of
the training corpus (the lexical resource is extracted
on the training corpus) and the last line represents
the same quantities computed on all configurations.
Table 6 shows two interesting results: firstly the
high variability of coverage with respect to configu-
rations, and secondly the low coverage when the lex-
ical resource is computed on the training corpus, this
fact being consistent with the conclusions of (Bikel,
2004). A parser trained on a treebank cannot be ex-
pected to reliably select the correct governor in lex-
ically sensitive cases.
Conf. occ. pres. cov. T cov.
OBJ 1017 709 0.70 0.21
SBJ 1210 825 0.68 0.24
ADJ 1791 1239 0.69 0.33
NdeN 1909 1287 0.67 0.31
VdeN 189 107 0.57 0.16
NaN 123 61 0.50 0.20
VaN 422 273 0.65 0.23
NcN 220 55 0.25 0.10
VcV 165 93 0.56 0.04
? 7046 4649 0.66 0.27
Table 6: Coverage of the lexical resource over FTB DEV.
4.2.2 Correction Rate
While coverage measures how many instantiated
configurations that occur in the treebank are actu-
ally present in the lexical resource, it does not mea-
sure if the information present in the lexical resource
can actually help correcting the errors made by the
parser.
We define Correction Rate (CR) as a way to ap-
proximate the usefulness of the data. Given a word
d present in a sentence S and a configuration C, the
set of all potential governors of d in configuration
C, in all the n-best parses produced by the parser is
computed. This set is noted G = {g1, . . . , gj}. Let
us note GL the element of G that maximizes the lex-
ical affinity score. When the lexical resource gives
no score to any of the elements of G, GL is left un-
specified.
Ideally, G should not be the set of governors in
the n-best parses but the set of all possible governors
for d in sentence S. Since we have no simple way
to compute the latter, we will content ourselves with
the former as an approximation of the latter.
Let us note GH the governor of d in the (first)
best parse produced and GR the governor of d in the
correct parse. CR measures the effect of replacing
GH with GL.
We have represented in table 7 the different sce-
narios that can happen when comparing GH , GR
and GL.
GL = GR or GL unspec. CC
GH = GR GL 6= GR CE
GL = GR EC
GH 6= GR GL 6= GR or GL unspec. EE
GR /? G NA
Table 7: Five possible scenarios when comparing the
governor of a word produced by the parser (GH ), in
the reference parse (GR) and according to the lexical re-
source (GL).
In scenarios CC and CE, the parser did not make
a mistake (the first letter, C, stands for correct). In
scenario CC, the lexical affinity score was compat-
ible with the choice of the parser or the lexical re-
source did not select any candidate. In scenario CE,
the lexical resource introduced an error. In scenar-
ios EC and EE, the parser made an error. In EC,
the error was corrected by the lexical resource while
in EE, it wasn?t. Either because the lexical resource
candidate was not the correct governor or it was un-
specified. The last case, NA, indicates that the cor-
rect governor does not appear in any of the n-best
parses. Technically this case could be integrated in
EE (an error made by the parser was not corrected
by the lexical resource) but we chose to keep it apart
781
since it represents a case where the right solution
could not be found in the n-best parse list (the cor-
rect governor is not a member of set G).
Let?s note nS the number of occurrences of sce-
nario S for a given configuration. We compute CR
for this configuration in the following way:
CR =
old error number - new error number
old error number
=
nEC ? nCE
nEE + nEC + nNA
When CR is equal to 0, the correction did not have
any impact on the error rate. When CR> 0, the error
rate is reduced and if CR < 0 it is increased1.
CR for each configuration is reported in table 8.
The counts of the different scenarios have also been
reported.
Conf. nCC nCE nEC nEE nNA CR
OBJ 992 30 51 5 17 0.29
SBJ 1131 35 61 16 34 0.23
ADJ 2220 42 16 20 6 -0.62
NdeN 2083 93 42 44 21 -0.48
VdeN 150 2 49 1 13 0.75
NaN 89 5 21 10 2 0.48
VaN 273 19 132 8 11 0.75
NcN 165 17 12 31 12 -0.09
VcN 120 21 14 11 5 -0.23
? 7223 264 398 146 121 0.20
Table 8: Correction Rate of the lexical resource with re-
spect to FTB DEV.
Table 8 shows very different results among con-
figurations. Results for PP attachments VdeN, VaN
and NaN are quite good (a CR of 75% for a given
configuration, as VdeN indicates that the number of
errors on such a configuration is decreased by 25%).
It is interesting to note that the parser behaves quite
badly on these attachments: their accuracy (as re-
ported in table 3) is, respectively 74.68, 69.1 and
70.64. Lexical affinity helps in such cases. On
the other hand, some attachments like configuration
ADJ and NdeN, for which the parser showed very
good accuracy (96.6 and 92.2) show very poor per-
formances. In such cases, taking into account lexical
affinity creates new errors.
1One can note, that contrary to coverage, CR does not mea-
sure a characteristic of the lexical resource alone, but the lexical
resource combined with a parser.
On average, using the lexical resource with this
simple strategy of systematically replacing GH with
GL allows to decrease by 20% the errors made on
our 9 configurations and by 2.5% the global error
rate of the parser.
4.3 Filtering Data with Ambiguity Threshold
The data used to extract counts is noisy: it con-
tains errors made by the parser. Ideally, we would
like to take into account only non ambiguous sen-
tences, for which the parser outputs a single parse
hypothesis, hopefully the good one. Such an ap-
proach is obviously doomed to fail since almost ev-
ery sentence will be associated to several parses.
Another solution would be to select sentences for
which the parser has a high confidence, using confi-
dence measures as proposed in (Sa?nchez-Sa?ez et al,
2009; Hwa, 2004). But since we are only interested
in some parts of sentences (usually one attachment),
we don?t need high confidence for the whole sen-
tence. We have instead used a parameter, defined on
single dependencies, called the ambiguity measure.
Given the n best parses of a sentence and a depen-
dency ?, present in at least one of the n best parses,
let us note C(?) the number of occurrences of ? in
the n best parse set. We note AM(?) the ambiguity
measure associated to ?. It is computed as follows:
AM(?) = 1?
C(?)
n
An ambiguity measure of 0 indicates that ? is non
ambiguous in the set of the n best parses (the word
that constitutes the dependent in ? is attached to the
word that constitutes the governor in ? in all the n-
best analyses). When n gets large enough this mea-
sure approximates the non ambiguity of a depen-
dency in a given sentence.
Ambiguity measure is used to filter the data when
counting the number of occurrences of a configura-
tion: only occurrences that are made of dependen-
cies ? such that AM(?) ? ? are taken into account.
? is called the ambiguity threshold.
The results of coverage and CR given above were
computed for ? equal to 1, which means that, when
collecting counts, all the dependencies are taken into
account whatever their ambiguity is. Table 9 shows
coverage and CR for different values of ? . As ex-
pected, coverage decreases with ? . But, interest-
782
ingly, decreasing ? , from 1 down to 0.2 has a posi-
tive influence on CR. Ambiguity threshold plays the
role we expected: it allows to reduce noise in the
data, and corrects more errors.
? = 1.0 ? = 0.4 ? = 0.2 ? = 0.0
cov/CR cov/CR cov/CR cov/CR
OBJ 0.70/0.29 0.58/0.36 0.52/0.36 0.35/0.38
SBJ 0.68/0.23 0.64/0.23 0.62/0.23 0.52/0.23
ADJ 0.69/-0.62 0.61/-0.52 0.56/-0.52 0.43/-0.38
NdeN 0.67/-0.48 0.58/-0.53 0.52/-0.52 0.38/-0.41
VdeN 0.57/0.75 0.44/0.73 0.36/0.73 0.20/0.30
NaN 0.50/0.48 0.34/0.42 0.28/0.45 0.15/0.48
VaN 0.65/0.75 0.50/0.8 0.41/0.80 0.26/0.48
NcN 0.25/-0.09 0.19/0 0.16/0.02 0.07/0.13
VcV 0.56/-0.23 0.42/-0.07 0.28/0.03 0.08/0.07
Avg 0.66/0.2 0.57/0.23 0.51/0.24 0.38/0.17
Table 9: Coverage and Correction Rate on FTB DEV for
several values of ambiguity threshold.
5 Integrating Lexical Affinity in the Parser
We have devised three methods for taking into ac-
count lexical affinity scores in the parser. The first
two are post-processing methods, that take as input
the n-best parses produced by the parser and mod-
ify some attachments with respect to the information
given by the lexical resource. The third method in-
troduces the lexical affinity scores as new features in
the parsing model. The three methods are described
in 5.1, 5.2 and 5.3. They are evaluated in 5.4.
5.1 Post Processing Method
The post processing method is quite simple. It is
very close to the method that was used to compute
the Correction Rate of the lexical resource, in 4.2.2:
it takes as input the n-best parses produced by the
parser and, for every configuration occurrence C
found in the first best parse, the set (G) of all po-
tential governors of C, in the n-best parses, is com-
puted and among them, the word that maximizes the
lexical affinity score (GL) is identified.
Once GL is identified, one can replace the choice
of the parser (GH ) with GL. This method is quite
crude since it does not take into account the confi-
dence the parser has in the solution proposed. We
observed, in 4.2.2 that CR was very low for configu-
rations for which the parser achieves good accuracy.
In order to introduce the parser confidence in the fi-
nal choice of a governor, we compute C(GH) and
C(GL) which respectively represent the number of
times GH and GL appear as the governor of config-
uration C. The choice of the final governor, noted
G?, depends on the ratio of C(GH) and C(GL). The
complete selection strategy is the following:
1. if GH = GL or GL is unspecified, G? = GH .
2. if GH 6= GL, G? is determined as follows:
G? =
{
GH if
C(GH)
C(GL)
> ?
GL otherwise
where ? is a coefficient that is optimized on the
development data set.
We have reported, in table 10 the values of CR,
for the 9 different features, using this strategy, for
? = 1. We do not report the values of CR for other
values of ? since they are very close to each other.
The table shows several noticeable facts. First, the
new strategy performs much better than the former
one (crudely replacing GH by GL), the value of CR
increased from 0.2 to 0.4, which means that the er-
rors made on the nine configurations are now de-
creased by 40%. Second, CR is now positive for ev-
ery configuration: the number of errors is decreased
for every configuration.
Conf. OBJ SUJ ADJ NdeN VdeN
CR 0.45 0.46 0.14 0.05 0.73
Conf. NaN VaN NcN VcV ?
CR 0.12 0.8 0.12 0.1 0.4
Table 10: Correction Rate on FTB DEV when taking into
account parser confidence.
5.2 Double Parsing Method
The post processing method performs better than the
naive strategy that was used in 4.2.2. But it has an
important drawback: it creates inconsistent parses.
Recall that the parser we are using is based on a sec-
ond order model, which means that the score of a de-
pendency depends on some neighboring ones. Since
with the post processing method only a subset of the
dependencies are modified, the resulting parse is in-
consistent: the score of some dependencies is com-
puted on the basis of other dependencies that have
been modified.
783
In order to compute a new optimal parse tree
that preserves the modified dependencies, we have
used a technique proposed in (Mirroshandel and
Nasr, 2011) that modifies the scoring function of the
parser in such a way that the dependencies that we
want to keep in the parser output get better scores
than all competing dependencies.
The double parsing method is therefore a three
stage method. First, sentence S is parsed, producing
the n-best parses. Then, the post processing method
is used, modifying the first best parse. Let?s note
D the set of dependencies that were changed in this
process. In the last stage, a new parse is produced,
that preserves D.
5.3 Feature Based Method
In the feature based method, new features are
added to the parser that rely on lexical affinity
scores. These features are of the following form:
?C, lg, ld, ?C(s)?, where C is a configuration num-
ber, s is the lexical affinity score (s = s(C, lg, ld))
and ?c(?) is a discretization function.
Discretization of the lexical affinity scores is nec-
essary in order to fight against data sparseness. In
this work, we have used Weka software (Hall et al,
2009) to discretize the scores with unsupervised bin-
ning. Binning is a simple process which divides
the range of possible values a parameter can take
into subranges called bins. Two methods are im-
plemented in Weka to find the optimal number of
bins: equal-frequency and equal-width. In equal-
frequency binning, the range of possible values are
divided into k bins, each of which holds the same
number of instances. In equal-width binning, which
is the method we have used, the range are divided
into k subranges of the same size. The optimal num-
ber of bins is the one that minimizes the entropy of
the data. Weka computes different number of bins
for different configurations, ranging from 4 to 10.
The number of new features added to the parser is
equal to
?
C B(C) where C is a configuration and
B(C) is the number of bins for configuration C.
5.4 Evaluation
The three methods described above have been evalu-
ated on FTB TEST. Results are reported in table 11.
The three methods outperformed the baseline (the
state of the art parser for French which is a second
order graph based method) (Bohnet, 2010). The best
performances were obtained by the Double Parsing
method that achieved a labeled relative error reduc-
tion of 7, 1% on predicted POS tags, yielding the
best parsing results on the French Treebank. It per-
forms better than the Post Processing method, which
means that the second parsing stage corrects some
inconsistencies introduced in the Post Processing
method. The performances of the Feature Based
method are disappointing, it achieves an error reduc-
tion of 1.4%. This result is not easy to interpret. It
is probably due to the limited number of new fea-
tures introduced in the parser. These new features
probably have a hard time competing with the large
number of other features in the training process.
pred. POS tags gold POS tags
punct no punct punct no punct
BL LAS 88.02 90.24 88.88 91.12
UAS 90.02 92.50 90.71 93.20
PP LAS 88.45 90.73 89.46 91.78
UAS 90.61 93.20 91.44 93.86
DP LAS 88.87 91.10 89.72 91.90
UAS 90.84 93.30 91.58 93.99
FB LAS 88.19 90.33 89.29 91.43
UAS 90.22 92.62 91.09 93.46
Table 11: Parser accuracy on FTB TEST using the
standard parser (BL) the post processing method (PP),
the double parsing method (DP) and the feature based
method.
6 Conclusion
Computing lexical affinities, on large corpora, for
specific lexico-syntactic configurations that are hard
to disambiguate has shown to be an effective way
to increase the performances of a parser. We have
proposed in this paper one method to compute lexi-
cal affinity scores as well as three ways to introduce
this new information in a parser. Experiments on a
French corpus showed a relative decrease of the er-
ror rate of 7.1% Labeled Accuracy Score.
Acknowledgments
This work has been funded by the French Agence
Nationale pour la Recherche, through the projects
SEQUOIA (ANR-08-EMER-013) and EDYLEX
(ANR-08-CORD-009).
784
References
A. Abeille?, L. Cle?ment, and F. Toussenel. 2003. Building
a treebank for french. In Anne Abeille?, editor, Tree-
banks. Kluwer, Dordrecht.
E.H. Anguiano and M. Candito. 2011. Parse correction
with specialized models for difficult attachment types.
In Proceedings of EMNLP.
M. Bansal and D. Klein. 2011. Web-scale features for
full-scale parsing. In Proceedings of ACL, pages 693?
702.
D. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
B. Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of ACL, pages 89?97.
M. Candito and B. Crabbe?. 2009. Improving generative
statistical parsing with semi-supervised word cluster-
ing. In Proceedings of the 11th International Confer-
ence on Parsing Technologies, pages 138?141.
M. Candito and D. Seddah. 2010. Parsing word clusters.
In Proceedings of the NAACL HLT Workshop on Sta-
tistical Parsing of Morphologically-Rich Languages,
pages 76?84.
M. Candito, B. Crabbe?, P. Denis, and F. Gue?rin. 2009.
Analyse syntaxique du franc?ais : des constituants aux
de?pendances. In Proceedings of Traitement Automa-
tique des Langues Naturelles.
W. Chen, J. Kazama, K. Uchimoto, and K. Torisawa.
2009. Improving dependency parsing with subtrees
from auto-parsed data. In Proceedings of EMNLP,
pages 570?579.
K.W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational linguistics, 16(1):22?29.
P. Denis and B. Sagot. 2010. Exploitation d?une
ressource lexicale pour la construction d?un e?tiqueteur
morphosyntaxique e?tat-de-l?art du franc?ais. In Pro-
ceedings of Traitement Automatique des Langues Na-
turelles.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The WEKA data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
R. Hwa. 2004. Sample selection for statistical parsing.
Computational Linguistics, 30(3):253?276.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proceedings of the
ACL HLT, pages 595?603.
S. Ku?bler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Synthesis Lectures on Human Lan-
guage Technologies, 1(1):1?127.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational linguistics,
19(2):313?330.
D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In Proceedings of
HLT NAACL, pages 152?159.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In Proceedings of HLT-EMNLP,
pages 523?530.
S.A. Mirroshandel and A. Nasr. 2011. Active learning
for dependency parsing using partially annotated sen-
tences. In Proceedings of International Conference on
Parsing Technologies.
P. Nakov and M. Hearst. 2005. Using the web as an
implicit training set: application to structural ambigu-
ity resolution. In Proceedings of HLT-EMNLP, pages
835?842.
A. Nasr, F. Be?chet, J-F. Rey, B. Favre, and Le Roux J.
2011. MACAON: An NLP tool suite for processing
word lattices. In Proceedings of ACL.
E. Pitler, S. Bergsma, D. Lin, and K. Church. 2010. Us-
ing web-scale N-grams to improve base NP parsing
performance. In Proceedings of COLING, pages 886?
894.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with lr models and parser ensem-
bles. In Proceedings of the CoNLL shared task session
of EMNLP-CoNLL, volume 7, pages 1044?1050.
R. Sa?nchez-Sa?ez, J.A. Sa?nchez, and J.M. Bened??. 2009.
Statistical confidence measures for probabilistic pars-
ing. In Proceedings of RANLP, pages 388?392.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of EACL, pages 331?338.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In Proceed-
ings of EMNLP, pages 551?560.
M. Volk. 2001. Exploiting the WWW as a corpus to
resolve PP attachment ambiguities. In Proceedings of
Corpus Linguistics.
G. Zhou, J. Zhao, K. Liu, and L. Cai. 2011. Exploiting
web-derived selectional preference to improve statisti-
cal dependency parsing. In Proceedings of HLT-ACL,
pages 1556?1565.
785
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 162?168, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
LIPN-CORE: Semantic Text Similarity using n-grams, WordNet, Syntactic
Analysis, ESA and Information Retrieval based Features
Davide Buscaldi, Joseph Le Roux,
Jorge J. Garc??a Flores
Laboratoire d?Informatique de Paris Nord,
CNRS, (UMR 7030)
Universite? Paris 13, Sorbonne Paris Cite?,
F-93430, Villetaneuse, France
{buscaldi,joseph.le-roux,jgflores}
@lipn.univ-paris13.fr
Adrian Popescu
CEA, LIST,
Vision & Content
Engineering Laboratory
F-91190 Gif-sur-Yvette, France
adrian.popescu@cea.fr
Abstract
This paper describes the system used by the
LIPN team in the Semantic Textual Similarity
task at *SEM 2013. It uses a support vector re-
gression model, combining different text sim-
ilarity measures that constitute the features.
These measures include simple distances like
Levenshtein edit distance, cosine, Named En-
tities overlap and more complex distances like
Explicit Semantic Analysis, WordNet-based
similarity, IR-based similarity, and a similar-
ity measure based on syntactic dependencies.
1 Introduction
The Semantic Textual Similarity task (STS) at
*SEM 2013 requires systems to grade the degree of
similarity between pairs of sentences. It is closely
related to other well known tasks in NLP such as tex-
tual entailment, question answering or paraphrase
detection. However, as noticed in (Ba?r et al, 2012),
the major difference is that STS systems must give a
graded, as opposed to binary, answer.
One of the most successful systems in *SEM
2012 STS, (Ba?r et al, 2012), managed to grade pairs
of sentences accurately by combining focused mea-
sures, either simple ones based on surface features
(ie n-grams), more elaborate ones based on lexical
semantics, or measures requiring external corpora
such as Explicit Semantic Analysis, into a robust
measure by using a log-linear regression model.
The LIPN-CORE system is built upon this idea of
combining simple measures with a regression model
to obtain a robust and accurate measure of tex-
tual similarity, using the individual measures as fea-
tures for the global system. These measures include
simple distances like Levenshtein edit distance, co-
sine, Named Entities overlap and more complex dis-
tances like Explicit Semantic Analysis, WordNet-
based similarity, IR-based similarity, and a similar-
ity measure based on syntactic dependencies.
The paper is organized as follows. Measures are
presented in Section 2. Then the regression model,
based on Support Vector Machines, is described in
Section 3. Finally we discuss the results of the sys-
tem in Section 4.
2 Text Similarity Measures
2.1 WordNet-based Conceptual Similarity
(Proxigenea)
First of all, sentences p and q are analysed in or-
der to extract all the included WordNet synsets. For
each WordNet synset, we keep noun synsets and put
into the set of synsets associated to the sentence, Cp
and Cq, respectively. If the synsets are in one of the
other POS categories (verb, adjective, adverb) we
look for their derivationally related forms in order
to find a related noun synset: if there is one, we put
this synsets in Cp (or Cq). For instance, the word
?playing? can be associated in WordNet to synset
(v)play#2, which has two derivationally related
forms corresponding to synsets (n)play#5 and
(n)play#6: these are the synsets that are added
to the synset set of the sentence. No disambiguation
process is carried out, so we take all possible mean-
ings into account.
GivenCp andCq as the sets of concepts contained
in sentences p and q, respectively, with |Cp| ? |Cq|,
162
the conceptual similarity between p and q is calcu-
lated as:
ss(p, q) =
?
c1?Cp
max
c2?Cq
s(c1, c2)
|Cp|
(1)
where s(c1, c2) is a conceptual similarity measure.
Concept similarity can be calculated by different
ways. For the participation in the 2013 Seman-
tic Textual Similarity task, we used a variation of
the Wu-Palmer formula (Wu and Palmer, 1994)
named ?ProxiGenea? (from the french Proximite?
Ge?ne?alogique, genealogical proximity), introduced
by (Dudognon et al, 2010), which is inspired by the
analogy between a family tree and the concept hi-
erarchy in WordNet. Among the different formula-
tions proposed by (Dudognon et al, 2010), we chose
the ProxiGenea3 variant, already used in the STS
2012 task by the IRIT team (Buscaldi et al, 2012).
The ProxiGenea3 measure is defined as:
s(c1, c2) =
1
1 + d(c1) + d(c2)? 2 ? d(c0)
(2)
where c0 is the most specific concept that is present
both in the synset path of c1 and c2 (that is, the Least
Common Subsumer or LCS). The function returning
the depth of a concept is noted with d.
2.2 IC-based Similarity
This measure has been proposed by (Mihalcea et
al., 2006) as a corpus-based measure which uses
Resnik?s Information Content (IC) and the Jiang-
Conrath (Jiang and Conrath, 1997) similarity metric:
sjc(c1, c2) =
1
IC(c1) + IC(c2)? 2 ? IC(c0)
(3)
where IC is the information content introduced by
(Resnik, 1995) as IC(c) = ? logP (c).
The similarity between two text segments T1 and
T2 is therefore determined as:
sim(T1, T2) =
1
2
?
?
?
?
w?{T1}
max
w2?{T2}
ws(w,w2) ? idf(w)
?
w?{T1}
idf(w)
+
?
w?{T2}
max
w1?{T1}
ws(w,w1) ? idf(w)
?
w?{T2}
idf(w)
?
?
?(4)
where idf(w) is calculated as the inverse document
frequency of word w, taking into account Google
Web 1T (Brants and Franz, 2006) frequency counts.
The semantic similarity between words is calculated
as:
ws(wi, wj) = max
ci?Wi,cjinWj
sjc(ci, cj). (5)
where Wi and Wj are the sets containing all synsets
in WordNet corresponding to word wi and wj , re-
spectively. The IC values used are those calcu-
lated by Ted Pedersen (Pedersen et al, 2004) on the
British National Corpus1.
2.3 Syntactic Dependencies
We also wanted for our systems to take syntac-
tic similarity into account. As our measures are
lexically grounded, we chose to use dependen-
cies rather than constituents. Previous experiments
showed that converting constituents to dependen-
cies still achieved best results on out-of-domain
texts (Le Roux et al, 2012), so we decided to use
a 2-step architecture to obtain syntactic dependen-
cies. First we parsed pairs of sentences with the
LORG parser2. Second we converted the resulting
parse trees to Stanford dependencies3.
Given the sets of parsed dependenciesDp andDq,
for sentence p and q, a dependency d ? Dx is a
triple (l, h, t) where l is the dependency label (for in-
stance, dobj or prep), h the governor and t the depen-
dant. We define the following similarity measure be-
tween two syntactic dependencies d1 = (l1, h1, t1)
and d2 = (l2, h2, t2):
dsim(d1, d2) = Lev(l1, l2)
?
idfh ? sWN (h1, h2) + idft ? sWN (t1, t2)
2
(6)
where idfh = max(idf(h1), idf(h2)) and idft =
max(idf(t1), idf(t2)) are the inverse document fre-
quencies calculated on Google Web 1T for the gov-
ernors and the dependants (we retain the maximum
for each pair), and sWN is calculated using formula
2, with two differences:
? if the two words to be compared are antonyms,
then the returned score is 0;
1
http://www.d.umn.edu/?tpederse/similarity.html
2
https://github.com/CNGLdlab/LORG-Release
3We used the default built-in converter provided with the
Stanford Parser (2012-11-12 revision).
163
? if one of the words to be compared is not in
WordNet, their similarity is calculated using
the Levenshtein distance.
The similarity score between p and q, is then cal-
culated as:
sSD(p, q) = max
?
?
?
?
di?Dp
max
djinDq
dsim(di, dj)
|Dp|
,
?
di?Dq
max
djinDp
dsim(di, dj)
|Dq|
?
?
?
(7)
2.4 Information Retrieval-based Similarity
Let us consider two texts p and q, an Information Re-
trieval (IR) system S and a document collection D
indexed by S. This measure is based on the assump-
tion that p and q are similar if the documents re-
trieved by S for the two texts, used as input queries,
are ranked similarly.
Let be Lp = {dp1 , . . . , dpK} and Lq =
{dq1 , . . . , dqK}, dxi ? D the sets of the top K docu-
ments retrieved by S for texts p and q, respectively.
Let us define sp(d) and sq(d) the scores assigned by
S to a document d for the query p and q, respectively.
Then, the similarity score is calculated as:
simIR(p, q) = 1?
?
d?Lp?Lq
?
(sp(d)?sq(d))2
max(sp(d),sq(d))
|Lp ? Lq|
(8)
if |Lp ? Lq| 6= ?, 0 otherwise.
For the participation in this task we indexed a
collection composed by the AQUAINT-24 and the
English NTCIR-85 document collections, using the
Lucene6 4.2 search engine with BM25 similarity.
The K value was empirically set to 20 after some
tests on the STS 2012 data.
2.5 ESA
Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007) represents meaning as a
4
http://www.nist.gov/tac/data/data_desc.html#AQUAINT-2
5
http://metadata.berkeley.edu/NTCIR-GeoTime/
ntcir-8-databases.php
6
http://lucene.apache.org/core
weighted vector of Wikipedia concepts. Weights
are supposed to quantify the strength of the relation
between a word and each Wikipedia concept using
the tf-idf measure. A text is then represented as a
high-dimensional real valued vector space spanning
all along the Wikipedia database. For this particular
task we adapt the research-esa implementation
(Sorg and Cimiano, 2008)7 to our own home-made
weighted vectors corresponding to a Wikipedia
snapshot of February 4th, 2013.
2.6 N-gram based Similarity
This feature is based on the Clustered Keywords Po-
sitional Distance (CKPD) model proposed in (Bus-
caldi et al, 2009) for the passage retrieval task.
The similarity between a text fragment p and an-
other text fragment q is calculated as:
simngrams(p, q) =
?
?x?Q
h(x, P )
1
d(x, xmax)
?n
i=1wi
(9)
Where P is the set of n-grams with the highest
weight in p, where all terms are also contained in q;
Q is the set of all the possible n-grams in q and n
is the total number of terms in the longest passage.
The weights for each term and each n-gram are cal-
culated as:
? wi calculates the weight of the term tI as:
wi = 1?
log(ni)
1 + log(N)
(10)
Where ni is the frequency of term ti in the
Google Web 1T collection, and N is the fre-
quency of the most frequent term in the Google
Web 1T collection.
? the function h(x, P ) measures the weight of
each n-gram and is defined as:
h(x, Pj) =
{ ?j
k=1wk if x ? Pj
0 otherwise
(11)
7
http://code.google.com/p/research-esa/
164
Where wk is the weight of the k-th term (see
Equation 10) and j is the number of terms that
compose the n-gram x;
? 1d(x,xmax) is a distance factor which reduces the
weight of the n-grams that are far from the
heaviest n-gram. The function d(x, xmax) de-
termines numerically the value of the separa-
tion according to the number of words between
a n-gram and the heaviest one:
d(x, xmax) = 1 + k? ln(1 + L) (12)
where k is a factor that determines the impor-
tance of the distance in the similarity calcula-
tion and L is the number of words between a
n-gram and the heaviest one (see Equation 11).
In our experiments, k was set to 0.1, the default
value in the original model.
2.7 Other measures
In addition to the above text similarity measures, we
used also the following common measures:
2.7.1 Cosine
Given p = (wp1 , . . . , wpn) and q =
(wq1 , . . . , wqn) the vectors of tf.idf weights asso-
ciated to sentences p and q, the cosine distance is
calculated as:
simcos(p,q) =
n?
i=1
wpi ? wqi
?
n?
i=1
wpi2 ?
?
n?
i=1
wqi2
(13)
The idf value was calculated on Google Web 1T.
2.7.2 Edit Distance
This similarity measure is calculated using the
Levenshtein distance as:
simED(p, q) = 1?
Lev(p, q)
max(|p|, |q|)
(14)
where Lev(p, q) is the Levenshtein distance be-
tween the two sentences, taking into account the
characters.
2.7.3 Named Entity Overlap
We used the Stanford Named Entity Recognizer
by (Finkel et al, 2005), with the 7 class model
trained for MUC: Time, Location, Organization,
Person, Money, Percent, Date. Then we calculated a
per-class overlap measure (in this way, ?France? as
an Organization does not match ?France? as a Loca-
tion):
ONER(p, q) =
2 ? |Np ?Nq|
|Np|+ |Nq|
(15)
where Np and Nq are the sets of NEs found, respec-
tively, in sentences p and q.
3 Integration of Similarity Measures
The integration has been carried out using the
?-Support Vector Regression model (?-SVR)
(Scho?lkopf et al, 1999) implementation provided
by LIBSVM (Chang and Lin, 2011), with a radial
basis function kernel with the standard parameters
(? = 0.5).
4 Results
In order to evaluate the impact of the different fea-
tures, we carried out an ablation test, removing one
feature at a time and training a new model with the
reduced set of features. In Table 2 we show the re-
sults of the ablation test for each subset of the *SEM
2013 test set; in Table 1 we show the same test on the
whole test set. Note: the results have been calculated
as the Pearson correlation test on the whole test set
and not as an average of the correlation scores cal-
culated over the composing test sets.
Feature Removed Pearson Loss
None 0.597 0
N-grams 0.596 0.10%
WordNet 0.563 3.39%
SyntDeps 0.602 ?0.43%
Edit 0.584 1.31%
Cosine 0.596 0.10%
NE Overlap 0.603 ?0.53%
IC-based 0.598 ?0.10%
IR-Similarity 0.510 8.78%
ESA 0.601 ?0.38%
Table 1: Ablation test for the different features on the
whole 2013 test set.
165
FNWN Headlines OnWN SMT
Feature Pearson Loss Pearson Loss Pearson Loss Pearson Loss
None 0.404 0 0.706 0 0.694 0 0.301 0
N-grams 0.379 2.49% 0.705 0.12% 0.698 ?0.44% 0.289 1.16%
WordNet 0.376 2.80% 0.695 1.09% 0.682 1.17% 0.278 2.28%
SyntDeps 0.403 0.08% 0.699 0.70% 0.679 1.49% 0.284 1.62%
Edit 0.402 0.19% 0.689 1.70% 0.667 2.72% 0.286 1.50%
Cosine 0.393 1.03% 0.683 2.38% 0.676 1.80% 0.303 ?0.24%
NE Overlap 0.410 ?0.61% 0.700 0.67% 0.680 1.37% 0.285 1.58%
IC-based 0.391 1.26% 0.699 0.75% 0.669 2.50% 0.283 1.76%
IR-Similarity 0.426 ?2.21% 0.633 7.33% 0.589 10.46% 0.249 5.19%
ESA 0.391 1.22% 0.691 1.57% 0.702 ?0.81% 0.275 2.54%
Table 2: Ablation test for the different features on the different parts of the 2013 test set.
FNWN Headlines OnWN SMT ALL
N-grams 0.285 0.532 0.459 0.280 0.336
WordNet 0.395 0.606 0.552 0.282 0.477
SyntDeps 0.233 0.409 0.345 0.323 0.295
Edit 0.220 0.536 0.089 0.355 0.230
Cosine 0.306 0.573 0.541 0.244 0.382
NE Overlap 0.000 0.216 0.000 0.013 0.020
IC-based 0.413 0.540 0.642 0.285 0.421
IR-based 0.067 0.598 0.628 0.241 0.541
ESA 0.328 0.546 0.322 0.289 0.390
Table 3: Pearson correlation calculated on individual features.
The ablation test show that the IR-based feature
showed up to be the most effective one, especially
for the headlines subset (as expected), and, quite sur-
prisingly, on the OnWN data. In Table 3 we show
the correlation between each feature and the result
(feature values normalised between 0 and 5): from
this table we can also observe that, on average, IR-
based similarity was better able to capture the se-
mantic similarity between texts. The only exception
was the FNWN test set: the IR-based similarity re-
turned a 0 score 178 times out of 189 (94.1%), indi-
cating that the indexed corpus did not fit the content
of the FNWN sentences. This result shows also the
limits of the IR-based similarity score which needs
a large corpus to achieve enough coverage.
4.1 Shared submission with INAOE-UPV
One of the files submitted by INAOE-UPV,
INAOE-UPV-run3 has been produced using seven
features produced by different teams: INAOE, LIPN
and UMCC-DLSI. We contributed to this joint sub-
mission with the IR-based, WordNet and cosine fea-
tures.
5 Conclusions and Further Work
In this paper we introduced the LIPN-CORE sys-
tem, which combines semantic, syntactic an lexi-
cal measures of text similarity in a linear regression
model. Our system was among the best 15 runs for
the STS task. According to the ablation test, the best
performing feature was the IR-based one, where a
sentence is considered as a query and its meaning
represented as a set of documents indexed by an IR
system. The second and third best-performing mea-
sures were WordNet similarity and Levenshtein?s
edit distance. On the other hand, worst perform-
ing similarity measures were Named Entity Over-
lap, Syntactic Dependencies and ESA. However, a
correlation analysis calculated on the features taken
one-by-one shows that the contribution of a feature
166
on the overall regression result does not correspond
to the actual capability of the measure to represent
the semantic similarity between the two texts. These
results raise the methodological question of how to
combine semantic, syntactic and lexical similarity
measures in order to estimate the impact of the dif-
ferent strategies used on each dataset.
Further work will include richer similarity mea-
sures, like quasi-synchronous grammars (Smith and
Eisner, 2006) and random walks (Ramage et al,
2009). Quasi-synchronous grammars have been
used successfully for paraphrase detection (Das and
Smith, 2009), as they provide a fine-grained model-
ing of the alignment of syntactic structures, in a very
flexible way, enabling partial alignments and the in-
clusion of external features, like Wordnet lexical re-
lations for example. Random walks have been used
effectively for paraphrase recognition and as a fea-
ture for recognizing textual entailment. Finally, we
will continue analyzing the question of how to com-
bine a wide variety of similarity measures in such a
way that they tackle the semantic variations of each
dataset.
Acknowledgments
We would like to thank the Quaero project and the
LabEx EFL8 for their support to this work.
References
[Ba?r et al2012] Daniel Ba?r, Chris Biemann, Iryna
Gurevych, and Torsten Zesch. 2012. Ukp: Computing
semantic textual similarity by combining multiple
content similarity measures. In Proceedings of the
6th International Workshop on Semantic Evaluation,
held in conjunction with the 1st Joint Conference
on Lexical and Computational Semantics, pages
435?440, Montreal, Canada, June.
[Brants and Franz2006] Thorsten Brants and Alex Franz.
2006. Web 1t 5-gram corpus version 1.1.
[Buscaldi et al2009] Davide Buscaldi, Paolo Rosso,
Jose? Manuel Go?mez, and Emilio Sanchis. 2009. An-
swering questions with an n-gram based passage re-
trieval engine. Journal of Intelligent Information Sys-
tems (JIIS), 34(2):113?134.
[Buscaldi et al2012] Davide Buscaldi, Ronan Tournier,
Nathalie Aussenac-Gilles, and Josiane Mothe. 2012.
8http://www.labex-efl.org
Irit: Textual similarity combining conceptual simi-
larity with an n-gram comparison method. In Pro-
ceedings of the 6th International Workshop on Se-
mantic Evaluation (SemEval 2012), Montreal, Que-
bec, Canada.
[Chang and Lin2011] Chih-Chung Chang and Chih-Jen
Lin. 2011. LIBSVM: A library for support vector
machines. ACM Transactions on Intelligent Systems
and Technology, 2:27:1?27:27. Software available
at http://www.csie.ntu.edu.tw/?cjlin/
libsvm.
[Das and Smith2009] Dipanjan Das and Noah A. Smith.
2009. Paraphrase identification as probabilistic quasi-
synchronous recognition. In Proc. of ACL-IJCNLP.
[Dudognon et al2010] Damien Dudognon, Gilles Hubert,
and Bachelin Jhonn Victorino Ralalason. 2010.
Proxige?ne?a : Une mesure de similarite? conceptuelle.
In Proceedings of the Colloque Veille Strate?gique Sci-
entifique et Technologique (VSST 2010).
[Finkel et al2005] Jenny Rose Finkel, Trond Grenager,
and Christopher Manning. 2005. Incorporating non-
local information into information extraction systems
by gibbs sampling. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?05, pages 363?370, Stroudsburg, PA,
USA. Association for Computational Linguistics.
[Gabrilovich and Markovitch2007] Evgeniy Gabrilovich
and Shaul Markovitch. 2007. Computing seman-
tic relatedness using wikipedia-based explicit semantic
analysis. In Proceedings of the 20th international joint
conference on Artifical intelligence, IJCAI?07, pages
1606?1611, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
[Jiang and Conrath1997] J.J. Jiang and D.W. Conrath.
1997. Semantic similarity based on corpus statistics
and lexical taxonomy. In Proc. of the Int?l. Conf. on
Research in Computational Linguistics, pages 19?33.
[Le Roux et al2012] Joseph Le Roux, Jennifer Foster,
Joachim Wagner, Rasul Samad Zadeh Kaljahi, and
Anton Bryl. 2012. DCU-Paris13 Systems for the
SANCL 2012 Shared Task. In The NAACL 2012 First
Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL), pages 1?4, Montre?al, Canada,
June.
[Mihalcea et al2006] Rada Mihalcea, Courtney Corley,
and Carlo Strapparava. 2006. Corpus-based and
knowledge-based measures of text semantic similarity.
In Proceedings of the 21st national conference on Ar-
tificial intelligence - Volume 1, AAAI?06, pages 775?
780. AAAI Press.
[Pedersen et al2004] Ted Pedersen, Siddharth Patward-
han, and Jason Michelizzi. 2004. Wordnet::similarity:
measuring the relatedness of concepts. In Demon-
stration Papers at HLT-NAACL 2004, HLT-NAACL?
167
Demonstrations ?04, pages 38?41, Stroudsburg, PA,
USA. Association for Computational Linguistics.
[Ramage et al2009] Daniel Ramage, Anna N. Rafferty,
and Christopher D. Manning. 2009. Random walks
for text semantic similarity. In Proceedings of the
2009 Workshop on Graph-based Methods for Natural
Language Processing, pages 23?31. The Association
for Computer Linguistics.
[Resnik1995] Philip Resnik. 1995. Using information
content to evaluate semantic similarity in a taxonomy.
In Proceedings of the 14th international joint confer-
ence on Artificial intelligence - Volume 1, IJCAI?95,
pages 448?453, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
[Scho?lkopf et al1999] Bernhard Scho?lkopf, Peter
Bartlett, Alex Smola, and Robert Williamson. 1999.
Shrinking the tube: a new support vector regression
algorithm. In Proceedings of the 1998 conference on
Advances in neural information processing systems II,
pages 330?336, Cambridge, MA, USA. MIT Press.
[Smith and Eisner2006] David A. Smith and Jason Eisner.
2006. Quasi-synchronous grammars: Alignment by
soft projection of syntactic dependencies. In Proceed-
ings of the HLT-NAACL Workshop on Statistical Ma-
chine Translation, pages 23?30, New York, June.
[Sorg and Cimiano2008] Philipp Sorg and Philipp Cimi-
ano. 2008. Cross-lingual Information Retrieval with
Explicit Semantic Analysis. In Working Notes for the
CLEF 2008 Workshop.
[Wu and Palmer1994] Zhibiao Wu and Martha Palmer.
1994. Verbs semantics and lexical selection. In Pro-
ceedings of the 32nd annual meeting on Association
for Computational Linguistics, ACL ?94, pages 133?
138, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
168
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 400?405,
Dublin, Ireland, August 23-24, 2014.
LIPN: Introducing a new Geographical Context Similarity Measure and a
Statistical Similarity Measure Based on the Bhattacharyya Coefficient
Davide Buscaldi, Jorge J. Garc??a Flores, Joseph Le Roux, Nadi Tomeh
Laboratoire d?Informatique de Paris Nord, CNRS (UMR 7030)
Universit?e Paris 13, Sorbonne Paris Cit?e, Villetaneuse, France
{buscaldi,jgflores,joseph.le-roux,nadi.tomeh}@lipn.univ-paris13.fr
Bel
?
em Priego Sanchez
Laboratoire LDI (Lexique, Dictionnaires, Informatique)
Universit?e Paris 13, Sorbonne Paris Cit?e, Villetaneuse, France
LKE, FCC, BUAP, San Manuel, Puebla, Mexico
belemps@gmail.com
Abstract
This paper describes the system used by
the LIPN team in the task 10, Multilin-
gual Semantic Textual Similarity, at Sem-
Eval 2014, in both the English and Span-
ish sub-tasks. The system uses a sup-
port vector regression model, combining
different text similarity measures as fea-
tures. With respect to our 2013 partici-
pation, we included a new feature to take
into account the geographical context and
a new semantic distance based on the
Bhattacharyya distance calculated on co-
occurrence distributions derived from the
Spanish Google Books n-grams dataset.
1 Introduction
After our participation at SemEval 2013 with
LIPN-CORE (Buscaldi et al., 2013) we found that
geography has an important role in discriminating
the semantic similarity of sentences (especially in
the case of newswire). If two events happened in
a different location, their semantic relatedness is
usually low, no matter if the events are the same.
Therefore, we worked on a similarity measure able
to capture the similarity between the geographic
contexts of two sentences. We tried also to rein-
force the semantic similarity features by introduc-
ing a new measure that calculates word similari-
ties on co-occurrence distributions extracted from
Google Books bigrams. This measure was intro-
duced only for the Spanish runs, due to time con-
straints. The regression model used to integrate
the features was the ?-Support Vector Regression
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence de-
tails:http://creativecommons.org/licenses/by/4.0/
model (?-SVR) (Sch?olkopf et al., 1999) imple-
mentation provided by LIBSVM (Chang and Lin,
2011), with a radial basis function kernel with the
standard parameters (? = 0.5). We describe all
the measures in Section 2; the results obtained by
the system are detailed in Section 3.
2 Similarity Measures
In this section we describe the measures used as
features in our system. The description of mea-
sures already used in our 2013 participation is less
detailed than the description of the new ones. Ad-
ditional details on the measures may be found in
(Buscaldi et al., 2013). When POS tagging and
NE recognition were required, we used the Stan-
ford CoreNLP
1
for English and FreeLing
2
3.1 for
Spanish.
2.1 WordNet-based Conceptual Similarity
This measure has been introduced in order to mea-
sure similarities between concepts with respect to
an ontology. The similarity is calculated as fol-
lows: first of all, words in sentences p and q are
lemmatised and mapped to the related WordNet
synsets. All noun synsets are put into the set of
synsets associated to the sentence, C
p
and C
q
, re-
spectively. If the synsets are in one of the other
POS categories (verb, adjective, adverb) we look
for their derivationally related forms in order to
find a related noun synset: if there exists one, we
put this synset in C
p
(or C
q
). No disambigua-
tion process is carried out, so we take all possible
meanings into account.
Given C
p
and C
q
as the sets of concepts con-
tained in sentences p and q, respectively, with
1
http://www-nlp.stanford.edu/software/corenlp.shtml
2
http://nlp.lsi.upc.edu/freeling/
400
|C
p
| ? |C
q
|, the conceptual similarity between p
and q is calculated as:
ss(p, q) =
?
c
1
?C
p
max
c
2
?C
q
s(c
1
, c
2
)
|C
p
|
where s(c
1
, c
2
) is a conceptual similarity mea-
sure. Concept similarity can be calculated in dif-
ferent ways. We used a variation of the Wu-Palmer
formula (Wu and Palmer, 1994) named ?Proxi-
Genea3?, introduced by (Dudognon et al., 2010),
which is inspired by the analogy between a family
tree and the concept hierarchy in WordNet. The
ProxiGenea3 measure is defined as:
s(c
1
, c
2
) =
1
1 + d(c
1
) + d(c
2
)? 2 ? d(c
0
)
where c
0
is the most specific concept that is
present both in the synset path of c
1
and c
2
(that is,
the Least Common Subsumer or LCS). The func-
tion returning the depth of a concept is noted with
d.
2.2 IC-based Similarity
This measure has been proposed by (Mihalcea et
al., 2006) as a corpus-based measure which uses
Resnik?s Information Content (IC) and the Jiang-
Conrath (Jiang and Conrath, 1997) similarity met-
ric. This measure is more precise than the one
introduced in the previous subsection because it
takes into account also the importance of concepts
and not only their relative position in the hierarchy.
We refer to (Buscaldi et al., 2013) and (Mihalcea
et al., 2006) for a detailed description of the mea-
sure. The idf weights for the words were calcu-
lated using the Google Web 1T (Brants and Franz,
2006) frequency counts, while the IC values used
are those calculated by Ted Pedersen (Pedersen et
al., 2004) on the British National Corpus
3
.
2.3 Syntactic Dependencies
This measure tries to capture the syntactic simi-
larity between two sentences using dependencies.
Previous experiments showed that converting con-
stituents to dependencies still achieved best results
on out-of-domain texts (Le Roux et al., 2012), so
we decided to use a 2-step architecture to obtain
syntactic dependencies. First we parsed pairs of
sentences with the LORG parser
4
. Second we con-
3
http://www.d.umn.edu/ tpederse/similarity.html
4
https://github.com/CNGLdlab/LORG-Release
verted the resulting parse trees to Stanford depen-
dencies
5
.
Given the sets of parsed dependencies D
p
and
D
q
, for sentence p and q, a dependency d ? D
x
is a triple (l, h, t) where l is the dependency label
(for instance, dobj or prep), h the governor and
t the dependant. The similarity measure between
two syntactic dependencies d
1
= (l
1
, h
1
, t
1
) and
d
2
= (l
2
, h
2
, t
2
) is the levenshtein distance be-
tween the labels l
1
and l
2
multiplied by the aver-
age of idf
h
? s
WN
(h
1
, h
2
) and idf
t
? s
WN
(t
1
, t
2
),
where idf
h
and idf
t
are the inverse document fre-
quencies calculated on Google Web 1T for the
governors and the dependants (we retain the max-
imum for each pair), respectively, and s
WN
is cal-
culated using formula ??. NOTE: This measure
was used only in the English sub-task.
2.4 Information Retrieval-based Similarity
Let us consider two texts p and q, an IR system S
and a document collection D indexed by S. This
measure is based on the assumption that p and q
are similar if the documents retrieved by S for the
two texts, used as input queries, are ranked simi-
larly.
Let be L
p
= {d
p
1
, . . . , d
p
K
} and L
q
=
{d
q
1
, . . . , d
q
K
}, d
x
i
? D the sets of the top K
documents retrieved by S for texts p and q, respec-
tively. Let us define s
p
(d) and s
q
(d) the scores as-
signed by S to a document d for the query p and
q, respectively. Then, the similarity score is calcu-
lated as:
sim
IR
(p, q) = 1?
?
d?L
p
?L
q
?
(s
p
(d)?s
q
(d))
2
max(s
p
(d),s
q
(d))
|L
p
? L
q
|
if |L
p
? L
q
| 6= ?, 0 otherwise.
For the participation in the English sub-task we
indexed a collection composed by the AQUAINT-
2
6
and the English NTCIR-8
7
document collec-
tions, using the Lucene
8
4.2 search engine with
BM25 similarity. The Spanish index was cre-
ated using the Spanish QA@CLEF 2005 (agencia
EFE1994-95, El Mundo 1994-95) and multiUN
5
We used the default built-in converter provided with the
Stanford Parser (2012-11-12 revision).
6
http://www.nist.gov/tac/data/data desc.html#AQUAINT-
2
7
http://metadata.berkeley.edu/NTCIR-GeoTime/ntcir-8-
databases.php
8
http://lucene.apache.org/core
401
(Eisele and Chen, 2010) collections. The K value
was set to 70 after a study detailed in (Buscaldi,
2013).
2.5 N-gram Based Similarity
This measure tries to capture the fact that similar
sentences have similar n-grams, even if they are
not placed in the same positions. The measure is
based on the Clustered Keywords Positional Dis-
tance (CKPD) model proposed in (Buscaldi et al.,
2009) for the passage retrieval task.
The similarity between a text fragment p and
another text fragment q is calculated as:
sim
ngrams
(p, q) =
?
?x?Q
h(x, P )
?
n
i=1
w
i
d(x, x
max
)
Where P is the set of the heaviest n-grams in p
where all terms are also contained in q; Q is the
set of all the possible n-grams in q, and n is the
total number of terms in the longest sentence. The
weights for each term w
i
are calculated as w
i
=
1 ?
log(n
i
)
1+log(N)
where n
i
is the frequency of term
t
i
in the Google Web 1T collection, and N is the
frequency of the most frequent term in the Google
Web 1T collection. The weight for each n-gram
(h(x, P )), with |P | = j is calculated as:
h(x, P ) =
{
?
j
k=1
w
k
if x ? P
0 otherwise
The function d(x, x
max
) determines the minimum
distance between a n-gram x and the heaviest one
x
max
as the number of words between them.
2.6 Geographical Context Similarity
We observed that in many sentences, especially
those extracted from news corpora, the compati-
bility of the geographic context between the sen-
tences is an important clue to determine if the sen-
tences are related or not. This measure tries to
measure if the two sentences refer to events that
took place in the same geographical area. We built
a database of geographically-related entities, using
geo-WordNet (Buscaldi and Rosso, 2008) and ex-
panding it with all the synsets that are related to a
geographically grounded synset. This implies that
also adjectives and verbs may be used as clues for
the identification of the geographical context of a
sentence. For instance, ?Afghan? is associated to
?Afghanistan?, ?Sovietize? to ?Soviet Union?, etc.
The Named Entities of type PER (Person) are also
used as clues: we use Yago
9
to check whether the
NE corresponds to a famous leader or not, and in
the affirmative case we include the related nation
to the geographical context of the sentence. For in-
stance, ?Merkel? is mapped to ?Germany?. Given
G
p
and G
q
the sets of places found in sentences p
and q, respectively, the geographical context simi-
larity is calculated as follows:
sim
geo
(p, q) = 1?log
K
?
?
?
1 +
?
x?G
p
min
y?G
q
d(x, y)
max(|G
p
|, |G
q
|)
?
?
?
Where d(x, y) is the spherical distance in Km. be-
tween x and y, and K is a normalization factor set
to 10000 Km. to obtain similarity values between
1 and 0.
2.7 2-grams ?Spectral? Distance
This measure is used to calculate the seman-
tic similarity of two words on the basis of their
context, according to the distributional hypothe-
sis. The measure exploits bi-grams in the Google
Books n-gram collection
10
and is based on the dis-
tributional hypothesis, that is, ?words that tend to
appear in similar contexts are supposed to have
similar meanings?. Given a word w, we calcu-
late the probability of observing a word x know-
ing that it is preceded by w as p(x|w) = p(w ?
x)/p(w) = c(?wx?)/c(?w?), where c(?wx?) is
the number of bigrams ?w x? observed in Google
Books (counting all publication years) 2-grams
and c(?w?) is the number of occurrences of w ob-
served in Google Books 1-grams. We calculate
also the probability of observing a word y know-
ing that it is followed by w as p(y|w) = p(w ?
y)/p(w) = c(?yw?)/c(?w?). In such a way, we
may obtain for a word w
i
two probability distri-
butions D
w
i
p
and D
w
i
f
that can be compared to the
distributions obtained in the same way for another
word w
j
. Therefore, we calculate the distance of
two words comparing the distribution probabilities
built in this way, using the Bhattacharyya coeffi-
cient:
9
http://www.mpi-inf.mpg.de/yago-naga/yago/
10
https://books.google.com/ngrams/datasets
402
sf
(w
i
, w
j
) = ? log
(
?
x?X
?
D
w
i
f
(x) ?D
w
j
f
(x)
)
s
p
(w
i
, w
j
) = ? log
(
?
x?X
?
D
w
i
p
(x) ?D
w
j
p
(x)
)
the resulting distance between w
i
and w
j
is cal-
culated as the average between s
f
(w
i
, w
j
) and
s
p
(w
i
, w
j
). All words in sentence p are compared
to the words of sentence q using this similarity
value. The words that are semantically closer are
paired; if a word cannot be paired (average dis-
tance with any of the words in the other sentence
> 10), then it is left unpaired. The value used as
the final feature is the averaged sum of all distance
scores.
2.8 Other Measures
In addition to the above text similarity measures,
we used also the following common measures:
Cosine
Cosine distance calculated between
p = (w
p
1
, . . . , w
p
n
) and q = (w
q
1
, . . . , w
q
n
), the
vectors of tf.idf weights associated to sentences
p and q, with idf values calculated on Google Web
1T.
Edit Distance
This similarity measure is calculated using the
Levenshtein distance on characters between the
two sentences.
Named Entity Overlap
This is a per-class overlap measure (in this way,
?France? as an Organization does not match
?France? as a Location) calculated using the Dice
coefficient between the sets of NEs found, respec-
tively, in sentences p and q.
3 Results
3.1 Spanish
In order to train the Spanish model, we trans-
lated automatically all the sentences in the English
SemEval 2012 and 2013 using Google Translate.
We also built a corpus manually using definitions
from the RAE
11
(Real Academia Espa?nola de la
Lengua). The definitions were randomly extracted
and paired at different similarity levels (taking into
11
http://www.rae.es/
account the Dice coefficient calculated on the def-
initions bag-of-words). Three annotators gave in-
dependently their similarity judgments on these
paired definitions. A total of 200 definitions were
annotated for training. The official results for the
Spanish task are shown in Table 1. In Figure 1 we
show the results obtained by taking into account
each individual feature as a measure of similarity
between texts. These results show that the combi-
nation was always better than the single features
(as expected), and the feature best able to capture
semantic similarity alone was the cosine distance.
In Table 2 we show the results of the ablation
test, which shows that the features that most con-
tributed to improve the results were the IR-based
similarity for the news dataset and the cosine dis-
tance for the Wikipedia dataset. The worst feature
was the NER overlap (not taking into account it
would have allowed us to gain 2 places in the final
rankings).
Wikipedia News Overall
LIPN-run1 0.65194 0.82554 0.75558
LIPN-run2 0.71647 0.8316 0.7852
LIPN-run3 0.71618 0.80857 0.77134
Table 1: Spanish results (Official runs).
The differences between the three submit-
ted runs are only in the training set used.
LIPN-run1 uses all the training data available
together, LIPN-run3 uses a training set com-
posed by the translated news for the news dataset
and the RAE training set for the Wikipedia dataset;
finally, the best run LIPN-run2 uses the same
training sets of run3 together to build a single
model.
3.2 English
Our participation in the English task was ham-
pered by some technical problems which did not
allow us to complete the parsing of the tweet data
in time. As a consequence of this and some er-
rors in the scripts launched to finalize the experi-
ments, the submitted results were incomplete and
we were able to detect the problem only after the
submission. We show in Table 3 the official re-
sults of run1 with the addition of the results on the
OnWN dataset calculated after the participation to
the task.
403
Figure 1: Spanish task: results taking into account the individual features as semantic similarity mea-
sures.
Ablated feature Wikipedia News Overall diff
LIPN-run2 (none) 0.7165 0.8316 0.7852 0.00%
1:CKPD 0.7216 0.8318 0.7874 0.22%
2:WN 0.7066 0.8277 0.7789 ?0.63%
3:Edit Dist 0.708 0.8242 0.7774 ?0.78%
4:Cosine 0.6849 0.8235 0.7677 ?1.75%
5:NER overlap 0.7338 0.8341 0.7937 0.85%
6:Mihalcea-JC 0.7103 0.8301 0.7818 ?0.34%
7:IRsim 0.7161 0.8026 0.7677 ?1.74%
8:geosim 0.7185 0.8325 0.7865 0.14%
9:Spect. Dist 0.7243 0.8311 0.7880 0.28%
Table 2: Spanish task: ablation test.
Dataset Correlation
Complete (official + OnWN) 0.6687
Complete (only official) 0.5083
deft-forum 0.4544
deft-news 0.6402
headlines 0.6527
images 0.8094
OnWN (unofficial) 0.8039
tweet-news 0.5507
Table 3: English results (Official run + unofficial
OnWN).
4 Conclusions and Future Work
The introduced measures were studied on the
Spanish subtask, observing a limited contribu-
tion from geographic context similarity and spec-
tral distance. The IR-based measure introduced
in 2013 proved to be an important feature for
newswire-based datasets as in the 2013 English
task, even when trained on a training set derived
from automatic translation, which include many
errors. Our participation in the English subtask
was inconclusive due to the technical faults experi-
enced to produce our results. We will nevertheless
take into account the lessons learned in this partic-
ipation for future ones.
Acknowledgements
Part of this work has been carried out with the sup-
port of LabEx-EFL (Empirical Foundation of Lin-
guistics) strand 5 (computational semantic analy-
sis). We are also grateful to CoNACyT (Consejo
NAcional de Ciencia y Tecnologia) for support to
404
this work.
References
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
corpus version 1.1.
Davide Buscaldi and Paolo Rosso. 2008. Geo-
WordNet: Automatic Georeferencing of WordNet.
In Proceedings of the International Conference on
Language Resources and Evaluation, LREC 2008,
Marrakech, Morocco.
Davide Buscaldi, Paolo Rosso, Jos?e Manuel G?omez,
and Emilio Sanchis. 2009. Answering ques-
tions with an n-gram based passage retrieval engine.
Journal of Intelligent Information Systems (JIIS),
34(2):113?134.
Davide Buscaldi, Joseph Le Roux, Jorge J. Garcia Flo-
res, and Adrian Popescu. 2013. Lipn-core: Seman-
tic text similarity using n-grams, wordnet, syntac-
tic analysis, esa and information retrieval based fea-
tures. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 1: Pro-
ceedings of the Main Conference and the Shared
Task: Semantic Textual Similarity, pages 162?168,
Atlanta, Georgia, USA, June. Association for Com-
putational Linguistics.
Davide Buscaldi. 2013. Une mesure de similarit?e
s?emantique bas?ee sur la recherche d?information. In
5`eme Atelier Recherche d?Information SEmantique
- RISE 2013, pages 81?91, Lille, France, July.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/
?
cjlin/libsvm.
Damien Dudognon, Gilles Hubert, and Bachelin Jhonn
Victorino Ralalason. 2010. Proxig?en?ea : Une
mesure de similarit?e conceptuelle. In Proceedings of
the Colloque Veille Strat?egique Scientifique et Tech-
nologique (VSST 2010).
Andreas Eisele and Yu Chen. 2010. Multiun:
A multilingual corpus from united nation docu-
ments. In Daniel Tapias, Mike Rosner, Ste-
lios Piperidis, Jan Odjik, Joseph Mariani, Bente
Maegaard, Khalid Choukri, and Nicoletta Calzo-
lari (Conference Chair), editors, Proceedings of the
Seventh conference on International Language Re-
sources and Evaluation, pages 2868?2872. Euro-
pean Language Resources Association (ELRA), 5.
J.J. Jiang and D.W. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proc. of the Int?l. Conf. on Research in Computa-
tional Linguistics, pages 19?33.
Joseph Le Roux, Jennifer Foster, Joachim Wagner,
Rasul Samad Zadeh Kaljahi, and Anton Bryl.
2012. DCU-Paris13 Systems for the SANCL 2012
Shared Task. In The NAACL 2012 First Workshop
on Syntactic Analysis of Non-Canonical Language
(SANCL), pages 1?4, Montr?eal, Canada, June.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceedings
of the 21st national conference on Artificial intelli-
gence - Volume 1, AAAI?06, pages 775?780. AAAI
Press.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the re-
latedness of concepts. In Demonstration Papers at
HLT-NAACL 2004, HLT-NAACL?Demonstrations
?04, pages 38?41, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Bernhard Sch?olkopf, Peter Bartlett, Alex Smola, and
Robert Williamson. 1999. Shrinking the tube: a
new support vector regression algorithm. In Pro-
ceedings of the 1998 conference on Advances in neu-
ral information processing systems II, pages 330?
336, Cambridge, MA, USA. MIT Press.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, ACL ?94, pages 133?138, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
405
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 67?75,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Handling Unknown Words in Statistical Latent-Variable Parsing Models for
Arabic, English and French
Mohammed Attia, Jennifer Foster, Deirdre Hogan, Joseph Le Roux, Lamia Tounsi,
Josef van Genabith?
National Centre for Language Technology
School of Computing, Dublin City University
{mattia,jfoster,dhogan,jleroux,ltounsi,josef}@computing.dcu.ie
Abstract
This paper presents a study of the impact
of using simple and complex morphological
clues to improve the classification of rare and
unknown words for parsing. We compare
this approach to a language-independent tech-
nique often used in parsers which is based
solely on word frequencies. This study is ap-
plied to three languages that exhibit different
levels of morphological expressiveness: Ara-
bic, French and English. We integrate infor-
mation about Arabic affixes and morphotac-
tics into a PCFG-LA parser and obtain state-
of-the-art accuracy. We also show that these
morphological clues can be learnt automati-
cally from an annotated corpus.
1 Introduction
For a parser to do a reasonable job of analysing free
text, it must have a strategy for assigning part-of-
speech tags to words which are not in its lexicon.
This problem, also known as the problem of un-
known words, has received relatively little attention
in the vast literature on Wall-Street-Journal (WSJ)
statistical parsing. This is likely due to the fact that
the proportion of unknown words in the standard
English test set, Section 23 of the WSJ section of
Penn Treebank, is quite small. The problem mani-
fests itself when the text to be analysed comes from
a different domain to the text upon which the parser
has been trained, when the treebank upon which the
parser has been trained is limited in size and when
?Author names are listed in alphabetical order. For further
correspondence, contact L. Tounsi, D. Hogan or J. Foster.
the language to be parsed is heavily inflected. We
concentrate on the latter case, and examine the prob-
lem of unknown words for two languages which lie
on opposite ends of the spectrum of morphologi-
cal expressiveness and for one language which lies
somewhere in between: Arabic, English and French.
In our experiments we use a Berkeley-style latent-
variable PCFG parser and we contrast two tech-
niques for handling unknown words within the gen-
erative parsing model: one in which no language-
specific information is employed and one in which
morphological clues (or signatures) are exploited.
We find that the improvement accrued from look-
ing at a word?s morphology is greater for Arabic
and French than for English. The morphological
clues we use for English are taken directly from the
Berkeley parser (Petrov et al, 2006) and those for
French from recent work on French statistical pars-
ing with the Berkeley parser (Crabbe? and Candito,
2008; Candito et al, 2009). For Arabic, we present
our own set of heuristics to extract these signatures
and demonstrate a statistically significant improve-
ment of 3.25% over the baseline model which does
not employ morphological information.
We next try to establish to what extent these clues
can be learnt automatically by extracting affixes
from the words in the training data and ranking these
using information gain. We show that this automatic
method performs quite well for all three languages.
The paper is organised as follows: In Section 2
we describe latent variable PCFG parsing models.
This is followed in Section 3 by a description of our
three datasets, including statistics on the extent of
the unknown word problem in each. In Section 4, we
67
present results on applying a version of the parser
which uses a simple, language-agnostic, unknown-
word handling technique to our three languages. In
Section 5, we show how this technique is extended
to include morphological information and present
parsing results for English and French. In Section 6,
we describe the Arabic morphological system and
explain how we used heuristic rules to cluster words
into word-classes or signatures. We present parsing
results for the version of the parser which uses this
information. In Section 7, we describe our attempts
to automatically determine the signatures for a lan-
guage and present parsing results for the three lan-
guages. Finally, in Section 8, we discuss how this
work might be fruitfully extended.
2 Latent Variable PCFG Parsing
Johnson (1998) showed that refining treebank cate-
gories with parent information leads to more accu-
rate grammars. This was followed by a collection of
linguistically motivated propositions for manual or
semi-automatic modifications of categories in tree-
banks (Klein and Manning, 2003). In PCFG-LAs,
first introduced by Matsuzaki et al (2005), the re-
fined categories are learnt from the treebank us-
ing unsupervised techniques. Each base category
? and this includes part-of-speech tags ? is aug-
mented with an annotation that refines its distribu-
tional properties.
Following Petrov et al (2006) latent annotations
and probabilities for the associated rules are learnt
incrementally following an iterative process consist-
ing of the repetition of three steps.
1. Split each annotation of each symbol into n
(usually 2) new annotations and create rules
with the new annotated symbols. Estimate1 the
probabilities of the newly created rules.
2. Evaluate the impact of the newly created anno-
tations and discard the least useful ones. Re-
estimate probabilities with the new set of anno-
tations.
3. Smooth the probabilities to prevent overfitting.
We use our own parser which trains a PCFG-LA us-
ing the above procedure and parses using the max-
1Estimation of the parameters is performed by running Ex-
pectation/Maximisation on the training corpus.
rule parsing algorithm (Petrov et al, 2006; Petrov
and Klein, 2007). PCFG-LA parsing is relatively
language-independent but has been shown to be very
effective on several languages (Petrov, 2009). For
our experiments, we set the number of iterations to
be 5 and we test on sentences less than or equal to
40 words in length. All our experiments, apart from
the final one, are carried out on the development sets
of our three languages.
3 The Datasets
Arabic We use the the Penn Arabic Treebank
(ATB) (Bies and Maamouri, 2003; Maamouri and
Bies., 2004). The ATB describes written Modern
Standard Arabic newswire and follows the style and
guidelines of the English Penn-II treebank. We use
the part-of-speech tagset defined by Bikel and Bies
(Bikel, 2004). We employ the usual treebank split
(80% training, 10% development and 10% test).
English We use the Wall Street Journal section of
the Penn-II Treebank (Marcus et al, 1994). We train
our parser on sections 2-21 and use section 22 con-
catenated with section 24 as our development set.
Final testing is carried out on Section 23.
French We use the French Treebank (Abeille? et
al., 2003) and divide it into 80% for training, 10%
for development and 10% for final results. We fol-
low the methodology defined by Crabbe? and Can-
dito (2008): compound words are merged and the
tagset consists of base categories augmented with
morphological information in some cases2.
Table 1 gives basic unknown word statistics for
our three datasets. We calculate the proportion of
words in our development sets which are unknown
or rare (specified by the cutoff value) in the corre-
sponding training set. To control for training set
size, we also provide statistics when the English
training set is reduced to the size of the Arabic and
French training sets and when the Arabic training set
is reduced to the size of the French training set. In an
ideal world where training set sizes are the same for
all languages, the problem of unknown words will
be greatest for Arabic and smallest for English. It is
2This is called the CC tagset: base categories with verbal
moods and extraction features
68
language cutoff #train #dev #unk %unk language #train #dev #unk %unk
Arabic 0 594,683 70,188 3794 5.40 Reduced English 597,999 72,970 2627 3.60
- 1 - - 6023 8.58 (Arabic Size) - - 3849 5.27
- 5 - - 11,347 16.17 - - - 6700 9.18
- 10 - - 15,035 21.42 - - - 9083 12.45
English 0 950,028 72,970 2062 2.83 Reduced Arabic 266,132 70,188 7027 10.01
- 1 - - 2983 4.09 (French Size) - - 10,208 14.54
- 5 - - 5306 7.27 - - - 16,977 24.19
- 10 - - 7230 9.91 - - - 21,434 30.54
French 0 268,842 35,374 2116 5.98 Reduced English 265,464 72,970 4188 5.74
- 1 - - 3136 8.89 (French Size) - - 5894 8.08
- 5 - - 5697 16.11 - - - 10,105 13.85
- 10 - - 7584 21.44 - - - 13,053 17.89
Table 1: Basic Unknown Word Statistics for Arabic, French and English
reasonable to assume that the levels of inflectional
richness have a role to play in these differences.
4 A Simple Lexical Probability Model
The simplest method for handling unknown words
within a generative probabilistic parsing/tagging
model is to reserve a proportion of the lexical rule
probability mass for such cases. This is done by
mapping rare words in the training data to a spe-
cial UNKNOWN terminal symbol and estimating rule
probabilities in the usual way. We illustrate the pro-
cess with the toy unannotated PCFG in Figures 1
and 2. The lexical rules in Fig. 1 are the original
rules and the ones in Fig. 2 are the result of apply-
ing the rare-word-to-unknown-symbol transforma-
tion. Given the input sentence The shares recovered,
the word recovered is mapped to the UNKNOWN to-
ken and the three edges corresponding to the rules
NNS ? UNKNOWN, V BD ? UNKNOWN and
JJ ? UNKNOWN are added to the chart at this posi-
tion. The disadvantage of this simple approach is ob-
vious: all unknown words are treated equally and the
tag whose probability distribution is most dominated
by rare words in the training will be deemed the
most likely (JJ for this example), regardless of the
characteristics of the individual word. Apart from
its ease of implementation, its main advantage is its
language-independence - it can be used off-the-shelf
for any language for which a PCFG is available.3
One parameter along which the simple lexical
3Our simple lexical model is equivalent to the Berkeley sim-
pleLexicon option.
probability model can vary is the threshold used to
decide whether a word in the training data is rare or
?unknown?. When the threshold is set to n, a word
in the training data is considered to be unknown if it
occurs n or fewer times. We experiment with three
thresholds: 1, 5 and 10. The result of this experi-
ment for our three languages is shown in Table 2.
The general trend we see in Table 2 is that the
number of training set words considered to be un-
known should be minimized. For all three lan-
guages, the worst performing grammar is the one
obtained when the threshold is increased to 10. This
result is not unexpected. With this simple lexical
probability model, there is a trade-off between ob-
taining good guesses for words which do not occur
in the training data and obtaining reliable statistics
for words which do. The greater the proportion of
the probability mass that we reserve for the unknown
word section of the grammar, the more performance
suffers on the known yet rare words since these are
the words which are mapped to the UNKNOWN sym-
bol. For example, assume the word restructuring oc-
curs 10 times in the training data, always tagged as
a VBG. If the unknown threshold is less than ten and
if the word occurs in the sentence to be parsed, a
VBG edge will be added to the chart at this word?s
position with the probability 10/#VBG. If, however,
the threshold is set to 10, the word (in the training set
and the input sentence) will be mapped to UNKNOWN
and more possibilities will be explored (an edge for
each TAG ? UNKNOWN rule in the grammar). We
can see from Table 1 that at threshold 10, one fifth
69
VBD -> fell 50/153
VBD -> reoriented 2/153
VBD -> went 100/153
VBD -> latched 1/153
NNS -> photofinishers 1/201
NNS -> shares 200/201
JJ -> financial 20/24
JJ -> centrist 4/24
DT -> the 170/170
Figure 1: The original toy PCFG
VBD -> fell 50/153
VBD -> UNKNOWN 3/153
VBD -> went 100/153
NNS -> UNKNOWN 1/201
NNS -> shares 200/201
JJ -> financial 20/24
JJ -> UNKNOWN 4/24
DT -> the 170/170
Figure 2: Rare ? UNKNOWN
VBD -> fell 50/153
VBD -> UNK-ed 3/153
VBD -> went 100/153
NNS -> UNK-s 1/201
NNS -> shares 200/201
JJ -> financial 20/24
JJ -> UNK-ist 4/24
DT -> the 170/170
Figure 3: Rare ? UN-
KNOWN+SIGNATURE
Unknown Threshold Recall Precision F-Score Tagging Accuracy
Arabic
1 78.60 80.49 79.53 94.03
5 77.17 79.81 78.47 91.16
10 75.32 78.69 76.97 89.06
English
1 89.20 89.73 89.47 95.60
5 88.91 89.74 89.33 94.66
10 88.00 88.97 88.48 93.61
French
1 83.60 84.17 83.88 94.90
5 82.31 83.10 82.70 92.99
10 80.87 82.05 81.45 91.56
Table 2: Varying the Unknown Threshold with the Simple Lexical Probability Model
of the words in the Arabic and French development
sets are unknown, and this is reflected in the drop in
parsing performance at these thresholds.
5 Making use of Morphology
Unknown words are not all the same. We exploit this
fact by examining the effect on parsing accuracy of
clustering rare training set words using cues from
the word?s morphological structure. Affixes have
been shown to be useful in part-of-speech tagging
(Schmid, 1994; Tseng et al, 2005) and have been
used in the Charniak (Charniak, 2000), Stanford
(Klein and Manning, 2003) and Berkeley (Petrov et
al., 2006) parsers. In this section, we contrast the
effect on parsing accuracy of making use of such in-
formation for our three languages of interest.
Returning to our toy English example in Figures 1
and 2, and given the input sentence The shares re-
covered, we would like to use the fact that the un-
known word recovered ends with the past tense
suffix -ed to boost the probability of the lexical
rule V BD ? UNKNOWN. If we specialise the
UNKNOWN terminal using information from English
morphology, we can do just that, resulting in the
grammar in Figure 3. Now the word recovered is
mapped to the symbol UNK-ed and the only edge
which is added to the chart at this position is the one
corresponding to the rule V BD ? UNK-ed.
For our English experiments we use the unknown
word classes (or signatures) which are used in the
Berkeley parser. A signature indicates whether a
words contains a digit or a hyphen, if a word starts
with a capital letter or ends with one of the following
English suffixes (both derivational and inflectional):
-s, -ed, -ing, -ion, -er, -est, -ly, -ity, -y and -al.
For our French experiments we employ the same
signature list as Crabbe? and Candito (2008), which
itself was adapted from Arun and Keller (2005).
This list consists of (a) conjugation suffixes of regu-
70
lar verbs for common tenses (eg. -ons, -ez, -ent. . . )
and (b) derivational suffixes for nouns, adverbs and
adjectives (eg. -tion, -ment, -able. . . ).
The result of employing signature information
for French and English is shown in Table 3. Be-
side each f-score the absolute improvement over the
UNKNOWN baseline (Table 2) is given. For both
languages there is an improvement at all unknown
thresholds. The improvement for English is statis-
tically significant at unknown thresholds 1 and 10.4
The improvement is more marked for French and is
statistically significant at all levels.
In the next section, we experiment with signature
lists for Arabic.5
6 Arabic Signatures
In order to use morphological clues for Arabic we
go further than just looking at suffixes. We exploit
all the richness of the morphology of this language
which can be expressed through morphotactics.
6.1 Handling Arabic Morphotactics
Morphotactics refers to the way morphemes com-
bine together to form words (Beesley, 1998; Beesley
and Karttunen, 2003). Generally speaking, morpho-
tactics can be concatenative, with morphemes either
prefixed or suffixed to stems, or non-concatenative,
with stems undergoing internal alternations to con-
vey morphosyntactic information. Arabic is consid-
ered a typical example of a language that employs
non-concatenative morphotactics.
Arabic words are traditionally classified into three
types: verbs, nouns and particles. Adjectives take
almost all the morphological forms of, and share the
same templatic structures with, nouns. Adjectives,
for example, can be definite, and are inflected for
case, number and gender.
There are a number of indicators that tell us
whether the word is a verb or a noun. Among
4Statistical significance was determined using the strati-
fied shuffling method. The software used to perform the test
was downloaded from http://www.cis.upenn.edu/
?
dbikel/software.html.
5An inspection of the Berkeley Arabic grammar (available
at http://code.google.com/p/berkeleyparser/
downloads/list) shows that no Arabic-specific signatures
were employed. The Stanford parser uses 9 signatures for Ara-
bic, designed for use with unvocalised text. An immediate fu-
ture goal is to test this signature list with our parser.
these indicators are prefixes, suffixes and word tem-
plates. A template (Beesley and Karttunen, 2003) is
a kind of vocalization mould in which a word fits. In
derivational morphology Arabic words are formed
through the amalgamation of two tiers, namely, root
and template. A root is a sequence of three (rarely
two or four) consonants which are called radicals,
and the template is a pattern of vowels, or a com-
bination of consonants and vowels, with slots into
which the radicals of the root are inserted.
For the purpose of detection we use the reverse
of this information. Given that we have a word, we
try to extract the stem, by removing prefixes and suf-
fixes, and match the word against a number of verbal
and nominal templates. We found that most Ara-
bic templatic structures are in complementary dis-
tribution, i.e. they are either restricted to nominal
or verbal usage, and with simple regular expression
matching we can decide whether a word form is a
noun or a verb.
6.2 Noun Indicators
In order to detect that a word form is a noun (or ad-
jective), we employ heuristic rules related to Arabic
prefixes/suffixes and if none of these rules apply we
attempt to match the word against templatic struc-
tures. Using this methodology, we are able to detect
95% of ATB nouns.6
We define a list of 42 noun templates which are
used to indicate active/passive participle nouns, ver-
bal nouns, nouns of instrument and broken plural
nouns (see Table 4 for some examples). Note that
templates ending with taa marboutah ?ap? or start-
ing with meem madmoumah ?mu? are not consid-
ered since they are covered by our suffix/prefix rules,
which are as follows:
1- The definite article prefix ?  or in Buckwalter
transliteration ?Al?.
2- The tanween suffix

, 

,

 or ?N?, ?F?, ?K?, ?AF?.
3- The feminine plural suffix HA, or ?+At?.
4- The taa marboutah ending ? or ?ap? whether as a
6The heuristics we developed are designed to work on dia-
critized texts. Although diacritics are generally ignored in mod-
ern writing, the issue of restoring diacritics has been satisfac-
torily addressed by different researchers. For example, Nelken
and Shieber (2005) presented an algorithm for restoring diacrit-
ics to undiacritized MSA texts with an accuracy of over 90%
and Habasah et al (2009) reported on a freely-available toolkit
(MADA-TOKAN) an accuracy of over 96%.
71
Unknown Threshold Recall Precision F-Score Tagging Accuracy
Arabic
1 80.67 82.19 *81.42 (+ 1.89) 96.32
5 80.66 82.81 *81.72 (+ 3.25) 95.15
10 79.86 82.49 *81.15 (+ 4.18) 94.38
English
1 ***89.64 89.95 89.79 (+ 0.32) 96.44
5 89.16 89.80 89.48 (+ 0.15) 96.32
10 89.14 89.78 **89.46 (+ 0.98) 96.21
French
1 85.15 85.77 *85.46 (+ 1.58) 96.13
5 84.08 84.80 *84.44 (+ 1.74) 95.54
10 84.21 84.78 *84.49 (+ 3.04) 94.68
Table 3: Baseline Signatures for Arabic, French and English
statistically significant with *:p < 10?4, **: p < 10?3, ***: p < 0.004,
Template Name Regular Specification
Arabic Buckwalter Expression
?A

?
	
?

	
K 

{inofiEAl {ino.i.A. verbal noun (masdar)
?A

?
	
??

mifoEAl mi.o.A. noun instrument
??

	
?


J?

? musotafoEil musota.o.i. noun participle
?J


?

A

	
?

? mafAEiyl ma.A.iy. noun plural
?

?
	
?


J?

{isotafoEal {isota.o.a. verb
??

?

	
? fuwEil .uw.i. verb passive
Table 4: Sample Arabic Templatic Structures for Nouns and Verbs
feminine marker suffix or part of the word.
5- The genitive case marking kasrah 

, or ?+i?.
6- Words of length of at least five characters ending
with doubled yaa ?


or ?y??.
7- Words of length of at least six characters ending
with alif mamdoudah and hamzah Z  or ?A??.
8- Words of length of at least seven characters start-
ing with meem madmoumah ? or ?mu?.
6.3 Verb Indicators
In the same way, we define a list of 16 templates and
we combine them with heuristic rules related to Ara-
bic prefixes/suffixes to detect whether a word form
is exclusively a verb. The prefix/suffix heuristics are
as follows:
9-The plural marker suffix  ? or ?uwA? indicates a
verb.
10- The prefixes H , ?


,
	
?
,



,

? or ?sa?, ?>a?,
?>u?, ?na?, ?nu?, ?ya?, ?yu?, ?ta?, ?tu? indicate im-
prefective verb.
The verbal templates are less in number than the
noun templates yet they are no less effective in de-
tecting the word class (see Table 4 for examples).
Using these heuristics we are able to detect 85% of
ATB verbs.
6.4 Arabic Signatures
We map the 72 noun/verb classes that are identi-
fied using our hand-crafted heuristics into sets of
signatures of varying sizes: 4, 6, 14, 21, 25, 28
and 72. The very coarse-grained set considers just
4 signatures UNK-noun, UNK-verb, UNK-num,
and UNK and the most fine-grained set of 72 signa-
tures associates one signature per heuristic. In ad-
dition, we have evaluated the effect of reordering
rules and templates and also the effect of collating
all signatures satisfying an unknown word. The re-
sults of using these various signatures sets in parsing
72
UNK
NUM NOUN VERB
digits (see section 6.2) (see section 6.3)
Al definiteness tashkil At suffix ap suffix imperfect
rule 1 rules 2 and 5 rule 3 rule 4 rule 10
y? suffix A? suffix mu prefix verbal noun templates suffixes
rule 6 rule 7 rule 8 3 groupings dual/plural suffixes
plural templates participle active templates participle passive templates instrument templates passive templates
4 groupings
other templates verbal templates
5 groupings
Table 6: Arabic signatures
Cutoff 1 5 10
4 80.78 80.71 80.09
6 81.14 81.16 81.06
14 80.88 81.45 81.19
14 reorder 81.39 81.01 80.81
21 81.38 81.55 81.35
21 reorder 81.20 81.13 80.58
21 collect 80.94 80.56 79.63
25 81.18 81.25 81.26
28 81.42 81.72 (+ 3.25) 81.15
72 79.64 78.87 77.58
Table 5: Baseline Signatures for Arabic
our Arabic development set are presented in Table 5.
We achieve our best labeled bracketing f-score using
28 signatures with an unknown threshold of five. In
fact we get an improvement of 3.25% over using no
signatures at all (see Table 2). Table 3 describes in
more detail the scores obtained using the 28 signa-
tures present in Table 6. Apart from the set contain-
ing 72 signatures, all of the baseline signature sets in
Table 5 yield a statistically significant improvement
over the generic UNKNOWN results (p < 10?4).
7 Using Information Gain to Determine
Signatures
It is clear that dividing the UNKNOWN terminal into
more fine-grained categories based on morpholog-
ical information helps parsing for our three lan-
guages. In this section we explore whether useful
morphological clues can be learnt automatically. If
they can, it means that a latent-variable PCFG parser
can be adapted to any language without knowledge
of the language in question since the only language-
specific component in such a parser is the unknown-
signature specification.
In a nutshell, we extract affix features from train-
ing set words7 and then use information gain to rank
these features in terms of their predictive power in a
POS-tagging task. The features deemed most dis-
criminative are then used as signatures, replacing
our baseline signatures described in Sections 5 and
6. We are not going as far as actual POS-tagging,
but rather seeing whether the affixes that make good
features for a part-of-speech tagger also make good
unknown word signatures.
We experiment with English and French suffixes
of length 1-3 and Arabic prefixes and suffixes of var-
ious lengths as well as stem prefixes and suffixes of
length 2, 4 and 6. For each of our languages we
experiment with several information gain thresholds
on our development sets and we fix on an English
signature list containing 24 suffixes, a French list
containing 48 suffixes and an Arabic list containing
38 prefixes and suffixes.
Our development set results are presented in Ta-
ble 7. For all three languages, the information gain
signatures perform at a comparable level to the base-
line hand-crafted signatures (Table 3). For each
of the three unknown-word handling techniques, no
signature (UNKNOWN), hand-crafted signatures and
information gain signatures, we select the best un-
known threshold for each language?s development
set and apply these grammars to our test sets. The
f-scores are presented in Table 8, along with the up-
per bounds obtained by parsing with these grammars
in gold-tag mode. For French, the effect of tagging
accuracy on overall parse accuracy is striking. The
improvements that we get from using morphological
signatures are greatest for Arabic8 and smallest for
7We omit all function words and high frequency words be-
cause we are interested in the behaviour of words which are
likely to be similar to rare words.
8Bikel?s parser trained on the same Arabic data and tested
on the same input achieves an f-score of 76.50%. We trained
a 5-split-merge-iteration Berkeley grammar and parsed with the
73
Unknown Threshold Recall Precision F-Score Tagging Accuracy
Arabic IG
1 80.10 82.15 *81.11 (+ 1.58) 96.53
5 80.03 82.49 *81.32 (+ 2.85) 95.30
10 80.17 82.40 *81.27 (+ 4.3) 94.66
English IG
1 89.38 89.87 89.63 (+ 0.16) 96.45
5 89.54 90.22 ***89.88 (+ 0.55) 96.41
10 89.22 90.05 *89.63 (+ 1.15) 96.19
French IG
1 84.78 85.36 *85.07 (+ 1.19) 96.17
5 84.63 85.24 **84.93 (+ 2.23) 95.30
10 84.18 84.80 *84.49 (+ 3.09) 94.68
Table 7: Information Gain Signature Results
statistically significant with *:p < 10?4, **: p < 2 ? 10?4, ***: p < 0.005
Language No Sig Baseline Sig IG Sig
Arabic 78.34 *81.59 *81.33
Arabic Gold Tag 81.46 82.43 81.90
English 89.48 89.65 89.77
English Gold Tag 89.94 90.10 90.23
French 83.74 *85.77 **85.55
French Gold Tag 88.82 88.41 88.86
statistically significant with *: p < 10?4, **: p < 10?3
Table 8: F-Scores on Test Sets
English. The results for the information gain signa-
tures are promising and warrant further exploration.
8 Conclusion
We experiment with two unknown-word-handling
techniques in a statistical generative parsing model,
applying them to Arabic, French and English. One
technique is language-agnostic and the other makes
use of some morphological information (signatures)
in assigning part-of-speech tags to unknown words.
The performance differences from the two tech-
niques are smallest for English, the language with
the sparsest morphology of the three and the small-
est proportion of unknown words in its development
set. As a result of carrying out these experiments,
we have developed a list of Arabic signatures which
can be used with any statistical parser which does
Berkeley parser, achieving an f-score of 75.28%. We trained the
Berkeley parser with the -treebank SINGLEFILE option so that
English signatures were not employed.
its own tagging. We also present results which show
that signatures can be learnt automatically.
Our experiments have been carried out using gold
tokens. Tokenisation is an issue particularly for Ara-
bic, but also for French (since the treebank contains
merged compounds) and to a much lesser extent for
English (unedited text with missing apostrophes). It
is important that the experiments in this paper are re-
peated on untokenised text using automatic tokeni-
sation methods (e.g. MADA-TOKAN).
The performance improvements that we demon-
strate for Arabic unknown-word handling are obvi-
ously just the tip of the iceberg in terms of what can
be done to improve performance on a morpholog-
ically rich language. The simple generative lexical
probability model we use can be improved by adopt-
ing a more sophisticated approach in which known
and unknown word counts are combined when esti-
mating lexical rule probabilities for rare words (see
Huang and Harper (2009) and the Berkeley sophis-
ticatedLexicon training option). Further work will
also include making use of a lexical resource exter-
nal to the treebank (Goldberg et al, 2009; Habash,
2008) and investigating clustering techniques to re-
duce data sparseness (Candito and Crabbe?, 2009).
Acknowledgements
This research is funded by Enterprise Ireland
(CFTD/07/229 and PC/09/037) and the Irish Re-
search Council for Science Engineering and Tech-
nology (IRCSET). We thank Marie Candito and our
three reviewers for their very helpful suggestions.
74
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel,
2003. Treebanks: Building and Using Parsed
Corpora, chapter Building a Treebank for French.
Kluwer, Dordrecht.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
French. In ACL. The Association for Computer Lin-
guistics.
Kenneth R. Beesley and Lauri Karttunen. 2003. Finite
State Morphology. CSLI studies in computational lin-
guistics.
Kenneth R. Beesley. 1998. Arabic morphology using
only finite-state operations. In The Workshop on Com-
putational Approaches to Semitic Languages.
Ann Bies and Mohammed Maamouri. 2003. Penn Ara-
bic Treebank guidelines. Technical Report TB-1-28-
03.
Dan Bikel. 2004. On the Parameter Space of Generative
Lexicalized Parsing Models. Ph.D. thesis, University
of Pennslyvania.
Marie Candito and Benoit Crabbe?. 2009. Improving gen-
erative statistical parsing with semi-supervised word
clustering. In Proceedings of IWPT?09.
Marie Candito, Beno??t Crabbe?, and Djame? Seddah. 2009.
On statistical parsing of French with supervised and
semi-supervised strategies. In Proceedings of the
EACL 2009 Workshop on Computational Linguis-
tic Aspects of Grammatical Inference, pages 49?57,
Athens, Greece, March.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the Annual Meeting of the
North American Association for Computational Lin-
guistics (NAACL-00), pages 132?139, Seattle, Wash-
ington.
Beno??t Crabbe? and Marie Candito. 2008. Expe?riences
d?analyse syntaxique statistique du franc?ais. In Actes
de TALN.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities.
In EACL, pages 327?335. The Association for Com-
puter Linguistics.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
Mada+tokan: A toolkit for Arabic tokenization, di-
acritization, morphological disambiguation, pos tag-
ging, stemming and lemmatization. In Proceedings of
the 2nd International Conference on Arabic Language
Resources and Tools (MEDAR).
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in arabic-english statistical
machine translation. In Proceedings of Association for
Computational Linguistics, pages 57?60.
Zhongqiang Huang and Mary Harper. 2009. Self-
training pcfg grammars with latent annotations across
languages. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
Singapore, August.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Dan Klein and Chris Manning. 2003. Accurate unlex-
icalised parsing. In Proceedings of the 41st Annual
Meeting of the ACL.
Mohammed Maamouri and Ann Bies. 2004. Developing
an Arabic Treebank: Methods, guidelines, procedures,
and tools. In Workshop on Computational Approaches
to Arabic Script-based Languages, COLING.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
Proceedings of the 1994 ARPA Speech and Natural
Language Workshop, pages 114?119, Princeton, New
Jersey.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the ACL,
pages 75?82, Ann Arbor, June.
Rani Nelken and Stuart M. Shieber. 2005. Arabic dia-
critization using weighted finite-state transducers. In
ACL-05 Workshop on Computational Approaches to
Semitic Languages.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL, Rochester, NY, April.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the ACL, Sydney,
Australia, July.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Berkeley, Berkeley, CA, USA.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing (NeMLaP-1), pages 44?49.
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morphological features help POS tagging
of unknown words across language varieties. In Pro-
ceedings of the Fourth SIGHAN Workshop on Chinese
Language Processing.
75
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 55?61,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Statistical Parsing of Spanish and Data Driven Lemmatization
Joseph Le Roux? Beno?t Sagot? Djam? Seddah?,?
? Laboratoire d?Informatique Paris Nord, Universit? Paris Nord, UMR CNRS 7030
?Alpage, INRIA & Universit? Paris Diderot
? Universit? Paris Sorbonne
leroux@univ-paris13.fr, benoit.sagot@inria.fr, djame.seddah@paris-sorbonne.fr
Abstract
Although parsing performances have greatly
improved in the last years, grammar inference
from treebanks for morphologically rich lan-
guages, especially from small treebanks, is
still a challenging task. In this paper we in-
vestigate how state-of-the-art parsing perfor-
mances can be achieved on Spanish, a lan-
guage with a rich verbal morphology, with a
non-lexicalized parser trained on a treebank
containing only around 2,800 trees. We rely
on accurate part-of-speech tagging and data-
driven lemmatization to provide parsing mod-
els able to cope lexical data sparseness. Pro-
viding state-of-the-art results on Spanish, our
methodology is applicable to other languages
with high level of inflection.
1 Introduction
Grammar inference from treebanks has become the
standard way to acquire rules and weights for pars-
ing devices. Although tremendous progress has
been achieved in this domain, exploiting small tree-
banks is still a challenging task, especially for lan-
guages with a rich morphology. The main difficulty
is to make good generalizations from small exam-
ple sets exhibiting data sparseness. This difficulty
is even greater when the inference process relies
on semi-supervised or unsupervised learning tech-
niques which are known to require more training ex-
amples, as these examples do not explicitly contain
all the information.
In this paper we want to explore how we can cope
with this difficulty and get state-of-the-art syntac-
tic analyses with a non-lexicalized parser that uses
modern semisupervised inference techniques. We
rely on accurate data-driven lemmatization and part-
of-speech tagging to reduce data sparseness and ease
the burden on the parser. We try to see how we
can improve parsing structure predictions solely by
modifying the terminals and/or the preterminals of
the trees. We keep the rest of the tagset as is.
In order to validate our method, we perform ex-
periments on the Cast3LB constituent treebank for
Spanish (Castillan). This corpus is quite small,
around 3,500 trees, and Spanish is known to have
a rich verbal morphology, making the tag set quite
complex and difficult to predict. Cowan and Collins
(2005) and Chrupa?a (2008) already showed inter-
esting results on this corpus that will provide us with
a comparison for this work, especially on the lexical
aspects as they used lexicalized frameworks while
we choose PCFG-LAs.
This paper is structured as follows. In Section 2
we describe the Cast3LB corpus in details. In Sec-
tion 3 we present our experimental setup and results
which we discuss and compare in Section 4. Finally,
Section 5 concludes the presentation.
2 Data Set
The Castillan 3LB treebank (Civit and Mart?, 2004)
contains 3,509 constituent trees with functional an-
notations. It is divided in training (2,806 trees), de-
velopment (365 trees) and test (338 trees).
We applied the transformations of Chrupa?a
(2008) to the corpus where CP and SBAR nodes
are added to the subordinate and relative clauses but
we did not perform any other transformations, like
the coordination modification applied by Cowan and
Collins (2005).
The Cast3LB tag set is rich. In particular part-of-
speech (POS) tags are fine-grained and encode pre-
cise morphological information while non-terminal
tags describe subcategorization and function labels.
55
Without taking functions into account, there are 43
non-terminal tags. The total tag set thus comprises
149 symbols which makes the labeling task chal-
lenging.
The rich morphology of Spanish can be observed
in the treebank through word form variation. Table 1
shows some figures extracted from the corpus (train-
ing, development and test). In particular the word
form/lemma ratio is 1.54, which is similar to other
Romance language treebanks (French FTB and Ital-
ian ITB).
# of tokens 94 907
# of unique word forms 17 979
# of unique lemmas 11 642
ratio word form/lemma 1.54
Table 1: C3LB properties
Thus, we are confronted with a small treebank
with a rich tagset and a high word diversity. All
these conditions make the corpus a case in point for
building a parsing architecture for morphologically-
rich languages.
3 Experiments
We conducted experiments on the Cast3LB develop-
ment set in order to test various treebank modifica-
tions, that can be divided in two categories: (i) mod-
ification of the preterminal symbols of the treebank
by using simplified POS tagsets; (ii) modification of
the terminal symbols of the treebank by replacing
word tokens by lemmas.
3.1 Experimental Setup
In this section we describe the parsing formalism
and POS tagging settings used in our experiments.
PCFG-LAs To test our hypothesis, we use the
grammatical formalism of Probabilistic Context-
Free Grammars with Latent Annotations (PCFG-
LAs) (Matsuzaki et al, 2005; Petrov et al, 2006).
These grammars depart from the standard PCFGs by
automatically refining grammatical symbols during
the training phase, using unsupervised techniques.
They have been applied successfully to a wide range
of languages, among which French (Candito and
Seddah, 2010), German (Petrov and Klein, 2008),
Chinese and Italian (Lavelli and Corazza, 2009).
For our experiments, we used the LORG PCFG-
LA parser implementing the CKY algorithm. This
software also implements the techniques from Attia
et al (2010) for handling out-of-vocabulary words,
where interesting suffixes for part-of-speech tagging
are collected on the training set, ranked according
to their information gain with regards to the part-
of-speech tagging task. Hence, all the experiments
are presented in two settings. In the first one, called
generic, unknown words are replaced with a dummy
token UNK, while in the second one, dubbed IG, we
use the collected suffixes and typographical infor-
mation to type unknown words.1 We retained the 30
best suffixes of length 1, 2 and 3.
The grammar was trained using the algorithm
of Petrov and Klein (2007) using 3 rounds of
split/merge/smooth2 . For lexical rules, we applied
the strategy dubbed simple lexicon in the Berkeley
parser. Rare words ? words occurring less than 3
times in the training set ? are replaced by a special
token, which depends on the OOV handling method
(generic or IG), before collecting counts.
POS tagging We performed parsing experiments
with three different settings regarding POS infor-
mation provided as an input to the parser: (i) with
no POS information, which constitutes our base-
line; (ii) with gold POS information, which can be
considered as a topline for a given parser setting;
(iii) with POS information predicted using the MElt
POS-tagger (Denis and Sagot, 2009), using three
different tagsets that we describe below.
MElt is a state-of-the-art sequence labeller that
is trained on both an annotated corpus and an ex-
ternal lexicon. The standard version of MElt relies
on Maximum-Entropy Markov models (MEMMs).
However, in this work, we have used a multiclass
perceptron instead, as it allows for much faster train-
ing with very small performance drops (see Table 2).
For training purposes, we used the training section
of the Cast3LB (76,931 tokens) and the Leffe lexi-
con (Molinero et al, 2009), which contains almost
800,000 distinct (form, category) pairs.3
We performed experiments using three different
1Names generic and IG originally come from Attia et al
(2010).
2We tried to perform 4 and 5 rounds but 3 rounds proved to
be optimal on this corpus.
3Note that MElt does not use information from the exter-
56
TAGSET baseline reduced2 reduced3
Nb. of tags 106 42 57
Multiclass Perceptron
Overall Acc. 96.34 97.42 97.25
Unk. words Acc. 91.17 93.35 92.30
Maximum-Entropy Markov model (MEMM)
Overall Acc. 96.46 97.42 97.25
Unk. words Acc. 91.57 93.76 92.87
Table 2: MElt POS tagging accuracy on the Cast3LB
development set for each of the three tagsets. We pro-
vide results obtained with the standard MElt algorithm
(MEMM) as well as with the multiclass perceptron, used
in this paper, for which training is two orders of magni-
tude faster. Unknown words represent as high as 13.5 %
of all words.
tagsets: (i) a baseline tagset which is identical
to the tagset used by Cowan and Collins (2005)
and Chrupa?a (2008); with this tagset, the training
corpus contains 106 distinct tags;
(ii) the reduced2 tagset, which is a simplification
of the baseline tagset: we only retain the first two
characters of each tag from the baseline tagset; with
this tagset, the training corpus contains 42 distinct
tags;
(iii) the reduced3 tagset, which is a variant of
the reduced2 tagset: contrarily to the reduced2
tagset, the reduced3 tagset has retained the mood
information for verb forms, as it proved relevant
for improving parsing performances as shown by
(Cowan and Collins, 2005); with this tagset, the
training corpus contains 57 distinct tags.
Melt POS tagging accuracy on the Cast3LB de-
velopment set for these three tagsets is given in Ta-
ble 2, with overall figures together with figures com-
puted solely on unknown words (words not attested
in the training corpus, i.e., as high as 13.5 % of all
tokens).
3.2 Baseline
The first set of experiments was conducted with the
baseline POS tagset. Results are summarized in Ta-
ble 3. This table presents parsing statistics on the
Cast3LB development set in the 3 POS settings in-
nal lexicon as constraints, but as features. Therefore, the set of
categories in the external lexicon need not be identical to the
tagset. In this work, the Leffe categories we used include some
morphological information (84 distinct categories).
troduced above (i) no POS provided, (ii) gold POS
provided and (iii) predicted POS provided. For each
POS tagging setting it shows labeled precision, la-
beled recall, labeled F1-score, the percentage of ex-
act match and the POS tagging accuracy. The latter
needs not be the same as presented in Section 3.1 be-
cause (i) punctuation is ignored and (ii) if the parser
cannot use the information provided by the tagger,
it is discarded and the parser performs POS-tagging
on its own.
MODEL LP LR F1 EXACT POS
Word Only
Generic 81.42 81.04 81.23 14.47 90.89
IG 80.15 79.60 79.87 14.19 85.01
Gold POS
Generic 87.83 87.49 87.66 30.59 99.98
IG 86.78 86.53 86.65 27.96 99.98
Pred. POS
Generic 84.47 84.39 84.43 22.44 95.82
IG 83.60 83.66 83.63 21.78 95.82
Table 3: Baseline PARSEVAL scores on Cast3LB dev. set
(? 40 words)
As already mentioned above, this tagset contains
106 distinct tags. On the one hand it means that POS
tags contain useful information. On the other hand it
also means that the data is already sparse and adding
more sparseness with the IG suffixes and typograph-
ical information is detrimental. This is a major dif-
ference between this POS tagset and the two follow-
ing ones.
3.3 Using simplified tagsets
We now turn to the modified tagsets and measure
their impact on the quality of the syntactic analyses.
Results are summarized in Table 4 for the reduced2
tagset and in Table 5 for reduced3. In these two set-
tings, we can make the following remarks.
? Parsing results are better with reduced3, which
indicates that verbal mood is an important fea-
ture for correctly categorizing verbs at the syn-
tactic level.
? When POS tags are not provided, using suffixes
and typographical information improves OOV
word categorization and leads to a better tag-
ging accuracy and F1 parsing score (78.94 vs.
81.81 for reduced2 and 79.69 vs. 82.44 for re-
duced3).
57
? When providing the parser with POS tags,
whether gold or predicted, both settings show
an interesting difference w.r.t. to unknown
words handling. When using reduced2, the IG
setting is better than the generic one, whereas
the situation is reversed in reduced3. This indi-
cates that reduced2 is too coarse to help finely
categorizing unknown words and that the re-
finement brought by IG is beneficial, however
the added sparseness. For reduced3 it is diffi-
cult to say whether it is the added richness of
the POS tagset or the induced OOV sparseness
that explains why IG is detrimental.
MODEL LP LR F1 EXACT POS
Word Only
Generic 78.86 79.02 78.94 15.23 88.18
IG 81.89 81.72 81.81 16.17 92.19
Gold POS
Generic 86.56 85.90 86.23 26.64 100.00
IG 86.90 86.63 86.77 29.28 100.00
Pred. POS
Generic 84.16 83.81 83.99 21.05 96.76
IG 84.57 84.32 84.45 21.38 96.76
Table 4: PARSEVAL scores on Cast3LB development set
with reduced2 tagset (? 40 words)
MODEL LP LR F1 EXACT POS
Word Only
Generic 79.61 79.78 79.69 14.90 87.29
IG 82.57 82.31 82.44 14.24 91.63
Gold POS
Generic 88.08 87.69 87.89 30.59 100.00
IG 87.56 87.31 87.43 29.61 100.00
Pred. POS
Generic 85.56 85.38 85.47 23.03 96.56
IG 85.32 85.24 85.28 23.36 96.56
Table 5: PARSEVAL scores on Cast3LB development set
with reduced3 tagset (? 40 words)
3.4 Lemmatization Impact
Being a morphologically rich language, Spanish ex-
hibits a high level of inflection similar to several
other Romance languages, for example French and
Italian (gender, number, verbal mood). Furthermore,
Spanish belongs to the pro-drop family and clitic
pronouns are often affixed to the verb and carry
functional marks. This makes any small treebank
of this language an interesting play field for statis-
tical parsing. In this experiment, we want to use
lemmatization as a form of morphological cluster-
ing. To cope with the loss of information, we pro-
vide the parser with predicted POS. Lemmatization
is carried out by the morphological analyzer MOR-
FETTE, (Chrupa?a et al, 2008) while POS tagging
is done by the MElt tagger. Lemmatization perfor-
mances are on a par with previously reported results
on Romance languages (see Table 6)
TAGSET ALL SEEN UNK (13.84%)
baseline 98.39 99.01 94.55
reduced2 98.37 98.88 95.18
reduced3 98.24 98.88 94.23
Table 6: Lemmatization performance on the Cast3LB.
To make the parser less sensitive to lemmatization
and tagging errors, we train both tools on a 20 jack-
kniffed setup4. Resulting lemmas and POS tags are
then reinjected into the train set. The test corpora
is itself processed with tools trained on the unmod-
ified treebank. Results are presented Table 7. They
show an overall small gain, compared to the previ-
ous experiments but provide a clear improvement on
the richest tagset, which is the most difficult to parse
given its size (106 tags).
First, we remark that POS tagging accuracy with
the baseline tagset when no POS is provided is lower
than previously observed. This can be easily ex-
plained: it is more difficult to predict POS with mor-
phological information when morphological infor-
mation is withdrawn from input.
Second, and as witnessed before, reduction of the
POS tag sparseness using a simplified tagset and in-
crease of the lexical sparseness by handling OOV
words using typographical information have adverse
effects. This can be observed in the generic Pre-
dicted POS section of Table 7 where the baseline
tagset is the best option. On the other hand, in IG
Predicted POS, using the reduced3 is better than
baseline and reduced2. Again this tagset is a trade-
off between rich information and data sparseness.
4The training set is split in 20 chunks and each one is pro-
cessed with a tool trained on the 19 other chunks. This enables
the parser to be less sensitive to lemmatization and/or pos tag-
ging errors.
58
TAGSET LR LP F1 EX POS
Word Only ? Generic
baseline 79.70 80.51 80.1 15.23 74.04
reduced2 79.19 79.78 79.48 15.56 89.25
reduced3 79.92 80.03 79.97 13.16 87.67
Word Only ? IG
baseline 80.67 81.32 80.99 15.89 75.02
reduced2 80.54 81.3 80.92 15.13 90.93
reduced3 80.52 80.94 80.73 15.13 88.53
Pred. POS ? Generic
baseline 85.03 85.57 85.30 23.68 95.68
reduced2 83.98 84.73 84.35 23.36 96.78
reduced3 84.93 85.19 85.06 21.05 96.60
Pred. POS ? IG
baseline 84.60 85.06 84.83 23.68 95.68
reduced2 84.29 84.82 84.55 21.71 96.78
reduced3 84.86 85.39 85.12 22.70 96.60
Table 7: Lemmmatization Experiments
In all cases reduced2 is below the other tagsets
wrt. to Parseval F1 although tagging accuracy is bet-
ter. We can conclude that it is too poor from an in-
formational point of view.
4 Discussion
There is relatively few works actively pursued on
statistical constituency parsing for Spanish. The ini-
tial work of Cowan and Collins (2005) consisted
in a thorough study of the impact of various mor-
phological features on a lexicalized parsing model
(the Collins Model 1) and on the performance gain
brought by the reranker of Collins and Koo (2005)
used in conjunction with the feature set developed
for English. Direct comparison is difficult as they
used a different test set (approximately, the concate-
nation of our development and test sets). They report
an F-score of 85.1 on sentences of length less than
40.5
However, we are directly comparable with Chru-
pa?a (2008)6 who adapted the Collins Model 2 to
Spanish. As he was focusing on wide coverage LFG
grammar induction, he enriched the non terminal an-
notation scheme with functional paths rather than
trying to obtain the optimal tagset with respect to
pure parsing performance. Nevertheless, using the
5See http://pauillac.inria.fr/~seddah/
spmrl-spanish.html for details on comparison with that
work.
6We need to remove CP and SBAR nodes to be fairly com-
parable.
same split and providing gold POS, our system pro-
vides better performance (around 2.3 points better,
see Table 8).
It is of course not surprising for a PCFG-LA
model to outperform a Collins? model based lexi-
calized parser. However, it is a fact that, on such
small treebank configurations, PCFG-LA are cru-
cially lacking annotated data. It is only by greatly
reducing the POS tagset and using either a state-of-
the-art tagger or a lemmatizer (or both), that we can
boost our system performance.
The sensitivity of PCFG-LA models to lexical data
sparseness was also shown on French by Seddah
et al (2009). In fact they showed that perfor-
mance of state-of-the-art lexicalized parsers (Char-
niak, Collins models, etc.) were crossing that
of Berkeley parsers when the training set contains
around 2500?3000 sentences. Here, with around
2,800 sentences of training data, we are probably
in a setting where both parser types exhibit simi-
lar performances, as we suspect French and Spanish
to behave in the same way. It is therefore encour-
aging to notice that our approach, which relies on
accurate POS tagging and lemmatization, provides
state-of-the-art performance. Let us add that a simi-
lar method, involving only MORFETTE, was applied
with success to Italian within a PCFG-LA frame-
work and French with a lexicalized parser, both lead-
ing to promising results (Seddah et al, 2011; Seddah
et al, 2010).
5 Conclusion
We presented several experiments reporting the im-
pact of lexical sparseness reduction on non lexical-
ized statistical parsing. We showed that, by using
state-of-the-art lemmatization and POS tagging on
a reduced tagset, parsing performance can be on a
par with lexicalized models that manage to extract
more information from a small corpus exhibiting a
rich lexical diversity. It remains to be seen whether
applying the same kind of simplifications to the rest
of the tagset, i.e. on the internal nodes, can further
improve parse structure quality. Finally, the methods
we presented in this paper are not language specific
and can be applied to other languages if similar re-
sources exist.
59
TAGSET MODE TOKENS ALL ? 70 ? 40
reduced3 Gen. pred. POS 83.92 84.27 85.08
eval. w/o CP/SBAR 84.02 84.37 85.24
baseline IG pred. lemma & POS 84.15 84.40 85.26
eval. w/o CP/SBAR 84.34 84.60 85.45
reduced3 Gen. gold POS 86.21 86.63 87.84
eval. w/o CP/SBAR 86.35 86.77 88.01
baseline gold POS 83.96 84.58 ?
(Chrupa?a, 2008)
Table 8: PARSEVAL F-score results on the Cast3LB test set
Acknowledgments
Thanks to Grzegorz Chrupa?a and Brooke Cowan for
answering our questions and making data available
to us. This work is partly funded by the French Re-
search Agency (EDyLex, ANR-09-COORD-008).
References
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for arabic, english and
french. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Marie Candito and Djam? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with morfette. In
In Proceedings of LREC 2008, Marrakech, Morocco.
ELDA/ELRA.
Grzegorz Chrupa?a. 2008. Towards a machine-learning
architecture for lexical functional grammar parsing.
Ph.D. thesis, Dublin City University.
M. Civit and M. A. Mart?. 2004. Building cast3lb: A
spanish treebank. Research on Language and Compu-
tation, 2(4):549 ? 574.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25?69.
B. Cowan and M. Collins. 2005. Morphology and
reranking for the statistical parsing of spanish. In Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 795?802. Association for Computa-
tional Linguistics.
Pascal Denis and Beno?t Sagot. 2009. Coupling an anno-
tated corpus and a morphosyntactic lexicon for state-
of-the-art pos tagging with less human effort. In Pro-
ceedings of PACLIC 2009, Hong-Kong, China.
Alberto Lavelli and Anna Corazza. 2009. The berkeley
parser at the evalita 2009 constituency parsing task. In
EVALITA 2009 Workshop on Evaluation of NLP Tools
for Italian.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), pages 75?
82.
Miguel A. Molinero, Beno?t Sagot, and Lionel Nicolas.
2009. A morphological and syntactic wide-coverage
lexicon for spanish: The leffe. In Proceedings of the
International Conference RANLP-2009, pages 264?
269, Borovets, Bulgaria, September. Association for
Computational Linguistics.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
404?411, Rochester, New York, April. Association for
Computational Linguistics.
Slav Petrov and Dan Klein. 2008. Parsing german with
latent variable grammars. In Proceedings of the ACL
Workshop on Parsing German.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Djam? Seddah, Marie Candito, and Benoit Crabb?. 2009.
Cross parser evaluation and tagset variation: A French
Treebank study. In Proceedings of the 11th Interna-
tion Conference on Parsing Technologies (IWPT?09),
pages 150?161, Paris, France, October. Association
for Computational Linguistics.
Djam? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
60
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Djam? Seddah, Joseph Le Roux, and Beno?t Sagot. 2011.
Towards using data driven lemmatization for statisti-
cal constituent parsing of italian. In Working Notes of
EVALITA 2011, Rome, Italy, December.
61
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 89?99,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Generative Constituent Parsing and Discriminative Dependency Reranking:
Experiments on English and French
Joseph Le Roux Beno?t Favre? Alexis Nasr? Seyed Abolghasem Mirroshandel?,?
LIPN, Universit? Paris Nord ? CNRS UMR 7030, Villetaneuse, France
?LIF, Universit? Aix-Marseille ? CNRS UMR 7279, Marseille, France
?Computer Engineering Department, Sharif university of Technology, Tehran, Iran
leroux@univ-paris13.fr, benoit.favre@lif.univ-mrs.fr,
alexis.nasr@lif.univ-mrs.fr, ghasem.mirroshandel@lif.univ-mrs.fr
Abstract
We present an architecture for parsing in two
steps. A phrase-structure parser builds for
each sentence an n-best list of analyses which
are converted to dependency trees. These de-
pendency structures are then rescored by a dis-
criminative reranker. Our method is language
agnostic and enables the incorporation of ad-
ditional information which are useful for the
choice of the best parse candidate. We test
our approach on the the Penn Treebank and
the French Treebank. Evaluation shows a sig-
nificative improvement on different parse met-
rics.
1 Introduction
Two competing approaches exist for parsing natural
language. The first one, called generative, is based
on the theory of formal languages and rewriting sys-
tems. Parsing is defined here as a process that trans-
forms a string into a tree or a tree forest. It is of-
ten grounded on phrase-based grammars ? although
there are generative dependency parsers ? in partic-
ular context-free grammars or one of their numer-
ous variants, that can be parsed in polynomial time.
However, the independence hypothesis that under-
lies this kind of formal system does not allow for
precise analyses of some linguistic phenomena, such
as long distance and lexical dependencies.
In the second approach, known as discriminative,
the grammar is viewed as a system of constraints
over the correct syntactic structures, the words of the
sentence themselves being seen as constraints over
the position they occupy in the sentence. Parsing
boils down to finding a solution that is compatible
with the different constraints. The major problem of
this approach lies in its complexity. The constraints
can, theoretically, range over any aspect of the final
structures, which prevents from using efficient dy-
namic programming techniques when searching for
a global solution. In the worst case, final structures
must be enumerated in order to be evaluated. There-
fore, only a subset of constraints is used in imple-
mentations for complexity reasons. This approach
can itself be divided into formalisms relying on logic
to describe constraints, as the model-theoretic syn-
tax (Pullum and Scholz, 2001), or numerical for-
malisms that associate weights to lexico-syntactic
substructures. The latter has been the object of some
recent work thanks to progresses achieved in the
field of Machine Learning. A parse tree is repre-
sented as a vector of features and its accuracy is
measured as the distance between this vector and the
reference.
One way to take advantage of both approaches
is to combine them sequentially, as initially pro-
posed by Collins (2000). A generative parser pro-
duces a set of candidates structures that constitute
the input of a second, discriminative module, whose
search space is limited to this set of candidates.
Such an approach, parsing followed by reranking,
is used in the Brown parser (Charniak and Johnson,
2005). The approach can be extended in order to
feed the reranker with the output of different parsers,
as shown by (Johnson and Ural, 2010; Zhang et al,
2009).
In this paper we are interested in applying rerank-
ing to dependency structures. The main reason is
that many linguistic constraints are straightforward
to implement on dependency structures, as, for ex-
ample, subcategorization frames or selectional con-
straints that are closely linked to the notion of de-
89
pendents of a predicate. On the other hand, depen-
dencies extracted from constituent parses are known
to be more accurate than dependencies obtained
from dependency parsers. Therefore the solution we
choose is an indirect one: we use a phrase-based
parser to generate n-best lists and convert them to
lists of dependency structures that are reranked. This
approach can be seen as trade-off between phrase-
based reranking experiments (Collins, 2000) and the
approach of Carreras et al (2008) where a discrimi-
native model is used to score lexical features repre-
senting unlabelled dependencies in the Tree Adjoin-
ing Grammar formalism.
Our architecture, illustrated in Figure 1, is based
on two steps. During the first step, a syntagmatic
parser processes the input sentence and produces n-
best parses as well as their probabilities. They are
annotated with a functional tagger which tags syn-
tagms with standard syntactic functions subject, ob-
ject, indirect object . . . and converted to dependency
structures by application of percolation rules. In the
second step, we extract a set of features from the
dependency parses and the associated probabilities.
These features are used to reorder the n-best list
and select a potentially more accurate parse. Syn-
tagmatic parses are produced by the implementation
of a PCFG-LA parser of (Attia et al, 2010), simi-
lar to (Petrov et al, 2006), a functional tagger and
dependency converter for the target language. The
reranking model is a linear model trained with an
implementation of the MIRA algorithm (Crammer et
al., 2006)1.
Charniak and Johnson (2005) and Collins (2000)
rerank phrase-structure parses and they also include
head-dependent information, in other words unla-
belled dependencies. In our approach we take into
account grammatical functions or labelled depen-
dencies.
It should be noted that the features we use are very
generic and do not depend on the linguistic knowl-
edge of the authors. We applied our method to En-
glish, the de facto standard for testing parsing tech-
nologies, and French which exhibits many aspects of
a morphologically rich language. But our approach
could be applied to other languages, provided that
1This implementation is available at https://github.
com/jihelhere/adMIRAble.
the resources ? treebanks and conversion tools ? ex-
ist.
(1) PCFG-LA n-best constituency parses
(2) Function annotation
(3) Conversion to dependency parses
(4) Feature extraction
(5) MIRA reranking
w
Final constituency & dependency parse
Input text
Figure 1: The parsing architecture: production of the n-
best syntagmatic trees (1) tagged with functional labels
(2), conversion to a dependency structure (3) and feature
extraction (4), scoring with a linear model (5). The parse
with the best score is considered as final.
The structure of the paper is the following: in
Section 2 we describe the details of our generative
parser and in Section 3 our reranking model together
with the features templates. Section 4 reports the re-
sults of the experiments conducted on the Penn Tree-
bank (Marcus et al, 1994) as well as on the Paris 7
Treebank (Abeill? et al, 2003) and Section 5 con-
cludes the paper.
2 Generative Model
The first part of our system, the syntactic analysis
itself, generates surface dependency structures in a
sequential fashion (Candito et al, 2010b; Candito
et al, 2010a). A phrase structure parser based on
Latent Variable PCFGs (PCFG-LAs) produces tree
structures that are enriched with functions and then
converted to labelled dependency structures, which
will be processed by the parse reranker.
90
2.1 PCFG-LAs
Probabilistic Context Free Grammars with Latent
Annotations, introduced in (Matsuzaki et al, 2005)
can be seen as automatically specialised PCFGs
learnt from treebanks. Each symbol of the gram-
mar is enriched with annotation symbols behaving
as subclasses of this symbol. More formally, the
probability of an unannotated tree is the sum of the
probabilities of its annotated counterparts. For a
PCFG-LA G, R is the set of annotated rules, D(t)
is the set of (annotated) derivations of an unanno-
tated tree t, and R(d) is the set of rules used in a
derivation d. Then the probability assigned by G to
t is:
PG(t) =
?
d?D(t)
PG(d) =
?
d?D(t)
?
r?R(d)
PG(r) (1)
Because of this alternation of sums and products
that cannot be optimally factorised, there is no ex-
act polynomial dynamic programming algorithm for
parsing. Matsuzaki et al (2005) and Petrov and
Klein (2007) discuss approximations of the decod-
ing step based on a Bayesian variational approach.
This enables cubic time decoding that can be fur-
ther enhanced with coarse-to-fine methods (Char-
niak and Johnson, 2005).
This type of grammars has already been tested
on a variety of languages, in particular English
and French, giving state-of-the-art results. Let us
stress that this phrase-structure formalism is not lex-
icalised as opposed to grammars previously used in
reranking experiments (Collins, 2000; Charniak and
Johnson, 2005). The notion of lexical head is there-
fore absent at parsing time and will become avail-
able only at the reranking step.
2.2 Dependency Structures
A syntactic theory can either be expressed with
phrase structures or dependencies, as advocated for
in (Rambow, 2010). However, some information
may be simpler to describe in one of the representa-
tions. This equivalence between the modes of repre-
sentations only stands if the informational contents
are the same. Unfortunately, this is not the case
here because the phrase structures that we use do
not contain functional annotations and lexical heads,
whereas labelled dependencies do.
This implies that, in order to be converted
into labelled dependency structures, phrase struc-
ture parses must first be annotated with functions.
Previous experiments for English and French as
well (Candito et al, 2010b) showed that a sequential
approach is better than an integrated one for context-
free grammars, because the strong independence hy-
pothesis of this formalism implies a restricted do-
main of locality which cannot express the context
needed to properly assign functions. Most func-
tional taggers, such as the ones used in the following
experiments, rely on classifiers whose feature sets
can describe the whole context of a node in order to
make a decision.
3 Discriminative model
Our discriminative model is a linear model
trained with the Margin-Infused Relaxed Algorithm
(MIRA) (Crammer et al, 2006). This model com-
putes the score of a parse tree as the inner product
of a feature vector and a weight vector represent-
ing model parameters. The training procedure of
MIRA is very close to that of a perceptron (Rosen-
blatt, 1958), benefiting from its speed and relatively
low requirements while achieving better accuracy.
Recall that parsing under this model consists in
(1) generating a n-best list of constituency parses
using the generative model, (2) annotating each of
them with function tags, (3) converting them to de-
pendency parses, (4) extracting features, (5) scoring
each feature vector against the model, (6) selecting
the highest scoring parse as output.
For training, we collect the output of feature ex-
traction (4) for a large set of training sentences and
associate each parse tree with a loss function that de-
notes the number of erroneous dependencies com-
pared to the reference parse tree. Then, model
weights are adjusted using MIRA training so that the
parse with the lowest loss gets the highest score. Ex-
amples are processed in sequence, and for each of
them, we compute the score of each parse according
to the current model and find an updated weight vec-
tor that assigns the first rank to the best parse (called
oracle). Details of the algorithm are given in the fol-
lowing sections.
91
3.1 Definitions
Let us consider a vector space of dimensionmwhere
each component corresponds to a feature: a parse
tree p is represented as a sparse vector ?(p). The
model is a weight vector w in the same space where
each weight corresponds to the importance of the
features for characterizing good (or bad) parse trees.
The score s(p) of a parse tree p is the scalar product
of its feature vector ?(p) and the weight vector w.
s(p) =
m?
i=1
wi?i(p) (2)
Let L be the n-best list of parses produced by the
generative parser for a given sentence. The highest
scoring parse p? is selected as output of the reranker:
p? = argmax
p?L
s(p) (3)
MIRA learning consists in using training sen-
tences and their reference parses to determine the
weight vector w. It starts with w = 0 and modifies
it incrementally so that parses closest to the refer-
ence get higher scores. Let l(p), loss of parse p,
be the number of erroneous dependencies (governor,
dependent, label) compared to the reference parse.
We define o, the oracle parse, as the parse with the
lowest loss in L.
Training examples are processed in sequence as
an instance of online learning. For each sentence,
we compute the score of each parse in the n-best
list. If the highest scoring parse differs from the or-
acle (p? 6= o), the weight vector can be improved.
In this case, we seek a modification of w ensuring
that o gets a better score than p? with a difference
at least proportional to the difference between their
loss. This way, very bad parses get pushed deeper
than average parses. Finding such weight vector
can be formulated as the following constrained opti-
mization problem:
minimize: ||w||2 (4)
subject to: s(o)? s(p?) ? l(o)? l(p?) (5)
Since there is an infinity of weight vectors that
satisfy constraint 5, we settle on the one with the
smallest magnitude. Classical constrained quadratic
optimization methods can be applied to solve this
problem: first, Lagrange multipliers are used to in-
troduce the constraint in the objective function, then
the Hildreth algorithm yields the following analytic
solution to the non-constrained problem:
w? = w + ? (?(o)? ?(p?)) (6)
? = max
[
0,
l(o)? l(p?)? [s(o)? s(p?)]
||?(o)? ?(p?)||2
]
(7)
Here, w? is the new weight vector, ? is an up-
date magnitude and [?(o)? ?(p?)] is the difference
between the feature vector of the oracle and that of
the highest scoring parse. This update, similar to
the perceptron update, draws the weight vector to-
wards o while pushing it away from p?. Usual tricks
that apply to the perceptron also apply here: (a) per-
forming multiple passes on the training data, and (b)
averaging the weight vector over each update2. Al-
gorithm 1 details the instructions for MIRA training.
Algorithm 1 MIRA training
for i = 1 to t do
for all sentences in training set do
Generate n-best list L from generative parser
for all p ? L do
Extract feature vector ?(p)
Compute score s(p) (eq. 2)
end for
Get oracle o = argminp l(p)
Get best parse p? = argmaxp s(p)
if p? 6= o then
Compute ? (eq. 7)
Update weight vector (eq. 6)
end if
end for
end for
Return average weight vector over updates.
3.2 Features
The quality of the reranker depends on the learning
algorithm as much as on the feature set. These fea-
tures can span over any subset of a parse tree, up to
the whole tree. Therefore, there are a very large set
of possible features to choose from. Relevant fea-
tures must be general enough to appear in as many
2This can be implemented efficiently using two weight vec-
tors as for the averaged perceptron.
92
parses as possible, but specific enough to character-
ize good and bad configurations in the parse tree.
We extended the feature set from (McDonald,
2006) which showed to be effective for a range of
languages. Our feature templates can be categorized
in 5 classes according to their domain of locality.
In the following, we describe and exemplify these
templates on the following sentence from the Penn
treebank, in which we target the PMOD dependency
between ?at? and ?watch.?
Probability Three features are derived from the
PCFG-LA parser, namely the posterior proba-
bility of the parse (eq. 1), its normalized prob-
ability relative to the 1-best, and its rank in the
n-best list.
Unigram Unigram features are the most simple as
they only involve one word. Given a depen-
dency between position i and position j of type
l, governed by xi, denoted xi
l
? xj , two fea-
tures are created: one for the governor xi and
one for the dependent xj . They are described
as 6-tuples (word, lemma, pos-tag, is-governor,
direction, type of dependency). Variants with
wildcards at each subset of tuple slots are also
generated in order to handle sparsity.
In our example, the dependency between
?looked? and ?at? generates two features:
[at, at, IN, G, R, PMOD] and
[looked, look, NN, D, L, PMOD]
And also wildcard features such as:
[-, at, IN, G, R, PMOD], [at,
-, IN, G, R, PMOD] ...
[at, -, -, -, -, PMOD]
This wildcard feature generation is applied to
all types of features. We will omit it in the re-
mainder of the description.
Bigram Unlike the previous template, bigram fea-
tures model the conjunction of the governor
and the dependent of a dependency relation,
like bilexical dependencies in (Collins, 1997).
Given dependency xi
l
? xj , the feature cre-
ated is (word xi, lemma xi, pos-tag xi, word
xj , lemma xj , pos-tag xj , distance3 from i to
j, direction, type).
The previous example generates the following
feature:
[at, at, IN, watch, watch, NN,
2, R, PMOD]
Where 2 is the distance between ?at? and
?watch?.
Linear context This feature models the linear con-
text between the governor and the dependent
of a relation by looking at the words between
them. Given dependency xi
l
? xj , for each
word from i + 1 to j ? 1, a feature is created
with the pos-tags of xi and xj , and the pos tag
of the word between them (no feature is create
if j = i + 1). An additional feature is created
with pos-tags at positions i? 1, i, i+ 1, j ? 1,
j, j +1. Our example yields the following fea-
tures:
[IN, PRP$, NN], and [VBD, IN,
PRP$, PRP$, NN, .].
Syntactic context: siblings This template and the
next one look at two dependencies in two con-
figurations. Given two dependencies xi
l
? xj
and xi
m
? xk, we create the feature (word,
lemma, pos-tag for xi, xj and xk, distance from
i to j, distance from i to k, direction and type of
each of the two dependencies). In our example:
[looked, look, VBD, I, I, PRP,
at, at, IN, 1, 1, L, SBJ, R,
ADV]
Syntactic context: chains Given two dependencies
xi
l
? xj
m
? xk, we create the feature (word,
lemma, pos-tag of xi, xj and xk, distance from
i to j, distance from i to k, direction and type of
each of the two dependencies). In our example:
[looked, look, VBD, at, at, IN,
watch, watch, NN, 1, 2, R, ADV,
3In every template, distance features are quantified in 7
classes: 1, 2, 3, 4, 5, 5 to 10, more.
93
R, PMOD]
It is worth noting that our feature templates only
rely on information available in the training set, and
do not use any external linguistic knowledge.
4 Experiments
In this section, we evaluate our architecture on
two corpora, namely the Penn Treebank (Marcus et
al., 1994) and the French Treebank (Abeill? et al,
2003). We first present the corpora and the tools
used for annotating and converting structures, then
the performances of the phrase structure parser alone
and with the discriminative reranker.
4.1 Treebanks and Tools
For English, we use the Wall Street Journal sections
of the Penn Treebank. We learn the PCFG-LA from
sections 02-214. We then use FUNTAG (Chrupa?a
et al, 2007) to add functions back to the PCFG-LA
analyses. For the conversion to dependency struc-
tures we use the LTH tool (Johansson and Nugues,
2007). In order to get the gold dependencies, we run
LTH directly on the gold parse trees. We use sec-
tion 22 for development and section 23 for the final
evaluation.
For French, we use the Paris 7 Treebank (or
French Treebank, FTB). As in several previous ex-
periments we decided to divide the 12,350 phrase
structure trees in three sets: train (80%), develop-
ment (10%) and test (10%). The syntactic tag set for
French is not fixed and we decided to use the one
described in (Candito and Crabb?, 2009) to be able
to compare this system with recent parsing results
on French. As for English, we learn the PCFG-LA
without functional annotations which are added af-
terwards. We use the dependency structures devel-
oped in (Candito et al, 2010b) and the conversion
toolkit BONSA?. Furthermore, to test our approach
against state of the art parsing results for French
we use word clusters in the phrase-based parser as
in (Candito and Crabb?, 2009).
For both languages we constructed 10-fold train-
ing data from train sets in order to avoid overfitting
the training data. The trees from training sets were
divided into 10 subsets and the parses for each sub-
set were generated by a parser trained on the other
4Functions are omitted.
9 subsets. Development and test parses are given by
a parser using the whole training set. Development
sets were used to choose the best reranking model.
For lemmatisation, we use the MATE lemmatiser
for English and a home-made lemmatiser for French
based on the lefff lexicon (Sagot, 2010).
4.2 Generative Model
The performances of our parser are summarised in
Figure 2, (a) and (b), where F-score denotes the Par-
seval F-score5, and LAS and UAS are respectively
the Labelled and Unlabelled Attachment Score of
the converted dependency structures6. We give or-
acle scores (the score that our system would get if
it selected the best parse from the n-best lists) when
the parser generates n-best lists of depth 10, 20, 50
and 100 in order to get an idea of the effectiveness
of the reranking process.
One of the issues we face with this approach is
the use of an imperfect functional annotator. For
French we evaluate the loss of accuracy on the re-
sulting dependency structure from the gold develop-
ment set where functions have been omitted. The
UAS is 100% but the LAS is 96.04%. For English
the LAS from section 22 where functions are omit-
ted is 95.35%.
From the results presented in this section we can
make two observations. First, the results of our
parser are at the state of the art on English (90.7%
F-score) and on French (85.7% F-score). So the
reranker will be confronted with the difficult task of
improving on these scores. Second, the progression
margin is sensible with a potential LAS error reduc-
tion of 41% for English and 40.2% for French.
4.3 Adding the Reranker
4.3.1 Learning Feature Weights
The discriminative model, i.e. template instances
and their weights, is learnt on the training set parses
obtained via 10-fold cross-validation. The genera-
tive parser generates 100-best lists that are used as
learning example for the MIRA algorithm. Feature
extraction produces an enormous number of fea-
tures: about 571 millions for English and 179 mil-
5We use a modified version of evalb that gives the ora-
cle score when the parser outputs a list of candidates for each
sentence.
6All scores are measured without punctuation.
94
(a) Oracle Scores on PTB dev set (b) Oracle Scores on FTB dev set
8990
9192
9394
9596
97
1 10 20 50 100Size of n-best list
UASLASF-score
Oracle s
core
86
88
90
92
94
86
88
90
92
94
1 10 20 50 100Size of n?best list
Oracle s
core
UASLASF-score
(c) Reranker scores on PTB dev set (d) Reranker scores on FTB dev set
899
091
929
394
1 10 20 50 100Size of n-best list
UASLASF-score
Reranke
d score
868
788
899
091
1 10 20 50 100Size of n?best list
UASLASF-score
Rerank
er score
Figure 2: Oracle and reranker scores on PTB and FTB data on the dev. set, according to the depth of the n-best.
lions for French. Let us remark that this large set of
features is not an issue because our discriminative
learning algorithm is online, that is to say it consid-
ers only one example at a time, and it only gives
non-null weights to useful features.
4.3.2 Evaluation
In order to test our system we first tried to eval-
uate the impact of the length of the n-best list over
the reranking predictions7. The results are shown in
Figure 2, parts (c) and (d).
For French, we can see that even though the LAS
and UAS are consistently improving with the num-
ber of candidates, the F-score is maximal with 50
candidates. However the difference between 50 can-
didates and 100 candidates is not statistically signifi-
cant. For English, the situation is simpler and scores
improve continuously on the three metrics.
Finally we run our system on the test sets for both
treebanks. Results are shown8 in Table 1 for En-
glish, and Table 2 for French. For English the im-
provement is 0.9% LAS, 0.7% Parseval F-score and
7The model is always trained with 100 candidates.
8F < 40 is the parseval F-score for sentences with less than
40 words.
0.8% UAS.
Baseline Reranker
F 90.4 91.1
F < 40 91.0 91.7
LAS 88.9 89.8
UAS 93.1 93.9
Table 1: System results on PTB Test set
For French we have improvements of 0.3/0.7/0.9.
If we add a template feature indicating the agree-
ment between part-of-speech provided by the PCFG-
LA parser and a part-of-speech tagger (Denis
and Sagot, 2009), we obtain better improvements:
0.5/0.8/1.1.
Baseline Reranker Rerank + MElt
F 86.6 87.3 87.4
F < 40 88.7 89.0 89.2
LAS 87.9 89.0 89.2
UAS 91.0 91.9 92.1
Table 2: System results on FTB Test set
95
4.3.3 Comparison with Related Work
We compare our results with related parsing re-
sults on English and French.
For English, the main results are shown in Ta-
ble 3. From the presented data, we can see that
indirect reranking on LAS may not seem as good
as direct reranking on phrase-structures compared to
F-scores obtained in (Charniak and Johnson, 2005)
and (Huang, 2008) with one parser or (Zhang et
al., 2009) with several parsers. However, our sys-
tem does not rely on any language specific feature
and can be applied to other languages/treebanks. It
is difficult to compare our system for LAS because
most systems evaluate on gold data (part-of-speech,
lemmas and morphological information) like Bohnet
(2010).
Our system also compares favourably with the
system of Carreras et al (2008) that relies on a more
complex generative model, namely Tree Adjoining
Grammars, and the system of Suzuki et al (2009)
that makes use of external data (unannotated text).
F LAS UAS
Huang, 2008 91.7 ? ?
Bohnet, 2010 ? 90.3 ?
Zhang et al 2008 91.4 ? 93.2
Huang and Sagae, 2010 ? ? 92.1
Charniak et al 2005 91.5 90.0 94.0
Carreras et al 2008 ? ? 93.5
Suzuki et al 2009 ? ? 93.8
This work 91.1 89.8 93.9
Table 3: Comparison on PTB Test set
For French, see Table 4, we compare our system
with the MATE parser (Bohnet, 2010), an improve-
ment over the MST parser (McDonald et al, 2005)
with hash kernels, using the MELT part-of-speech
tagger (Denis and Sagot, 2009) and our own lemma-
tiser.
We also compare the French system with results
drawn from the benchmark performed by Candito et
al. (2010a). The first system (BKY-FR) is close to
ours without the reranking module, using the Berke-
ley parser adapted to French. The second (MST-
FR) is based on MSTParser (McDonald et al, 2005).
These two system use word clusters as well.
The next section takes a close look at the models
of the reranker and its impact on performance.
F < 40 LAS UAS
This work 89.2 89.2 92.1
MATE + MELT ? 89.2 91.8
BKY-FR 88.2 86.8 91.0
MST-FR ? 88.2 90.9
Table 4: Comparison on FTB Test set
4.3.4 Model Analysis
It is interesting to note that in the test sets, the
one-best of the syntagmatic parser is selected 52.0%
of the time by the reranker for English and 34.3% of
the time for French. This can be explained by the
difference in the quantity of training data in the two
treebanks (four times more parses are available for
English) resulting in an improvement of the quality
of the probabilistic grammar.
We also looked at the reranking models, specifi-
cally at the weight given to each of the features. It
shows that 19.8% of the 571 million features have
a non-zero weight for English as well as 25.7% of
the 179 million features for French. This can be ex-
plained by the fact that for a given sentence, features
that are common to all the candidates in the n-best
list are not discriminative to select one of these can-
didates (they add the same constant weight to the
score of all candidates), and therefore ignored by the
model. It also shows the importance of feature engi-
neering: designing relevant features is an art (Char-
niak and Johnson, 2005).
We took a closer look at the 1,000 features of
highest weight and the 1,000 features of lowest
weight (negative) for both languages that represent
the most important features for discriminating be-
tween correct and incorrect parses. For English,
62.0% of the positive features are backoff features
which involve at least one wildcard while they are
85.9% for French. Interestingly, similar results hold
for negative features. The difference between the
two languages is hard to interpret and might be due
in part to lexical properties and to the fact that these
features may play a balancing role against towards
non-backoff features that promote overfitting.
Expectedly, posterior probability features have
the highest weight and the n-best rank feature has the
highest negative weight. As evidenced by Table 5,
96
en (+) en (-) fr (+) fr (-)
Linear 30.4 36.1 44.8 44.0
Unigram 20.7 16.3 9.7 8.2
Bigram 27.4 29.1 20.8 24.4
Chain 15.4 15.3 13.7 19.4
Siblings 5.8 3.0 10.8 3.6
Table 5: Repartition of weight (in percentage) in the
1,000 highest (+) and lowest (-) weighted features for En-
glish and French.
among the other feature templates, linear context oc-
cupies most of the weight mass of the 1,000 highest
weighted features. It is interesting to note that the
unigram and bigram templates are less present for
French than for English while the converse seems to
be true for the linear template. Sibling features are
consistently less relevant.
In terms of LAS performance, on the PTB test
set the reranked output is better than the baseline
on 22.4% of the sentences while the opposite is true
for 10.4% of the sentences. In 67.0% of the sen-
tences, they have the same LAS (but not necessar-
ily the same errors). This emphasises the difficulty
of reranking an already good system and also ex-
plains why oracle performance is not reached. Both
the baseline and reranker output are completely cor-
rect on 21.3% of the sentences, while PCFG-LA cor-
rectly parses 23% of the sentences and the MIRA
brings that number to 26%.
Figures 3 and 4 show hand-picked sentences for
which the reranker selected the correct parse. The
French sentence is a typical difficult example for
PCFGs because it involves a complex rewriting rule
which might not be well covered in the training
data (SENT ? NP VP PP PONCT PP PONCT PP
PONCT). The English example is tied to a wrong
attachment of the prepositional phrase to the verb
instead of the date, which lexicalized features of the
reranker handle easily.
5 Conclusion
We showed that using a discriminative reranker, on
top of a phrase structure parser, based on converted
dependency structures could lead to significant im-
provements over dependency and phrase structure
parse results. We experimented on two treebanks
for two languages, English and French and we mea-
sured the improvement of parse quality on three dif-
ferent metrics: Parseval F-score, LAS and UAS,
with the biggest error reduction on the latter. How-
ever the gain is not as high as expected by looking
at oracle scores, and we can suggest several possible
improvements on this method.
First, the sequential approach is vulnerable to cas-
cading errors. Whereas the generative parser pro-
duces several candidates, this is not the case of the
functional annotators: these errors are not amend-
able. It should be possible to have a functional tag-
ger with ambiguous output upon which the reranker
could discriminate. It remains an open question as
how to integrate ambiguous output from the parser
and from the functional tagger. The combination
of n-best lists would not scale up and working on
the ambiguous structure itself, the packed forest as
in (Huang, 2008), might be necessary. Another pos-
sibility for future work is to let the phrase-based
parser itself perform function annotation, but some
preliminary tests on French showed disappointing
results.
Second, designing good features, sufficiently gen-
eral but precise enough, is, as already coined
by Charniak and Johnson (2005), an art. More for-
mally, we can see several alternatives. Dependency
structures could be exploited more thoroughly using,
for example, tree kernels. The restricted number of
candidates enables the use of more global features.
Also, we haven?t used any language-specific syntac-
tic features. This could be another way to improve
this system, relying on external linguistic knowledge
(lexical preferences, subcategorisation frames, cop-
ula verbs, coordination symmetry . . . ). Integrating
features from the phrase-structure trees is also an op-
tion that needs to be explored.
Third this architecture enables the integration of
several systems. We experimented on French using a
part-of-speech tagger but we could also use another
parser and either use the methodology of (Johnson
and Ural, 2010) or (Zhang et al, 2009) which fu-
sion n-best lists form different parsers, or use stack-
ing methods where an additional parser is used as
a guide for the main parser (Nivre and McDonald,
2008).
Finally it should be noted that this system does not
rely on any language specific feature, and thus can
be applied to languages other that French or English
97
NNSStocks
NP
VBDwere CD698 CDmillion
QP
NNSbushels
NP
INon NNPMay CD31
NP
INof DTthis NNyear
NP
PP
NP
PP
VP
..
S
NNSStocks
NP
VBDwere CD698 CDmillion
QP
NNSbushels
NP
INon NNPMay CD31
NP
PP
INof DTthis NNyear
NP
PP
VP
..
S
depen
dency parse
s
synta
gmati
c
parse
s
Before reranking After reranking
Figure 3: English sentence from the WSJ test set for which the reranker selected the correct tree while the first
candidate of the n-best list suffered from an incorrect attachment.
SENT
NP VN PP
PONCT
NP
PONCTNPP NPP V VPP P AP DET ADJ PONCT P ADJADJ
SENT
NP VN PP
PONCT NP PONCTNPP NPP V VPP P AP P NC PONCT P ADJADJ NP
PP PP
NPP NPP V VPP P ADJ PONCT DET ADJ PONCT P ADJ PONCT NPP NPP V VPP P ADJ PONCT P NC PONCT P ADJ PONCTdepen
dency parses
syntag
matic parses
Before reranking After reranking
Figure 4: Sentence from the FTB for which the best parse according to baseline was incorrect, probably due to the
tendency of the PCFG-LA model to prefer rules with more support. The reranker selected the correct parse.
without re-engineering new reranking features. This
makes this architecture suitable for morphologically
rich languages.
Acknowledgments
This work has been funded by the French Agence
Nationale pour la Recherche, through the project
SEQUOIA (ANR-08-EMER-013).
References
Anne Abeill?, Lionel Cl?ment, and Toussenel Fran?ois,
2003. Treebanks, chapter Building a treebank for
French. Kluwer, Dordrecht.
M. Attia, J. Foster, D. Hogan, J. Le Roux, L. Tounsi, and
J. van Genabith. 2010. Handling Unknown Words in
Statistical Latent-Variable Parsing Models for Arabic,
English and French. In Proceedings of SPMRL.
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In Proceedings
of COLING.
M.-H. Candito and B. Crabb?. 2009. Improving Gen-
erative Statistical Parsing with Semi-Supervised Word
Clustering. In Proceedings of IWPT 2009.
M.-H. Candito, J. Nivre, P. Denis, and E. Henestroza An-
guiano. 2010a. Benchmarking of Statistical Depen-
dency Parsers for French. In Proceedings of COL-
ING?2010.
Marie Candito, Beno?t Crabb?, and Pascal Denis. 2010b.
Statistical French Dependency Parsing : Treebank
Conversion and First Results. In Proceedings of
LREC2010.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, Dynamic Programming and the Perceptron for
Efficient, Feature-rich Parsing. In CONLL.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of ACL.
98
Grzegorz Chrupa?a, Nicolas Stroppa, Josef van Genabith,
and Georgiana Dinu. 2007. Better training for func-
tion labeling. In Proceedings of RANLP, Borovets,
Bulgaria.
Michael Collins. 1997. Three Generative, Lexicalised
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the ACL.
Michael Collins. 2000. Discriminative Reranking for
Natural Language Parsing. In Proceedings of ICML.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
ShalevShwartz, and Yoram Singer. 2006. Online
Passive-Aggressive Algorithm. Journal of Machine
Learning Research.
Pascal Denis and Beno?t Sagot. 2009. Coupling an anno-
tated corpus and a morphosyntactic lexicon for state-
of-the-art pos tagging with less human effort. In Pro-
ceedings PACLIC 23, Hong Kong, China.
Liang Huang. 2008. Forest Reranking: Discriminative
Parsing with Non-Local Features. In Proceedings of
ACL.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of NODALIDA 2007, pages 105?112,
Tartu, Estonia, May 25-26.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the Berkeley and Brown Parsers. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 665?668, Los An-
geles, California, June. Association for Computational
Linguistics.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The penn tree-
bank: Annotating predicate argument structure. In
Proceedings of the ARPA Speech and Natural Lan-
guage Workshop.
Takuya Matsuzaki, Yusuke Miyao, and Jun ichi Tsujii.
2005. Probabilistic CFG with Latent Annotations. In
Proceedings of ACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online Large-Margin Training of Dependency
Parsers. In Association for Computational Linguistics
(ACL).
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL, pages 950?958.
Slav Petrov and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In HLT-NAACL, pages
404?411.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In ACL.
Geoffrey K. Pullum and Barbara C. Scholz. 2001. On the
distinction between model-theoretic and generative-
enumerative syntactic frameworks. In Logical Aspects
of Computational Linguistics.
Owen Rambow. 2010. The Simple Truth about Depen-
dency and Phrase Structure Representations: An Opin-
ion Piece. In NAACL HLT.
Frank Rosenblatt. 1958. The Perceptron: A Probabilistic
Model for Information Storage and Organization in the
Brain. Psychological Review.
Beno?t Sagot. 2010. The lefff, a freely available and
large-coverage lexicon for french. In Proceedings of
LREC 2010, La Valette, Malta.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2-Volume 2,
pages 551?560. Association for Computational Lin-
guistics.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In Proceedings of EMNLP.
99
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 114?120,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Pipeline Approach to Supervised Error Correction
for the QALB-2014 Shared Task
Nadi Tomeh
?
Nizar Habash
?
Ramy Eskander
?
Joseph Le Roux
?
{nadi.tomeh,leroux}@lipn.univ-paris13.fr
?
nizar.habash@nyu.edu
?
, ramy@ccls.columbia.edu
?
?
Universit?e Paris 13, Sorbonne Paris Cit?e, LIPN, Villetaneuse, France
?
Computer Science Department, New York University Abu Dhabi
?
Center for Computational Learning Systems, Columbia University
Abstract
This paper describes our submission to
the ANLP-2014 shared task on auto-
matic Arabic error correction. We present
a pipeline approach integrating an er-
ror detection model, a combination of
character- and word-level translation mod-
els, a reranking model and a punctuation
insertion model. We achieve an F
1
score
of 62.8% on the development set of the
QALB corpus, and 58.6% on the official
test set.
1 Introduction
Devising algorithms for automatic error correction
generated considerable interest in the community
since the early 1960s (Kukich, 1992) for at least
two reasons. First, typical NLP tools lack in ro-
bustness against errors in their input. This sen-
sitivity jeopardizes their usefulness especially for
unedited text, which is prevalent on the web. Sec-
ond, automated spell and grammar checkers facil-
itate text editing and can be of great help to non-
native speakers of a language. Several resources
and shared tasks appeared recently, including the
HOO task (Dale and Kilgarriff, 2010) and the
CoNLL task on grammatical error correction (Ng
et al., 2013b). In this paper we describe our partic-
ipation to the first shared task on automatic error
correction for Arabic (Mohit et al., 2014).
While non-word errors are relatively easy to
handle, the task is more challenging for gram-
matical and semantic errors. Detecting and cor-
recting such errors require context-sensitive ap-
proaches in order to capture the dependencies be-
tween the words of a text at various lexical and se-
mantic levels. All the more so for Arabic which
brings dependence down to the morphological
level (Habash, 2010).
A particularity interesting approach to error cor-
rection relies on statistical machine translation
(SMT) (Brockett et al., 2006), due to its context-
sensitivity and data-driven aspect. Therefore, the
pipeline system which we describe in Section 2
has as its core a phrase-based SMT component
(PBSMT) (Section 2.3). Nevertheless, several fac-
tors may hinder the success of this approach, such
as data sparsity, discrepancies between transla-
tion and error correction tasks, and the difficulty
of incorporating context-sensitive features into the
SMT decoder.
We address all these issues in our system which
achieves a better correction quality than a simple
word-level PBSMT baseline on the QALB corpus
(Zaghouani et al., 2014) as we show in our exper-
iments in Section 3.
2 Pipeline Approach to Error Correction
The PBSMT system accounts for context by learn-
ing, from a parallel corpus of annotated errors,
mappings from erroneous multi-word segments of
text to their corrections, and using a language
model to help select the suitable corrections in
context when multiple alternatives are present.
Furthermore, since the SMT approach is data-
driven, it is possible to address multiple types of
errors at once, as long as examples of them appear
in the training corpus. These errors may include
non-word errors, wrong lexical choices and gram-
matical errors, and can also handle normalization
issues (Yvon, 2010).
One major issue is data sparsity, since large
amount of labeled training data is necessary to
provide reliable statistics of all error types. We ad-
114
dress this issue by backing-off the word-level PB-
SMT model with a character-level correction com-
ponent, for which richer statistics can be obtained.
Another issue may stem from the inherent dif-
ference in nature between error correction and
translation. Unlike translation, the input and out-
put vocabularies in the correction task overlap sig-
nificantly, and the majority of input words are typi-
cally correct and are copied unmodified to the out-
put. The SMT system should handle correct words
by selecting their identities from all possible op-
tions, which may fail resulting in over-correction.
To help the SMT decoder decide, we augment our
pipeline with a problem zone detection compo-
nent, which supplies prior information on which
input words need to be corrected.
The final issue concerns the difficulty of incor-
porating features that require context across phrase
boundaries into the SMT decoder. A straightfor-
ward alternative is to use such features to rerank
the hypotheses in the SMT n-best hypotheses lists.
Since punctuation is particularity noisy in Ara-
bic data, we add a specialized punctuation inser-
tion component to our pipeline, depicted in Figure
1.
2.1 Error Detection
We formalize the error detection problem as a
sequence labeling problem (Habash and Roth,
2011). Errors are classified into substitution, in-
sertion and deletion errors. Substitutions involve
an incorrect word form that should be replaced by
another correct form. Insertions are words that
are incorrectly added into the text and should be
deleted. Deletions are simply missing words that
should be added.
We group all error classes into a simple binary
problem tag: a word from the input text is tagged
as ?PROB? if it is the result of an insertion or
a substitution of a word. Deleted words, which
cannot be tagged themselves, cause their adjacent
words to be marked as PROB instead. In this way,
the subsequent components in the pipeline can be
alerted to the possibility of a missing word via its
surroundings. Any words not marked as PROB are
given an ?OK? tag.
Gold tags, necessary for training, can be gener-
ated by comparing the text to its correction using
some sequence alignment technique, for which we
use SCLITE (Fiscus, 1998).
For this task, we use Yamcha (Kudo and Mat-
sumoto, 2003) to train an SVM classifier using
morphological and lexical features. We employ
a quadratic polynomial kernel. The static feature
window context size is set to +/- 2 words; the pre-
vious two (dynamic) predicted tags are also used
as features.
The feature set includes the surface forms and
their normalization after ?Alef?, ?Ya? and digit
normalization, the POS tags and the lemmas of the
words. These morphological features are obtained
using MADA 3.0 (Habash et al., 2009).
1
We also
use a set of word, POS and lemma 3-gram lan-
guage models scores as features. These LMs are
built using SRILM (Stolcke, 2002).
The error detection component is integrated into
the pipeline by concatenating the predicted tags
with the words of the input text. The SMT model
uses this additional information to learn distinct
mappings conditional on the predicted correctness
of words.
2.2 Character-level Back-off Correction
Each word that is labeled as error (PROB) in the
output of the error detection component is mapped
to multiple possible corrections using a weighted
finite-state transducer similar to the transducers
used in speech recognition (Mohri et al., 2002).
The WFST, for which we used OpenFST (Al-
lauzen et al., 2007), operates on the character
level, and the character mapping is many-to-many
(similar to the phrase-based SMT framework).
The score of each proposed correction is a com-
bination of the scores of character mappings used
to build it. The list is filtered using WFST scores
and an additional character-level LM score. The
result is a list of error-tagged words and their cor-
rection suggestions, which constitutes a small on-
the-fly phrase table used to back-off primary PB-
SMT table.
During training, the mapping dictionary is
learned from the training after aligning it at the
character level using SCLITE. Mapping weights
are computed as their normalized frequencies in
the aligned training corpus.
2.3 Word-level PBSMT Correction
We formalize the correction process as a phrase-
based statistical machine translation problem
(Koehn et al., 2003), at the word-level, and solve
1
We did not use MADAMIRA (the newest version of
MADA) since it was not available when this component was
built.
115
Character-level 
Correction 
Error 
Detection 
Word-level 
PBSMT Correction 
N-best 
Reranking 
Punctuation 
Insertion 
,	 ? .	 ?
Input Error-tagged text 
N-best hypotheses 
Reranked best hypothesis Output 
Back-off Phrase 
tables 
Primary 
Figure 1: Input text is run through the error detection component which labels the problematic words.
The labeled text is then fed to the character-level correction components which constructs a back-off
phrase table. The PBSMT component then uses two phrase tables to generate n-best correction hy-
potheses. The reranking component selects the best hypothesis, and pass it to the punctuation insertion
component in order to produce the final output.
it using Moses, a well-known PBSMT tool (Koehn
et al., 2007). The decoder constructs a correction
hypothesis by first segmenting the input text into
phrases, and mapping each phrase into its best cor-
rection using a combination of scores including a
context-sensitive LM score.
Unlike translation, error correction is mainly
monotonic, therefore we set disallow reordering
by setting the distortion limit in Moses to 0.
2
When no mapping can be found for a given
phrase in the primary phrase table, the decoder
looks it up in the back-off model. The decoder
searches the space of all possible correction hy-
potheses, resulting from alternative segmentations
and mappings, and returns the list of n-best scor-
ing hypotheses.
2.4 N-best List Reranking
In this step, we combine LM information with lin-
guistically and semantically motivated features us-
ing learning to rank methods (Tomeh et al., 2013).
Discriminative reranking (Liu, 2009) allows each
hypothesis to be represented as an arbitrary set of
features without the need to explicitly model their
interactions. Therefore, the system benefits from
global and potentially complex features which are
not available to the baseline decoder.
Each hypothesis in an n-best list is represented
by a d-dimensional feature vector. Word error rate
(WER) is computed for each hypotheses by com-
paring it to the reference correction. The resulting
2
Only 0.14% of edits in the QALB corpus are actually
reordering.
scored n-best list is used for supervised training
of a reranking model. We employ a pairwise ap-
proach to ranking which takes pairs of hypotheses
as instances in learning, and formalizes the rank-
ing problem as pairwise classification.
For this task we use RankSVM (Joachims,
2002) which is a method based on Support Vec-
tor Machines (SVMs). We use only linear kernels
to keep complexity low. We use a rich set of fea-
tures including LM scores on surface forms, POS
tags and lemmas. We also use a feature based on a
global model of the semantic coherence of the hy-
potheses (Tomeh et al., 2013). The new top ranked
hypothesis is the output of this step which is then
fed to the next component.
2.5 Punctuation Insertion
We developed a model that predicts the occurrence
of periods and commas in a given Arabic text.
The core model is a decision tree classifier trained
on the QALB parallel training data using WEKA
(Hall et al., 2009). For each space between two
words, the classifier decides whether or not to in-
sert a punctuation mark, using a window size of
three words surrounding the underlying space.
The model uses the following features:
? A class punctuation feature, that is whether to
insert a period, a comma or none at the cur-
rent space location;
? The part-of-speech of the previous word;
? The existence of a conjunctive or connective
proclitic in the following word; that is a ?wa?
116
Precision?Recall Curve
Recall
Prec
ision
0.0 0.2 0.4 0.6 0.8 1.0
0.2
0.4
0.6
0.8
1.0
?
8.33
?
5.02
?
1.7
1.61
4.93
AUC= 0.715PRBE= 0.483, Cutoff= ?0.349Prec@rec(0.800)= 0.345, Cutoff= ?1.045
Figure 2: Evaluation of the error detection com-
ponent. AUC: Area Under the Curve, PRBE:
precision-recall break-even point. Classifier
thresholds are displayed on the right vertical axis.
or ?fa? proclitic that is either a conjunction, a
sub-conjunction or a connective particle.
We obtain POS and proclitic information using
MADAMIRA (Pasha et al., 2014). The output of
this component is the final output of the system.
3 Experiments
All the models we use in our pipeline are trained
in a supervised way using the training part of the
QALB corpus (Zaghouani et al., 2014), while we
reserve the development part of the corpus for test-
ing.
3.1 Error detection
We evaluate the error detection binary classifier in
terms of standard classification measures as shown
in Figure 2. Each point on the curve is computed
by selecting a threshold on the classifier score.
The threshold we use correspond to recall equal
to 80%, at which the precision is very low which
leaves much room for improvement in the perfor-
mance of the error detection component.
3.2 Character-level correction
We evaluate the character-level correction model
by measuring the percentage of erroneous phrases
that have been mapped to their in-context refer-
ence corrections. We found this percentage to be
41% on QALB dev data. We limit the size of
such phrases to one in order to focus on out-of-
vocabulary words.
3.3 Punctuation insertion
To evaluate the punctuation insertion indepen-
dently from the pipeline, we first remove the pe-
riods and commas from input text. Considering
only the locations where periods and commas ex-
ist, our model gives a recall of 49% and a precision
of 53%, giving an F
1
-score of 51%.
When we apply our punctuation model in the
correction pipeline, we find that it is always better
to keep the already existing periods and commas
in the input text instead of overwriting them by
the model prediction.
While developing the model, we ran experi-
ments where we train the complete list of fea-
tures produced by MADAMIRA; that is part-of-
speech, gender, number, person, aspect, voice,
case, mood, state, proclitics and enclitics. This
was done for two preceding words and two follow-
ing words. However, the results were significantly
outperformed by our final set-up.
3.4 The pipeline
The performance of the pipeline is evaluated in
terms of precision, recall and F
1
as computed by
the M
2
Scorer (Dahlmeier and Ng, 2012b). The
results presented in Table 1 show that a simple
PBSMT baseline achieves relatively good perfor-
mance compared to more sophisticated models.
The character-level back-off model helps by im-
proving recall at the expense of decreased preci-
sion. The error detection component hurts the per-
formance which could be explained by its intrin-
sic bad performance. Since more investigation is
needed to clarify on this point, we drop this com-
ponent from our submission. Both reranking and
punctuation insertion improve the performance.
Our system submission to the shared task (back-
off+PBSMT+Rank+PI) resulted in an F
1
score of
58.6% on the official test set, with a precision of
76.9% and a recall of 47.3%.
4 Related Work
Both rule-based and data-driven approaches to
error correction can be found in the literature
(Sidorov et al., 2013; Berend et al., 2013; Yi et
al., 2013) as well as hybridization of them (Putra
and Szabo, 2013). Unlike our approach, most of
117
System PR RC F
1
PBSMT 75.5 49.5 59.8
backoff+PBSMT 74.1 51.8 60.9
ED+backoff+PBSMT 61.3 45.4 52.2
backoff+PBSMT+Rank 75.7 52.1 61.7
backoff+PBSMT+Rank+PI 74.9 54.2 62.8
Table 1: Pipeline precision, recall and F
1
scores.
ED: error detection, PI: punctuation insertion.
the proposed systems build distinct models to ad-
dress individual types of errors (see the CoNLL-
2013, 2014 proceedings (Ng et al., 2013a; Ng
et al., 2014), and combine them afterwords us-
ing Integer Linear Programming for instance (Ro-
zovskaya et al., 2013). This approach is relatively
time-consuming when the number of error types
increases.
Interest in models that target all errors at once
has increased, using either multi-class classifiers
(Farra et al., 2014; Jia et al., 2013), of-the-shelf
SMT techniques (Brockett et al., 2006; Mizu-
moto et al., 2011; Yuan and Felice, 2013; Buys
and van der Merwe, 2013; Buys and van der
Merwe, 2013), or building specialized decoders
(Dahlmeier and Ng, 2012a).
Our system addresses the weaknesses of the
SMT approach using additional components in a
pipeline architecture. Similar work on word-level
and character-level model combination has been
done in the context of translation between closely
related languages (Nakov and Tiedemann, 2012).
A character-level correction model has also been
considered to reduce the out-of-vocabulary rate in
translation systems (Habash, 2008).
5 Conclusion and Future Work
We described a pipeline approach based on
phrase-based SMT with n-best list reranking. We
showed that backing-off word-level model with a
character-level model improves the performance
by ameliorating the recall of the system.
The main focus of our future work will be on
better integration of the error detection model, and
on exploring alternative methods for combining
the character and the word models.
Acknowledgments
This material is partially based on research funded
by grant NPRP-4-1058-1-168 from the Qatar Na-
tional Research Fund (a member of the Qatar
Foundation). The statements made herein are
solely the responsibility of the authors.
Nizar Habash performed most of his contri-
bution to this paper while he was at the Center
for Computational Learning Systems at Columbia
University.
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. Openfst: A
general and efficient weighted finite-state transducer
library. In CIAA, pages 11?23.
Gabor Berend, Veronika Vincze, Sina Zarrie?, and
Rich?ard Farkas. 2013. Lfg-based features for noun
number and article grammatical errors. In Proceed-
ings of the Seventeenth Conference on Computa-
tional Natural Language Learning: Shared Task,
pages 62?67, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Chris Brockett, William B. Dolan, and Michael Ga-
mon. 2006. Correcting esl errors using phrasal
smt techniques. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th Annual Meeting of the Association
for Computational Linguistics, ACL-44, pages 249?
256, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Jan Buys and Brink van der Merwe. 2013. A tree
transducer model for grammatical error correction.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task, pages 43?51, Sofia, Bulgaria, August. Associ-
ation for Computational Linguistics.
Daniel Dahlmeier and Hwee Tou Ng. 2012a. A beam-
search decoder for grammatical error correction. In
EMNLP-CoNLL, pages 568?578.
Daniel Dahlmeier and Hwee Tou Ng. 2012b. Better
evaluation for grammatical error correction. In HLT-
NAACL, pages 568?572.
Robert Dale and Adam Kilgarriff. 2010. Helping our
own: Text massaging for computational linguistics
as a new shared task. In INLG.
Noura Farra, Nadi Tomeh, Alla Rozovskaya, and Nizar
Habash. 2014. Generalized character-level spelling
error correction. In ACL (2), pages 161?167.
Jon Fiscus. 1998. Speech Recognition Scor-
ing Toolkit (SCTK). National Institute of Standard
Technology (NIST). http://www.itl.nist.
gov/iad/mig/tools/.
Nizar Habash and Ryan M. Roth. 2011. Using deep
morphology to improve automatic error detection in
arabic handwriting recognition. In Proceedings of
118
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ?11, pages 875?884, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Khalid
Choukri and Bente Maegaard, editors, Proceedings
of the Second International Conference on Arabic
Language Resources and Tools. The MEDAR Con-
sortium, April.
Nizar Habash. 2008. Four Techniques for Online
Handling of Out-of-Vocabulary Words in Arabic-
English Statistical Machine Translation. In Pro-
ceedings of ACL-08: HLT, Short Papers, pages 57?
60, Columbus, Ohio.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Zhongye Jia, Peilu Wang, and Hai Zhao. 2013. Gram-
matical error correction as multiclass classification
with single model. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 74?81, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of
the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ?02,
pages 133?142.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of the Human Language Technology and
North American Association for Computational Lin-
guistics Conference (HLT/NAACL), pages 127?133,
Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Karen Kukich. 1992. Techniques for automatically
correcting words in text. ACM Comput. Surv.,
24(4):377?439, December.
Tie-Yan Liu. 2009. Learning to Rank for Informa-
tion Retrieval. Now Publishers Inc., Hanover, MA,
USA.
Tomoya Mizumoto, Mamoru Komachi, Masaaki Na-
gata, and Yuji Matsumoto. 2011. Mining revision
log of language learning sns for automated japanese
error correction of second language learners. In IJC-
NLP, pages 147?155.
Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wa-
jdi Zaghouani, and Ossama Obeid. 2014. The First
QALB Shared Task on Automatic Text Correction
for Arabic. In Proceedings of EMNLP Workshop on
Arabic Natural Language Processing, Doha, Qatar,
October.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer Speech & Language,
16(1):69?88.
Preslav Nakov and J?org Tiedemann. 2012. Combin-
ing word-level and character-level models for ma-
chine translation between closely-related languages.
In ACL (2), pages 301?305.
Hwee Tou Ng, Joel Tetreault, Siew Mei Wu, Yuanbin
Wu, and Christian Hadiwinoto, editors. 2013a. Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared Task.
Association for Computational Linguistics, Sofia,
Bulgaria, August.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013b. The conll-
2013 shared task on grammatical error correction.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task, pages 1?12, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant, editors. 2014. Proceedings of the
Eighteenth Conference on Computational Natural
Language Learning: Shared Task. Association for
Computational Linguistics, Baltimore, Maryland,
June.
Arfath Pasha, Mohamed Al-Badrashiny, Mona T. Diab,
Ahmed El Kholy, Ramy Eskander, Nizar Habash,
Manoj Pooleery, Owen Rambow, and Ryan Roth.
2014. Madamira: A fast, comprehensive tool for
morphological analysis and disambiguation of ara-
bic. In LREC, pages 1094?1101.
Desmond Darma Putra and Lili Szabo. 2013. Uds
at conll 2013 shared task. In Proceedings of the
Seventeenth Conference on Computational Natural
Language Learning: Shared Task, pages 88?95,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
119
Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,
and Dan Roth. 2013. The university of illinois sys-
tem in the conll-2013 shared task. In Proceedings of
the Seventeenth Conference on Computational Natu-
ral Language Learning: Shared Task, pages 13?19,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Grigori Sidorov, Anubhav Gupta, Martin Tozer, Do-
lors Catala, Angels Catena, and Sandrine Fuentes.
2013. Rule-based system for automatic grammar
correction using syntactic n-grams for english lan-
guage learning (l2). In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 96?101, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), volume 2, pages 901?904, Denver,
CO.
Nadi Tomeh, Nizar Habash, Ryan Roth, Noura Farra,
Pradeep Dasigi, and Mona Diab. 2013. Reranking
with linguistic and semantic features for arabic op-
tical character recognition. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
549?555, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Bong-Jun Yi, Ho-Chang Lee, and Hae-Chang Rim.
2013. Kunlp grammatical error correction system
for conll-2013 shared task. In Proceedings of the
Seventeenth Conference on Computational Natural
Language Learning: Shared Task, pages 123?127,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Zheng Yuan and Mariano Felice. 2013. Constrained
grammatical error correction using statistical ma-
chine translation. In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task, pages 52?61, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Franc?ois Yvon. 2010. Rewriting the orthography
of sms messages. Natural Language Engineering,
16:133?159, 3.
Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-
sama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura
Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014.
Large scale arabic error annotation: Guidelines and
framework. In Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation (LREC?14), Reykjavik, Iceland, May. Euro-
pean Language Resources Association (ELRA).
120
