Evangelising Language Technology:
A Practically-Focussed Undergraduate Program
Robert Dale, Diego Moll? Aliod and Rolf Schwitter
Centre for Language Technology
Division of Information and Communication Sciences
Macquarie University, Sydney, Australia
{rdale|diego|rolfs}@ics.mq.edu.au
Abstract
This paper describes an
undergraduate program in Language
Technology that we have developed
at Macquarie University. We
question the industrial relevance of
much that is taught in NLP courses,
and emphasize the need for a
practical orientation as a means to
growing the size of the field. We
argue that a more evangelical
approach, both with regard to
students and industry, is required.
The paper provides an overview of
the material we cover, and makes
some observations for the future on
the basis of our experiences so far.
1 Introduction
This paper describes our experiences in setting
up an undergraduate program in language
technology, with a particular emphasis on the
philosophy that lies behind the decisions we
have made in designing this program.
In Section 2, we sketch the background to
the program, and outline the perspective we
take on teaching in this area. Against this
backdrop, in Section 3, we describe the
orientation and content of the program in some
detail. In Section 4 we discuss the evaluation of
the program, identify some lessons we have
learned regarding what works and what
doesn?t, and point to where we intend to go in
the future.
2 Background
2.1 How The Program Came About
Our program is hosted by the Department of
Computing at Macquarie, which offers a typical
range of computer science courses. At this
university, standard undergraduate degree
programs are three years in length. Students
may elect to stay on for a fourth year in order to
obtain an honours degree, although in a
marketable area like computing, relatively few
students stay on beyond third year. The
teaching year is split into two thirteen week
semesters, with the first semester running from
March through June and the second semester
from August to November.
In 2000, we obtained government funding
to set up an undergraduate program in
language technology.1 To obtain this funding,
we argued that skills in the language
technologies were critical to the development of
the next generations of computer interfaces,
echoing statements made by many both in
industry and academia. Central to our proposal
was the identification of the twin streams of (a)
spoken language interaction and (b) smart text
processing, particularly with regard to the Web;
we took the view that these two major areas
would define the future of commercial NLP
activities over the next five years. Our proposal
emphasised heavily a practical orientation,
whereby we set our goal to be the training of
knowledge workers who will design and
develop practical applications in these areas.
Our proposal was supported by a number of
industry partners, including the Australian
branches of Motorola, Sun Microsystems,
Philips Speech Systems, and the government
research agency CSIRO.
1 We will refrain from entering into an argument
here as to the appropriate semantic distinctions
between the terms ?language technology?, ?natural
language processing? and ?computational linguistics?.
For current purposes, we?ll simply assume that all
three terms effectively cover approximately the same
territory.
                     July 2002, pp. 27-32.  Association for Computational Linguistics.
              Natural Language Processing and Computational Linguistics, Philadelphia,
         Proceedings of the Workshop on Effective Tools and Methodologies for Teaching
2.2 Our Philosophical Orientation
Our perception was that, in many institutions,
natural language processing and computational
linguistics courses tended to share two
particular characteristics.
First, relatively few institutions have more
than one course at undergraduate level that
provides material in this area. In many cases,
material in NLP or CL appears only as part of a
more general course on Artificial Intelligence.
This is of course determined by a range of local
factors, including inevitably the interests and
knowledge of available staff. However, an
important factor in many institutions that do not
have a long-established and strong research
group in the area is the widely-held sentiment
that NLP is a somewhat peripheral topic, or a
subject of purely theoretical interest. This makes
it hard for those staff who are interested in
teaching in this area to argue for a significant
presence in the curriculum.
A second observation is that the material
taught in introductory courses often tends to
focus on what we might call computational
syntax: writing grammars and building parsers.
Again, there are good reasons for this: some
would argue that you can?t do much else until
this material is covered, and this is clearly the
corner of NLP that is most well-established with
consolidated results, as reflected by the balance
of coverage found in texts such as [Allen 1995]
and [Jurafsky and Martin 2000], and, perhaps
less so than in the past, the topic coverage at
conferences such as ACL and Coling.2
With regard to the first of these
observations, we take a strong position. If, as a
community, we believe our own rhetoric about
NLP being critical for machine interfaces and
information processing technologies of the
future, then NLP needs to become a much more
central part of computing curricula: every
student should be exposed to this area. Our
desire, presumably shared by most who work in
the area, is to see the field of NLP grow, with
many more knowledgeable practitioners,
particularly in industry.
2 One of the authors recently completed a book
project that had as its goal the production of a
resource that would meet this concern by providing a
more balanced coverage of different aspects of NLP:
see Dale et al[2000]. Unfortunately, this book is too
large and expensive in its current form for use in our
courses.
With regard to our second observation,
however, we take the view that the focus
adopted in much undergraduate teaching in this
area does not support this goal as well as it
might. Teaching students about grammars and
parsers may serve as a suitable introduction to
further study in the area, but the bulk of
students who undertake undergraduate degrees
will go on to work in industry; only a minority
are likely to work in research laboratories or
undertake doctoral studies. Consequently,
those graduates who find themselves in a
position where they might have the opportunity
to use language processing techniques for the
development of sophisticated applications are
unlikely to have the full range of tools they need
at their disposal. The relatively narrow focus of
much undergraduate NLP teaching may also be
in part responsible for the fairly widespread
view amongst the uninitiated that NLP is
basically about parsing and not much else. This
perception results in occasional postings to
bulletin boards where senders from outside the
NLP research community request a ?parser?,
with their queries expressed in terms that make
it clear that they believe this one component will
solve all their NLP problems.
2.3 The Importance of the Job Market
We believe that if NLP is really to grow into a
field of substantial visibility and worth in the
wider industry community, there is a need to
raise the status of study in NLP beyond that of a
niche interest. The key to making this happen is
to emphasize the practical utility of work in the
field.
There is a real chicken-and-egg situation
here. We will only see an explosion in the
number of real NLP applications if there are
more well-rounded NLP practitioners working
in industry exploring and developing those
applications; but students are very savvy about
the job market, and, faced with a choice, are
unlikely to choose an NLP course over, say, a
networking course, when faced with the relative
proportions of job ads they see in the press and
on the web.
There are two related consequences of this.
First, evangelism is critical: we need to get more
trained students out there, offering NLP
solutions to problems. At the same time, we
need to give students concepts and techniques
that enable them to provide those solutions. We
need to provide material that students can see is
relevant, and that can be used in many contexts.
In our analysis, the job market for skills in
language processing, to the extent that it is
identifiable, consists of two major segments.
First, and most obviously, there are
companies that develop voice applications: there
are a great many companies now working in
this area, and voice recognition is a recognized
industry sector.
Second, there are companies that might use
NLP techniques in developing applications that
process, maintain and reuse documents,
whether on the desktop or on the Web. While
the first of these segments is quite clearly
identifiable, it is much more difficult to identify
a sector that focuses on using NLP techniques
on text. With some notable exceptions (and
these are largely small startups), we do not tend
to find companies whose focus is NLP. This is
not really surprising; NLP is just one tool
amongst many that might be used in document
processing, and document processing is
something that crops up in many contexts.
We therefore have a particular challenge
here: we need to communicate to students that
NLP is something they may be able to use in
their future careers, but we can?t point to many
job ads that specifically request NLP skills. The
intuition of those working in the field is that this
stuff ought to be something that can make a
difference in the processing of documents, but
there is not a lot of visible evidence that it is
being used in those situations. Anecdotal
personal experience suggests that many
companies would benefit from the application
of NLP skills but are not aware of this. One
suspects that organizations may often be
making use of techniques that we might want to
think of as NLP, but that these techniques are
not recognized as such.
3 The Program
Given the above, our goal was to construct a
range of courses that covered a broad range of
material that students might be able to use in
their subsequent careers. To emphasise the
practical orientation of what we wanted to do,
we deliberately pitched the program as being
concerned with Language Technology, rather
than as a program in either Natural Language
Processing or Computational Linguistics.
There is clearly something of an evangelical
element to this: we wanted to make students
aware of a broad range of techniques that we
would label Language Technology, with the
goal that, over time and as these students enter
the work force, an awareness would start to
spread that these techniques are widely usable.
This is not a short-term strategy: it takes several
years for the results of these efforts to permeate
through the system to a stage where they can be
evaluated, but it is essential to get started.
In this section, we present a summary of the
material we deliver in the courses that make up
our program. More detail on each of these
courses, and the program as a whole, can be
found at http://www.clt.mq.edu.au/Teaching.
The program consists of four courses that focus
principally on Language Technology, and an
additional course that looks more broadly at
technologies for working with the web. Figure 1
shows the prerequisite structure that currently
holds between these courses.
3.1 Comp248: An Introduction to
Natural Language Processing
Taught in the second half of second year, this is
the course in our program that most closely
matches the typical undergraduate NLP course.
The design of this course was driven by a desire
to show students that they could build a useful,
functioning application using NLP techniques;
to this end, we felt it was important not to teach
only computational syntax, but also something
about semantics. Our position here is that
syntactic processing is only a means to an end,
348:
Intelligent Text
Processing
248:
Introduction to
NLP
249:
Web Technology
349:
Interactive NL
Systems
448:
Advanced
Topics in NLP
Figure 1 : The Prerequisite Structure
and we felt it important to quickly get students
to the stage where they could actually see some
practical import of what they were doing. To
this end, in the first half of the course we take a
fairly standard approach to teaching Prolog,
whereby the students do some rudimentary
morphological processing, build some Definite
Clause Grammars, and learn about parsing
techniques. In the second half of the course, we
add semantics to the mix: although we teach an
introduction to lambda calculus at this stage, for
the practical work we focus on a much
shallower approach to semantics (effectively
semantic grammars), and the students build a
NL database query system that allows them to
ask questions of a database of flights. Along the
way they learn about unification-based
grammar, case frames, lexical resources,
WordNet, and semantic networks. The guiding
principle throughout is relevance to building a
practical application.
3.2 Comp249: Web Technology
Although this course is part of our Language
Technology program, it does not contain a
significant language technology element (at
least as the term is currently construed). It turns
out that the background material taught here
has proven to be very useful in other courses we
teach, so we are considering binding this course
more tightly to the others. The course covers:
Perl programming, web design, client-server
computing, search engines, XML and related
technologies, database integration, privacy and
security, VoiceXML, and content management;
inevitably, with such broad coverage, most
topics are treated relatively briefly.
Our goal for this course is to target a
student body who have little awareness of what
NLP is and to get them to see LT in a wider
perspective. The success of this course, which is
by far the most popular of the units in the
program, has led us to explore better ways of
leveraging this interest.
3.3 Comp348: Intelligent Text
Processing
At the third year level, we offer two courses that
take the second year material as a base. We
noted earlier that we viewed the job market as
consisting of two relatively distinct sectors, one
concerned with voice processing and one
concerned with document processing. This
perception is very deliberately reflected in the
individual biases of the third year offerings;
Comp348 addresses the needs of document
processing, whereas Comp349, discussed later,
leans more towards voice processing.
The course on intelligent text processing
covers basics of text processing using Perl;
tokenisation and sentence segmentation, text
summarisation; information retrieval; corpus-
based approaches, part of speech tagging, word
sense disambiguation, information extraction;
and machine translation. Again, this is a lot of
material to cover, and inevitably we only skim
the surface of many topics. However, in the first
offering of the course, students did significant
assignments in both text summarisation (using
sentence extraction) and information extraction.
The latter assignment was run roughly along the
lines of the Message Understanding
Conferences: using conference announcements
as a data set, the students were provided with a
training set on the basis of which they built an
information extraction system; this was then
tested against unseen data, and scores were
automatically derived. Now in its second
offering, our intention is to use anaphor
resolution as the focus of an assignment.
Our goal in this course is to provide
students with a toolset for text processing from
a language technology perspective. We focus on
relatively shallow methods, since these are the
methods students are most likely to find
themselves using in their subsequent careers.
Our driving aim here is for our alumni to
recognize that LT provides solutions.
3.4 Comp349: Interactive Natural
Language Systems
As already indicated, this course aims to
provide knowledge that students need in order
to be effective in the voice processing industry
sector.
The focus here is on, effectively, text- and
speech-based dialog systems. In the first half of
the course, we cover a significant amount of
relatively theoretical material, covering question
answering systems, database interfaces, and
answer extraction. Students build a quite
sophisticated text-based natural language query
system.
In the second half of the course, we attempt
to apply the theoretical ideas in the very
practical context of building spoken language
dialog systems. We begin by using the CSLU
Toolkit3, which the students use to build a voice
banking application. We then introduce
VoiceXML in some detail; using a PC-based
development environment, students build a
simple flight reservations system.4
We place a heavy emphasis here on aspects
of voice user inferface (VUI) design; in the
practical half of the course, the materials we use
take a similar approach to that taken in vendor
courses that aim to train dialog designers and
grammar writers. At the same time, we have as
an important aim a clear exposition of the
relationship between the ideas explored in
research systems and commercially deployed
systems; in practice it can be very hard to see a
path from the former to the latter. We make
clear to students that our goal is to teach them
how to build practical dialog applications now,
but to get them to think about what the next
generations of such applications might be in the
light of the results that come out of research
laboratories.
3.5 Comp448: Advanced Topics in
Natural Language Processing
For those students who stay on for a fourth year,
we run a course that is more driven by a
selection of specific research topics. At the time
of writing, the first offering of this course is
being delivered. We are using the course to
cover in more depth core topics that are only
really touched upon in earlier courses, with
more detailed exploration of word sense
disambiguation, anaphora resolution, discourse
structure and natural language generation. The
course is seminar-based, with a high proportion
3 This toolkit provides an excellent environment
for teaching students to think about issues such as
dialog flow, as well as introducing them to many
other aspects of spoken language dialog systems. See
http:// cslu.cse.ogi.edu/toolkit/.
4 We have experimented with a number of
different VoiceXML development environments
which are freely available over the web; each has its
advantages and disadvantages. Currently we?ve had
most success with Motorola?s MADK : see
http://developers.motorola.com/developers/. At the
time of writing, however, this does not support the
new VoiceXML 2.0 standard, so we are considering
other alternatives.
of student presentations, and an assignment in
anaphor resolution.
The level of interest amongst students at
this level is such that we expect to offer
additional honours level courses later in the
current academic year.
4 Outcomes and Issues
The program has been operating since the
second half of 2000. Since that time, we have
taught Comp248 twice and Comp349 once;
Comp249 and Comp348 are currently being
taught for the second time; and Comp448 is
being taught for the first time.
It is too early to establish to what extent the
material we have taught is impacting on
graduates? work practices: the first students to
complete degrees that incorporate our courses
are only now graduating. However, we have
made use of a number of feedback and review
mechanisms over the last 18 months, and these
have already provided us with new ideas for
how to improve what we are trying to do.
4.1 Evaluating Course Content
We make use of the typical infrastructure made
available for evaluation purposes: student-staff
liaison committees, formal questionnaires, and
also a significant amount of informal feedback
through discussions with students. We also
have a management advisory board with
representation from industry; this meets twice a
year to review the development of the program
and to comment on its industrial relevance.
Generally, the courses have been very well
received by the students who take them. Our
advisory board is very comfortable with the
material we teach, but we suffer here from the
problem that the voice recognition industry is
better represented here than the hard-to-define
document processing industry alluded to
earlier. So, we have strong evidence that
students find the material interesting,
challenging and informative; our industry
partners think we are going in the right
direction; but we have yet to demonstrate that
the wider industry community will see a benefit
from students who have grasped this material.
4.2 Course Materials
We have faced a not insignificant problem in
finding appropriate course materials for these
courses, with the consequence that we have had
to develop most things from scratch. For the
first offering of Comp248, the introductory NLP
course, we used Allen [1995]; in the second
offering, we found Covington [1994] to be more
useful. Although this is technically out of print,
Prentice Hall has a technology for producing
short print runs on demand.
The materials problem was more severe in
our third year courses, since there are no even
vaguely adequate textbooks for the material we
wanted to cover. We provide students with a
comprehensive reading packet, but it is not easy
to find appropriate survey or introductory
readings in the various topic areas we cover. As
a consequence of this we are exploring the
possibility of writing a textbook that covers the
material in each of these courses.
5 Lesssons Learned and Future
Directions
Eighteen months from the start of the program,
we are reasonably assured that we are going in
the right direction; some things, inevitably,
require fine tuning. We note here some key
consequences of our experiences so far.
5.1 Voice Captures the Imagination
Perhaps not surprisingly, it is the study of voice
recognition that has really captured students?
imaginations. The level of enthusiasm
generated in a laboratory full of students
wearing headsets talking to their machines is
wonderful to watch (although the working
environment doesn?t do a lot for speech
recognizer accuracy). With this in mind, we are
reworking our second year course, Comp248, so
that it will contain some of the voice material
currently used in third year. We are also
considering an emphasis here on technology
that students might meet outside of the
curriculum, such as chatterbots. Our strategy
here is to entice students into the area with
appealing content, and draw them into the more
theoretically challenging material in later
courses.
5.2 Document Processing as a Theme
It has become obvious that our Web Technology
course could play a more coherent role in our
program. One obvious direction we are
pursuing is to cement the two strands identified
earlier even further, by seeing the Web
Technology course specifically as a precursor for
the Intelligent Text Processing course. At the
same time, we are considering broadening the
third year course to cover Document Processing
more generally, as a way of making its relevance
more apparent; a shift of this kind might also
permit the inclusion of more material on
information retrieval and related technologies,
which are of some significance from an industry
perspective.
5.3 Linguistic Background
We have met the common, and not unexpected,
problem that some students do not have a
sufficient grasp of linguistic matters to perform
satisfactorily in this area. To this end, we have
initiated the introduction of a first year course
that covers basic aspects of linguistics, logic and
computation, taught by ourselves in conjunction
with the University?s Departments of
Philosophy and Linguistics.
5.4 Conclusions
So far, our program has been seen as very
successful from an academic perspective, and
has generated significant interest amongst
students. Our next challenge is to persuade the
wider industry to see students with this training
as very valuable assets. We have instituted an
alumni program that will attempt to track these
students, with the expectation of some
preliminary feedback being available by the end
of the calendar year.
References
James Allen [1995] Natural Language
Understanding. Benjamin Cummings, Menlo
Park, CA.
Michael Covington [1994] Natural Language
Processing for Prolog Programmers. Prentice Hall,
NJ.
Robert Dale, Hermann Moisl and Harold
Somers [2000] Handbook of Natural Language
Processing. Marcel Dekker, NY.
Daniel Jurafsky and James Martin [2000] Speech
and Language Processing: An Introduction to
Natural Language Processing, Computational
Linguistics and Speech Recognition. Prentice Hall,
NJ.
Answer Extraction 
Towards better Evaluations of NLP  Systems 
Ro l f  Schwi t te r  and D iego  Mo l l~  and Rache l  Fourn ie r  and Michae lHess  
Depar tment  of Informat ion Technology 
Computat iona l  Linguistics Group 
University of Zurich 
CH-8057 Zurich 
\[schwitter, molla, fournier, hess\] @ifi. unizh, ch 
Abst rac t  
We argue that reading comprehension tests are 
not particularly suited for the evaluation of 
NLP systems. Reading comprehension tests are 
specifically designed to evaluate human reading 
skills, and these require vast amounts of world 
knowledge and common-sense r asoning capa- 
bilities. Experience has shown that this kind of 
full-fledged question answering (QA) over texts 
from a wide range of domains is so difficult for 
machines as to be far beyond the present state 
of the art of NLP. To advance the field we pro- 
pose a much more modest evaluation set:up, viz. 
Answer Extraction (AE) over texts from highly 
restricted omains. AE aims at retrieving those 
sentences from documents that contain the ex- 
plicit answer to a user query. AE is less ambi- 
tious than full-fledged QA but has a number of 
important advantages over QA. It relies mainly 
on linguistic knowledge and needs only a very 
limited amount of world knowledge and few in- 
ference rules. However, it requires the solution 
of a number of key linguistic problems. This 
makes AE a suitable task to advance NLP tech- 
niques in a measurable way. Finally, there is a 
real demand for working AE systems in techni: 
cal domains. We outline how evaluation proce- 
dures for AE systems over real world domains 
might look like and discuss their feasibility. 
1 On  the  Des ign  o f  Eva luat ion  
Methods  for  NLP  Systems 
The idea that the systematic and principled 
evaluation of document processing systems is 
crucial for the development of the field as a 
whole has gained wide acceptance in the com- 
munity during the last decade. In a num- 
ber of large-scale projects (among them TREC 
(Voorhees and Harman, 1998) and MUC (MUC- 
7, 1998)), evaluation procedures for specific 
types of systems have been used extensively, and 
refined over the years. Three things were com- 
mon to these evaluations: First, the systems to 
be evaluated were each very closely tied to a par- 
ticular task (document retrieval and information 
extraction, respectively). Second, the evalua- 
tion was of the black box type, i.e. it considered 
only system input-output relations without re- 
gard to the specific mechanisms by which the 
outputs were obtained. Third, the amount of 
data to be processed was enormous (several gi- 
gabytes for TREC). 
There is general agreement that these com- 
petitive evaluations had a striking and bene- 
ficial effect on the performance of the various 
systems tested over the years. However, it is 
also recognized (albeit less generally) that these 
evaluation experiments also had the, less ben- 
eficial, effect that the participating systems fo- 
cussed increasingly more narrowly on those few 
parameters that were measured in the evalua- 
tion, to the detriment of more general prop- 
erties. In some cases this meant that power- 
ful and linguistically interesting but slow sys- 
tems were dropped in favour of shallow but fast 
systems with precious little linguistic content. 
Thus the system with which SRI participated 
in the MUC-3 evaluation in 1991, TACITUS 
(Hobbs et al, 1991), a true text-understanding 
system, was later replaced by FASTUS (Appelt 
et al, 1995; Hobbs et al, 1996), a much sim- 
pler, and vastly faster, information extraction 
system. The reason was that TACITUS was 
spending so much of its time attempting to make 
sense of portions of the text that were irrelevant 
to the task that recall was mediocre. We ar- 
gue that the set-up of these competitive valu- 
ations, and in particular the three parameters 
mentioned above, drove the development of the 
participating systems towards becoming impres- 
20 
sive feats of engineering, fine-tuned to one very 
specific task, but with limited relevance outside 
this task and with little linguistically relevant 
content. We argue that these evaluations there- 
fore did not drive progress in Computational 
Linguistics very much. 
We therefore think it a timely idea to con- 
ceive of evaluation methodologies which mea- 
sure the linguistically relevant functions of NLP 
systems and thus advance Computational Lin- 
guistics as a science rather than as an engineer- 
ing discipline. The suggestion made by the or- 
ganizers of this workshop on how this could be 
achieved has-four comPonents. First, they sug- 
gest to use full-fledged text-based question an- 
swering (QA) as task. Second, they suggest a 
relatively small amount off text (compared with 
the volumes of text used in TREC) as test data. 
Third they (seem to) suggest o .use texts from 
a wide range off domains. Finally they suggest 
to use pre-existing question/answer pairs, de- 
veloped for and tested on humans, as evaluation 
benchmark (Hirschman et al, 1999). 
However, our experience in the field leads us 
to believe that this evaluation set-up will not 
help Computational Linguistics as much as it 
would be needed, mainly because it is way too 
ambitious. We fear that this fact will force de- 
velopers, again, to design all kinds of ad-hoc so- 
lutions and efficiency hacks which will severely 
limit the scientific relevance of the resulting sys- 
tems. We argue that three of the four compo- 
nents of the suggested set-up must be reduced 
considerably in scope to make the test-bed help- 
ful. 
First, we think the task is too difficult. Full- 
fledged QA on the basis of natural language 
texts is far beyond the present state of the 
art. The example of the text-based QA sys- 
tem LILOG (Herzog and Rollinger, 1991) has 
shown that the analysis of texts to the depth 
required for real QA over their contents is so re- 
source intensive as to be unaffordable in any real 
world context.  After an investment of around 65 
person-years of work the LILOG system could 
answer questions over a few (reputedly merely 
three) texts of around one page length each from 
an extremely narrow domain (city guides and 
the like). We think it is fair to say that the situ- 
ation in our field has not changed enough in the 
meantime to invalidate this finding. 
Second, we agree that the volume off data to 
be used should be relatively small. We must 
avoid that the sheer pressure of the volumes of 
texts to be processed forces system developers 
to use shallow methods. 
Third, we think it is very important o restrict 
the domain of the task. We certainly do not ar- 
gue in favour of some toy domain but we get 
the impression that the reading comprehension 
texts under consideration cover a far too wide 
range of topics. We think that technical man- 
uals are a better choice. They cover a narrow 
domain (such as computer operating systems, 
or airplanes), and they also use a relatively re- 
stricted type of language with a reasonably clear 
semantic foundation. 
Fourth, we think that tests that are specif- 
ically designed to evaluate to what extent a 
human being understands a text are intrinsi- 
cally unsuitable for our present purposes. Al- 
though it would admittedly be very convenient 
to have "well written" texts, "good" questions 
about them and the "correct" answers all in one 
package, the texts are not "real world" language 
(in that they were written specifically for these 
tests), and the questions are:just far too difficult, 
primarily because they rely on exactly those 
components of language understanding where 
humans excel and computers are abominably 
poor (inferences over world knowledge). 
In Section 2 we outline what kinds of prob- 
lems would have to be solved by a QA sys- 
tem if it were to answer the test questions 
given in (WRC, 2000). Most of the prob- 
lems would require enormous amounts of world 
knowledge and vast numbers of lexical inference 
rules for a solution, on top of all the "classi- 
cal" linguistic problems our field has been strug- 
gling with (ambiguities, anaphoric references, 
synonymy/hyponymy).  We will then argue in 
Section 3 that a more restricted kind of task, 
Answer Extraction, is better suited as experi- 
mental set-up as it would focus our forces on 
these unsolved but reasonably well-understood 
problems, rather than divert them to the ill- 
understood and fathomless black' hole of world 
knowledge. In Section 4, we will finally outline 
how evaluation procedures in this context might 
look like. 
21 
...k 
2 Why Read ing  Comprehens ion  
Tests  v ia QA are  Too :Difficult 
Reading comprehension tests are designed to 
measure how well human readers understand 
what they read. Each story comes with a set 
of questions about information that is stated 
or implied in the text. The readers demon- 
strate their understanding of the story by an- 
swering the questions about it. Thus, read- 
ing comprehension tests assume a cognitive pro- 
cess of human beings. This process involves ex- 
panding the mental model of a text by using 
its implications and presuppositions, retrieving 
the stored information, performing inferences to 
make implicit information explicit, and generat- 
ing the surface strings that express this infor- 
mation. Many different forms of knowledge take 
part in this process: linguistic, procedural and 
world knowledge. All these forms coalesce in 
the memory of the reader and it is very difficult 
to clearly distinguish and reconstruct them in a 
QA system. At first sight the story published in 
(WRC, 2000) is easy to understand because the 
sentences are short and cohesive. But it turns 
out that a classic QA system would need vast 
amounts of knowledge and inference rules in or- 
der to understand the text and to give sensible 
answers. 
Let us investigate what kind of information 
a full-fledged QA system needs in order to an- 
swer the questions that come with the reading 
comprehension test (Figure 1) and discuss how 
difficult it is to provide this information. 
To answer the first question 
(1) Who collects maple sap? 
the system needs to know that the mass noun 
sap in the text sentence 
Farmers collect the sap. 
is indeed the maple sap mentioned in the 
question. The compound noun maple sap is a se- 
mantically narrower term than the noun sap and 
encodes an implicit relation between the first el- 
ement maple and the head noun sap. This rela- 
tion names the origin of the material. Since no 
explicit information about the relation between 
the two objects is available in the text an ideal 
QA system would have to assume such a relation 
by a form of abductive reasoning. 
How.Maple  Syrup is Made 
Maple syrup comes from sugar maple trees. At 
one time, maple syrup was used to make sugar. 
This is why the tree is called a "sugar" maple 
tree. 
Sugar maple trees make sap. Farmers collect he 
sap. The best time to collect sap is in February 
land March. The nights must be cold and the 
days warm. 
The framer drills a few small holes in each tree. 
He puts a spout in each hole. Then he hangs 
a bucket on the end of each spout. The bucket 
has a cover to keep rain and snow out. The sap 
drips into the bucket. About 10 gallons of sap 
come from each hole. 
1. Who collects maple sap? 
(Farmers) 
2. What does the farmer hang from a spout? 
(A bucket) 
3. When is sap collected? 
(February and March) 
4. Where does the maple sap come from? 
(Sugar maple trees) 
5. Why is the bucket covered? 
(to keep rain and snow out) 
Figure 1: Reading comprehension test 
To answer the second question 
(2) What does the farmer hang from a spout? 
successfully the system would need at least 
three different kinds of knowledge: 
First, it would need discourse knowledge to 
resolve the intersentential co-reference between 
the anaphor he and the antecedent the farmer 
in the following text sequence: 
The farmer drills- a few small holes in each 
tree. \[...\] Then he hangs a bucket ... 
Although locating antecedents has proved to 
be one of the hard problems of natural lan- 
guage processing, the anaphoric reference reso- 
lution can be done easily in this case because the 
antecedent is the most recent preceding noun 
phrase thgt agrees in gender, number and per- 
son. 
22 
Second, the system would require linguistic 
knowledge to deal with the synonymy relation 
between hang on and hang .from, and the at- 
tachment ambiguity of the prepositional phrase 
used in the text sentence and the query. 
Third, the system needs an inference rule that 
makes somehow clear that the noun phrase a 
spout expressed in the query is entailed in the 
more complex noun phrase the end of each spout 
in the text sentence. Additionally, to process 
this relation the system would require an infer- 
ence rule of the form: 
IF X does Y to EACH Z 
THEN X does Y to A Z. 
The third question 
(3) When is sap collected? 
asks for the time point when' ~ap is collected 
but the text gives only a rule-like recommenda- 
tion 
The best time to collect sap is in February 
and March. 
with an additional constraint 
The nights must be cold and the days warm. 
and does not say that the sap is in fact col- 
lected in February and March. The bridging 
inference that the system would need to model 
here is not founded on linguistic knowledge but 
on world knowledge. Solving this problem is 
very hard. It could be argued that default rules 
may solve such problems but it is not clear 
whether formal methods are able to handle the 
sort of default reasoning required for represent- 
ing common-sense reasoning. 
To give an answer for the fourth question 
(4) Where does the maple sap come .from? 
the system needs to know that maple sap 
comes from sugar maple trees. This informa- 
tion is not explicitly available in the text. In- 
stead of saying where maple sap comes from the 
text says where maple syrup comes from: 
Maple syrup comes .from sugar maple trees. 
23 
There exists a metonymy relation between 
these two compound nouns. The compound 
noun maple syrup (i.e. product) can only be 
substituted by maple sap (i.e. material), if the 
system is able to deal with metonymy. Together 
with the information in the sentence 
Sugar maple trees make sap. 
and an additional exical inference rule in 
form of a meaning postulate 
IF X makes Y THEN Y comes from X. 
the system could deduce (in theory) first sap 
and then by abductive reasoning assume that 
the sap found is maple sap. Meaning postulates 
are true by virtue of the meaning they link. Ob- 
servation cannot prove them false. 
To answer the fifth question 
(5) Why is the bucket covered? 
the system needs to know that the syntac- 
tically different expressions has a cover and is 
covered have the same propositional content. 
The system needs an explicit lexical inference 
rule in form of a conditional equivalence 
IF Conditions 
THEN X has a cover ~-> X is covered. 
that converts the verbal phrase with the nom- 
inal expression i to a the corresponding passive 
construction (and vice versa) taking the present 
context into consideration. 
As these concrete xamples how, the task of 
QA over this simple piece of text is frighten- 
ingly difficult. Finding the correct answers to 
the questions requires far more information that 
one would think at first. Apart from linguistic 
knowledge a vast amount of world knowledge 
and a number of bridging inferences are nec- 
essary to answer these seemingly simple ques- 
tions. For human beings bridging inferences 
are automatic and for the most part uncon- 
scious. The hard task consists in reconstructing 
all this information coming from different knowl- 
edge sources and modeling the suitable inference 
rules in a general way so that the system scales 
up. 
3 Answer  Ext ract ion  as an  
A l te rnat ive  Task  
An alternative to QA is answer extraction (AE). 
The general goal of AE is the same as that of 
QA, to find answers to user queries in textual 
documents. But the way to achieve this is differ- 
ent. Instead of generating the answer from the 
information given in the text (possibly in im- 
plicit form only), an AE system will retrieve the 
specific sentence(s) in the text that contain(s) 
the explicit answer to the query. In addition, 
those phrases in the sentence that represent the 
explicit a_nswer to the query may be highlighted. 
For example, let us assume that the following 
sentence is in the text (and we are going to use 
examples from a technical domain, that of the 
Unix user's manual): 
(1) cp copies the contents of filenamel onto 
filename2. 
If the user asks the query 
Which command copies files? 
a QA system will return: 
cp 
However, an AE system will return all the 
sentences in the text that directly answer the 
question, among them (1). 
Obviously, an AE system is far less power- 
ful than a real QA system. Information that 
is not explicit in a text will not be found, let 
alone information that must be derived from 
textual information together with world knowl- 
edge. But AE has a number of important ad- 
vantages over QA as a test paradigm. First, an 
obvious advantage of this approach is that the 
user receives first-hand information, right from 
the text, rather than system-generated replies. 
It is therefore much easier for the user to de- 
termine whether the result is reliable. Second, 
it is a realistic task (as the systems we are de- 
scribing below proves) as there is no need to 
generate natural language output, and there is 
less need to perform complex inferences because 
it merely looks up things in the texts which axe 
explicitly there. It need not use world knowl- 
edge. Third, it requires the solution of a num- 
ber of well-defined and truly important linguistic 
problems and is therefore well suited to measure, 
and advance, progress in these respects. We will 
come to this later. And finally, there is a real 
demand for working AE systems in technical do- 
mains since the standard IR approaches just do 
not work in a satisfactory manner in many appli- 
cations where the user is in pressure to quickly 
find a specific answer to a specific question, and 
not just (potentially long) lists of pointers to 
(potentially large) documents that may (or may 
not) be relevant o the query. Examples of ap- 
plications are on-line software help systems, in- 
terfaces to machine-readable technical manuals, 
help desk systems in large organizations, and 
public enquiry systems accessible over the Web. 
The basic procedure we use in our approach 
to AE is as follows: In an off-line stage, the 
documents are processed and the core mean- 
ing of each sentence is extracted and stored as 
so-called minimal logical forms. In an on-line 
stage, the user query is also processed to pro- 
duce a minimal ogical form. In order to retrieve 
answer sentences from the document collection, 
the minimal logical form of the query is proved, 
by a theorem prover, over the minimal logical 
forms of the entire document collection (Moll~t 
et al, 1998). Note that this method will not re- 
trieve patently wrong answer sentences like bkup 
files all copies on the hard disk in response to 
queries like Which command copies files? This 
is the kind of response we inevitably get if we 
use some variation of the bag-of-words approach 
adopted by IR based systems not performing 
any kind of content analysis. 
We are currently developing two AE sys- 
tems. The first, ExtrAns, uses deep linguis- 
tic analysis to perform AE over the Unix man- 
pages. The prototype of this system uses 500 
Unix manpages, and it can be tested over the 
Web \[http://www.ifi.unizh.ch/cl/extrans\]. In 
the second (new) project, WebExtrAns, we in- 
tend to perform AE-over the "Aircraft Main- 
tenance Manual" of the Airbus 320 (ADRES, 
1996). The larger volume of data (about 900 kg 
of printed paper!) will represent an opportunity 
to test the scalability of an AE system that uses 
deep linguistic analysis. 
There is a number of important areas of re- 
search that ExtrAns and WebExtrAns, and by 
extension any AE system, has to focus on. First 
of all, in order to generate the logical form of the 
24 
sentences, the following must be tackled: Find- 
ing the verb arguments, performing disambigua- 
tion, anaphora resolution, and coping with nom- 
inalizations, passives, ditransitives, compound 
nouns, synonymy, and hyponymy (Moll~t et al, 
1998; Mollh and Hess, 2000). Second, the very 
idea of producing the logical forms of real-world 
text requires the formalization of the logical 
form notation so that it is expressive nough but 
still remaining usable (Schwitter et al, 1999). 
Finally, the goal of producing a practical system 
for a real-world application eeds to address the 
issue of robustness and scalability (Moll~t and 
Hess, 1999).-- 
Note that the fact that AE and QA share the 
same goal makes it possible to start a project 
that initially performs AE, and gradually en- 
hance and extend it with inference and gener- 
ation modules, until we get a full-fledged QA 
system. This is the long-time g0al of our cur- 
rent series of projects on AE. 
4 Eva luat ing  the  Resu l ts  
Instead of using reading comprehension tests 
that are meant for humans, not machines, we 
should produce the specific tests that would 
evaluate the AE capability of machines. Here 
is our proposal. 
Concerning test queries, it is always better to 
use real world queries than queries that were ar- 
tificially constructed to match a portion of text. 
Experience has shown time and again that real 
people tend to come up with questions different 
from those the test designers could think of. By 
using, as we suggest, manuals of real world sys- 
tems, it is possible to tap the interaction of real 
users with this system as a source of real ques- 
tions (we do this by logging the questions ub- 
mitted to our system over the Web). Another 
way of finding queries is to consult he FAQ lists 
concerning a given system sometimes available 
on the Web. In both cases you will have to fil- 
ter out those queries that have no answers in the 
document collection or that are clearly beyond 
the scope of the system to evaluate (for exam- 
ple, if the inference needed to answer a query is 
too complex, even for a human judge). 
Concerning answers, the principal measures 
for the AE task must be recall and precision, 
applied to individual answer sentences. Recall 
is the number of correct answer sentences the 
system retrieved divided by the total number 
of correct answers in the entire document col- 
lection. Precis ion is the number of correct an- 
swer sentences the system retrieved ivided by 
the total number of answers it returned. As is 
known all too well, recall is nearly impossible to 
determine in an exact fashion for all but toy ap- 
plications ince the totality of correct answers in 
the entire document collection has to be found 
mainly by hand. Almost certainly one will have 
to resort to (hopefully) representative samples 
of documents to arrive at a reasonable approxi- 
mation to this value. Precision is easier to deter- 
mine although even this step can become very 
time consuming in real world applications. 
If, on the other hand, one only needs to do 
an approximate evaluation of the AE system, it 
would be possible to find a representative s t of 
correct answers by making a person write the 
ideal answers, and then automatically finding 
the sentences in the documents that are seman- 
tically close to these ideal answers. Semantic 
closeness between a sentence and the ideal an- 
swer can be computed by combining the suc- 
c inctness and correctness of the sentence with 
respect to the ideal answer. Succinctness and 
correctness are the counterparts ofprecision and 
recall, but on the sentence level. These mea- 
sures can be computed by checking the overlap 
of words between the sentence and the ideal an- 
swer (Hirschman et al, 1999), but we suggest a 
more content-based approach. 
Our proposal is to compare not words in a 
sentence, but their logical forms. Of course, this 
comparison can be done only if it is possible to 
agree on how logical forms should look like, to 
compute them, and to perform comparisons be- 
tween them. The second and third conditions 
can be fulfilled if the logical forms are simple 
lists of predicates that contain some minimal se- 
mantic information, as it is the case in ExtrAns 
(Schwitter et al, 1999). In this paper we will 
use a simplification of the minimal ogical forms 
used by ExtrAns. Below are two sentences with 
their logical forms: 
(1) rm removes one or more files. 
remove(x ,y ) ,  rm(x) ,  f i le(y) 
(2) csplit pr ints  the character counts .for each 
file created, and removes any files it creates 
i f  an error occurs. 
25 
print(x,y), csplit(x), character-count(y), 
remove(x ,z ) ,  fi le(z), create(x,z), oc- 
cur(e), error(e) 
As an example of how to compute succinct- 
ness and correctness, take the following ques- 
tion: 
Which command removes files? 
The ideal answer is a full sentence that con- 
tains the information given by the question and 
the information requested. Since rm is the com- 
mand used to remove files, the ideal answer is: 
rm removes  f i les.  
remove(x,y), rm(x), file(y) 
Instead of computing the overlap of words, 
succinctness and correctness ofa sentence can be 
determined by computing the overlap of predi- 
cates. The overlap of the predicates (overlap 
henceforth) of two sentences is the maximum 
set of predicates that can be used as part of the 
logical form in both sentences. The predicates 
in boldface in the two examples above indicate 
the overlap with the ideal answer: 3 for (1), and 
2 for (2). 
Succinctness of a sentence with respect o an 
ideal answer (precision on the sentence level) is 
the ratio between the overlap and the total num- 
ber of predicates in the sentence. Succinctness 
is, therefore, 3/3=1 for (1), and 2/8=0.25 for 
(2). 
Correctness of a sentence with respect o an 
ideal answer (recall on the sentence level) is the 
ratio between the overlap and the number of 
predicates in the ideal answer. In the exam- 
ples above, correctness i 3/3=1 for (1), and 
2/3=0.66 for (2). 
A combined measure of succinctness and cor- 
rectness could be used to determine the seman- 
tic closeness of the sentences to the ideal an- 
swer. By establishing a threshold to the seman- 
tic closeness, one can find the sentences in the 
documents that are answers to the user's query. 
The advantage of using overlap of predicates 
against overlap of words is that the relations be- 
tween the words also affect the measure for suc- 
cinctness and correctness. We can see this in 
the following artificial example. Let us suppose 
that the ideal answer to a query is: 
Madrid defeated Barcelona. 
defeat(x,y), madrid(x), barcelona(y) 
The following candidate sentence produces 
the same predicates: 
Barcelona defeated Madrid. 
defeat(x,y), madr id (y ) ,  barce lona(x)  
However, at most two predicates only can be 
chosen at the same time (in boldface), because 
of the restrictions of the arguments.  In the 
ideal answer, the first argument of "defeat" is 
Madrid and the second argument is Barcelona. 
In the candidate sentence, however, the argu- 
ments are reversed (the name of the variables 
have no effect on this). The overlap is, therefore, 
2. Succinctness and correctness are 2/3=0.66 
and 2/3=0.66, respectively. 
5 Conc lus ion  
We are convinced that reading comprehension 
tests are too difficult for the current state of 
art in natural language processing. Our anal- 
ysis of the Maple Syrup story shows how much 
world knowledge and inference rules are needed 
to actually answer the test questions correctly. 
Therefore, we think that a more restricted kind 
of task that focuses rather on tractable problems 
than on AI-hard problems of question-answering 
(QA) is better suited to take our field a step 
further. Answer Extraction (AE) is an alter- 
native to QA that relies mainly O n linguistic 
knowledge. AE aims at retrieving those exact 
passages of a document hat directly answer a 
given user query. AE is less ambitious than full- 
fledged QA since the answers are not generated 
from a knowledge base but looked up in the doc- 
uments. These documents come from a well- 
defined (technical) domain and consist of a rela- 
tively small volume of data. Our test queries are 
real world queries that express a concrete infor- 
mation need. To evaluate our AE systems, we 
propose besides precision and recall two addi- 
tional measures: succinctness and correctness. 
They measure the quality of answer sentences 
on the sentence level and are computed on the 
basis of the overlap of logical predicates. 
To round out the picture, we address the ques- 
tions in (WRC, 2000) in the view of what we said 
in this paper: 
26 
Q: Can such exams \[reading comprehension 
tests\] be used to evaluate computer-based lan- 
guage understanding effectively and e~ciently? 
A: We think that no language unders tand-  
ing system will currently be able to answer a sig- 
nificant proportion of such questions, which will 
make evaluation results difficult at best, mean- 
ingless at worst. 
Q: Would they provide an impetus and test 
bed for interesting and useful research? 
A: We think that the impetus they might pro- 
vide would drive development in the wrong di- 
rection, viz. towards the creation of (possibly 
impressive) engineering feats without much lin- 
guistically interestingcontent. 
Q: Are they too hard for current technology? 
A: Definitely, and by a long shot. 
Q: Or are they too easy, such that simple 
hacks can score high, although there is clearly 
no understanding involved? ., 
A: "Simple hacks" would almost certainly 
score higher than linguistically interesting meth- 
ods but not because the task is too simple but 
because it is far too difficult. 
References 
ADRES, 1996. A319/A320/A321 Aircraft 
Maintenance Manual. Airbus Industrie, 
Blagnac Cedex, France. Rev. May 1. 
Douglas E. Appelt, Jerry R. Hobbs, John Bear, 
David Israel, Megumi Kameyama, Andy 
Kehler, David Martin, Karen Myers, and 
Mabry Tyson. 1995. SRI International FAS- 
TUS system MUC-6 test results and analysis. 
In Proc. Sixth Message Understanding Con- 
\]erence (MUC-6), Columbia, Maryland. 
Otthein Herzog and Claus-Rainer Rollinger, ed- 
itors. 1991. Text Understanding in LILOG: 
Integrating Computational Linguistics and 
Artificial Intelligence - final report on the 
IBM Germany LILOG project, volume 546 of 
Lecture Notes in Computer Science. Springer- 
Verlag, Berlin. 
Lynette Hirschman, Marc Light, Eric Breck, and 
John D. Burger. 1999. Deep Read: A read- 
ing comprehension system. In Proc. A CL '99. 
University of Maryland. 
Jerry Hobbs, Douglas E. Appelt, John S. Bear, 
Mabry Tyson, and David Magerman. 1991. 
The TACITUS system: The MUC-3 experi- 
ence. Technical report, AI Center, SRI Inter- 
national, Menlo Park, CA. 
Jerry R. Hobbs, Douglas E. Appelt, John Bear, 
David Israel, Megumi Kameyama, Mark 
Stickel, and Mabry Tyson. 1996. FASTUS: 
A cascaded finite-state transducer for extract- 
ing information from natural-language t xt. 
In E. Roche and Y. Schabes, editors, Finite 
State Devices for Natural Language Process- 
ing. MIT Press, Cambridge, MA. 
Diego Moll~ and Michael Hess. 1999. On 
the scalability of the answer extraction sys- 
tem "ExtrAns". In Proc. Applications of 
Natural Language to Information Systems 
(NLDB'99), pages 219-224, Klagenfurt, Aus- 
tria. 
Diego Moll~ and Michael Hess. 2000. Deal- 
ing with ambiguities in an answer extrac- 
tion system. In Representation and Treatment 
of Syntactic Ambiguity in Natural Language 
Processing, Paris. ATALA. 
Diego Moll~, Jawad Berri, and Michael Hess. 
1998. A real world implementation f answer 
extraction. In Proc. of the 9th International 
Conference and Workshop on Database and 
Expert Systems. Workshop "Natural Language 
and Information Systems" (NLIS'98), pages 
143-148, Vienna, August. 
MUC-7. 1998. Proc. of the seventh mes- 
sage understanding conference (MUC-7). 
http://www.muc.saic.com. 
Rolf Schwitter, Diego Moll~, and Michael Hess. 
1999. Extrans - -  answer extraction from 
technical documents by minimal ogical forms 
and selective highlighting. In Proc. Third In- 
ternational Tbilisi Symposium on Language, 
Logic and Computation, Batumi, Georgia. 
http://www.ifi.unizh.ch/cl/. 
Ellen M. Voorhees and Donna Harman. 1998. 
Overview of the seventh Text REtrieval Con- 
ference (TREC-7). In Ellen M. Voorhees and 
Donna Harman, editors, The Seventh Text 
REtrieval Conference (TREC-7), number 
500-242 in NIST Special Publication, pages 1- 
24. NIST-DARPA, Government Printing Of- 
rice. 
WRC. 2000. Workshop on reading compre- 
hension texts as evaluation for computer- 
based language understanding systems. 
http://www.gte.com/AboutGTE/gto/anlp- 
naacl2000/comprehension.html. 
27 
Coling 2010: Poster Volume, pages 1113?1121,
Beijing, August 2010
Controlled Natural Languages for Knowledge Representation
Rolf Schwitter
Centre for Language Technology
Macquarie University
Rolf.Schwitter@mq.edu.au
Abstract
This paper presents a survey of research
in controlled natural languages that can be
used as high-level knowledge representa-
tion languages. Over the past 10 years
or so, a number of machine-oriented con-
trolled natural languages have emerged
that can be used as high-level interface
languages to various kinds of knowledge
systems. These languages are relevant to
the area of computational linguistics since
they have two very interesting properties:
firstly, they look informal like natural lan-
guages and are therefore easier to write
and understand by humans than formal
languages; secondly, they are precisely
defined subsets of natural languages and
can be translated automatically (and often
deterministically) into a formal target lan-
guage and then be used for automated rea-
soning. We present and compare the most
mature of these novel languages, show
how they can balance the disadvantages
of natural languages and formal languages
for knowledge representation, and discuss
how domain specialists can be supported
writing specifications in controlled natural
language.
1 Introduction
Natural languages are probably the most expres-
sive knowledge representation languages that ex-
ist; they are easy for humans to use and under-
stand, and they are so powerful that they can
even serve as their own metalanguages. Ironi-
cally, it is just this expressive quality that makes
natural languages notoriously difficult for a com-
puter to process and understand because a lot of
relevant information is usually not stated explic-
itly in an utterance but only implied by the hu-
man author or speaker. There exist ? of course ?
many useful resources and automated techniques
that partly compensate for the lack of this back-
ground knowledge, and there are many useful ap-
plications that require only shallow processing
of natural languages. But there exist ? without
doubt ? many potential application scenarios that
would benefit from deeper (axiom-based) knowl-
edge that can be created and modified in a human-
friendly way.
Formal languages (Monin, 2003) have been
suggested and used as knowledge representation
languages since they have a well-defined syntax,
an unambiguous semantics and support automated
reasoning. But these languages are often rather
difficult for domain specialists to understand and
cause a cognitive distance to the application do-
main that is not inherent in natural language.
One way to bridge the gap between a natural
language and a formal language is the use of a
controlled natural language (CNL) that can me-
diate between these languages. CNLs are engi-
neered subsets of natural languages whose gram-
mar and vocabulary have been restricted in a sys-
tematic way in order to reduce both ambiguity and
complexity of full natural languages.
Traditionally, CNLs have been grouped into
two broad categories: human-oriented CNLs and
machine-oriented CNLs (Huijsen, 1998). The
main objective of human-oriented CNLs is to im-
prove the readability and comprehensibility of
technical documentation (e.g. maintenance doc-
1113
umentation (ASD Simplified Technical English1)
and to simplify and standardise human-human
communication for specific purposes (e.g. for
trade or for air traffic control (see (Pool, 2006)
for an overview)). The primary goal of machine-
oriented CNLs is to improve the translatability
of technical documents (e.g. machine translation
(Nyberg and Mitamura, 2000)) and the acquisi-
tion, representation, and processing of knowledge
(e.g. for knowledge systems (Fuchs et al, 2008)
and in particular for the Semantic Web (Schwitter
et al, 2008)).
Human- and machine-oriented CNLs have been
designed with different goals in mind, and it is not
surprising that their coverage can be quite differ-
ent. O?Brien (2003) shows that there is not much
overlap between the rule sets of CNLs in these two
categories nor among the rule sets within a cate-
gory. But since the structure of these CNLs is usu-
ally simpler and more predictable than the struc-
ture of full natural language, CNLs are in general
easier for humans to understand and easier for a
computer to process. An ideal CNL for knowl-
edge representation should also be effortless to
write and expressive enough to describe the prob-
lem at hand.
In this paper, we will survey machine-oriented
CNLs that can be used for knowledge represen-
tation and can serve as high-level interface lan-
guages to knowledge systems. The rest of this pa-
per is structured as follows: In Section 2, we intro-
duce the most mature general-purpose CNLs and
discuss the motivation for their design and inves-
tigate their characteristics. In Section 3, we dis-
cuss some theoretical issues regarding the expres-
sivity and complexity of CNLs. Building on these
theoretical considerations, we look in Section 4
at a number of machine-oriented CNLs that have
been developed specifically as interface languages
to the Semantic Web. In Section 5, we discuss the
importance of supporting the writing process of
CNLs in an suitable way and compare three dif-
ferent techniques. In Section 6, we discuss differ-
ent approaches that have been used to evaluate the
writability and understandability of CNLs, and fi-
nally in Section 7, we present our conclusions.
1http://www.asd-ste100.org/
2 General-Purpose CNLs
In this section we focus on a number of machine-
oriented CNLs that have been designed to serve
as knowledge representation languages. These
CNLs are general-purpose languages in the sense
that they have not been developed for a spe-
cific scenario or a particular application domain.
These languages can be used where traditional
formal languages are used otherwise. The aim
of these languages is to equip domain specialists
with an expressive knowledge representation lan-
guage that is on the one hand easy to learn, use
and understand and on the other hand fully pro-
cessable by a computer.
2.1 Attempto Controlled English (ACE)
ACE (Fuchs et al, 2008) is a CNL that cov-
ers a well-defined subset of English that can be
translated unambiguously into first-order logic
via discourse representation structures (Kamp and
Reyle, 1993) and then be used for automated rea-
soning. ACE is defined by a small set of con-
struction rules that describe its syntax and a small
set of interpretation rules that disambiguate con-
structs that might appear ambiguous in full En-
glish. The vocabulary of ACE consists of pre-
defined function words (e.g. determiners, con-
junctions, and pronouns), some predefined fixed
phrases (e.g. there is, it is false that), and con-
tent words (nouns, proper names, verbs, adjec-
tives, and adverbs). ACE supports language con-
structs such as:
? active and passive verbs (and modal verbs);
? strong negation (e.g. no, does not) and weak
negation (e.g. is is not provable that);
? subject and object relative clauses;
? declarative, interrogative, imperative and
conditional sentences;
? various forms of anaphoric references to
noun phrases (e.g. he, himself, the man, X).
It is important to note that the meaning of words
in ACE is not predefined; the user is expected to
define their meaning by ACE sentences or import
these definitions from an existing formal ontology.
1114
Here is a simple example of an ACE text together
with a question:
Every company that buys at least three
machines gets a discount. Six Swiss
companies each buy one machine. A
German company buys four machines.
Who gets a discount?
Note that ACE uses disambiguation markers
(e.g. each) on the surface level and mathematical
background knowledge about natural numbers in
order to answer the question above. This mathe-
matical knowledge is implemented as a set of Pro-
log predicates which are executed during the proof
(question answering process).
ACE is supported by various tools2, among
them a text editor that helps users to construct cor-
rect ACE sentences with the help of hints and er-
ror messages, a parser that translates ACE texts
into discourse representation structures, a para-
phraser that reflects the interpretation of the ma-
chine in CNL, and a Satchmo-style reasoning en-
gine that can be used for consistency and redun-
dancy checking as well as for question answering.
Applications of ACE include software and hard-
ware specifications, agent control, legal and med-
ical regulations, and ontology construction.
2.2 Processable English (PENG)
PENG (White and Schwitter, 2009) is a CNL that
is similar to ACE but adopts a more light-weight
approach in the sense that it covers a smaller but
fully tractable subset of English. The language
processors of ACE and PENG are both based
on grammars that are written in a definite clause
grammar (DCG) notation. These DCGs are en-
hanced with feature structures and specifically de-
signed to translate declarative and interrogative
sentences into a first-order logic notation via dis-
course representation structures. In contrast to the
original version of ACE that uses the DCG di-
rectly and resolves anaphoric references only after
a discourse representation structure has been con-
structed, PENG transforms the DCG into a for-
mat that can be processed by a top-down chart
parser and resolves anaphoric references during
2http://attempto.ifi.uzh.ch/site/
tools/
the parsing process while a discourse representa-
tion structure is built up. PENG has been designed
for an incremental parsing approach and was the
first CNL that was supported by a predictive editor
(Schwitter et al, 2003). The PENG system pro-
vides text- and menu-based writing support that
removes some of the burden of learning and re-
membering the constraints of the CNL from the
user and generates a paraphrase that clarifies the
interpretation for each sentence that the user en-
ters. PENG?s text editor dynamically enforces
the grammatical restrictions of the CNL via look-
ahead information while a text is written. For each
word form that the user enters into the editor, a list
of options is generated incrementally by the chart
parser to inform the user about how the structure
of the current sentence can be continued. The syn-
tactic restrictions ensure that the text follows the
rules of the CNL so that it can be translated un-
ambiguously into the formal target language (first-
order logic) and be processed by a theorem prover.
In order to illustrate how PENG can be used
to reconstruct a problem in controlled natural lan-
guage, we use an example from the TPTP problem
library3. The problems in this library are usually
used to test the capacity of automated reasoning
tools and are translated manually by a human into
the formal target language. For reasons of space,
we use here one of the simpler problems of the li-
brary; the puzzle PUZ012-1 below is also known
as ?The Mislabeled Boxes?:
There are three boxes a, b, and c on a
table. Each box contains apples or ba-
nanas or oranges. No two boxes con-
tain the same thing. Each box has a la-
bel that says it contains apples or says
it contains bananas or says it contains
oranges. No box contains what it says
on its label. The label on box a says
?apples?. The label on box b says ?or-
anges?. The label on box c says ?ba-
nanas?. You pick up box b and it con-
tains apples. What do the other two
boxes contain?
In order to solve this puzzle by a computer,
we have to reconstruct it and augment it with the
3http://www.cs.miami.edu/?tptp/
1115
relevant background knowledge. The main prob-
lems that we face here for machine-processing are
the following ones: some of the constructions in
the problem description are ambiguous (e.g. the
antecedent for the personal pronoun it is open
to two interpretations); the semantic relation be-
tween some content words is not explicit (e.g. the
relation between the actual things in the box and
the names on the labels that describe these things);
and some of the constructions are not relevant at
all for the solution of the problem (e.g. that the
three boxes are on the table). Here is a possible
reconstruction of this puzzle in PENG:
The label of the box a says APPLES.
The label of the box b says ORANGES.
The label of the box c says BANANAS.
APPLES stands for apples. ORANGES
stands for oranges. BANANAS stands
for bananas. All apples are fruits. All
bananas are fruits. All oranges are
fruits. Each box contains the apples
or contains the bananas or contains the
oranges. It is not the case that a box
contains fruits and that the label of the
box says something that stands for those
fruits. It is not the case that a box X
contains fruits and that a box Y con-
tains those fruits. The box b contains
the apples. What does the box a con-
tain? What does the box c contain?
Note that this reconstruction makes information
that is implicit or only assumed in the original
problem description explicit in PENG.
PENG has recently been used for the construc-
tion of an interface to a situation awareness system
(Baader et al, 2009) but the language can be used
for similar applications to ACE.
2.3 Computer Processable Language (CPL)
CPL (Clark et al, 2010) is a controlled language
that has been developed at Boeing Research and
Technology. In contrast to ACE which applies a
small set of strict interpretation rules, and in con-
trast to PENG, which relies on a predictive editor,
the CPL interpreter directly resolves various types
of ambiguities using heuristic rules for preposi-
tional phrase attachment, word sense disambigua-
tion, semantic role labeling, compound noun in-
terpretation, metonymy resolution, and other lan-
guage processing activities.
CPL accepts three types of sentences: ground
facts, questions, and rules. In the case of ground
facts, a basic CPL sentence takes one of the fol-
lowing three forms:
? There is|are NP
? NP verb [NP] [PP]*
? NP is|are passive-verb [by NP] [PP]*
Verbs can include auxiliaries and particles, and
nouns in noun phrases can be modified by other
nouns, prepositional phrases, and adjectives. In
the case of questions, CPL accepts five forms; the
two main forms are:
? What is NP?
? Is it true that Sentence?
In the case of rules, CPL accepts sentence pat-
terns of the form:
? IF Sentence [AND Sentence]* THEN Sen-
tence [AND Sentence]*
Parsing of CPL is performed bottom-up with
the help of a broad coverage chart parser that uses
preference for common word attachment patterns
stored in a manually constructed database. Dur-
ing parsing, a simplified logical form is generated
for basic sentences by rules that run in parallel to
the grammar rules. There is no explicit quanti-
fier scoping for these basic sentences and some
disambiguation decisions (e.g., word sense and
semantic relationships) are deferred and handled
by the inference engine that makes a ?best guess?
of word sense assignments using WordNet4. The
logical form is used to generate ground Knowl-
edge Machine (KM) assertions. KM5 is a frame-
based language with first-order semantics. The
KM interpreter employs a sophisticated machin-
ery for reasoning, including reasoning about ac-
tions using a situation calculus mechanism. Rules
4http://wordnet.princeton.edu/
5http://userweb.cs.utexas.edu/users/
mfkb/km.html
1116
are entered by the user who writes CPL sentences
with the help of rule templates. There exist seven
templates for this purpose: three of them create
standard logical implications and the rest describe
preconditions and effects of actions. Each CPL
sentence is interpreted interactively. The system
paraphrases its interpretation back to the user, al-
lowing the user to spot and fix misinterpretations.
Sentences that express states add facts to a sit-
uation, and sentences that express actions trig-
ger rules that update the situation, reflecting the
changes that the action has on the situation. The
user can ask questions about an emerging situation
directly in CPL.
While CPL relies on heuristics, CPL-Lite is a
slimmed down version of CPL that can be in-
terpreted deterministically in a similar fashion to
PENG. Each CPL-Lite sentence corresponds to a
single binary relation between two entities. CPL-
Lite distinguishes three types of relations: noun-
like relations (e.g. the age of <x> is <y>), verb-
like relations (e.g. <x> causes <y>), and pre-
position-like relations (e.g. <x> is during <y>).
Interestingly, CPL-Lite has the same expressiv-
ity as CPL, but CPL-Lite is more verbose and
grammatically more restricted. For example, the
following two CPL sentences:
1. A man drives a car along a road for 1 hour.
2. The speed of the car is 30 km/h.
can be expressed (or better reconstructed) in an
unambiguous way in CPL-Lite:
3. A person drives a vehicle.
4. The path of the driving is a road.
5. The duration of the driving is 1 hour.
6. The speed of the driving is 30 km/h.
Note that the user used here the noun person in-
stead of man and vehicle instead of car during this
reconstruction process because only these words
were available in the system?s ontology.
CPL and CPL-Lite have been mainly used to
encode general and domain specific common-
sense knowledge and to allow knowledge engi-
neers to pose queries in a comprehensible way.
2.4 Other General-Purpose CNLs
Common Logic Controlled English (CLCE)6 is a
proposal for a CNL ? similar to ACE and PENG
? that has been designed as a human interface lan-
guage for the ISO standard Common Logic (CL)7.
However, CLCE itself is not part of this stan-
dard but uses Common Logic semantics. CLCE
supports full first-order logic with equality sup-
plemented with an ontology for sets, sequences,
and integers. The primary syntactic restrictions
are the use of present tense verbs, singular nouns,
and variables instead of pronouns. Despite these
limitations, CLCE can express the kind of English
used in software specifications, mathematics text-
books, and definitions and axioms found in formal
ontologies.
Formalized-English (Martin, 2002) is another
proposal for a CNL that can be used as a gen-
eral knowledge representation language. This lan-
guage has a relatively simple structure and is de-
rived from a conventional knowledge represen-
tation language. Formalized-English contains a
number of formal-looking language elements and
is therefore not a strict subset of standard English.
3 Theoretical Considerations
During the design of a CNL one has to pay atten-
tion to two important theoretical issues: the ex-
pressive power of the envisaged language and its
computational complexity. E2V (Pratt-Hartmann,
2003) is a CNL that mainly grew out of theoret-
ical studies about the expressivity and complex-
ity of natural language fragments. E2V corre-
sponds to the decidable two-variable fragment of
first-order logic (L2). This fragment is interest-
ing since it has the so-called finite model property.
That means if a formula of L2 is satisfiable, then it
is satisfiable in a finite model. E2V includes deter-
miners (every, no, a), nouns, transitive verbs, verb
phrase negation, relative, reflexive, and personal
pronouns. Without any writing support it is diffi-
cult to decide if a sentence is in E2V or not. For
example, one reading of sentence (7) is in E2V,
the other one is not:
6http://www.jfsowa.com/clce/specs.htm
7ISO/IEC24707:2007
1117
7. Every artist who employs a carpenter de-
spises every beekeeper who admires him.
On the syntactic level, E2V is a subset of ACE
with the exception that pronouns (e.g. him) al-
ways refer to the closest (acceptable) noun in the
syntax tree (e.g. artist) and not to the closest (ac-
ceptable) noun that occurs in the surface structure
(e.g. carpenter). This is because the E2V inter-
pretation relies on the two-variable fragment of
first-order logic. Note that sentence (7) has the
following two possible representations (8 and 9)
in first-order logic:
8. ?x1 (artist(x1) & ?x2
(carpenter(x2) & employ(x1,x2)) ->
?x3 (beekeeper(x3) &
admire(x3,x1) -> despise(x1,x3)))
9. ?x1 ?x2 (artist(x1) &
(carpenter(x2) & employ(x1,x2) ->
?x3 (beekeeper(x3) &
admire(x3,x2) -> despise(x1,x3)))
Although there are three variables in the for-
mula (8) that correspond to the three nouns in
sentence (7), the variables x2 and x3 never oc-
cur free in the same sub-formula. Therefore, the
number of variables can be reduced by replacing
x3 through x2. This technique can not be applied
to the variables in formula (9).
E2V has been extended in various ways (Pratt-
Hartmann and Third, 2006) and one extension in-
cludes counting determiners (e.g. at least three,
at most five, exactly four). These determiners will
not in general translate into the two-variable frag-
ment of first-order logic, but into the fragment
C2, which adds counting quantifiers to the two-
variable fragment. The satisfiability problem of
this fragment is still decidable and its expressivity
and computational complexity is similar to those
description logic languages that build the founda-
tion of the Semantic Web.
4 CNLs for the Semantic Web
Recently, a number of CNLs have been developed
that can serve as front-end to those formal lan-
guages that are used in the context of the Semantic
Web8. These CNLs can be used by domain spe-
cialists who prefer familiar natural language-like
notations over formal ones for authoring and ver-
balising formal ontologies.
ACE View (Kaljurand, 2007) is a CNL editor
that supports a defined subset of ACE that can be
used as an alternative syntax for the Semantic Web
languages OWL and SWRL. ACE View integrates
two mappings: one from ACE to OWL/SWRL
and one from OWL to ACE. These mappings are
not bidirectional in a strict sense since the OWL
to ACE mapping also covers OWL axioms and
expression types that the ACE to OWL mapping
does not generate.
Sydney OWL Syntax (SOS) (Cregan et al,
2007) is a proposal for a CNL that builds upon
PENG and provides a syntactically bidirectional
mapping to OWL-DL. SOS is strictly bidirec-
tional: each statement can be translated into OWL
functional-style syntax and vice versa. The bidi-
rectional translation is achieved with the help of a
definite clause grammar that generates the target
notation during the parsing process. In contrast to
ACE, syntactic constructs of OWL are always car-
ried over one-to-one to SOS. Thus, semantically
equivalent OWL statements that use different syn-
tactical constructs are always mapped to different
SOS statements.
Rabbit (Hart et al, 2008) is a CNL designed for
a scenario where a domain expert and an ontology
engineer work together to build an ontology. The
construction process is supported by a text-based
ontology editor. The editor accepts Rabbit sen-
tences, helps to resolve possible syntax errors, and
translates well-formed sentences into OWL. The
semantics of some Rabbit constructs is controver-
sial (e.g. exclusive interpretation of disjunction)
and hard to align with the semantics of OWL.
Lite Natural Language (Bernardi et al, 2007)
is a CNL based on Categorial Grammar; it has
the same expressivity as the description logic DL-
Lite. DL-Lite is a tractable fragment of OWL
and has polynomial time complexity for the main
reasoning tasks. DL-Lite is expressive enough
to capture relational databases and UML (Unified
Modeling Language) diagrams.
8http://www.w3.org/TR/owl2-overview/
1118
CLOnE (Funk et al, 2007) is a CNL that is
built on top of the natural language processing
framework GATE9. CLOnE is a simple ontol-
ogy authoring language that consists of eleven
sentence patterns which roughly correspond to
eleven OWL axiom patterns. It is unclear whether
CLOnE can be extended in a systematic way to
cover larger fragments of OWL.
The three controlled languages ACE, SOS, and
Rabbit are compared in more detail in Schwitter et
al. (2008). There exist three other CNL research
streams that are closely related to the Semantic
Web: CNLs for querying Semantic Web content
(Bernstein and Kaufmann, 2006); CNLs for main-
taining semantic wikis (Kuhn, 2009; Kuhn, 2010);
and CNLs for describing rules and policies (De
Coi et al, 2009).
5 Writing Support for CNLs
Writing a specification in CNL is not an easy task
since the author has to stick to the rules of the con-
trolled language. Writing in CNL is in essence
a normative process that prescribes how humans
should use language to communicate effectively
with a computer in order to achieve a particu-
lar goal. The challenge here is to develop in-
terface techniques that make the writing process
as unobtrusive and effortless as possible. Three
main techniques have been suggested to support
the writing process of CNLs: the use of error mes-
sages, conceptual authoring, and predictive feed-
back.
Error messages seem to be the most obvious
way to support the writing of a text in CNL, and
many CNLs (among them (Clark et al, 2010;
Fuchs et al, 2008)) use this technique. The user
is supposed to learn and remember the restrictions
of the CNL and then to write the text following
the memorised rules. If the parsing process fails,
then the CNL system tries to identify the cause
of the error and provides one or more suggestions
for how to fix the error. The problem with this
technique is that the input might be an unrestricted
sentence and a useful error message would require
in the worst case knowledge of the sort that is
needed for processing full natural language.
9http://gate.ac.uk/
Conceptual authoring (Power et al, 2009) is
a technique that allows authors to edit a knowl-
edge base on the semantic level by refining spe-
cific categories and properties that occur in CNL
sentences via a hierarchy of menu options. The
selection of an option by the author results in an
update of the underlying model and triggers the
generation of a new sentence that can then be fur-
ther refined. This method relies on natural lan-
guage generation techniques and makes the anal-
ysis of CNL sentences unnecessary. The problem
with this technique is that it does not allow the au-
thor to specify new knowledge that is not already
encoded in the knowledge base; it is basically a
technique for knowledge authoring and visualiza-
tion and does not provide an independent knowl-
edge representation language.
Predictive feedback (Schwitter et al, 2003;
Kuhn and Schwitter, 2008) is a technique that in-
forms the authors during the writing process about
the approved structures of the CNL. This tech-
nique relies on interfaces that are aware of the
grammar and can look-ahead within this grammar.
Using this technique the author receives immedi-
ate feedback while a text is written and cannot
enter sentences that are not in the scope of the
grammar. The grammar of the language PENG
has been designed from the beginning to be used
in a predictive editor and is processed by a chart
parser that is able to generate the look-ahead in-
formation. The following example illustrates how
a predictive editor works:
? A [ adjective | common noun ]
? A man [ verb | who | ?does not? ]
? A man works [ ?.? | preposition | adverb ]
In this example the look-ahead information
consists of syntactic categories, word forms and
punctuation marks; all these elements are imple-
mented as hypertext links. Selecting a hypertext
link for a syntactic category displays approved
word forms and selecting a word form or a punc-
tuation mark directly adds this element to the text.
Kuhn (2010) shows in an number of experiments
that predictive editors are easy for untrained users
to use and argues that predictive feedback is the
best way to support the writing process of CNLs.
1119
6 Evaluating CNLs
Over the past years, a number of different user
experiments have been designed to measure var-
ious usability aspects of CNLs (see (Kuhn, 2010)
for an introduction). These experiments can be
grouped into three different categories: task-based
experiments, paraphrase-based experiments, and
graph-based experiments.
In task-based experiments (for example, (Kauf-
mann and Bernstein, 2007)), human subjects re-
ceive a certain task that requires them to use a
CNL as an interface language to a knowledge base
together with a tool that potentially supports the
writing process. These experiments test how easy
or difficult it is to write in these controlled lan-
guages using the given tool, but they do not test
the understandability of these languages.
Paraphrase-based experiments (for example,
(Hart et al, 2008)) aim to evaluate the understand-
ability of a CNL in a tool-independent way. Hu-
man subjects receive a statement in CNL and a
choice of paraphrases in full natural language, and
then have to select the correct paraphrase. These
experiments scale well with the expressivity of the
CNL but it is difficult to guarantee that the para-
phrases are understood in the intended way.
Graph-based experiments (for example,
(Kuhn, 2010)) try to overcome the problems of
paraphrase-based experiments. In order to test the
understandability of CNLs and formal languages,
a graph-based notation is used to describe a
situation accompanied with statements in the
language to be tested. The human subjects have
to decide which of these statements are true and
which ones are false with respect to the situation
illustrated by the graph notation.
The reported results of these experiments in the
literature provide strong evidence that CNLs are
easier to write and easier to understand for domain
specialists than formal languages.
7 Conclusions
It is an exciting time to work on controlled natural
languages. In this paper, we surveyed a number
of machine-oriented controlled natural languages
that can be used instead of formal languages for
representing knowledge. These controlled nat-
ural languages look like English but correspond
to a formal target language. Anyone who can
read English has already the basic skills to under-
stand these controlled natural languages. Writing
a specification in controlled natural language is a
bit harder: it requires that the author either learns
the language in order to be able to stay within
its syntactic and semantic restrictions or that he
uses an intelligent authoring tool that supports the
writing process and enforces the restrictions of the
language.
Machine-oriented controlled natural languages
can be translated automatically (and often deter-
ministically) into a formal target language (e.g.
into full first-order logic or into a version of de-
scription logics). These languages can be used
to express the kind of information that occurs in
software specifications, formal ontologies, busi-
ness rules, and legal and medical regulations.
In summary, an ideal machine-oriented con-
trolled natural language should fulfill at least the
following requirements: (a) it should have a well-
defined syntax and a precise semantics that is de-
fined by an unambiguous mapping into a logic-
based representation; (b) it should look as natural
as possible and be based on a subset of a certain
natural language; (c) it should be easy for humans
to write and understand and easy for a machine to
process; and (d) it should have the necessary ex-
pressivity that is required to describe a problem in
the respective application domain.
Of course these requirements can be in con-
flict with each other and therefore careful com-
promises need to be made when a new controlled
natural language is designed. This design process
offers many interesting research challenges for re-
searchers in the area of computational linguistics
and artificial intelligence. This research is driven
by the overall goal to close the gap between natu-
ral and formal languages and to allow for true col-
laboration between humans and machines in the
near future.
Acknowledgments
I would like to thank to three anonymous review-
ers of Coling 2010 for their valuable feedback and
to Robert Dale for comments and suggestions on
previous versions of this paper.
1120
References
Baader, Franz, Andreas Bauer, Peter Baumgartner,
Anne Cregan, Alfredo Gabaldon, Krystian Ji, Kevin
Lee, Dave Rajaratnam and R. Schwitter. 2009. A
Novel Architecture for Situation Awareness Sys-
tems, In: Proceedings of TABLEAUX 2009, LNAI
5607, pp. 77?92.
Bernardi, Raffaella, Diego Calvanese, and Camilo
Thorne. 2007. Lite Natural Language. In: Pro-
ceedings of IWCS-7.
Bernstein, Abraham and Esther Kaufmann. 2006.
GINO ? a guided input natural language ontology
editor. In: Proceedings of ISWC 2006, LNCS 4273,
pp. 144?157.
Clark, Peter, Phil Harrison, William R. Murray, and
John Thompson. 2010 Naturalness vs. Predictabil-
ity: A Key Debate in Controlled Languages. In:
Proceedings 2009 Workshop on Controlled Natural
Languages (CNL?09).
Cregan, Anne, Rolf Schwitter, and Thomas Meyer.
2007. Sydney OWL Syntax ? towards a Controlled
Natural Language Syntax for OWL 1.1. In: Pro-
ceedings of OWLED 2007, CEUR, vol. 258.
De Coi, Juri L., Norbert E. Fuchs, Kaarel Kaljurand,
Tobias Kuhn. 2009. Controlled English for Rea-
soning on the Semantic Web. In: LNCS, vol. 5500,
pp. 276?308.
Fuchs, Norbert E., Kaarel Kaljurand, and Tobias Kuhn.
2008. Attempto Controlled English for Knowledge
Representation. In: Reasoning Web, LNCS, vol.
5224, pp. 104?124.
Funk, Adam, Valentin Tablan, Kalina Bontcheva,
Hamish Cunningham, Brian Davis, and Siegfried
Handschuh. 2007. CLOnE: Controlled Language
for Ontology Editing. In: Proceedings of ISWC
2007.
Hart, Glen, Martina Johnson, and Catherine Dolbear.
2008. Rabbit: Developing a controlled natural lan-
guage for authoring ontologies. In: Proceedings of
ESWC 2008, LNCS, vol. 5021, pp. 348?360.
Huijsen, Willem-Olaf. 1998. Controlled Language ?
An Introduction. In: Proceedings of CLAW 98, pp.
1?15.
Kaljurand, Kaarel. 2007. Attempto Controlled En-
glish as a Semantic Web Language. PhD The-
sis. Faculty of Mathematics and Computer Science,
University of Tartu.
Kamp, Hans and Uwe Reyle. 1993. From Discourse
to Logic. Kluwer, Dordrecht.
Kaufmann, Esther and Abraham Bernstein. 2007.
How Useful Are Natural Language Interfaces to the
Semantic Web for Casual End-Users? In: Proceed-
ings of ISWC/ASWC 2007, NLCS, vol. 4825, pp.
281?294.
Kuhn, Tobias and Rolf Schwitter. 2008. Writing Sup-
port for Controlled Natural Languages. In: Pro-
ceedings of ALTA 2008, pp. 46?54.
Kuhn, Tobias. 2009. How controlled English can im-
prove semantic wikis. In: Proceedings of SemWiki
2009, CEUR, vol. 464.
Kuhn, Tobias. 2010. Controlled English for Knowl-
edge Representation. Doctoral Thesis. Faculty of
Economics, Business Administration and Informa-
tion Technology of the University of Zurich.
Martin, Philippe. 2002. Knowledge representation
in CGLF, CGIF, KIF, Frame-CG and Formalized-
English. In: Proceedings of ICCS 2002, LNAI, vol.
2393, pp. 77?91.
Monin, Jean-Franc?ois. 2003. Understanding Formal
Methods. Springer-Verlag, London.
Nyberg, Eric H. and Teruko Mitamura. 2000. The
KANTOO Machine Translation Environment. In:
Proceedings of AMTA 2000, LNCS, vol. 1934, pp.
192?195.
O?Brien, Sharon. 2003. Controlling controlled english
? an analysis of several controlled language rule
sets. In: Proceedings of EAMT-CLAW 03, Dublin
City University, Ireland, pp. 105?114.
Pool, Jonathan. 2006. Can Controlled Languages
Scale to the Web? In: Proceedings of the 5th Int.
Workshop on Controlled Language Applications.
Power, Richard, Robert Stevens, Donia Scott, and Alan
Rector. 2009. Editing OWL through generated
CNL. In: Pre-Proceedings of the Workshop on CNL
2009, CEUR, vol. 448.
Pratt-Hartmann, Ian. 2003. A two-variable fragment
of English. In: Journal of Logic, Language and In-
formation, 12(1), pp. 13?45.
Pratt-Hartmann, Ian and Allan Third. 2006. More
fragments of language: the case of ditransitive
verbs. In: Notre Dame Journal of Formal Logic,
47(2), pp. 151?177.
Schwitter, Rolf, Anna Ljungberg, and David Hood.
2003. ECOLE ? A Look-ahead Editor for a Con-
trolled Language. In: Proceedings of EAMT-
CLAW03, pp. 141?150.
Schwitter, Rolf, Kaarel Kaljurand, Anne Cregan,
Catherine Dolbear, and Glen Hart. 2008. A
comparison of three controlled natural languages
for OWL 1.1. In: Proceedings of OWLED 2008,
CEUR, vol. 496.
White, Colin and Rolf Schwitter. 2009. An Update on
PENG Light. In: Proceedings of ALTA 2009, pp.
80?88.
1121
