Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 29?32,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
A Novel Word Segmentation Approach for
Written Languages with Word Boundary Markers
Han-Cheol Cho
?
, Do-Gil Lee
?
, Jung-Tae Lee
?
, Pontus Stenetorp
?
, Jun?ichi Tsujii
?
and Hae-Chang Rim
?
?
Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan
?
Dept. of Computer & Radio Communications Engineering, Korea University, Seoul, Korea
{hccho,pontus,tsujii}@is.s.u-tokyo.ac.jp, {dglee,jtlee,rim}@nlp.korea.ac.kr
Abstract
Most NLP applications work under the as-
sumption that a user input is error-free;
thus, word segmentation (WS) for written
languages that use word boundary mark-
ers (WBMs), such as spaces, has been re-
garded as a trivial issue. However, noisy
real-world texts, such as blogs, e-mails,
and SMS, may contain spacing errors that
require correction before further process-
ing may take place. For the Korean lan-
guage, many researchers have adopted a
traditional WS approach, which eliminates
all spaces in the user input and re-inserts
proper word boundaries. Unfortunately,
such an approach often exacerbates the
word spacing quality for user input, which
has few or no spacing errors; such is the
case, because a perfect WS model does
not exist. In this paper, we propose a
novel WS method that takes into consider-
ation the initial word spacing information
of the user input. Our method generates
a better output than the original user in-
put, even if the user input has few spacing
errors. Moreover, the proposed method
significantly outperforms a state-of-the-art
Korean WS model when the user input ini-
tially contains less than 10% spacing er-
rors, and performs comparably for cases
containing more spacing errors. We be-
lieve that the proposed method will be a
very practical pre-processing module.
1 Introduction
Word segmentation (WS) has been a fundamen-
tal research issue for languages that do not have
word boundary markers (WBMs); on the con-
trary, other languages that do have WBMs have re-
garded the issue as a trivial task. Texts segmented
with such WBMs, however, could contain a hu-
man writer?s intentional or un-intentional spacing
errors; and even a few spacing errors can cause
error-propagation for further NLP stages.
For written languages that have WBMs, such as
for the Korean language, the majority of recent
research has been based on a traditional WS ap-
proach (Nakagawa, 2004). The first step of the
traditional approach is to eliminate all spaces in
the user input, and then re-locate the proper places
to insert WBMs. One state-of-the-art Korean WS
model (Lee et al, 2007) is known to achieve a per-
formance of 90.31% word-unit precision, which is
comparable with other WS models for the Chinese
or Japanese language.
Still, there is a downside to the evaluation
method. If the user input has a few or no spac-
ing errors, traditional WS models may cause more
spacing errors than it correct because they produce
the same output regardless the word spacing states
of the user input.
In this paper, we propose a new WS method that
takes into account the word spacing information
from the user input. Our proposed method first
generates the best word spacing states for the user
input by using a traditional WS model; however
the method does not immediately apply the out-
put. Secondly, the method estimates a threshold
based on the word spacing quality of the user in-
put. Finally, the method uses the new word spac-
ing states that have probabilities that are higher
than the threshold.
The most important contribution of the pro-
posed method is that, for most cases, the method
generates an output that is better than the user in-
put. The experimental results show that the pro-
posed method produces a better output than the
user input even if the user input has less than 1%
spacing errors in terms of the character-unit pre-
cision. Moreover, the proposed method outper-
forms (Lee et al, 2007) significantly, when the
29
user input initially contains less than 10% spacing
errors, and even performs comparably, when the
input contains more than 10% errors. Based on
these results, we believe that the proposed method
would be a very practical pre-processing module
for other NLP applications.
The paper is organized as follows: Section 2 ex-
plains the proposed method. Section 3 shows the
experimental results. Finally, the last section de-
scribes the contributions of the proposed method.
2 The Proposed Method
The proposed method consists of three steps: a
baseline WS model, confidence and threshold es-
timation, and output optimization. The following
sections will explain the steps in detail.
2.1 Baseline Word Segmentation Model
We use the tri-gram Hidden Markov Model
(HMM) of (Lee et al, 2007) as the baseline WS
model; however, we adopt the Maximum Like-
lihood (ML) decoding strategy to independently
find the best word spacing states. ML-decoding
allows us to directly compare each output to the
threshold. There is little discrepancy in accuracy
when using ML-decoding, as compared to Viterbi-
decoding, as mentioned in (Merialdo, 1994).
1
Let o
1,n
be a sequence of n-character user input
without WBMs, x
t
be the best word spacing state
for o
t
where 1 ? t ? n. Assume that x
t
is either 1
(space after o
t
) or 0 (no space after o
t
). Then each
best word spacing state x?
t
for all t can be found by
using Equation 1.
x?
t
= argmax
i?(0,1)
P (x
t
= i|o
1,n
) (1)
= argmax
i?(0,1)
P (o
1,n
, x
t
= i) (2)
= argmax
i?(0,1)
?
x
t?2
,x
t?1
P (x
t
= i|x
t?2
, o
t?1
, x
t?1
, o
t
)
?
?
x
t?1
P (o
t+1
|o
t?1
, x
t?1
, o
t
, x
t
= i)
?
?
x
t+1
P (o
t+2
|o
t
, x
t
= i, o
t+1
, x
t+1
) (3)
Equation 2 is derived by applying the Bayes?
rule and by eliminating the constant denominator.
Moreover, the equation is simplified, as is Equa-
tion 3, by using the Markov assumption, and by
1
In the preliminary experiment, Viterbi-decoding showed
a 0.5% higher word-unit precision.
eliminating the constant parts. Every part of Equa-
tion 3 can be calculated by adding the probabilities
of all possible combinations of x
t?2
, x
t?1
, x
t+1
and x
t+2
values.
The model is trained by using the relative fre-
quency information of the training data, and a
smoothing technique is applied to relieve the data-
sparseness problem which is the linear interpola-
tion of n-grams that are used in (Lee et al, 2007).
2.2 Confidence and Threshold Estimation
We set a variable threshold that is proportional to
the word spacing quality of the user input, Confi-
dence. Formally, we can define the threshold T as
a function of a confidence C, as in Equation 4.
T = f(C) (4)
Then, we define the confidence as is done in
Equation 5. Because calculating such a variable
is impossible, we estimate the value by substi-
tuting the word spacing states produced by the
baseline WS model, x
WS
1,n
, with the correct word
spacing states, x
correct
1,n
, as is done in Equation 6.
This estimation is based on the assumption that
the word spacing states of the WS model is suf-
ficiently similar to the correct word spacing states
in the character-unit precision.
2
C =
# of x
input
t
same to x
correct
t
# of x
input
t
(5)
?
# of x
input
t
same to x
WS
t
# of x
input
t
(6)
?
n
?
?
?
?
n
?
k=1
P (x
input
k
|o
1,n
) (7)
To handle the estimation error for short sen-
tences, we use the probability generating word
spacing states of the user input with the length nor-
malization as shown in Equation 7.
Figure 1 shows that the estimated confidence of
Equation 7 is almost linearly proportional to the
true confidence of Equation 5, thus suggesting that
the threshold T can be defined as a function of the
estimated confidence of Equation 7.
3
2
In the experiment with the development data, the base-
line WS model shows about 97% character-unit precision.
3
The development data is generated by randomly intro-
ducing spacing errors into correctly spaced sentences. We
think that this reflects various intentional and un-intentional
error patterns of individuals.
30
20%30%
40%50%
60%70%
80%90%
100%
100% 96% 92% 88% 84% 80%
Estim
ated C
onfid
ence
True Confidence
Figure 1: The relationship between estimated con-
fidence and true confidence
To keep the focus on the research subject of this
paper, we simply assume f(x) = x as in Equation
8, for the threshold function f .
T ? f(C) = C (8)
In the experimental results, we confirm that
even this simple threshold function can be help-
ful in improving the performance of the proposed
method against traditional WS models.
2.3 Output Optimization
After completing the two steps described in Sec-
tion 2.1 and 2.2, we have acquired the new spacing
states for the user input generated by the baseline
WS model, and the threshold measuring the word
spacing quality of the user input.
The proposed method only applies a part of the
new word spacing states to the user input, which
have probabilities that are higher than the thresh-
old; further the method discards the other new
word spacing states that have probabilities that are
lower than the threshold. By rejecting the unreli-
able output of the baseline WS model in this way,
the proposed method can effectively improve the
performance when the user input contains a rela-
tively small number of spacing errors.
3 Experimental Results
Two types of experiments have been performed.
In the first experiment, we investigate the level of
performance improvement based on different set-
tings of the user input?s word spacing error rate.
Because it is nearly impossible to obtain enough
test data for any error rate, we generate pseudo test
data in the same way that we generate develop-
ment data.
4
In the second experiment, we attempt
4
See Footnote 3.
figuring out whether the proposed method really
improves the word spacing quality of the user in-
put in a real-world setting.
3.1 Performance Improvement according to
the Word Spacing Error Rate of User
Input
For the first experiment, we use the Sejong corpus
5
from 1998-1999 (1,000,000 Korean sentences) for
the training data, and ETRI corpus (30,000 sen-
tences) for the test data (ETRI, 1999). To gener-
ate the test data that have spacing errors, we make
twenty one copies of the test data and randomly
insert spacing errors from 0% to 20% in the same
way in which we made the development data. We
feel that this strategy can model both the inten-
tional and un-intentional human error patterns.
In Figure 2, the x-axis indicates the word spac-
ing error rate of the user input in terms of the
character-unit precision, and the y-axis shows the
word-unit precision of the output. Each graph de-
picts the word-unit precision of the test corpus,
a state-of-the-art Korean WS model (Lee et al,
2007), the baseline WS model, and the proposed
method.
Although Lee?s model is known to perform
comparably with state-of-the-art Chinese and
Japanese WS models, it does not necessarily sug-
gest that the word spacing quality of the model?s
output is better than the user input. In Figure 2,
Lee?s model exacerbates the user input when it has
spacing errors that are lower than 3%.
The proposed method, however, produces a bet-
ter output, even if the user input has 1% spacing er-
rors. Moreover, the proposed method shows a con-
siderably better performance within the 10% spac-
ing error range, as compared to Lee?s model, al-
though the baseline WS model itself does not out-
performs Lee?s model. The performance improve-
ment in this error range is fairly significant be-
cause we found that the spacing error rate of texts
collected for the second experiment was about
9.1%.
3.2 Performance Comparison with Web Text
having Usual Error Rate
In the second experiment, we attempt finding out
whether the proposed method can be beneficial un-
der real-world circumstances. Web texts, which
consist of 1,000 erroneous sentences from famous
5
Details available at: http://www.sejong.or.kr/eindex.php
31
84%
86%
88%
90%
92%
94%
96%
98%
100%
0% 2% 4% 6% 8% 10% 12% 14% 16% 18% 20%
w
or
d-u
nit
 
 
pr
ec
isio
n
word spacing error rate of user input (in character-unit precision)
Test corpus Lee's model Baseline WS model Proposed method
Figure 2: Performance improvement according to the word spacing error rate of user input
Method Web Text
Test Corpus 70.89%
Lee?s Model 70.45%
Baseline WS Model 69.13%
Proposed Method 73.74%
Table 1: Performance comparison with Web text
Web portals and personal blogs, were collected
and used as the test data. Since the test data tend
to have a similar error rate to the narrow standard
deviation, we computed the overall performance
over the average word spacing error rate, which is
9.1%. The baseline WS model is trained on the
Sejong corpus, described in Section 3.1.
The test result is shown in Table 1. The
overall performance of Lee?s model, the baseline
WS model and the proposed method decreased
by roughly 18%. We hypothesize that the per-
formance degradation probably results from the
spelling errors of the test data, and the inconsis-
tencies that exist between the training data and the
test data. However, the proposed method still im-
proves the word spacing quality of the user input
by 3%, while the two traditional WS models de-
grades the quality. Such a result indicates that
the proposed method is effective for real-world
environments, as we had intended. Furthermore,
we also believe that the performance can be im-
proved if a proper training corpus is provided, or
if a spelling correction method is integrated.
4 Conclusion
In this paper, we proposed a new WS method that
uses the word spacing information of the user in-
put, for languages with WBMs. By utilizing the
user input, the proposed method effectively refines
the output of the baseline WS model and improves
the overall performance.
The most important contribution of this work is
that it produces an output that is better than the
user input even if it contains few spacing errors.
Therefore, the proposed method can be applied as
a pre-processing module for practical NLP appli-
cations without introducing a risk that would gen-
erate a worse output than the user input. Moreover,
the performance is notably better than a state-of-
the-art Korean WS model (Lee et al, 2007) within
the 10% spacing error range, which human writers
seldom exceed. It also performs comparably, even
if the user input contains more than 10% spacing
errors.
5 Acknowledgment
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Special Coordination Funds for Promoting
Science and Technology (MEXT, Japan).
References
ETRI. 1999. Pos-tag guidelines. Technical report.
Electronics and Telecomminications Research Insti-
tute.
Do-Gil Lee, Hae-Chang Rim, and Dongsuk Yook.
2007. Automatic Word Spacing Using Probabilistic
Models Based on Character n-grams. IEEE Intelli-
gent Systems, 22(1):28?35.
Bernard Merialdo. 1994. Tagging English text with a
probabilistic model. Comput. Linguist., 20(2):155?
171.
Tetsuji Nakagawa. 2004. Chinese and Japanese word
segmentation using word-level and character-level
information. In COLING ?04, page 466, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
32
Proceedings of the Workshop on BioNLP: Shared Task, pages 107?110,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Multi-Phase Approach to Biomedical Event Extraction
Hyoung-Gyu Lee, Han-Cheol Cho, Min-Jeong Kim
Joo-Young Lee, Gumwon Hong, Hae-Chang Rim
Department of Computer and Radio Communications Engineering
Korea University
Seoul, South Korea
{hglee,hccho,mjkim,jylee,gwhong,rim}@nlp.korea.ac.kr
Abstract
In this paper, we propose a system for biomed-
ical event extraction using multi-phase ap-
proach. It consists of event trigger detector,
event type classifier, and relation recognizer
and event compositor. The system firstly iden-
tifies triggers in a given sentence. Then, it
classifies the triggers into one of nine pre-
defined classes. Lastly, the system examines
each trigger whether it has a relation with
participant candidates, and composites events
with the extracted relations. The official score
of the proposed system recorded 61.65 preci-
sion, 9.40 recall and 16.31 f-score in approxi-
mate span matching. However, we found that
the threshold tuning for the third phase had
negative effect. Without the threshold tuning,
the system showed 55.32 precision, 16.18 re-
call and 25.04 f-score.
1 Introduction
As the volume of biomedical literature grows expo-
nentially, new biomedical terms and their relations
are also generated. However, it is still not easy for
researchers to access necessary information quickly
since it is lost within large volumes of text. This is
the reason that the study of information extraction
is receiving the attention of biomedical and natural
language processing (NLP) researchers today.
In the shared task, the organizers provide partic-
ipants with raw biomedical text, tagged biomedical
terms (proteins), and the analyzed data with various
NLP techniques such as tokenization, POS-tagging,
phrase structure and dependency parsing and so on.
The expected results are the events, which exist in
the given text, consisting of a trigger and its partici-
pant(s) (Kim et al, 2009).
The proposed system consists of three phases;
event trigger detection phase(TD phase), event type
classification phase(TC phase), relation recognition
and event composition phase(RE phase). It works in
the following manner. Firstly, it identifies triggers of
a given biomedical sentence. Then, it classifies trig-
gers into nine pre-defined classes. Lastly, the sys-
tem finds the relations between triggers and partic-
ipant candidates by examining each trigger whether
it has relations with participant candidates, and com-
posites events with the extracted relations. In the
last phase, multiple relations of the same trigger
can be combined into an event for Binding event
type. In addition, multiple relations can be com-
bined and their participant types can be classified
into not only theme but also cause for three Regu-
lation event types.
In this paper, we mainly use dependency pars-
ing information of the analyzed data because sev-
eral previous studies for SRL have improved their
performance by using features extracted from this
information (Hacioglu, 2004; Tsai et al, 2006).
In the experimental results, the proposed system
showed 68.46 f-score in TD phase, 85.20 accuracy
in TC phase, 89.91 f-score in the initial step of RE
phase and 81.24 f-score in the iterative step of RE
phase, but officially achieved 61.65 precision, 9.40
recall and 16.31 f-score in approximate span match-
ing. These figures were the lowest among twenty-
four shared-task participants. However, we found
that the threshold tuning for RE phase had caused
a negative effect. It deteriorates the f-score of the
107
Event Trigger Detector
Event Type Classifier
Relation Recognizer &
Event Compositor
Initial Step
Iterative Step
Source Data
Analyzed Data
Result of Event Extraction
Figure 1: System Architecture
proposed system by enlarging the gap between pre-
cision and recall. With the default threshold, the sys-
tem showed better result in the final test data, 55.32
precision, 16.18 recall and 25.04 f-score with the
rank 17th among 24 teams.
2 System Description
Figure 1 shows our bio-event extraction system
which consists of Event Trigger Detector, Event
Type Classifier and Relation Recognizer & Event
Compositor. Each component includes single or
multiple Maximum Entropy models trained by gold
annotation data. The inputs of the system are source
data and analyzed data. The former is raw text with
entity annotation, and the latter is tokenized, POS
tagged and parsed data of the raw text.1
Because the event type is useful to recognize the
relation, we perform TC phase before RE phase.
One of important characteristics of bio-event is
that one event as well as a protein may participate
in another event. Considering this, we designed the
system in which the Relation Recognizer be per-
formed through two steps. In the initial step, the sys-
tems examines each trigger whether it has the rela-
tions with only proteins, and composites events with
recognized relations. In the iterative step, it repeat-
edly examines remained triggers in the same man-
1We used the GDep result provided by organizers of the
shared task as analyzed data.
ner. This step allows the system to extract chain-
style events, which means that one event participates
in another one and the other participates in the for-
mer.
To increase the f-score, we tuned a threshold for
RE phase which is a binary classification task; de-
ciding whether a given relation candidate is correct
one or not. When the output probability of a maxi-
mum entropy model is lower than the threshold, we
discard a relation candidate.
2.1 Event Trigger Detection
We assume that an event trigger is a single word.
In other words, we do not consider the multi-word
trigger detection. Because the trigger statistic in
the training data showed that about 93% of triggers
are single word, we concentrated on the single word
trigger detection.
This phase is simply defined as the task that clas-
sify whether each token is a trigger or not in a doc-
ument. It is necessary to select targets to classify
among all tokens, because a set of all tokens includes
too many negative examples. For this, the follow-
ing filtering rules are applied to each token. Though
these rules filtered out 69.5% of tokens, the trigger
recall was 94.8%.
? Filter out tokens whose POS tag is not matched
to anything among NN, NNS, VB, VBD, VBG,
VBN, VBP, VBZ, JJ and JJR.
? Filter out tokens that are a biomedical named
entity.
? Filter out sentences that do not have any pro-
teins.
Proposed features for the binary classification of
tokens include both features similar to those used in
(Hacioglu, 2004; Tsai et al, 2006; Ahn, 2006) and
novel ones. The selected feature set is showed in
Table 1.
2.2 Event Type Classification
In TC phase, tokens recognized as trigger are clas-
sified into nine pre-defined classes. Although more
than a dozen features had been tested, the features
except word and lemma features hardly contributed
to the performance improvement. The tuned feature
set is showed in Table 2.
108
Word level features
- Token word
- Token lemma
- Token POS
- POSs of previous two tokens
- Distance, word and POS of the nearest protein
- Positional independence: Whether a noun or a
verb is adjacent to the current token
Dependency level features
- Dependency label path of the nearest protein
- The existence of protein in family: This feature is
motivated by the study in (Hacioglu, 2004)
- A boolean feature which is true if token?s child is
a proposition and the chunk of the child include a
protein
- A boolean feature which is true if token?s child is
a protein and its dependency label is OBJ
Table 1: Features for event trigger detection
Features for the event type classification
- Trigger word
- Trigger lemma
- A boolean feature which is true if a protein exists
within left and right two words
Table 2: Features for event type classification
We found that TC phase showed relatively high
precision and recall with simple lexical features in
the experiment. However, it was quite difficult to
find additional features that could improve the per-
formance.
2.3 Relation Recognition and Event
Composition
In the last phase, the system examines each trigger
whether it has relations with participant candidates,
and composites events with the extracted relations.
(A relation consists of one trigger and one partici-
pant)
We devised a two-step process, consisting of ini-
tial and iterative steps, because a participant candi-
date can be a protein or an event. In the initial step,
the system finds relations between triggers and pro-
tein participant candidates. Features are explained
in Table 3. Then, it generates one event with one
relation for event types that have only one partici-
pant. For Binding event type, the system combines
at most three relations of the same trigger into one
Word level features
- Trigger word
- Trigger lemma
- Trigger type (I-1)
- Entity word
- Entity type (I-2)
- Word sequence between T&P (I-1)
- Word distance
- Existence of another trigger between T&P
- The number of triggers of above feature
- Existence of another participant candidate
- The number of participants of above feature
Dependency level features
- Trigger dependency label (I-1)
- Entity dependency label
- Lemma of trigger?s head word (I-1)
- POS of trigger?s head word
- Lemma of entity?s head word (I-1)
- POS of entity?s head word
- Lemma of trigger?s head word + Lemma of en-
tity?s head word
- Right lemma of trigger?s head word
- 2nd right lemma of trigger?s head word (I-1)
- Right lemma of entity?s head word
- 2nd right lemma of entity?s head word (I-1)
- Dependency path between T&P
- Dependency distance between T&P
- Direct descendant: a participant candidate is a di-
rect descendant of a given trigger
Table 3: Features for relation recognition between a trig-
ger and a participant (T&P)
event. For Regulation event types, we trained a bi-
nary classifier to classify participants of a Regulation
event into theme or cause. Features for participant
type classification is explained in Table 4. Among
multiple participants of a Regulation event, only two
participants having highest probabilities for theme
and cause constitute one event.
In the iterative step, the system finds relations be-
tween triggers and event participant candidates that
were extracted in the previous step, and generates
events in the same manner. The system performs it-
erative steps three times to find chain events.
Features are basically common in the initial (I-1)
step and the iterative (I-2) step, but some features
improve the performance only in one step. In order
to represent the difference in Table 3, we indicate (I-
1) when a feature is used in the initial step only, and
indicate (I-2) when it used in the iterative step only.
109
Word level features
- Trigger word
- Trigger lemma
- Participant words - event?s trigger words if a par-
ticipant is an event
- Left lemma of a participant
- Right lemma of a participant
- Trigger word + Participant words
- Trigger lemma + Participant lemmas
- Participant lemmas
- Right lemma of a trigger
- 2nd right lemma of a trigger
- Right lemma of a participant
- 2nd left lemma of a participant
Dependency level features
- Dependency path
- Dependency relation to trigger?s head
- Dependency relation to participant?s head
- POS pattern of common head chunk of a trigger
and a participant
- POS pattern of common head chunk of a trigger
and a participant + The presence of an object word
in dependency path
Table 4: Features of the participant type classifier for
Regulation events
3 Experimental Result
Table 5 shows the official results of the final test
data. After the feature selection, we have performed
the experiments with the development data to tune
the threshold to be used in RE phase. The work im-
proved the performance slightly. The new thresh-
old discovered by the work was 0.65 rather than
the default value, 0.5. However, we found that the
tuned threshold was over-fitted to development data.
When we tested without any threshold change, the
proposed system showed better f-score by reducing
the gap between precision and recall. Table 6 shows
the performance in this case.
Nevertheless, recall is still quite lower than preci-
sion in Table 6. The reason is that many triggers are
not detected in TD phase. The recall of the trigger
detector was 63% with the development data. An-
alyzing errors of TD phase, we found that the sys-
tem missed terms such as role, prevent while it easily
detected bio-terms such as phosphorylation, regula-
tion. It implies that the word feature causes not only
high precision but also low recall in TD phase.
Event equality recall precision f-score
Strict 8.99 58.97 15.60
Approximate Span 9.40 61.65 16.31
Table 5: The official results with threshold tuning
Event equality recall precision f-score
Strict 15.46 52.85 23.92
Approximate Span 16.18 55.32 25.04
Table 6: The results without threshold tuning
4 Conclusion
In this paper, we have presented a biomedical
event extraction system consisting of trigger detec-
tor, event type classifier and two-step participant rec-
ognizer. The system uses dependency parsing and
predicate argument information as main sources for
feature extraction.
For future work, we would like to increase the
performance of TD phase by adopting two-step
method similar to RE phase. We also will exploit
more analyzed data such as phrase structure parsing
information to improve the performance.
References
Kadri Hacioglu. 2004. Semantic Role Labeling Using
Dependency Trees. In Proceedings of COLING-2004,
Geneva, Switzerland.
Richard Tzong-Han Tsai, Wen-Chi Chou, Yu-Chun Lin,
Cheng-Lung Sung, Wei Ku, Ying-shan Su, Ting-Yi
Sung and Wen-Lian Hsu. 2006. BIOSMILE: Adapt-
ing Semantic Role Labeling for Biomedical Verbs: An
Exponential Model Coupled with Automatically Gen-
erated Template Features. In Proceedings of BioNLP-
2006.
Mihai Surdeanu, Sanda Harabagiu, John Williams and
Paul Aarseth. 2003. Using Predicate-Argument Struc-
tures for Information Extraction. In Proceedings of
ACL-2003, Sapporo, Japan.
David Ahn. 2006. The stages of event extraction. In Pro-
ceedings of Workshop On Annotating And Reasoning
About Time And Events.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop.
110
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 233?237
Manchester, August 2008
Semantic Dependency Parsing using N-best Semantic Role Sequences and
Roleset Information
Joo-Young Lee, Han-Cheol Cho, and Hae-Chang Rim
Natural Language Processing Lab.
Korea University
Seoul, South Korea
{jylee,hccho,rim}@nlp.korea.ac.kr
Abstract
In this paper, we describe a syntactic and
semantic dependency parsing system sub-
mitted to the shared task of CoNLL 2008.
The proposed system consists of five mod-
ules: syntactic dependency parser, predi-
cate identifier, local semantic role labeler,
global role sequence candidate generator,
and role sequence selector. The syntac-
tic dependency parser is based on Malt
Parser and the sequence candidate gen-
erator is based on CKY style algorithm.
The remaining three modules are imple-
mented by using maximum entropy classi-
fiers. The proposed system achieves 76.90
of labeled F1 for the overall task, 84.82 of
labeled attachment, and 68.71 of labeled
F1 on the WSJ+Brown test set.
1 Introduction
In the framework of the CoNLL08 shared task
(Surdeanu et al, 2008), a system takes POS tagged
sentences as input and produces sentences parsed
for syntactic and semantic dependencies as output.
A syntactic dependency is represented by an ID
of head word and a dependency relation between
the head word and its modifier in a sentence. A
Semantic dependency is represented by predicate
rolesets and semantic arguments for each predi-
cate.
The task combines two sub-tasks: syntactic
dependency parsing and semantic role labeling.
Among the sub-tasks, we mainly focus on the se-
mantic role labeling task. Compared to previous
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
CoNLL 2004 and 2005 shared tasks (Carreras and
Ma`rquez, 2004; Carreras and Ma`rquez, 2005) and
other semantic role labeling research, major dif-
ferences of our semantic role labeling task are 1)
considering nominal predicates and 2) identify-
ing roleset of predicates. Based on our observa-
tion that verbal predicate and nominal predicate
have have different characteristics, we decide to
build diffent classification modeles for each pred-
icate types. The modeles use same features but,
their statistical parameters are different. In this
paper, maximum entropy1 is used as the classifi-
cation model, but any other classification models
such as Naive Bayse, SVM, etc. also can be used.
To identify roleset, we investigate a roleset match
scoring method which evaluate how likely a roleset
is matched with the given predicate.
2 System Description
The proposed system sequentially performs syn-
tactic dependency parsing, predicate identification,
local semantic role classification, global sequence
generation, and roleset information based selec-
tion.
2.1 Syntactic Dependency Parsing
In the proposed system, Malt Parser (Nivre et
al., 2007) is adopted as the syntactic dependency
parser. Although the training and test set of
CoNLL08 use non-projective dependency gram-
mar, we decide to use projective parsing algorithm,
Nivre arc-standard, and projective/non-
projective conversion functions that Malt Parser
provides. The reason is that non-projective parsing
shows worse performance than projective parsing
with conversion in our preliminary experiment.
1We use Zhang Le?s MaxEnt toolkit, http://homepages.
inf.ed.ac.uk/s0450736/maxent toolkit.html
233
We projectize the non-projective training sen-
tences in the training set to generate projective sen-
tences. And then, the parser is trained with the
transformed sentences. Finally, the parsing result
is converted into non-projective structure by using
a function of Malt Parser.
2.2 Predicate Identification
Unlike previous semantic role labeling task (Car-
reras and Ma`rquez, 2004; Carreras and Ma`rquez,
2005), predicates of sentences are not provided
with input in the CoNLL08. It means that a sys-
tem needs to identify which words in a sentence
are predicates.
We limit predicate candidates to the words that
exist in the frameset list of Propbank and Nom-
bank. Propbank and Nombank provide lists of
about 3,100 verbal predicates and about 4,400
nominal predicates. After dependency parsing,
words which are located in the frameset list are se-
lected as predicate candidates. The predicate iden-
tifier determines if a candidate is a predicate or not.
The identifier is implemented by using two maxi-
mum entropy models, the one is for verbal predi-
cates and the other is for nominal predicates. The
following features are used for predicate identifi-
cation:
Common Features
For Predicate Identification
- Lemma of Previous Word
- Lemma of Current Word
- Lemma of Next Word
- POS of Previous Word
- POS of Current Word
- POS of Next Word
- Dependency Label of Previous Word
- Dependency Label of Current Word
- Dependency Label of Next Word
Additional Features for Verbal Predicate
- Lemma + POS of Current Word
- Trigram Lemma of Previous, Current,
and Next Word
Additional Features for Nominal Predicate
- Lemma of Head of Current Word
- POS of Head of Current Word
- Dependency Label of Head of Current Word
Verbal predicate identifier shows 87.91 of F1 and
nominal predicate identifier shows 81.58 of F1.
Through a brief error analysis, we found that main
bottle neck for verbal predicate is auxiliary verb
be and have.
2.3 Local Semantic Role Labeling
Prediate identification is followed by argument la-
beling. For the given predicate, the system first
eliminates inappropriate argument candidates. The
argument identification uses different strategies for
verbs, nouns, and other predicates.
The argument classifier extracts features and la-
bels semantic roles. None is used to indicate that
a word is not a semantic argument. The classifier
also uses different maximum entropy models for
verbs, nouns, and other predicates
2.3.1 Argument Candidate Identification
As mentioned by Pradhan et al (2004), ar-
gument identification poses a significant bottle-
neck to improving performance of Semantic Role
Labeling system. We tried an algorithm moti-
vated from Hacioglu (2004) which defined a tree-
structured family membership of a predicate to
identify more probable argument candidates and
prune the others. However, we find that it works
for verb and other predicate type, but does not
work properly for noun predicate type. The main
reason is due to the characteristics of arguments
of noun predicates. First of all, a noun predicate
can be an argument for itself, whereas a verb pred-
icate cannot be. Secondly, dependency relation
paths from a noun predicate to its arguments are
usually shorter than a verb predicate. Although
some dependency relation paths are long, they ac-
tually involve non-informative relations like IN,
MD, or TO. Finally, major long distance relation
paths could be identified by several path patterns
acquired from the corpus.
Based on the above analysis, we specify a new
argument identification strategy for nominal pred-
icate type. The argument identifier regards a pred-
icate and its nearest neighbors - its parent and chil-
dren - as argument candidates. However, if the
POS tag of a nearest neighbor is IN, MD, or TO, it
will be ignored and the next nearest candidates will
be used. Moreover, several patterns (three consec-
utive nouns, adjective and two consecutive nouns,
two nouns combined with conjunction, and etc.)
are applied to find long distance argument candi-
dates.
234
2.3.2 Argument Classification
For argument classification, various features
have been used. Primarily, we tested a set of fea-
tures suggested by Hacioglu (2004). The voice of
the predicate, left and right words, its POS tag for
a predicate, and lexical clues for adjunctive argu-
ments also have been tested. Based on the type
of predicate (i.e. verb predicate, noun predicate,
and other predicate) three classification models are
trained by using maximum entropy with the fol-
lowing same features:
Features for Argument Classification
- Dependen Relation Type
- Family Membership
- Position
- Lemma of Head Word
- POS of Head Word
- Path
- POS Pattern of Predicate?s Children
- Relation Pattern of Predicate?s Children
- POS Pattern of Predicate?s Siblings
- Relation Pattern of Predicate?s Siblings
- POS of candidate
- Lemma of Left Word of Candidate
- POS of Left Word of Candidate
- Lemma of Right Word of Candidate
- POS of Right Word of Candidate
The classifier produces a list of possible seman-
tic roles and its probabilities for each word in the
given sentence.
2.4 Global Semantic Role Sequence
Generation
For local semantic role labeling, we assume that
semantic roles of words are independent of each
other. Toutanova et al (2005) and Surdeanu et
al. (2007) show that global constraint and opti-
mization are important in semantic role labeling.
We use CKY-based dynamic programming strat-
egy, similar to Surdeanu et al (2007), to verify
whether role sequences satisfy global constraint
and generate candidates of global semantic role se-
quences.
In this paper, we just use one constraint: no
duplicate arguments are allowed for verbal pred-
icates. For verbal predicates, CKY module builds
a list of all kinds of combinations of semantic roles
augmented with their probabilities. While building
the list of semantic role sequences, it removes the
sequences that violate the global constraint. The
output of CKY module is the list of semantic role
sequences satisfying the global constraint.
2.5 Global Sequence Selection using Roleset
Information
Finally, we need to select the most likely semantic
role sequence. In addition, we need to identify a
roleset for a predicate. We perform these tasks by
finding a role sequence and roleset maximizing a
score on the following formula:
? ? c+ ? ? rf + ? ? mc (1)
where, c, rf , mc are role sequence score, relative
frequence of roleset, and matching score with role-
set respectively. ?, ?, ? are tuning parameters of
each factor and decided empirically by using de-
velopment set. In this paper, we set ?, ?, ? to 0.5,
0.3, 0.2, respectively.
The role sequence score is calculated in the
global semantic role sequence generation ex-
plained in Section 2.4. The relative frequency of a
roleset means how many times the roleset occurred
in the training set compared to the total occurrence
of the predicate. It can be easily estimated by
MLE.
The remaining problem is how to calculate the
matching score. We use maximum entropy models
as binary classifiers which output match and not-
match and use probability of match as matching
score. The features used for the roleset matching
classifiers are based on following intuitions:
? If core roles (e.g., A0, A2, etc) defined in
a roleset occur in a given role sequence, it
seems to be the right roleset for the role se-
quence.
? If matched core roles are close to or have de-
pendency relations with a predicate, it seems
to be the right roleset.
? If a roleset has a particle and the predicate of
a sentence also has that particle, it seems to
be the right roleset. For example, the lemma
of predicate node for the roleset cut.05
in frameset file ?cut.xml.gz? is cut back, so
the particle of cut.05 is back. If the predicate
of a sentence also has particle ?back?, it seems
to be the right roleset.
? If example node of a roleset in frameset file
has a functional word for certain core role that
235
also exists in a given sentence, it seems to be
the right roleset. For example, example node
is defined as follows2:
<roleset id="cut.09" ...>
<example>
<text>
As the building?s new owner,
Chase will have its work cut
out for it.
</text>
<arg n="1">its work</arg>
<rel>cut out</rel>
<arg n="2" f="for">it</arg>
</example>
</roleset>
Here, semantic role A2 has functional word
for. If a given role sequence has A2 and its
word is ?for?, than this role sequence probably
matches that roleset.
Based on these intuitions, we use following fea-
tures for roleset matching:
? Core Role Matching Count The number of
core roles exist in both roleset definition and
given role sequence
? Distance of Matched Core Role Distance
between predicate and core role which ex-
ists in both roleset and given role sequence.
We use number of word and dependency path
length as a distance
? Indication for Same Particle It becomes
yes if given predicate and roleset have same
particle. (otherwise no)
? Indication for Same Functional Word It be-
comes yes if one of core argument is same to
the functional word of roleset. (otherwise no)
To train the roleset match classifiers, we extract
semantic role sequence and its roleset from train-
ing data as a positive example. And then, we gen-
erate negative examples by changing its roleset to
other roleset of that predicate. For example, the
above sentence in <text> node3 becomes a pos-
itive example for cut.09 and negative examples
for other roleset such as cut.01, cut.02, etc.
2Some nodes are omitted to simplify the definition of ex-
ample.
3Of cause, we assume that this sentence exist in training
corpus. So, we will extract it from corpus, not from frameset
file.
WSJ+Brown WSJ Brown
LM 76.90 77.96 68.34
LA 84.82 85.69 77.83
LF 68.71 69.95 58.63
Table 1: System performance. LM, LA, LF means
macro labeled F1 for the overall task, labeled at-
tachment for syntactic dependencies, and labeled
F1 for semantic dependencies, respectively
Labeled Prec. Labeled Rec. Labeled F1
88.68 73.89 80.28
Table 2: Performance of Local Semantic Role La-
beler n WSJ test set. Gold parsing result, correct
predicates, and correct rolesets are used.
3 Experimental Result
We have tested our system with the test set and
obtained official results as shown in Table 1. We
have also experimented on each module and ob-
tained promising results.
We have tried to find the upper bound of the
local semantic role labeling module. Table 2
shows the performance when gold syntactic pars-
ing result, correct predicates, and correct rolesets
are given. Comparing to phrase structure parser
based semantic role labelings such as Pradhan et
al. (2005) and Toutanova et al (2005), our local
semantic role labeler needs to enhance the perfor-
mance. We will try to add some lexical features or
chunk features in future works.
Next, we have analyzed the effect of roleset
based selector. Table 3 shows the effect of match-
ing score and relative frequency which are the
weighted factor of selection described in section
2.5. Here, baseline means that it selects a role se-
quence which has the highest score in CKY mod-
ule and roleset is chosen randomly. The results
show that roleset matching score and relative fre-
quency of roleset are effective to choose the correct
role sequence and identify roleset.
4 Conclusion
In this paper, we have described a syntactic and
semantic dependency parsing system with five dif-
ferent modules. Each module is developed with
maximum entropy classifiers based on different
predicate types. In particular, dependency relation
compression method and extracted path patterns
are used to improve the performance in the argu-
236
Prec. Rec. F1
Baseline (c) 69.34 58.42 63.41
+ mc 71.40 60.20 65.32
+ rf 75.94 63.98 69.45
+ mc, rf 76.46 64.45 69.95
Table 3: Semantic scores of global sequence selec-
tion in WSJ test set. mc, rf means matching score
and relative frequency, respectively
ment candidate identification. The roleset match-
ing method is devised to select the most appropri-
ate role sequence and to identify the correct role-
set.
However, the current features for roleset match-
ing seem to be not enough and other useful features
are expected to be found in the future work. There
is also a room for improving the method to inte-
grate the role sequence score, matching score, and
the relative frequency.
References
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, Erwin Marsi. 2007. MaltParser: A
Language-Independent System for Data-Driven De-
pendency Parsing. Natural Language Engineering,
13(2):95?135.
Kadri Hacioglu. 2008. Semantic role labeling using
dependency trees. In COLING ?04: Proceedings of
Proceedings of the 20th international conference on
Computational Linguistics. Morristown, NJ, USA.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic
role labeling. In ACL ?05: Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics. Morristown, NJ, USA.
Mihai Surdeanu and Richard Johansson and Adam
Meyers and Llu??s Ma`rquez and Joakim Nivre. 2008.
The CoNLL-2008 Shared Task on Joint Parsing of
Syntactic and Semantic Dependencies. In Proceed-
ings of the 12th Conference on Computational Natu-
ral Language Learning (CoNLL-2008).
Mihai Surdeanu, Llu??s Ma`rquez, Xavier Carreras, and
Pere Comas. 2007. Combination Strategies for Se-
mantic Role Labeling. The Journal of Artificial In-
telligence Research, 29:105?151.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, and Daniel Jurafsky.
2005. Support Vector Learning for Semantic Argu-
ment Classification. Machine Learning. 60:11?39.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2004. Shallow Se-
mantic Parsing Using Support Vector Machines. In
Proceedings of the Human Language Technology
Conference/North American chapter of the Associ-
ation of Computational Linguistics (HLT/NAACL).
Boston, MA, USA.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 Shared Task: Semantic Role
Labeling. In Proceedings of CoNLL-2005.
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduc-
tion to the CoNLL-2004 Shared Task: Semantic Role
Labeling. In Proceedings of CoNLL-2004.
237
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 132?140,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Towards Event Extraction from Full Texts on Infectious Diseases
Sampo Pyysalo? Tomoko Ohta? Han-Cheol Cho? Dan Sullivan?
Chunhong Mao? Bruno Sobral? Jun?ichi Tsujii??? Sophia Ananiadou??
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?Virginia Bioinformatics Institute, Virginia Tech, Blacksburg, Virginia, USA
?School of Computer Science, University of Manchester, Manchester, UK
?National Centre for Text Mining, University of Manchester, Manchester, UK
{smp,okap,priancho,tsujii}@is.s.u-tokyo.ac.jp
{dsulliva,cmao,sobral}@vbi.vt.edu
Sophia.Ananiadou@manchester.ac.uk
Abstract
Event extraction approaches based on ex-
pressive structured representations of ex-
tracted information have been a significant
focus of research in recent biomedical nat-
ural language processing studies. How-
ever, event extraction efforts have so far
been limited to publication abstracts, with
most studies further considering only the
specific transcription factor-related subdo-
main of molecular biology of the GENIA
corpus. To establish the broader relevance
of the event extraction approach and pro-
posed methods, it is necessary to expand
on these constraints. In this study, we pro-
pose an adaptation of the event extraction
approach to a subdomain related to infec-
tious diseases and present analysis and ini-
tial experiments on the feasibility of event
extraction from domain full text publica-
tions.
1 Introduction
For most of the previous decade, biomedical In-
formation Extraction (IE) efforts have focused pri-
marily on tasks that allow extracted information
to be represented as simple pairs of related enti-
ties. This representation is applicable to many IE
targets of interest, such as gene-disease associa-
tions (Chun et al, 2006) and protein-protein inter-
actions (Ne?dellec, 2005; Krallinger et al, 2007).
However, it has limited applicability to advanced
applications such as semantic search, Gene On-
tology term annotation, and pathway extraction,
tasks for which and relatively few resources or sys-
tems (e.g. (Rzhetsky et al, 2004)) have been intro-
duced. A number of recent studies have proposed
more expressive representations of extracted in-
formation, introducing resources supporting ad-
vanced IE approaches (Pyysalo et al, 2007; Kim
et al, 2008; Thompson et al, 2009; Ananiadou
et al, 2010a). A significant step in the develop-
ment of domain IE methods capable of extract-
ing this class of representations was taken in the
BioNLP?09 shared task on event extraction, where
24 teams participated in an IE task setting requir-
ing the extraction of structured representations of
multi-participant biological events of several types
(Kim et al, 2009).
While the introduction of structured event ex-
traction resources and methods has notably ad-
vanced the state of the art in biomedical IE rep-
resentations, the focus of event extraction studies
carries other limitations frequently encountered in
domain IE efforts. Specifically, resources anno-
tated for biomedical events contain exclusively
texts from publication abstracts, typically further
drawn from small subdomains of molecular biol-
ogy. These choices constrain not only the types of
texts but also the types of events considered, re-
stricting the applicability of event extraction. This
paper presents results from one ongoing effort to
extend an event extraction approach over these
boundaries, toward event extraction from full text
documents in the domain of infectious diseases.
In this study, we consider the subdomain related
to Type IV secretion systems as a model subdo-
main of interest within the broad infectious dis-
eases domain. Type IV secretion systems (T4SS)
are mechanisms for transferring DNA and pro-
teins across cellular boundaries. T4SS are found
in a broad range of Bacteria and in some Ar-
chaea. These translocation systems enable gene
transfer across cellular membranes thus contribut-
ing to the spread of antibiotic resistance and viru-
132
Figure 1: Event representation example. Inhibition of binding caused by phosphorylation is represented
using three events. The shaded text background identifies the text bindings of the events and entities.
lence genes making them an especially important
mechanism in infectious disease research (Juhas et
al., 2008). Type IV secretion systems are found in
plant pathogens, such as Agrobacterium tumefa-
ciens, the cause of crown gall disease as well as in
animal pathogens, such as Helicobacter pylori, a
cause of severe gastric disease. The study of T4SS
has been hampered by the lack of consistent termi-
nology to describe genes and proteins associated
with the translocation mechanism thus motivating
the use of natural language processing techniques
to enhance information retrieval and information
extraction from relevant literature.
2 Event Extraction for the T4SS Domain
This section presents the application of an event
extraction approach to the T4SS domain.
2.1 Event Extraction
We base our information extraction approach on
the model introduced in the BioNLP?09 shared
task on event extraction. Central to this approach
is the event representation, which can capture
the association of multiple participants in varying
roles and numbers and treats events as primary ob-
jects of annotation, thus allowing events to be par-
ticipants in other events. Further, both entities and
events are text-bound, i.e. anchored to specific ex-
pressions in text (Figure 1).
The BioNLP?09 shared task defined nine event
types and five argument types (roles): Theme spec-
ifies the core participant(s) that an event affects,
Cause the cause of the the event, Site a specific
domain or region on a participant involved in the
event, and ToLoc and AtLoc locations associated
with localization events (Table 1). Theme and
Cause arguments may refer to either events or
gene/gene product entities, and other arguments
refer to other physical entities. The Theme ar-
gument is always mandatory, while others can be
omitted when a relevant participant is not stated.
The event types were originally defined to cap-
ture statements of biologically relevant changes in
Event type Args Example
Gene expression T 5-LOX is coexpressed
Transcription T IL-4 transcription
Protein catabolism T IkB-A proteolysis
Localization T,L translocation of STAT6
Phosphorylation T,S NF90 was phosphorylated
Binding T+,S+ Nmi interacts with STAT
Regulation T,C,S IL-4 gene control
Positive regulation T,C,S IL-12 induced binding
Negative regulation T,C,S suppressed dimerization
Table 1: Event types targeted in the BioNLP?09
shared task and their arguments, with minimal
examples of each event type. Arguments ab-
breviate for (T)heme, (C)ause, (S)ite and L for
ToLoc/AtLoc, with ?+? identifying arguments
than can occur multiple times. The expression
marked as triggering the event shown in italics.
the state of entities in a target subdomain involv-
ing transcription factors in human blood cells. In
adapting the approach to new domains, some ex-
tension of the event types is expected to be nec-
essary. By contrast, the argument types and the
general design of the representation are intended
to be general, and to maintain compatibility with
existing systems we aim to avoid modifying these.
2.2 T4SS Domain
A corpus of full-text publications relating to the
T4SS subdomain of the infectious diseases do-
main annotated for biological entities and terms of
interest to domain experts was recently introduced
by (Ananiadou et al, 2010b). In the present study,
we use this corpus as a reference standard defin-
ing domain information needs. In the following
we briefly describe the corpus annotation and the
view it provides of the domain.
The T4SS corpus annotation covers four classes
of tagged entities and terms: Bacteria, Cellular
components, Biological Processes, and Molecular
functions. The latter three correspond to the three
Gene Ontology (GO) (Ashburner et al, 2000) top-
level sub-ontologies, and terms of these types were
annotated with reference to both GO and relevance
to the interests of domain experts, with guidelines
133
Bacterium
A. tumefaciens 32.7%
H. pylori 20.0%
L. pneumophila 16.3%
E. coli 12.3%
B. pertussis 3.0%
Cell component
T4SS 5.2%
Ti plasmid 5.1%
outer membrane 4.2%
membrane 3.5%
genome 3.4%
Biological process
virulence 14.1%
conjugation 7.9%
localization 6.1%
nuclear import 5.8%
transfer 5.1%
Molecular function
nucleotide-binding 20.3%
ATPase activity 17.3%
NTP-binding 14.7%
ATP-binding 12.2%
DNA-binding 9.1%
Table 2: Most frequently tagged terms (after normalization) and their relative frequencies of all tagged
entities of each of the four types annotated in the T4SS corpus.
Type Annotations
Bacteria 529
Cellular component 2237
Biological process 1873
Molecular function 197
Table 3: Statistics for the existing T4SS corpus
annotation.
requiring that marked terms be both found in GO
and associated with T4SS. These constraints as-
sure that the corpus is relevant to the informa-
tion needs of biologists working in the domain and
that it can be used as a reference for the study of
automatic GO annotation. In the work introduc-
ing the corpus, the task of automatic GO anno-
tation was studied as facilitating improved infor-
mation access, such as advanced search function-
ality: GO annotation can allow for search by se-
mantic classes or co-occurrences of terms of speci-
fied classes. The event approach considered in this
study further extends on these opportunities in in-
troducing a model allowing e.g. search by specific
associations of the concepts of interest.
The previously created annotation of the T4SS
corpus covers 27 full text publications totaling
15143 pseudo-sentences (text sentences plus table
rows, references, etc.) and 244942 tokens.1 A to-
tal of nearly 5000 entities and terms are annotated
in these documents; Table 2 shows the most fre-
quently tagged terms of each type after basic nor-
malization of different surface forms, and Table 3
gives the per-class statistics. Domain characteris-
tics are clearly identifiable in the first three tagged
types, showing disease-related bacteria, their ma-
jor cellular components, and processes related to
movement, reproduction and infection. The last
term type is dominated by somewhat more generic
binding-type molecular functions.
In addition to the four annotated types it was
1While the document count is modest compared to that
of abstract-based corpora, we estimate that in terms of the
amount of text (tokens) the corpus corresponds to over 1000
abstracts, comparable in size to e.g. the GENIA event corpus
(Kim et al, 2008).
recognized during the original T4SS corpus anno-
tation that genes and gene products are centrally
important for domain information needs, but their
annotation was deferred to focus on novel cate-
gories. As part of the present study, we introduce
annotation for gene/gene product (GGP) mentions
(Section 3.2), and in the following discussion of
applying an event extraction approach to the do-
main the availability of this class annotation as an
additional category is assumed.
2.3 Adaptation of the Event Model
The event model involves two primary categories
of representation: physical entities such as genes
and proteins are elementary (non-structured) and
their mentions annotated as typed spans of text,2
and events and processes (?things that happen?)
are represented using the structured event repre-
sentation described in Section 2.1. This division
applies straightforwardly to the T4SS annotations,
suggesting an approach where bacteria and cell
components retain their simple tagged-term repre-
sentation and the biological processes and molec-
ular functions are given an event representation.
In the following, we first analyze correspondences
between the latter two classes and BioNLP?09
shared task events, and then proceed to study the
event arguments and their roles as steps toward a
complete event model for the domain.
Molecular functions, the smallest class tagged
in the T4SS corpus, are highly uniform: almost
75% involve binding, immediately suggesting rep-
resentation using the Binding class of events de-
fined in the applied event extraction model. The
remaining functions are ATPase activity, together
with its exact GO synonyms (e.g. ATP hydrolase
activity) accounting for 19% of the terms, the gen-
eral type hydrolysis (4.5%), and a small number
of rare other functions. While these have no cor-
respondence with previously defined event types,
2Normalization identifying e.g. the Uniprot entry corre-
sponding to a protein mention may also be necessary, but here
excluded from consideration an independent issue.
134
Class Category Freq
Location
Transfer 27.6%
Localization 15.6%
Import/export 14.5%
Virulence 14.1%
High-level Assembly 8.7%
process Conjugation 8.3%
Secretion 8.1%
(Other) 1.8%
Table 4: Categorization of T4SS corpus biologi-
cal processes and relative frequency of mentions
of each category of the total tagged.
their low overall occurrence counts make them of
secondary interest as extraction targets.
The biological processes are considerably more
diverse. To identify general categories, we per-
formed a manual analysis of the 217 unique nor-
malized terms annotated in the corpus as biologi-
cal processes (Table 4). We find that the majority
of the instances (58%) relate to location or move-
ment. As related types of statements are anno-
tated as Localization events in the applied model,
we propose to apply this event type and differen-
tiate between the specific subtypes on the basis of
the event arguments. A further 39% are of cate-
gories that can be viewed as high-level processes.
These are distinct from the events considered in
the BioNLP?09 shared task in involving coarser-
grained events and larger-scale participants than
the GGP entities considered in the task: for ex-
ample, conjugation occurs between bacteria, and
virulence may involve a human host.
To analyze the role types and arguments char-
acteristic of domain events, we annotated a small
sample of tagged mentions for the most fre-
quent types in the broad classification discussed
above: Binding for Molecular function, Transfer
for Location-related, and Virulence for High-level
process. The statistics of the annotated 65 events
are shown in Tables 5, 6 and 7. For Binding, we
find that while an estimated 90% of events in-
volve a GGP argument, the other participant of
the binding is in all cases non-GGP, most fre-
quently of Nucleotide type (e.g. NTP/ATP). While
only GGP Binding arguments were considered in
the shared task events, the argument structures are
typical of multi-participant binding and this class
of expressions are in scope of the original GE-
NIA Event corpus annotation (Kim et al, 2008).
Event annotations could thus potentially be de-
rived from existing data. Localization event
arguments show substantially greater variety and
Freq Arguments
78% Theme: GGP, Theme: Nucleotide
5.5% Theme: GGP, Theme: DNA
5.5% Theme: GGP, Theme: Sugar
5.5% Theme: Protein family, Theme: DNA
5.5% Theme: Protein, Theme: Nucleotide
Table 5: Binding event arguments.
Freq Arguments
16% Theme: DNA, From/To: Organism
16% Theme: DNA
16% Theme: Cell component
12% Theme: DNA, To: Organism
8% Theme: Protein family, From/To: Organism
4% Theme: GGP
4% Theme: GGP, To: Organism
4% Theme: GGP, From: Organism
4% Theme: Protein family, From: Organism
4% Theme: Protein family
4% Theme: Organism, To: Cell component
4% Theme: DNA From: Organism, To: Cell component
4% (no arguments)
Table 6: Localization (Transfer) event arguments.
Freq Arguments
64% Cause: GGP
16% Theme:Organism, Cause: GGP
8% Cause: Organism
8% (no arguments)
4% Cause: Protein family
Table 7: Process (Virulence) arguments.
some highly domain-specific argument combina-
tions, largely focusing on DNA and Cell compo-
nent (e.g. phagosome) transfer, frequently involv-
ing transfer between different organisms. While
the participants are almost exclusively of types
that do not appear in Localization events in exist-
ing annotations, the argument structures are stan-
dard and in our judgment reasonably capture the
analyzed statements, supporting the applicability
of the general approach. Finally, the argument
analysis shown in Table 7 supports the previous
tentative observation that the high-level biologi-
cal processes are notably different from previously
considered event types: for over 80% of these pro-
cesses no overtly stated Theme could be identified.
We take this to indicate that the themes ? the core
participants that the processes concern ? are ob-
vious in the discourse context and their overt ex-
pression would be redundant. (For example, in
the context virulence obviously involves a host and
conjugation involves bacteria.) By contrast, in the
corpus the entities contributing to these processes
are focused: a participant we have here analyzed
as Cause is stated in over 90% of cases. This
135
Sentences Tokens
Abstracts 150 3789
Full texts 448 13375
Total 598 17164
Table 8: Statistics for the selected subcorpus.
novel pattern of event arguments suggests that the
event model should be augmented to capture this
category of high-level biological processes. Here,
we propose an event representation for these pro-
cesses that removes the requirement for a Theme
and substitutes instead a mandatory Cause as the
core argument. In the event annotation and exper-
iments, we focus on this newly proposed class.
3 Annotation
This section describes the new annotation intro-
duced for the T4SS corpus.
3.1 Text Selection
The creation of exhaustive manual annotation for
the full T4SS corpus represents a considerable an-
notation effort. Due to resource limitations, for
this study we did not attempt full-scope annota-
tion but instead selected a representative subset of
the corpus texts. We aimed to select texts that pro-
vide good coverage of the text variety in the T4SS
corpus and can be freely redistributed for use in re-
search. We first selected for annotation all corpus
documents with at least a freely available PubMed
abstract, excluding 3 documents. As the corpus
only included a single freely redistributable Open
Access paper, we extended full text selection to
manuscripts freely available as XML/HTML (i.e.
not only PDF) via PubMed Central. While these
documents cannot be redistributed in full, their
text can be reliably combined with standoff anno-
tations to recreate the annotated corpus.
In selected full-text documents, to focus anno-
tation efforts on sections most likely to contain re-
liable new information accessible to natural lan-
guage processing methods, we further selected the
publication body text, excluding figures and tables
and their captions, and removed Methods and Dis-
cussion sections. We then removed artifacts such
as page numbers and running heads and cleaned
remaining errors from PDF conversion of the orig-
inal documents. This selection produced a subcor-
pus of four full-text documents and 19 abstracts.
The statistics for this corpus are shown in Table 8.
GGP GGP/sentence
Abstracts 124 0.82
Full texts 394 0.88
Total 518 0.87
Table 9: Statistics for the GGP annotation.
3.2 Gene/Gene Product Annotation
As gene and gene product entities are central to
domain information needs and the core entities of
the applied event extraction approach, we first in-
troduced annotation for this entity class. We cre-
ated manual GGP annotation following the an-
notation guidelines of the GENIA GGP Corpus
(Ohta et al, 2009). As this corpus was the source
of the gene/protein entity annotation provided as
the basis of the BioNLP shared task on event ex-
traction, adopting its annotation criteria assures
compatibility with recently introduced event ex-
traction methods. Briefly, the guidelines spec-
ify tagging for minimal continuous spans of spe-
cific gene/gene product names, without differen-
tiating between DNA/RNA/protein. A ?specific
name? is understood to be a a name that allows
a domain expert to identify the entry in a rele-
vant database (Entrez gene/Uniprot) that the name
refers to. Only GGP names are tagged, excluding
descriptive references and the names of related en-
tities such as complexes, families and domains.
The annotation was created on the basis of an
initial tagging created by augmenting the output
of the BANNER tagger (Leaman and Gonzalez,
2008) by dictionary- and regular expression-based
tagging. This initial high-recall markup was then
corrected by a human annotator. To confirm that
the annotator had correctly identified subdomain
GGPs and to check against possible error intro-
duced through the machine-assisted tagging, we
performed a further verification of the annotation
on approx. 50% of the corpus sentences: we com-
bined the machine- and human-tagged annotations
as candidates, removed identifying information,
and asked two domain experts to identify the cor-
rect GGPs. The two sets of independently pro-
duced judgments showed very high agreement:
holding one set of judgments as the reference stan-
dard, the other would achieve an f-score of 97%
under the criteria presented in Section 4.2. We
note as one contributing factor to the high agree-
ment that the domain has stable and systematically
applied GGP naming criteria. The statistics of the
full GGP annotation are shown in Table 9.
136
Events Event/sentence
Abstracts 15 0.1
Full texts 5 0.01
Additional 80 2.2
Total 100 0.16
Table 10: Statistics for the event annotation.
3.3 Event Annotation
Motivated by the analysis described in Section 2.3,
we chose to focus on the novel category of asso-
ciations of GGP entities in high-level processes.
Specifically, we chose to study biological pro-
cesses related to virulence, as these are the most
frequent case in the corpus and prototypical of the
domain. We adopted the GENIA Event corpus an-
notation guidelines (Kim et al, 2008), marking as-
sociations between specific GGPs and biological
processes discussed in the text even when these
are stated speculatively or their existence explic-
itly denied. As the analysis indicated this category
of processes to typically involve a single stated
participant in a fixed role, annotations were ini-
tially recorded as (GGP, process) pairs and later
converted into an event representation.
During annotation, the number of annotated
GGP associations with the targeted class of pro-
cesses in the T4SS subcorpus was found to be too
low to provide material for both training and test-
ing a supervised learning-based event extraction
approach. To extend the source data, we searched
PubMed for cases where a known T4SS-related
protein co-occurred with an expression known to
relate to the targeted process class (e.g. virulence,
virulent, avirulent, non-virulent) and annotated a
further set of sentences from the search results for
both GGPs and their process associations. As the
properties of these additional examples could not
be assured to correspond to those of the targeted
domain texts, we used these annotations only as
development and training data, performing evalu-
ation on cases drawn from the T4SS subcorpus.
As the annotation target was novel, we per-
formed two independent sets of judgments for all
annotated cases, jointly resolving disagreements.
Although initial agreement was low, for a final set
of judgments we measured high agreement, corre-
sponding to 93% f-score when holding one set of
judgments as the gold standard. The statistics of
the annotation are shown in Table 10. Annotations
are sparse in the T4SS subcorpus and, as expected,
very dense in the targeted additional data.
4 Experiments
4.1 Methods
For GGP tagging experiments, we applied a state-
of-the-art tagger with default settings as reference
and a custom tagger for adaptation experiments.
As the reference tagger, we applied a recent re-
lease of BANNER (Leaman and Gonzalez, 2008)
trained on the GENETAG corpus (Tanabe et al,
2005). The corpus is tagged for gene and protein-
related entities and its texts drawn from a broad
selection of PubMed abstracts. The current revi-
sion of the tagger3 achieves an f-score of 86.4%
on the corpus, competitive with the best result re-
ported in the BioCreative II evaluation (Wilbur et
al., 2007), 87.2%. The custom tagger4 follows the
design of BANNER in both the choice of Con-
ditional Random Fields (Lafferty et al, 2001) as
the applied learning method and the basic feature
design, but as a key extension can further adopt
features from external dictionaries as both positive
and negative indicators of tagged entities. Tagging
experiments were performed using a document-
level 50/50 split of the GGP-annotated subcorpus.
For event extraction, we applied an adapta-
tion of the approach of the top-ranking system in
the BioNLP?09 shared task (Bjo?rne et al, 2009):
all sentences in the input text were parsed with
the McClosky-Charniak (2008) parser and the re-
sulting phrase structure analyses then converted
into the Stanford Dependency representation us-
ing conversion included in the Stanford NLP tools
(de Marneffe et al, 2006). Trigger recognition
was performed with a simple regular expression-
based tagger covering standard surface form vari-
ation. Edge detection was performed using a su-
pervised machine learning approach, applying the
LibSVM (Chang and Lin, 2001) Support Vector
Machine implementation with a linear kernel and
the feature representation of Bjo?rne et al (2009),
building largely around the shortest dependency
path connecting a detected trigger with a candi-
date participant. The SVM regularization parame-
ter was selected by a sparse search of the parame-
ter space with evaluation using cross-validation on
the training set. As the class of events targeted for
extraction in this study are of a highly restricted
type, each taking only of a single mandatory Cause
argument, the construction of events from detected
3http://banner.sourceforge.net
4http://www-tsujii.is.s.u-tokyo.ac.jp/
NERsuite/
137
Precision Recall F-score
Abstracts 68.1% 89.5% 77.3%
Full texts 56.9% 80.7% 66.7%
Total 59.4% 82.8% 69.2%
Table 11: Initial GGP tagging results.
triggers and edges could be implemented as a sim-
ple deterministic rule.
4.2 Evaluation Criteria
For evaluating the performance of the taggers we
apply a relaxed matching criterion that accepts a
match between an automatically tagged and a gold
standard entity if the two overlap at least in part.
This relaxation is adopted to focus on true tagging
errors. The GENETAG entity span guidelines dif-
fer from the GENIA GGP guidelines adopted here
in allowing the inclusion of e.g. head nouns when
names appear in modifier position, while the an-
notation guidelines applied here require marking
only the minimal name.5 When applying strict
matching criteria, a substantial number of errors
may trace back to minor boundary differences
(Wang et al, 2009), which we consider of sec-
ondary interest to spurious or missing tags. Over-
all results are microaverages, that is, precision, re-
call and f-score are calculated from the sum of true
positive etc. counts over individual documents.
For event extraction, we applied the BioNLP?09
shared task event extraction criteria (Kim et al,
2009) with one key change: to make it possible
to evaluate the extraction of the high-level pro-
cess participants, we removed the requirement that
all events must define a Theme as their core argu-
ment.
4.3 Gene/Gene Product Tagging
The initial GGP tagging results using BANNER
are shown in Table 11. We find that even for the
relaxed overlap matching criterion, the f-score is
nearly 10% points lower than reported on GENE-
TAG in the evaluation on abstracts. For full texts,
performance is lower yet by a further 10% points.
In both cases, the primary problem is the poor
precision of the tagger, indicating that many non-
GGPs are spuriously tagged.
To determine common sources of error, we per-
formed a manual analysis of 100 randomly se-
lected falsely tagged strings (Table 12). We find
5GENETAG annotations include e.g. human ets-1 protein,
whereas the guidelines applied here would require marking
only ets-1.
Category Freq Examples
GGP family or group 34% VirB, tmRNA genes
Figure/table 26% Fig. 1B, Table 1
Cell component 10% T4SS, ER vacuole
Species/strain 9% E. coli, A348deltaB4.5
Misc. 9% step D, Protocol S1
GGP domain or region 4% Pfam domain
(Other) 8% TrIP, LGT
Table 12: Common sources of false positives in
GGP tagging.
Precision Recall F-score
Abstracts 90.5% 95.7% 93.1%
Full texts 90.0% 93.2% 91.6%
Total 90.1% 93.8% 91.9%
Table 13: GGP tagging results with domain adap-
tation.
that the most frequent category consists of cases
that are arguably correct by GENETAG annota-
tion criteria, which allow named protein families
of groups to be tagged. A similar argument can
be made for domains or regions. Perhaps not sur-
prisingly, a large number of false positives relate
to features common in full texts but missing from
the abstracts on which the tagger was trained, such
as figure and table references. Finally, systematic
errors are made for entities belonging to other cat-
egories such as named cell components or species.
To address these issues, we applied a domain-
adapted custom tagger that largely replicates the
features of BANNER, further integrating infor-
mation from the UMLS Metathesaurus,6 which
provides a large dictionary containing terms cov-
ering 135 different semantic classes, and a cus-
tom dictionary of 1081 domain GGP names, com-
piled by (Ananiadou et al, 2010b). The non-GGP
UMLS Metathesaurus terms provided negative in-
dicators for reducing spurious taggings, and the
custom dictionary positive indicators. Finally, we
augmented the GENETAG training data with 10
copies7 of the training half of the T4SS GGP cor-
pus as in-domain training data.
Table 13 shows the results with the domain-
adapted tagger. We find dramatically improved
performance for both abstracts and full texts,
showing results competitive with the state of the
art performance on GENETAG (Wilbur et al,
2007). Thus, while the performance of an un-
adapted tagger falls short of both results reported
6http://www.nlm.nih.gov/research/umls/
7As the GENETAG corpus is considerably larger than the
T4SS GGP corpus, replication was used to assure that suffi-
cient weight is given to the in-domain data in training.
138
Precision Recall F-score
Co-occurrence 65% 100% 78%
Machine learning 81% 85% 83%
Table 14: Event extraction results.
on GENETAG and levels necessary for practi-
cal application, adaptation addressing common
sources of error through the adoption of general
and custom dictionaries and the use of a small
set of in-domain training data was successful in
addressing these issues. The performance of the
adapted tagger is notably high given the modest
size of the in-domain data, perhaps again reflect-
ing the consistent GGP naming conventions of the
subdomain.
4.4 Event Extraction
We performed an event extraction experiment fol-
lowing the training and test split described in Sec-
tion 3.3. Table 14 shows the results of the ap-
plied machine learning-based method contrasted
with a co-occurrence baseline replacing the edge
detection with a rule that extracts a Cause edge for
all trigger-GGP combinations co-occurring within
sentence scope. This approach achieves 100% re-
call as the test data was found to only contain
events where the arguments are stated in the same
sentence as the trigger.
The results show that the machine learning ap-
proach achieves very high performance, matching
the best results reported for any single event type
in the BioNLP?09 shared task (Kim et al, 2009).
The very high co-occurrence baseline result sug-
gests that the high performance largely reflects the
relative simplicity of the task. With respect to
the baseline result, the machine-learning approach
achieves a 21% relative reduction in error.
While this experiment is limited in both scope
and scale, it suggests that the event extraction ap-
proach can be beneficially applied to detect do-
main events represented by novel argument struc-
tures. As a demonstration of feasibility the result
is encouraging for both the applicability of event
extraction to this specific new domain and for the
adaptability of the approach to new domains in
general.
5 Discussion and Conclusions
We have presented a study of the adaptation of an
event extraction approach to the T4SS subdomain
as a step toward the introduction of event extrac-
tion to the broader infectious diseases domain. We
applied a previously introduced corpus of subdo-
main full texts annotated for mentions of bacte-
ria and terms from the three top-level Gene On-
tology subontologies as a reference defining do-
main information needs to study how these can
be met through the application of events defined
in the BioNLP?09 Shared Task on event extrac-
tion. Analysis indicated that with minor revision
of the arguments, the Binding and Localization
event types could account for the majority of both
biological processes and molecular functions of
interest. We further identified a category of ?high-
level? biological processes such as the virulence
process typical of the subdomain, which necessi-
tated extension of the considered event extraction
model.
Based on argument analysis, we proposed a rep-
resentation for high-level processes in the event
model that substitutes Cause for Theme as the
core argument. We further produced annotation
allowing an experiment on the extraction of the
dominant category of virulence processes with
gene/gene product (GGP) causes, annotating 518
GGP mentions and 100 associations between these
and the processes. Experiments indicated that with
annotated in-domain resources both the GGP enti-
ties and their associations with processes could be
extracted with high reliability.
In future work we will extend the model and
annotation proposed in this paper to the broader
infectious diseases domain, introducing annotated
resources and extraction methods for advanced in-
formation access. All annotated resources intro-
duced in this study are available from the GENIA
project homepage.8
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan),
the National Institutes of Health, grant number
HHSN272200900040C, and the Joint Information
Systems Committee (JISC, UK).
References
Sophia Ananiadou, Sampo Pyysalo, Junichi Tsujii, and
Douglas B. Kell. 2010a. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology. (to appear).
8http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/
139
Sophia Ananiadou, Dan Sullivan, Gina-Anne Levow,
Joseph Gillespie, Chunhong Mao, Sampo Pyysalo,
Jun?ichi Tsujii, and Bruno Sobral. 2010b. Named
entity recognition for bacterial type IV secretion sys-
tems. (manuscript in review).
M Ashburner, CA Ball, JA Blake, D Botstein, H But-
ler, JM Cherry, AP Davis, K Dolinski, SS Dwight,
JT Eppig, MA Harris, DP Hill, L Issel-Tarver,
A Kasarskis, S Lewis, JC Matese, JE Richardson,
M Ringwald, GM Rubin, and G Sherlock. 2000.
Gene ontology: tool for the unification of biology.
Nature genetics, 25:25?29.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting complex biological events with rich graph-
based feature sets. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 10?18, Boulder, Colorado, June. Association
for Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Hong-Woo Chun, Yoshimasa Tsuruoka, Jin-Dong
Kim, Rie Shiba, Naoki Nagata, Teruyoshi Hishiki,
and Jun?ichi Tsujii. 2006. Extraction of gene-
disease relations from medline using domain dic-
tionaries and machine learning. In Proceedings of
the Pacific Symposium on Biocomputing (PSB?06),
pages 4?15.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC?06),
pages 449?454.
Mario Juhas, Derrick W. Crook, and Derek W. Hood.
2008. Type IV secretion systems: tools of bacterial
horizontal gene transfer and virulence. Cellular mi-
crobiology, 10(12):2377?2386.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008. Corpus annotation for mining biomedical
events from lterature. BMC Bioinformatics, 9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing
in Biomedicine (BioNLP) NAACL 2009 Workshop,
pages 1?9.
Martin Krallinger, Florian Leitner, and Alfonso Valen-
cia. 2007. Assessment of the Second BioCreative
PPI task: Automatic Extraction of Protein-Protein
Interactions. In L. Hirschman, M. Krallinger, and
A. Valencia, editors, Proceedings of Second BioCre-
ative Challenge Evaluation Workshop, pages 29?39.
John D. Lafferty, Andrew McCallum, and Fernando
C . N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In ICML ?01: Proceedings of the
18th International Conference on Machine Learn-
ing, pages 282?289.
R. Leaman and G. Gonzalez. 2008. Banner: an ex-
ecutable survey of advances in biomedical named
entity recognition. Pacific Symposium on Biocom-
puting, pages 652?663.
David McClosky and Eugene Charniak. 2008. Self-
Training for Biomedical Parsing. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (ACL-HLT?08), pages 101?104.
Claire Ne?dellec. 2005. Learning Language in
Logic - Genic Interaction Extraction Challenge. In
J. Cussens and C. Ne?dellec, editors, Proceedings
of the 4th Learning Language in Logic Workshop
(LLL05), pages 31?37.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, and
Jun?ichi Tsujii. 2009. Incorporating GENETAG-
style annotation to GENIA corpus. In Proceedings
of Natural Language Processing in Biomedicine
(BioNLP) NAACL 2009 Workshop, pages 106?107,
Boulder, Colorado. Association for Computational
Linguistics.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for infor-
mation extraction in the biomedical domain. BMC
Bioinformatics, 8(50).
Andrey Rzhetsky, Ivan Iossifov, Tomohiro Koike,
Michael Krauthammer, Pauline Kra, Mitzi Mor-
ris, Hong Yu, Pablo Ariel Duboue?, Wubin Weng,
W. John Wilbur, Vasileios Hatzivassiloglou, and
Carol Friedman. 2004. GeneWays: A system for
extracting, analyzing, visualizing, and integrating
molecular pathway data. Journal of Biomedical In-
formatics, 37(1):43?53.
Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne
Matten, and John Wilbur. 2005. Genetag: a tagged
corpus for gene/protein named entity recognition.
BMC Bioinformatics, 6(Suppl 1):S3.
Paul Thompson, Syed Iqbal, John McNaught, and
Sophia Ananiadou. 2009. Construction of an anno-
tated corpus to support biomedical information ex-
traction. BMC Bioinformatics, 10(1):349.
Yue Wang, Jin-Dong Kim, Rune Saetre, Sampo
Pyysalo, and Jun?ichi Tsujii. 2009. Investigat-
ing heterogeneous protein annotations toward cross-
corpora utilization. BMC Bioinformatics, 10(1):403.
John Wilbur, Lawrence Smith, and Lorraine Tanabe.
2007. BioCreative 2. Gene Mention Task. In
L. Hirschman, M. Krallinger, and A. Valencia, ed-
itors, Proceedings of Second BioCreative Challenge
Evaluation Workshop, pages 7?16.
140
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 65?73,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Automatic Acquisition of Huge Training Data
for Bio-Medical Named Entity Recognition
Yu Usami? ? Han-Cheol Cho? Naoaki Okazaki? and Jun?ichi Tsujii?
?Aizawa Laboratory, Department of Computer Science, The University of Tokyo, Tokyo, Japan
? Tsujii Laboratory, Department of Computer Science, The University of Tokyo, Tokyo, Japan
? Inui Laboratory, Department of System Information Sciences, Tohoku University, Sendai, Japan
? Microsoft Research Asia, Beijing, China
{yusmi, hccho}@is.s.u-tokyo.ac.jp
okazaki@ecei.tohoku.ac.jp
jtsujii@microsoft.com
Abstract
Named Entity Recognition (NER) is an im-
portant first step for BioNLP tasks, e.g., gene
normalization and event extraction. Employ-
ing supervised machine learning techniques
for achieving high performance recent NER
systems require a manually annotated corpus
in which every mention of the desired seman-
tic types in a text is annotated. However, great
amounts of human effort is necessary to build
and maintain an annotated corpus. This study
explores a method to build a high-performance
NER without a manually annotated corpus,
but using a comprehensible lexical database
that stores numerous expressions of seman-
tic types and with huge amount of unanno-
tated texts. We underscore the effectiveness of
our approach by comparing the performance
of NERs trained on an automatically acquired
training data and on a manually annotated cor-
pus.
1 Introduction
Named Entity Recognition (NER) is the task widely
used to detect various semantic classes such as
genes (Yeh et al, 2005), proteins (Tanabe and
Wilbur, 2002), and diseases in the biomedical field.
A na??ve approach to NER handles the task as a
dictionary-matching problem: Prepare a dictionary
(gazetteer) containing textual expressions of named
entities of specific semantic types. Scan an input
text, and recognize a text span as a named entity if
the dictionary includes the expression of the span.
Although this approach seemingly works well, it
presents some critical issues. First, the dictionary
must be comprehensive so that every NE mention
can be found in the dictionary. This requirement
for dictionaries is stringent because new terminol-
ogy is being produced continuously, especially in
the biomedical field. Second, this approach might
suffer from an ambiguity problem in which a dic-
tionary includes an expression as entries for multi-
ple semantic types. For this reason, we must use
the context information of an expression to make
sure that the expression stands for the target seman-
tic type.
Nadeau and Sekine (2007) reported that a strong
trend exists recently in applying machine learning
(ML) techniques such as Support Vector Machine
(SVM) (Kazama et al, 2002; Isozaki and Kazawa,
2002) and Conditional Random Field (CRF) (Set-
tles, 2004) to NER, which can address these issues.
In this approach, NER is formalized as a classifi-
cation problem in which a given expression is clas-
sified into a semantic class or other (non-NE) ex-
pressions. Because the classification problem is usu-
ally modeled using supervised learning methods, we
need a manually annotated corpus for training NER
classifier. However, preparing manually annotated
corpus for a target domain of text and semantic types
is cost-intensive and time-consuming because hu-
man experts are needed to reliably annotate NEs in
text. For this reason, manually annotated corpora
for NER are often limited to a specific domain and
covers a small amount of text.
In this paper we propose a novel method for au-
tomatically acquiring training data for NER from a
comprehensible lexical database and huge amounts
of unlabeled text. This paper presents four contribu-
65
Gene or Protein name
Official name
Aliases
References
Figure 1: Example of an Entrez Gene record.
tions:
1. We show the ineffectiveness of a na??ve
dictionary-matching for acquiring a training
data automatically and the significance of the
quality of training data for supervised NERs
2. We explore the use of reference information
that bridges the lexical database and unlabeled
text for acquiring high-precision and low-recall
training data
3. We develop two strategies for expanding NE
annotations, which improves the recall of the
training data
4. The proposed method acquires a large amount
of high-quality training data rapidly, decreasing
the necessity of human efforts
2 Proposed method
The proposed method requires two resources to ac-
quire training data automatically: a comprehen-
sive lexical database and unlabeled texts for a tar-
get domain. We chose Entrez Gene (National Li-
brary of Medicine, 2005) as the lexical database be-
cause it provides rich information for lexical entries
and because genes and proteins constitute an im-
portant semantic classes for Bio NLP. Entrez Gene
consists of more than six million gene or protein
records, each of which has various information such
as the official gene (protein) name, synonyms, or-
ganism, description, and human created references.
Figure 1 presents an example of an Entrez Gene
record. We created a dictionary by collecting offi-
cial gene (protein) names and their synonyms from
the Entrez Gene records. For unlabeled text, we use
the all 2009 release MEDLINE (National Library
of Medicine, 2009) data. MEDLINE consists of
about ten million abstracts covering various fields of
biomedicine and health. In our study, we focused on
recognizing gene and protein names within biomed-
ical text.
Our process to construct a NER classifier is as fol-
lows: We apply the GENIA tagger (Tsuruoka et al,
2005) to split the training data into tokens and to at-
tach part of speech (POS) tags and chunk tags. In
this work, tokenization is performed by an external
program that separates tokens by a space, hyphen,
comma, period, semicolon, or colon character. Part
of speech tags present grammatical roles of tokens,
e.g. verbs, nouns, and prepositions. Chunk tags
compose tokens into syntactically correlated seg-
ments, e.g. verb phrases, noun phrases, and preposi-
tional phrases. We use the IOBES notation (Ratinov
and Roth, 2009) to represent NE mentions with label
sequences, thereby NER is formalized as a multi-
class classification problem in which a given token
is classified into IOBES labels. To classify labels of
tokens, we use a linear kernel SVM which applies
the one-vs.-the-rest method (Weston and Watkins,
1999) to extend binary classification to multi-class
classification. Given the t-th token xt in a sentence,
we predict the label yt,
yt = argmax
y
s(y|xt, yt?1).
In this equation, s(y|xt, yt?1) presents the score
(sum of feature weights) when the token xt is la-
beled y. We use yt?1 (the label of the previous to-
ken) to predict yt, expecting that this feature behaves
as a label bigram feature (also called translation fea-
ture) in CRF. If the sentence consists of x1 to xT , we
repeat prediction of labels sequentially from the be-
ginning (y1) to the end (yT ) of a sentence. We used
LIBLINEAR (Fan et al, 2008) as an SVM imple-
mentation.
Table 1 lists the features used in the classifier
modeled by SVM. For each token (?Human? in the
example of Table 1), we created several features in-
cluding: token itself (w), lowercase token (wl), part
of speech (pos), chunk tag (chk), character pattern of
66
Name Description Example Value
w token Human
wl token in small letters human
pos part of speech NNP
chk chunk tag B-NP
shape entity pattern ULLLL
shaped entity pattern 2 UL
type token type InitCap
pn(n = 1...4) prefix n characters (H,Hu,Hum,Huma)
sn(n = 1...4) suffix n characters (n,an,man,uman)
Table 1: Example of features used in machine learning
process.
token (shape), character pattern designated (shaped),
token type (type), prefixes of length n (pn), and suf-
fixes of length n (sn). More precisely, the character
pattern of token (shape) replaces each character in
the token with either an uppercase letter (U), a low-
ercase letter (L), or a digit (D). The character pat-
tern designated (shaped) is similar to a shape feature,
but the consecutive character types are reduced to
one symbol, for example, ?ULLLL? (shape) is rep-
resented with ?UL? (shaped) in the example of Ta-
ble 1). The token type (type) represents whether the
token satisfies some conditions such as ?begins with
a capital letter?, ?written in all capitals?, ?written
only with digits?, or ?contains symbols?. We created
unigram features and bigram features (excluding wl,
pn, sn) from the prior 2 to the subsequent 2 tokens
of the current position.
2.1 Preliminary Experiment
As a preliminary experiment, we acquired training
data using a na??ve dictionary-matching approach.
We obtained the training data from all 2009 MED-
LINE abstracts with an all gene and protein dictio-
nary in Entrez Gene. The training data consisted of
nine hundred million tokens. We constructed a NER
classifier using only four million tokens of the train-
ing data because of memory limitations. For evalua-
tion, we used the Epigenetics and Post-translational
Modification (EPI) corpus BioNLP 2011 Shared
Task (SIGBioMed, 2011). Only development data
and training data are released as the EPI corpus at
present, we used both of the data sets for evalua-
tion in this experiment. Named entities in the corpus
are annotated exhaustively and belong to a single se-
mantic class, Gene or Gene Product (GGP) (Ohta
et al, 2009). We evaluated the performance of the
Method A P R F1
dictionary matching 92.09 39.03 42.69 40.78
trained on acquired data 85.76 10.18 23.83 14.27
Table 2: Results of the preliminary experiment.
(a) It is clear that in culture media of AM,
cystatin C and cathepsin B are present as
proteinase?antiproteinase complexes.
(b) Temperature in the puerperium is higher
in AM, and lower in PM.
Figure 2: Dictionary-based gene name annotating exam-
ple (annotated words are shown in italic typeface).
NER on four measures: Accuracy (a), Precision (P),
Recall (R), and F1-measure (F1). We used the strict
matching criterion that a predicted named entity is
correct if and only if the left and the right bound-
aries are both correct.
Table 2 presents the evaluation results of this ex-
periment. The first model ?dictionary matching?
performs exact dictionary-matching on the test cor-
pus. It achieves a 40.78 F1-score. The second model
?trained on acquired data? uses the training data
acquired automatically for constructing NER clas-
sifier. It scores very low-performance (14.27 F1-
score), even compared with the simple dictionary-
matching NER. Exploring the annotated training
data, we investigate why this machine learning ap-
proach shows extremely low performance.
Figure 2 presents an example of the acquired
training data. The word ?AM? in the example (a)
is correct because it is gene name, although ?AM?
in the example (b) is incorrect because ?AM? in (b)
is the abbreviation of ante meridiem, which means
before noon. This is a very common problem, espe-
cially with abbreviations and acronyms. If we use
this noisy training data for learning, then the result
of NER might be low because of such ambiguity. It
is very difficult to resolve errors in the training data
even with the help of machine learning methods.
2.2 Using Reference Information
To obtain high-precision data, we used reference in-
formation included with each record in Entrez Gene.
Figure 3 portrays a simple example of reference in-
formation. It shows the reference information of the
67
 PMID 1984484: 
 It is clear that in culture media of AM, 
cystatin C and cathepsin B are present as 
proteinase-antiproteinase complexes.
Gene: AM
Entrez Gene Records
MEDLINE Abstracts
 PMID 23456:
 Temperature in puerperium is higher in AM, 
lower in PM.
Reference
Figure 3: Reference to MEDLINE abstract example.
Entrez Gene record which describes that the gene
?AM?. The reference information indicates PMIDs
in which the gene or protein is described.
We applied the rule whereby we annotated a
dictionary-matching in each MEDLINE abstract
only if they were referred by the Entrez Gene
records. Figure 3 shows that the gene ?AM? has
reference to the MEDLINE abstract #1984484 only.
Using this reference information between the En-
trez Gene record ?AM? and the MEDLINE abstract
#1984484, we can annotate the expansion ?AM? in
MEDLINE abstract #1984484 only. In this way, we
can avoid incorrect annotation such as example b in
Figure 2.
We acquired training data automatically using ref-
erence information, as follows:
1. Construct a gene and protein dictionary includ-
ing official names, synonyms and reference in-
formation in Entrez Gene
2. Apply a dictionary-matching on the all MED-
LINE abstracts with the dictionary
3. Annotate the MEDLINE abstract only if it was
referred by the Entrez Gene records which de-
scribe the matched expressions
We obtained about 48,000,000 tokens of training
data automatically by using this process using all the
2009 MEDLINE data. This training data includes
about 3,000,000 gene mentions.
? ... in the following order: tna, gltC, gltS,
pyrE; gltR is located near ...
? The three genes concerned (designated
entA, entB and entC) ...
? Within the hypoglossal nucleus large
amounts of acetylcholinesterase (AChE)
activity are ...
Figure 4: False negative examples.
2.3 Training Data Expansion
In the previous section, we were able to obtain train-
ing data with high-precision by exploiting reference
information in the Entrez Gene. However, the result-
ing data include many false negatives (low-recall),
meaning that correct gene names in the data are
unannotated. Figure 4 presents an example of miss-
ing annotation. In this figure, all gene mentions
are shown in italic typeface. The underlined en-
tities were annotated by using the method in Sec-
tion 2.2, because they were in the Entrez Gene dic-
tionary and this MEDLINE abstract was referred by
these entities. However, the entities in italic type-
face with no underline were not annotated, because
these gene names in Entrez Gene have no link to
this MEDLINE abstract. Those expressions became
false negatives and became noise for learning. This
low-recall problem occurred because no guarantee
exists of exhaustiveness in Entrez Gene reference in-
formation.
To improve the low-recall while maintaining
high-precision, we focused on coordination struc-
tures. We assumed that coordinated noun phrases
belong to the same semantic class. Figure 5 portrays
the algorithm for the annotation expansion based
on coordination analysis. We expanded training
data annotation using this coordination analysis al-
gorithm to improve annotation recall. This algo-
rithm analyzes whether the words are reachable or
not through coordinate tokens such as ?,?, ?.?, or
?and? from initially annotated entities. If the words
are reachable and their entities are in the Entrez
Gene records (ignoring reference information), then
they are annotated.
68
Input: Sequence of sentence tokens S, Set of
symbols and conjunctions C, Dictionary with-
out reference D, Set of annotated tokens A
Output: Set of Annotated tokens A
begin
for i = 1 to |S| do
if S[i] ? A then
j ? i? 2
while 1 ? j ? |S| ? S[j] ? D ? S[j] /?
A ? S[j + 1] ? C do
A? A ? {S[j]}
j ? j ? 2
end while
j ? i + 2
while 1 ? j ? |S| ? S[j] ? D ? S[j] /?
A ? S[j ? 1] ? C do
A? A ? {S[j]}
j ? j + 2
end while
end if
end for
Output A
end
Figure 5: Coordination analysis algorithm.
2.4 Self-training
The method described in Section 2.3 reduces false
negatives based on coordination structures. How-
ever, the training data contain numerous false neg-
atives that cannot be solved through coordination
analysis. Therefore, we used a self-training algo-
rithm to automatically correct the training data. In
general, a self-training algorithm obtains training
data with a small amount of annotated data (seed)
and a vast amount of unlabeled text, iterating this
process (Zadeh Kaljahi, 2010):
1. Construct a classification model from a seed,
then apply the model on the unlabeled text.
2. Annotate recognized expressions as NEs.
3. Add the sentences which contain newly anno-
tated expressions to the seed.
In this way, a self-training algorithm obtains a huge
amount of training data.
Input: Labeled training data D, Machine
learning algorithm A, Iteration times n,
Threshold ?
Output: Training data Tn
begin
T0 ? A seed data from D
i? 0
D ? D\T0
while i 6= n do
Mi ? Construct model with Ti
U ? Sample some amount of data from D
L? Annotate U with model Mi
Unew ?Merge U with L if their confidence
values are larger than ?
Ti+1 ? Ti ? Unew
D ? D\U
i? i + 1
end while
Output Tn
end
Figure 6: Self-training algorithm.
In contrast, our case is that we have a large
amount of training data with numerous false neg-
atives. Therefore, we adapt a self-training algo-
rithm to revise the training data obtained using the
method described in Section 2.3. Figure 6 shows
the algorithm. We split the data set (D) obtained in
Section 2.3 into a seed set (T0) and remaining set
(D\T0). Then, we iterate the cycle (0 ? i ? n):
1. Construct a classification model (Mi) trained
on the training data (Ti).
2. Sample some amount of data (U ) from the re-
maining set (D).
3. Apply the model (Mi) on the sampled data (U ).
4. Annotate entities (L) recognized by this model.
5. Merge newly annotated expressions (L) with
expressions annotated in Section 2.3 (U ) if
their confidence values are larger than a thresh-
old (?).
6. Add the merged data (Unew) to the training data
(Ti).
69
In this study, we prepared seed data of 683,000 to-
kens (T0 in Figure 6). In each step, 227,000 tokens
were sampled from the remaining set (U ).
Because the remaining set U has high precision
and low recall, we need not revise NEs that were
annotated in Section 2.3. It might lower the qual-
ity of the training data to merge annotated entities,
thus we used confidence values (Huang and Riloff,
2010) to revise annotations. Therefore, we retain the
NE annotations of the remaining setU and overwrite
a span of a non-NE annotation only if the current
model predicts the span as an NE with high confi-
dence. We compute the confidence of the prediction
(f(x)) which a token x is predicted as label y as,
f(x) = s(x, y)?max(?z 6=ys(x, z)).
Here, s(x, y) denotes the score (the sum of feature
weights) computed using the SVM model described
in the beginning of Section 2. A confidence score
presents the difference of scores between the pre-
dicted (the best) label and the second-best label. The
confidence value is computed for each token label
prediction. If the confidence value is greater than
a threshold (?) and predicted as an NE of length 1
token (label S in IOBES notation), then we revise
the NE annotation. When a new NE with multiple
tokens (label B, I, or E in IOBES notation) is pre-
dicted, we revise the NE annotation if the average
of confidence values is larger than a threshold (?).
If a prediction suggests a new entity with multiple
tokens xi, ..., xj , then we calculate the average of
confidence values as
f(xi, ..., xj) =
1
j ? i + 1
j
?
k=i
f(xk).
The feature set presented in the beginning of Sec-
tion 2 uses information of the tokens themselves.
These features might overfit the noisy seed set, even
if we use regularization in training. Therefore, when
we use the algorithm of Figure 6, we do not gen-
erate token (w) features from tokens themselves but
only from tokens surrounding the current token. In
other words, we hide information from the tokens of
an entity, and learn models using information from
surrounding words.
Method A P R F1
dictionary matching 92.09 39.03 42.69 40.78
svm 85.76 10.18 23.83 14.27
+ reference 93.74 69.25 39.12 50.00
+ coordination 93.97 66.79 47.44 55.47
+ self-training 93.98 63.72 51.18 56.77
Table 3: Evaluation results.
3 Experiment
The training data automatically generated using the
proposed method have about 48,000,000 tokens and
3,000,000 gene mentions. However, we used only
about 10% of this data because of the computational
cost. For evaluation, we chose to use the BioNLP
2011 Shared Task EPI corpus and evaluation mea-
sures described in Section 2.1.
3.1 Evaluation of Proposed Methods
In the previous section, we proposed three methods
for automatic training data acquisition. We first in-
vestigate the effect of these methods on the perfor-
mance of NER. Table 3 presents evaluation results.
The first method ?dictionary matching? simply
performs exact string matching with the Entrez Gene
dictionary on the evaluation corpus. It achieves a
40.78 F1-measure; this F1-measure will be used as
the baseline performance. The second method, as
described in Section 2.1, ?svm? uses training data
generated automatically from the Entrez Gene and
unlabeled texts without reference information of the
Entrez Gene. The third method, ?+ reference? ex-
ploits the reference information of the Entrez Gene.
This method drastically improves the performance.
As shown in Table 3, this model achieves the highest
precision (69.25%) with comparable recall (39.12%)
to the baseline model with a 50.00 F1-measure. The
fourth method, ?+ coordination?, uses coordination
analysis results to expand the initial automatic an-
notation. Compared to the ?+ reference? model, the
annotation expansion based on coordination analy-
sis greatly improves the recall (+8.32%) with only
a slight decrease of the precision (-2.46%). The
last method ?+ self-training? applies a self-training
technique to improve the performance further. This
model achieves the highest recall (51.18%) among
all models with a reasonable cost in the precision.
70
Figure 7: Results of self-training.
To analyze the effect of self-training, we evalu-
ated the performance of this model for each itera-
tion. Figure 7 shows the F1-measure of the model
as iterations increase. The performance improved
gradually. It did not converge even for the last iter-
ation. The size of the training data at the 17th itera-
tion was used in Table 3 experiment. It is the same
to the size of the training data for other methods.
3.2 Comparison with a Manually Annotated
Corpus
NER systems achieving state-of-the-art performance
are based mostly on supervised machine learn-
ing trained on manually annotated corpus. In
this section, we present a comparison of our best-
performing NER model with a NER model trained
on manually annotated corpus. In addition to the
performance comparison, we investigate how much
manually annotated data is necessary to outperform
our best-performing system. In this experiment, we
used only the development data for evaluation be-
cause the training data are used for training the NER
model.
We split the training data of EPI corpus randomly
into 20 pieces and evaluated the performance of
the conventional NER system as the size of manu-
ally annotated corpus increases. Figure 8 presents
the evaluation results. The performance of our our
best-performing NER is a 62.66 F1-measure; this
is shown as horizontal line in Figure 8. The NER
model trained on the all training data of EPI cor-
Figure 8: Manual annotation vs. our method.
pus achieves a 67.89 F1-measure. The result shows
that our best-performing models achieve compara-
ble performance to that of the NER model when us-
ing about 40% (60,000 tokens, 2,000 sentences) of
the manually annotated corpus.
3.3 Discussion
Although the proposed methods help us to obtain
training data automatically with reasonably high
quality, we found some shortcomings in these meth-
ods. For example, the annotation expansion method
based on coordination analysis might find new enti-
ties in the training data precisely. However, it was
insufficient in the following case.
tna loci, in the following order: tna, gltC,
gltS, pyrE; gltR is located near ...
In this example, all gene mentions are shown in
italic typeface. The words with underline were ini-
tial annotation with reference information. The sur-
rounding words represented in italic typeface are an-
notated by annotation expansion with coordination
analysis. Here, the first word ?tna? shown in italic
typeface in this example is not annotated, although
its second mention is annotated at the annotation ex-
pansion step. We might apply the one sense per dis-
course (Gale et al, 1992) heuristic to label this case.
Second, the improvement of self-training tech-
niques elicited less than a 1.0 F1-measure. To as-
certain the reason for this small improvement, we
analyzed the distribution of entity length both origi-
71
Original
Added
0% 25% 50% 75% 100%
Length 1 Length 2 Length 3 More than 4
Figure 9: Distribution of entity length.
nally included entities and newly added entities dur-
ing self-training, as shown in Figure 9. They repre-
sent the ratio of entity length to the number of total
entities. Figure 9 shows the added distribution of
entity length (Added) differs from the original one
(Original). Results of this analysis show that self-
training mainly annotates entities of the length one
and barely recognizes entities of the length two or
more. It might be necessary to devise a means to fol-
low the corpus statistics of the ratio among the num-
ber of entities of different length as the self-training
iteration proceeds.
4 Related Work
Our study focuses mainly on achieving high per-
formance NER without manual annotation. Several
previous studies aimed at reducing the cost of man-
ual annotations.
Vlachos and Gasperin (2006) obtained noisy
training data from FlyBase1 with few manually an-
notated abstracts from FlyBase. This study sug-
gested the possibility of acquiring high-quality train-
ing data from noisy training data. It used a boot-
strapping method and a highly context-based classi-
fiers to increase the number of NE mentions in the
training data. Even though the method achieved a
high-performance NER in the biomedical domain, it
requires curated seed data.
Whitelaw et al (2008) attempted to create ex-
tremely huge training data from the Web using a
seed set of entities and relations. In generating train-
ing data automatically, this study used context-based
tagging. They reported that quite a few good re-
sources (e.g., Wikipedia2) listed entities for obtain-
ing training data automatically.
1http://flybase.org/
2http://www.wikipedia.org/
Muramoto et al (2010) attempted to create train-
ing data from Wikipedia as a lexical database and
blogs as unlabeled text. It collected about one mil-
lion entities from these sources, but they did not re-
port the performance of the NER in their paper.
5 Conclusions
This paper described an approach to the acquisi-
tion of huge amounts of training data for high-
performance Bio NER automatically from a lexical
database and unlabeled text. The results demon-
strated that the proposed method outperformed
dictionary-based NER. Utilization of reference in-
formation greatly improved its precision. Using co-
ordination analysis to expand annotation increased
recall with slightly decreased precision. Moreover,
self-training techniques raised recall. All strategies
presented in the paper contributed greatly to the
NER performance.
We showed that the self-training algorithm
skewed the length distribution of NEs. We plan
to improve the criteria for adding NEs during self-
training. Although we obtained a huge amount of
training data by using the proposed method, we
could not utilize all of acquired training data be-
cause they did not fit into the main memory. A fu-
ture direction for avoiding this limitation is to em-
ploy an online learning algorithm (Tong and Koller,
2002; Langford et al, 2009), where updates of fea-
ture weights are done for each training instance. The
necessity of coordination handling and self-training
originates from the insufficiency of reference infor-
mation in the lexical database, which was not de-
signed to be comprehensive. Therefore, establish-
ing missing reference information from a lexical
database to unlabeled texts may provide another so-
lution for improving the recall of the training data.
References
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Li-
brary for Large Linear Classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural Lan-
guage, pages 233?237.
72
Ruihong Huang and Ellen Riloff. 2010. Inducing
domain-specific semantic class taggers from (almost)
nothing. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
275?285.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient sup-
port vector classifiers for named entity recognition. In
Proceedings of the 19th international conference on
Computational linguistics - Volume 1, pages 1?7.
Jun?ichi Kazama, Takaki Makino, Yoshihiro Ohta, and
Jun?ichi Tsujii. 2002. Tuning support vector ma-
chines for biomedical named entity recognition. In
Proceedings of the ACL-02 workshop on Natural lan-
guage processing in the biomedical domain - Volume
3, pages 1?8.
John Langford, Lihong Li, and Tong Zhang. 2009.
Sparse online learning via truncated gradient. J. Mach.
Learn. Res., 10:777?801.
Hideki Muramoto, Nobuhiro Kaji, Naoki Suenaga, and
Masaru Kitsuregawa. 2010. Learning semantic cat-
egory tagger from unlabeled data. In The Fifth NLP
Symposium for Yung Researchers. (in Japanese).
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Lingvisti-
cae Investigationes, 30(1):3?26.
National Library of Medicine. 2005. Entrez Gene. avail-
able at http://www.ncbi.nlm.nih.gov/gene.
National Library of Medicine. 2009. MEDLINE. avail-
able at http://www.ncbi.nlm.nih.gov/.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, Yue
Wang, and Jun?ichi Tsujii. 2009. Incorporating
genetag-style annotation to genia corpus. In Proceed-
ings of the Workshop on Current Trends in Biomedical
Natural Language Processing, pages 106?107.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning, pages 147?155.
Burr Settles. 2004. Biomedical named entity recognition
using conditional random fields and rich feature sets.
In Proceedings of the International Joint Workshop on
Natural Language Processing in Biomedicine and its
Applications, pages 104?107.
SIGBioMed. 2011. BioNLP 2011 Shared Task.
http://sites.google.com/site/bionlpst/.
Lorraine K. Tanabe and W. John Wilbur. 2002. Tagging
gene and protein names in biomedical text. Bioin-
formatics/computer Applications in The Biosciences,
18:1124?1132.
Simon Tong and Daphne Koller. 2002. Support vector
machine active learning with applications to text clas-
sification. J. Mach. Learn. Res., 2:45?66.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun ?ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances in
Informatics, volume 3746, pages 382?392.
Andreas Vlachos and Caroline Gasperin. 2006. Boot-
strapping and evaluating named entity recognition in
the biomedical domain. In Proceedings of the HLT-
NAACL BioNLP Workshop on Linking Natural Lan-
guage and Biology, pages 138?145.
Jason Weston and Chris Watkins. 1999. Support vec-
tor machines for multi-class pattern recognition. In
ESANN?99, pages 219?224.
Casey Whitelaw, Alex Kehlenbeck, Nemanja Petrovic,
and Lyle Ungar. 2008. Web-scale named entity recog-
nition. In Proceeding of the 17th ACM conference on
Information and knowledge management, pages 123?
132.
Alexander Yeh, Alexander Morgan, Marc Colosimo, and
Lynette Hirschman. 2005. Biocreative task 1a: gene
mention finding evaluation. BMC Bioinformatics,
6(1):S2.
Rasoul Samad Zadeh Kaljahi. 2010. Adapting self-
training for semantic role labeling. In Proceedings of
the ACL 2010 Student Research Workshop, pages 91?
96.
73
