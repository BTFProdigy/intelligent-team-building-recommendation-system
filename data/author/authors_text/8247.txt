Categorizing Unknown Words: Using Decision Trees to Identify 
Names and Misspellings 
J an ine  Too le  
Natura l  Language Laboratory  
Depar tment  of Comput ing  Science 
S imon Fraser Univers i ty 
Burnaby,  BC,  Canada  VSA IS6 
toole@cs.sfu.ca 
Abst rac t  
This paper introduces a system for categorizing un- 
known words. The system is based on a multi- 
component architecture where each component is re- 
sponsible for identifying one class of unknown words. 
The focus of this paper is the components hat iden- 
tify names and spelling errors. Each component 
uses a decision tree architecture to combine multiple 
types of evidence about the unknown word. The sys- 
tem is evaluated using data from live closed captions 
- a genre replete with a wide variety of unknown 
words. 
1 In t roduct ion  
In any real world use, a Natural Language Process- 
ing (NLP) system will encounter words that are not 
in its lexicon, what we term 'unknown words'. Un- 
known words are problematic because a NLP system 
will perform well only if it recognizes the words that 
it is meant o analyze or translate: the more words a 
system does not recognize the more the system's per- 
formance will degrade. Even when unknown words 
are infrequent, they can have a disproportionate ef-
fect on system quality. For example, Min (1996) 
found that while only 0.6% of words in 300 e-mails 
were misspelled, this meant that 12% of the sen- 
tences contained an error (discussed in (Min and 
Wilson, 1998)). 
Words may be unknown for many reasons: the 
word may be a proper name, a misspelling, an ab- 
breviation, a number, a morphological variant of a 
known word (e.g. recleared), or missing from the 
dictionary. The first step in dealing with unknown 
words is to identify the class of the unknown word; 
whether it is a misspelling, a proper name, an ab- 
breviation etc. Once this is known, the proper ac- 
tion can be taken, misspellings can be corrected, ab- 
breviations can be expanded and so on, as deemed 
necessary by the particular text processing applica- 
tion. In this paper we introduce a system for cat- 
egorizing unknown words. The system is based on 
a multi- component architecture where each compo- 
nent is responsible for identifying one category of 
unknown words. The main focus of this paper is the 
components hat identify names and spelling errors. 
Both components use a decision tree architecture to 
combine multiple types of evidence about the un- 
known word. Results from the two components are 
combined using a weighted voting procedure. The 
system is evaluated using data from live closed cap- 
tions - a genre replete with a wide variety of un- 
known words. 
This paper is organized as follows. In section 2 
we outline the overall architecture of the unknown 
word categorizer. The name identifier and the mis- 
spelling identifier are introduced in section 3. Perfor- 
mance and evaluation issues are discussed in section 
4. Section 5 considers portability issues. Section 6 
compares the current system with relevant preced- 
ing research. Concluding comments can be found in 
section 6. 
2 Sys tem Arch i tec ture  
The goal of our research is to develop a system that 
automatically categorizes unknown words. Accord- 
ing to our definition, an unknown word is a word 
that is not contained in the lexicon of an NLP sys- 
tem. As defined, 'unknown-ness' i  a relative con- 
cept: a word that is known to one system may be 
unknown to another system. 
Our research is motivated by the problems that 
we have experienced in translating live closed cap- 
tions: live captions are produced under tight time 
constraints and contain many unknown words. Typ- 
ically, the caption transcriber has a five second win- 
dow to transcribe the broadcast dialogue. Because 
of the live nature of the broadcast, there is no op- 
portunity to post-edit he transcript in any way. Al- 
though motivated by our specific requirements, the 
unknown word categorizer would benefit any NLP 
system that encounters unknown words of differ- 
ing categories. Some immediately obvious domains 
where unknown words are frequent include e-mail 
messages, internet chat rooms, data typed in by call 
centre operators, etc. 
To deal with these issues we propose a multi- 
component architecture where individual compo- 
nents specialize in identifying one particular type of 
173 
unknown word. For example, the misspelling iden- 
tifier will specialize in identifying misspellings, the 
abbreviation component will specialize in identify- 
ing abbreviations, etc. Each component will return 
a confidence measure of the reliability of its predic- 
tion, c.f. (Elworthy, 1998). The results from each 
component are evaluated to determine the final cat- 
egory of the word. 
There are several advantages to this approach. 
Firstly, the system can take advantage of existing 
research. For example, the name recognition mod- 
ule can make use of the considerable research that 
exists on name recognition, e.g. (McDonald, 1996), 
(Mani et al, 1996). Secondly, individual compo- 
nents can be replaced when improved models are 
available, without affecting other parts of the sys- 
tem. Thirdly, this approach is compatible with in- 
corporating multiple components ofthe same type to 
improve performance (cf. (van Halteren et al, 1998) 
who found that combining the results of several part 
of speech taggers increased performance). 
3 The  Cur rent  Sys tem 
In this paper we introduce a simplified version 
of the unknown word categorizer: one that con- 
tains just two components: misspelling identifica- 
tion and name identification. In this section we in- 
troduce these components and the 'decision: compo- 
nent which combines the results from the individual 
modules. 
3.1 The Name Identif ier 
The goal of the name identifier is to differentiate be- 
tween those unknown words which are proper names, 
and those which are not. We define a name as word 
identifying a person, place, or concept hat would 
typically require capitalization i English. 
One of the motivations for the modular architec- 
ture introduced above, was to be able to leverage 
existing research. For example, ideally, we should 
be able to plug in an existing proper name recog- 
nizer and avoid the problem of creating our own. 
However, the domain in which we are currently op- 
erating - live closed captions - makes this approach 
difficult. Closed captions do not contain any case 
information, all captions are in upper case. Exist- 
ing proper name recognizers rely heavily on case to 
identify names, hence they perform poorly on our 
data. 
A second isadvantage of currently available name 
recognizers i  that they do not generally return a 
confidence measure with their prediction. Some 
indication of confidence is required in the multi- 
component architecture we have implemented. How- 
ever, while currently existing name recognizers are 
inappropriate for the needs of our domain, future 
name recognizers may well meet these requirements 
and be able to be incorporated into the architecture 
we propose. 
For these reasons we develop our own name iden- 
tifier. We utilize a decision tree to model the charac- 
teristics of proper names. The advantage ofdecision 
trees is that they are highly explainable: one can 
readily understand the features that are affecting 
the analysis (Weiss and Indurkhya, 1998). Further- 
more, decision trees are well-suited for combining a
wide variety of information. 
For this project, we made use of the decision tree 
that is part of IBM's Intelligent Miner suite for data 
mining. Since the point of this paper is to describe 
an application of decision trees rather than to ar- 
gue for a particular decision tree algorithm, we omit 
further details of the decision tree software. Sim- 
ilar results should be obtained by using other de- 
cision tree software. Indeed, the results we obtain 
could perhaps be improved by using more sophisti- 
cated decision-tree approaches such as the adaptive- 
resampling described in (Weiss et al 1999). 
The features that we use to train the decision tree 
are intended to capture the characteristics of names. 
We specify a total of ten features for each unknown 
word. These identify two features of the unknown 
word itself as well as two features for each of the two 
preceding and two following words. 
The first feature represents the part of speech of 
the word. Vv'e use an in-house statistical tagger 
(based on (Church, 1988)) to tag the text in which 
the unknown word occurs. The tag set used is a 
simplified version of the tags used in the machine- 
readable version of the Oxford Advanced Learners 
Dictionary (OALD). The tag set contains just one 
tag to identify nouns. 
The second feature provides more informative tag- 
ging for specific parts of speech (these are referred 
to as 'detailed tags' (DETAG)). This tagset consists 
of the nine tags listed in Table 1. All parts of speech 
apart from noun and punctuation tags are assigned 
the tag 'OTHER;. All punctuation tags are assigned 
the tag 'BOUNDARY'. Words identified as nouns 
are assigned one of the remaining tags depending on 
the information provided in the OALD (although the 
unknown word, by definition, will not appear in the 
OALD, the preceding and following words may well 
appear in the dictionary). If the word is identified in 
the OALD as a common oun it is assigned the tag 
'COM'. If it is identified in the OALD as a proper 
name it is assigned the tag 'NAME'. If the word is 
specified as both a name and a common oun (e.g. 
'bilF), then it is assigned the tag 'NCOM'. Pronouns 
are assigned the tag 'PRON'. If the word is in a list 
of titles that we have compiled, then the tag 'TITLE' 
is assigned. Similarly, if the word is a member of the 
class of words that can follow a name (e.g. 'jr'), then 
the tag 'POST ~ is assigned. A simple rule-based sys- 
174 
COM common oun 
NAME name 
NCOM name and common oun 
PRONOUN pronoun 
TITLE title 
POST post-name word 
BOUNDARY boundary marker 
OTHER not noun or boundary 
UNKNOWN unknown noun 
Table 1: List of Detailed Tags (DETAG) 
Corpus frequency 
Word length 
Edit distance 
Ispell information 
Character sequence frequency 
Non-English characters 
Table 2: Features used in misspelling decision tree 
tern is used to assign these tags. 
If we were dealing with data that contains case 
information, we would also include fields represent- 
ing the existence/non-existence of initial upper case 
for the five words. However, since our current data 
does not include case information we do not include 
these features. 
3.2 The Misspel l ing Identi f ier 
The goal of the misspelling identifier is to differenti- 
ate between those unknown words which are spelling 
errors and those which are not. We define a mis- 
spelling as an unintended, orthographically incorrect 
representation (with respect o the NLP system) of a 
word. A misspelling differs from the intended known 
word through one or more additions, deletions, sub- 
stitutions, or reversals of letters, or the exclusion of 
punctuation such as hyphenation or spacing. Like 
the definition of 'unknown word', the definition of a 
misspelling is also relative to a particular NLP sys- 
tem. 
Like the name identifier, we make use of a decision 
tree to capture the characteristics of misspellings. 
The features we use are derived from previous re- 
search, including our own previous research on mis- 
spelling identification. An abridged list of the fea- 
tures that are used in the training data is listed in 
Table 2 and discussed below. 
Corpus frequency: (Vosse, 1992) differentiates 
between misspellings and neologisms (new words) 
in terms of their frequency. His algorithm classi- 
fies unknown words that appear infrequently as mis- 
spellings, and those that appear more frequently as 
neologisms. Our corpus frequency variable specifies 
the frequency of each unknown word in a 2.6 million 
word corpus of business news closed captions. 
I~'ord Length: (Agirre et al, 1998) note that 
their predictions for the correct spelling of mis- 
spelled words are more accurate for words longer 
than four characters, and much less accurate for 
shorter words. This observation can also be found in 
(Kukich, 1992). Our word length variables measures 
the number of characters in each word. 
Edit distance: Edit-distance isa metric for iden- 
tifying the orthographic similarity of two words. 
Typically, one edit-distance corresponds to one sub- 
stitution, deletion, reversal or addition of a charac- 
ter. (Damerau, 1964) observed that 80% of spelling 
errors in his data were just one edit-distance from 
the intended word. Similarly, (Mitton, 1987) found 
that 70% of his data was within one edit-distance 
from the intended word. Our edit distance feature 
represents the edit distance from the unknown word 
to the closest suggestion produced by the unix spell 
checker, ispell. If ispell does not produce any sugges- 
tions, an edit distance of thirty is assigned. In pre- 
vious work we have experimented with more sophis- 
ticated distance measures. However, simple edit dis- 
tance proved to be the most effective (Toole, 1999). 
Character sequence frequency: A characteris- 
tic of some misspellings i that they contain charac- 
ter sequences which are not typical of the language, 
e.g.tlted, wful. Exploiting this information is a stan- 
dard way of identifying spelling errors when using a 
dictionary is not desired or appropriate, e.g. (Hull 
and Srihari, 1982), (Zamora et al, 1981). 
To calculate our character sequence feature, we 
firstly determine the frequencies of the two least fre- 
quent character tri-gram sequences in the word in 
each of a selection of corpora. In previous work we 
included each of these values as individual features. 
However, the resulting trees were quite unstable as 
one feature would be relevant o one tree, whereas 
a different character sequence feature would be rel- 
evant to another tree. To avoid this problem, we 
developed a composite feature that is the sum of all 
individual character sequence frequencies. 
Non-English characters: This binary feature 
specifies whether a word contains a character that is 
not typical of English words, such as accented char- 
acters, etc. Such characters are indicative of foreign 
names or transmission noise (in the case of captions) 
rather than misspellings. 
3.3 Decision Making Component  
The misspelling identifier and the name identifier 
will each return a prediction for an unknown word. 
In cases where the predictions are compatible, e.g. 
where the name identifier predicts that it is a name 
and the spelling identifier predicts that it is not 
a misspelling, then the decision is straightforward. 
Similarly, if both decision trees make negative pre- 
dictions, then we can assume that the unknown word 
175 
is neither a misspelling nor a name, but some other 
category of unknown word. 
However, it is also possible that both the spelling 
identifier and the name identifier will make positive 
predictions. In these cases we need a mechanism 
to decide which assignment is upheld. For the pur- 
poses of this paper, we make use of a simple heuris- 
tic where in the case of two positive predictions the 
one with the highest confidence measure is accepted. 
The decision trees return a confidence measure for 
each leaf of the tree. The confidence measure for a 
particular leaf is calculated from the training data 
and corresponds to the proportion of correct predic- 
tions over the total number of predictions at this 
leaf. 
4 Eva luat ion  
In this section we evaluate the unknown word cat- 
egorizer introduced above. We begin by describing 
the training and test data. Following this, we eval- 
uate the individual components and finally, we eval- 
uate the decision making component. 
The training and test data for the decision tree 
consists of 7000 cases of unknown words extracted 
from a 2.6 million word corpus of live business news 
captions. Of the 7000 cases, 70.4% were manually 
identified as names and 21.3% were identified as mis- 
spellings.The remaining cases were other types of 
unknown words such as abbreviations, morphologi- 
cal variants, etc. Seventy percent of the data was 
randomly selected to serve as the training corpus. 
The remaining thirty percent, or 2100 records, was 
reserved as the test corpus. The test data consists of 
ten samples of 2100 records selected randomly with 
replacement from the test corpus. 
We now consider the results of training a decision 
tree to identify misspellings using those features we 
introduced in the section on the misspelling identi- 
fier. The tree was trained on the training data de- 
scribed above. The tree was evaluated using each of 
the ten test data sets. The average precision and 
recall data for the ten test sets are given in Ta- 
ble 3, together with the base-line case of assuming 
that we categorize all unknown words as names (the 
most common category). With the baseline case we 
achieve 70.4% precision but with 0% recall. In con- 
trast, the decision tree approach obtains 77.1% pre- 
cision and 73.8% recall. 
We also trained a decision tree using not only the 
features identified in our discussion on misspellings 
but also those features that we introduced in our 
discussion of name identification. The results for 
this tree can be found in the second line of Table 
3. The inclusion of the additional features has in- 
creased precision by approximately 5%. However, it 
has also decreased recall by about the same amount. 
The overall F-score is quite similar. It appears that 
the name features are not predictive for identifying 
misspellings in this domain. This is not surprising 
considering that eight of the ten features specified 
for name identification are concerned with features 
of the two preceding and two following words. Such 
word-external information is of little use in identify- 
ing a misspelling. 
An analysis of the cases where the misspelling de- 
cision tree failed to identify a misspelling revealed 
two major classes of omissions. The first class con- 
tains a collection of words which have typical char- 
acteristics of English words, but differ from the in- 
tended word by the addition or deletion of a syllable. 
Words in this class include creditability for credi- 
bility, coordmatored for coordinated, and represen- 
tires for representatives. The second class contains 
misspellings that differ from known words by the 
deletion of a blank. Examples in this class include 
webpage, crewmembers, and rainshower. The second 
class of misspellings can be addressed by adding a 
feature that specifies whether the unknown word can 
be split up into two component known words. Such 
a feature should provide strong predictability for the 
second class of words. The first class of words are 
more of a challenge. These words have a close ho- 
mophonic relationship with the intended word rather 
than a close homographic relationship (as captured 
by edit distance). Perhaps this class of words would 
benefit from a feature representing phonetic distance 
rather than edit distance. 
Among those words which were incorrectly iden- 
tified as misspellings, it is also possible to identify 
common causes for the misidentification. Among 
these words are many foreign words which have 
character sequences which are not common in En- 
glish. Examples include khanehanalak, phytopla~2k- 
ton, brycee1~. 
The results for our name identifier are given in 
Table 4. Again, the decision tree approach is a sig- 
nificant improvement over the baseline case. If we 
take the baseline approach and assume that all un- 
known words are names, then we would achieve a 
precision of 70.4%. However, using the decision tree 
approach, we obtain 86.5% precision and 92.9% re- 
call. 
We also trained a tree using both the name and 
misspelling features. The results can be found in 
the second line of Table 4. Unlike the case when we 
trained the misspelling identifier on all the features, 
the extended tree for the name identifier provides 
increased recall as well as increased precision. Un- 
like the case with the misspelling decision-tree, the 
misspelling-identification features do provide predic- 
tive information for name identification. If we review 
the features, this result seems quite reasonable: fea- 
tures such as corpus frequency and non-English char- 
acters can provide evidence for/against name iden- 
176 
Baseline Precision/Recall 
Misspelling features only 70.4%/0% 
All features 
Precision Recall F-score 
73.8% 77 .1% 75.4 
82.8% 68.9% 75.2 
Table 3: Precision and recall for misspelling identification 
tification as well as for/against misspelling identifi- 
cation. For example, an unknown word that occurs 
quite frequently (such as clinton) is likely to be a 
name, whereas an unknown word that occurs infre- 
quently (such as wful) is likely to be a misspelling. 
A review of the errors made by the name iden- 
tifier again provides insight for future development. 
Among those unknown words that are names but 
which were not identified as such are predominantly 
names that can (and did) appear with determiners. 
Examples of this class include steelers in the steelers, 
and pathfinder in the pathfinder. Hence, the name 
identifier seems adept at finding the names of indi- 
vidual people and places, which typically cannot be 
combined with determiners. But, the name identi- 
fier has more problems with names that have similar 
distributions to common nouns. 
The cases where the name identifier incorrectly 
identifies unknown words as names also have identifi- 
able characteristics. These examples mostly include 
words with unusual character sequences such as the 
misspellings xetion and fwlamg. No doubt these 
have similar characteristics to foreign names. As 
the misidentified words are also correctly identified 
as misspellings by the misspelling identifier, these 
are less problematic. It is the task of the decision- 
making component to resolve issues such as these. 
The final results we include are for the unknown 
word categorizer itself using the voting procedure 
outlined in previous discussion. As introduced pre- 
viously, confidence measure is used as a tie-breaker 
in cases where the two components make positive 
decision. We evaluate the categorizer using preci- 
sion and recall metrics. The precision metric identi- 
fies the number of correct misspelling or name cat- 
egorizations over the total number of times a word 
was identified as a misspelling or a name. The re- 
call metric identifies the number of times the system 
correctly identifies a misspelling or name over the 
number of misspellings and names existing in the 
data. As illustrated in Table 5, the unknown word 
categorizer achieves 86% precision and 89.9% recall 
on the task of identifying names and misspellings. 
An examination of the confusion matrix of the tie- 
breaker decisions is also revealing. We include the 
confusion matrix for one test data set in Table 6. 
Firstly, in only about 5% of the cases was it nec- 
essary to revert to confidence measure to determine 
the category of the unknown word. In all other cases 
the predictions were compatible. Secondly, in the 
majority of cases the decision-maker rules in favour 
of the name prediction. In hindsight his is not sur- 
prising since the name decision tree has higher re- 
suits and hence is likely to have higher confidence 
measures. 
A review of the largest error category in this con- 
fusion matrix is also insightful. These are cases 
where the decision-maker classifies the unknown 
word as a name when it should be a misspelling (37 
cases). The words in this category are typically ex- 
amples where the misspelled word has a phonetic 
relationship with the intended word. For example, 
temt for tempt, floyda for florida, and dimow part of 
the intended word democrat. Not surprisingly, it was 
these types of words which were identified as prob- 
lematic for the current misspelling identifier. Aug- 
menting the misspelling identifier with features to 
identify these types of misspellings hould also lead 
to improvement in the decision-maker. 
We find these results encouraging: they indicate 
that the approach we are taking is productive. Our 
future work will focus on three fronts. Firstly, we 
will improve our existing components by developing 
further features which are sensitive to the distinction 
between names and misspellings. The discussion in 
this section has indicated several promising direc- 
tions. Secondly, we will develop components o iden- 
tify the remaining types of unknown words, such as 
abbreviations, morphological variants, etc. Thirdly, 
we will experiment with alternative decision-making 
processes. 
5 Examining Portability 
In this paper we have introduced a means for iden- 
tifying names and misspellings from among other 
types of unknown words and have illustrated the pro- 
cess using the domain of closed captions. Although 
not explicitly specified, one of the goals of the re- 
search has been to develop an approach that will be 
portable to new domains and languages. 
We are optimistic that the approach we have de- 
veloped is portable. The system that we have de- 
veloped requires very little in terms of linguistic re- 
sources. Apart from a corpus of the new domain 
and language, the only other requirements are some 
means of generating spelling suggestions (ispell is 
available for many languages) and a part-of-speech 
tagger. For this reason, the unknown word cate- 
gorizer should be portable to new languages, even 
where extensive language resources do not exist. If 
177 
Baseline Precision Precision Recall F-score 
Name features only 70.4% 86.5% 92.9% 89.6 
All Features 91.8% 94.5% 93.1 
Table 4: Precision and recall for name identification 
Precision Recall F-score 
Predicting Names and Misspellings 86.6% 89.9% 88.2 
Table 5: Precision and recall for decision-making component 
more information sources are available, then these 
can be readily included in the information provided 
to the decision tree training algorithm. 
For many languages, the features used in the 
unknown word categorizer may well be sufficient. 
However, the features used do make some assump- 
tions about the nature of the writing system used. 
For example, the edit distance feature in the mis- 
spelling identifier assumes that words consist of al- 
phabetic haracters which have undergone substitu- 
tion/addition/deletion. However, this feature will be 
less useful in a language such as Japanese or Chinese 
which use ideographic haracters. However, while 
the exact features used in this paper may be inap- 
propriate for a given language, we believe the generM 
approach is transferable. In the case of a language 
such as Japanese, one would consider the means by 
which misspellings differ from their intended word 
and identify features to capture these differences. 
6 Re la ted  Research  
There is little research that has focused on differen- 
tiating the different types of unknown words. For 
example, research on spelling error detection and 
correction for the most part assumes that all un- 
known words are misspellings and makes no attempt 
to identify other types of unknown words, e.g. (Elmi 
and Evens, 1998). Naturally, these are not appropri- 
ate comparisons for the work reported here. How- 
ever, as is evident from the discussion above, previ- 
ous spelling research does provide an important role 
in suggesting productive features to include in the 
decision tree. 
Research that is more similar in goal to that out- 
lined in this paper is Vosse (Vosse, 1992). Vosse uses 
a simple algorithm to identify three classes of un- 
known words: misspellings, neologisms, and names. 
Capitalization is his sole means of identifying names. 
However, capitalization i formation is not available 
in closed captions. Hence, his system would be inef- 
fective on the closed caption domain with which we 
are working. (Granger, 1983) uses expectations gen- 
erated by scripts to anMyze unknown words. The 
drawback of his system is that it lacks portability 
since it incorporates scripts that make use of world 
knowledge of the situation being described; in this 
case, naval ship-to-shore messages. 
Research that is similar in technique to that re- 
ported here is (Baluja et al, 1999). Baluja and his 
colleagues use a decision tree classifier to identify 
proper names in text. They incorporate three types 
of features: word level (essentially utilizes case in- 
formation), dictionary-level (comparable to our is- 
pell feature), and POS information (comparable to 
our POS tagging). Their highest F-score for name 
identification is 95.2, slightly higher than our name 
identifier. However, it is difficult to compare the 
two sets of results since our tasks are slightly dif- 
ferent. The goal of Baluja's research, and all other 
proper name identification research, is to identify all 
those words and phrases in the text which are proper 
names. Our research, on the other hand, is not con- 
cerned with all text, but only those words which are 
unknown. Also preventing comparison is the type of 
data that we deal with. Baluja's data contains case 
information whereas ours does not- the lack of case 
information makes name identification significantly 
more difficult. Indeed, Baluja's results when they 
exclude their word-level (case) features are signifi- 
cantly lower: a maximum F-score of 79.7. 
7 Conclusion 
In this paper we have introduced an unknown word 
eategorizer that can identify misspellings and names. 
The unknown word categorizer consists of individ- 
ual components, each of which specialize in iden- 
tifying a particular class of unknown word. The 
two existing components are implemented as deci- 
sion trees. The system provides encouraging results 
when evaluated against a particularly challenging 
domain: transcripts from live closed captions. 
References  
E. Agirre, K. Gojenola, K. Sarasola,  and A. Vouti- 
lainen. 1998. Towards a single proposal in spelling 
correction. In Proceedings of the 36th Ammal 
Meeting of the ACL and the 17th International 
1" /~ 178
Predicted Spelling Predicted Name 
Neither name nor misspelling 0 6 
Misspelling 10 37 
Name 4 43 
Table 6: Confusion matrix for decision maker: includes only those examples where both components made 
a positive prediction. 
Conference o1~ Computational Linguistics, pages 
22-28. 
S. Baluja, V. Mittal, and R.. Sukthankar. 1999. 
Applying machine learning for high performance 
named-entity extraction. In Proceedings of the 
Colzference of the Pacific Association for Com- 
putational Linguistics , pages 365-378. 
K. Church 1988. A stochastic parts program and 
noun phrase parser for unrestricted text. In Pro- 
ceedings of the Second Conference on Applied Nat- 
ural Language Processing, pages 136-143. 
F. Damerau. 1964. A technique for computer detec- 
tion and correction of spelling errors. Communi- 
cations of the ACM, 7:171-176. 
M. Elmi and M. Evens. 1998. Spelling correction 
using context. In Proceedings of the 36th Annual 
Meeting of the A CL and the 17th hlternational 
Collference on Computational Linguistics, pages 
360-364. 
D. Elworthy. 1998. Language identification with 
confidence limits. In Proceedings of the 6th Work- 
shop on Very large Corpora. 
R. Granger. 1983. The nomad system: expectation- 
based detection and correction of errors during un- 
derstanding of syntactically and semantically ill- 
formed text. American Journal of Computational 
Linguistics, 9:188-198. 
J. Hull and S. Srihari. 1982. Experiments in text 
recognition with binary n-gram and viterbi algo- 
rithms. IEEE Trans. Patt. Anal. Machine b~tell. 
PAMI-4, 5:520-530. 
K.. Kukich. 1992. Techniques for automatically cor- 
recting words in text. ACM Computing Surveys, 
24:377-439. 
I. Mani, R. McMillan, S. Luperfoy, E. Lusher, and 
S. Laskowski, 1996. Corpus Processing for Lexical 
Acquisition, chapter Identifying unknown proper 
names in newswire text. MIT Press, Cambridge. 
D. McDonald, 1996. Corpus Processing for Lexi- 
cal Acquisition, chapter Internal and external ev- 
idence in the identification and semantic atego- 
rization of proper names. MIT Press, Cambridge. 
K. Min and W. Wilson. 1998. Integrated control of 
chart items for error repair. In Proceedings of the 
36th Annual Meeting of the Association for Com- 
putational Linguistics and the 17th hlternational 
Conferet~ce on Computational Linguistics. 
K. Min. 1996. Hierarchical Error Re.covery Based 
on Bidirectional Chart Parsing Techniques. Ph.D. 
thesis, University of NSW, Sydney, Australia. 
R. Mitton. 1987. Spelling checkers, spelling coffee- 
tots, and the misspellings of poor spellers. Inf. 
Process. Manage, 23:495-505. 
J. Toole 1999 Categorizing Unknown Words: A de- 
cision tree-based misspelling identifier In Foo, N 
(ed.) Advanced Topics in Artificial h2telligence, 
pages 122-133. 
H. van Halteren, J. Zavrel, and W. Daelemans. 1998. 
Improving data driven word class tagging by sys- 
tem combination. In Proceedings of the 36th An- 
nual Meeting of the ACL and the 17th Interna- 
tional Conference on Computational Linguistics, 
pages 491-497. 
T. Vosse. 1992. Detecting and correcting morpho- 
syntactic errors in real texts. In Proceedin9s of 
the 3rd Conference o11 Applied Natural Language 
Processing, pages 111-118. 
S. Weiss and N. Indurkhya. 1998. Predictive Data 
Mining. Morgan Kauffman Publishers. 
S. Weiss, and C. Apte, and F. Damerau, and 
D. Johnson, and F. Oles and T. Goetz, and 
T. Hampp. 1999 Maximizing text-mining per- 
formance. IEEE Intelligent Systems and their 
Applications, 14(4):63-69 
E. Zamora, J. Pollock, and A. Zamora. 1981. The 
use of tri-gram analysis for spelling error detec- 
tion. he Process. Manage., 17:305-316. 
179
Pre-processing Closed Captions for Machine Translation 
Dav ide  Turcato  Fred Popowich  Paul  McFet r idge  
Dev lan  N icho lson  Jan ine  Too le  
Natural Language Laboratory, School of Computing Science, Simon Fraser University 
8888 University Drive, Burnaby, British Columbia, V5A 1S6, Canada 
and 
gavagai Technology Inc. 
P.O. 374, 3495 Cambie Street, Vancouver, British Columbia, V5Z 4R3, Canada 
{turk, popowich, mcfet, devl an, toole}?cs, sfu. ca 
Abst rac t  
We describe an approach to Machine Transla- 
tion of transcribed speech, as found in closed 
captions. We discuss how the colloquial nature 
and input format peculiarities of closed captions 
are dealt with in a pre-processing pipeline that 
prepares the input for effective processing by 
a core MT system. In particular, we describe 
components for proper name recognition and 
input segmentation. We evaluate the contribu- 
tion of such modules to the system performance. 
The described methods have been implemented 
on an MT system for translating English closed 
captions to Spanish and Portuguese. 
1 In t roduct ion  
Machine Translation (MT) technology can be 
embedded in a device to perform real time 
translation of closed captions included in TV 
signals. While speed is one factor associated 
with the construction of such a device, another 
factor is the language type and format. The 
challenges posed by closed captions to MT can 
be attributed to three distinct characteristics: 
Firstly, closed captions are transcribed 
speech. Although closed captions are not a com- 
pletely faithful transcription of TV programs, 
they render spoken language and therefore the 
language used is typically colloquial (Nyberg 
and Mitamura, 1997). They contain many of 
the phenomena which characterize spoken lan- 
guage: interjections, repetitions, tuttering, el- 
lipsis, interruptions, hesitations. Linguistically 
and stylistically they differ from written lan- 
guage: sentences are shorter and poorly struc- 
tured, and contain idiomatic expressions, un- 
grammaticality, etc. The associated ifficulties 
stem from the inherently colloquial nature of 
closed captions, and, to different degrees, of 
all forms of transcribed speech (Hindle, 1983). 
Such difficulties require a different approach 
than is taken for written documents. 
Secondly, closed captions come in a specific 
format, which poses problems for their optimal 
processing. Closed-captioners may often split 
a single utterance between two screens, if the 
character limit for a screen has been exceeded. 
The split is based on consideration about string 
length, rather than linguistic considerations, 
hence it can happen at non-constituent bound- 
aries (see Table 1), thus making the real time 
processing of the separate segments problem- 
atic. Another problem is that captions have no 
upper/lower case distinction. This poses chal- 
lenges for proper name recognition since names 
cannot be identified by an initial capital. Addi- 
tionally, we cannot rely on the initial uppercase 
letter to identify a sentence initial word. This 
problematic aspect sets the domain of closed 
captions apart from most text-to-text MT do- 
mains, making it more akin, in this respect, to 
speech translation systems. Although, from a 
technical point of view, such input format char- 
acteristics could be amended, most likely they 
are not under a developer's control, hence they 
have to be presumed. 
Thirdly, closed captions are used under oper- 
ational constraints. Users have no control over 
the speed of the image or caption flow so (s)he 
must comprehend the caption in the limited 
time that the caption appears on the screen. 
Accordingly, the translation of closed captions 
is a "time-constrained" application, where the 
user has limited time to comprehend the system 
output. Hence, an MT system should produce 
translations comprehensible within the limited 
time available to the viewer. 
In this paper we focus on the first two fac- 
tors, as the third has been discussed in (Toole 
et al, 1998). We discuss how such domain- 
38 
good evening, i'm jim lehrer. 
on the "newshour" tonight, four members of congress debate the 
u.n. deal with iraq; paul solman tells the troubled story of 
indonesia's currency; mark 
shields and paul gigot analyze the political week; 
and elizabeth farnsworth explains how the universe is getting 
larger. 
Table 1: Closed caption script fragment. 
dependent, problematic factors are dealt with 
in a pre-processing pipeline that prepares the 
input for processing by a core MT system. The 
described methods have been implemented for 
an MT system that translates English closed 
captions to Spanish and Portuguese. All the 
examples here refer to the Spanish module. 
2 P re -process ing  des ign  
Input pre-processing is essential in an embedded 
real time system, in order to simplify the core 
processing and make it both time- and memory- 
effective. In addition to this, we followed the 
guideline of separating domain-dependent pro- 
cesses and resources from general purpose ones. 
On the one hand, grammars and lexicons are 
costly resources. It would be desirable for them 
to be domain-independent a d portable across 
different domains, as well as declarative and 
bidirectional. On the other hand, a domain with 
distinctive characteristics requires ome specific 
treatment, if a system aims at robustness. We 
decided to have a domain independent core MT 
system, locating the domain dependent process- 
ing in a pipeline of low-level components, easy  
to implement, aiming at fast and robust pro- 
cessing and using limited linguistic knowledge. 
We use declarative and bidirectional gram- 
mars and lexicons. The lexicMist approach is 
indeed suitable to the closed caption domain, 
e.g. in terms of its capability of handling loosely 
structured or incomplete sentences. Also, the 
linguistic resources are geared towards this do- 
main in terms of grammatical nd lexical cover- 
age. However, our system architecture and for- 
malism make them equally usable in any other 
domain and translation direction, as the linguis- 
tic knowledge therein contained is valid in any 
domain. For the architecture we refer the reader 
to (Popowich et al, 1997). In the rest of this 
paper we focus on the pre-processing module 
39 
and how it deals with the issues discussed in 
the introduction. 
The task of the pre-processing pipeline is to 
make the input amenable to a linguistically- 
principled, domain independent reatment. 
This task is accomplished in two ways: 
1. By normalizing the input, i.e. removing 
noise, reducing the input to standard typo- 
graphical conventions, and also restructur- 
ing and simplifying it, whenever this can be 
done in a reliable, meaning-preserving way. 
2. By annotating the input with linguistic in- 
formation, whenever this can be reliably 
done with a shallow linguistic analysis, to 
reduce input ambiguity and make a full lin- 
guistic analysis more manageable. 
Figure (1) shows the system architecture, 
with a particular emphasis on the pre- 
processing pipeline. The next section describes 
the pipeline up to tagging. Proper name 
recognition and segmentation, which deal more 
specifically with the problems described in the 
introduction, are discussed in further sections. 
3 Normal i za t ion  and  tagg ing  
The label normalization groups three compo- 
nents, which clean up and tokenize the input. 
The text-level normalization module performs 
operations at the string level, such as remov- 
ing extraneous text and punctuation (e.g. curly 
brackets , used to mark off sound effects), or re- 
moving periods from abbreviations. E.g.: 
(I) "I went to high school in the u.s." 
"I went to high school in the usa." 
The tokenizer breaks a line into words. The 
token-level normalization recognizes and an- 
notates tokens belonging to special categories 
Pre-processing 
Normalization 
\[Text-level normalization \]
\[ Tokenization ) 
\[Token-level normalization I 
+ 
\[ Proper name recognition \] 
\[ Segmentation "') 
Core MT 
system 
Anal' ,sis \] 
+ 
i I Oo.o a, on I 
,1 
\ ]Pos t -process ing  ) 
Figure 1: System architecture. 
(times, numbers, etc.), expands contractions, 
recognizes, normalizes and annotates tutters 
(e.g. b-b-b-bright), identifies compound words 
and converts number words into digits. E.g.: 
(2) "I" "went" "to" "high" "school" 
"in" "the" "usa" " " 
"I" "went" "to" "high school" "in" 
"the" "usa" " " 
(3) "W-wh-wha~'s" "that" "?"0  
"what"/stutter "is" "that" "?" 
Note that annotations associated with tokens 
are carried along the entire translation process, 
so as to be used in producing the output (e.g. 
stutters are re-inserted in the output). 
The tagger assigns parts of speech to tokens. 
Part of speech information is used by the subse- 
quent pre-processing modules, and also in pars- 
ing, to prioritize the most likely lexical assign- 
ments of ambiguous items. 
4 P roper  name recogn i t ion  
Proper names are ubiquitous in closed captions 
(see Table 1). Their recognition is important 
for effective comprehension of closed captions, 
particularly in consideration of two facts: (i) 
users have little time to mentally rectify a mis- 
translation; (ii) a name can occur repeatedly 
in a program (e.g. a movie), with an annoy- 
ing effect if it is systematically mistranslated 
(e.g. a golf tournament where the golfer named 
Tiger Woods is systematically referred to as los 
bosques del tigre, lit. 'the woods of the tiger'). 
Name recognition is made harder in the closed 
caption domain by the fact that no capitaliza- 
tion information is given, thus making unusable 
all methods that rely on capitalization as the 
main way to identify candidates (Wolinski et al, 
1995) (Wacholder et al, 1997). For instance, an 
expression like 'mark sh ie lds ' ,  as occurs in Ta- 
ble (1), is problematic in the absence of capital- 
ization, as both 'mark' and ' sh ie lds '  are three- 
way ambiguous (proper name, common noun 
and verb). Note that this identical problem may 
be encountered if an MT system is embedded 
in a speech-to-speech translation as well. This 
situation forced us to explore different ways of 
identifying proper names. 
The goal of our recognizer is to identify 
proper names in a tagged line and annotate 
them accordingly, in order to override any 
other possiblelexical assignment in the follow- 
ing modules. The recognizer also overrides pre- 
vious tokenization, by possibly compounding 
two or more tokens into a single one, which 
will be treated as such thereafter. Besides part 
of speech, the only other information used by 
the recognizer is the lexical status of words, i.e. 
their ambiguity class (i.e. the range of possible 
syntactic ategories it can be assigned) or their 
status as an unknown word (i.e. a word that 
is not in the lexicon). The recognizer scans an 
input line from left to right, and tries to match 
40 
each item against a sequences of patterns. Each 
pattern expresses constraints (in terms of word, 
part of speech tag and lexical status) on the 
item under inspection and its left and right con- 
texts. Any number of items can be inspected to 
the left and right of the current item. Such pat- 
terns also make use of regular expression bpera- 
tors (conjunction, disjunction, negation, Kleene 
star). For instance (a simplified version of) a 
pattern might look like the following: 
(4) /the/DEW (NOUNIADJ)*\] X' \['NOUN\] 
where we adopt the convention of representing 
words by lowercase strings, part of speech tags 
by uppercase strings and variables by primed 
Xs. The left and right context are enclosed 
in square brackets, respectively to the left and 
right of the current item. They can also con- 
tain special markers for the beginning and end 
of a line, and for the left or right boundary of 
the proper name being identified. This way to- 
kenization can be overridden and separate to- 
kens joined into a single name. Constraints on 
the lexical status of items are expressed as pred- 
icates associated with pattern elements, e.g.: 
(5) proper_and_common (X') 
A pattern like the one above (4-5) would 
match a lexically ambiguous proper/common 
noun preceded by a determiner (with any num- 
ber of nouns or adjectives in between), and not 
followed by a noun (e.g. ' the b i l l  i s . . . ' ) .  Be- 
sides identifying proper names, some patterns 
may establish that a given item is not a name 
(as in the case above). A return value is as- 
sociated with each pattern, specifying whether 
the current match is or is not a proper name. 
Once a successful match occurs, no further pat- 
terns are tried. Patterns are ordered from more 
to less specific. At the bottom of the pattern 
sequence are the simplest patterns, e.g.: 
(6) ( \[\] X' \[\] ), proper_and_common(X') 
yes 
which is the default assignment for words like 
'b i l l '  if no other pattern matched. However 
(6) is overridden by more specific patterns like: 
(7) ( \[x''\] x' \[\] ) ,  
proper_and_common (X'), common(X") 
no 
41 
(s) ( \[x' \]  x' \[\] ) ,  
proper_and_common(X'), proper(X")  
yes 
The former pattern covers cases like 
' te lecommunicat ions  b i l l ' ,  preventing 
'b i l l '  from being interpreted as a proper 
name, the latter covers cases like 'damian 
b i l l ' ,  where 'b i l l '  is more likely to be a name. 
In general, the recognizer tries to disam- 
biguate lexically ambiguous nouns or to as- 
sign a category to unknown words on the ba- 
sis of the available context. However, in prin- 
ciple any word could be turned into a proper 
name. For instance, verbs or adjectives can 
be turned into proper names, when the con- 
text contains strong cues, like a title. Increas- 
ingly larger contexts provide evidence for more 
informed guesses, which override guesses based 
on narrower contexts. Consider the following 
examples that show how a word or expression 
is treated ifferently depending on the available 
context. Recognized names are in italics. 
(9) biZ~ ~ 
(i0) the bill is ... 
(11) the b i l l  clinton is . . .  
(12) the  b i l l  c l in ton  admin is t ra t ion  is  
The lexically ambiguous bill, interpreted as 
a proper name in isolation, becomes a common 
noun if preceded by a determiner. However, 
the interpretation reverts to proper name if an- 
other noun follows. Likewise the unknown word 
clinton is (incorrectly) interpreted as a com- 
mon noun in (11), as it is the last item of a 
noun phrase introduced by a determiner, but it 
becomes a proper name if another noun follows. 
We also use a name memory ,  which patterns 
have access to. As proper names are found in an 
input stream, they are added to the name mem- 
ory. A previous occurrence of a proper name is 
used as evidence in making decisions about fur- 
ther occurrences. The idea is to cache names 
occurred in an 'easy' context (e.g. a name pre- 
ceded by a title, which provides trong evidence 
for its status as a proper name), to use them 
later to make decisions in 'difficult' contexts, 
where the internal evidence would not be suffi- 
cient to support a proper name interpretation. 
Hence, what typically happens is that the same 
name in the same context is interpreted iffer- 
ently at different imes, if previously the name 
has occurred in an 'easy' context and has been 
memorized. E.g.: 
(13) the individual title went to tiger 
woods. 
mr. tiger woods struggled today 
with a final round 80. 
name-memory 
the short well publicized 
professional life of t iger  woods 
has been an open book. 
The name memory was designed to suit the 
peculiarity of closed captions. Typically, in this 
domain proper names have a low dispersion. 
They are concentrated in sections of an input 
stream (e.g. the name of the main characters 
in a movie), then disappear for long sections 
(e.g. after the movie is over). Therefore, a 
name memory needs to be reset to reflect such 
changes. However, it is problematic to decide 
when to reset the name memory. Even if it was 
possible to detect when a new program starts, 
one should take into account the possible sce- 
nario of an MT system embedded in a consumer 
product, in which case the user might unpre- 
dictably change channel at any time. In or- 
der to keep a name memory aligned with the 
current program, without any detection of pro- 
gram changes, we structured the name memory 
as a relatively short queue (first in, first out). 
Every time a new item is added to the end of 
the queue, the first item is removed and all the 
other items are shifted. Moreover, we do not 
check whether a name is already in the mem- 
ory. Every time a suitable item is found, we 
add it to the memory, regardless of whether it 
is already there. Hence, the same item could 
be present wice or more in the memory at any 
given time. The result of this arrangement is 
that a name only remains in the memory :for a 
relatively short time. It can only remain :\[or a 
longer time if it keeps reappearing frequently in 
the input stream (as typically happens), other- 
wise it is removed shortly after it stopped ap- 
pearing. In this way, the name memory is kept 
42 
# of items 
Proper names correctly identified 
False positives 
False negatives 
152 
8 
57 
Table 2: Name recognition evaluation results. 
aligned with the current program, with only a 
short transition period, during which names no 
longer pertinent are still present in the memory, 
before getting replaced by pertinent ones. 
The recognizer currently contains 63 pat- 
terns. We tested the recognizer on a sample of 
1000 lines (5 randomly chosen continuous frag- 
ments of 200 lines each). The results, shown in 
table (2), illustrate a recall of 72.7% and a pre- 
cision of 95.0%. These results reflect our cau- 
tious approach to name recognition. Since the 
core MT system has its own means of identify- 
ing some proper names (either in the lexicon or 
via default assignments o unknown words) we 
aimed at recognizing names in pre-processing 
only when this could be done reliably. Note 
also that 6 out of the 8 false positives were iso- 
lated interjections that would be better left un- 
translated (e.g. p f foo ,  e l  smacko), or closed 
captioner's typos (e.g. yo4swear).  
5 Segmentation 
Segmentation breaks a line into one or more 
segments, which are passed separately to sub- 
sequent modules (Ejerhed, 1996) (Beeferman et 
al., 1997). In translation, segmentation is ap- 
plied to split a line into a sequence of transla- 
tionally self-contained units (Lavie et al, 1996). 
In our system, the translation units we iden- 
tify are syntactic units, motivated by cross- 
linguistic considerations. Each unit is a con- 
stituent that dan be translated independently. 
Its translation is insensitive to the context in 
which the unit occurs, and the order of the units 
is preserved by translation. 
One motivation for segmenting is that pro- 
cessing is faster: syntactic ambiguity is reduced, 
and backtracking from a module to a previ- 
ous one does not involve re-processing an en- 
tire line, but only the segment hat failed. A 
second motivation is robustness: a failure in 
one segment does not involve a failure in the 
entire line, and error-recovery can be limited 
only to a segment. Further motivations are pro- 
vided by the colloquial nature of closed cap- 
tions. A line often contains fragments with a 
loose syntactic relation to each other and to the 
main clause: vocatives, false starts, tag ques- 
tions, etc. These are most easily translated as 
individual segments. Parenthetical expressions 
are often also found in the middle of a main 
clause, thus making complete parses problem- 
atic. However, the solution involves a heavier 
intervention than just segmenting. Dealing with 
parentheticals requires restructuring a line, and 
reducing it to a 'normal' form which ideally al- 
ways has parenthetical expressions at one end of 
a sentence (under the empirical assumption that 
the overall meaning is not affected). We will 
see how this kind of problem is handled in seg- 
mentation. A third motivation is given by the 
format of closed captions, with input lines split 
across non-constituent boundaries. One solu- 
tion would be delaying translation until a sen- 
tence boundary is found, and restructuring the 
stored lines in a linguistically principled way. 
However, the requirements of real time transla- 
tion (either because of real time captioning at 
the source, or because the MT system is embed- 
ded in a consumer product), together with the 
requirement that translations be aligned with 
the source text and, above all, with the images, 
makes this solution problematic. The solution 
we are left with, if we want lines to be bro- 
ken along constituent boundaries, is to further 
segment a sentence, even at the cost of some- 
times separating elements that should go to- 
gether for an optimal translation. We also ar- 
gued elsewhere (Toole et al, 1998) that in a 
time-constrained application the output gram- 
maticality is of paramount importance, even at 
the cost of a complete meaning equivalence with 
the source. For this reason, we also simplify 
likely problematic input, when a simplification 
is possible without affecting the core meaning. 
To sum up, the task at hand is broader than 
just segmentation: re-ordering of constituents 
and removal of words are also required, to syn- 
tactically 'normalize' the input. As with name 
recognition, we aim at using efficient and easy 
to implement techniques, relying on limited lin- 
guistic information. The segmenter works by 
matching input lines against a set of templates 
represented by pushdown transducers. Each 
transducer is specified in a fairly standard way 
(Gazdar and Mellish, 1989, 82), by defining an 
initial state, a final state, and a set of transitions 
of the following form: 
(14) (State I, State2, Label, Transducer> 
Such a transition specifies that Transducer 
can move from Statel to State2 when the in- 
put specified by Label is found. Label can be 
either a pair (InputSymbol, OutputSymbol) or 
the name of another transducer, which needs 
to be entirely traversed for the transition from 
State l  to State2 to take place. An input sym- 
bol is a <Word, Tag> pair. An output symbol 
is an integer anging from 0 to 3, specifying to 
which of two output segments an input sym- 
bol is assigned (0 = neither segment, 3 = both 
segments, 1 and 2 to be interpreted in the ob- 
vious way). The output codes are then used to 
perform the actual split of a line. A successful 
match splits a line into two segments at most. 
However, on a successful split, the resulting seg- 
ments are recursively fed to the segmenter, until 
no match is found. Therefore, there is no limit 
to the number of segments obtained from an 
input line. The segmenter currently contains 
37 top-level transducers, i.e. segmenting pat- 
terns. Not all of them are used at the same time. 
The implementation of patterns is straightfor- 
ward and the segmenter can be easily adapted 
to different domains, by implementing specific 
patterns and excluding others. For instance, a 
very simple patterns plit a line at every comma, 
a slightly more sophisticated one, splits a line at 
every comma, unless tagged as a coordination; 
other patterns plit a final adverb, interjection, 
prepositional phrase, etc. 
Note that a segment can be a discontinuous 
part of a line, as the same output code can be 
assigned to non-contiguous elements. This fea- 
ture is used, e.g., in restructuring a sentence, as 
when a parenthetical expression is encountered. 
Thefollowing example shows an input sentence, 
an assignment, and a resulting segmentation. 
(15) this, however, is a political 
science course. 
(16) this/2 ,/0 however/l ,/i is/2 a/2 
political/2 science/2 course/2. 
(17) I. however , 
43 
2. this is a po l i t ica l  sc ience 
course 
We sometimes use the segmenter's ability to 
simplify the input, e.g. with adverbs like just, 
which are polysemous and difficult to translate, 
but seldom contribute to the core meaning of a 
sentence. 
6 Per fo rmance  
We ran a test to evaluate how the recognizer 
and segmenter affected the quality of transla- 
tions. We selected asample of 200 lines of closed 
captioning, comprising four continuous sections 
of 50 lines each. The sample was run through 
the MT system twice, once with the recognizer 
and segmenter activated and once without. The 
results were evaluated by two native Spanish 
speakers. We adopted a very simple evalua- 
tion measure, asking the subjects to tell whether 
one translation was better than the other. The 
translations differed for 32 input lines out of 200 
(16%). Table (3) shows the evaluation results, 
with input lines as the unit of measurement. 
The third column shows the intersection of the 
two evaluations, i.e. the evaluations on which 
the two subjects agreed. The three rows show 
how often the translation was better (i) with 
pre-processing, (ii) without pre-processing, or 
(iii) no difference could be appreciated. 
The results show a discrepancy in the evalu- 
ations. One evaluator also pointed out that it 
is hard to make sense of transcribed closed cap- 
tions, without the audio-visual context. These 
two facts seem to point out that an appropri- 
ate evaluation should be done in the operational 
context in which closed captions are normally 
used. Still, the intersection of the subjects' eval- 
uations shows that pre-processing improves the 
output quality. In three of the four cases where 
the two evaluators agreed that pre-processing 
yielded a worse result, the worse performance 
was due to an incorrect name recognition oi" seg- 
mentation. However, in two of the three cases, 
the original problem was an incorrect agging. 
Note that even when the name recognizer 
and segmenter are off, the system can identify 
some names, and recover from translation fail- 
ures by piecing together translations of frag- 
ments. Therefore, what was being tested was 
not so much name recognition and segmenting 
44 
per se, but the idea of having separate modules 
for such tasks in the system front end. 
Finally, the test did not take into account 
speed, as we set higher time thresholds than 
an embedded application would require. Since 
segmentation reduces processing time, it is also 
expected to reduce the impact of tighter time 
thresholds, all other things being equal. 
We are planning to conduct an operational 
evaluation of the system. The goal is to evalu- 
ate the system output in its proper visual con- 
text, and compare the results with parallel re- 
sults for human translated closed captions. Dif- 
ferent groups of participants will watch a video 
With either human- or machine-translated sub- 
titles, and complete a questionnaire based on 
the subtitles in the video. The questionnaire 
will contain a set of questions to elicit the sub- 
ject's assessment on the translation quality, and 
a set of questions to assess the subject's level of 
comprehension f the program. 
7 Conc lus ion  
It is apparent hat the peculiarity of closed 
captions, both in terms of transcribed speech 
characteristic and constraints due to the input 
format, require an ad hoc treatment, consider- 
ably different from the approaches suitable for 
written documents. Yet the knowledge about 
a language (or the bilingual knowledge about 
a language-pair) is largely invariant across dif- 
ferent applications domains and should there- 
fore be portable from one application domain 
to another. The architecture we have proposed 
strives to combine the need for domain indepen- 
dent linguistic resources and linguistically prin- 
cipled methods with the need for robust MT 
systems tuned to real world, noisy and idiosyn- 
cratic input, as encountered when embedding 
MT in real woi:ld devices. 
In terms of adequacy, a standard evaluation 
and a comparison among different MT systems 
frtom different domains is hard, as the ade- 
quacy of a system depends on its application 
(Church and Hovy, 1993). This is even truer 
with-closed captions, where the use of transla- 
tion output is heavily influenced by operational 
constraints (time constraints, the presence of 
images, sound, etc.). In some cases such con- 
straints may place a heavier burden on a system 
(e.g. the time constraint), in some other cases 
Judge 1 Judge 2 Both agreed 
Better with pre~processing 
Better without pre-processing 
No difference 
21 16 15 
4 12 4 
7 4 3 
Table 3: Evaluation results. 
they can make an imperfect ranslation accept- 
able (e.g. the presence of images and sounds). 
We did not attempt an assessment in absolute 
terms, which we believe should take into ac- 
count the operational environment and involve 
real-world users. More modestly, we aimed at 
showing that our pre-processing techniques pro- 
vide an improvement in performance. 
Our work on closed captions also shows that 
the challenges coming from this domain, even 
in terms on low-level issues of input format, can 
lead to interesting developments of new linguis- 
tic techniques. We believe that our solutions to 
specific problems (namely, proper name recog- 
nition and segmentation) in the closed caption 
domain bear relevance to a wider context, and 
offer techniques that can be usefully employed 
in a wider range of applications. 
Re ferences  
Doug Beeferman, Adam Berger, and John Laf- 
ferty. 1997. Text segmentation using expo- 
nential models. In Proceedings of the Second 
Conference on Empirical Methods in Natu- 
ral Language Processing (EMNLP-2), Prov- 
idence, USA. 
Kenneth W. Church and Eduard H. Hovy. 
1993. Good applications for crummy machine 
translation. Machine Translation, 8:239-258. 
Eva Ejerhed. 1996. Finite state segmentation 
of discourse into clauses. In A. Kornai, ed- 
itor, Proceedings of the ECAI-96 Workshop 
Extended Finite State Models of Language, 
Budapest,Hungary. 
Gerald Gazdar and Christopher S. Mellish. 
1989. Natural Language Processing in PRO- 
LOG: an Introduction to Computational Lin- 
guistics. Addison-Wesley Publishing Com- 
pany, Wokingham, England. 
Donald Hindle. 1983. Deterministic parsing of 
syntactic non-fluencies. In Proceedings ofthe 
21st Annual Meeting of the Association for 
Computational Linguistics (ACL-83), pages 
123-128, Cambridge, Massachusetts, USA. 
Alon Lavie, Donna Gates, Noah Coccaro, and 
Lori Levin. 1996. Input segmentation of 
spontaneous peech in janus: a speech- 
to-speech translation system. In Proceed- 
ings of ECAI-96 Workshop on Dialogue Pro- 
cessing in Spoken Language Systems, Bu- 
dapest,Hungary. 
Eric Nyberg and Teruko Mitamura. 1997. A 
real-time MT system for translating broad- 
cast captions. In Proceedings ofthe Sixth Ma- 
chine Translation Summit, pages 51-57, San 
Diego, California, USA. 
Fred Popowich, Davide Turcato, Olivier Lau- 
rens, Paul McFetridge, J. Devlan Nicholson, 
Patrick McGivern, Maricela Corzo-Pena, Lisa 
Pidruchney, and Scott MacDonald. 1997. A 
lexicalist approach to the translation of collo- 
quial text. In Proceedings ofthe 7th Interna- 
tional Conference on Theoretical nd Method- 
ological Issues in Machine Translation, pages 
76-86, Santa Fe, New Mexico, USA. 
Janine Toole, Davide Turcato, Fred Popowich, 
Dan Fass, and Paul McFetridge. 1998. Time- 
constrained Machine Translation. In Proceed- 
ings of the Third Conference of the Associa- 
tion for Machine Translation in the Ameri- 
cas (AMTA-98), pages 103-112, Langhorne, 
Pennsylvania, USA. 
Nina Wacholder, Yael Ravin, and Misook Choi. 
? 1997. Disambiguation of proper names in 
texts. In Proceedings of the Fifth Confer- 
ence on Applied Natural Language Processing 
(ANLP-97), pages 202-208, Washington, DC, 
USA. Association for Computational Linguis- 
tics. 
Francis Wolinski, Frantz Vichot, and Bruno Dil- 
let. 1995. Automatic processing of proper 
names in texts. In Proceedings of the 7th 
Conference of the European Chapter of the 
Asscociation for Computational Linguistics 
(EACL-95), pages 23-30, Dublin, Ireland. 
45 
Adapt ing a synonym database to specif ic domains 
Dav ide  Turcato  Fred Popowich  Jan ine  Toole  
Dan Pass Dev lan  N icho lson  Gordon T i sher  
gavagai Technology Inc. 
P.O. 374, 3495 Ca~abie Street, Vancouver, British Columbia, V5Z 4R3, Canada 
and 
Natural Language Laboratory, School of Computing Science, Simon Fraser University 
8888 University Drive, Burnaby, British Columbia, V5A 1S6, Canada 
{turk, popowich ,toole, lass, devl an, gt i sher}@{gavagai, net, as, sfu. ca} 
Abst ract  
This paper describes a method for 
adapting ageneral purpose synonym 
database, like WordNet, to a spe- 
cific domain, where only a sub- 
set of the synonymy relations de- 
fined in the general database hold. 
The method adopts an eliminative 
approach, based on incrementally 
pruning the original database. The 
method is based on a preliminary 
manual pruning phase and an algo- 
rithm for automatically pruning the 
database. This method has been im- 
plemented and used for an Informa- 
tion Retrieval system in the aviation 
domain. 
1 In t roduct ion  
Synonyms can be an important resource for 
Information Retrieval (IR) applications, and 
attempts have been made at using them to 
expand query terms (Voorhees, 1998). In 
expanding query terms, overgeneration is as 
much of a problem as incompleteness or lack 
of synonym resources. Precision can dramat- 
ically drop because of false hits due to in- 
correct synonymy relations. This problem is 
particularly felt when IR is applied to docu- 
ments in specific technical domains. In such 
cases, the synonymy relations that hold in the 
specific domain are only a restricted portion 
of the synonymy relations holding for a given 
language at large. For instance, a set of syn- 
onyms like 
(1) {cocaine, cocain, coke, snow, C} 
valid for English, would be detrimental in a 
specific domain like weather eports, where 
both snow and C (for Celsius) occur very fre- 
quently, but never as synonyms of each other. 
We describe a method for creating a do- 
main specific synonym database from a gen- 
eral purpose one. We use WordNet (Fell- 
baum, 1998) as our initial database, and we 
draw evidence from a domain specific corpus 
about what synonymy relations hold in the 
domain. 
Our task has obvious relations to word 
sense disambiguation (Sanderson, 1997) (Lea- 
cock et al, 1998), since both tasks are based 
on identifying senses of ambiguous words in 
a text. However, the two tasks are quite dis- 
tinct. In word sense disambiguation, a set of 
candidate senses for a given word is checked 
against each occurrence of the relevant word 
in a text, and a single candidate sense is se- 
lected for each occurrence ofthe word. In our 
synonym specialization task a set of candidate 
senses for a given word is checked against an 
entire corpus, and a subset of candidate senses 
is selected. Although the latter task could be 
reduced to the former (by disambiguating all
occurrences of a word in a test and taking 
the union of the selected senses), alternative 
approaches could also be used. In a specific 
domain, where words can be expected to be 
monosemous to a large extent, synonym prun- 
ing can be an effective alternative (or a com- 
plement) to word sense disambiguation. 
From a different perspective, our 
task is also related to the task of as- 
signing Subject Field Codes (SFC) to 
a terminological resource, as done by 
Magnini and Cavagli~ (2000) for WordNet. 
Assuming that a specific domain corresponds 
to a single SFC (or a restricted set of SFCs, 
at most), the difference between SFC as- 
signment and our task is that the former 
assigns one of many possible values to a given 
synset (one of all possible SFCs), while the 
latter assigns one of two possible values (the 
words belongs or does not belong to the SFC 
representing the domain). In other words, 
SFC assignment is a classification task, while 
ours can be seen as either a filtering or 
ranking task. 
Adopting a filtering/ranking perspective 
makes apparent hat the synonym pruning 
task can also be seen as an eliminative pro- 
cess, and as such it can be performed incre- 
mentally. In the following section we will 
show how such characteristics have been ex- 
ploited in performing the task. 
In section 2 we describe the pruning 
methodology, while section 3 provides a prac- 
tical example from a specific domain. Con- 
clusions are offered in section 4. 
2 Methodo logy  
2.1 Out l ine  
The synonym pruning task aims at improv- 
ing both the accuracy and the speed of a syn- 
onym database. In order to set the terms of 
the problem, we find it useful to partition the 
set of synonymy relations defined in WordNet 
into three classes: 
. Relations irrelevant to the specific do- 
main (e.g. relations involving words that 
seldom or never appear in the specific do- 
main) 
. Relations that are relevant but incorrect 
in the specific domain (e.g. the syn- 
onymy of two words that do appear in the 
specific domain, but are only synonyms 
in a sense irrelevant o the specific do- 
main); 
3. Relations that are relevant and correct in 
the specific domain. 
The creation of a domain specific database 
aims at removing relations in the first two 
classes (to improve speed and accuracy, re- 
spectively) and including only relations in the 
third class. 
The overall goal of the described method 
is to inspect all synonymy relations in Word- 
Net and classify each of them into one of the 
three aforementioned classes. We define a 
synonymy relation as a binary relation be- 
tween two synonym terms (with respect to 
? a particular sense). Therefore, a WordNet 
synset containing n terms defines ~11 k syn- 
onym relations. The assignment of a syn- 
onymy relation to a class is based on evidence 
drawn from a domain specific corpus. We use 
a tagged and lemmatized corpus for this pur- 
pose. Accordingly, all frequencies used in the 
rest of the paper are to be intended as fre- 
quencies of ( lemma, tag) pairs. 
The pruning process is carried out in three 
steps: (i) manual pruning; (ii) automatic 
pruning; (iii) optimization. The first two 
steps focus on incrementally eliminating in- 
correct synonyms, while the third step focuses 
on removing irrelevant synonyms. The three 
steps are described in the following sections. 
2.2 Manua l  p run ing  
Different synonymy relations have a different 
impact on the behavior of the application in 
which they are used, depending on how fre- 
quently each synonymy relation is used. Rela- 
tions involving words frequently appearing in 
either queries or corpora have a much higher 
impact (either positive or negative) than re- 
lations involving rarely occurring words. E.g. 
the synonymy between snow and C has a 
higher impact on the weather eport domain 
(or the aviation domain, discussed in this pa- 
per) than the synonymy relation between co- 
caine and coke. Consequently, the precision of 
a synonym database obviously depends much 
more on frequently used relations than on 
rarely used ones. Another important consid- 
eration is that judging the  correctness of a 
given synonymy relation in a given domain is 
often an elusive issue: besides clearcut cases, 
there is a large gray area where judgments 
may not be trivial even for humans evalua- 
tots. E.g. given the following three senses of 
the noun approach 
(2) a. {approach, approach path, glide 
path, glide slope} 
(the final path followed by an air- 
craft as it is landing) 
b. {approach, approach shot} 
(a relatively short golf shot in- 
tended to put the ball onto the 
putting green) 
c. {access, approach} 
(a way of entering or leaving) 
it would be easy to judge the first and second 
senses respectively relevant and irrelevant o 
the aviation domain, but the evaluation of the 
third sense would be fuzzier. 
The combination of the two remarks above 
induced us to consider a manual pruning 
phase for the terms of highest 'weight' as a 
good investment of human effort, in terms of 
rate between the achieved increase in preci- 
sion and the amount of work involved. A 
second reason for performing an initial man- 
ual pruning is that its outcome can be used 
as a reliable test set against which automatic 
pruning algorithms can be tested. 
Based on such considerations, we included a 
manual phase in the pruning process, consist- 
ing of two steps: (i) the ranking of synonymy 
relations in terms of their weight in the spe- 
cific domain; (ii) the actual evaluation of the 
correctness of the top ranking synonymy re- 
lation, by human evaluators. 
2.2.1 Rank ing  of  synonymy re lat ions  
The goal of ranking synonymy relations is 
to associate them with a score that estimates 
how often a synonymy relation is likely to 
be used in the specific domain. The input 
database is sorted by the assigned scores, and 
the top ranking words are checked for manual 
pruning. Only terms appearing in the domain 
specific corpus are considered at this stage. 
In this way the benefit of manual pruning is 
maximized. Ranking is based on three sorting 
criteria, listed below in order of priority. 
Cr i te r ion  1. Since a term that does ap- 
pear in the domain corpus must have at least 
one valid sense in the specific domain, words 
with only one sense are not good candidates 
for pruning (under the assumption of com- 
pleteness of the synonym database). There- 
fore .polysemous terms are prioritized over 
monosemous terms. 
Cr i te r ion  2. The second and third sort- 
ing criteria axe similar, the only difference be- 
ing that the second criterion assumes the ex- 
istence of some inventory of relevant queries 
(a term list, a collection of previous queries, 
etc.), ff such an inventory is not available, the 
second sorting criterion can be omitted. If the 
inventory is available, it is used to check which 
synonymy relations are actually to be used in 
queries to the domain corpus. Given a pair 
(ti,tj) of synonym terms, a score (which we 
name scoreCQ) is assigned to their synonymy 
relation, according to the following formula: 
(3) scoreCQij = 
(fcorpusi * fqueryj) + 
(fcorpusj ? fqueryi) 
where fcorpusn and fqueryn are, respec- 
tively, the frequencies of a term in the domain 
corpus and in the inventory of query terms. 
The above formula aims at estimating how 
often a given synonymy relation is likely to 
be actually used. In particular, each half of 
the formula estimates how often a given term 
in the corpus is likely to be matched as a syn- 
onym of a given term in a query. Consider, 
e.g., the following situation (taken form the 
aviation domain discussed in section 3.1): 
(4) fcorpuSsnow = 3042 
f querysnow = 2 
fcorpusc = 9168 
f queryc = 0 
It is estimated that C would be matched 
18336 times as a synonym for snow (i.e 9168 
* 2), while snow would never be matched as 
a synonym for C, because C never occurs as 
a query term. Therefore scoreCQs,~ow,c is 
18336 (i.e. 18336 + 0). 
Then, for each polysemous term i and 
synset s such that i E s, the following score is 
computed: 
Table 1: Frequencies of sample synset erms. 
j fcorpusj fqueryj 
cocaine 1 0 
cocain 0 0 
coke 8 0 
C 9168 0 
(5) scorePolyCQ i,~ = 
E{scoreCQi,~lj ~ s A i ? j} 
E.g., i f  ,5' is the synset in (1), then 
scorePolyCQs~ow,s is "the sum of 
scoreCQsnow,coc~ine, scoreCQsnow,eocain, 
scoreCQsnow,eoke and scoreCQ,no~o,c. Given 
the data in Table 1 (taken again from our 
aviation domain) the following scoreCQ 
would result: 
(6) scoreCQsnow,cocaine -~2 
scoreCQsnow,cocain = 0 
scoreCQs~ow,cok~ = 16 
scoreCQsno~o,c = 18336 
Therefore, scorePolyCQsnow,s would equal 
18354. 
The final score assigned to each polysemous 
term tl is the highest scorePolyCQi,s. For 
snow, which has the following three senses 
(7) a. {cocaine, cocaine, coke, C, snow} 
(a narcotic (alkaloid) extracted 
from coca leaves) 
b. {snow} 
(a layer of snowflakes (white crys- 
tals of frozen water) covering the 
ground) 
c. {snow, snowfall} 
(precipitation falling from clouds 
in the form of ice crystals) 
the highest score would be the one computed 
above. 
Cr i ter ion 3. The third criterion assigns 
a score in terms of domain corpus frequency 
alone. It is used to further rank terms that 
do not occur in the query term inventory (or 
when no query term inventory is available). It 
is computed in the same way as the previous 
score, with the only difference that a value of 
1 is conventionally assumed for fquery (the 
frequency of a term in the inventory of query 
terms). 
2.2.2 Correctness  evaluat ion 
All the synsets containing the top rank- 
ing terms, according to the hierarchy of crite- 
ria described above, are manuMly checked for 
pruning. For each term, all the synsets con- 
taining the term are clustered together and 
presented to a human operator, who exam- 
ines each (term, synset) pair and answers the 
question: does the term belong to the synset 
in the specific domain? Evidence about the 
answer is drawn from relevant examples auto- 
matically extracted from the domain specific 
corpus. E.g., following up on our example in 
the previous section, the operator would be 
presented with the word snow associated with 
each of the synsets in (7) and would have to 
provide a yes/no answer for each of them. In 
the specific case, the answer would be likely 
to be 'no' for (7a) and 'yes' for (75) and (7c). 
The evaluator is presented with all the 
synsets involving a relevant term (even 
those that did not rank high in terms of 
scorePoIyCQ) in order to apply a contrastive 
approach. It might well be the case that the 
correct sense for a given term is one for which 
the term has no synonyms at all (e.g. 7b in 
the example), therefore all synsets for a given 
term need to be presented to the evaiuator 
in order to make an informed choice. The 
evaluator provides a yes/no answer for all the 
(term, synset) he/she is presented with (with 
some exceptions, as explained in section 3.1). 
2.3 Automat ic  p run ing  
The automatic pruning task is analogous to 
manual pruning in two respects: (i) its in- 
put is the set of synonymy relations involving 
WordNet polysemous words appearing in the 
domain specific orpus; (ii) it is performed by 
examining all (term, synset) input pairs and 
answering the question: does the term belong 
to the synset in the specific domain? How- 
ever, while the manual pruning task was re- 
garded as a filtering task, where a human eval- 
4 
uator assigns a boolean value to each pruning 
candidate, the automatic pruning task can 
be more conveniently regarded as a ranking 
task, where all the pruning candidates are as- 
signed a score, measuring how appropriate a 
given sense is for a given word, in the do- 
main at hand. The actual pruning is left as 
a subsequent step. Different pruning thresh- 
olds can be applied to the ranked list, based 
on different considerations (e.g. depending on 
whether astronger emphasis i  put on the pre- 
cision or the recall of the resulting database). 
The score is based on the frequencies of both 
words in the synset (except the word under 
consideration) and words in the sense gloss. 
We also remove from the gloss all words be- 
longing to a stoplist (a stoplist provided with 
WordNet was used for this purpose). The fol- 
lowing scoring formula is used: 
(8) (average_synset_frequeney/ 
synset_cardinality k) .4- 
(average_gloss_frequency~ 
gloss_cardinality :) 
Note that the synset cardinality does not 
include the word under consideration, reflect- 
ing the fact the word's frequency is not used 
in calculating the score. Therefore a synset 
only containing the word under consideration 
and no synonyms is assigned cardinality 0. 
The goal is to identify (term, sense) pairs 
not pertaining to the domain. For this rea- 
son we tend to assign high scores to candi- 
dates for which we do not have enough evi- 
dence about their inappropriateness. This is 
why average frequencies are divided by some 
factor which is function of the number of av- 
eraged frequencies, in order to increase the 
Scores based on little evidence (i.e. fewer av- 
eraged numbers). In the sample application 
described in section 3 the value of k was set 
to 2. For analogous reasons, we convention- 
ally assign a very high score to candidates for 
which we have no evidence (i.e. no words in 
both the synset and the gloss). If either the 
synset or the gloss is empty, we conventionally 
double the score for the gloss or the synset, 
respectively. We note at this point that our 
final ranking list are sorted in reverse order 
with respect o the assigned scores, since we 
are focusing on removing incorrect items. At 
the top of the list are the items that receive 
the lowest score, i.e. that are more likely to 
be incorrect (term, sense) associations for our 
domain (thus being the best candidates to be 
pruned out). 
Table 2 shows the ranking of the senses 
for the word C in the aviation domain. In 
the table, each term is followed by its corpus 
frequency, separated by a slash. From each 
synset the word C itself has been removed, 
as well as the gloss words found in the stop 
list. Therefore, the table only contains the 
words that contribute to the calculation of the 
sense's core. E.g. the score for the first sense 
in the list is obtained from the following ex- 
pression: 
(9) ((0 + 57)/2/22) + 
( (8+0+0+ 198+9559+0+1298)/7/72 ) 
The third sense in the list exemplifies the 
case of an empty synset (i.e. a synset orig- 
inally containing only the word under con- 
sideration). In this case the score obtained 
from the gloss is doubled. Note that the ob- 
viously incorrect sense of C as a narcotic is 
in the middle of the list. This is due to a tag- 
ging problem, as the word leaves in the gloss 
was tagged as verb instead of noun. Therefore 
it was assigned a very high frequency, as the 
verb leave, unlike the noun leaf, is very com- 
mon in the aviation domain. The last sense 
in the list also requires a brief explanation. 
The original word in the gloss was 10S. How- 
ever, the pre-processor that was used before 
tagging the glosses recognized S as an abbre- 
viation for South and expanded the term ac- 
cordingly. It so happens that both words 10 
and South are very frequent in the aviation 
corpus we used, therefore the sense was as- 
signed a high score. 
2.4 Optimization 
The aim of this phase is to improve the access 
speed to the synonym database, by removing 
all information that is not likely to be used. 
The main idea is to minimize the size of the 
Score 
Table 2: Ranking of synsets containing the word C 
Frequencies 
39.37 
62.75 
224.28 
synset: 
gloss: 
synset: 
gloss: 
synset: 
gloss: 
241.69 synset: 
gloss: 
585.17 synset: 
gloss: 
743.28 synset: 
gloss: 
1053.43 synset: 
gloss: 
ATOMIC_NUMBEK_6/O, CAKBON/57 
ABUNDANT/8, NONMETALLIC/O, TETRAVALENT/O, ELEMENT/198 
0CCUR/9559, ALLOTROPIC/O, FOKM/1298 
AMPEre-SECOND/O, COULOMB/O 
UNIT/3378, ELECTRICAL/2373, CHARGE/523, EQUAL/153 
AMOUNT/1634, CHARGE/523, TKANSFEK/480, CUKKENT/242, 1/37106 
AMPEre/4, 1/37106 
0 
GENEKAL-PUKPOSE/O, PROGRAMING/O, LANGUAGE/445, CLOSELY/841 
ASSOCIATE/543, UNIX/O, OPEKATE/5726, SYSTEM/49863 
COCAIN/O, COCAINE/i, COKE/8, SNOW/3042 
NARCOTIC/i, ALKALOID/O, EXTKACT/31, COCA/I, LEAVE/24220 
LIGHT_SPEED/I, SPEED_OF_LIGHT/O 
SPEED/14665, LIGHT/22481, TRAVEL/f05, VACUUM/192 
DEGREE_CELSIUS/24, DEGREEiENTIGRADE/28 
DEGKEE/43617, CENTIGRADE/34, SCALE/540, TEMPERATURE/2963 
I00/0, CENTRED/O, CENTUKY/31, HUNDRED/O, ONE_C/O 
TEN/Z3, 10/16150, SOUTH/12213 
database in such a way that the database be- 
havior remains unchanged. Two operations 
are performed at the stage: (i) a simple rel- 
evance  tes t  to remove irrelevant erms (i.e. 
terms not pertaining to the domain at hand); 
(ii) a redundancy check, to remove informa- 
tion that, although perhaps relevant, does not 
affect the database behavior. 
2.4.1 Re levance  tes t  
Terms not appearing in the domain cor- 
pus are considered not relevant o the spe- 
cific domain and removed from the synonym 
database. The rationale underlying this step 
is to remove from the synonym database syn- 
onymy relations that are never going to be 
used in the specific domain. In this way the ef- 
ficiency of the module can be increased, by re- 
ducing the size of the database and the num- 
ber of searches performed (synonyms that are 
known to never appear are not searched for), 
without affecting the system's matching at- 
curacy. E.g., the synset in (10a) would be 
reduced to the synset in (10b). 
(10) a. AMPERE-SECOND/O, COULOMB/O, 
C/9168 
b. C/9168 
2.4.2 Redundancy  check 
The final step is the removal of redundant 
synsets, possibly as a consequence of the pre- 
vious pruning steps. Specifically, the follow- 
ing synsets are removed: 
? Synsets containing a single term (al- 
though the associated sense might be a 
valid one for that term, in the specific 
domain). 
? Duplicate synsets, i.e. identical (in terms 
of synset elements) to some other synset 
not being removed (the choice of the only 
synset o be preserved is arbitrary). 
E.g., the synset in (10b) would be finMly 
removed at this stage. 
3 Sample  app l i ca t ion  
The described methodology was applied to 
the aviation domain. We used the Aviation 
Safety Information System (ASRS) corpus 
(h 'e tp : / /as rs .  a rc .nasa .gov / )  as our avia- 
tion specific corpus. The resulting domain- 
specific database is being used in an IR ap- 
plication that retrieves documents relevant 
to user defined queries, expressed as phrase 
patterns, and identifies portions of text that 
are instances of the relevant phrase patterns. 
The application makes use of Natural Lan- 
guage Processing (NLP) techniques (tagging 
and partial parsing) to annotate documents. 
User defined queries are matched against such 
annotated corpora. Synonyms are used to 
expand occurrences of specific words in such 
queries. In the following two sections we de- 
scribe how the pruning process was performed 
and provide some results. 
3.1 Adapt ing  Wordnet  to the  
av iat ion  domain  
A vocabulary of relevant query terms was 
made available by a user of our IR applica- 
tion and was used in our ranking of synonymy 
relations. Manual pruning was performed on 
the 1000 top ranking terms, with which 6565 
synsets were associated overall. The manual 
pruning task was split between two human 
evaluators. The evaluators were programmers 
members of our staff. They were English na- 
tive speakers who had acquaintance with our 
IR application and with the goals of the man- 
ual pruning process, but no specific training 
or background on lexicographic or WordNet- 
related tasks. For each of the 1000 terms, 
the evaluators were provided with a sample 
of 100 (at most) sentences where the rele- 
vant word occurred in the ASRS corpus. 100 
of the 1000 manually checked clusters (i.e. 
groups of synsets referring to the same head 
term) were submitted to both evaluators (576 
synsets overall), in order to check the rate 
of agreement of their evaluations. The eval- 
uators were allowed to leave synsets unan- 
swered, when the synsets only contained the 
head term (and at least one other synset in 
the cluster had been deemed correct). Leav- 
ing out the cases when one or both evalua- 
tors skipped the answer, there remained 418 
synsets for which both answered. There was 
agreement in 315 cases (75%) and disagree- 
ment in 103 cases (25%). A sample of senses 
on which the evaluators disagreed is shown in 
(11). In each case, the term being evaluated 
is the first in the synset. 
(11) a. {about, around} 
(in the area or vicinity) 
b. {accept, admit, take, take on} 
(admit into a group or commu- 
nity) 
c. {accept, consent, go for} 
(give an affirmative reply to) 
d. {accept, swallow} 
(tolerate or accommodate oneself 
to) 
e. {accept, take} 
(be designed to hold or take) 
f. {accomplished, effected, estab- 
lished} 
(settled securely and uncondi- 
tionally) 
g. {acknowledge, know, recognize} 
(discern) 
h. {act, cognitive operation, cogni- 
tive process, operation, process} 
(the performance of some com- 
posite cognitive activity) 
i. {act, act as, play} 
(pretend to have certain qualities 
or state of mind) 
j. {action, activeness, activity} 
(the state of being active) 
k. {action, activity, natural action, 
natural process} 
(a process existing in or produced 
by nature (rather than by the in- 
tent of human beings)) 
It should be noted that the 'yes' and 'no' 
answers were not evenly distributed between 
the evaluators. In 80% of the cases of dis- 
agreement, i  was evaluator A answering 'yes' 
and evaluator B answering 'no'. This seems 
to suggest han one of the reasons for dis- 
agreement was a different degree of strictness 
in evaluating. Since the evaluators matched 
a sense against an entire corpus (represented 
by a sample of occurrences), one common sit- 
uation may have been that a sense did oc- 
cur, but very rarely. Therefore, the evaluators 
may have applied different criteria in judging 
how many occurrences were needed to deem 
a sense correct. This discrepancy, of course, 
may compound with the fact that the differ- 
ences among WordNet senses can sometimes 
be very subtle. 
Automatic pruning was performed on 
the entire WordNet database, regardless of 
whether candidates had already been manu- 
ally checked or not. This was done for test- 
ing purposes, in order to check the results of 
automatic pruning against the test set ob- 
tained from manual pruning. Besides asso- 
ciating ASRS frequencies with all words in 
synsets and glosses, we also computed fre- 
quencies for collocations (i.e. multi-word 
terms) appearing in synsets. The input to 
automatic pruning was constituted by 10352 
polysemous terms appearing at least once in 
ASRS the corpus. Such terms correspond to 
37494 (term, synset) pairs. Therefore, the 
latter was the actual number of pruning can- 
didates that  were ranked. 
The check of WordNet senses against ASRS 
senses was only done unidirectionally, i.e. 
we only checked whether WordNet senses 
were attested in ASRS. Although it would 
be interesting to see how often the appropri- 
ate, domain-specific senses were absent from 
WordNet, no check of this kind was done. We 
took the simplifying assumption that Word- 
Net be complete, thus aiming at assigning at 
least one WordNet sense to each term that 
appeared in both WordNet and ASRS. 
3.2 Resu l ts  
In order to test the automatic pruning per- 
formance, we ran the ranking procedure on 
a test set taken from the manually checked 
files. This file had been set apart and had 
not been used in the preliminary tests on the 
automatic pruning algorithm. The test set 
included 350 clusters, comprising 2300 candi- 
dates. 1643 candidates were actually assigned 
an evaluation during manual pruning. These 
were used for the test. We extracted the 1643 
relevant items from our ranking list, then we 
incrementally computed precision and recall 
in terms of the items that had been manually 
checked by our human evaluators. The re- 
sults are shown in figure 1. As an example of 
how this figure can be interpreted, taking into 
consideration the top 20% of the ranking list 
(along the X axis), an 80% precision (Y axis) 
means that 80% of the items encountered so 
far had been removed in manual pruning; a 
27% recall (Y axis) means that 27% of the 
overall manually removed items have been en- 
countered so far. 
The automatic pruning task was intention- 
ally framed as a ranking problem, in order to 
leave open the issue of what pruning threshold 
would be optimal. This same approach was 
taken in the IR application in which the prun- 
ing procedure was embedded. Users are given 
the option to set their own pruning threshold 
(depending on whether they focus more on 
precision or recall), by setting a value spec- 
ifying what precision they require. Pruning 
is performed on the top section of the rank- 
ing list that guarantees the required precision, 
according to the correlation between precision 
and amount of pruning shown in figure 1. 
A second test was designed to check 
whether there is a correlation between the 
levels of confidence of automatic and man- 
ual pruning. For this purpose we used the 
file that had been manually checked by both 
human evaiuators. We took into account he 
candidates that had been removed by at least 
one evaluator: the candidates that were re- 
moved by both evaluators were deemed to 
have a high level of confidence, while those 
removed by only one evaluator were deemed 
to have a lower level of confidence. Then we 
checked whether the two classes were equally 
distributed in the automatic pruning ranking 
list, or whether higher confidence candidates 
tended to be ranked higher than lower con- 
fidence ones. The results are shown in fig- 
ure 2, where the automatic pruning recall for 
each class is shown. For any given portion 
of the ranking list higher confidence candi- 
dates (solid lines) have a significantly higher 
recall than lower confidence candidates (dot- 
Table 3: WordNet optimization results. 
DB Synsets Word-senses 
Full WN 99,642 174,008 
Reduced WN 9,441 23,368 
ted line). 
Finally, table 3 shows the result of applying 
the described optimization techniques alone, 
i.e. without any prior pruning, with respect 
to the ASRS corpus. The table shows how 
many synsets and how many word-senses are 
contained in the full Wordnet database and in 
its optimized version. Note that such reduc- 
tion does not involve any loss of accuracy. 
4 Conc lus ions  
There is a need for automatically or semi- 
automatically adapting NLP components o 
specific domain, if such components are to be 
effectively used in IR applications without in- 
volving labor-intensive manual adaptation. A 
key part of adapting NLP components ospe- 
cific domains is the adaptation of their lexical 
and terminological resources. It may often be 
the case that a consistent section of a general 
purpose terminological resource is irrelevant 
to a specific domain, thus involving an unnec- 
essary amount of ambiguity that affects both 
the accuracy and efficiency of the overall NLP 
component. In this paper we have proposed 
a method for adapting a general purpose syn- 
onym database to a specific domain. 
Evaluating the performance of the pro- 
posed pruning method is not a straightfor- 
ward task, since there are no other results 
available on a similar task, to the best of our 
knowledge. However, a comparison between 
the results of manual and automatic pruning 
provides ome useful hints. In particular: 
? The discrepancy between the evaluation 
of human operators hows that the task 
is elusive even for humans (the value of 
the agreement evaluation statistic n for 
our human evaluators was 0.5); 
? however, the correlation between the 
level of confidence of human evaluations 
and scores assigned by the automatic 
pruning procedure shows that the auto- 
matic pruning algorithm captures ome 
significant aspect of the problem. 
Although there is probably room for im- 
proving the automatic pruning performance, 
the preliminary results how that the current 
approach is pointing in the right direction. 
Re ferences  
Christiane Fellbaum, editor. 1998. Wordnet: An 
Electronic Lexical Database. MIT Press Books. 
Claudia Leacock, Martin Chodorow, and 
George A. Miller. 1998. Using corpus tatistics 
and WordNet relations for sense identification. 
Computational Linguistics, 24(1):147-165. 
Bernardo Magnini and Gabriela Cavaglih. 2000. 
Integrating Subject Field Codes into WordNet. 
In Maria Gavrilidou, George Carayannis, Stella 
Markantonatou, Stelios Piperidis, and Gregory 
Stainhaouer, editors, Proceedings of the Sec- 
ond International Conference on Language Re- 
sources and Evaluation (LREC-PO00), pages 
1413-1418, Athens, Greece. 
Mark Sanderson. 1997. Word Sense Disambigua- 
tion and Information Retrieval. Ph.D. thesis, 
Department ofComputing Science at the Uni- 
versity of Glasgow, Glasgow G12. Technical 
Report (TR-1997-7). 
Ellen M. Voorhees. 1998. Using WordNet for text 
retrieval. In Fellbaum (Fellbaum, 1998), chap- 
ter 12, pages 285-303. 
9 
~2 
~9 
100 i I I I 
95 
90 
85 
+? i 
75 
70 
65 
60 
55 
0 
100 
80 
60 
40 
20 
0 I I I I 
0 20 40 60 80 100 
Top % of ranking list 
F igure 1: Precision and recall of automat ic  pruning 
10 
~9 
cg 
100 
80 
60 
40 
20 
I I I I t - - ' - /  
-- _ t / f  j" _ 
-- r l  j --  
/ J _  ; _ 
- - J "  I I I I 
0 20 40 60 80 100 
Top % of ranking list 
Figure 2: A recall comparison for different confidence rates 
11 
