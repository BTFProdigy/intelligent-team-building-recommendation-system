Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 159?167,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Animacy Annotation in the Hindi Treebank
Itisree Jena, Riyaz Ahmad Bhat, Sambhav Jain and Dipti Misra Sharma
Language Technologies Research Centre, IIIT-Hyderabad, India
{itisree|riyaz.bhat|sambhav.jain}@research.iiit.ac.in, dipti@iiit.ac.in
Abstract
In this paper, we discuss our efforts to anno-
tate nominals in the Hindi Treebank with the
semantic property of animacy. Although the
treebank already encodes lexical information
at a number of levels such as morph and part
of speech, the addition of animacy informa-
tion seems promising given its relevance to
varied linguistic phenomena. The suggestion
is based on the theoretical and computational
analysis of the property of animacy in the con-
text of anaphora resolution, syntactic parsing,
verb classification and argument differentia-
tion.
1 Introduction
Animacy can either be viewed as a biological prop-
erty or a grammatical category of nouns. In a
strictly biological sense, all living entities are ani-
mate, while all other entities are seen as inanimate.
However, in its linguistic sense, the term is syn-
onymous with a referent?s ability to act or instigate
events volitionally (Kittila? et al, 2011). Although
seemingly different, linguistic animacy can be im-
plied from biological animacy. In linguistics, the
manifestation of animacy and its relevance to lin-
guistic phenomena have been studied quite exten-
sively. Animacy has been shown, cross linguisti-
cally, to control a number of linguistic phenomena.
Case marking, argument realization, topicality or
discourse salience are some phenomena, highly cor-
related with the property of animacy (Aissen, 2003).
In linguistic theory, however, animacy is not seen
as a dichotomous variable, rather a range capturing
finer distinctions of linguistic relevance. Animacy
hierarchy proposed in Silverstein?s influential arti-
cle on ?animacy hierarchy? (Silverstein, 1986) ranks
nominals on a scale of the following gradience: 1st
pers> 2nd pers> 3rd anim> 3rd inanim. Several such
hierarchies of animacy have been proposed follow-
ing (Silverstein, 1986), one basic scale taken from
(Aissen, 2003) makes a three-way distinction as hu-
mans > animates > inanimates. These hierarchies can
be said to be based on the likelihood of a referent of
a nominal to act as an agent in an event (Kittila? et
al., 2011). Thus higher a nominal on these hierar-
chies higher the degree of agency/control it has over
an action. In morphologically rich languages, the
degree of control/agency is expressed by case mark-
ing. Case markers capture the degree of control a
nominal has in a given context (Hopper and Thomp-
son, 1980; Butt, 2006). They rank nominals on the
continuum of control as shown in (1)1. Nominals
marked with Ergative case have highest control and
the ones marked with Locative have lowest.
Erg > Gen > Inst > Dat > Acc > Loc (1)
Of late the systematic correspondences between
animacy and linguistic phenomena have been ex-
plored for various NLP applications. It has been
noted that animacy provides important informa-
tion, to mention a few, for anaphora resolution
(Evans and Orasan, 2000), argument disambiguation
(Dell?Orletta et al, 2005), syntactic parsing (?vre-
lid and Nivre, 2007; Bharati et al, 2008; Ambati et
al., 2009) and verb classification (Merlo and Steven-
1Ergative, Genitive, Instrumental, Dative, Accusative and
Locative in the given order.
159
son, 2001). Despite the fact that animacy could play
an important role in NLP applications, its annota-
tion, however, is not usually featured in a treebank
or any other annotated corpora used for developing
these applications. There are a very few annotation
projects that have included animacy in their anno-
tation manual, following its strong theoretical and
computational implications. One such work, mo-
tivated by the theoretical significance of the prop-
erty of animacy, is (Zaenen et al, 2004). They
make use of a coding scheme drafted for a para-
phrase project (Bresnan et al, 2002) and present
an explicit annotation scheme for animacy in En-
glish. The annotation scheme assumes a three-way
distinction, distinguishing Human, Other animates
and Inanimates. Among the latter two categories
?Other animates? is further sub-categorized into
Organizations and Animals, while the category of
?Inanimates? further distinguishes between con-
crete and non-concrete, and time and place nomi-
nals. As per the annotation scheme, nominals are
annotated according to the animacy of their referents
in a given context. Another annotation work that
includes animacy for nominals is (Teleman, 1974),
however, the distinction made is binary between hu-
man and non-human referents of a nominal in a
given context. In a recent work on animacy annota-
tion, Thuilier et al (2012) have annotated a multi-
source French corpora with animacy and verb se-
mantics, on the lines of (Zaenen et al, 2004). Apart
from the manual annotation for animacy, lexical re-
sources like wordnets are an important source of this
information, if available. These resources usually
cover animacy, though indirectly (Fellbaum, 2010;
Narayan et al, 2002). Although a wordnet is an
easily accessible resource for animacy information,
there are some limitations on its use, as discussed
below:
1. Coverage: Hindi wordnet only treats common
nouns while proper nouns are excluded (except
famous names) see Table 1. The problem is se-
vere where the domain of text includes more
proper than common nouns, which is the case
with the Hindi Treebank as it is annotated on
newspaper articles.
2. Ambiguity: Since words can be ambiguous, the
animacy listed in wordnet can only be used in
presence of a high performance word sense dis-
ambiguation system. As shown in Table 2, only
38.02% of nouns have a single sense as listed in
Hindi Wordnet.
3. Metonymy or Complex Types: Domains like
newspaper articles are filled with metonymic
expressions like courts, institute names, coun-
try names etc, that can refer to a building, a ge-
ographical place or a group of people depend-
ing on the context of use. These words are not
ambiguous per se but show different aspects of
their semantics in different contexts (logically
polysemous). Hindi wordnet treats these types
of nouns as inanimate.
Nominals in HTB Hindi WordNet Coverage
78,136 65,064 83.27%
Table 1: Coverage of Hindi WordNet on HTB Nominals.
HTB Nominals Single Unique Sense
with WN Semantics in Hindi WordNet
65,064 24,741 (38.02%)
Table 2: Nominals in HTB with multiple senses
Given these drawbacks, we have included ani-
macy information manually in the annotation of the
Hindi Treebank, as discussed in this work. In the
rest, we will discuss the annotation of nominal ex-
pressions with animacy and the motivation for the
same, the discussion will follow as: Section 2 gives
a brief overview of the Hindi Treebank with all its
layers. Section 3 motivates the annotation of nom-
inals with animacy, followed by the annotation ef-
forts and issues encountered in Section 4. Section
5 concludes the paper with a discussion on possible
future directions.
2 Description of the Hindi Treebank
In the following, we give an overview of the Hindi
Treebank (HTB), focusing mainly on its dependency
layer. The Hindi-Urdu Treebank (Palmer et al,
2009; Bhatt et al, 2009) is a multi-layered and
multi-representational treebank. It includes three
levels of annotation, namely two syntactic levels and
one lexical-semantic level. One syntactic level is a
dependency layer which follows the CPG (Begum
160
et al, 2008), inspired by the Pa?n. inian grammati-
cal theory of Sanskrit. The other level is annotated
with phrase structure inspired by the Chomskyan ap-
proach to syntax (Chomsky, 1981) and follows a bi-
nary branching representation. The third layer of an-
notation, a purely lexical semantic one, encodes the
semantic relations following the English PropBank
(Palmer et al, 2005).
In the dependency annotation, relations are
mainly verb-centric. The relation that holds between
a verb and its arguments is called a kar.aka relation.
Besides kar.aka relations, dependency relations also
exist between nouns (genitives), between nouns and
their modifiers (adjectival modification, relativiza-
tion), between verbs and their modifiers (adver-
bial modification including subordination). CPG
provides an essentially syntactico-semantic depen-
dency annotation, incorporating kar.aka (e.g., agent,
theme, etc.), non-kar.aka (e.g. possession, purpose)
and other (part of) relations. A complete tag set of
dependency relations based on CPG can be found in
(Bharati et al, 2009), the ones starting with ?k? are
largely Pa?n. inian kar.aka relations, and are assigned
to the arguments of a verb. Figure 1 encodes the de-
pendency structure of (5), the preterminal node is a
part of speech of a lexical item (e.g. NN,VM, PSP).
The lexical items with their part of speech tags are
further grouped into constituents called chunks (e.g.
NP, VGF) as part of the sentence analysis. The de-
pendencies are attached at the chunk level, marked
with ?drel? in the SSF format. k1 is the agent of
an action (KAyA ?eat?), whereas k2 is the object or
patient.
(5) s\@yA n
Sandhya-Erg
sb
apple-Nom
KAyA
eat-Perf
?
?Sandhya ate an apple.?
<Sentence id = ?1?>
Offset Token Tag Feature structure
1 (( NP <fs name=?NP? drel=?k1:VGF?>
1.1 s\@yA NNP<fs af=?s\@yA,n,f,sg,3,o,0,0?>
1.2 n PSP <fs af=?n,psp,,,,,,?>
))
2 (( NP <fs name=?NP2? drel=?k2:VGF?>
2.1 sb NN <fs af=?sb,n,m,sg,3,d,0,0?>
))
3 (( VGF<fs name=?VGF?>
3.1 KAyA VM <fs af=?KA,v,m,sg,any,,yA,yA?>
))
</Sentence>
Figure 1: Annotation of an Example Sentence in SSF.
Despite the fact that the Hindi Treebank already
features a number of layers as discussed above, there
have been different proposals to enrich it further.
Hautli et al (2012) proposed an additional layer to
the treebank, for the deep analysis of the language,
by incorporating the functional structure (or
f-structure) of Lexical Functional Grammar which
encodes traditional syntactic notions such as sub-
ject, object, complement and adjunct. Dakwale et
al. (2012) have also extended the treebank with
anaphoric relations, with a motive to develop a data
driven anaphora resolution system for Hindi. Given
this scenario, our effort is to enrich the treebank
with the animacy annotation. In the following
sections, we will discuss in detail, the annotation of
the animacy property of nominals in the treebank
and the motive for the same.
3 Motivation: In the Context of
Dependency Parsing
Hindi is a morphologically rich language, gram-
matical relations are depicted by its morphology
via case clitics. Hindi has a morphologically
split-ergative case marking system (Mahajan, 1990;
Dixon, 1994). Case marking is dependent on the
aspect of a verb (progressive/perfective), transitivity
(transitive/intransitive) and the type of a nominal
(definite/indefinite, animate/inanimate). Given
this peculiar behavior of case marking in Hindi,
arguments of a verb (e.g. transitive) have a number
of possible configurations with respect to the case
marking as shown in the statistics drawn from
the Hindi Treebank released for MTPIL Hindi
Dependency parsing shared task (Sharma et al,
2012) in Table 3. Almost in 15% of the transitive
clauses, there is no morphological case marker on
any of the arguments of a verb which, in the context
of data driven parsing, means lack of an explicit
cue for machine learning. Although, in other cases
there is a case marker, at least on one argument of a
verb, the ambiguity in case markers (one-to-many
mapping between case markers and grammatical
functions as presented in Table 4) further worsens
the situation (however, see Ambati et al (2010) and
Bhat et al (2012) for the impact of case markers on
parsing Hindi/Urdu). Consider the examples from
161
(6a-e), the instrumental se is extremely ambiguous.
It can mark the instrumental adjuncts as in (6a),
source expressions as in (6b), material as in (6c),
comitatives as in (6d), and causes as in (6e).
K2-Unmarked K2-Marked
K1-Unmarked 1276 741
K1-Marked 5373 966
Table 3: Co-occurrence of Marked and Unmarked verb argu-
ments (core) in HTB.
n/ne ko/ko s/se m\/meN pr/par kA/kaa
(Ergative) (Dative) (Instrumental) (Locative) (Locative) (Genitive)
k1(agent) 7222 575 21 11 3 612
k2(patient) 0 3448 451 8 24 39
k3(instrument) 0 0 347 0 0 1
k4(recipient) 0 1851 351 0 1 4
k4a(experiencer) 0 420 8 0 0 2
k5(source) 0 2 1176 12 1 0
k7(location) 0 1140 308 8707 3116 19
r6(possession) 0 3 1 0 0 2251
Table 4 : Distribution of case markers across case function.
(6a) mohn n
Mohan-Erg
cAbF s
key-Inst
taAlA
lock-Nom
KolA
open
?
?Mohan opened the lock with a key.?
(6b) gFtaA n
Geeta-Erg
Ed?F s
Delhi-Inst
sAmAn
luggage-Nom
m\gvAyA
procure
?
?Geeta procured the luggage from Delhi.?
(6c) m EtakAr n
sculptor-Erg
p(Tr s
stone-Inst
m Eta
idol-Nom
bnAyF
make
?
?The sculptor made an idol out of stone.?
(6d) rAm kF
Ram-Gen
[yAm s
Shyaam-Inst
bAta
talk-Nom
h  I
happen
?
?Ram spoke to Shyaam.?
(6e) bAErf s
rain-Inst
kI Psl\
many crops-Nom
tabAh
destroy
ho gyF\
happen-Perf
?
?Many crops were destroyed due to the rain.?
(7) EcEwyA
bird-Nom
dAnA
grain-Nom
c  g rhF h{
devour-Prog
?
?A bird is devouring grain.?
A conventional parser has no cue for the disam-
biguation of instrumental case marker se in exam-
ples (6a-e) and similarly, in example (7), it?s hard
for the parser to know whether ?bird? or ?grain? is
the agent of the action ?devour?. Traditionally, syn-
tactic parsing has largely been limited to the use
of only a few lexical features. Features like POS-
tags are way too coarser to provide deep informa-
tion valuable for syntactic parsing while on the other
hand lexical items often suffer from lexical ambi-
guity or out of vocabulary problem. So in oder to
assist the parser for better judgments, we need to
complement the morphology somehow. A careful
observation easily states that a simple world knowl-
edge about the nature (e.g. living-nonliving, arti-
fact, place) of the participants is enough to disam-
biguate. For Swedish, ?vrelid and Nivre (2007) and
?vrelid (2009) have shown improvement, with an-
imacy information, in differentiation of core argu-
ments of a verb in dependency parsing. Similarly
for Hindi, Bharati et al (2008) and Ambati et al
(2009) have shown that even when the training data
is small simple animacy information can boost de-
pendency parsing accuracies, particularly handling
the differentiation of core arguments. In Table 5,
we show the distribution of animacy with respect to
case markers and dependency relations in the anno-
tated portion of the Hindi Treebank. The high rate
of co-occurrence between animacy and dependency
relations makes a clear statement about the role an-
imacy can play in parsing. Nominals marked with
dependency relations as k1 ?agent?, k4 ?recipient?,
k4a ?experiencer? are largely annotated as human
while k3 ?instrument? is marked as inanimate,
which confirms our conjecture that with animacy
information a parser can reliably predict linguistic
patterns. Apart from parsing, animacy has been re-
ported to be beneficial for a number of natural lan-
guage applications (Evans and Orasan, 2000; Merlo
and Stevenson, 2001). Following these computa-
tional implications of animacy, we started encoded
this property of nominals explicitly in our treebank.
In the next section, we will present these efforts fol-
162
lowed by the inter-annotator agreement studies.
Human Other-Animates Inanimate
k1
n/ne (Erg) 2321 630 108
ko/ko (Dat/Acc) 172 8 135
s/se (Inst) 6 0 14
m\/me (Loc) 0 0 7
pr/par (Loc) 0 0 1
kA/kaa (Gen) 135 2 99
? (Nom) 1052 5 3072
k2
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 625 200 226
s/se (Inst) 67 0 88
m\/me (Loc) 2 0 6
pr/par (Loc) 5 0 37
kA/kaa (Gen) 15 0 14
? (Nom) 107 61 2998
k3
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 0 0 0
s/se (Inst) 2 0 199
m\/me (Loc) 0 0 0
pr/par (Loc) 0 0 0
kA/kaa (Gen) 0 0 0
? (Nom) 0 0 20
k4
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 597 0 13
s/se (Inst) 53 0 56
m\/me (Loc) 0 0 0
pr/par (Loc) 0 0 0
kA/kaa (Gen) 0 0 0
? (Nom) 7 0 8
k4a
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 132 0 8
s/se (Inst) 4 0 2
m\/me (Loc) 0 0 0
pr/par (Loc) 0 0 0
kA/kaa (Gen) 1 0 0
? (Nom) 56 0 1
k5
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 0 0 0
s/se (Inst) 7 0 460
m\/me (Loc) 0 0 1
pr/par (Loc) 0 0 0
kA/kaa (Gen) 0 0 0
? (Nom) 0 0 2
k7
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 4 0 0
s/se (Inst) 3 0 129
m\/me (Loc) 0 1977 1563
pr/par (Loc) 66 0 1083
kA/kaa (Gen) 0 0 8
? (Nom) 5 0 1775
r6
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 0 0 0
s/se (Inst) 1 0 0
m\/me (Loc) 0 0 0
pr/par (Loc) 0 0 0
kA/kaa (Gen) 156 80 605
? (Nom) 13 3 25
Table 5: Distribution of semantic features with respect
to case markers and dependency relations a.
ak1 ?agent?, k2 ?patient?, k3 ?instrument?, k4 ?recipient?,
k4a ?experiencer?, k5 ?source?, k7 ?location?, r6 ?possession?
4 Animacy Annotation
Following Zaenen et al (2004), we make a three-
way distinction, distinguishing between Human,
Other Animate and In-animate referents of a
nominal in a given context. The animacy of a ref-
erent is decided based on its sentience and/or con-
trol/volitionality in a particular context. Since, pro-
totypically, agents tend to be animate and patients
tend to be inanimate (Comrie, 1989), higher ani-
mates such as humans, dogs etc. are annotated as
such in all contexts since they frequently tend to be
seen in contexts of high control. However, lower
animates such as insects, plants etc. are anno-
tated as ?In-animate? because they are ascribed
less or no control in human languages like inan-
imates (Kittila? et al, 2011). Non-sentient refer-
ents, except intelligent machines and vehicles, are
annotated as ?In-animate? in all contexts. Intel-
ligent machines like robots and vehicles, although,
lack any sentience, they possess an animal like be-
havior which separates them from inanimate nouns
with no animal resemblance, reflected in human lan-
guage as control/volitionality. These nouns unlike
humans and other higher animates are annotated as
per the context they are used in. They are anno-
tated as ?Other animate? only in their agentive
roles. Nominals that vary in sentience in varying
contexts are annotated based on their reference in a
given context as discussed in Subsection 4.2. These
nominals include country names referring to geo-
graphical places, teams playing for the country, gov-
ernments or their inhabitants; and organizations in-
cluding courts, colleges, schools, banks etc. Un-
like Zaenen et al (2004) we don?t further categorize
?Other Animate? and ?In-animate? classes. We
163
don?t distinguish between Organizations and Ani-
mals in ?Other Animate? and Time and Place in
?In-animates?.
The process of animacy annotation in the Hindi
Treebank is straight forward. For every chunk in a
sentence, the animacy of its head word is captured
in an ?attribute-value? pair in SSF format, as
shown in Figure 3. Hitherto, around 6485 sentence,
of the Hindi Treebank, have been annotated with
the animacy information.
<Sentence id = ?1?>
Offset Token Tag Feature structure
1 (( NP <fs name=?NP? drel=?k1:VGF?
semprop=?human?>
1.1 mohn NNP <fs af=?mohn,n,m,sg,3,d,0,0?>
1.2 n PSP <fs af=?n,psp,,,,,,? name=?n?>
))
2 (( NP <fs name=?NP2? drel=?k4:VGF?
semprop=?other-animate?>
2.1 Eb?F NN <fs af=?Eb?F,n,f,sg,3,d,0,0?>
2.2 ko PSP <fs af=?ko,psp,,,,,,? name=?ko?>
))
3 (( NP <fs name=?NP3? drel=?k3:VGF?
semprop=?inanimate?>
3.1 botal NN <fs af=?botal ,n,f,sg,3,d,0,0?>
3.2 s PSP <fs af=?s,psp,,,,,,?>
))
4 (( NP <fs name=?NP4? drel=?k2:VGF?
semprop=?inanimate?>
4.1 d D NN <fs af=?d D,n,m,sg,3,d,0,0?>
))
5 (( VGF <fs name=?VGF?>
5.1 EplAyA VM <fs af=?EplA,v,m,sg,any,,yA,yA?>
))
</Sentence>
Figure 3: Semantic Annotation in SSF.
(8) mohn n
Mohan-Erg
Eb?F ko
cat-Dat
botal s
bottle-Inst
d D
milk-Nom
EplAyA
drink-Perf
?
?Mohan fed milk to the cat with a bottle.?
In the following, we discuss some of the interest-
ing cross linguistic phenomena which added some
challenge to the annotation.
4.1 Personification
Personification is a type of meaning extension
whereby an entity (usually non-human) is given
human qualities. Personified expressions are an-
notated, in our annotation procedure, as Human,
since it is the sense they carry in such contexts.
However, to retain their literal sense, two attributes
are added. One for their context bound sense
(metaphorical) and the other for context free sense
(literal). In example (9), waves is annotated with
literal animacy as In-animante and metaphoric
animacy as Human, as shown in Figure 4 (offset
2).
<Sentence id = ?1?>
Offset Token Tag Feature structure
1 (( NP <fs name=?NP? drel=?k7p:VGF? >
1.1 sAgr NNC <fs af=?sAgr,n,m,sg,3,d,0,0?>
1.2 taV NN <fs af=?taV,n,m,sg,3,d,0,0?>
1.3 pr PSP <fs af=?pr,psp,,,,,,?>
))
2 (( NP <fs name=?NP2? drel=?k1:VGF?
semprop=?inanimate?
metaphoric=?human?>
2.1 lhr\ NN <fs af=?lhr\,n,f,pl,3,d,0,0?>
))
3 (( VGF <fs name=?VGF?>
3.1 nAc VM <fs af=?nAc,v,any,any,any,,0,0?>
3.2 rhF VAUX <fs af=?rhF,v,f,sg,any,,ya,ya?>
3.3 h{\ AUX <sf AF=?h{\,v,any,pl,1,,he,he?>
))
</Sentence>
Figure 4: Semantic Annotation in SSF.
(9) sAgr taV pr
sea coast-Loc
lhr\
waves-Nom
nAc rhF h{\
dance-Prog
?
?Waves are dancing on the sea shore.?
4.2 Complex Types
The Hindi Treebank in largely built on newspa-
per corpus. Logically polysemous expressions
(metonymies) such as government, court,
newspaper etc. are very frequent in news re-
porting. These polysemous nominals can exhibit
contradictory semantics in different contexts. In
example (10a), court refers to a person (judge) or
a group of persons (jury) while in (10b) it is a
building (see Pustejovsky (1996) for the semantics
of complex types). In our annotation procedure,
such expressions are annotated as per the sense or
reference they carry in a given context. So, in case
of (10a) court will be annotated as Human while
in (10b) it will be annotated as In-animante.
(10a) adAlta n
court-Erg
m  kdm kA
case-Gen
P{\slA
decision-Nom
s  nAyA
declare-Perf
?
?The court declared its decision on the case.?
164
(10b) m{\
I-Nom
adAlta m\
court-Loc
h ?
be-Prs
?I am in the court.?
4.3 Inter-Annotator Agreement
We measured the inter-annotator agreement on a
set of 358 nominals (?50 sentences) using Cohen?s
kappa. We had three annotators annotating the same
data set separately. The nominals were annotated
in context i.e., the annotation was carried consider-
ing the role and reference of a nominal in a partic-
ular sentence. The kappa statistics, as presented in
Table 6, show a significant understanding of anno-
tators of the property of animacy. In Table 7, we
report the confusion between the annotators on the
three animacy categories. The confusion is high for
?Inanimate? class. Annotators don?t agree on this
category because of its fuzziness. As discussed ear-
lier, although ?Inanimate? class enlists biologically
inanimate entities, some entities may behave like an-
imates in some contexts. They may be sentient and
have high linguistic control in some contexts. The
difficulty in deciphering the exact nature of the ref-
erence of these nominals, as observed, is the reason
behind the confusion. The confusion is observed for
nouns like organization names, lower animates and
vehicles. Apart from the linguistically and contextu-
ally defined animacy, there was no confusion, as ex-
pected, in the understanding of biological animacy.
Annotators ?
ann1-ann2 0.78
ann1-ann3 0.82
ann2-ann3 0.83
Average ? 0.811
Table 6: Kappa Statistics
Human Other-animate Inanimate
Human 71 0 14
Other-animate 0 9 5
Inanimate 8 10 241
Table 7: Confusion Matrix
5 Conclusion and Future Work
In this work, we have presented our efforts to enrich
the nominals in the Hindi Treebank with animacy
information. The annotation was followed by the
inter-annotator agreement study for evaluating the
confusion over the categories chosen for annotation.
The annotators have a significant understanding of
the property of animacy as shown by the higher val-
ues of Kappa (?). In future, we plan to continue the
animacy annotation for the whole Hindi Treebank.
We also plan to utilize the annotated data to build
a data driven automatic animacy classifier (?vrelid,
2006). From a linguistic perspective, an annotation
of the type, as discussed in this paper, will also be of
great interest for studying information dynamics and
see how semantics interacts with syntax in Hindi.
6 Acknowledgments
The work reported in this paper is supported by the
NSF grant (Award Number: CNS 0751202; CFDA
Number: 47.070). 2
References
Judith Aissen. 2003. Differential object marking:
Iconicity vs. economy. Natural Language & Linguis-
tic Theory, 21(3):435?483.
B.R. Ambati, P. Gade, S. Husain, and GSK Chaitanya.
2009. Effect of minimal semantics on dependency
parsing. In Proceedings of the Student Research Work-
shop.
B.R. Ambati, S. Husain, J. Nivre, and R. Sangal. 2010.
On the role of morphosyntactic features in Hindi de-
pendency parsing. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 94?102. As-
sociation for Computational Linguistics.
R. Begum, S. Husain, A. Dhwaj, D.M. Sharma, L. Bai,
and R. Sangal. 2008. Dependency annotation scheme
for Indian languages. In Proceedings of IJCNLP. Cite-
seer.
Akshar Bharati, Samar Husain, Bharat Ambati, Sambhav
Jain, Dipti Sharma, and Rajeev Sangal. 2008. Two se-
mantic features make all the difference in parsing ac-
curacy. Proceedings of ICON, 8.
2Any opinions, findings, and conclusions or recommenda-
tions expressed in this material are those of the author(s) and do
not necessarily reflect the views of the National Science Foun-
dation.
165
A. Bharati, D.M. Sharma, S. Husain, L. Bai, R. Begum,
and R. Sangal. 2009. AnnCorra: TreeBanks for Indian
Languages Guidelines for Annotating Hindi TreeBank
(version?2.0).
R.A. Bhat, S. Jain, and D.M. Sharma. 2012. Experi-
ments on Dependency Parsing of Urdu. In Proceed-
ings of TLT11 2012 Lisbon Portugal, pages 31?36.
Edic?es Colibri.
R. Bhatt, B. Narasimhan, M. Palmer, O. Rambow, D.M.
Sharma, and F. Xia. 2009. A multi-representational
and multi-layered treebank for hindi/urdu. In Pro-
ceedings of the Third Linguistic Annotation Workshop,
pages 186?189. Association for Computational Lin-
guistics.
Joan Bresnan, Jean Carletta, Richard Crouch, Malvina
Nissim, Mark Steedman, Tom Wasow, and Annie Za-
enen. 2002. Paraphrase analysis for improved genera-
tion, link project.
Miriam Butt. 2006. The dative-ergative connection. Em-
pirical issues in syntax and semantics, 6:69?92.
N. Chomsky. 1981. Lectures on Government and Bind-
ing. Dordrecht: Foris.
Bernard Comrie. 1989. Language universals and lin-
guistic typology: Syntax and morphology. University
of Chicago press.
Praveen Dakwale, Himanshu Sharma, and Dipti M
Sharma. 2012. Anaphora Annotation in Hindi Depen-
dency TreeBank. In Proceedings of the 26th Pacific
Asia Conference on Language, Information, and Com-
putation, pages 391?400, Bali,Indonesia, November.
Faculty of Computer Science, Universitas Indonesia.
Felice Dell?Orletta, Alessandro Lenci, Simonetta Mon-
temagni, and Vito Pirrelli. 2005. Climbing the
path to grammar: A maximum entropy model of sub-
ject/object learning. In Proceedings of the Workshop
on Psychocomputational Models of Human Language
Acquisition, pages 72?81. Association for Computa-
tional Linguistics.
R.M.W. Dixon. 1994. Ergativity. Number 69. Cam-
bridge University Press.
Richard Evans and Constantin Orasan. 2000. Improv-
ing anaphora resolution by identifying animate entities
in texts. In Proceedings of the Discourse Anaphora
and Reference Resolution Conference (DAARC2000),
pages 154?162.
Christiane Fellbaum. 2010. WordNet. Springer.
A. Hautli, S. Sulger, and M. Butt. 2012. Adding an an-
notation layer to the Hindi/Urdu treebank. Linguistic
Issues in Language Technology, 7(1).
Paul J Hopper and Sandra A Thompson. 1980. Tran-
sitivity in grammar and discourse. Language, pages
251?299.
Seppo Kittila?, Katja Va?sti, and Jussi Ylikoski. 2011.
Case, Animacy and Semantic Roles, volume 99. John
Benjamins Publishing.
A.K. Mahajan. 1990. The A/A-bar distinction and move-
ment theory. Ph.D. thesis, Massachusetts Institute of
Technology.
Paola Merlo and Suzanne Stevenson. 2001. Auto-
matic verb classification based on statistical distribu-
tions of argument structure. Computational Linguis-
tics, 27(3):373?408.
Dipak Narayan, Debasri Chakrabarty, Prabhakar Pande,
and Pushpak Bhattacharyya. 2002. An experience in
building the indo wordnet-a wordnet for hindi. In First
International Conference on Global WordNet, Mysore,
India.
Lilja ?vrelid and Joakim Nivre. 2007. When word or-
der and part-of-speech tags are not enough ? Swedish
dependency parsing with rich linguistic features. In
Proceedings of the International Conference on Recent
Advances in Natural Language Processing (RANLP),
pages 447?451.
Lilja ?vrelid. 2006. Towards robust animacy classifica-
tion using morphosyntactic distributional features. In
Proceedings of the Eleventh Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics: Student Research Workshop, pages 47?
54. Association for Computational Linguistics.
Lilja ?vrelid. 2009. Empirical evaluations of animacy
annotation. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL).
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. volume 31, pages 71?106. MIT Press.
M. Palmer, R. Bhatt, B. Narasimhan, O. Rambow, D.M.
Sharma, and F. Xia. 2009. Hindi Syntax: Annotat-
ing Dependency, Lexical Predicate-Argument Struc-
ture, and Phrase Structure. In The 7th International
Conference on Natural Language Processing, pages
14?17.
J. Pustejovsky. 1996. The Semantics of Complex Types.
Lingua.
Dipti Misra Sharma, Prashanth Mannem, Joseph van-
Genabith, Sobha Lalitha Devi, Radhika Mamidi, and
Ranjani Parthasarathi, editors. 2012. Proceedings of
the Workshop on Machine Translation and Parsing in
Indian Languages. The COLING 2012 Organizing
Committee, Mumbai, India, December.
Michael Silverstein. 1986. Hierarchy of features and
ergativity. Features and projections, pages 163?232.
Ulf Teleman. 1974. Manual fo?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
166
Juliette Thuilier, Laurence Danlos, et al 2012. Seman-
tic annotation of French corpora: animacy and verb
semantic classes. In LREC 2012-The eighth interna-
tional conference on Language Resources and Evalu-
ation.
Annie Zaenen, Jean Carletta, Gregory Garretson, Joan
Bresnan, Andrew Koontz-Garboden, Tatiana Nikitina,
M Catherine O?Connor, and Tom Wasow. 2004. Ani-
macy Encoding in English: why and how. In Proceed-
ings of the 2004 ACL Workshop on Discourse Anno-
tation, pages 118?125. Association for Computational
Linguistics.
167
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 87?93,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Language Identification in Code-Switching Scenario
Naman Jain
LTRC, IIIT-H, Hyderabad, India
naman.jain@research.iiit.ac.in
Riyaz Ahmad Bhat
LTRC, IIIT-H, Hyderabad, India
riyaz.bhat@research.iiit.ac.in
Abstract
This paper describes a CRF based token
level language identification system en-
try to Language Identification in Code-
Switched (CS) Data task of CodeSwitch
2014. Our system hinges on using con-
ditional posterior probabilities for the in-
dividual codes (words) in code-switched
data to solve the language identification
task. We also experiment with other lin-
guistically motivated language specific as
well as generic features to train the CRF
based sequence labeling algorithm achiev-
ing reasonable results.
1 Introduction
This paper describes our participation in the Lan-
guage Identification in Code-Switched Data task
at CodeSwitch 2014 (Solorio et al., 2014). The
workshop focuses on NLP approaches for the
analysis and processing of mixed-language data
with a focus on intra sentential code-switching,
while the shared task focuses on the identifica-
tion of the language of each word in a code-
switched data, which is a prerequisite for ana-
lyzing/processing such data. Code-switching is a
sociolinguistics phenomenon, where multilingual
speakers switch back and forth between two or
more common languages or language-varieties,
in the context of a single written or spoken
conversation. Natural language analysis of code-
switched (henceforth CS) data for various NLP
tasks like Parsing, Machine Translation (MT), Au-
tomatic Speech Recognition (ASR), Information
Retrieval (IR) and Extraction (IE) and Semantic
Processing, is more complex than monolingual
data. Traditional NLP techniques perform miser-
ably when processing mixed language data. The
performance degrades at a rate proportional to the
amount and level of code-switching present in the
data. Therefore, in order to process such data,
a separate language identification component is
needed, to first identify the language of individual
words.
Language identification in code-switched data
can be thought of as a sub-task of a document
level language identification task. The latter aims
to identify the language a given document is writ-
ten in (Baldwin and Lui, 2010), while the former
addresses the same problem, however at the token
level. Although, both the problems have separate
goals, they can fundamentally be modeled with a
similar set of features and techniques. However,
language identification at the word level is more
challenging than a typical document level lan-
guage identification problem. The number of fea-
tures available at document level is much higher
than at word level. The available features for word
level identification are word morphology, syllable
structure and phonemic (letter) inventory of the
language(s). Since these features are related to the
structure of a word, letter based n-gram models
have been reported to give reasonably accurate and
comparable results (Dunning, 1994; Elfardy and
Diab, 2012; King and Abney, 2013; Nguyen and
Dogruoz, 2014; Lui et al., 2014). In this work, we
present a token level language identification sys-
tem which mainly hinges on the posterior prob-
abilities computed using n-gram based language
models.
The rest of the paper is organized as follows: In
Section 2, we discuss about the data of the shared
task. In Section 3, we discuss the methodology
we adapted to address the problem of language
identification, in detail. Experiments based on our
methodology are discussed in Section 4. In Sec-
tion 5, we present the results obtained, with a brief
discussion. Finally we conclude in Section 6 with
some future directions.
87
2 Data
The Language Identification in the Code-Switched
(CS) data shared task is meant for language
identification in 4 language pairs (henceforth
LP) namely, Nepali-English (N-E), Spanish-
English (S-E), Mandarin-English (M-E) and Mod-
ern Standard Arabic-Arabic dialects (MSA-A). So
as to get familiar with the training and testing data,
trial data sets consisting of 20 tweets each, corre-
sponding to all the language-pairs, were first re-
leased. Additional test data as ?surprise genre? for
S-E, N-E and MSA-A were also released, which
comprised of data from Facebook, blogs and Ara-
bic commentaries.
2.1 Tag Description
Each word in the training data is classified into
one of the 6 different classes which are, Lang1,
Lang2, Mixed, Other, Ambiguous and NE.
?Lang1? and ?Lang2? tags correspond to words
specific to the languages in an LP. ?Mixed? words
are those words that are partially in both the lan-
guages. ?Ambiguous? words are the ones that
could belong to either of the language. All gib-
berish and unintelligible words and words that
do not belong to any of the languages fall under
?Other? category. ?Named Entities? (NE) com-
prise of proper names that refer to people, places,
organizations, locations, movie titles and song ti-
tles etc.
2.2 Data Format and Data Crawling
Due to Twitter policies, distributing the data di-
rectly is not possible in the shared task and thus the
trial, training and testing data are provided as char
offsets with label information along with tweetID
1
and userID
2
. We use twitter
3
python script to crawl
the tweets and our own python script to further to-
kenize and synchronize the tags in the data.
Since the data for ?surprise genre? comes from
different social media sources, the ID format
varies from file to file but all the other details are
kept as is. In addition to the details, the tokens ref-
erenced by the offsets are provided unlike Twitter
data. (1) and (2) below, show the format of tweets
in train and test data respectively, while (3) shows
a typical tweet in the surprise genre data.
1
Each tweet on Twitter has a unique tweetID
2
Each user on Twitter carries a userID
3
http://emnlp2014.org/workshops/CodeSwitch/scripts/
twitter.zip
(1) TweetID UserID startIndex endIndex Tag
(2) TweetID UserID startIndex endIndex
(3) SocialMediaID UserID startIndex endIn-
dex Word
2.3 Data Statistics
The CS data is divided into two types of
tweets (henceforth posts)
4
namely, Code-switched
posts and Monolingual posts. Table 1 shows the
original number of posts that are released for the
shared task for all LPs, along with their tag counts.
Due to the dynamic nature of social media, the
posts can be either deleted or updated and thus
different participants would have crawled different
number of posts. Thus, to come up with a compa-
rable platform for all the teams, the intersection of
data from all the users is used as final testing data
to report the results. Table 1 shows the number of
tweets or posts in testing data that are finally used
for the evaluation.
3 Methodology
We divided the language identification task into
a pipeline of 3 sub-tasks namely Pre-Processing,
Language Modeling, and Sequence labeling using
CRF
5
. The pipeline is followed for all the LPs with
some LP specific variations in selecting the most
relevant features to boost the results.
3.1 Pre-Processing
In the pre-processing stage, we crawl the tweets
from Twitter given their offsets in the training data
and then tokenize and synchronize the words with
their tags as mentioned in Section 2.2. For each
LP we separate out the tokens into six classes to
use the data for Language Modeling and also to
manually analyze the language specific properties
to be used as features further in sequence labeling
. While synchronizing the words in a tweet with
their tags, we observed that some offsets do not
match with the words and this would lead to mis-
match of labels with tokens and thus degrade the
quality of training data.
To filter out the incorrect instances from the
training data, we frame pattern matching rules
which are specific to the languages present. But
this filtering is done only for the words present in
4
In case of twitter data, we have tweets but in case of sur-
prise genre data we have posts
5
Conditional Random Field
88
Language Pairs # Tweets # Tokens
CodeSwitched Monolingual Ambiguous Lang1 Lang2 Mixed NE Other
T
r
a
i
n
MSA-A dialects 774 5,065 1,066 79,134 16,291 15 14,112 8,699
Mandarin-English 521 478 0 12,114 2,431 12 1,847 1,025
Nepali-English 7,203 2,790 126 45,483 60,697 117 3982 35,651
Spanish-English 3,063 8,337 344 77,107 33,099 51 2,918 27,227
T
e
s
t
MSA-A dialects I 32 2,300 11 44,314 141 0 5,939 3,902
Mandarin-English 247 66 0 4,703 881 1 254 442
Nepali-English 2,665 209 0 12,286 17,216 60 1,071 9,635
Spanish-English 471 1,155 43 7,040 5,549 12 464 4,311
MSA-A dialects II 293 1,484 119 10,459 14,800 2 4,321 2,940
S
u
r
p
r
i
s
e
MSA-A dialects - - 110 2,687 6,930 3 1,097 1,190
Nepali-English 20 82 0 173 699 0 127 88
Spanish-English 22 27 1 636 306 1 38 120
Table 1: Data Statistics
?Lang1? and ?Lang2? classes. There are two rea-
sons to consider these labels. First, ?Lang1? and
?Lang2? classes hold maximum share of words in
any LP as shown in Table 1, and thus have a higher
impact on the overall accuracy of the language
identification system. In addition to the above,
these categories correspond to the focus point of
the shared task. Second, for ?Ambiguous?, ?NE?
and ?Other? categories, it is difficult to find the
patterns according to their definitions. Although
rules can be framed for ?Mixed? category, since
their count is too less as compared to the other
categories (Table 1), it is of no use to train a sepa-
rate language model with very less number of in-
stances.
For Mandarin and Arabic data sets, any word
present in Roman script is excluded from the data.
Similarly for English and Nepali, if any word con-
tains characters other than Roman or numeral they
are excluded from the data. In addition to the
rule for English and Nepali, the additional alpha-
bets in Spanish are also included in the set of Ro-
man and numeral entries. Table 2 shows the num-
ber of words that remained in each of the lan-
guages/dialects, after the preprocessing.
One of the bonus points in the shared task is
that 3 out of 4 LPs share ?English? as their sec-
ond language. In order to increase the training size
for English, we merged all the English words into
a single file and thus reduced the number of lan-
guage models to be trained from 8 to 6, one for
each language (or dialect).
Language Data Size Average Token Length
Arabic 10,380 8.14
English 105,014 3.83
Mandarin 12,874 4.99
MSA 53,953 8.93
Nepali 35,620 4.26
Spanish 32,737 3.96
Table 2: Data Statistics after Filtering
3.2 Language Modeling
In this stage, we train separate smoothed n-gram
based language models for each language in an LP.
We compute the conditional probability for each
word using these language models, which is then
used as a feature, among others for sequence la-
beling to finally predict the tags.
3.2.1 N-gram Language Models
Given a word w, we compute the conditional prob-
ability corresponding to k
6
classes c
1
, c
2
, ... , c
k
as:
p(c
i
|w) = p(w|c
i
) ? p(c
i
) (1)
The prior distribution p(c) of a class is es-
timated from the respective training sets shown
in Table 2. Each training set is used to train a
separate letter-based language model to estimate
the probability of word w. The language model
p(w) is implemented as an n-gram model using
the IRSTLM-Toolkit (Federico et al., 2008) with
Kneser-Ney smoothing. The language model is
6
In our case value of k is 2 as there are 2 languages in an
LP
89
defined as:
p(w) =
n
?
i=1
p(l
i
|l
i?1
i?k
) (2)
where l is a letter and k is a parameter indicating
the amount of context used (e.g., k=4 means 5-
gram model).
3.3 CRF based Sequence Labeling
After Language Modeling, we use CRF-based
(Conditional Random Fields (Lafferty et al.,
2001)) sequence labeling to predict the labels of
words in their surrounding context. The CRF algo-
rithm predicts the class of a word in its surround-
ing context taking into account other features not
explicitly represented in its structure.
3.3.1 Feature Set
In order to train CRF models, we define a feature
set which is a hybrid combination of three sub-
types of features namely, Language Model Fea-
tures (LMF), Language Specific Features (LSF)
and Morphological Features (MF).
LMF: This sub-feature set consists of poste-
rior probability scores calculated using language
models for each language in an LP. Although
we trained language models only for ?Lang1?
and ?Lang2? classes, we computed the probabil-
ity scores for all the words belonging to any of the
categories.
LSF: Each language carries some specific traits
that could assist in language identification. In
this sub-feature set we exploited some of the lan-
guage specific features exclusively based on the
description of the tags provided. The common fea-
tures for all the LPs are HAS NUM (Numeral is
present in the word), HAS PUNC (Punctuation is
present in the word), IS NUM (Word is a numeral),
IS PUNC (word is a punctuation or a collection of
punctuations), STARTS NUM (word starts with a
numeral) and STARTS PUNC (word starts with a
punctuation). All these features are used to gener-
ate variations to distinguish ?Other? class from rest
of the classes during prediction.
Two features exclusively used for the English
sharing LPs are HAS CAPITAL (capital letters are
present in the word) and IS ENGLISH (word be-
longs to English or not). HAS CAPITAL is used
to capture the capitalization property of the En-
glish writing system. This feature is expected to
help in the identification of ?NEs?. IS ENGLISH is
used to indicate whether a word is an valid English
word or not, based on its presence in English dic-
tionaries. We used dictionaries available in PyEn-
chant
7
.
For the M-E LP, we are using ?TYPE?
8
as a
feature with possible values as ENGLISH, MAN-
DARIN, NUM, PUNC and OTHER. If all the
characters in the word are English alphabets EN-
GLISH is taken as the value and Mandarin oth-
erwise. Similar checks are used for NUM and
PUNC types. But if no case is satisfied, OTHER
is taken as the value.
We observed that the above features did not con-
tribute much to distinguish between any of the tags
in case of the MSA-A LP. Since this pair consists
of two different dialects of a language rather than
two different languages, the posterior probabilities
would be close to each other as compared to other
LPs. Thus we use the difference of these probabil-
ities as a feature in order to discriminate ambigu-
ous words or NEs that are spelled similarly.
MF: This sub-feature set comprises of the mor-
phological features corresponding to a word. We
automatically extracted these features using a
python script. The first feature of this set is a bi-
nary length variable (MORE/LESS) depending on
the length of the word with threshold value 4. The
other 8 features capture the prefix and suffix prop-
erties of a word, 4 for each type. In prefix type,
4, 3, 2 and 1 characters, if present, are taken from
the beginning of a word as 4 features. Similarly
for the suffix type, 1, 2, 3 and 4 characters, again
if present, are taken from the end of a word as 4
features. In both the cases if any value is miss-
ing, it is kept as NULL (LL). (4) below, shows
a typical example from English data with the MF
sub-feature set for the word ?one?, where F1 rep-
resents the value of binary length variable, F2-F5
and F6-F9 represent the prefix and suffix features
respectively.
(4) one
Word
Less
F1
LL
F2
one
F3
on
F4
o
F5
LL
F6
one
F7
ne
F8
e
F9
3.3.2 Context Window
Along with the above mentioned features, we
chose an optimal context template to train the CRF
7
PyEnchant is a spell checking library in Python
(http://pythonhosted.org/pyenchant/)
8
Since it captures the properties of IS NUM and
IS PUNC, these features are not used again
90
models. We selected the window size to be 5, with
2 words before and after the target word. Furnish-
ing the training, testing and surprise genre data
with the features discussed in 3.3.1, we trained 4
CRF models on training data using feature tem-
plates based on the context decided. These mod-
els are used to finally predict the tags on the testing
and surprise genre data.
4 Experiments
The pipeline mentioned in Section 3 was used for
the language identification task for all the LPs.
We carried out a series of experiments with pre-
processing to clean the training data and also to
synchronize the testing data. We also did some
post-processing to handle language and tag spe-
cific cases.
In order to generate language model scores,
we trained 6 language models (one for each lan-
guage/dialect) on the filtered-out training data as
mentioned in Table 2. We experimented with dif-
ferent values of n-gram to select the optimal value
based on the F1-measure. Table 3 shows the opti-
mal order of n-gram, selected corresponding to the
highest value of F1-score. Using the optimal value
of n-gram, language models have been trained and
then posterior probabilities have been calculated
using equation (1).
Finally, we trained separate CRF models for
each LP, using the CRF++
9
tool kit based on the
features described in Section 3.3.1 and the feature
template in Section 3.3.2. To empirically find the
relevance of features we also performed leave-one
out experiments so as to decide the optimal fea-
tures for the language identification task (more de-
tails in Section 4.1). Then, using these CRF mod-
els, tags were predicted on the testing and surprise
genre datasets.
Language-Pair N-gram
MSA-A 5
M-E 5
N-E 6
S-E 5
Table 3: Optimal Value of N-gram
4.1 Feature Ranking
We expect that some features would be more im-
portant than others and would impact the task
9
http://crfpp.googlecode.com/svn/trunk/doc/index.html?
source=navbar
of language identification irrespective of the lan-
guage pair. In order to identify such optimal fea-
tures for the language identification task, we rank
them based on their information gain scores.
4.1.1 Information Gain
We used information gain to score features ac-
cording to their expected usefulness for the task at
hand. Information gain is an information theoretic
concept that measures the amount of knowledge
that is gained about a given class by having access
to a particular feature. If f is the occurrence an
individual feature and
?
f the non-occurrence of a
feature, information gain can be measured by the
following formula:
G(x) = P (f)
?
P (y|f)logP (y|f)
+ P (
?
f)
?
logP (y|
?
f)logP (y|
?
f)
(3)
For each language pair, the importance of fea-
ture types are represented by the following order:
? MSA-A dialects: token > word morphology
> posterior probabilities > others
? Mandarin-English: token > posterior prob-
abilities > word morphology > language
type > others
? Nepali-English: token > posterior probabil-
ities > word morphology > dictionary > oth-
ers
? Spanish-English: token > posterior proba-
bilities > word morphology > others > dic-
tionary
Apart from MSA-A dialects, top 3 features sug-
gested by information gain are token and its sur-
rounding context, posterior probabilities and word
morphology. For Arabic dialects word morphol-
ogy is more important than posterior probabilities.
It could be due to the fact that Arabic dialects share
a similar phonetic inventory and thus have similar
posterior probabilities. However, they differ sig-
nificantly in their morphological structure (Zaidan
and Callison-Burch, 2013).
We also carried out leave-one-out experiments
over all the features to ascertain their impact on the
classification performance. The results of these
experiments are shown in Table (5). Accuracies
are averaged over 5-fold cross-validation.
91
Token Level
Language Pairs Ambiguous Lang1 Lang2 Mixed NE Other
R P F1 R P F1 R P F1 R P F1 R P F1 R P F1 Overall Accuracy
T
e
s
t
MSA-A I 0.00 0.00 0.00 0.92 0.95 0.94 0.40 0.03 0.06 - - - 0.70 0.77 0.73 0.90 0.85 0.87 0.90
M-E - - - 0.98 0.98 0.98 0.67 0.66 0.67 0.00 1.00 0.00 0.84 0.38 0.53 0.22 0.71 0.33 0.88
N-E - - - 0.95 0.93 0.94 0.98 0.96 0.97 0.00 1.00 0.00 0.39 0.79 0.52 0.94 0.96 0.95 0.95
S-E 0.00 1.00 0.00 0.88 0.81 0.84 0.83 0.90 0.86 0.00 1.00 0.00 0.16 0.40 0.23 0.83 0.80 0.82 0.83
MSA-A II 0.00 0.00 0.00 0.91 0.47 0.62 0.36 0.84 0.51 0.00 1.00 0.00 0.59 0.80 0.68 0.80 0.71 0.75 0.60
S
u
r
p
r
i
s
e
MSA-A 0.00 0.00 0.00 0.94 0.38 0.54 0.46 0.93 0.61 0.00 1.00 0.00 0.52 0.78 0.62 0.96 0.96 0.96 0.62
N-E - - - 0.92 0.76 0.84 0.95 0.89 0.91 - - - 0.35 0.92 0.50 0.85 0.89 0.87 0.86
S-E 0.00 1.00 0.00 0.86 0.81 0.83 0.82 0.87 0.85 0.00 1.00 0.00 0.15 0.40 0.22 0.82 0.78 0.80 0.94
Table 4: Token Level Results
Left Out Feature MSA-A M-E N-E S-E
Context 76.32 94.07 93.97 92.30
Morphology 79.29 93.67 93.98 93.51
Probability 79.24 89.16 93.86 93.28
Dictionary - 87.75 93.73 92.99
Language Type - 87.97 - -
Others 78.80 83.84 92.10 92.20
All Features 79.37 95.11 94.52 93.54
Table 5: Leave-one-out Experiments
5 Results and Discussion
Each language identification system is evaluated
against two data tracks namely, ?Testing? and ?Sur-
prise Genre? data as mentioned in Section 2. Sur-
prise genre data of Mandarin-English LP was not
provided, so no results are available. All the results
are provided on two levels, comment/post/tweet
and token level. Tables 4 and 6 show results of our
language identification system on both the levels
respectively.
In case of Tweets, systems are evaluated using
the following measures: Accuracy, Recall, Preci-
sion and F-Score. However at token level, sys-
tems are evaluated separately for each tag in an
LP using Recall, Precision and F1-Score as the
measures. Table 4 shows that the results for ?Am-
biguous? and ?Mixed? categories are either miss-
ing (due to absence of tokens in that category), or
have 0.00 F1-Score. One obvious reason could be
the sparsity of data for these categories.
6 Conclusion and Future Work
In this paper, we have described a CRF based to-
ken level language identification system that uses a
set of naive easily computable features guarantee-
ing reasonable accuracies over multiple language
pairs. Our analysis showed that the most important
Language Pairs Tweet Level
Accuracy Recall Precision F-score
T
e
s
t
MSA-A I 0.605 0.719 0.025 0.048
M-E 0.751 0.814 0.863 0.838
N-E 0.948 0.979 0.966 0.972
S-E 0.835 0.773 0.692 0.730
MSA-A II 0.469 0.823 0.213 0.338
S
u
r
p
r
i
s
e
MSA-A 0.457 0.833 0.128 0.222
N-E 0.735 0.900 0.419 0.571
S-E 0.830 0.765 0.689 0.725
Table 6: Comment/Post/Tweet Level Results
feature is the word structure which in our system
is captured by n-gram posterior probabilities and
word morphology. Our analysis of Arabic dialects
shows that word morphology plays an important
role in the identification of mixed codes of closely
related languages.
7 Acknowledgement
We would like to thank Himani Chaudhry for her
valuable comments and suggestions that helped us
to improve the quality of the paper.
References
Timothy Baldwin and Marco Lui. 2010. Language
identification: The long and the short of the mat-
ter. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 229?237. Association for Computational Lin-
guistics.
Ted Dunning. 1994. Statistical identification of lan-
guage. Computing Research Laboratory, New Mex-
ico State University.
Heba Elfardy and Mona T Diab. 2012. Token level
identification of linguistic code switching. In COL-
ING (Posters), pages 287?296.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. Irstlm: an open source toolkit for han-
92
dling large scale language models. In Interspeech,
pages 1618?1621.
Ben King and Steven P Abney. 2013. Labeling the
languages of words in mixed-language documents
using weakly supervised methods. In HLT-NAACL,
pages 1110?1119.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning, pages 282?289.
Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. volume 2, pages 27?40.
Dong Nguyen and A Seza Dogruoz. 2014. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identification in code-
switched data. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
EMNLP 2014, Conference on Empirical Methods in
Natural Language Processing, Octobe, 2014, Doha,
Qatar.
Omar F Zaidan and Chris Callison-Burch. 2013. Ara-
bic dialect identification. Computational Linguis-
tics, 40(1):171?202.
93
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 47?55,
October 29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Adapting Predicate Frames for Urdu PropBanking
Riyaz Ahmad Bhat
?
, Naman Jain
?
, Dipti Misra Sharma
?
, Ashwini Vaidya
?
,
Martha Palmer
?
, James Babani
?
and Tafseer Ahmed
?
LTRC, IIIT-H, Hyderabad, India
?
University of Colorado, Boulder, CO 80309 USA
?
DHA Suffa University, Karachi, Pakistan
?
{riyaz.bhat, naman.jain}@research.iiit.ac.in, dipti@iiit.ac.in,
{vaidyaa,mpalmer, james.babani}@colorado.edu, tafseer@dsu.edu.pk
Abstract
Hindi and Urdu are two standardized reg-
isters of what has been called the Hindus-
tani language, which belongs to the Indo-
Aryan language family. Although, both
the varieties share a common grammar,
they differ significantly in their vocabulary
to an extent where both become mutually
incomprehensible (Masica, 1993). Hindi
draws its vocabulary from Sanskrit while
Urdu draws its vocabulary from Persian,
Arabic and even Turkish. In this paper,
we present our efforts to adopt frames of
nominal and verbal predicates that Urdu
shares with either Hindi or Arabic for
Urdu PropBanking. We discuss the fea-
sibility of porting such frames from either
of the sources (Arabic or Hindi) and also
present a simple and reasonably accurate
method to automatically identify the ori-
gin of Urdu words which is a necessary
step in the process of porting such frames.
1 Introduction
Hindi and Urdu, spoken primarily in northern In-
dia and Pakistan, are socially and even officially
considered two different language varieties. How-
ever, such a division between the two is not es-
tablished linguistically. They are two standard-
ized registers of what has been called the Hindus-
tani language, which belongs to the Indo-Aryan
language family. Masica (1993) explains that,
while they are different languages officially, they
are not even different dialects or sub-dialects in
a linguistic sense; rather, they are different liter-
ary styles based on the same linguistically defined
sub-dialect. He further explains that at the collo-
quial level, Hindi and Urdu are nearly identical,
both in terms of core vocabulary and grammar.
However, at formal and literary levels, vocabu-
lary differences begin to loom much larger (Hindi
drawing its higher lexicon from Sanskrit and Urdu
from Persian and Arabic) to the point where the
two styles/languages become mutually unintelligi-
ble. In written form, not only the vocabulary but
the way Urdu and Hindi are written makes one be-
lieve that they are two separate languages. They
are written in separate orthographies, Hindi be-
ing written in Devanagari, and Urdu in a modi-
fied Persio-Arabic script. Given such (apparent)
divergences between the two varieties, two paral-
lel treebanks are being built under The Hindi-Urdu
treebanking Project (Bhatt et al., 2009; Xia et al.,
2009). Both the treebanks follow a multi-layered
and multi-representational framework which fea-
tures Dependency, PropBank and Phrase Structure
annotations. Among the two treebanks the Hindi
treebank is ahead of the Urdu treebank across all
layers. In the case of PropBanking, the Hindi tree-
bank has made considerable progress while Urdu
PropBanking has just started.
The creation of predicate frames is the first step
in PropBanking, which is followed by the actual
annotation of verb instances in corpora. In this
paper, we look at the possibility of porting re-
lated frames from Arabic and Hindi PropBanks for
Urdu PropBanking. Given that Urdu shares its vo-
cabulary with Arabic, Hindi and Persian, we look
at verbal and nominal predicates that Urdu shares
with these languages and try to port and adapt their
frames from the respective PropBanks instead of
creating them afresh. This implies that identifi-
cation of the source of Urdu predicates becomes
a necessary step in this process. Thus, in order
to port the relevant frames, we need to first iden-
tify the source of Urdu predicates and then extract
their frames from the related PropBanks. To state
briefly, we present the following as contributions
of this paper:
? Automatic identification of origin or source
of Urdu vocabulary.
47
? Porting and adapting nominal and verbal
predicate frames from the PropBanks of re-
lated languages.
The rest of the paper is organised as follows: In
the next Section we discuss the Hindi-Urdu tree-
banking project with the focus on PropBanking.
In Section 3, we discuss our efforts to automati-
cally identify the source of Urdu vocabulary and
in Section 4, we discuss the process of adapting
and porting Arabic and Hindi frames for Urdu
PropBanking. Finally we conclude with some
future directions in Section 5.
2 A multi-layered,
multi-representational treebank
Compared to other existing treebanks, Hindi/Urdu
Treebanks (HTB/UTB) are unusual in that they are
multi-layered. They contain three layers of anno-
tation: dependency structure (DS) for annotation
of modified-modifier relations, PropBank-style
annotation (PropBank) for predicate-argument
structure, and an independently motivated phrase-
structure (PS). Each layer has its own framework,
annotation scheme, and detailed annotation guide-
lines. Due to lack of space and relevance to our
work, we only look at PropBanking with reference
to Hindi PropBank, here.
2.1 PropBank Annotation
The first PropBank, the English PropBank (Kings-
bury and Palmer, 2002), originated as a one-
million word subset of the Wall Street Journal
(WSJ) portion of Penn Treebank II (an English
phrase structure treebank). The verbs in the Prop-
Bank are annotated with predicate-argument struc-
tures and provide semantic role labels for each
syntactic argument of a verb. Although these
were deliberately chosen to be generic and theory-
neutral (e.g., ARG0, ARG1), they are intended
to consistently annotate the same semantic role
across syntactic variations. For example, in both
the sentences John broke the window and The win-
dow broke, ?the window? is annotated as ARG1
and as bearing the role of ?Patient?. This reflects
the fact that this argument bears the same seman-
tic role in both the cases, even though it is realized
as the structural subject in one sentence and as the
object in the other. This is the primary difference
between PropBank?s approach to semantic role la-
bels and the Paninian approach to karaka labels,
which it otherwise resembles closely. PropBank?s
ARG0 and ARG1 can be thought of as similar
to Dowty?s prototypical ?Agent? and ?Patient?
(Dowty, 1991). PropBank provides, for each sense
of each annotated verb, its ?roleset?, i.e., the possi-
ble arguments of the predicate, their labels and all
possible syntactic realizations. The primary goal
of PropBank is to supply consistent, simple, gen-
eral purpose labeling of semantic roles for a large
quantity of coherent text that can provide training
data for supervised machine learning algorithms,
in the same way that the Penn Treebank supported
the training of statistical syntactic parsers.
2.1.1 Hindi PropBank
The Hindi PropBank project has differed signif-
icantly from other PropBank projects in that the
semantic role labels are annotated on dependency
trees rather than on phrase structure trees. How-
ever, it is similar in that semantic roles are defined
on a verb-by-verb basis and the description at
the verb-specific level is fine-grained; e.g., a
verb like ?hit? will have ?hitter? and ?hittee?.
These verb-specific roles are then grouped into
broader categories using numbered arguments
(ARG). Each verb can also have a set of modifiers
not specific to the verb (ARGM). In Table 1,
PropBank-style semantic roles are listed for
the simple verb de ?to give?. In the table, the
numbered arguments correspond to the giver,
thing given and recipient. Frame file definitions
are created manually and include role information
as well as a unique roleset ID (e.g. de.01 in Table
1), which is assigned to every sense of a verb. In
addition, for Hindi the frame file also includes the
transitive and causative forms of the verb (if any).
Thus, the frame file for de ?give? will include
dilvaa ?cause to give?.
de.01 to give
Arg0 the giver
Arg1 thing given
Arg2 recipient
Table 1: A Frame File
The annotation process for the PropBank takes
place in two stages: the creation of frame files for
individual verb types, and the annotation of pred-
icate argument structures for each verb instance.
The annotation for each predicate in the corpus
is carried out based on its frame file definitions.
48
The PropBank makes use of two annotation tools
viz. Jubilee (Choi et al., 2010b) and Cornerstone
(Choi et al., 2010a) for PropBank instance annota-
tion and PropBank frame file creation respectively.
For annotation of the Hindi and Urdu PropBank,
the Jubilee annotation tool had to be modified to
display dependency trees and also to provide ad-
ditional labels for the annotation of empty argu-
ments.
3 Identifying the source of Urdu
Vocabulary
Predicting the source of a word is similar to lan-
guage identification where the task is to identify
the language a given document is written in. How-
ever, language identification at word level is more
challenging than a typical document level lan-
guage identification problem. The number of fea-
tures available at document level is much higher
than at word level. The available features for word
level identification are word morphology, syllable
structure and phonemic (letter) inventory of the
language(s).
In the case of Urdu, the problem is even more
complex as the borrowed words don?t necessarily
carry the inflections of their source language and
don?t retain their identity as such (they undergo
phonetic changes as well). For example, khabar
?news? which is an Arabic word declines as per
the morphological paradigm of feminine nom-
inals in Hindi and Urdu as shown in Table (2).
However, despite such challenges, if we look at
the character histogram in Figure (1), we can still
identify the source of a sufficiently large portion
of Urdu vocabulary just by using letter-based
heuristics. For example neither Arabic nor Persian
has aspirated consonants like bH, ph Aspirated
Bilabial Plosives; tSh, dZH Aspirated Alveolar
Fricatives; ?H Aspirated Retroflex Plosive; gH, kh
Aspirated Velar Plosives etc. while Hindi does.
Similarly, the following sounds occur only in
Arabic and Persian: Z Fricative Postalveolar; T,
D Fricative Dental; ? Fricative Pharyngeal; X
Fricative Uvular etc. Using these heuristics we
could identify 2,682 types as Indic, and 3,968
as either Persian or Arabic out of 12,223 unique
types in the Urdu treebank (Bhat and Sharma,
2012).
Singular Plural
Direct khabar khabarain
Oblique khabar khabaron
Table 2: Morphological Paradigm of khabar
This explains the efficiency of n-gram based ap-
proaches to either document level or word level
language identification tasks as reported in the re-
cent literature on the problem (Dunning, 1994;
Elfardy and Diab, 2012; King and Abney, 2013;
Nguyen and Dogruoz, 2014; Lui et al., 2014).
In order to predict the source of an Urdu word,
we frame two classification tasks: (1) binary clas-
sification into Indic and Persio-Arabic and, (2) tri-
class classification into Arabic, Indic and Persian.
Both the problems are modeled using smoothed n-
gram based language models.
3.1 N-gram Language Models
Given a word w to classify into one of k classes
c
1
, c
2
, ... , c
k
, we will choose the class with the
maximum conditional probability:
c
?
= argmax
c
i
p(c
i
|w)
= argmax
c
i
p(w|c
i
) ? p(c
i
)
(1)
The prior distribution p(c) of a class is esti-
mated from the respective training sets shown in
Table (3). Each training set is used to train a
separate letter-based language model to estimate
the probability of word w. The language model
p(w) is implemented as an n-gram model using
the IRSTLM-Toolkit (Federico et al., 2008) with
Kneser-Ney smoothing. The language model is
defined as:
p(w) =
n
?
i=1
p(l
i
|l
i?1
i?k
) (2)
where, l is a letter and k is a parameter indicat-
ing the amount of context used (e.g., k = 4 means
5-gram model).
3.2 Etymological Data
In order to prepare training and testing data
marked with etymological information for our
classification experiments, we used the Online
1
http://www.langsci.ucl.ac.uk/ipa/IPA chart %28C%
292005.pdf
49
bH Z T ? X D sQ tQ dQ Q DQ G f tSh q ?H gH khdZH N ? ph S ? th t?h d?H tS b d g H k dZ m l n p s r t t? V j d?
0
5 ? 10
?2
0.1
0.15
0.2
0.25
0.3
0.35
Alphabets in IPA
1
R
e
l
a
t
i
v
e
F
r
e
q
u
e
n
c
y
Arabic
Hindi
Persian
Urdu
Figure 1: Relative Distribution of Arabic, Hindi, Persian and Urdu Alphabets (Consonants only)
Urdu Dictionary
2
(henceforth OUD). OUD has
been prepared under the supervision of the e-
government Directorate of Pakistan
3
. Apart from
basic definition and meaning, it provides etymo-
logical information for more than 120K Urdu
words. Since the dictionary is freely
4
available
and requires no expertise for extraction of word
etymology which is usually the case with manual
annotation, we could mark the etymological infor-
mation on a reasonably sized word list in a limited
time frame. The statistics are provided in Table
(3). We use Indic as a cover term for all the words
that are either from Sanskrit, Prakrit, Hindi or lo-
cal languages.
Language Data Size Average Token Length
Arabic 6,524 6.8
Indic 3,002 5.5
Persian 4,613 6.5
Table 3: Statistics of Etymological Data
2
http://182.180.102.251:8081/oud/default.aspx
3
www.e-government.gov.pk
4
We are not aware of an offline version of OUD.
3.3 Experiments
We carried out a number of experiments in order
to explore the effect of data size and the order of
n-gram models on the classification performance.
By varying the size of training data, we wanted to
identify the lower bound on the training size with
respect to the classification performance. We var-
ied the training size per training iteration by 1%
for n-grams in the order 1-5 for both the classifi-
cation problems. For each n-gram order 100 ex-
periments were carried out, i.e overall 800 exper-
iments for binary and tri-class classification. The
impact of training size on the classification perfor-
mance is shown in Figures (2) and (3) for binary
and tri-class classification respectively. As ex-
pected, at every iteration the additional data points
introduced into the training data increased the per-
formance of the model. With a mere 3% of the
training data, we could reach a reasonable accu-
racy of 0.85 in terms of F-score for binary classi-
fication and for tri-class classification we reached
the same accuracy with 6% of the data.
Similarly, we tried different order n-gram mod-
els to quantify the effect of character context on
50
the classification performance. As with the in-
crease in data size, increasing the n-gram order
profoundly improved the results. In both the clas-
sification tasks, unigram based models converge
faster than the higher order n-gram based models.
The obvious reason for it is the small, finite set of
characters that a language operates with (? 37 in
Arabic, ? 39 in Persian and ? 48 in Hindi). A
small set of words (unique in our case) is probably
enough to capture at least a single instance of each
character. As no new n-gram is introduced with
subsequent additions of new tokens in the training
data, the accuracy stabilizes. However, the accu-
racy with higher order n-grams kept on increas-
ing with an increase in the data size, though it was
marginal after 5-grams. The abrupt increase after
8,000 training instances is probably due to the ad-
dition of an unknown bigram sequence(s) to the
training data. In particular, the Recall of Persio-
Arabic increased by 2.2%.
0 0.2 0.4 0.6 0.8 1 1.2
?10
4
0.5
0.6
0.7
0.8
0.9
1
Training Data Size
F
-
S
c
o
r
e
1-gram
2-gram
3-gram
4-gram
Figure 2: Learning Curves for Binary Classifica-
tion of Urdu Vocabulary
3.4 Results
We performed 10-fold cross validation over all the
instances of the etymological data for both the bi-
nary and tri-class classification tasks. We split the
data into training and testing sets with a ratio of
80:20 using the stratified sampling. Stratified sam-
pling distributes the samples of each class in train-
ing and testing sets with the same percentage as in
the complete set. For all the 10-folds, the order of
0 0.2 0.4 0.6 0.8 1 1.2
?10
4
0.5
0.6
0.7
0.8
0.9
1
Training Data Size
F
-
S
c
o
r
e
1-gram
2-gram
3-gram
4-gram
Figure 3: Learning Curves for Tri-class Classifi-
cation of Urdu Vocabulary
n-gram was varied again from 1-5. Tables (4) and
(5) show the consolidated results for these tasks
with a frequency based baseline to evaluate the
classification performance. In both the tasks, we
achieved highest accuracy with language models
trained with 5-gram letter sequence context. The
best results in terms of F-score are 0.96 and 0.93
for binary and tri-class classification respectively.
Type Precision (P) Recall (R) F1-Score (F)
Baseline 0.40 0.50 0.40
1-gram 0.89 0.89 0.89
2-gram 0.95 0.95 0.95
3-gram 0.96 0.96 0.96
4-gram 0.96 0.96 0.96
5-gram 0.96 0.96 0.96
Table 4: Results of 10-fold Cross Validation on
Binary Classification
Although, we have achieved quite reasonable
accuracies in both the tasks, a closer look at the
confusion matrices shown in Tables (6) and (7)
show that we can still improve the accuracies by
balancing the size of data across classes. In binary
classification our model is more biased towards
Persio-Arabic as the data is highly imbalanced.
Our binary classifier misclassifies 0.86% of Indic
tokens as Persio-Arabic since the prior probability
of the latter is much higher than that of the former.
While in the case of tri-class classification, using
51
Type Precision (P) Recall (R) F1-Score (F)
Baseline 0.15 0.33 0.21
1-gram 0.83 0.83 0.83
2-gram 0.89 0.89 0.89
3-gram 0.91 0.91 0.91
4-gram 0.93 0.93 0.93
5-gram 0.93 0.93 0.93
Table 5: Results of 10-fold Cross Validation on
Tri-Class Classification
higher order n-gram models can resolve the
prominent confusion between Arabic and Persian.
Since both Arabic and Persian share almost the
same phonetic inventory, working with lower
order n-gram models doesn?t seem ideal.
Class Indic Persio-Arabic
Indic 235 60
Persio-Arabic 15 1,057
Table 6: Confusion Matrix of Binary Classifica-
tion
Class Arabic Indic Persian
Arabic 605 5 26
Indic 11 268 18
Persian 22 9 415
Table 7: Confusion Matrix of Tri-class Classifica-
tion
4 Adapting Frames from Arabic and
Hindi PropBanks
As discussed in Section 2.1.1, the creation of pred-
icate frames precedes the actual annotation of verb
instances in a given corpus. In this section, we de-
scribe our approach towards the first stage of Urdu
PropBanking by adapting related predicate frames
from Arabic and Hindi PropBanks (Palmer et al.,
2008; Vaidya et al., 2011). Since a PropBank
is not available for Persian, we could only adapt
those predicate frames which are shared with Ara-
bic and Hindi.
Although, Urdu shares or borrows most of its
literary vocabulary from Arabic and Persian, it re-
tains its simple verb (as opposed to compound or
complex verbs) inventory from Indo-Aryan ances-
try. Verbs from Arabic and Persian are borrowed
less frequently, although there are examples such
as ?khariid? buy, ?farma? say etc.
5
This over-
lap in the verb inventory between Hindi and Urdu
might explain the fact that they share the same
grammar.
The fact that Urdu shares its lexicon with these
languages, prompted us towards exploring the
possibility of using their resources for Urdu Prop-
Banking. We are in the process of adapting frames
for those Urdu predicates that are shared with ei-
ther Arabic or Hindi.
Urdu frame file creation must be carried out for
both simple verbs and complex predicates. Since
Urdu differs very little in simple verb inventory
from Hindi, this simplifies the development pro-
cess as the frames could be ported easily. How-
ever, this is not the case with nominal predicates.
In Urdu, many nominal predicates are borrowed
from Arabic or Persian as shown in Table (8).
Given that a PropBank for Persian is not available,
the task of creating the frames for nominal predi-
cates in Urdu would have been fairly daunting in
the paucity of the Arabic PropBank, as well.
Simple Verbs Nominal Predicates
Language Total Unique Total Unique
Arabic 12 1 6,780 765
Hindi 7,332 441 1,203 258
Persian 69 3 2,276 352
Total 7,413 445 10,259 1,375
Table 8: Urdu Treebank Predicate Statistics
4.1 Simple Verbs
The simple verb inventory of Urdu and Hindi is
almost similar, so the main task was to locate and
extract the relevant frames from Hindi frame files.
Fortunately, with the exception of farmaa ?say?,
all the other simple verbs which Urdu borrows
from Persian or Arabic (cf. Table (8)) were also
borrowed by Hindi. Therefore, the Hindi sim-
ple verb frame files sufficed for porting frames for
Urdu simple verbs.
There were no significant differences found be-
tween the Urdu and Hindi rolesets, which describe
either semantic variants of the same verb or its
causative forms. Further, in order to name the
frame files with their corresponding Urdu lemmas,
we used Konstanz?s Urdu transliteration scheme
5
Borrowed verbs often do not function as simple verbs
rather they are used like nominals in complex predicate con-
structions such as mehsoos in ?mehsoos karnaa? to feel.
52
(Malik et al., 2010) to convert a given lemma into
its romanized form. Since the Hindi frame files
use the WX transliteration scheme
6
, which is not
appropriate for Urdu due to lack of coverage for
Persio-Arabic phonemes or sounds like dQ ?pha-
ryngealized voiced alveolar stop?. The frame files
also contain example sentences for each predicate,
in order to make the PropBank annotation task eas-
ier. While adapting the frame files from Hindi
to Urdu, simply transliterating such examples for
Urdu predicates was not always an option, because
sentences consisting of words with Sanskrit origin
may not be understood by Urdu speakers. Hence,
all the examples in the ported frames have been
replaced with Urdu sentences by an Urdu expert.
In general we find that the Urdu verbs are quite
similar to Hindi verbs, and this simplified our task
of adapting the frames for simple verbs. The
nouns, however, show more variation. Since a
large proportion (up to 50%) of Urdu predicates
are expressed using verb-noun complex predi-
cates, nominal predicates play a crucial role in our
annotation process and must be accounted for.
4.2 Complex Predicates
In the Urdu treebank, there are 17,672 predicates,
of which more than half have been identified as
noun-verb complex predicates (NVC) at the de-
pendency level. Typically, a noun-verb complex
predicate chorii ?theft? karnaa ?to do? has two
components: a noun chorii and a light verb karnaa
giving us the meaning ?steal?. The verbal compo-
nent in NVCs has reduced predicating power (al-
though it is inflected for person, number, and gen-
der agreement as well as tense, aspect and mood)
and its nominal complement is considered the true
predicate. In our annotation of NVCs, we fol-
low a procedure common to all PropBanks, where
we create frame files for the nominal or the ?true?
predicate (Hwang et al., 2010). An example of a
frame file for a noun such as chorii is described in
Table (9).
The creation of a frame file for the set of
true predicates that occur in an NVC is impor-
tant from the point of view of linguistic annota-
tion. Given the large number of NVCs, a semi-
automatic method has been proposed for creating
Hindi nominal frame files, which saves the man-
ual effort required for creating frames for nearly
6
http://en.wikipedia.org/wiki/WX notation
Frame file for chorii-n(oun)
chorii.01: theft-n light verb: kar?do; to steal?
Arg0 person who steals
Arg1 thing stolen
chorii.02 : theft-n light verb: ho ?be/become; to
get stolen?
Arg1 thing stolen
Table 9: Frame file for predicate noun chorii
?theft? with two frequently occurring light verbs
ho and kar. If other light verbs are found to occur,
they are added as additional rolesets as chorii.03,
chorii.04 and so on.
3,015 unique Hindi noun and light verb combina-
tions (Vaidya et al., 2013).
For Urdu, the process of nominal frame file cre-
ation is preceded by the identification of the ety-
mological origin for each nominal. If that nomi-
nal has an Indic or Arabic origin, relevant frames
from Arabic or Hindi PropBanks were adapted for
Urdu. On the other hand, if the Urdu nominal orig-
inates from Persian, then frame creation will be
done either manually or using other available Per-
sian language resources, in the future.
In Table (8), there are around 258 nominal pred-
icates that are common in Hindi and Urdu, so we
directly ported their frames from Hindi PropBank
with minor changes as was done for simple verb
frames. Out of 765 nominal predicates shared with
Arabic, 308 nominal predicate frames have been
ported to Urdu. 98 of these nominal predicate
frames were already present in the Arabic Prop-
Bank and were ported as such. However, for the
remaining 667 unique predicates, frames are be-
ing created manually by Arabic PropBanking ex-
perts and will be ported to Urdu once they become
available.
Porting of Arabic frames to Urdu is not that triv-
ial. We observed that while Urdu borrows vocabu-
lary from Arabic it does not borrow all the senses
for some words. In such cases, the rolesets that are
irrelevant to Urdu have to be discarded manually.
The example sentences for all the frames ported
from Arabic PropBank have to be sourced from
either the web or manually created by an Urdu ex-
pert, as was the case with Hindi simple verbs.
5 Conclusion
In this paper we have exploited the overlap be-
tween the lexicon of Urdu, Arabic and Hindi for
the creation of predicate frames for Urdu Prop-
53
Banking. We presented a simple and accurate clas-
sifier for the identification of source or origin of
Urdu vocabulary which is a necessary step in the
overall process of extraction of predicate frames
from the related PropBanks. In the case of sim-
ple verbs that occur in the Urdu treebank, we have
extracted all the frames from the Hindi PropBank
and adapted them for Urdu PropBanking. Simi-
larly for complex predicates, frames for Urdu tree-
bank nominal predicates are extracted from Hindi
as well as from Arabic PropBanks. Since a Prop-
Bank is not available for Persian, the creation
of frames for shared predicates with Persian is a
prospect for future work. We plan to create these
frames either manually or semi-automatically, us-
ing the available Persian Dependency treebanks
(Rasooli et al., 2011; Rasooli et al., 2013).
Acknowledgments
We would like to thank Himani Chaudhry for her
valuable comments that helped to improve the
quality of this paper.
The work reported in this paper is supported by
the NSF grant (Award Number: CNS 0751202;
CFDA Number: 47.070)
7
.
References
Riyaz Ahmad Bhat and Dipti Misra Sharma. 2012.
A dependency treebank of urdu and its evaluation.
In Proceedings of the Sixth Linguistic Annotation
Workshop, pages 157?165. Association for Compu-
tational Linguistics.
Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer,
Owen Rambow, Dipti Misra Sharma, and Fei Xia.
2009. A multi-representational and multi-layered
treebank for hindi/urdu. In Proceedings of the Third
Linguistic Annotation Workshop, pages 186?189.
Association for Computational Linguistics.
Jinho D Choi, Claire Bonial, and Martha Palmer.
2010a. Propbank frameset annotation guidelines us-
ing a dedicated editor, cornerstone. In LREC.
Jinho D Choi, Claire Bonial, and Martha Palmer.
2010b. Propbank instance annotation guidelines us-
ing a dedicated editor, jubilee. In LREC.
David Dowty. 1991. Thematic proto-roles and argu-
ment selection. Language, 67(3):547?619.
7
Any opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the author(s)
and do not necessarily reflect the views of the National Sci-
ence Foundation.
Ted Dunning. 1994. Statistical identification of lan-
guage. Computing Research Laboratory, New Mex-
ico State University.
Heba Elfardy and Mona T Diab. 2012. Token level
identification of linguistic code switching. In COL-
ING (Posters), pages 287?296.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. Irstlm: an open source toolkit for han-
dling large scale language models. In Interspeech,
pages 1618?1621.
Jena D Hwang, Archna Bhatia, Clare Bonial, Aous
Mansouri, Ashwini Vaidya, Nianwen Xue, and
Martha Palmer. 2010. Propbank annotation of mul-
tilingual light verb constructions. In Proceedings of
the Fourth Linguistic Annotation Workshop, pages
82?90. Association for Computational Linguistics.
Ben King and Steven P Abney. 2013. Labeling the
languages of words in mixed-language documents
using weakly supervised methods. In HLT-NAACL,
pages 1110?1119.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In LREC. Citeseer.
Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. Transactions of the Asso-
ciation for Computational Linguistics, 2:27?40.
Muhammad Kamran Malik, Tafseer Ahmed, Sebastian
Sulger, Tina B?ogel, Atif Gulzar, Ghulam Raza, Sar-
mad Hussain, and Miriam Butt. 2010. Transliterat-
ing urdu for a broad-coverage urdu/hindi lfg gram-
mar. In LREC.
Colin P Masica. 1993. The Indo-Aryan Languages.
Cambridge University Press.
Dong Nguyen and A Seza Dogruoz. 2014. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing.
Martha Palmer, Olga Babko-Malaya, Ann Bies,
Mona T Diab, Mohamed Maamouri, Aous Man-
souri, and Wajdi Zaghouani. 2008. A pilot arabic
propbank. In LREC.
Mohammad Sadegh Rasooli, Amirsaeid Moloodi,
Manouchehr Kouhestani, and Behrouz Minaei-
Bidgoli. 2011. A syntactic valency lexicon for
persian verbs: The first steps towards persian de-
pendency treebank. In 5th Language & Technology
Conference (LTC): Human Language Technologies
as a Challenge for Computer Science and Linguis-
tics, pages 227?231.
Mohammad Sadegh Rasooli, Manouchehr Kouhestani,
and Amirsaeid Moloodi. 2013. Development of a
persian syntactic dependency treebank. In Proceed-
ings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational
54
Linguistics: Human Language Technologies, pages
306?314.
Ashwini Vaidya, Jinho D Choi, Martha Palmer, and
Bhuvana Narasimhan. 2011. Analysis of the hindi
proposition bank using dependency structure. In
Proceedings of the 5th Linguistic Annotation Work-
shop, pages 21?29. Association for Computational
Linguistics.
Ashwini Vaidya, Martha Palmer, and Bhuvana
Narasimhan. 2013. Semantic roles for nominal
predicates: Building a lexical resource. NAACL
HLT 2013, 13:126.
Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer,
and Dipti Misra Sharma. 2009. Towards a multi-
representational treebank. In The 7th International
Workshop on Treebanks and Linguistic Theories.
Groningen, Netherlands.
55
