Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 11?21,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
Comparison of the Baseline Knowledge-, Corpus-, and Web-based Similarity
Measures for Semantic Relations Extraction
Alexander Panchenko
Center for Natural Language Processing (CENTAL)
Universite? catholique de Louvain, Belgium
alexander.panchenko@student.uclouvain.be
Abstract
Unsupervised methods of semantic relations
extraction rely on a similarity measure be-
tween lexical units. Similarity measures differ
both in kinds of information they use and in
the ways how this information is transformed
into a similarity score. This paper is making
a step further in the evaluation of the avail-
able similarity measures within the context
of semantic relation extraction. We compare
21 baseline measures ? 8 knowledge-based, 4
corpus-based, and 9 web-based metrics with
the BLESS dataset. Our results show that
existing similarity measures provide signifi-
cantly different results, both in general per-
formances and in relation distributions. We
conclude that the results suggest developing a
combined similarity measure.
1 Introduction
Semantic relations extraction aims to discover
meaningful lexico-semantic relations such as syn-
onyms and hyponyms between a given set of lexi-
cally expressed concepts. Automatic relations dis-
covery is a subtask of automatic thesaurus con-
struction (see Grefenstette (1994), and Panchenko
(2010)).
A set of semantic relations R between a set of
concepts C is a binary relation R ? C ? T ? C,
where T is a set of semantic relation types. A re-
lation r ? R is a triple ?ci, t, cj? linking two con-
cepts ci, cj ? C with a semantic relation of type
t ? T . We are dealing with six types of semantic
relations: hyperonymy, co-hyponymy, meronymy,
event (associative), attributes, and random: T =
{hyper, coord,mero, event, attri, random}. We
describe analytically and compare experimentally
methods, which discover set of semantic relations
R? for a given set of concepts C. A semantic relation
extraction algorithm aims to discover R? ? R.
One approach for semantic relations extraction
is based on the lexico-syntactic patterns which are
constructed either manually (Hearst, 1992) or semi-
automatically (Snow et al, 2004). The alternative
approach, adopted in this paper, is unsupervised (see
e.g. Lin (1998a) or Sahlgren (2006)). It relies on
a similarity measure between lexical units. Vari-
ous measures are available. We compare 21 base-
line measures: 8 knowledge-based, 4 corpus-based,
and 9 web-based. We would like to answer on two
questions: ?What metric is most suitable for the un-
supervised relation extraction??, and ?Does various
metrics capture the same semantic relations??. The
second question is particularly interesting for devel-
oping of a meta-measure combining several metrics.
This information may also help us choose a measure
well-suited for a concrete application.
We extend existing surveys in three ways. First,
we ground our comparison on the BLESS dataset1,
which is open, general, and was never used before
for comparing all the considered metrics. Secondly,
we face corpus-, knowledge-, and web-based, which
was never done before. Thirdly, we go further than
most of the comparisons and thoroughly compare
the metrics with respect to relation types they pro-
vide. We report empirical relation distributions for
1http://sites.google.com/site/
geometricalmodels/sharedevaluation
11
each measure and check if they are significantly dif-
ferent. Next, we propose a way to find the measures
with the most and the least similar relation distribu-
tions. Finally, we report information about redun-
dant measures in an original way ? in a form of an
undirected graph.
2 Methodology
2.1 Similarity-based Semantic Relations
Discovery
We use an unsupervised approach to calculate set
of semantic relations R between a given set of
concepts C (see algorithm 1). The method uses
one of 21 similarity measures described in sections
2.2 to 2.4. First, it calculates the concept?concept
similarity matrix S with a measure sim. Since
some similarity measures output scores outside
the interval [0; 1] we transform them with the
function normalize as following: S? (S?min(S))max(S) .
If we deal with a dissimilarity measure, we ad-
ditionally transform its score S to similarity as
following: S ? 1 ? normalize(S). Finally, the
function threshold calculates semantic relations R
between concepts C with the k-NN thresholding:
?|C|
i=1 {?ci, t, cj? : cj ? top k% concepts ? sij ? ?} .
Here k is the percent of the top similar concepts
to a concept ci, and ? is a small value which
ensures than nearly-zero pairwise similarities sij
will be ignored. Thus, the method links each
concept ci with k% of its nearest neighbours.
Algorithm 1: Computing semantic relations
Input: Concepts C, Sim.parameters P ,
Threshold k, Min.similarity value ?
Output: Unlabeled semantic relations R?
1 S? sim(C,P ) ;
2 S? normalize(S) ;
3 R?? threshold(S, k, ?) ;
4 return R? ;
Below we list the pairwise similarity measures
sim used in our experiments with references to the
original papers, where all details can be found.
2.2 Knowledge-based Measures
The knowledge-based metrics use a hierarchical se-
mantic network in order to calculate similarities.
Some of the metrics also use counts derived from
a corpus. We evaluate eight knowledge-based mea-
sures listed below. Let us describe them in the fol-
lowing notations: cr is the root concept of the net-
work; h is the height of the network; len(ci, cj) is
the length of the shortest path in the network be-
tween concepts; cij is a lowest common subsumer
of concepts ci and cj ; P (c) is the probability of the
concept, estimated from a corpus (see below). Then,
the Inverted Edge Count measure (Jurafsky and Mar-
tin, 2009, p. 687) is
sij = len(ci, cj)
?1; (1)
Leacock and Chodorow (1998) measure is
sij = ?log
len(ci, cj)
2h
; (2)
Resnik (1995) measure is
sij = ?log(P (cij)); (3)
Jiang and Conrath (1997) measure is
sij = (2log(P (cij))?(log(P (ci))+log(P (cj))))
?1;
(4)
Lin (1998b) measure is
sij = (
2log(P (cij))
log(P (ci) + log(P (cj))
; (5)
Wu and Palmer (1994) measure is
sij =
2len(cr, cij)
len(ci, cij) + len(cj , cij) + 2len(cr, cij)
.
(6)
Extended Lesk (Banerjee and Pedersen, 2003) mea-
sure is
sij =
?
ci?Ci
?
cj?Cj
simg(ci, cj), (7)
where simg is a gloss-based similarity measure, and
setCi includes concept ci and all concepts which are
directly related to it.
Gloss Vectors measure (Patwardhan and Peder-
sen, 2006) is calculated as a cosine (9) between con-
text vectors vi and vj of concepts ci and cj . A con-
text vector calculated as following:
vi =
?
?j:cj?Gi
fj . (8)
12
Here fj is a first-order co-occurrence vector, derived
from the corpus of all glosses, and Gi is concate-
nation of glosses of the concept ci and all concepts
which are directly related to it.
We experiment with measures relying on the
WORDNET 3.0 (Miller, 1995) as a semantic net-
work and SEMCOR as a corpus (Miller et al, 1993).
2.3 Corpus-based measures
We use four measures, which rely on the bag-
of-word distributional analysis (BDA) (Sahlgren,
2006). They calculate similarity of concepts ci, cj
as similarity of their feature vectors fi, fj with
the following formulas (Jurafsky and Martin, 2009,
p. 699): cosine
sij =
fi ? fj
?fi? ?fj?
, (9)
Jaccard
sij =
?N
k=1min(fik, fjk)
?N
k=1max(fik, fjk)
, (10)
Manhattan
sij =
N?
k=1
|fik ? fjk|, (11)
Euclidian
sij =
?
?
?
?
N?
k=1
(fik ? fjk)2. (12)
The feature vector fi is a first-order co-occurrence
vector. The context of a concept includes all
words from a sentence where it occurred, which
pass a stop-word filter (around 900 words) and a
stop part-of-speech filter (nouns, adjectives, and
verbs are kept). The frequencies fij are normalized
with Poinwise Mutual Information (PMI): fij =
log(fij/(count(ci)count(fj))). In our experiments
we use two general English corpora (Baroni et al,
2009): WACYPEDIA (800M tokens), and PUKWAC
(2000M tokens). These corpora are POS-tagged
with the TreeTagger (Schmid, 1994).
2.4 Web-based measures
The web-based metrics use the Web text search en-
gines in order to calculate the similarities. They rely
on the number of times words co-occur in the doc-
uments indexed by an information retrieval system.
Let us describe these measures in the following no-
tation: hi is the number of documents (hits) returned
by the system by the query ?ci?; hij is the number
of hits returned by the query ?ci AND cj?; and M
is number of documents indexed by the system. We
use two web-based measures: Normalized Google
Distance (NGD) (Cilibrasi and Vitanyi, 2007):
sij =
max(log(hi, hj))? log(hij)
log(M)?min(log(hi), log(hj))
, (13)
and PMI-IR similarity (Turney, 2001) :
sij = log
(
hij
?
i
?
j hihj
hihj
?
i hij
)
. (14)
We experiment with 5 NGD measures based on Ya-
hoo, YahooBoss 2, Google, Google over Wikipedia,
and Factiva 3; and with 4 PMI-IR measures based
on YahooBoss, Google, Google over Wikipedia, and
Factiva. We perform search among all indexed docu-
ments or within the domain wikipedia.org (we
denote the latter measures with the postfix -W).
2.5 Classification of the measures
It might help to understand the results if we men-
tion that (1) - (6) are measures of semantic similar-
ity, while (7) and (8) are measures of semantic relat-
edness. Semantic relatedness is a more general no-
tion than semantic similarity (Budanitsky and Hirst,
2001). A measure of semantic similarity uses only
hierarchical and equivalence relations of the seman-
tic network, while a measure of semantic related-
ness also use relations of other types. Furthermore,
measures (1), (2), (3), are ?pure? semantic similar-
ity measures since they use only semantic network,
while (3), (4), and (5) combine information from a
semantic network and a corpus.
The corpus-based and web-based measures are
calculated differently, but they are both clearly dis-
tributional in nature. In that respect, the web-based
measures use the Web as a corpus. Figure 1 contains
2http://developer.yahoo.com/search/boss/
3http://www.factiva.com/
13
Figure 1: Classification of the measures used in the paper.
a more precise classification of the considered mea-
sures, according to their properties. Finally, both (8)
and (9)-(12), rely on the vector space model.
2.6 Experimental Setup
We experiment with the knowledge-based measures
implemented in the WORDNET::SIMILARITY pack-
age (Pedersen et al, 2004). Our own implemen-
tation is used in the experiments with the corpus-
based measures and the web-based measures rely-
ing on the YAHOO BOSS search engine API. We
use the MEASURES OF SEMANTIC RELATEDNESS
web service 4 to assess the other web measures.
The evaluation was done with the BLESS set
of semantic relations. It relates 200 target con-
cepts to some 8625 relatum concepts with 26554 se-
mantic relations (14440 are correct and 12154 are
random). Every relation has one of the following
six types: hyponymy, co-hyponymy, meronymy, at-
tribute, event, and random. The distribution of re-
lations among those types is given in table 1. Each
concept is a single English word.
3 Results
3.1 Comparing General Performance of the
Similarity Measures
In our evaluation semantic relations extraction was
viewed as a retrieval task. Therefore, for every met-
ric we calculated precision, recall, and F1-measure
with respect to the golden standard. Let R? be set of
extracted semantic relations, and R be set of seman-
tic relations in the BLESS. Then
Precision =
|R ? R?|
|R?|
, Recall =
|R ? R?|
|R|
.
An extracted relation ?ci, t?, cj? ? R? matches a re-
lation from the evaluation dataset ?ci, t, cj? ? R if
4http://cwl-projects.cogsci.rpi.edu/msr/
Figure 2: Precision-recall graph of the six similarity mea-
sures (kNN threshold value k = 0? 52%).
t 6= random. Thus, an extracted relation is correct
if it has any type in BLESS, but random.
General performance of the measures is presented
in table 1 (columns 2-4). The Resnik measure (3) is
the best among the knowledge-based measures; the
NGD (13) measure relying on the Yahoo search en-
gine is the best results among the web-based mea-
sures. Finally, the cosine measure (9) (BDA-Cos) is
the best among all the measures. The table 2 demon-
strate some extracted relations discovered with the
BDA-Cos measure.
In table 1 we ranked the measures based on their
F-measure when precision is fixed at 80% (see fig-
ure 2). We have chosen this precision level, be-
cause it is a point when automatically extracted
relations start to be useful. It is clear from the
precision-recall graph (figure 2) that if another pre-
cision level is fixed then ranking of the metrics will
change. Analysis of this and similar plots for other
measures shows us that: (1) the best knowledge-
based metric is Resnik; (2) the BDA-Cos is the
best among the corpus-based measures, but BDA-
Jaccard is very close to it; (3) the three best web-
based measures are NGD-Google (within the preci-
sion range 100-90%), NGD-Factiva (within the pre-
cision range 90%-87%), and NGD-Yahoo (starting
from the precision level 87%). In these settings,
choose of the most suitable metric may depend on
the application. For instance, if just a few precise
relations are needed then NGD-Google is a good
choice. On the other hand, if we tolerate a slightly
less precision, and if we need many relations then
the BDA-Cos is the best choice.
Figure 3 depicts learning curve of the BDA-Cos
14
Figure 3: Learning curves of the BDA-Cos on the Wa-
Cypedia and PukWaC corpora (0.1M?2000M tokens).
Figure 4: Percent of co-hyponyms among all correctly
extracted relations for the six best measures.
measure.Dependence of the F-measure at the preci-
sion level of 80% from the corpus size is not linear.
F-measure improves up to 44% when we increase
corpus size from 1M to 10M tokens; increasing cor-
pus from 10M to 100M tokens gives the improve-
ment of 16%; finally, increasing corpus from 100M
to 2000M tokens gives the improvement of only 3%.
3.2 Comparing Relation Distributions of the
Similarity Measures
In this section, we are trying to figure out what
types of semantic relations the measures find. We
compare distributions of semantic relations against
the BLESS dataset. Generally, if two measures
have equal general performances, one may want to
choose a metric which provides more relations of a
certain type, depending on the application. This in-
formation may be also valuable in order to decide
which metrics to combine in a meta-metric.
Distribution of Relation Types. In this sec-
tion, we estimate empirical relation distribution of
the metrics over five relation types: hyponymy, co-
hyponymy, meronymy, attribute, and event. To do so
we calculate percents of correctly extacted relations
of type t for a each measure:
Percent =
R?t
|R ? R?|
, where
?
t?T
R?t = |R ? R?|.
Here |R ? R?| is a set of all correctly extracted rela-
tions, and R?t is a set of extracted relations of type t.
Figure 4 demonstrates that percent of extracted rela-
tions of certain type depends on the value of k (c.f.
section 2.1). For instance, if k = 10% then 77%
of extracted relations by Resnik are co-hyponyms,
but if k = 40% then the same measure outputs 40%
of co-hyponyms. We report relations distribution at
two levels of the threshold k ? 10% and 40%.
The empirical distributions are reported in
columns 5-9 of the table 1. Each of those columns
correspond to one semantic relation type t, and con-
tains two numbers: p10 ? percent of relations of type
t when k = 10%, and p40 ? percent of relations of
type t when k = 40%. We represent those two val-
ues in the following format: p10|p40. For instance,
77|40 behind the Resnik measure means that when
k = 10% it extracts 77% of co-hypernyms, and
when k = 40% it extracts 40% of co-hypernyms.
If the threshold k is 10% then the biggest frac-
tion of extracted relations are co-hyponyms ? from
35% for BDA-Manhattan to 77% for Resnik mea-
sure. At this threshold level, the knowledge-based
measures mostly return co-hyponyms (60% in aver-
age) and hyperonyms (23% in average). The corpus-
based metrics mostly return co-hyponyms (38% in
average) and event relations (26% in average). The
web-based measures return many (48% in average)
co-hyponymy relations.
If the threshold k is 40% then relation distribution
of all the measures significantly changes. Most of
the relations returned by the knowledge-based mea-
sures are co-hyponyms (36%) and meronyms (24%).
The majority of relations discovered by the corpus-
based metrics are co-hyponyms (33% ), event rela-
tions (26%), and meronyms (20.33%). The web-
based measures at this threshold value return many
event relations (32%).
15
General Performance Semantic Relations Distribution
Measure k Recall F1 hyper,% coord,% attri,% mero,% event,%
Resnik 40% 0.59 0.68 9 | 14 77 | 40 4 | 8 6 | 22 4 | 15
Inv.Edge-Counts 38% 0.56 0.66 22 | 15 61 | 40 4 | 8 7 | 22 6 | 15
Leacock-Chodorow 38% 0.56 0.66 22 | 15 61 | 40 4 | 8 7 | 22 6 | 15
Wu Palmer 37% 0.54 0.65 20 | 15 64 | 42 3 | 8 7 | 22 5 | 13
Lin 36% 0.53 0.64 30 | 16 52 | 31 4 | 7 8 | 29 5 | 16
Gloss Overlap 36% 0.53 0.63 5 | 6 52 | 34 7 | 12 18 | 21 18 | 27
Jiang-Conrath 35% 0.52 0.63 38 | 16 45 | 30 4 | 6 8 | 29 5 | 18
Extended Lesk 30% 0.45 0.57 21 | 14 39 | 30 1 | 9 29 | 28 9 | 19
BDA-Cos 52% 0.76 0.78 9 | 7 42 | 27 11 | 20 15 | 17 23 | 30
BDA-Jaccard 51% 0.75 0.77 10 | 7 45 | 27 8 | 16 16 | 20 20 | 27
BDA-Manhattan 37% 0.54 0.65 7 | 6 35 | 24 17 | 22 10 | 15 31 | 34
BDA-Euclidian 21% 0.30 0.44 7 | 7 31 | 18 20 | 26 12 | 13 30 | 37
NGD-Yahoo 46% 0.68 0.74 7 | 6 51 | 30 9 | 18 17 | 20 15 | 25
NGD-Factiva 47% 0.66 0.72 10 | 8 44 | 28 8 | 19 23 | 22 16 | 25
NGD-YahooBOSS 35% 0.51 0.63 13 | 10 54 | 36 4 | 10 14 | 20 15 | 22
NGD-Google 33% 0.48 0.60 1 | 7 41 | 28 45 | 19 2 | 19 11 | 28
NGD-Google-W 29% 0.43 0.56 8 | 9 45 | 31 8 | 14 20 | 21 19 | 25
PMI-YahooBOSS 29% 0.43 0.56 15 | 12 53 | 38 3 | 9 15 | 20 13 | 20
PMI-Factiva 25% 0.28 0.44 8 | 8 42 | 30 10 | 17 21 | 20 18 | 24
PMI-Google 12% 0.18 0.29 8 | 8 55 | 35 7 | 15 17 | 21 12 | 22
PMI-Google-W 9% 0.13 0.23 12 | 11 47 | 38 7 | 11 20 | 20 13 | 19
Random measure 8 | 9 24 | 25 20 | 19 22 | 20 26 | 27
BLESS dataset 9 25 20 19 27
Table 1: Columns 2-4: Recall and F-measure when Precision= 0.8 (correct relations of all types vs random relations).
Columns 5-9: percent of extracted relations of a certain type with respect to all correctly extracted relations, when
threshold k equal 10% or 40%. The best measure are sorted by F-measure; the best measures are in bold.
ant banana fork missile salmon
cockroach (coord) mango (coord) prong (mero) warhead (mero) trout (coord)
grasshopper (coord) pineapple (coord) spoon (coord) weapon (hyper) mackerel (coord)
silverfish (coord) papaya (coord) knife (coord) deploy (event) herring (coord)
wasp (coord) pear (coord) lift (event) nuclear (attri) fish (event)
insect (hyper) ripe (attri) fender (random) bomb (coord) tuna (coord)
arthropod (hyper) peach (coord) plate (coord) destroy (event) oily (attri)
industrious (attri) coconut (coord) rake (coord) rocket (coord) poach (event)
ladybug (coord) fruit (hyper) shovel (coord) arm (hyper) catfish (coord)
bee (coord) apple (coord) handle (mero) propellant (mero) catch (event)
beetle (coord) apricot (coord) sharp (attri) bolster (random) fresh (attri)
locust (coord) strawberry (coord) spade (coord) launch (event) cook (event)
dragonfly (coord) ripen (event) napkin (coord) deadly (attri) cod (coord)
hornet (coord) plum (coord) cutlery (hyper) country (random) smoke (event)
creature (hyper) grapefruit (coord) head (mero) strike (event) seafood (hyper)
crawl (event) cherry (coord) scissors (coord) defuse (event) eat (event)
Table 2: Examples of the discovered semantic relations with the bag-of-words distributional analysis (BDA-Cos).
16
Interestingly, for the most of the measures, per-
cent of extracted hyponyms and co-hyponyms de-
creases as the value of k increase, while the percent
of other relations increases. In order to make it clear,
we grayed cells of the table 1 when p10 ? p40.
Similarity to the BLESS Distribution. In this
section, we check if relation distributions (see ta-
ble 1) are completely biased by the distribution in
the evaluation dataset. We compare relation dis-
tributions of the metrics with the distribution in
the BLESS on the basis of the ?2 goodness of fit
test 5 (Agresti, 2002) with df = 4. A random simi-
larity measure is completely biased by the distribu-
tion in the evaluation dataset: ?2 = 5.36, p = 0.252
for k = 10% and ?2 = 3.17, p = 0.53 for k = 40%.
On the other hand, distributions of all the 21 mea-
sures are significantly different from the distribution
in the BLESS (p < 0.001). The value of chi-square
statistic varies from ?2 = 89.94 (NGD-Factiva,
k = 10%) to ?2 = 4000 (Resnik, k = 10%).
Independence of Relation Distributions. In this
section, we check whether relation distributions of
the various measures are significantly different. In
order to do so, we perform the chi-square indepen-
dence test on the table 1. Our experiments shown
that there is a significant interaction between the
type of the metric and the relations distribution:
?2 = 10487, p < 0.001, df = 80 for all the metrics;
?2 = 2529, df = 28, p < 0.001 for the knowledge-
based metrics; ?2 = 245, df = 12, p < 0.001 for
the corpus-based metrics; and ?2 = 3158, df =
32, p < 0.001 for the web-based metrics. Thus,
there is a clear dependence between the type of mea-
sure and the type of relation it extracts.
Most Similar and Dissimilar Measures. In this
section, we would like to find the most similar and
disimilar measures. This information is particularly
useful for the combination of the metrics. In order to
find redundant measures, we calculate distance xij
beween measures simi and simj , based on the ?2-
statistic:
xij = xji =
?
t? T
(|R?it| ? |R?
j
t |)
2
|R?jt |
, (15)
where R?it is ensemble of correctly extracted rela-
5Here and below, we calculate the ?2 statistic from the table
1 (columns 5-9), where percents are replaced with frequencies.
tions of type t with measure simi. We calculate
these distances for all pairs of measures and then
rank the pairs according to the value of xij . Ta-
ble 3 present list of the most similar and dissimi-
lar metrics obtained this way. Figure 7 reports in a
compact way all the pairwise similarities (xij)21?21
between the 21 metrics. In this graph, an edge
links two measures, which have the distance value
xij < 220. The graph was drawn with the Fruchter-
man and Reingold (1991) force-directed layout al-
gorithm. One can see that relation distributions of
the web- and corpus-based measures are quite sim-
ilar. The knowledge-based measures are much dif-
ferent from them, but similar among themselves.
Distribution of Similarity Scores. In this sec-
tion, we compare distributions of similarity scores
across relation types with the following procedure:
(1) Pick a closest relatum concept cj per relation
type t for each target concept ci. (2) Convert sim-
ilarity scores associated to each target concept to z-
scores. (3) Summarize the distribution of similari-
ties across relations by plotting the z-scores grouped
by relations in a box plot. (4) Verify the statistical
significance of the differences in similarity scores
across relations by performing the Tukey?s HSD test.
Figure 6 presents the distributions of similarities
across various relation types for Resnik, BDA-Cos,
and NGD-Yahoo. First, meaningful relation types
for these three measures are significantly different
(p < 0.001) from random relations. The only ex-
ception is the Resnnik measure ? its similarity scores
for the attribute relations are not significantly differ-
ent (p = 0.178) from random relations. Thus, the
best three measures provide scores which let us sep-
arate incorrect relations from the correct ones if an
appropriate threshold k is set. Second, the similar-
ity scores have highest values for the co-hyponymy
relations. Third, BDA-Cos, BDA-Jaccard, NGD-
Yahoo, NGD-Factiva, and PMI-YahooBoss provide
the best scores. They let us clearly (p < 0.001) sep-
arate meaningful relations from the random ones.
From the other hand, the poorest scores were pro-
vided by BDA-Manhattan, BDA-Euclidian, NGD-
YahooBoss, and NGD-Google, because their scores
let us clearly separate only co-hyponyms from the
random relations.
Corpus Size. Table 1 presented relation distribu-
tion of the BDA-Cos trained on the 2000M token
17
Figure 5: Semantic relations distribution function of cor-
pus size (BDA-Cos measure, PukWaC corpus).
corpus UKWAC. Figure 5 shows the relation dis-
tribution function of the corpus size. First, if cor-
pus size increases then percent of attribute relations
decreases, while percent of co-hyponyms increases.
Second, corpus size does not drastically influence
the distribution for big corpora. For instance, if
we increase corpus size from 100M to 2000M to-
kens then the percent of relations change on 3% for
attributes, on 3% co-hyponyms, on 1% events, on
0.7% hyperonyms, and on 0.4% meronyms.
4 Related Work
Prior research provide us information about gen-
eral performances of the measures considered in
this paper, but not necessarily on the task of se-
mantic relations extraction. For instance, Mihal-
cea et al (2006) compare two corpus-based (PMI-IR
and LSA) and six knowledge-based measures on the
task of text similarity computation. The authors re-
port that PMI-IR is the best measure; that, similarly
to our results, Resnik is the best knowledge-based
measure; and that simple average over all 8 mea-
sures is even better than PMI-IR. Budanitsky and
Hirst (2001) report that Jiang-Conrath is the best
knowledge-based measure for the task of spelling
correction. Patwardhan and Pedersen (2006) eval-
uate six knowledge-based measures on the task of
word sense disambiguation and report the same re-
sult. This contradicts our results, since we found
Resnik to be the best knowledge-based measure.
Peirsman et al (2008) compared general per-
formances and relation distributions of distribu-
tional methods using a lexical database. Sahlgren
(2006) evaluated syntagmatic and paradigmatic bag-
of-word models. Our findings mostly fits well these
and other (e.g. Curran (2003) or Bullinaria and Levy
(2007)) results on the distributional analysis. Lind-
sey et al (2007) compared web-based measures.
Authors suggest that a small search domain is better
than the whole Internet. Our results partially confirm
this observation (NGD-Factiva outperforms NGD-
Google), and partially contradicts it (NGD-Yahoo
outperforms NGD-Factiva).
Van de Cruys (2010) evaluates syntactic, and bag-
of-words distributional methods and suggests that
the syntactic models are the best for the extraction of
tight synonym-like similarity. Wandmacher (2005)
reports that LSA produces 46.4% of associative rela-
tions, 15.2% of synonyms, antonyms, hyperonyms,
co-hyponyms, and meronyms, 5.6% of syntactic re-
lations, and 32.8% of erroneous relations. We can-
not compare these results to ours, since we did not
evaluate neither LSA nor syntactic models.
A common alternative to our evaluation method-
ology is to use the Spearman?s rank correlation
coefficient (Agresti, 2002) to compare the results
with the human judgments, such as those obtained
by Rubenstein and Goodenough (1965) or Miller
and Charles (1991).
5 Conclusion and Future Work
This paper has compared 21 similarity measures be-
tween lexical units on the task of semantic relation
extraction. We compared their general performances
and figured out that Resnik, BDA-Cos, and NGD-
Yahoo provide the best results among knowledge-
, corpus-, and web-based measures, correspond-
ingly. We also found that (1) semantic relation dis-
tributions of the considered measures are signifi-
cantly different; (2) all measures extract many co-
hyponyms; (3) the best measures provide the scores
which let us clearly separate correct relations from
the random ones.
The analyzed measures provide complimentary
types of semantic information. This suggests de-
veloping a combined measure of semantic similar-
ity. A combined measure is not presented here since
designing an integration technique is a complex re-
search goal on its own right. We will address this
problem in our future research.
18
Figure 6: Distribution of similarities accross relation types for Resnik, BDA-Cos, and NGD-Yahoo measures.
Most Similar Measures Most Disimilar Measures
simi simj xij simi simj xij
Leacock-Chodorow Inv.Edge-Counts 0 NGD-Google Extended Lesk 39935.16
BDA-Jaccard BDA-Cos 7.17 Jiang-Conrath NGD-Google 27478.90
NGD-YahooBOSS PMI-YahooBOSS 19.58 Lin NGD-Google 17527.22
Wu-Palmer Inv.Edge-Counts 24.00 NGD-Google Wu-Palmer 17416.95
Wu-Palmer Leacock-Chodorow 24.00 NGD-Google PMI-YahooBOSS 13390.66
BDA-Manhattan BDA-Euclidian 25.37 Inv.Edge-Counts NGD-Google 12012.79
PMI-Google-W NGD-Factiva 27.65 Leacock-Chodorow NGD-Google 12012.79
PMI-Google NGD-Yahoo 33.42 NGD-Google Resnik 11750.41
NGD-Google-W NGD-Factiva 40.03 NGD-Google NGD-YahooBOSS 11556.69
NGD-W PMI-Factiva 42.17 BDA-Euclidian Extended Lesk 8411.66
Gloss Overlap NGD-Yahoo 53.64 NGD-Factiva NGD-Google 8066.75
NGD-Factiva PMI-Factiva 58.13 BDA-Euclidian Resnik 6829.71
Lin Jiang-Conrath 58.42 PMI-Google-W NGD-Google 6574.62
Gloss Overlap NGD-Google-W 62.46 BDA-Manhattan Extended Lesk 6428.47
Table 3: List of the most and least similar measures (k = 10%).
Figure 7: Measures grouped according to similarity of their relation distributions with (15). An edge links measures
simi and simj if xij < 220. The knowledge-, corpus-, and web-based measures are marked in red, blue, and green
correspondingly and with the prefixes ?K?,?C?,and ?W?. The best measures are marked with a big circle.
19
6 Acknowledgments
I would like to thank Thomas Franc?ois who kindly
helped with the evaluation methodology, and my
supervisor Dr. Ce?drick Fairon. The two anony-
mous reviewers, Ce?drick Fairon, Thomas Franc?ois,
Jean-Leon Bouraoui, and Andew Phillipovich pro-
vided comments and remarks, which considerably
improved quality of the paper. This research is sup-
ported by Wallonie-Bruxelles International.
References
Alan Agresti. Categorical Data Analysis (Wiley Se-
ries in Probability and Statistics). Wiley series
in probability and statistics. Wiley Interscience,
Hoboken, NJ, 2 edition, 2002.
Satanjeev Banerjee and Ted Pedersen. Extended
gloss overlaps as a measure of semantic related-
ness. In International Joint Conference on Ar-
tificial Intelligence, volume 18, pages 805?810,
2003.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. The wacky wide web: A col-
lection of very large linguistically processed web-
crawled corpora. Language Resources and Eval-
uation, 43(3):209?226, 2009.
Alexander Budanitsky and Graeme Hirst. Se-
mantic distance in WordNet: An experimental,
application-oriented evaluation of five measures.
In Workshop on WordNet and Other Lexical Re-
sources, volume 2, 2001.
John A. Bullinaria and Joseph P. Levy. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39(3):510, 2007.
Rudi L. Cilibrasi and Paul M. B. Vitanyi. The
Google Similarity Distance. IEEE Trans. on
Knowl. and Data Eng., 19(3):370?383, 2007.
James R. Curran. From distributional to semantic
similarity. PhD thesis, University of Edinburgh,
2003.
Thomas M. J. Fruchterman and Edward M. Rein-
gold. Graph drawing by force-directed placement.
Software: Practice and Experience, 21(11):1129?
1164, 1991.
Gregory Grefenstette. Explorations in Automatic
Thesaurus Discovery (The Springer International
Series in Engineering and Computer Science).
Springer, 1 edition, 1994. ISBN 0792394682.
Marti A. Hearst. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings
of the 14th conference on Computational linguis-
tics, pages 539?545, Morristown, NJ, USA, 1992.
Association for Computational Linguistics.
Jay J. Jiang and David W. Conrath. Semantic Simi-
larity Based on Corpus Statistics and Lexical Tax-
onomy. In International Conference Research on
Computational Linguistics (ROCLING X), pages
19?33, 1997.
Daniel Jurafsky and James H. Martin. Speech and
Language Processing: An Introduction to Nat-
ural Language Processing, Computational Lin-
guistics, and Speech Recognition. Prentice Hall,
2009.
Claudia Leacock and Martin Chodorow. Combin-
ing Local Context and WordNet Similarity for
Word Sense Identification. An Electronic Lexical
Database, pages 265?283, 1998.
Dekang Lin. Automatic retrieval and clustering of
similar words. In Proceedings of the 17th interna-
tional conference on Computational linguistics-
Volume 2, pages 768?774. Association for Com-
putational Linguistics, 1998a.
Dekang Lin. An Information-Theoretic Definition
of Similarity. In In Proceedings of the 15th Inter-
national Conference on Machine Learning, pages
296?304, 1998b.
Robert Lindsey, Vladislav D. Veksler, Alex
Grintsvayg, and Wayne D. Gray. Be wary of what
your computer reads: the effects of corpus selec-
tion on measuring semantic relatedness. In 8th
International Conference of Cognitive Modeling,
ICCM, 2007.
Rado Mihalcea, Corley Corley, and Carlo Strappa-
rava. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceed-
ings of the National Conference on Artificial In-
telligence, volume 21, page 775. Menlo Park,
CA; Cambridge, MA; London; AAAI Press; MIT
Press, 2006.
20
George A. Miller. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39?
41, 1995.
George A. Miller and Walter G. Charles. Contextual
correlates of semantic similarity. Language and
Cognitive Processes, 6(1):1?28, 1991.
George A. Miller, Claudia Leacock, Randee Tengi,
and Ross T. Bunker. A semantic concordance.
In Proceedings of the workshop on Human Lan-
guage Technology, pages 303?308. Association
for Computational Linguistics, 1993.
Alexander Panchenko. Can we automatically re-
produce semantic relations of an information re-
trieval thesaurus? In 4th Russian Summer School
in Information Retrieval, pages 13?18. Voronezh
State University, 2010.
Siddharth Patwardhan and Ted Pedersen. Using
WordNet-based context vectors to estimate the se-
mantic relatedness of concepts. Making Sense of
Sense: Bringing Psycholinguistics and Computa-
tional Linguistics Together, page 1, 2006.
Ted Pedersen, Siddharth Patwardhan, and Jason
Michelizzi. WordNet:: Similarity: measuring the
relatedness of concepts. In Demonstration Papers
at HLT-NAACL 2004 on XX, pages 38?41. Asso-
ciation for Computational Linguistics, 2004.
Yves Peirsman, Kris Heylen, and Dirk Speelman.
Putting things in order. First and second order
context models for the calculation of semantic
similarity. Proceedings of the 9th Journe?es in-
ternationales d?Analyse statistique des Donne?es
Textuelles (JADT 2008), pages 907?916, 2008.
Philip Resnik. Using Information Content to Eval-
uate Semantic Similarity in a Taxonomy. In Pro-
ceedings of the 14th International Joint Confer-
ence on Artificial Intelligence., volume 1, pages
448?453, 1995.
H. Rubenstein and J.B. Goodenough. Contextual
correlates of synonymy. Communications of the
ACM, 8(10):627?633, 1965.
Magnus Sahlgren. The Word-Space Model: Us-
ing distributional analysis to represent syntag-
matic and paradigmatic relations between words
in high-dimensional vector spaces. PhD thesis,
Stockholm University, 2006.
Helmut Schmid. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. pages 44?49, 1994.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
Learning syntactic patterns for automatic hyper-
nym discovery. Advances in Neural Information
Processing Systems (NIPS), 17:1297?1304, 2004.
Peter Turney. Mining the Web for Synonyms: PMI-
IR versus LSA on TOEFL. In Proceedings of the
twelfth european conference on machine learning
(ecml-2001), 2001.
Tim Van de Cruys. Mining for Meaning: The Ex-
traction of Lexicosemantic Knowledge from Text.
PhD thesis, University of Groningen, 2010.
Tonio Wandmacher. How semantic is Latent Seman-
tic Analysis? Proceedings of TALN/RECITAL,
2005.
Zhibiao Wu and Martha Palmer. Verbs seman-
tics and lexical selection. In Proceedings of the
32nd annual meeting on Association for Compu-
tational Linguistics, pages 133?138. Association
for Computational Linguistics, 1994.
21
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 10?18,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
A Study of Hybrid Similarity Measures for Semantic Relation Extraction
Alexander Panchenko and Olga Morozova
Center for Natural Language Processing (CENTAL)
Universite? catholique de Louvain, Belgium
{alexander.panchenko, olga.morozova}@uclouvain.be
Abstract
This paper describes several novel hybrid
semantic similarity measures. We study
various combinations of 16 baseline mea-
sures based on WordNet, Web as a cor-
pus, corpora, dictionaries, and encyclope-
dia. The hybrid measures rely on 8 com-
bination methods and 3 measure selection
techniques and are evaluated on (a) the task
of predicting semantic similarity scores and
(b) the task of predicting semantic relation
between two terms. Our results show that
hybrid measures outperform single mea-
sures by a wide margin, achieving a correla-
tion up to 0.890 and MAP(20) up to 0.995.
1 Introduction
Semantic similarity measures and relations are
proven to be valuable for various NLP and IR
applications, such as word sense disambiguation,
query expansion, and question answering.
Let R be a set of synonyms, hypernyms, and
co-hyponyms of terms C, established by a lexi-
cographer. A semantic relation extraction method
aims at discovering a set of relations R? approx-
imating R. The quality of the relations provided
by existing extractors is still lower than the quality
of the manually constructed relations. This moti-
vates the development of new relation extraction
methods.
A well-established approach to relation extrac-
tion is based on lexico-syntactic patterns (Auger
and Barrie`re, 2008). In this paper, we study an al-
ternative approach based on similarity measures.
These methods do not return a type of the rela-
tion between words (R? ? C ? C). However,
we assume that the methods should retrieve a mix
of synonyms, hypernyms, and co-hyponyms for
practical use in text processing applications and
evaluate them accordingly.
A multitude of measures was used in the pre-
vious research to extract synonyms, hypernyms,
and co-hyponyms. Five key approaches are those
based on a distributional analysis (Lin, 1998b),
Web as a corpus (Cilibrasi and Vitanyi, 2007),
lexico-syntactic patterns (Bollegala et al, 2007),
semantic networks (Resnik, 1995), and defini-
tions of dictionaries or encyclopedias (Zesch et
al., 2008a). Still, the existing approaches based on
these single measures are far from being perfect.
For instance, Curran and Moens (2002) compared
distributional measures and reported Precision@1
of 76% for the best one. For improving the per-
formance, some attempts were made to combine
single measures, such as (Curran, 2002; Ceder-
berg and Widdows, 2003; Mihalcea et al, 2006;
Agirre et al, 2009; Yang and Callan, 2009). How-
ever, most studies are still not taking into account
the whole range of existing measures, combining
mostly sporadically different methods.
The main contribution of the paper is a system-
atic analysis of 16 baseline measures, and their
combinations with 8 fusion methods and 3 tech-
niques for the combination set selection. We are
first to propose hybrid similarity measures based
on all five extraction approaches listed above; our
combined techniques are original as they exploit
all key types of resources usable for semantic re-
lation extraction ? corpus, web corpus, semantic
networks, dictionaries, and encyclopedias. Our
experiments confirm that the combined measures
are more precise than the single ones. The best
found hybrid measure combines 15 baseline mea-
sures with the supervised learning. It outperforms
10
Figure 1: (a) Single and (b) hybrid relation extractors
based on similarity measures.
all tested single and combined measures by a large
margin, achieving a correlation of 0.870 with hu-
man judgements and MAP(20) of 0.995 on the re-
lation recognition task.
2 Similarity-based Relation Extraction
In this paper a similarity-based relation extraction
method is used. In contrast to the traditional
approaches, relying on a single measure, our
method relies on a hybrid measure (see Figure 1).
A hybrid similarity measure combines several
single similarity measures with a combination
method to achieve better extraction results. To
extract relations R? between terms C, the method
calculates pairwise similarities between them
with the help of a similarity measure. The
relations are established between each term
c ? C and the terms most similar to c (its nearest
neighbors). First, a term-term (C ? C) similarity
matrix S is calculated with a similarity measure
sim, as depicted in Figure 1 (a). Then, these
similarity scores are mapped to the interval [0; 1]
with a norm function as follows: S? = S?min(S)max(S) .
Dissimilarity scores are transformed into sim-
ilarity scores: S? = 1 ? norm(S). Finally,
the knn function calculates semantic relations
between terms with a k-NN thresholding: R? =
?|C|
i=1 {?ci, cj? : (cj ? top k% of ci) ? (sij > 0)} .
Here, k is a percent of top similar terms to a term
ci. Thus, the method links each term ci with k%
of its nearest neighbours.
3 Single Similarity Measures
A similarity measure extracts or recalls a sim-
ilarity score sij ? S between a pair of terms
ci, cj ? C. In this section we list 16 baseline
measures exploited by hybrid measures. The mea-
sures were selected as (a) the previous research
suggests that they are able to capture synonyms,
hypernyms, and co-hyponyms; (b) they rely on all
main resources used to derive semantic similarity
? semantic networks, Web as a corpus, traditional
corpora, dictionaries, and encyclopedia.
3.1 Measures Based on a Semantic Network
We test 5 measures relying on WORDNET seman-
tic network (Miller, 1995) to calculate the simi-
larities: Wu and Palmer (1994) (1), Leacock and
Chodorow (1998) (2), Resnik (1995) (3), Jiang
and Conrath (1997) (4), and Lin (1998a) (5).
These measures exploit the lengths of the short-
est paths between terms in a network and proba-
bility of terms derived from a corpus. We use im-
plementation of the measures available in WORD-
NET::SIMILARITY (Pedersen et al, 2004).
A limitation of these measures is that similari-
ties can only be calculated upon 155.287 English
terms from WordNet 3.0. In other words, these
measures recall rather than extract similarities.
Therefore, they should be considered as a source
of common lexico-semantic knowledge for a hy-
brid semantic similarity measure.
3.2 Web-based Measures
Web-based metrics use Web search engines for
calculation of similarities. They rely on the num-
ber of times the terms co-occur in the documents
as indexed by an information retrieval system.
We use 3 baseline web measures based on index
of YAHOO! (6), BING (7), and GOOGLE over
the domain wikipedia.org (8). These three
measures exploit Normalized Google Distance
(NGD) formula (Cilibrasi and Vitanyi, 2007) for
transforming the number of hits into a similarity
score. Our own system implements BING mea-
sure, while Measures of Semantic Relatedness
(MSR) web service1 calculates similarities with
YAHOO! and GOOGLE.
The coverage of languages and vocabularies by
web-based measures is huge. Therefore, it is as-
sumed that they are able to extract new lexico-
semantic knowledge. Web-based measures are
limited by constraints of a search engine API
(hundreds of thousands of queries are needed).
1http://cwl-projects.cogsci.rpi.edu/msr/
11
3.3 Corpus-based Measures
We tested 5 measures relying on corpora to cal-
culate similarity of terms: two baseline distri-
butional measures, one novel measure based on
lexico-syntactic patterns, and two other baseline
measures. Each of them uses a different corpus.
Corpus-based measures are able to extract sim-
ilarity between unknown terms. Extraction capa-
bilities of these measures are limited by a corpus.
If terms do not occur in a text, then it would be
impossible to calculate similarities between them.
Distributional Measures
These measures are based on a distributional
analysis of a 800M tokens corpus WACYPE-
DIA (Baroni et al, 2009) tagged with TREETAG-
GER and dependency-parsed with MALTPARSER.
We rely on our own implementation of two distri-
butional measures. The distributional measure (9)
performs Bag-of-words Distributional Analysis
(BDA) (Sahlgren, 2006). We use as features the
5000 most frequent lemmas (nouns, adjectives,
and verbs) from a context window of 3 words,
excluding stopwords. The distributional measure
(10) performs Syntactic Distributional Analysis
(SDA) (Lin, 1998b). For this one, we use as
features the 100.000 most frequent dependency-
lemma pairs. In our implementation of SDA a
term ci is represented with a feature ?dtj , wk?,
if wk is not in a stoplist and dtj has one of the
following dependency types: NMOD, P, PMOD,
ADV, SBJ, OBJ, VMOD, COORD, CC, VC, DEP,
PRD, AMOD, PRN, PRT, LGS, IOBJ, EXP, CLF,
GAP . For both BDA and SDA: the feature matrix
is normalized with PointwiseMutual Information;
similarities between terms are calculated with a
cosine between their respective feature vectors.
Pattern-based Measure
We developed a novel similarity measure Pat-
ternWiki (13), which relies on 10 lexico-syntactic
patterns. 2 First, we apply the patterns to the WA-
CYPEDIA corpus and get as a result a list of con-
cordances (see below). Next, we select the con-
cordances which contain at least two terms from
the input vocabulary C. The semantic similarity
sij between each two terms ci, cj ? C is equal
to the number of their co-occurences in the same
concordance.
The set of the patterns we used is a compilation
2Available at http://http://cental.fltr.ucl.ac.
be/team/?morozova/pattern-wiki.tar.gz
of the 6 classical Hearst (1992) patterns, aiming at
the extraction of hypernymic relations, as well as
3 patterns retrieving some other hypernyms and
co-hyponyms and 1 synonym extraction pattern,
which we found in accordance with Hearst?s pat-
tern discovery algorithm. The patterns are en-
coded in a form of finite-state transducers with the
help of a corpus processing tool UNITEX 3 (Pau-
mier, 2003). The main graph is a cascade of the
subgraphs, each of which encodes one of the pat-
terns. For example, Figure 2 presents the graph
which extracts, e. g.:
? such diverse {[occupations]} as
{[doctors]}, {[engineers]} and
{[scientists]}[PATTERN=1]
Figure brackets mark the noun phrases, which are
in the semantic relation, nouns and compound
nouns stand between the square brackets. Uni-
tex enables the exclusion of meaningless adjec-
tives and determiners out of the tagging, while
the patterns containing them are still being recog-
nized. So, the notion of a pattern has more general
sense with respect to other works such as (Bolle-
gala et al, 2007), where each construction with
a different lexical item, a word form or even a
punctuation mark is regarded as a unique pat-
tern. The nouns extracted from the square brack-
ets are lemmatized with the help of DELA dictio-
nary4, which consists of around 300,000 simple
and 130,000 compound words. If the noun to ex-
tract is a plural form of a noun in the dictionary,
then it is re-written into the respective singular
form. Semantic similarity score is equal to the
number of co-occurences of terms in the square
brackets within the same concordance (the num-
ber of extractions between the terms).
Other Corpus-based Measures
In addition to the three measures presented
above, we use two other corpus-based measures
available via the MSR web service. The mea-
sure (11) relies on the Latent Semantic Analysis
(LSA) (Landauer and Dumais, 1997) trained on
the TASA corpus (Veksler et al, 2008). LSA cal-
culates similarity of terms with a cosine between
their respective vectors in the ?concept space?.
The measure (12) relies on the NGD formula (see
Section 3.2), where counts are derived from the
Factiva corpus (Veksler et al, 2008).
3http://igm.univ-mlv.fr/?unitex/
4Available at http://infolingu.univ-mlv.fr/
12
Figure 2: An example of a UNITEX graph for hypernym extraction (subgraphs are marked with gray; <E>
defines zero; <DET> defines determiners; bold symbols and letters outside of the boxes are annotation tags)
3.4 Definition-based Measures
We test 3 measures which rely on explicit defini-
tions of terms specified in dictionaries. The first
metric WktWiki (14) is a novel similarity measure
that stems from the Lesk algorithm (Pedersen et
al., 2004) and the work of Zesch et al (2008a).
WktWiki operates on Wiktionary definitions and
relations and Wikipedia abstracts. WktWiki cal-
culates similarity as follows. First, definitions for
each input term c ? C are built. A ?definition?
is a union of all available glosses, examples, quo-
tations, related words, and categories from Wik-
tionary and a short abstract of the corresponding
Wikipedia article (a name of the article must ex-
actly match the term c). We use all senses corre-
sponding to a surface form of term c. Then, each
term c ? C of the 1000 most frequent lemmas
is represented as a bag-of-lemma vector, derived
from its ?definition?. Feature vectors are normal-
ized with Pointwise Mutual Information and simi-
larities between terms are calculated with a cosine
between them. Finally, the pairwise similarities
between terms S are corrected. The highest simi-
larity score is assigned to the pairs of terms which
are directly related in Wiktionary. 5
WktWiki is different to the work of Zesch et al
(2008b) in three aspects: (a) terms are represented
in a word space, and not in a document space;
(b) both texts from Wiktionary and Wikipedia are
used; (c) relations of Wiktionary are used to up-
date similarity scores.
In addition to WktWiki, we operate with 2
baseline measures relying on WordNet glosses
available in a WORDNET::SIMILARITY package:
Gloss Vectors (Patwardhan and Pedersen, 2006)
5We used JWKTL library (Zesch et al, 2008a), as an API to
Wiktionary and DBpedia.org as a source of Wikipedia short ab-
stracts (dumps were downloaded in October 2011).
(15) and Extended Lesk (Banerjee and Pedersen,
2003) (16). The key difference between WktWiki
and WordNet-based measures is that the latter
uses definitions of related terms.
Extraction capabilities of definition-based mea-
sures are limited by the number of available def-
initions. As of October 2011, WordNet con-
tains 117.659 definitions (glosses); Wiktionary
contains 536.594 definitions in English and
4.272.902 definitions in all languages; Wikipedia
has 3.866.773 English articles and around 20.8
millons of articles in all languages.
4 Hybrid Similarity Measures
A hybrid similarity measure combines several sin-
gle similarity measures described above with one
of the combination methods described below.
4.1 Combination Methods
A goal of a combination method is to produce
similarity scores which perform better than the
scores of input single measures. A combination
method takes as an input a set of similarity ma-
trices {S1, . . . ,SK} produced by K single mea-
sures and outputs a combined similarity matrix
Scmb. We denote as skij a pairwise similarity score
of terms ci and cj produced by k-th measure. We
test the 8 following combination methods:
Mean. A mean ofK pairwise similarity scores:
Scmb =
1
K
K
?
k=1
Sk ? scmbij =
1
K
?
k=1,K
skij .
Mean-Nnz. A mean of those pairwise similar-
ity scores which have a non-zero value:
scmbij =
1
|k : skij > 0, k = 1,K|
?
k=1,K
skij .
13
Mean-Zscore. A mean of K similarity scores
transformed into Z-scores:
scmbij =
1
K
K
?
k=1
skij ? ?k
?k
,
where ?k is a mean and ?k is a standard deviation
of similarity scores of k-th measure (Sk).
Median. A median of K pairwise similarities:
scmbij = median(s1ij , . . . , sKij ).
Max. A maximum of K pairwise similarities:
scmbij = max(s1ij , . . . , sKij ).
Rank Fusion. First, this combination method
converts each pairwise similarity score skij to a
rank rkij . Here, rkij = 5 means that term cj is the
5-th nearest neighbor of the term ci, according to
the k-th measure. Then, it calculates a combined
similarity score as a mean of these pairwise ranks:
scmbij = 1K
?
k=1,K rkij .
Relation Fusion. This combination method
gathers and unites the best relations provided by
each measure. First, the method retrieves rela-
tions extracted by single measures with the func-
tion knn described in Section 2. We have empiri-
cally chosen an ?internal? kNN threshold of 20%
for this combination method. Then, a set of ex-
tracted relations Rk, obtained from the k-th mea-
sure, is encoded as an adjacency matrix Rk . An
element of this matrix indicates whether terms ci
and cj are related:
rkij =
{
1 if semantic relation ?ci, cj? ? Rk
0 else
The final similarity score is a mean of adjacency
matrices: Scmb = 1K
?K
i=1 Ri. Thus, if two mea-
sures are combined and the first extracted the re-
lation between ci and cj , while the second did not,
then the similarity sij will be equal to 0.5.
Logit. This combination method is based on
logistic regression (Agresti, 2002). We train a bi-
nary classifier on a set of manually constructed
semantic relations R (we use BLESS and SN
datasets described in Section 5). Positive training
examples are ?meaningful? relations (synonyms,
hyponyms, etc.), while negative training examples
are pairs of semantically unrelated words (gener-
ated randomly and verified manually). A seman-
tic relation ?ci, cj? ? R is represented with a vec-
tor of pairwise similarities between terms ci, cj
calculated with K measures (s1ij , . . . , sKij ) and a
binary variable rij (category):
rij =
{
0 if ?ci, cj? is a random relation
1 otherwise
For evaluation purposes, we use a special 10-fold
cross validation ensuring that all relations of one
term c are always in the same training/test fold.
The results of the training are K + 1 coefficients
of regression (w0, w1, . . . , wK). We apply the
model to combine similarity measures as follows:
scmbij =
1
1 + e?z
, z = w0 +
K
?
k=1
wkskij .
4.2 Combination Sets
Any of the 8 combination methods presented
above may combine from 2 to 16 single
measures. Thus, there are
?16
m=2 Cm16 =
?16
m=2
16!
m!(16?m)! = 65535 ways to choose which
single measures to use in a combination method.
We apply three methods to find an efficient com-
bination of measures in this search space: expert
choice of measures, forward stepwise procedure,
and analysis of a logistic regression model.
Expert choice of measures is based on the an-
alytical and empirical properties of the measures.
We chose 5 or 9 measures which perform well and
rely on complimentary resources: corpus, Web,
WordNet, etc. Additionally, we selected a group
of all measures except for one which has shown
the worst results on all datasets. Thus, accord-
ing to this selection method we have chosen three
groups of measures (see Section 3 and Table 1 for
notation):
? E5 = {3, 9, 10, 13, 14}
? E9 = {1, 3, 9 ? 11, 13 ? 16}
? E15 = {1, 2, 3, 4, 5, 6, 8 ? 16}
Forward stepwise procedure is a greedy algo-
rithm which works as follows. It takes as an in-
put all measures, a method of their combination
such as Mean, and a criterion such as Precision
at k = 50. It starts with a void set of measures.
Then, at each iteration it adds to the combination
one measure which brings the biggest improve-
ment to the criterion. The algorithm stops when
no measure can improve the criteria. According
14
to this method, we have chosen four groups of the
measures 6:
? S7 = {9 ? 11, 13 ? 16}
? S8a = {9 ? 16}
? S8b = {1, 9 ? 11, 13 ? 16}
? S10 = {1, 6, 9 ? 16}
The last measure selection technique is based
on analysis of logistic regression trained on all 16
measures as features. Only measures with pos-
itive coefficients are selected. According to this
method, 12 measures were chosen:
? R12 = {3, 5, 6, 8 ? 16}
We test combination methods on the 8 sets of
measures specified above. Remarkably, all three
selection techniques constantly choose six fol-
lowing measures ? 9, 10, 11, 14, 15, 16, i. e., C-
BowDA, C-SynDA, C-LSA-Tasa, D-WktWiki,
N-GlossVectors, and N-ExtendedLesk.
5 Evaluation
Evaluation relies on human judgements about se-
mantic similarity and on manually constructed se-
mantic relations. 7
Human Judgements Datasets. This kind of
ground truth enables direct assessment of measure
performance and indirect assessment of extraction
quality with this measure. Each of these datasets
consists of N tuples ?ci, cj , sij?, where ci, cj are
terms, and sij is their similarity obtained by hu-
man judgement. We use three standard human
judgements datasets ? MC (Miller and Charles,
1991), RG (Rubenstein and Goodenough, 1965)
and WordSim353 (Finkelstein et al, 2001), com-
posed of 30, 65, and 353 pairs of terms respec-
tively. Let s = (si1, si2, . . . , siN ) be a vector of
ground truth scores, and s? = (s?i1, s?i2, . . . , s?iN )
be a vector of similarity scores calculated with a
similarity measure. Then, the quality of this mea-
sure is assessed with Spearman?s correlation be-
tween s and s?.
Semantic Relations Datasets. This kind
of ground truth enables indirect assessment of
measure performance and direct assessment of
6We used Mean as a hybrid measure and the following
criteria: MAP(20), MAP(50), P(10), P(20) and P(50). We
kept measures which were selected by most of the criteria.
7An evaluation script is available at http://cental.
fltr.ucl.ac.be/team/?panchenko/sre-eval/
extraction quality with the measure. Each
of these datasets consists of a set of seman-
tic relations R, such as ?agitator, syn, activist?,
?hawk , hyper, predator?, ?gun, syn,weapon?, and
?dishwasher, cohypo, reezer?. Each ?target? term
has roughly the same number of meaningful and
random relations. We use two semantic relation
datasets: BLESS (Baroni and Lenci, 2011) and
SN. The first is used to assess hypernyms and co-
hyponyms extraction. BLESS relates 200 target
terms (100 animate and 100 inanimate nouns) to
8625 relatum terms with 26554 semantic relations
(14440 are meaningful and 12154 are random).
Every relation has one of the following types: hy-
pernym, co-hyponym, meronym, attribute, event,
or random. We use the second dataset to evalu-
ate synonymy extraction. SN relates 462 target
terms (nouns) to 5910 relatum terms with 14682
semantic relations (7341 are meaningful and 7341
are random). We built SN from WordNet, Roget?s
thesaurus, and a synonyms database 8.
This kind of evaluation is based on the number
of correctly extracted relations with the method
described in Section 2. Let R?k be a set of ex-
tracted semantic relations at a certain level of
the kNN threshold k. Then, precision, recall,
and mean average precision (MAP) at k are cal-
culated correspondingly as follows: P (k) =
|R?R?k|
|R?k|
, R(k) = |R?R?k||R| ,M(k) =
1
k
?k
i=1 P (i).
The quality of a similarity measure is assessed
with the six following statistics: P (10), P (20),
P (50), R(50), M(20), and M(50).
6 Results
Table 1 and Figure 3 present performance of the
single and hybrid measures on the five ground
truth datasets listed above. The first three columns
of the table contain correlations with human
judgements, while the other columns present per-
formance on the relation extraction task.
The first part of the table reports on scores of
16 single measures. Our results show that the
measures are indeed complimentary ? there is no
measure which performs best on all datasets. For
instance, the measure based on a syntactic dis-
tributional analysis C-SynDA performed best on
the MC dataset achieving a correlation of 0.790;
the WordNet measure WN-LeacockChodorow
achieved the top score of 0.789 on the RG dataset;
8http://synonyms-database.downloadaces.com
15
Figure 3: Precision-Recall graphs calculated on the BLESS dataset of (a) 16 single measures and the best hybrid
measure H-Logit-E15; (b) 8 hybrid measures.
the corpus based measure C-NGD-Factiva was
best on the WordSim353 dataset, achieving a cor-
relation of 0.600. On the BLESS dataset, syn-
tactic distributional analysis C-SynDA performed
best for high precision among single measures
achieving MAP(20) of 0.984, while the bag-of-
words distributional measure C-BowDA was the
best for high recall with R(50) of 0.772. On
the SN dataset, the WordNet-based measure N-
WuPalmer was best both for precision and recall.
The second part of Table 1 presents perfor-
mance of the hybrid measures. Our results show
that if signals from complimentary resources are
used, then the retrieval of semantically similar
words is significantly improved. Most of the hy-
brid measures outperform the single measures on
all the datasets. We tested each of the 8 combina-
tion methods presented in Section 4.1 with each
of the 8 sets of measures specified in Section 4.2.
We report on the best metrics among all 64 hy-
brid measures. A notion H-Mean-S8a means that
the Mean combination method provides the best
results with the set of measures S8a.
Measures based on the mean of non-zero simi-
larities H-MeanNnz-S8a and H-MeanNnz-E5 per-
formed best on MC and WordSim353 datasets re-
spectively. They achieved correlations of 0.878
and 0.740, which is higher than scores of any
other measure. At the same time, measure H-
MeanZscore-S8b provided the best scores on the
RG dataset among all single and hybrid measures,
achieving correlation of 0.890. Supervised mea-
sure H-Logit-E15 based on Logistic Regression
provided the very best results on both semantic
relations datasets BLESS and SN. Furthermore, it
outperformed all single and hybrid measures on
that task, in terms of both precision and recall,
achieving MAP(20) of 0.995 and R(50) of 0.818
on BLESS and MAP(20) of 0.993 and R(50) of
0.819 on SN. H-Logit-E15 makes use of 15 simi-
larity measures and disregards only the worst sin-
gle measure W-NGD-Bing.
As we can see in Figure 3 (b), combining simi-
larity scores with a Max function appears to be the
worst solution. Combination methods based on an
average and a median, including Rank and Rela-
tion Fusion, perform much better. These methods
provide quite similar results: in the high precision
range, they perform nearly as well as a supervised
combination. Relation Fusion even manages to
slightly outperform Logit on the first 10-15 k-NN
(see Figure 3). However, all unsupervised com-
bination methods are significantly worse if higher
recall is needed.
We conclude that the H-Logit-E15 is the best
hybrid similarity measure for semantic relation
extraction and in terms of plausibility with human
judgements among all single and hybrid measures
examined in this paper.
7 Discussion
Hybrid measures achieve higher precision and re-
call than single measures. First, it is due to
the reuse of common lexico-semantic information
(such as that a ?car? is a synonym of a ?vehicle?)
via knowledge- and definition-based measures.
Measures based on WordNet and dictionary defi-
nitions achieve high precision as they rely on fine-
grained manually constructed resources. How-
ever, due to limited coverage of these resources,
16
Similarity Measure MC RG WS BLESS SN
? ? ? P(10) P (20) M(20) P(50) M(50) R(50) P(10) P(20) M(20) P(50) M(50) R(50)
Random 0.056 -0.047 -0.122 0.546 0.542 0.549 0.544 0.546 0.522 0.504 0.502 0.507 0.499 0.502 0.498
1. N-WuPalmer 0.742 0.775 0.331 0.974 0.929 0.972 0.702 0.879 0.674 0.982 0.959 0.981 0.766 0.917 0.763
2. N-Leack.Chod. 0.724 0.789 0.295 0.953 0.901 0.954 0.702 0.863 0.648 0.984 0.953 0.981 0.757 0.913 0.755
3. N-Resnik 0.784 0.757 0.331 0.970 0.933 0.970 0.700 0.879 0.647 0.948 0.908 0.948 0.724 0.874 0.722
4. N-JiangConrath 0.719 0.588 0.175 0.956 0.872 0.920 0.645 0.817 0.458 0.931 0.857 0.911 0.625 0.808 0.570
5. N-Lin 0.754 0.619 0.204 0.949 0.884 0.918 0.682 0.822 0.451 0.939 0.877 0.920 0.611 0.827 0.566
6. W-NGD-Yahoo 0.330 0.445 0.254 0.940 0.907 0.941 0.783 0.885 0.648 ? ? ? ? ? ?
7. W-NGD-Bing 0.063 0.181 0.060 0.724 0.706 0.713 0.650 0.690 0.600 0.659 0.619 0.671 0.633 0.648 0.633
8. W-NGD-GoogleWiki 0.334 0.502 0.251 0.874 0.837 0.872 0.703 0.814 0.649 ? ? ? ? ? ?
9. C-BowDA 0.693 0.782 0.466 0.971 0.947 0.969 0.836 0.928 0.772 0.974 0.932 0.968 0.742 0.896 0.740
10. C-SynDA 0.790 0.786 0.491 0.985 0.953 0.984 0.811 0.925 0.749 0.978 0.945 0.972 0.751 0.907 0.743
11. C-LSA-Tasa 0.694 0.605 0.566 0.968 0.937 0.967 0.802 0.912 0.740 0.903 0.846 0.895 0.641 0.803 0.609
12. C-NGD-Factiva 0.603 0.599 0.600 0.959 0.916 0.959 0.786 0.894 0.681 0.906 0.857 0.904 0.731 0.835 0.543
13. C-PatternWiki 0.461 0.542 0.357 0.972 0.951 0.976 0.944 0.957 0.287 0.920 0.904 0.907 0.891 0.900 0.295
14. D-WktWiki 0.759 0.754 0.521 0.943 0.905 0.946 0.750 0.876 0.679 0.922 0.887 0.918 0.725 0.854 0.656
15. D-GlossVectors 0.653 0.738 0.322 0.894 0.860 0.901 0.742 0.843 0.686 0.932 0.899 0.933 0.722 0.864 0.709
16. D-ExtenedLesk 0.792 0.718 0.409 0.937 0.866 0.939 0.711 0.843 0.657 0.952 0.873 0.943 0.655 0.832 0.654
H-Mean-S8a 0.834 0.864 0.734 0.994 0.980 0.994 0.870 0.960 0.804 0.985 0.965 0.985 0.788 0.928 0.787
H-MeanZscore-S8a 0.830 0.864 0.728 0.994 0.981 0.993 0.874 0.961 0.808 0.986 0.967 0.986 0.793 0.932 0.792
H-MeanNnz-S8a 0.843 0.847 0.740 0.993 0.977 0.991 0.865 0.956 0.799 0.986 0.967 0.985 0.803 0.933 0.802
H-Median-S10 0.821 0.842 0.647 0.995 0.976 0.992 0.843 0.950 0.779 0.975 0.934 0.970 0.724 0.892 0.721
H-Max-S7 0.802 0.816 0.654 0.979 0.957 0.979 0.839 0.936 0.775 0.980 0.957 0.979 0.786 0.922 0.785
H-RankFusion-S10 ? ? ? 0.994 0.978 0.993 0.864 0.956 0.798 0.976 0.929 0.971 0.745 0.896 0.744
H-RelationFusion-S10 ? ? ? 0.996 0.982 0.995 0.840 0.952 0.758 0.986 0.963 0.981 0.781 0.920 0.749
H-Logit-E15 0.793 0.870 0.690 0.995 0.987 0.995 0.885 0.968 0.818 0.995 0.984 0.993 0.821 0.951 0.819
H-MeanNnz-E5 0.878 0.878 0.482 0.986 0.956 0.984 0.784 0.922 0.725 0.975 0.938 0.969 0.768 0.906 0.766
H-MeanZscore-S8b 0.844 0.890 0.616 0.992 0.977 0.991 0.844 0.953 0.780 0.995 0.985 0.995 0.815 0.950 0.814
Table 1: Performance of 16 single and 8 hybrid similarity measures on human judgements datasets (MC, RG,
WordSim353) and semantic relation datasets (BLESS and SN). The best scores in a group (single/hybrid) are in
bold; the very best scores are in grey. Correlations in italics mean p > 0.05, otherwise p ? 0.05.
they only can determine relations between a lim-
ited number of terms. On the other hand, mea-
sures based on web and corpora are nearly unlim-
ited in their coverage, but provide less precise re-
sults. Combination of the measures enables keep-
ing high precision for frequent terms (e. g., ?dis-
ease?) present in WordNet and dictionaries, and
empowers calculation of relations between rare
terms unlisted in the handcrafted resources (e. g.,
?bronchocele?) with web and corpus measures.
Second, combinations work well because, as it
was found in previous research (Sahlgren, 2006;
Heylen et al, 2008), different measures provide
complementary types of semantic relations. For
instance, WordNet-based measures score higher
hypernyms than associative relations; distribu-
tional analysis score high co-hyponyms and syn-
onyms, etc. In that respect, a combination helps
to recall more different relations. For example, a
WordNet-based measure may return a hypernym
?salmon, seafood?, while a corpus-based measure
would extract a co-hyponym ?salmon, mackerel?.
Finally, the supervised combination method
works better than unsupervised ones because
of two reasons. First, the measures generate
scores which have quite different distributions on
the range [0; 1]. The averaging of such scores
may be suboptimal. Logistic Regression over-
comes this issue by assigning appropriate weights
(w1, . . . , wk) to the measures in the linear combi-
nation z. Second, training procedure enables the
model to assign higher weights to the measures
which provide better results, while for the meth-
ods based on averaging all weight are equal.
8 Conclusion
In this work, we designed and studied several
hybrid similarity measures in the context of se-
mantic relation extraction. We have undertaken
a systematic analysis of 16 baseline measures, 8
combination methods, and 3 measure selection
techniques. The combined measures were thor-
oughly evaluated on five ground truth datasets:
MC, RG, WordSim353, BLESS, and SN. Our re-
sults have shown that the hybrid measures out-
perform the single measures on all datasets. In
particular, a combination of 15 baseline corpus-
, web-, network-, and dictionary-based measures
with Logistic Regression provided the best re-
sults. This method achieved a correlation of 0.870
with human judgements and MAP(20) of 0.995
and Recall(50) of 0.818 at predicting semantic re-
lation between terms.
This paper also sketched two novel single
similarity measures performing comparably with
the baselines ? WktWiki, based on definitions
of Wikipedia and Wiktionary; and PatternWiki,
based on patterns applied on Wikipedia abstracts.
In the future research, we are going to apply the
developed methods to query expansion.
17
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of NAACL-HLT 2009, pages 19?27.
Alan Agresti. 2002. Categorical Data Analysis (Wiley
Series in Probability and Statistics). 2 edition.
Alain Auger and Caroline Barrie`re. 2008. Pattern-
based approaches to semantic relation extraction: A
state-of-the-art. Terminology Journal, 14(1):1?19.
Satanjeev Banerjee and Ted Pedersen. 2003. Ex-
tended gloss overlaps as a measure of semantic re-
latedness. In IJCAI, volume 18, pages 805?810.
Marco Baroni and Alexandro Lenci. 2011. How we
blessed distributional semantic evaluation. GEMS
(EMNLP), 2011, pages 1?11.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide web:
A collection of very large linguistically processed
web-crawled corpora. LREC, 43(3):209?226.
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007.
Measuring semantic similarity between words us-
ing web search engines. In WWW, volume 766.
S. Cederberg and D. Widdows. 2003. Using LSA and
noun coordination information to improve the pre-
cision and recall of automatic hyponymy extraction.
In Proceedings HLT-NAACL, page 111118.
Rudi L. Cilibrasi and Paul M. B. Vitanyi. 2007. The
Google Similarity Distance. IEEE Trans. on Knowl.
and Data Eng., 19(3):370?383.
James R. Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In Pro-
ceedings of the ACL-02 workshop on Unsupervised
Lexical Acquisition, pages 59?66.
James R. Curran. 2002. Ensemble methods for au-
tomatic thesaurus extraction. In Proceedings of the
EMNLP-02, pages 222?229. ACL.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In WWW 2001, pages 406?414.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In ACL, pages
539?545.
Kris Heylen, Yves Peirsman, Dirk Geeraerts, and Dirk
Speelman. 2008. Modelling word similarity: an
evaluation of automatic synonymy extraction algo-
rithms. LREC?08, pages 3243?3249.
Jay J. Jiang and David W. Conrath. 1997. Semantic
Similarity Based on Corpus Statistics and Lexical
Taxonomy. In ROCLING X, pages 19?33.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to plato?s problem: The latent semantic
analysis theory of acquisition, induction, and repre-
sentation of knowledge. Psych. review, 104(2):211.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining Local Context and WordNet Similarity for
Word Sense Identification. An Electronic Lexical
Database, pages 265?283.
Dekang Lin. 1998a. An Information-Theoretic Defi-
nition of Similarity. In ICML, pages 296?304.
Dekang Lin. 1998b. Automatic retrieval and cluster-
ing of similar words. In ACL, pages 768?774.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In AAAI?06,
pages 775?780.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1?28.
G. A. Miller. 1995. Wordnet: a lexical database for
english. Communications of ACM, 38(11):39?41.
Siddharth Patwardhan and Ted Pedersen. 2006. Using
WordNet-based context vectors to estimate the se-
mantic relatedness of concepts. Making Sense of
Sense: Bringing Psycholinguistics and Computa-
tional Linguistics Together, page 1.
Se?bastien Paumier. 2003. De la reconnaissance de
formes linguistiques a` l?analyse syntaxique. Ph.D.
thesis, Universite? de Marne-la-Valle?e.
Ted Pedersen, Siddaharth Patwardhan, and Jason
Michelizzi. 2004. Wordnet:: Similarity: measur-
ing the relatedness of concepts. In Demonstration
Papers at HLT-NAACL 2004, pages 38?41. ACL.
Philip Resnik. 1995. Using Information Content to
Evaluate Semantic Similarity in a Taxonomy. In
IJCAI, volume 1, pages 448?453.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627?633.
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis.
Vladislav D. Veksler, Ryan Z. Govostes, andWayne D.
Gray. 2008. Defining the dimensions of the human
semantic space. In 30th Annual Meeting of the Cog-
nitive Science Society, pages 1282?1287.
Zhibiao Wu and Martha Palmer. 1994. Verbs se-
mantics and lexical selection. In Proceedings of
ACL?1994, pages 133?138.
Hui Yang and Jamie Callan. 2009. A metric-based
framework for automatic taxonomy induction. In
ACL-IJCNLP, page 271279.
Torsen Zesch, Christof Mu?ller, and Irina Gurevych.
2008a. Extracting lexical semantic knowledge
from wikipedia and wiktionary. In Proceedings of
LREC?08, pages 1646?1652.
Torsen Zesch, Christof Mu?ller, and Irina Gurevych.
2008b. Using wiktionary for computing semantic
relatedness. In Proceedings of AAAI, page 45.
18
Proceedings of the TextGraphs-8 Workshop, pages 79?87,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
A Graph-Based Approach to Skill Extraction from Text?
Ilkka Kivima?ki1, Alexander Panchenko4,2, Adrien Dessy1,2, Dries Verdegem3,
Pascal Francq1, Ce?drick Fairon2, Hugues Bersini3 and Marco Saerens1
1ICTEAM, 2CENTAL, Universite? catholique de Louvain, Belgium
3IRIDIA, Universite? libre de Bruxelles, Belgium
4 Digital Society Laboratory LLC, Russia
Abstract
This paper presents a system that performs
skill extraction from text documents. It out-
puts a list of professional skills that are rele-
vant to a given input text. We argue that the
system can be practical for hiring and man-
agement of personnel in an organization. We
make use of the texts and the hyperlink graph
of Wikipedia, as well as a list of professional
skills obtained from the LinkedIn social net-
work. The system is based on first computing
similarities between an input document and
the texts of Wikipedia pages and then using a
biased, hub-avoiding version of the Spreading
Activation algorithm on the Wikipedia graph
in order to associate the input document with
skills.
1 Introduction
One of the most difficult tasks of an employer can
be the recruitment of a new employee out of a long
list of applicants. Another challenge of the employer
is to keep track of the skills and know-how of their
employees in order to direct the right people to work
on things they know. In the scientific community,
editors of journals and committees of conferences
always face the task of assigning suitable reviewers
for a tall pile of submitted papers. The tasks de-
scribed above are example problems of expertise re-
trieval (Balog et al, 2012). It is a subfield of in-
formation retrieval that focuses on inferring asso-
ciations between people, expertise and information
content, such as text documents.
?Part of this work has been funded by projects with the
?Re?gion wallonne?. We thank this institution for giving us the
opportunity to conduct both fundamental and applied research.
In addition, we thank Laurent Genard and Ste?phane Dessy for
their contributions for the work.
In this paper, we propose a method that makes a
step towards a solution of these problems. We de-
scribe an approach for the extraction of professional
skills associated with a text or its author. The goal of
our system is to automatically extract a set of skills
from an input text, such as a set of articles written
by a person. Such technology can be potentially
useful in various contexts, such as the ones men-
tioned above, along with expertise management in a
company, analysis of professional blogs, automatic
meta-data extraction, etc.
For succeeding in our goal, we exploit Wikipedia,
a list of skills obtained from the LinkedIn social net-
work and the mapping between them. Our method
consists of two phases. First, we analyze a query
document with a vector space model or a topic
model in order to associate it with Wikipedia arti-
cles. Then, using these initial pages, we use the
Spreading Activation algorithm on the hyperlink
graph of Wikipedia in order to find articles that cor-
respond to LinkedIn skills and are related or central
to the initial pages.
One difficulty with this approach is that it of-
ten results in some skills, which can be identified
as hubs of the Wikipedia graph, constantly being
retrieved, regardless of what the input is. In or-
der to avoid this pitfall, we bias the activation to
avoid spreading to general, or popular nodes. We
try different measures of node popularity to redirect
the spreading and perform evaluative experiments
which show that this biasing in fact improves re-
trieval results.
We have built a web service that enables anyone
to test our skill extraction system. The name of the
system is Elisit, an abbreviation from ?Expertise
Localization from Informal Sources and Information
79
Technologies? and conveying the idea of trying to
elicit, i.e. draw forth latent information about exper-
tise in a target text. According to the best of our
knowledge, we are the first to propose such a system
and describe openly the method behind it.
2 Related work
The recent review of Balog et al (2012) gives a
thorough presentation of the problems of expertise
retrieval and of the methodology used for solving
them. They classify these problems in subcategories
of expert retrieval and expert profiling. The former
means the task of providing a name of a person who
is an expert in a field that is presented as a query,
while the latter means assigning expertise to a per-
son, or some other entity based on information that
is available of that entity. Recent expertise retrieval
research has focused on the TREC enterprise track,
which uses the TREC W3C and CERC corpora (Ba-
log et al, 2008). These datasets contain annotated
crawls of websites. The task in the TREC enterprise
challenge is to build a model that performs expert
retrieval and document retrieval based on a set of
query topics, which correspond to expertise areas.
Our approach is quite different from the one used
in the TREC challenge, as we focus on a fixed
list of skills gathered from the LinkedIn website.
Thus, we were not able to directly compare our sys-
tem to the systems participating in the TREC enter-
prise track. Our problem shares some resemblance
with the INEX entity-ranking track (Demartini et al,
2010), where the goal was to rank Wikipedia pages
related to queries about a given topic. Our skill re-
trieval task can also be seen as an entity ranking task,
where the entities are Wikipedia pages that corre-
spond to skills.
LinkedIn has developed methods for defining
skills and for finding relations between them (Sko-
moroch et al, 2012). These techniques are used in
their service, for example, for recommending job
opportunities to the users. The key difference of
our technology is that it allows a user to search
for skills by submitting an arbitrary text, instead of
only searching for skills related to a certain skill.
Although expertise retrieval has been an active re-
search topic for some time, there have not been
many methods for explicitly assigning particular
skills to text content or people producing text con-
tent.
Our method consists of two steps. First, we ap-
ply a text similarity method to detect the relevant
Wikipedia pages. Second, we enrich the results
with graph mining techniques using the hyperlink
graph of Wikipedia. We have not found a simi-
lar combination being applied for skill extraction
before, although both parts have been well studied
in similar contexts before. For instance, Steyvers
et al (2004) proposed the Author-Topic Model, a
graphical model based on LDA (Blei et al, 2003),
that associates authors of texts with topics detected
from those texts.
Wikipedia has been already used in NLP research
both as a corpus and as a semantic network. Its hy-
perlink graph is a collaboratively constructed net-
work, as opposed to manually crafted networks
such as WordNet (Miller, 1995). Gabrilovich and
Markovitch (2007) introduced Explicit Semantic
Analysis (ESA), where the words of a document are
represented as mixtures of concepts, i.e. Wikipedia
pages, according to their occurence in the body texts
of the pages. The experimental results show that this
strategy works very well and outranks, for exam-
ple, LSA (Landauer and Dumais, 1997) in the task
of measuring document similarity. ESA was later
extended by taking into account the graph structure
provided by the links in Wikipedia (Yeh et al, 2009).
The authors of this work used a PageRank-based al-
gorithm on the graph for measuring word and doc-
ument similarity. This approach was coined Wiki-
Walk.
Associating the elements of a text document un-
der analysis with Wikipedia pages involves itself al-
ready many problems often encountered in NLP. The
process where certain words and multiword expres-
sions are associated with a certain Wikipedia page
has been called Wikification (Mihalcea and Csomai,
2007). In our work, we take a more general ap-
proach, and try to associate the full input text to a
set of Wikipedia pages according to different vec-
tor space models. The models and the details of this
strategy are explained in section 3.3.
The Elisit system uses the Spreading Activa-
tion algorithm on the Wikipedia graph to establish
associations between texts and skills. We chose
to use Spreading Activation, as it tries to simulate
80
a cognitive associative memory (Anderson, 1983),
and the Wikipedia hyperlink network can be under-
stood as an associative network. The simulation
works by finding associations in a network of con-
cepts by spreading pulses of activation from con-
cepts into their neighbours. In the context of NLP,
the Spreading Activation algorithm has been tradi-
tionally used for word sense disambiguation (Hirst,
1988) and information retrieval (Crestani, 1997).
Gouws et al (2010) have shown that this algorithm,
applied to the Wikipedia graph, can also be used to
measure conceptual and document similarity.
3 Methodology
In this section, we will explain how the Elisit
skill extraction system works. We will first ex-
plain how the system uses data from Wikipedia and
LinkedIn. Then, we will describe the two main
components of the system, the text2wiki mod-
ule, which associates a query document with related
Wikipedia pages, and the wiki2skill module,
which aims to associate the Wikipedia pages found
by the text2wiki module with Wikipedia pages
that correspond to skills.
3.1 Wikipedia texts and links
Each page in Wikipedia contains a text that may in-
clude hyperlinks to other pages. We make the as-
sumption that there is a meaningful semantic rela-
tionship between the pages that are linked with each
other and that the Wikipedia hyperlink graph can be
exploited as an associative network. The properties
of the hyperlink structure of Wikipedia and the na-
ture of the information contained in the links have
been investigated by Koolen (2011).
In addition to the encyclopedia pages, Wikipedia
also contains, among others, category, discussion
and help pages. In our system, we are only interested
in the encyclopedia pages and the hyperlinks be-
tween them. We are using data downloaded1 on May
2nd 2012. This dump encompasses 3,983,338 pages
with 247,560,469 links, after removal of the redi-
rect pages. The Wikipedia graph consists of a giant
Strongly Connected Component (SCC) of 3,744,419
nodes, 4130 SCC?s of sizes from 61 to 2 nodes and
228,881 nodes that form their own SCC?s.
1http://dumps.wikimedia.org/
3.2 LinkedIn skills
We gathered a list of skills from the LinkedIn social
network2. The list includes skills which the users
can assign to their profiles. This enables the site
to recommend new contacts or open job opportu-
nities to each user. The skills in the list have been
generated by an automated process developed by
LinkedIn (Skomoroch et al, 2012). The process de-
cides, whether a word or a phrase or a skill suggested
by a user is actually a skill through an analysis of the
text contained in the user profile pages.
Each LinkedIn skill has its own webpage that con-
tains information about the skill. One piece of infor-
mation contained in most of these pages is a link
to a Wikipedia article. According to Skomoroch et
al. (2012), LinkedIn automatically builds this map-
ping. However, some links are manually verified
through crowdsourcing. Not all skill pages contain
a link to Wikipedia, but these skills are often ei-
ther very specific or ambiguous. Thus, we decided
to remove these skills from our final list. The list
of skills used in the system was extracted from the
LinkedIn site in September 2012. After removal of
the skills without a link to Wikipedia, the list con-
tained 27,153 skills.
3.3 text2wiki module
The goal of the text2wiki module is to retrieve
Wikipedia articles that are relevant to an input text.
The output of the module is a vector of sim-
ilarities between the input document and all arti-
cles of the English Wikipedia that contain at least
300 characters. There are approximately 3.3 mil-
lion such pages. We only retrieve the 200 Wikipedia
pages that are most similar to the input document.
Thus, each input text is represented as a sparse vec-
tor a(0), which has 200 non-zero elements out of
3,983,338 dimensions corresponding to the full list
of Wikipedia pages. Each non-zero value ai(0) of
this vector is a semantic similarity of the query with
the i-th Wikipedia article. This approach stems from
ESA, mentioned above. The vector a(0) is given as
input to the second module wiki2skill.
The text2wiki module relies on the Gensim
library (R?ehu?r?ek and Sojka, 2010)3. In particular,
2http://www.linkedin.com/skills
3http://radimrehurek.com/gensim
81
we have used four different text similarity func-
tions, based respectively on the classical Vector
Space Models (VSM?s) (Berry et al, 1994), LSA
and LDA:
(a) TF-IDF (300,000 dimensions)
(b) LogEntropy (300,000 dimensions)
(c) LogEntropy + LSA (200 dimensions)
(d) LogEntropy + LDA (200 topics)
First, each text is represented as a vector x in
a space of the 300,000 most frequent terms in the
corpus, each appearing at least in 10% of the docu-
ments (excluding stopwords). We limited the num-
ber of dimensions to 300,000 to reduce computa-
tional complexity. The models (a) and (b) directly
use this representation, while for (c) and (d) this ini-
tial representation is transformed to a vector x? in a
reduced space of 200 dimensions/topics. For LSA
and LDA, the number of dimensions is often empir-
ically selected from the range [100 ? 500] (Foltz,
1996; Bast and Majumdar, 2005). We followed this
practice. From the vector representations (x or x?),
the similarity between the input document and each
Wikipedia article is computed using the cosine sim-
ilarity.
Pairwise comparison of a vector of 300,000 di-
mensions against 3.3 million vectors of the same
size has a prohibitive computational cost. To make
our application practical, we use an inverted index of
Gensim to efficiently retrieve articles semantically
related to an input document.
3.4 wiki2skill module
The wiki2skill module performs the Spread-
ing Activation algorithm using the initial activations
provided by the text2wiki module and returns a
vector of final activations of all the nodes of the net-
work and a vector containing the activations of only
the nodes corresponding to skills.
The basic idea of Spreading Activation is to ini-
tially activate a set of nodes in a network and then
iteratively spread the activation into the neighbour-
ing nodes. This can actually be interpreted in many
ways opening up a wide space of algorithms that can
lead to different results. One attempt for an exact
definition of the Spreading Activation algorithm can
be found in the work of Shrager et al (1987). Their
formulation states that if a(0) is a vector containing
the initial activations of each node of the network,
then after each iteration, or time step, or pulse t, the
vector of activations is
a(t) = ?a(t? 1) + ?WTa(t? 1) + c(t), (1)
where ? ? [0, 1] is a decay factor which controls the
conservation of activation during time, ? ? [0, 1] is
a friction factor, which controls the amount of acti-
vation that nodes can spread to their neighbors, c(t)
is an activation source vector and W is a weighted
adjacency matrix, where the weights control the
amount of activation that flows through each link in
the network. In some cases, iterating eq. (1) leads
to a converged activation state, but often, especially
when dealing with large networks, it is more prac-
tical to set the number of pulses, T , to some fixed,
low number.
As already stated, this formulation of Spread-
ing Activation spans a wide space of different al-
gorithms. In particular, this space contains many
random walk based algorithms. By considering the
case where ? = 0, ? = 1, c(t) = 0 and where
the matrix W is row-stochastic, the Spreading Ac-
tivation model boils down to a random walk model
with a transition probability matrix W, where a(t)
contains the proportion of random walkers at each
node when the initial proportions are given by a(0).
When the situation is changed by choosing c(t) =
(1 ? ?)a(0), we obtain a bounded Random Walk
with Restart model (Pan et al, 2004; Mantrach et
al., 2011).
Early experiments with the first versions of the al-
gorithm revealed an activation bias towards nodes
that correspond to very general Wikipedia pages
(e.g. the page ?ISBN?, which is often linked to in
the References section of Wikipedia pages). These
nodes have a high input degree, but are often not rel-
evant for the given query. This problem is often en-
countered when analysing large graphs with random
walk based measures. It is known that they can be
dominated by the stationary distribution of the cor-
responding Markov Chain (Brand, 2005).
To tackle this problem, we assign link weights
according to preferential transition probabilities,
which define biased random walks that try to avoid
hub nodes. They have been studied e.g. in the con-
text of stochastic routing of packages in scale-free
82
networks (Fronczak and Fronczak, 2009). These
weights are given by
w?ij =
pi?j
?
k:(i,k)?E
pi?k
, (2)
where pij is a popularity index and ? is a biasing
parameter, which controls the amount of activation
that flows from node i to node j based on the pop-
ularity of node j. For the popularity index, we con-
sidered three options. First, we tried simply the in-
put degree of a node. As a second option, we used
the PageRank score of the node (Page et al, 1999)
which corresponds to the node?s weight in the sta-
tionary distribution of a random surfer that surfs
Wikipedia by clicking on hyperlinks randomly. As a
third popularity index, we used a score based on the
HITS algorithm (Kleinberg, 1999), which is simi-
lar to PageRank, but instead assigns two scores, an
authority score and a hub score. In short, a page
has a high authority score, if it is linked to by many
hub pages, and vice versa. In the case of HITS, the
popularity index was defined as the product of the
authority and hub scores of the node. When ? = 0,
wij is equal for all links leaving from node i, but
when ? < 0, activation will flow more to less popu-
lar nodes and less to popular nodes. We included the
selection of a suitable value for ? as a parameter to
be tuned along with the rest of the spreading strat-
egy in quantitative experiments that are presented in
section 5.2. These experiments show that biasing
the activation to avoid spreading to popular nodes
indeed improves retrieval results.
We also decided to investigate whether giving
more weight to links that exist in both directions
would improve results. The Wikipedia hyperlink
graph is directed, but in some cases two pages may
contain a link to each other. We thus adjust the link
weights wij so that wij = ?w?ij if (j, i) ? E and
wij = w?ij otherwise, where ? ? 1 is a bidirectional
link weight. With large values of ?, more activation
will flow through bidirectional links than links that
exist only in one direction. After this weighting,
the final link weight matrix W is obtained by nor-
malizing each element with its corresponding row
sum to make the matrix row-stochastic. This makes
the model easier to interpret by considering random
walks. However, in a traditional Spreading Activa-
tion model the matrix W is not required to be row-
stochastic. We plan to investigate in the future, how
much the normalization affects the results.
The large size of the Wikipedia graph challenges
the use of Spreading Activation. In order to pro-
vide a usable web service, we would need the system
to provide results fast, preferably within fractions of
seconds. So far, we have dealt with this issue within
the wiki2skill module by respresenting the link
weight matrix W of the whole Wikipedia graph us-
ing the sparse matrix library SciPy4. Each itera-
tion of the Spreading Activation is then achieved by
simple matrix arithmetic according to eq. (1). As
a result, the matrix W must be precomputed from
the adjacency matrix for a given value of the bias-
ing parameter ? and the bidirectional link weight ?
when the system is launched. Thus, they cannot be
selected separately for each query from the system.
Currently, the system can perform one iteration of
spreading activation within less than one second, de-
pending on the sparsity of the activation vector. Our
experiments indicate that the results are quite stable
after five spreading iterations, meaning that we nor-
mally get results with the wiki2skill module in
about one to three seconds.
4 The Elisit skill extraction system
The Elisit system integrates the text2wiki
and the wiki2skill modules. We have built a
web application5 which lets everyone try our method
and use it from third-party applications. Due to this
web service, the Elisit technology can be eas-
ily integrated into systems performing skill search,
email or document analysis, HR automatization,
analysis of professional blogs, automatic meta-data
extraction, etc. The web interface presents the user
the result of the skill extraction (a list of skills) as
well as the result of the text2wiki module (a list
of Wikipedia pages). Each retrieved skill also con-
tains a link to the corresponding Wikipedia page.
Figure 1 presents an example of results provided
by the Elisit system. It lists skills extracted
from the abstract of the chapter Support vector ma-
chines and machine learning on documents from
4http://www.scipy.org/
5GUI: http://elisit.cental.be/; RESTful web
service: http://elisit.cental.be:8080/.
83
Figure 1: Skills extracted from a text about text document
categorization.
Introduction to Information Retrieval by Manning
et al (2008). As one can observe, the Wikipedia
pages found by the text2wiki module represent
many low-level topics, such as ?Desicion bound-
ary?, ?Ranking SVM? or ?Least square SVM?. On
the other hand, the skills retrieved after using the
wiki2skill module provide high-level topics rel-
evant to the input text, such as ?SVM?, ?Machine
Learning? or ?Classification?. These general topics
are more useful, since a user, such as an HR man-
ager, may be confused by too low-level skills.
5 Experiments & results
5.1 Evaluation of the text2wiki module
In order to compare the four text similarity func-
tions, we collected p = 200, 000 pairs of semanti-
cally related documents from the ?See also? sections
of Wikipedia articles. A good model is supposed
to assign a high similarity to these pairs. However,
since the distribution of similarity scores depends
on the model, one cannot simply compare the mean
similarity s? over the set of pairs. Thus, we used a
Model z-score
TF-IDF 8459
LogEntropy 4370
LogEntropy + LDA 2317
LogEntropy + LSA 2143
Table 1: Comparison of different text similarity functions
on the Wikipedia ?See also? dataset.
z-score as evaluation metric. The z-scores are com-
puted as
z =
s?? ??
?
??2/p
(3)
where ?? and ?? are sample estimates of mean and
standard deviation of similarity scores for a given
model. These sample estimates have been calculated
from a set of 1,000,000 randomly selected pairs of
articles. Table 1 presents the results of this experi-
ment. It appears that more complex models (LSA,
LDA) are outperformed on this task by the simpler
vector space models (TF-IDF, LogEntropy). This
can be just a special case with this experimental
setting and perhaps another choice of the number
of topics could give better results. Thus, further
meta-parameter optimization of LSA and LDA is
one approach for improving the performance of the
text2wiki module.
5.2 Evaluation of the wiki2skill module
In order to find the optimal strategy of applying
Spreading Activation, we designed an evaluation
protocol relying on related skills listed on each
LinkedIn skill page. These are automatically se-
lected by computing similarities between skills from
user profiles (Skomoroch et al, 2012). Each skill
page contains at most 20 related skills.
For the evaluation procedure, we choose an initial
node i, corresponding to a LinkedIn skill, and acti-
vate it by setting a(0) = ei, that is a vector contain-
ing 1 in its i-th element and zeros elsewhere. Then,
we compute a(T ) with some spreading strategy and
for some number of steps T , filter out the skill nodes
and rank them according to their final activations. To
measure how well the related skills are represented
in this ranked list of skills, we use Precision at 1, 5
and 10, and R-Precision to evaluate the accuracy of
the first ranked results and Recall at 100 to see how
well the algorithm manages to activate all of the re-
84
lated skills.
There are many LinkedIn skills that are not well
represented in the Wikipedia graph, because of am-
biguity issues, for instance. To prevent these anoma-
lies from causing misguiding results, we selected a
fixed set of 16 representative skills for the evalua-
tion. These skills were ?Statistics?, ?Hidden Markov
Models?, ?Telecommunications?, ?MeeGo?, ?Digi-
tal Printing?, ?OCR?, ?Linguistics?, ?Speech Syn-
thesis?, ?Classical?, ?Impressionist?, ?Education?,
?Secondary Education?, ?Cinematography?, ?Exec-
utive producer?, ?Social Sciences?, ?Political Soci-
ology?.
Developing a completely automatic optimisation
scheme for this model selection task would be diffi-
cult because of the number of different parameters,
the size of the Wikipedia graph and the heuristic na-
ture of the whole methodology. Thus, we decided to
rely on a manual evaluation of the results.
Exploring the whole space of algorithms spanned
by eq. (1) would be too demanding as well. That is
why we have so far tested only a few models. In the
preliminary experiments that we conducted with the
system, we observed that using a friction factor ?
smaller than one had little effect on the results, and
thus we decided to always use ? = 1. Otherwise,
we experimented with three models, which we will
simply refer to as models 1, 2 and 3 and which we
define as follows
? model 1: ? = 0 and c(t) = 0;
? model 2: ? = 1 and c(t) = 0;
? model 3: ? = 0 and c(t) = a(0).
In model 1, activation is not conserved in a node
but only depends on the activation it has received
from its neighbors after each pulse. In contrast, the
activation that a node receives is completely con-
served in model 2. Model 3 corresponds to the Ran-
dom Walk with Restart model, where the initial ac-
tivation is fed to the system at each pulse. Models
1 and 2 eventually converge to a stationary distribu-
tion that is independent of the initial activation vec-
tor. This can be beneficial in situations where some
of the initially activated nodes are noisy, or irrele-
vant, because it allows the initial activation to die
out, or at least become lower than the activation of
other, possibly more relevant nodes. With Model 3,
the initially activated nodes remain always among
the most activated nodes, which is not necessarily a
robust choice.
The outcomes of the experiments demonstrated
that model 2 and model 3 perform equally well. In-
deed, these models are very similar, and apparently
their small differences do not affect the results much.
However, model 1 provided constantly worse results
than the two other models. Thus, we decided to use
model 3, corresponding to the Random Walk with
Restart model, in the system and in selecting the rest
of the spreading strategy.
We also evaluated different settings for the link
weighting scheme. Here, we faced a startling result,
namely that increasing the bidirectional link weight
? all the way up to the value ? = 15 kept improving
the results according to almost all evaluation mea-
sures. This would indicate that links that exist in
only one direction do not convey a lot of semantic
relatedness. However, we assume that this is a phe-
nomenon caused by the nature of the experiment and
the small subset of skills used in it, and not necessar-
ily a general phenomenon for the whole Wikipedia
graph. In our experiments, the improvement was
more drastic in the range ? ? [1, 5] after which a
damping effect can be observed. For this reason,
we decided to set the bidirectional link weight in the
Elisit system to ? = 5.
We observed a similar phenomenon for the num-
ber of pulses T . Increasing its value up to T = 8 im-
proved constantly the results. However, again, there
was no substantial change in the results in the range
T ? [5, 8]. In the web service, the number of pulses
of the spreading activation can be determined by the
user.
In addition to the parameters discussed above, the
link weighting involves the popularity index pij and
the biasing parameter ?. An overview of the ef-
fect of these two choices can be seen in Table 2,
which presents the results with the different eval-
uation measures. These results were obtained by
setting parameters as described earlier in this sec-
tion. First, we can see from this table that using
negative values for ? in the weighting improves re-
sults compared to the natural random walk, i.e. the
case ? = 0. This indicates that our strategy of bi-
asing the spreading of activation to avoid popular
nodes indeed improves the results. We can also see
85
Pre@1 Pre@5 Pre@10 R-Pre Rec@100
? din PR HITS din PR HITS din PR HITS din PR HITS din PR HITS
0 0 0 0 0.119 0.119 0.119 0.156 0.156 0.156 0.154 0.154 0.154 0.439 0.439 0.439
-0.2 0 0 0 0.206 0.238 0.206 0.222 0.216 0.213 0.172 0.193 0.185 0.469 0.469 0.494
-0.4 0 0 0 0.225 0.263 0.169 0.203 0.200 0.150 0.185 0.204 0.148 0.503 0.498 0.476
-0.6 0 0 0.063 0.238 0.225 0.119 0.200 0.197 0.141 0.186 0.193 0.119 0.511 0.517 0.418
-0.8 0 0 0 0.213 0.181 0.075 0.191 0.197 0.113 0.171 0.185 0.109 0.515 0.524 0.384
-1 0 0 0 0.169 0.156 0.063 0.178 0.197 0.091 0.154 0.172 0.097 0.493 0.518 0.336
Table 2: The effect of the biasing parameter ? and the choice of popularity index on the results in the evaluation of the
wiki2skill module.
that using Pagerank as the popularity index provided
overall better results than using the input degree,
which again yielded better results than using HITS.
Thus, biasing according to the input connections of
nodes seems more preferable than biasing accord-
ing to co-citation or co-reference connections. The
low scores with Precision@1 are understandable,
because of the low number of positives (at most 20
related skills) in comparison to the total number of
skills (over 27,000). In the Elisit system, we use
the Pagerank score as the popularity index and set
the value of the biasing parameter to ? = ?0.4.
5.3 Evaluation of the whole Elisit system
We adapted the evaluation procedure used for the
wiki2skill module, described in the previous
section, in order to test the whole Elisit sys-
tem. This time, instead of activating the node of
a given skill, we activated the nodes found by the
text2wiki module when fed with the Wikipedia
article corresponding to the skill. We run the Spread-
ing Activation algorithm with the setup presented in
the previous section. To make the evaluation more
realistic, the initial activation of the target skill node
is set to zero (instead of 1, i.e. the cosine of a vector
with itself).
The system allows its user to set the number of
initially activated nodes. We investigated the ef-
fect of this choice by measuring Precision and Re-
call according to the related skills, and by looking
at the average rank of the target skill on the list of
final activations. However, there was no clear trend
in the results when testing with 1-200 initially ac-
tivated nodes. Nevertheless, we have noticed that
using more than 20 initially activated nodes rarely
improves the results. We must also emphasize that
the choice of the number of initially activated nodes
depends on the query, especially its length.
We also wanted to compare the different VSM?s
VSM Pre@1 Pre@5 Pre@10 R-Pre Rec@100
TF-IDF 0.042 0.231 0.214 0.190 0.516
LogEntropy 0.068 0.216 0.212 0.193 0.525
LogEnt + LSA 0.042 0.180 0.181 0.163 0.491
LogEnt + LDA 0.089 0.193 0.174 0.159 0.470
Table 3: Comparison of the different models of the
text2wiki module in the performance of the whole
Elisit system.
of the text2wiki module when using the whole
Elisit system. We did this by comparing Pre-
cision and Recall at different ranks w.r.t. the re-
lated skills of the target skill found on LinkedIn.
Thus, this experiment combines the experiments in-
troduced in sections 5.1, where the evaluation was
based on the ?See also? pages, and 5.2, where we
used a set of 16 target skills and their related skills.
Table 3 reports the Precision and Recall values ob-
tained with the different VSM?s. These values result
from an average over 12 different numbers of ini-
tially activated nodes. They confirm the conclusion
drawn from the experiment in section 5.1, namely
that the LogEntropy and TF-IDF models outperform
LSA and LDA models for this task.
6 Conclusion and future work
We have presented a method for skill extraction
based on Wikipedia articles, their hyperlink graph,
and a set of skills built by LinkedIn. We have also
presented the Elisit system as a reference imple-
mentation of this method. This kind of a system
has many potential applications, such as knowledge
management in a company or recommender systems
of websites. We have demonstrated with examples
and with quantitative evaluations that the system in-
deed extracts relevant skills from text. The evalu-
ation experiments have also allowed us to compare
and finetune different strategies and parameters of
the system. For example, we have shown that using
a bias to avoid the spreading of activation to popular
86
nodes of the graph improves retrieval results.
This work is still in progress, and we have many
goals for improvement. One plan is to compute link
weights based on the contents of linked pages using
their vector space representation in the text2wiki
module. The method and system proposed in the
paper could also be extended to other languages. Fi-
nally, our methodology can potentially be used to
different problems than skill extraction by substitut-
ing the LinkedIn skills with a list of Wikipedia pages
from another domain.
References
John R Anderson. 1983. A spreading activation theory of
memory. Journal Of Verbal Learning And Verbal Behavior,
22(3):261?295.
Krisztian Balog, Paul Thomas, Nick Craswell, Ian Soboroff, Pe-
ter Bailey, and Arjen P De Vries. 2008. Overview of the trec
2008 enterprise track. Technical report, DTIC Document.
Krisztian Balog, Yi Fang, Maarten de Rijke, Pavel Serdyukov,
and Luo Si. 2012. Expertise retrieval. Foundations and
Trends in Information Retrieval, 6(2-3):127?256.
Holger Bast and Debapriyo Majumdar. 2005. Why spectral
retrieval works. In Proceedings of the 28th annual interna-
tional ACM SIGIR conference on Research and development
in information retrieval, pages 11?18. ACM.
Michael W. Berry, Susan T. Dumais, and Gavin W. O?Brien.
1994. Using linear algebra for intelligent information re-
trieval. Technical Report UT-CS-94-270.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003.
Latent dirichlet alocation. Journal of Machine Learning Re-
search, 3:993?1022, March.
Matthew Brand. 2005. A random walks perspective on max-
imizing satisfaction and profit. Proceedings of the 2005
SIAM International Conference on Data Mining.
Fabio Crestani. 1997. Application of spreading activation tech-
niques in information retrieval. Artificial Intelligence Re-
view, 11(6):453?482.
Gianluca Demartini, Tereza Iofciu, and Arjen P De Vries. 2010.
Overview of the inex 2009 entity ranking track. In Focused
Retrieval and Evaluation, pages 254?264. Springer.
Peter W Foltz. 1996. Latent semantic analysis for text-based
research. Behavior Research Methods, Instruments, & Com-
puters, 28(2):197?202.
Agata Fronczak and Piotr Fronczak. 2009. Biased random
walks in complex networks: The role of local navigation
rules. Physical Review E, 80(1):016107.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Comput-
ing semantic relatedness using wikipedia-based explicit se-
mantic analysis. In IJCAI?07: Proceedings of the 20th in-
ternational joint conference on Artifical intelligence, pages
1606?1611, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Stephan Gouws, G-J van Rooyen, and Herman A. Engelbrecht.
2010. Measuring conceptual similarity by spreading acti-
vation over wikipedia?s hyperlink structure. In Proceedings
of the 2nd Workshop on The People?s Web Meets NLP: Col-
laboratively Constructed Semantic Resources, pages 46?54,
Beijing, China, August. Coling 2010 Organizing Committee.
Graeme Hirst. 1988. Semantic interpretation and ambiguity.
Artificial Intelligence, 34(2):131?177.
Jon M. Kleinberg. 1999. Authoritative sources in a hyperlinked
environment. Journal of the ACM, 46(5):604?632.
Marijn Koolen. 2011. The Meaning Of Structure: the Value
of Link Evidence for Information Retrieval. Ph.D. thesis,
University of Amsterdam, The Netherlands.
Thomas K. Landauer and Susan T. Dumais. 1997. A solu-
tion to plato?s problem: The latent semantic analysis theory
of acquisition, induction, and representation of knowledge.
Psych. review, 104(2):211.
Christopher D Manning, Prabhakar Raghavan, and Hinrich
Schu?tze. 2008. Introduction to information retrieval, vol-
ume 1. Cambridge University Press Cambridge.
Amin Mantrach, Nicolas van Zeebroeck, Pascal Francq,
Masashi Shimbo, Hugues Bersini, and Marco Saerens.
2011. Semi-supervised classification and betweenness com-
putation on large, sparse, directed graphs. Pattern Recogni-
tion, 44(6):1212?1224.
Rada Mihalcea and Andras Csomai. 2007. Wikify!: link-
ing documents to encyclopedic knowledge. In Ma?rio J.
Silva, Alberto H. F. Laender, Ricardo A. Baeza-Yates, Debo-
rah L. McGuinness, Bj?rn Olstad, ?ystein Haug Olsen, and
Andre? O. Falca?o, editors, CIKM, pages 233?242. ACM.
George A Miller. 1995. Wordnet: a lexical database for english.
Communications of the ACM, 38(11):39?41.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Wino-
grad. 1999. The pagerank citation ranking: Bringing order
to the web. Technical Report 1999-0120, Computer Science
Department, Stanford University.
Jia-Yu Pan, Hyung-Jeong Yang, Christos Faloutsos, and Pinar
Duygulu. 2004. Automatic multimedia cross-modal corre-
lation discovery. In Proceedings of the tenth ACM SIGKDD
international conference on Knowledge discovery and data
mining, pages 653?658. ACM.
Radim R?ehu?r?ek and Petr Sojka. 2010. Software Framework
for Topic Modelling with Large Corpora. In Proceedings
of the LREC 2010 Workshop on New Challenges for NLP
Frameworks, pages 45?50, Valletta, Malta, May. ELRA.
Jeff Shrager, Tad Hogg, and Bernardo A Huberman. 1987.
Observation of phase transitions in spreading activation net-
works. Science, 236(4805):1092?1094.
Peter N Skomoroch, Matthew T Hayes, Abhishek Gupta, and
Dhanurjay AS Patil. 2012. Skill customization system, Jan-
uary 24. US Patent App. 13/357,360.
Mark Steyvers, Padhraic Smyth, Michal Rosen-Zvi, and
Thomas Griffiths. 2004. Probabilistic author-topic models
for information discovery. In Proceedings of the 10th ACM
International Conference on Knowledge Discovery and Data
Mining. ACM Press.
Eric Yeh, Daniel Ramage, Christopher D. Manning, Eneko
Agirre, and Aitor Soroa. 2009. Wikiwalk: Random walks
on wikipedia for semantic relatedness. In Graph-based
Methods for Natural Language Processing, pages 41?49.
The Association for Computer Linguistics.
87
