Bagging and Boost ing a Treebank Parser 
John C. Henderson 
The MITRE Corporat ion  
202 Bur l ington  Road 
Bedford ,  MA 01730 
jhndrsn@mit re .o rg  
Eric Br i l l  
M ic roso f t  Research  
1 M ic roso f t  Way 
Redmond,  WA 98052 
br i l l@microso f t . com 
Abstract  
Bagging and boosting, two effective machine learn- 
ing techniques, are applied to natural anguage pars- 
ing. Experiments using these techniques with a 
trainable statistical parser are described. The best 
resulting system provides roughly as large of a gain 
in F-measure as doubling the corpus size. Error 
analysis of the result of the boosting technique re- 
veals some inconsistent annotations in the Penn 
Treebank, suggesting a semi-automatic method for 
finding inconsistent treebank annotations. 
1 Int roduct ion 
Henderson and Brill (1999) showed that independent 
human research efforts produce parsers that can be 
combined for an overall boost in accuracy. Finding 
an ensemble of parsers designed to complement each 
other is clearly desirable. The parsers would need 
to be the result of a unified research effort, though, 
in which the errors made by one parser are targeted 
with priority by the developer of another parser. 
A set of five parsers which each achieve only 40% 
exact sentence accuracy would be extremely valu- 
able if they made errors in such a way that at least 
two of the five were correct on any given sentence 
(and the others abstained or were wrong in different 
ways). 100% sentence accuracy could be achieved 
by selecting the hypothesis that was proposed by 
the two parsers that agreed completely. 
In this paper, the task of automatically creating 
complementary parsers is separated from the task of 
creating a single parser. This facilitates tudy of the 
ensemble creation techniques in isolation. The result 
is a method for increasing parsing performance by 
creating an ensemble of parsers, each produced from 
data using the same parser induction algorithm. 
2 Bagging and Pars ing 
2.1 Background 
The work of Efron and Tibshirani (1993) enabled 
Breiman's refinement and application of their tech- 
niques for machine learning (Breiman, 1996). His 
technique is called bagging, short for "bootstrap ag- 
gregating". In brief, bootstrap techniques and bag- 
ging in particular educe the systematic biases many 
estimation techniques introduce by aggregating es- 
timates made from randomly drawn representative 
resamplings of those datasets. 
Bagging attempts to find a set of classifiers which 
are consistent with the training data, different from 
each other, and distributed such that the aggregate 
sample distribution approaches the distribution of 
samples in the training set. 
Algorithm: Bagging Predictors 
(Breiman, 1996) (1) 
Given: training set  = {(yi ,x~), i  E {1. . .m}} 
drawn from the set A of possible training sets where 
Yi is the label for example x~, classification i duction 
algorithm q2 : A --* ? with classification algorithm 
Ce ? and ? :  X- -~Y.  
1. Create k bootstrap replicates o f / :  by sampling 
m items from E with replacement. Call them 
L1. . .Lk .  
2. For each j e {1. . .k},  Let Cj = ~(? j )  be the 
classifier induced using Lj as the training set. 
3. If Y is a discrete set, then for each x~ observed 
in the test set, yi = mode(?j (x i ) . . .  Cj(x~)). y~ 
is the value predicted by the most predictors, 
the majority vote. 
2.2 Bagging for Parsing 
An algorithm that applies the technique of bagging 
to parsing is given in Algorithm 2. Previous work on 
combining independent parsers is leveraged to pro- 
duce the combined parser. The rest of the algorithm 
is a straightforward transformation of bagging for 
classifiers. Exploratory work in this vein was de- 
scribed by HajiC et al (1999). 
Algorithm: Bagging A Parser (2) 
Given: A corpus (again as a funct ion)C :S?T ~ N, 
S is the set of possible sentences, and T is the set 
of trees, with size m = \[C\] = ~s, t  C(s, t) and parser 
induction algorithm g. 
1. Draw k bootstrap replicates C1 ... Ck of C each 
containing m samples of (s,t) pairs randomly 
34 
picked from the domain of C according to the 
distribution D(s , t )  = C(s,t)/\]C\]. Each boot- 
strap replicate is a bag of samples, where each 
sample in a bag is drawn randomly with replace- 
ment from the bag corresponding to C. 
2. Create parser f~ = g(Ci) for each i. 
3. Given a novel sentence 8test E Ctest ,  combine 
the collection of hypotheses ti = fi(Stest) us- 
ing the unweighted constituent voting scheme 
of Henderson and Brill (1999). 
2.3 Exper iment  
The training set for these experiments was sections 
01-21 of the Penn Treebank (Marcus et al, 1993). 
The test set was section 23. The parser induction 
algorithm used in all of the experiments in this pa- 
per was a distribution of Collins's model 2 parser 
(Collins, 1997). All comparisons made below refer 
to results we obtained using Collins's parser. 
The results for bagging are shown in Figure 2 and 
Table 1. The row of figures are (from left-to-right) 
training set F-measure ~,test set F-measure, percent 
perfectly parsed sentences in training set, and per- 
cent perfectly parsed sentences in test set. An en- 
semble of bags was produced one bag at a time. In 
the table, the In i t ia l  row shows the performance 
achieved when the ensemble contained only one bag, 
F inal(X)  shows the performance when the ensem- 
ble contained X bags, BestF gives the performance 
of the ensemble size that gave the best F-measure 
score. Tra inBestF and TestBestF give the test set 
performance for the ensemble size that performed 
the best on the training and test sets, respectively. 
On the training set al of the accuracy measures 
are improved over the original parser, and on the 
test set there is clear improvement in precision and 
recall. The improvement on exact sentence accuracy 
for the test set is significant, but only marginally so. 
The overall gain achieved on the test set by bag- 
ging was 0.8 units of F-measure, but because the 
entire corpus is not used in each bag the initial per- 
formance is approximately 0.2 units below the best 
previously reported result. The net gain using this 
technique is 0.6 units of F-measure. 
3 Boosting 
3.1 Background 
The AdaBoost algorithm was presented by Fre- 
und and Schapire in 1996 (Freund and Schapire, 
1996; Freund and Schapire, 1997) and has become a
widely-known successful method in machine learn- 
ing. The AdaBoost algorithm imposes one con- 
straint on its underlying learner: it may abstain from 
making predictions about labels of some samples, 
1This is the balanced version ofF-measure, where precision 
and recall are weighted equally. 
but it must consistently be able to get more than 
50?-/o accuracy on the samples for which it commits 
to a decision. That accuracy is measured accord- 
ing to the distribution describing the importance of 
samples that it is given. The learner must be able 
to get more correct samples than incorrect samples 
by mass of importance on those that it labels. This 
statement of the restriction comes from Schapire and 
Singer's study (1998). It is called the weak learning 
criterion. 
Schapire and Singer (1998) extended AdaBoost by 
describing how to choose the hypothesis mixing co- 
efficients in certain circumstances and how to incor- 
porate a general notion of confidence scores. They 
also provided a better characterization f its theo- 
retical performance. The version of AdaBoost used 
in their work is shown in Algorithm 3, as it is the 
version that most amenable to parsing. 
Algorithm: AdaBoost 
(F reund and Schapire, 1997") (3) 
Given: Training set /: as in bagging, except yi E 
{-1,  1 } is the label for example xi. Initial uniform 
distribution D1 (i) = 1/m. Number of iterations, T. 
Counter t = 1. tI,, ?~, and ? are as in Bagging. 
1. Create Lt by randomly choosing with replace- 
ment m samples from L: using distribution Dt. 
2. Classifier induction: Ct ~- ~(Lt)  
3. Choose at E IR. 
4. Adjust and normalize the distribution. Zt is a 
normalization coefficient. 
1 
D, + , ( i) = -~- Dt ( i ) exp(-c~tYiCt( xi ) ) 
5. Increment t. Quit if t > T. 
6. Repeat from step 1. 
7. The final hypothesis i
~)boost(:g) ~- sign Z ~t?,(x) 
t 
The value of at should generally be chosen to min- 
imize 
Z Dt (i) exp(-a~ Yi Ct (x,)) 
i 
in order to minimize the expected per-sample train- 
ing error of the ensemble, which Schapire and Singer 
show can be concisely expressed by I-\] Zt. They also 
give several examples for how to pick an appropriate 
a, and selection generally depends on the possible 
outputs of the underlying learner. 
Boosting has been used in a few NLP systems. 
Haruno et al (1998) used boosting to produce more 
accurate classifiers which were embedded as control 
35
Set Instance P R F Gain Exact Gain 
Training Original Parser 96.25 96.31 96.28 NA 64.7 NA 
Initial 93.61 93.63 93.62 0.00 55.5 0.0 
BestF(15) 96.16 95.86 96.01 2.39 62.1 6.6 
Final(15) 96.16 95.86 96.01 2.39 62.1 6.6 
Test Original Parser 88.73 88.54 88.63 NA 34.9 NA 
Initial 88.43 88.34 88.38 0.00 33.3 0.0 
TrainBestF(15) 89.54 88.80 89.17 0.79 34.6 1.3 
TestBestF(13) 89.55 88.84 89.19 0.81 34.7 1.4 
Final(15) 89.54 88.80 89.17 0.79 34.6 1.3 
Table 1: Bagging the Treebank 
mechanisms of a parser for Japanese. The creators 
of AdaBoost used it to perform text classification 
(Schapire and Singer, 2000). Abney et al (1999) 
performed part-of-speech tagging and prepositional 
phrase attachment using AdaBoost as a core compo- 
nent. They found they could achieve accuracies on 
both tasks that were competitive with the state of 
the art. As a side effect, they found that inspecting 
the samples that were consistently given the most 
weight during boosting revealed some faulty anno- 
tations in the corpus. In all of these systems, Ad- 
aBoost has been used as a traditional classification 
system. 
3.2 Boosting for Parsing 
Our goal is to recast boosting for parsing while con- 
sidering a parsing system as the embedded learner. 
The formulation is given in Algorithm 4. The in- 
tuition behind the additive form is that the weight 
placed on a sentence should be the sum of the weight 
we would like to place on its constituents. The 
weight on constituents that are predicted incorrectly 
are adjusted by a factor of 1 in contrast o a factor 
of ~ for those that are predicted incorrectly. 
Algorithm: Boosting A Parser (4) 
Given corpus C with size m = IC I = ~s.~C(s, t )  
and parser induction algorithm g. Initial uniform 
distribution Dl(i) = 1/m. Number of iterations, T. 
Counter t = 1. 
1. Create Ct by randomly choosing with replace- 
ment m samples from C using distribution Dr. 
2. Create parser ft ~ g(Ct). 
3. Choose at E R (described below). 
4. Adjust and normalize the distribution. Zt is 
a normalization coefficient. For all i, let parse 
tree ~-~' -- ft(s,). Let ~(T,c) be a function indi- 
cating that c is in parse tree r, and ITI is the 
number of constituents in tree T. T(s) is the set 
of constituents that are found in the reference 
or hypothesized annotation for s. 
Dt+l ( i )  : 
1 - , 
cET(s i )  
5. Increment . Quit if t > T. 
6. Repeat from step 1. 
7. The final hypothesis is computed by combin- 
ing the individual constituents. Each parser Ct 
in the ensemble gets a vote with weight at for 
the constituents they predict. Precisely those 
constituents with weight strictly larger than 
1 ~--~t at are put into the final hypothesis. 
A potential constituent can be considered correct 
if it is predicted in the hypothesis and it exists in 
the reference, or it is not predicted and it is not in 
the reference. Potential constituents that do not ap- 
pear in the hypothesis or the reference should not 
make a big contribution to the accuracy computa- 
tion. There are many such potential constituents, 
and if we were maximizing a function that treated 
getting them incorrect the same as getting a con- 
stituent that appears in the reference correct, we 
would most likely decide not to predict any con- 
stituents. 
Our model of constituent accuracy is thus sim- 
ple. Each prediction correctly made over T(s) will be 
given equal weight. That is, correctly hypothesizing 
a constituent in the reference will give us one point, 
but a precision or recall error will cause us to miss 
one point. Constituent accuracy is then a/(a+b+c),  
where a is the number of constituents correctly hy- 
pothesized, b is the number of precision errors and c 
is the number of recall errors. 
In Equation 1, a computation of aca as described 
is shown. 
Otca = 
D( i )  
i c6T(si) 
D( i )  
i cCT(s i )  
Boosting algorithms were developed that at- 
tempted to maximize F-measure, precision, and re- 
call by varying the computation of a, giving results 
too numerous to include here. The algorithm given 
here performed the best of the lot, but was only 
marginally better for some metrics. 
(1: 
36 
Set Instance P R F Gain Exact Gain 
Training Original Parser 96.25 96.31 96.28 NA 64.7 NA 
Initial 93.54 93.61 93.58 0.00 54.8 0.0 
BestF(15) 96.21 95.79 96.00 2.42 57.3 2.5 
Final(15) 96.21 95.79 96.00 2.42 57.3 2.5 
Test Original Parser 88.73 88.54 88.63 NA 34.9 NA 
Initial 88.05 88.09 88.07 0.00 33.3 0.0 
TrainBestF(15) 89.37 88.32 88.84 0.77 33.0 -0.3 
TestBestF(14) 89.39 88.41 88.90 0.83 33.4 0.1 
Final(15) 89.37 88.32 88.84 0.77 33.0 -0.3 
Table 2: Boosting the Treebank 
3.3 Experiment 
The experimental results for boosting are shown in 
Figure 3 and Table 2. There is a large plateau in 
performance from iterations 5 through 12. Because 
of their low accuracy and high degree of specializa- 
tion, the parsers produced in these iterations had 
little weight during voting and had little effect on 
the cumulative decision making. 
As in the bagging experiment, it appears that 
there would be more precision and recall gain to 
be had by creating a larger ensemble. In both the 
bagging and boosting experiments ime and resource 
constraints dictated our ensemble size. 
In the table we see that the boosting algorithm 
equaled bagging's test set gains in precision and re- 
call. The In i t ia l  performance for boosting was 
lower, though. We cannot explain this, and expect 
it is due to unfortunate resampling of the data dur- 
ing the first iteration of boosting. Exact sentence 
accuracy, though, was not significantly improved on 
the test set. 
Overall, we prefer bagging to boosting for this 
problem when raw performance is the goal. There 
are side effects of boosting that are useful in other 
respects, though, which we explore in Section 4.2. 
3.3.1 Weak Learning Criterion Violations 
It was hypothesized in the course of investigating the 
failures of the boosting algorithm that the parser in- 
duction system did not satisfy the weak learning cri- 
terion. It was noted that the distribution of boosting 
weights were more skewed in later iterations. Inspec- 
tion of the sentences that were getting much mass 
placed upon them revealed that their weight was be- 
ing boosted in every iteration. The hypothesis was 
that the parser was simply unable to learn them. 
39832 parsers were built to test this, one for each 
sentence in the training set. Each of these parsers 
was trained on only a single sentence 2 and evaluated 
on the same sentence. It was discovered that a full 
4764 (11.2%) of these sentences could not be parsed 
completely correctly by the parsing system. 
2The sentence was replicated 10 times to avoid threshold- 
ing effects in the learner. 
3.3.2 Corpus Trimming 
In order to evaluate how well boosting worked with 
a learner that better satisfied the weak learning cri- 
terion, the boosting experiment was run again on 
the Treebank minus the troublesome sentences de- 
scribed above. The results are in Table 3. This 
dataset produces a larger gain in comparison to the 
results using the entire Treebank. The initial ac- 
curacy, however, is lower. We hypothesize that the 
boosting algorithm did perform better here, but the 
parser induction system was learning useful informa- 
tion in those sentences that it could not memorize 
(e.g. lexical information) that was successfully ap- 
plied to the test set. 
In this manner we managed to clean our dataset o 
the point that the parser could learn each sentence 
in isolation. The corpus-makers cannot necessarily 
be blamed for the sentences that could not be mem- 
orized. All that can be said about those sentences 
is that for better or worse, the parser's model would 
not accommodate hem. 
4 Corpus Analys is  
4.1 Noisy Corpus: Empirical Investigation 
To acquire experimental evidence of noisy data, dis- 
tributions that were used during boosting the sta- 
ble corpus were inspected. The distribution was ex- 
pected to be skewed if there was noise in the data, or 
be uniform with slight fluctuations if it fit the data 
well. 
We see how the boosting weight distribution 
changes in Figure 1. The individual curves are in- 
dexed by boosting iteration in the key of the figure. 
This training run used a corpus of 5000 sentences. 
The sentences are ranked by the weight they are 
given in the distribution, and sorted in decreasing or- 
der by weight along the x-axis. The distribution was 
smoothed by putting samples into equal weight bins, 
and reporting the average mass of samples in the bin 
as the y-coordinate. Each curve on this graph cor- 
responds to a boosting iteration. We used 1000 bins 
for this graph, and a log scale on the x-axis. Since 
there were 5000 samples, all samples initially had a 
y-value of 0.0002. 
37
Set Instance P R F Gain Exact Gain 
Training Original Parser 96.25 96.31 96.28 NA 64.7 NA 
Initial 94.60 94.68 94.64 0.00 62.2 0.0 
BestF(8) 97.38 97.00 97.19 2.55 63.1 0.9 
Final(15) 97.00 96.17 96.58 1.94 55.0 -7.2 
Test Original Parser 88.73 88.54 88.63 NA 34.9 NA 
Initial 87.43 87.21 87.32 0.00 32.6 0.0 
TrainBestF(8) 89.12 87.62 88.36 1.04 32.8 0.2 
TestBestF(6) 89.07 87.77 88.42 1.10 32.9 0.4 
Final(15) 89.18 87.19 88.18 0.86 31.7 -0.8 
Table 3: Boosting the Stable Corpus 
0.05 
0.045 
0.04 
0035 
0.03 
~' o.o2s I 
0,02 
0.015 
0.01 
0.005 
0 
, , . . . , i " . 
2 . . . . . . .  
3 . . . . . . .  
4 ? 
5 . . . . .  
6 . . . .  
7 . . . . . . . .  
8 . . . . . . .  
9 . . . . . . . . .  
1 0 - -  
11  . . . . . . .  
i 
Figure 1: Weight Change During Boosting 
Notice first that the left endpoints of the lines 
move from bottom to top in order of boosting it- 
eration. The distribution becomes monotonically 
more skewed as boosting progresses. Secondly we 
see by the last iteration that most of the weight is 
concentrated on less than 100 samples. This graph 
shows behavior consistent with noise in the corpus 
on which the boosting algorithm is focusing. 
4.2 T reebank  Inconsistencies 
There are sentences in the corpus that can be learned 
by the parser induction algorithm in isolation but 
not in concert because they contain conflicting in- 
formation. Finding these sentences leads to a better 
understanding of the quality of our corpus, and gives 
an idea for where improvements in annotation qual- 
ity can be made. Abney et al (1999) showed a 
similar corpus analysis technique for part of speech 
tagging and prepositional phrase tagging, but for 
parsing we must remove errors introduced by the 
parser as we did in Section 3.3.2 before questioning 
the corpus quality. A particular class of errors, in- 
consistencies, can then be investigated. Inconsistent 
annotations are those that appear plausible in iso- 
lation, but which conflict with annotation decisions 
made elsewhere in the corpus. 
In Figure 5 we show a set of trees selected from 
within the top 100 most heavily weighted trees at 
the end of 15 iterations of boosting the stable cor- 
pus.Collins's parser induction system is able to learn 
to produce any one of these structures in isolation, 
but the presence of conflicting information in differ- 
ent sentences prevents it from achieving 100% accu- 
racy on the set. 
5 Training Corpus Size Effects 
We suspect our best parser diversification techniques 
gives performance gain approximately equal to dou- 
bling the size of the training set. While this cannot 
be directly tested without hiring more annotators, 
an expected performance bound for a larger train- 
ing set can be produced by extrapolating from how 
well the parser performs using smaller training sets. 
There are two characteristics of training curves for 
large corpora that can provide such a bound: train- 
ing curves generally increase monotonically in the 
absence of over-training, and their first derivatives 
generally decrease monotonically. 
Set Sentences P R 
50 
100 
500 
1000 
5000 
10000 
20000 
39832 
50 
100 
500 
1000 
5000 
10000 
20000 
39832 
F Exact 
67.57 32.15 43.57 5.4 
69,03 56.23 61.98 8,5 
78,12 75.46 76.77 18,2 
81.36 80.70 81.03 22.9 
87.28 87.09 87.19 34.1 
89.74 89.56 89.65 41.0 
92.42 92.40 92.41 50.3 
96.25 96.31 96.28 64.7 
68.13 32.24 43.76 4.7 
69.90 54.19 61.05 7.8 
78.72 75.33 76.99 19.1 
81.61 80.68 81.14 22.2 
86.03 85.43 85.73 28.6 
87.29 86.81 87.05 30.8 
87.99 87.87 87.93 32.7 
88.73 88.54 88.63 34.9 
Table 4: Effects of Varying Training Corpus Size 
The training curves we present in Figure 4 and Ta- 
ble 4 suggest hat roughly doubling the corpus size 
38 
in the range of interest (between 10000 and 40000 
sentences) gives a test set F-measure gain of approx- 
imately 0.70. 
Bagging achieved significant gains of approxi- 
mately 0.60 over the best reported previous F- 
measure without adding any new data. In this re- 
spect, these techniques how promise for making 
performance gains on large corpora without adding 
more data or new parsers. 
6 Conc lus ion  
We have shown two methods, bagging and boosting, 
for automatically creating ensembles of parsers that 
produce better parses than any individual in the en- 
semble. Neither of the algorithms exploit any spe- 
cialized knowledge of the underlying parser induc- 
tion algorithm, and the data used in creating the 
ensembles has been restricted to a single common 
training set to avoid issues of training data quantity 
affecting the outcome. 
Our best bagging system performed consistently 
well on all metrics, including exact sentence accu- 
racy. It resulted in a statistically significant F- 
measure gain of 0.6 over the performance of the base- 
line parser. That baseline system is the best known 
Treebank parser. This gain compares favorably with 
a bound on potential gain from increasing the corpus 
size. 
Even though it is computationally expensive to 
create and evaluate a small (15-30) ensemble of 
parsers, the cost is far outweighed by the opportu- 
nity cost of hiring humans to annotate 40000 more 
sentences. The economic basis for using ensemble 
methods will continue to improve with the increasing 
value (performance p r price) of modern hardware. 
Our boosting system, although dominated by the 
bagging system, also performed significantly better 
than the best previously known individual parsing 
result. We have shown how to exploit the distri- 
bution created as a side-effect of the boosting al- 
gorithm to uncover inconsistencies in the training 
corpus. A semi-automated technique for doing this 
as well as examples from the Treebank that are in- 
consistently annotated were presented. Perhaps the 
biggest advantage ofthis technique is that it requires 
no a priori notion of how the inconsistencies can be 
characterized. 
7 Acknowledgments  
We would like to thank Michael Collins for enabling 
all of this research by providing us with his parser 
and helpful comments. 
This work was funded by NSF grant IRI-9502312. 
The views expressed in this paper are those of the 
authors and do not necessarily reflect the views of 
the MITRE Corporation. This work was done while 
both authors were at Johns Hopkins University. 
Re ferences  
Steven Abney, Robert E. Schapire, and Yoram 
Singer. 1999. Boosting applied to tagging and PP 
attachment. In Proceedings of the Joint SIGDAT 
Conference on Empirical Methods in Natural Lan- 
guage Processing and Very Large Corpora, pages 
38-45, College Park, Maryland. 
L. Breiman. 1996. Bagging predictors. In Machine 
Learning, volume 24, pages 123-140. 
Michael Collins. 1997. Three generative, lexicalised 
models for statistical parsing. In Proceedings of 
the Annual Meeting of the Association for Com- 
putational Linguistics, volume 35, Madrid. 
B. Efron and R. Tibshirani. 1993. An Introduction 
to the Bootstrap. Chapman and Hall. 
Y. Freund and R.E. Schapire. 1996. Experiments 
with a new boosting algorithm. In Proceedings of 
the International Conference on Machine Learn- 
ing. 
Y. Freund and R.E. Schapire. 1997. A decision- 
theoretic generalization f on-line learning and an 
application to boosting. Journal of Computer and 
Systems Sciences, 55(1):119-139, Aug. 
Jan Haji~, E. Brill, M. Collins, B. Hladka, D. Jones, 
C. Kuo, L. Ramshaw, O. Schwartz, C. Tillmann, 
and D. Zeman. 1999. Core natural language 
processing technology applicable to multiple lan- 
guages. Prague Bulletin of Mathematical Linguis- 
tics, 70. 
Masahiko Haruno, Satoshi Shirai, and Yoshifumi 
Ooyama. 1998. Using decision trees to construct 
a practical parser. In Proceedings of the 36th 
Annual Meeting of the Association for Compu- 
tational Linguistics and 17th International Con- 
ference on Computational Linguistics, volume 1, 
pages 505-511, Montreal, Canada. 
John C. Henderson and Eric Brill. 1999. Exploiting 
diversity in natural language processing: Combin- 
ing parsers. In Proceedings of the Fourth Confer- 
ence on Empirical Methods in Natural Language 
Processing, College Park, Maryland. 
Mitchell P. Marcus, Beatrice Santorini, and 
Mary Ann Marcinkiewicz. 1993. Building a large 
annotated corpus of english: The Penn Treebank. 
Computational Linguistics, 19(2):313-330. 
Robert E. Schapire and Yoram Singer. 1998. Im- 
proved boosting algorithms using confidence-rated 
predictions. In Proceedings of the Eleventh An- 
nual Conference on Computational Learning The- 
ory, pages 80-91. 
Robert E. Schapire and Yoram Singer. 2000. Boos- 
texter: A boosting-based system for text catego- 
rization. Machine Learning, 39(2/3):1-34, May. 
To appear. 
"4Q 39
\ 
j J  
i 
i ?/ 
'...\]~'.,.. 
AN 
b~ 
.=. 
O O 
O 
b~ 
..= 
b~ 
40
, , z _~ 
/~ ~j 
~_. ~g 
0 
O 
41 
Integrated Feasibility Experiment for Bio-Security: IFE-Bio
A TIDES Demonstration
Lynette Hirschman, Kris Concepcion, Laurie Damianos, David Day, John Delmore, Lisa Ferro,
John Griffith, John Henderson, Jeff Kurtz, Inderjeet Mani, Scott Mardis, Tom McEntee, Keith
Miller, Beverly Nunan, Jay Ponte, Florence Reeder, Ben Wellner, George Wilson, Alex Yeh
The MITRE Corporation
Bedford, Massachusetts, USA and
McLean, Virginia, USA
781-271-7789
lynette@mitre.org
ABSTRACT
As part of MITRE?s work under the DARPA TIDES
(Translingual Information Detection, Extraction and
Summarization) program, we are preparing a series of
demonstrations to showcase the TIDES Integrated Feasibility
Experiment on Bio-Security (IFE-Bio).  The current
demonstration illustrates some of the resources that can be made
available to analysts tasked with monitoring infectious disease
outbreaks and other biological threats.
Keywords
Translation, information extraction, summarization, topic
detection and tracking, system integration.
1. INTRODUCTION
The long-term goal of TIDES is to provide delivery of
information on demand in real-time from live on-line sources. For
IFE-Bio, the resources made available to the analyst include e-
mail, news groups, digital library resources, and eventually (in
later versions), topic-specific segments from broadcast news.
Because of the emphasis on global monitoring, there is a need to
process incoming information in multiple languages.  The system
must deliver the appropriate information content in the
appropriate form and in the appropriate language (taken for now
to be English). This means that the IFE-Bio system will have to
deliver news stories, clusters of relevant documents, threaded
discussions, alerts on new events, tables, summaries (particularly
over document collections), answers to questions, graphs and geo-
spatial  temporal displays of information.
The demonstration system for the Human Language Technology
Conference in March 2001 represents an early stage of the full
IFE-Bio system, with an emphasis on end-to-end processing.
Future demonstrations will make use of MITRE?s Catalyst
architecture, providing an efficient, scalable architecture to
facilitate  integration of multiple stages of linguistic processing.
By June 2001, the IFE-Bio system will provide richer linguistic
processing through the integration of modules contributed by
other TIDES participants. By June 2002, the IFE-Bio system will
include additional functionality, such as real-time broadcast news
feeds, new machine translation components, support for question-
answering, cross-language information retrieval, multi-document
summarization, automatic extraction and normalization of
temporal and spatial information, and automated geospatial and
temporal displays.
2. The IFE-Bio System
The current demonstration (March 2001) highlights the basic
functionality required by an analyst, including:
? Capture of sources, including e-mail, digital library
material, news groups, and web-based resources;
? Categorizing of the sources into multiple orthogonal
hierarchies useful to the analyst, e.g., disease, region, news
source, language;
? Processing of the information through various stages,
including ?zoning? of the text to select the relevant portions
for processing; named entity detection, event detection,
extraction of temporal information, summarization, and
translation from Spanish, Portuguese, and Chinese into
English;
? Access to the information through use of any mail and news
group reader, which allows the analyst to organize, save, and
share the information in a familiar, readily accessible
environment;
? Display of the information in alternate forms, including
color-tagged documents, tables, summaries, graphs, and
geospatial, map-based displays.
Figure 1 below shows the overall functionality envisioned
for the IFE-Bio system, including capture, categorizing,
processing, access and display.
Collection capability for the current IFE-Bio system includes
email, news groups, journals, and Web resources. We have a
complete copy of the ProMED mailings (a moderated source
tracking global infectious disease outbreaks), and are routinely
collecting other information sources from the World Health
Organization and CDC.  In addition, we are collecting several
general global news feeds. Current volume is around 2000
messages per day; we estimate capacity for the current system at
around 4500 messages/day. Once we have integrated a filtering
capability, we expect the volume of messages saved in IFE-Bio
should drop significantly, since many of the global news services
report on a wide range of events and not all need to be passed on
to IFE-Bio analysts.  The categorizing of sources is done based on
the message header. The header is synthesized by extracting key
information about disease name, the country, and other relevant
information such as type of victim and source of information, as
well as date of message receipt.
The processing for the current demonstration system uses a
limited subset of the Catalyst architecture capabilities and a
number of in-house linguistic modules. The linguistic modules in
the current demonstration system include tokenization, sentence
segmentation, part-of-speech tagging, named entity detection,
temporal extraction (Mani and Wilson 2000) and source-specific
event detection.  In addition, we have incorporated the
CyberTrans embedded machine translation system which ?wraps?
available machine translation engines to make them available via
an e-mail or Web interface (Reeder 2000). Single document
summarization is performed by the MITRE WebSumm system
(Mani and Bloedorn 1999).
We carefully chose a light-weight interface mechanism for
delivery of the information to the analyst.  By treating the
incoming streams of data as feeds to a news server, the analyst can
inspect and organize the information using a familiar news and e-
mail browser. The analyst can subscribe to areas of interest, flag
important messages, watch specific threads, and create tailored
filters for monitoring outbreaks. The stories are crossed-posted to
multiple relevant news groups, based on the information in the
header, e.g., a story on Ebola in Africa would be cross posted to
the Africa regional newsgroup and to the Ebola disease
newsgroup. Search by subject and date allow the analyst to select
subsets of the messages for further processing, annotation or
sharing.  The news client provides notification of incoming
messages. In later versions, we plan to integrate topic detection
and tracking capabilities, to provide improved filtering and
routing of messages, as well as detection of new topics.  The use
of this simple delivery mechanism provides a familiar
environment with almost no learning curve, and it avoids issues of
platform and operating system dependence.
Finally, the system makes use of several different devices to
display the information appropriately. Figure 2 shows the layout
of the Netscape news browser interface.  It includes the list of
newsgroups that have been subscribed to (on the left), the list of
messages from the chosen newsgroup (on top), and a particular
message with color-coded named entities (including disease terms
displayed in red, so that they are easy to spot in the message).
What is the status of the
current Ebola outbreak?
The epidemic is contained;
as of 12/22/00, there were 
421 cases with 162 deaths
Interaction
CDC
WHO
Medical
literatureEmail:
ProMed
~ 2500
 stories/day
Internl
News
Sources  Capture
Translingual 
Information 
Detection 
Extraction 
Summarization 
U niden tif ied h emor rhagic  f
U niden tif ied h emor rhagic  f
Ebola hemorr hagic  fever  in
Re :  Ebo la hemorrhagi. ..
R e: Ebola hemo rrha gi...
ProMED
A nnotator
Ja ne Analyst
10 /17/00 1 9:37
10 /17/00 2 0:42
10 /18/00 7 :42
High
Norm al
Normal
read
rep lied
ProMED
10 /18/00 1 2:3 4 High un read
Ebola hemorr hagic  fever  in
Sour ce
D ate
Priority Status
10   99
0   105
1   57
0   10
2   34
0   50
1   1
0   25
5   200
0   45
0   0
0   0
0   0
0   0
0   6
0   32
0   3
0   1
High
Norm al
High
High
Ebola hemorrhagic feve r  -  Ugan da
U nf ilte red
O utbr eak
     C holer a
     D engue  Fe ve r
     Eb ola
I nfras tructure?
N atu ral  Di sas. ..
Spi lls
A cc id en ts
W M D Tra ckin. ..
Sus picious Il ln. ..
Sus picious De.. .
Pos sible  Biol o. ..
Pathogen threa ?
- --- --- ---------------------
W orkspa ce
      E bola
      D ra fts
      Re ports
D isease
R e: Ebola hemo rrha gi...
Location
U NK
U NK
Ebola
Ebola
Ebola
Ebola
Rabies
Rabies
U gan da
U gan da
U gan da
K eny a
U gan da
IHT
ProMED
WHO
Jo e Analyst
D ate
10 /14/00 2 3:06
10 /15/00 1 0:50
10 /16/00 2 1:45
10 /17/00 1 9:12
read
read
read
read
un read
Date: 10/16/00
Disease: Ebo la
Descripto r: hem orrh agic fever
Locatio n:          Ugan da
Disease Date:     10/14/00
Ho spital: mission ary hosp ital  in Gulu
New cases:  at least  7
Total  cases: 51
Total  dead:       31
Ebola hemorr hagic  fever  -
Ugand an M ini stry  ide ntif ies Eb ola  virus as t he c ause of  the outbreak.  KA MP ALA :
The  dreade d Eb ola  virus that struck over 300  peopl e i n Kikwit,  in  t he D emocratic
Rep ub lic  of Con go  in  1995, has ki lled 31  people in northe rn Ugan da.  A  U gandan
M ini s tr y of Heal th  sta tement  said l aboratory test s had r eveale d that  the Ebola vi rus
was  t he caus e of the  epidemi c hemorr hagic feve r whi ch has been r agi ng in the  G ulu
dis trict  since Septe mbe r.   Thr ee  of the dea d wer e s tud ent nur ses , who tre ated the first
Eb ola  patients admitt ed to a  Lac or  mis sionary hosp it al in  Gu lu  tow n.  A  task force
he ade d by G ul u dis trict adm ini str ator, Walte r O ch ora , has bee n se t up to co-or dina te
efforts to control the epi demi c.  F ie ld offic ials i n  Gul u tol d the Ka mpala-based Ne w
H ttp: //ti des2000.mi tre.org/
Pr oM ED /10162000/34n390h.ht ml
U gan da
News Repository
CATALYSTEntity Tagging
Event Extraction
Translation
Summarization
Alerting
Change detection
Threading
Cross-language IR
Topic clustering
Figure 1: Overview of the IFE-Bio Demonstration System
Local,
private
workspace
Documents
automatically
categorized
into shared,
tailorable
hierarchy
Sort by disease, location, source, date, etc.
Associated
meta-data:
header,
event,
summary,
named-entity
Figure 2: Screenshot of IFE-Bio Interface Using News Group Reader
Figure 3: Sample Summarization Automatically Generated by WebSumm
There are multiple display modalities available. The message in
Figure 2 contains a short tabular display in the beginning,
identifying disease, region and victim type. Below that is a URL
to a document summary, created by MITRE?s WebSumm system
(see Figure 3 for a sample summary).   If an incoming message is
in a language other than English, then CyberTrans is called to run
code set and language identification modules, and the language is
translated into English for further processing. Figure 4 below
shows a sample translated message; note that there are a number
of untranslated words, but it is still possible to get the gist of the
message.
In addition, we are working on a mechanism to provide
geographic and eventually, temporal display of outbreak
information. Figure 5 shows the stages of processing involved.
Stage 1 shows onamed entity and temporal tagging to identify the
items of interest. These are combined into disease events by
further linguistic processing; the result is shown in the table in
Stage 2. This spreadsheet of events serves as input for a map-
based display, shown in Stage 3. The graph plots number of new
cases and number of cumulative cases over time.  In the map, the
size of the outer dot represents total number of cases to date, and
the inner dot represents new cases.  This allows the analyst to
visualize spread of the disease, as well as the stage of the outbreak
(spreading or subsiding).
3. REFERENCES
[1] Mani, I. and Bloedorn, E. (1999). "Summarizing
Similarities and Among Related Documents".
Information Retrieval 1(1): 35-67.
[2] Mani, I. and Wilson, G. (2000). "Robust Temporal
Processing of News," Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL'2000), 69-76. New Brunswick, New
Jersey. Association for Computational Linguistics.
[3] Reeder, F.  (2000) "At Your Service:  Embedded MT
as a Service",  NAACL Workshop on Embedded MT,
March, 2000.
Figure 4: Translation from Portuguese to English Produced by CyberTrans
1. Annotate entities of interest via XML
Dise a se Source Country City_na m eDa te Ca se s Ne w _ca se s De a d
Ebola PROM ED Uganda G ula 26-O ct-2000 182 17 64
Ebola PROM ED Uganda G ula 5-Nov-2000 280 14 89
Ebola PROM ED Uganda G ulu 13-O ct-2000 42 9 30
Ebola PROM ED Uganda G ulu 15-O ct-2000 51 7 31
Ebola PROM ED Uganda G ulu 16-O ct-2000 63 12 33
Ebola PROM ED Uganda G ulu 17-O ct-2000 73 2 35
Ebola PROM ED Uganda G ulu 18-O ct-2000 94 21 39
Ebola PROM ED Uganda G ulu 19-O ct-2000 111 17 41
2. Assemble entities into events
0
50
100
150
200
250
300
350
400
10/
13/
200
0
10/
20/
200
0
10/
27/
200
0
11/
3/2
000
11/
10/
200
0
11/
17/
200
0
11/
24/
200
0
T IME
Nu
m
be
r C
as
es
Cases
New_cases
Dead
3. Display events...
   Total Cases   New Cases
Figure 5: Steps in Extraction to Support Temporal and Geospatial Displays of Disease Outbreak
Exploiting Diversity for Answering Questions
John Burger and John Henderson
The MITRE Corporation
202 Burlington Road
Bedford, MA 02144, USA
  john,jhndrsn  @mitre.org
Abstract
We describe initial experiments in combining
the output of question answering systems using
data from the 2002 TREC Question Answering
task. We explore several distance-based com-
bining methods, as well as a number of dis-
tance metrics involving both word and charac-
ter ngrams.
1 Introduction
Progress in question answering technology can be mea-
sured as individual systems improve in accuracy, but it
is not the only way to witness technological progress. A
question one can ask is how well we can perform auto-
matic question answering as a community. If we were
asked to enter an Earth English system in an intergalac-
tic TREC, how well would we do? One easy answer is
that we would perform as well as the best QA system.
A second answer is that perhaps we could do even better
by combining systems?this might be expected to work
if different systems were independent in their errors. The
follow-up question is how would we build such a system?
Lower bounds on the highest possible performance
current technology can achieve on a given dataset have
practical value, as well. They allow us to better estimate
how well systems are doing with respect to the underlying
difficulty of the dataset, and continually provide perfor-
mance targets that are known to be achievable. Without
such lower bounds on optimal performance, one cannot
determine if technological progress in a domain has sim-
ply stalled.
NIST?s ROVER system for combining speech recog-
nizer output gives ASR researchers an updated goal to
shoot for after every evaluation, as well as an implicit
measure of the extent to which systems are making the
same errors (Fiscus, 1997). The work herein initiates a
similar set of experiments for question answering tech-
nology.
2 Background: The TREC Question
Answering Task
Under the auspices of the National Institute for Standards
and Technology (NIST), the Text Retrieval Conferences
(TREC) have been an annual opportunity for the infor-
mation retrieval community to evaluate techniques in a
variety of tasks. For the last four years, TREC has in-
cluded a question answering activity, wherein commer-
cial and academic groups from around the world can eval-
uate systems designed to retrieve answers to questions,
rather than simply documents from queries (Voorhees,
2002). This last year, 34 groups participated by running
their systems on 500 previously unseen questions, against
a corpus of approximately one million newswire docu-
ments. The task was to retrieve a single, short phrasal
answer to each question from one of the documents, re-
turning the answer string along with the document iden-
tifier. Answers were evaluated as strictly correct by the
NIST assessors only if the indicated document justified
the answer appropriately, and if no extraneous material
was included in the answer string.
For example, Question 1399 from the 2002 evaluation
was:
What mythical Scottish town appears for one
day every 100 years?
Participating systems returned Hong Kong, Tartan,
Lockerbie, and Brigadoon, as well as a number of other
candidates. Only Brigadoon was judged to be correct,
and only if the system also pointed to a document that
explicitly justified that answer?a document that simply
mentioned the town was insufficient. Systems also had
the option of indicating that they believed a question to
be unanswerable from the corpus, by returning the NIL
document ID.
This year, TREC QA participants were encouraged to
develop confidence assessment techniques for their sys-
tems. Systems returned the answer set sorted by decreas-
ing confidence that each answer was correct. This rank-
ing was taken into account by the main evaluation metric,
average precision. This is defined as follows:
 	
 
 

	
MiTAP for SARS Detection 
 
 
Laurie E. Damianos, Samuel Bayer, 
Michael A. Chisholm, John Henderson,  
Lynette Hirschman, William Morgan, 
Marc Ubaldino, Guido Zarrella 
The MITRE Corporation 
202 Burlington Road 
Bedford, MA 01730 
{laurie, sam, chisholm, 
jhndrsn, lynette, wmorgan, 
ubaldino,jzarrella}@mitre.org 
James M. Wilson, V, MD and  
Marat G. Polyak 
Division of Integrated Biodefense 
ISIS Center, Georgetown University 
2115 Wisconsin Avenue Suite 603 
Washington, DC 20007 
{wilson, mgp5} 
@isis.imac.georgetown.edu 
 
Abstract 
The MiTAP prototype for SARS detection 
uses human language technology for detect-
ing, monitoring, and analyzing potential indi-
cators of infectious disease outbreaks and 
reasoning for issuing warnings and alerts. Mi-
TAP focuses on providing timely, multi-
lingual information access to analysts, domain 
experts, and decision-makers worldwide. Data 
sources are captured, filtered, translated, 
summarized, and categorized by content. 
Critical information is automatically extracted 
and tagged to facilitate browsing, searching, 
and scanning, and to provide key terms at a 
glance. The processed articles are made avail-
able through an easy-to-use news server and 
cross-language information retrieval system 
for access and analysis anywhere, any time. 
Specialized newsgroups and customizable fil-
ters or searches on incoming stories allow us-
ers to create their own view into the data 
while a variety of tools summarize, indicate 
trends, and provide alerts to potentially rele-
vant spikes of activity. 
1 
2 
Background 
Potentially catastrophic biological events that threaten 
US national security are steadily increasing in fre-
quency. These events pose immediate danger to ani-
mals, plants, and humans. Current disease surveillance 
systems are inadequate for detecting indicators early 
enough to ensure the rapid response needed to combat 
these biological events and corresponding public reac-
tion. Recent examples of outbreaks include both the 
HIV/AIDS and foot and mouth pandemics, the spread of 
West Nile virus to and across the US, the escape of Rift 
Valley Fever from Africa, SARS, and the translocation 
of both mad cow disease (BSE) and monkey pox to the 
United States.  
Biological surveillance systems in the United States 
rely most heavily on human medical data for signs of 
epidemic activity. These systems span multiple organi-
zations and agencies, are often not integrated, and have 
no alerting capability. As a result, responders have an 
insufficient amount of lead time to prepare for biologi-
cal events or catastrophes. 
Indications and Warnings (I&Ws) provide the poten-
tial for early alert of impending biological events, per-
haps weeks to months in advance. Sources of I&Ws 
include transportation data, telecommunication traffic, 
economic indices, Internet news, RSS feeds (RSS) in-
cluding weblogs, commerce, agricultural surveillance, 
weather, and other environmental data. Retrospective 
analyses of major infectious disease outbreaks (e.g., 
West Nile Virus and SARS) show that I&Ws were pre-
sent weeks to months in advance, but these indicators 
were missed because data sources were difficult to ob-
tain and hard to integrate. As a result, the available in-
formation was not utilized for appropriate national and 
international response. This illuminates a critical need in 
biodefense for an integrated system linking I&Ws for 
biological events from multiple and disparate sources 
with the response community. 
Introduction 
MiTAP (Damianos et al 2002) was originally devel-
oped by the MITRE Corporation under the Defense 
Advanced Research Projects Agency (DARPA) 
Translingual Information Detection Extraction and 
Summarization (TIDES) program. TIDES aims to revo-
lutionize the way that information is obtained from hu-
man language by enabling people to find and interpret 
relevant information quickly and effectively, regardless 
of language or medium. MiTAP was initially created for 
tracking and monitoring infectious disease outbreaks 
and other biological threats as part of a DARPA Inte-
grated Feasibility Experiment in biosecurity to explore 
the integration of synergistic TIDES language process-
ing technologies applied to a real world domain. The 
system has since been expanded to other domains such 
as weapons of mass destruction, satellite monitoring, 
and suspect terrorist activity. In addition, researchers 
and analysts are examining hundreds of MiTAP data 
sources for differing perspectives on conflict and hu-
manitarian relief efforts. 
Our newest MiTAP prototype explores the integra-
tion of outputs from operational data mining (anomaly 
detection), human language technology (information 
extraction, temporal tagging, machine translation, cross-
language information retrieval), and visualization tools 
to detect SARS-specific I&Ws in Asia, with relevance 
to pathogen translocation to the United States. Using 
feeds from English and Chinese language newswire, 
weblogs, and other Internet data, the system translates 
Chinese text data and tracks keyword combinations 
thought to represent I&Ws specific to SARS outbreaks 
in China. Analysts can use cross-language information 
retrieval for retrospective analysis and improving the 
I&W model, save searches to use as filters on incoming 
data, view trends, and visualize the data along a time-
line. Figure 1 shows an overview of the prototype. 
Warnings generated by this MiTAP prototype are in-
tended to complement traditional biosurveillance and 
communications already in use by the international pub-
lic health community. This system represents an expan-
sion of current US surveillance capabilities to detect 
biological agents of catastrophic potential.
 
 
Figure 1 Overview of the MiTAP prototype for SARS detection. 
3 Component Technologies 
The MiTAP prototype relies extensively on human 
language technology and expert system reasoning. 
Below, MiTAP capabilities are described briefly 
along with their contributing component 
technologies. 
3.1 
3.2 
3.3 
3.4 
3.5 
Information Processing 
After Internet news sources are captured and 
normalized, they are passed through a zoner using 
human-generated rules to identify source, date, and 
other information such as headline, or title, and 
content. The Alembic natural language analyzer (Ab-
erdeen et al 1995; Vilain and Day 1996) processes 
the zoned messages to identify paragraph, sentence, 
and word boundaries as well as part-of-speech tags. 
The messages then pass through the Alembic named 
entity recognizer for identification and tagging of 
person, organization, location, and disease names. 
Finally, the article is processed by the TempEx 
normalizing time expression tagger (Mani and Wil-
son 2000). 
For Chinese and other non-English sources, the 
CyberTrans machine translation system (Miller et al 
2001) is used to translate articles automatically into 
English. CyberTrans wraps commercial and research 
translation engines to produce a common set of 
interfaces; the current prototype makes use of the 
SYSTRAN Chinese-English system.  
RSS feeds can provide a high volume textual ge-
stalt.  Weblogs, in particular, are a good source of 
timely text, some of which is topical and all of which 
is based on personal observations and experiences. 
Aggregate measurements on these feeds can provide 
indications of public health-related phenom-
ena.  Consider the relative rates of words and phrases 
such as "stay home from" or "pneumonia.?  Geotem-
poral location of non-seasonal spikes in relative rank 
of these strings can establish suspicion for further 
investigation by I&W experts. 
Browsing 
English language data and pairs of foreign language 
documents and their translated versions are made 
available on a news server (INN 2001) for browsing. 
The system categorizes and bins articles into 
newsgroups based on their content. To do this, the 
system relies on a combination of the information 
extraction results as well as human-generated rules 
for pattern matching. Newsgroups are created to 
provide multiple perspectives on the data; analysts 
can subscribe to specific disease tracking 
newsgroups, regional newsgroups, specific data 
source newsgroups, or to customized topic tracking 
newsgroups that may be based on several related 
subjects. 
Tagged entities in each article are color-coded to 
enable rapid scanning of information and easy identi-
fication of key names. The five most frequently men-
tioned locations in each article as well as the top five 
people are presented as a list for quick reference. 
Information Retrieval 
To supplement access to the articles on the news 
server and to allow for retrospective analysis, articles 
are indexed using the Lucene information retrieval 
system (The Jakarta Project 2001) for English 
language documents and using PSE (Darwish 2002) 
for foreign language documents. Web links are 
maintained between foreign language documents and 
their translated versions to allow for more accurate 
human translations of selected documents. 
Analysts can perform full text, source-specific 
queries over the entire set of archived documents and 
view the retrieved results as a relevance-ranked list or 
as a plot across a timeline. A cross-language informa-
tion retrieval interface allows users to search in Eng-
lish across the Chinese language sources. 
Users can also save specific search constraints to 
be used as filters on incoming data. These saved 
searches provide a simple analytic capability as well 
as an alerting feature. (See below.) 
Analysis 
To assist analysts in identifying relevant and related 
articles, we have integrated multi-document summa-
rization and watch lists. Columbia University?s 
Newsblaster (McKeown et al 2002) automatically 
detects daily topics, clusters MiTAP articles around 
those topics, and generates multi-document summari-
zations which are made available on the news server. 
Multiple technologies (e.g., coreference, information 
extraction) from Alias I, Inc. (Baldwin et al 2002) 
produces comprehensive views on specific named 
entities (i.e., people or disease) across MiTAP docu-
ments. These views are summarized through ranked 
lists, highlighting important topics of the day and 
activities which might indicate disease outbreak.  
Finely-tuned searches can be saved and applied as 
filters or topic tracking mechanisms. These saved 
searches are automatically updated at specific inter-
vals and can be aggregated and displayed visually as 
bar graphs to reveal spikes of activity that otherwise 
might go undetected. 
Alerting 
The MiTAP prototype has two separate alerting ca-
pabilities: saved searches and an integrated expert 
system. The saved search functionality allows ana-
lysts to set thresholds for alerting purposes. For ex-
ample, MiTAP can send email when any new article 
arrives, when a specified maximum number of arti-
cles arrives, or when the daily number of new articles 
increases by some percentage of the total or moving 
average. 
The Human Language Indication Detector 
(HLID) performs data fusion on a number of dispa-
rate sources, compressing a large volume of informa-
tion into a smaller but more significant set of alerts. 
HLID monitors a variety of sources including MiTAP 
articles, information events in RSS feeds, and other 
dynamically updated information on the World Wide 
Web. HLID analyzes events from these sources in 
real time and generates an estimate of significance 
for each, complete with an audit trail of supporting 
and negating evidence. This allows an analyst to di-
rect a search for indicators towards interesting data 
while reducing the time spent investigating false 
alarms and insignificant events.  
HLID is composed of four major components. 
The first is an event collector, which monitors a data 
source and triggers action when an event is observed. 
These events are sent to the rule based reasoning en-
gine, an expert system shell (JESS 2004) with hand 
authored rules. The engine performs vetting and ini-
tial investigation of each event by identifying corre-
lated events, corroborating or invalidating evidence, 
and references to supporting information. The engine 
can also supplement its knowledge base by perform-
ing a directed search via the query management sys-
tem, which allows retrieval of information from a 
wide variety of sources including databases and web 
pages. Lastly, the alerting mechanism disseminates 
the conclusions reached by the system and provides 
an interface that allows an analyst to launch a deeper 
search for indicators and warnings. 
4 
5 
Acknowledgments 
This work has been funded, in part, by the Defense 
Advanced Research Projects Agency Translingual 
Information Detection Extraction and Summarization 
program under contract numbers DAAB07-01-C-
C201 and W15P7T-04-C-D001, the Office of the 
Secretary of Defense in support of the Coalition Pro-
visional Authority in Baghdad, and a MITRE Special 
Initiative for Rapid Integration of Novel Indications 
and Warnings for SARS. 
References 
Aberdeen, J., Burger, J., Day, D., Hirschman, L., 
Robinson, P., and Vilain, M. 1995. MITRE: De-
scription of the Alembic System as Used for 
MUC-6. In Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6). 
Baldwin, B,, Moore, M., Ross, A., Shah, D.  2002. 
Trinity Information Access System. Proceedings of 
Human Lanuage Technology Conference, San 
Diego, CA. 
Damianos, L., Ponte, J., Wohlever, S., Reeder, F., 
Day, D., Wilson, G., Hirschman, L. 2002. MiTAP, 
Text and Audio Processing for Bio-Security: A 
Case Study In Proceedings of IAAI-2002: The 
Fourteenth Innovative Applications of Artificial 
Intelligence Conference, Edmonton, Alberta, Can-
ada. 
Darwish, K. PSE: A Small Search Engine written in 
Perl 2002 
http://tides.umiacs.umd.edu/software.html 
INN: InterNetNews, Internet Software Consortium 
2001, http://www.isc.org/products/INN.  
The Jakarta Project, 2001 
http://jakarta.apache.org/lucene/docs/index.html. 
JESS: the Rule Engine for the Java? Platform 2004 
http://herzberg.ca.sandia.gov/jess/  
Mani, I. and Wilson, G. 2000. Robust Temporal 
Processing of News. In Proceedings of the 38th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL'2000), 69-76. 
McKeown, K., Barzilay, R., Evan, D., Hatzivassi-
loglou, V., Klavans, J., Sable, C., Schiffman, B., 
Sigelman, S. 2002. Tracking and Summarizing 
News on a Daily Basis with Columbia's Newsblas-
ter. In Proceedings of HLT 2002: Human Lan-
guage Technology Conference. 
Miller, K., Reeder, F., Hirschman, L., Palmer, D. 
2001. Multilingual Processing for Operational 
Users, NATO Workshop on Multilingual Process-
ing at EUROSPEECH. 
RSS RDF Site Summary http://purl.org/rss/1.0/spec 
Vilain, M. and Day, D. 1996. Finite-state phrase 
parsing by rule sequences. In Proceedings of the 
1996 International Conference on Computational 
Linguistics (COLING-96), Copenhagen, Denmark. 
Direct Maximization of Average Precision by Hill-Climbing, with a
Comparison to a Maximum Entropy Approach
William Morgan and Warren Greiff and John Henderson
The MITRE Corporation
202 Burlington Road MS K325
Bedford, MA 01730
{wmorgan, greiff, jhndrsn}@mitre.org
Abstract
We describe an algorithm for choosing term
weights to maximize average precision. The
algorithm performs successive exhaustive
searches through single directions in weight
space. It makes use of a novel technique for
considering all possible values of average pre-
cision that arise in searching for a maximum in
a given direction. We apply the algorithm and
compare this algorithm to a maximum entropy
approach.
1 Introduction
This paper presents an algorithm for searching term
weight space by directly hill-climbing on average pre-
cision. Given a query and a topic?that is, given a set
of terms, and a set of documents, some of which are
marked ?relevant??the algorithm chooses weights that
maximize the average precision of the document set when
sorted by the sum of the weighted terms. We show that
this algorithm, when used in the larger context of finding
?optimal? queries, performs similar to a maximum en-
tropy approach, which does not climb directly on average
precision.
This work is part of a larger research program on the
study of optimal queries. Optimal queries, for our pur-
poses, are queries that best distinguish relevant from non-
relevant documents for a corpus drawn from some larger
(theoretical) population of documents. Although both
performance on the training data and generalization abil-
ity are components of optimal queries, in this paper we
focus only on the former.
2 Motivation
Our initial approach to the study of optimal queries em-
ployed a conditional maximum entropy model. This
model exhibited some problematic behavior, which mo-
tivated the development of the weight search algorithm
described here.
The maximum entropy model is used as follows. It is
given a set of relevant and non-relevant documents and a
vector of terms (the query). For any document, the model
predicts the probability of relevance for that document
based on the Okapi term frequency (tf ) scores (Robertson
and Walker, 1994) for the query terms within it. Queries
are developed by starting with the best possible one-term
query and adding individual terms from a candidate set
chosen according to a mutual information criterion. As
each term is added, the model coefficients are set to max-
imize the probability of the empirical data (the document
set plus relevance judgments), as described in Section 4.
Treating the model coefficients as term weights yields
a weighted query. This query produces a retrieval status
value (RSV) for each document that is a monotonically
increasing function of the probability of relevance, in ac-
cord with the probability ranking principle (Robertson,
1977). We can then calculate the average precision of the
document set as ordered by these RSVs.
As each additional query term represents another de-
gree of freedom, one would expect model performance to
improve at each step. However, we noted that the addition
of a new term would occasionally result in a decrease in
average precision?despite the fact that the model could
have chosen a zero weight for the newly added term.
Figure 1 shows an example of this phenomenon for one
TREC topic.
This is the result of what might be called ?metric di-
vergence?. While we use average precision to evaluate
the queries, the maximum entropy model maximizes the
likelihood of the training data. These two metrics occa-
sionally disagree in their evaluation of particular weight
vectors. In particular, maximum entropy modeling may
favor increasing the estimation of documents lower in the
ranking at the expense of accuracy in the prediction of
highly ranked documents. This can increase training data
likelihood yet have a detrimental effect on average preci-
sion.
The metric divergence problem led us to consider an al-
ternative approach for setting term weights which would
hill-climb on average precision directly. In particular, we
were interested in evaluating the results produced by the
maximum entropy approach?how much was the maxi-
mization of likelihood affecting the ultimate performance
as measured by average precision? The algorithm de-
scribed in the following section was developed to this
end.
3 The Weight Search Algorithm
The general behavior of the weight search algorithm is
similar to the maximum entropy modeling described in
Section 2?given a document corpus and a term vector,
it seeks to maximize average precision by choosing a
weight vector that orders the documents optimally. Un-
like the maximum entropy approach, the weight search
algorithm hill-climbs directly on average precision.
The core of the algorithm is an exhaustive search of a
single direction in weight space. Although each direction
is continuous and unbounded, we show that the search
can be performed with a finite amount of computation.
This technique arises from a natural geometric interpreta-
tion of changes in document ordering and how they affect
average precision.
At the top level, the algorithm operates by cycling
through different directions in weight space, performing
an exhaustive search for a maximum in each direction,
until convergence is reached. Although a global maxi-
mum is found in each direction, the algorithm relies on a
greedy assumption of unimodality and, as with the max-
imum entropy model, is not guaranteed to find a global
maximum in the multi-dimensional space.
3.1 Framework
This section formalizes the notion of weight space and
what it means to search for maximum average precision
within it.
Queries in information retrieval can be treated as vec-
tors of terms t1, t2, ? ? ? , tN . Each term is, as the name
suggests, an individual word or phrase that might oc-
cur in the document corpus. Every term t i has a weight
?i determining its ?importance? relative to the other
terms of the query. These weights form a weight vec-
tor ? = ??1 ?2 ? ? ? ?N ?. Further, given a document
corpus ?, for each document dj ? ? we have a ?value
vector? ?j = ??j1 ?j2 ? ? ? ?jN ?, where each ?value?
?ji ? < gives some measure of term ti within document
dj?typically the frequency of occurrence or a function
thereof. In the case of the standard tf-idf formula, ? ji
is the term frequency and ?i the inverse document fre-
quency.
If the document corpus and set of terms is held fixed,
the average precision calculation can be considered a
function f : <N ? [0, 1] mapping ? to a single aver-
age precision value. Finding the weight vectors in this
5 10 15 20
0.
2
0.
4
0.
6
0.
8
1.
0
0.
2
0.
4
0.
6
0.
8
1.
0
number of terms
a
ve
ra
ge
 p
re
cis
io
n
Figure 1: Average precision by query size as generated
by the maximum entropy model for TREC topic 307.
context is then the familiar problem of finding maxima in
an N -dimensional landscape.
3.2 Powell?s algorithm
One general approach to this problem of searching a
multi-dimensional space is to decompose the problem
into a series of iterated searches along single directions
within the space. Perhaps the most basic technique, cred-
ited to Powell, is simply a round-robin-style iteration
along a set of unchanging direction vectors, until conver-
gence is reached (Press et al, 1992, pp. 412-420). This
is the approach used in this study.
Formally, the procedure is as follows. You are given
a set of direction vectors ?1, ?2, ? ? ? , ?N and a starting
point pi0. First move pi0 to the maximum along ?1 and
call this pi1, i.e. pi1 = pi0 + ?1?1 for some scalar ?1.
Next move pi1 to the maximum along ?2 and call this
pi2, and so on, until the final point piN . Finally, replace
pi0 with piN and repeat the entire process, starting again
with ?1. Do this until some convergence criterion is met.
This procedure has no guaranteed rate of convergence,
although more sophisticated versions of Powell?s algo-
rithm do. In practice this has not been a problem.
3.3 Exhaustively searching a single direction
Powell?s algorithm can make use of any one-dimensional
search technique. Rather than applying a completely gen-
eral hill-climbing search, however, in the case where doc-
ument scores are calculated by a linear equation on the
terms, i.e.
?j =
N
?
i=1
?i?ji = ? ? ?j
as they are in the tf-idf formula, we can exhaustively
search in a single direction of the weight space in an effi-
cient manner. This potentially yields better solutions and
potentially converges more quickly than a general hill-
climbing heuristic.
scale
do
cu
m
en
t s
co
re
a
a
b
b
e
c
c
?1
?2
f
f
d d
Figure 2: Sample plot of ? versus ? for a given direction.
The insight behind the algorithm is as follows. Given
a direction ? in weight space and a starting point pi, the
score of each document is a linear function of the scale ?
along ? from pi:
?j = ? ? ?j
= (pi + ??) ? ?j
= pi ? ?j + ? (? ? ?j) .
i.e. document di?s score, plotted against ?, is a line with
slope ? ? ?i and y-intercept pi ? ?j .
Consider the graph of lines for all documents, such as
the example in Figure 2. Each vertical slice of the graph,
at some point ? on the x axis, represents the order of the
documents when ? = ?; specifically, the order of the
documents is given by the order of the intersections of
the lines with the vertical line at x = ?.
Now consider the set of intersections of the document
lines. Given two documents dr and ds, their intersection,
if it exists, lies at point ?rs = (?xrs, ?yrs) where
?xrs =
pi ? (?s ? ?r)
? ? (?r ? ?s)
, and
?yrs = pi ? ?r + ?xrs (? ? ?r)
(Note that this is undefined if ? ? ?r = ? ? ?s, i.e., if the
document lines are parallel.)
Let ? be the set of all such document intersection
points for a given direction, document set and term vec-
tor. Note that more than two lines may intersect at the
same point, and that two intersections may share the same
x component while having different y components.
Now consider the set ?x, defined as the projection of
? onto the x axis, i.e. ?x = {? | ? ? ? ? s.t. ?x = ?}.
The points in ?x represent precisely those values of ?
where two or more documents are tied in score. There-
fore, the document ordering changes at and only at these
points of intersection; in other words, the points in ?x
partition the range of ? into at most M(M ?1)/2+1 re-
gions, where M is the total number of documents. Within
a given region, document ordering is invariant and hence
average precision is constant. As we can calculate the
boundaries of, and the document ordering and average
precision within, each region, we now have a way of find-
ing the maximum across the entire space by evaluating
a finite number of regions. Each of the O(M 2) regions
requires an O(M log M) sort, yielding a total computa-
tional bound of O(M 3 log M).
In fact, we can further reduce the computation by ex-
ploiting the fact that the change in document ordering be-
tween any two regions is known and is typically small.
The weight search algorithm functions in this manner. It
sorts the documents completely to determine the order-
ing in the left-most region. Then, it traverses the regions
from left to right and updates the document ordering in
each, which does not require a sort. Average precision
can be incrementally updated based on the document or-
dering changes. This reduces the computational bound to
O(M2 log M), the requirement for the initial sort of the
O(M2) intersection points.
4 Experiment Setup
In order to compare the results of the weight search al-
gorithm to those of the maximum entropy model, we em-
ployed the same experiment setup. We ran on 15 topics,
which were manually selected from the TREC 6, 7, and
8 collections (Voorhees and Harman, 2000), with the ob-
jective of creating a representative subset. The document
sets were divided into randomly selected training, valida-
tion and test ?splits?, comprising 25%, 25%, and 50%,
respectively, of the complete set.
For each query, a set of candidate terms was selected
based on mutual information between (binary) term oc-
currence and document relevance. From this set, terms
were chosen individually to be included in the query,
and coefficients for all terms were calculated using L-
BFGS, a quasi-Newton unconstrained optimization algo-
rithm (Zhu et al, 1994).
For experimenting with the weight search algorithm,
we investigated queries of length 1 through 20 for each
topic, so each topic involved 20 experiments. The first
term weight was fixed at 1.0. The single-term queries
did not require a weight search, as the weight of a single
term does not affect the average precision score. For the
remaining 19 experiments for each topic, the direction
vectors ? were chosen such that the algorithm searched
a single term weight at a time. For example, a query with
5 10 15 20
0.
2
0.
4
0.
6
0.
8
1.
0
0.
2
0.
4
0.
6
0.
8
1.
0
number of terms
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
301
302
307
330
332
347
352
375
383
384
388
391
407
425
439
Figure 3: Average precision versus query size for the
weight search algorithm. Each line represents a topic.
i terms used the i ? 1 directions
?i,1 = ?0 1 0 0 ? ? ? 0?,
?i,2 = ?0 0 1 0 ? ? ? 0?,...
?i,i?1 = ?0 0 0 0 ? ? ? 1?.
The two-term query for a topic started the search from the
point pi2,0 = ?1 0?, and each successive experiment for
that topic was initialized with the starting point pi0 equal
to the final point in the previous iteration, concatenated
with a 0. The ?value vectors? ?j used in all experiments
were Okapi tf scores.
5 Results
The average precision scores obtained by the maximum
entropy and weight search algorithm experiments are
listed in Table 1. The ?Best AP? and ?No. Terms?
columns describe the query size at which average preci-
sion was best and the score at that point. These columns
show that the maximum entropy approach performs just
as well as the average precision hill-climber, and in some
cases actually performs slightly better. This suggests that
the metric divergence as seen in Figure 1 did not prohibit
the maximum entropy approach from maximizing aver-
age precision in the course of maximizing likelihood.
The ?5 term AP? column compares the performance
of the algorithms on smaller queries. The weight search
algorithm shows a slight advantage over the maximum
entropy model on 10 of the 15 topics and equal perfor-
mance on the others, but definitive conclusions are diffi-
cult at this stage.
Figure 3 shows the average precision achieved by the
weight search algorithm, for all 20 query sizes and for
all 15 topics. Unlike the maximum entropy results,
the algorithm is guaranteed to yield monotonically non-
decreasing scores.
Topic 5 term AP Best AP No. Terms
WS ME WS ME WS ME
301 0.68 0.67 0.90 0.90 >20 >20
302 0.88 0.86 1.00 1.00 10 10
307 0.57 0.56 0.98 0.89 >20 >20
330 0.65 0.61 1.00 1.00 10 10
332 0.74 0.72 0.99 1.00 >20 18
347 0.78 0.78 1.00 1.00 17 14
352 0.55 0.55 0.94 0.93 >20 >20
375 0.92 0.92 1.00 1.00 9 9
383 0.89 0.89 1.00 1.00 9 9
384 0.77 0.73 1.00 1.00 8 8
388 0.82 0.80 1.00 1.00 7 7
391 0.64 0.63 0.99 0.98 >20 >20
407 0.83 0.83 1.00 1.00 9 9
425 0.75 0.73 1.00 1.00 12 12
439 0.53 0.51 1.00 1.00 17 16
Table 1: Average precision achieved for weight search
(WS) and maximum entropy (ME) algorithms.
6 Conclusions
We developed an algorithm for exhaustively searching a
continuous and unbounded direction in term weight space
in O(M2 log M) time. Initial results suggest that the
maximum entropy approach performs as well as this al-
gorithm, which hill-climbs directly on average precision,
allaying our concerns that the metric divergence exhib-
ited by the maximum entropy approach is a problem for
studying optimal queries.
References
William H. Press, Brian P. Flannery, Saul A. Teukolsky,
and William T. Vetterling. 1992. Numerical Recipes
in C: The Art of Scientic Computing. Cambridge Uni-
versity Press, second edition.
S. E. Robertson and S. Walker. 1994. Some simple effec-
tive approximations to the 2-Poisson model for proba-
bilistic weighted retrieval. In W. Bruce Croft and C. J.
van Rijsbergen, editors, Proc. 17th SIGIR Conference
on Information Retrieval.
S. E. Robertson. 1977. The probability ranking principle
in IR. Journal of Documentation, 33:294?304.
E. M. Voorhees and D. K. Harman. 2000. Overview of
the eighth Text REtrieval Conference (TREC-8). In
E. M. Voorhees and D. K. Harman, editors, The Eighth
Text REtreival Conference (TREC-8). NIST Special
Publication 500-246.
C. Zhu, R. Byrd, P. Lu, and J. Nocedal. 1994. LBFGS-B:
Fortran subroutines for large-scale bound constrained
optimization. Technical Report NAM-11, EECS De-
partment, Northwestern University.
Coaxing Confidences from an Old Friend: 
Probabilistic Classifications from Transformation Rule Lists 
Radu F lo r ian*  John  C.  Henderson  t Grace  Nga i*  
*Department of Computer  Science 
Johns Hopkins University 
Baltimore, MD 21218, USA 
{rf lorian,gyn}@cs.jhu.edu 
tThe MITRE Corporat ion  
202 Bur l ington Road 
Bedford,  MA 01730, USA 
jhndrsn@mitre .org 
Abst rac t  
Transformation-based l arning has been success- 
fully employed to solve many natural language 
processing problems. It has many positive fea- 
tures, but one drawback is that it does not provide 
estimates of class membership probabilities. 
In this paper, we present a novel method for 
obtaining class membership robabilities from a 
transformation-based rule list classifier. Three ex- 
periments are presented which measure the model- 
ing accuracy and cross-entropy ofthe probabilistic 
classifier on unseen data and the degree to which 
the output probabilities from the classifier can be 
used to estimate confidences in its classification 
decisions. 
The results of these experiments show that, for 
the task of text chunking 1, the estimates produced 
by this technique are more informative than those 
generated by a state-of-the-art decision tree. 
1 In t roduct ion  
In natural language processing, a great amount of 
work has gone into the development of machine 
learning algorithms which extract useful linguistic 
information from resources such as dictionaries, 
newswire feeds, manually annotated corpora and 
web pages. Most of the effective methods can 
be roughly divided into rule-based and proba- 
bilistic algorithms. In general, the rule-based 
methods have the advantage of capturing the 
necessary information in a small and concise set 
of rules. In part-of-speech tagging, for exam- 
ple, rule-based and probabilistic methods achieve 
comparable accuracies, but rule-based methods 
capture the knowledge in a hundred or so simple 
rules, while the probabilistic methods have a 
very high--dimensional parameter space (millions 
of parameters). 
One of the main advantages of probabilistic 
methods, on the other hand, is that they include a 
measure of uncertainty in their output. This can 
take the form of a probability distribution over 
potential outputs, or it may be a ranked list of 
IA11 the experiments are performed on text chnnklng. 
The technique presented is general-purpose, however, and 
can be applied to many tasks for which transformation- 
based learning performs well, without changing the inter- 
rials of the learner. 
candidate outputs. These uncertainty measures 
are useful in situations where both the classifi- 
cation of an sample and the system's confidence 
in that classification are needed. An example of 
this is a situation in an ensemble system where 
ensemble members disagree and a decision must 
be made about how to resolve the disagreement. 
A similar situation arises in pipeline systems, such 
as a system which performs parsing on the output 
of a probabilistic part-of-speech tagging. 
Transformation-based learning (TBL) (Brill, 
1995) is a successful rule-based machine learning 
algorithm in natural language processing. It has 
been applied to a wide variety of tasks, including 
part of speech tagging (Roche and Schabes, 1995; 
Brill, 1995), noun phrase chvnklng (Ramshaw and 
Marcus, 1999), parsing (Brill, 1996; Vilain and 
Day, 1996), spelling correction (Mangu and Brill, 
1997), prepositional phrase attachment (Brill and 
Resnik, 1994), dialog act tagging (Samuel et 
al., 1998), segmentation and message understand- 
ing (Day et al, 1997), often achieving state- 
of-the-art performance with a small and easily- 
understandable list of rules. 
In this paper, we describe a novel method 
which enables a transformation-based classifier to 
generate a probability distribution on the class 
labels. Application of the method allows the 
transformation rule list to retain the robustness of 
the transformation-based algorithms, while bene- 
fitting from the advantages ofa probabilistic clas- 
sifter. The usefulness of the resulting probabilities 
is demonstrated bycomparison with another state- 
of-the-art classifier, the C4.5 decision tree (Quin- 
lan, 1993). The performance of our algorithm 
compares favorably across many dimensions: it 
obtains better perplexity and cross-entropy; an 
active learning algorithm using our system outper- 
forms a similar algorithm using decision trees; and 
finally, our algorithm has better rejection curves 
than a similar decision tree. Section 2 presents the 
transformation based learning paradigm; Section 
3 describes the algorithm for construction of the 
decision tree associated with the transformation 
based list; Section 4 describes the experiments 
in detail and Section 5 concludes the paper and 
outlines the future work. 
26 
2 Trans format ion  ru le  l i s t s  
The central idea of transformation-based l arn- 
ing is to learn an ordered list of rules which 
progressively improve upon the current state of 
the training set. An initial assignment is made 
based on simple statistics, and then rules are 
greedily learned to correct he mistakes, until no 
net improvement can be made. 
These definitions and notation will be used 
throughout the paper: 
? X denotes the sample space; 
? C denotes the set of possible classifications of 
the samples; 
? The state space is defined as 8 = X x C. 
? 7r will usually denote a predicate defined on 
X; 
? A rule r is defined as a predicate - class label 
- time tuple, (~r,c,t), c E C,t E N, where t is 
the learning iteration in which when the rule 
was learned, its position in the list. 
? A rule r = (~r, c, t) applies to a state (z, y) if 
7r(z) = true and c # y. 
Using a TBL framework to solve a problem as- 
sumes the existence of: 
? An initial class assignment (mapping from X 
to ,.9). This can be as simple as the most 
common class label in the training set, or it 
can be the output from another classifier. 
? A set of allowable templates for rules. These 
templates determine the predicates the rules 
will test, and they have the biggest influence 
over the behavior of the system. 
? An objective function for learning. Unlike in 
many other learning algorithms, the objective 
function for TBL will typically optimize the 
evaluation function. An often-used method is 
the difference in performance resulting from 
applying the rule. 
At the beginning of the learning phase, the 
training set is first given an initial class assign- 
ment. The system then iteratively executes the 
following steps: 
1. Generate all productive rules. 
2. For each rule: 
(a) Apply to a copy of the most recent state 
of the training set. 
(b) Score the result using the objective func- 
tion. 
3. Select he rule with the best score. 
4. Apply the rule to the current state of the 
training set, updating it to reflect his change. 
5. Stop if the score is smaller than some pre-set 
threshold T. 
6. Repeat from Step 1. 
The system thus learns a list of rules in a greedy 
fashion, according to the objective function. When 
no rule that improves the current state of the 
training set beyond the pre-set threshold can 
be found, the training phase ends. During the 
evaluation phase, the evaluation set is initialized 
with the same initial class assignment. Each rule 
is then applied, in the order it was learned, to the 
evaluation set. The final classification is the one 
attained when all rules have been applied. 
3 Probab i l i ty  es t imat ion  w i th  
t rans format ion  ru le  l i s t s  
Rule lists are infamous for making hard decisions, 
decisions which adhere entirely to one possibility, 
excluding all others. These hard decisions are 
often accurate and outperform other types of 
classifiers in terms of exact-match accuracy, but 
because they do not have an associated proba- 
bility, they give no hint as to when they might 
fail. In contrast, probabilistic systems make soft 
decisions by assigning a probability distribution 
over all possible classes. 
There are many applications where soft deci- 
sions prove useful. In situations such as active 
learning, where a small number of samples are 
selected for annotation, the probabilities can be 
used to determine which examples the classifier 
was most unsure of, and hence should provide the 
most extra information. A probabilistic system 
can also act as a filter for a more expensive 
system or a human expert when it is permitted 
to reject samples. Soft decision-making is also 
useful when the system is one of the components 
in a larger decision-malting process, as is the case 
in speech recognition systems (Bald et al, 1989), 
or in an ensemble system like AdaBoost (Freund 
and Schapire, 1997). There are many other 
applications in which a probabilistic lassifier is 
necessary, and a non-probabHistic classifier cannot 
be used instead. 
3.1 Estimation via conversion to decision 
tree 
The method we propose to obtain probabilis- 
tic classifications from a transformation rule list 
involves dividing the samples into equivalence 
classes and computing distributions over each 
equivalence class. At any given point in time i, 
each sample z in the training set has an associated 
state si(z) = (z,~l). Let R (z )  to be the set of rules 
r~ that applies to the state el(z), 
R(z) = {ri ~ 7~Ir~ applies to si(z)} 
An  equivalence class consists of all the samples 
z that have the same R(z). Class probability 
assignments are then estimated using statistics 
computed on the equivalence classes. 
27 
An illustration of the conversion from a rule 
list to a decision tree is shown below. Table 1 
shows an example transformation rule list. It is 
straightforward to convert this rule list into a de- 
cision pylon (Bahl et al, 1989)~. which can be used 
to represent all the possible sequences of labels 
assigned to a sample during the application of the 
TBL  algorithm. The decision pylon associated 
with this particular rule list is displayed on the left 
side of Figure 1. The decision tree shown on the 
right side of Figure 1 is constructed such that the 
samples stored in any leaf have the same class label 
sequence as in the displayed decision pylon. In 
the decision pylon, "no" answers go straight down; 
in the decision tree, "yes" answers take the right 
branch. Note that a one rule in the transformation 
rule list can often correspond to more than one 
node in the decision tree. 
Initial label = A 
I f  Q1 and label=A then  label+-B 
I f  Q2 and label=A then  labele-B 
I f  Q3 and label=B then  label~A 
Table I: Example of a Transformation Rule List. 
Figure 1: Converting the transformation rule list 
from Table 1 to a decision tree, 
The conversion from a transformation rule list 
to a decision tree is presented as a recursive 
procedure. The set of samples in the training set 
is transformed to a set of states by applying the 
initial class assignments. A node n is created for 
each of the initial class label assignments c and all 
states labeled c are assigned to n. 
The following recursive procedure is invoked 
with an initial "root" node, the complete set of 
states (from the corpus) and the whole sequence 
of rules learned uring training: 
A lgor i thm:  Ru leL is tToDec is ionTree  
(RLTDT)  
Input :  
* A set/3 of N states ((Zl, Yl) --- (ZN, YN)) with 
labels Yi E C; 
? A set 7~ of M rules (ro,rl . . .rM) where ri = 
Do:  
1. If 7~ is empty, the end of the rule list has been 
reached. Create a leaf node, n, and estimate 
the probability class distribution based on the 
true classifications of the states in 13. Return 
n.  
2. Let rj  = (Ir j ,yj , j )  be the lowest-indexed rule 
in 7~. Remove it from 7~. 
3. Split the data in/3 using the predicate 7rj and 
the current hypothesis uch that samples on 
which 7rj returns true are on the right of the 
split: 
BL = {x E BlTrj(x ) = false} 
/3R = {x E/31 j(x) = true} 
4. If IBLI > K and IBRI > K,  the split is 
acceptable: 
(a) Create a new internal node, n; 
(b) Set the question: q(n) = 7rj; 
(c) Create the left child of n using a recursive 
call to RLTDT(BL, 7~); 
(d) Create the right child of n using a recur- 
sive call to RLTDT(BR, 7~); 
(e) Return node n. 
Otherwise, no split is performed using rj.  
Repeat from Step 1. 
The parameter K is a constant that determines the 
minimum weight that a leaf is permitted to have, 
effectively pruning the tree during construction. 
In all the experiments, K was set to 5. 
3.2 Fur ther  growth  o f  the  decis ion t ree  
When a rule list is converted into a decision tree, 
there are often leaves that are inordinately heavy 
because they contain a large number of samples. 
Examples of such leaves are those containing 
samples which were never transformed by any 
of the rules in the rule list. These populations 
exist either because they could not be split up 
during the rule list learning without incurring a 
net penalty, or because any rule that acts on them 
has an objective function score of less than the 
threshold T. This is sub-optimal for estimation 
because when a large portion of the corpus falls 
into the same equivalence class, the distribution 
assigned to it reflects only the mean of those 
samples. The undesirable consequence is that all 
of those samples are given the same probability 
distribution. 
To ameliorate this problem, those samples are 
partitioned into smaller equivalence classes by 
further growing the decision tree. Since a decision 
tree does not place all the samples with the same 
current label into a single equivalence class, it does 
not get stuck in the same situation as a rule list 
m in which no change in the current state of 
corpus can be made without incurring a net loss 
in performance. 
28 
Continuing to grow the decision tree that was 
converted from a rule list can be viewed from 
another angle. A highly accurate prefix tree 
for the final decision tree is created by tying 
questions together during the first phase of the 
growth process (TBL). Unlike traditional decision 
trees which select splitting questions for a node 
by looking only at the samples contained in the 
local node, this decision tree selects questions by 
looking at samples contained in all nodes on the 
frontier whose paths have a suM< in common. An 
illustration of this phenomenon can be seen in 
Figure 1, where the choice to split on Question 
3 was made from samples which tested false 
on the predicate of Question 1, together with 
samples which tested false on the predicate of 
Question 2. The result of this is that questions 
are chosen based on a much larger population than 
in standard decision tree growth, and therefore 
have a much greater chance of being useful and 
generalizable. This alleviates the problem of over- 
partitioning of data, which is a widely-recognized 
concern during decision tree growth. 
The decision tree obtained from this conversion 
can be grown further. When the rule list 7~ is 
exhausted at Step 1, instead of creating a leaf 
node, continue splitting the samples contained in 
the node with a decision tree induction algorithm. 
The splitting criterion used in the experiments is 
the information gain measure. 
4 Exper iments  
Three experiments that demonstrate the effec- 
tiveness and appropriateness of our probability 
estimates are presented in this section. The 
experiments are performed on text chunking, a 
subproblem ofsyntactic parsing. Unlike full pars- 
ing, the sentences are divided into non-overlapping 
phrases, where each word belongs to the lowest 
parse constituent that dominates it. 
The data used in all of these experiments i  
the CoNLL-2000 phrase chunking corpus (CoNLL, 
2000). The corpus consists of sections 15-18 and 
section 20 of the Penn Treebank (Marcus et al, 
1993), and is pre-divided into a 8936-sentence 
(211727 tokens) training set and a 2012-sentence 
(47377 tokens) test set. The chunk tags are 
derived from the parse tree constituents, and the 
part-of-speech tags were generated by the Brill 
tagger (Brill, 1995). 
As was noted by Ramshaw & Marcus (1999), 
text chunking can be mapped to a tagging task, 
where each word is tagged with a chunk tag 
representing the phrase that it belongs to. An 
example sentence from the corpus is shown in 
Table 4. As a contrasting system, our results 
are compared with those produced by a C4.5 
decision tree system (henceforth C4.5). The 
reason for using C4.5 is twofold: firstly, it is a 
widely-used algorithm which achieves state-.of-the- 
art performance on a broad variety of tasks; and 
Word 
A.P. 
Green 
currently 
has 
2,664,098 
shares 
outstanding 
POS tag 
NNP 
NNP 
RB 
VBZ 
CD 
NNS 
JJ 
Chunk Tag 
B-NP 
I-NP 
B-ADVP 
B-VP 
B-NP 
I-NP 
B-ADJP 
O 
Table 2: Example of a sentence with chunk tags 
secondly, it belongs to the same class of classifiers 
as our converted transformation-based rule list 
(henceforth TBLDT). 
To perform a fair evaluation, extra care was 
taken to ensure that both C4.5 and TBLDT 
explore as similar a sample space as possible. The 
systems were allowed to consult the word, the 
part-of-speech, and the chunk tag of all examples 
within a window of 5 positions (2 words on either 
side) of each target example. 2 Since multiple 
features covering the entire vocabulary of the 
training set would be too large a space for C4.5 
to deal with, in all of experiments where TBLDT 
is directly compared with C4.5, the word types 
that both systems can include in their predicates 
are restricted to the most "ambiguous" 100 words 
in the training set, as measured by the number of 
chunk tag types that are assigned to them. The 
initial prediction was made for both systems using 
a class assignment based solely on the part-of- 
speech tag of the word. 
Considering chunk tags within a contextual win- 
dow of the target word raises a problem with C4.5. 
A decision tree generally trains on independent 
samples and does not take into account changes 
of any features in the context. In our case, the 
samples are dependent; the classification ofsample 
i is a feature for sample i + 1, which means that 
changing the classification for sample i affects 
the context of sample i + 1. To address this 
problem, the C4.5 systems are trained with the 
correct chlmk~ in the left context. When the 
system is used for classification, input is processed 
in a left-to-right manner;and the output of the 
system is fed forward to be used as features 
in the left context of following samples. Since 
C4.5 generates probabilities for each classification 
decision, they can be redirected into the input for 
the next position. Providing the decision treewith 
this confidence information effectively allows it to 
perform a limited search over the entire sentence. 
C4.5 does have one advantage over TBLDT, 
however. A decision tree can be trained using the 
subsetting feature, where questions asked are of 
the form: "does feature f belong to the set FT'. 
This is not something that a TBL can do readily, 
2The TBL templates are similar to those used in 
l~am.~haw and Marcus (1999). 
29 
but since the objective is in comparing TBLDT to 
another state-of-the-art system, this feature was 
enabled. 
4.1 Evaluation Measures 
The most commonly used measure for evaluating 
tagging tasks is tag accuracy, lit is defined as 
Accuracy = # of correctly tagged examples 
of examples 
In syntactic parsing, though, since the task is 
to identify the phrasal components, it is more 
appropriate o measure the precision and recall: 
# of correct proposed phrases 
Precision = 
# of proposed phrases 
# of correct proposed phrases 
Recall = # of correct phrases 
To facilitate the comparison of systems with dif- 
ferent precision and recall, the F-measure metric 
is computed as a weighted harmonic mean of 
precision and recall: 
(82 + 1) ? Precision x Recall 
= 
82 x Precision + Recall 
The ~ parameter is used to give more weight to 
precision or recall, as the task at hand requires. 
In all our experiments, ~ is set to 1, giving equal 
weight o precision and recall. 
The reported performances are all measured 
with the evaluation tool provided with the CoNLL 
corpus (CoNLL, 2000). 
4.2 Active Learning 
To demonstrate the usefulness of obtaining proba- 
bilities from a transformation rule list, this section 
describes an application which utilizes these prob- 
abilities, and compare the resulting performance 
of the system with that achieved by C4.5. 
Natural language processing has traditionally 
required large amounts of annotated ata from 
which to extract linguistic properties. However, 
not all data is created equal: a normal distribu- 
tion of aunotated ata contains much redundant 
information. Seung et al (1992) and Freund et 
al. (1997) proposed a theoretical ctive learning 
approach, where samples are intelligently selected 
for annotation. By eliminating redundant infor- 
mation, the same performance can be achieved 
while using fewer resources. Empirically, active 
learning has been applied to various NLP tasks 
such as text categorization (Lewis and Gale, 1994; 
Lewis and Catlett, 1994; Liere and Tadepalli, 
1997), part-of-speech tagging (Dagan and Engel- 
son, 1995; Engelson and Dagan, 1996), and base 
noun phrase chunbiug (Ngai and Yarowsky, 2000), 
resulting in significantly large reductions in the 
quantity of data needed to achieve comparable 
performance. 
This section presents two experimental results 
which show the effectiveness of the probabilities 
generated by the TBLDT. The first experiment 
compares the performance achieved by the active 
learning algorithm using TBLDT with the perfor- 
mance obtained by selecting samples equentially 
from the training set. The second experiment 
compares the performances achieved by TBLDT 
and C4.5 training on samples elected by active 
learning. 
The following describes the active learning algo- 
rithm used in the experiments: 
1. Label an initial T1 sentences ofthe corpus; 
2. Use the machine learning algorithm (G4.5 or 
TBLDT) to obtain chunk probabilities on the 
rest of the training data; 
3. Choose T2 samples from the rest of the train- 
ing set, specifically the samples that optimize 
an evaluation function f ,  based on the class 
distribution probability of each sample; 
4. Add the samples, including their "true" classi- 
fication 3 to the training pool and retrain the 
system; 
5. If a desired number of samples is reached, 
stop, otherwise repeat from Step 2. 
The evaluation function f that was used in our 
experiments is:
where H(UIS, i ) is the entropy of the chllnk 
probability distribution associated with the word 
index i in sentence S. 
Figure 2 displays the performance (F-measure 
and chllnk accuracy) of a TBLDT system trained 
on samples elected by active learning and the 
same system trained on samples elected sequen- 
tially from the corpus versus the number of words 
in the annotated tralniug set. At each step of 
the iteration, the active learning-trained TBLDT 
system achieves a higher accuracy/F-measure, or, 
conversely, is able to obtain the same performance 
level with less training data. Overall, our system 
can yield the same performance as the sequential 
system with 45% less data, a significant reduction 
in the annotation effort. 
Figure 3 shows a comparison between two active 
learning experiments: one using TBLDT and the 
other using C4.5. 4 For completeness, a sequential 
run using C4.5 is also presented. Even though 
C4.5 examines a larger space than TBLDT by 
SThe true (reference or gold standard) classification is 
available in this experiment. In an annotation situation, 
the samples are sent o human annotators for labeling. 
4As mentioned arlier, both the TBLDT and C4.5 were 
limited to the same 100 most ambiguous words in the 
corpus to ensure comparability. 
3O 
84 
AL?TBLDT ' ~ ' ' ' 
i i i I i 
(a) F-measure vs. number of words in trrdniug set 
Oil 
AL* 'mI .~ ' - -  . . . .  
i I 
(b) Chunk Accuracy vs. number of words in training 
set 
Figure 2: Performance of the TBLDT system versus sequential choice. 
87 
86 
| -  
81 
...=2- ' 
, . r .~ . . . r  - - I~ ' ' ' '~ ' '~"  
\ [~"  
i 
I i L i 
(a) F-measure vs. number of words in tr~inln s set 
31 
AL?'nBL (I0? ~ )  ~ '  
J i i ~ i i 
(b) Accuracy vs. number of words in training set 
Figure 3: Performance of the TBLDT system versus the DT system 
utilizing the feature subset predicates, TBLDT 
still performs better. The difference in accuracy at 
26200 words (at the end of the active learning run 
for TBLDT) is statistically significant at a 0.0003 
level. 
As a final remark on this experiment, note that 
at an annotation level of 19000 words, the fully 
lexicalized TBLDT outperformed the C4.5 system 
by making 15% fewer errors. 
4.3 Re jec t ion  curves 
It is often very useful for a classifier to be able 
to offer confidence scores associated with its deci- 
sions. Confidence scores are associated with the 
probability P(C(z) correct\[z) where C(z) is the 
classification of sample z. These scores can be 
used in real-life problems to reject samples that 
the the classifier is not sure about, in which case 
a better observation, or a human decision, might 
be requested. The performance of the classifier 
is then evaluated on the samples that were not 
rejected. This experiment framework is well- 
established in machine learning and optimization 
research (Dietterich and Bakiri, 1995; Priebe et 
al., 1999). 
Since non-probabilistic classifiers do not offer 
any insights into how sure they are about a 
particular classification, it is not easy to obtain 
confidence scores from them. A probabilistic 
classifier, in contrast, offers information about the 
class probability distribution of a given sample. 
Two measures that can be used in generating 
confidence scores are proposed in this section. 
The first measure, the entropy H of the class 
probability distribution of a sample z, C(z) = 
{p(CllZ),p(c2\[z)...p(cklZ)}, i s  a measure  of the 
uncertainty in the distribution: 
k 
HCCCz)) = - I=) log2 pC Iz) 
i=I 
The higher the entropy of the distribution of 
class probability estimates, the more uncertain the 
0.99 
0.98 
0.97 
g~ 
0.~ 
~0.95  
0.94 
0.93 
///.._.-.--f" 
/ / C4.5 (hard d=fisions)__ i 
I / / / . -  ..... ~ % _ .. ~.; 
':" \] 
I I I I I I I I I 
0J O2 O3 0.4 0.5 O.6 0,7 O.8 O.9 Z 
l~c~t of rej~xaed ~
(a)  Subcorpus  (batch)  re jec t ion  
0~ 1 i i 1 
0.985 " 
0~8 
O.975 
097 
0.965 
0.96 - TBL-DT 
0.955 
095 - C4_5 (soft decisi 
0.945 ..- .... 
0.94 .-.:.-.-.r.-. r.~---.':.'.':.'. "
0.935 \[ 
0 0.2 0.4 0.6 ~8 1 
Probability of th~ most lflmly tag 
(b)  Thresho ld  (on l ine)  re jec t ion  
Figure 4: Rejection curves. 
classifier is of its classification. The samples e- 
lected for rejection are chosen by sorting the data 
using the entropies of the estimated probabilities, 
and then selecting the ones with highest entropies. 
The resulting curve is a measure of the correlation 
between the true probability distribution and the 
one given by the classifier. 
Figure 4(a) shows the rejection curves for the 
TBLDT system and two C4.5 decision trees - one 
which receives a probability distribution as input 
("soft" decisions on the left context) , and one 
which receives classifications ("hard" decisions on 
all fields). At the left of the curve, no samples 
are rejected; at the right side, only the samples 
about which the classifiers were most certain are 
kept (the samples with minimum entropy). Note 
that the y-values on the right side of the curve are 
based on less data, effectively introducing wider 
variance in the curve as it moves right. 
As shown in Figure 4(a), the C4.5 classifier 
that has access to the left context chunk tag 
probability distributions behaves better than the 
other C4.5 system, because this information about 
the surrounding context allows it to effectively 
perform a shallow search of the classification 
space. The TBLDT system, which also receives 
a probability distribution on the chunk tags in 
the left context, clearly outperforms both C4.5 
systems at all rejection levels. 
The second proposed measure is based on the 
probability of the most likely tag. The assumption 
here is that this probability is representative of 
how certain the system is about the classifica- 
tion. The samples are put in bins based on 
the probability of the most likely chnnk tag, and 
accuracies are computed for each bin (these bins 
are cumulative, meaning that a sample will be 
included in all the bins that have a lower threshold 
than the probability of its most likely chnnl? 
tag). At each accuracy level, a sample will be 
rejected if the probability of its most likely chnn~ 
Cross Entropy 
TBLDT 1.2944 0.2580 
DT+probs 1.4150 0.3471 
DT 1.4568 0.3763 
Table 3: Cross entropy and perplexities for two 
C4.5 systems and the TBLDT system 
is below the accuracy level. The resulting curve 
is a measure of the correlation between the true 
distribution probability and the probability of the 
most likely chunk tag, i.e. how appropriate those 
probabilities are as confidence measures. Unlike 
the first measure mentioned before, a threshold 
obtained using this measure can be used in an 
online manner to identify the samples of whose 
classification the system is confident. 
Figure 4(b) displays the rejection curve for 
the second measure and the same three systems. 
TBLDT again outperforms both C4.5 systems, at 
all levels of confidence. 
In summary, the TBLDT system outperforms 
both C4.5 systems presented, resulting in fewer re- 
jections for the same performance, or, conversely, 
better performance at the same rejection rate. 
4.4 Perp lex i ty  and  Cross Ent ropy  
Cross entropy is a goodness measure for probabil- 
ity estimates that takes into account he accuracy 
of the estimates as well as the classification accu- 
racy of the system. It measures the performance 
of a system trained on a set of samples distributed 
according to the probability distribution p when 
tested on a set following a probability distribution 
q. More specifically, we utilize conditional cross 
entropy, which is defined as 
n (C lX)  = - q (=) -  q(cl=) ? log2 pC@:) 
zEX ?EC 
where X is the set of examples and C is the set of 
chnnlr tags, q is the probabil i ty distribution on the 
32 
Chunk  
Type  
A c c u r a c y  
(%) 
Precisionl Recall 
(%) I (%) 
Overall 95.23 92.02 92.50 
ADJP  - 75.69 68.95 
ADVP - 80.88 78.64 
CONJP  - 40.00 44.44 
INTJ - 50.00 50.00 
LST - 0.00 0.00 
NP  - 92.18 92.72 
PP  95.89 97.90 
PRT  - 67.80 75.47 
SBAR 88.71 82.24 
VP 92.00 92.87 
Fi 
92.26! 
72.16 
79.74 
42.11 
50.00 
0.00 
92.45 
96.88 
71.43 
85.35 
92.44 
Table 4: Performance of TBLDT on the CoNLL 
Test Set 
test document and p is the probability distribution 
on the train corpus. 
The cross entropy metric fails if any outcome is 
given zero probability by the estimator. To avoid 
this problem, estimators are "smoothed", ensuring 
that novel events receive non-zero probabilities. 
A very simple smoothing technique (interpolation 
with a constant) was used for all of these systems. 
A closely related measure is perplexity, defined 
as 
P = 2~(cl x) 
The cross entropy and perplexity results for the 
various estimation schemes are presented in Table 
? 3. The TBLDT outperforms both C4.5 systems, 
obtaining better cross-entropy and chunk tag per- 
plexity. This shows that the overall probability 
distribution obtained from the TBLDT system 
better matches the true probability distribution. 
This strongly suggests hat probabilities generated 
this way can be used successfully in system com- 
bination techniques such as voting or boosting. 
4.5 Chunk ing  per formance 
It is worth noting that the transformation-based 
system used in the comparative graphs in Figure 
3 was not r, uning at full potential. As described 
earlier, the TBLDT system was only allowed to 
consider words that C4.5 had access to. However, 
a comparison between the corresponding TBLDT 
curves in Figures 2 (where the system is given 
access to all the words) and 3 show that a 
transformation-based system given access to all 
the words performs better than the one with a 
restricted lexicon, which in turn outperforms the 
best C4.5 decision tree system both in terms of 
accuracy and F-measure. 
Table 4 shows the performance of the TBLDT 
system on the full CoNLL  test set, broken down 
by chunk type. Even though the TBLDT results 
could not be compared with other published re- 
sults on the same task and data (CoNLL  will 
not take place until September 2000), our system 
significantly outperforms a similar system trained 
with a C4.5 decision tree, shown in Table 5, both 
in chunk accuracy and F-measure. 
Chunk 
Type 
Accuracy 
(%) 
ADVP 
CONJP  
IJrecision 
(%) 
Recall 
(%) 
Overall 93.80 90.02 90.26 
ADJP 65.58 64.38 
74.14 76.79 
33.33 
INTJ 50.00 50.00 
LST 0.00 0.00 
NP  91.00 90.93 
PP  92.70 96.36 
PRT  71.13 65.09 
SBAR 86.35 61.50 
VP  90.71 91.22 
I Fz 
90.14 
64.98 
75.44 
33.33 
50.00 
0.00 
90.96 
94.50 
67.98 
71.83 
90.97 
Table 5: Performance of C4.5 on the CoNLL Test 
Set 
5 Conclus ions 
In this paper we presented a novel way to convert 
transformation rule lists, a common paradigm in 
natural anguage processing, into a form that is 
equivalent in its classification behavior, but is 
capable of providing probability estimates. Using 
this approach, favorable properties of transfor- 
mation rule lists that makes them popular for 
language processing are retained, while the many 
advantages of a probabilistic system axe gained. 
To demonstrate he efficacy of this approach, 
the resulting probabilities were tested in three 
ways: directly measuring the modeling accuracy 
on the test set via cross entropy, testing the 
goodness of the output probabilities in a active 
learning algorithm, and observing the rejection 
curves attained from these probability estimates. 
The experiments clearly demonstrate that the 
resulting probabilities perform at least as well as 
the ones generated by C4.5 decision trees, resulting 
in better performance in all cases. This proves that 
the resulting probabilistic lassifier is as least as 
good as other state-of-the-art p obabilistic models. 
The positive results obtained suggest hat the 
probabilistic lassifier obtained from transforma- 
tion rule lists can be successfully used in machine 
learning algorithms that require soft-decision clas- 
sifters, such as boosting or voting. Future research 
will include testing the behavior of the system 
under AdaBoost (Freund and Schapire, 1997). We 
also intend to investigate the effects that other 
decision tree growth and smoothing techniques 
may have on continued refinement of the converted 
rule list. 
6 Acknowledgements  
We thank Eric Brill, Fred Jelinek and David 
Yaxowsky for their invaluable advice and sugges- 
tions. In addition we would like to thank David 
Day, Ben Weliner and the anonymous reviewers 
for their useful comments and suggestions on the 
paper . . . .  
The views expressed in this paper are those of 
the authors and do not necessarily reflect he views 
33 
of the MITRE Corporation. It  was performed 
as a collaborative ffort at \]both MITRE and 
the Center for Language and ',Speech Processing, 
Johns Hopkins University, Baltimore, MD. It was 
supported by NSF grants numbered IRI-9502312 
and IRI-9618874, as well as the MITRE-Sponsored 
Research program. 
References 
L. Bahl, P. Brown, P. de Souza, and R. Mercer. 1989. 
A tree-based statistical language model for natural 
language speech recognition. IEEE Transactions on 
Acoustics, Speech and Signal Processing, 37:1001- 
1008. 
E. BriU and P. Resnik. 1994. A rule-based approach 
to prepositional phrase attachment disambiguation. 
In Proceedings of the Fifteenth International Con- 
ference on Computational Linguistics (COLING- 
199~), pages 1198--1204, Kyoto. 
E. BrllL 1995. Transformation-based rror-driven 
learning and natural language processing: A case 
study in part of speech tagging. Computational 
Linguistics, 21(4):543-565. 
E. Brill, 1996. Learning to Parse with Transforma- 
tions. In H. Bunt and M. Tomita (eds.) Recent 
Advances in Parsing Technology, Kluwer. 
CoNLL. 2000. Shared task for computational natu- 
ral language learning (CoNLL), 2000. http://lcg- 
ww w.uia.ac.be/conU2000/chunking. 
I. Dagan and S. Engelson. 1995. Committee-based 
sampling for training probabilistic lassifiers. In 
Proceedings ofInternational Conference on Machine 
Learning (ICML) 1995, pages 150-157. 
D. Day, J. Aberdeen, L. Hirschman, R. Kozierok, 
P. Robinson, and M. Vllaln. 1997. Mixed-initiative 
development of language processing systems. In 
Fifth Conference on Applied Natural Language Pro- 
cessing, pages 348-355. Association for Computa- 
tional Linguistics, March. 
T. G. Dietterich and G. Bakiri. 1995. Solving multi- 
class learning problems via error-correcting output 
codes. Journal of Artificial Intelligence Research, 
2:263-286. 
S. Engelson and I. Dagan. 1996. Minlmi~.ing manual 
annotation cost in supervised training fxom corpora. 
In Proceedings of ACL 1996, pages 319-326, Santa 
Cruz, CA. Association for Computational Linguis- 
tics. 
Y. Freund and R.E. Schapire. 1997. A decision- 
theoretic generalization of on-fine learning and an 
application to boosting. Journal of Computer and 
System Sciences, 55(1):119--139. 
Y. Fremad, H. S. Senng, E. Shamir, and N. Tishby. 
1997. Selective sampling using the query by com- 
mittee algorithm. Machine Learning, 28:133-168. 
D. Lewis and J. Catlett. 1994. Heterogeneous n- 
certainty sampling for supervised learning. In Pro- 
ceedings of the 11th International Conference on 
Machine Learning, pages 139---147. 
D. Lewis and W. Gale. 1994. A sequential algorithm 
for training text classifiers. In Proceedings ofA CM- 
SIGIR 1994, pages 3-12. ACM-SIGIR. 
R. Liere and P. Tadepalli. 1997. Active learning with 
committees for text categorization. In Proceedings 
of the Fourteenth National Conference on Artificial 
Intelligence, pages 591-596. AAAI. 
L. Mangu and E. Brill. 1997. Automatic rule acquisi- 
tion for spelling correction. In Proceedings of the 
Fourteenth International Conference on Machine 
Learning, pages 734-741, Nashville, Tennessee. 
M. P. Marcus, B. Santorini, and M. A. Mareinkiewicz. 
1993. Building a large annotated corpus of english: 
The Penn Treebank. Computational Linguistics, 
19(2):313-330. 
G. Ngai and D. Yarowsky. 2000. Rule writing or 
annotation: Cost-efficient resource usage for base 
noun phrase chunking. In Proceedings ofA CL 2000. 
Association for Computational Linguistics. 
C. E. Priebe, J.-S. Pang, and T. Olson. 1999. Opt lmiT. 
ing mine classification performance. In Proceedings 
of the JSM. American Statistical Association. 
J. R. Qnlnlan. 1993. C~.5: Programs for machine 
learning. Morgan Kanfmann, San Mateo, CA. 
L. Ramshaw and M. Marcus, 1999. Text Chunk- 
ing Using Transformation-based Learning. In S. 
Armstrong, K.W. Church, P. Isabelle, S. Mauzi, 
E. Tzoukermann and D. Yarowsky (eds.) Natural 
Language Processing Using Very Large Corpora, 
Kluwer. 
E. Roche and Y. Schabes. 1995. Computational 
linguistics. Deterministic Part of Speech Tagging 
with Finite State Transducers, 21(2):227-253. 
K. Samuel, S. Carberry, and K. Vijay-Shanker. 1998. 
Dialogue act tagging with transformation-based 
learning. In Proceedings of the 17th International 
Conference on Computational Linguistics and the 
36th Annual Meeting of the Association for Com- 
putational Linguistics, pages 1150-1156, Montreal, 
Quebec, Canada. 
H. S. Senng, M. Opper, and H. Sompolinsky. 1992. 
Query by committee. In Proceedings of the Fifth 
Annual A CM Workshop on Computational Learning 
Theory, pages 287-294. ACM. 
M. Vilain and D. Day. 1996. Finite-state parsing 
by rule sequences. In International Conference on 
Computational Linguistics, pages 274-279, Copen- 
hagen, Denmark, August. 
34 
               	 
        	       	   
        
     Word Alignment Baselines
John C. Henderson
The MITRE Corporation
202 Burlington Road
Bedford, Massachusetts, USA
jhndrsn@mitre.org
Abstract
Simple baselines provide insights into the value
of scoring functions and give starting points
for measuring the performance improvements
of technological advances. This paper presents
baseline unsupervised techniques for perform-
ing word alignment based on geometric and
word edit distances as well as supervised fu-
sion of the results of these techniques using the
nearest neighbor rule.
1 Introduction
Simple baselines provide insights into the value of scor-
ing functions and give starting points for measuring the
performance improvements of technological advances.
This paper presents baseline unsupervised techniques for
performing word alignment based on geometric and word
edit distances as well as supervised fusion of the results
of these techniques using the nearest neighbor rule.
2 Alignment as binary classification
One model for the task of aligning words in a left-
hand-side (LHS) segment with those in a right-hand-side
(RHS) segment is to consider each pair of tokens as a po-
tential alignment and build a binary classifier to discrimi-
nate between correctly and incorrectly aligned pairs. Any
of n source language words to align with any of m target
language words, resulting in 2nm possible alignment con-
figurations. This approach allows well-understood binary
classification tools to address the problem. However, the
assumption made in this approach is that the alignments
are independent and identically distributed (IID). This is
false, but the same assumption is made by the alignment
evaluation metrics. This approach also introduces diffi-
culty in incorporating knowledge of adjacency of aligned
pairs, and HMM approaches to word alignment show that
this knowledge is important (Och and Ney, 2000).
All of the techniques presented in this work approach
the problem as a binary classification task.
2.1 Random baseline
A randomized baseline was created which flips a coin to
mark alignments. The bias of the coin is chosen to maxi-
mize the F-measure on the trial dataset, and the resulting
performance gives insight into the inherent difficulty of
the task. If the categorization task was balanced, with ex-
actly half of the paired tokens being marked as aligned,
then the precision, recall, and F-measure of the coin with
the best bias would have all been 50%. The preponder-
ance of non-aligned tokens shifted the F-measure away
from 50%, to the 5-10% range, suggesting that only about
10% of the pairs were aligned. An aligner performing
worse than this baseline would perform better by invert-
ing its predictions.
3 Unsupervised methods
There are a number of alignment techniques that can be
used to align texts when one lacks the benefit of a large
aligned corpus. These unsupervised techniques take ad-
vantage of general knowledge of the language pair to be
aligned. Their relative simplicity and speed allow them
to be used in places where timeliness is of utmost impor-
tance, as well as to be quickly tuned on a small dataset.
3.1 Final punctuation
Many LHS segments end in a punctuation mark that is
aligned with the final punctuation of the corresponding
RHS. A high precision aligner that marks only that align-
ment is useful for debugging the larger alignment system.
3.2 Length ratios
Short words such as stop words tend to align with short
words and long words such as names tend to align with
long words. This weak hypothesis is worth pursuit be-
cause a similar hypothesis was useful for aligning sen-
Romanian-English English-French
Method P% R% F% AER% P% R% F% AER %
random 2.62 2.74 2.68 97.32 11.46 10.99 11.22 88.72
fpunct 100.00 2.92 5.67 94.33 100.00 2.07 4.06 80.27
len (eq. 1) 8.73 29.85 13.51 86.49 18.45 29.32 22.65 78.10
exact 53.55 14.24 22.49 77.51 82.56 3.98 7.59 67.45
wdiag (eq. 4) 23.50 57.89 33.45 66.55 38.56 38.85 38.70 58.27
wedit (eq. 2) 50.49 26.59 34.83 65.17 56.54 7.51 13.26 58.43
lcedit 50.32 26.93 35.08 64.92 56.20 7.62 13.43 58.10
cbox (eq. 7) 30.56 49.74 37.86 62.14 44.53 33.74 38.39 53.14
cdiag (eq. 6) 31.52 49.57 38.53 61.47 45.06 30.66 36.49 53.22
freqratio (eq. 8) 10.53 26.07 15.00 85.00 27.77 10.26 14.98 69.91
P (L|R) (eq. 9) 9.45 36.54 15.02 84.98 15.72 21.86 18.29 81.41
P (R|L) (eq. 10) 8.80 16.98 11.59 88.41 13.65 10.26 11.71 81.54
bos (eq. 11) 20.42 20.07 20.24 79.76 35.32 10.65 16.37 59.82
bnnrule 84.88 25.04 38.68 61.32 86.55 8.30 15.14 45.38
nnrule 65.89 63.29 64.57 35.43 35.89 35.43 35.66 58.50
Table 1: Trial set results.
tences (Gale and Church, 1991; Brown et al, 1991). The
observation can be codified as a distance between the
word at position i on the LHS and the word at position
j on the RHS
Dlen(i, j) = 1 ?
4 ? L(li) ? L(rj)
(L(li) + L(rj))2
(1)
where L(li) is the length of the token at position i on the
LHS. Note that Dlen is similar to a normalized harmonic
mean, ranging from 0 to 1.0, with the minimum achieved
when the lengths are the same. A threshold on Dlen is
used to turn this distance metric into a classification rule.
3.3 Edit distances
The language pairs in the experiments were drawn from
Western languages, filled with cognates and names. An
obvious way to start finding cognates in languages that
share character sets is by comparing the edit distance be-
tween words.
Three word edit distances were investigated, and
thresholds tuned to turn them into classification rules.
Dexact indicates exact match with a zero distance and a
mismatch with value of 1.0. Dwedit is the minimum num-
ber of character edits (insertions, deletions, substitutions)
required to transform one word into another, normalized
by the lengths. It can be interpreted as an edit distance
rate, edits per character:
Dwedit(i, j) =
edits(li, rj)
L(li) + L(rj)
(2)
Dlcedit is the same as Dwedit, except both arguments are
lower-cased prior to the edit distance calculation.
3.4 Dotplot geometry
Geometric approaches to bilingual alignment have been
used with great success in both finding anchor points
and aligning sentences (Fung and McKeown, 1994;
Melamed, 1996). Three distance metrics were created to
incorporate the knowledge that all of the aligned pairs use
roughly the same word order. In every case, the distance
of the pair of words from a diagonal in the dotplot was
used.
In the metrics below, the L1 norm distance from a point
(i, j) to a line from (0, 0) to (I, J) is
dL1(i, I, j, J) =
?
?
?
?
i
I ?
j
J
?
?
?
?
(3)
The first metric, Dwdiag, is a normalized distance of
the (i, j) pair of tokens to the diagonal on the word dot-
plot
Dwdiag(i, j) = dL1(i, Lw(l), j, Lw(r)) (4)
where Lw(l) is the length of the LHS in words.
The next two distances are character based, comparing
the box containing aligned characters from the words at
position (i, j) with the diagonal line on the character dot-
plot. Let Lc(li) be the number of characters preceding
the ith word in the LHS.
Let the left edge of the box be bl = Lc(li), the right
edge of the box be br = Lc(li+1), the bottom edge of
the box be bb = Lc(rj), and the top edge of the box be
bt = Lc(rj+1). The center of the box formed by the
words at (i, j) is
(ic, jc) =
(
bl + br
2 ,
bb + bt
2
)
(5)
Romanian-English English-French
Method P% R% F% AER% P% R% F% AER %
random 3.44 3.99 3.69 96.31 12.26 12.19 12.22 87.74
fpunct 93.95 3.76 7.23 92.77 99.55 2.55 4.98 80.33
len (eq. 1) 8.90 32.49 13.97 86.03 18.45 29.50 22.70 76.92
exact 44.55 13.84 21.12 78.88 81.92 5.33 10.00 64.19
wdiag (eq. 4) 21.98 60.00 32.17 67.83 39.27 42.62 40.88 56.40
wedit (eq. 2) 41.09 22.35 28.95 71.05 56.45 8.38 14.60 58.86
lcedit 43.02 21.18 28.39 71.61 56.07 8.53 14.81 58.59
cbox (eq. 7) 27.15 48.06 34.70 65.30 41.49 34.40 37.62 55.87
cdiag (eq. 6) 26.93 45.11 33.72 66.28 42.56 31.37 36.12 55.22
freqratio (eq. 8) 10.06 27.35 14.71 85.29 28.47 11.27 16.15 69.12
P (L|R) (eq. 9) 9.84 29.33 14.74 85.26 15.24 22.81 18.28 80.79
P (R|L) (eq. 10) 9.64 18.52 12.68 87.32 15.20 12.93 13.97 79.40
bos (eq. 11) 21.77 18.17 19.81 80.19 35.81 12.92 18.99 58.53
bnnrule 79.59 18.84 30.25 69.75 86.99 10.12 18.13 44.19
nnrule 51.67 42.03 46.35 53.65 35.43 35.12 35.27 57.93
Table 2: NON-OFFICIAL test set results (ignoring elements aligned with null).
One character metric is the distance from the center
of the character box to the diagonal line of the character
dotplot, where Lc(l) is the character length of the entire
LHS segment.
Dcdiag(i, j) = dL1(ic, Lc(l), jc, Lc(r)) (6)
The distance of the box to the diagonal line is the sec-
ond character metric
Dcbox =
?
?
?
0 if diagonal intersects box
min( dL1(bl, Lc(l), bt, Lc(r)), else
dL1(br, Lc(l), bb, Lc(r)))
(7)
4 Data-driven and supervised methods
The distance metrics and associated classifiers described
above were all optimized on the trial data, but they re-
quired optimization of at most one parameter, a threshold
on the distance. Four metrics were investigated that used
the larger dataset to estimate larger models, with param-
eters for every pair of collocated words in the training
dataset.
4.1 Likelihoods
Three likelihood-based distance metrics were investi-
gated, and the first is the relative likelihood of the aligned
pairs of words. c(li, LHS) is the number of times the
word li was seen in the LHS of the aligned corpus.
Dfreqratio(i, j) = 1 ?
min(c(li, LHS), c(rj , RHS))
max(c(li, LHS), c(rj , RHS))
(8)
The next two are conditional probabilities of seeing
one of the words given that the other word from the pair
was seen in an aligned sentence. Here RHSx means the
right-hand-side of aligned pair number x in the parallel
corpus.
P (L|R)(i, j) = P (li ? LHSx|rj ? RHSx) (9)
P (R|L)(i, j) = P (rj ? RHSx|li ? LHSx)(10)
Note that neither of these is satisfactory as a probabilistic
lexicon because they give stop words such as determiners
high probability for every conditioning token.
4.2 Bag-of-segments distance
The final data-driven measure that was investigated con-
siders the bag of segments (bos) in which the words ap-
pear. The result of the calculation is the Tanimoto dis-
tance between the bag of segments that word li appears
in and the bag of segments that word rj appears in.
Dbos(i, j) =
?
x |c(li, LHSx) ? c(rj , RHSx)|
?
x max(c(li, LHSx), c(rj , RHSx))(11)
5 Nearest neighbor rule
The nearest neighbor rule is a well-known classification
algorithm that provably converges to the Bayes Error
Rate of a classification task as dataset size grows (Duda
et al, 2001). The distance metrics described above were
used to train a nearest neighbor rule classifier, each metric
providing distance in one dimension. To provide compa-
rability of distances in the different dimensions, the dis-
tribution of points in each dimension was normalized to
have zero mean and unit variance (? = 0, ? = 1). The
L2 norm, Euclidean distance, was used to compute dis-
tance between points.
Two versions of the nearest neighbor rule were ex-
plored. In the first, the binary decisions of the classifiers
were used as features, and in the second the distances
provided by the classifiers were used as features.
6 Experiments
Two datasets of different language pairs were used to
evaluate these measures: Romanian-English and English-
French. The measures were optimized on a trial dataset
and then evaluated blind on a test set. The Romanian-
English trial data was 17 sentences long and the English-
French trial dataset was 37 sentences. Additionally,
approximately 1.1 million aligned English-French sen-
tences and 48,000 Romanian-English sentences were
used for the set of supervised experiments.
Four measures were used to evaluate the classifiers:
precision, recall, F-measure, and alignment error rate
(AER). Precision and recall are the ratios of matching
aligned pairs to the number of predicted pairs and the
number of reference pairs respectively. F-measure is the
harmonic mean of precision and recall. AER differenti-
ates between ?sure? and ?possible? aligned pairs in the
reference, requiring hypotheses to match those that are
?sure? and permitting them to match those that are ?pos-
sible?. (Och and Ney, 2000).
7 Results
Table 1 shows results of the explored methods on the
trial data, ordered by degree of supervision and AER on
the Romanian-English dataset. The biased coin random
aligner is indicated as random and the final punctuation
aligner is fpunct. The classifier based on relative length
is len. The three edit distance measures are exact match
(exact), edit distance (wedit), and lower-case edit dis-
tance (lcedit). The geometric measures are word distance
to the diagonal (wdiag), distance to the character diago-
nal, (cdiag), and distance from the character box made
by the word pair to the character diagonal, (cbox).
The aligners that take advantage of the training data
are below the first horizontal line inside the table. fre-
qratio is the classifier based on the relative frequency of
the two tokens, P (L|R) aligns words in the LHS with
words from the RHS that are often collocated in the train-
ing sentences, and the reverse for P (R|L). The bag-of-
documents distance classifier is evaluated in bos.
The two supervised fusion methods are presented in
the final two lines of the file: the binary nearest neigh-
bor rule based on the classification output of the align-
ers (bnnrule), and the nearest neighbor rule based on
the distances produced by the aligners (nnrule). Both
of these results are leave-one-out estimates of perfor-
mance from the trial set. Note that there is incomplete
dominance: the binary representation was superior for
English-French and the distance representation was su-
perior for Romanian-English.
Table 2 shows results of the explored methods on the
test data. The presented order is the same as the order
in Table 1. None of the results varied widely from ob-
servations on the trial dataset, suggesting that none of the
classifiers were drastically overtrained in the course of
optimization on the trial data.
8 Conclusion
Several baseline alignment systems were presented. The
individual scores of the different aligners give insight
into the relative contributions of the features they exploit.
Word length matching appears to be the least important
feature, followed by character edit distance (attempting to
match cognates), and geometric dotplot distances appear
to contribute most strongly to alignment performance.
The supervised probabilistic models perform poorly on
their own, probably because of the unconstrained way
in which they were trained and applied. When all fea-
tures are combined in concert into a larger alignment sys-
tem using the nearest neighbor rule, they perform better
than individual aligners, but the question remains of what
space should be used for modeling the points (distances
versus binary decisions).
References
Peter F. Brown, Jennifer C. Lai, and Robert L. Mercer.
1991. Aligning sentences in parallel corpora. In Pro-
ceedings of the Annual Meeting of the ACL, pages 169?
176.
R. O. Duda, P. E. Hart, and D. G. Stork. 2001. Pattern
Classification. John Wiley and Sons Inc.
Pascale Fung and Kathleen McKeown. 1994. Aligning
noisy parallel corpora across language groups: Word
pair feature matching by dynamic time warping. In
Proceedings of AMTA-94, pages 81?88, Columbia,
Maryland. Association for Machine Translation in the
Americas.
William A. Gale and Kenneth W. Church. 1991. A pro-
gram for aligning sentences in bilingual corpora. Com-
putational Linguistics, 19:75?102.
I. Dan Melamed. 1996. A geometric approach to map-
ping bitext correspondence. In Proceedings of the First
Conference on Empirical Methods in Natural Lan-
guage Processing, Philadelphia, PA, May.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Conference of the Association for Computa-
tional Linguistics., pages 440?447, Hong Kong.
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 175?182,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Gaming Fluency: Evaluating the Bounds and Expectations of
Segment-based Translation Memory
John Henderson and William Morgan
The MITRE Corporation
202 Burlington Road
Bedford, MA 01730
{jhndrsn,wmorgan}@mitre.org
Abstract
Translation memories provide assis-
tance to human translators in produc-
tion settings, and are sometimes used
as first-pass machine translation in as-
similation settings because they pro-
duce highly fluent output very rapidly.
In this paper, we describe and eval-
uate a simple whole-segment transla-
tion memory, placing it as a new lower
bound in the well-populated space of
machine translation systems. The re-
sult is a new way to gauge how far ma-
chine translation has progressed com-
pared to an easily understood baseline
system.
The evaluation also sheds light on the
evaluation metric and gives evidence
showing that gaming translation with
perfect fluency does not fool bleu the
way it fools people.
1 Introduction and background
Translation Memory (TM) systems provide
roughly concordanced results from an archive of
previously translated materials. They are typ-
ically used by translators who want computer
assistance for searching large archives for tricky
translations, and also to help ensure a group
of translators rapidly arrive at similar terminol-
ogy (Macklovitch et al, 2000). Several compa-
nies provide commercial TMs and systems for
using and sharing them. TMs can add value to
computer assisted translation services (Drugan,
2004).
Machine Translation (MT) developers make
use of similar historical archives (parallel texts,
bitexts), to produce systems that perform a task
very similar to TMs. But while TM systems
and MT systems can appear strikingly simi-
lar, (Marcu, 2001) key differences exist in how
they are used.
TMs often need to be fast because they are
typically used interactively. They aim to pro-
duce highly readable, fluent output, usable in
document production settings. In this setting,
errors of omission are more easily forgiven than
errors of commission so, just like MT, TM out-
put must look good to users who have no access
to the information in source texts.
MT, on the other hand, is often used in as-
similation settings, where a batch job can of-
ten be run on multiple processors. This permits
variable rate output and allows slower systems
that produce better translations to play a part.
Batch MT serving a single user only needs to run
at roughly the same rate the reader can consume
its output.
Simple TMs operate on an entire translation
segment, roughly the size of a sentence or two,
while more sophisticated TMs operate on units
of varying size: a word, a phrase, or an entire
segment (Callison-Burch et al, 2004). Mod-
ern approaches to MT, especially statistical MT,
typically operate on more fine-grained units,
words and phrases (Och and Ney, 2004). The re-
lationship between whole segment TM and MT
can be viewed as a continuum of translation
granularity:
175
-ff
segments
simple TM
words
MThybrid TM
Simple TM systems, focusing on segment-level
granularity, lie at one extreme, and word-
for-word, IBM-model MT systems on the
other. Example-Based MT (EBMT), phrase-
based, and commercial TM systems likely lie
somewhere in between.
This classification motivates our work here.
MT systems have well-studied and popular eval-
uation techniques such as bleu (Papineni et al,
2001). In this paper we lay out a methodology
for evaluating TMs along the lines of MT evalu-
ation. This allows us to measure the raw relative
value of TM and MT as translation tools, and to
develop expectations for how TM performance
increases as the size of the memory increases.
There are many ways to perform TM segmen-
tation and phrase extraction. In this study, we
use the most obvious and simple condition?a
full segment TM. This gives a lower bound on
real TM performance, but a lower bound which
is not trivial.
Section 2 details the architecture of our simple
TM. Section 3 describes experiments involving
different strategies for IR, oracle upper bounds
on TM performance as the memory grows, and
techniques for rescoring the retrievals. Section 4
discusses the results of the experiments.
2 A Simple Chinese-English
Translation Memory
For our experiments below, we constructed a
simple translation memory from a sentence-
aligned parallel corpus. The system consists of
three stages. A source-language input string is
rewritten to form an information retrieval (IR)
query. The IR engine is called to return a list
of candidate translation pairs. Finally a single
target-language translation as output is chosen.
2.1 Query rewriting
To retrieve a list of translation candidates from
the IR engine, we first create a query which is
a concatenation of all possible ngrams of the
source sentence, for all ngram sizes from 1 to
a fixed n.
We rely on the fact that the Chinese data
in the translation memory is tokenized and in-
dexed at the unigram level. Each Chinese char-
acter in the source sentence is tokenized indi-
vidually, and we make use of the IR engine?s
phrase query feature, which matches documents
in which all terms in the phrase appear in con-
secutive order, to create the ngrams. For exam-
ple, to produce a trigram + bigram + unigram
query for a Chinese sentence of 10 characters, we
would create a query consisting of eight three-
character phrases, nine two-character phrases,
and 10 single-character ?phrases?. All phrases
are weighted equally in the query.
This approach allows us to perform lookups
for arbitrary ngram sizes. Depending on the
specifics of how idf is calculated, this may yield
different results from indexing ngrams directly,
but it is advantageous in terms of space con-
sumed and scalability to different ngram sizes
without reindexing.
This is a slight generalization of the success-
ful approach to Chinese information retrieval us-
ing bigrams (Kwok, 1997). Unlike that work,
we perform no second stage IR after query ex-
pansion. Using a segmentation-independent en-
gineering approach to Chinese IR allows us to
sidestep the lack of a strong segmentation stan-
dard for our heterogeneous parallel corpus and
prepares us to rapidly move to other languages
with segmentation or lemmatization challenges.
2.2 The IR engine
Simply for performance reasons, an IR engine,
or some other sort of index, is needed to imple-
ment a TM (Brown, 2004). We use the open-
source Lucene v1.4.3, (Apa, 2004) as our IR en-
gine. Lucene scores candidate segments from
the parallel text using a modified tf-idf formula
that includes normalizations for the input seg-
ment length and the candidate segment length.
We did not modify any Lucene defaults for these
experiments.
To form our translation memory, we indexed
all sentence pairs in the translation memory cor-
pora, each pair as a separate document. We
176
Source
TM output
However , everything depended on the missions to be decided by the Security Council .
The presentations focused on the main lessons learned from their activities in the field .
It is wrong to commit suicide or to use ones own body as a weapon of destruction .
There was practically full employment in all sectors .
One reference translation (of four)
Doug Collins said, ?He may appear any time. It really depends on how he feels.?
At present, his training is defense oriented but he also practices shots.
He is elevating the intensity to test whether his body can adapt to it.
So far as his knee is concerned, he thinks it heals a hundred percent after the surgery.?
Table 1: Typical TM output. Excerpt from a story about athlete Michael Jordan.
indexed in such a way that IR searches can be
restricted to just the source language side or just
the target language side.
2.3 Rescoring
The IR engine returns a list of candidate trans-
lation pairs based on the query string, and the
final stage of the TM process is the selection of
a single target-language output sentence from
that set.
We consider a variety of selection metrics in
the experiments below. For each metric, the
source-language side of each pair in the candi-
date list is evaluated against the original source
language input string. The target language seg-
ment of the pair with the highest score is then
output as the translation.
In the case of automated MT evaluation met-
rics, which are not necessarily symmetric, the
source-language input string is treated as the
reference and the source-language side of each
pair returned by the IR engine as the hypothe-
sis.
All tie-breaking is done via tf-idf , i.e. if multi-
ple entries share the same score, the one ranked
higher by the search engine will be output.
Table 1 gives a typical example of how the TM
performs. Four contiguous source segments are
presented, followed by TM output and finally
one of the reference translations for those source
segments. The only indicator of the translation
quality available to monolingual English speak-
ers is the awkwardness of the segments as a
group. By design, the TM performs with perfect
fluency at the segment level.
3 Experiments
We performed several experiments in the course
of optimizing this TM, all using the same set
of parallel texts for the TM database and
multiple-reference translation corpus for eval-
utation. The parallel texts for the TM come
from several Chinese-English parallel corpora,
all available from the Linguistic Data Consor-
tium (LDC). These corpora are described in Ta-
ble 2. We discarded any sentence pairs that
seemed trivially incomplete, corrupt, or other-
wise invalid. In the case of LDC2002E18, in
which sentences were aligned automatically and
confidence scores produced for each alignment,
we dropped all pairs with scores above 9, indi-
cating poor alignment. No duplication checks
were performed. Our final corpus contained ap-
proximately 7 million sentence pairs and con-
tained 3.2 GB of UTF-8 data.
Our evaluation corpus and reference corpus
177
come from the data used in the NIST 2002 MT
competition. (NIST, 2002). The evaluation cor-
pus is 878 segments of Chinese source text. The
reference corpus consists of four independent
human-generated reference English translations
of the evaluation corpus.
All performance measurements were made us-
ing a fast reimplementation of NIST?s bleu.
bleu exhibits a high correlation with human
judgments of translation quality when measur-
ing on large sections of text (Papineni et al,
2001). Furthermore, using bleu allowed us to
compare our performance to that of other sys-
tems that have been tested with the same eval-
uation data.
3.1 An upper bound on whole-segment
translation memory
Our first experiment was to determine an upper
bound for the entire translation memory corpus.
In other words, given an oracle that picks the
best possible translation from the translation
memory corpus for each segment in the evalu-
ation corpus, what is the bleu score for the re-
sulting document? This score is unlikely to ap-
proach the maximum, bleu =100 because this
oracle is constrained to selecting a translation
from the target language side of the parallel cor-
pus. All of the calculations for this experiment
are performed on the target language side of the
parallel text.
We were able to take advantage of a trait
particular to bleu for this experiment, avoid-
ing many of bleu score calculations required
to assess all of the 878 ? 7.5 million combina-
tions. bleu produces a score of 0 for any hy-
pothesis string that doesn?t share at least one
4-gram with one reference string. Thus, for
each set of four references, we created a Lucene
query that returned all translation pairs which
matched at least one 4-gram with one of the ref-
erences. We picked the top segment by calcu-
lating bleu scores against the references, and
created a hypothesis document from these seg-
ments.
Note that, for document scores, bleu?s
brevity penalty (BP) is applied globally to an
entire document and not to individual segments.
Thus, the document score does not necessarily
increase monotonically with increases in scores
of individual segments. As more than 99% of
the segment pairs we evaluated yielded scores of
zero, we felt this would not have a significant
effect on our experiments. Also, the TM does
not have much liberty to alter the length of the
returned segments. Individual segments were
chosen to optimize bleu score, and the result-
ing documents exhibited appropriately increas-
ing scores. While there is no efficient strategy
for whole-document bleu maximization, an it-
erative rescoring of the entire document while
optimizing the choice of only one candidate seg-
ment at a time could potentially yield higher
scores than those we report here.
3.2 TM performance with varied
Ngram length
The second experiment was to determine the ef-
fect that different ngram sizes in the Chinese IR
query have on the IR engine?s ability to retrieve
good English translations.
We considered cumulative ngram sizes from 1
to 7, i.e. unigram, unigram + bigram, unigram
+ bigram + trigram, and so on. For each set
of ngram sizes, we created a Lucene query for
every segment of the (Chinese) evaluation cor-
pus. We then produced a hypothesis document
by combining the English sides of the top re-
sults returned by Lucene for each query. The
hypothesis document was evaluated against the
reference corpora by calculating a bleu score.
While it was observed that IR perfor-
mance is maximized by performing bigram
queries (Kwok, 1997), we had reason to believe
the TM would not be similar. TMs must at-
tempt to match short sequences of stop words
that indicate grammar as well as more tradi-
tional content words. Note that our system
performed neither stemming nor stop word (or
ngram) removal on the input Chinese strings.
3.3 An upper bound on TM N-best list
rescoring
The next experiment was to determine an upper
bound on the performance of tf-idf for differ-
ent result set sizes, i.e. for different (maximum)
178
LDC Id Description Pairs
LDC2002E18 Xinhua Chinese-English Parallel News Text v. 1.0 beta 2 64,371
LDC2002E58 Sinorama Chinese-English Parallel Text 103,216
LDC2003E25 Hong Kong News Parallel Text 641,308
LDC2004E09 Hong Kong Hansard Parallel Text 1,247,294
LDC2004E12 UN Chinese-English Parallel Text v. 2 4,979,798
LDC2000T47 Hong Kong Laws Parallel Text 302,945
Total 7,338,932
Table 2: Sentence-aligned parallel corpora used for the creation of the translation memory. The
?pairs? column gives the number of translation pairs available after trivial pruning.
numbers of translation pairs returned by the IR
engine. This experiment describes the trade-off
between more time spent in the IR engine cre-
ating a longer list of returns and the potential
increase in translation score.
To determine how much IR was ?enough? IR,
we performed an oracle experiment on different
IR query sizes. For each segment of the evalua-
tion corpus, we performed a cumulative 4-gram
query as described in Section 4.2. We produced
the n-best list oracle?s hypothesis document by
selecting the English translation from this result
set with the highest bleu score when evaluated
against the corresponding segment from the ref-
erence corpus. We then evaluated the hypoth-
esis documents against the reference corpus by
computing bleu scores.
3.4 N-best list rescoring with several
MT evaluation metrics
The fourth experiment was to determine
whether we could improve upon tf-idf by apply-
ing automated MT metrics to pick the best sen-
tence from the top n translation pairs returned
by the IR engine. We compared a variety of
metrics from MT evaluation literatures. All of
these were run on the tokens in the source lan-
guage side of the IR result, comparing against
the single pseudo-reference, the original source
language segment. While many of these metrics
aren?t designed to perform well with one refer-
ence, they stand in as good approximate string
matching algorithms.
The score that the IR engine associates with
each segment is retained and marked as tf-idf
in this experiment. Naturally, bleu (Papineni
et al, 2001) was the first choice metric, as it
was well-matched to the target language evalu-
ation function. rouge was a reimplementation
of ROUGE-L from (Lin and Och, 2004). It com-
putes an F-measure from precision and recall
that are both based on the longest common sub-
sequence of the hypothesis and reference strings.
wer-g is a variation on traditional word error
rate that was found to correlate very well with
human judgments (Foster et al, 2003), and per
is the traditional position-independent error rate
that was also shown to correlate well with hu-
man judgments (Leusch et al, 2003). Finally,
a random metric was added to show the bleu
value one could achieve by selecting from the top
n strictly by chance.
After the individual metrics are calculated
for these segments, a uniform-weight log-linear
combination of the metrics is calculated and
used to produce a new rank ordering under the
belief that the different metrics will make pre-
dictions that are constructive in aggregate.
4 Results
4.1 An upper bound for whole-sentence
TM
Figure 1 shows the maximum possible bleu
score that can an oracle can achieve by selecting
the best English-side segment from the parallel
text. The upper bound achieved here is a bleu
score of 17.7, and this number is higher than
the best performing system in the correspond-
ing NIST evaluation.
Note the log-linear growth in the resulting
179
 7
 8
 9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 10000  100000  1e+06  1e+07
O
ra
cle
 B
LE
U 
sc
or
e
Corpus size (segments)
Size bleu
73389 7.88
366947 10.82
733893 12.58
3669466 16.27
7338932 17.69
Figure 1: Oracle bounds on TM performance as
corpus size increases.
 2.5
 3
 3.5
 4
 4.5
 5
 5.5
 6
 1  2  3  4  5  6  7
BL
EU
 s
co
re
 o
f T
M
Max n-gram length
Ngrams in query bleu
1 2.72
1,2 4.73
1,2,3 5.68
1,2,3,4 5.87
1,2,3,4,5 5.80
1,2,3,4,5,6 5.52
1,2,3,4,5,6,7 5.48
Figure 2: bleu scores for different cumulative
ngram sizes, when retrieving only the first trans-
lation pair.
bleu score of the TM with increasing database
size. As the database is increased by a factor
of ten, the TM gains approximately 5 points of
bleu. While this trend has a natural limit at
20 orders of magnitude, it is unlikely that this
amount of text, let alne parallel text, will be a
indexed in the foreseeable future. This rate is
more useful in interpolation, giving an idea of
how much could be gained from adding to cor-
pora that are smaller than 7.5 million segments.
4.2 The effect of ngram size on Chinese
tf-idf retrieval
Figure 2 shows that our best performance is
realized when IR queries are composed of cu-
mulative 4-grams (i.e. unigrams + bigrams +
trigrams + 4-grams). As hypothesized, while
longer sequences are not important in document
retrieval in Chinese IR, they convey information
that is useful in segment retrieval in the trans-
lation memory. For the remainder of the ex-
periments, we restrict ourselves to cumulative
4-gram queries.
Note that the 4-gram result here (bleu of
5.87) provides the baseline system performance
measure as well as the value when the segments
are reranked according to tf-idf .
4.3 Upper bounds for tf-idf
Figure 3 gives the n-best list rescoring bounds.
The upper bound continues to increase up to
the top 1000 results. The plateau achieved af-
ter 1000 IR results suggests that is little to be
gained from further IR engine retrieval.
Note the log-linear growth in the bleu score
the oracle achieves as the n-best list extends on
the left side of the figure. As the list length
is increased by a factor of ten, the oracle up-
per bound on performance increases by roughly
3 points of bleu. Of course, for a system to
perform as well as the oracle does becomes pro-
gressively harder as the n-best list size increases.
Comparing this result with the experiment
in section 4.1 indicates that making the oracle
choose among Chinese source language IR re-
sults and limiting its view to the 1000 results
given by the IR engine incurs only a minor re-
duction of the oracle?s bleu score, from 17.7 to
180
16.3. This is one way to measure the impact
of crossing this particular language barrier and
using IR rather than exhaustive search.
 4
 6
 8
 10
 12
 14
 16
 18
 0.1  1  10  100  1000  10000  100000  1e+06  1e+07
O
ra
cle
 B
LE
U 
sc
or
e
N-best list size
Size bleu score
1 5.87
5 8.47
10 9.51
50 12.09
100 13.18
500 15.36
1000 16.29
7338932 17.69
Figure 3: bleu scores for different (maximum)
numbers of translation pairs returned by IR en-
gine, where the optimal segment is chosen from
the results by an oracle.
4.4 Using automated MT metrics to
pick the best TM sentence
Each metric was run on the top 1000 results
from the IR engine, on cumulative 4-gram
queries. Each metric was given the (Chinese)
evaluation corpus segment as the single refer-
ence, and scored the Chinese side of each of the
1000 resulting translation pairs against that ref-
erence. The hypothesis document for each met-
ric consisted of the English side of the transla-
tion pair with the best score for each segment.
These documents were scored with bleu against
the reference corpus. Ties (e.g. cases where a
metric gave all 1000 pairs the same score) were
broken with tf-idf .
Results of the rescoring experiment run on
Metric bleu
bleu 6.20
wer-g 5.90
rouge 5.88
tf-idf 5.87
per 5.72
random 3.32
log(tf-idf )
+log(bleu)
+log(rouge)
-log(wer-g)
-log(per) 6.56
Table 3: bleu scores for different metrics when
picking the best translation from 100 translation
pairs returned by the IR engine.
an n-best list of size 100 are given in Table 3.
Choosing from 1000 pairs did not give better
results. Choosing from only 10 gave worse re-
sults. The random baseline given in the table
represents the expected score from choosing ran-
domly among the top 100 IR returns. While the
scores of the individual metrics aside from per
and bleu reveal no differences, bleu and the
combination metric performed better than the
individual metrics.
Surprisingly, tf-idf was outperformed only by
bleu and the combination metric. While we
hoped to gain much more from n-best list rescor-
ing on this task, reaching toward the limits dis-
covered in section 4.3, the combination metric
was less than 0.5 bleu points below the lower
range of systems that were entered in the NIST
2002 evals. The bleu scores of research systems
in that competition roughly ranged between 7
and 15. Of course, each of the segments pro-
duced by the TM exhibit perfect fluency.
5 Discussion
The maximum bleu score attained by a TM we
describe (6.56) would place it in last place in the
NIST 2002 evals, but by less than 0.5 bleu. Suc-
cessive NIST competitions have exhibited im-
pressive system progress, but each year there
have been newcomers who score near (or in some
cases lower than) our simple TM baseline.
181
We have presented several experiments that
quantitatively describe how well a simple TM
performs when measured with a standard MT
evaluation measure, bleu. We showed that the
translation performance of a TM grows as a log-
linear function of corpus size below 7.5 million
segments. We showed, somewhat surprisingly,
only 1000 IR returns need be evaluated by a
rescorer to get within 1 bleu point of the max-
imum possible score attainable by the TM.
In future work, we expect to validate these
results with other language pairs. One question
is: how well does this simple IR query expansion
address segmented languages and languages that
allow more liberal word order? Supervised train-
ing of n-best reranking schemes would also de-
termine how far the oracle bound can be pushed.
The computationally more expensive reranking
procedure that attempts to optimize bleu on
the entire document should be investigated to
determine how much can be gained by better
global management of the brevity penalty.
Finally, we believe it?s worth noting the degree
to which high fluency of the TM output could
potentially mislead target-language-only readers
in their estimation of the system?s performance.
Table 1 is representative of system output, and
is a good example of why translations should not
be judged solely on the fluency of a few segments
of target language output.
References
Apache Software Foundation, 2004. Lucene 1.4.3
API. http://lucene.apache.org/java/docs/api/.
Ralf D. Brown. 2004. A modified burrows-
wheeler transform for highly-scalable example-
based translation. In Machine Translation: From
Real Users to Research, Proceedings of the 6th
Conference of the Association for Machine Trans-
lation (AMTA-2004), Washington, D.C., USA.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2004. Searchable translation memo-
ries. In Proceedings of ASLIB Translation and the
Computer 26.
Joanna Drugan. 2004. Multilingual document man-
agement and workflow in the european institu-
tions. In Proceedings of ASLIB Translation and
the Computer 26.
George Foster, Simona Gandrabur, Cyril Goutte,
Erin Fitzgerald, Alberto Sanchis, Nicola Ueffing,
John Blatz, and Alex Kulesza. 2003. Confidence
estimation for machine translation. Technical re-
port, JHU Center for Language and Speech Pro-
cessing.
K. L. Kwok. 1997. Comparing representations in
chinese information retrieval. In SIGIR ?97: Pro-
ceedings of the 20th annual international ACM SI-
GIR conference on Research and development in
information retrieval, pages 34?41, New York, NY,
USA. ACM Press.
G. Leusch, N. Ueffing, and H. Ney. 2003. A
novel string-to-string distance measure with ap-
plications to machine translation evaluation. In
Proc. of the Ninth MT Summit, pages 240?247.
Chin-Yew Lin and Franz Josef Och. 2004. Or-
ange: a method for evaluating automatic evalu-
ation metrics for machine translation. In Proceed-
ings of the 20th International Conference on Com-
putational Linguistics (COLING 2004), Geneva,
Switzerland, August.
E. Macklovitch, M. Simard, and Ph. Langlais. 2000.
Transsearch: A free translation memory on the
world wide web. In Second International Con-
ference On Language Resources and Evaluation
(LREC), volume 3, pages 1201?1208, Athens
Greece, jun.
Daniel Marcu. 2001. Towards a unified approach
to memory- and statistical-based machine trans-
lation. In ACL, pages 378?385.
NIST. 2002. The NIST 2002 machine trans-
lation evaluation plan (MT-02). NIST
web site. http://www.nist.gov/speech/
tests/mt/doc/2002-MT-EvalPlan-v1.3.pdf.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine
translation. Computational Linguistics, 30(4).
K. Papineni, S. Roukos, T. Ward, and W-J. Zhu.
2001. BLEU: a method for automatic evalua-
tion of machine translation. Technical Report
RC22176 (W0109-022), IBM Research Division,
Thomas J. Watson Research Center.
182
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1301?1309,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Discriminating Gender on Twitter
John D. Burger and John Henderson and George Kim and Guido Zarrella
The MITRE Corporation
202 Burlington Road
Bedford, Massachusetts, USA 01730
{john,jhndrsn,gkim,jzarrella}@mitre.org
Abstract
Accurate prediction of demographic attributes from
social media and other informal online content is
valuable for marketing, personalization, and legal in-
vestigation. This paper describes the construction of
a large, multilingual dataset labeled with gender, and
investigates statistical models for determining the
gender of uncharacterized Twitter users. We explore
several different classifier types on this dataset. We
show the degree to which classifier accuracy varies
based on tweet volumes as well as when various
kinds of profile metadata are included in the models.
We also perform a large-scale human assessment us-
ing Amazon Mechanical Turk. Our methods signifi-
cantly out-perform both baseline models and almost
all humans on the same task.
1 Introduction
The rapid growth of social media in recent years, exem-
plified by Facebook and Twitter, has led to a massive
volume of user-generated informal text. This in turn has
sparked a great deal of research interest in aspects of so-
cial media, including automatically identifying latent de-
mographic features of online users. Many latent features
have been explored, but gender and age have generated
great interest (Schler et al, 2006; Burger and Henderson,
2006; Argamon et al, 2007; Mukherjee and Liu, 2010;
Rao et al, 2010). Accurate prediction of these features
would be useful for marketing and personalization con-
cerns, as well as for legal investigation.
In this work, we investigate the development of high-
performance classifiers for identifying the gender of
Twitter users. We cast gender identification as the ob-
vious binary classification problem, and explore the use
of a number of text-based features. In Section 2, we de-
scribe our Twitter corpus, and our methods for labeling
a large subset of this data for gender. In Section 3 we
discuss the features that are used in our classifiers. We
describe our Experiments in Section 4, including our ex-
ploration of several different classifier types. In Section 5
we present and analyze performance results, and discuss
some directions for acquiring additional data by simple
self-training techniques. Finally in Section 6 we summa-
rize our findings, and describe extensions to the work that
we are currently exploring.
2 Data
Twitter is a social networking and micro-blogging plat-
form whose users publish short messages or tweets. In
late 2010, it was estimated that Twitter had 175 million
registered users worldwide, producing 65 million tweets
per day (Miller, 2010). Twitter is an attractive venue
for research into social media because of its large vol-
ume, diverse and multilingual population, and the gener-
ous nature of its Terms of Service. This has led many re-
searchers to build corpora of Twitter data (Petrovic et al,
2010; Eisenstein et al, 2010). In April 2009, we began
sampling data from Twitter using their API at a rate of
approximately 400,000 tweets per day. This represented
approximately 2% of Twitter?s daily volume at the time,
but this fraction has steadily decreased to less than 1% by
2011. This decrease is because we sample roughly the
same number of tweets every day while Twitter?s overall
volume has increased markedly. Our corpus thus far con-
tains approximately 213 million tweets from 18.5 million
users, in many different languages.
In addition to the tweets that they produce, each Twitter
user has a profile with the following free-text fields:
? Screen name (e.g., jsmith92, kingofpittsburgh)
? Full name (e.g., John Smith, King of Pittsburgh)
? Location (e.g., Earth, Paris)
? URL (e.g., the user?s web site, Facebook page, etc.)
? Description (e.g., Retired accountant and grandfa-
ther)
All of these except screen name are completely op-
tional, and all may be changed at any time. Note that none
1301
Users Tweets
Training 146,925 3,280,532
Development 18,380 403,830
Test 18,424 418,072
Figure 1: Dataset Sizes
of the demographic attributes we might be interested in
are present, such as gender or age. Thus, the existing
profile elements are not directly useful when we wish to
apply supervised learning approaches to classify tweets
for these target attributes. Other researchers have solved
this problem by using labor-intensive methods. For ex-
ample, Rao et al (2010) use a focused search methodol-
ogy followed by manual annotation to produce a dataset
of 500 English users labeled with gender. It is infeasible
to build a large multilingual dataset in this way, however.
Previous research into gender variation in online dis-
course (Herring et al, 2004; Huffaker, 2004) has found
it convenient to examine blogs, in part because blog sites
often have rich profile pages, with explicit entries for gen-
der and other attributes of interest. Many Twitter users
use the URL field in their profile to link to another facet
of their online presence. A significant number of users
link to blogging websites, and many of these have well-
structured profile pages indicating our target attributes. In
many cases, these are not free text fields. Users on these
sites must select gender and other attributes from drop-
down menus in order to populate their profile informa-
tion. Accordingly, we automatically followed the Twitter
URL links to several of the most represented blog sites
in our dataset, and sampled the corresponding profiles.
By attributing this blogger profile information to the as-
sociated Twitter account, we created a corpus of approx-
imately 184,000 Twitter users labeled with gender.
We partitioned our dataset by user into three distinct
subsets, training, development, and test, with sizes as in-
dicated in Figure 1. That is, all the tweets from each user
are in a single one of the three subsets. This is the corpus
we use in the remainder of this paper.
This method of gleaning supervised labels for our
Twitter data is only useful if the blog profiles are in turn
accurate. We conducted a small-scale quality assurance
study of these labels. We randomly selected 1000 Twitter
users from our training set and manually examined the
description field for obvious indicators of gender, e.g.,
mother to 3 boys or just a dude. Only 150 descriptions
(15% of the sample) had such an explicit gender cue. 136
of these also had a blog profile with the gender selected,
and in all of these the gender cue from the user?s Twit-
ter description agreed with the corresponding blog pro-
file. This may only indicate that people who misrepresent
their gender are simply consistent across different aspects
of their online presence. However, the effort involved in
maintaining this deception in two different places sug-
gests that the blog labels on the Twitter data are largely
reliable.
Initial analysis using the blog-derived labels showed
that our corpus is composed of 55% females and 45%
males. This is consistent with the results of an earlier
study which used name/gender correlations to estimate
that Twitter is 55% female (Heil and Piskorski, 2009).
Figure 2 shows several statistics broken down by gender,
including the Twitter users who did not indicate their gen-
der on their blog profile. In our dataset females tweet at a
higher rate than males and in general users who provide
their gender on their blog profile produce more tweets
than users who do not. Additionally, of the 150 users
who provided a gender cue in their Twitter user descrip-
tion, 105 were female (70%). Thus, females appear more
likely to provide explicit indicators about their gender in
our corpus.
The average number of tweets per user is 22 and is
fairly consistent across our traing/dev/test splits. There
is wide variance, however, with some users represented
by only a single tweet, while the most prolific user in our
sample has nearly 4000 tweets.
It is worth noting that many Twitter users do not tweet
in English. Table 3 presents an estimated breakdown of
language use in our dataset. We ran automatic language
ID on the concatenated tweet texts of each user in the
training set. The strong preponderance of English in our
dataset departs somewhat from recent studies of Twitter
language use (Wauters, 2010). This is likely due in part to
sampling methodology differences between the two stud-
ies. The subset of Twitter users who also use a blog site
may be different from the Twitter population as a whole,
and may also be different from the users tweeting during
the three days of Wauters?s study. There are also possible
longitudinal differences: English was the dominant lan-
guage on Twitter when the online service began in 2006,
and this was still the case when we began sampling tweets
in 2009, but the proportion of English tweets had steadily
dropped to about 50% in late 2010. Note that we do not
use any explicit encoding of language information in any
of the experiments described below.
Our Twitter-blog dataset may not be entirely represen-
tative of the Twitter population at general, but this has
at least one advantage. As with any part of the Inter-
net, spam is endemic to Twitter. However by sampling
only Twitter users with blogs we have largely filtered out
spammers from our dataset. Informal inspection of a few
thousand tweets revealed a negligible number of commer-
cial tweets.
3 Features
Tweets are tagged with many sources of potentially dis-
criminative metadata, including timestamps, user color
1302
Users Tweets Mean tweets
Count Percentage Count Percentage per user
Female 100,654 42.3% 2,429,621 47.7% 24.1
Male 83,075 35.0 1,672,813 32.8 20.1
Not provided 53,817 22.7 993,671 19.5 18.5
Figure 2: Gender distribution in our blog-Twitter dataset
Language Users Percentage
English 98,004 66.7%
Portuguese 21,103 14.4
Spanish 8,784 6.0
Indonesian 6,490 4.4
Malay 1,401 1.0
German 1,220 0.8
Chinese 985 0.7
Japanese 962 0.7
French 878 0.6
Dutch 761 0.5
Swedish 686 0.5
Filipino 643 0.4
Italian 631 0.4
Other 4,377 3.0
Figure 3: Language ID statistics from training set
preferences, icons, and images. We have restricted our
experiments to a subset of the textual sources of features
as listed in Figure 4.
We use the content of the tweet text as well as three
fields from the Twitter user profile described in Section 2:
full name, screen name, and description. For each user in
our dataset, a field is in general a set of text strings. This
is obviously true for tweet texts but is also the case for
the profile-based fields since a Twitter user may change
any part of their profile at any time. Because our sam-
ple spans points in time where users have changed their
screen name, full name or description, we include all of
the different values for those fields as a set. In addition,
a user may leave their description and full name blank,
which corresponds to the empty set.
In general, our features are quite simple. Both word-
and character-level ngrams from each of the four fields
are included, with and without case-folding. Our fea-
ture functions do not count multiple occurrences of the
same ngram. Initial experiments with count-valued fea-
ture functions showed no appreciable difference in per-
formance. Each feature is a simple Boolean indicator
representing presence or absence of the word or character
ngram in the set of text strings associated with the partic-
ular field. The extracted set of such features represents
the item to the classifier.
For word ngrams, we perform a simple tokenization
Feature extraction
Char
ngrams
Word
ngrams
Distinct
features
Screen name 1?5 none 432,606
Full name 1?5 1 432,820
Description 1?5 1?2 1,299,556
Tweets 1?5 1?2 13,407,571
Total 15,572,522
Figure 4: Feature types and counts
that separates words at transitions between alphanumeric
characters and non-alphanumeric.1 We make no attempt
to tokenize unsegmented languages such as Chinese, nor
do we perform morphological analysis on language such
as Korean; we do no language-specific processing at all.
We expect the character-level ngrams to extract useful in-
formation in the case of such languages.
Figure 4 indicates the details and feature counts for the
fields from our training data. We ignore all features ex-
hibited by fewer than three users.
4 Experiments
We formulate gender labeling as the obvious binary clas-
sification problem. The sheer volume of data presents
a challenge for many of the available machine learning
toolkits, e.g. WEKA (Hall et al, 2009) orMALLET (Mc-
Callum, 2002). Our 4.1 million tweet training corpus
contains 15.6 million distinct features, with feature vec-
tors for some experiments requiring over 20 gigabytes
of storage. To speed experimentation and reduce the
memory footprint, we perform a one-time feature genera-
tion preprocessing step in which we convert each feature
pattern (such as ?caseful screen name character trigram:
Joh?) to an integer codeword. The learning algorithms
do not access the codebook at any time and instead deal
solely with vectors of integers. We compress the data fur-
ther by concatenating all of a user?s features into a single
vector that represents the union of every tweet produced
by that user. This condenses the dataset to about 180,000
vectors occupying 11 gigabytes of storage.
We performed initial feasibility experiments using a
wide variety of different classifier types, including Sup-
port Vector Machines, Naive Bayes, and Balanced Win-
1We use the standard regular expression pattern \b.
1303
now2 (Littlestone, 1988). These initial experiments were
based only on caseful word unigram features from tweet
texts, which represent less than 3% of the total feature
space but still include large numbers of irrelevant fea-
tures. Performance as measured on the development set
ranged from Naive Bayes at 67.0% accuracy to Balanced
Winnow2 at 74.0% accuracy. A LIBSVM (Chang and
Lin, 2001) implementation of SVM with a linear ker-
nel achieved 71.8% accuracy, but required over fifteen
hours of training time while Winnow needed less than
seven minutes. No classifier that we evaluated was able
to match Winnow?s combination of accuracy, speed, and
robustness to increasing amounts of irrelevant features.
We built our own implementation of the BalancedWin-
now2 algorithm which allowed us to iterate repeatedly
over the training data on disk rather than caching the en-
tire dataset in memory. This reduced our memory re-
quirements to the point that we were able to train on the
entire dataset using a single machine with 8 gigabytes of
RAM.
We performed a grid search to select learning parame-
ters by measuring their affect on Winnow?s performance
on the development set. We found that two sets of pa-
rameters were required: a low learning rate (0.03) was
effective when using only one type of input feature (such
as only screen name features, or only tweet text features),
and a higher learning rate (0.20) was required when mix-
ing multiple types of features in one classifier. In both
cases we used a relatively large margin (35%) and cooled
the learning rate by 50% after each iteration.
These learning parameters were used during all of the
experiments that follow. All gender prediction models
were trained using data from the training set and evalu-
ated on data from the development set. The test set was
held out entirely until we finalized our best performing
models.
4.1 Field combinations
We performed a number of experiments with the Winnow
algorithm described above. We trained it on the train-
ing set and evaluated on the development set for each of
the four user fields in isolation, as well as various com-
binations, in order to simulate different use cases for sys-
tems that perform gender prediction from social media
sources. In some cases we may have all of the metadata
fields available above, while in other cases we may only
have a sample of a user?s tweet content or perhaps just
one tweet. We simulated the latter condition by randomly
selecting a single tweet for each dev and test user; this
tweet was used for all evaluations of that user under the
single-tweet condition. Note, however, that for training
the single tweet classifier, we do not concatenate all of a
user?s tweets as described above. Instead, we pair each
user in the training set with each of their tweets in turn,
in order to take advantage of all the training data. This
amounted to over 3 million training instances for the sin-
gle tweet condition.
We paid special attention to three conditions: single
tweet, all fields, and all tweets. For these conditions, we
evaluated the learned models on the training data, the de-
velopment set, and the test set, to study over-training and
generalization. Note that for all experiments, the evalua-
tion includes some users who have left their full name or
description fields blank in their profile.
In all cases, we compare results to a maximum likeli-
hood baseline that simply labels all users female.
4.2 Human performance
We wished to compare our classifier?s efficacy to human
performance on the same task. A number of researchers
have recently experimented with the use of Amazon Me-
chanical Turk (AMT) to create and evaluate human lan-
guage data (Callison-Burch and Dredze, 2010). AMT
and other crowd-sourcing platforms allow simple tasks to
be posted online for large numbers of anonymous work-
ers to complete.
We used AMT to measure human performance on gen-
der determination for the all tweets condition. Each AMT
worker was presented with all of the tweet texts from
a single Twitter user in our development set and asked
whether the author was male or female. We redundantly
assigned five workers to each Twitter user, for a total of
91,900 responses from 794 different workers. We experi-
mented with a number of ways to combine the five human
labels for each item, including a simple majority vote and
a more sophisticated scheme using an expectation maxi-
mization algorithm.
4.3 Self-training
Our final experiments were focused on exploring the use
of unlabeled data, of which we have a great deal. We
performed some initial experiments on a self-training ap-
proach to labeling more data. We trained the all-fields
classifier on half of our training data, and applied it to the
other half. We trained a new classifier on this full train-
ing set, which now included label errors introduced by the
limitations of the first classifier. This provided a simula-
tion of a self-training setup using half the training data.
Any robust gains due to self-training should be revealed
by this setup.
5 Results
5.1 Field combinations
Figure 5 shows development set performance on various
combinations of the user fields, all of which outperform
the maximum likelihood baseline that classifies all users
as female. The single most informative field with respect
1304
Baseline (F) 54.9%
One tweet text 67.8
Description 71.2
All tweet texts 75.5
Screen name (e.g. jsmith92) 77.1
Full name (e.g. John Smith) 89.1
Tweet texts + screen name 81.4
Tweet texts + screen name + description 84.3
All four fields 92.0
Figure 5: Development set accuracy using various fields
Condition Train Dev Test
Baseline (F) 54.8% 54.9 54.3
One tweet text 77.8 67.8 66.5
Tweet texts 77.9 75.5 74.5
All fields 98.6 92.0 91.8
Figure 6: Accuracy on the training, development and test sets
to gender is the user?s full name, which provides an accu-
racy of 89.1%. Screen name is often a derivative of full
name, and it too is informative (77.1%), as is the user?s
self-assigned description (71.2).
Using only tweet texts performs better than using only
the user description (75.5% vs. 71.2). Tweet texts are
sufficient to decrease the error by nearly half over the
all-female prior. It appears that the tweet texts con-
vey more about a Twitter user?s gender than their own
self-descriptions. Even a single (randomly selected)
tweet text contains some gender-indicative information
(67.2%). These results are similar to previous work. Rao
et al (2010) report results of 68.7% accuracy on gender
from tweet texts alone using an ngram-only model, ris-
ing to 72.3 with hand-crafted ?sociolinguistic-based? fea-
tures. Test set differences aside, this is comparable with
the ?All tweet texts? line in Figure 5, where we achieve
an accuracy of 75.5%.
Performance of models built from various aggregates
of the four basic fields are shown in Figure 5 as well. The
combination of tweet texts and a screen name represents
a use case common to many different social media sites,
such as chat rooms and news article comment streams.
The performance of this combination (81.4%) is signif-
icantly higher than either of the individual components.
As we have observed, full name is the single most infor-
mative field. It out-performs the combination of the other
three fields, which perform at 84.3%. Finally, the classi-
fier that has access to features from all four fields is able
to achieve an accuracy of 92.0%.
The final test set accuracy is shown in Figure 6. This
test set was held out entirely during development and has
been evaluated only with the four final models reported
Rank MI Feature f P (Female|f)
1 0.0170 ! 0.601
2 0.0164 : 0.656
3 0.0163 lov 0.687
4 0.0162 love 0.680
5 0.0161 lov 0.676
6 0.0160 love 0.689
7 0.0160 ! 0.618
8 0.0149 :) 0.697
9 0.0148 y! 0.687
10 0.0145 my 0.637
11 0.0143 love 0.691
12 0.0143 haha 0.705
13 0.0141 my 0.634
14 0.0140 my 0.637
15 0.0140 :) 0.697
16 0.0139 my 0.634
17 0.0138 ! i 0.711
18 0.0138 hah 0.698
19 0.0137 hah 0.714
20 0.0135 so 0.661
21 0.0134 haha 0.714
22 0.0132 so 0.661
23 0.0128 i 0.618
24 0.0127 ooo 0.708
25 0.0126 ! i 0.743
26 0.0123 i lov 0.728
27 0.0120 ove 0.671
28 0.0117 ay! 0.718
29 0.0116 aha 0.678
30 0.0116 <3 0.856
31 0.0115 cute 0.826
32 0.0114 i lo 0.704
33 0.0114 :)$ 0.701
34 0.0110 :( 0.731
35 0.0109 :)$ 0.701
36 0.0109 !$ 0.614
37 0.0107 ahah 0.716
38 0.0106 <3 0.857
464 0.0051 ht | 0.506
465 0.0051 hank 0.641
466 0.0051 too 0.659
467 0.0051 yay! 0.818
468 0.0051 http | 0.506
469 0.0051 htt | 0.506
624 0.0047 Googl | 0.317
625 0.0047 ing! 0.718
626 0.0047 hair 0.749
627 0.0047 b 0.573
628 0.0047 y : 0.725
629 0.0046 Goog | 0.318
Figure 7: A selection of tweet text features, ranked by mutual
information. Character ngrams in Courier, words in bold.
Underscores are spaces, $ matches the end of the tweet text.
| marks ?male? features.1305
in this figure. The difference between the scores on the
train and development sets show how well the model can
fit the data. There are features in the user name and user
screen name fields that make the data trivially separable.
The tweet texts, however, present more ambiguity for the
learners. The difference between the development and
test set scores suggest that only minimal hill-climbing oc-
curred during our development.
We have performed experiments to better understand
how performance scales with training data size. Figure 8
shows how performance increases for both the all-fields
and tweet-texts-only classifiers as we train on more users,
with little indication of leveling off.
As discussed in Section 2, there is wide variance in
the number of tweets available from different users. In
Figure 9 we show how the tweet text classifier?s accu-
racy increases as the number of tweets from the user in-
creases. Each point is the average classifier accuracy for
the user cohort with exactly that many tweets in our dev
set. Performance increases given more tweets, although
the averages get noisy for the larger tweet sets, due to
successively smaller cohort sizes.
Some of the most informative features from tweet texts
are shown in Figure 7, ordered by mutual information
with gender. There are far more of these strong features
for the female category than the male: only five of the top
1000 features are associated more strongly with males,
i.e. they have lower P (Female|feature) than the prior,
P (Female) = 0.55.
Some of these features are content-based (hair, and
several fragments of love), while others are stylistic (ooo,
several emoticons). The presence of http as a strong
male feature might be taken to indicate that men include
links in their tweet texts far more often than women,
but a cursory examination seems to show instead that
women are simply more likely to include ?bare? links,
e.g., emnlp.org vs. http://emnlp.org.
5.2 Human performance
Figure 10 shows the results of the human performance
benchmarks using Amazon Mechanical Turk. The raw
per-response performance is 60.4%, only moderately bet-
ter than the all-female baseline. When averaged across
workers, however, this improves substantially, to 68.7.
This would seem to indicate that there were a few poor
workers who did many annotations, and in fact when we
limit the performance average to those workers who pro-
duced 100 or more responses, we do see a degradation to
62.2.
The problem of poor quality workers is endemic to
anonymous crowd sourcing platforms like Mechanical
Turk. A common way to combat this is to use redun-
dancy, with a simple majority vote to choose among mul-
tiple responses for each item. This allows us to treat the
Baseline 54.9
Average response 60.4
Average worker 68.7
Average worker (100 or more responses) 62.2
Worker ensemble, majority vote 65.7
Worker ensemble, EM-adjusted vote 67.3
Winnow all-tweet-texts classifier 75.5
Figure 10: Comparing with humans on the all tweet texts task
five workers who responded to each item as an ensem-
ble. As Figure 10 indicates, this provides some improve-
ment over the raw result (65.7% vs. 60.4). A different
approach, first proposed by Dawid and Skene (1979), is
to use an expectation maximization algorithm to estimate
the quality of each source of labels, as well as estimate the
posterior for each item. In this case, the first is an AMT
worker?s capability and the second is the distribution of
gender labels for each Twitter user.
The Dawid and Skene approach has previously been
applied to Mechanical Turk responses (Ipeirotis et al,
2010). We used their implementation on our AMT re-
sults but with only moderate improvement over the sim-
ple majority ensemble (67.3% vs. 65.7). All of the aggre-
gate human results are substantially below the all-tweet-
texts classifier score, suggesting that this is a difficult
task for people to perform. As Figure 11 indicates, most
workers perform below 80% accuracy, and less than 5%
of the prolific workers out-perform the automatic classi-
fier. These high-scoring workers may indeed be good at
the task, or they may have simply been assigned a less-
difficult subset of the data. Figure 12 illustrates this by
showing aligned worker performance and classifier per-
formance on the precise set of items that each worker
performed on. Here we see that, with few exceptions,
the automatic classifier performs as well or better than
the AMT workers on their subset.
5.3 Self-training
Finally, as described in Section 4.3, we performed some
initial experiments on a self-training approach to label-
ing more data. As described above the all-fields classi-
fier achieves an accuracy of 92% on the development set
when trained on the full training set. Training on half of
the training data results in a drop to 91.1%. The sec-
ond classifier trained on the full training set, but with
some label errors introduced by the first, had further de-
graded performance of 90.9%. Apparently the errorful la-
bels introduced by the simplistic self-training procedure
overwhelmed any new information that might have been
gained from the additional data. We are continuing to ex-
plore ways to use the large amounts of unsupervised data
in our corpus.
1306
Figure 8: Performance increases when training with more users
Figure 9: Performance increases with more tweets from target user
1307
Figure 11: Human accuracy in rank order (100 responses or more), with classifier performance (line)
Figure 12: Classifier vs. human accuracy on the same subsets (100 responses or more)
1308
6 Conclusion
In this paper, we have presented several configurations of
a language-independent classifier for predicting the gen-
der of Twitter users. The large dataset used for construc-
tion and evaluation of these classifiers was drawn from
Twitter users who also completed blog profile pages.
These classifiers were tested on the largest set of
gender-tagged tweets to date that we are aware of. The
best classifier performed at 92% accuracy, and the clas-
sifier relying only on tweet texts performed at 76% ac-
curacy. Human performance was assessed on this latter
condition, and only 5% of 130 humans performed 100 or
more classifications with higher accuracy than this ma-
chine.
In future work, we will explore how well such models
carry over to gender identification in other informal on-
line genres such as chat and forum comments. Further-
more, we have been able to assign demographic features
beside gender, including age and location, to our Twit-
ter dataset. We have begun to build classifiers for these
features as well.
Acknowledgements
The authors would like to thank the anonymous review-
ers. This work was funded under the MITRE Innovation
Program.
References
Shlomo Argamon, Moshe Koppel, James W. Pennebaker, and
Jonathan Schler. 2007. Mining the blogosphere: Age,
gender, and the varieties of self-expression. First Monday,
12(9), September.
John D. Burger and John C. Henderson. 2006. An exploration
of observable features related to blogger age. In Computa-
tional Approaches to Analyzing Weblogs: Papers from the
2006 AAAI Spring Symposium. AAAI Press.
Chris Callison-Burch and Mark Dredze. 2010. Creating speech
and language data with Amazon?s Mechanical Turk. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechanical Turk,
CSLDAMT ?10. Association for Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library
for support vector machines. Software available at http:
//www.csie.ntu.edu.tw/?cjlin/libsvm.
A.P. Dawid and A.M. Skene. 1979. Maximum likelihood esti-
mation of observer error-rates using the EM algorithm. Jour-
nal of the Royal Statistical Society. Series C (Applied Statis-
tics), 28(1).
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith, and
Eric P. Xing. 2010. A latent variable model for geographic
lexical variation. In Conference on Empirical Methods on
Natural Language Processing.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer,
Peter Reutemann, and Ian H. Witten. 2009. The WEKA data
mining software: An update. SIGKDD Explorations, 11(1).
Bill Heil and Mikolaj Jan Piskorski. 2009. New Twitter re-
search: Men follow men and nobody tweets. Harvard Busi-
ness Review, June 1.
Susan C. Herring, Inna Kouper, Lois Ann Scheidt, and Eli-
jah L. Wright. 2004. Women and children last: The discur-
sive construction of weblogs. In L. Gurak, S. Antonijevic,
L. Johnson, C. Ratliff, and J. Reyman, editors, Into the Bl-
ogosphere: Rhetoric, Community, and Culture of Weblogs.
http://blog.lib.umn.edu/blogosphere/.
David Huffaker. 2004. Gender similarities and differences in
online identity and language use among teenage bloggers.
Master?s thesis, Georgetown University. http://cct.
georgetown.edu/thesis/DavidHuffaker.pdf.
Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang. 2010.
Quality management on Amazon Mechanical Turk. In
Proceedings of the Second Human Computation Workshop
(KDD-HCOMP 2010).
Nick Littlestone. 1988. Learning quickly when irrelevant at-
tributes abound: A new linear-threshold algorithm. Machine
Learning, 2, April.
Andrew Kachites McCallum. 2002. MALLET: A machine
learning for language toolkit. http://mallet.cs.
umass.edu.
Claire Cain Miller. 2010. Why Twitter?s C.E.O.
demoted himself. New York Times, October 30.
http://www.nytimes.com/2010/10/31/
technology/31ev.html.
Arjun Mukherjee and Bing Liu. 2010. Improving gender clas-
sification of blog authors. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language Process-
ing, Cambridge, MA, October. Association for Computa-
tional Linguistics.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko. 2010. The
Edinburgh Twitter corpus. In Computational Linguistics in a
World of Social Media. AAAI Press. Workshop at NAACL.
Delip Rao, David Yarowsky, Abhishek Shreevats, and Manaswi
Gupta. 2010. Classifying latent user attributes in Twitter.
In 2nd International Workshop on Search and Mining User-
Generated Content. ACM.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and James
Pennebaker. 2006. Effects of age and gender on blogging.
In Computational Approaches to Analyzing Weblogs: Papers
from the 2006 AAAI Spring Symposium. AAAI Press, March.
Robin Wauters. 2010. Only 50% of Twitter mes-
sages are in English, study says. TechCrunch, Febru-
ary 1. http://techcrunch.com/2010/02/24/
twitter-languages/.
1309
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 9?12,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Navigating Large Comment Threads With CoFi
Christine Doran, Guido Zarrella and John C. Henderson The MITRE Corporation Bedford, MA {cdoran,jzarrella,jhndrsn}@mitre.org  
Abstract 
Comment threads contain fascinating and use-ful insights into public reactions, but are chal-lenging to read and understand without computational assistance. We present a tool for exploring large, community-created com-ments threads in an efficient manner. 1 Introduction The comments made on blog posts and news arti-cles provide both immediate and ongoing public reaction to the content of the post or article. When a given site allows users to respond to each other (?threaded? responses), the comment sets become a genuine public conversation. However, this in-formation can be difficult to access. Comments are typically not indexed by search engines, the vol-ume is often enormous, and threads may continue to be added to over months or even years. This makes it hard to find particular information of in-terest (say, a mention of a particular company in a set of thousands of YouTube comments), or to un-derstand the gist of the discussion at a high-level. Our goal in this work was to create a simple tool which would allow people to rapidly ingest useful information contained in large community-created comment threads, where the volume of data precludes manual inspection. To this end, we created CoFi (Comment Filter), a language-independent, web-based interactive browser for single comment threads. 2 How CoFi works For a given set of comments, we create a distinct CoFi instance. Each instance is over a natural data set, e.g. all comments from a particular discussion group, comments attached to an individual news article, or tweets resulting from a topical search. Creating a CoFi instance has three steps: harvest-
ing the comments, clustering the comments, and responding to user interactions while they visualize and navigate (sorting and filtering) the dataset. 2.1 Harvesting the data Our comments are harvested from individual web sites. These need not be in English, or even in a single language. Typically, sites use proprietary javascript to present comments. Each web site has a unique interface and formatting to serve the comments to web browsers, and there is no general purpose tool to gather comments everywhere. The CoFi approach has been to factor this part of the problem into one harvesting engine per web site. Some sites provide an API that simplifies the prob-lem of harvesting comments that contain particular keywords. On other sites, there seems to be no re-liable alternative to developer ingenuity when it comes to altering the harvesting engines to ac-commodate data formats. Thus, we note that the harvesting activity is only semi-automated. 2.2 Clustering the data Once harvesting is complete, the rest of the process is automatic. Clusters are generated and labeled using a pipeline of machine learning tools. The open source package MALLET provides many of our document ingestion and clustering components (McCallum, 2002). Our processing components are language-independent and can be used with non-English or mixed language data sets. Specifically, we use a combination of Latent Dirichlet Allocation (LDA), K-Means clustering, and calculation of mutual information. LDA mod-els each document (aka comment) as a mixture of latent topics, which are in turn comprised of a probability distribution over words (Chen, 2011, gives a good overview). It?s an unsupervised algo-rithm that performs approximate inference. The topics it infers are the ones that best explain the statistical distributions of words observed in the 
9
data. It is highly parallelizable and so it scales well to very large data sets. In practice we ask LDA to search for 5k topics, where k is the number of clus-ters we will eventually display to the user. The second step is to perform K-Means cluster-ing on the documents, where the documents are represented as a mixture of LDA topics as de-scribed above, and the clustering chooses k clusters that minimize the differences between documents in the cluster while maximizing the difference be-tween documents that are not in the same clusters. This step is fast, in part because of the fact that we have already reduced the number of input features down to 5k (rather than having one feature for each word observed in the entire dataset.) Finally, we give the clusters titles by perform-ing a calculation of mutual information (MI) for each word or bigram in each cluster. Specifically, clustering terms (both words and bigrams) that oc-cur frequently in one cluster but rarely in other clusters will receive high scores. The terms with the highest MI scores are used as cluster labels. One significant advantage of this completely unsupervised approach is that CoFi is more robust to the language of comment data, e.g. grammatical and spelling inconsistency, informal language, which are a challenge for rule-based and super-vised NLP tools.  In addition to the machine-generated topic clus-ters, CoFi allows user-defined topics. These are search terms and topic labels hand-created by a domain expert. CoFi partitions the comments into machine-generated topics and also assigns each comment to any of the matching predefined topics. This approach is useful for domain experts, ena-bling them to quickly find things they already know they want while allowing them to also take advantage of unexpected topics which emerge from the system clustering. 2.3 Creating the visualizations CoFi uses the JQuery, Flot, and g.Raphael javascript libraries to provide a dynamic, respon-sive interface. When the user visits a CoFi URL, the data is downloaded into their browser which then computes the visualization elements locally, allowing fast response times and offline access to 
the data. The JQuery library is central to all of the javascript processing that CoFi performs, and ensures that all features of the interface are cross-compatible with major browser versions. The interface provides the ability to drill down further into any data, allowing the user to click on any aspect of the analysis to obtain more detail. Since the visualization is calculated locally, the software can create dynamically updated timelines that show the user how any subset of their data has changed over time. It is also important to prioritize all data present-ed to the user, allowing them to focus on the most useful documents first. CoFi applies an automatic summarization technique to perform relevance sorting. We evaluated several state-of-the-art au-tomatic document summarization techniques and settled on a Kullback-Leibler divergence inspired by techniques described in Kumar et al (2009). The ?relevance? sort relies on a measure of how representative each comment is relative to the en-tire collection of comments that the user is viewing at the time. This allows us to rapidly rank tens of thousands of comments in the order of their rele-vance to a summary. Several of the approaches we tested were chosen from among the leaders of NIST?s 2004 Document Understanding Conference (DUC) summarization evaluation. Many of them used slight variants of KL divergence for sentence scoring. We also implemented Lin & Bilmes? (2010) Budgeted Maximization of Submodular Functions system, which performed best according to the DUC evaluation. However, even after apply-ing a scaling optimization inspired by the ?buck-shot? technique of Cutting et al (1992) the processing speed was still too slow for dealing with datasets containing more than 10000 small documents. The KL divergence approach scales linearly in the number of comments while still of-fering cutting edge qualitative performance. This means that the calculation can be done on the fly in javascript in the browser when the user requests a relevance sort. This allows CoFi to tailor the re-sults to whatever sub-selection of data is currently being displayed. For CoFi?s typical use cases this computation can be completed in under 2 seconds. 
10
3 The CoFi Interface CoFi takes a set of comments and produces the interactive summary you see in Figure 1. CoFi works best when a user is operating with between 200 and 10,000 comments. With small numbers of comments, there may not be enough data for CoFi to find interesting topic clusters. With very large numbers of comments, a user?s web browser may struggle to display all comments while maintaining sufficient responsiveness.  The raw data is available for inspection in many ways. The summary screen in Figure 1 presents a list of automatically-discovered clusters on the left-hand side (typically 10-30, this is a parameter of the clustering algorithm), the posting volume time-line on the top, and some overall statistics and characteristic words and posters in the middle. The user can return to this view at any point using the Overview button. At the top of the page, CoFi pre-sents the total number of comments and partici-pants, and a summary of the level of threading, which is a good indicator of how interactive the data set is. Where community ratings appear on a site, we also present the highest and lowest rated comments (this is solely based on the community rating, and not on our relevance calculation). In the 
middle of the display are two hyperlinked word clouds containing the highest frequency words and users. Selecting one of the top words or users has the same effect as searching for that term in one of the Search boxes?both of these approaches will present the user with matching comments with the term highlighted, and color coding to indicate clus-ter membership. The links from most popular words and most active users bring up a multi-graph view as in Figure 3.  Each time a set of comments is selected, either via a cluster, full text search, or filtering on a par-ticular commenter, the set is presented to the user in a sorted order with the comments most repre-sentative of the set ordered above those that are less representative. In this way, the user can quick-ly get a handle on what the set is about without reading all of the items in detail. The comments can also be sorted into the original temporal order, which can be useful to see how a comment thread evolves over time, or to view an original comment and threaded replies in a nested ordering. Figure 2 shows a single cluster in CoFi. The full thread timeline now has a red overlay for the selected subset of comments.  
Figure 1: CoFi top level summary view 
11
 At the bottom of the cluster lists, there is a View All Comments option. Sorting the entire set by rel-evance gives a good snapshot of most and least useful comments in the thread. From any of the views, clicking on a user name will display all comments from that user, and clicking on the comment ID will present that sub-thread; top-level comments are numbered X, while replies are la-beled X.X.  The CoFi interface also allows the user to export individual comments, marking those comments as having been ?handled? and routed to a particular person. This makes it easier to incre-mentally process comments as they arrive. We have applied CoFi to 72 distinct data sets, including forum discussions, news article, blog and YouTube comments, Twitter and comments on regulatory changes submitted to government offic-es via Regulations.gov. These last documents are much longer than those CoFi was intended to han-dle, but CoFi was nonetheless able to support in-teresting analysis. In one instance, we identified a clear case of ?astroturfing? (fake grassroots movement) based on the CoFi clusters. Acknowledgements Over the course of this project, many people have supported our work. We?d particularly like to thank Mark Maybury, Robert Peller at USSOUTHCOM, and Marty Ryan, Robert Battle and Nathan Vuong at the MITRE Miami site.  This  
 technical data was produced for the U. S. Govern-ment under Contract No. W15P7T-11-C-F600, and is subject to the Rights in Technical Data-Noncommercial Items clause at DFARS 252.227-7013 (NOV 1995). ? 2012 The MITRE Corpora-tion. All Rights Reserved. Approved for Public Release: 12-1507. Distribution Unlimited. MITRE Document number MP120212. References Chen, Edwin (2011). Introduction to Latent Direchlet Allocation, http://blog.echen.me/2011/08/22/ introduction-to-latent-dirichlet-allocation/#comments Cutting, D., Karger, D., Pedersen, J., and Tukey, J. (1992). Scatter/Gather: a cluster-based approach to browsing large document collections. Proceedings of 15th Annual International ACM SIGIR conference, New York, NY, USA, 318-329 Kumar, C., P. Pingali, and V. Verma (2009). Estimating Risk Of Picking a Sentence for Document Summari-zation. Proceedings of CICLing 2009, LNCS 5449, 571-581. Lin, H. and Bilmes, J. (2010). Multi-document summa-rization via budgeted maximization of submodular functions. Proceedings of Human Language Tech-nologies 2010, Los Angeles, CA, USA, 912-920. McCallum, Andrew Kachites (2002). MALLET: A Ma-chine Learning for Language Toolkit. Mishne, Gilard and Natalie Glance (2006). Leave a re-ply: An Analysis of Weblog Comments. In Workshop on the Weblogging Ecosystem, 15th International World Wide Web Conference, May. 
Figure 3: The "small multiples" view of frequent contributors 
Figure 2: Single cluster view 
12
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 101?110,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Approved for Public Release; Distribution Unlimited. 13-1876
Discriminating Non-Native English with 350 Words
John Henderson, Guido Zarrella, Craig Pfeifer and John D. Burger
The MITRE Corporation
202 Burlington Road
Bedford, MA 01730-1420, USA
{jhndrsn,jzarrella,cpfeifer,john}@mitre.org
Abstract
This paper describes MITRE?s participation in
the native language identification (NLI) task
at BEA-8. Our best effort performed at an ac-
curacy of 82.6% in the eleven-way NLI task,
placing it in a statistical tie with the best per-
forming systems. We describe the variety
of machine learning approaches that we ex-
plored, including Winnow, language model-
ing, logistic regression and maximum-entropy
models. Our primary features were word and
character n-grams. We also describe several
ensemble methods that we employed for com-
bining these base systems.
1 Introduction
Investigations into the effect of authors? latent at-
tributes on language use have a long history in lin-
guistics (Labov, 1972; Biber and Finegan, 1993).
The rapid growth of social media has sparked in-
creased interest in automatically identifying author
attributes such as gender and age (Schler et al, 2006;
Burger and Henderson, 2006; Argamon et al, 2007;
Mukherjee and Liu, 2010; Rao et al, 2010). There
is also a long history of computational aids for lan-
guage pedagogy, both for first- and second-language
acquisition. In particular, automated native language
identification (NLI) is a useful aid to second lan-
guage learning. This is our first foray into NLI,
although we have recently described experiments
aimed at identifying the gender of unknown Twit-
ter authors (Burger et al, 2011). We performed well
using only character and word n-grams as evidence.
In the present work, we apply that same approach
to NLI, and combine it with several other baseline
classifiers.
In the remainder of this paper, we describe our
high-performing system for identifying the native
language of English writers. We explore a varied
set of learning algorithms and present two ensem-
ble methods used to produce a better system than
any of the individuals. In Section 2 we describe the
data and task in detail as well as the evaluation met-
ric. In Section 3 we discuss details of the particular
system configuration that scored best for us. We de-
scribe our experiments in Section 4, including our
exploration of several different classifier types and
parametrizations. In Section 5 we present and an-
alyze performance results, and inspect some of the
features that were useful in discrimination. Finally
in Section 6 we summarize our findings, and de-
scribe possible extensions to the work.
2 Task, data and evaluation
Native Language Identification was a shared task or-
ganized as part of the Eighth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions, 2013. The task was to identify an author?s
native language based on an English essay.
The data provided consisted of a set of 12,100
Test of English as a Foreign Language (TOEFL) ex-
aminations contributed by the Educational Testing
Service (Blanchard et al, to appear). These were
English essays written by native speakers of Arabic,
Chinese, French, German, Hindi, Italian, Japanese,
Korean, Spanish, Telugu, and Turkish. A set of 1000
essays for each language was identified as training
data, along with 100 per language for development,
101
and another 100 per language for a final test set. The
mean length of an essay is 348 words.
The primary evaluation metric for shared task
submissions was simple accuracy: the fraction of the
test essays for which the correct native language was
identified. A baseline accuracy would thus be about
9% (one out of eleven). Results were also reported
in terms of F-measure on a per-language basis. F-
measure is a harmonic mean of precision and recall:
F = 2PRP+R . For the evaluation, the precision de-
nominator was the number of items labeled with a
particular language by the system and the recall de-
nominator was the number of items marked with a
particular language in the reference set.
The training, development, and test sets all had
balanced distributions across the native languages,
so error rates and accuracy did not favor any partic-
ular language in any set.
3 System overview
The systems we used to generate results for the NLI
competition were all machine-learning-based, with
no handwritten rules or features. The final submitted
systems were ensembles built from the outputs and
confidence scores of independent eleven-way multi-
nomial classifiers.
3.1 Features
The features used to build these systems were
language-independent and were generated using the
same infrastructure designed for the experiments de-
scribed in Burger et al (2011).
We incorporated a variety of binary features into
our systems, each of which was hashed into a 64-bit
numeric representation using MurmurHash3 (Ap-
pleby, 2011). The bulk of our features were case-
sensitive word- and character-based n-grams, in
which a feature was turned ?on? if its sequence of
words or characters appeared at least once in the text
of an essay. We also added binary features describ-
ing surface characteristics of the text such as average
word length and word count. Features were sepa-
rated into tracks such that the word unigram ?i? and
the character unigram ?i? would each generate a dis-
tinct feature.
Part of speech tag n-grams were added to the
feature set after reviewing performance results in
Brooke and Hirst (2012). We used the Stan-
ford log-linear part of speech tagger described in
Toutanova et al (2003), with the english-left3words-
distsim.tagger pretrained model and the Penn Tree-
bank tagset. The tagger was run on each essay and
outputs were incorporated as sequence features with
n-grams up to length 5.
3.2 Classifiers
Carnie1 is a MITRE-developed linear classifier
that implements the Winnow2 algorithm of Carvalho
and Cohen (2006), generalized for multinomial clas-
sification. Carnie was developed to perform clas-
sification of short, noisy texts with many training
examples. It maintains one weight per feature per
output class, and performs multiplicative updates
that reinforce weights corresponding to the correct
class while penalizing weights associated with the
top-scoring incorrect class. The learner is mistake-
driven and performs an update of size  after an error
or when the ratio of weight masses of the correct and
top incorrect classes is below 1 + ?. It iterates over
the training data, cooling its updates after each itera-
tion. For the purposes of these experiments, an input
to Carnie was the text of a single TOEFL essay, and
the output was the highest scoring class and several
related scores.
SRI?s Language Modeling Toolkit (SRILM) is
a toolkit for sequence modeling that continues to
be relevant after more than a decade of develop-
ment (Stolcke, 2002). It can be used to both build
models of sequence likelihoods and to evaluate like-
lihoods of previously unseen sequences. Building
a multinomial text classifier with a language model
toolkit involves building one model for each target
class and choosing the label whose model gives the
highest probability.
Many smoothing methods are implemented by
SRILM, along with a variety of n-gram filter-
ing techniques. The out-of-the-box default con-
figuration produces trigram models with Good-
Turing smoothing. It worked well for this com-
petition. Using open vocabulary models (-unk),
turning off sentence boundary insertion (-no-sos
-no-eos) and treating each essay as one sentence
1It is named for entertainers who guess personal character-
istics of carnival goers.
102
worked best in our development environment.
LIBLINEAR is a popular open source library for
classification of large, sparse data. We experimented
with several of their standard Support Vector Ma-
chine and logistic regression configurations (Fan et
al., 2008). We selected multiclass `2-regularized
logistic regression with the dual-form solver and
default parameters. Inputs to the model were bi-
nary features generated from a single TOEFL essay.
Features for this model were generated by Carnie.
The model provided probability estimates for each
candidate output class (L1) for each essay, which
were then combined with the outputs of Carnie and
SRILM in an ensemble to produce a single predic-
tion.
3.3 Ensembles
The classifiers described above were selected for in-
clusion as components in a larger ensemble on the
basis of their performance and the observation that
errors committed by these systems were not highly
correlated. We used the entirety of our training data
for construction of each component system, leaving
scant data available for estimating parameters of en-
sembles. This scenario led us to choose naive Bayes
to combine the outputs of the original components.
Given h1, . . . , hk hypothesis labels from k differ-
ent systems, one approximates the conditional like-
lihood of the reference label P (R|H1 . . . Hk) using
the Bayes transform and the development set esti-
mates of P (Hi|R). One investigates all possible la-
bels to decode r? = argmaxr P (r)
?
i P (hi|r). The
class balance in every set we operated on made the
prior P (r) irrelevant for maximization and simpli-
fied many of the denominators along the way. This
is a typical formulation of naive Bayes.
Confidence All of our component systems pro-
duce scores as well as a predicted label. Carnie pro-
duces (non-probability) scores for all of the candi-
date labels, SRILM produces log-probabilities and
perplexities, and LIBLINEAR produces P (h|r), the
likelihood of each of the possible labels. We ex-
perimented with several transformations of those
scores to best use them to predict correctness of
their hypothesis. There were several graphical mod-
els we could use for folding these scores into the
Bayes ensemble, and we chose a simple, discretized
P (H,S|R). We evenly partitioned and relabeled our
system outputs according to their scores (S), and
used those partition labels in the Bayes ensemble.
Thus when a particular reference label was scored
in the ensemble during decoding, both its prediction
and score contributed to the label in the naive Bayes
table lookup.
3.4 Best configuration
We submitted five systems with a variety of con-
figurations. One of our systems was our individual
Carnie system on its own for calibration. The other
four were ensembles.
The best system we submitted was a Bayes en-
semble of the Carnie, SRILM, and LIBLINEAR
components each trained on the train+development
sets. Carnie was trained for twelve iterations with
 = 0.03, ? = 0.05, and a cooling rate of 0.1.
SRILM models were trained for open vocabulary
and the default trigram, Good-Turing setting. Lo-
gistic regression from LIBLINEAR was run with `2
regularization and using the dual form solver.
Parameters for the Bayes model were collected
from the development set when the components
were trained only on the training set. A grid search
was performed over likely candidates for ?, the
Dirichlet parameter, and ?, the number of score-
based partitions, resulting in ? = 0.03125 and ? =
2. The grid search was performed with the compo-
nent models trained only on the training set and us-
ing 10-fold cross validation on the development set.
4 Experiments
In all experiments described below, systems were
trained initially on the 9900 training examples alone,
with the 1100 item development set held back to al-
low for hyperparameter estimation. When prepar-
ing our final test set submissions, the development
set was folded into the training data, and all models
were re-trained on this new dataset containing 11000
items.
4.1 Baselines
How hard is the NLI task? Simple baselines of-
ten give us a quick glimpse into what matters in a
NLP task. In Figure 1, we give accuracy results
on ten different baselines we trained on the training
103
Baseline Accuracy(%)
random 9.1
char length 9.6
SRILM(letter unigram) 10.8
word length 12.0
proficiency 14.9
SRILM(letter bigram) 15.1
JS(vowels) 20.6
JS(consonants) 33.8
JS(vowels+consonants) 34.1
JS(bag-of-words) 52.5
Figure 1: Simple baseline development set scores.
set and evaluated on the development set. Predic-
tions based on simple character and word lengths
show only slight gains over random. Using the
high/medium/low proficiency score that accompa-
nied the data similarly gives a tiny amount of infor-
mation over baseline (14.9%). We ignored those rat-
ings elsewhere in our work, to focus on the core task
of prediction based on essay content.
We collected some simple distributions of vowel
and consonant clusters and used them for predic-
tion, scoring with Jensen-Shannon divergence. JS
divergence is a symmetrized form of KL divergence
to alleviate the mathematical problem involved with
missing observations. It has behaved well in the
context of language processing applications (Lee,
1999). The score progression from consonant clus-
ters, to vowel clusters, to words suggests that there
is NLI information scattered at various levels of sur-
face features.
4.2 Varied Carnie configurations
Carnie?s out-of-the-box configuration is one that has
been optimized for application to micro-blogs and
other ungrammatical short texts. While our hypoth-
esis was that this configuration would be well suited
to analysis of English TOEFL essays, we investi-
gated a number of possible techniques to help Carnie
adapt to the new domain.
We began by performing a grid search to select
model hyperparameters that enabled our standard
configuration to generalize well from the training
dataset to the development dataset. These values of
, ?, and cooling rate were then applied to various
new feature configurations.
The standard configuration included binary fea-
tures for word unigrams and bigrams, character n-
grams of sizes 1 to 5, and surface features. We
experimented here with word trigrams, character 6-
grams, and lowercased character n-grams of sizes 1
to 6. We also added skip bigrams, which were or-
dered word pairs in which 1 to 6 intervening words
were omitted. We incorporated part of speech tags in
a number of ways, including POS n-grams of lengths
1 to 5, POS k-skip bigrams with k ranging from 1 to
6, and POS n-grams in which closed-class POS tags
were replaced with the actual content word used.
We also measured the impact of using frequency-
weighted features.
Our standard approach with Carnie is to perform
multinomial classification using one model trained
on all the data simultaneously. We experimented
with other ways of framing the NLI problem, such
as building eleven binary classifiers, each of which
was trained on all of the data but with the sole task
of accepting or rejecting a single candidate L1. We
also partitioned the training data to build 55 binary
classifiers for all possible pairs of L1s. These bi-
nary classifiers were then combined via a voting
mechanism to select a single winner. This allowed
us to apply focused efforts to improve discrimina-
tion in language pairs which Carnie found challeng-
ing, such as Hindi-Telugu or Japanese-Korean. To
this end, we collected a substantial amount of ad-
ditional out-of-domain training data from the web-
sites lang8.com (70,000 entries) and gohackers.com
(40,000 entries). Although we did not use this
data in our final submission, we performed experi-
ments to measure the value of this new data in the
TOEFL11 domain with no adaptation, with feature
filtering to limit training features to items observed
in the test sets, and with ?frustratingly easy? do-
main adaptation, EasyAdapt, described in Daum?
and Marcu (2007).
4.3 Varied SRILM configurations
SRILM offers a number of parameters for ex-
perimentation. We hill-climbed on the train-
ing/development split to select a good configura-
tion. We experimented with n-gram lengths from
1-5 (bag of words through word 5-grams), using the
tokenization given by the NLI organizers. We tried
the lighter weight smoothing techniques offered by
104
System Confidence MRD
Carnie s(h1)/s(h2) 343
s(h1)/
?
i s(hi) 268
s(h1)? s(h2) 72
SRILM log p(h1)/ log p(h2) 315.7
log p(h1)? log p(h2) 315.3
ppl1(h1)/ppl1(h2) 315.12
ppl1(h1)? ppl1(h2) 260
ppl1 77
log p(h1) 40
MaxEnt
?
i p(hi) log p(hi) 385.7
(JCarafe) p(h1) 383.15
log p(h1) 383.15
p(h1)/p(h2) 373.75
log p(h1)/ log p(h2) 379.8
LIBLINEAR
?
i p(hi) log p(hi) 379.8
Figure 2: Confidence candidates measured in Mean Rank
Difference between correct and incorrect labels.
SRILM including Good-Turing, Witten-Bell, Ris-
tad?s natural discounting, both modified and original
Kneser-Ney. We built both closed vocabulary and
open vocabulary language models and with special
symbols added for sentence boundaries.
4.4 Component confidence experiments
Our components generate scores, but those scores
were not always scaled in the same way. Winnow
(in Carnie) is a margin-based, mistake-driven learner
generating scores which are interpretable only as
sums of weights. SRILM produces log p(dj |hi),
but renormalizing those (with priors) into estimates
of p(hi|dj) is unreliable because the different sub-
models are not connected with smoothing. Logistic
regression produces a distribution for p(hi|dj). We
aimed to express these notions of confidence in a
way that was common to all systems. We did this by
relabeling system hypotheses after sorting by confi-
dence, but not all metrics were equally good at this
sorting.
We performed an ad hoc assessment of several
candidate scoring functions. Our goal was to find
functions that best separated correct answers from
incorrect answers in a sorted ranking. We ran several
candidates on our development set and measured the
difference between the mean rank of correct answers
and the mean rank of incorrect answers. Figure 2
displays the results. In each case h1 was the best hy-
pothesis generated by the system and h2 is second
best. p(?) indicates probabilities, s(?) indicates non-
probability scores. We chose those functions with
the highest values.
4.5 Simple models for combination
In this work, we focused our ensembles only on the
output of our individual components, ignoring the
features from the original data that they attempt to
model. The base systems are all trained to minimize
errors, and did not appear to have any particular
preferential capabilities. Thus we rely on them en-
tirely for the primary processing and focus on their
outputs.
In our naive Bayes formulation, the random vari-
ables produced by the component systems (H) need
not take on values directly comparable with the ref-
erence labels to be predicted (R). We experimented
with folding in several one-shot systems that pro-
duced labels in {L, L?}, for particular native lan-
guage groups, but none of these proved to be good
complements for the components described above.
To cope with decode-time configurations of H
that hadn?t been seen during estimation, we used
a Dirichlet prior on R in this ensemble. A sin-
gle parameter, ?, was introduced. Thus our esti-
mates for P (hi|r) were based on smoothed counts:
c(hi,r)+?
c(r)+?|R| . The search for ? was performed using
cross-validation on the development set.
Assignment In many prediction settings, we know
that our evaluation data consists of examples drawn
from a particular allocation of candidate classes.
One can take advantage of this in a probabilistic
setting by doing a global search for the maximum
likelihood assignment of the test documents to the
L1 languages under the constraint that each L1 lan-
guage must have a particular occupancy by the doc-
uments ? in this case, an even split. More generally,
once we have p(hi|dj) for each candidate language
hi and document dj , we can find an assignmentA =
{(i, j) : ?i,j = 1} that maximizes the likelihood
P (H|D) =
?
(i,j)?A p(hi|dj) =
?
i,j p(hi|dj)
?i,j
under the constraints that
?
i ?i,j = |D|/|H| and?
j ?i,j = 1. The first constraint says that each lan-
guage should get an even allocation of documents
assigned to it and the second constraint says that
105
each document should be assigned to only one lan-
guage. This reduces to a maximum weight match-
ing on
?
i,j ?i,j log p(hi|dj). This problem is di-
rectly convertible into a max flow problem or a lin-
ear program. It can be solved with methods such
as the Hungarian algorithm, Ford-Fulkerson, or lin-
ear programming. In our case, we used LPSOLVE2
to find this global maximum. This looks at first
glance like an integer programming problem, but
one can relax the constraints into inequalities and
still be guaranteed that the solution will end up with
all ?i,j landing on either zero or one in the right
amounts. We applied this assignment combination
as a post-processing step to the probabilities gener-
ated in the naive Bayes ensemble and also to the raw
LIBLINEAR outputs. The hope in doing this is that
the optimizer will move the less likely assignments
around appropriately while preserving the assign-
ments where it has more confidence. We observed
mixed results on our development set and submitted
two systems using this ensemble technique.
4.6 Other components explored
LIBLINEAR provides an implementation of a linear
SVM as well as a logistic regression package. We
experimented with various combinations of `1- and
`2 -loss SVMs, with both `1 and `2-regularization,
but in the end opted to use the `2-regularized logistic
regression due to slightly superior performance and
the ease with which we could extract eleven values
of P (H) for inclusion in our ensemble.
Another component that was tested in develop-
ment of our ensemble systems was a maximum en-
tropy classifier. This particular effort used the imple-
mentation from JCarafe,3 which uses L-BFGS for
optimization.
We approached the NLI task as document classi-
fication, following a typical JCarafe recipe (Gibson
et al, 2007). The class of the document is the native
language of the author. Each document was treated
as a bag of words, and several classes of features
were extracted: token n-gram frequency, character
n-gram frequency, part of speech n-gram frequency.
The feature mix that produced the best score was
token bigrams and trigrams, character trigrams and
2http://lpsolve.sourceforge.net
3https://github.com/wellner/jcarafe
L1 Mean F Our Best F
GER 1 0.776 1 0.921
ITA 2 0.757 2 0.88
CHI 3 0.723 4 0.85
JPN 4 0.708 5 0.837
FRE 5 0.701 7 0.818
TEL 6 0.667 3 0.802
KOR 7 0.665 6 0.827
TUR 8 0.656 8 0.81
ARA 9 0.65 3 0.872
SPA 10 0.631 10 0.768
HIN 11 0.606 11 0.762
Figure 3: L1s by empirical prediction difficulty. Mean F
incorporates all submissions by all competition teams.
POS trigrams. A feature frequency threshold of 5
was used to curb the number of features.
5 Results
Our best performing ensemble was 82.6% accurate
when scored on the competition test set, and was
composed of Carnie, SRILM, and logistic regres-
sion, using naive Bayes to combine the subsystem
outputs and confidence scores into a single predic-
tion. The best performing subsystem during system
development scored 79.3% on the test set in isola-
tion, demonstrating once again the value of combin-
ing systems that make independent errors.
Certain L1s gave our systems more difficulty than
others. Our best submitted F-measure scores ranged
from 0.921 for German to 0.762 for Hindi. Fig-
ure 3 demonstrates that our systems? scores were
highly correlated with average scores from all sub-
missions by all teams (R2 = 0.84). From this we
infer that our performance differences between L1s
may be explained by inherent difficulties in certain
languages or by the selection of similar L1s as a part
of the competition task, rather than quirks of our ap-
proach. Our submissions do appear to have a partic-
ular advantage on Arabic and Korean, relative to the
field.
Figure 4 shows the overall performance of our
submissions and subsystems on the development
and test evaluation sets.
Our scores dropped 4 to 5% between development
and test evaluations, representing significant overfit-
106
Configuration dev % test %
Components
base Carnie 82.6
+ trigrams 83.1
+ POS tags 83.6 79.3
1v1 voted Carnie 79.4
SRILM 77.1
MaxEnt 77.7
Linear SVM 81.9
Logistic Regression 83.4
assignment(LR) 82.4
Ensembles
bayes(Carnie,SRILM,LR) 87.3 82.6
assign(Carnie,SRILM,LR) 86.5 82.0
assign(Carnie,SRILM,MaxEnt) 86.4 82.3
bayes(Carnie,SRILM) 86.9 81.7
Figure 4: Results.
ting to the development set. The development set
was used for model selection, ensemble parameteri-
zation, and eventually as additional training data for
final submissions. Later tests showed that this fi-
nal retraining actually reduced the Carnie score by
0.9%.
Figure 4 also shows the effect of various efforts to
improve our baseline Carnie system. Adding part-
of-speech n-grams and word trigrams as features
improved the score on the development set by 1%
in total. Meanwhile many of our experiments with
new types of features yielded no gains. Lowercased
character n-grams, skip bigrams and all non-vanilla
formulations of part-of-speech tags provided no im-
provement and were discarded.
It was observed that all of our systems showed
a strong preference for binary features over
frequency-weighted inputs. In the case of the
JCarafe classifier, switching to binary features
yielded a 10% accuracy gain. Although JCarafe
didn?t provide a gain over the ensemble of Carnie,
SRILM, and LIBLINEAR logistic regression, de-
velopment set results indicated that JCarafe served
capably as a replacement for LIBLINEAR in some
ensembles.
We also measured the impact of using out-of-
domain Japanese and Korean L1 data to train a pair-
wise JPN/KOR system. Only 78.5% of JPN and
KOR texts were correctly identified in our eleven-
Rank L1 Score Feature
14 GER 21.05 (for,example)
40 GER 15.95 (have,to)
55 HIN 14.80 (as,compared,to)
57 ITA 14.60 (I,think,that)
58 TEL 14.18 (and,also)
60 HIN 13.97 (as,compared)
79 TEL 12.82 (the,people)
96 TEL 12.14 (for,a)
101 ITA 11.83 (that,in)
116 ITA 10.94 (think,that)
119 GER 10.93 (has,to)
120 TEL 10.89 (with,the,statement)
Figure 5: Word n-gram features predicting particular L1.
way baseline system. We restricted train and evalu-
ation data to only those two L1s and found our base-
line technique was 86.5% accurate. When we added
our out-of-domain data with no domain adaptation
technique, that score dropped to 82.0%. Removing
features that didn?t appear in our test set only raised
the score to 82.5%. However, the EasyAdapt tech-
nique (Daum? and Marcu, 2007) showed promise.
By making an additional source-specific copy of
each feature, we were able to raise the score to
88.5%. While this result was of limited applicabil-
ity in our final submission, and was therefore not
submitted to the open data competition task, we be-
lieve that this technique may prove useful in en-
abling cross-domain NLI system transfer.
Figure 5 provides a small sample of word-level
features discovered by the Winnow classifier. The
table shows the rank of each n-gram relative to all
features, and the native language that the feature
predicts. The weight assigned by the Winnow2 al-
gorithm is not readily interpretable, although higher
weights indicate a stronger association.
Similarly, the top character n-grams can be seen in
Figure 7, along with manually selected examples of
each. These features can be seen to mainly fall into
several broad categories. There are mentions of the
authors? home countries as in Korean, Italian and
Turkey. There are also characteristic misspellings
and infelicities such as personnaly, perhaps incor-
rectly modeled from the French personnellement.
It is worth noting that the weights (and thus the
ranks) for the top character n-gram features are
107
System Accuracy (%) Errors
Carnie 80.4 2153
SRILM 74.5 2800
LIBLINEAR 80.8 2116
ensemble-assign 81.9 1990
ensemble-Bayes 82.2 1961
Figure 6: Training set cross-validation results.
higher than for the top word features, indicating that
Winnow found the former to be more informative.
Finally, the top part-of-speech n-gram features are
shown in Figure 8, again with manually selected
examples. These features have similar weights
to the character n-gram features and for the most
part seem to represent ungrammatical constructions
(e.g., the first feature indicates that a personal pro-
noun followed by an uninflected verb predicts Chi-
nese). However, there are some perfectly grammat-
ical items that are indicative of a particular native
language (e.g., as compared to for Hindi). One pos-
sible explanation might be a dominant L2 pedagogy
for that language.
5.1 Cross-validation results
The task organizers requested that the participants
run a ten-fold cross validation on a particular split of
the union of the training and development sets after
the evaluation was over. Results of our leading com-
ponent systems and ensemble systems are presented
in Table 6. These are comparable with the TOEFL-
11 column of Figure 3 in Tetreault et al (2012).
6 Conclusion
In this paper, we have presented MITRE?s partici-
pation in the native language identification task at
BEA-8. Our best system was a naive Bayes ensem-
ble combining component systems that used Win-
now, language modeling and logistic regression ap-
proaches, all using relatively simple character and
word n-gram features. This ensemble performed at
an accuracy of 82.6% in the eleven-way NLI task,
placing it in a statistical tie with the winning systems
submitted by 29 teams. For individual native lan-
guages, our submission performed best among the
participants on Arabic, as ranked by F-measure.
In addition to the three base systems in our best
ensemble, we experimented with a maximum en-
tropy classifier and an assignment-based ensemble
method. We described a variety of experiments we
performed to determine the best configurations and
settings for the various systems. We also covered
experiments aimed at using out-of-domain data for
several native languages. In future work we will ex-
pand upon these, with the goal of applying domain
adaptation approaches.
One concern with NLI as framed in this evalua-
tion is the interaction between native language and
essay topic. The distribution of topics was very sim-
ilar in the various subcorpora, but in more natural
settings this is unlikely to be the case, and there is
a danger of overtraining on topic, to the detriment
of language identification performance. This is es-
pecially problematic for a highly lexical approach
such as ours. In future work, we intend to explore
the extent of this effect, using topic-based splits of
the corpus. Our initial experiments to remedy this
problem are likely to involve domain adaptation ap-
proaches, such as Daum? and Marcu (2007).
As described above, we have had success using
the Winnow-based system Carnie for other latent au-
thor attributes, such as gender. We would like to ex-
plore ensembles similar to those described here for
these attributes as well.
The techniques described in this paper success-
fully identified an author?s native language 82.6% of
the time using a sample of text averaging less than
350 words in length. Future work could study the
interaction of text length and NLI performance, in-
cluding texts shorter than 140 characters in length.
Acknowledgments
This work was funded under the MITRE Innovation
Program. Approved for Public Release; Distribution
Unlimited: 13-1876.
References
Austin Appleby. 2011. MurmurHash, mur-
mur3. https://sites.google.com/site/
murmurhash/.
Shlomo Argamon, Moshe Koppel, James W. Pennebaker,
and Jonathan Schler. 2007. Mining the blogosphere:
Age, gender, and the varieties of self-expression. First
Monday, 12(9), September.
108
Rank L1 Score Feature Snippet
1 KOR 57.34 orea first thing that Korean college students usually buy
2 GER 48.68 ,_tha the fact , that people have less moral values
3 SPA 23.65 omen consequences related with the enviroment and the atmosphere
4 ARA 23.23 _alot becouse you have alot of knowledge
6 TUR 22.84 s_abo their searchings about the products
11 ITA 21.56 Ital the Italian scholastic system
19 TEL 20.19 d_als the whole system and also the concept
20 TUR 19.96 urk in Turkey all young people go to the parties
21 CHI 19.51 Ta Take school teachers for example
23 GER 19.34 _-_ constantly - or as mentioned before even exponentially - breaking
27 JPN 17.62 s_,_I For those reasons , I think
32 FRE 16.90 ndeed Indeed , facts are just applications of ideas
36 JPN 16.57 apan been getting weaker these days in Japan .
37 FRE 16.57 onn I personnaly prefer
38 GER 16.04 ,_bec would be great , because so everyone
41 SPA 15.92 esa its not necesary to ask
47 HIN 15.23 in_i the main idea and concept
53 ITA 14.93 act_ due to the fact that too much
74 ITA 13.00 ,_in academic subjects and , in the mean time
81 TEL 12.74 h_ou cannot do with out a tour guide
Figure 7: Character n-gram features predicting particular L1.
Rank L1 Score Feature Snippet
35 CHI 16.58 (PRP,VB) What if he go and see
43 CHI 15.85 (NNS,POS) products ?s
45 SPA 15.41 (NNS,NNS) companies universities
59 TEL 14.05 (RB,IN,VBG) Usually in schooling
64 TEL 13.95 (DT,NNS,WDT) the topics which
65 TUR 13.71 (IN,DT,IN) after a while
66 TEL 13.69 (IN,VBG) in telling
69 TUR 13.42 (VBG,DT,NNS) learning the ways
70 HIN 13.39 (IN,VBN,TO) as compared to
80 HIN 12.81 (FW) [foreign word]
Figure 8: Part of Speech n-gram features predicting particular L1.
109
Douglas Biber and Edward Finegan, editors. 1993. Soci-
olinguistic Perspectives on Register. Oxford studies in
sociolinguistics. Oxford University Press.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. to appear. TOEFL11:
A Corpus of Non-Native English. Technical report,
Educational Testing Service.
Julian Brooke and Graeme Hirst. 2012. Robust, Lexical-
ized Native Language Identification. In Proceedings
of COLING 2012, pages 391?408, Mumbai, India, De-
cember.
John D. Burger and John C. Henderson. 2006. An ex-
ploration of observable features related to blogger age.
In Computational Approaches to Analyzing Weblogs:
Papers from the 2006 AAAI Spring Symposium. AAAI
Press.
John D. Burger, John Henderson, George Kim, and Guido
Zarrella. 2011. Discriminating gender on twitter.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1301?1309, Edinburgh, Scotland, UK, July. Associa-
tion for Computational Linguistics.
Vitor R. Carvalho and William W. Cohen. 2006. Single-
pass online learning: performance, voting schemes
and online feature selection. In Proceedings of
the 12th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ?06,
pages 548?553, New York, NY, USA. ACM.
Hal Daum? and D Marcu. 2007. Frustratingly easy do-
main adaptation. In Proceedings of the Association for
Computational Linguistics, volume 45, page 256.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
John Gibson, Ben Wellner, and Susan Lubar. 2007.
Adaptive web-page content identification. In Proceed-
ings of the 9th Annual ACM International Workshop
on Web information and Data Management, WIDM
?07, pages 105?112, New York, NY, USA. ACM.
William Labov. 1972. Sociolinguistic Patterns. Conduct
& Communication Series. University of Pennsylvania
Press.
Lillian Lee. 1999. Measures of distributional similar-
ity. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics, ACL ?99,
pages 25?32, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Arjun Mukherjee and Bing Liu. 2010. Improving gen-
der classification of blog authors. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, Cambridge, MA, October.
Association for Computational Linguistics.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In 2nd International Workshop on
Search and Mining User-Generated Content. ACM.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and
James Pennebaker. 2006. Effects of age and gender on
blogging. In Computational Approaches to Analyzing
Weblogs: Papers from the 2006 AAAI Spring Sympo-
sium. AAAI Press, March.
Andreas Stolcke. 2002. SRILM?an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
volume 2, pages 901?904.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in native
language identification. In Proceedings of COLING
2012, pages 2585?2602, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL, pages 252?259.
110
Proceedings of the Fourth Workshop on Teaching Natural Language Processing, pages 35?41,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning from OzCLO, the Australian Computational and Linguistics 
Olympiad 
 
Dominique Estival 
U. of Western Sydney 
d.estival@uws.edu.au 
John Henderson 
U. of Western Australia 
john.henderson@uwa.edu.au 
Mary Laughren 
U. of Queensland 
m.laughren@uq.edu.au 
 
Diego Moll? 
Macquarie U. 
diego.molla-
aliod@mq.edu.au 
Cathy Bow 
Charles Darwin U. 
cathy.bow@cdu.edu.au 
Rachel Nordlinger 
U. of Melbourne 
racheln@unimelb.edu.au 
Verna Rieschild 
Macquarie U. 
verna.rieschild@mq.edu.au 
Andrea C. Schalley 
Griffith U. 
a.schalley@griffith.edu.au 
 Alexander W. Stanley 
Macquarie U. 
alexander.stanley@students.mq.edu.au  
Colette Mrowa-Hopkins 
Flinders U.  
colette.mrowa-hopkins@flinders.edu.au 
 
 
Abstract 
The Australian Computational and Linguistics 
Olympiad (OzCLO) started in 2008 in only 
two locations and has since grown to a na-
tionwide competition with almost 1500 high 
school students participating in 2013. An Aus-
tralian team has participated in the Interna-
tional Linguistics Olympiad (ILO) every year 
since 2009. This paper describes how the 
competition is run (with a regional First 
Round and a final National Round) and the or-
ganisation of the competition (a National 
Steering Committee and Local Organising 
Committees for each region) and discusses the 
particular challenges faced by Australia (tim-
ing of the competition and distance between 
the major population centres). One major fac-
tor in the growth and success of OzCLO has 
been the introduction of the online competi-
tion, allowing participation of students from 
rural and remote country areas. The organisa-
tion relies on the good-will and volunteer 
work of university and school staff but the 
strong interest among students and teachers 
shows that OzCLO is responding to a demand 
for linguistic challenges. 
1 Introduction 
The Australian Computational and Linguistic 
Olympiad (OzCLO, www.ozclo.org.au) began as 
an idea in late 2007, largely prompted by a par-
ent in Ballarat, a small town in Victoria, who 
came across the North American competition 
(NACLO, Radev et al 2008) on the internet and 
thought it was something that her daughter 
would be interested in doing.  Her emails to the 
organisers of NACLO, asking about the likeli-
hood of such an event being run in Australia, led 
to initiating contact with the Australasian Lan-
guage Technology Association (ALTA) with the 
suggestion that a computational linguistic olym-
piad be established in Australia. Dominique Es-
tival (then at Appen Pty Ltd, and a member of 
the ALTA Steering Committee) took on the pro-
ject and, jointly with Jane Simpson (then from 
the University of Sydney), Rachel Nordlinger 
and Jean Mulder (from the University of Mel-
bourne), ran the first ever Australian Computa-
tional and Linguistic Olympiad in 2008, with 
financial support from HCSNet (the Human 
Communication Science Network), and help 
from ALTA (the Australasian Language Tech-
nology Association), ALS (the Australian Lin-
guistic Society) and CSIRO (the Commonwealth 
Scientific and Industrial Research Organisation).  
The first competition was held in two locations ? 
the University of Melbourne (Victoria) and the 
University of Sydney (New South Wales) ? with 
a total of 119 students participating from 22 
schools.  Given the success of this first competi-
tion, 2009 saw the addition of four new locations 
around Australia (Adelaide, South Australia; 
Brisbane, Queensland; Canberra, ACT; Perth, 
Western Australia) and the sending of the na-
tional winning team to the International Linguis-
tic Olympiad in Wroclaw, Poland. Since then 
OzCLO has run every year, with the recent addi-
tion of two regions (NSW-North in 2010 and 
Northern Territory in 2013) and the participation 
of an Australian team in every ILO.  
35
2 Philosophy, Aims and Principles 
The immediate aim of OzCLO (Simpson and 
Henderson, 2010) is to introduce high school 
students to language puzzles from which they 
can learn about the richness, diversity and sys-
tematic nature of language, and develop their 
reasoning skills. The general value of this type of 
knowledge and skills in high school education 
has not been specifically articulated to potential 
participants or their teachers, schools or parents, 
as it has in the UK (UKLO, 2011; Hudson and 
Sheldon, 2013). However, informal feedback and 
the participation rate both indicate a widespread 
perception in the school sector that this type of 
activity has educational value, albeit with differ-
ent focuses in different schools. For many of the 
schools that participate, OzCLO provides a 
means to meet their institutional responsibility to 
provide extra-curricular activities that are intel-
lectually stimulating and broadening for aca-
demically high-achieving students (under rubrics 
such as ?gifted and talented?). Some schools offer 
OzCLO to a wider range of students.  
The broader aim of OzCLO is to promote 
awareness of, and interest in linguistics and 
computational linguistics in high schools and in 
the wider community, and more specifically to 
increase enrolments in these disciplines at uni-
versity level. A further goal is that this will ulti-
mately attract people to careers in these areas. 
Linguistics has traditionally had little recognition 
at high school level in Australia, even within 
language education, although more recently there 
is linguistics content at upper high school level in 
the English Language course in Victoria and in 
the new national English curriculum. OzCLO has 
been running in most regions long enough to see 
participants reaching university, and although 
there has been no proper research on the impact 
of OzCLO on enrolments, there is anecdotal evi-
dence that some former participants have chosen 
to study at least some linguistics. 
Consistent with the key aim of promoting in-
terest, OzCLO operates on the principles that 
participation should be fun and should offer 
achievable if challenging tasks to a wide range of 
students across science and humanities interests, 
especially in the First Round. Schools are pro-
vided with a training package of problems which 
starts with a simple morphological analysis that 
is suitable to do as a whole-class exercise even if 
they do not proceed to the competition itself. In 
both rounds participation takes place in school-
based teams, rather than individual competition. 
This is partly to encourage students to learn to 
communicate their analytical ideas, to collabo-
rate effectively, and to provide mutual support 
and social interaction. It also offers some organ-
isational advantages in terms of registration and 
marking. Because team members may have dif-
ferent levels of ability, the competition process 
does not necessarily identify the highest achiev-
ing individuals, but this risk is out-weighed by 
the benefits of teams. The organisation of the 
First Round as separate competitions in each re-
gion provides each team with a smaller pool to 
compete in initially and a distinct level of local 
achievement. However, since there are consider-
able differences in the number of teams in each 
region, and the top teams from each region are 
invited into the National Round, the national 
competition does not necessarily consist of the 
highest achieving teams nationally and there is 
currently discussion of methods to minimise this 
effect. Finally, the results are structured to rec-
ognise participation as well as high achievement: 
in addition to recognising the top teams, all 
teams receive certificates in the categories Gold 
(top ?25%), Silver (next ?25%) and Bronze (re-
mainder). 
3 Organising the Annual Competition 
3.1 University level 
All Australian states and territories (with the ex-
ception of Tasmania) now participate in OzCLO 
and there is typically one Local Organising 
Committee (LOC) for each geographical region. 
There are currently eight LOCs (soon to be nine 
with the addition of a third New South Wales 
region).  Each LOC has the responsibility for 
student and school liaison, university space 
booking, recruiting volunteer academic and stu-
dent helpers, running the competitions, publicis-
ing the event locally, and finding cash or in-kind 
sponsorship (e.g. for rooms, venues, printing and 
prizes). 
The National Steering Committee (NSC) 
comprises the Chair of each LOC, the Problems 
Coordinator, the Treasurer, the OZCLO Web-
master and the Online Competition Coordinator.  
The NSC?s role is to coordinate between LOCs, 
make and implement OzCLO decisions, and co-
ordinate national sponsorships and publicity. A 
training package is developed by the NSC and 
provided online each year, on the OzCLO web-
site and within the online competition site. The 
NSC Chair has the responsibility of ensuring the 
coordination and execution of tasks for OzCLO, 
36
both nationally and internationally. The NSC 
Chair and the Problems Coordinator liaise with 
ELCLO (English Linguistics and Computational 
Linguistics Olympiads) with regard to develop-
ing annual problem sets, and with the Interna-
tional Linguistics Olympiad (ILO/IOL) with re-
gard to the international competition. NSC mem-
bers may have dual responsibilities. 
 Because of the distances between regional 
centres, the NSC meetings are all conducted via 
teleconferences, and committee members share 
documents and records using Airset, a cloud-
based collaboration site. 
3.2 School level 
OzCLO operates on a democratic basis, with the 
devolution of decision making passing from NSC 
to LOC to school teacher to students. Teacher 
and student feedback often contributes to NSC 
discussions. Information is disseminated to 
school teachers through the website as well as 
through emails from the region?s LOC. This in-
formation is also shared via Facebook and Twit-
ter accounts. Training sessions are provided 
online, at universities and, in some cases, within 
schools.  Teachers register teams of 4 members 
at the Junior (Years 9 and 10) or Senior level 
(Years 11-12) online. There is no limit to regis-
trations for the online competition, but registra-
tions for the offline competition (in which stu-
dents typically attend the organising University 
campus) may be constrained by University venue 
availability issues. Some schools have Linguis-
tics Clubs, and OzCLO is a strong focus for their 
activities.  In some regions, schools with over 80 
participating students request in-house training 
and invigilation for an offline First Round. 
3.3 The public face of OzCLO 
OzCLO has a website (www.ozclo.org.au) and a 
social media presence with Twitter and Facebook 
accounts for communications and promotion. 
Most LOCs have been successful in gaining pub-
licity for OzCLO through their University media 
departments. Many schools publish pictures and 
items about OzCLO achievements in their school 
newsletters. Some individual schools have fea-
tured in the local press after results of competi-
tions have been published. OzCLO has also fea-
tured in national radio segments. 
4 The OzCLO Competition  
4.1 Competitions Rounds  
The OzCLO competition consists of two rounds, 
a regional or state-wide First Round and a Na-
tional Round. In both, school-based teams of up 
to four students attempt to solve five or six lin-
guistic problems in two hours. The teams are 
divided into Senior and Junior sections, with the 
Senior teams drawn from the last two years of 
high school (Tears 11 and 12) while the Junior 
teams are drawn from the two preceding years 
(Years 9 and 10). The same problems sets and 
competition conditions hold for both Senior and 
Junior teams. The top three teams from each 
LOC are invited to go on to the National Round 
which is held under the same conditions. If the 
top Junior team is not in the overall top three 
teams, then it is also invited. The Senior team 
which wins the National Round is invited to rep-
resent Australia at the ILO. 
4.2 Problem sets 
In its first two years, OzCLO greatly benefited 
from NACLO, which allowed use of their prob-
lem sets. Some additional problems were com-
posed by linguists engaged in the running of the 
competition, or their colleagues. Since 2009, 
OzCLO has been part of ELCLO, the English 
Language Computational Linguistics Olympiad, 
in which participating countries (Australia, Ire-
land, North America and the United Kingdom) 
contribute to a shared set of problems. Because 
of the OzCLO rationale described above, an at-
tempt is made to try to have a mix of problems 
based on data from a wide range of languages, 
and also a wide range of data types. Different 
levels of difficulty are included so that students 
have the satisfaction of being able to solve most 
of the problems. The aim is to show students that 
analysing language phenomena can be fun as 
well as challenging, and also that linguistic skills 
can be applied to some very practical tasks. The 
problems include: deciphering non-Roman 
scripts; translation tasks involving typical mor-
phological and syntactic analysis; computational 
linguistic tasks; search for phonological rules, or 
linguistic reconstruction. 
4.3 Training for ILO 
Since 2009, an Australian team has participated 
in every ILO. While the main goal of OzCLO 
has always been the promotion of language stud-
ies, linguistic knowledge and analysis skills in 
37
Australian high schools, the appeal of potentially 
participating in an international competition has 
proved an additional incentive for many of the 
students and their teachers. However, because of 
the rationale for OzCLO discussed above, the 
problems used in the First Round and even the 
National Round are not nearly as difficult as the 
actual ILO problems. Therefore the Australian 
team needs to be given additional training before 
competing at the international level. This training 
was first provided by a coach accompanying the 
team at the ILO but we have found that this was 
insufficient and too late to be helpful. We now 
provide training sessions aimed at solving ILO-
level problems to the winning team prior to trav-
elling to the ILO. This has resulted in higher re-
sults, including an individual silver medal in 
2011 and honourable mentions in 2010 and 2012. 
5 Participation 2008-2013 
OzCLO has evolved from 22 schools and 119 
competing students in 2008 to 87 schools and 
1,451 competing students in 2013. Some schools 
have participated each year, and there has been a 
steady increase in new schools. Private and se-
lective government schools have so far been the 
majority in most regions, but the numbers of 
government schools participating are growing. 
All participating schools are highly enthusiastic 
about the OzCLO competitions. 
OzCLO naturally attracts schools keen on of-
fering a new kind of challenge to students in 
their GATS (gifted and talented students) pro-
grammes. However, teachers (not only language 
teachers, but also mathematics and computer sci-
ences teachers) also comment that OzCLO is a 
rare kind of competition because it provides fun, 
challenge, stimulation and team work for any 
student. 
A challenge for Australia compared with 
Europe or North America is the enormous dis-
tance between rural and metropolitan areas, mak-
ing it difficult for many schools in rural areas to 
participate in an offline University-based compe-
tition. The advent of the online option gives ur-
ban, rural and country remote students equity in 
access. Thanks to this plus a strong marketing 
drive in that state, numbers have increased dra-
matically in Queensland. In other regions, some 
schools prefer the university campus experience 
offered by the offline option. 
As Table 1 shows, numbers have increased 
steadily over the six years since inception. In 
2013, Australia?s population of 23 million has 
provided nearly as many Linguistics Olympiads 
competitors as has the United States and Canada 
combined, whose population figures are fifteen 
times more than Australia?s. The OzCLO par-
ticipation rate is 6.4 per 100,000 population. For 
UKLO it is 4.55, and for NACLO 0.49. 
6 Going on-line 
In the first four years of OzCLO?s existence, the 
competition was offered on campus by academic 
staff volunteers from a number of mainly metro-
politan Universities. Participating teams travelled 
from their schools to the respective Universities? 
campuses to take part in the Training Session and 
the First Round, except for NSW, where several 
OzCLO representatives also travelled to schools 
with a large participation base, in order to run the 
competition at the school. Teachers often re-
ported that these visits to the University campus 
were a highlight for the participating students 
who very much enjoyed the experience. 
Nonetheless, a number of drawbacks to this 
approach became apparent quite early. These 
included: 
? The difficulty of organising suitable venues 
on campus for running the competition due to 
the timing of the First Round (usually coin-
ciding with Universities? Orientation Week or 
their first weeks of teaching in the first semes-
ter). 
? The distance factor with the result that only 
schools within travel distance could partici-
pate in the competition (in the case of Queen-
sland, for instance, no school beyond a dis-
tance of about 100kms from campus partici-
pated in the offline competition). Given the 
size of Australia, most regional and rural 
schools were thus virtually excluded from 
competing. 
? Constraints on availability of venues and 
markers put a cap on the overall number of 
students who could compete in each region. 
Thus, the number of schools and the number 
of students per school had to be limited by the 
local committees from the outset (e.g. in 
Queensland, only two teams per school were 
able to compete, although some schools 
wished to enrol many more).  
 
 
38
LOC 
 
2008 
Schools/ 
students 
2009 
Schools/ 
Students 
2010 
Schools/ 
students 
2011 
Schools/ 
students 
2012 
Schools/ 
students 
2013 
Schools/ 
students 
Region  
population 
000s 
Participants 
per 100,000 
population 
NSW-S 10 
64 
14 
105 
[fn/a] 
92 
15 
279 
12 
289 
9 
312 
7,314 5.24 
NSW-N n/a  n/a  5 
40 
7 
58 
5 
60 
6 
71 
VIC 12 
55 
11 
90 
[fn/a] 
120 
9 
115 
16 
245 
18 
304 
5,649 5.38 
ACT n/a  7 
30 
5 
83 
5 
72 
9 
136 
9 
161 
377 42.76 
QLD n/a  11 
60 
15 
90 
15 
106 
20 
312 
25 
377 
4,585 8.22 
SA n/a  [fn/a]  
29 
5 
33 
3 
19 
4 
27 
3 
34 
1,658 2.05 
NT n/a  n/a  n/a  n/a  n/a  6 
80 
236 33.86 
WA n/a  10 
78 
11 
144 
16 
143 
14 
120 
12 
120 
2,451 4.90 
TAS n/a n/a n/a n/a n/a n/a 512 0 
Overall  119 
students 
392 
students 
602 
students 
792  
students 
1069  
students 
1459  
students 
22,786 6.40 
Table 1:  Participation schools/students 
(n/a = not applicable = LOC was not participating;  [fn/a] =figure not available 
 
In order to address these issues, it was de-
cided to offer an online option in 2012, using 
Griffith University?s Learning Management 
System. This lifted restrictions on numbers 
(both school and students per school), and 
schools were able to compete from anywhere 
in Australia if they so wished. As a result, 
schools located as far as 1,500 kms from the 
metropolitan areas have successfully partici-
pated in the competition, and some schools 
registered more than 20 teams in the latest 
competition. With the online option, the over-
all number of participants has increased dra-
matically (see Table 2). For instance, Victoria 
saw the number of their participants double 
from 2011 to 2012, while numbers in Queen-
sland nearly tripled. Even in those regions that 
shifted to exclusively offering the online op-
tion (such as Queensland in the last two years), 
most schools have remained in the competi-
tion. 
 
 
 2012 2013 
LOC Online students On campus students Online Students On-campus Students 
NSW-S 91 198 120 192 
NSW-N 60 [on/a] 8 63 
VIC 137 108 195 109 
ACT 64 72 115 46 
QLD 312 [on/a] 377 on/a 
SA 0 27 34 on/a 
WA 28 92 120 on/a 
NT n/a n/a 80 on/a 
Table 2: Participation numbers by mode (online/on-campus) 
 (n/a = not applicable (LOC was not participating);  [fn/a] =figure not available;  
[on/a] =option not available) 
39
In terms of students competing online vs. 
on-campus, except for the NSW-N region, 
there is a distinct shift towards participating 
online. Feedback from teachers has shown that 
in many cases it is easier for teams to stay 
within the school grounds for the competition 
rather than to travel to the University campus. 
For some schools, however, travelling to the 
University campus is still one of the major 
benefits they would not want to lose. For this 
reason most LOCs offer both on-campus and 
online options. Some regions choose to only 
offer the online option (with a training session 
at the University). 
Teams participating online have access to 
training materials and all the necessary infor-
mation, which is made available through the 
OzCLO website well before the competition 
day. This site also allows teams to familiarise 
themselves with the online testing system. On 
the day, all teams across Australia compete at 
the same time on the same day and within the 
same two hour period (to compensate for time 
zone differences, teams started at 12:00 in 
WA, 13:30 in the NT, 14:00 in QLD, 14:30pm 
in SA and 15:00 in the ACT, NSW and VIC in 
the 2013 competition). 
In terms of process and technical require-
ments, each participating team needs access to 
an Internet-enabled computer on the day of the 
competition. No special software is required on 
the school?s computers. The problem set is 
made available to teachers shortly before the 
competition commences, in order to allow 
them to print and copy the problems for the 
students. Students usually work on the paper 
copy, and then access the computer to enter 
their responses.  There is also a virtual class-
room set up for live communication during the 
competition, in order to allow students and 
teachers to ask questions but also to show stu-
dents that there are hundreds of competitors 
participating from around the country at the 
same time.  
Overall, the addition of the online alterna-
tive has been a very beneficial development for 
OzCLO. The strong growth in overall partici-
pant numbers over recent years is not simply 
due to the online option, but this has certainly 
played a major role. It remains to be seen if 
there is even more potential for growth ? espe-
cially in areas outside of the major cities. 
7 Challenges 
One of the main challenges OzCLO faces is 
the timing of the competition in relation to the 
schedule of the international linguistics compe-
titions. The Australian school year begins in 
February and ends in December, and the uni-
versity year is roughly March to November, in 
contrast to the September-June academic cal-
endars of the northern hemisphere. In order for 
an Australian team to be selected with enough 
time to prepare for participation in the ILO, the 
National Round needs to be held before the 
Easter break (March/April). For Universities 
and schools, this creates a very rushed timeline 
at the busiest time of the school/academic year. 
As mentioned earlier, another challenge for 
Australia is the vast distances between metro-
politan areas, where most of the universities 
are located. In spite of the success of the online 
competition, so far OzCLO has had mostly a 
metropolitan base and has not yet fully en-
gaged in marketing to regional and rural areas 
across the whole country. Targeting appropri-
ate teachers within schools can also be a chal-
lenge, as experience has shown that often the 
information does not filter through to the rele-
vant teachers (these are usually the coordina-
tors of Languages, Gifted Education, Mathe-
matics, or Computing programmes). Contact-
ing the professional associations for the differ-
ent teaching specialties could ensure that in-
formation is disseminated more efficiently. 
Funding is not guaranteed, and fundraising 
efforts are not rewarded every year. All organ-
isational efforts at University and school level 
depend on good-will and volunteering as well 
as donations. Changes in Heads of Depart-
ments in Universities and principals in schools 
can impact negatively on funds and participa-
tion levels. This means that core issues need to 
be resolved again every year, for example, the 
ongoing maintenance of the OzCLO web-
site/online registration system, which is both a 
challenge and a solution to other issues. The 
OzCLO website hosting is provided by Mac-
quarie University and the site is maintained by 
a student volunteer.  It has served as the central 
hub of information, with other modes (email, 
Facebook and Twitter) leading back to it for 
detailed information.  In addition to ordinary 
information, it also enables self-service regis-
tration, and the automated generation of PDF 
certificates after the competition. These facili-
ties and the volunteer support of the webmaster 
40
have significantly lowered the administrative 
and financial overhead for the organisers.  
An additional problem for OzCLO is the 
division of Australia?s most populated state 
(NSW, with almost a third of Australia?s popu-
lation) into northern and southern regions, 
which leads to one state providing double the 
competitors of other states into the National 
Round. A model is needed whereby all com-
petitors, no matter whether they come from a 
small or a large region, have an equal opportu-
nity to compete in the National Round.  
Finally, while OzCLO has been able to con-
tribute a number of linguistic problems to the 
ELCLO pool, it has proved extremely difficult 
to obtain contributions from Computational 
Linguistics (Estival, 2011). 
8 Conclusions 
In conclusion, running the OzCLO competition 
has been an activity well worth the effort, and 
it is very rewarding that it has become a fixture 
in the academic calendar for many schools. 
Students, teachers and principals have been 
extremely positive about the experience, giv-
ing encouraging feedback and expressing 
strong support for the competition. The recent 
increases in participation rates have come from 
new regions (only one Australian state cur-
rently has no LOC, but possibilities are being 
explored in this area), new schools, and larger 
numbers from individual schools (up to 100 
participants from a single school). Some 
schools have started a linguistics club as after 
school activity, and others are promoting their 
experiences on social media.  
While there is no data currently available 
regarding any effect on enrolments in tertiary 
linguistics programs, increased interest in and 
awareness of linguistics is certainly a positive 
outcome for a discipline which faces chal-
lenges of funding and viability. The coopera-
tion of academics from universities across the 
country in all the LOCs and the NSC, plus the 
support of the Australian Linguistics Society 
(ALS) and of the Australasian Language 
Technology Association (ALTA), make the 
competition a truly national event. This means 
that the competition is not dependent on any 
one single person or institution (although com-
petition within particular regions is), and al-
lows for further growth. Ongoing funding and 
continued support from both universities and 
schools across the country should see contin-
ued growth in the popularity and spread of the 
competition. 
References  
Derzhanski, Ivan, and Thomas Payne. (2010). The 
linguistic olympiads: academic competitions in 
linguistics for secondary school students. Lin-
guistics at school: language awareness in pri-
mary and secondary education, ed. By Kristin 
Denham and Anne Lobeck, 213?26. Cambridge: 
Cambridge University Press. 
Estival, Dominique. (2011). ?OzCLO: The Austra-
lian Computational Linguistic Olympiad?. Pro-
ceedings of the Australasian Language Technol-
ogy Association Workshop 2011. Canberra, Aus-
tralia.  
Hudson, Richard, and Sheldon, Neil. (2013). ?Lin-
guistics at School: The UK Linguistics Olym-
piad.? Language and Linguistics 
pass, Volume 7, Issue 2. pp. 
Radev, Dragomir R., Lori S. Levin, and Thomas E. 
Payne. (2008). ?The North American Computa-
tional Linguistics Olympiad (NACLO)?. In Pro-
ceedings of The Third Workshop on Issues in 
Teaching Computational Linguistics, Columbus, 
OH, USA. 
91?104,  
Simpson, Jane, and Henderson, John. (2010).  Aus-
tralian Computational and Linguistics Olympiad. 
Cross Section, Vol. 20, No. 3, July 2010: 10-14.  
United Kingdom Linguistics Olympiad Committee. 
(n.d.) The Linguistics Olympiads: Lots of fun, 
but are they educa-
tional? http://www.uklo.org/?page_id=35  .
41
