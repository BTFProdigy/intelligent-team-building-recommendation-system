First Joint Conference on Lexical and Computational Semantics (*SEM), pages 575?578,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Tiantianzhu7:System Description of Semantic Textual Similarity (STS) in
the SemEval-2012 (Task 6)
Tiantian Zhu
Department of Computer Science and
Technology
East China Normal University
51111201046@student.ecnu.edu.cn
Man Lan
Department of Computer Science and
Technology
East China Normal University
mlan@cs.ecnu.edu.cn
Abstract
This paper briefly reports our submissions to
the Semantic Textual Similarity (STS) task
in the SemEval 2012 (Task 6). We first use
knowledge-based methods to compute word
semantic similarity as well as Word Sense Dis-
ambiguation (WSD). We also consider word
order similarity from the structure of the sen-
tence. Finally we sum up several aspects of
similarity with different coefficients and get
the sentence similarity score.
1 Introduction
The task of semantic textual similarity (STS) is to
measure the degree of semantic equivalence between
two sentences. It plays an increasingly important
role in several text-related research and applications,
such as text mining, Web page retrieval, automatic
question-answering, text summarization, and ma-
chine translation. The goal of the Semeval 2012 STS
task (task 6) is to build a unified framework for the
evaluation of semantic textual similarity modules for
different systems and to characterize their impact on
NLP applications.
Generally, there are two ways to measure sim-
ilarity of two sentences, i.e, corpus-based meth-
ods and knowledge-based methods. The corpus-
based method typically computes sentence similar-
ity based on the frequency of word occurrence or the
co-occurrence between collocated words. For ex-
ample, in (Islam and Inkpen, 2008) they proposed a
corpus-based sentence similarity measure as a func-
tion of string similarity, word similarity and com-
mon word order similarity (CWO). The knowledge-
based method computes sentence similarity based
on the semantic information collected from knowl-
edge bases. With the aid of a number of success-
ful computational linguistic projects, many seman-
tic knowledge bases are readily available, for ex-
ample, WordNet, Spatial Date Transfer Standard,
Gene Ontology, etc. Among them, the most widely
used one is WordNet, which is organized by mean-
ings and developed at Princeton University. Sev-
eral methods computed word similarity by using
WordNet, such as the Lesk method in (Banerjee and
Pedersen, 2003), the lch method in (Leacock and
Chodorow, 1998)and the wup method in (Wu and
Palmer, 1994). Generally, although the knowledge-
based methods heavily depend on the knowledge
bases, they performed much better than the corpus-
based methods in most cases. Therefore, in our STS
system, we use a knowledge-based method to com-
pute word similarity.
The rest of this paper is organized as follows. Sec-
tion 2 describes our system. Section 3 presents the
results of our system.
2 System Description
Usually, a sentence is composed of some nouns,
verbs, adjectives, adverbs and/or some stop words.
We found that these words carry a lot of informa-
tion, especially the nouns and verbs. Although the
adjectives and adverbs also make contribution to the
semantic meaning of the sentence, they are much
weaker than the nouns and verbs. So we consider
to measure the sentence semantic similarities from
three aspects. We define the following three types of
similarity from two compared sentences to measure
575
the semantic similarity: (1) Noun Similarity to mea-
sure the similarity between the nouns from the two
compared sentences, (2) Verb Similarity to measure
the similarity between Verbs, (3) ADJ-ADV Simi-
larity to measure the similarity between the adjec-
tives and adverbs from each sentence. Besides the
semantic information similarity, we also found that
the structure of the sentences carry some informa-
tion which cannot be ignored. Therefore, we define
the last aspect of the sentence similarity as Word Or-
der Similarity. In the following we will introduce the
different components of our system.
2.1 POS
As a basic natural language processing technique,
part of speech tagging is to identify the part of
speech of individual words in the sentence. In or-
der to compute the three above semantic similari-
ties, we first identify the nouns, verbs, adjectives,
and adverbs in the sentence. Then we can calculate
the Noun Similarity, Verb Similarity and ADJ-ADV
Similarity from two sentences.
2.2 Semantic similarity between words
The word similarity measurement have important
impact on the performance of sentence similarity.
Currently, many lexical resources based approaches
perform comparatively well to compute semantic
word similarities. However, the exact resources they
are based are quite different. For example, some are
based on dictionary and/or thesaurus, and others are
based on WordNet.
WordNet is a machine-readable lexical database.
The words in Wordnet are classified into four cat-
egories, i.e., nouns, verbs, adjectives and adverbs.
WordNet groups these words into sets of syn-
onyms called synsets, provides short definitions, and
records the various semantic relations between these
synsets. The synsets are interlinked by means of
conceptual-semantic and lexical relations. Word-
Net alo provides the most common relationships
include Hyponym/Hypernym (i.e., is-a relationships)
and Meronym/Holonym (i.e., part-of relationships).
Nouns and verbs are organized into hierarchies
based on the hyponymy/hypernym relation between
synsets while adjectives and adverbs are not.
In this paper, we adopt the wup method in (Wu
and Palmer, 1994) to estimate the semantic similar-
ity between two words, which estimates the seman-
tic similarity between two words based on the depth
of the two words in WordNet and the depth of their
least common subsumer (LCS), where LCS is de-
fined as the common ancestor deepest in the taxon-
omy.
For example, given two words, w1 and w2, the
semantic similarity s(w1,w2) is the function of their
depth in the taxonomy and the depth of their least
common subsumer. If d1 and d2 are the depth of
w1 and w2 in WordNet, and h is the depth of their
least common subsumer in WordNet, the semantic
similarity can be written as:
s(w1, w2) =
2.0 ? h
d1 + d2
(1)
2.3 Word Sense Disambiguation
Word Sense Disambiguation (WSD) is to identify
the actual meaning of a word according to the con-
text. In our word similarity method, we take the
nearest meaning of two words into consideration
rather than their actual meaning. More impor-
tantly, the nearest meaning does not always repre-
sent the actual meaning. In our system, we used
a WSD algorithm proposed by (Ted Pedersen et
al.,2005), which computes semantic relatedness of
word senses using extended gloss overlaps of their
dictionary definitions. We utilize this WSD algo-
rithm for each sentence to get the actual meaning of
each word before computing the word semantic sim-
ilarity.
2.4 Semantic Similarity
We adopt a similar way to compute the three types of
semantic similarities. Here we take Noun Similarity
as an example.
Suppose sentence s1 and s2 are the two sentences
to be compared, s1 has a nouns while s2 has b nouns.
Then we get a ? b noun pairs and use the word sim-
ilarity method mentioned in section 2.2 to compute
the Noun Similarity of each noun pair. After that,
for each noun, we choose its highest score in noun
pairs as its similarity score. Then we use the formula
below to compute the Noun Similarity.
SimNoun =
(
?c
i=1 ni) ? (a + b)
2ab
(2)
576
where c represents the number of noun words in
sequence a and sequence b, c = min(a, b); ni rep-
resents the highest matching similarity score of i-th
word in the shorter sequence with respect to one of
the words in the longer sequence; and
?c
i=1 ni rep-
resents the sum of the highest matching similarity
score between the words in sequence a and sequence
b. Similarly, we can get SimV erb. Since there is no
Hyponym/Hypernym relation for adjectives and ad-
verbs in WordNet, we just compute ADJ-ADV Sim-
ilarity based on the frequency of overlap of simple
words.
2.5 Word Order Similarity
We believe that word order information also make
contributions to sentence similarity. In most cases,
the longer common sequence (LCS) the two sen-
tences have, the higher similarity score the sentences
get. For example the pair of sentences s1 and s2, we
remove all the punctuation from the sentences:
? s1: But other sources close to the sale said
Vivendi was keeping the door open to further
bids and hoped to see bidders interested in in-
dividual assets team up
? s2: But other sources close to the sale said
Vivendi was keeping the door open for further
bids in the next day or two
Since the length of the longest common sequence
is 14, we use the following formula to compute the
word order similarity.
SimWordOrder =
lengthofLCS
shorterlength
(3)
where the shorter length means the length of the
shorter sentence.
2.6 Overall Similarity
After we have the Noun Similarity, Verb Similar-
ity, ADJ-ADV Similarity and Word Order Similar-
ity, we calculate the Overall Similarity of two com-
pared sentences based on these four scores of simi-
larity. We combine them in the following way:
Simsent = aSimNoun + bSimV erb+
cSimADJ?ADV + dSimWordOrder
(4)
Where a, b, c and d are the coefficients which
denote the contribution of each aspect to the over-
all sentence similarity, For different data collections,
we empirically set different coefficients, for exam-
ple, for the MSR Paraphrase data, the four coeffi-
cients are set as 0.5, 0.3, 0.1, 0.1, because it is hard
to get the highest score 5 even when the two sen-
tences are almost the same meaning, We empirically
set a threshold, if the score exceeds the threshold we
set the score 5.
3 Experiment and Results on STS
Firstly, Stanford parser1 is used to parse each
sentence and to tag each word with a part of
speech(POS). Secondly, WordNet SenseRelate All-
Words2, a WSD tool from CPAN is used to disam-
biguate and to assign a sense for each word based on
the assigned POS.
We submitted three runs: run 1 with WSD, run 2
without WSD, run 3 removing stop words and with-
out WSD. The stoplist is available online3. Table 1
lists the performance of these three systems as well
as the baseline and the rank 1 results on STS task in
SemEval 2012.
We can see that run1 gets the best result, which
means WSD has improved the accuracy of sentence
similarity. Run3 gets better result than run2, which
proves that stop words do disturb the computation of
sentence similarity, removing them is a better choice
in our system.
4 Conclusion
In our work, we adopt a knowledge-based word sim-
ilarity method with WSD to measure the seman-
tic similarity between two sentences from four as-
pects: Noun Similarity, Verb Similarity, ADJ-ADV
Similarity and Word Order Similarity. The results
show that WSD improves the pearson coefficient at
some degree. However, our system did not get a
good rank. It indicates there still exists many prob-
lems such as wrong POS tag and wrong WSD which
might lead to wrong meaning of one word in a sen-
tence.
1http://nlp.stanford.edu/software/lex-parser.shtml
2http://search.cpan.org/Tedpederse/WordNet-SenseRelate-
AllWords-0.19
3http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-
smart-stop-list/english.stop
577
Table 1: STS system configuration and results on STS task.
Run ALL ALLnrm Mean MSRpar MSRvid SMTeur OnWN SMTnews
rank 1 .7790 .8579 .6773 .6830 .8739 .5280 .6641 .4937
baseline .3110 .6732 .4356 .4334 .2996 .4542 .5864 .3908
1 .4533 .7134 .4192 .4184 .5630 .2083 .4822 .2745
2 .4157 .7099 .3960 .4260 .5628 .1546 .4552 .1923
3 .4446 .7097 .3740 .3411 .5946 .1868 .4029 .1823
Acknowledgments
The authors would like to thank the organizers for
their invaluable support making STS a first-rank and
interesting international event.
References
Chukfong Ho, Masrah Azrifah Azmi Murad, Rabiah Ab-
dul Kadir, Shyamala C. Doraisamy. 2010. Word Sense
Disambiguation-based Sentence Similarity. In Proc.
COLING-ACL, Beijing.
Jin Feng, Yiming Zhou, Trevor Martin. 2008. Sen-
tence Similarity based on Relevance. Proceedings of
IPMU?08, Torremolinos.
Yuhua Li, David McLean, Zuhair A. Bandar, James D.
O?Shea, and Keeley Crockett. 2009. Sentence Simi-
larity Based on Semantic Nets and Corpus Statistics.
LIN LI, XIA HU, BI-YUN HU, JUN WANG, YI-MING
ZHOU. 2009. MEASURING SENTENCE SIMILAR-
ITY FROM DIFFERENT ASPECTS.
Islam Aminul and Diana Inkpen. 2008. Semantic Text
Similarity Using Corpus-Based Word Similarity and
String Similarity. ACM Transactions on Knowledge
Discovery from Data.
Banerjee and Pedersen. 2003. Extended gloss overlaps
as a measure of semantic relatedness. In Proceed-
ings of the Eighteenth International Joint Conference
on Artificial Intelligence (IJCAI-03), pages805C810,
Acapulco, Mexico.
Leacock and Chodorow. 1998. Combining local con-
text and WordNet similarity for word sense identifica-
tion. In Christiane Fellbaum, editor, WordNet: An
Electronic Lexical Database. The MIT Press, Cam-
bridge,MA.
Z.Wu and M.Palmer. 1994. Verbs semantics and
lexical selection. In Proceedings of the 32nd an-
nual meeting on Association for Computional Linguis-
tics,Morristown, NJ, USA.
Ted Pedersen, Satanjeev Banerjee, Siddharth Patward-
han. 2005. Maximizing Semantic Relatedness to Per-
form Word Sense Disambiguation.
578
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 124?131, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
ECNUCS: Measuring Short Text Semantic Equivalence Using Multiple
Similarity Measurements
Tian Tian ZHU
Department of Computer Science and
Technology
East China Normal University
51111201046@student.ecnu.edu.cn
Man LAN?
Department of Computer Science and
Technology
East China Normal University
mlan@cs.ecnu.edu.cn
Abstract
This paper reports our submissions to the
Semantic Textual Similarity (STS) task in
?SEM Shared Task 2013. We submitted three
Support Vector Regression (SVR) systems in
core task, using 6 types of similarity mea-
sures, i.e., string similarity, number similar-
ity, knowledge-based similarity, corpus-based
similarity, syntactic dependency similarity and
machine translation similarity. Our third sys-
tem with different training data and different
feature sets for each test data set performs the
best and ranks 35 out of 90 runs. We also sub-
mitted two systems in typed task using string
based measure and Named Entity based mea-
sure. Our best system ranks 5 out of 15 runs.
1 Introduction
The task of semantic textual similarity (STS) is to
measure the degree of semantic equivalence between
two sentences, which plays an increasingly impor-
tant role in natural language processing (NLP) ap-
plications. For example, in text categorization (Yang
and Wen, 2007), two documents which are more
similar are more likely to be grouped in the same
class. In information retrieval (Sahami and Heil-
man, 2006), text similarity improves the effective-
ness of a semantic search engine by providing in-
formation which holds high similarity with the input
query. In machine translation (Kauchak and Barzi-
lay, 2006), sentence similarity can be applied for
automatic evaluation of the output translation and
the reference translations. In question answering
(Mohler and Mihalcea, 2009), once the question and
the candidate answers are treated as two texts, the
answer text which has a higher relevance with the
question text may have higher probability to be the
right one.
The STS task in ?SEM Shared Task 2013 consists
of two subtasks, i.e., core task and typed task, and
we participate in both of them. The core task aims
to measure the semantic similarity of two sentences,
resulting in a similarity score which ranges from 5
(semantic equivalence) to 0 (no relation). The typed
task is a pilot task on typed-similarity between semi-
structured records. The types of similarity to be
measured include location, author, people involved,
time, events or actions, subject and description as
well as the general similarity of two texts (Agirre et
al., 2013).
In this work we present a Support Vector Re-
gression (SVR) system to measure sentence seman-
tic similarity by integrating multiple measurements,
i.e., string similarity, knowledge based similarity,
corpus based similarity, number similarity and ma-
chine translation metrics. Most of these similari-
ties are borrowed from previous work, e.g., (Ba?r et
al., 2012), (S?aric et al, 2012) and (de Souza et al,
2012). We also propose a novel syntactic depen-
dency similarity. Our best system ranks 35 out of
90 runs in core task and ranks 5 out of 15 runs in
typed task.
The rest of this paper is organized as follows. Sec-
tion 2 describes the similarity measurements used in
this work in detail. Section 3 presents experiments
and the results of two tasks. Conclusions and future
work are given in Section 4.
124
2 Text Similarity Measurements
To compute semantic textual similarity, previous
work has adopted multiple semantic similarity mea-
surements. In this work, we adopt 6 types of
measures, i.e., string similarity, number similarity,
knowledge-based similarity, corpus-based similar-
ity, syntactic dependency similarity and machine
translation similarity. Most of them are borrowed
from previous work due to their superior perfor-
mance reported. Besides, we also propose two syn-
tactic dependency similarity measures. Totally we
get 33 similarity measures. Generally, these simi-
larity measures are represented as numerical values
and combined using regression model.
2.1 Preprocessing
Generally, we perform text preprocessing before we
compute each text similarity measurement. Firstly,
Stanford parser1 is used for sentence tokenization
and parsing. Specifically, the tokens n?t and ?m are
replaced with not and am. Secondly, Stanford POS
Tagger2 is used for POS tagging. Thirdly, Natu-
ral Language Toolkit3 is used for WordNet based
Lemmatization, which lemmatizes the word to its
nearest base form that appears in WordNet, for ex-
ample, was is lemmatized as is, not be.
Given two short texts or sentences s1 and s2, we
denote the word set of s1 and s2 as S1 and S2, the
length (i.e., number of words) of s1 and s2 as |S1|
and |S2|.
2.2 String Similarity
Intuitively, if two sentences share more strings, they
are considered to have higher semantic similarity.
Therefore, we create 12 string based features in con-
sideration of the common sequence shared by two
texts.
Longest Common sequence (LCS). The widely
used LCS is proposed by (Allison and Dix, 1986),
which is to find the maximum length of a com-
mon subsequence of two strings and here the sub-
sequence need to be contiguous. In consideration of
the different length of two texts, we compute LCS
1http://nlp.stanford.edu/software/lex-parser.shtml
2http://nlp.stanford.edu/software/tagger.shtml
3http://nltk.org/
similarity using Formula (1) as follows:
SimLCS =
Length of LCS
min(|S1|, |S2|)
(1)
In order to eliminate the impacts of various forms
of word, we also compute a Lemma LCS similarity
score after sentences being lemmatized.
word n-grams. Following (Lyon et al, 2001), we
calculate the word n-grams similarity using the Jac-
card coefficient as shown in Formula (2), where p is
the number of n-grams shared by s1 and s2, q and r
are the number of n-grams not shared by s1 and s2,
respectively.
Jacc = pp + q + r (2)
Since we focus on short texts, here only n=1,2,3,4
is used in this work. Similar with LCS, we also com-
pute a Lemma n-grams similarity score.
Weighted Word Overlap (WWO). (S?aric et al,
2012) pointed out that when measuring sentence
similarity, different words may convey different con-
tent information. Therefore, we consider to assign
more importance to those words bearing more con-
tent information. To measure the importance of each
word, we use Formula (3) to calculate the informa-
tion content for each word w:
ic(w) = ln
?
w??C freq(w?)
freq(w) (3)
where C is the set of words in the corpus and
freq(w) is the frequency of the word w in the cor-
pus. To compute ic(w), we use the Web 1T 5-gram
Corpus4, which is generated from approximately
one trillion word tokens of text from Web pages.
Obviously, the WWO scores between two sen-
tences is non-symmetric. The WWO of s2 by s1 is
given by Formula (4):
Simwwo(s1, s2) =
?
w?S1?S2 ic(w)
?
w??S2 ic(w?)
(4)
Likewise, we can get Simwwo(s2, s1) score.
Then the final WWO score is the harmonic mean of
Simwwo(s1, s2) and Simwwo(s2, s1). Similarly, we
get a Lemma WWO score as well.
4http://www.ldc.upenn.edu/Catalog/docs/LDC2006T13
125
2.3 Knowledge Based Similarity
Knowledge based similarity approaches rely on
a semantic network of words. In this work
all knowledge-based word similarity measures are
computed based on WordNet. For word similarity,
we employ four WordNet-based similarity metrics:
the Path similarity (Banea et al, 2012); the WUP
similarity (Wu and Palmer, 1994); the LCH similar-
ity (Leacock and Chodorow, 1998); the Lin similar-
ity (Lin, 1998). We adopt the NLTK library (Bird,
2006) to compute all these word similarities.
In order to determine the similarity of sentences,
we employ two strategies to convert the word simi-
larity into sentence similarity, i.e., (1) the best align-
ment strategy (align) (Banea et al, 2012) and (2) the
aggregation strategy (agg) (Mihalcea et al, 2006).
The best alignment strategy is computed as below:
Simalign(s1, s2) =
(? +
?|?|
i=1 ?i) ? (2|S1||S2|)
|S1| + |S2|
(5)
where ? is the number of shared terms between s1
and s2, list ? contains the similarities of non-shared
words in shorter text, ?i is the highest similarity
score of the ith word among all words of the longer
text. The aggregation strategy is calculated as be-
low:
Simagg(s1, s2) =
?
w?S1(maxSim(w, S2) ? ic(w))
?
w?{S1} ic(w)
(6)
where maxSim(w,S2) is the highest WordNet-
based score between word w and all words of sen-
tence S2. To compute ic(w), we use the same cor-
pus as WWO, i.e., the Web 1T 5-gram Corpus. The
final score of the aggregation strategy is the mean of
Simagg(s1, s2) and Simagg(s2, s1). Finally we get
8 knowledge based features.
2.4 Corpus Based Similarity
Latent Semantic Analysis (LSA) (Landauer et al,
1997). In LSA, term-context associations are cap-
tured by means of a dimensionality reduction op-
eration performing singular value decomposition
(SVD) on the term-by-context matrix T , where T
is induced from a large corpus. We use the TASA
corpus5 to obtain the matrix and compute the word
5http://lsa.colorado.edu/
similarity using cosine similarity of the two vectors
of the words. After that we transform word similar-
ity to sentence similarity based on Formula (5).
Co-occurrence Retrieval Model (CRM) (Weeds,
2003). CRM is based on a notion of substitutabil-
ity. That is, the more appropriate it is to substitute
word w1 in place of word w2 in a suitable natural
language task, the more semantically similar they
are. The degree of substitutability of w2 with w1
is dependent on the proportion of co-occurrences of
w1 that are also the co-occurrences of w2, and the
proportion of co-occurrences of w2 that are also the
co-occurrences of w1. Following (Weeds, 2003), the
CRM word similarity is computed using Formula
(7):
SimCRM (w1, w2) =
2 ? |c(w1) ? c(w2)|
|c(w1)| + |c(w2)|
(7)
where c(w) is the set of words that co-occur with
w. We use the 5-gram part of the Web 1T 5-gram
Corpus to obtain c(w). If two words appear in one
5-gram, we will treat one word as the co-occurring
word of each other. To obtain c(w), we propose two
methods. In the first CRM similarity, we only con-
sider the word w with |c(w)| > 200, and then take
the top 200 co-occurring words ranked by the co-
occurrence frequency as its c(w). To relax restric-
tions, we also present an extended CRM (denoted
by ExCRM), which extends the CRM list that all w
with |c(w)| > 50 are taken into consideration, but
the maximum of |c(w)| is still set to 200. Finally,
these two CRM word similarity measures are trans-
formed to sentence similarity using Formula (5).
2.5 Syntactic Dependency Similarity
As (S?aric et al, 2012) pointed out that dependency
relations of sentences often contain semantic infor-
mation, in this work we propose two novel syntactic
dependency similarity features to capture their pos-
sible semantic similarity.
Simple Dependency Overlap. First we measure the
simple dependency overlap between two sentences
based on matching dependency relations. Stanford
Parser provides 53 dependency relations, for exam-
ple:
nsubj(remain ? 16, leader ? 4)
dobj(return ? 10, home ? 11)
126
where nsubj (nominal subject) and dobj (direct ob-
ject) are two dependency types, remain is the gov-
erning lemma and leader is the dependent lemma.
Two syntactic dependencies are considered equal
when they have the same dependency type, govern-
ing lemma, and dependent lemma.
Let R1 and R2 be the set of all dependency rela-
tions in s1 and s2, we compute Simple Dependency
Overlap using Formula (8):
SimSimDep(s1, s2) =
2 ? |R1 ? R2| ? |R1||R2|
|R1| + |R2|
(8)
Special Dependency Overlap. Several types of de-
pendency relations are believed to contain the pri-
mary content of a sentence. So we extract three roles
from those special dependency relations, i.e., pred-
icate, subject and object. For example, from above
dependency relation dobj, we can extract the object
of the sentence, i.e., home. For each of these three
roles, we get a similarity score. For example, to cal-
culate Simpredicate, we denote the sets of predicates
of two sentences as Sp1 and Sp2. We first use LCH to
compute word similarity and then compute sentence
similarity using Formula (5). Similarly, the Simsubj
and Simobj are obtained in the same way. In the end
we average the similarity scores of the three roles as
the final Special Dependency Overlap score.
2.6 Number Similarity
Numbers in the sentence occasionally carry similar-
ity information. If two sentences contain different
sets of numbers even though their sentence structure
is quite similar, they may be given a low similarity
score. Here we adopt two features following (S?aric
et al, 2012), which are computed as follow:
log(1 + |N1| + |N2|) (9)
2 ? |N1 ? N2|/(|N1| + |N2|) (10)
where N1 and N2 are the sets of all numbers in s1
and s2. We extract the number information from
sentences by checking if the POS tag is CD (cardinal
number).
2.7 Machine Translation Similarity
Machine translation (MT) evaluation metrics are de-
signed to assess whether the output of a MT sys-
tem is semantically equivalent to a set of reference
translations. The two given sentences can be viewed
as one input and one output of a MT system, then
the MT measures can be used to measure their se-
mantic similarity. We use the following 6 lexical
level metrics (de Souza et al, 2012): WER, TER,
PER, NIST, ROUGE-L, GTM-1. All these measures
are obtained using the Asiya Open Toolkit for Auto-
matic Machine Translation (Meta-) Evaluation6.
3 Experiment and Results
3.1 Regression Model
We adopt LIBSVM7 to build Support Vector Regres-
sion (SVR) model for regression. To obtain the op-
timal SVR parameters C, g, and p, we employ grid
search with 10-fold cross validation on training data.
Specifically, if the score returned by the regression
model is bigger than 5 or less than 0, we normalize
it as 5 or 0, respectively.
3.2 Core Task
The organizers provided four different test sets to
evaluate the performance of the submitted systems.
We have submitted three systems for core task, i.e.,
Run 1, Run 2 and Run 3. Run 1 is trained on all
training data sets with all features except the num-
ber based features, because most of the test data do
not contain number. Run 2 uses the same feature sets
as Run 1 but different training data sets for different
test data as listed in Table 1, where different training
data sets are combined together as they have simi-
lar structures with the test data. Run 3 uses different
feature sets as well as different training data sets for
each test data. Table 2 shows the best feature sets
used for each test data set, where ?+? means the fea-
ture is selected and ?-? means not selected. We did
not use the whole feature set because in our prelimi-
nary experiments, some features performed not well
on some training data sets, and they even reduced
the performance of our system. To select features,
we trained two SVR models for each feature, one
with all features and another with all features except
this feature. If the first model outperforms the sec-
ond model, this feature is chosen.
Table 3 lists the performance of these three sys-
tems as well as the baseline and the best results on
6http://nlp.lsi.upc.edu/asiya/
7http://www.csie.ntu.edu.tw/ cjlin/libsvm/
127
Test Training
Headline MSRpar
OnWN+FNWN MSRpar+OnWN
SMT SMTnews+SMTeuroparl
Table 1: Different training data sets used for each test data set
type Features Headline OnWN and FNWN SMT
LCS + + -
Lemma LCS + + -
String N-gram + 1+2gram 1gram
Based Lemma N-gram + 1+2gram 1gram
WWO + + +
Lemma WWO + + +
Path,WUP,LCH,Lin + + +
Knowledge +aligh
Based Path,WUP,LCH,Lin + + +
+ic-weighted
Corpus LSA + + +
Based CRM,ExCRM + + +
Simple Dependency + + +
Syntactic Overlap
Dependency Special Dependency + - +
Overlap
Number Number + - -
WER - + +
TER - + +
PER + + +
MT NIST + + -
ROUGE-L + + +
GTM-1 + + +
Table 2: Best feature combination for each data set
System Mean Headline OnWN FNWN SMT
Best 0.6181 0.7642 0.7529 0.5818 0.3804
Baseline 0.3639 0.5399 0.2828 0.2146 0.2861
Run 1 0.3533 0.5656 0.2083 0.1725 0.2949
Run 2 0.4720 0.7120 0.5388 0.2013 0.2504
Run 3 (rank 35) 0.4967 0.6799 0.5284 0.2203 0.3595
Table 3: Final results on STS core task
STS core task in ?SEM Shared Task 2013. For the
three runs we submitted to the task organizers, Run
3 performs the best results and ranks 35 out of 90
runs. Run 2 performs much better than Run 1. It in-
dicates that using different training data sets for dif-
ferent test sets indeed improves results. Run 3 out-
performs Run 2 and Run 1. It shows that our feature
selection process for each test data set does help im-
128
prove the performance too. From this table, we find
that different features perform different on different
kinds of data sets and thus using proper feature sub-
sets for each test data set would make improvement.
Besides, results on the four test data sets are quite
different. Headline always gets the best result on
each run and OnWN follows second. And results
of FNWN and SMT are much lower than Headline
and OnWN. One reason of the poor performance of
FNWN may be the big length difference of sentence
pairs. That is, sentence from WordNet is short while
sentence from FrameNet is quite longer, and some
samples even have more than one sentence (e.g. ?do-
ing as one pleases or chooses? VS ?there exist a
number of different possible events that may happen
in the future in most cases, there is an agent involved
who has to consider which of the possible events will
or should occur a salient entity which is deeply in-
volved in the event may also be mentioned?). As
a result, even though the two sentences are similar
in meaning, most of our measures would give low
scores due to quite different sentence length.
In order to understand the contributions of each
similarity measurement, we trained 6 SVR regres-
sion models based on 6 types on MSRpar data set.
Table 4 presents the Pearson?s correlation scores
of the 6 types of measurements on MSRpar. We
can see that the corpus-based measure achieves the
best, then the knowledge-based measure and the MT
measure follow. Number similarity performs sur-
prisingly well, which benefits from the property of
data set that MSRpar contains many numbers in sen-
tences and the sentence similarity depends a lot on
those numbers as well. The string similarity is not
as good as the knowledge-based, the corpus-based
and the MT similarity because of its disability of ex-
tracting semantic characteristics of sentence. Sur-
prisingly, the Syntactic dependency similarity per-
forms the worst. Since we only extract two features
based on sentence dependency, they may not enough
to capture the key semantic similarity information
from the sentences.
3.3 Typed Task
For typed task, we also adopt a SVR model for
each type. Since several previous similarity mea-
sures used for core task are not suitable for evalu-
ation of the similarity of people involved, time pe-
Features results
string 0.4757
knowledge-based 0.5640
corpus-based 0.5842
syntactic dependency 0.3528
number 0.5278
MT metrics 0.5595
Table 4: Pearson correlation of features of the six aspects
on MSRpar
riod, location and event or action involved, we add
two Named Entity Recognition (NER) based fea-
tures. Firstly we use Stanford NER8 to obtain per-
son, location and date information from the whole
text with NER tags of ?PERSON?, ?LOCATION?
and ?DATE?. Then for each list of entity, we get two
feature values using the following two formulas:
SimNER Num(L1NER, L2NER) =
min(|L1NER|, |L2NER|)
max(|L1NER|, |L2NER|)
(11)
SimNER(L1NER, L2NER) =
Num(equalpairs)
|L1NER| ? |L2NER|
(12)
where LNER is the list of one entity type from
the text, and for two lists of NERs L1NER and
L2NER, there are |L1NER| ? |L2NER| NER pairs.
Num(equalpairs) is the number of equal pairs.
Here we expand the condition of equivalence: two
NERs are considered equal if one is part of another
(e.g. ?John Warson? VS ?Warson?). Features and
content we used for each similarity are presented in
Table 5. For the three similarities: people involved,
time period, location, we compute the two NER
based features for each similarity with NER type of
?PERSON?, ?LOCATION? and ?DATE?. And for
event or action involved, we add the above 6 NER
feature scores as its feature set. The NER based sim-
ilarity used in description is the same as event or ac-
tion involved but only based on ?dcDescription? part
of text. Besides, we add a length feature in descrip-
tion, which is the ratio of shorter length and longer
length of descriptions.
8http://nlp.stanford.edu/software/CRF-NER.shtml
129
Type Features Content used
author string based (+ knowledge based for Run2) dcCreator
people involved NER based whole text
time period NER based whole text
location NER based whole text
event or action involved NER based whole text
subject string based (+ knowledge based for Run2) dcSubject
description string based, NER based,length dcDescription
General the 7 similarities above
Table 5: Feature sets and content used of 8 type similarities of Typed data
We have submitted two runs. Run 1 uses only
string based and NER based features. Besides fea-
tures used in Run 1, Run 2 also adds knowledge
based features. Table 6 shows the performance of
our two runs as well as the baseline and the best re-
sults on STS typed task in ?SEM Shared Task 2013.
Our Run 1 ranks 5 and Run 2 ranks 7 out of 15 runs.
Run 2 performed worse than Run 1 and the possible
reason may be the knowledge based method is not
suitable for this kind of data. Furthermore, since we
only use NER based features which involves three
entities for these similarities, they are not enough to
capture the relevant information for other types.
4 Conclusion
In this paper we described our submissions to the
Semantic Textual Similarity Task in ?SEM Shared
Task 2013. For core task, we collect 6 types of simi-
larity measures, i.e., string similarity, number sim-
ilarity, knowledge-based similarity, corpus-based
similarity, syntactic dependency similarity and ma-
chine translation similarity. And our Run 3 with dif-
ferent training data and different feature sets for each
test data set ranks 35 out of 90 runs. For typed task,
we adopt string based measure, NER based mea-
sure and knowledge based measure, our best system
ranks 5 out of 15 runs. Clearly, these similarity mea-
sures are not quite enough. For the core task, in our
future work we will consider the measures to eval-
uate the sentence difference as well. For the typed
task, with the help of more advanced IE tools to ex-
tract more information regarding different types, we
need to propose more methods to evaluate the simi-
larity.
Acknowledgments
The authors would like to thank the organizers and
reviewers for this interesting task and their helpful
suggestions and comments, which improved the fi-
nal version of this paper. This research is supported
by grants from National Natural Science Foundation
of China (No.60903093), Shanghai Pujiang Talent
Program (No.09PJ1404500), Doctoral Fund of Min-
istry of Education of China (No.20090076120029)
and Shanghai Knowledge Service Platform Project
(No.ZF1213).
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Lloyd Allison and Trevor I Dix. 1986. A bit-string
longest-common-subsequence algorithm. Information
Processing Letters, 23(5):305?310.
Carmen Banea, Samer Hassan, Michael Mohler, and
Rada Mihalcea. 2012. Unt: A supervised synergistic
approach to semantic text similarity. pages 635?642.
First Joint Conference on Lexical and Computational
Semantics (*SEM).
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. pages 435?440. First Joint Conference on Lex-
ical and Computational Semantics (*SEM).
Steven Bird. 2006. Nltk: the natural language toolkit. In
Proceedings of the COLING/ACL on Interactive pre-
sentation sessions, pages 69?72. Association for Com-
putational Linguistics.
130
System general author people time location event subject description mean
Best 0.7981 0.8158 0.6922 0.7471 0.7723 0.6835 0.7875 0.7996 0.7620
Baseline 0.6691 0.4278 0.4460 0.5002 0.4835 0.3062 0.5015 0.5810 0.4894
Run 1 0.6040 0.7362 0.3663 0.4685 0.3844 0.4057 0.5229 0.6027 0.5113
Run 2 0.6064 0.5684 0.3663 0.4685 0.3844 0.4057 0.5563 0.6027 0.4948
Table 6: Final results on STS typed task
Jose? Guilherme C de Souza, Matteo Negri, Trento Povo,
and Yashar Mehdad. 2012. Fbk: Machine trans-
lation evaluation and word similarity metrics for se-
mantic textual similarity. pages 624?630. First Joint
Conference on Lexical and Computational Semantics
(*SEM).
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings of
the main conference on Human Language Technol-
ogy Conference of the North American Chapter of the
Association of Computational Linguistics, pages 455?
462. Association for Computational Linguistics.
Thomas K Landauer, Darrell Laham, Bob Rehder, and
Missy E Schreiner. 1997. How well can passage
meaning be derived without using word order? a com-
parison of latent semantic analysis and humans. In
Proceedings of the 19th annual meeting of the Cog-
nitive Science Society, pages 412?417.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265?283.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th inter-
national conference on Machine Learning, volume 1,
pages 296?304. San Francisco.
Caroline Lyon, James Malcolm, and Bob Dickerson.
2001. Detecting short passages of similar text in large
document collections. In Proceedings of the 2001
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 118?125.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the na-
tional conference on artificial intelligence, volume 21,
page 775. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999.
Michael Mohler and Rada Mihalcea. 2009. Text-to-text
semantic similarity for automatic short answer grad-
ing. In Proceedings of the 12th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 567?575. Association for Computa-
tional Linguistics.
Mehran Sahami and Timothy D Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In Proceedings of the 15th interna-
tional conference on World Wide Web, pages 377?386.
ACM.
Frane S?aric, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic. 2012. Takelab: Systems
for measuring semantic text similarity. pages 441?
448. First Joint Conference on Lexical and Compu-
tational Semantics (*SEM).
Julie Elizabeth Weeds. 2003. Measures and applications
of lexical distributional similarity. Ph.D. thesis, Cite-
seer.
Zhibiao Wu and Martha Palmer. 1994. Verbs semantics
and lexical selection. In Proceedings of the 32nd an-
nual meeting on Association for Computational Lin-
guistics, pages 133?138. Association for Computa-
tional Linguistics.
Cha Yang and Jun Wen. 2007. Text categorization based
on similarity approach. In Proceedings of Interna-
tional Conference on Intelligence Systems and Knowl-
edge Engineering (ISKE).
131
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 408?413, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
ECNUCS: A Surface Information Based System Description of Sentiment
Analysis in Twitter in the SemEval-2013 (Task 2)
Tian Tian ZHU and Fang Xi ZHANG and Man LAN?
Department of Computer Science and Technology
East China Normal University
51111201046,51111201041@ecnu.edu.cn; mlan@cs.ecnu.edu.cn
Abstract
This paper briefly reports our submissions
to the two subtasks of Semantic Analysis in
Twitter task in SemEval 2013 (Task 2), i.e.,
the Contextual Polarity Disambiguation task
(an expression-level task) and the Message
Polarity Classification task (a message-level
task). We extract features from surface infor-
mation of tweets, i.e., content features, Micro-
blogging features, emoticons, punctuation and
sentiment lexicon, and adopt SVM to build
classifier. For subtask A, our system on twit-
ter data ranks 2 on unconstrained rank and on
SMS data ranks 1 on unconstrained rank.
1 Introduction
Micro-blogging today has become a very popular
communication tool among Internet users. Millions
of messages are appearing daily in popular web sites
that provide services for Micro-blogging and one
popularly known is Twitter1. Through the twitter
platform, users share either information or opin-
ions about personalities, politicians, products, com-
panies, events (Prentice and Huffman, 2008) etc. As
a result of the rapidly increasing number of tweets,
mining sentiments expressed in tweets has attracted
more and more attention, which is also one of the
basic analysis utility functions needed by various ap-
plications.
The task of Sentiment Analysis in Twitter is
to identify the sentiment of tweets and get a bet-
ter understanding of how sentiment is conveyed in
1http://www.twitter.com
tweets and texts, which consists of two sub-tasks,
i.e., the Contextual Polarity Disambiguation task
(an expression-level task) and the Message Polarity
Classification task (a message-level task). The con-
textual polarity disambiguation task (subtask A) is
to determine whether a given message containing a
marked instance of a word or a phrase is positive,
negative or neutral in that context. The message
polarity classification task (subtask B) is to decide
whether a given message is of positive, negative, or
neutral sentiment and for messages conveying both
a positive and negative sentiment, whichever is the
stronger sentiment should be chosen (Wilson et al,
2013). We participate in these two tasks.
In recent years, many researchers have proposed
methods to analyze sentiment in twitter. For exam-
ple, (Pak and Paroubek, 2010) used a Part of Speech
(POS) tagger on the tweets and found that some POS
taggers can help identify the sentiment of tweets.
They found that objective tweets often contain more
nouns than subjective tweets. However, subjective
tweets may carry more adjectives and adverbs than
objective tweets. Besides, (Davidov et al, 2010)
proved that emoticon and punctuation like excla-
mation mark are good features when distinguishing
the sentiment of tweets. In addition, some senti-
ment lexicons like SentiWordNet (Baccianella et al,
2010) and MPQA Subjectivity Lexicon (Wilson et
al., 2009) have been adopted to calculate the senti-
ment score of tweets (Zirn et al, 2011).
The rest of this paper is organized as follows. Sec-
tion 2 describes our approach for subtask 1, i.e.,
the Contextual Polarity Disambiguation task. Sec-
tion 3 describes our approach for subtask 2, i.e., the
408
message polarity classification task. Concluding re-
marks is in Section 4.
2 System Description of Contextual
Polarity Disambiguation
For the Contextual Polarity Disambiguation task,
we first extract features from multiple aspects, i.e.,
punctuation, emoticons, POS tags, instance length
and sentiment lexicon features. Then we adopt poly-
nomial SVM to build classification models. Accord-
ing to the definition of this task, the given instance
has been marked by a start position and an end posi-
tion rather than a whole tweet. So we first record the
frequency of the first three kinds of features in this
given instance. To avoid interference from the num-
ber of words in given instance, we then normalize
the feature values by the length of instance.
2.1 Preprocessing
Typically, most tweets contain informal language
expressions, with creative spelling and punctuation,
misspellings, slang, new words, URLs, and genre-
specific terminology and abbreviations, such as,
?RT? for ?re-tweet? and #hashtags, which are a type
of tagging for Twitter messages. Therefore, working
with these informal text genres presents challenges
for natural language processing beyond those typ-
ically encountered when working with more tradi-
tional text genres, such as newswire data. So we
perform text preprocessing in order to remedy as
many informal texts as possible. Firstly, we per-
form normalization to convert creative spelling and
misspelling into its right spelling. For example, any
repetition of more than 3 continuous letters are re-
duced back to 1 letter (e.g. ?noooo? is reduced to
?no?). In addition, according to the Internet slang
dictionary2, we convert each slang to its complete
form, for example, ?aka? is rewritten as ?also known
as?. After that, we use the Stanford parser3 for to-
kenization and the Stanford POS Tagger4 for POS
tagging. Finally, Natural Language Toolkit5 is used
for WordNet based Lemmatization.
2http://www.noslang.com
3http://nlp.stanford.edu/software/lex-parser.shtml
4http://nlp.stanford.edu/software/tagger.shtml
5http://nltk.org/
2.2 Features
2.2.1 Punctuation
Typically, punctuation may express user?s senti-
ment to a certain extent. For example, many excla-
mation marks (!) in tweet may indicate strong feel-
ings or high volume (shouting). Therefore, given
a marked instance, we record the frequency of the
following four types of punctuation: (1) exclama-
tion mark (!), (2) question mark (?), (3) double or
single quotation marks( ? and ??), (4) sum of the
above three punctuation. Then the punctuation fea-
ture value is normalized by the length of instance.
2.2.2 Emoticons
We create two features that capture the number of
positive and negative emoticons. Table 1 lists the
two types of emoticons. We also use the union of
the two emoticon sets as a feature. In total, we have
three emoticon features.
Positive Emoticons Negative Emoticons
:-) : ) :D :-D =) ;) :( :-( : ( ;(
;-) ; ) ;D ;-D (; :) ;-( ; ( ):
:-P ;-P XD (-: (-; :o) ;o) -/ :/ ;-/ ;/
:0) ;0) ? ? T T T0T ToT
Table 1: List of emoticons
2.2.3 POS
According to the finding of (Pak and Paroubek,
2010), POS taggers help to identify the sentiment
of tweets. Therefore, we record the frequency of
the following four POS features, i.e., noun (?NN?,
?NNP?, ?NNS? and ?NNPS? POS tags are grouped
into noun feature), verb (?VB?, ?VBD?, ?VBG?,
?VBN?, ?VBP? and ?VBZ? POS tags are grouped
into verb feature), adjective (?JJ?, ?JJR? and ?JJS?
POS tags are grouped into adjective feature) and
adverb (?RB?, ?RBR? and ?RBS? POS tags are
grouped into adverb feature). Then we normalize
them by the length of given instance.
2.2.4 Sentiment lexicon Features
For each word in a given instance, we use three
sentiment lexicons to identify its sentiment polarity
and calculate its sentiment weight, i.e., SentiWord-
Net (Baccianella et al, 2010), MPQA Subjectivity
Lexicon (Wilson et al, 2009) and an Unigram Lex-
icon made from the Large Movie Review Dataset
409
v1.0 (Maas et al, 2011). To calculate the sentiment
score for this instance, we use the following formula
to sum up the sentiment score of each word:
Senti(I) =
?
w?I
Num(w) ? Senti weight
Length(I)
(1)
where I represents the given instance and w repre-
sents each word in I . The Senti weight is calcu-
lated based on the word in the instance and the cho-
sen sentiment lexicon. That is, for each word in the
instance, we have different Senti weight values for
it since we use different sentiment lexicons. Below
we describe the calculation of Senti weight values
for a word in three sentiment lexicons. Note that
Num(w) is always 1 since most words appear one
time in a instance.
SentiWordNet. SentiWordNet is a lexical resource
for sentiment analysis, which assigns each synset of
WordNet (Stark and Riesenfeld, 1998) three senti-
ment scores: positivity, negativity, objectivity (e.g.
living#a#3, positivity: 0.5, negativity: 0.125, ob-
jectivity: 0.375), where sum of these three scores
is always 1. For one concept, if its positive score
and negative score are all 0, we treat it as objective
concept; otherwise, we treat it as subjective concept.
And we take the first sense as the concept of each
word.
We extract three features from SentiWordNet, i.e.,
SUBWordNet, POSWordNet and NEGWordNet.
The Senti weight of SUBWordNet records
whether a word is subjective. If it is subjective,
we set Senti weight as 1, otherwise 0. Similarly,
the Senti weight values of POSWordNet and
NEGWordNet indicate the positive score and the
negative score of the given word. Considering
some negation terms may reverse the sentiment
orientation of instance, we manually generate a
negation term list (e.g. ?not?, ?never?, etc.,) and if a
negation term appears in the instance, we switch the
POSWordNet to NEGWordNet and vice versa. Be-
sides, we adopt another feature to record the ratio of
POSWordNet/NEGWordNet. If the denominator is
0, i.e., NEGWordNet = 0, that means, the word has
the strongest positive sentiment orientation, then we
set 10*POSWordNet as its feature value.
MPQA. The MPQA Subjectivity Lexicon contains
about 8, 000 subjective words. Each word in the
lexicon has two types of sentiment strength: strong
subjective and weak subjective, and four kinds of
sentiment polarity: positive, negative, both (positive
and negative) and neutral. Therefore we calculate
three features from this lexicon, i.e., SUBMPQA,
POSMPQA and NEGMPQA. For the SUBMPQA
feature, if the word has strong or weak subjective,
we set its Senti weight as 1 or 0.5 accordingly.
For the POSMPQA (NEGMPQA) feature, we set
Senti weight as 1, or 0.5 or 0 if the word has strong
positive (negative), or weak positive (negative) or
neutral. We also reverse the sentiment orientation
of POSMPQA and NEGMPQA if a negation term
appears.
Unigram Lexicon. Unlike the above two lexicons
in themselves which provide sentiment polarity and
sentiment strength for each word, we also utilize the
third lexicon to calculate the sentiment information
statistically. Therefore we generate an unigram lex-
icon by ourselves from a large Movie Review data
set(Maas et al, 2011) which contains 25, 000 posi-
tive and 25, 000 negative movie reviews. We calcu-
late the Senti weight of each word appears in the
data set as the ratio of the frequency of this word
in positive reviews to that in negative reviews and
record this feature as SentiUL.
Clearly, since we use additional data set to de-
velop a sentiment lexicon which is used to generate
this SentiUL feature, this feature is worked with all
other features to train the unconstrained system.
2.2.5 Other features
In addition, we collect three other features: (1)
length of instance, (2) uppercase word (e.g. ?WTO?
or ?Machine Learning?), (3) URL. For the uppercase
word and URL features, we record the frequency of
them and then normalize them by the instance length
as well.
2.3 Experiment and Results
2.3.1 Classification Algorithm
We adopt LibSVM6 to build polynomial kernel-
based SVM classifiers. We have also tried linear ker-
nel but get no improvement. To obtain the optimal
parameters for SVM, such as c and g, we perform
grid search with 10-fold cross validation on training
6http://www.csie.ntu.edu.tw/ cjlin/libsvm/
410
data.
2.3.2 Results and Discussion
In section 2, we obtained 22 features in total. To
train the constrained model, we used the above de-
scribed 21 features (except SentiUL) and used all
above 22 features to train the unconstrained model.
We combined the provided training and develop-
ment data by the organizers as our final training
data. And we should apologize for our misunder-
standing of the definitions of the constrained and
unconstrained condition. As the official definition
of unconstrained model, participates are allowed
to add other data to expand the training data sets,
but our unconstrained model only adds one fea-
ture (SentiUL) which is got from other data set.
Therefore, we actually submitted two results of con-
strained model. But we still refer this model trained
on all features as unconstrained model for it ap-
peared in the unconstrained list of official results.
There are two kinds of test data: 4, 435 twitter in-
stances and 2, 334 SMS message instances. Table
2 list the F-score and averaged F-score of positive,
negative and neutral class of each test data set.
On one hand, from the table we can see that
whether on constrained or unconstrained model, the
results on twitter data are slightly better than those
of SMS data. However, this difference is not signifi-
cant. This indicates that the model trained on twitter
data performs well on SMS data. And it also shows
that twitter data and SMS data are linguistically sim-
ilar with each other in nature. On the other hand, we
find that on each test data set, there is little differ-
ence between the constrained model and the uncon-
strained model, which indicates the SentiUL feature
does not have discriminating power by itself. How-
ever, since we had not used other labeled or unla-
beled data to extend the training data set, we cannot
draw a conclusion on this. Besides, our results con-
tain no neutral items even though the classifier we
used is multivariate. One reason may be the neutral
instances in training data is too sparse for the classi-
fier to learn.
On twitter data, our system ranks 2 under un-
constrained model and ranks 10 under constrained
model. On SMS data, our system ranks first under
unconstrained model and ranks 7 under constrained
model.
3 System Description of Message Polarity
Classification
Unlike the previous subtask, the Message Polarity
classification task focuses on the whole tweet rather
than a marked sequence of given instance. Firstly,
we perform text preprocessing as Task A. Besides
the previous described features, we also extract fol-
lowing features.
3.1 Features
3.1.1 Micro-blogging features
We adopted three tweet domain-specific features,
i.e., #hashtags, @USERS, URLs. We calculate the
frequency of the three features and normalize them
by the length of instance.
3.1.2 n-gram features
We used unigrams to capture the content of
tweets.
3.2 Classification Algorithm
We adopted two different classifiers in preliminary
experiments, i.e., maximum entropy and SVM. We
used the Mallet tool (McCallum, 2002) to perform
Maximum Entropy classification and LibSVM7 with
a linear kernel, where the default setting is adopted
in all experiments.
3.3 Results on Training Data
In the first experiment, we used only content fea-
tures and LibSVM classifier to do our experiments.
The results were listed in Table 3. From Table 3,
we found that the system with unigram without re-
moving stop words performs the best. The possible
reason was that Microblogs are always short (con-
strained in 140 words) and removing stop words
would cause information missing in such a short
text. In addition, although bigrams improved the
performance to some extern, they added the feature
space many more and might affect other features. So
in our final systems, we used only unigram feature
and did not remove stop words.
In the second experiment, we compared all fea-
tures described before with two learning algorithms.
The results were shown in Table 4, where 1 indi-
cates unigram, 2 indicates micro-blog, 3 indicates
7http://www.csie.ntu.edu.tw/ cjlin/libsvm/
411
System F-pos F-neg F-neu average F(pos and neg)
twitter-constrained 0.8506 0.7390 0.0 0.7948
twitter-unconstrained 0.8561 0.7468 0.0 0.8015
SMS-constrained 0.7727 0.7611 0.0 0.7669
SMS-unconstrained 0.7645 0.7824 0.0 0.7734
Table 2: Results of our systems on subtask A test data
features F-pos F-neg F-neu average F(pos and neg) acc(%)
unigrams 0.6356 0.3381 0.7122 0.4869 63.75
unigrams(remove stop words) 0.6046 0.3453 0.6988 0.4750 62.13
bigrams 0.5186 0.0196 0.6625 0.2691 55.85
unigrams+bigrams 0.6234 0.3724 0.7043 0.4979 63.18
Table 3: Results of our systems on on subtask B training data using content features
features F-pos F-neg F-neu average F(pos and neg) acc(%)
MaxEnt SVM MaxEnt SVM MaxEnt SVM MaxEnt SVM MaxEnt SVM
1 0.6178 0.6356 0.3696 0.3381 0.6848 0.7122 0.4937 0.4869 61.56 63.75
1+2 0.6403 0.6339 0.4207 0.4310 0.6990 0.7184 0.5305 0.5324 63.75 64.89
1+2+3 0.6328 0.6512 0.4051 0.4371 0.6975 0.7232 0.5190 0.5442 63.18 65.75
1+2+3+4 0.6488 0.6593 0.4587 0.4481 0.7083 0.7288 0.5538 0.5537 64.89 66.41
2+3+4 0.5290 0.5201 0.2897 0.2643 0.6503 0.6411 0.4093 0.3922 55.85 54.80
Table 4: Results of our systems on subtask B training data using all features and two learning algorithms
punctuation, 4 indicates sentiment lexicon features.
From Table 4, the best performance was obtained
by using all these features. Since the performance
of Maximum Entropy and SVM in terms of F-score
was comparable to each other, we finally chose SVM
since it achieved a better accuracy than MaxEnt.
3.4 Results on Test Data
We combined the provided training and develop-
ment data by the organizers as our final training data.
There were two kinds of test data: 3, 813 tweets and
2, 094 SMS messages . Table 5 listed the results of
our final systems on the tweet and SMS data sets by
using all above described features and SVM algo-
rithm.
From Table 5, on one hand, we can see that the
overall performance of SMS test data is inferior to
twitter data, for the reason may be that the domain
of features are all based on twitter data, and maybe
not quite suitable for SMS data. However, this dif-
ferent is not significant. On the other hand, we also
can find that there is no obvious distinction between
the constrained and the unconstrained model on each
test data.Also from Table 5, the F-score for positive
instances is higher than negative instances, and it
is interesting that most of other participants?systems
results show the same consequence. One of the rea-
son may be the positive instance in training data are
more than negative instances both in training data
and test data.
Our result on twitter message is 0.5842 , while
on SMS is 0.5477. Compared with the highest av-
erage F-score 0.6902 in twitter data and 0.6848 in
SMS data, our system does not perform very well.
On the one hand , pre-processing was roughly , then
features extracted were not suited in classification
stage. On the other hand, in classification stage all
parameters were default when used LibSVM. These
might cause low performance. In future, we may
overcome the insufficient described above and take
hashtags? sentiment inclination and the source files
of URLs into consideration to enhance the perfor-
mance.
412
System F-pos F-neg F-neu average F(pos and neg)
twitter-constrained 0.6671 0.4338 0.7124 0.5505
twitter-unconstrained 0.6775 0.4908 0.7204 0.5842
SMS-constrained 0.5796 0.4846 0.7801 0.5321
SMS-unconstrained 0.5818 0.5137 0.7612 0.5477
Table 5: Results of our systems on subtask B test data
4 Conclusion
In this work we extracted features from four aspects,
including surface information of twitters and senti-
ment lexicons like SentiWordNet and MPQA Lexi-
con. On the contextual polarity disambiguation task,
our system ranks 2 on twitter (unconstrained) rank
and ranks 1 on SMS (unconstrained) rank.
Acknowledgements
The authors would like to thank the organizers and
reviewers for this interesting task and their helpful
suggestions and comments, which improves the fi-
nal version of this paper. This research is supported
by grants from National Natural Science Foundation
of China (No.60903093), Shanghai Pujiang Talent
Program (No.09PJ1404500), Doctoral Fund of Min-
istry of Education of China (No. 20090076120029)
and Shanghai Knowledge Service Platform Project
(No. ZF1213).
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. Sentiwordnet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10), Valletta, Malta, may. European Lan-
guage Resources Association (ELRA).
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages 241?249. Association for Computational Lin-
guistics.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan
Huang, Andrew Y. Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 142?150, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
corpus for sentiment analysis and opinion mining. In
Proceedings of LREC, volume 2010.
Sara Prentice and Ethan Huffman. 2008. Social medias
new role in emergency management. Idaho National
Laboratory, pages 1?5.
Michael M Stark and Richard F Riesenfeld. 1998. Word-
net: An electronic lexical database. In Proceedings of
11th Eurographics Workshop on Rendering. Citeseer.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis.
Computational linguistics, 35(3):399?433.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan
Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013.
Semeval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the 7th International Workshop on
Semantic Evaluation. Association for Computational
Linguistics.
Ca?cilia Zirn, Mathias Niepert, Heiner Stuckenschmidt,
and Michael Strube. 2011. Fine-grained sentiment
analysis with structural features. In Proceedings of
the 5th international Joint conference on natural Lan-
guage Processing (iJcnLP-2011), volume 167.
413
