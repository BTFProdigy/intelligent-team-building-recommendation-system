A context-dependent algorithm for generating
locative expressions in physically situated environments
John D. Kelleher & Geert-Jan M. Kruijff
Language Technology Lab
German Research Center for Artificial Intelligence (DFKI)
Saarbru?cken, Germany
{kelleher,gj}@dfki.de
Abstract
This paper presents a framework for generating
locative expressions. The framework addresses
the issue of combinatorial explosion inherent in
the construction of relational context models by:
(a) contextually defining the set of objects in the
context that may function as a landmark, and (b)
sequencing the order in which spatial relations are
considered using a cognitively motivated hierar-
chy of relations.
1 Introduction
Our long-term goal is to develop embodied conversational
robots that are capable of natural, fluent visually situated di-
alog with one or more interlocutors. An inherent aspect of
visually situated dialog is reference to objects located in the
physical environment. In this paper, we present a computa-
tional framework for the generation of a spatial locative ex-
pressions in such contexts.
In the simplest form of locative expression, a prepositional
phrase modifies a noun phrase to explicitly specify the loca-
tion of the object. (1) is an example of the type of locative
we focus on generating. In this example, the book is the sub-
ject of the expression and the table is the object. Following
[Langacker, 1987], we use the terms trajector and landmark
to respectively denote the subject and the object of a locative
expression: the location of the trajector is specified relative to
the landmark by the semantics of the preposition.
(1) a. the book [subject] on the table [object]
Generating locative expressions is part of the general field
of generating referring expressions (GRE). Most GRE algo-
rithms deal with the same problem: given a domain descrip-
tion and a target object generate a description of the target
object that distinguishes it from the other objects in the do-
main. The term distractor objects is used to describe the
objects in the context excluding the trajector that at a given
point in processing fulfil the description of the target object
that has been generated. The description generated is said to
be distinguishing when the set of distractor objects is empty.
Several GRE algorithms have addressed the issue of gen-
erating locative expressions [Dale and Haddock, 1991; Ho-
racek, 1997; Gardent, 2002; Krahmer and Theune, 2002;
Varges, 2004]. However, all these algorithms assume the
GRE component has access to a predefined scene model.
For an embodied conversational robot functioning in dynamic
partially known environments this assumption is a serious
drawback. If an agent wishes to generate a contextually ap-
propriate reference it cannot assume the availability of a do-
main model, rather it must dynamically construct one. How-
ever, constructing a model containing all the relationships be-
tween all the entities in the domain is prone to combinatorial
explosion, both in terms of the number of objects in the con-
text (the location of each object in the scene must be checked
against all the other objects in the scene) and number of inter-
object spatial relations (as a greater number of spatial rela-
tions will require a greater number of comparisons between
each pair of objects.1 Moreover, the context free a priori con-
struction of such an exhaustive scene model is cognitively im-
plausible. Psychological research indicates that spatial rela-
tions are not preattentively perceptually available [Treisman
and Gormican, 1988]. Rather, their perception requires atten-
tion [Logan, 1994; 1995]. These findings point to subjects
constructing contextually dependent reduced relational scene
models, rather than an exhaustive context free model.
Contributions In this paper we present a framework for
generating locative expressions. This framework addresses
the issue of combinatorial explosion inherent in relational
scene model construction by incrementally creating a series
of reduced scene models. Within each scene model only one
spatial relations is considered and only a subset of objects
are considered as candidate landmarks. This reduces both the
number of relations that must be computed over each object
pair and the number of object pairs. The decision as to which
relations should be included in each scene model is guided
by a cognitively motivated hierarchy of spatial relations. The
set of candidate landmarks in a given scene is dependent on
the set of objects in the scene that fulfil the description of the
1In English, the vast majority of spatial locatives are binary, some
notable exceptions include: between, amongst etc. However, we will
not deal with these exceptions in this paper
target object and the relation that is being considered.
Overview In ?2 we present some background data relevant
to our discussion. In ?3 we present our GRE framework. In
?4 we illustrate the framework with a worked example and
expand on some of the issues relevant to the framework. We
end with conclusions.
2 Data
When one considers that the English lexicon of spatial prepo-
sitions numbers above eighty members (not considering com-
pounds such as right next to) [Landau, 1996], the combina-
torial aspect of relational scene model construction becomes
apparent. It should be noted that for our purposes, the sit-
uation is somewhat ameliorated by the fact that a distinc-
tion can be made between static and dynamic prepositions:
static prepositions primarily2 denote the location of an object,
dynamic prepositions primarily denote the path of an object
[Jackendoff, 1983; Herskovits, 1986], see (2). However, even
focusing exclusively on the set of static prepositions does not
remove the combinatorial issues effecting the construction of
a scene model.
(2) a. the tree is behind [static] the house
b. the man walked across [dynamic] the road
In general, the set of static prepositions can be decomposed
into two sets called topological and projective. Topological
prepositions are the category of prepositions referring to a
region that is proximal to the landmark; e.g., at, near, etc.
Often, the distinctions between the semantics of the differ-
ent topological prepositions is based on pragmatic contraints,
for example the use of at licences the trajector to be in con-
tact with the landmark, by contrast the use of near does not.
Projective prepositions describe a region projected from the
landmark in a particular direction, the specification of the di-
rection is dependent on the frame of reference being used;
e.g., to the right of, to the left of, etc.
The semantics of static prepositions exhibit both qualita-
tive and quantitative properties. The qualitative aspect of their
semantics is evident when they are used to denote an object
by contrasting its location with the distractor objects location.
Taking Figure 1 as a visual context the locative expression the
circle on the left of the square exhibits the contrastive seman-
tics of a projective preposition. Only one of the circles in the
scene is located in the region to the right of the square. Tak-
ing Figure 2 as a visual context the locative expression the
circle near the black square illustrates the contrastive seman-
tics of a topological preposition. Again, of the two circles in
the scene only one of them may be appropriately described
as being near the black square, the other circle is more ap-
propriately described as being near the white square. The
quantitative aspect of the semantics of static prepositions is
evident when they denote an object using a relative scale. In
the context provided by Figure 3 the locative the circle to the
right of the square exhibits the relative semantics of a projec-
tive preposition. Although both the circles are located to the
2Static prepositions can be used in dynamic contexts, e.g. the
man ran behind the house, and dynamic prepositions can be used in
static ones, e.g. the tree lay across the road.
right of the square it is possible to adjudicate between them
based on their location in the region. The relative semantics
of a topological preposition can also be illustrated using Fig-
ure 3. A description such as the circle near the square could
be applied to either circle if the other circle was not present.
However, when both are present it is possible to interpret the
reference based on their relative proximity to the landmark
the square.
Figure 1: Visual context used to illustrate the contrastive se-
mantics of projective prepositions.
Figure 2: Visual context used to illustrate the contrastive se-
mantics of topological prepositions.
Figure 3: Visual context used to illustrate the relative seman-
tics of topological and projective prepositions.
3 Approach
The approach we adopt to generating locative expressions in-
volves extending the incremental algorithm [Dale and Reiter,
1995]. The motivation for this is the polynomial complexity
of the incremental algorithm. The incremental algorithm iter-
ates through the properties of the target and for each property
computes the set of distractor objects for which (a) the con-
junction of the properties selected so far, and (b) the current
property hold. A property is added to the list of selected prop-
erties if it reduces the size of the distractor object set. The al-
gorithm succeeds when all the distractors have been ruled out,
it fails if all the properties have been processed and there are
still some distractor objects. The algorithm can be refined by
ordering the checking of properties according to fixed prefer-
ences, e.g. first a taxonomic description of the target, second
an absolute property such as colour, third a relative property
such as size. [Dale and Reiter, 1995] also stipulate that the
type description of the target should be included in the de-
scription even if its inclusion does not distinguish the target
from any of the distractors, see Algorithm 1.
However, before applying the incremental algorithm we
must construct a context model within which we can check
whether or not the description generated distinguishes the tar-
get object. In order to constrain the combinatorial issues in-
herent in relational scene model construction we construct
a series of reduced scene models, rather than constructing
one complex exhaustive model. This construction process is
driven by a hierarchy of spatial relations and the partition-
ing of the context model into objects that may and may not
function as landmarks. These two components are developed
Algorithm 1 The Basic Incremental Algorithm
Require: T = target object; D = set of distractor objects.
Initialise: P = {type, colour, size}; DESC = {}
for i = 0 to |P | do
if |D| 6= 0 then
D? = {x : x ? D,P i(x) = P i(T )}
if |D?| < |D| then
DESC = DESC ? P i(T )
D = {x : x ? D,P i(x) = P i(T )}
end if
else
Distinguishing description generated
if type(x) 6? DESC then
DESC = DESC ? type(x)
end if
return DESC
end if
end for
Failed to generate distinguishing description
return DESC
in the next two sections. In ?3.1 we develop the hierarchy
of spatial relations and in ?3.2 we develop a classification of
landmarks and use these groupings to create a definition of
a distinguishing locative description. In ?3.3 we present the
generation algorithm that integrates these components.
3.1 Cognitive Ordering of Contexts
Psychological research indicates that spatial relations are not
preattentively perceptually available [Treisman and Gormi-
can, 1988]. Rather, their perception requires attention [Lo-
gan, 1994; 1995]. These findings point to subjects construct-
ing contextually dependent reduced relational scene models,
rather than an exhaustive context free model. Mimicking this,
we have developed an approach to context model construc-
tion that attempts to constrain the combinatorial explosion
inherent in the construction of relational context models by
incrementally constructing a series of reduced context mod-
els. Each context model focuses on a different spatial relation.
The ordering of the spatial relations is based on the cognitive
load of interpreting the relation. In this section, we motivate
and develop the ordering of relations used.
It seems reasonable to asssume that it takes less effort to
describe one object than two. Consequently, following the
Principle of Minimal Cooperative Effort [Clark and Wilkes-
Gibbs, 1986], a speaker should only use a locative expression
when they cannot create a distinguishing description of the
target object using a simple feature based approach. More-
over, the Principle of Sensitivity [Dale and Reiter, 1995]
states that when producing a referring expression, the speaker
should prefer features which the hearer is known to be able
to interpret and perceive. This points to a preference, due
to cognitive load, towards descriptions that distinguish an ob-
ject using purely physical and easily perceivable features over
descriptions that use spatial expressions. Psycholinguistic
results support this preference [van der Sluis and Krahmer,
2004].
Similarly, we can distinguish between the cognitive loads
of processing different forms of spatial relations. In com-
paring the cognitive load associated with different spatial re-
lations it is important to recognize that they are represented
and processed at several levels of abstraction. For example,
the geometric level, where metric properties are dealt with,
the functional level, where the specific properties of spatial
entities deriving from their functions in space are considered,
and the pragmatic level, which gathers the underlying prin-
ciples that people use in order to discard wrong relations or to
deduce more information [Edwards and Moulin, 1998]. Our
discussion is grounded at the geometric level of representa-
tion and processing.
Focusing on static prepositions, it is reasonable to
Figure 4: Cognitive load of
reference forms
propose that topologi-
cal prepositions have a
lower perceptual load
than projective prepo-
sitions, due to the
relative ease of per-
ceiving two objects
that are close to each
other and the complex
processing required to
handle frame of refer-
ence ambiguity [Carlson-Radvansky and Irwin, 1994;
Carlson-Radvansky and Logan, 1997]. Figure 4 lists these
preferences, with further distinctions among features: objects
type is the easiest to process, before absolute gradable
predicates (e.g. color), which is still easier than relative
gradable predicates (e.g. size) [Dale and Reiter, 1995].
This topological versus projective preference can be fur-
ther refined if we consider the contrastive and relative uses
of these relations noted in ?2. Perceiving and interpreting a
constrastive use of a spatial relation is computationally easier
than judging a relative use. Finally, within the set of projec-
tive prepositions, psycholinguistic data indicates a perceptu-
ally based ordering of the relations: above/below are easier
to percieve and interpret than in front of /behind which in turn
are easier than to the right of /to the left of [Bryant et al,
1992; Gapp, 1995].
In sum, we would like to propose the following ordering
of spatial relations:
1. topological contrastive
2. topological relative
3. projective constrastive [above/below, front/back/,
right/left]
4. projective relative [above/below, front/back, right/left]
For each level of this hierarchy we require a computational
model of the semantics of the relation at that level that acco-
modates both contrastive and relative representations. In ?2
we noted that the distinctions between the semantics of the
different topological prepositions is often based on functional
and pragmatic issues.3 Currently, however, more psycholin-
guistic data is required to distinguish the cognitive load asso-
ciated with the different topological prepositions. We use the
3See inter alia [Talmy, 1983; Herskovits, 1986; Vandeloise,
1991; Fillmore, 1997; Garrod et al, 1999] for more discussion on
these differences
model of topological proximity developed in [Kelleher and
Kruijff, 2005] to model all the relations at this level. Using
this model we can define the extent of a region proximal to
an object. If the trajector or one of the distractor objects is
the only object within the region of proximity around a given
landmark this is taken to model a contrastive use of a topo-
logical relation relative to that landmark. If the landmark?s
region of proximity contains more than one object from the
trajector and distractor object set then it is a relative use of
a topological relation. We handle the issue of frame of ref-
erence ambiguity and model the semantics of projective pre-
postions using the framework developed in [Kelleher and van
Genabith, 2005]. Here again, the contrastive-relative distinc-
tion is dependent on the number of objects within the region
of space defined by the preposition.
3.2 Landmarks and Distinguishing Descriptions
In order to use a locative expression an object in the context
must be selected to function as the landmark. An implicit
assumption in selecting an object to function as a landmark is
that the hearer can easily identify and locate the object within
the context. A landmark can be: the speaker (3)a, the hearer
(3)b, the scene (3)c, an object in the scene (3)d, or a group of
objects in the scene (3)e.4
(3) a. the ball on my right [speaker]
b. the ball to your left [hearer]
c. the ball on the right [scene]
d. the ball to the left of the box [an object in the
scene]
e. the ball in the middle [group of objects]
Currently, new empirical research is required to see if
there is a preference order between these landmark cate-
gories. Intuitively, in most situations, either of the inter-
locutors are ideal landmarks because the speaker can natu-
rally assume that the hearer is aware of the speaker?s location
and their own. Focusing on instances where an object in the
scene is used as a landmark, several authors [Talmy, 1983;
Landau, 1996; Gapp, 1995] have noted a trajector-landmark
asymmetry: generally, the landmark object is more perma-
nently located, larger, and taken to have greater geometric
complexity. These characteristics are indicative of salient ob-
jects and empirical results support this correlation between
object salience and landmark selection [Beun and Cremers,
1998]. However, the salience of an object is intrinsically
linked to the context it is embedded in. For example, in the
context provided by Figure 5 the ball has a relatively high
salience, because it is a singleton, despite the fact that it is
smaller and geometrically less complex than the other fig-
ures. Moreover, in this context, the ball is the only object in
the scene that can function as a landmark without recourse to
using the scene itself or a grouping of objects in the scene.
Clearly, deciding which objects in a given context are suit-
able to function as landmarks is a complex and contextually
dependent process. Some of the factors effecting this decision
4See [Gorniak and Roy, 2004] for further discussion on the use
of spatial extrema of the scene and groups of objects in the scene as
landmarks
Figure 5: Visual context used to illustrate the relative seman-
tics of topological and projective prepositions.
are object salience and the functional relationships between
objects. However, one basic constraint on landmark selection
is that the landmark should be distinguishable from the trajec-
tor. For example, given the context in Figure 5 and all other
factors being equal, using a locative such as the man to the left
of the man would be much less helpful than using the man to
the right of the ball. Following this observation, we treat an
object as a candidate landmark if the trajector object can be
distinguished from it using the basic incremental algorithm,
Algorithm 1.5 Furthermore, a trajector landmark is a mem-
ber of the candidate landmark set that stands in relation to
the trajector and a distractor landmark is a member of the
candidate landmark set that stands in relation to a distractor
object under the relation being considered. Using these cat-
egories of landmark we can define a distinguishing locative
description as a locative description where there is trajector
landmark that can be distinguished from all the members of
the set of distractor landmarks under the relation used in the
locative.
We can illustrate these different categories of landmark
using Figure 6 as the visual context. In this context, if
W1 is taken as the target object, the distractor set equals
{T1,B1,W2,B2}. Running the basic incremental algorithm
would generate the description white block. This distin-
guishes W1 from T1, B1 and B2 but not from W2. Conse-
quently, the set of candidate landmarks equals {T1,B1,B2}.
If we now create a context model for the relation near the set
of trajector landmarks would be {T1,B1} and the set of dis-
tractor landmarks would be {B1,B2}. Obviously, B1 cannot
be distinguished from all the distractor landmarks as it cannot
be distinguished from itself. As a result, B1 cannot function
as the landmark for a distinguishing locative description for
W1 using the relation near. However, T1 can be distinguished
from the distractor landmarks B1 and B2 by its type, triangle.
So the white block near the triangle would be considered a
distinguishing description.
Figure 6: Visual context used to illustrate the different cate-
gories of landmark.
5As noted by one of our reviewers, one unwanted effect of this
definition of a landmark is that it precludes the generation of descrip-
tions that use a landmark that are themselves distinguished using a
locative expression. For example, the block to the right of the block
which has a ball on it.
3.3 Algorithm
The basic approach is to try to generate a distinguishing de-
scription using the standard incremental algorithm. If this
fails, we divide the context into three components:
the trajector: the target object,
the distractor objects: the objects that match the descrip-
tion generated for the target object by the standard in-
cremental algorithm,
the set of candidate landmarks: the objects that do not
match the description generated for the target object by
the standard incremental algorithm.
We then begin to iterate through the hierarchy of relations
and for each relation we create a context model that defines
the set of trajector and distractor landmarks. Once a context
model has been created we iterate through the trajector land-
marks (using a salience ordering if there is more than one)6
and try to create a distinguishing locative description. A dis-
tinguishing locative description is created by using the basic
incremental algorithm to distinguish the trajector landmark
from the distractor landmarks. If we succeed in generating a
distinguishing locative description we return the description
and stop processing. Algorithm 2 lists the steps in the algo-
rithm.
Algorithm 2 The Locative Algorithm
Require: T = target object; D = set of distractor objects; R = hier-
archy of relations.
DESC = Basic-Incremental-Algorithm(T,D)
if DESC 6= Distinguishing then
create CL the set of candidate landmarks
CL = {x : x 6= T,DESC(x) = false}
for i = 0 to |R| do
create a context model for relation Ri consisting of TL the
set of trajector landmarks and DL the set of distractor land-
marks
DL = {z : z ? CL,Ri(D, z) = true}
TL = {y : y ? CL, y 6? DL,Ri(T, y) = true}
for j = 0 to |TL| by salience(TL) do
LANDDESC = Basic-Incremental-Algorithm(TLj , DL)
if LANDDESC = Distinguishing then
Distinguishing locative generated
return {DESC,Ri,LANDDESC}
end if
end for
end for
end if
FAIL
If we cannot create a distinguishing locative description we
are faced with a choice of: (1) iterate on to the next relation
6We model both visual and linguistic salience. Visual salience
is computed using a modified version of the visual saliency algo-
rithm described in [Kelleher and van Genabith, 2004]. Discourse
salience is computed based on recency of mention as defined in [Ha-
jicova?, 1993] except we represent the maximum overall salience in
the scene as 1, and use 0 to indicate object is not salient. We in-
tegrate these two components by summing them and dividing the
result by 2.
in the hierarchy, (2) create an embedded locative description
that distinguishes the landmark. Currently, we prefer option
(1) over (2), preferring the dog to the right of the car over
the dog near the car to the right of the house. However, the
algorithm can generate these longer embedded descriptions if
needed. This is done by replacing the call to the basic incre-
mental algorithm for the trajector landmark object with a call
to the whole locative expression generation algorithm, with
the trajector landmark as the target object and the set of dis-
tractor landmarks as the distractor objects. Algorithm 3 lists
the steps in the recursive version of the algorithm.
Algorithm 3 The Recursive Locative Algorithm
Require: T = target object; D = set of distractor objects; R = hier-
archy of relations.
DESC = Basic-Incremental-Algorithm(T,D)
if DESC 6= Distinguishing then
create CL the set of candidate landmarks
CL = {x : x 6= T,DESC(x) = false}
for i = 0 to |R| do
create a context model for relation Ri consisting of TL the
set of trajector landmarks and DL the set of distractor land-
marks
DL = {z : z ? CL,Ri(D, z) = true}
TL = {y : y ? CL, y 6? DL,Ri(T, y) = true}
for j = 0 to |TL| by salience(TL) do
LANDDESC =
Recursive-Locative-Algorithm(T=TLj ,D=DL,R)
if LANDDESC = Distinguishing then
Distinguishing locative generated
return {DESC,Ri,LANDDESC}
end if
end for
end for
end if
FAIL
For both versions of the locative algorithm an important
consideration is the issue of infinite regression. As noted by
[Dale and Haddock, 1991] a compositional GRE system may,
in certain contexts, generate an infinite description by trying
to distinguish the landmark in terms of the trajector and the
trajector in terms of the landmark, see (4). However, this in-
finite recursion can only occur if the context is not modified
between calls to the algorithm. This issue does not effect Al-
gorithm 2 because each call to the algorithm results in the
domain being partitioned into those objects that can and can-
not be used as landmarks. One effect of this partitioning is a
reduction in the number of object pairs that relations must be
computed for. However, and more importantly for this dis-
cussion, another consequence of this partitioning is that the
process of creating a distinguishing description for a land-
mark is carried out in a context that is a subset of the context
the trajector description was generated in. The distractor set
used during the generation of a landmark description is the
set of distractor landmarks. This minimally excludes the tra-
jector object, since by definition the landmark objects cannot
fulfill the description of the trajector generated by the basic
incremental algorithm. This naturally removes the possibility
for the algorithm to distinguish a landmark using its trajector.
Figure 7: A visual scene and the topological analsis of R1
and R2
(4) the bowl on the table supporting the bowl on the table
supporting the bowl ...
4 Discussion
We can illustrate the framework using the visual context pro-
vided by the scene on the left of Figure 7. This context con-
sists of two red boxes R1 and R2 and two blue balls B1 and
B2. Imagine that we want to refer to B1. We begin by call-
ing the locative incremental algorithm, Algorithm 2. This
in turn calls the basic incremental algorithm, Algorithm 1,
which will return the property ball. However, this is not suf-
ficient to create a distinguishing description as B2 is also a
ball. In this context the set of candidate landmarks equals
{R1,R2} and the first relation in the hierarchy is topological
proximity, which we model using the algorithm developed
in [Kelleher and Kruijff, 2005]. The image on the right of
Figure 7 illustrates the analysis of the scene using this frame-
work: the green region on the left defines the area deemed to
be proximal to R1, and the yellow region on the right defines
the area deemed to be proximal to R2. It is evident that B1
is in the area proximal to R1, consequently R1 is classified as
a trajector landmark. As none of the distractors (i.e., B2) are
located in a region that is proximal to a candidate landmark
there are no distractor landmarks. As a result when the basic
incremental algorithm is called to create a distinguishing de-
scription for the trajector landmark R1 it will return box and
this will be deemed to be a distinguishing locative descrip-
tion. The overall algorithm will then return the vector {ball,
proximal, box} which would result in the realiser generating
a reference of the form: the ball near the box.
The relational hierarchy used by the framework has some
commonalities with the relational subsumption hierarchy pro-
posed in [Krahmer and Theune, 2002]. However, there are
two important differences between them. First, an implica-
tion of the subsumption hierarchy proposed in [Krahmer and
Theune, 2002] is that the semantics of the relations at lower
levels in the hierarchy are subsumed by the semantics of their
parent relations. For example, in the portion of the subsump-
tion hierarchy illustrated in [Krahmer and Theune, 2002] the
relation next to subsumes the relations left of and right of.
By contrast, the relational hierarchy developed here is based
solely on the relative cognitive load associated with the se-
mantics of the spatial relations and makes no claims as to the
semantic relationships between the semantics of the spatial
relations. Secondly, [Krahmer and Theune, 2002] do not use
their relational hierarchy to guide the construction of domain
models.
By providing a basic contextual definition of a landmark
we are able to partition the context in an appropriate manner.
This partitioning has two advantages:
1. it reduces the complexity of the context model con-
struction, as the relationships between the trajector and
the distractor objects or between the distractor objects
themselves do not need to be computed;
2. the context used during the generation of a landmark
description is always a subset of the context used for
a trajector (as the trajector, its distractors and the other
objects in the domain that do not stand in relation to
the trajector or distractors under the relation being con-
sidered are excluded). As a result the framework avoids
the issue of infinite recusion. Furthermore, the trajector-
landmark relationship is automatically included as a
property of the landmark as its feature based descrip-
tion need only distinguish it from objects that stand in
relation to one of the distractor objects under the same
spatial relationship.
.
In future work we will focus on extending the framework
to handle some of the issues effecting the incremental algo-
rithm, see [van Deemter, 2001]. For example, generating
locative descriptions containing negated relations, conjunc-
tions of relations and involving sets of objects (sets of trajec-
tors and landmarks).
5 Conclusions
In this paper we have argued that an if an embodied conver-
sational agent functioning in dynamic partially known envi-
ronments wishes to generate contextually appropriate locative
expressions it must be able to construct a context model that
explicitly marks the spatial relations between objects in the
scene. However, the construction of such a model is prone
to the issue of combinatorial explosion both in terms of the
number of objects in the context (the location of each object
in the scene must be checked against all the other objects in
the scene) and number of inter-object spatial relations (as a
greater number of spatial relations will require a greater num-
ber of comparisons between each pair of objects.
We have presented a framework that address this issue by:
(a) contextually defining the set of objects in the context that
may function as a landmark, and (b) sequencing the order
in which spatial relations are considered using a cognitively
motivated hierarchy of relations. Defining the set of objects in
the scene that may function as a landmark reduces the number
of object pairs that a spatial relation must be computed over.
Sequencing the consideration of spatial relations means that
in each context model only one relation needs to be checked
and in some instances the agent need not compute some of the
spatial relations, as it may have succeeded in generating a dis-
tinguishing locative using a relation earlier in the sequence.
A further advantage of our approach stems from the parti-
tioning of the context into those objects that may function as
a landmark and those that may not. As a result of this parti-
tioning the algorithm avoids the issue of infinite recursion, as
the partitioning of the context stops the algorithm from dis-
tinguishing a landmark using its trajector.
References
[Beun and Cremers, 1998] R.J. Beun and A. Cremers. Object ref-
erence in a shared domain of conversation. Pragmatics and Cog-
nition, 6(1/2):121?152, 1998.
[Bryant et al, 1992] D.J. Bryant, B. Tversky, and N. Franklin. In-
ternal and external spatial frameworks representing described
scenes. Journal of Memory and Language, 31:74?98, 1992.
[Carlson-Radvansky and Irwin, 1994] L.A. Carlson-Radvansky
and D. Irwin. Reference frame activation during spatial term
assignment. Journal of Memory and Language, 33:646?671,
1994.
[Carlson-Radvansky and Logan, 1997] L.A. Carlson-Radvansky
and G.D. Logan. The influence of reference frame selection on
spatial template construction. Journal of Memory and Language,
37:411?437, 1997.
[Clark and Wilkes-Gibbs, 1986] H. Clark and D. Wilkes-Gibbs.
Referring as a collaborative process. Cognition, 22:1?39, 1986.
[Dale and Haddock, 1991] R. Dale and N. Haddock. Generating
referring expressions involving relations. In Proceeding of the
Fifth Conference of the European ACL, pages 161?166, Berlin,
April 1991.
[Dale and Reiter, 1995] R. Dale and E. Reiter. Computational inter-
pretations of the Gricean maxims in the generation of referring
expressions. Cognitive Science, 19(2):233?263, 1995.
[Edwards and Moulin, 1998] G. Edwards and B. Moulin. Towards
the simulation of spatial mental images using the vorono?? model.
In P. Oliver and K.P. Gapp, editors, Representation and process-
ing of spatial expressions, pages 163?184. Lawrence Erlbaum
Associates., 1998.
[Fillmore, 1997] C. Fillmore. Lecture on Deixis. CSLI Publica-
tions, 1997.
[Gapp, 1995] K.P. Gapp. Angle, distance, shape, and their relation-
ship to projective relations. In Proceedings of the 17th Confer-
ence of the Cognitive Science Society, 1995.
[Gardent, 2002] C Gardent. Generating minimal definite descrip-
tions. In Proceedings of the 40th International Confernce of the
Association of Computational Linguistics (ACL-02), pages 96?
103, 2002.
[Garrod et al, 1999] S. Garrod, G. Ferrier, and S. Campbell. In and
on: investigating the functional geometry of spatial prepositions.
Cognition, 72:167?189, 1999.
[Gorniak and Roy, 2004] P. Gorniak and D. Roy. Grounded seman-
tic composition for visual scenes. Journal of Artificial Intelli-
gence Research, 21:429?470, 2004.
[Hajicova?, 1993] E. Hajicova?. Issues of sentence structure and dis-
course patterns. In Theoretical and Computational Linguistics,
volume 2, Charles University, Prague, 1993.
[Herskovits, 1986] A Herskovits. Language and spatial cognition:
An interdisciplinary study of prepositions in English. Studies
in Natural Language Processing. Cambridge University Press,
1986.
[Horacek, 1997] H. Horacek. An algorithm for generating referen-
tial descriptions with flexible interfaces. In Proceedings of the
35th Annual Meeting of the Association for Computational Lin-
guistics, Madrid, 1997.
[Jackendoff, 1983] R. Jackendoff. Semantics and Cognition. Cur-
rent Studies in Linguistics. The MIT Press, 1983.
[Kelleher and Kruijff, 2005] J. Kelleher and G.J. Kruijff. A
context-dependent model of proximity is physically situated en-
vironments. In Proceedings of the 2nd ACL-SIGSEM Workshop
on The Linguistic Dimensions of Prepositions and their Use in
Computational Linguistics Formalisms and Applications, 2005.
[Kelleher and van Genabith, 2004] J. Kelleher and J. van Genabith.
A false colouring real time visual salency algorithm for refer-
ence resolution in simulated 3d environments. AI Review, 21(3-
4):253?267, 2004.
[Kelleher and van Genabith, 2005] J. Kelleher and J. van Genabith.
In press: A computational model of the referential semantics of
projective prepositions. In P. Saint-Dizier, editor, Dimensions
of the Syntax and Semantics of Prepositions. Kluwer Academic
Publishers, Dordrecht, The Netherlands, 2005.
[Krahmer and Theune, 2002] E. Krahmer and M. Theune. Efficient
context-sensitive generation of referring expressions. In K. van
Deemter and R. Kibble, editors, Information Sharing: Reference
and Presupposition in Language Generation and Interpretation.
CLSI Publications, Standford, 2002.
[Landau, 1996] B Landau. Multiple geometric representations of
objects in language and language learners. In P Bloom, M. Pe-
terson, L Nadel, and M. Garrett, editors, Language and Space,
pages 317?363. MIT Press, Cambridge, 1996.
[Langacker, 1987] R.W. Langacker. Foundations of Cognitive
Grammar: Theoretical Prerequisites, volume 1. Standford Uni-
versity Press, 1987.
[Logan, 1994] Gordon D. Logan. Spatial attention and the appre-
hension of spatial realtions. Journal of Experimental Psychology:
Human Perception and Performance, 20:1015?1036, 1994.
[Logan, 1995] G.D. Logan. Linguistic and conceptual control of vi-
sual spatial attention. Cognitive Psychology, 12:523?533, 1995.
[Talmy, 1983] L. Talmy. How language structures space. In H.L.
Pick, editor, Spatial orientation. Theory, research and applica-
tion, pages 225?282. Plenum Press, 1983.
[Treisman and Gormican, 1988] A. Treisman and S. Gormican.
Feature analysis in early vision: Evidence from search assyme-
tries. Psychological Review, 95:15?48, 1988.
[van Deemter, 2001] K. van Deemter. Generating referring expres-
sions: Beyond the incremental algorithm. In 4th Int. Conf. on
Computational Semantics (IWCS-4), Tilburg, 2001.
[van der Sluis and Krahmer, 2004] I van der Sluis and E Krahmer.
The influence of target size and distance on the production of
speech and gesture in multimodal referring expressions. In Pro-
ceedings of International Conference on Spoken Language Pro-
cessing (ICSLP04), 2004.
[Vandeloise, 1991] C. Vandeloise. Spatial Prepositions: A Case
Study From French. The University of Chicago Press, 1991.
[Varges, 2004] S. Varges. Overgenerating referring expressions in-
volving relations and booleans. In Proceedings of the 3rd Inter-
national Conference on Natural Language Generation, Univer-
sity of Brighton, 2004.
Applying Computational Models of Spatial
Prepositions to Visually Situated Dialog
John D. Kelleher?
Dublin Institute of Technology
Fintan J. Costello??
University College Dublin
This article describes the application of computational models of spatial prepositions to visually
situated dialog systems. In these dialogs, spatial prepositions are important because people
often use them to refer to entities in the visual context of a dialog. We first describe a generic
architecture for a visually situated dialog system and highlight the interactions between the
spatial cognition module, which provides the interface to the models of prepositional semantics,
and the other components in the architecture. Following this, we present two new computational
models of topological and projective spatial prepositions. The main novelty within these models
is the fact that they account for the contextual effect which other distractor objects in a visual
scene can have on the region described by a given preposition. We next present psycholinguistic
tests evaluating our approach to distractor interference on prepositional semantics, and illustrate
how these models are used for both interpretation and generation of prepositional expressions.
1. Introduction
A growing number of computer applications share a visualized (virtual or real) space
with the user, for example graphic design programs, computer games, navigation aids,
robot systems, and so forth. If these systems are to be equipped with dialog interfaces,
they must be able to participate in visually situated dialog. Visually situated dialog is
spoken from a particular point of view within a physical or simulated context. From
theoretical linguistic and cognitive perspectives, visually situated dialog systems are
interesting as they provide ideal testbeds for investigating the interaction between
language and vision. From a human?computer interaction (HCI) perspective, visually
situated dialog systems promise many advantages to users interacting with these
systems. In this article we describe computational models for the interpretation and
generation of visually situated locative expressions involving topological and projective
spatial prepositions.
Contributions An inherent aspect of visually situated dialog is reference to objects
in the physical environment in which the dialog occurs. People often use locative
? School of Computing, Dublin Institute of Technology, Kevin Street, Dublin 8, Ireland. E-mail:
john.kelleher@comp.dit.ie.
?? School of Computer Science and Informatics, University College Dublin, Belfield, Dublin 4, Ireland.
E-mail: fintan.costello@ucd.ie.
Submission received: 31 July 2006; revised submission received: 30 March 2007; accepted for publication:
4 July 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 2
expressions, in particular spatial prepositions, to pick out objects in the visual envi-
ronment. In this article we present computational models of the semantics of spatial
prepositions and illustrate how these models can be used in a visually situated dialog
system for reference resolution and generation. These models are designed to handle
reference resolution and generation in complex visual environments containing multi-
ple objects, and to account for the contextual influence which the presence of multiple
objects has on the semantics of spatial prepositions. In this our models move beyond
other accounts, which typically do not model the contextual influence of other objects
on spatial semantics. Because most real-world visual scenes are complex and contain
multiple objects, our models for the semantics of spatial prepositions are important for
visually situated dialog systems intended to operate usefully in the real world.
Overview We begin in Section 2 by describing some terminology we use when
discussing locative expressions. In Section 3 we present an abstract architecture for
a visually situated dialog system and, using this architecture, illustrate how the spa-
tial reasoning component of the architecture interacts with the other components of
the system. In Section 4 we review psycholinguistic data on the semantics of spatial
prepositions. Section 5 reviews previous computational models of spatial prepositional
semantics. Section 6 presents our computational models accounting for the semantics of
spatial prepositions and the influence of visual context on those semantics, and Section 7
presents psycholinguistic evaluation of these models. Section 8 presents applications of
the models in implemented systems. Section 8.1 presents an application of our models
to the interpretation of locative expressions, based on Kelleher, Kruijff, and Costello
(2006), and Section 8.2 presents algorithms which use these models to generate locative
expressions to identify objects in visual scenes from Kelleher and Kruijff (2006).
2. Terminology
Our computational models are designed to interpret and generate locative expressions
involving spatial prepositions. The term locative expression describes ?an expression
involving a locative prepositional phrase together with whatever the phrase modifies
(noun, clause, etc.)? (Herskovits 1986, page 7). In this article we use the term target
(T) to refer to the object that is being located by a locative expression and the term
landmark1 (L) to refer to the object relative to which the target?s location is described;
see Example (1). We will use the term distractor to describe any object in the visual
context that is neither the landmark nor the target.
Example 1
[The man]T near [the table]L.
The English lexicon of spatial prepositions numbers above 80 members (not consid-
ering compounds such as right next to) (Landau 1996). Within this set a distinction can
be made between static and dynamic prepositions: static prepositions primarily2 denote
1 There is a wealth of terms used in the literature describing locative expressions. The terms local object,
figure object, and trajector are all equivalent to our term target while the terms reference object, ground,
and relatum are equivalent to our term landmark.
2 Static prepositions can be used in dynamic contexts, for example, the man ran behind the house, and
dynamic prepositions can be used in static ones, for example, the tree lay across the road.
272
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Figure 1
Architecture of a visually situated dialog system.
the location of an object, dynamic prepositions primarily denote the path of an object
(Jackendoff 1983; Herskovits 1986), see Examples (2) and (3).
Example 2
The tree is [behind]static the house.
Example 3
The man walked [across]dynamic the road.
In general, the set of static prepositions can be decomposed into two sets called
topological and projective. Topological prepositions are the category of prepositions
referring to a region that is proximal to the landmark; for example, at, near. Often, the
distinctions between the semantics of the different topological prepositions is based
on pragmatic constraints, for example the use of at licenses the target to be in contact
with the landmark, while the use of near does not. Projective prepositions describe a
region projected from the landmark in a particular direction, with the specification of
the direction dependent on the frame of reference3 being used; for example, to the right
of, to the left of.
3. Visually Situated Dialog System Architecture
In this section we present an abstract implementation-independent architecture for a
visually situated dialog system and highlight the role played by spatial reasoning in the
functioning of the system. In particular, we describe howmodels of spatial prepositional
semantics are important for reference resolution and generation.
The distinguishing characteristic of a visually situated dialog system is that the
system has the ability to visually perceive the environment in which a dialog is situated.
Consequently, these systems use both visual and linguistic contextual information to
understand user commands and to generate linguistic descriptions of the environment.
Figure 1 illustrates the visual dialog system architecture we will describe. The arrows in
the figure represent data flows through the system; the boxes are the main information
processing components.
3 In the context of projective prepositions, a frame of reference consists of six half-line axes with a shared
origin; in English, these axes are usually labelled front, back, right, left, above, below. In English, three
different frames of reference are distinguished: absolute, intrinsic, and viewer-centered. Interestingly
however, although the use of a tripartite system is common in European languages, this is not universal,
with many languages taking different approaches here. We direct the interested reader to Levinson (1996,
2003) and Levelt (1996) for further discussion on frames of reference.
273
Computational Linguistics Volume 35, Number 2
Figure 2
Example input and output data from a vision subsystem.
There are two information inputs into this system: the vision subsystem and the
speech interpretation pipeline. The vision subsystem directly updates the system?s
representation of the visual context. The basic requirements for the vision subsystem
are that it is able to detect and categorize the objects in the visual context and can
provide geometric positioning information for each visible object. Figure 2 illustrates
the analysis that a vision subsystem may generate for a given scene.
The speech interpretation pipeline begins with speech recognition. This module
takes a speech utterance from the user and creates a string representation of it. The
parser uses this string to construct a structured representation of the input. Parsers
range in function from wide-coverage syntactic focused parsers, such as Cahill et al?s
(2004) probabilistic Lexical-Functional Grammar (LFG) parser, to narrow coverage se-
mantic based parsers, for example the CoSy parser (Kruijff, Kelleher, and Hawes 2006).
Figure 3 illustrates the types of analyses produced by these different types of parsers for
the input string is the box near the ball? The parse tree on the left was generated using a
probabilistic wide-coverage LFG parser.4 The parse tree provides a syntactic analysis of
the input string.
Generally, parsers developed for interactive dialog systems integrate semantic, as
well as syntactic, information in their grammars. In these parsers the elements in the
lexicon and grammar are based on an analysis of the entities and relations of the
specific domain the system is designed for. These parsers sacrifice coverage for depth of
analysis. For a dialog system, the advantage of this deeper analysis is that the semantic
information in the parser?s output can be used by the dialog manager to relate the input
to the rest of the dialog. The parse structure on the right of Figure 3 illustrates the type of
semantically rich representation that an interactive dialog system parser might produce
(this particular representation was generated by the CoSy parser).
The CoSy parser uses a Combinatory Categorial Grammar that represents linguistic
meaning using an ontologically rich sorted relational structure (Baldridge and Kruijff
4 A demo of the parser is available at: http://lfg-demo.computing.dcu.ie/lfgparser.html. The parser
also provides detailed LFG f-structures for input strings.
274
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Figure 3
Example parse structures for the string is the box near the ball?
2002, 2003). Within this representation the statement b2:phys-objmeans that the referent
b2 is of type phys-obj (i.e., a physical object as defined by the ontology the grammar in-
dexes). The semantic contribution of the prepositional phrase near the ball is represented
by the <Location> structure and its subcomponents. This structure describes a locative
prepositional phrase containing a static preposition that locates the referent b2 in the
region r that is proximal to the landmark described by the <Anchor> subcomponent. It
should be noted that the syntactic and semantic representation of prepositions within
grammars is an area of ongoing research (see Gawron 1986; Tseng 2000; Beermann and
Hellan 2004). The analysis presented here of the prepositional phrase near the ball is
intended to illustrate some of the semantic features that prepositions may introduce
into a grammar and is not intended as a comprehensive account of how prepositions
should be grammatically represented.
The final stage in the interpretation pipeline is to categorize how the utterance
relates to the current dialog context. This categorization is driven by the dialog manager
and involves interpreting an utterance as a dialog act (Bunt 1994; Carletta et al 1997;
Klein 1999). One of the important tasks in this process is resolving the references
in the input. Consequently, the dialog manager may invoke the reference resolution
component. Reference resolution is one of two functions in the architecture where
spatial reasoning plays an important role. From a computational perspective, reference
resolution involves two main tasks:
1. Creating and maintaining a model of what the system considers as
mutual knowledge (this model should contain all the objects that are
available for reference and their properties)
2. Matching the representation introduced by a given referring expression
to an element (or elements) in the set of possible referents
In a visually situated dialog a referring expression may be exophoric (i.e., denote
an object in the visual context which has not yet been mentioned in the dialog) or it
may be anaphoric (i.e., access a representation of a previous referring expression in the
dialog context). People often use the spatial location of an object, described using spatial
prepositions, when making exophoric references. As a result, in order to interpret these
references the system must have access to models of the semantics of the prepositions
275
Computational Linguistics Volume 35, Number 2
Figure 4
The mapping performed by the spatial reasoning module from qualitative to geometric
representations during the interpretation of a locative expression.
used. In this architecture this access is provided through the spatial reasoning compo-
nent. Figure 4 illustrates the translation between the qualitative, parser-generated, and
the geometric, vision subsystem?generated representations that must be performed in
order to interpret a spatial locative expression.
At different stages during a dialog the dialog manager may recognize that the
system needs to generate a response to the last input utterance. For example, the
utterance may have been a question, such as where is x? or which x?. In such cases,
the dialog manager informs the content planner of this. The role of the content planner
is to determine the semantic content that should be included in the system?s output,
rather than the linguistic realization of this content. Indeed, the content planner may
generate a logical representation closer to the parse structure on the right of Figure 3
than to a natural language description.
Generating referring expressions (GRE) is a key stage in content planning. GRE is
the second function in the architecture where spatial reasoning plays an important role.
The function of the GRE component is to determine the set of properties that distinguish
a particular target object from the other objects in the scene. For example, in response
to a question such as which x? the GRE component may determine that a color and type
description is sufficient to distinguish the target object, resulting in an answer such as the
blue x being linguistically realized. However, it may be the case that the location of the
target in the scene is the only way to distinguish it. In such cases, the GRE component
needs access to computational models of the spatial prepositions if it is to determine
which spatial relation is most suitable. Figure 5 illustrates the translation from a geo-
metric to qualitative representation that is performed during the GRE process by the
spatial reasoning module when a locative description is being generated by the system.
Once the content planning and GRE processes have been completed, the realizer
determines a surface linguistic form in which this content can be conveyed. Finally, the
speech synthesis systems generate the speech output for the linguistic string created by
the realizer.
4. Psycholinguistic Data on Spatial Prepositions
Spatial reasoning is a complex activity that involves at least two levels of process-
ing: a geometric level where metric, topological, and projective properties are handled
276
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Figure 5
The mapping performed by the spatial reasoning module from geometric to qualitative to
representations during the generation of a locative description.
(Herskovits 1986), and a functional level where the normal function of an entity affects
the spatial relationships attributed to it in a context (Coventry and Garrod 2004).
There has been much experimental work done on spatial reasoning and language.
Some of this work has focused on functional aspects of prepositional semantics (e.g.,
Hayward and Tarr 1995; Coventry 1998; Garrod, Ferrier, and Campbell 1999), and some
on geometric factors (Gapp 1995; Logan and Sadler 1996; Regier and Carlson 2001). In
this article we are primarily concerned with the geometric semantics of prepositions
and, consequently, our review will focus on the experimental data that addresses geo-
metric factors. We will begin by reviewing the experimental data describing topological
spatial prepositions. Following this, we will then review data relating to projective
prepositions.
Topological prepositions denote a region that is proximal to a landmark. Subse-
quently we discuss previous psycholinguistic experiments, focusing on how contex-
tual factors such as distance, size, and salience may affect proximity. We also present
examples showing that the location of other objects in a scene may interfere with the
acceptability of a proximal description to locate a target relative to a landmark.
Logan and Sadler (1996) examined the semantics of several spatial prepositions. In
their experiments, a human subject was shown sentences of the form the X is [relation]
the O, each with a picture of a spatial configuration of an O in the center of an invisible
7 ? 7 cell grid, and an X in one of the 48 surrounding positions. The subject then had
to rate how well the sentence described the picture, on a scale from 1 (bad) to 9 (good).
Figure 6 gives the mean goodness rating for the relation ?near to? as a function of the
position occupied by X (Logan and Sadler 1996). It is clear from Figure 6 that ratings
diminish as the distance between X and O increases, but also that even at the extremes
of the grid the ratings were still above 1 (minimum rating).
Besides distance there are also other factors that determine the applicability of a
proximal relation. For example, given prototypical size, the region denoted by near the
building is larger than that of near the apple. Moreover, an object?s salience could influence
the determination of the proximal region associatedwith it; as with size, themore salient
an object is the larger the proximal region associated with it (Gapp 1994).
Finally, the two scenes in Figure 7 show interference as a contextual factor. For the
scene on the left we can use the blue box is near the black box to describe object (c). This
seems inappropriate in the scene on the right. Placing an object (d) beside (b) appears
277
Computational Linguistics Volume 35, Number 2
Figure 6
A 7 ? 7 cell grid with mean goodness ratings for the relation the X is near O as a function of the
position occupied by X.
to interfere with the appropriateness of using a proximal relation to locate (c) relative to
(b), even though the absolute distance between (c) and (b) has not changed.
There are several important features that are evident from these data. First, given a
context, subjects have the ability to grade the applicability of a spatial relation. Logan
and Sadler (1996) introduced the term spatial template to describe the representation of
the regions of acceptability associated with a preposition. A spatial template is centered
on the landmark and identifies for each point in its space the acceptability of the
spatial relationship between the landmark and the target appearing at that point being
described by the preposition. Second, there is empirical evidence pointing to the effects
of distance between that landmark and the target, and landmark salience and size on the
applicability of a proximity-based preposition. Finally, the examples presented point to
the fact that the location of other distractor objects in context may also interfere with the
applicability of a preposition. (The model of proximity we present in Section 6 captures
all these factors.)
Figure 8 is a representation of the spatial template for the projective preposition
above described in Logan and Sadler (1996). The main points of note relating to these
data are that there are three regions in the spatial template (good, acceptable, and
bad) and these regions are symmetric around the canonical direction of the preposition
with acceptability approaching 0 as the angular deviation from the canonical direction
approaches 90 degrees. However, it should be noted that these data were gathered dur-
ing an interpretation task and that the task may have affected the subjects? responses.
Figure 7
Proximity and distractor interference.
278
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Figure 8
Spatial template for the preposition above (Logan and Sadler 1996), where LM represents the
landmark and the arrow shows the canonical direction associated with the preposition.
Although the subjects may have rated some of the areas on the far right and left of
the landmark as acceptable with respect to interpreting an utterance such as above the
landmark, this does not mean that they would use the word above to describe a target
object in these regions relative to the landmark. This highlights the fact that people may
be more accommodating when they are interpreting a locative description (for example,
they may extend the allowable angular deviation to 90 degrees) but be more specific
when generating a locative description.
5. Previous Models of Topological and Projective Spatial Prepositions
There has been much research on the formal properties and interactions of topological
relations, for example Cohn et al (1997) and Kuipers (2000). However, before these
higher-level frameworks can be applied to real-world data, a model of proximity that
is capable of segmenting a region at the metric or geometric level is required. At
this geometric level previous approaches to modeling topological prepositions have
adopted one of two approaches to defining the region of proximity. The first is to adopt a
Voronoi segmentation of space. Under this approach the region considered as proximal
to an object is the area surrounding it that is closer to it than to any other object in the
scene. The second is to define the proximal region in terms of the size of the landmark.
For example, Gapp (1995) defines the area of proximity as the region within ten times
the size of the landmark object in each direction. However, neither of these approaches
consider the effect that the locations of other objects in the scene have on the proximity.
Consequently, they cannot distinguish between the different context provided by the
two images in Figure 7.
Several models of projective prepositions have been proposed (Yamada 1993;
Olivier and Tsujii 1994; Gapp 1995; Fuhr et al 1998; Regier and Carlson 2001; Kelleher
and van Genabith 2006). Yamada (1993) introduced the concept of a potential field
function to capture the gradation of applicability across the region described by the
preposition. Later work (Olivier and Tsujii 1994; Gapp 1995) highlighted the issue of
defining the intended frame of reference. Building on this work and the psycholin-
guistic results of Carlson-Radvansky and Logan (1997), Kelleher and van Genabith
279
Computational Linguistics Volume 35, Number 2
(2006) developed a computational model that constructed amodified spatial template in
situations where frame of reference ambiguity occurred. Fuhr et al (1998) used models
of prepositional semantics in order to interpret natural language commands to a robotic
arm. Fuhr et al segmented the space around an object into different regions based
on the sides and vertices of the object?s bounding box. One of the drawbacks of this
system, however, was that it could not distinguish between the position of two or more
objects that were fully enclosed within a given region. Finally, Regier and Carlson (2001)
developed a vector sum algorithm to compute the applicability of a projective relation
between a landmark and a target. However, as with previous topological models, none
of these models consider the influence of other objects in the context of the landmark
target relationship. For example, the introduction of the long black object into image
2 in Figure 9 affects the interpretability of a reference such as the blue square above the
white rectangle. In the next section we describe new models designed to account for the
influence of other objects in the semantics of spatial prepositions.
6. Models of Visual Context in Topological and Projective Spatial Prepositions
If a computational model is going to accommodate the gradation of applicability across
a preposition?s spatial template it must define the semantics of the preposition as
some sort of continuum function. A potential field model is one form of continuum
measure that is widely used (Yamada 1993; Gapp 1994; Olivier and Tsujii 1994; Regier
and Carlson 2001). Using this approach, a model of a preposition?s spatial template
is constructed using a set of normalized equations that, for a given origin and point,
computes a value that represents the cost of accepting that point as the interpretation of
the preposition.
Each equation used to construct the potential field representation of a preposition?s
spatial template models a different geometric constraint specified by the preposition?s
semantics. For example, for topological prepositions such as near, an equation inversely
proportional to the distance between a point and a landmark would be used, while
for projective prepositions such as to the right of, an equation modeling the angular de-
viation of a point from the idealized direction denoted by the preposition would be in-
cluded in the construction set; Gapp (1995) and Logan and Sadler (1996) both noted that
acceptability of a projective preposition being used to describe a location approaches
0 as the angular deviation of that location approaches 90 degrees. The potential field
is then constructed by assigning each point in the field an overall potential by inte-
grating the results computed for that point by each of the equations in the construc-
tion set.
Figure 9
Projective prepositions and distractor interference.
280
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
This potential field approach does not, however, account for the influence of other
objects in the visual scene on the semantics of a topological or projective preposition.
The basic idea in our computational models is to extend the potential field approach by
overlaying the potential fields for each object in a visual scene and combining those
fields to produce relative potential fields for topological or projective prepositions.
These relative potential fields represent the semantics of those prepositions as modified
by the presence of other objects in the visual scene.
6.1 Computational Model of Topological Prepositions
In this section we describe a model of relative proximity that uses (1) the distance
between objects, (2) the size and salience of the landmark object, and (3) the location
of other objects in the scene. Our model is based on first computing absolute proximity
between each point and each landmark in a scene, and then combining or overlaying
the resulting absolute proximity fields to compute the relative proximity of each point
to each landmark.
6.1.1 Computing Absolute Proximity Fields. We first compute for each landmark an ab-
solute proximity field giving each point?s proximity to that landmark, independent of
proximity to any other landmark. We compute fields on the projection of the scene onto
the 2D-plane, represented as a 2D-array of points. At each point P in that array, the
absolute proximity for landmark L is
proxabs(L,P) = (1? distnormalized(L,P)) ? salience(L) (1)
In this equation the absolute proximity for a point P and a landmark L is a function of
both the distance between the point and the location of the landmark, and the salience
of the landmark.
To represent distance we use a normalized distance function distnormalized(L,P),
which returns a value between 0 and 1.5 The smaller the distance between L and P,
the higher the absolute proximity value returned, that is, the more acceptable it is to say
that P is close to L. In this way, this component of the absolute proximity field captures
the gradual gradation in applicability evident in Logan and Sadler (1996).
We model the influence of visual and discourse salience on absolute proximity as
a function salience(L), returning a value between 0 and 1 that represents the relative
salience of the landmark L in the scene (Equation (2)). For the current purposes we
assume that the relative salience of an object is the average of its visual salience (Svis)
and discourse salience (Sdisc).
6
salience(L) = (Svis(L)+ Sdisc(L))/2 (2)
5 We normalize by computing the distance between the two points, and then dividing this distance by the
maximum distance between point L and any point in the scene.
6 There are, of course, many other operators that could be used to combine visual and linguistic salience,
such as maximum (MAX(Svis(L),Sdisc(L))) or probabilistic OR (Svis(L)+ Sdisc(L)? (Svis(L)? Sdisc(L))).
We currently have no way of deciding among these operators. Fortunately, however, the modular nature
of our framework would allow us to change the computation of relative salience without impacting other
aspects of our model, should evidence in favor of one or other operator become available.
281
Computational Linguistics Volume 35, Number 2
Visual salience Svis is computed using the algorithm of Kelleher and van Genabith
(2004). Computing a relative salience for each object in a scene is based on its perceivable
size and its centrality relative to the viewer?s focus of attention. The algorithm returns
scores in the range of 0 to 1. As the algorithm captures object size, we can model
the effect of landmark size on proximity through the salience component of absolute
proximity. The discourse salience (Sdisc) of an object is computed based on recency of
mention (Hajicova? 1993) except we represent the maximum overall salience in the scene
as 1, and use 0 to indicate that the landmark is not salient in the current context.
Figure 10 shows computed absolute proximity with salience values of 1, 0.6, and
0.5, for points from the upper-left to the lower-right of a 2D plane, with the landmark
at the center of that plane. The graph shows how salience influences absolute proximity
in our model: For a landmark with high salience, points far from the landmark can still
have high absolute proximity to it.
6.1.2 Computing Relative Proximity Fields. Once we have constructed absolute proximity
fields for the landmarks in a scene, our next step is to overlay these fields to produce a
measure of relative proximity to each landmark at each point. For this we first select
a landmark, and then iterate over each point in the scene comparing the absolute
proximity of the selected landmark at that point with the absolute proximity of all other
landmarks at that point. The relative proximity of a selected landmark at a point is
equal to the absolute proximity field for that landmark at that point, minus the highest
absolute proximity field for any other landmark at that point:
proxrel(P,L) = proxabs(P,L)? MAX
?LX =L
proxabs(P,LX) (3)
The idea here is that the other landmark with the highest absolute proximity is acting
in competition with the selected landmark. If that other landmark?s absolute proximity
Figure 10
Absolute proximity ratings for landmark L centered in a 2D plane, points ranging from plane?s
upper-left corner (??3,?3?) to lower right corner (?3,3?).
282
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
is higher than the absolute proximity of the selected landmark, the selected landmark?s
relative proximity for the point will be negative. If the competing landmark?s absolute
proximity is slightly lower than the absolute proximity of the selected landmark, the
selected landmark?s relative proximity for the point will be positive, but low. Only when
the competing landmark?s absolute proximity is significantly lower than the absolute
proximity of the selected landmark will the selected landmark have a high relative
proximity for the point in question.
In Equation (3) the proximity of a given point to a selected landmark rises as
that point?s distance from the landmark decreases (the closer the point is to the land-
mark, the higher its proximity score for the landmark will be), but falls as that point?s
distance from some other landmark decreases (the closer the point is to some other
landmark, the lower its proximity score for the selected landmark will be). Figure 11
shows the relative proximity fields of two landmarks, L1 and L2, computed using
Equation (3) in a 1-dimensional (linear) space. The two landmarks have different de-
grees of salience: a salience of 0.5 for L1 and of 0.6 for L2 (represented by the different
sizes of the landmarks). In this figure, any point where the relative proximity for one
particular landmark is above the zero line represents a point which is proximal to
that landmark, rather than to the other landmark. The extent to which that point is
above zero represents its degree of proximity to that landmark. The overall proximal
area for a given landmark is the overall area for which its relative proximity field is
above zero. The left and right borders of the figure represent the boundaries (walls) of
the area.
Figure 11 illustrates three main points. First, the overall size of a landmark?s prox-
imal area is a function of the landmark?s position relative to the other landmark and
to the boundaries. For example, landmark L2 has a large open space between it and
the right boundary: Most of this space falls into the proximal area for that landmark.
Landmark L1 falls into quite a narrow space between the left boundary and L2. L1
thus has a much smaller proximal area in the figure than L2. Second, the relative
proximity field for a landmark is a function of that landmark?s salience. This can be
seen in Figure 11 by considering the space between the two landmarks. In that space
the width of the proximal area for L2 is greater than that of L1, because L2 is more
salient.
Figure 11
Graph of relative proximity fields for two landmarks L1 and L2. Relative proximity fields were
computed with salience scores of 0.5 for L1 and 0.6 for L2.
283
Computational Linguistics Volume 35, Number 2
Figure 12
Example scene.
The third point concerns areas of ambiguous proximity in Figure 11: areas in which
neither of the landmarks have a significantly higher relative proximity than the other.
There are two such areas in the Figure. The first is between the two landmarks, in
the region where one relative proximity field line crosses the other. These points are
ambiguous in terms of relative proximity because these points are equidistant from
those two landmarks. The second ambiguous area is at the extreme right of the space
shown in Figure 11. This area is ambiguous because this area is distant from both
landmarks: Points in this area would not be judged proximal to either landmark. The
question of ambiguity in relative proximity judgments is considered in more detail in
Section 8.1.
We will illustrate the different stages of the proximity model using the situation
illustrated in Figure 12. The task is to decide whether the target object is proximal to
the landmark object. Figure 13 illustrates the absolute potential field for the landmark
Figure 13
The absolute proximity fields for the landmark.
284
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Figure 14
The absolute proximity fields for the landmark and the distractor.
object. Figure 14 illustrates the absolute potential fields for the landmark and the
distractor object. Figure 15 illustrates the relative proximity field that results from the
interaction between the landmark and distractors absolute proximity fields. Figure 16
illustrates the application of the threshold to the landmark?s relative proximity field. If
the target object is located in the region where the landmark?s relative proximity field
Figure 15
The landmark?s relative proximity field.
285
Computational Linguistics Volume 35, Number 2
Figure 16
Applying the threshold to the landmark?s relative proximity field.
is above the threshold the target is deemed to be proximal to the landmark. Figure 16
demonstrates the contextual influence which the distractor object has on the landmark?s
relative proximity field: The field shrinks on the side of the landmark near the distractor,
but expands on the side away from the landmark.
6.2 Computational Model of Projective Prepositions
The two main factors that impact on the applicability of a projective preposition de-
scribing the spatial relationship between a target object and a landmark are the angular
deviation of the target object?s position from the canonical direction described by the
preposition relative to the landmark and the distance of the target object from the
landmark.
The vector originating from the center of the landmark to the viewer?s position
describes the canonical search axis for in front of. We can produce the search vectors
for the other projective prepositions (behind, left, right) by rotating this front vector on a
horizontal plane. Once the canonical vectorc for a given projective preposition has been
selected, the angular deviation of a given point P position relative to the landmark L can
be computed using Equation (4):
angle( LP,c ) = cos
?1
?
?
LP ?c
?
?
?
LP
?
?
?
|c |
?
? (4)
where LP is the vector from landmark L to point P and c is the canonical vector for the
projective preposition in question. This equation gives the angle between LP and that
canonical vector.
286
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Using this equation, and the normalized distance measure described in Section 6.1,
we define an absolute potential field for the acceptability of a projective preposition
with canonical vector c for landmark L as follows:
projabs(L,P,c ) = 0 if (angle( LP,c ) > 90)or(distnormalized(L,P) = 0),
= (angle( LP,c )/distnormalized(L,P)) otherwise (5)
In this equation, if the angle between a point P and the canonical vector c is greater than
90 degrees, or if the distance between the landmark and the point is 0, the acceptability
of that point for the projective preposition is 0. Otherwise, the acceptability of that
point is equal to the angle between that point and the canonical vector, divided by the
normalized distance between that point and the landmark.
We use this absolute potential field for projective prepositions in the same way
that we used the absolute field for proximity in our model of topological prepositions.
Once we have computed the absolute potential field for each point relative to the
landmark we then do the same process for each of the distractor landmarks. We
then overlay the landmark applicabilities with those of the distractors by subtracting
the maximum applicability of any of the distractors at a point from the landmark?s
applicability at that point, producing a relative potential field for the projective prepo-
sition as in Equation (6):
projrel(P,L,c ) = projabs(P,Lc )? MAX
?LX =L
projabs(P,LX,c ) (6)
We then apply a threshold, and the region above this threshold is taken to define
the area described by the projective preposition. Note that we can use Equation (6) to
compute relative potential fields for various different projective prepositions (in front of,
behind, left, right, above, below) by selecting the different canonical vectors corresponding
to those prepositions.
Figures 17 through 20 illustrate the different stages in this process. In these images
the origin is at the front right corner, the x-axis runs from right to left, the y-axis
from front to back, and the z-axis is the vertical. The higher the z-axis value the more
applicability the preposition. Figure 17 defines the baseline applicability of z = 0.1. We
use this baseline because dividing an angular deviation by distance will never result in
a zero value; rather applicability will approach 0 asymptotically. The baseline provides
a cut-off point for applicability. Figure 18 illustrates the potential field computed for
right of a landmark positioned at x = 100, y = 200, z = 0 with a search axis of x = 1,
y = 0. Figure 19 illustrates the potential fields computed for right of the landmark and
a distractor object positioned at x = 150, y = 400, z = 0. Finally, Figure 20 illustrates the
potential field that results for right of the landmark when the distractor potential field is
subtracted from it. Figure 20 demonstrates the contextual influence which the distractor
object has on the landmark?s relative potential field for the preposition: the size of the
field is reduced by the presence of the distractor object.
7. Psycholinguistic Evaluations of Our Models
We now describe an experiment which tests our approach to relative proximity by
examining the changes in people?s judgments of the appropriateness of the expression
near being used to describe the relationship between a target and landmark object in
287
Computational Linguistics Volume 35, Number 2
Figure 17
A baseline applicability is set to z = 0.1.
an image where a distractor object is present. All objects in these images were colored
shapes: circles, triangles, or squares.
7.1 Materials and Procedure
All images used in this experiment contained a central landmark object and a target
object, usually with a third distractor object. The landmark was always placed in the
Figure 18
The potential field describing the absolute applicability model for right of the landmark.
288
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Figure 19
The potential fields describing the absolute applicability model for right of the landmark and
right of a distractor object.
middle of a 7 ? 7 grid. Images were divided into eight groups of six images each. Each
image in a group contained the target object placed in one of six different cells on the
grid, numbered from 1 to 6. Figure 21 shows how we number these target positions
according to their nearness to the landmark.
Groups are organized according to the presence and position of a distractor object.
In group a the distractor is directly above the landmark, in group b the distractor is
Figure 20
The resulting potential field for right of the landmark with the baseline applied to it.
289
Computational Linguistics Volume 35, Number 2
Figure 21
Relative locations of landmark (L) target positions (1. . . 6) and distractor landmark positions
(a. . .g) in images used in the experiment.
rotated 45 degrees clockwise from the vertical, in group c it is directly to the right of
the landmark, in d it is rotated 135 degrees clockwise from the vertical, and so on. The
distractor object is always the same distance from the central landmark. In addition to
the distractor groups a,b,c,d,e,f, and g, there is an eighth group, group x, in which no
distractor object occurs.
In the experiment, each image was displayed with a sentence of the form The
is near the , with a description of the target and landmark, respectively. The sentence
was presented under the image. Twelve participants took part in this experiment. All
participants were native English speakers and all volunteered to take part. Participants
were not linguists and were naive to the formal interpretation of spatial prepositions
and to the hypotheses being tested in the experiment. Participants were asked to rate
the acceptability of the sentence as a description of the image using a 10-point scale,
with zero denoting not acceptable at all; 4 or 5 denoting moderately acceptable; and 9
perfectly acceptable. Figure 22 illustrates a trial from the experiment. Each participant
rated every image in the experiment. Images were presented in random order to control
for learning effects.
7.2 Results and Discussion
There was significant agreement between participants across all 48 images. The average
pair-wise correlation between participants? responses was r = 0.68. There was a signifi-
cant correlation of responses between every pair of participants (p < 0.01 for all pairs).
We assess participants? responses by comparing their average proximity judgments
with those predicted by the absolute proximity equation (Equation (1)), and by the
relative proximity equation (Equation (3)). For both equations we assume that all objects
have a salience score of 1. With salience equal to 1, the absolute proximity equation
relates proximity between target and landmark objects to the distance between those
two objects, so that the closer the target is to the landmark the higher its proximity will
be. With salience equal to 1, the relative proximity equation relates proximity to both
distance between target and landmark and distance between target and distractor, so
290
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Figure 22
An example trial from the proximity experiment.
that the proximity of a given target object to a landmark rises as that target?s distance
from the landmark decreases but falls as the target?s distance from some other distractor
object decreases. It should be noted that proximity scores in both Equations (1) and
(3) are multiplied by a constant salience and that the evaluations we describe below
(correlation, multiple regression) factor out multiplication by a constant. Consequently,
choosing a particular value for salience does not affect our evaluation results.
In analyzing our results we are comparing our basic equation for absolute proximity
(Equation (1), in which proximity falls with increasing distance between target and
landmark) with the ?relative proximity? extension of this equation (Equation (3), in
which proximity falls with increasing distance between target and landmark, but rises
with distance to distractor). Because both equations are quite similar (both are based on
target?landmark distance, which is obviously the prime factor in proximity judgments),
we expect both equations to produce quite similar responses. We expect, however, that
the relative proximity equations will produce responses which are reliably closer to
people?s proximity judgments than those produced by the absolute proximity equation.
We initially used Spearman?s rank-order correlation to compare people?s average
proximity scores with those produced by Equation (1) (absolute proximity) and Equa-
tion (3) (relative proximity) for each group. For each group this analysis replaces each
proximity score with its rank within that group, and then compares the ranks. Where
the ranks returned by an equation and the ranks from participants? average proximity
scores are identical, the correlation will be 1.0; where the ranks differ, the correlation
will drop. For the absolute proximity equation, the correlation was 1.0 in six of the
groups, and .94 in the two remaining groups (group c and group g). For the relative
proximity equation, the Spearman?s rank-order correlation with people?s responses was
1.0 in each of the eight groups. The fact that the relative proximity equation has a rank-
order correlation of 1.0 in all groups while the absolute proximity equation fails to reach
1.0 in two groups (predicting proximity-ranks incorrectly in those two groups) suggests
291
Computational Linguistics Volume 35, Number 2
that the relative-proximity equation is a better model of people?s proximity responses.
However, the fact that there are somany correlations of 1.0 means that Spearman?s rank-
order correlation is not particularly useful in distinguishing between the two equations.
We therefore use Pearson?s product-moment correlation to compare people?s average
proximity scores with those produced by the absolute and relative proximity equations.
Rather than comparing ranks, this analysis compares actual proximity values.
Figure 23 shows the product-moment correlations between people?s average prox-
imity ratings and those produced by Equation (1) (absolute proximity) and by Equa-
tion (3) (relative proximity) for the eight groups in the experiment. In analyzing these
correlations we had two concerns: first, to see whether, for each individual group, the
correlation produced by Equation (3) was reliably different from that produced by
Equation (1); and second, to see whether across all the groups, the correlation produced
by Equation (3) was reliably higher than that produced by Equation (1). In regard
to the first question, we did not expect there to be particularly large differences in
correlation between the two equations, because both are based on target?landmark
distance. Because we know target?landmark distance to be a good predictor of people?s
proximity judgments we expected Equation (1) to have a high correlation with people?s
proximity judgments, and we expected Equation (3) to improve on that correlation.
However, because the correlation from Equation (1) was already high, any improvement
in correlation from Equation (3) would be relatively small. Indeed this is what is seen
across the seven groups of interest: The average correlation from Equation (1) is high
(average 0.93), the average correlation from Equation (3) is higher (average 0.99), but
the difference between the two correlations is relatively small. Using Fisher?s technique
for comparing correlation coefficients we find no reliable difference between correlation
coefficients in any group.
Given that the correlations for both Equations (1) and (3) are high we examined
whether the results returned by Equation (3) were reliably closer to human judgments
than those from Equation (1). For the 42 images where a distractor object was present we
recorded which equation gave a result that was closer to the participants? normalized
average for that image. In 28 cases Equation (3) was closer, and in 14 Equation (1) was
closer (a 2:1 advantage for Equation (3), significant in a sign test: n+ = 28,n? = 14,Z =
2.2, p < 0.05). We conclude that proximity judgments for objects in our experiment
are best represented by relative proximity as computed in Equation (3). These results
support our ?relative? model of proximity.7
In addition to these analyses, we also carried out a multiple regression analysis of
participants? responses in the experiment, with target?landmark distance and target?
distractor distance as the predictor variables, and participant response as the depen-
dent variable. Because our experiment involved repeated-measures data, we followed
the procedure for regression analysis of repeated-measures data described by Lorch
and Myers (1990). This involves computing individual multiple regression for each
participant in our experiment, and then using a t-test to analyze the regression coef-
ficients produced for target?distractor distance and target?landmark distance in those
equations, across all participants. Recall that in our relative proximity equation (Equa-
tion (3)) target?landmark distance had a negative coefficient (as target?landmark dis-
tance increased, judgments of target?landmark proximity fell) whereas target?distractor
7 Note that, in order to display the relationship between proximity values given by participants, computed
in Equation (1), and computed in Equation (3), the values displayed in Figure 23 are normalized so that
proximity values have a mean of 0 and a standard deviation of 1. This normalization simply means that
all values fall in the same region of the scale, and can be easily compared visually.
292
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Figure 23
Comparison between normalized proximity scores observed and computed for each group.
293
Computational Linguistics Volume 35, Number 2
distance had a positive coefficient (as target?distractor distance increased, judgments
of target?landmark proximity increased). Our prediction, therefore, is that across these
multiple regression analyses of participants? responses, the target?landmark distance
variable will reliably have a negative coefficient, whereas the target?distractor variable
will reliably have a positive coefficient.
Table 1 shows the regression coefficients obtained for the target?landmark distance
variable and the target?distractor distance variable, across the 12 participants in our
experiment. As this table shows, the regression coefficient for target?landmark distance
was significantly more likely to be negative (as predicted) whereas the regression
coefficient for target?distractor distance was significantly more likely to be positive
(again, as predicted). A single-group t-test showed that both target?landmark regres-
sion coefficients and target?distractor regression coefficients reliably differed from zero
(t(11) = ?8.64, p < 0.01; t(11) = 2.23, p < 0.05) indicating that both of these predictor
variables had a significant and reliable effect on participants? responses in the exper-
iment. There was no concern about collinearity between predictor variables in these
regression analyses, as the correlation between those variables (r = 0.38, %var = 0.14)
was much lower than that between the predictor variables and the dependent variable
(r = 0.93 or higher). Together these regression results, the sign-test results, and the
comparative correlations described earlier all support the model of relative proximity
as described in Equation (3).
8. Applications of the Models
The model of proximity presented here has been implemented and used as a compo-
nent in a human?robot dialog system (Kelleher and Kruijff 2006; Kelleher, Kruijff, and
Costello 2006). The proximity and projective models have also been integrated into the
LIVE virtual environment (Kelleher, Costello, and van Genabith 2005). In this section
we will describe how the models are used in these systems to interpret and generate
locative expressions.
Table 1
Regression coefficients from individual analyses of subjects data in proximity experiment.
participant target?landmark distance target?distractor distance
1 ?2.02 0.27
2 ?1.53 0.09
3 ?3.06 0.03
4 ?2.45 ?0.02
5 ?2.23 0.06
6 ?0.97 0.32
7 ?3.09 0.42
8 ?1.78 0.02
9 ?0.80 0.16
10 ?1.48 ?0.24
11 ?3.29 0.01
12 ?3.29 0.44
M ?2.17 0.13
SE 0.88 0.20
t ?8.64* 2.23**
*p < 0.01, **p < 0.05.
294
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
8.1 Interpreting Spatial References
We use the computational models of Section 6 to interpret spatial references to objects.
In this section we illustrate how we use our model of relative proximity to ground the
interpretation of a locative expression containing a topological preposition. Returning
to the architecture described in Section 3, the basic steps triggering the interpretation
of a locative are: (1) the user utters a command, such as pick up the ball near the red box,
(2) the speech recognition module processes the speech signal and passes the resulting
string to the parser, (3) the parser constructs a formal representation of the meaning of
the utterance, (4) the dialog manager categorizes the utterance to be a command and,
also, recognizes the need to resolve the referring expression the ball near the red box. At
this point the reference resolution module is triggered.
The first stage in reference resolution is to retrieve the context against which the
reference is to be resolved. This involves accessing the context model and retrieving
the set of currently accessible objects. This set is then subdivided into the set of objects
fulfilling the landmark description, the set of objects fulfilling the description of the
target object, and the set of objects fulfilling neither description.
For each candidate landmark and each object that is neither a candidate landmark
nor a candidate target we compute an absolute proximity field. For each landmark we
convert its absolute proximity field into a relative proximity field by overlaying the
absolute proximity fields of the other landmarks and the other objects in the context
that are neither candidate landmarks nor target objects. For this we iterate over each
point in the scene, and compare the competing absolute proximity scores at each point.
If the primary landmark?s (i.e., the landmark with the highest relative proximity at the
point) relative proximity exceeds the next highest relative proximity score at a given
point by more than a predefined confidence interval, the point is in the proximity region
anchored around the primary landmark. Otherwise, we take it as ambiguous and not in
the proximal region that is being interpreted. The motivation for the confidence interval
is to capture situations where the difference in relative proximity scores between the
primary landmark and one or more landmarks at a given point is relatively small.
Figure 24 illustrates the parsing of a scene into the regions ?near? two landmarks. The
relative proximity fields of the two landmarks are identical to those in Figure 11, using a
confidence interval of 0.1. Ambiguous points are where the proximity ambiguity series
is plotted at 0.5. The regions ?near? each landmark are those areas of the graph where
each landmark?s relative proximity series is the highest plot on the graph.
Figure 24 illustrates an important aspect of our model: the comparison of relative
proximity fields naturally defines the extent of vague proximal regions. For example,
see the region right of L2 in Figure 24. The extent of L2?s proximal region in this
direction is bounded by the interference effect of L1?s relative proximity field. Because
the landmarks? relative proximity scores converge, the area on the far right of the image
is ambiguous with respect to which landmark it is proximal to. In effect, the model
captures the fact that the area is relatively distant from both landmarks. In Section 8.2
we describe a cognitive load hierarchy of prepositions and how we use this to generate
locative expressions. Following this hierarchy, objects located in the area on the far right
of the image should be described with a projective relation such as to the right of L2
rather than a proximal relation like near L2.
8.1.1 An Example. To illustrate the model further we will apply the model to a real scene.
Figure 25 shows a real scene on the left-hand side, and a rendering of the scene analysis
on the right-hand side. For the shown scene analysis we have assumed all objects to
295
Computational Linguistics Volume 35, Number 2
Figure 24
Graph of ambiguous regions overlaid on relative proximity fields for landmarks L1 and L2,
with confidence interval = 0.1 and different salience scores for L1 (0.5) and L2 (0.6). Locations
of landmarks are marked on the x-axis.
have an equal salience: on the left, the blue ball; in the middle, the red ball; and on
the right, the green ball. As the analysis correctly shows, each object has a proximity
potential field (shown in its own color) but, due to interference between potential
fields, we see that proximity is usually ambiguous between at least two landmarks.
The regions that are ambiguous between two landmarks are colored using a mixture of
the colors. The white area denotes the regions defined as being ambiguous between the
three objects.
Imagine we now place a second blue ball in the scene and the user inputs the
command pick up the blue ball near the red ball. As explained previously, when the system
starts interpreting this reference it will split the context into a set of candidate target
objects, consisting of the two blue balls in the scene, the set of candidate landmarks,
consisting of the one red ball, and the set of remaining objects, the green ball. It will
then compute proximity fields for each of the candidate landmarks and the other objects
in the scene that are not candidate targets. It will then overlay these proximity fields
Figure 25
Scene analysis.
296
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
to compute the relative proximity fields around each landmark. Figure 26 illustrates
the resulting proximity fields. As can be seen from the image the original blue ball is
inside the red ball?s proximity field and consequently it will be selected as the ball to be
picked up.
This analysis highlights two important aspects of the model. First, we can observe
an interference effect between the red ball and the green ball: The potential field repre-
senting proximity to the red ball forms an ellipsoid, being inhibited to the right through
interference with the potential field of the green ball. Second, the proximity field of
the red ball is much larger then that of the green ball; this is due to the relatively high
linguistic salience of the red ball compared to the green ball due to it being mentioned
in the reference.
8.2 Generating References
In this section we illustrate how we use our models of the semantics of spatial preposi-
tions to guide the generation of a locative expression in visual situated contexts.
In the architecture described earlier, the GRE component is triggered by the content
manager. Similar to reference resolution, GRE will first retrieve the context from the
context model and generate the reference relative to this context. If a locative expression
is necessary the GRE component has three things to decide: (1) what properties of the
target object to include, (2) which object in the scene should be used as a landmark and
how should that be described, and (3) which spatial relation to use (and hence which
preposition to use).
Several GRE algorithms have addressed the issue of generating locative expressions
(Dale and Haddock 1991; Horacek 1997; Gardent 2002; Krahmer and Theune 2002;
Figure 26
Interpreting the blue ball near the red ball.
297
Computational Linguistics Volume 35, Number 2
Varges 2004). However, all these algorithms assume the GRE component has access to
a predefined scene model that defines all the spatial relations between all the entities
in the scene. For many visually situated dialog systems, in particular robotic dialog
systems, this assumption is a serious drawback for these algorithms. If an agent wishes
to generate a contextually appropriate reference it cannot assume the availability of
a domain model, rather it must dynamically construct one. Moreover, constructing a
model containing all the spatial relationships between all the entities in the domain
is prone to combinatorial explosion, both in terms of the number of objects in the
context (the location of each object in the scene must be checked against all the other
objects in the scene) and number of inter-object spatial relations (as a greater number
of spatial relations will require a greater number of comparisons between each pair
of objects). Furthermore, the context-free a priori construction of such an exhaustive
scene model is cognitively implausible. Psychological research indicates that spatial
relations are not preattentively perceptually available (Treisman and Gormican 1988).
Rather, their perception requires attention (Logan 1994, 1995). These findings point to
subjects constructing contextually dependent reduced relational scene models, rather
than an exhaustive context-free model.
The approach we adopt to generating locative expressions addresses the issue of
combinatorial explosion inherent in relational scene model construction by incremen-
tally creating a series of reduced scene models. Within each scene model only one
spatial relation is considered and only a subset of objects are considered as candidate
landmarks. This reduces both the number of relations that must be computed over each
object pair and the number of object pairs. The decision as to which relations should be
included in each scene model is guided by a cognitively-motivated hierarchy of spatial
relations. The set of candidate landmarks in a given scene is dependent on the set of
objects in the scene that fulfill the description of the target object and the semantic relation
that is being considered.
We use Dale and Reiter?s (1995) incremental GRE algorithm as the starting point
for the generation framework. The incremental algorithm iterates through the proper-
ties of the target object and for each property computes the set of distractor objects for
which the conjunction of the properties selected so far, and the current property, hold. A
property is added to the list of selected properties if it reduces the size of the distractor
object set. The algorithm succeeds when all the distractors have been ruled out; it fails
if all the properties have been processed and there are still some distractor objects.
The algorithm can be refined by ordering the checking of properties according to fixed
preferences; for example, first a taxonomic description of the target, second an absolute
property such as color, third a relative property such as size. Dale and Reiter also
stipulate that the type description of the target should be included in the description
even if its inclusion does not distinguish the target from any of the distractors; see
Algorithm 1. Dale and Reiter argue that this algorithm has a polynomial complexity
and that the theoretical run time can be characterized as nd ? nl: the run time depends
solely on the number of distractor objects nd and the number of properties considered
in iterations nl. If we assume that nd and nl are both proportional to n, the number of ob-
jects being considered, then the complexity of the incremental algorithm is of order n2.
The incremental algorithm generates a description (in terms of type, color, and
size) which distinguishes a given target object from a set of distractor objects (if such
a description exists). However, we wish to generate locative expressions which iden-
tify objects, rather than simple descriptions. These locative expressions may contain
a description of a landmark object (in terms of type, color, or size), of a target object
(type, color, or size), and a topological or projective preposition relating those two
298
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Algorithm 1 The Basic Incremental Algorithm
Require: T = target object; D = set of distractor objects.
Initialize: P = {type, color, size}; DESC = {}
for i = 0 to |P| do
if |D| 
= 0 then
D? = {x : x ? D,Pi(x) = Pi(T)}
if |D?| < |D| then
DESC = DESC ? Pi(T)
D = {x : x ? D,Pi(x) = Pi(T)}
end if
else
Distinguishing description generated
if type(x) 
? DESC then
DESC = DESC ? type(x)
end if
return DESC
end if
end for
Failed to generate distinguishing description
return DESC
objects. To generate such locative expressions we repeatedly call the basic incremental
algorithm for a sequence of different possible spatial relations. The fact that each call to
the algorithm uses a different spatial relation results in a different set of objects from
the context being defined as candidate landmarks for each function call. If a given
spatial relation allows the basic incremental algorithm to generate a description which
distinguishes the target object from the set of distractor objects, that spatial relation is
used to generate an expression identifying that object. Otherwise we move on and call
the basic incremental algorithm for the next spatial relation in our sequence.
When generating a referring expression, we use a sequence of possible forms of
reference ordered by assumed cognitive load (see Figure 27), with simpler forms of
reference (those identifying object type, for example) coming early in the sequence
and more complex forms (those involving projective prepositions, for example) coming
later. This means that our approach will preferentially produce simpler expressions to
identify an object, and only if no such simple expressions can be found which distin-
guish that object successfully will more complex topological or projective prepositions
Figure 27
Cognitive load of reference forms.
299
Computational Linguistics Volume 35, Number 2
be produced. Our sequence of relations can be extended to include relations of ternary
and higher arity such as the ball between the box and the triangle or the ball near the box and
the triangle.
We use the models of topological and projective prepositions described in Sec-
tions 6.1 and 6.2 to define the regions around a landmark to which a given topological or
projective description applies. If the target or one of the distractor objects is the only ob-
ject within that region around a given landmark, this is taken to represent a contrastive
use of a preposition relative to that landmark. If that region contains more than one
object from the target and distractor object set, then it is a relative use of the preposition.
8.2.1 Landmarks and Distinguishing Descriptions. In order to use a locative expression,
an object in the context must be selected to function as the landmark. An implicit
assumption in selecting an object to function as a landmark is that the hearer can easily
identify and locate the object within the context. As shown in Example (4), a landmark
can be the speaker, the hearer, the scene, an object in the scene, or a group of objects in
the scene.8
Example 4
? the ball on my right [speaker]
? the ball to your left [hearer]
? the ball on the right [scene]
? the ball to the left of the box [an object in the scene]
? the ball in the middle [group of objects]
Clearly, deciding which objects in a given visual context can function as landmarks
is a complex process. Some of the factors effecting this decision are object salience
and the functional relationships between objects. However, one basic constraint on
landmark selection is that the landmark should be distinguishable from the target. For
example, in the context provided by Figure 28 the ball has a relatively high salience,
because it is a singleton, despite the fact that it is smaller and geometrically less complex
than the other figures. Moreover, in this context, the ball is the only object in the scene
that can function as a landmark without recourse to using the scene itself or a grouping
of objects in the scene. Given the context in Figure 28 and all other factors being equal,
using a locative such as the man to the left of the man would be much less helpful than
using the man to the right of the ball.
Following this observation, we treat an object as a candidate landmark if the
following conditions are met:
1. The object is not the target.
2. The object is not a member of the distractor set.
Furthermore, a target landmark is a member of the candidate landmark set that stands
in relation to the target under the relation being considered and a distractor landmark
8 See Gorniak and Roy (2004) for further discussion on the use of spatial extrema of the scene and groups
of objects in the scene as landmarks.
300
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Figure 28
Visual context used to illustrate the relative semantics of topological and projective prepositions.
is a member of the candidate landmark set that stands in relation to a distractor object
under the relation being considered.
Using these categories of landmark we can define a distinguishing locative de-
scription as a locative description where there is a target landmark that can be dis-
tinguished using the basic incremental algorithm from all the members of the set of
distractor landmarks which stand under the relation used in the locative expression.
Given this, our approach is to try to generate a distinguishing description using the
standard incremental algorithm. If this fails, we divide the context into three compo-
nents: the target, the distractor objects, and the set of candidate landmarks. We then
begin to iterate through the hierarchy of relations and for each relation we create a
context model that defines the set of target and distractor landmarks. Once a context
model has been created we iterate through the target landmarks (using a salience order-
ing if there is more than one) and try to create a distinguishing locative description. A
distinguishing locative description is created by using the basic incremental algorithm
to distinguish the target landmark from the distractor landmarks. If we succeed in
generating a distinguishing locative description we return the description and stop
processing.
Algorithm 2 The Locative Incremental Algorithm
Require: T = target object; D = set of distractor objects; R = hierarchy of relations.
DESC = Basic-Incremental-Algorithm(T,D)
if DESC 
= Distinguishing then
create CL the set of candidate landmarks
CL = {x : x 
= T,DESC(x) = false}
for i = 0 to |R| do
create a context model for relation Ri consisting of TL the set of target landmarks and DL the set
of distractor landmarks
TL = {y : y ? CL,Ri(T, y) = true}
DL = {z : z ? CL,Ri(D, z) = true}
for j = 0 to |TL| by salience(TL) do
LANDDESC = Basic-Incremental-Algorithm(TLj, DL)
if LANDDESC = Distinguishing then
Distinguishing locative generated
return {DESC,Ri,LANDDESC}
end if
end for
end for
end if
FAIL
301
Computational Linguistics Volume 35, Number 2
If we cannot create a distinguishing locative description we move on to the next,
more complex spatial relation in the sequence of spatial relations, and attempt to gener-
ate a distinguishing locative description using that relation. This process continues until
either a distinguishing expression is produced or no possible spatial relations remain.
This algorithm runs the basic incremental algorithm a number of times for each
candidate relation in the list of possible relations. The length of this list will be a
constant; call it R. For each candidate relation, the number of times the incremental
algorithm runs is equal to the number of TL objects (the number of objects which
don?t fulfill the description of the target created by the current run of the incremental
algorithm, and which the target object stands under the currently selected relation to).
Call the number of TL objects nTL and note that nTL must be less than, and proportional
to, n (the total number of objects). The number of times the basic incremental algorithm
can run, in our system, is then proportional to NTL ? R; replacing with nTL with n gives
n? R runs of the basic incremental algorithm. Inserting the complexity of the basic
incremental algorithm into this, we get an overall complexity of n2 ? n? R = n3 ? R,
which although worse than the basic incremental algorithm?s n2 complexity, is still
polynomial.
This algorithm cannot generate embedded locative descriptions, such as the bag on
the chair near the window, because it does not use spatial relations as properties to
describe the landmark. However, these descriptions can be generated if needed by
replacing the call to the basic incremental algorithm for the landmark object with a
call to the whole locative expression algorithm, using the target landmark as the target
object and the set of distractor landmarks as the distractors. A nice consequence of
this approach to generating embedded locative descriptions is that infinite descriptions
(e.g., the bag on the chair supporting the bag on the chair ...) will not be generated as the
target object is excluded from the context that the landmark?s description is generated
in. However, the cost of being able to generate these embedded descriptions is a higher
exponential complexity.
8.2.2 An Example. We can illustrate the framework using the visual context provided
by the scene on the left of Figure 29. This context consists of two red boxes R1 and
Figure 29
A visual scene and the topological analysis of R1 and R2.
302
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
R2 and two blue balls B1 and B2. Imagine that we want to refer to B1. We begin by
calling the locative incremental algorithm, Algorithm 2. This in turn calls the basic
incremental algorithm, Algorithm 1, which will return the property ball. However, this
is not sufficient to create a distinguishing description as B2 is also a ball. In this context
the set of candidate landmarks equals {R1,R2} and the first relation in the hierarchy is
topological proximity, which we model as described in Section 6.1. The image on the
right of Figure 29 illustrates the analysis of the scene using this framework: The green
region on the left defines the area deemed to be proximal to R1, and the yellow region
on the right defines the area deemed to be proximal to R2. It is evident that B1 is in
the area proximal to R1; consequently R1 is classified as a target landmark. As none of
the distractors (i.e., B2) are located in a region that is proximal to a candidate landmark
there are no distractor landmarks. As a result when the basic incremental algorithm is
called to create a distinguishing description for the target landmark R1 it will return
box and this will be deemed to be a distinguishing locative description. The overall
algorithm will then return the vector {ball, proximal, box} which would result in the
realizer generating a reference of the form: the ball near the box.
9. Conclusions and Future Work
In this article we have described the application of computational models of spatial
prepositions to visually situated dialog systems. These computational models allow sys-
tems to both interpret and generate expressionswhich refer to topological and projective
relations between objects in the visual environment. The computational models of spa-
tial prepositions we present are designed to handle reference resolution and generation
in complex visual environments containing multiple objects. In particular, these models
are designed to account for the contextual influence which the presence of multiple
objects has on the semantics of topological and projective prepositions. In this respect
our computational models move beyond other accounts of the semantics of spatial
prepositions, which typically do not model the contextual influence of other objects
on spatial semantics. Because most real-world visual scenes are complex and contain
multiple objects, our computational models for the semantics of spatial prepositions are
important for visually situated dialog systems intended to operate successfully in the
real world.
Clearly there are many interesting areas for future work. To date our research has
focused on a small number of static topological and projective prepositions. We feel,
however, that our framework will apply usefully to a range of other more complex static
and dynamic prepositions, for example: between, among, within, along, beside, around.
These prepositions either involve several objects or multiple areas and, consequently,
our account of the effect of distractor objects on the target?landmark relationship could
provide a worthwhile perspective on their semantics.
This leads to another promising area for future work. Although our current model
was designed to accommodate multiple distractor objects, our empirical studies have
focused on cases where there is only one distractor. An important aim for future research
is to extend these studies and test the model in situations with multiple distractors.
From a theoretical point of view, we feel that our approach to the semantics of spa-
tial prepositions illustrates an important point for researchers working on the semantics
of natural language in general: that it is possible to investigate and model semantics not
solely as a linguistic phenomenon, but also in terms of non-linguistic factors such as
the visual environment in which language is used. For example, in the psychological
evaluations described in Section 7 we found that the semantic applicability of ?near? to
303
Computational Linguistics Volume 35, Number 2
the relationship between a target and a landmark object was reliably influenced by the
presence and location of a third, distractor, object, which was not part of the linguistic
context. That the semantics of language is influenced by non-linguistic factors is an old
point and an obvious one: however, we think that our research on visually-situated
dialog systems makes a useful contribution by showing that these systems provide
ideal testbeds for investigating the interaction between language and vision, and for
developing detailed and useful computational models of how those interactions work.
References
Baldridge, J. and G. J. M. Kruijff. 2002.
Coupling CCG and hybrid logic
dependency semantics. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL-02),
pages 319?326, Philadelphia, PA.
Baldridge, J. and G. J. M. Kruijff. 2003.
Multi-modal combinatory categorial
grammar. In Proceedings of the 10th
Conference of the European Chapter of the
Association for Computational Linguistics
(EACL-03), volume 1, pages 211?218,
Budapest.
Beermann, D. and L. Hellan. 2004. Semantic
decomposition in a computational HPSG
grammar: A treatment of aspect and
context-dependent directionals. In
Proceedings of the HPSG04 Conference,
pages 357?377, Leuven.
Bunt, H. 1994. Context and dialogue control.
Think, 3:19?31.
Cahill, A., M. Burke, R. O?Donovan, J. van
Genabith, and A. Way. 2004. Long-distance
dependency resolution in automatically
acquired wide-coverage PCFG-based
LFG approximations. In Proceedings of the
42nd Annual Meeting of the Association for
Computational Linguistics (ACL-04),
pages 320?327, Barcelona.
Carletta, J., A. Isard, S. Isard, J. C. Kowtko,
G. Doherty-Sneddon, and A. H. Anderson.
1997. The reliability of a dialogue structure
coding scheme. Computational Linguistics,
23(1):13?32.
Carlson-Radvansky, L. A. and G. D. Logan.
1997. The influence of reference frame
selection on spatial template construction.
Journal of Memory and Language, 37:411?437.
Cohn, A. G., B. Bennett, J. M. Gooday, and
N. Gotts. 1997. RCC: A calculus for region
based qualitative spatial reasoning.
GeoInformatica, 1:275?316.
Coventry, K. R. 1998. Spatial prepositions,
functional relations, and lexical
specification. In P. Olivier and
K. P. Gapp, editors, Representation and
Processing of Spatial Expressions. Lawrence
Erlbaum Associates, Hillsdale, NJ,
pages 247?262.
Coventry, K. R. and S. Garrod. 2004. Saying,
Seeing and Acting. The Psychological
Semantics of Spatial Prepositions. Essays in
Cognitive Psychology Series. Lawrence
Erlbaum Associates, Hillsdale, NJ.
Dale, R. and N. Haddock. 1991. Generating
referring expressions involving relations.
In Proceedings of the 5th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL-91),
pages 161?166, Berlin.
Dale, R. and E. Reiter. 1995. Computatinal
interpretations of the gricean maxims in
the generation of referring expressions.
Cognitive Science, 18:233?263.
Fuhr, T., G. Socher, C. Scheering, and
G. Sagerer. 1998. A three-dimensional
spatial model for the interpretation
of image data. In P. Olivier and
K. P. Gapp, editors, Representation and
Processing of Spatial Expressions. Lawrence
Erlbaum Associates, Hillsdale, NJ,
pages 103?118.
Gapp, K. P. 1994. Basic meanings of
spatial relations: Computation and
evaluation in 3D space. In Proceedings
of the 12th National Conference on Artificial
Intelligence (AAAI-94), Seattle, WA,
pages 1393?1398.
Gapp, K. P. 1995. An empirically validated
model for computing spatial relations. In
Proceedings of the 19th German Conference on
Artificial Intelligence (KI-95), Bielefeld,
Germany, pages 245?256.
Gardent, C. 2002. Generating minimal
definite descriptions. In Proceedings of
the 40th Annual Meeting of the Association
of Computational Linguistics (ACL-02),
pages 96?103, Philadelphia, PA.
Garrod, S., G. Ferrier, and S. Campbell. 1999.
In and on: Investigating the functional
geometry of spatial prepositions.
Cognition, 72:167?189.
Gawron, J. M. 1986. Situations and
prepositions. Linguistics and Philosophy,
9(3):327?382.
Gorniak, P. and D. Roy. 2004. Grounded
semantic composition for visual scenes.
Journal of Artificial Intelligence Research,
21:429?470.
304
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Hajicova?, E. 1993. Issues of Sentence Structure
and Discourse Patterns, volume 2 of
Theoretical and Computational Linguistics.
Charles University Press, Prague.
Hayward, W. G. and M. J. Tarr. 1995. Spatial
language and spatial representation.
Cognition, 55:39?84.
Herskovits, A. 1986. Language and Spatial
Cognition: An Interdisciplinary Study of
Prepositions in English. Studies in Natural
Language Processing. Cambridge
University Press, Cambridge, UK.
Horacek, H. 1997. An algorithm for
generating referential descriptions with
flexible interfaces. In Proceedings of the
35th Annual Meeting of the Association for
Computational Linguistics, pages 206?213,
Madrid.
Jackendoff, R. 1983. Semantics and Cognition.
Current Studies in Linguistics. The MIT
Press, Cambridge, MA.
Kelleher, J., F. Costello, and J. van Genabith.
2005. Dynamically structuring, updating
and interrelating representations of visual
and lingusitic discourse context. Artificial
Intelligence, 167(1?2):62?102.
Kelleher, J. and J. van Genabith. 2004. Visual
salience and reference resolution in
simulated 3D environments. AI Review,
21(3-4):253?267.
Kelleher, J. and J. van Genabith. 2006. A
computational model of the referential
semantics of projective prepositions.
In P. Saint-Dizier, editor, Syntax and
Semantics of Prepositions, Speech and
Language Processing. Kluwer Academic
Publishers, Dordrecht, The Netherlands,
pages 199?216.
Kelleher, J. D. and G. J. Kruijff. 2006.
Incremental generation of spatial
referring expressions in situated dialog.
In Proceedings of the 3rd Joint Conference
of the International Committee on
Computational Linguistics and the
Association for Computational Linguistics
(COLING/ACL-06), pages 1041?1048,
Sydney.
Kelleher, J. D., G. J. Kruijff, and F. Costello.
2006. Proximity in context: an empirically
grounded computation model of
proximity for processing topological
spatial expressions. In Proceedings of the
3rd Joint Conference of the International
Committee on Computational Linguistics
and the Association for Computational
Linguistics (COLING/ACL-06),
pages 745?752, Sydney.
Klein, M. 1999. An overview of the state of
the art of coding schemes for dialogue act
annotation. In V. Matousek, P. Mautner,
J. Ocelikova?, and P. Sojka, editors, Text,
Speech and Dialogue (TSD?99), Lecture
Notes in Computer Science. Springer,
Berlin/Heidelberg, pages 274?297.
Krahmer, E. and M. Theune. 2002. Efficient
context-sensitive generation of referring
expressions. In K. van Deemter and
R. Kibble, editors, Information Sharing:
Reference and Presupposition in Language
Generation and Interpretation. CLSI
Publications, Stanford, CA, pages 223?263.
Kruijff, Geert-Jan, John Kelleher, and Nick
Hawes. 2006. Information fusion for
visual reference resolution in dynamic
situated dialogue. In Elisabeth Andre,
Laila Dybkjaer, Wolfgang Minker,
Heiko Neumann, and Michael Weber,
editors, In Proceedings of Perception and
Interactive Technologies (PIT06),
volume 4021 of Lecture Notes in Computer
Science. Springer Berlin/Heidelberg,
pages 117?128.
Kuipers, Benjamin. 2000. The spatial
semantic hierarchy. Artificial Intelligence,
19:191?233.
Landau, B. 1996. Multiple geometric
representations of objects in language
and language learners. In P. Bloom,
M. Peterson, L. Nadel, and M. Garrett,
editors, Language and Space. MIT Press,
Cambridge, MA, pages 317?363.
Levelt, W. J. M. 1996. Perspective taking and
ellipsis in spatial descriptions. In
M. Bloom, P. Peterson, L. Nadell, and
M. Garrett, editors, Language and Space.
MIT Press, Cambridge, MA, pages 77?108.
Levinson, S. 1996. Frame of reference and
Molyneux?s question: Crosslinguistic
evidence. In M. Bloom, P. Peterson,
L. Nadell, and M. Garrett, editors,
Language and Space. MIT Press,
Cambridge, MA, pages 109?170.
Levinson, S. 2003. Space in Language and
Cognition: Explorations in Cognitive
Diversity. Cambridge University Press,
Cambridge, UK.
Logan, G. D. 1994. Spatial attention and the
apprehension of spatial relations. Journal
of Experimental Psychology: Human
Perception and Performance, 20:1015?1036.
Logan, G. D. 1995. Linguistic and conceptual
control of visual spatial attention.
Cognitive Psychology, 12:523?533.
Logan, G. D. and D. D. Sadler. 1996.
A computational analysis of the
apprehension of spatial relations.
In M. Bloom, P. Peterson, L. Nadell,
and M. Garrett, editors, Language and
305
Computational Linguistics Volume 35, Number 2
Space. MIT Press, Cambridge, MA,
pages 493?529.
Lorch, R. F. and J. L. Myers. 1990.
Regression analyses of repeated
measures data in cognitive research.
Journal of Experimental Psychology:
Learning, Memory, and Cognition,
16(1):149?157.
Olivier, P. and J. Tsujii. 1994. Quantitative
perceptual representation of prepositional
semantics. Artificial Intelligence Review,
8:147?158.
Regier, T and L. Carlson. 2001. Grounding
spatial language in perception:
An empirical and computational
investigation. Journal of Experimental
Psychology: General, 130(2):273?298.
Treisman, A. and S. Gormican. 1988. Feature
analysis in early vision: Evidence from
search assymetries. Psychological Review,
95:15?48.
Tseng, J. L. 2000. The Representation and
Selection of Prepositions. Ph.D. thesis,
University of Edinburgh.
Varges, S. 2004. Overgenerating referring
expressions involving relations and
booleans. In Proceedings of the 3rd
International Conference on Natural
Language Generation (INLG-04),
pages 171?181, Brighton.
Yamada, A. 1993. Studies in Spatial
Descriptions Understanding Based on
Geometric Constraints Satisfaction.
Ph.D. thesis, University of Kyoto.
306
This article has been cited by:
1. Timothy Baldwin, Valia Kordoni, Aline Villavicencio. 2009. Prepositions in Applications:
A Survey and Introduction to the Special IssuePrepositions in Applications: A Survey and
Introduction to the Special Issue. Computational Linguistics 35:2, 119-149. [Citation] [PDF]
[PDF Plus]
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 745?752,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Proximity in Context: an empirically grounded computational model of
proximity for processing topological spatial expressions?
John D. Kelleher
Dublin Institute of Technology
Dublin, Ireland
john.kelleher@comp.dit.ie
Geert-Jan M. Kruijff
DFKI GmbH
Saarbruc?ken, Germany
gj@dfki.de
Fintan J. Costello
University College Dublin
Dublin, Ireland
fintan.costello@ucd.ie
Abstract
The paper presents a new model for context-
dependent interpretation of linguistic expressions
about spatial proximity between objects in a nat-
ural scene. The paper discusses novel psycholin-
guistic experimental data that tests and verifies the
model. The model has been implemented, and en-
ables a conversational robot to identify objects in a
scene through topological spatial relations (e.g. ?X
near Y?). The model can help motivate the choice
between topological and projective prepositions.
1 Introduction
Our long-term goal is to develop conversational
robots with which we can have natural, fluent sit-
uated dialog. An inherent aspect of such situated
dialog is reference to aspects of the physical envi-
ronment in which the agents are situated. In this
paper, we present a computational model which
provides a context-dependent analysis of the envi-
ronment in terms of spatial proximity. We show
how we can use this model to ground spatial lan-
guage that uses topological prepositions (?the ball
near the box?) to identify objects in a scene.
Proximity is ubiquitous in situated dialog, but
there are deeper ?cognitive? reasons for why we
need a context-dependent model of proximity to
facilitate fluent dialog with a conversational robot.
This has to do with the cognitive load that process-
ing proximity expressions imposes. Consider the
examples in (1). Psycholinguistic data indicates
that a spatial proximity expression (1b) presents a
heavier cognitive load than a referring expression
identifying an object purely on physical features
(1a) yet is easier to process than a projective ex-
pression (1c) (van der Sluis and Krahmer, 2004).
?The research reported here was supported by the CoSy
project, EU FP6 IST ?Cognitive Systems? FP6-004250-IP.
(1) a. the blue ball
b. the ball near the box
c. the ball to the right of the box
One explanation for this preference is that
feature-based descriptions are easier to resolve
perceptually, with a further distinction among fea-
tures as given in Figure 1, cf. (Dale and Reiter,
1995). On the other hand, the interpretation and
realization of spatial expressions requires effort
and attention (Logan, 1994; Logan, 1995).
Figure 1: Cognitive load
Similarly we
can distinguish be-
tween the cognitive
loads of processing
different forms of
spatial relations.
Focusing on static
prepositions, topo-
logical prepositions
have a lower cognitive load than projective
prepositions. Topological prepositions (e.g.
?at?, ?near?) describe proximity to an object.
Projective prepositions (e.g. ?above?) describe a
region in a particular direction from the object.
Projective prepositions impose a higher cognitive
load because we need to consider different spatial
frames of reference (Krahmer and Theune, 1999;
Moratz and Tenbrink, 2006). Now, if we want
a robot to interact with other agents in a way
that obeys the Principle of Minimal Cooperative
Effort (Clark and Wilkes-Gibbs, 1986), it should
adopt the simplest means to (spatially) refer to an
object. However, research on spatial language in
human-robot interaction has primarily focused on
the use of projective prepositions.
We currently lack a comprehensive model for
topological prepositions. Without such a model,
745
a robot cannot interpret spatial proximity expres-
sions nor motivate their contextually and pragmat-
ically appropriate use. In this paper, we present
a model that addresses this problem. The model
uses energy functions, modulated by visual and
discourse salience, to model how spatial templates
associated with other landmarks may interfere to
establish what are contextually appropriate ways
to locate a target relative to these landmarks. The
model enables grounding of spatial expressions
using spatial proximity to refer to objects in the
environment. We focus on expressions using topo-
logical prepositions such as ?near? or ?at?.
Terminology. We use the term target (T) to
refer to the object that is being located by a spa-
tial expression, and landmark (L) to refer to the
object relative to which the target?s location is de-
scribed: ?[The man]T near [the table]L.? A dis-
tractor is any object in the visual context that is
neither landmark nor target.
Overview ?2 presents contextual effects we can
observe in grounding spatial expressions, includ-
ing the effect of interference on whether two ob-
jects may be considered proximal. ?3 discusses a
model that accounts for all these effects, and ?4 de-
scribes an experiment to test the model. ?5 shows
how we use the model in linguistic interpretation.
2 Data
Below we discuss previous psycholinguistic expe-
rients, focusing on how contextual factors such as
distance, size, and salience may affect proximity.
We also present novel examples, showing that the
location of other objects in a scene may interfere
with the acceptability of a proximal description to
locate a target relative to a landmark. These exam-
ples motivate the model in ?3.
 
 
  
1.74 1.90 2.84 3.16 2.34 1.81 2.13 
2.61 3.84 4.66 4.97 4.90 3.56 3.26 
4.06 5.56 7.55 7.97 7.29 4.80 3.91 
3.47 4.81 6.94 7.56 7.31 5.59 3.63 
4.47 5.91 8.52 O 7.90 6.13 4.46 
3.25 4.03 4.50 4.78 4.41 3.47 3.10 
1.84 2.23 2.03 3.06 2.53 2.13 2.00 
Figure 2: 7-by-7 cell grid with mean goodness ratings for
the relation the X is near O as a function of the position oc-
cupied by X.
Spatial reasoning is a complex activity that in-
volves at least two levels of processing: a geomet-
ric level where metric, topological, and projective
properties are handled, (Herskovits, 1986); and a
functional level where the normal function of an
entity affects the spatial relationships attributed to
it in a context, cf. (Coventry and Garrod, 2004).
We focus on geometric factors.
Although a lot of experimental work has been
done on spatial reasoning and language (cf.
(Coventry and Garrod, 2004)), only Logan and
Sadler (1996) examined topological prepositions
in a context where functional factors were ex-
cluded. They introduced the notion of a spatial
template. The template is centred on the land-
mark and identifies for each point in its space the
acceptability of the spatial relationship between
the landmark and the target appearing at that point
being described by the preposition. Logan &
Sadler examined various spatial prepositions this
way. In their experiments, a human subject was
shown sentences of the form ?the X is [relation]
the O?, each with a picture of a spatial configura-
tion of an O in the center of an invisible 7-by-7
cell grid, and an X in one of the 48 surrounding
positions. The subject then had to rate how well
the sentence described the picture, on a scale from
1(bad) to 9(good). Figure 2 gives the mean good-
ness rating for the relation ?near to? as a function
of the position occupied by X (Logan and Sadler,
1996). It is clear from Figure 2 that ratings dimin-
ish as the distance between X and O increases, but
also that even at the extremes of the grid the rat-
ings were still above 1 (min. rating).
Besides distance there are also other factors that
determine the applicability of a proximal relation.
For example, given prototypical size, the region
denoted by ?near the building? is larger than that
of ?near the apple? (Gapp, 1994). Moreover, an
object?s salience influences the determination of
the proximal region associated with it (Regier and
Carlson, 2001; Roy, 2002).
Finally, the two scenes in Figure 3 show inter-
ference as a contextual factor. For the scene on the
left we can use ?the blue box is near the black box?
to describe object (c). This seems inappropriate in
the scene on the right. Placing an object (d) beside
(b) appears to interfere with the appropriateness
of using a proximal relation to locate (c) relative
to (b), even though the absolute distance between
(c) and (b) has not changed.
Thus, there is empirical evidence for several
746
Figure 3: Proximity and distance
contextual factors determining the applicability of
a proximal description. We argued that the loca-
tion of other distractor objects in context may also
interfere with this applicability. The model in ?3
captures all these factors, and is evaluated in ?4.
3 Computational Model
Below we describe a model of relative proximity
that uses (1) the distance between objects, (2) the
size and salience of the landmark object, and (3)
the location of other objects in the scene. Our
model is based on first computing absolute prox-
imity between each point and each landmark in a
scene, and then combining or overlaying the re-
sulting absolute proximity fields to compute the
relative proximity of each point to each landmark.
3.1 Computing absolute proximity fields
We first compute for each landmark an absolute
proximity field giving each point?s proximity to
that landmark, independent of proximity to any
other landmark. We compute fields on the pro-
jection of the scene onto the 2D-plane, a 2D-array
ARRAY of points. At each point P in ARRAY ,
the absolute proximity for landmark L is
proxabs = (1 ? distnormalised(L,P,ARRAY ))
? salience(L).
(1)
In this equation the absolute proximity for a
point P and a landmark L is a function of both
the distance between the point and the location of
the landmark, and the salience of the landmark.
To represent distance we use a normalised
distance function distnormalised (L, P, ARRAY ),
which returns a value between 0 and 1.1 The
smaller the distance between L and P , the higher
the absolute proximity value returned, i.e. the
more acceptable it is to say that P is close to L. In
this way, this component of the absolute proximity
field captures the gradual gradation in applicabil-
ity evident in Logan and Sadler (1996).
1We normalise by computing the distance between the
two points, and then dividing this distance it by the maximum
distance between point L and any point in the scene.
We model the influence of visual and dis-
course salience on absolute proximity as a func-
tion salience(L), returning a value between 0 and
1 that represents the relative salience of the land-
mark L in the scene (2). The relative salience of
an object is the average of its visual salience (Svis )
and discourse salience (Sdisc),
salience(L) = (Svis(L) + Sdisc(L))/2 (2)
Visual salience Svis is computed using the algo-
rithm of Kelleher and van Genabith (2004). Com-
puting a relative salience for each object in a scene
is based on its perceivable size and its centrality
relative to the viewer?s focus of attention. The al-
gorithm returns scores in the range of 0 to 1. As
the algorithm captures object size we can model
the effect of landmark size on proximity through
the salience component of absolute proximity. The
discourse salience (Sdisc) of an object is computed
based on recency of mention (Hajicova?, 1993) ex-
cept we represent the maximum overall salience in
the scene as 1, and use 0 to indicate that the land-
mark is not salient in the current context. 
 
0
0.1
0.2
0.3
0.4
0.5
0.?
0.?
0.?
0.?
1
?-3?-3? ?-2?-2? ?-1?-1? L ?1?1? ?2?2? ?3?3?
point location
pro
xim
ity 
rati
n?
???ol?te proximity to L? ?alien?e 1
???ol?te proximity to L? ?alien?e 0.?
???ol?te proximity to L? ?alien?e 0.5
 
Figure 4: Absolute proximity ratings for landmark L cen-
tered in a 2D plane, points ranging from plane?s upper-left
corner (<-3,-3>) to lower right corner(<3,3>).
Figure 4 shows computed absolute proximity
with salience values of 1, 0.6, and 0.5, for points
from the upper-left to the lower-right of a 2D
plane, with the landmark at the center of that
plane. The graph shows how salience influences
absolute proximity in our model: for a landmark
with high salience, points far from the landmark
can still have high absolute proximity to it.
3.2 Computing relative proximity fields
Once we have constructed absolute proximity
fields for the landmarks in a scene, our next step
is to overlay these fields to produce a measure of
747
relative proximity to each landmark at each point.
For this we first select a landmark, and then iter-
ate over each point in the scene comparing the ab-
solute proximity of the selected landmark at that
point with the absolute proximity of all other land-
marks at that point. The relative proximity of a
selected landmark at a point is equal to the abso-
lute proximity field for that landmark at that point,
minus the highest absolute proximity field for any
other landmark at that point (see Equation 3).
proxrel(P,L) = proxabs(P,L)? MAX
?LX #=L
proxabs(P,LX )
(3)
The idea here is that the other landmark with the
highest absolute proximity is acting in competi-
tion with the selected landmark. If that other land-
mark?s absolute proximity is higher than the ab-
solute proximity of the selected landmark, the se-
lected landmark?s relative proximity for the point
will be negative. If the competing landmark?s ab-
solute proximity is slightly lower than the abso-
lute proximity of the selected landmark, the se-
lected landmark?s relative proximity for the point
will be positive, but low. Only when the compet-
ing landmark?s absolute proximity is significantly
lower than the absolute proximity of the selected
landmark will the selected landmark have a high
relative proximity for the point in question.
In (3) the proximity of a given point to a se-
lected landmark rises as that point?s distance from
the landmark decreases (the closer the point is to
the landmark, the higher its proximity score for the
landmark will be), but falls as that point?s distance
from some other landmark decreases (the closer
the point is to some other landmark, the lower its
proximity score for the selected landmark will be).
Figure 5 shows the relative proximity fields of two
landmarks, L1 and L2, computed using (3), in a
1-dimensional (linear) space. The two landmarks
have different degrees of salience: a salience of
0.5 for L1 and of 0.6 for L2 (represented by the
different sizes of the landmarks). In this figure,
any point where the relative proximity for one par-
ticular landmark is above the zero line represents
a point which is proximal to that landmark, rather
than to the other landmark. The extent to which
that point is above zero represents its degree of
proximity to that landmark. The overall proximal
area for a given landmark is the overall area for
which its relative proximity field is above zero.
The left and right borders of the figure represent
the boundaries (walls) of the area.
Figure 5 illustrates three main points. First, the
overall size of a landmark?s proximal area is a
function of the landmark?s position relative to the
other landmark and to the boundaries. For exam-
ple, landmark L2 has a large open space between
it and the right boundary: Most of this space falls
into the proximal area for that landmark. Land-
mark L1 falls into quite a narrow space between
the left boundary and L2. L1 thus has a much
smaller proximal area in the figure than L2. Sec-
ond, the relative proximity field for some land-
mark is a function of that landmark?s salience.
This can be seen in Figure 5 by considering the
space between the two landmarks. In that space
the width of the proximal area for L2 is greater
than that of L1, because L2 is more salient.
The third point concerns areas of ambiguous
proximity in Figure 5: areas in which neither of
the landmarks have a significantly higher relative
proximity than the other. There are two such areas
in the Figure. The first is between the two land-
marks, in the region where one relative proxim-
ity field line crosses the other. These points are
ambiguous in terms of relative proximity because
these points are equidistant from those two land-
marks. The second ambiguous area is at the ex-
treme right of the space shown in Figure 5. This
area is ambiguous because this area is distant from
both landmarks: points in this area would not be
judged proximal to either landmark. The ques-
tion of ambiguity in relative proximity judgments
is considered in more detail in ?5. 
 
????
????
????
????
????
?
???
???
???
???
???
?? ??
point lo?ation?
rela
tive
 pro
xim
ity
???????? ????? ????? ????? ?? ??
???????? ????? ??? ?? ????? ?? ??
 
Figure 5: Graph of relative proximity fields for two land-
marks L1 and L2. Relative proximity fields were computed
with salience scores of 0.5 for L1 and 0.6 for L2.
4 Experiment
Below we describe an experiment which tests our
approach (?3) to relative proximity by examining
748
the changes in people?s judgements of the appro-
priateness of the expression near being used to de-
scribe the relationship between a target and land-
mark object in an image where a second, distractor
landmark is present. All objects in these images
were coloured shapes, a circle, triangle or square.
4.1 Material and Procedure
All images used in this experiment contained a
central landmark object and a target object, usu-
ally with a third distractor object. The landmark
was always placed in the middle of a 7-by-7 grid.
Images were divided into 8 groups of 6 images
each. Each image in a group contained the target
object placed in one of 6 different cells on the grid,
numbered from 1 to 6. Figure 6 shows how we
number these target positions according to their
nearness to the landmark. 
 
  
       
       
       
       
       
       
       
1 2 
4 5 a 
6 
g L c 
e 
b 
d f 
3 
Figure 6: Relative locations of landmark (L) target posi-
tions (1..6) and distractor landmark positions (a..g) in images
used in the experiment.
Groups are organised according to the presence
and position of a distractor object. In group a the
distractor is directly above the landmark, in group
b the distractor is rotated 45 degrees clockwise
from the vertical, in group c it is directly to the
right of the landmark, in d it is rotated 135 de-
grees clockwise from the vertical, and so on. The
distractor object is always the same distance from
the central landmark. In addition to the distractor
groups a,b,c,d,e,f and g, there is an eighth group,
group x, in which no distractor object occurs.
In the experiment, each image was displayed
with a sentence of the form The is near the ,
with a description of the target and landmark re-
spectively. The sentence was presented under the
image. 12 participants took part in this experi-
ment. Participants were asked to rate the accept-
ability of the sentence as a description of the im-
age using a 10-point scale, with zero denoting not
acceptable at all; four or five denoting moderately
acceptable; and nine perfectly acceptable.
4.2 Results and Discussion
We assess participants? responses by comparing
their average proximity judgments with those pre-
dicted by the absolute proximity equation (Equa-
tion 1), and by the relative proximity equation
(Equation 3). For both equations we assume
that all objects have a salience score of 1. With
salience equal to 1, the absolute proximity equa-
tion relates proximity between target and land-
mark objects to the distance between those two ob-
jects, so that the closer the target is to the landmark
the higher its proximity will be. With salience
equal to 1, the relative proximity equation re-
lates proximity to both distance between target and
landmark and distance between target and distrac-
tor, so that the proximity of a given target object
to a landmark rises as that target?s distance from
the landmark decreases but falls as the target?s dis-
tance from some other distractor object decreases.
Figure 7 shows graphs comparing participants?
proximity ratings with the proximity scores com-
puted by Equation 1 (the absolute proximity equa-
tion), and by Equation 3 (the relative proximity
equation), for the images in group x and in the
other 7 groups. In the first graph there is no dif-
ference between the proximity scores computed
by the two equations, since, when there is no dis-
tractor object present the relative proximity equa-
tion reduces to the absolute proximity equation.
The correlation between both computed proximity
scores and participants? average proximity scores
for this group is quite high (r = 0.95). For the re-
maining 7 groups the proximity value computed
from Equation 1 gives a fair match to people?s
proximity judgements for target objects (the aver-
age correlation across these seven groups in Fig-
ure 7 is around r = 0.93). However, relative
proximity score as computed in Equation 3 signifi-
cantly improves the correlation in each graph, giv-
ing an average correlation across the seven groups
of around r = 0.99 (all correlations in Figure 7
are significant p < 0.01).
Given that the correlations for both Equation 1
and Equation 3 are high we examined whether the
results returned by Equation 3 were reliably closer
to human judgements than those from Equation 1.
For the 42 images where a distractor object was
present we recorded which equation gave a result
that was closer to participants? normalised aver-
749
??
?
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
????????????????????????????
? ??? ? ?????? ???????
????????????????????????????
? ??? ? ?????? ???????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?????????????????????????????
????????? ???????????
????????????????????????????
????????????????????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?????????????????????????????
? ??? ???????? ??? ??
????????????????????????????
????????????????????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?????????????????????????????
? ??? ??? ???? ??????
????????????????????????????
? ??? ??? ???? ?????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?????????????????????????????
??????? ?????????????
????????????????????????????
??????? ???????????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?????????????????????????????
? ??? ??? ???? ??????
????????????????????????????
? ??? ??? ???? ?????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?????????????????????????????
? ??? ??? ???? ??????
????????????????????????????
????????????????????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?
?????????????????????????????
? ??? ??? ???? ??????
????????????????????????????
????????????????????
?????????????????
Figure 7: comparison between normalised proximity scores observed and computed for each group.
age for that image. In 28 cases Equation 3 was
closer, while in 14 Equation 1 was closer (a 2:1
advantage for Equation 3, significant in a sign test:
n+ = 28, n? = 14, Z = 2.2, p < 0.05). We con-
clude that proximity judgements for objects in our
experiment are best represented by relative prox-
imity as computed in Equation 3. These results
support our ?relative? model of proximity.2
It is interesting to note that Equation 3 over-
estimates proximity in the cases (a, b and g)
2Note that, in order to display the relationship between
proximity values given by participants, computed in Equa-
tion 1, and computed in Equation 3, the values displayed in
Figure 7 are normalised so that proximity values have a mean
of 0 and a standard deviation of 1. This normalisation simply
means that all values fall in the same region of the scale, and
can be easily compared visually.
where the distractor object is closest to the targets
and slightly underestimates proximity in all other
cases. We will investigate this in future work.
5 Expressing spatial proximity
We use the model of ?3 to interpret spatial ref-
erences to objects. A fundamental requirement
for processing situated dialogue is that linguistic
meaning provides enough information to establish
the visual grounding of spatial expressions: How
can the robot relate the meaning of a spatial ex-
pression to a scene it visually perceives, so it can
locate the objects which the expression applies to?
Approaches agree here on the need for ontolog-
ically rich representations, but differ in how these
are to be visually grounded. Oates et al (2000)
750
and Roy (2002) use machine learning to obtain
a statistical mapping between visual and linguis-
tic features. Gorniak and Roy (2004) use manu-
ally constructed mappings between linguistic con-
structions, and probabilistic functions which eval-
uate whether an object can act as referent, whereas
DeVault and Stone (2004) use symbolic constraint
resolution. Our approach to visual grounding of
language is similar to the latter two approaches.
We use a Combinatory Categorial Grammar
(CCG) (Baldridge and Kruijff, 2003) to describe
the relation between the syntactic structure of
an utterance and its meaning. We model mean-
ing as an ontologically richly sorted, relational
structure, using a description logic-like framework
(Baldridge and Kruijff, 2002). We use OpenCCG
for parsing and realization.3
(2) the box near the ball
@{b:phys?obj}(box
& ?Delimitation?unique
& ?Number?singular
& ?Quantification?specific singular)
& @{b:phys?obj}?Location?(r : region & near
& ?Proximity?proximal
& ?Positioning?static)
& @{r :region}?FromWhere?(b1 : phys ? obj
& ball
& ?Delimitation?unique
& ?Number?singular
& ?Quantification?specific singular)
Example (2) shows the meaning representation
for ?the box near the ball?. It consists of sev-
eral, related elementary predicates (EPs). One
type of EP represents a discourse referent as a
proposition with a handle: @{b:phys?obj}(box)
means that the referent b is a physical object,
namely a box. Another type of EP states de-
pendencies between referents as modal relations,
e.g. @{b:phys?obj}?Location?(r : region & near)
means that discourse referent b (the box) is located
in a region r that is near to a landmark. We repre-
sent regions explicitly to enable later reference to
the region using deictic reference (e.g. ?there?).
Within each EP we can have semantic features,
e.g. the region r characterizes a static location of b
and expresses proximity to a landmark. Example
(2) gives a ball in the context as the landmark.
We use the sorting information in the utter-
ance?s meaning (e.g. phys-obj, region) for further
3http://www.sf.net/openccg/
interpretation using ontology-based spatial rea-
soning. This yields several inferences that need to
hold for the scene, like DeVault and Stone (2004).
Where we differ is in how we check whether these
inferences hold. Like Gorniak and Roy (2004), we
map these conditions onto the energy landscape
computed by the proximity field functions. This
enables us to take into account inhibition effects
arising in the actual situated context, unlike Gor-
niak & Roy or DeVault & Stone.
We convert relative proximity fields into prox-
imal regions anchored to landmarks to contextu-
ally interpret linguistic meaning. We must decide
whether a landmark?s relative proximity score at
a given point indicates that it is ?near? or ?close
to? or ?at? or ?beside? the landmark. For this we
iterate over each point in the scene, and compare
the relative proximity scores of the different land-
marks at each point. If the primary landmark?s
(i.e., the landmark with the highest relative prox-
imity at the point) relative proximity exceeds the
next highest relative proximity score by more than
a predefined confidence interval the point is in the
vague region anchored around the primary land-
mark. Otherwise, we take it as ambiguous and not
in the proximal region that is being interpreted.
The motivation for the confidence interval is to
capture situations where the difference in relative
proximity scores between the primary landmark
and one or more landmarks at a given point is rel-
atively small. Figure 8 illustrates the parsing of a
scene into the regions ?near? two landmarks. The
relative proximity fields of the two landmarks are
identical to those in Figure 5, using a confidence
interval of 0.1. Ambiguous points are where the
proximity ambiguity series is plotted at 0.5. The
regions ?near? each landmark are those areas of
the graph where each landmark?s relative proxim-
ity series is the highest plot on the graph.
Figure 8 illustrates an important aspect of our
model: the comparison of relative proximity fields
naturally defines the extent of vague proximal re-
gions. For example, see the region right of L2 in
Figure 8. The extent of L2?s proximal region in
this direction is bounded by the interference ef-
fect of L1?s relative proximity field. Because the
landmarks? relative proximity scores converge, the
area on the far right of the image is ambiguous
with respect to which landmark it is proximal to.
In effect, the model captures the fact that the area
is relatively distant from both landmarks. Follow-
751
Figure 8: Graph of ambiguous regions overlaid on relative
proximity fields for landmarks L1 and L2, with confidence
interval=0.1 and different salience scores for L1 (0.5) and L2
(0.6). Locations of landmarks are marked on the X-axis.
ing the cognitive load model (?1), objects located
in this region should be described with a projective
relation such as ?to the right of L2? rather than a
proximal relation like ?near L2?, see Kelleher and
Kruijff (2006).
6 Conclusions
We addressed the issue of how we can provide
a context-dependent interpretation of spatial ex-
pressions that identify objects based on proxim-
ity in a visual scene. We discussed available
psycholinguistic data to substantiate the useful-
ness of having such a model for interpreting and
generating fluent situated dialogue between a hu-
man and a robot, and that we need a context-
dependent representation of what is (situationally)
appropriate to consider proximal to a landmark.
Context-dependence thereby involves salience of
landmarks as well as inhibition effects between
landmarks. We presented a model in which we
can address these issues, and we exemplified how
logical forms representing the meaning of spa-
tial proximity expressions can be grounded in this
model. We tested and verified the model using a
psycholinguistic experiment. Future work will ex-
amine whether the model can be used to describe
the semantics of nouns (such as corner) that ex-
press vague spatial extent, and how the model re-
lates to the functional aspects of spatial reasoning.
References
J. Baldridge and G.J.M. Kruijff. 2002. Coupling CCG and
hybrid logic dependency semantics. In Proceedings of
ACL 2002, Philadelphia, Pennsylvania.
J. Baldridge and G.J.M. Kruijff. 2003. Multi-modal combi-
natory categorial grammar. In Proceedings of EACL 2003,
Budapest, Hungary.
H. Clark and D. Wilkes-Gibbs. 1986. Referring as a collab-
orative process. Cognition, 22:1?39.
K.R. Coventry and S. Garrod. 2004. Saying, Seeing and
Acting. The Psychological Semantics of Spatial Preposi-
tions. Essays in Cognitive Psychology Series. Lawrence
Erlbaum Associates.
R. Dale and E. Reiter. 1995. Computatinal interpretations of
the gricean maxims in the generation of referring expres-
sions. Cognitive Science, 18:233?263.
D. DeVault and M. Stone. 2004. Interpreting vague utter-
ances in context. In Proceedings of COLING 2004, vol-
ume 2, pages 1247?1253, Geneva, Switzerland.
K.P. Gapp. 1994. Basic meanings of spatial relations: Com-
putation and evaluation in 3d space. In Proceedings of
AAAI-94, pages 1393?1398.
P. Gorniak and D. Roy. 2004. Grounded semantic compo-
sition for visual scenes. Journal of Artificial Intelligence
Research, 21:429?470.
E. Hajicova?. 1993. Issues of Sentence Structure and Dis-
course Patterns, volume 2 of Theoretical and Computa-
tional Linguistics. Charles University Press.
A Herskovits. 1986. Language and spatial cognition: An
interdisciplinary study of prepositions in English. Stud-
ies in Natural Language Processing. Cambridge Univer-
sity Press.
J.D. Kelleher and G.J. Kruijff. 2006. Incremental genera-
tion of spatial referring expressions in situated dialog. In
Proceedings ACL/COLING ?06, Sydney, Australia.
J. Kelleher and J. van Genabith. 2004. Visual salience and
reference resolution in simulated 3d environments. AI Re-
view, 21(3-4):253?267.
E. Krahmer and M. Theune. 1999. Efficient generation of
descriptions in context. In R. Kibble and K. van Deemter,
editors, Workshop on the Generation of Nominals, ESS-
LLI?99, Utrecht, The Netherlands.
G.D. Logan and D.D. Sadler. 1996. A computational analy-
sis of the apprehension of spatial relations. In M. Bloom,
P.and Peterson, L. Nadell, and M. Garrett, editors, Lan-
guage and Space, pages 493?529. MIT Press.
G.D. Logan. 1994. Spatial attention and the apprehension
of spatial relations. Journal of Experimental Psychology:
Human Perception and Performance, 20:1015?1036.
G.D. Logan. 1995. Linguistic and conceptual control of vi-
sual spatial attention. Cognitive Psychology, 12:523?533.
R. Moratz and T. Tenbrink. 2006. Spatial reference in
linguistic human-robot interaction: Iterative, empirically
supported development of a model of projective relations.
Spatial Cognition and Computation.
T. Oates, Z. Eyler-Walker, and P.R. Cohen. 2000. Toward
natural language interfaces for robotic agents: Ground-
ing linguistic meaning in sensors. In Proceedings of the
Fourth International Conference on Autonomous Agents,
pages 227?228.
T Regier and L. Carlson. 2001. Grounding spatial language
in perception: An empirical and computational investi-
gation. Journal of Experimental Psychology: General,
130(2):273?298.
D.K. Roy. 2002. Learning words and syntax for a scene
description task. Computer Speech and Language, 16(3).
I.F. van der Sluis and E.J. Krahmer. 2004. The influence of
target size and distance on the production of speech and
gesture in multimodal referring expressions. In R. Kibble
and K. van Deemter, editors, ICSLP04.
752
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1041?1048,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Incremental generation of spatial referring expressions
in situated dialog?
John D. Kelleher
Dublin Institute of Technology
Dublin, Ireland
john.kelleher@comp.dit.ie
Geert-Jan M. Kruijff
DFKI GmbH
Saarbru?cken, Germany
gj@dfki.de
Abstract
This paper presents an approach to incrementally
generating locative expressions. It addresses the is-
sue of combinatorial explosion inherent in the con-
struction of relational context models by: (a) con-
textually defining the set of objects in the context
that may function as a landmark, and (b) sequenc-
ing the order in which spatial relations are consid-
ered using a cognitively motivated hierarchy of re-
lations, and visual and discourse salience.
1 Introduction
Our long-term goal is to develop conversational
robots with whom we can interact through natural,
fluent, visually situated dialog. An inherent as-
pect of visually situated dialog is reference to ob-
jects located in the physical environment (Moratz
and Tenbrink, 2006). In this paper, we present a
computational approach to the generation of spa-
tial locative expressions in such situated contexts.
The simplest form of locative expression is a
prepositional phrase, modifying a noun phrase to
locate an object. (1) illustrates the type of locative
we focus on generating. In this paper we use the
term target (T) to refer to the object that is being
located by a spatial expression and the term land-
mark (L) to refer to the object relative to which
the target?s location is described.
(1) a. the book [T] on the table [L]
Generating locative expressions is part of the
general field of generating referring expressions
(GRE). Most GRE algorithms deal with the same
problem: given a domain description and a target
object, generate a description of the target object
that distinguishes it from the other objects in the
domain. We use distractor objects to indicate the
?The research reported here was supported by the CoSy
project, EU FP6 IST ?Cognitive Systems? FP6-004250-IP.
objects in the context excluding the target that at
a given point in processing fulfill the description
of the target object that has been generated. The
description generated is said to be distinguishing
if the set of distractor objects is empty.
Several GRE algorithms have addressed the is-
sue of generating locative expressions (Dale and
Haddock, 1991; Horacek, 1997; Gardent, 2002;
Krahmer and Theune, 2002; Varges, 2004). How-
ever, all these algorithms assume the GRE compo-
nent has access to a predefined scene model. For
a conversational robot operating in dynamic envi-
ronments this assumption is unrealistic. If a robot
wishes to generate a contextually appropriate ref-
erence it cannot assume the availability of a fixed
scene model, rather it must dynamically construct
one. However, constructing a model containing all
the relationships between all the entities in the do-
main is prone to combinatorial explosion, both in
terms of the number objects in the context (the lo-
cation of each object in the scene must be checked
against all the other objects in the scene) and num-
ber of inter-object spatial relations (as a greater
number of spatial relations will require a greater
number of comparisons between each pair of ob-
jects).1 Also, the context free a priori construction
of such an exhaustive scene model is cognitively
implausible. Psychological research indicates that
spatial relations are not preattentively perceptually
available (Treisman and Gormican, 1988), their
perception requires attention (Logan, 1994; Lo-
gan, 1995). Subjects appear to construct contex-
tually dependent reduced relational scene models,
not exhaustive context free models.
Contributions We present an approach to in-
1In English, the vast majority of spatial locatives are bi-
nary, some notable exceptions include: between, amongst etc.
However, we will not deal with these exceptions in this paper.
1041
crementally generating locative expressions. It ad-
dresses the issue of combinatorial explosion in-
herent in relational scene model construction by
incrementally creating a series of reduced scene
models. Within each scene model only one spatial
relation is considered and only a subset of objects
are considered as candidate landmarks. This re-
duces both the number of relations that must be
computed over each object pair and the number of
object pairs. The decision as to which relations
should be included in each scene model is guided
by a cognitively motivated hierarchy of spatial re-
lations. The set of candidate landmarks in a given
scene is dependent on the set of objects in the
scene that fulfil the description of the target object
and the relation that is being considered.
Overview ?2 presents some relevant back-
ground data. ?3 presents our GRE approach. ?4
illustrates the framework on a worked example
and expands on some of the issues relevant to the
framework. We end with conclusions.
2 Data
If we consider that English has more than eighty
spatial prepositions (omitting compounds such as
right next to) (Landau, 1996), the combinatorial
aspect of relational scene model construction be-
comes apparent. It should be noted that for our
purposes, the situation is somewhat easier because
a distinction can be made between static and dy-
namic prepositions: static prepositions primarily2
denote the location of an object, dynamic preposi-
tions primarily denote the path of an object (Jack-
endoff, 1983; Herskovits, 1986), see (2). How-
ever, even focusing just on the set of static prepo-
sitions does not remove the combinatorial issues
effecting the construction of a scene model.
(2) a. the tree is behind [static] the house
b. the man walked across [dyn.] the road
In general, static prepositions can be divided
into two sets: topological and projective. Topo-
logical prepositions are the category of preposi-
tions referring to a region that is proximal to the
landmark; e.g., at, near, etc. Often, the distinc-
tions between the semantics of the different topo-
logical prepositions is based on pragmatic con-
traints, e.g. the use of at licences the target to be
2Static prepositions can be used in dynamic contexts, e.g.
the man ran behind the house, and dynamic prepositions can
be used in static ones, e.g. the tree lay across the road.
in contact with the landmark, whereas the use of
near does not. Projective prepositions describe a
region projected from the landmark in a particular
direction; e.g., to the right of, to the left of. The
specification of the direction is dependent on the
frame of reference being used (Herskovits, 1986).
Static prepositions have both qualitative and
quantitative semantic properties. The qualitative
aspect is evident when they are used to denote an
object by contrasting its location with that of the
distractor objects. Using Figure 1 as visual con-
text, the locative expression the circle on the left
of the square illustrates the contrastive semantics
of a projective preposition, as only one of the cir-
cles in the scene is located in that region. Taking
Figure 2, the locative expression the circle near
the black square shows the contrastive semantics
of a topological preposition. Again, of the two cir-
cles in the scene only one of them may be appro-
priately described as being near the black square,
the other circle is more appropriately described as
being near the white square. The quantitative as-
pect is evident when a static preposition denotes
an object using a relative scale. In Figure 3 the
locative the circle to the right of the square shows
the relative semantics of a projective preposition.
Although both the circles are located to the right of
the square we can distinguish them based on their
location in the region. Figure 3 also illustrates the
relative semantics of a topological preposition Fig-
ure 3. We can apply a description like the circle
near the square to either circle if none other were
present. However, if both are present we can inter-
pret the reference based on relative proximity to
the landmark the square.
Figure 1: Visual context illustrating contrastive se-
mantics of projective prepositions
Figure 2: Visual context illustrating contrastive se-
mantics of topological prepositions
Figure 3: Visual context illustrating relative se-
mantics of topological and projective prepositions
1042
3 Approach
We base our GRE approach on an extension of
the incremental algorithm (Dale and Reiter, 1995).
The motivation for basing our approach on this al-
gorithm is its polynomial complexity. The algo-
rithm iterates through the properties of the target
and for each property computes the set of distrac-
tor objects for which (a) the conjunction of the
properties selected so far, and (b) the current prop-
erty hold. A property is added to the list of se-
lected properties if it reduces the size of the dis-
tractor object set. The algorithm succeeds when
all the distractors have been ruled out, it fails if all
the properties have been processed and there are
still some distractor objects. The algorithm can
be refined by ordering the checking of properties
according to fixed preferences, e.g. first a taxo-
nomic description of the target, second an absolute
property such as colour, third a relative property
such as size. (Dale and Reiter, 1995) also stipulate
that the type description of the target should be in-
cluded in the description even if its inclusion does
not make the target distinguishable.
We extend the original incremental algorithm
in two ways. First we integrate a model of ob-
ject salience by modifying the condition under
which a description is deemed to be distinguish-
ing: it is, if all the distractors have been ruled out
or if the salience of the target object is greater
than the highest salience score ascribed to any
of the current distractors. This is motivated by
the observation that people can easily resolve un-
derdetermined references using salience (Duwe
and Strohner, 1997). We model the influence
of visual and discourse salience using a function
salience(L), Equation 1. The function returns
a value between 0 and 1 to represent the relative
salience of a landmark L in the scene. The relative
salience of an object is the average of its visual
salience (Svis ) and discourse salience (Sdisc),
salience(L) = (Svis(L) + Sdisc(L))/2 (1)
Visual salience Svis is computed using the algo-
rithm of (Kelleher and van Genabith, 2004). Com-
puting a relative salience for each object in a scene
is based on its perceivable size and its centrality
relative to the viewer focus of attention, return-
ing scores in the range of 0 to 1. The discourse
salience (Sdisc) of an object is computed based
on recency of mention (Hajicova?, 1993) except
we represent the maximum overall salience in the
scene as 1, and use 0 to indicate that the landmark
is not salient in the current context. Algorithm 1
gives the basic algorithm with salience.
Algorithm 1 The Basic Incremental Algorithm
Require: T = target object; D = set of distractor objects.
Initialise: P = {type, colour, size}; DESC = {}
for i = 0 to |P | do
if T salience() >MAXDISTRACTORSALIENCE then
Distinguishing description generated
if type(x) !? DESC then
DESC = DESC ? type(x)
end if
return DESC
else
D? = {x : x ? D,P i(x) = P i(T )}
if |D?| < |D| then
DESC = DESC ? P i(T )
D = {x : x ? D,P i(x) = P i(T )}
end if
end if
end for
Failed to generate distinguishing description
return DESC
Secondly, we extend the incremental algorithm
in how we construct the context model used by
the algorithm. The context model determines to
a large degree the output of the incremental al-
gorithm. However, Dale and Reiter do not de-
fine how this set should be constructed, they only
write: ?[w]e define the context set to be the set of
entities that the hearer is currently assumed to be
attending to? (Dale and Reiter, 1995, pg. 236).
Before applying the incremental algorithm we
must construct a context model in which we can
check whether or not the description generated
distinguishes the target object. To constrain the
combinatorial explosion in relational scene model
construction we construct a series of reduced
scene models, rather than one complex exhaus-
tive model. This construction is driven by a hi-
erarchy of spatial relations and the partitioning of
the context model into objects that may and may
not function as landmarks. These two components
are developed below. ?3.1 discusses a hierarchy of
spatial relations, and ?3.2 presents a classification
of landmarks and uses these groupings to create a
definition of a distinguishing locative description.
In ?3.3 we give the generation algorithm integrat-
ing these components.
3.1 Cognitive Ordering of Contexts
Psychological research indicates that spatial re-
lations are not preattentively perceptually avail-
able (Treisman and Gormican, 1988). Rather,
their perception requires attention (Logan, 1994;
1043
Logan, 1995). These findings point to subjects
constructing contextually dependent reduced rela-
tional scene models, rather than an exhaustive con-
text free model. Mimicking this, we have devel-
oped an approach to context model construction
that constrains the combinatorial explosion inher-
ent in the construction of relational context mod-
els by incrementally building a series of reduced
context models. Each context model focuses on
a different spatial relation. The ordering of the
spatial relations is based on the cognitive load of
interpreting the relation. Below we motivate and
develop the ordering of relations used.
We can reasonably asssume that it takes less
effort to describe one object than two. Follow-
ing the Principle of Minimal Cooperative Effort
(Clark and Wilkes-Gibbs, 1986), one should only
use a locative expression when there is no distin-
guishing description of the target object using a
simple feature based approach. Also, the Princi-
ple of Sensitivity (Dale and Reiter, 1995) states
that when producing a referring expression, one
should prefer features the hearer is known to be
able to interpret and see. This points to a prefer-
ence, due to cognitive load, for descriptions that
identify an object using purely physical and easily
perceivable features ahead of descriptions that use
spatial expressions. Experimental results support
this (van der Sluis and Krahmer, 2004).
Similarly, we can distinguish between the cog-
nitive loads of processing different forms of spa-
tial relations. In comparing the cognitive load as-
sociated with different spatial relations it is im-
portant to recognize that they are represented and
processed at several levels of abstraction. For ex-
ample, the geometric level, where metric prop-
erties are dealt with, the functional level, where
the specific properties of spatial entities deriving
from their functions in space are considered, and
the pragmatic level, which gathers the underly-
ing principles that people use in order to discard
wrong relations or to deduce more information
(Edwards and Moulin, 1998). Our discussion is
grounded at the geometric level.
Focusing on static prepositions, we assume
topological prepositions have a lower percep-
tual load than projective ones, as perceiving
two objects being close to each other is eas-
ier than the processing required to handle frame
of reference ambiguity (Carlson-Radvansky and
Irwin, 1994; Carlson-Radvansky and Logan,
1997). Figure 4 lists the preferences, further
Figure 4: Cognitive load
discerning objects
type as the easi-
est to process, be-
fore absolute grad-
able predicates (e.g.
color), which is still
easier than relative
gradable predicates
(e.g. size) (Dale and Reiter, 1995).
We can refine the topological versus projective
preference further if we consider their contrastive
and relative uses of these relations (?2). Perceiv-
ing and interpreting a contrastive use of a spatial
relation is computationally easier than judging a
relative use. Finally, within projective preposi-
tions, psycholinguistic data indicates a perceptu-
ally based ordering of the relations: above/below
are easier to percieve and interpret than in front
of /behind which in turn are easier than to the right
of /to the left of (Bryant et al, 1992; Gapp, 1995).
In sum, we propose the following ordering: topo-
logical contrastive < topological relative < pro-
jective constrastive < projective relative.
For each level of this hierarchy we require a
computational model of the semantics of the rela-
tion at that level that accomodates both contrastive
and relative representations. In ?2 we noted that
the distinctions between the semantics of the dif-
ferent topological prepositions is often based on
functional and pragmatic issues.3 Currently, how-
ever, more psycholinguistic data is required to dis-
tinguish the cognitive load associated with the dif-
ferent topological prepositions. We use the model
of topological proximity developed in (Kelleher et
al., 2006) to model all the relations at this level.
Using this model we can define the extent of a re-
gion proximal to an object. If the target or one of
the distractor objects is the only object within the
region of proximity around a given landmark this
is taken to model a contrastive use of a topologi-
cal relation relative to that landmark. If the land-
mark?s region of proximity contains more than one
object from the target and distractor object set then
it is a relative use of a topological relation. We
handle the issue of frame of reference ambiguity
and model the semantics of projective prepostions
using the framework developed in (Kelleher et al,
2006). Here again, the contrastive-relative distinc-
3See inter alia (Talmy, 1983; Herskovits, 1986; Vande-
loise, 1991; Fillmore, 1997; Garrod et al, 1999) for more
discussion on these differences
1044
tion is dependent on the number of objects within
the region of space defined by the preposition.
3.2 Landmarks and Descriptions
If we want to use a locative expression, we must
choose another object in the scene to function as
landmark. An implicit assumption in selecting a
landmark is that the hearer can easily identify and
locate the object within the context. A landmark
can be: the speaker (3)a, the hearer (3)b, the scene
(3)c, an object in the scene (3)d, or a group of ob-
jects in the scene (3)e.4
(3) a. the ball on my right [speaker]
b. the ball to your left [hearer]
c. the ball on the right [scene]
d. the ball to the left of the box [an object
in the scene]
e. the ball in the middle [group of ob-
jects]
Currently, we need new empirical research to
see if there is a preference order between these
landmark categories. Intuitively, in most situa-
tions, either of the interlocutors are ideal land-
marks because the speaker can naturally assume
that the hearer is aware of the speaker?s location
and their own. Focusing on instances where an
object in the scene is used as a landmark, several
authors (Talmy, 1983; Landau, 1996; Gapp, 1995)
have noted a target-landmark asymmetry: gener-
ally, the landmark object is more permanently lo-
cated, larger, and taken to have greater geometric
complexity. These characteristics are indicative of
salient objects and empirical results support this
correlation between object salience and landmark
selection (Beun and Cremers, 1998). However, the
salience of an object is intrinsically linked to the
context it is embedded in. For example, in Figure
5 the ball has a relatively high salience, because
it is a singleton, despite the fact that it is smaller
and geometrically less complex than the other fig-
ures. Moreover, in this scene it is the only object
that can function as a landmark without recourse
to using the scene itself or a grouping of objects.
Clearly, deciding which objects in a given con-
text are suitable to function as landmarks is a com-
plex and contextually dependent process. Some
of the factors effecting this decision are object
4See (Gorniak and Roy, 2004) for further discussion on
the use of spatial extrema of the scene and groups of objects
in the scene as landmarks
Figure 5: Landmark salience
salience and the functional relationships between
objects. However, one basic constraint on land-
mark selection is that the landmark should be dis-
tinguishable from the target. For example, given
the context in Figure 5 and all other factors be-
ing equal, using a locative such as the man to the
left of the man would be much less helpful than
using the man to the right of the ball. Following
this observation, we treat an object as a candidate
landmark if the following conditions are met: (1)
the object is not the target, and (2) it is not in the
distractor set either.
Furthermore, a target landmark is a member
of the candidate landmark set that stands in re-
lation to the target. A distractor landmark is a
member of the candidate landmark set that stands
in the considered relation to a distractor object. We
then define a distinguishing locative description
as a locative description where there is target land-
mark that can be distinguished from all the mem-
bers of the set of distractor landmarks under the
relation used in the locative.
3.3 Algorithm
We first try to generate a distinguishing descrip-
tion using Algorithm 1. If this fails, we divide the
context into three components: the target, the dis-
tractor objects, and the set of candidate landmarks.
We then iterate through the set of candidate land-
marks (using a salience ordering if there is more
than one, cf. Equation 1) and try to create a distin-
guishing locative description. The salience order-
ing of the landmarks is inspired by (Conklin and
McDonald, 1982) who found that the higher the
salience of an object the more likely it appears in
the description of the scene it was embedded in.
For each candidate landmark we iterate through
the hierarchy of relations, checking for each re-
lation whether the candidate can function as a tar-
get landmark under that relation. If so we create
a context model that defines the set of target and
distractor landmarks. We create a distinguishing
locative description by using the basic incremental
algorithm to distinguish the target landmark from
the distractor landmarks. If we succeed in generat-
ing a distinguishing locative description we return
1045
the description and stop.
Algorithm 2 The Locative Incremental Algorithm
DESC = Basic-Incremental-Algorithm(T,D)
if DESC != Distinguishing then
create CL the set of candidate landmarks
CL = {x : x != T,DESC(x) = false}
for i = 0 to |CL| by salience(CL) do
for j = 0 to |R| do
if Rj (T,CLi)=true then
TL = {CLi}
DL = {z : z ? CL,Rj (D, z) = true}
LANDDESC = Basic-Incremental-
Algorithm(TL, DL)
if LANDDESC = Distinguishing then
Distinguishing locative generated
return {DESC,Rj ,LANDDESC}
end if
end if
end for
end for
end if
FAIL
If we cannot create a distinguishing locative de-
scription we face two choices: (1) iterate on to the
next relation in the hierarchy, (2) create an embed-
ded locative description distinguishing the land-
mark. We adopt (1) over (2), preferring the dog
to the right of the car over the dog near the car
to the right of the house. However, we can gener-
ate these longer embedded descriptions if needed,
by replacing the call to the basic incremental algo-
rithm for the landmark object with a call to the
whole locative expression generation algorithm,
using the target landmark as the target object and
the set of distractor landmarks as the distractors.
An important point in this context is the issue
of infinite regression (Dale and Haddock, 1991).
A compositional GRE system may in certain con-
texts generate an infinite description, trying to dis-
tinguish the landmark in terms of the target, and
the target in terms of the landmark, cf. (4). But,
this infinite recursion can only occur if the con-
text is not modified between calls to the algorithm.
This issue does not affect Algorithm 2 as each call
to the algorithm results in the domain being parti-
tioned into those objects we can and cannot use as
landmarks. This not only reduces the number of
object pairs that relations must be computed for,
but also means that we need to create a distin-
guishing description for a landmark on a context
that is a strict subset of the context the target de-
scription was generated in. This way the algorithm
cannot distinguish a landmark using its target.
(4) the bowl on the table supporting the bowl
on the table supporting the bowl ...
3.4 Complexity
The computational complexity of the incremental
algorithm is O(nd*nl ), with nd the number of dis-
tractors, and nl the number of attributes in the final
referring description (Dale and Reiter, 1995). This
complexity is independent of the number of at-
tributes to be considered. Algorithm 2 is bound by
the same complexity. For the average case, how-
ever, we see the following. For one, with every
increase in nl , we see a strict decrease in nd : the
more attributes we need, the fewer distractors we
strictly have due to the partitioning into distrac-
tor and target landmarks. On the other hand, we
have the dynamic construction of a context model.
This latter factor is not considered in (Dale and
Reiter, 1995), meaning we would have to multiply
O(nd*nl ) with a constant Kctxt for context con-
struction. Depending on the size of this constant,
we may see an advantage of our algorithm in that
we only consider a single spatial relation each time
we construct a context model, we avoid an expo-
nential number of comparisons: we need to make
at most nd * (nd ? 1) comparisons (and only nd
if relations are symmetric).
4 Discussion
We examplify the approach on the visual scene on
the left of Figure 6. This context consists of two
red boxes R1 and R2 and two blue balls B1 and
B2. Imagine that we want to refer to B1. We be-
gin by calling Algorithm 2. This in turn calls Al-
gorithm 1, returning the property ball. This is not
sufficient to create a distinguishing description as
B2 is also a ball. In this context the set of can-
didate landmarks equals {R1,R2}. We take R1 as
first candidate landmark, and check for topologi-
cal proximity in the scene as modeled in (Kelle-
her et al, 2006). The image on the right of Fig-
ure 6 illustrates the resulting scene analysis: the
green region on the left defines the area deemed to
be proximal to R1, and the yellow region on the
right defines the area proximal to R2. Clearly, B1
is in the area proximal to R1, making R1 a tar-
get landmark. As none of the distractors (i.e., B2)
are located in a region that is proximal to a can-
didate landmark there are no distractor landmarks.
As a result when the basic incremental algorithm
is called to create a distinguishing description for
the target landmark R1 it will return box and this
will be deemed to be a distinguishing locative de-
scription. The overall algorithm will then return
1046
Figure 6: A visual scene and the topological anal-
sis of R1 and R2
the vector {ball, proximal, box} which would re-
sult in the realiser generating a reference of the
form: the ball near the box.5
The relational hierarchy used by the frame-
work has some commonalities with the relational
subsumption hierarchy proposed in (Krahmer and
Theune, 2002). However, there are two important
differences between them. First, an implication of
the subsumption hierarchy proposed in (Krahmer
and Theune, 2002) is that the semantics of the rela-
tions at lower levels in the hierarchy are subsumed
by the semantics of their parent relations. For ex-
ample, in the portion of the subsumption hierarchy
illustrated in (Krahmer and Theune, 2002) the re-
lation next to subsumes the relations left of and
right of. By contrast, the relational hierarchy de-
veloped here is based solely on the relative cogni-
tive load associated with the semantics of the spa-
tial relations and makes not claims as to the se-
mantic relationships between the semantics of the
spatial relations. Secondly, (Krahmer and Theune,
2002) do not use their relational hierarchy to guide
the construction of domain models.
By providing a basic contextual definition of
a landmark we are able to partition the context
in an appropriate manner. This partitioning has
two advantages. One, it reduces the complexity
of the context model construction, as the relation-
ships between the target and the distractor objects
or between the distractor objects themselves do
not need to be computed. Two, the context used
during the generation of a landmark description
is always a subset of the context used for a tar-
get (as the target, its distractors and the other ob-
jects in the domain that do not stand in relation
to the target or distractors under the relation being
considered are excluded). As a result the frame-
work avoids the issue of infinite recusion. Further-
more, the target-landmark relationship is automat-
5For more examples, see the videos available at
http://www.dfki.de/cosy/media/.
ically included as a property of the landmark as its
feature based description need only distinguish it
from objects that stand in relation to one of the dis-
tractor objects under the same spatial relationship.
In future work we will focus on extending the
framework to handle some of the issues effect-
ing the incremental algorithm, see (van Deemter,
2001). For example, generating locative descrip-
tions containing negated relations, conjunctions of
relations and involving sets of objects (sets of tar-
gets and landmarks).
5 Conclusions
We have argued that an if a conversational robot
functioning in dynamic partially known environ-
ments needs to generate contextually appropriate
locative expressions it must be able to construct
a context model that explicitly marks the spatial
relations between objects in the scene. However,
the construction of such a model is prone to the
issue of combinatorial explosion both in terms of
the number objects in the context (the location of
each object in the scene must be checked against
all the other objects in the scene) and number of
inter-object spatial relations (as a greater number
of spatial relations will require a greater number
of comparisons between each pair of objects.
We have presented a framework that addresses
this issue by: (a) contextually defining the set of
objects in the context that may function as a land-
mark, and (b) sequencing the order in which spa-
tial relations are considered using a cognitively
motivated hierarchy of relations. Defining the set
of objects in the scene that may function as a land-
mark reduces the number of object pairs that a spa-
tial relation must be computed over. Sequencing
the consideration of spatial relations means that
in each context model only one relation needs to
be checked and in some instances the agent need
not compute some of the spatial relations, as it
may have succeeded in generating a distinguishing
locative using a relation earlier in the sequence.
A further advantage of our approach stems from
the partitioning of the context into those objects
that may function as a landmark and those that
may not. As a result of this partitioning the al-
gorithm avoids the issue of infinite recursion, as
the partitioning of the context stops the algorithm
from distinguishing a landmark using its target.
We have employed the approach in a system for
Human-Robot Interaction, in the setting of object
1047
manipulation in natural scenes. For more detail,
see (Kruijff et al, 2006a; Kruijff et al, 2006b).
References
R.J. Beun and A. Cremers. 1998. Object reference in a
shared domain of conversation. Pragmatics and Cogni-
tion, 6(1/2):121?152.
D.J. Bryant, B. Tversky, and N. Franklin. 1992. Internal
and external spatial frameworks representing described
scenes. Journal of Memory and Language, 31:74?98.
L.A. Carlson-Radvansky and D. Irwin. 1994. Reference
frame activation during spatial term assignment. Journal
of Memory and Language, 33:646?671.
L.A. Carlson-Radvansky and G.D. Logan. 1997. The influ-
ence of reference frame selection on spatial template con-
struction. Journal of Memory and Language, 37:411?437.
H. Clark and D. Wilkes-Gibbs. 1986. Referring as a collab-
orative process. Cognition, 22:1?39.
E. Jeffrey Conklin and David D. McDonald. 1982. Salience:
the key to the selection problem in natural language gen-
eration. In ACL Proceedings, 20th Annual Meeting, pages
129?135.
R. Dale and N. Haddock. 1991. Generating referring ex-
pressions involving relations. In Proceeding of the Fifth
Conference of the European ACL, pages 161?166, Berlin,
April.
R. Dale and E. Reiter. 1995. Computational interpretations
of the Gricean maxims in the generation of referring ex-
pressions. Cognitive Science, 19(2):233?263.
I. Duwe and H. Strohner. 1997. Towards a cognitive model
of linguistic reference. Report: 97/1 - Situierte Ku?nstliche
Kommunikatoren 97/1, Univerista?t Bielefeld.
G. Edwards and B. Moulin. 1998. Towards the simula-
tion of spatial mental images using the vorono?? model. In
P. Oliver and K.P. Gapp, editors, Representation and pro-
cessing of spatial expressions, pages 163?184. Lawrence
Erlbaum Associates.
C. Fillmore. 1997. Lecture on Deixis. CSLI Publications.
K.P. Gapp. 1995. Angle, distance, shape, and their relation-
ship to projective relations. In Proceedings of the 17th
Conference of the Cognitive Science Society.
C Gardent. 2002. Generating minimal definite descrip-
tions. In Proceedings of the 40th International Confernce
of the Association of Computational Linguistics (ACL-02),
pages 96?103.
S. Garrod, G. Ferrier, and S. Campbell. 1999. In and on:
investigating the functional geometry of spatial preposi-
tions. Cognition, 72:167?189.
P. Gorniak and D. Roy. 2004. Grounded semantic compo-
sition for visual scenes. Journal of Artificial Intelligence
Research, 21:429?470.
E. Hajicova?. 1993. Issues of sentence structure and discourse
patterns. In Theoretical and Computational Linguistics,
volume 2, Charles University, Prague.
A Herskovits. 1986. Language and spatial cognition: An
interdisciplinary study of prepositions in English. Stud-
ies in Natural Language Processing. Cambridge Univer-
sity Press.
H. Horacek. 1997. An algorithm for generating referential
descriptions with flexible interfaces. In Proceedings of the
35th Annual Meeting of the Association for Computational
Linguistics, Madrid.
R. Jackendoff. 1983. Semantics and Cognition. Current
Studies in Linguistics. The MIT Press.
J. Kelleher and J. van Genabith. 2004. A false colouring real
time visual salency algorithm for reference resolution in
simulated 3d environments. AI Review, 21(3-4):253?267.
J.D. Kelleher, G.J.M. Kruijff, and F. Costello. 2006. Prox-
imity in context: An empirically grounded computational
model of proximity for processing topological spatial ex-
pressions. In Proceedings ACL/COLING 2006.
E. Krahmer and M. Theune. 2002. Efficient context-sensitive
generation of referring expressions. In K. van Deemter
and R. Kibble, editors, Information Sharing: Reference
and Presupposition in Language Generation and Interpre-
tation. CLSI Publications, Standford.
G.J.M. Kruijff, J.D. Kelleher, G. Berginc, and A. Leonardis.
2006a. Structural descriptions in human-assisted robot vi-
sual learning. In Proceedings of the 1st Annual Confer-
ence on Human-Robot Interaction (HRI?06).
G.J.M. Kruijff, J.D. Kelleher, and Nick Hawes. 2006b. Infor-
mation fusion for visual reference resolution in dynamic
situated dialogue. In E. Andre?, L. Dybkjaer, W. Minker,
H.Neumann, and M. Weber, editors, Perception and Inter-
active Technologies (PIT 2006). Springer Verlag.
B Landau. 1996. Multiple geometric representations of ob-
jects in language and language learners. In P Bloom,
M. Peterson, L Nadel, and M. Garrett, editors, Language
and Space, pages 317?363. MIT Press, Cambridge.
G. D. Logan. 1994. Spatial attention and the apprehension
of spatial realtions. Journal of Experimental Psychology:
Human Perception and Performance, 20:1015?1036.
G.D. Logan. 1995. Linguistic and conceptual control of vi-
sual spatial attention. Cognitive Psychology, 12:523?533.
R. Moratz and T. Tenbrink. 2006. Spatial reference in
linguistic human-robot interaction: Iterative, empirically
supported development of a model of projective relations.
Spatial Cognition and Computation.
L. Talmy. 1983. How language structures space. In H.L.
Pick, editor, Spatial orientation. Theory, research and ap-
plication, pages 225?282. Plenum Press.
A. Treisman and S. Gormican. 1988. Feature analysis in
early vision: Evidence from search assymetries. Psycho-
logical Review, 95:15?48.
K. van Deemter. 2001. Generating referring expressions:
Beyond the incremental algorithm. In 4th Int. Conf. on
Computational Semantics (IWCS-4), Tilburg.
I van der Sluis and E Krahmer. 2004. The influence of target
size and distance on the production of speech and gesture
in multimodal referring expressions. In Proceedings of
International Conference on Spoken Language Processing
(ICSLP04).
C. Vandeloise. 1991. Spatial Prepositions: A Case Study
From French. The University of Chicago Press.
S. Varges. 2004. Overgenerating referring expressions in-
volving relations and booleans. In Proceedings of the 3rd
International Conference on Natural Language Genera-
tion, University of Brighton.
1048
Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 1?8,
Trento, Italy, April 2006. c?2006 Association for Computational Linguistics
Spatial Prepositions in Context:
The Semantics of near in the Presence of Distractor Objects
Fintan J. Costello,
School of Computer Science and Informatics,
University College Dublin,
Dublin, Ireland.
fintan.costello@ucd.ie
John D. Kelleher
School of Computing,
Dublin Institute of Technology,
Dublin, Ireland.
John.Kelleher@comp.dit.ie
Abstract
The paper examines how people?s judge-
ments of proximity between two objects
are influenced by the presence of a third
object. In an experiment participants were
presented with images containing three
shapes in different relative positions, and
asked to rate the acceptability of a loca-
tive expression such as ?the circle is near
the triangle? as descriptions of those im-
ages. The results showed an interaction
between the relative positions of objects
and the linguistic roles that those objects
play in the locative expression: proximity
was a decreasing function of the distance
between the head object in the expression
and the prepositional clause object, and an
increasing function the distance between
the head and the third, distractor object.
This finding leads us to a new account for
the semantics of spatial prepositions such
as near.
1 Introduction
In this paper, we present an empirical study of the
cognitive representations underpinning the uses of
proximal descriptions in locative spatial expres-
sions. A spatial locative expression consists of a
locative prepositional phrase together with what-
ever the phrase modifies (noun, clause, etc.). In
their simplest form, a locative expression consists
of a prepositional phrase modifying a noun phrase,
for example the man near the desk. People often
use spatial locatives to denote objects in a visual
scene. Understanding such references involves co-
ordination between a perceptual event and a lin-
guistic utterance. Consequently, the study of spa-
tial locatives affords the opportunity to examine
some aspects of the grounding of language in non-
language.
The conception of space underlying spatial
locatives is fundamentally relativistic: the location
of one object is specified relative to another whose
location is usually assumed by the speaker to be
known by the hearer. Moreover, underpinning this
relativistic notion of space is the concept of prox-
imity. Consequently, the notion of proximity is
an important concept at the core of human spatial
cognition. Proximal spatial relationships are often
described using topological prepositions, e.g. at,
on, near, etc.
Terminology In this paper we use the term tar-
get (T) to refer to the head of a locative expression
(the object which is being located by that expres-
sion) and the term landmark (L) to refer to the ob-
ject in the prepositional phrase in that expression
(relative to which the head?s location is described),
see Example (1).
Example 1. [The man]T near [the table]L.
We will use the term distractor to describe any
object in the visual context that is neither the land-
mark nor the target.
Contributions The paper reports on a psy-
cholinguistic experiment that examines proximity.
Previous psycholinguistic work on proximal rela-
tions, (Logan and Sadler, 1996), has not exam-
ined the effects other objects in the scene (i.e., dis-
tractors) may have on the spatial relationship be-
tween a landmark and a target. The experiment
described in this paper compares peoples? judge-
ments of proximity between target and landmark
objects when they are presented alone and when
there are presented along with other distractor ob-
jects. Based on the results of this experiment we
1
propose a new model for the semantics of spatial
prepositions such as near.
Overview In ?2 we review previous work. In
?3 we describe the experiment. In ?4 we present
the results of the experiment and our analysis. The
paper finishes with conclusions, ?5.
2 Related Work
In this section we review previous psycholinguis-
tic experiments that examined proximal spatial re-
lations. We then present example spatial contexts,
that the previous experiments did not examine,
which motivate the hypothesis tested in this paper:
the location of other objects in a scene can inter-
fere with the acceptability of a proximal descrip-
tion being used to describe the spatial relationship
between a landmark and a target. 
 
 
 
1.74 1.90 2.84 3.16 2.34 1.81 2.13 
2.61 3.84 4.66 4.97 4.90 3.56 3.26 
4.06 5.56 7.55 7.97 7.29 4.80 3.91 
3.47 4.81 6.94 7.56 7.31 5.59 3.63 
4.47 5.91 8.52 O 7.90 6.13 4.46 
3.25 4.03 4.50 4.78 4.41 3.47 3.10 
1.84 2.23 2.03 3.06 2.53 2.13 2.00 
Figure 1: 7-by-7 cell grid with mean goodness rat-
ings for the relation near as a function of the posi-
tion occupied by X.
Spatial reasoning is a complex activity that in-
volves at least two levels of representation and rea-
soning: a geometric level where metric, topologi-
cal, and projective properties are handled, (Her-
skovits, 1986); and a functional level where the
normal function of an entity affects the spatial re-
lationships attributed to it in context (for example,
the meaning of ?near? for a bomb is quite differ-
ent from the meaning of ?near? for other objects of
the same size; (Vandeloise, 1991; Coventry, 1998;
Garrod et al, 1999)).
There has been a lot of experimental work
done on spatial reasoning and language: (Carlson-
Radvansky and Irwin, 1993; Carlson-Radvansky
and Irwin, 1994; Hayward and Tarr, 1995;
Gapp, 1995; Logan and Sadler, 1996; Carlson-
Radvansky and Logan, 1997; Coventry, 1998;
Garrod et al, 1999; Regier and Carlson, 2001;
Kelleher and Costello, 2005). Of these only
(Logan and Sadler, 1996) examined topological
prepositions in a context where functional factors
were excluded.
The term spatial template denotes the repre-
sentation of the regions of acceptability associated
with a preposition. It is centred on the landmark
and identifies for each point in its space the accept-
ability of the spatial relationship between the land-
mark and the target appearing at that point being
described by the preposition (Logan and Sadler,
1996).
The concept of a spatial template emerged from
psycholinguistic experiments reported in (Logan
and Sadler, 1996). These experiments examined
various spatial prepositions. In these experiments,
a human subject was shown sentences, each with a
picture of a spatial configuration. Every sentence
was of the form ?The X is [relation] the O?. The
accompanying picture contained anO in the center
of an invisible 7-by-7 cell grid, and an X in one of
the 48 surrounding positions. The subject then had
to rate howwell the sentence described the picture,
on a scale from 1(bad) to 9(good).
Figure 1 gives the mean goodness rating for the
relation ?near to? as a function of the position oc-
cupied by the X, as reported in (Logan and Sadler,
1996). If we plot the mean goodness rating for
?near? against the distance between target X and
landmark O, we get the graph in Figure 2.
Figure 2: Mean goodness rating vs. distance be-
tween X and O.
Both the figure and the graph make it clear that
the ratings diminish as we increase the distance
between X and O. At the same time, we can ob-
serve that even at the extremes of the grid the rat-
ings were still above 1 (the minimum rating). In-
2
deed, in the four corners of the grid, the points
most distant from the landmark, the mean ratings
nearly average twice the minimum rating.
However in certain contexts other factors, apart
from the distance between the landmark and the
target, affect the applicability of a proximal rela-
tion as a description of the target?s position rela-
tive to the landmark. For example, consider the
two scenes (side-view) given in Figure 3. In the
scene on the left-hand side, we can use the de-
scription ?the blue box is near the black box? to
describe object (a). However, consider now the
scene on the right-hand side. In this context, the
description ?the blue box is near the black box?
seems inappropriate as an expression describing
(a). The placing of object (c) beside (b) would
appear to interfere with the appropriateness of us-
ing a proximal relation to locate (a) relative to (b),
even though the absolute distance between (a) and
(b) has not changed.
Figure 3: Proximity and distance
In summary, there is empirical evidence that in-
dicates that as the distance between the landmark
and the target increases the applicability of a prox-
imal description decreases. Furthermore, there is
anecdotal evidence that the location of other dis-
tractor objects in context may interfere with appli-
cability of a proximal description between a target
and landmark object. The experiment presented in
this paper is designed to empirically test the affect
of distractor objects on proximity judgements.
3 Experiment
This work examines the impact of distractor ob-
jects on subjects? judgment of proximity between
the target and the landmark objects. To do this, we
examine the changes in participants judgements of
the appropriateness of the topological preposition
near being used to describe a spatial configuration
of the target and landmark objects when a distrac-
tor object was present and when it was removed.
Topological prepositions (e.g., at, on, in, near)
are often used to describe proximal spatial rela-
tionships. However, the semantics of a given topo-
logical preposition also reflects functional (Garrod
et al, 1999), directional (Logan and Sadler, 1996)
and topological factors.1 Consequently, it was im-
portant to control for these factors during the de-
sign of the experiment.
Functional factors were controlled for by us-
ing simple shapes in the stimuli. The preposition
near was used to control the impact of directional
factors. Previous psycholinguistic work indicated
that near was not affected by any directional pref-
erences. Finally, the influence of topological fac-
tors was controlled for by ensuring that the land-
mark and target maintained a consistent topolog-
ical relationship (the objects never touched, over-
lapped or were contained in other objects).
We approached our experiment with expecta-
tion that people?s proximity judgments between a
target and a landmark will be a decreasing func-
tion of the distance between those two objects: the
smaller the distance between a landmark and a tar-
get object, the higher the proximity rating people
will give for those two objects. We expect that the
presence of a distractor object will also influence
proximity judgments, and examine two different
hypotheses about how that influence will work: a
target-centered hypothesis and landmark-centered
hypothesis. In the target-centered hypothesis, peo-
ple?s judgments of proximity between a target and
a landmark will be a decreasing function of dis-
tance between those two objects, but an increas-
ing function of distance between the target and
the distractor object. Under this hypothesis, if
the distractor object is near the target object, this
will interfere with and lower people?s judgments
of proximity between the target and the landmark.
In the landmark-centered hypothesis, by contrast,
people?s judgments of proximity between a tar-
get and a landmark will be a decreasing function
of distance between those objects, but an increas-
ing function of distance between the landmark and
the distractor object. Under this hypothesis, if
the distractor object is near the landmark, it will
interfere with and lower people?s judgments of
proximity between target and landmark. We test
these hypotheses by varying target-distractor dis-
tance in our materials, but maintaining landmark-
distractor distance constant. If the target-centered
hypothesis is correct, then people?s judgments of
proximity should vary with target-distractor dis-
tance. If the landmark-centered hypothesis is cor-
rect, target-distractor distance should not influence
1See (Cohn et al, 1997) for a description different topo-
logical relationships.
3
people?s judgments of proximity.
3.1 Material and Subjects
All images used in this experiment contained a
central landmark and a target. In most of the im-
ages there was also another object, which we will
refer to as the distractor. All of these objects were
coloured shapes, a circle, triangle or square. How-
ever, none of the images contained two objects that
were the same shape or the same colour. 
 
 
 
       
       
       
       
       
       
       
1 2 
4 5 a 
6 
g L c 
e 
b 
d f 
3 
Figure 4: Relative locations of landmark (L) tar-
get positions (1..6) and distractor positions (a..g)
in images used in the experiment.
The landmark was always placed in the mid-
dle of a seven by seven grid (row four, column
four). There were 48 images in total, divided into
8 groups of 6 images each. Each image in a group
contained the target object placed in one of 6 dif-
ferent cells on the grid, numbered from 1 to 6 (see
Figure 4). As Figure 4 shows, we number these
target positions according to their nearness to the
landmark.
Each group, then, contains images with targets
at positions 1, 2, 3, 4, 5 and 6. Groups are organ-
ised according to the presence and position of a
distractor object. Figure 4 shows the 7 different
positions used for the distractor object, labelled
a,b,c,d,e,f and g. In each of these positions the
distractor is equidistant from the landmark. In
group a the distractor is directly above the land-
mark, in group b the distractor is rotated 45 de-
grees clockwise from the vertical, in group c it is
directly to the right of the landmark, in d is rotated
135 degrees clockwise from the vertical, and so
on. Notice that some of these distractor positions
(b,d, and f ) are not aligned with the grid. This re-
alignment is necessary to ensure that the distractor
object is always the same distance from the land-
mark. Each of these groups of images used in the
experiment corresponds to one of these 7 distrac-
tor positions, with a distractor object occurring at
that position for every image in that group. In ad-
dition, there is an eight group (which we label as
group x), in which no distractor object occurs.
Previous studies of how people judge proxim-
ity have typically examined judgments where the
target is above, below, to the left or right of the
landmark. The results of these studies showed
that these distinctions are relatively unimportant,
and the gradient of proximity observed tends to be
symmetrical around the landmark. For this reason,
in our study we ignore these factors and present
landmark, target and distractor randomly rotated
(so that some participants in our experiment will
see the image with target at position 1 and distrac-
tor at position a in a rotated form where position 1
is below the landmark and position a is to the right
of the landmark, but others will see the same rela-
tive positions at different rotations). In each image
all objects present were placed exactly at the cen-
ter of the cell representing their position.
During the experiment, each image was dis-
played with a sentence of the form The is near
the . The blanks were filled with a descrip-
tion of the target and landmark respectively. The
sentence was presented under the image. 12 par-
ticipants took part in this experiment.
3.2 Procedure
There were 48 trials, constructed from the follow-
ing variables: 8 distractor conditions * 6 target po-
sitions. To avoid sequence effects the landmark,
target and distractor colour and shape were ran-
domly modified for each trial and the distractor
condition and target location were randomly se-
lected for each trial. Each trial was randomly re-
flected across the horizontal, vertical, or diagonal
axes. Trials were presented in a different random
order to each participant.
Participants were instructed that they would be
shown sentence-picture pairs and were be asked to
rate the acceptability of the sentence as a descrip-
tion of the picture using a 10-point scale, with zero
denoting not acceptable at all; four or five denot-
ing moderately acceptable; and nine perfectly ac-
4
Figure 5: Experiment instructions.
ceptable. Figure 5 presents the instructions given
to each participant before the experiment. Trials
were self-paced, and the experiments lasted about
25-30 minutes. Figure 6 illustrates how the trials
were presented.
4 Results and Discussion
There are two questions we want to ask in our ex-
amination of people?s proximity judgments in the
presence of distractor objects. First, does the pres-
ence of a distractor make any noticable difference
in people?s judgements of proximity? Second, if
the presence of a distractor does influence prox-
imity judgements, is this influence target-centered
(based on the distance between the target object
and the distractor) or landmark-centered (based on
the distance between the landmark and the distrac-
tor).
We address the first question (does the distractor
object have an influence on proximity judgments)
by comparing the results obtained for images in
group x (in which there was no distractor) with re-
Figure 6: Sample trial from the experiment.
sults obtained from other groups. In particular, we
compare the results from this group with those ob-
tained from groups c, d and e: the three groups in
which the distractor object is furthest from the set
of target positions used (as Figure 4 shows, dis-
tractor positions c, d, and e are all on the opposite
side of the landmark from the set of target posi-
tions). We focus on comparison with groups c, d,
and e because results for the other groups are com-
plicated by the fact that people?s proximity judg-
ments are influenced by the closeness of a distrac-
tor object to the target (as we will see later).
 
 
   
0
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6target location
prox
imity
 rati
ng group xgroup cgroup dgroup e
   
Figure 7: mean proximity rating for target loca-
tions for group x (no distractor) and groups c, d,
and e (distractors present behind landmark)
Figure 7 shows the average proximity rating
given by participants for the 6 targets 1 to 6 for
group x (in which there was no distractor object)
and for groups c, d, and e (in which distractors oc-
curred on the opposite side of the landmark from
the target). Clearly, all three sets of distractor re-
sponses are very similar to each other, and are
all noticably different from the no-distractor re-
sponse. This difference was shown to be statis-
tically significant in a by-subjects analysis com-
paring subjects? responses for groups c,d and e
with their responses for group x. This compar-
ison showed that subjects produced significantly
lower proximity ratings for group c than group x
(Wilcoxon signed-rank test W+ = 55.50,W? =
10.50, N = 11, p <= 0.05), lower ratings for
group d than group x (W+ = 48.50,W? =
6.50, N = 11, p <= 0.05) and lower ratings
for group e than group x (W+ = 51.50,W? =
3.50, N = 11, p <= 0.01). (We exclude one
subject from this analysis because they mistakenly
gave the lowest possible proximity rating of 0 to
the item closest to the landmark in group x).
5
Figure 8: comparison between normalised proximity scores observed and computed for each group.
These results show that the presence of a dis-
tractor object reliably influences people?s proxim-
ity judgements. But how does this influence op-
erate? We examine this by comparing our exper-
imental results with those expected by the target-
centered and the landmark-centered hypotheses.
We can formalise the landmark-centered hy-
pothesis about proximity judgements as follows.
Let T be the target whose proximity to the land-
mark we?re trying to judge, let L and D be the
landmark and distractor objects respectively, and
let dist(A,B) be the computed distance between
6
two objects. Then under the landmark-centered
hypothsis, the relationship between proximity and
distance-to-landmark in our experiment should be
as in Equation 1:
prox(T,L) ?= ?dist(T,L) + dist(L,D) (1)
Equation 1 states that the judged proximity of a
target to a landmark rises as the target?s distance
from the landmark falls (the closer the target is to
the landmark, the higher its proximity score for
the landmark will be), but falls as the distance be-
tween the landmark and the distractor falls (the
closer the distractor is to the landmark, the lower
the proximity score for the target will be). Re-
call, however, that in the design of our materials,
the distance from landmark to distractor was kept
constant. When applied to our materials, there-
fore, Equation 1 reduces to
prox(T,L) ?= ?dist(T,L) (2)
Equation 2 gives a good fit to people?s proxim-
ity judgments for targets in our experiment. For
group x (the set of images for which there was no
distractor object, just a target and the landmark),
the correlation between?dist(T,D) and people?s
average proximity scores for target T was high
(r = 0.95). The first graph in Figure 8 illustrates
this correlation, comparing the average proximity
value given by participants for each target in group
x with the computed proximity value for each tar-
get in that group from Equation 2.
We next compare our experimental data with
the results expected by the target-centered hypoth-
esis for proximity judgments. Under this hypothe-
sis, the judged proximity of a target to a landmark
rises as the target?s distance from the landmark de-
creases (the closer the target is to the landmark, the
higher its proximity score for the landmark will
be), but falls as the target?s distance from the dis-
tractor decreases (the closer the target is to the dis-
tractor, the lower its proximity score for the land-
mark will be). This relationship can be formalised
as in Equation 3:
prox(T,L) ?= ?dist(T,L) + dist(T,D) (3)
Equation 3 states that if a target object is close
to the landmark and far from the distractor it will
have a high proximity score for that landmark.
However, if it is close to the landmark and close
to the distractor, its proximity score will be lower.
The remaining seven graphs in Figure 8 as-
sess this account by comparing the average prox-
imity value given by participants for each target
in the distractor groups a to g with the proxim-
ity value for each target in that group computed
from Equation 2 (the landmark-centered equation)
and with the proximity value for each target com-
puted from Equation 3 (the target-centered equa-
tion). As these graphs show, for each group the
proximity value computed from Equation 2 gives
a fair match to people?s proximity judgements for
target objects (the average correlation across the
seven groups is around r = 0.93). However, the
distance-to-distractor term in the computation of
proximity in Equation 3 significantly improves the
correlation in each graph, giving an average corre-
lation across the seven groups of around r = 0.99.
We conclude that participants? proximity judge-
ments for objects in our experiment are best rep-
resented by the model described in Equation 3, in
which the proximity of a target to a landmark is a
decreasing function of the target?s distance from
that landmark and an increasing function of the
target?s distance from distractor objects.
Note that, in order to clearly display the rela-
tionship between proximity values given by par-
ticipants for target objects, proximity computed
in Equation 2 (using target-to-landmark distance
only), and proximity computed in Equation 3 (us-
ing target-to-landmark and target-to-distractor dis-
tances) the values displayed in Figure 8 are nor-
malised so that, across all groups and targets,
the average proximity values given by participants
have a mean of 0 and a standard deviation of 1,
as do the proximity values computed in Equation
2 and those computed in Equation 3. This nor-
malisation simply means that all values fall in the
same region of the scale, and can be easily com-
pared visually. This normalisation has no effect
on the correlations obtained between the observed
and computed proximity values.
5 Conclusions
This paper described a psycholinguistic experi-
ment that investigated the cognitive representa-
tions underpinning spatial descriptions of proxim-
ity. The results showed that peoples? proximity
judgments for objects in the presence of distrac-
tors can be modelled in a straightforward way us-
7
ing the relation described in Equation 3, in which
proximity falls with the target?s distance from the
landmark, but rises with the target?s distance from
a distractor object. This means that if a target ob-
ject is close to the landmark and far from the dis-
tractor it will have a high proximity rating for that
landmark. However, if it is close to the landmark
but also close to the distractor, its proximity rating
will fall. These results suggest that the linguis-
tic roles that objects play in a locative expression
have an influence on people?s judgments of prox-
imity: proximity was a decreasing function of the
distance between the object in the head position in
the expression (the target) and that in the preposi-
tional clause position (the landmark), and an in-
creasing function the distance between the head
and the third, distractor object. This finding ex-
tends previous results on peoples? judgments of
proximity for objects.
It?s noticable, however, that the match to peo-
ple?s responses obtained by Equation 3 for items
in group a is less good than that obtained in any
of the other groups. Of all the distractors, distrac-
tor a was closer to the target object than any other
distractor. It may be that there is some other prox-
imity or occlusion effect acting in people?s judge-
ments of proximity for items in group a. Future
work will be necessary to clarify this point.
References
L.A. Carlson-Radvansky and D. Irwin. 1993. Frames of ref-
erence in vision and language: Where is above? Cogni-
tion, 46:223?224.
L.A. Carlson-Radvansky and D. Irwin. 1994. Reference
frame activation during spatial term assignment. Journal
of Memory and Language, 33:646?671.
L.A. Carlson-Radvansky and G.D. Logan. 1997. The influ-
ence of reference frame selection on spatial template con-
struction. Journal of Memory and Language, 37:411?437.
A GCohn, B Bennett, J MGooday, and NGotts. 1997. RCC:
a calculus for region based qualitative spatial reasoning.
GeoInformatica, 1:275?316.
K.R. Coventry. 1998. Spatial prepositions, functional re-
lations, and lexical specification. In P. Olivier and K.P.
Gapp, editors, Representation and Processing of Spatial
Expressions, pages 247?262. Lawrence Erlbaum Asso-
ciates.
K.P. Gapp. 1995. An empirically validated model for com-
puting spatial relations. In KI - Kunstliche Intelligenz,
pages 245?256.
S. Garrod, G. Ferrier, and S. Campbell. 1999. In and on:
investigating the functional geometry of spatial preposi-
tions. Cognition, 72:167?189.
W.G. Hayward and M.J. Tarr. 1995. Spatial language and
spatial representation. Cognition, 55:39?84.
A Herskovits. 1986. Language and spatial cognition: An
interdisciplinary study of prepositions in English. Stud-
ies in Natural Language Processing. Cambridge Univer-
sity Press.
J. Kelleher and F. Costello. 2005. Cognitive representations
of projective prepositions. In Proceedings of the Second
ACL-Sigsem Workshop of The Linguistic Dimensions of
Prepositions and their Use in Computational Linguistic
Formalisms and Applications.
G.D. Logan and D.D. Sadler. 1996. A computational analy-
sis of the apprehension of spatial relations. In M. Bloom,
P.and Peterson, L. Nadell, and M. Garrett, editors, Lan-
guage and Space, pages 493?529. MIT Press.
T Regier and L. Carlson. 2001. Grounding spatial language
in perception: An empirical and computational investi-
gation. Journal of Experimental Psychology: General,
130(2):273?298.
C. Vandeloise. 1991. Spatial Prepositions: A Case Study
From French. The University of Chicago Press.
8
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 230?234,
Dublin, Ireland, August 23-24, 2014.
DIT: Summarisation and Semantic Expansion in Evaluating Semantic
Similarity
Magdalena Kacmajor
IBM Technology Campus
Dublin Software Lab
Ireland
magdalena.kacmajor@ie.ibm.com
John D. Kelleher
Applied Intelligence Research Centre
Dublin Institute of Technology
Ireland
john.d.kelleher@dit.ie
Abstract
This paper describes an approach to im-
plementing a tool for evaluating seman-
tic similarity. We investigated the poten-
tial benefits of (1) using text summarisa-
tion to narrow down the comparison to the
most important concepts in both texts, and
(2) leveraging WordNet information to in-
crease usefulness of cosine comparisons
of short texts. In our experiments, text
summarisation using a graph-based algo-
rithm did not prove to be helpful. Se-
mantic and lexical expansion based upon
word relationships defined in WordNet in-
creased the agreement of cosine similarity
values with human similarity judgements.
1 Introduction
This paper describes a system that addresses the
problem of assessing semantic similarity between
two different-sized texts. The system has been ap-
plied to SemEval-2014 Task 3, Cross-Level Se-
mantic Similarity (Jurgens et al, 2014). The appli-
cation is limited to a single comparison type, that
is, paragraph to sentence.
The general approach taken can be charac-
terised as text summarisation followed by a pro-
cess of semantic expansion and finally similarity
computation using cosine similarity.
The rationale for applying summarisation is to
focus the comparison on the most important ele-
ments of the text by selecting key words to be used
in the similarity comparison. This summarisation
approach is based on the assumption that if sum-
mary of a paragraph is similar to the summary sen-
tence paired with the paragraph in the task dataset,
then the original paragraph and sentence pair must
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
have been similar and so should receive a high
similarity rating.
The subsequent semantic expansion is intended
to counteract the problem arising from the small
size of both compared text units. The similarity
metric used by the system is essentially a func-
tion of word overlap. However, because both the
paragraphs and sentences being compared are rel-
atively short, the probability of a word overlap -
even between semantically similar texts - is quite
small. Therefore prior to estimating the similarity
between the texts we extend the word vectors cre-
ated by the summarisation process with the syn-
onyms and other words semantically and lexically
related to the words occurring in the text.
By using cosine similarity measure, we nor-
malize the lengths of word vectors representing
different-sized documents (paragraphs and sen-
tences).
The rest of the paper is organized as follows:
section 2 describes the components of the sys-
tem in more detail; section 3 describes parame-
ters used in the experiments we conducted and
presents our results; and section 4 provides con-
cluding remarks.
2 System Description
2.1 Overview
There are four main stages in the system process-
ing pipeline: (1) text pre-processing; (2) summari-
sation; (3) semantic expansion; (4) computing the
similarity scores. In the following sections we de-
scribe each of these stages in turn.
2.2 Pre-processing
Paragraphs and sentences are tokenized and anno-
tated using Stanford CoreNLP
1
. The annotations
include part-of-speech (POS), lemmatisation, de-
pendency parse and coreference resolution. Then
1
http://nlp.stanford.edu/software/corenlp.shtml
230
the following processes are applied: (a) Token se-
lection, the system ignores tokens with POS other
than nouns, adjectives and verbs (in our exper-
iments we tested various combinations of these
three categories); (b) Token merging, the criteria
for merging can be more restrictive (same word
form) or less restrictive ? based on same lemma,
or even same lemma ignoring POS; (c) Stopword
removal, we apply a customized stopword list to
exclude verbs that have very general meaning.
In this way, each text unit is processed to pro-
duce a filtered set of tokens which at the next step
can be directly transformed into nodes in the graph
representation of the text. Dependency and coref-
erence annotations will be used for defining edges
in the graph.
2.3 Summarisation system
Summarisation has been implemented using Tex-
tRank (Mihalcea and Tarau, 2004), an itera-
tive graph-based ranking algorithm derived from
PageRank (Brin and Page, 1998).
The ranking is based on the following principle:
when node i links to node j, a vote is cast that in-
creases the rank of node j. The strength of the vote
depends on the importance (rank) of the casting
node, thus the algorithm is run iteratively until the
ranks stop changing beyond a given threshold, or
until a specified limit of iterations is reached.
To apply this algorithm to paragraphs and sen-
tences, our system builds a graph representation
for each of these text units, with nodes represent-
ing the tokens selected and merged at the pre-
ceding stage. The nodes are connected by co-
occurrence (Mihalcea and Tarau, 2004), depen-
dency and/or coreference relations. Next, a un-
weighted or weighted version of the ranking algo-
rithm is iterated until convergence.
For each test unit, the output of the summariser
is a list of words sorted by rank. Depending on
the experimental setup, the summariser forwards
on all processed words, or only a subset of top-
ranked words.
2.4 Lexico-semantic expansion
For each word returned from the summariser, we
retrieve all (or a predetermined number of) synsets
that have this word as a member. For each re-
trieved synset, we also identify synsets related
through semantic and lexical relations. Finally, us-
ing all these synsets we create the synonym group
for a word that includes all the members of these
synsets.
If a word has many different senses, then the
synonym group grows large, and the chances that
the sense of a given member of this large group
will match the sense of the original word are
shrinking. To account for this fact, each member
of the synonym group is assigned a weight using
Equation 1. This weight is simply 1 divided by
the count of the number of words in the synonym
group.
synweight =
1
#SynonymGroup
(1)
At the end of this process for each document
we have the set of words that occurred in the doc-
ument, and each of these words has a synonym
group associated with it. All of the members of
the synonym groups have a weight value.
2.5 Similarity comparison
Cosine similarity is used to compute the similar-
ity for each paragraph-sentence pair. For this cal-
culation each text (paragraph or sentence) is rep-
resented by a bag-of-words vector containing all
the words derived from the text together with their
synonym groups.
The bag-of-words can be binary or frequency
based, with the counts optionally modified by the
word ranks. The counts for words retrieved from
WordNet are weighted with synweights, which
means that they are usually represented by very
small numbers. However, if a match is found be-
tween a WordNet word and a word observed in
the document, the weight of both is adjusted ac-
cording to semantic match rules. These rules have
been established empirically, and are presented in
section 3.3.1.
The cosine values for each paragraph-sentence
pair are not subject to any further processing.
3 Experiments
3.1 Dataset
All experiments were carried out on training
dataset provided by SemEval-2014 Task 3 for
paragraph to sentence comparisons.
3.2 Parameters
For each stage in the pipeline, there is a set of pa-
rameters whose values influence the final results.
Each set of parameters will be discussed next.
231
3.2.1 Pre-processing parameters
The parameters used for pre-processing determine
the type and number of nodes included in the
graph:
? POS: Parts-of-speech that are allowed into
the graph, e.g. only nouns and verbs, or
nouns, verbs and adjectives.
? Merging criteria: The principle by which
we decide whether two tokens should be rep-
resented by the same node in the graph.
? Excluded verbs: The contents of the stop-
word list.
3.2.2 Summarisation parameters
These parameters control the structure of the graph
and the results yielded by TextRank algorithm.
The types of nodes in the graph are already de-
cided at the pre-processing stage.
? Relation type: In order to link the nodes
(words) in the graph representation of a docu-
ment, we use co-occurrence relations (Mihal-
cea and Tarau, 2004), dependency relations
and coreference relations. The two latter are
defined based on the Stanford CoreNLP an-
notations, whereas a co-occurrence edge is
created when two words appear in the text
within a word span of a specified length. The
co-occurrence relation comes with two addi-
tional parameters:
? Window size: Maximum number of
words constituting the span.
? Window application: The window can
be applied before or after filtering away
tokens of unwanted POS, i.e. we can re-
quire either the co-occurrence within the
original text or in the filtered text.
? Graph type: A document can be represented
as an unweighted or weighted graph. In
the second case we use a weighted version
of TextRank algorithm (Mihalcea and Tarau,
2004) in which the strength of a vote depends
both on the rank of the casting node and on
the weight of the link producing the vote.
? Edge weights: In general, the weight
of an edge between any two nodes de-
pends on the number of identified rela-
tions, but we also experimented with as-
signing different weights depending on
the relation type.
? Normalisation: This parameter refers to nor-
malising word ranks computed for the longer
and the shorter text unit.
? Word limit: The maximum number of top-
ranked words included in vector representa-
tion of the longer text. May be equal to the
number of words in the shorter of the two
compared texts, or fixed at some arbitrary
value.
3.2.3 Semantic extension parameters
The following factors regulate the impact of addi-
tional words retrieved from WordNet:
? Synset limit: The maximum number of
synsets (word senses) retrieved from Word-
Net per each word. Can be controlled by
word ranks returned from the summariser.
? Synonym limit: The maximum number of
synonyms (per synset) added to vector repre-
sentation of the document. Can be controlled
by word ranks.
? WordNet relations: The types of semantic
and lexical relations used to acquire addi-
tional synsets.
3.2.4 Similarity comparison parameters
? Bag-of-words model: The type of bag-of-
word used for cosine comparisons.
? Semantic match weights: The rules for ad-
justing weights of WordNet words that match
observed words from the other vector.
3.3 Results
The above parameters in various combinations
were applied in an extensive series of experiments.
Contrary to our expectations, the results indicate
that the summariser has either no impact or has a
negative effect. Table 1 presents the set of param-
eters that seem to have impact, and the values that
resulted in best scores, as calculated by SemEval
Task 3 evaluation tool against the training dataset.
3.3.1 Discussion
In the course of experiments we consistently ob-
served higher performance when all words from
both compared documents were included, as op-
posed to selecting top-ranked words from the
longer document. Furthermore, less restrictive cri-
teria for merging tended to give better results.
232
Parameter Value
Word limit no limit
POS JJ, NN, V
Merging criteria lemma, ignore POS
Custom stopword list yes
Synset limit 15
Synonym limit no limit
WordNet relations similar to, pertainym,
hypernym
Bag-of-words model binary
Table 1: Parameter values yielding the best scores.
We noticed clear improvement after extend-
ing word vectors with synonyms and related
words. WordNet relations that contributed most
are similar to, hypernym (ISA relation), pertainym
(relational adjective) and derivationally related
form. The results obtained before and after apply-
ing summarisation and lexico-semantic expansion
(while keeping other parameters fixed at values re-
ported in Table 1) are shown in Table 2.
`
`
`
`
`
`
`
`
`
`
`
`
`
`
Word ranks
Expansion
No Yes
Ignored 0.728 0.755
Used to select top-rank words 0.690 0.716
Used to control synset limit N/A 0.752
Used to weight vector counts 0.694 N/A
Table 2: The effects of applying text summarisa-
tion and lexico-semantic expansion.
Table 3 summarises the most efficient rules for
adjusting weights in word vectors when a match
has been found between an observed word from
one vector and a WordNet word in the other vec-
tor. The rules are as follows: (1) If the match is be-
tween an observed word from the paragraph vector
and a WordNet word from the sentence vector, the
weight of both is set to 0.25; (2) If the match is be-
tween an observed word from the sentence vector
and the WordNet word from the paragraph vector,
the weight of both is set to 0.75; (3) If the match is
between two WordNet words, one from the para-
graph and one from the sentence, the weight of
both is set to whichever synweight is higher; (4)
If the match is between two observed words, the
weight of both is set to 1.
We received slightly better results after setting a
limit on the number of included word senses, and
P
P
P
P
P
P
P
P
Paragr.
Sent.
Obs. word WordNet word
Observed word 1.0 0.25
WordNet word 0.75 max(synweight)
Table 3: Optimal weights for semantic match.
after ignoring a few verbs with particularly broad
meaning.
3.3.2 Break-down into categories
Pearson correlation between gold standard and the
submitted results was 0.785. Table 4 shows the
correlations within each category, both for the test
set and the train set. The results are very con-
sistent across datasets, except for Reviews which
scored much lower with the test data. The over-
all result was lower with the training data because
of higher number of examples in Metaphoric cat-
egory, where the performance of our system was
extremely poor.
Category Test data Train data
newswire 0.907 0.926
cqa 0.778 0.779
metaphoric 0.099 -0.16
scientific 0.856 -
travel 0.880 0.887
review 0.752 0.884
overall 0.785 0.755
Table 4: Break-down of the results.
4 Conclusions
We described our approach, parameters used in the
system, and the results of experiments. Text sum-
marisation didn?t prove to be helpful. One possi-
ble explanation of the neutral or negative effect of
summarisation is the small size of the texts units:
with the limited number of words available for
comparison, any procedure reducing this already
scarce set may be disadvantageous.
The results benefited from adding synonyms
and semantically and lexically related words.
Lemmatisation and merging same-lemma words
regardless the POS, as well as ignoring very gen-
eral verbs seem to be helpful.
The best performance has been observed in
Newswire category. Finally, given that the simi-
larity metric used by the system is essentially a
233
function of word overlap between the two texts,
it is not surprising that the system struggled with
metaphorically related texts.
References
Sergey Brin and Lawrence Page. 1998. The Anatomy
of Large-Scale Hypertextual Web Search Engine.
In Computer Networks and ISDN Systems, 30(1-
7):107?117.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Cambridge, MA: MIT Press.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. SemEval-2014 Task 3:
Cross-Level Semantic Similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014). August 23-24, 2014, Dublin,
Ireland.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing Order into Texts. In Proceedings of
EMNLP 2004:404?411, Barcelona, Spain.
George A. Miller. 1995. WordNet: A Lexical
Database for English. Communications of the ACM,
Vol. 38, No. 11:39?41.
234
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 619?623,
Dublin, Ireland, August 23-24, 2014.
TCDSCSS: Dimensionality Reduction to Evaluate Texts of Varying
Lengths - an IR Approach
Arun Jayapal
Dept of Computer Science
Trinity College Dublin
jayapala@cs.tcd.ie
Martin Emms
Dept of Computer Science
Trinity College Dublin
martin.emms@cs.tcd.ie
John D.Kelleher
School of Computing
Dublin Institute of Technology
john.d.kelleher@dit.ie
Abstract
This paper provides system description of
the cross-level semantic similarity task for
the SEMEVAL-2014 workshop. Cross-
level semantic similarity measures the de-
gree of relatedness between texts of vary-
ing lengths such as Paragraph to Sen-
tence and Sentence to Phrase. Latent Se-
mantic Analysis was used to evaluate the
cross-level semantic relatedness between
the texts to achieve above baseline scores,
tested on the training and test datasets. We
also tried using a bag-of-vectors approach
to evaluate the semantic relatedness. This
bag-of-vectors approach however did not
produced encouraging results.
1 Introduction
Semantic relatedness between texts have been
dealt with in multiple situations earlier. But it is
not usual to measure the semantic relatedness of
texts of varying lengths such as Paragraph to Sen-
tence (P2S) and Sentence to Phrase (S2P). This
task will be useful in natural language process-
ing applications such as paraphrasing and summa-
rization. The working principle of information re-
trieval system is the motivation for this task, where
the queries are not of equal lengths compared to
the documents in the index. We attempted two
ways to measure the semantic similarity for P2S
and S2P in a scale of 0 to 4, 4 meaning both texts
are similar and 0 being dissimilar. The first one
is Latent Semantic Analysis (LSA) and second, a
bag-of-vecors (BV) approach. An example of tar-
get similarity ratings for comparison type S2P is
provided in table 1.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence de-
tails: http://creativecommons.org/licenses/
by/4.0/
Sentence: Schumacher was undoubtedly one of
the very greatest racing drivers there has ever
been, a man who was routinely, on every lap, able
to dance on a limit accessible to almost no-one
else.
Score Phrase
4 the unparalleled greatness
of Schumachers driving abilities
3 driving abilities
2 formula one racing
1 north-south highway
0 orthodontic insurance
Table 1: An Example - Sentence to Phrase simi-
larity ratings for each scale
2 Data
The task organizers provided training data, which
included 500 pairs of P2S, S2P, Phrase to Word
(P2W) and their similarity scores. The training
data for P2S and S2P included text from different
genres such as Newswire, Travel, Metaphoric and
Reviews. In the training data for P2S, newswire
text constituted 36% of the data, while reviews
constituted 10% of the data and rest of the three
genres shared 54% of the data.
Considering the different genres provided in the
training data, a chunk of data provided for NIST
TAC?s Knowledge Base Population was used for
building a term-by-document matrix on which
to base the LSA method. The data included
newswire text and web-text, where the web-text
included data mostly from blogs. We used 2343
documents from the NIST dataset
1
, which were
available in eXtended Markup Language format.
Further to the NIST dataset, all the paragraphs
in the training data
2
of paragraph to sentence were
added to the dataset. To add these paragraphs to
the dataset, we converted each paragraph into a
1
Distributed by LDC (Linguistic Data Consortium)
2
provided by the SEMEVAL task-3 organizers
619
new document and the documents were added to
the corpus. The unique number of words identi-
fied in the corpus were approximately 40000.
3 System description
We tried two different approaches for evaluating
the P2S and S2P. Latent Semantic Analysis (LSA)
using SVD worked better than the Bag-of-Vectors
(BV) approach. The description of both the ap-
proaches are discussed in this section.
3.1 Latent Semantic Analysis
LSA has been used for information retrieval al-
lowing retrieval via vectors over latent, arguably
conceptual, dimensions, rather than over surface
word dimensions (Deerwester et al., 1990). It was
thought this would be of advantage for comparison
of texts of varying length.
3.1.1 Representation
The data corpus was converted into a mxn term-
by-document matrix, A, where the counts (c
m,n
)
of all terms (w
m
) in the corpus are represented
in rows and the respective documents (d
n
) in
columns:
A =
?
?
?
?
?
d
1
d
2
? ? ? d
n
w
1
c
1,1
c
1,2
? ? ? c
1,n
w
2
c
2,1
c
2,2
? ? ? c
2,n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w
m
c
m,1
c
m,2
? ? ? c
m,n
?
?
?
?
?
The document indexing rules such as text tok-
enization, case standardization, stop words re-
moval, token stemming, and special characters and
punctuations removal were followed to get the ma-
trix A.
Singular Value Decomposition (SVD) decom-
poses the matrix into U , ? and V matrices (ie.,
A = U?V
T
) such that U and V are orthonormal
matrices and ? is a diagonal matrix with singular
values. Retaining just the first k columns of U and
V , gives an approximation of A
A ? A
k
= U
k
?
k
V
T
k
(1)
According to LSA, the columns of U
k
are thought
of as representing latent, semantic dimensions,
and an arbitrary m-dimensional vector
#?
v can be
projected onto this semantic space by taking the
dot-product with each column of U
k
; we will call
the result
#      ?
v
sem
.
In the experiments reported later, the m-
dimensional vector
#?
v is sometimes a vector of
word counts, and sometimes a thresholded or
?boolean? version, mapping all non-zero numbers
to 1.
3.1.2 Similarity Calculation
To evaluate the similarity of a paragraph, p, and a
sentence, s, first these are represented as vectors of
word counts,
#?
p and
#?
s , then these are projected in
the latent semantic space, to give
#      ?
p
sem
and
#      ?
s
sem
,
and then between these the cosine similarity met-
ric is calculated:
cos(
#      ?
p
sem
.
#      ?
s
sem
) =
#      ?
p
sem
.
#      ?
s
sem
|
#      ?
p
sem
|.|
#      ?
s
sem
|
(2)
The cosine similarity metric provides a similarity
value in the range of 0 to 1, so to match the target
range of 0 to 4, the cosine values were multiplied
by 4. Exactly the same procedure is used for the
sentence to phrase comparison.
Further, the number of retained dimensions of
U
k
was varied, giving different dimensionalities
of the LSA space. The results of testing at the re-
duced dimensions are discussed in 4.1
3.2 Bag-of-Vectors
Another method we experimented on could be
termed a ?bag-of-vectors? (BV) approach: each
word in an item to be compared is replaced by a
vector representing its co-occurrence behavior and
the obtained bags of vectors enter into the compar-
ison process.
3.2.1 Representation
For the BV approach, the same data sources as was
used for the LSA approach is turned into a m?m
term-by-term co-occurrence matrix C:
C =
?
?
?
?
?
?
?
w
1
w
2
? ? ? w
m
w
1
c
1,1
c
1,2
? ? ? c
1,m
w
2
c
2,1
c
2,2
? ? ? c
2,m
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w
m
c
m,1
c
m,2
? ? ? c
m,m
?
?
?
?
?
?
?
The same preprocessing steps as for the LSA ap-
proach applied (text tokenization, case standard-
ization, stop words removal, special characters and
punctuations removal). Via C, if one has a bag-
of-words representing a paragraph, sentence or
phrase, one can replace it by a bag-of-vectors, re-
placing each word w
i
by the corresponding row of
C ? we will call these rows word-vectors.
620
3.2.2 Similarity Calculation
For calculating P2S similarity, the procedure is
as follows. The paragraph and sentence are tok-
enized, and stop-words were removed and are rep-
resented as two vectors
#?
p and
#?
s .
For each word p
i
from
#?
p , its word vector from
C is found, and this is compared to the word vector
for each word s
i
in
#?
s , via the cosine measure. The
highest similarity score for each word p
i
in
#?
p is
stored in a vector
# ?
S
p
shown in (3). The overall
semantic similarity score between paragraph and
sentence is then the mean value of the vector
# ?
S
p
?
4 ? see (4).
S
p
=
[
S
p
1
S
p
2
? ? ? S
p
i
]
(3)
S
sim
=
?
n
i=1
S
p
i
n
? 4 (4)
Exactly corresponding steps are carried out for the
S2P similarity. Although experiments were car-
ried out this particular BV approach, the results
were not encouraging. Details of the experiments
carried out are explained in 4.2.
4 Experiments
Different experiments were carried out using LSA
and BV systems described in sections 3.1 and 3.2
on the dataset described in section 2. Pearson
correlation and Spearman?s rank correlation were
the metrics used to evaluate the performance of
the systems. Pearson correlation provides the de-
gree of similarity between the system?s score for
each pair and the gold standard?s score for the said
pair while Spearman?s rank correlation provides
the degree of similarity between the rankings of
the pairs according to similarity.
4.1 LSA
The LSA model was used to evaluate the semantic
similarity between P2S and S2P.
4.1.1 Paragraph to Sentence
An initial word-document matrix A was built by
extracting tokens just based on spaces, stop words
removed and tokens sorted in alphabetical order.
As described in 3.1.1, via the SVD of A, a ma-
trix U
k
is obtained which can be used to project an
m dimensional vector into a k dimensional one.
In one setting the paragraph and sentence vec-
tors which are projected into the LSA space have
unique word counts for their dimensions. In an-
other setting before projection, these vectors are
Dimensions 100% 90% 50% 30% 10%
Basic word-doc representation 0.499 - 0.494 0.484 0.426
Evaluation-boolean counts 0.548 - 0.533 0.511 0.420
Constrained tokenization 0.368 0.564 0.540 0.516 0.480
Added data 0.461 0.602 0.568 0.517 0.522
Table 2: Pearson scores at different dimensions -
Paragraph to Sentence
thresholded into ?boolean? versions, with 1 for ev-
ery non-zero count.
The Pearson scores for these settings are in the
first and second rows of table 2. They show the
variation with the number of dimensions of the
LSA representation (that is the number of columns
of U that are kept)
3
. An observation is that the
usage of boolean values instead of word counts
showed improved results.
Further experiments were conducted, retaining
the boolean treatment of the vectors to be pro-
jected. In a new setting, further improvements
were made to the pre-processing step, creating a
new word-document matrix A using constrained
tokenization rules, removing unnecessary spaces
and tabs, and tokens stemmed
4
. The performance
of the similarity calculation is shown as the third
row of Table 2: there is a trend of increase in cor-
relation scores with respect to the increase in di-
mensionality up to a maximum of 0.564, reached
at 90% dimension.
0 20 40 60 80 1000.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
Percent Dimensions maintained
Sem
anti
c si
mila
rity
 
 
Basic word?doc representationEvaluation with Boolean valuesConstrained TokenizationAdded data representation
Figure 1: Paragraph to Sentence - Pearson corre-
lation scores for four different experiments at dif-
ferent dimensions
3
(represented in percent) of U
k
Not convinced with the pearson scores, more
3
Here, the dimension X% means k = (X/100) ? N ,
whereN is the total number of columns in A in the unreduced
SVD.
4
Stemmed using Porter Stemmer module availabe from
http://tartarus.org/?martin/PorterStemmer/
621
documents were added to the dataset to build a
new word-document matrix representation A. The
documents included all the paragraphs from the
training set. Each paragraph provided in the train-
ing set was added to the dataset as a separate docu-
ment. The experiment was performed maintaining
the settings from the previous experiment and the
results are shown in the fourth row of table 2. The
increase in trend of correlation scores with respect
to the increase in dimensionality is followed by the
new U produced from A after applying SVD. Fig-
ure 2 provides the distribution of similarity scores
evaluated at 90% dimension of the model with re-
spect to the gold standard.
Further to compare the performance of different
experiments, all the experiment results are plotted
in Figure 1. It can be observed that every subse-
quent model built has shown improvements in per-
formance. The first two experiments shown in the
first two rows of table 2 are shown in red and blue
lines in the figure. It can be observed that in both
the settings, the pearson correlation scores were
increasing as the the number of dimensions main-
tained also increased, whereas in the other two set-
tings, the pearson correlation scores reached their
maximum at 90% and came down at 100% di-
mension, which is unexpected and so is not jus-
tified. It is observed from Figure 2 that the scores
0 100 200 300 400 5000
0.5
1
1.5
2
2.5
3
3.5
4
Training data Examples
Sim
ilari
ty s
core
s
Figure 2: Semantic similarity scores - Gold stan-
dard (Line plot) vs System scores (Scatter plot) for
examples in training data
of the system in scatter plot are not always clus-
tered around the gold standard scores, plotted as a
line. As the gold standard score goes up, the sys-
tem prediction accuracy has come down. One rea-
son for this pattern can be attributed to the train-
ing set which had data mostly data from Newswire
Dimensions 100% 90% 70% 50% 30% 10%
Basic word-doc
representation 0.493 - - 0.435 0.423 0.366
Evaluation
boolean counts 0.472 - - 0.449 0.430 0.363
Constrained
tokenization 0.498 0.494 0.517 0.485 0.470 0.434
Added
data 0.493 0.504 0.498 0.498 0.488 0.460
Table 3: Pearson scores at different dimensions
3
-
Sentence to Phrase
and webtext. Therefore, during evaluation all the
words from paragraph and/or sentence would not
have got a position while getting projected on the
latent semantic space, which we believe has pulled
down the accuracy.
4.1.2 Sentence to Phrase
The experiments carried out for P2S provided in
4.1.1 were conducted for S2P examples as well.
The pearson scores produced by different experi-
ments at different dimensions are provided in ta-
ble 3. This table shows that the latest word-
document representation made with added docu-
ments, did not have any impact on the correlation
scores, while the earlier word-document represen-
tation provided in 3
rd
row, which used the original
dataset preprocessed with constrained tokeniza-
tion rules, removing unnecessary spaces and tabs,
and tokens stemmed, provided better correlation
score at 70% dimension. Further the comparison
of different experiments carried out at different
settings are plotted in Figure 3.
0 20 40 60 80 1000.35
0.4
0.45
0.5
0.55
Percent Dimensions maintained
Sem
anti
c si
mila
rity
 
 
Basic word?doc representationEvaluation with Boolean valuesConstrained TokenizationAdded data representation
Figure 3: Sentence to Phrase - Pearson correlation
scores for four different experiments at different
dimensions
3
(represented in percentage) of U
k
622
4.2 Bag of Vectors
BV was tested in two different settings. The
first representation was created with bi-gram co-
occurance count as mentioned in section 3.2.1 and
experiments were carried out as mentioned in sec-
tion 3.2.2. This produced negative Pearson corre-
lation scores for P2S and S2P. Then we created an-
other representation by getting co-occurance count
in a window of 6 words in a sentence, on evalua-
tion produced correlation scores of 0.094 for P2S
and 0.145 for S2P. As BV showed strong negative
results, we did not continue using the method for
evaluating the test data. But we strongly believe
that the BV approach can produce better results if
we could compare the sentence to the paragraph
rather than the paragraph to the sentence as men-
tioned in section 3.2.2. During similarity calcula-
tion, when comparing sentence to the paragraph,
for each word in the sentence, we look for the best
semantic match from the paragraph, which would
increase the mean value by reducing the number of
divisions representing the number of words in the
sentence. In the current setting, it is believed that
while computing the similarity for the paragraph
to sentence, the words in the paragraph (longer
text) will consider a few words in the sentence to
be similar multiple times. This could not be right
when we compare the texts of varying lengths.
5 Conclusion and Discussion
On manual verification, it was identified that the
dataset used to build the representation did not
have documents related to the genres Metaphoric,
CQA and Travel. The original dataset mostly had
documents from Newswire text and blogs which
included reviews as well. Further, it can be identi-
fied from tables 2 and 3, the word-document rep-
resentation with added documents from the train-
ing set improved Pearson scores. This allowed to
assume that the dataset did not have completely
relevant set of documents to evaluate the training
set which included data from different genres. For
evaluation of the model on test data, we submitted
two runs and best of them reported Pearson score
of 0.607 and 0.552 on P2S and S2P respectively.
In the future work, we should be able to experi-
ment with more relevant data to build the model
using LSI and also use statistically strong unsu-
pervised classifier pLSI (Hofmann T, 2001) for the
same task. Further to this, as discussed in 4.2 we
would be able to experiment with the BV approach
by comparing the sentence to the paragraph, which
we believe will yield promising results to compare
the texts of varying lengths.
References
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer and Richard Harshman
1990. Indexing by latent semantic analysis Jour-
nal of the American society for information science,
41(6):391?401
Thomas Hofmann 2001. Unsupervised Learning
by Probabilistic Latent Semantic Analysis Journal
Machine Learning, Volume 42 Issue 1-2, January-
February 2001 Pages 177 - 196
623
Referring Expression Generation Challenge 2008 DIT System Descriptions
J.D. Kelleher
School of Computing
Dublin Institute of Technology
john.kelleher@comp.dit.ie
B. Mac Namee
School of Computing
Dublin Institute of Technology
brian.macnamee@comp.dit.ie
1 Task 1: Attribute Selection
This section describes the two systems developed at
DIT for the attribute selection track of the REG 2008
challenge. Both of theses systems use an incremen-
tal greedy search to generate descriptions, similar
to the incremental algorithm described in (Dale and
Reiter, 1995). The output of these incremental algo-
rithms are, to a large extent, determined by the order
in which the algorithm tests the target object?s at-
tributes for inclusion in the description. Indeed, the
major difference between the two systems described
in this section is the mechanism used to order the
attributes for inclusion.
1.1 DIT-FBI System
The DIT-FBI system selects the next attribute to be
tested for inclusion in the description by ordering
each attribute based on its frequency in the subset of
the training corpus that is defined by the test trial?s
domain (i.e., furniture versus people) and condition
(+LOC versus -LOC). Attributes are selected in de-
scending order of frequency (i.e. the attribute that
occurred most frequently in the relevant subset of
the training corpus is selected first). Where two or
more attributes have the same frequency of occur-
rence the first attribute found with that frequency is
selected. The type attribute is always included in the
description. Other attributes are included in the de-
scription if they exclude at least 1 distractor from the
set of distractors that fulfil the description generated
prior to that attribute?s selection.
The mapping from qualitative linguistics descrip-
tions, such as middle or centre, to the TUNA cor-
pus? quantitative location attribute values, i.e, x-
dimension and y-dimension, can result in both the x-
dimension and y-dimension attributes being included
in the target set. These cases, where both the dimen-
sional attributes are required, are difficult to capture
because each of the dimensional attributes would be
sufficiently discriminative to result in a distinguish-
ing description simply by their lone inclusion. As a
result, a rule was put in place whereby if we have
included either of the dimensional attributes we in-
clude the other dimensional attribute if the included
one refers to the center of the display (i.e., x=3,
y=2).
The algorithm terminates when a distinguishing
description has been generated (i.e., all the distrac-
tors have been excluded) or when all of the target?s
attributes have been tested for inclusion in the de-
scription. Table 1 lists the results for the system.
Furniture People Both
Dice 0.816 0.702 0.763
MASI 0.606 .452 0.535
Accuracy 37 17 54
Minimality 80 68 148
Uniqueness 0 0 0
Table 1: Results for furniture, people and both domains.
1.2 DIT-TVAS System
In the DIT-TVAS system the selection of the next
target attribute to test for inclusion in the description
is based on the prior probability of a target?s attribute
with a particular value being used to describe the
target, given that the target has a particular attribute
with that particular value. This prior is computed
by counting the number of trials in the training cor-
pus where the target description included a partic-
221
ular attribute-value pair and dividing this count by
the number of trials in the training corpus where the
target?s properties listed a particular attribute-value
pair.
For example, there are 143 trials in the Reg-08-
Challenge training corpus where the target?s type at-
tribute had the value chair, and in 134 of these tri-
als the description of the target included the attribute
value pair type-chair. As a result, the atribute-value
pair type-chair has a prior probability of being used
to describe the target, given that the target properties
contain this attribute-value combination, of 134
143
?
0.937. Table 2 lists the priors for each attribute-
value combination in the furniture corpus and Table
3 lists the priors for each attribute-value combina-
tion in the people corpus (for space reasons these
tables do not include the priors for the x-dimension,
y-dimension and other attributes-value pairs). Given
these tables and a test trial, the next attribute-value
pair to be tested for inclusion in the description of
the test target is the attibute-value with the highest
prior that has not already been tested for inclusion
and that the target object fulfils.
As is evident from Table 2 and Table 3, there is
no significant difference between the priors of some
attribute-value pairs. For example, in the furniture
domain orientation=left and hasShirt=true have pri-
ors of 0.023 and 0.022 respectively. In order to
avoid situations where a non-significant difference
in priors unduly biases the system toward the inclu-
sion of a particular attribute-value pair, each time
an attribute-vaule pair has been selected for testing
the DIT-TVAS system checks whether there are any
other attribute-value pairs that have not been previ-
ously tested for inclusion in the description of the
target and whose prior is within 5% of the prior of
the attribute-value that has been selected for testing.
In cases where this test returns one or more attribute-
value pairs, the system uses the attribute-value pair
whose inclusion would exclude the most amount of
distractors. Finally, if there is a tie between one or
more attribute-value pairs with respect to distractor
exclusion this is resolved by slecting the attribute-
value pair with the highest prior. Table 4 lists the
results for the system.
Attribute VALUE Sel Occur Prior
TYPE fan 41 42 0.976
TYPE chair 134 143 0.937
TYPE sofa 43 48 0.896
COLOUR green 35 40 0.875
COLOUR blue 75 86 0.872
COLOUR red 82 96 0.854
TYPE desk 73 86 0.849
COLOUR grey 81 97 0.835
ORIENTATION back 25 51 0.490
SIZE small 56 130 0.431
ORIENTATION front 31 86 0.360
SIZE large 61 189 0.324
ORIENTATION left 22 86 0.256
ORIENTATION right 28 96 0.292
Table 2: Prior?s for each attribute-value pair in the furni-
ture domain. Sel: how often an attribute-value pair was
included in a description; Occur: how often an attribute-
vaule pair appeared in targets in the training corpus.
Prior?s listed to three decimal places.
Attribute VALUE Sel Occur Prior
TYPE person 225 274 0.821
hasBeard true 123 181 0.680
hasGlasses true 117 184 0.636
hasSuit true 4 94 0.43
hasHair true 36 233 0.155
hasHair false 6 41 0.146
AGE old 15 132 0.114
ORIENTATION right 2 44 0.045
ORIENTATION front 4 143 0.028
ORIENTATION left 2 87 0.023
hasShirt true 3 136 0.022
hasTie true 2 94 0.021
AGE young 2 142 0.014
hasShirt false 0 138 0
hasBeard false 0 93 0
hasGlasses false 0 90 0
hasTie false 0 180 0
hasSuit false 0 180 0
Table 3: Prior?s for each attribute-value pair in the furni-
ture domain. Sel: how often an attribute-value pair was
included in a description; Occur: how often an attribute-
vaule pair appeared in targets in the training corpus.
Prior?s listed to three decimal places.
222
Furniture People Both
Dice 0.778 0.709 0.746
MASI 0.540 .426 0.488
Accuracy 33 15 48
Uniqueness 80 68 148
Minimality 0 0 0
Table 4: Results for furniture, people and both domains.
2 Task 2: Realisation
This section describes the two systems developed at
DIT for the realisation track of the REG 2008 chal-
lenge. The DIT-CBSR system, Section 2.1, uses a
case-based reasoning approach to realization, which
(Daelemans and van den Bosch, 2005) have recently
argued is an appropriate machine learning approach
to natural language processing. The DIT-RBR sys-
tem, Section 2.2, uses a set of hand-crafted domain-
specific rules to generate descriptions.
2.1 DIT-CBSR System
Cased-Based Reasoning attempts to use a history
of past problems and their solutions to solve newly
arising problems. The solution to a new problem is
generated by finding the problem in the set of train-
ing problems the system has previously seen (i.e. the
case base) which most closely matches it and adapt-
ing its solution.
The DIT-CBSR system uses a relatively simple
case matching algorithm. When a new trial requires
sentence generation it?s attribute set is matched
against all of the cases in the training set to deter-
mine which cases is matches most closely. This
matching firstly considers only attributes and their
values. There are three kinds of matches that can
arise from this process:
? Perfect match: the attributes used in both the
query case and the case from the case-base
match perfectly as do their values.
? Partial match: the attribute used by both the
query case and the case from the case-base
match perfectly, but the attribute values do not
match.
? No match: no member of the case-base has a
list of attributes used that match those required
by the query case.
Slightly different actions are taken depending on
the type of match achieved. These are as follows.
Perfect Match Perfect matches are the easiest to
deal with as little effort is required in order to pro-
duce a useful sentence. In fact, if only one per-
fect match is found then that trial?s word string is
used, unedited, as the generated sentence. However,
things become a little more interesting if more than
one case in the case-base matches the query case
perfectly (remembering that the match is only based
on the attributes used and their values). In this case,
the list of matches is first trimmed of any cases that
are not based on the same image as the query case,
as long as this does not remove all cases. If this
does remove all cases we revert to the original set of
matching cases. In either instance, the word strings
in the set of remaining matching cases are consid-
ered to determine if there are any duplicates. If there
are, the word string that appears most frequently is
used as the generated sentence. If there are no dupli-
cates, the shortest word string in the set is chosen.
Partial Match Partial matches occur when there
is no example in the case-base for which all of the
attribute values are the same as those of the query
case. However, there are some case whose attributes
match the query, but whose attribute values are dif-
ferent. This set of cases is sub-divided based on the
number of attribute values in each case that match
those in the query case. This results in a set of cases
that share the highest match score. From this set al
of those cases that are not based on the same im-
age as the query case are removed, as long as this
does not completely empty the set. If this trimming
would completely empty the set it is not performed.
The trial with the shortest word string from the set of
remaining candidate matches is selected. The word
string associated with the selected trial needs to be
modified to account for the disparity between its at-
tribute values and those of the query case. This mod-
ification is done by replacing all substrings in the
selected case?s word string that arise from attribute
values not matching those in the query case with the
substring that is most commonly associated in the
training corpus with the query case?s attribute value.
All of these substrings are identified using the anno-
tated word string element present in each trial.
223
No Match When no match is found a simple rule-
based realiser is used to construct a sentence match-
ing the attribute value set of the query case. The
rules used by the realiser are based on the most com-
mon strings found in the corpus for each attribute
value pair.
Table 5 lists the results for the system.
Furniture People Both
String-edit distance 3.95 4.81 4.34
Accuracy score 12 6 18
Table 5: Results for furniture, people and both domains.
2.2 DIT-RBR System
In Section 2.1 we noted that if no match was found in
the case-base a simple rule-based realiser was used.
This rule-based realiser, the DIT-RBR system, uses a
sequence of IF-THEN rules based on a study of the
frequencies and order of the phrases used to realise
specific attribute-value pairs in the training corpus.
Theses phrase are easily extracted from the anno-
tated word-string xml element. The great advantage
of this algorithm is that it always able to return a
string given an input. However, the rule-set is spe-
cific to the task and would not generalise as well as
the DIT-CBSR system. Due to space restrictions we
do not list the rules used by the system. Table 6 lists
the results for the system.
Furniture People Both
String edit distance 3.613 4.132 3.851
Accuracy 11 3 14
Table 6: Results for furniture, people and both domains.
3 Task 3: Referring Expression
Generation
This section describes describes our approach to task
3 of REG Challenge 2008. Each of the systems de-
scribed in this section simply chains together DIT
solutions to task 1 and task 2.
3.1 DIT-FBI-CBSR System
The DIT-FBI-CBSR system chains together the DIT-
FBI attribute selection system, described in 1.1, and
the DIT-CBSR system, described in Section 2.1. Ta-
ble 7 lists the results for the system.
Furniture People Both
String-edit distance 4.45 5.162 4.777
Accuracy score 7 1 8
Table 7: Results for furniture, people and both domains.
3.2 DIT-TVAS-RBR Task 3 System
The DIT-TVAS-RBR system chains together the
DIT-TVAS attribute selection system, described in
Section 1.2, and the DIT-RBR realiser, described in
Section 2.2. Table 8 lists the results for the system.
Furniture People Both
String-edit distance 4.725 5.178 4.905
Accuracy score 4 0 4
Table 8: Results for furniture, people and both domains.
References
Walter Daelemans and Antal van den Bosch. 2005.
Memory-Based Language Processing. Cambridge
University Press.
R. Dale and E. Reiter. 1995. Computatinal interpreta-
tions of the gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 18:233?263.
224
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 38?42,
Gothenburg, Sweden, 26-27 April 2014.
c?2014 Association for Computational Linguistics
Evaluation of a Substitution Method for Idiom Transformation in
Statistical Machine Translation
Giancarlo D. Salton and Robert J. Ross and John D. Kelleher
Applied Intelligence Research Centre
School of Computing
Dublin Institute of Technology
Ireland
giancarlo.salton@mydit.ie {robert.ross,john.d.kelleher}@dit.ie
Abstract
We evaluate a substitution based technique
for improving Statistical Machine Transla-
tion performance on idiomatic multiword
expressions. The method operates by per-
forming substitution on the original idiom
with its literal meaning before translation,
with a second substitution step replac-
ing literal meanings with idioms follow-
ing translation. We detail our approach,
outline our implementation and provide
an evaluation of the method for the lan-
guage pair English/Brazilian-Portuguese.
Our results show improvements in trans-
lation accuracy on sentences containing
either morphosyntactically constrained or
unconstrained idioms. We discuss the con-
sequences of our results and outline poten-
tial extensions to this process.
1 Introduction
Idioms are a form of figurative multiword expres-
sions (MWE) that are ubiquitous in speech and
written text across a range of discourse types.
Idioms are often characterized in terms of their
having non-literal and non-compositional mean-
ing whilst occasionally sharing surface realiza-
tions with literal language uses (Garrao and Dias,
2001). For example the multiword expression s/he
took the biscuit can have both a figurative mean-
ing of being (pejoratively) remarkable, and a lit-
eral meaning of removing the cookie.
It is notable that idioms are a compact form
of language use which allow large fragments of
meaning with relatively complex social nuances
to be conveyed in a small number of words, i.e.,
idioms can be seen as a form of compacted regu-
larized language use. This is one reason why id-
iom use is challenging to second language learners
(see, e.g., Cieslicka(2006)).
Another difficulty for second language learners
in handling idioms is that idioms can vary in terms
of their morphosyntactic constraints or fixedness
(Fazly et al., 2008). On one hand some idiomatic
expressions such as popped the question are highly
fixed with syntactic and lexical variations consid-
ered unacceptable usage. On the other hand id-
ioms such as hold fire are less fixed with variations
such as hold one?s fire and held fire considered to
be acceptable instances of the idiom type.
For reasons such as those outlined above id-
ioms can be challenging to human speakers; but
they also pose a great challenge to a range of
Natural Language Processing (NLP) applications
(Sag et al., 2002). While idiomatic expressions,
and more generally multiword expressions, have
been widely studied in a number of NLP domains
(Acosta et al., 2011; Moreno-Ortiz et al., 2013),
their investigation in the context of machine trans-
lation has been more limited (Bouamor et al.,
2011; Salton et al., 2014).
The broad goal of our work is to advance ma-
chine translation by improving the processing of
idiomatic expressions. To that end, in this paper
we introduce and evaluate our initial approach to
the problem. We begin in the next section by giv-
ing a brief review of the problem of idiom pro-
cessing in a Statistical Machine Translation (SMT)
context. Following that we outline our substitu-
tion based solution to idiom processing in SMT.
We then outline a study that we have conducted to
evaluate our initial method. This is followed with
results and a brief discussion before we draw con-
clusions and outline future work.
2 Translation & Idiomatic Expressions
The current state-of-the-art in machine transla-
tion is phrase-based SMT (Collins et al., 2005).
Phrase-based SMT systems extend basic word-by-
word SMT by splitting the translation process into
3 steps: the input source sentence is segmented
38
into ?phrases? or multiword units; these phrases
are then translated into the target language; and fi-
nally the translated phrases are reordered if needed
(Koehn, 2010). Although the term phrase-based
translation might imply the system works at the
semantic or grammatical phrasal level, it is worth
noting that the concept of a phrase in SMT is
simply a frequently occurring sequence of words.
Hence, standard SMT systems do not model id-
ioms explicitly (Bouamor et al., 2011).
Given the above, the question arises as to how
SMT systems can best be enhanced to account for
idiom usage and other similar multiword expres-
sions. One direct way is to use a translation dic-
tionary to insert the idiomatic MWE along with its
appropriate translation into the SMT model phrase
table along with an estimated probability. While
this approach is conceptually simple, a notable
drawback with such a method is that while the
MWEs may be translated correctly the word or-
der in the resulting translation is often incorrect
(Okuma et al., 2008).
An alternative approach to extending SMT to
handle idiomatic and other MWEs is to leave the
underlying SMT model alone and instead perform
intelligent pre- and post-processing of the transla-
tion material. Okuma et al. (2008) is an example
of this approach applied to a class of multi- and
single word expressions. Specifically, Okuma et
al. (2008) proposed a substitution based pre and
post processing approach that uses a dictionary of
surrogate words from the same word class to re-
place low frequency (or unseen) words in the sen-
tences before the translation with high frequency
words from the same word class. Then, follow-
ing the translation step, the surrogate words are
replaced with the original terms. Okuma et al.?s
direct focus was not on idioms but rather on place
names and personal names. For example, given
an English sentence containing the relatively in-
frequent place name Cardiff , Okuma et al.?s ap-
proach would: (1) replace this low frequency place
name with a high frequency surrogate place name,
e.g. New York; (2) translate the updated sentence;
and (3) replace the surrogate words with the cor-
rect translation of the original term.
The advantage of this approach is that the word
order of the resulting translation has a much higher
probability of being correct. While this method
was developed for replacing just one word (or a
highly fixed name) at a time and those words must
be of the same open-class category, we see the ba-
sic premise of pre- and post- substitution as also
applicable to idiom substitution.
3 Methodology
The hypothesis we base our approach on is that
the work-flow that a human translator would have
in translating an idiom can be reproduced in an al-
gorithmic fashion. Specifically, we are assuming a
work-flow whereby a human translator first iden-
tifies an idiomatic expression within a source sen-
tence, then ?mentally? replaces that idiom with its
literal meaning. Only after this step would a trans-
lator produce the target sentence deciding whether
or not to use an idiom on the result. For simplicity
we assumed that the human translator should use
an idiom in the target language if available. While
this work-flow is merely a proposed method, we
see it as plausible and have developed a compu-
tational method based on this work-flow and the
substitution technique employed by (Okuma et al.,
2008).
Our idiom translation method can be explained
briefly in terms of a reference architecture as de-
picted in Figure 1. Our method makes use of 3
dictionaries and 2 pieces of software. The first
dictionary contains entries for the source language
idioms and their literal meaning, and is called the
?Source Language Idioms Dictionary?. The sec-
ond dictionary meanwhile contains entries for the
target language idioms and their literal meaning,
and is called the ?Target Language Idioms Dictio-
nary?. The third dictionary is a bilingual dictio-
nary containing entries for the idioms in the source
language pointing to their translated literal mean-
ing in the target language. This is the ?Bilingual
Idiom Dictionary?.
The two pieces of software are used in the pre-
and post-processing steps. The first piece of soft-
ware analyzes the source sentences, consulting the
?Source Language Idioms Dictionary?, to iden-
tify and replace the source idioms with their lit-
eral meaning in the source language. During this
first step the partially rewritten source sentences
are marked with replacements. Following the sub-
sequent translation step the second piece of soft-
ware is applied for the post-processing step. The
software first looks into the marked sentences to
obtain the original idioms. Then, consulting the
?Bilingual Idiom Dictionary?, the software tries to
match a substring with the literal translated mean-
39
Figure 1: Reference Architecture for Substitution Based Idiom Translation Technique.
ing in the target translation. If the literal mean-
ing is identified, it then checks the ?Target Lan-
guage Idioms Dictionary? for a corresponding id-
iom for the literal use in the target language. If
found, the literal wording in the target translation
is then replaced with an idiomatic phrase from the
target language. However if in the post-processing
step the original idiom substitution is not found, or
if there are no corresponding idioms in the target
language, then the post-processing software does
nothing.
4 Study Design
We have developed an initial implementa-
tion of our substitution approach to SMT
based idiom translation for the language pair
English/Brazillian-Portugese. To evaluate our
method we created test corpora where each sen-
tence contained an idiom, and compared the
BLEU scores (Papineni et al., 2002) of a baseline
SMT system when run on these test corpora with
the BLEU scores for the same SMT system when
we applied our pre and post processing steps. No
sentences with literal uses of the selected idiom
form were used in this experiment.
Consequently, three corpora were required for
this experiment in addition to the three idiomatic
resources introduced in the last section. The
first corpus was an initial large sentence-aligned
bilingual corpus that was used to build a SMT
model for the language pair English/Brazilian-
Portuguese. The second corpus was the first of two
test corpora. This corpus contained sentences with
?highly fixed? idioms and will be referred to as the
?High Fixed Corpus?. Finally a second test corpus
containing sentences with ?low fixed? idioms, the
?Low Fixed Corpus?, was also constructed. In or-
der to make results comparable across test corpora
the length of sentences in each of the two test cor-
pora were kept between fifteen and twenty words.
To create the initial large corpus a series of
small corpora available on the internet were com-
piled into one larger corpus which was used to
train a SMT system. The resources used in this
step were Fapesp-v2 (Aziz and Specia, 2011), the
OpenSubtitles2013
1
corpus, the PHP Manual Cor-
pus
2
and the KDE4 localizaton files (v.2)
3
. No
special tool was used to clean these corpora and
the files were compiled as is.
To create the ?High Fixed Corpus? and ?Low
Fixed Corpus? we built upon the previous work of
Fazly et al. (2008) who identified a dataset of 17
?highly fixed? English verb+noun idioms, and 11
?low fixed? English verb+noun idioms. Based on
these lists our two test corpora were built by ex-
tracting English sentences from the internet which
contained instances of each of the high and low
fixed idiom types. Each collected sentence was
manually translated into Brazilian-Portuguese, be-
fore each translations was manually checked and
corrected by a second translator. Ten sentences
were collected for each idiom type. This resulted
in a High Fixed corpus consisting of 170 sentences
1
http://opus.lingfil.uu.se/OpenSubtitles2013.php
2
http://opus.lingfil.uu.se/PHP.php
3
http://opus.lingfil.uu.se/KDE4.php
40
containing idiomatic usages of those idioms, and
a Low-Fixed corpus consisting of 110 sentences
containing instances of low-fixed idioms.
As indicated three idiomatic resources were
also required for the study. These were: a dic-
tionary of English idioms and their literal mean-
ings; a dictionary of Brazilian-Portuguese idioms
and their literal meanings; and a bilingual dictio-
nary from English to Brazilian-Portuguese. The
English idioms dictionary contained entries for the
idioms pointing to their literal English meanings,
along with some morphological variations of those
idioms. The Brazilian-Portuguese idioms dictio-
nary similarly contained entries for the idioms
pointing to their literal meanings with some mor-
phological variations of those idioms. Finally, the
bilingual dictionary contained entries for the same
idioms along with morphological variations of the
English idioms dictionary but pointing to their lit-
eral translated meaning. The Oxford Dictionary of
English idioms and the Cambridge Idioms Dictio-
nary were used to collect the literal meanings of
the English idioms. Literal meanings were manu-
ally translated to Brazilian-Portuguese.
Following resource collection and construction
a SMT model for English/Brazilian-Portuguese
was trained using the Moses toolkit (Koehn et al.,
2007) using its baseline settings. The corpus used
for this training consisted of 17,288,109 pairs of
sentences (approximately 50% of the initial col-
lected corpus), with another 34,576 pairs of sen-
tences used for the ?tuning? process. Following
this training and tuning process the baseline trans-
lation accuracy, or BLEU scores, were calculated
for the two test corpora, i.e., for the ?High Fixed
Corpus?, and the ?Low Fixed Corpus?.
Having calculated the baseline BLEU scores,
the substitution method was then applied to re-
translate each of the two test corpora. Specifi-
cally both the ?High Fixed Corpus? and the ?Low
Fixed Corpus? were passed through our extended
pipeline with new substitution based translations
constructed for each of the test corpora. BLEU
scores were then calculated for these two out-
put corpora that were built using the substitution
method.
5 Results and Discussion
Table 1 presents the results of the evaluation.
The BLEU scores presented in the table compare
the baseline SMT system against our proposed
method for handling English idiomatic MWE of
the verb+noun type.
Corpus Baseline Substitution
High Idiomatic 23.12 31.72
Low Idiomatic 24.55 26.07
Table 1: Experiment?s results.
Overall the results are positive. For both the
high and low idiomatic corpora we find that apply-
ing the pre- and post-processing substitution ap-
proach improves the BLEU score of the SMT sys-
tem. However, it is notable that the High-Fixed
idiomatic corpus showed a considerably larger in-
crease in BLEU score than was the case for the
Low-Fixedness idiomatic cases, i.e., a positive in-
crease of 8.6 versus 1.52. To investigate further
we applied a paired t-test to test for significance
in mean difference between baseline and substitu-
tion methods for both the high-fixed and low-fixed
test corpora. While the results for the ?High Id-
iomatic Corpus? demonstrated a statistically sig-
nificant difference in BLEU scores (p  0.05),
the difference between the baseline and substitu-
tion method was not statistically significant for the
case of the ?Low Idiomatic Corpus? (p ? 0.7).
We believe the lack of improvement in the case
of low fixed idioms may be caused by a higher
morphosyntactic variation in the translations of the
low fixed idioms. This higher variation makes the
post-processing step of our approach (which re-
quires matching a substring in the translated sen-
tence) more difficult for low fixed idioms with the
result that our approach is less effective for these
idioms.
It is worth noting that the same SMT system
(without the substitution extension) achieved a
BLEU score of 62.28 on a corpus of sentences
from general language; and, achieved an average
BLEU score of 46.48 over a set of 5 corpora of
sentences that did not contain idioms and were
of simlar length to the idiomatic corpora used in
this study (15 to 20 words). Both these BLEU
scores are higher than the scores we report in Ta-
ble 1 for our substitution method. This indicates
that although our substitution approach does im-
prove BLEU scores when translating idioms there
is still a lot of work to be done to solve the prob-
lems posed by idioms to SMT.
41
6 Conclusion
Our results indicate that this substitution approach
does improve the performance of the system.
However, we are aware that this method is not the
entire solution for the MWE problem in SMT. The
effectiveness of the approach is dependent on the
fixedness of the idiom being translated.
This approach relies on several language re-
sources, including: idiomatic dictionaries in the
source and target languages and a bilingual dic-
tionary containing entries for the idioms in the
source language aligned with their translated lit-
eral meaning in the target language. In future
work we investigate techniques that we can use to
(semi)automatically address dictionary construc-
tion. We will also work on enabling the system
to distinguish between idiomatic vs. literal usages
of idioms.
Acknowledgments
Giancarlo D. Salton would like to thank CAPES
(?Coordenac??ao de Aperfeic?oamento de Pessoal de
N??vel Superior?) for his Science Without Borders
scholarship, proc n. 9050-13-2. We would like
to thank Acassia Thabata de Souza Salton for her
corrections on the Brazilian-Portuguese transla-
tion of sentences containing idioms.
References
Otavio Costa Acosta, Aline Villavicencio, and Vi-
viane P. Moreira. 2011. Identification and Treat-
ment of Multiword Expressions applied to Informa-
tion Retrieval. In Proceedings of the Workshop on
Multiword Expressions: from Parsing and Genera-
tion to the Real World (MWE 2011), pages 101?109.
Wilker Aziz and Lucia Specia. 2011. Fully automatic
compilation of a portuguese-english and portuguese-
spanish parallel corpus for statistical machine trans-
lation. In STIL 2011.
Dhouha Bouamor, Nasredine Semmar, and Pierre
Zweigenbaum. 2011. Improved Statistical Machine
Translation Using MultiWord Expressions. In Pro-
ceedings of the International Workshop on Using
Linguistic Information for Hybrid Machine Trans-
lation), pages 15?20.
Anna Cie?slicka. 2006. Literal salience in on-line pro-
cessing of idiomatic expressions by second language
learners. 22(2):115?144.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of the 43rd Annual
Meeting of the ACL, pages 531?540.
Afsanesh Fazly, Paul Cook, and Suzanne Stevenson.
2008. Unsupervised Type and Token Identification
of Idiomatic Expressions. In Computational Lin-
guistics, volume 35, pages 61?103.
Milena U. Garrao and Maria C. P. Dias. 2001. Um
Estudo de Express?oes Cristalizadas do Tipo V+Sn e
sua Inclus?ao em um Tradutor Autom?atico Bil??ng?ue
(Portugu?es/Ingl?es). In Cadernos de Traduc??ao, vol-
ume 2, pages 165?182.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In 45th Annual Meeting of the Association for Com-
putational Linguistics.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York. 2 Ed.
Antonio Moreno-Ortiz, Chantal P?erez-Hern?andez, and
M.
?
Angeles Del-Olmo. 2013. Managing Multiword
Expressions in a Lexicon-Based Sentiment Analysis
System for Spanish. In Proceedings of the 9th Work-
shop on Multiword Expressions (MWE 2013), pages
1?10.
Hideo Okuma, Hirofumi Yamamoto, and Eiichiro
Sumita. 2008. Introducing Translation Dictionary
Into Phrase-based SMT. In IEICE - Transactions
on Information and Systems, number 7, pages 2051?
2057.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311?318.
Ivan A. Sag, Thimothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
Expressions: A Pain in the Neck for NLP. In
Computational Linguistics and Intelligent Text Pro-
cessing: Third International Conference: CICLing-
2002, Lecture Notes in Computer Science, volume
2276, pages 1?15.
Giancarlo D. Salton, Robert J. Ross, and John D.
Kelleher. 2014. An Empirical Study of the Im-
pact of Idioms on Phrase Based Statistical Machine
Translation of English to Brazilian-Portuguese. In
Third Workshop on Hybrid Approaches to Transla-
tion (HyTra) at 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics.
42
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 36?41,
Gothenburg, Sweden, April 27, 2014.
c?2014 Association for Computational Linguistics
An Empirical Study of the Impact of Idioms on Phrase Based Statistical
Machine Translation of English to Brazilian-Portuguese
Giancarlo D. Salton and Robert J. Ross and John D. Kelleher
Applied Intelligence Research Centre
School of Computing
Dublin Institute of Technology
Ireland
giancarlo.salton@mydit.ie {robert.ross,john.d.kelleher}@dit.ie
Abstract
This paper describes an experiment to
evaluate the impact of idioms on Statis-
tical Machine Translation (SMT) process
using the language pair English/Brazilian-
Portuguese. Our results show that on sen-
tences containing idioms a standard SMT
system achieves about half the BLEU
score of the same system when applied to
sentences that do not contain idioms. We
also provide a short error analysis and out-
line our planned work to overcome this
limitation.
1 Introduction and Motivation
An idiom is an expression whose meaning is not
compositional (Xatara, 2001). In other words
the meaning of an idiom is not simply the joint
meaning of the individual words (Garrao and Dias,
2001). For example, the expression kick the bucket
has an idiomatic meaning (to die) that has nothing
to do with the meaning of kick or bucket.
Idioms are a type of multi-word expressions
(MWEs) often used in a large variety of texts and
by human speakers and thus appear in all lan-
guages (Fazly et al., 2008). Consequently, they
pose problems to most Natural Language Process-
ing (NLP) applications (Sag et al., 2002). Nev-
ertheless, they often have been overlooked by re-
searchers in NLP (Fazly et al., 2008).
As a class, idioms exhibit a number of prop-
erties that make them difficult to handle for NLP
applications. For example, idiomatic expressions
vary with respect to how morphosyntatically fixed
they are. An idiomatic expression is highly fixed
if the replacement of any of its constituents by a,
syntactically or semantically, similar word causes
the idiomatic meaning of the expression to be lost
(Fazly et al., 2008). An example of a highly fixed
idiom in English is the expression by and large.
Idioms that are highly fixed can be represented as
words-with-spaces by an NLP system (Sag et al.,
2002). If, however, an idiomatic meaning persists
across morphosyntactic variations of an expres-
sion, the idiom can be described as a low fixed id-
iom, for example, hold fire and its variations hold
one?s fire and held fire. The words-with-spaces ap-
proach does not work for these ?more flexible? ex-
ample of idioms (Fazly et al., 2008). Another fea-
ture of idioms that make them difficult for NLP
system to process is that idiomatic expressions
have both idiomatic and literal (non-idiomatic) us-
ages. Consequently, NLP systems need to distin-
guish between these types of usages (Fazly et al.,
2008).
One of the most important NLP applications
that is negatively affected by idioms is Statistical
Machine Translation (SMT) systems. The current
state-of-the-art in SMT are phrase-based systems
(Collins et al., 2005). Phrase-based SMT systems
extend the basic SMT word-by-word approach by
splitting the translation process into 3 steps: the in-
put source sentence is segmented into ?phrases? or
multi-word units; these phrases are translated into
the target language; and the translated phrases are
reordered if needed (Koehn, 2010).
It is worth highlighting that although the term
phrase-based translation seems to imply the sys-
tem works at a phrasal level, the concept of a
phrase to these systems is simply a frequently
occurring sequence of words and not necessarily
a semantic or grammatical phrase. These sys-
tems thus limit themselves to a direct translation
of phrases without any syntactic or semantic con-
text. Hence, standard phrase-based SMT systems
do not model idioms explicitly (Bouamor et al.,
2011). Unfortunately modelling idioms in order
to improve SMT is not well studied (Ren et al.,
2009) and examples of the difficulties in translat-
ing these expressions can be seen in the quality of
the resultant output of most Machine Translation
36
systems (Vieira and Lima, 2001).
Our long-term research goal is to investigate
how the translation of idiomatic expressions may
be improved. We will initially focus on the case
of English/Brazillian-Portugese but we intend our
work to be generalizable to other language pairs.
As a first step on this research program we wished
to scope the impact of idioms on an SMT system.
In order to test this we ran an experiment that com-
pared the BLEU scores of an SMT system when it
was tested on three distinct sentence aligned cor-
pora. Two of these test corpora consisted of sen-
tences containing idiomatic (rather than literal) us-
ages of idiomatic expressions and the other cor-
pus consisted of sentences that did not contain any
idioms. By comparing the BLEU score of a ma-
chine translation system on each of these corpora
we hoped to gauge the size of the research prob-
lem we are addressing.
The paper is organized as follows: Section 2 de-
scribes the design and creation of the corpora used
in the experiments; Section 3 presents the experi-
ment?s methodology; Section 4 reports the results
found; and Section 5 both discusses the results and
describes an approach to the problem that we will
implement in future work.
2 Related work
The work of Fazly et al. (2008) has provided an
inspirational basis for our work. Fazly?s work fo-
cused on the study of idioms and in particular their
identification and analysis in terms of the syntactic
and semantic fixedness. Fazly study did not how-
ever explore the impact of idioms on SMT.
Some related work in translating idioms can
be found in: Garrao and Dias (2001) where the
verb+noun combinations and their inclusion in an
online automatic translator is explored; Ren et al.
(2009) which makes use of a domain constrained
bilingual multi-word dictionary to improve the
MT results; Bouamor et al. (2011) which ex-
plores a hybrid approach for extracting MWEs and
their translation in a French-English corpus; and
Bungum et al. (2013) which also uses dictionaries
to capture MWEs.
None of these works compares the BLEU score
of sentences containing and not containing idioms.
And also, none of these works address the idioms
problem for the English/Brazilian-Portuguese lan-
guage pair using SMT phrase-based systems.
3 Corpora Design and Collection
The experiment we describe in this paper had two
direct targets: (a) we wished to quantify the ef-
fect of idioms on the performance of an SMT
system; and (b) we wanted to better understand
the differences (if any) between high and low
fixed idioms with respect to their impact on SMT
systems. Consequently, in order to run the ex-
periments four corpora were needed: one ini-
tial large sentence-aligned bilingual corpus was
needed to build an SMT model for the language
pair English/Brazilian-Portuguese; a test corpus
containing sentences with ?highly fixed? idioms
called the ?High Idiomatic Corpus?; another test
corpus containing sentences with ?low fixed? id-
ioms called the ?Low Idiomatic Corpus?; and a
last corpus with sentences not containing idioms
called the ?Clean Corpus?. In order to make the
results comparable the length of each sentence in
the three test corpora was kept between 15 to 20
words. All of these corpora were constructed by
hand and in the cases of the ?High Idiomatic Cor-
pus? and ?Low Idiomatic Corpus? care was taken
to ensure that all the sentences in these corpora
contained idiomatic usages of the relevant idioms.
To create the initial large corpus a series of
small corpora available on the internet were com-
piled into one larger corpus which was used to
train a SMT system. The resources used in this
step were the Fapesp-v2 (Aziz and Specia, 2011),
the OpenSubtitles2013
1
corpus, the PHP Manual
Corpus
2
and the KDE4 localizaton files (v.2)
3
. No
special tool was used to clean these corpora and
the files were compiled as is.
Idioms are a heterogeneous class; consequently,
in order to better control the experiment we de-
cided to focus on a particular type of idiom -
specifically the idiomatic expressions formed from
the combination of a verb and a noun as its di-
rect object (verb+noun combinations), for exam-
ple hit+road and lose+head. Verb+noun combi-
nations are a subclass of MWE which are notable
for their cross-lingual occurrence and high vari-
ability, both lexical and semantic (Baldwin and
Kim, 2010). Also, it is worth noting that it is possi-
ble for a particular verb+noun combination to have
both idiomatic and literal usages and these usages
must be distinguished if an NLP system is to pro-
1
http://opus.lingfil.uu.se/OpenSubtitles2013.php
2
http://opus.lingfil.uu.se/PHP.php
3
http://opus.lingfil.uu.se/KDE4.php
37
cess a sentence appropriately.
Fazly et al. (2008) named a dataset of 17
?highly fixed? English verb+noun idioms, for ex-
ample cut+figure, and that list was used to build
our ?Highly Idiomatic Corpus?. This corpus con-
sisted of 170 sentences containing idiomatic us-
ages of these idioms, 10 sentences per idiom in
the list. These English sentences were collected
from the internet and manually translated into
Brazilian-Portuguese. After that these translations
were then manually checked and corrected by a
second translator.
Fazly et al. (2008) also named a dataset of
11 ?low fixed? English verb+noun idioms, for ex-
ample get+wind, and that list was used to build
our ?Low Idiomatic Corpus?. This corpus con-
sisted of 110 sentences containing idiomatic us-
ages of these idioms, 10 sentences per idiom in
the list. These English sentences were also col-
lected from the internet and manually translated
into Brazilian-Portuguese. After this step these
translations were also manually checked and cor-
rected by a second translator. Table 1 presents
the English verb+noun combinations used in this
experiment and their Brazilian-Portuguese trans-
lations.
In order to have a valid comparison between the
translation results of sentences containing and not
containing idioms the ?Clean Corpus? was built. It
consisted of 850 sentences with their translations
and was created by sampling sentences of the ap-
propriate length (15-20 words) that did not contain
idioms from the large bilingual corpus (that we de-
scribed earlier) which we created to train the SMT
system. These sentences were then removed from
that corpus. Because the initial corpus was cre-
ated from the union of corpora from different do-
mains the ?Clean Corpus? was randomly split into
5 datasets containing 170 sentences each in or-
der to ensure no specific influence of any of those
domains on the BLEU score. We called these
?Clean1? to ?Clean5?. Special care was taken to
not have any idioms in any of the sentences in
these corpora.
As we wanted to collect 10 sentences for each
verb+noun idiomatic combination and due to the
limitations of sentence length (15 to 20 words) we
were not able to collect the ?High Idiomatic Cor-
pus? and the ?Low Idiomatic Corpus? from the
training corpus. Thus, the samples were collected
from the Internet.
4 Methodology
As a first step for this experiment, a SMT
model for the English/Brazilian-Portuguese lan-
guage pair was trained using the Moses toolkit
(Koehn et al., 2007) following its ?baseline? set-
tings (Koehn et al., 2008). The corpus used for this
training consisted of 17,288,109 pairs of sentences
(approximately 50% of the initial collected cor-
pus), with another 34,576 pairs of sentences used
for the ?tuning? process.
English Brazilian-Portuguese
blow+top perder+paci?encia
blow+trumpet ?gabar-se?
cut+figure causar+impress?ao
find+foot ?adaptar-se?
get+nod ?obter permiss?ao?
give+sack ?ser demitido?, ?demitir?
have+word ter+conversa
hit+road ?cair na estrada?
hit+roof ?ficar zangado?
kick+heel ?deixar esperando?
lose+thread ?perder o fio da meada?
make+face* fazer+careta
make+mark deixar+marca
pull+plug ?cancelar algo?
pull+punch ?esconder algo?
pull+weight ?fazer sua parte?
take+heart ?ficar confiante?
blow+whistle ?botar a boca no trombone??
get+wind ouvir+murm?urios
hit+wall ?dar de cara num muro?
hold+fire ?conter-se?
lose+head* perder+cabec?a
make+hay dar+grac?as
make+hit fazer+sucesso
make+pile fazer+grana
make+scene* fazer+cena
pull+leg pegar+p?e
see+star* ver+estrela
Table 1: The English verb+noun combinations
used in this experiment and their Brazilian-
Portuguese Translations. The idioms marked with
an * have direct translations of its constituents re-
sulting in a MWE with the same idiomatic mean-
ing in Brazilian-Portuguese. Also, note that not all
translations results in a verb+noun idiom in the tar-
get language. Those are presented between double
quotes and italics.
38
In the second step the BLEU scores for the
?High Idiomatic Corpus?, the ?Low Idiomatic
Corpus? and the five clean corpora were com-
puted. Then, the average of each evaluation for
the clean corpora was calculated.
5 Results and Analysis
Table 2 lists the SMT system BLEU scores for the
?High Idiomatic Corpus?, ?Low Idiomatic Cor-
pus?, and the average BLEU score for the clean
corpora (i.e, ?Clean1? to ?Clean5?). The differ-
ential between the BLEU scores for the clean cor-
pus and the idiomatic corpora (high and low) in-
dicates that English idiomatic expressions of the
verb+noun type pose a significant challenge to
standard phrase based SMT.
Corpus BLEU scores
High Idiomatic 23.12
Low Idiomatic 24.55
Clean (average) 46.28
Table 2: BLEU scores.
The corpora containing idioms achieved only
half of the average Clean Corpus score. As noted
earlier, some idioms have a direct translation from
English to Brazilian-Portuguese and could result
in straight forward translations that the basic SMT
system (without substitution) can handle correctly.
Given this, the BLEU scores for this subset of id-
ioms could be expected to be similar to the clean
corpus results. However, it is worth noting that
even for idioms that have direct translations, see
Table 1, the BLEU score for the sentences contain-
ing these idioms is still lower than average BLEU
score for the clean corpus. Using the Student?s t-
test, we found a statistical difference between the
?Low Idiomatic Corpus? and the ?Clean Corpus?
(p  0), and between the ?High Idiomatic Cor-
pus? and the ?Clean Corpus? (p 0).
The second question that we examined in the
experiment was whether there was a difference in
performance between the high and low fixed id-
ioms. Table 3 lists the BLEU scores for each of
the ?highly fixed? verb+noun combinations used
in the ?High Idiomatic Corpus? and Table 4 lists
the BLEU scores for each of the ?low fixed?
verb+noun combinations from the ?Low Idiomatic
Corpus?. Also, it is important to note that the
?High Idiomatic Corpus? and the ?Low Idiomatic
Corpus? have almost no difference in their BLEU
scores. We also found that there are almost no sta-
tistical difference (p = 0.85) between the ?High
Idiomatic Corpus? and ?Low Idiomatic Corpus?
which we believe indicates that both kinds of
verb+noun idiomatic combinations pose the same
problem to SMT.
?high fixed? verb+noun BLEU score
blow+top 22.08
blow+trumpet 19.38
cut+figure 20.15
find+foot 24.36
get+nod 22.06
give+sack 23.03
have+word 20.91
hit+road 24.53
hit+roof 21.34
kick+heel 18.85
lose+thread 21.81
make+face 28.62
make+mark 29.46
pull+plug 19.71
pull+punch 28.34
pull+weight 19.94
take+heart 23.41
Table 3: BLEU scores for individual ?high fixed?
verb+noun idiomatic combinations.
?low fixed? verb+noun BLEU score
blow+whistle 17.75
get+wind 19.06
hit+wall 16.52
hold+fire 23.26
lose+head 37.40
make+hay 15.87
make+hit 25.48
make+pile 25.31
make+scene 36.93
pull+leg 15.90
see+star 37.86
Table 4: BLEU scores for individual ?low fixed?
verb+noun idiomatic combinations.
6 Conclusions and Future Work
Certainly, these results are not surprising. BLEU
scores are generally dependent on the training and
test corpora; that said, it it worthwhile having a
quantification of the potential issues that idioms
pose for SMT. Due to the fact that BLEU scores
39
are dependent on the training and test corpora used
our results are corpus specific. However, these
results are our starting point to develop a hybrid
methodology.
As noted earlier, idioms are widely used in ev-
ery literary genre and new expressions come into
existence frequently. Thus, they must be properly
handled and translated by a Machine Translation
system. Given the results of our experiments it
is evident that the problem in translating idioms
has not been solved using a standard SMT system.
Such evidences and the relatively small amount
of current related work on idiomatic expression
translation, when compared with the amount of
work on other MT aspects, indicates that there is
likely not a trivial solution.
To start addressing these problems, we propose
a hybrid method inspired by the work developed
by Okuma et al. (2008) for translating unseen
words using bilingual dictionaries.
Our method, introduced in Salton et al. (2014),
work as a pre and post-processing step. We first
identify idioms in source sentences using an id-
iom dictionary. Then, we substitute the idiom in
the source sentence with its literal meaning, taken
from the dictionary and record the fact that this
sentence contained a substituted idiom. For all
sentences that are recorded as containing a substi-
tution, after the translation we check if the orig-
inal idiom that occurred in the source sentence
has a corresponding idiom in the target language
by consulting a separate bilingual dictionary. If
there is a corresponding idiom in the target lan-
guage then the translation of the literal meaning of
the source language idiom is replaced with the tar-
get language idiom. If there are no related idioms
on the target language, this post-processing step is
avoided and the translation is done.
This approach relies on a number of dictionar-
ies being available. Developing these resources
is non-trivial and in order to scale our approach
to broad coverage a large part of our future work
will focus on automating (as much as possible)
the development of these language resources. An-
other problem that we will address in future work
is ensuring that we apply substitution appropri-
ately. There are at least two situations where care
must be taken. First, a given expression may be
used both as an idiom and literally. Consequently,
we need to develop mechanisms that will enable
our preprocessing step to distinguish between id-
iomatic and non-idiomatic usages. Second, some
idiomatic expressions have direct translations. For
these expressions we expect that the substitution
method may under-perform the standard SMT sys-
tem. Ideally, we would like to be able to control
the substitution method so that these particular ex-
pressions are allowed through the preprocessing
and are handled by the standard SMT pipeline.
However, for now, considering the proportion of
expressions with direct translations in comparison
with the overall number of expressions is very low;
we hope that this problem will not have too ad-
verse an impact on our approach. Beyond these
issues, while we anticipate that our substitution
based approached will work reasonably well for
?high fixed? idioms, we are aware that the varia-
tion in ?low fixed? idioms may require us to extend
the system in order to handle this variation.
Acknowledgments
Giancarlo D. Salton would like to thank CAPES
(?Coordenac??ao de Aperfeic?oamento de Pessoal de
N??vel Superior?) for his Science Without Borders
scholarship, proc n. 9050-13-2. We would like
to thank Acassia Thabata de Souza Salton for her
corrections on the Brazilian-Portuguese transla-
tion of sentences containing idioms.
References
Wilker Aziz and Lucia Specia. 2011. Fully automatic
compilation of a portuguese-english and portuguese-
spanish parallel corpus for statistical machine trans-
lation. In STIL 2011.
Timothy Baldwin and Su Nam Kim. 2010. Multi-
word Expressions. In Nitin Indurkhya and Fred J.
Damerau, editors, Handbook of Natural Language
Processing, Second Edition. CRC Press, Taylor and
Francis Group.
Dhouha Bouamor, Nasredine Semmar, and Pierre
Zweigenbaum. 2011. Improved Statistical Machine
Translation Using MultiWord Expressions. In Pro-
ceedings of the International Workshop on Using
Linguistic Information for Hybrid Machine Trans-
lation), pages 15?20.
Lars Bungum, Bj?orn Gamb?ack, Andr?e Lynum, and Er-
win Marsi. 2013. Improving Word Translation Dis-
ambiguation by Capturing Multiword Expressions
with Dictionaries. In Proceedings of the 9th Work-
shop on Multiword Expressions (MWE 2013), pages
21?30.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause Restructuring for Statistical Machine
40
Translation. In Proceedings of the 43rd Annual
Meeting of the ACL, pages 531?540.
Afsanesh Fazly, Paul Cook, and Suzanne Stevenson.
2008. Unsupervised Type and Token Identification
of Idiomatic Expressions. In Computational Lin-
guistics, volume 35, pages 61?103.
Milena U. Garrao and Maria C. P. Dias. 2001. Um
Estudo de Express?oes Cristalizadas do Tipo V+Sn e
sua Inclus?ao em um Tradutor Autom?atico Bil??ng?ue
(Portugu?es/Ingl?es). In Cadernos de Traduc??ao, vol-
ume 2, pages 165?182.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In 45th Annual Meeting of the Association for Com-
putational Linguistics.
Philipp Koehn, Abhishek Arun, and Hieu Hoang.
2008. Towards better machine translation quality for
the German-English language pairs. In Proceedings
of the Third Workshop on Statistical Machine Trans-
lation, pages 139?142.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York. 2 Ed.
Hideo Okuma, Hirofumi Yamamoto, and Eiichiro
Sumita. 2008. Introducing Translation Dictionary
Into Phrase-based SMT. In IEICE - Transactions
on Information and Systems, number 7, pages 2051?
2057.
Zhixiang Ren, Yajuan Lu, Jie Cao, Qun Liu, and
Yun Huang. 2009. Improving statistical machine
translation using domain bilingual multiword ex-
pressions. In Proceedings of the 2009 Workshop on
Multiword Expressions, ACL-IJCNLP 2009, pages
47?54.
Ivan A. Sag, Thimothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
Expressions: A Pain in the Neck for NLP. In
Computational Linguistics and Intelligent Text Pro-
cessing: Third International Conference: CICLing-
2002, Lecture Notes in Computer Science, volume
2276, pages 1?15.
Giancarlo D. Salton, Robert J. Ross, and John D. Kelle-
her. 2014. Evaluation of a Substitution Method for
Idiom Transformation in Statistical Machine Trans-
lation. In The 10th Workshop on Multiword Expres-
sions (MWE 2014) at 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics.
Renata Vieira and Vera Lcia S. Lima. 2001.
Lingu??stica Computacional: Princ??pios e aplicac??oes.
In Ana Teresa Martins & D??bio Leandro Borges
(eds.), As Tecnologias da informac??ao e a quest?ao so-
cial: anais.
Cl?audia M. Xatara. 2001. O Ensino do L?exico: As Ex-
press?oes Idiom?aticas. In Trabalhos em Lingu??stica
Aplicada, volume 37, pages 49?59.
41
Proceedings of the 25th International Conference on Computational Linguistics, pages 1?8,
Dublin, Ireland, August 23-29 2014.
The Effect of Sensor Errors in Situated Human-Computer Dialogue
Niels Schuette
Dublin Institute of Technology
niels.schutte
@student.dit.ie
John Kelleher
Dublin Institute of Technology
john.d.kelleher
@dit.ie
Brian Mac Namee
Dublin Institute of Technology
brian.macnamee
@dit.ie
Abstract
Errors in perception are a problem for computer systems that use sensors to perceive the envi-
ronment. If a computer system is engaged in dialogue with a human user, these problems in
perception lead to problems in the dialogue. We present two experiments, one in which partici-
pants interact through dialogue with a robot with perfect perception to fulfil a simple task, and a
second one in which the robot is affected by sensor errors and compare the resulting dialogues to
determine whether the sensor problems have an impact on dialogue success.
1 Introduction
Computer systems that can engage in natural language dialogue with human users are known as dia-
logue systems. A special class of dialogue systems are situated dialogue systems, which are dialogue
systems that operate in a spatial context. Situated dialogue systems are an active research topic (e.g.
(Kelleher, 2006)). Recently opportunities for more practical applications of situated dialogue systems
have arisen due to advances in the robustness of speech recognition and the increasing proliferation of
mobile computer systems such as mobile phones or augmented reality glasses.
When a dialogue system operates in a situated context, it needs the ability to perceive the environment.
Perception, such as computer vision, always has the potential of producing errors, such as failing to
notice an object or misrecognizing an object. We are interested in the effect of perception-based errors
on human-computer dialogue. If the human user and the system have shared view, false perception by the
system will lead to a divergence between the user?s understanding of the environment and the system?s
understanding. Such misunderstandings are frequent in human-human dialogue and human speakers use
different strategies to establish a shared understanding or common ground (Clark and Schaefer, 1989).
We investigated this problem in an earlier work based on a corpus of human dialogue (Schuette et al.,
2012) and are currently moving toward the same problem in human-computer dialogue.
The problem of misunderstandings in human-computer dialogue has previously mostly been addressed
under the aspect of problems arising from problems in speech recognition or language understanding (e.g.
(Aberdeen and Ferro, 2003; Shin et al., 2002; L?opez-C?ozar et al., 2010)). The problem of producing
referring expressions when it is not certain that the other participant shares the same perception and
understanding of the scene has been addressed by (Horacek, 2005). More recently (Liu et al., 2012)
performed a similar experiment in the context of human-human interaction. Their work was chiefly
concerned with the generation of referring expressions.
We report on a work in progress in which we investigate the effect of sensor problems on human-
computer dialogue using a dialogue system for a simulated robot. We describe two experiments we
performed so far. Both experiments are based on a shared experimental platform. In the first experiment
participants interact with a simulated robot using a text based dialogue interface to complete a series of
tasks. In the second experiment the participants again interact with the robot, except this time errors are
introduced into the robots perception. The goal of the second experiment is to investigate what effect
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
(a) The interaction window.
(b) The simulation view.
Figure 1: The user interface.
the presence of sensor errors has on the dialogue and the task performance and compare it to the results
from the first experiment. It should be emphasized that the goal of the experiments is not to evaluate the
performance of the dialogue system, but to investigate the effect of perception errors on the dialogues.
2 Experiment Methodology
The experiments were performed using an experiment system that was developed for this experiment.
It consists of a simulated world and a dialogue system. The world contains a number of objects such
as boxes and balls. These object can be manipulated by an abstract simulated robot arm. The dialogue
system is a frame based dialogue system that uses the Stanford Parser (Klein and Manning, 2003) for
parsing. The simulation environment was implement using Microsoft Robotics Studio. The system is
capable of understanding and performing a range of simple to complicated spatial action instructions
such as ?Put the ball behind the red box? or ?Pick up the red ball between the green box and the yellow
box?.
The participants interact with the system through the user interface shown in Figure 1. It consists of
two elements. The simulation window shows a rendering of the simulation world that is updated in real
time. The interaction window provides access to a text based chat interface that the participants use to
interact with the simulated robot. When the participant sends a request to the system, the system analyses
the input and attempts to perform it in the simulation world. If it can not perform the request, it replies
through the user interface and explains its problem.
The robot?s perception is provided by a simulated vision system. In general its perception is correct,
but sensor errors can be introduced. For example, it can be specified that the robot perceives entire
objects or some of their properties incorrectly.
Each run of the experiment consisted of a sequence of test scenes. Each scene consisted of a start
scene and a target scene. The start scene determined how the objects in the simulation world were
arranged at the beginning of the test scene. The target scene was presented to the participants as an
image in the interaction window. The participants? task was to interact with the robot to recreate the
target scene in the simulation world.
After a participant had successfully recreated the target scene, the system automatically advanced to
the next scene. The participants were also offered the option to abandon a scene and go on to the next
one if they thought they would not be able to complete the current scene.
All utterances by the participant and the system are transcribed and annotated with their semantic
2
(a) Scene 1 (b) Target scene 1 (c) Scene 4 (d) Target scene 4
Figure 2: Two scenes from Experiment 1 and their target scenes.
interpretation. The system also logs metrics that are used in the evaluation of dialogue systems to describe
the cost of a dialogue, such as the task completion rate, the number of utterances, the completion time
and the number of errors (Walker et al., 1997).
In the following we describe two experiments we performed with this setup so far. In the first exper-
iment participants completed a series of tasks. In the second experiment, participants also completed a
series of tasks. In this iteration however, errors were introduced into the system?s perception.
3 Experiment 1
The first experiment uses the basic version of the experiment system. The purpose of the experiment
was to establish how difficult the basic experiment task would be and to create a set of performance
measurements that could be used to compare this version of the system to later ones.
3.1 Instructions
The participants were provided with an instruction manual that described the experiment, introduced the
user interface and provided example interactions. Participants were encouraged to abandon a scene if
they felt that they would not be able to complete it. After reading the instructions, the participants were
shown a video recording of some example interactions with the system. This was done to prime the
participants towards using language and concepts that were covered by the system. No time limit was
set for experiment.
3.2 Test Scenes
The set of test scenes contained 10 scenes in total. Figure 2 shows some of the start scenes together with
their respective target scenes. Scene 1 (Figure 2a) is an example of a simple scene. Scene 4 (Figure 2c)
is an example of a more complex scene.
The scenes were presented in fixed order. The two initial scenes contained simple tasks. Their main
purpose is to allow the participants to gain practical experience with interacting with the system before
approaching the actual test scenes. The remaining scenes were designed to elicit specific referring
expressions. To transform a scene into its target scene, the participants had to move a number objects
from their original location to their respective target location as specified in the target scene. To get the
robot to move a target to a location, the participants had to specify which target the robot should move
(e.g. ?Take the red ball?), and specify where to move it (e.g. ?Put it behind the green box on the left?).
The complexity of the this task depends on the objects contained in the scene and their placement in
relation to each other. We were particularly interested in getting the participants to use specific objects
as landmarks in their referring expressions, and designed the scenes in such a way that participants
were influenced towards specific expressions. This was done with the motive of using landmark objects
as targets for perception errors in the second experiment. For each scene a set of target conditions was
specified that determined when a scene was complete.
3.3 Participants
In total 11 participants participated in the experiment. Most of them were native English speakers or
non-native speakers who had been speaking English for a number of years. Two of the participants were
3
(a) The start scene. (b) The target scene.
(c) The start scene as per-
ceived by the robot.
Figure 3: One of the scenes from Experiment 2.
female, the rest were male. The participants were between 20 and 50 years of age. All were college
sciences graduates who worked with computers on a daily basis.
3.4 Results
In total 11 participants completed the experiments. This resulted in a total of 110 interactions, two
of which had to be discarded due to recording problems. A summary of the recorded metrics for this
experiment is given in Table 1. It shows for each scene:
? How many instructions the participants used on average to complete it.
? How long the participants needed to complete each scene on average.
? How many of the instructions the participants produced contained a reference that was either am-
biguous (it could not be resolved to a unique referent) or unresolved (no referent that matched the
referring expression was found).
? The final column show how often each scene was abandoned.
For the current investigation the last two columns are of primary interest. Participants had been in-
structed to abandon a scene if they thought that they would not be able to complete it. The fact that this
only occurred three times in 108 interactions indicates that the task was not very difficult and that the
dialogue system?s performance was adequate for the task. The percentage of unresolved references in
the second to last column is also interesting because it indicates how often participants made references
that the system was not able to resolve. Since there were no errors introduced at this stage, the figures
can be seen as a baseline for the system?s ability to understand referring expressions.
4 Experiment 2
The main purpose of the second experiment was to investigate how the introduction of sensor errors
would influence the interactions and the outcome.
4.1 Instructions
The participants were provided with an extended version of the instruction manual as well as the intro-
duction video from the first experiment. The manual was identical to the manual from Experiment 1
except for a small section that was added to explain that errors could occur in some of the scenes. The
participants were encouraged to either try to work around the errors or to abandon the scene if the thought
they would not be able to finish it. Again, no time limit was set.
4.2 Test Scenes
The set of test scenes was based on the set of test scenes for Experiment 1, except that this time sensor
errors were introduced. We investigated three possible error conditions. In the missing object condition,
the perception system did not register an object at all. In the colour misclassification, the system did
4
Scene name Average
number of
actions per
scene
Average
time per
scene
Percentage
of am-
biguous or
unresolved
references
Number
of times
aban-
doned
Scene 1 2.9 00:00:56 0 0
Scene 2 2.3 00:00:54 0 0
Scene 3 8.7 00:01:45 2.1 0
Scene 4 5.9 00:01:52 10.8 0
Scene 5 2 00:00:28 0 0
Scene 6 5.2 00:01:23 5.2 1 (? 9%)
Scene 7 2.6 00:00:40 0 0
Scene 8 5.8 00:01:06 3.1 1 (? 9%)
Scene 9 5.3 00:01:12 8.4 0
Scene 10 6.8 00:01:30 6.7 1 (? 9%)
Average 5.1 00:01:14 6 0.3
Table 1: Summary of the cost metrics for Phase 1. Few scenes were abandoned. The percentage of
unresolved references forms a baseline for the resolution performance of the system.
Scene name Average
number of
actions per
scene
Average
time per
scene
Percentage
of am-
biguous or
unresolved
references
Number of
times aban-
doned
Scene 1 2.29 00:00:59 2.6 0 (0%)
Scene 2 3.29 00:00:56 3.6 2(? 11.8%)
Scene 3 9.12 00:02:13 9.7 3 (? 17.6%)
Scene 4 9.88 00:01:58 10.1 5 (? 29.4%)
Scene 5 10.35 00:01:46 9.7 2 (? 11.8%)
Scene 6 12.82 00:02:43 7.3 9 (? 52.9%)
Scene 7 4.82 00:01:08 14.6 2 (? 11.8%)
Scene 8 3.35 00:00:47 8.8 1 (? 5.9%)
Scene 9 9.88 00:01:34 9.5 4 (? 23.5%)
Scene 10 9.59 00:01:47 9.8 5 (? 29.4%)
Scene 11 10.82 00:02:08 5.4 3 (? 17.6%)
Scene 12 7 00:01:21 8.4 1 (? 5.9%)
Scene 13 6.65 00:01:29 8 2 (? 11.8%)
Scene 14 11.7 00:03:10 8.5 17 (100%)
Scene 15 5.18 00:01:02 15.9 1 (? 5.9%)
Scene 16 4.88 00:01:04 14.5 1 (? 5.9%)
Scene 17 6.82 00:01:01 1.7 0 (0%)
Scene 18 8.65 00:02:00 6.8 1 (? 5.9%)
Scene 19 9.4 00:01:45 7.8 0 (0%)
Scene 20 6 00:01:17 6.9 0 (0%)
Average 7.6 00:01:36 8.5 2.95
Average (scenes
w/o errors)
6.1 00:01:20 4.9 0.5
Average (scenes
w/ errors)
8.3 00:01:44 10 4
Table 2: Summary of the cost metrics for Phase 2. Scenes that contained no errors are highlighted in
green. Compared to Table 1, scenes that contained errors were more often abandoned, and resolution
problems were more frequent.
5
perceive the affected object but determined its colour incorrectly. A green ball for example, might be
mistaken for a red ball. In the type misclassification condition, the system also perceives the object, but
determines the object?s type incorrectly, for example, a green ball might be mistaken for a green box. We
restricted the errors so that at most one object was affected per scene. This was done to create scenes that
contained errors, but would still be solvable in most cases without major communication breakdowns.
The impact a sensor error has on the interaction greatly depends on which object it affects, the context
the object appears in, and the role the object plays in the task. For example, if an object is affected that
does not need to be moved and that is unlikely to be mentioned as a landmark, it is likely that the error
will not be noticed by the participant, and have no influence on the dialogue at all. On the other hand, if
an error affects an object that absolutely needs to be moved in order to complete the task in such a way
that it becomes impossible to interact with the object (e.g. because the robot does not see the object at
all), it becomes effectively impossible to complete the task. In less severe cases, errors may introduce
problems that can be solved. For example, if the first attempt at a reference fails because a landmark
is not available to the system, the participant may reformulate the expression with a different landmark.
This highlights the fact that sensor errors can have different effects depending on the circumstances.
We therefore decided to design each scene and the errors for the second phase manually in order to
make sure that examples for as many problem combinations as possible were presented to the partici-
pants. We based the design of the scenes on our experiences from Experiment 1. We selected suitable
scenes and introduced errors such that the preferred expressions used in Experiment 1 would be affected.
Each new scene created this way together with the original scene formed a corresponding scene pairs.
Members of a pair can be compared against each other to assess the impact of errors in Experiment 2.
The final set of scenes contained 14 scenes with sensor errors. We added four more scenes without errors
to the test set. Their purpose was to complement the data from the first experiment, and to check if the
presence of errors in other scenes would influence the behaviour of the participants in non-error scenes.
We also added the two introductory scenes from the first experiment. They were always presented as
the first scenes. The remaining scenes were presented in randomized order to prevent learning effects.
Therefore each participant was presented with a set of 20 scenes. In total there were 22 corresponding
scene pairs.
Figure 3 contains an example of a scene from the second experiment that contained a perception error.
Figure 3a show the start scene as presented to the participant. Figure 3b shows the target scene that was
presented to the participant. Figure 3c shows the start scene as it was perceived by the robot (it mistakes
the green box for a ball).
Each scene was annotated with a set of target conditions and a set of sensor error specifications.
4.3 Participants
17 participants were recruited for the experiment from roughly the same demographic as the first exper-
iment. About half of the participants had participated in the first experiment. A space of about 60 days
was left between the first experiment and the second experiment to minimize any influence between the
experiments.
4.4 Results
In total 17 participants completed the experiment. This results in a total of 340 interactions. Two inter-
actions were lost, resulting in a set of 338 interactions. The results for this experiment are given in Table
2. The highlighted rows (Scene 1,2,17,18,19 and 20) refer to scenes in which no errors were introduced.
As in the first experiment, the two last columns are the most interesting ones. Overall it can be
observed that more scenes were abandoned than in the first experiment. Every scene except for the
ones without errors was abandoned at least once (Scene 14 was abandoned by all participants. This was
expected because it was designed to be not completable due to the errors). This indicates that the task
with the errors was more difficult than the one in the first experiment.
It also appears that unresolved or ambiguous references were more frequent than in the first experi-
ment. At the bottom of the table we present overall averages for the different metrics. It appears that
scenes with sensor errors generally show higher values than scenes without.
6
Figure 4: A boxplot comparing the number of actions between scenes from Experiment 1 and 2. Paired
plots with the same colour refer to corresponding scenes (continued in Figure 5).
5 Discussion and Analysis
Overall the results indicate that the introduction of sensor errors increases the difficulty of the task. The
results show that the participants had to abandon scenes with errors more often than scenes without
errors. On average they used more actions to complete scenes with errors. A possible explanation can
be found in the higher percentage of unresolved references. Participants attempted to refer to an object,
but the system was unable to interpret it as expected due to a sensor error. This forced the participants to
try a different expression to progress with the task. It should be noted that the number of unresolved and
ambiguous references at the present does not account for references that were resolved to an object that
was not the object intended by the speaker. We may approach this problem at a later stage.
Figure 4 and 5 visualize the distribution of the number of actions for all the corresponding scene pairs.
They are numbered 1 to 22. Plots labelled with a correspond to scenes without errors, plots labelled with
b to their counterparts with errors. For easier visual comprehension, we coloured pairs alternatingly in
blue and white.
In general it can be observed that the median number of actions is generally higher for scenes with
errors than for their non-error counterparts, and that the interquartile range also tends to be higher. The
distributions appear to be fairly spread out. This suggests that there is considerable variation between
participants. We performed t-tests between corresponding scenes to determine whether the differences
between corresponding scenes were significant. The test shows that 12 out of 22 pairs were significantly
different with a p-value below 0.05. We will investigate at a later stage how much the strength of the
correspondence depends on the type of the error that was introduced.
A comparison of the distribution of the completion times was less conclusive. For some correspon-
dence pairs, the median completion time is higher for error scenes, for other pairs it is lower. We con-
jecture that there is some sort of self-selection mechanism at work where participants who were less
confident with the task in the first place task abandoned scenes earlier than confident participants when
they encountered problems, but this will require further investigation.
To summarize: The presence of sensor errors appears to increase the difficulty of the task, although
the effect appears to be small in some cases. This was in some way to be expected because the errors
were designed to pose solvable problems and not lead to major communication breakdowns.
6 Future Work
The results from this experiment are still very fresh, and this paper represents the first step in their
analysis. In the next step we are going to try to identify strategies the participants employed once they
encountered an error and see how well they match up with the strategies we described for the human-
human domain (Schuette et al., 2012). We are also interested in finding out how strategies evolved over
the course of the experiment, and in how much variation there is between individual participants.
7
Figure 5: A boxplot comparing the number of actions between scenes from Experiment 1 and 2. Paired
plots with the same colour refer to corresponding scenes (continued from Figure 5).
We are currently preparing a third experiment based on the experiment setup. In this experiment, the
participants will be offered different ways of accessing the robot?s understanding of what it sees to the
participant. For example, in one condition, the system will be able to generate descriptions of how it
perceives the scene. The results of this third experiment will be evaluated in the context of the first and
second experiment.
References
John Aberdeen and Lisa Ferro. 2003. Dialogue patterns and misunderstandings. In ISCA Tutorial and Research
Workshop on Error Handling in Spoken Dialogue Systems.
Herbert H. Clark and Edward F. Schaefer. 1989. Contributing to discourse. Cognitive Science, pages 259?294.
Helmut Horacek. 2005. Generating referential descriptions under conditions of uncertainty. In Proceedings of the
10th European Workshop on Natural Language Generation (ENLG), pages 58?67. Citeseer.
J. D. Kelleher. 2006. Attention driven reference resolution in multimodal contexts. Artificial Intelligence Review,
25(1):2135.
Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics-Volume 1, page 423430. Association for Computational
Linguistics.
Changsong Liu, Rui Fang, and Joyce Y. Chai. 2012. Towards mediating shared perceptual basis in situated
dialogue. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,
page 140149. Association for Computational Linguistics.
Ram?on L?opez-C?ozar, Zoraida Callejas, and David Griol. 2010. Using knowledge of misunderstandings to increase
the robustness of spoken dialogue systems. Knowledge-Based Systems, 23(5):471?485, July.
Niels Schuette, John Kelleher, and Brian Mac Namee. 2012. A corpus based dialogue model for grounding in
situated dialogue. In Proceedings of the 1st Workshop on Machine Learning for Interactive Systems: Bridging
the Gap Between Language, Motor Control and Vision (MLIS-2012)., Montpellier, France, August.
Jongho Shin, Shrikanth S. Narayanan, Laurie Gerber, Abe Kazemzadeh, Dani Byrd, and others. 2002. Analysis
of user behavior under error conditions in spoken dialogs. In INTERSPEECH.
Marilyn A. Walker, Diane J. Litman, Candace A. Kamm, and Alicia Abella. 1997. PARADISE: a framework
for evaluating spoken dialogue agents. In Proceedings of the eighth conference on European chapter of the
Association for Computational Linguistics, page 271280. Association for Computational Linguistics.
8
Proceedings of the 25th International Conference on Computational Linguistics, pages 33?37,
Dublin, Ireland, August 23-29 2014.
Exploration of functional semantics of prepositions from corpora of
descriptions of visual scenes
Simon Dobnik
1
and John D. Kelleher
2?
1
University of Gothenburg, Centre for Language Technology,
Dept. of Philosophy, Linguistics & Theory of Science, Gothenburg, Sweden
2
Dublin Institute of Technology, Applied Intelligence Research Centre,
School of Computing, Dublin, Ireland
simon.dobnik@gu.se, john.d.kelleher@dit.ie
Abstract
We present a method of extracting functional semantic knowledge from corpora of descriptions
of visual scenes. Such knowledge is required for interpretation and generation of spatial descrip-
tions in tasks such as visual search. We identify semantic classes of target and landmark objects
related by each preposition by abstracting over WordNet taxonomy. The inclusion of such knowl-
edge in visual search should equip robots with a better, more human-like spatial cognition.
1 Introduction
Visual search is an area of growing research importance in mobile robotics; see (Sj?o?o, 2011; Kunze et
al., 2014) among others. Visual search involves directing the visual sensors of a robot with the goal
of locating a specific object. Several recent approaches have integrated non-visual (often linguistically
motivated information) into the visual search process. The intuition behind this is that if the robot knows
that object X is often located near/on/. . . object Y then in situations where Y is visually salient it may be
easier for the system to search by first locating Y and then use relational information to direct the search
for X. A key component of these approaches to visual search is the definition of spatial semantics of the
relational information. Appropriately modelling these semantics is crucial because fundamentally it is
these models that define the scope of the visual search in relation to Y.
In language spatial relations between objects are often expressed using locative expressions such as
?the apple above a bowl?, ?the computer is on the shelf? and ?the plane is over the house?. In these
expressions a target object is located relative to a landmark object using a preposition to describe the
spatial relation. Crucially, there are differences between prepositions with respect to how their spatial
relations are defined. The semantics of some prepositions can be modelled in terms of geometric prim-
itives whereas the semantics of other prepositions are sensitive to the functional relations between the
target and the landmark (Coventry and Garrod, 2004). Consider the example ?Alex is at her desk?. This
description refers to a situation where Alex is not only geometrically proximate to her desk but also
where she is sitting down and working. The extra constraints are coming from the functional relations
that normally exist between an individual and a desk.
Returning to visual search, being able to identify whether a given preposition is primarily geometric
of functional is important because this classification informs the design of the spatial semantics for the
preposition and hence the appropriate definition of the search relative to the landmark object. In this paper
we present some ongoing experiments which attempt to develop techniques that can classify prepositions
as geometric or functional.
2 Spatial descriptions
(Coventry and Garrod, 2004) show in the experiments with human observers of images of a man in
the rain holding an umbrella where the umbrella is providing a varying degree of protection from the
rain that ?above? is more sensitive to the geometrical component than ?over? and that ?over? is more
?
Both authors are equal contributors.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
33
sensitive to the object function component than ?above?. Descriptions of ?the umbrella is over a man?
were considered acceptable even in cases where the umbrella was held horizontally but was providing
protection from the rain.
Modelling of functional knowledge in the computational models of meaning of spatial prepositions is
not straightforward. Humans (and even expert linguists) do not seem to have clear intuitions what the
functional component of each preposition sense may be and hence they must be confirmed experimentally
(Garrod et al., 1999). This may take significant time for various combinations of prepositions and target
and landmark objects. One needs to develop a complex ontology of object properties and then associate
spatial prepositions with rules that pick out certain properties of objects for a particular preposition sense
(for examples of such rules see (Garrod et al., 1999, p.170)).
In this paper we describe a method of extraction of these meaning components from a corpus of de-
scriptions of visual scenes automatically. Unlike in the psycho-linguistic experiments described above
we examine general corpora that obtain a wide and unrestricted set of images that humans described
freely. The purpose of the experiment is to investigate whether functional knowledge can be extracted
from contextual language use. For example, can we make generalisations about the semantics of the
arguments that a particular prepositional sense takes automatically. Furthermore, we are also interested
if the distinctions between geometric and functional sensitivity of prepositions reported experimentally
could be determined this way. This information would allow us to weight the contributions of the ge-
ometric and functional knowledge when generating and interpreting spatial descriptions of scenes. We
hypothesise, that if a preposition is sensitive to functional meaning then there will be functional relations
between target and landmark objects that it is used with and consequently the preposition will will be
much more restrictive or specific in the semantic type of targets and landmarks that it requires. Other
prepositions may be more sensitive to the satisfaction of the geometric constraint and hence we expect
that they will co-occur with objects of more general semantic types.
3 Datasets and extraction of spatial descriptions
The goal of this work is to analyse the semantics of linguistic expressions that are used to describe the
relative location of objects in visual contexts. We base our analysis on two corpora of image descrip-
tions: specifically, the IAPR TC-12 Benchmark corpus (Grubinger et al., 2006)
1
which contains 20,000
images and multi-sentence descriptions and the 8K ImageFlickr dataset (Rashtchian et al., 2010)
2
which
contains 8108 images. In both corpora the situations and events represented by images are described by
several sentences which contain spatial descriptions with prepositions: in the first case all sentences are
by a single annotator and in the second case each sentence is by a different annotator. The descriptions
are geometrically constrained by the visual context. On the other hand, the describers? choice of the tar-
get and the landmarks objects and the preposition in these descriptions will tell us about their functional
semantics. The main pre-processing step was to extract parts of spatial expressions used in the image
descriptions. Once extracted each spatial expression was stored in a type with the following structure:
?preposition, target, landmark?. To do this extraction we first parsed both corpora of linguistic descrip-
tions for dependencies (Marneffe et al., 2006) using Stanford CoreNLP tools
3
. Then we wrote several
extraction rules that matched dependency parses and extracted all three parts of spatial expressions that
we are looking for. All words were lemmatized and converted to lower case, compound prepositions
such as ?on the left side of? were rewritten as single words and names, etc. were converted to their
named entity categories such as ?person?. The extracted patterns from both corpora were combined into
a single dataset from which we can determine their frequency counts.
4 Determining conceptual categories of objects
The intuition behind our experiment is that functionally defined prepositions can be distinguished from
geometrically defined prepositions by virtue of the fact that the functional relations between the target
1
http://imageclef.org/photodata
2
http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html
3
http://nlp.stanford.edu/software/corenlp.shtml
34
and landmark objects that are encoded in the semantics of functional prepositions result in less variance
across the object types that occur with geometric prepositions. In other words, the target objects that
occur with a functional preposition will be more semantically similar to each other than the target ob-
jects that occur with a geometric preposition. Likewise the landmark objects that occur with a functional
preposition will be more semantically similar than the landmark objects that occur with a geometric
preposition. In order to test this intuition we need to be able to cluster the target and landmark objects
that occur with a given preposition into conceptual categories. We can then define patterns that describe
the pairs of conceptual categories that a preposition occurs with. Based on the intuition that functional
prepositions occur with more specific object classes we would expect that functional prepositions gener-
ate more patterns of use and that the patterns include more specific classes of objects.
To determine conceptual categories that objects of prepositions belong to, WordNet (Fellbaum, 1998)
appears to be an ideal tool. It contains taxonomies of words that were constructed by humans using
their intuitions. While certain classification of words in the ontology are not entirely unproblematic it is
nonetheless considered a gold-standard resource for lexical semantics. In particular, we are interested in
finding out given a certain preposition what are the possible semantic classes of its target and landmark
objects. To determine the class synset (a sense in WordNet terminology) that covers a bag of words
best we use the class-labelling algorithm of Widdows (2003). Given a list of words, this algorithm finds
hypernyms which subsume as many as possible words in the list, as closely as possible. The algorithm
works by first defining the set of all possible hypernyms for the words in the list. In then computes a
score for each of the hypernyms: a hypernym score is incremented by a small positive value for each
word it subsumes (this positive value is defined as 1 divided by the square of the vertical distance in the
hierarchy between the hypernym and the word) and decremented by a small negative value g for each
word it does not subsume. The algorithm returns the hypernym with the highest score. Widdows (2003)
sets g to 0.25 for lists that contain 5 words on average. The lists in our experiments are much longer
so we scale g to the length of the list input: g = 0.25 ?
5
list length
. Given a bag of nouns we use the
class-labelling algorithm to determine the bets matching hypernym. Words that are subsumed by this
hypernym are labelled as belonging to this class and are removed from the bag of words. The algorithm
is repeated on the remaining words recursively until all words from the bag of words are exhausted. The
procedure allows us to greedily create classes of words that are most general categories representing
these words, the level of generality can be tweaked by the parameter g. The bag of words is allowed
to contain duplicates as these are indicators of coherent classes. Duplicate words are all covered by
a common hypernym and hence this hypernym will be given more weight in the overall scoring. The
hypernyms introduced by infrequent and non-similar words are given less weight. This filters words that
may be included due to an error.
5 Patterns of prepositional use
The algorithm for class-labelling of words backed by the WordNet ontology allows us to predict the
typical classes or semantic types of the landmark and the target objects related by a preposition. From
these several patterns can be generated. Such patterns can be used both in the interpretation of visual
scenes (visual search) or generation of spatial referring expressions that optimally constrain the set of
intended objects. We create the patterns by collecting all targets and all landmarks that occur with a
particular preposition. We apply the class labelling on the bags of words representing the targets and
the landmarks to obtain a set of classes representing targets and landmarks. Finally, for every tuple
?target, preposition, landmark? we replace target and landmark with target class and landmark class and
collect a set of ?target class, preposition, landmark class? patterns. This method assumes that targets
and landmarks are semantically independent of each other. Below are some examples of patterns that
our algorithm has found (the notation for the names of the objects corresponds to the names of synsets
in the WordNet taxonomy, the numbers in brackets indicate the number of examples out of total ex-
amples covered by this pattern): (i) travel.v.01 over object.n.01 (9/713), sunlight.n.01 over object.n.01
(13/713), bridge.n.01 over object.n.01 (23/713), bridge.n.01 over body of water.n.01 (42/713), air.v.03
over object.n.01 (36/713), artifact.n.01 over body of water.n.01 (42/713), artifact.n.01 over object.n.01
35
(175/713),. . . (ii) breeze.n.01 above body of water.n.01 (8/183), person.n.01 above artifact.n.01 (9/183),
artifact.n.01 above steer.v.01 (14/183), artifact.n.01 above entrance.n.01 (16/183) artifact.n.01 above ar-
tifact.n.01 (27/183),. . . (iii) person.n.01 under tree.n.01 (7/213), shirt.n.01 under sweater.n.01 (8/213),
person.n.01 under body of water.n.01 (11/213), person.n.01 under artifact.n.01 (13/213) artifact.n.01
under artifact.n.01 (16/213), person.n.01 under structure.n.01 (17/213), artifact.n.01 under structure.n.01
(21/213),. . . (iv) box.n.05 below window.n.08 (1/14), crown.n.04 below script.n.01 (2/14),. . .
4
The
patterns show that different prepositions which are seemingly synonyms when considering geometry
(?over?/?above? and ?below?/?under?) do relate different types of objects and from the labels of the
semantic classes we may speculate what kind of semantic knowledge the relations are sensitive to. Im-
portantly, the labels of the classes and different patterns extracted show that there may be several distinct
and even unrelated situations that a preposition is referring to. Consider for example, person.n.01 under
tree.n.01, shirt.n.01 under sweater.n.01 (8/213), person.n.01 under body of water.n.01 are denoting three
different kinds of situations which require distinct and unrelated geometric arrangements. Overall, the
results indicate that the method is able to extract functional knowledge which is a reflection of the way
humans conceptualise objects and relations between them and which may be useful for improving visual
processing of scenes.
Another question we set off to answer is whether from the patterns one can determine the functional
and geometric bias of a preposition. As noted previously, our application of the class labelling algorithm
is greedy and attempts to cover a large number of words. The more words are generalised over the
more generic classes are created. To counter this confounding factor we down-sampled the dataset by
creating, for the results we report here, 50 samples of 20 randomly chosen words. The same procedure
for creating patterns of prepositional use was applied as before. On each sampling iteration we estimate
for each preposition (i) the average depth of the target and landmark hypernyms in the WordNet
taxonomy, (ii) the number of patterns created, and (iii) the entropy of the patterns over the examples
in the dataset. Finally, we average all values obtained from iterations and we rank the prepositions by the
ascending values of the these measures as shown below:
(i) on (3.17), near (3.55), with (3.66), next to (3.83), of (3.95), between (4.17), in front of (4.26), above
(4.27), over (4.48), around (4.52), behind (4.65), from (4.74), at (4.89), under (4.93), for (4.97),
through (5.27), in (5.45)
(ii) on (10.5), with (11.5), near (12), next to (12.1), between (12.6), of (12.6), above (12.7), around (13),
in front of (13.1), over (13.7), from (13.8), behind (13.9), for (14.2), under (14.3), in (14.5), through
(15.1), at (15.2)
(iii) on (2.74), next to (3.05), with (3.07), near (3.1), between (3.2), of (3.29), above (3.33), around
(3.36), in front of (3.39), over (3.48), from (3.5), behind (3.51), for (3.59), under (3.62), in (3.62),
through (3.75), at (3.76)
With small variations all measures (i), (ii) and (iii) rank the prepositions very similarly. Items at the
beginning of the list are used with target and landmark objects that belong to more general classes (i),
they are covered by a lower number of preposition patterns (ii), and the entropy of these patterns is low
(iii)
5
. Hence, we expect that items at the top of the lists are less affected by the type of the target and
landmark objects than items at the bottom of the lists. They are the prepositions where the geometric
component is stronger to determine the spatial relation. On the other hand, prepositions at the bottom
of the list rely more on the functional component of meaning. The results predict the observations from
the literature. Although, being sometimes quite close ?above? precedes ?over? in respect to to all three
measures. ?Below? was not processed in this study as there were too few examples but ?under? is found
at the tail of the list. The ranking of other prepositions also aligns with our intuitions. For example,
?on?, appearing as the head of the list, requires a contact between objects (a geometric notion), whereas
?in?, appearing in the tail of the list requires that the target is constrained by the landmark (a functional
notion).
4
There are only 14 examples of ?below? in the dataset for which nearly always unique patterns were created.
5
Entropy balances between the number of classes and the frequency of items in them. Low entropy indicates that there is a
tendency of items clustering in small number of specific classes rather rather than being equally dispersed over classes.
36
6 Conclusions and further work
In the preceding discussion we have demonstrated a method of extracting (i) functional semantic knowl-
edge ? required for the generation and interpretation of spatial descriptions ? from the corpora of de-
scriptions referring to visual scenes and (ii) made predictions about the bias of spatial descriptions to
functional knowledge. We hope that this information can facilitate visual search as it further restricts the
set of possible situations and objects involved. We have constructed several patterns of preposition use
and have shown that a preposition such as ?under? may refer to several distinct situations constrained
by the knowledge of object function and that these situations would require geometric representations
that are likely to be quite different. This knowledge should allow us to create different routines for
visual search that could be applied over a wide set of domains and would better approximate the way
humans perceive and reason about space. The applicability of this information for visual search must be
properly evaluated in a visual application. Estimating the functional or geometric bias of prepositions
informs us for their modelling but more importantly confirms that the patterns extracted here follow the
experimental observations reported in the literature.
In the procedure we have made several design choices: the choice of the corpora and the way the
corpora is processed and information extracted, the algorithm with which words are labelled for semantic
classes and finally the method with which patterns are created. For example, before class labelling we
could use an algorithm that clusters words of similar hypernym depth into discrete classes over which
hypernyms are generalised. This would allow us to distinguish better between different situations that
a preposition is referring to. When creating patterns we assume that target and landmark objects are
independent of each other. However, this may not necessarily be the case. For example, the category of
the target object may constrain the category of the landmark which means that the latter category should
only be generalised over landmark words that occur with some target category.
References
Kenny R Coventry and Simon C Garrod. 2004. Saying, seeing, and acting: the psychological semantics of spatial
prepositions. Psychology Press, Hove, East Sussex.
Christiane Fellbaum. 1998. WordNet: an electronic lexical database. MIT Press, Cambridge, Mass.
Simon Garrod, Gillian Ferrier, and Siobhan Campbell. 1999. In and on: investigating the functional geometry of
spatial prepositions. Cognition, 72(2):167?189.
Michael Grubinger, Paul D. Clough, Henning M?uller, and Thomas Deselaers. 2006. The IAPR benchmark: A new
evaluation resource for visual information systems. In Proceedings of OntoImage 2006: Workshop on language
resources for content-based mage retrieval during LREC 2006, Genoa, Italy, 22 May. European Language
Resources Association.
Lars Kunze, Chris Burbridge, and Nick Hawes. 2014. Bootstrapping probabilistic models of qualitative spatial
relations for active visual object search. In AAAI Spring Symposium 2014 on Qualitative Representations for
Robots, Stanford University in Palo Alto, California, US, March, 24?26.
Marie-Catherine De Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed depen-
dency parses from phrase structure parses. In Proceedings of Int?l Conf. on Language Resources and Evaluation
(LREC), pages 449?454, Genoa, Italy. European Language Resources Association.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia Hockenmaier. 2010. Collecting image annotations
using Amazon?s Mechanical Turk. In Proceedings of the NAACL HLT 2010 Workshop on creating speech
and language data with Amazon?s Mechanical Turk, Los Angeles, CA, 6 June. North American Chapter of the
Association for Computational Linguistics (NAACL).
Kristoffer Sj?o?o. 2011. Functional understanding of space: Representing spatial knowledge using concepts
grounded in an agent?s purpose. Ph.D. thesis, KTH, Computer Vision and Active Perception (CVAP), Cen-
tre for Autonomous Systems (CAS), Stockholm, Sweden.
Dominic Widdows. 2003. Unsupervised methods for developing taxonomies by combining syntactic and statis-
tical information. In Proceedings of the 2003 Conference of the North American Chapter of the Association
for Computational Linguistics on Human Language Technology, volume 1, pages 197?204. Association for
Computational Linguistics.
37
