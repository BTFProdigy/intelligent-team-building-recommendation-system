Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 58?65,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
An Integrated Approach to Robust Processing
of Situated Spoken Dialogue
Pierre Lison
Language Technology Lab,
DFKI GmbH,
Saarbru?cken, Germany
pierre.lison@dfki.de
Geert-Jan M. Kruijff
Language Technology Lab,
DFKI GmbH,
Saarbru?cken, Germany
gj@dfki.de
Abstract
Spoken dialogue is notoriously hard to
process with standard NLP technologies.
Natural spoken dialogue is replete with
disfluent, partial, elided or ungrammatical
utterances, all of which are very hard to
accommodate in a dialogue system. Fur-
thermore, speech recognition is known to
be a highly error-prone task, especially for
complex, open-ended discourse domains.
The combination of these two problems
? ill-formed and/or misrecognised speech
inputs ? raises a major challenge to the de-
velopment of robust dialogue systems.
We present an integrated approach for ad-
dressing these two issues, based on a in-
cremental parser for Combinatory Cate-
gorial Grammar. The parser takes word
lattices as input and is able to handle ill-
formed and misrecognised utterances by
selectively relaxing its set of grammati-
cal rules. The choice of the most rele-
vant interpretation is then realised via a
discriminative model augmented with con-
textual information. The approach is fully
implemented in a dialogue system for au-
tonomous robots. Evaluation results on a
Wizard of Oz test suite demonstrate very
significant improvements in accuracy and
robustness compared to the baseline.
1 Introduction
Spoken dialogue is often considered to be one of
the most natural means of interaction between a
human and a robot. It is, however, notoriously
hard to process with standard language process-
ing technologies. Dialogue utterances are often in-
complete or ungrammatical, and may contain nu-
merous disfluencies like fillers (err, uh, mm), rep-
etitions, self-corrections, etc. Rather than getting
crisp-and-clear commands such as ?Put the red
ball inside the box!?, it is more likely the robot
will hear such kind of utterance: ?right, now, could
you, uh, put the red ball, yeah, inside the ba/ box!?.
This is natural behaviour in human-human interac-
tion (Ferna?ndez and Ginzburg, 2002) and can also
be observed in several domain-specific corpora for
human-robot interaction (Topp et al, 2006).
Moreover, even in the (rare) case where the ut-
terance is perfectly well-formed and does not con-
tain any kind of disfluencies, the dialogue sys-
tem still needs to accomodate the various speech
recognition errors thay may arise. This problem
is particularly acute for robots operating in real-
world noisy environments and deal with utterances
pertaining to complex, open-ended domains.
The paper presents a new approach to address
these two difficult issues. Our starting point is the
work done by Zettlemoyer and Collins on parsing
using relaxed CCG grammars (Zettlemoyer and
Collins, 2007) (ZC07). In order to account for
natural spoken language phenomena (more flex-
ible word order, missing words, etc.), they aug-
ment their grammar framework with a small set
of non-standard combinatory rules, leading to a
relaxation of the grammatical constraints. A dis-
criminative model over the parses is coupled with
the parser, and is responsible for selecting the most
likely interpretation(s) among the possible ones.
In this paper, we extend their approach in two
important ways. First, ZC07 focused on the treat-
ment of ill-formed input, and ignored the speech
recognition issues. Our system, to the contrary,
is able to deal with both ill-formed and misrec-
ognized input, in an integrated fashion. This is
done by augmenting the set of non-standard com-
binators with new rules specifically tailored to deal
with speech recognition errors.
Second, the only features used by ZC07 are syn-
tactic features (see 3.4 for details). We signifi-
cantly extend the range of features included in the
58
discriminative model, by incorporating not only
syntactic, but also acoustic, semantic and contex-
tual information into the model.
An overview of the paper is as follows. We first
describe in Section 2 the cognitive architecture in
which our system has been integrated. We then
discuss the approach in detail in Section 3. Fi-
nally, we present in Section 4 the quantitative eval-
uations on a WOZ test suite, and conclude.
2 Architecture
The approach we present in this paper is fully im-
plemented and integrated into a cognitive architec-
ture for autonomous robots. A recent version of
this system is described in (Hawes et al, 2007). It
is capable of building up visuo-spatial models of
a dynamic local scene, continuously plan and exe-
cute manipulation actions on objects within that
scene. The robot can discuss objects and their
material- and spatial properties for the purpose of
visual learning and manipulation tasks.
Figure 1: Architecture schema of the communica-
tion subsystem (only for comprehension).
Figure 2 illustrates the architecture schema for
the communication subsystem incorporated in the
cognitive architecture (only the comprehension
part is shown).
Starting with ASR, we process the audio signal
to establish a word lattice containing statistically
ranked hypotheses about word sequences. Subse-
quently, parsing constructs grammatical analyses
for the given word lattice. A grammatical analy-
sis constructs both a syntactic analysis of the ut-
terance, and a representation of its meaning. The
analysis is based on an incremental chart parser1
for Combinatory Categorial Grammar (Steedman
and Baldridge, 2009). These meaning represen-
tations are ontologically richly sorted, relational
1Built on top of the OpenCCG NLP library:
http://openccg.sf.net
structures, formulated in a (propositional) descrip-
tion logic, more precisely in the HLDS formal-
ism (Baldridge and Kruijff, 2002). The parser
compacts all meaning representations into a sin-
gle packed logical form (Carroll and Oepen, 2005;
Kruijff et al, 2007). A packed LF represents con-
tent similar across the different analyses as a single
graph, using over- and underspecification of how
different nodes can be connected to capture lexical
and syntactic forms of ambiguity.
At the level of dialogue interpretation, a packed
logical form is resolved against a SDRS-like di-
alogue model (Asher and Lascarides, 2003) to
establish contextual co-reference and dialogue
moves.
Linguistic interpretations must finally be associ-
ated with extra-linguistic knowledge about the en-
vironment ? dialogue comprehension hence needs
to connect with other subarchitectures like vision,
spatial reasoning or planning. We realise this
information binding between different modalities
via a specific module, called the ?binder?, which is
responsible for the ontology-based mediation ac-
cross modalities (Jacobsson et al, 2008).
2.1 Context-sensitivity
The combinatorial nature of language provides
virtually unlimited ways in which we can commu-
nicate meaning. This, of course, raises the ques-
tion of how precisely an utterance should then be
understood as it is being heard. Empirical stud-
ies have investigated what information humans use
when comprehending spoken utterances. An im-
portant observation is that interpretation in con-
text plays a crucial role in the comprehension of
utterance as it unfolds (Knoeferle and Crocker,
2006). During utterance comprehension, humans
combine linguistic information with scene under-
standing and ?world knowledge?.
Figure 2: Context-sensitivity in processing situ-
ated dialogue understanding
Several approaches in situated dialogue for
human-robot interaction have made similar obser-
59
vations (Roy, 2005; Roy and Mukherjee, 2005;
Brick and Scheutz, 2007; Kruijff et al, 2007): A
robot?s understanding can be improved by relating
utterances to the situated context. As we will see
in the next section, by incorporating contextual in-
formation into our model, our approach to robust
processing of spoken dialogue seeks to exploit this
important insight.
3 Approach
3.1 Grammar relaxation
Our approach to robust processing of spoken di-
alogue rests on the idea of grammar relaxation:
the grammatical constraints specified in the gram-
mar are ?relaxed? to handle slightly ill-formed or
misrecognised utterances.
Practically, the grammar relaxation is done
via the introduction of non-standard CCG rules
(Zettlemoyer and Collins, 2007). In Combinatory
Categorial Grammar, the rules are used to assem-
ble categories to form larger pieces of syntactic
and semantic structure. The standard rules are ap-
plication (<,>), composition (B), and type rais-
ing (T) (Steedman and Baldridge, 2009).
Several types of non-standard rules have been
introduced. We describe here the two most impor-
tant ones: the discourse-level composition rules,
and the ASR correction rules. We invite the reader
to consult (Lison, 2008) for more details on the
complete set of grammar relaxation rules.
3.1.1 Discourse-level composition rules
In natural spoken dialogue, we may encounter ut-
terances containing several independent ?chunks?
without any explicit separation (or only a short
pause or a slight change in intonation), such as
(1) ?yes take the ball no the other one on your
left right and now put it in the box.?
Even if retrieving a fully structured parse for
this utterance is difficult to achieve, it would be
useful to have access to a list of smaller ?discourse
units?. Syntactically speaking, a discourse unit
can be any type of saturated atomic categories -
from a simple discourse marker to a full sentence.
The type raising rule Tdu allows the conversion
of atomic categories into discourse units:
A : @if ? du : @if (Tdu)
where A represents an arbitrary saturated
atomic category (s, np, pp, etc.).
The rule>C is responsible for the integration of
two discourse units into a single structure:
du : @if, du : @jg ?
du : @{d:d-units}(list?
(?FIRST? i ? f)?
(?NEXT? j ? g)) (>C)
3.1.2 ASR error correction rules
Speech recognition is a highly error-prone task. It
is however possible to partially alleviate this prob-
lem by inserting new error-correction rules (more
precisely, new lexical entries) for the most fre-
quently misrecognised words.
If we notice e.g. that the ASR system frequently
substitutes the word ?wrong? for the word ?round?
during the recognition (because of their phonolog-
ical proximity), we can introduce a new lexical en-
try in the lexicon in order to correct this error:
round ` adj : @attitude(wrong) (2)
A set of thirteen new lexical entries of this type
have been added to our lexicon to account for the
most frequent recognition errors.
3.2 Parse selection
Using more powerful grammar rules to relax the
grammatical analysis tends to increase the number
of parses. We hence need a a mechanism to dis-
criminate among the possible parses. The task of
selecting the most likely interpretation among a set
of possible ones is called parse selection. Once all
the possible parses for a given utterance are com-
puted, they are subsequently filtered or selected
in order to retain only the most likely interpreta-
tion(s). This is done via a (discriminative) statisti-
cal model covering a large number of features.
Formally, the task is defined as a function F :
X ? Y where the domain X is the set of possible
inputs (in our case, X is the set of possible word
lattices), and Y the set of parses. We assume:
1. A function GEN(x) which enumerates all
possible parses for an input x. In our case,
this function simply represents the set of
parses of x which are admissible according
to the CCG grammar.
2. A d-dimensional feature vector f(x, y) ?
<d, representing specific features of the pair
(x, y). It can include various acoustic, syn-
tactic, semantic or contextual features which
can be relevant in discriminating the parses.
60
3. A parameter vector w ? <d.
The function F , mapping a word lattice to its
most likely parse, is then defined as:
F (x) = argmax
y?GEN(x)
wT ? f(x, y) (3)
where wT ? f(x, y) is the inner product
?d
s=1ws fs(x, y), and can be seen as a measure
of the ?quality? of the parse. Given the parameters
w, the optimal parse of a given utterance x can be
therefore easily determined by enumerating all the
parses generated by the grammar, extracting their
features, computing the inner product wT ?f(x, y),
and selecting the parse with the highest score.
The task of parse selection is an example of
structured classification problem, which is the
problem of predicting an output y from an input
x, where the output y has a rich internal structure.
In the specific case of parse selection, x is a word
lattice, and y a logical form.
3.3 Learning
3.3.1 Training data
In order to estimate the parameters w, we need a
set of training examples. Unfortunately, no corpus
of situated dialogue adapted to our task domain is
available to this day, let alne semantically anno-
tated. The collection of in-domain data via Wizard
of Oz experiments being a very costly and time-
consuming process, we followed the approach ad-
vocated in (Weilhammer et al, 2006) and gener-
ated a corpus from a hand-written task grammar.
To this end, we first collected a small set of
WoZ data, totalling about a thousand utterances.
This set is too small to be directly used as a cor-
pus for statistical training, but sufficient to cap-
ture the most frequent linguistic constructions in
this particular context. Based on it, we designed
a domain-specific CFG grammar covering most of
the utterances. Each rule is associated to a seman-
tic HLDS representation. Weights are automati-
cally assigned to each grammar rule by parsing our
corpus, hence leading to a small stochastic CFG
grammar augmented with semantic information.
Once the grammar is specified, it is randomly
traversed a large number of times, resulting in a
larger set (about 25.000) of utterances along with
their semantic representations. Since we are inter-
ested in handling errors arising from speech recog-
nition, we also need to ?simulate? the most fre-
quent recognition errors. To this end, we synthe-
sise each string generated by the domain-specific
CFG grammar, using a text-to-speech engine2,
feed the audio stream to the speech recogniser,
and retrieve the recognition result. Via this tech-
nique, we are able to easily collect a large amount
of training data3.
3.3.2 Perceptron learning
The algorithm we use to estimate the parameters
w using the training data is a perceptron. The al-
gorithm is fully online - it visits each example in
turn and updates w if necessary. Albeit simple,
the algorithm has proven to be very efficient and
accurate for the task of parse selection (Collins
and Roark, 2004; Collins, 2004; Zettlemoyer and
Collins, 2005; Zettlemoyer and Collins, 2007).
The pseudo-code for the online learning algo-
rithm is detailed in [Algorithm 1].
It works as follows: the parameters w are first
initialised to some arbitrary values. Then, for
each pair (xi, zi) in the training set, the algorithm
searchs for the parse y? with the highest score ac-
cording to the current model. If this parse happens
to match the best parse which generates zi (which
we shall denote y?), we move to the next example.
Else, we perform a simple perceptron update on
the parameters:
w = w + f(xi, y?)? f(xi, y?) (4)
The iteration on the training set is repeated T
times, or until convergence.
The most expensive step in this algorithm is
the calculation of y? = argmaxy?GEN(xi) w
T ?
f(xi, y) - this is the decoding problem.
It is possible to prove that, provided the train-
ing set (xi, zi) is separable with margin ? > 0, the
algorithm is assured to converge after a finite num-
ber of iterations to a model with zero training er-
rors (Collins and Roark, 2004). See also (Collins,
2004) for convergence theorems and proofs.
3.4 Features
As we have seen, the parse selection operates by
enumerating the possible parses and selecting the
2We used MARY (http://mary.dfki.de) for the
text-to-speech engine.
3Because of its relatively artificial character, the quality
of such training data is naturally lower than what could be
obtained with a genuine corpus. But, as the experimental re-
sults will show, it remains sufficient to train the perceptron
for the parse selection task, and achieve significant improve-
ments in accuracy and robustness. In a near future, we plan
to progressively replace this generated training data by a real
spoken dialogue corpus adapted to our task domain.
61
Algorithm 1 Online perceptron learning
Require: - set of n training examples {(xi, zi) : i = 1...n}
- T : number of iterations over the training set
- GEN(x): function enumerating possible parses
for an input x, according to the CCG grammar.
- GEN(x, z): function enumerating possible parses
for an input x and which have semantics z,
according to the CCG grammar.
- L(y) maps a parse tree y to its logical form.
- Initial parameter vector w0
% Initialise
w? w0
% Loop T times on the training examples
for t = 1...T do
for i = 1...n do
% Compute best parse according to current model
Let y? = argmaxy?GEN(xi) w
T ? f(xi, y)
% If the decoded parse 6= expected parse, update the
parameters
if L(y?) 6= zi then
% Search the best parse for utterance xi with se-
mantics zi
Let y? = argmaxy?GEN(xi,zi) w
T ? f(xi, y)
% Update parameter vector w
Set w = w + f(xi, y?)? f(xi, y?)
end if
end for
end for
return parameter vector w
one with the highest score according to the linear
model parametrised by w.
The accuracy of our method crucially relies on
the selection of ?good? features f(x, y) for our
model - that is, features which help discriminat-
ing the parses. They must also be relatively cheap
to compute. In our model, the features are of four
types: semantic features, syntactic features, con-
textual features, and speech recognition features.
3.4.1 Semantic features
What are the substructures of a logical form which
may be relevant to discriminate the parses? We de-
fine features on the following information sources:
1. Nominals: for each possible pair
?prop, sort?, we include a feature fi in
f(x, y) counting the number of nominals
with ontological sort sort and proposition
prop in the logical form.
2. Ontological sorts: occurrences of specific
ontological sorts in the logical form.
Figure 3: graphical representation of the HLDS
logical form for ?I want you to take the mug?.
3. Dependency relations: following (Clark and
Curran, 2003), we also model the depen-
dency structure of the logical form. Each
dependency relation is defined as a triple
?sorta, sortb, label?, where sorta denotes
the sort of the incoming nominal, sortb the
sort of the outgoing nominal, and label is the
relation label.
4. Sequences of dependency relations: number
of occurrences of particular sequences (ie. bi-
gram counts) of dependency relations.
The features on nominals and ontological sorts
aim at modeling (aspects of) lexical semantics -
e.g. which meanings are the most frequent for a
given word -, whereas the features on relations and
sequence of relations focus on sentential seman-
tics - which dependencies are the most frequent.
These features therefore help us handle lexical and
syntactic ambiguities.
3.4.2 Syntactic features
By ?syntactic features?, we mean features associ-
ated to the derivational history of a specific parse.
The main use of these features is to penalise to a
correct extent the application of the non-standard
rules introduced into the grammar.
To this end, we include in the feature vector
f(x, y) a new feature for each non-standard rule,
which counts the number of times the rule was ap-
plied in the parse.
62
pick
s/particle/np
cup
up corr
particle
s/np
>
the
np/n
ball
n
np >
s >
Figure 4: CCG derivation of ?pick cup the ball?.
In the derivation shown in the figure 4, the rule
corr (correction of a speech recognition error) is
applied once, so the corresponding feature value is
set to 1. The feature values for the remaining rules
are set to 0, since they are absent from the parse.
These syntactic features can be seen as a penalty
given to the parses using these non-standard rules,
thereby giving a preference to the ?normal? parses
over them. This mechanism ensures that the gram-
mar relaxation is only applied ?as a last resort?
when the usual grammatical analysis fails to pro-
vide a full parse. Of course, depending on the
relative frequency of occurrence of these rules in
the training corpus, some of them will be more
strongly penalised than others.
3.4.3 Contextual features
As we have already outlined in the background
section, one striking characteristic of spoken dia-
logue is the importance of context. Understanding
the visual and discourse contexts is crucial to re-
solve potential ambiguities and compute the most
likely interpretation(s) of a given utterance.
The feature vector f(x, y) therefore includes
various features related to the context:
1. Activated words: our dialogue system main-
tains in its working memory a list of contex-
tually activated words (cfr. (Lison and Krui-
jff, 2008)). This list is continuously updated
as the dialogue and the environment evolves.
For each context-dependent word, we include
one feature counting the number of times it
appears in the utterance string.
2. Expected dialogue moves: for each possible
dialogue move, we include one feature indi-
cating if the dialogue move is consistent with
the current discourse model. These features
ensure for instance that the dialogue move
following a QuestionYN is a Accept, Re-
ject or another question (e.g. for clarification
requests), but almost never an Opening.
3. Expected syntactic categories: for each
atomic syntactic category in the CCG gram-
mar, we include one feature indicating if the
category is consistent with the current dis-
course model. These features can be used to
handle sentence fragments.
3.4.4 Speech recognition features
Finally, the feature vector f(x, y) also includes
features related to the speech recognition. The
ASR module outputs a set of (partial) recognition
hypotheses, packed in a word lattice. One exam-
ple of such a structure is given in Figure 5. Each
recognition hypothesis is provided with an asso-
ciated confidence score, and we want to favour
the hypotheses with high confidence scores, which
are, according to the statistical models incorpo-
rated in the ASR, more likely to reflect what was
uttered.
To this end, we introduce three features: the
acoustic confidence score (confidence score pro-
vided by the statistical models included in the
ASR), the semantic confidence score (based on a
?concept model? also provided by the ASR), and
the ASR ranking (hypothesis rank in the word lat-
tice, from best to worst).
Figure 5: Example of word lattice
4 Experimental evaluation
We performed a quantitative evaluation of our ap-
proach, using its implementation in a fully inte-
grated system (cf. Section 2). To set up the ex-
periments for the evaluation, we have gathered a
corpus of human-robot spoken dialogue for our
task-domain, which we segmented and annotated
manually with their expected semantic interpreta-
tion. The data set contains 195 individual utter-
ances along with their complete logical forms.
4.1 Results
Three types of quantitative results are extracted
from the evaluation results: exact-match, partial-
match, and word error rate. Tables 1, 2 and 3 illus-
trate the results, broken down by use of grammar
relaxation, use of parse selection, and number of
recognition hypotheses considered.
63
Size of word lattice
(number of NBests)
Grammar
relaxation
Parse
selection Precision Recall F1-value
(Baseline) 1 No No 40.9 45.2 43.0
. 1 No Yes 59.0 54.3 56.6
. 1 Yes Yes 52.7 70.8 60.4
. 3 Yes Yes 55.3 82.9 66.3
. 5 Yes Yes 55.6 84.0 66.9
(Full approach) 10 Yes Yes 55.6 84.9 67.2
Table 1: Exact-match accuracy results (in percents).
Size of word lattice
(number of NBests)
Grammar
relaxation
Parse
selection Precision Recall F1-value
(Baseline) 1 No No 86.2 56.2 68.0
. 1 No Yes 87.4 56.6 68.7
. 1 Yes Yes 88.1 76.2 81.7
. 3 Yes Yes 87.6 85.2 86.4
. 5 Yes Yes 87.6 86.0 86.8
(Full approach) 10 Yes Yes 87.7 87.0 87.3
Table 2: Partial-match accuracy results (in percents).
Each line in the tables corresponds to a possible
configuration. Tables 1 and 2 give the precision,
recall and F1 value for each configuration (respec-
tively for the exact- and partial-match), and Table
3 gives the Word Error Rate [WER].
The first line corresponds to the baseline: no
grammar relaxation, no parse selection, and use of
the first NBest recognition hypothesis. The last
line corresponds to the results with the full ap-
proach: grammar relaxation, parse selection, and
use of 10 recognition hypotheses.
Size of word
lattice (NBests)
Grammar
relaxation
Parse
selection WER
1 No No 20.5
1 Yes Yes 19.4
3 Yes Yes 16.5
5 Yes Yes 15.7
10 Yes Yes 15.7
Table 3: Word error rate (in percents).
4.2 Comparison with baseline
Here are the comparative results we obtained:
? Regarding the exact-match results between
the baseline and our approach (grammar re-
laxation and parse selection with all fea-
tures activated for NBest 10), the F1-measure
climbs from 43.0 % to 67.2 %, which means
a relative difference of 56.3 %.
? For the partial-match, the F1-measure goes
from 68.0 % for the baseline to 87.3 % for
our approach ? a relative increase of 28.4 %.
? We obverse a significant decrease in WER:
we go from 20.5 % for the baseline to 15.7 %
with our approach. The difference is statisti-
cally significant (p-value for t-tests is 0.036),
and the relative decrease of 23.4 %.
5 Conclusions
We presented an integrated approach to the pro-
cessing of (situated) spoken dialogue, suited to
the specific needs and challenges encountered in
human-robot interaction.
In order to handle disfluent, partial, ill-formed
or misrecognized utterances, the grammar used by
the parser is ?relaxed? via the introduction of a
set of non-standard combinators which allow for
the insertion/deletion of specific words, the com-
bination of discourse fragments or the correction
of speech recognition errors.
The relaxed parser yields a (potentially large)
set of parses, which are then packed and retrieved
by the parse selection module. The parse selec-
tion is based on a discriminative model exploring a
set of relevant semantic, syntactic, contextual and
acoustic features extracted for each parse. The pa-
rameters of this model are estimated against an au-
tomatically generated corpus of ?utterance, logical
form? pairs. The learning algorithm is an percep-
tron, a simple albeit efficient technique for param-
eter estimation.
As forthcoming work, we shall examine the po-
tential extension of our approach in new direc-
tions, such as the exploitation of parse selection
for incremental scoring/pruning of the parse chart,
64
the introduction of more refined contextual fea-
tures, or the use of more sophisticated learning al-
gorithms, such as Support Vector Machines.
References
Nicholas Asher and Alex Lascarides. 2003. Logics of
Conversation. Cambridge University Press.
J. Baldridge and G.-J. M. Kruijff. 2002. Coupling
CCG and hybrid logic dependency semantics. In
ACL?02: Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics,
pages 319?326, Philadelphia, PA. Association for
Computational Linguistics.
T. Brick and M. Scheutz. 2007. Incremental natu-
ral language processing for HRI. In Proceeding of
the ACM/IEEE international conference on Human-
Robot Interaction (HRI?07), pages 263 ? 270.
J. Carroll and S. Oepen. 2005. High efficiency re-
alization for a wide-coverage unification grammar.
In Proceedings of the International Joint Confer-
ence on Natural Language Processing (IJCNLP?05),
pages 165?176.
Stephen Clark and James R. Curran. 2003. Log-linear
models for wide-coverage ccg parsing. In Proceed-
ings of the 2003 conference on Empirical methods in
natural language processing, pages 97?104, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In ACL
?04: Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics, page
111, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Michael Collins. 2004. Parameter estimation for
statistical parsing models: theory and practice of
distribution-free methods. In New developments in
parsing technology, pages 19?55. Kluwer Academic
Publishers.
R. Ferna?ndez and J. Ginzburg. 2002. A corpus study
of non-sentential utterances in dialogue. Traitement
Automatique des Langues, 43(2):12?43.
N. Hawes, A. Sloman, J. Wyatt, M. Zillich, H. Jacob-
sson, G.J. M. Kruijff, M. Brenner, G. Berginc, and
D. Skocaj. 2007. Towards an integrated robot with
multiple cognitive functions. In AAAI, pages 1548?
1553. AAAI Press.
Henrik Jacobsson, Nick Hawes, Geert-Jan Kruijff, and
Jeremy Wyatt. 2008. Crossmodal content bind-
ing in information-processing architectures. In Pro-
ceedings of the 3rd ACM/IEEE International Con-
ference on Human-Robot Interaction (HRI), Amster-
dam, The Netherlands, March 12?15.
P. Knoeferle and M.C. Crocker. 2006. The coordinated
interplay of scene, utterance, and world knowledge:
evidence from eye tracking. Cognitive Science.
G.J.M. Kruijff, P. Lison, T. Benjamin, H. Jacobsson,
and N.A. Hawes. 2007. Incremental, multi-level
processing for comprehending situated dialogue in
human-robot interaction. In Language and Robots:
Proceedings from the Symposium (LangRo?2007),
pages 55?64, Aveiro, Portugal, December.
Pierre Lison and Geert-Jan M. Kruijff. 2008. Salience-
driven contextual priming of speech recognition for
human-robot interaction. In Proceedings of the 18th
European Conference on Artificial Intelligence, Pa-
tras (Greece).
Pierre Lison. 2008. Robust processing of situated spo-
ken dialogue. Master?s thesis, Universita?t des Saar-
landes, Saarbru?cken.
D. Roy and N. Mukherjee. 2005. Towards situated
speech understanding: visual context priming of
language models. Computer Speech & Language,
19(2):227?248, April.
D.K. Roy. 2005. Semiotic schemas: A framework for
grounding language in action and perception. Artifi-
cial Intelligence, 167(1-2):170?205.
Mark Steedman and Jason Baldridge. 2009. Combina-
tory categorial grammar. In Robert Borsley and Ker-
sti Bo?rjars, editors, Nontransformational Syntax: A
Guide to Current Models. Blackwell, Oxford.
E. A. Topp, H. Hu?ttenrauch, H.I. Christensen, and
K. Severinson Eklundh. 2006. Bringing together
human and robotic environment representations ?
a pilot study. In Proc. of the IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems
(IROS), Beijing, China, October.
Karl Weilhammer, Matthew N. Stuttle, and Steve
Young. 2006. Bootstrapping language models
for dialogue systems. In Proceedings of INTER-
SPEECH 2006, Pittsburgh, PA.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI ?05, Proceedings of the 21st Con-
ference in Uncertainty in Artificial Intelligence, July
2005, pages 658?666.
Luke Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 678?687.
65
Proceedings of the ACL 2010 Student Research Workshop, pages 7?12,
Uppsala, Sweden, 13 July 2010.
c?2010 Association for Computational Linguistics
Towards Relational POMDPs for Adaptive Dialogue Management
Pierre Lison
Language Technology Lab
German Research Centre for Artificial Intelligence (DFKI GmbH)
Saarbr?ucken, Germany
Abstract
Open-ended spoken interactions are typi-
cally characterised by both structural com-
plexity and high levels of uncertainty,
making dialogue management in such set-
tings a particularly challenging problem.
Traditional approaches have focused on
providing theoretical accounts for either
the uncertainty or the complexity of spo-
ken dialogue, but rarely considered the
two issues simultaneously. This paper de-
scribes ongoing work on a new approach
to dialogue management which attempts
to fill this gap. We represent the interac-
tion as a Partially Observable Markov De-
cision Process (POMDP) over a rich state
space incorporating both dialogue, user,
and environment models. The tractability
of the resulting POMDP can be preserved
using a mechanism for dynamically con-
straining the action space based on prior
knowledge over locally relevant dialogue
structures. These constraints are encoded
in a small set of general rules expressed as
a Markov Logic network. The first-order
expressivity of Markov Logic enables us
to leverage the rich relational structure of
the problem and efficiently abstract over
large regions of the state and action spaces.
1 Introduction
The development of spoken dialogue systems for
rich, open-ended interactions raises a number of
challenges, one of which is dialogue management.
The role of dialogue management is to determine
which communicative actions to take (i.e. what to
say) given a goal and particular observations about
the interaction and the current situation.
Dialogue managers have to face several issues.
First, spoken dialogue systems must usually deal
with high levels of noise and uncertainty. These
uncertainties may arise from speech recognition
errors, limited grammar coverage, or from various
linguistic and pragmatic ambiguities.
Second, open-ended dialogue is characteristi-
cally complex, and exhibits rich relational struc-
tures. Natural interactions should be adaptive to
a variety of factors dependent on the interaction
history, the general context, and the user prefer-
ences. As a consequence, the state space necessary
to model the dynamics of the environment tends to
be large and sparsely populated.
These two problems have typically been ad-
dressed separately in the literature. On the one
hand, the issue of uncertainty in speech under-
standing is usually dealt using a range of proba-
bilistic models combined with decision-theoretic
planning. Among these, Partially Observable
Markov Decision Process (POMDP) models have
recently emerged as a unifying mathematical
framework for dialogue management (Williams
and Young, 2007; Lemon and Pietquin, 2007).
POMDPs provide an explicit account for a wide
range of uncertainties related to partial observabil-
ity (noisy, incomplete spoken inputs) and stochas-
tic action effects (the world may evolve in unpre-
dictable ways after executing an action).
On the other hand, structural complexity is
typically addressed with logic-based approaches.
Some investigated topics in this paradigm are
pragmatic interpretation (Thomason et al, 2006),
dialogue structure (Asher and Lascarides, 2003),
or collaborative planning (Kruijff et al, 2008).
These approaches are able to model sophisticated
dialogue behaviours, but at the expense of robust-
ness and adaptivity. They generally assume com-
plete observability and provide only a very limited
account (if any) of uncertainties.
We are currently developing an hybrid approach
which simultaneously tackles the uncertainty and
complexity of dialogue management, based on a
7
POMDP framework. We present here our ongo-
ing work on this issue. In this paper, we more
specifically describe a new mechanism for dy-
namically constraining the space of possible ac-
tions available at a given time. Our aim is to use
such mechanism to significantly reduce the search
space and therefore make the planning problem
globally more tractable. This is performed in two
consecutive steps. We first structure the state space
using Markov Logic Networks, a first-order prob-
abilistic language. Prior pragmatic knowledge
about dialogue structure is then exploited to derive
the set of dialogue actions which are locally ad-
missible or relevant, and prune all irrelevant ones.
The first-order expressivity of Markov Logic Net-
works allows us to easily specify the constraints
via a small set of general rules which abstract over
large regions of the state and action spaces.
Our long-term goal is to develop an unified
framework for adaptive dialogue management in
rich, open-ended interactional settings.
This paper is structured as follows. Section 2
lays down the formal foundations of our work,
by describing dialogue management as a POMDP
problem. We then describe in Section 3 our ap-
proach to POMDP planning with control knowl-
edge using Markov Logic rules. Section 4 dis-
cusses some further aspects of our approach and
its relation to existing work, followed by the con-
clusion in Section 5.
2 Background
2.1 Partially Observable Markov Decision
Processes (POMDPs)
POMDPs are a mathematical model for sequential
decision-making in partially observable environ-
ments. It provides a powerful framework for con-
trol problems which combine partial observability,
uncertain action effects, incomplete knowledge of
the environment dynamics and multiple, poten-
tially conflicting objectives.
Via reinforcement learning, it is possible to
automatically learn near-optimal action policies
given a POMDP model combined with real or sim-
ulated user data (Schatzmann et al, 2007).
2.1.1 Formal definition
A POMDP is a tuple ?S,A,Z, T,?, R?, where:
? S is the state space, which is the model of
the world from the agent?s viewpoint. It is
defined as a set of mutually exclusive states.
z
t
s
t
t
?
a
t
z
t+1
s
t+1
s
t+2
z
t+2
a
t+1
?
r(a
t
, s
t
) r(a
t+1
, s
t+1
)
Figure 1: Bayesian decision network correspond-
ing to the POMDP model. Hidden variables are
greyed. Actions are represented as rectangles to
stress that they are system actions rather than ob-
served variables. Arcs into circular nodes express
influence, whereas arcs into squared nodes are in-
formational. For readability, only one state is
shown at each time step, but it should be noted
that the policy pi is function of the full belief state
rather than a single (unobservable) state.
? A is the action space: the set of possible ac-
tions at the disposal of the agent.
? Z is the observation space: the set of obser-
vations which can be captured by the agent.
They correspond to features of the environ-
ment which can be directly perceived by the
agent?s sensors.
? T is the transition function, defined as T :
S ? A ? S ? [0, 1], where T (s, a, s
?
) =
P (s
?
|s, a) is the probability of reaching state
s
?
from state s if action a is performed.
? ? is the observation function, defined as
? : Z ? A ? S ? [0, 1], with ?(z, a, s
?
) =
P (z|a, s
?
), i.e. the probability of observing z
after performing a and being now in state s
?
.
? R is the reward function, defined as R :
S ? A ? <, R(s, a) encodes the utility for
the agent to perform the action a while in
state s. It is therefore a model for the goals or
preferences of the agent.
A graphical illustration of a POMDP model as
a Bayesian decision network is provided in Fig. 1.
In addition, a POMDP can include additional
parameters such as the horizon of the agent (num-
8
ber of look-ahead steps), and the discount factor
(weighting scheme for non-immediate rewards).
2.1.2 Beliefs and belief update
A key idea of POMDP is the assumption that the
state of the world is not directly accessible, and
can only be inferred via observation. Such uncer-
tainty is expressed in the belief state b, which is
a probability distribution over possible states, that
is: b : S ? [0, 1]. The belief state for a state
space of cardinality n is therefore represented in a
real-valued simplex of dimension (n?1).
This belief state is dynamically updated before
executing each action. The belief state update op-
erates as follows. At a given time step t, the agent
is in some unobserved state s
t
= s ? S . The
probability of being in state s at time t is writ-
ten as b
t
(s). Based on the current belief state b
t
,
the agent selects an action a
t
, receives a reward
R(s, a
t
) and transitions to a new (unobserved)
state s
t+1
= s
?
, where s
t+1
depends only on s
t
and a
t
. The agent then receives a new observation
o
t+1
which is dependent on s
t+1
and a
t
.
Finally, the belief distribution b
t
is updated,
based on o
t+1
and a
t
as follows
1
.
b
t+1
(s
?
)= P (s
?
|o
t+1
, a
t
, b
t
) (1)
=
P (o
t+1
|s
?
, a
t
, b
t
)P (s
?
|a
t
, b
t
)
P (o
t+1
|a
t
, b
t
)
(2)
=
P (o
t+1
|s
?
, a
t
)
?
s?S
P (s
?
|a
t
, s)P (s|a
t
, b
t
)
P (o
t+1
|a
t
, b
t
)
(3)
= ? ?(o
t+1
, s
?
, a
t
)
?
s?S
T (s, a
t
, s
?
)b
t
(s) (4)
where ? is a normalisation constant. An initial
belief state b
0
must be specified at runtime as a
POMDP parameter when initialising the system.
2.1.3 POMDP policies
Given a POMDP model ?S,A,Z, T, Z,R?, the
agent should execute at each time-step the action
which maximises its expected cumulative reward
over the horizon. The function pi : B ? A defines
a policy, which determines the action to perform
for each point of the belief space.
The expected reward for policy pi starting from
belief b is defined as:
J
pi
(b) = E
[
h
?
t=0
?
t
R(s
t
, a
t
) | b, pi
]
(5)
1
As a notational shorthand, we write P (s
t
=s) as P (s)
and P (s
t+1
=s
?
) as P (s
?
).
The optimal policy pi
?
is then obtained by optimiz-
ing the long-term reward, starting from b
0
:
pi
?
= argmax
pi
J
pi
(b
0
) (6)
The optimal policy pi
?
yields the highest expected
reward value for each possible belief state. This
value is compactly represented by the optimal
value function, noted V
?
, which is a solution to
the Bellman optimality equation (Bellman, 1957).
Numerous algorithms for (offline) policy opti-
misation and (online) planning are available. For
large spaces, exact optimisation is impossible and
approximate methods must be used, see for in-
stance grid-based (Thomson and Young, 2009) or
point-based (Pineau et al, 2006) techniques.
2.2 POMDP-based dialogue management
Dialogue management can be easily cast as a
POMDP problem, with the state space being a
compact representation of the interaction, the ac-
tion space being a set of dialogue moves, the ob-
servation space representing speech recognition
hypotheses, the transition function defining the
dynamics of the interaction (which user reaction
is to be expected after a particular dialogue move),
and the observation function describing a ?sensor
model? between observed speech recognition hy-
potheses and actual utterances. Finally, the reward
function encodes the utility of dialogue policies ?
it typically assigns a big positive reward if a long-
term goal has been reached (e.g. the retrieval of
some important information), and small negative
rewards for minor ?inconveniences? (e.g. prompt-
ing the user to repeat or asking for confirmations).
Our long-term aim is to apply such POMDP
framework to a rich dialogue domain for human-
robot interaction (Kruijff et al, 2010). These inter-
actions are typically open-ended, relatively long,
include high levels of noise, and require complex
state and action spaces. Furthemore, the dialogue
system also needs to be adaptive to its user (at-
tributed beliefs and intentions, attitude, attentional
state) and to the current situation (currently per-
ceived entities and events).
As a consequence, the state space must be ex-
panded to include these knowledge sources. Be-
lief monitoring is then used to continuously update
the belief state based on perceptual inputs (see
also (Bohus and Horvitz, 2009) for an overview of
techniques to extract such information). These re-
quirements can only be fullfilled if we address the
9
?curse of dimensionality? characteristic of tradi-
tional POMDP models. The next section provides
a tentative answer.
3 Approach
3.1 Control knowledge
Classical approaches to POMDP planning oper-
ate directly on the full action space and select the
next action to perform based on the maximisation
of the expected cumulative reward over the spec-
ified horizon. Such approaches can be used in
small-scale domains with a limited action space,
but quickly become intractable for larger ones, as
the planning time increases exponentially with the
size of the action space. Significant planning time
is therefore spend on actions which should be di-
rectly discarded as irrelevant
2
. Dismissing these
actions before planning could therefore provide
important computational gains.
Instead of a direct policy optimisation over the
full action space, our approach formalises action
selection as a two-step process. As a first step, a
set of relevant dialogue moves is constructed from
the full action space. The POMDP planner then
computes the optimal (highest-reward) action on
this reduced action space in a second step.
Such an approach is able to significantly reduce
the dimensionality of the dialogue management
problem by taking advantage of prior knowledge
about the expected relational structure of spoken
dialogue. This prior knowledge is to be encoded
in a set of general rules describing the admissible
dialogue moves in a particular situation.
How can we express such rules? POMDPs are
usually modeled with Bayesian networks which
are inherently propositional. Encoding such rules
in a propositional framework requires a distinct
rule for every possible state and action instance.
This is not a feasible approach. We therefore need
a first order (probabilistic) language able to ex-
press generalities over large regions of the state
action spaces. Markov Logic is such a language.
3.2 Markov Logic Networks (MLNs)
Markov Logic combines first-order logic and
probabilistic graphical models in a unified repre-
sentation (Richardson and Domingos, 2006). A
2
For instance, an agent hearing a user command such as
?Please take the mug on your left? might spent a lot of plan-
ning time calculating the expected future reward of dialogue
moves such as ?Is the box green?? or ?Your name is John?, which
are irrelevant to the situation.
Markov Logic Network L is a set of pairs (F
i
, w
i
),
where F
i
is a formula in first-order logic and w
i
is
a real number representing the formula weight.
A Markov Logic Network L can be seen as
a template for constructing markov networks
3
.
To construct a markov network from L, one has
to provide an additional set of constants C =
{c
1
, c
2
, ..., c
|C|
}. The resulting markov network
is called a ground markov network and is written
M
L,C
. The ground markov network contains one
feature for each possible grounding of a first-order
formula in L, with the corresponding weight. The
technical details of the construction of M
L,C
from
the two sets L and C is explained in several pa-
pers, see e.g. (Richardson and Domingos, 2006).
Once the markov network M
L,C
is constructed,
it can be exploited to perform inference over ar-
bitrary queries. Efficient probabilistic inference
algorithms such as Markov Chain Monte Carlo
(MCMC) or other sampling techniques can then
be used to this end (Poon and Domingos, 2006).
3.3 States and actions as relational structures
The specification of Markov Logic rules apply-
ing over complete regions of the state and action
spaces (instead of over single instances) requires
an explicit relational structure over these spaces.
This is realised by factoring the state and ac-
tion spaces into a set of distinct, conditionally in-
dependent features. A state s can be expanded into
a tuple ?f
1
, f
2
, ...f
n
?, where each sub-state f
i
is
assigned a value from a set {v
1
, v
2
, ...v
m
}. Such
structure can be expressed in first-order logic with
a binary predicate f
i
(s, v
j
) for each sub-state f
i
,
where v
j
is the value of the sub-state f
i
in s. The
same type of structure can be defined over actions.
This factoring leads to a relational structure of ar-
bitrary complexity, compactly represented by a set
of unary and binary predicates.
For instance, (Young et al, 2010) factors each
dialogue state into three independent parts s =
?s
u
, a
u
, s
d
?, where s
u
is the user goal, a
u
the last
user move, and s
d
the dialogue history. These
can be expressed in Markov Logic with predicates
such as UserGoal(s, s
u
), LastUserMove(s, a
u
),
or History(s, s
d
).
3
Markov networks are undirected graphical models.
10
3.4 Relevant action space
For a given state s, the relevant action space
RelMoves(A, s) is defined as:
{a
m
: a
m
? A ? RelevantMove(a
m
, s)} (7)
The truth-value of the predicate
RelevantMove(a
m
, s) is determined using a
set of Markov Logic rules dependent on both the
state s and the action a
m
. For a given state s,
the relevant action space is constructed via prob-
abilistic inference, by estimating the probability
P (RelevantMove(a
m
, s)) for each action a
m
,
and selecting the subset of actions for which the
probability is above a given threshold.
Eq. 8 provides a simple example of such
Markov Logic rule:
LastUserMove(s, a
u
) ? PolarQuestion(a
u
) ?
YesNoAnswer(a
m
)? RelevantMove(a
m
, s) (8)
It defines an admissible dialogue move for a situ-
ation where the user asks a polar question to the
agent (e.g. ?do you see my hand??). The rule speci-
fies that, if a state s contains a
u
as last user move,
and if a
u
is a polar question, then an answer a
m
of type yes-no is a relevant dialogue move for the
agent. This rule is (implicitly) universally quanti-
fied over s, a
u
and a
m
.
Each of these Markov Logic rules has a weight
attached to it, expressing the strength of the im-
plication. A rule with infinite weight and satisfied
premises will lead to a relevant move with prob-
ability 1. Softer weights can be used to describe
moves which are less relevant but still possible in
a particular context. These weights can either be
encoded by hand or learned from data (how to per-
form this efficiently remains an open question).
3.5 Rules application on POMDP belief state
The previous section assumed that the state s is
known. But the real state of a POMDP is never di-
rectly accessible. The rules we just described must
therefore be applied on the belief state. Ultimately,
we want to define a function Rel : <
n
? P(A),
which takes as input a point in the belief space
and outputs a set of relevant moves. For efficiency
reasons, this function can be precomputed offline,
by segmenting the state space into distinct regions
and assigning a set of relevant moves to each re-
gion. The function can then be directly called at
runtime by the planning algorithm.
Due to the high dimensionality of the belief
space, the above function must be approximated
to remain tractable. One way to perform this ap-
proximation is to extract, for belief state b, a set
S
m
of m most likely states, and compute the set
of relevant moves for each of them. We then de-
fine the global probability estimate of a being a
relevant move given b as such:
P (RelevantMove(a) | b, a) ?
?
s?S
m
P (RelevantMove(a, s) | s, a)? b(s) (9)
In the limit wherem? |S|, the error margin on
the approximation tends to zero.
4 Discussion
4.1 General comments
It is worth noting that the mechanism we just
outlined does not intend to replace the existing
POMDP planning and optimisation algorithms,
but rather complements them. Each step serves a
different purpose: the action space reduction pro-
vides an answer to the question ?Is this action rel-
evant??, while the policy optimisation seeks to an-
swer ?Is this action useful??. We believe that such
distinction between relevance and usefulness is
important and will prove to be beneficial in terms
of tractability.
It is also useful to notice that the Markov Logic
rules we described provides a ?positive? definition
of the action space. The rules were applied to pro-
duce an exhaustive list of all admissible actions
given a state, all actions outside this list being de
facto labelled as non-admissible. But the rules can
also provide a ?negative? definition of the action
space. That is, instead of generating an exhaustive
list of possible actions, the dialogue system can
initially consider all actions as admissible, and the
rules can then be used to prune this action space
by removing irrelevant moves.
The choice of action filter depends mainly on
the size of the dialogue domain and the availabil-
ity of prior domain knowledge. A ?positive? filter
is a necessity for large dialogue domains, as the
action space is likely to grow exponentially with
the domain size and become untractable. But the
positive definition of the action space is also sig-
nificantly more expensive for the dialogue devel-
oper. There is therefore a trade-off between the
costs of tractability issues, and the costs of dia-
logue domain modelling.
11
4.2 Related Work
There is a substantial body of existing work in
the POMDP literature about the exploitation of
the problem structure to tackle the curse of di-
mensionality (Poupart, 2005; Young et al, 2010),
but the vast majority of these approaches retain
a propositional structure. A few more theoreti-
cal papers also describe first-order MDPs (Wang
et al, 2007), and recent work on Markov Logic
has extended the MLN formalism to include some
decision-theoretic concepts (Nath and Domingos,
2009). To the author?s knowledge, none of these
ideas have been applied to dialogue management.
5 Conclusions
This paper described a new approach to exploit re-
lational models of dialogue structure for control-
ling the action space in POMDPs. This approach
is part of an ongoing work to develop a unified
framework for adaptive dialogue management in
rich, open-ended interactional settings. The dia-
logue manager is being implemented as part of a
larger cognitive architecture for talking robots.
Besides the implementation, future work will
focus on refining the theoretical foundations of
relational POMDPs for dialogue (including how
to specify the transition, observation and reward
functions in such a relational framework), as well
as investigating the use of reinforcement learning
for policy optimisation based on simulated data.
References
N. Asher and A. Lascarides. 2003. Logics of Conver-
sation. Cambridge University Press.
R. Bellman. 1957. Dynamic Programming. Princeton
University Press.
Dan Bohus and Eric Horvitz. 2009. Dialog in the open
world: platform and applications. In ICMI-MLMI
?09: Proceedings of the 2009 international confer-
ence on Multimodal interfaces, pages 31?38, New
York, NY, USA. ACM.
G.J.M. Kruijff, M. Brenner, and N.A. Hawes. 2008.
Continual planning for cross-modal situated clarifi-
cation in human-robot interaction. In Proceedings of
the 17th International Symposium on Robot and Hu-
man Interactive Communication (RO-MAN 2008),
Munich, Germany.
G.-J. M. Kruijff, P. Lison, T. Benjamin, H. Jacobsson,
H. Zender, and I. Kruijff-Korbayova. 2010. Situated
dialogue processing for human-robot interaction. In
H. I. Christensen, A. Sloman, G.-J. M. Kruijff, and
J. Wyatt, editors, Cognitive Systems. Springer Ver-
lag. (in press).
O. Lemon and O. Pietquin. 2007. Machine learn-
ing for spoken dialogue systems. In Proceedings
of the European Conference on Speech Commu-
nication and Technologies (Interspeech?07), pages
2685?2688, Anvers (Belgium), August.
A. Nath and P. Domingos. 2009. A language for rela-
tional decision theory. In Proceedings of the Inter-
national Workshop on Statistical Relational Learn-
ing.
J. Pineau, G. Gordon, and S. Thrun. 2006. Anytime
point-based approximations for large pomdps. Arti-
ficial Intelligence Research, 27(1):335?380.
H. Poon and P. Domingos. 2006. Sound and effi-
cient inference with probabilistic and deterministic
dependencies. In AAAI?06: Proceedings of the 21st
national conference on Artificial intelligence, pages
458?463. AAAI Press.
P. Poupart. 2005. Exploiting structure to efficiently
solve large scale partially observable markov deci-
sion processes. Ph.D. thesis, University of Toronto,
Toronto, Canada.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning, 62(1-2):107?136.
Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007. Agenda-based
user simulation for bootstrapping a POMDP dia-
logue system. In HLT ?07: Proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Technolo-
gies, pages 149?152, Rochester, New York, April.
Association for Computational Linguistics.
R. Thomason, M. Stone, and D. DeVault. 2006. En-
lightened update: A computational architecture for
presupposition and other pragmatic phenomena. In
Donna Byron, Craige Roberts, and Scott Schwenter,
editors, Presupposition Accommodation. Ohio State
Pragmatics Initiative.
B. Thomson and S. Young. 2009. Bayesian update
of dialogue state: A pomdp framework for spoken
dialogue systems. Computer Speech & Language,
August.
Ch. Wang, S. Joshi, and R. Khardon. 2007. First order
decision diagrams for relational mdps. In IJCAI?07:
Proceedings of the 20th international joint confer-
ence on Artifical intelligence, pages 1095?1100, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
J. Williams and S. Young. 2007. Partially observable
markov decision processes for spoken dialog sys-
tems. Computer Speech and Language, 21(2):231?
422.
S. Young, M. Ga?si?c, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson, and K. Yu. 2010. The hidden
information state model: A practical framework for
pomdp-based spoken dialogue management. Com-
puter Speech & Language, 24(2):150?174.
12
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 294?300,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Multi-Policy Dialogue Management
Pierre Lison
Logic and Natural Language Group
Department of Informatics
University of Oslo, Norway
Abstract
We present a new approach to dialogue man-
agement based on the use of multiple, inter-
connected policies. Instead of capturing the
complexity of the interaction in a single large
policy, the dialogue manager operates with a
collection of small local policies combined
concurrently and hierarchically. The meta-
control of these policies relies on an activation
vector updated before and after each turn.
1 Introduction
Many dialogue domains are naturally open-ended.
This is especially the case in situated dialogue,
where the conversational agent must operate in con-
tinuously changing environments where there is of-
ten no single, pre-specified goal to achieve. De-
pending on the situation and the (perceived) user re-
quests, many distinct tasks may be performed. For
instance, a service robot for the elderly might be
used for cleaning, monitoring health status, and de-
livering information. Each of these tasks features a
specific set of observations, goals, constraints, inter-
nal dynamics, and associated actions.
This diversity of tasks and models poses signif-
icant challenges for dialogue systems, and particu-
larly for dialogue management. Open-ended inter-
actions are indeed usually much more difficult to
model than classical slot-filling applications, where
the application domain can provide strong con-
straints on the possible dialogue transitions. Using
machine learning techniques to learn the model pa-
rameters can help alleviate this issue, but only if the
task can be efficiently factored and if a sufficient
amount of data is available. Once a model of the
interaction and its associated environment is avail-
able, a control policy then needs to be learned or
designed for the resulting state space. The extrac-
tion of good control policies can be computationally
challenging, especially for interactions which si-
multaneously combine partial observability (to deal
with noisy and incomplete observations) and large
state spaces (if the optimal behaviour depends on a
wide range of user- and context-specific factors) ?
which is the case for many open-ended domains.
In this paper, we present ongoing work on a new
approach to dialogue management which seeks to
address these issues by leveraging prior knowledge
about the interaction structure to break up the full
domain into a set of smaller, more predictable sub-
domains. Moving away from the idea of capturing
the full interaction complexity into a unique, mono-
lithic policy, we extend the execution algorithm of
the dialogue manager to directly operate with a col-
lection of small, interconnected local policies.
Viewing dialogue management as a decision pro-
cess over multiple policies has several benefits.
First, it is usually easier for the application devel-
oper to model several small, local interactions than
a single large one. Each local model can also be in-
dependently modified, extended or replaced without
interfering with the rest of the system, which is cru-
cial for system maintenance. Finally, different the-
oretical frameworks can be used for different poli-
cies, which means that the developer is free to decide
which approach is most appropriate to solve a spe-
cific problem, without having to commit to a unique
theoretical framework for the whole application. For
instance, one policy might be expressed as a solu-
tion to a Partially Observable Markov Decision Pro-
cess (POMDP) while another policy is encoded as a
294
hand-crafted finite-state controller, and the two can
be integrated in the same control algorithm.
One of the challenges when operating with mul-
tiple policies is the ?meta-control? of these policies.
At each turn, the system must know which policy
is currently in focus and is responsible for deciding
the next action to perform. Since dialogue manage-
ment operates under significant uncertainty, the sys-
tem can never be sure whether a given policy is ter-
minated or not. We thus need a ?soft? control mech-
anism which is able to explicitly account for the un-
certainty about the completion status of each policy.
This is precisely what we present in this paper.
The rest of the paper is as follows. We first pro-
vide general definitions of dialogue policies, and
present an algorithm for dialogue management oper-
ating on multiple policies. We then present an imple-
mentation of the algorithm together with an empiri-
cal evaluation of its performance, and conclude the
paper by comparing our approach to related work.
2 Background
We start by providing a generic definition of a pol-
icy which can hold independently of any particular
encoding. Dialogue policies can indeed generally
be decomposed in three basic functions, which are
called consecutively upon each turn: (1) observation
update, (2) action selection and (3) action update.
2.1 Observation update
The role of observation update is to modify the pol-
icy?s current state1 upon receiving a new observa-
tion, which can be linguistic or extra-linguistic.
Observation update is formally defined as a func-
tion OBS-UPDATE : S ? O ? S which takes as in-
put the current state s and a new observation o, and
outputs the updated state s?. For instance, a finite-
state controller is expressed by a set of nodesN and
edges E , where the state is expressed by the current
node, and the update mechanism is defined as:
OBS-UPDATE(s, o) =
{
s? if ? an edge s
o
?? s?
s otherwise
In information-state approaches (Larsson and
Traum, 2000), the update is encoded in a collection
1We adopt here a broad definition of the term ?state? to ex-
press any description of the agent?s current knowledge. In a
POMDP, the state thus corresponds to the belief state.
of update rules which can be applied to infer the new
state. In POMDP-based dialogue managers (Young
et al, 2010), the observation update corresponds to
the belief monitoring/filtering function.
2.2 Action selection
The second mechanism is action selection, whose
role is to select the optimal (communicative) action
to perform based on the new estimated state. The
action selection is a function pi : S ? Awhich takes
the updated state as input, and outputs the optimal
action to execute (which might be void).
Different encodings are possible for the action se-
lection mechanism. Finite-state controllers use a
straightforward mechanism for pi, since each state
node in the graph is directly associated with a unique
action. Information-state approaches provide a map-
ping between particular sets of states and actions
by way of selection rules. Decision-theoretic ap-
proaches such as MDPs and POMDPs rely on an
estimated action-value function which is to be max-
imised: pi(s) = argmaxaQ(s, a). The utility func-
tion Q(s, a) can be either learned from experience
or provided by the system designer.
2.3 Action update
Once the next action is selected and sent for execu-
tion, the final step is to re-update the dialogue state
given the action. Contrary to the two previous func-
tions which can be found in all approaches, this third
mechanism is optional and is only implemented in
some approaches to dialogue management.
Action update is formally defined as a function
ACT-UPDATE : S ? A ? S. Finite-state and
information-state approaches typically have no ex-
plicit account of action update. In (PO)MDPs ap-
proaches, the action update function is computed
with the transition function of the model.
3 Approach
3.1 Activation vector
To enable the dialogue manager to operate with mul-
tiple policies, we introduce the notion of activation
value. The activation value of a policy i is the prob-
ability P (?i) that this policy is in focus for the in-
teraction, where the random variable ?i denote the
activation of policy i. In the rest of this paper, we
295
shall use bt(?i) to denote the activation value of pol-
icy ? at time t, given all available information. The
bt(?i) value is dependent on both the completion
status of the policy itself and the activations of the
other policies: bt(?i) = P (?i|si, bt(?1), ...bt(?n)).
We group these values in an activation vector b? =
?b(?1)...b(?n)? which is updated after each turn.
3.2 Activation functions
To compute the activation values, we define the two
following functions associated with each policy:
1. LIKELIHOODi(s, o) : S ? O ? [0, 1] computes
the likelihood of the observation o if the policy
i is active and currently in state s. It is therefore
an estimate of the probability P (o|?i, s).
2. ACTIVATIONi(s) : S ? [0, 1] is used to deter-
mine the probability of policy i being active at
a given state s. In other words, it provides an
estimate for the probability P (?i|s).
These functions are implemented using heuris-
tics which depend on the encoding of the policy.
For a finite-state controller, we realise the function
LIKELIHOOD(s, o) by checking whether the observa-
tion matches one of the outward edges of the current
state node ? the likelihood returns a high probability
if such a match exists, and a low probability oth-
erwise. Similarly, the ACTIVATION function can be
defined using the graph structure of the controller:
ACTIVATION(s) =
{
1 if s non-final
? if s final with outgoing edges
0 if s final w/o outgoing edges
where ? is a constant between 0 and 1.
3.3 Constraints between policies
In addition to these activation functions, various
constraints can hold between the activation of re-
lated policies. Policies can be related with each
other either hierarchically or concurrently.
In a hierarchical mode, a policy A triggers an-
other policy B, which is then executed and returns
the control to policyA once it is finished. As in hier-
archical planning (Erol, 1996; Pineau, 2004), we im-
plement such hierarchy by distinguishing between
primitive actions and abstract actions. An abstract
action is an action which corresponds to the execu-
tion of another policy instead of leading directly to
a primitive action. With such abstract actions, the
system designer can define a hierarchical structure
of policies as illustrated in Figure 1. When a policy
A executes an abstract action pointing to policy B,
the activation value of policy B is increased and the
one of policy A proportionally decreased. This re-
mains so until policy B terminates, at which point
the activation is then transferred back to policy A.
Figure 1: Graphical illustration of a hierarchical policy
structure. Dotted lines denote abstract actions.
In a concurrent mode, policies stand on an equal
footing. When a given policy takes the turn after an
observation, the activations of all other concurrent
policies are decreased to reflect the fact that this part
of the interaction is now in focus. This redistribution
of the activation mass allows us to run several poli-
cies in parallel while at the same time expressing a
?preference? for the policy currently in focus. The
?focus of attention? is indeed crucial in verbal inter-
actions, and in linguistic discourse in general (Grosz
and Sidner, 1986) ? humans do not arbitrarily switch
from one topic to another and back, but rather con-
centrate on the most salient elements.
The set of constraints holding between the activa-
tion values of hierarchical and concurrent policies is
encoded in a simplified Bayesian network.
3.4 Execution algorithm
Algorithm 1 illustrates how the activation values
are exploited to select the optimal action for mul-
tiple policies. The algorithm relies on a set of pro-
cesses P , where a process i is associated with a spe-
cific policy, a current state si for the policy, and a
current activation value b(?i) ? b?. As we have
seen, each policy is fully described with five func-
tions: LIKELIHOOD(s, o), OBS-UPDATE(s, o), pi(s),
ACT-UPDATE(s, a), and ACTIVATION(s). A network
296
of conditional constraints C on the activation vector
is also given as input to the algorithm.
Algorithm 1 operates as follows. Upon receiv-
ing a new observation, the procedure loops over
all processes in P and updates the activation val-
ues b?(?i) for each given the likelihood of the ob-
servation (with ? as a normalisation factor). Once
this update is completed, the process p with the
highest activation is selected, and the function
GET-OPTIMAL-ACTION(p, o) is triggered.
Algorithm 1 : MAIN-EXECUTION (P, o)
Require: P: the current set of processes
Require: C: network of constraints on b?
Require: o: a new observation
1: for all i ? P do
2: P (o|?i, si)? LIKELIHOODi(si, o)
3: b?(?i)? ? ? P (o|?i, si) ? b(?i)
4: end for
5: Select process p? argmaxi b
?(?i)
6: a? ? GET-OPTIMAL-ACTION(p, o)
7: for all i ? P do
8: P (?i|si)? ACTIVATIONi(si)
9: Prune i from P if inactive
10: Compute b(?i) given P (?i|si) and C
11: end for
12: return a?
Within GET-OPTIMAL-ACTION, the state of the pro-
cess is updated given the observation, the next action
a? is selected using pi(s) and the state is updated
again given this selection. If the action is abstract,
the above-mentioned procedure is repeated until a
primitive action is reached. The resulting hierarchi-
cal structure is recorded in children(p) which details,
for each process p ? P , the list of its children pro-
cesses. To ensure consistency among the activation
values in this hierarchy, a constraint is added to C for
each process visited during execution.
Once the action a? is found, the activation values
b(?i) are recomputed according to the local activa-
tion function combined with the constraints C. Pro-
cesses which have become inactive (i.e. which have
transferred control to one parent process) are also
pruned from P . Finally, the action a? is returned.
Algorithm 2 : GET-OPTIMAL-ACTION (p, o)
Require: p: process with current state sp
Require: o: a new observation
Require: children(p): list of current processes di-
rectly or indirectly forked from p
1: sp ? OBS-UPDATEp(sp, o)
2: a? ? pip(sp)
3: sp ? ACT-UPDATEp(sp, a?)
4: if a? is an abstract action then
5: Fork new process q with policy from a?
6: Add q to set of current processes P
7: a? ? GET-OPTIMAL-ACTION(q, o)
8: children(p)? ?q?+ children(q)
9: else
10: children(p)? ??
11: end if
12: Add to C the constraint b(?p) =
(1?
?
i?children(p) b(?i)) ? P (?p|sp)
13: return a?
4 Evaluation
The described algorithm has been implemented and
tested with different types of policies. We present
here a preliminary experiment performed with a
small dialogue domain. The domain consists of a
(simulated) visual learning task between a human
and a robot in a shared scene including a small num-
ber of objects, described by various properties such
as color or shape. The human asks questions re-
lated to these object properties, and subsequently
confirms or corrects the robot?s answers ? as the case
may be. We account for the uncertainty both in the
linguistic inputs and in the visual perception.
We model this domain with two connected poli-
cies, one top policy handling the general interac-
tion (including engagement and closing acts), and
one bottom policy dedicated to answering each user
question. The top policy is encoded as a finite-state
controller and the bottom policy as a POMDP solved
using the SARSOP algorithm, available in the APPL
toolkit2 (Kurniawati et al, 2008). A sample run is
provided in Appendix A.
The experiment was designed to empirically com-
pare the performance of the presented algorithm
2http://bigbird.comp.nus.edu.sg/pmwiki/farm/appl/
297
with a simpler hierarchical control algorithm which
does not use any activation vector, but where the
top policy is blocked until the sub-policy releases
its turn. The policies themselves remain identical
in both scenarios. We implemented a handcrafted
user simulator for the domain, and tested the poli-
cies with various levels of artificial noise.
The average return for the two scenarios are pro-
vided in Figure 2. The results show that activation
values are beneficial for multi-policy dialogue man-
agement, especially in the presence of noise.. This is
due to the soft control behaviour provided by the ac-
tivation vector, which is more robust than hierarchi-
cal control. Activation values provide a more fine-
grained mechanism for expressing the completion
status of a policy, and therefore avoid fully ?block-
ing? the control at a given level.
0
3
6
9
12
15
0 5 10 15 20 25 30 35 40 45 50
A
v
e
r
a
g
e
 
r
e
t
u
r
n
 
p
e
r
 
d
i
a
l
o
g
u
e
Level of random noise (in %)
Policies with activation function
Policies with strict hierarchical control
Figure 2: Average return (as generated by the hand-
crafted user simulator) for the two connected policies,
using either the present algorithm or strict hierarchical
control. 400 runs are used for each level of noise.
5 Related work
The exploitation of prior structural knowledge in
control has a long history in the planning commu-
nity (Erol, 1996; Hauskrecht et al, 1998), and has
also been put forward in some approaches to di-
alogue modelling and dialogue management ? see
e.g. (Grosz and Sidner, 1990; Allen et al, 2000;
Steedman and Petrick, 2007; Bohus and Rudnicky,
2009). These approaches typically rely on a task de-
composition in goals and sub-goals, and assume that
the completion of each of these goals can be fully
observed. The novel aspect of our approach is pre-
cisely that we seek to relax this assumption of per-
fect knowledge of task completion. Instead, we treat
the activation/termination status of a given policy as
a hidden variable which is only indirectly observed
and whose value at each turn is determined via prob-
abilistic reasoning operations.
The idea of combining different dialogue man-
agement frameworks in a single execution process
has also been explored in previous work such as
(Williams, 2008), but only as a filtering mecha-
nism ? one policy constraining the results of an-
other. Related to the idea of concurrent policies,
(Turunen et al, 2005) describes a software frame-
work for distributed dialogue management, mostly
focussing on architectural aspects. In the same vein,
(Lemon et al, 2002; Nakano et al, 2008) describe
techniques for dialogue management respectively
based on multi-threading and multi-expert models.
(Cuaya?huitl et al, 2010) describe an reinforcement
learning approach for the optimisation of hierarchi-
cal MDP policies, but is not extended to other types
of policies. Closest to our approach is the PolCA+
algorithm for hierarchical POMDPs presented in
(Pineau, 2004), but unlike our approach, her method
does not support temporally extended actions, as the
top-down trace is repeated after each time step.
6 Conclusion
We introduced a new approach to dialogue manage-
ment based on multiple, interconnected policies con-
trolled by activation values. The values are updated
at the beginning and the end of each turn to reflect
the part of the interaction currently in focus.
It is worth noting that the only modification re-
quired in the policy specifications to let them run
in a multi-policy setting is the introduction of the
two functions LIKELIHOOD(s, o) and ACTIVATION(s).
The rest remains untouched and can be defined in-
dependently. The presented algorithm is therefore
well suited for the integration of dialogue policies
encoded in different theoretical frameworks.
Future work will focus on various extensions of
the approach and the use of more extensive evalua-
tion metrics. We are also investigating how to ap-
ply reinforcement learning techniques to learn the
model parameters in such multi-policy paradigms.
298
Acknowledgements
This work was supported by the EU FP7 IP project
??ALIZ-E: Adaptive Strategies for Sustainable Long-
Term Social Interaction? (FP7-ICT-248116) and by
a PhD research grant from the University of Oslo.
The author would like to thank Stephan Oepen, Erik
Velldal and Alex Rudnicky for their comments and
suggestions on earlier drafts of this paper.
References
J. Allen, D. Byron, M. Dzikovska, G. Ferguson,
L: Galescu, and A. Stent. 2000. An architecture for
a generic dialogue shell. Natural Language Engineer-
ing, 6:213?228, September.
D. Bohus and A. I. Rudnicky. 2009. The RavenClaw
dialog management framework: Architecture and sys-
tems. Computer Speech & Language, 23:332?361,
July.
H. Cuaya?huitl, S. Renals, O. Lemon, and H. Shimodaira.
2010. Evaluation of a hierarchical reinforcement
learning spoken dialogue system. Computer Speech
& Language, 24:395?429, April.
K. Erol. 1996. Hierarchical task network planning: for-
malization, analysis, and implementation. Ph.D. the-
sis, College Park, MD, USA.
B. J. Grosz and C. L. Sidner. 1986. Attention, inten-
tions, and the structure of discourse. Computational
Linguistics, 12:175?204, July.
B. J. Grosz and C. L. Sidner. 1990. Plans for discourse.
In P. R. Cohen, J. Morgan, and M. E. Pollack, ed-
itors, Intentions in Communication, pages 417?444.
MIT Press, Cambridge, MA.
M. Hauskrecht, N. Meuleau, L. P. Kaelbling, T. Dean, and
C. Boutilier. 1998. Hierarchical solution of markov
decision processes using macro-actions. In Proceed-
ings of Uncertainty in Artificial Intelligence (UAI),
pages 220?229.
H. Kurniawati, D. Hsu, and W.S. Lee. 2008. SARSOP:
Efficient point-based POMDP planning by approxi-
mating optimally reachable belief spaces. In Proc.
Robotics: Science and Systems.
S. Larsson and D. R. Traum. 2000. Information state and
dialogue management in the trindi dialogue move en-
gine toolkit. Natural Language Engineering, 6:323?
340, September.
O. Lemon, A. Gruenstein, A. Battle, and S. Peters. 2002.
Multi-tasking and collaborative activities in dialogue
systems. In Proceedings of the 3rd SIGDIAL work-
shop on Discourse and Dialogue, pages 113?124,
Stroudsburg, PA, USA.
M. Nakano, K. Funakoshi, Y. Hasegawa, and H. Tsujino.
2008. A framework for building conversational agents
based on a multi-expert model. In Proceedings of the
9th SIGDIAL Workshop on Discourse and Dialogue,
pages 88?91, Stroudsburg, PA, USA.
J. Pineau. 2004. Tractable Planning Under Uncertainty:
Exploiting Structure. Ph.D. thesis, Robotics Institute,
Carnegie Mellon University, Pittsburgh, PA.
M. Steedman and R. P. A. Petrick. 2007. Planning
dialog actions. In Proceedings of the 8th SIGDIAL
Workshop on Discourse and Dialogue (SIGdial 2007),
pages 265?272, Antwerp, Belgium, September.
M. Turunen, J. Hakulinen, K.-J. Ra?iha?, E.-P. Salonen,
A. Kainulainen, and P. Prusi. 2005. An architecture
and applications for speech-based accessibility sys-
tems. IBM Syst. J., 44:485?504, August.
J. D. Williams. 2008. The best of both worlds: Unify-
ing conventional dialog systems and POMDPs. In In-
ternational Conference on Speech and Language Pro-
cessing (ICSLP 2008), Brisbane, Australia.
S. Young, M. Gas?ic?, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson, and K. Yu. 2010. The hidden
information state model: A practical framework for
pomdp-based spoken dialogue management. Com-
puter Speech & Language, 24:150?174, April.
299
A Example of execution with two policies
We provide here an example of execution of Algo-
rithm 1 with the two policies described in the evalua-
tion section. Figure 3 illustrates the policy hierarchy,
which consists of two policies connected with an ab-
stract action. The finite-state graph of the top policy
is shown in Figure 4.
?hi!?
Bottom policy (POMDP)
Top policy (finite-state)
?how may I 
help you??
?goodbye!?
?the object is X?
?I don?t know the 
colour/shape?
?do you mean the 
1st/2nd object??
?could you 
repeat??
?which 
object??
(X=green,red, 
blue,round,
squared,etc.)
...
...
Figure 3: Hierarchical structure of the two policies.
(wait 1 s.)
Robot: ?hi!?
User: (received new intention)
Robot: ?how 
may I help you??
User: ?hi robot!?
(timeout)
Robot: (trigger 
POMDP)
Robot: ?goodbye!?
User: ?goodbye robot!?
User: (received 
new intention)
Figure 4: Finite-state controller for the top policy.
A.1 Example of interaction
Transcript A.1 provides a full example of interaction
as generated by the user simulator. Human utter-
ances can contain variable noise levels, which makes
it necessary to use clarification requests when the
user intention is unclear (lines 5 and 10).
Contextual knowledge can also influence how the
dialogue unfolds ? here, the robot knows that the
first object is blue but is uncertain about the second
object. Such contextual knowledge is expressed in
terms of hidden variables in the POMDP model.
The execution of Algorithm 1 for the above ex-
ample proceeds as follows. From line 1 to line 4, the
set P contains one single process, based on the top
Transcript A.1 Example of interaction
1: HUMAN: Hi robot!
2: ROBOT: Hi! [wait 1 s.]
3: ROBOT: How may I help you?
4: HUMAN: Could you tell me the colour of
the first object? [trigger POMDP]
5: ROBOT: Sorry, could you repeat?
6: HUMAN: What colour is the first object?
7: ROBOT: It is blue
8: HUMAN: That?s right
9: HUMAN: Now, could you tell me the colour
of the second object? [trigger POMDP]
10: ROBOT: Do you mean the second object?
11: HUMAN: Yes
12: ROBOT: I don?t know
13: HUMAN: It is green
14: ROBOT: Thanks
15: HUMAN: Ok I?m done. Goodbye!
16: ROBOT: Goodbye! [close interaction]
policy. This process is fully activated. At line 5, the
new user intention is mapped to the abstract action,
and thus to the creation of a second process based
the POMDP policy. This second process becomes
highly activated due its function ACTIVATION(s).
Given the constraint holding between the activation
values of two processs, the high activation of the bot-
tom process leads to a proportional reduction of the
activation value of the top process.
The utterance at line 8 is interpreted as a signal
that the user intention has been met, and the acti-
vation value of the bottom process is subsequently
decreased. The top process is then partially reac-
tivated, and interprets the utterance at line 9 as a
new user intention, leading to the creation of a new
POMDP process. This process handles the interac-
tion until the line 14, where the control is transferred
back to the finite-state controller.
The better performance of Algorithm 1 compared
to strict hierarchical control is due to the lines 8? 9.
In the presence of noise, the transition to the second
question might not be detected (if the confidence
scores of the utterance is below a fixed threshold). In
such case, the dialogue manager might stay ?stuck?
in the first POMDP process instead of interpreting
the utterance as a new question.
300
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 179?188,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Probabilistic Dialogue Models with Prior Domain Knowledge
Pierre Lison
Department of Informatics
University of Oslo, Norway
plison@ifi.uio.no
Abstract
Probabilistic models such as Bayesian Net-
works are now in widespread use in spoken
dialogue systems, but their scalability to com-
plex interaction domains remains a challenge.
One central limitation is that the state space
of such models grows exponentially with the
problem size, which makes parameter esti-
mation increasingly difficult, especially for
domains where only limited training data is
available. In this paper, we show how to cap-
ture the underlying structure of a dialogue do-
main in terms of probabilistic rules operating
on the dialogue state. The probabilistic rules
are associated with a small, compact set of pa-
rameters that can be directly estimated from
data. We argue that the introduction of this ab-
straction mechanism yields probabilistic mod-
els that are easier to learn and generalise bet-
ter than their unstructured counterparts. We
empirically demonstrate the benefits of such
an approach learning a dialogue policy for a
human-robot interaction domain based on a
Wizard-of-Oz data set.
1 Introduction
Spoken dialogue systems increasingly rely on prob-
abilistic models at various stages of their pipeline.
Statistical methods have notably been applied to
tasks such as disfluency detection (Lease et al,
2006), semantic parsing (Erdogan et al, 2002; He
and Young, 2005), dialogue act recognition (Stol-
cke et al, 2000; Lan et al, 2008), dialogue man-
agement (Frampton and Lemon, 2009; Young et al,
2010), natural language generation (Oh and Rud-
nicky, 2002; Lemon, 2011) and speech synthesis
(Zen et al, 2009).
There are two compelling reasons for this grow-
ing interest in statistical approaches: first, spoken
dialogue is pervaded with noise and uncertainty
(due to e.g. speech recognition errors, linguistic
and pragmatic ambiguities, and unknown user in-
tentions), which must be dealt with at all processing
stages. Second, a decisive advantage of probabilis-
tic models lies in their ability to be automatically
optimised from data, enabling statistically-based di-
alogue systems to exhibit conversational behaviours
that are often more robust, flexible and adaptive than
hand-crafted systems (Lemon and Pietquin, 2007).
Despite their success, the use of probabilistic
models also presents a number of challenges. The
most pressing issue is the paucity of appropriate data
sets. Stochastic models often require large amounts
of training data to estimate their parameters ? ei-
ther directly (Henderson et al, 2008) or indirectly
by way of a user simulator (Schatzmann et al, 2007;
Cuaya?huitl et al, 2010). Unfortunately, real interac-
tion data is scarce, expensive to acquire, and difficult
to transfer from one domain to another. Moreover,
many dialogue domains are inherently open-ended,
which means they are not limited to the completion
of a single task with predefined features but have to
represent a varying number of tasks, complex user
models and a rich, dynamic environment. Exam-
ples of such domains include human-robot interac-
tion (Kruijff et al, 2010), cognitive assistants and
companions (Nguyen, 2005; Cavazza et al, 2010),
and tutoring systems (Litman and Silliman, 2004;
Eskenazi, 2009). In such settings, the dialogue sys-
tem might need to track a large number of variables
in the course of the interaction, which quickly leads
to a combinatorial explosion of the state space.
There is an extensive body of work in the machine
179
learning and planning literature that shows how to
address this issue by relying on more expressive rep-
resentations, able to capture relevant aspects of the
problem structure in a compact manner. By taking
advantage of hierarchical or relational abstractions,
system developers can leverage their domain knowl-
edge to yield probabilistic models that are easier to
learn (due to a reduced number of parameters) and
more efficient to use (since the structure can be ex-
ploited by the inference algorithm).
The contributions of this paper are twofold. We
first present a new framework for encoding prior
knowledge in probabilistic dialogue models, based
on the concept of probabilistic rules. The frame-
work is very general and can accommodate a wide
spectrum of domains and learning tasks, from fully
statistical models with virtually no prior knowledge
to manually designed models with only a hand-
ful of parameters. Second, we demonstrate how
this framework can be exploited to learn stochas-
tic dialogue policies with limited data sets using a
Bayesian learning approach.
The following pages spell out the approach in
more detail. In Section 2, we provide the general
background on probabilistic models and their use in
spoken dialogue systems. We describe in Section 3
how to encode such models via probabilistic rules
and estimate their parameters from data. In Sec-
tion 4, we detail the empirical evaluation of our ap-
proach in a human-robot interaction domain, given
small amounts of data collected in Wizard-of-Oz ex-
periments. Finally, we discuss and compare our ap-
proach to related work in Section 5.
2 Background
2.1 Bayesian Networks
The probabilistic models used in this paper are ex-
pressed as directed graphical models, also known as
Bayesian Networks. Let X1...Xn denote a set of
random variables. Each variable Xi is associated
with a range of mutually exclusive values. In dia-
logue models, this range is often discrete and can be
explicitly enumerated: V al(Xi) = {x1i , ..., xmi }.
A Bayesian Network defines the joint probabil-
ity distribution P (X1...Xn) via conditional depen-
dencies between variables, using a directed graph
where each node corresponds to a variable Xi. Each
A
C
B
D
E
Value for B: P(B)
T 0.6
F 0.4
Value for A: P(A)
T 0.3
F 0.7
Value for C P(C)
T
 1.0 if (A=T ? B=T)
 0.0 otherwise
F
 0.0 if (A=T ? B=T)
 1.0 otherwise
Value 
for D:
P(D|C)
C=T C=F
T 0.2 0.99
F 0.8 0.01
Value 
for E:
P(E|C)
C=T C=F
T 0.5 0.4
F 0.5 0.6
Figure 1: Example of Bayesian network with 5 nodes.
The double circles denote a deterministic node. As
an example, the query P (A|D=T) gives the result
P (A=T|D=T) ? 0.18 and P (A=F|D=T) ? 0.82.
edge Xi ? Xj denotes a conditional dependence
between the two nodes, in which case Xi is said to
be a parent of Xj . A conditional probability distri-
bution P (Xi|Par(Xi)) is associated with each node
Xi, where Par(Xi) denotes the parents of Xi.
Conditional probability distributions (CPDs) can
be defined in various ways, from look-up tables
to deterministic distributions (Koller and Friedman,
2009). Together with the directed graph, the CPDs
fully determine the joint probability distribution of
the Bayesian Network. The network can be used for
inference by querying the distribution of a subset of
variables, often given some additional evidence, as
illustrated by the example in Figure 1.
2.2 Dialogue Models
A dialogue state s is usually decomposed into a set
of state variables s = {s1, ...sn} representing rel-
evant features of the interaction. For instance, the
state variables for a human-robot interaction sce-
nario might be composed of tasks to accomplish, the
interaction history, past events, as well as objects,
spatial locations and agents in the environment.
Given the uncertainty present in spoken dialogue,
many variables are only partially observable. We
thus encode our knowledge of the current state in
a distribution b(s) = P (s1, ..., sn) called the be-
lief state, which can be conveniently expressed as
a Bayesian Network (Thomson and Young, 2010).
This belief state b is regularly updated as new infor-
180
Speech 
recognition
Speech 
understanding
Generation
Speech 
synthesis
Extra-linguistic environment
User
input speech signal
(user utterance)
Recognition
hypotheses u
u
  
Utterance to 
synthesise u
m
output speech signal
(machine utterance)
Interpreted
utterance ?
u
Intended
response a
m
~
Belief state b
Belief 
update
Action 
selection
Dialogue management
Figure 2: Dialogue system architecture schema.
mation becomes available. As illustrated in Figure
2, the whole system pipeline can be formalised in
terms of inference steps over this belief state:
1. Upon detection of a new utterance, the speech
recogniser generates the N-best list of recogni-
tion hypotheses u?u = P (uu|o);
2. Speech understanding then searches for the
most likely dialogue act(s) realised in the ut-
terance: a?u = P (au|u?u,b);
3. The belief state is updated with the new inter-
preted dialogue act: b? = P (s?|a?u,b);
4. Based on the updated belief state, the action se-
lection searches for the optimal system action
to perform: a?m = arg maxam Q(am|b);
5. The system action is then realised in an utter-
ance um, which is again framed as a search for
u?m = arg maxum Q(um|b, am);
6. Finally, the dialogue state is re-updated given
the system action: b? = P (s?|am,b).
The models defined above use P (x|b) as a nota-
tional convenience for?si?V al(s) P (x|s=si)b(si).
The same holds for the estimated values u?u and a?u:
P (x|y?) =?yi?V al(y?) P (x|y=yi)P (y=yi).
3 Approach
The starting point of our approach is the observation
that dialogue often exhibits a fair amount of internal
structure. This structure can take several forms.
We can first note that the probability or utility
of a given output variable often depends on only a
small subset of input variables, although the num-
ber and identity of these variables might naturally
differ from action to action. The state variable en-
coding the physical location of a mobile robot is for
instance relevant for answering a user requesting its
location, but not for responding to a greeting act.
Moreover, the values of the dependent variables
can often be grouped into partitions yielding
similar outcomes, thereby reducing the problem
dimensionality. The partitions can generally be
expressed via logical conditions on the variable
values. As illustration, consider a dialogue where
the user can ask yes/no questions pertaining to the
colour of specific objects (e.g. ?Is the ball red??).
The utility of the system action Confirm depends
on two variables: the user dialogue act, for instance
au= VerifyColour(ball, red), and the object colour,
such as ball.colour = blue. The combination of
these two variables can take a wide range of values,
but the utility of Confirm only depends on two par-
titions: (VerifyColour(x, y) ? x.colour=y),
in which case the utility is positive, and
(VerifyColour(x, y) ? x.colour 6=y), in which
case it is negative.
We outline below a generic description frame-
work for expressing this internal structure, based on
the concept of probabilistic rules. The rules ex-
press the distribution of a dialogue model in terms of
structured mappings between input and output vari-
ables. At runtime, the rules are then combined to
perform inference on the dialogue state, i.e. to com-
pute the distribution of the output variables given the
input variables. As we shall see, this is done by in-
stantiating the rules and their associated variables
to construct an equivalent Bayesian Network used
for inference. The probabilistic rules thus function
as high-level templates for a classical probabilistic
model. The major benefit of this approach is that the
rule structure is described in exponentially fewer pa-
rameters than its plain counterpart, and is thus much
easier to learn and to generalise to unseen data.
3.1 Definitions
A probabilistic rule is defined as a condition-effect
mapping, where each condition is mapped to a set
of alternative effects, each being assigned a distinct
181
probability. The list of conditions is ordered and
takes the form of a ?if ... then ... else? case express-
ing the distribution of the output variables depending
on the inputs.
Formally, a rule r is defined as an ordered list
of cases ?c1, ...cn?, where each case ci is associated
with a condition ?i and a distribution over stochas-
tic effects {(?1i , p1i ), ..., (?ki , pki )}, where ?ji is a
stochastic effect and probability pji = P (?ji |?i),
where p1...ki satisfy the usual probability axioms.
The rule reads as such:
if (?1) then
{P (?11) = p11, ... P (?k1 ) = pk1}
...
else if (?n) then
{P (?1n) = p1n, ... P (?mn ) = pmn }
A final else case is implicitly added to the bottom of
the list, and holds if no other condition applies. If
not overridden, the default effect associated to this
last case is void ? i.e. it causes no changes to the
distribution over the output variables.
Conditions
The rule conditions are expressed as logical for-
mulae grounded in the input variables. They can be
arbitrarily complex formulae connected by conjunc-
tion, disjunction and negation. The conditions on
the input variables can be seen as providing a com-
pact partitioning of the state space to mitigate the
dimensionality curse. Without this partitioning in
alternative conditions, a rule ranging over m vari-
ables each of size n would need to enumerate nm
possible assignments. The partitioning with condi-
tions reduces this number to p mutually exclusive
partitions, where p is usually small.
Effects
The rule effects are defined similarly: given a con-
dition holding on a set of input variables, the asso-
ciated effects define specific value assignments for
the output variables. The effects can be limited to
a single variable or range over several output vari-
ables. For action selection, effects can also take the
form of assignments of utility values for a particular
action, i.e. Q(am = x) = y, where y is the scalar
value for the utility of action x.
Each effect is assigned a probability, and several
alternative stochastic effects can be defined for the
same case. If a unique effect is specified, it is then
implicitly assumed to hold with probability 1.0. The
probabilities of stochastic effects and the action util-
ities are treated as parameters, which can be either
hand-coded or estimated from data.
Example
The rules r1 and r2 below express the utilities of
two actions: the physical action ExecuteMov(X)
(with X representing the movement type), and the
clarification request AskRepeat.
r1 : if (au= RequestMov(X)) then
{Q(am= ExecuteMov(X)) = ?(1)r1 }
r2 : if (au 6= ? ? am 6= AskRepeat) then
{Q(am= AskRepeat) = ?(1)r2 }
else if (au 6= ?) then
{Q(am= AskRepeat) = ?(2)r2 }
Rule r1 specifies that, if the last user action au is
equal to RequestMov(X) (i.e. requesting the robot
to execute a particular movement X), the utility as-
sociated with ExecuteMov(X) is equal to the pa-
rameter ?1r1 . Similarly, the rule r2 specifies the util-ity of the clarification request AskRepeat, provided
that the last user action au is assigned to a value (i.e.
is different than ?). Two cases are distinguished in
r2, depending on whether the previous system ac-
tion was already AskRepeat. This partitioning en-
ables us to assign a distinct utility to the clarification
request if one follows the other, in order to e.g. pe-
nalise for the repeated clarification.
As illustration, assume that ?(1)r1 = 2.0, ?(1)r2 =
1.3, ?(2)r2 = 1.1, and that the belief state contains astate variable au with the following distribution:
P (au = RequestMov(LiftBothArms)) = 0.7
P (au = RequestMov(LiftLeftArm)) = 0.2
P (au = ?) = 0.1
The optimal system action in this case is there-
fore ExecuteMov(LiftBothArms) with utility 1.4,
followed by AskRepeat with utility 1.17, and
ExecuteMov(LiftLeftArm) with utility 0.4.
182
3.2 Inference
Given a belief state b, we perform inference by con-
structing a Bayesian Network corresponding to the
application of the rules. Algorithm 1 describes the
construction procedure, which operates as follows:
1. We initialise the Bayesian Network with the
variables in the belief state;
2. For every rule r in the rule set, we create a con-
dition node ?r and include the conditional de-
pendencies with its input variables;
3. We create an effect node ?r conditioned on ?r,
expressing the possible effects of the rule;
4. Finally, we create the (chance or value) nodes
corresponding to the output variables of the
rule, as specified in the effects.
Rule r2 described in the previous section would
for instance be translated into a condition node ?r2
with 3 values (corresponding to the specified con-
ditions and a default else condition if none applies)
and an effect node ?r2 also containing 3 values (the
two specified effects and a void effect associated
with the default condition). Figure 3 illustrates the
application of rules r1 and r2.
Once the Bayesian network is constructed,
queries can be evaluated using any standard algo-
rithm for exact or approximate inference. The proce-
dure is an instance of ground inference (Getoor and
Taskar, 2007), since the rule structure is grounded in
a standard Bayesian Network.
3.3 Parameter Learning
The estimation of the rule parameters can be per-
formed using a Bayesian approach by adding param-
eter nodes ? = ?1...?k to the Bayesian Network,
a
u
a
m
?
r1
?
r1
?
r2
?
r2
?
r2
?
r1
rule r
1
rule r
2
Q(a
m
)
a
m
Figure 3: Bayesian Network with the rules r1 and r2.
and updating their distribution given a collection of
training data. Each data sample d is a pair (bd, td),
where bd is the belief state for the specific sample,
and td the target value. The target value depends on
the model to learn ? for learning dialogue policies,
it corresponds to the selected action am.
Algorithm 1 : NETWORKCONSTRUCTION (b,R)
Require: b: Current belief state
Require: R: Set of probabilistic rules
1: B ? b
2: for all rule r ? R do
3: Ir ? INPUTNODES(r)
4: ?r ? CONDITIONNODE(r)
5: Add ?r and dependencies Ir ? ?r to B
6: ?r ? EFFECTNODE(r)
7: Add ?r and dependency ?r ? ?r to B
8: Or ? OUTPUTNODES(r)
9: for all output variable o ? Or do
10: Add/modify node o and dep. ?r ? o to B
11: end for
12: end for
13: return B
Algorithm 2 : PARAMETERLEARNING (R,?,D)
Require: R: Set of probabilistic rules
Require: ?: Parameters with prior distribution
Require: D: Training sample
1: for all data d ? D do
2: B ? NETWORKCONSTRUCTION(bd,R)
3: Add parameters nodes ? to B
4: for all ?i ? ? do
5: P (??i|d) = ? P (td|bd, ?i) P (?i)
6: end for
7: end for
8: return ?
To estimate the parameters ?, we start from an
initial prior distribution. Then, for each sample d
in the training data, we construct the correspond-
ing Bayesian Network from its belief state bd and
the rules, including nodes corresponding to the un-
known rule parameters. Then, for each parameter ?i,
we compute its posterior distribution given the data
(Koller and Friedman, 2009):
P (??i|d) = ? P (td|bd, ?i) P (?i) (1)
183
Given the number of parameters in our example do-
main and their continuous range, we used approxi-
mate inference to calculate the posterior efficiently,
via direct sampling from a set of parameter values.
The constant ? serves as a normalisation factor over
the sampled parameter values for ?i. The procedure
is repeated for every sample, as shown in Algorithm
2. The parameter distribution will thus progressively
narrow down its spread to the values providing the
best fit for the training data.
4 Evaluation
We evaluated our approach in the context of a dia-
logue policy learning task for a human-robot inter-
action scenario. The main question we decided to
address is the following: how much does the rule
structure contribute to the parameter estimation of
a given probabilistic model, especially for domains
with limited amounts of available data? The objec-
tive of the experiment was to learn the rule param-
eters corresponding to the policy model Q(am|s)
from a Wizard-of-Oz data collection. In this partic-
ular case, the parameters correspond to the utilities
of the various actions. The policy model used in the
experiment included a total of 14 rules.
We compared our approach with two baselines
which are essentially ?flattened? or rolled-out ver-
sions of the rule-based model. The input and output
variables remain identical, but they are directly con-
nected, without the ? and ? nodes serving as inter-
mediate structures. The two baselines are (1) a plain
multinomial model and (2) a linear model of the in-
put variables. We are thus comparing three versions
of the Q(am|s) model: two baselines where am is
directly dependent on the state variables, and our ap-
proach where the dependency is realised indirectly
through condition and effect nodes.
4.1 Experimental Setup
The scenario for the Wizard-of-Oz experiment in-
volved a human user and a Nao robot1 (see Figure
4). The user was instructed to teach the robot a se-
quence of basic movements (lift the left arm, step
forward, kneel down, etc.) using spoken commands.
The interaction included various dialogue acts such
1A programmable humanoid robot developed by Aldebaran
Robotics, http://www.aldebaran-robotics.com.
Figure 4: Human user interacting with the Nao robot.
as clarification requests, feedbacks, acknowledge-
ments, corrections, etc. Short examples of recorded
dialogues are provided in the appendix.
In addition to the policy model, the dialogue sys-
tem include a speech recognizer (Vocon 3200 from
Nuance) connected to the robot microphones, shal-
low components for dialogue act recognition and
generation, a text-to-speech module, and compo-
nents for planning the robot movements and control-
ling its motors in real-time. All components are con-
nected to the shared belief state, and read/write to it
as they process their data flow.
We collected a total of 20 interactions with 7
users and one wizard playing the role of the pol-
icy model, for a total of 1020 system turns, sum-
ming to around 1h of interaction. All the inter-
actions were performed in English. The wizard
only had access to the N-best list output from the
speech recogniser, and could then select which ac-
tion to perform from a list of 14 alternatives (such
as AskRepeat, DemonstrateMove, UndoMove,
AskForConfirmation, etc). Each selected action
was recorded along with the belief state (including
the full probability distribution for every state vari-
able) in effect at the time of the selection.
4.2 Analysis
The data set was split into training (75% of the sys-
tem turns) and test data (remaining 25%) used to
measure the accuracy of our policies. The accuracy
is defined as the percentage of actions corresponding
to the gold standard action selected by the wizard.
The parameter distributions are initialised with uni-
form priors, and are progressively refined as more
data points are processed. We calculated the accu-
racy by sampling over the parameters, performing
inference over the resulting models, and finally av-
eraging over the inference results.
184
025
50
75
100
0 152 304 456 608 760
A
c
c
u
r
a
c
y
 
o
n
 
t
e
s
t
i
n
g
 
s
e
t
 
(
i
n
 
%
)
Number of training samples
Rule-structured model
Linear model
Plain model
(a) Linear scale
0
25
50
75
100
0 2 11 47 190 760
A
c
c
u
r
a
c
y
 
o
n
 
t
e
s
t
i
n
g
 
s
e
t
 
(
i
n
 
%
)
Number of training samples
Rule-structured model
Linear model
Plain model
(b) Log-2 scale
Figure 5: Learning curves for the overall accuracy of the learned dialogue policy, on a held-out test set of 255 actions,
depending on the size of the training sample. The accuracy results are given for the plain, linear and rule-structured
policy models, using linear (left) and logarithmic scales (right).
Table 1 provides the accuracy results. The dif-
ferences between our model and the baselines are
statistically significant using Bonferroni-corrected
paired t-tests, with p-value < 0.0001. The 17% of
actions labelled as incorrect are mainly due to the
high degree of noise in the data set, and the some-
times inconsistent or unpredictable behaviour of the
wizard (regarding e.g. clarification requests).
It is instructive to analyse the learning curve of
the three models, shown in Figure 5. Given its
smaller number of parameters, the rule-structured
model is able to converge to near-optimal values af-
ter observing only a small fraction of the training
set. As the figure shows, the baseline models do also
improve their accuracies over time, but at a much
slower rate. The linear model is comparatively faster
than the plain model, but levels off towards the end,
possibly due to the non-linearity of some dialogue
strategies. The plain model continues its conver-
gence and would probably reach an accuracy simi-
lar to the rule-structured model if given much larger
amounts of training data. Note that since the pa-
rameters are initially uniformly distributed, the ac-
curacy is already non-zero before learning, since a
random assignment of parameters has a low but non-
zero chance of leading to the right action.
5 Discussion and Related Work
The idea of using structural knowledge in proba-
bilistic models has been explored in many direc-
Type of model Accuracy (in %)
Plain model 67.35
Linear model 61.85
Rule-structured model 82.82
Table 1: Accuracy results for the three action selection
models on a test set, using the full training set.
tions, both in the fields of decision-theoretic plan-
ning and of reinforcement learning (Hauskrecht et
al., 1998; Pineau, 2004; Lang and Toussaint, 2010;
Otterlo, 2012) and in statistical relational learning
(Jaeger, 2001; Richardson and Domingos, 2006;
Getoor and Taskar, 2007). The introduced struc-
ture may be hierarchical, relational, or both. As in
our approach, most of these frameworks rely on the
use of expressive representations as templates for
grounded probabilistic models.
In the dialogue management literature, most
structural approaches rely on a clear-cut task decom-
position into goals and sub-goals (Allen et al, 2000;
Steedman and Petrick, 2007; Bohus and Rudnicky,
2009), where the completion of each goal is assumed
to be fully observable, discarding any remaining un-
certainty. Information-state approaches to dialogue
management (Larsson and Traum, 2000; Bos et al,
2003) also rely on a shared state updated according
to a rich repository of rules, but contrary to the ap-
proach presented here, these rules are generally de-
terministic and do not include learnable parameters.
185
The literature on dialogue policy optimisation
with reinforcement learning also contains several
approaches dedicated to dimensionality reduction
for large state-action spaces, such as function ap-
proximation (Henderson et al, 2008), hierarchical
reinforcement learning (Cuaya?huitl et al, 2010) and
summary POMDPs (Young et al, 2010). Most of
these approaches rely on large but weakly struc-
tured state spaces (generally encoded as large lists
of features), which are suited for slot-filling dia-
logue applications but are difficult to transfer to
more open-ended or relational domains. The idea of
state space partitioning, implemented here via high-
level conditions, has also been explored in recent pa-
pers (Williams, 2010; Crook and Lemon, 2010). Fi-
nally, Cuaya?huitl (2011) describes a closely-related
approach using logic-based representations of the
state-action space for relational MDPs. His ap-
proach is however based on reinforcement learning
with a user simulator, while the learning procedure
presented here relies on supervised learning from a
limited data set. He also reduced his belief state
to fully observable variables, whereas we retain the
partial observability associated with each variable.
An important side benefit of structured repre-
sentations in probabilistic models is their improved
readability for human designers, who are able to
use these powerful abstractions to encode their prior
knowledge of the dialogue domain in the form of
pragmatic rules, generic background knowledge, or
task-specific constraints. There has been previ-
ous work on integrating expert knowledge into di-
alogue policy learning, using finite-state policies or
ad-hoc constraints to filter a plain statistical model
(Williams, 2008; Henderson et al, 2008). The ap-
proach presented in this paper is however more gen-
eral since it does not rely on an external filtering
mechanism but directly incorporates prior domain
knowledge into the statistical model.
6 Conclusions
We showed in this paper how to represent the under-
lying structure of probabilistic models for dialogue
using probabilistic rules. These rules are defined as
structured mappings over variables of the dialogue
state, specified using high-level conditions and ef-
fects. These rules can include parameters such as
effect probabilities or action utilities. Probabilistic
rules allow the system designer to exploit power-
ful generalisations in the dialogue domain specifi-
cation without sacrificing the probabilistic nature of
the model. The framework is very general and can
express a wide spectrum of models, from classical
models fully estimated from data to ones incorpo-
rating rich prior knowledge. The choice of model
within this spectrum is therefore essentially a design
decision dependent on the relative availabilities of
training data and domain knowledge.
We have also presented algorithms for construct-
ing Bayesian Networks corresponding to the appli-
cation of the rules and for estimating their parame-
ters from data using Bayesian inference. The pre-
sented approach has been implemented in a spo-
ken dialogue system for human-robot interaction,
and validated on a policy learning task based on a
Wizard-of-Oz data set. The empirical results have
shown that the rule structure enables the learning al-
gorithm to converge faster and with better generali-
sation performance.
We are currently working on extending this ap-
proach in two directions. First, we would like to ex-
tend our parameter estimation method to Bayesian
model-based reinforcement learning. The current
implementation operates in a supervised learning
mode, which requires expert data. Alternatively,
one could estimate the model parameters in a fully
online fashion, without any supervisory input, by
incorporating model uncertainty into the inference
and continuously adapting the parameter distribu-
tion from (real or simulated) interaction experience,
using the same Bayesian approach we have outlined
in this paper (Ross et al, 2011).
The second direction is the extension of our work
to tasks other than action selection. The framework
we have presented is not confined to dialogue pol-
icy learning but can be used to structure any proba-
bilistic model2. It is therefore possible to use proba-
bilistic rules as a unifying framework for all models
defined in a given architecture, and exploit it to per-
form joint optimisation of dialogue understanding,
action selection and generation.
2In fact, the dialogue understanding and generation models
used for the evaluation were already structured with probabilis-
tic rules, but with fixed, hand-crafted parameters.
186
Acknowledgements
The author would like to thank Stephan Oepen, Erik
Velldal and Amanda Stent for useful comments on
an earlier version of this paper.
References
J. Allen, D. Byron, M. Dzikovska, G. Ferguson,
L. Galescu, and A. Stent. 2000. An architecture for
a generic dialogue shell. Natural Language Engineer-
ing, 6:213?228.
D. Bohus and A. I. Rudnicky. 2009. The RavenClaw
dialog management framework: Architecture and sys-
tems. Computer Speech & Language, 23:332?361.
J. Bos, E. Klein, O. Lemon, and T. Oka. 2003. DIPPER:
Description and formalisation of an information-state
update dialogue system architecture. In 4th SIGdial
Workshop on Discourse and Dialogue, pages 115?124.
M. Cavazza, R. Santos de la Camara, M. Turunen,
J. Relan?o-Gil, J. Hakulinen, N. Crook, and D. Field.
2010. How was your day? an affective companion
ECA prototype. In Proceedings of the 11th SIGDIAL
Meeting on Discourse and Dialogue, pages 277?280.
P. A. Crook and O. Lemon. 2010. Representing uncer-
tainty about complex user goals in statistical dialogue
systems. In Proceedings of the 11th SIGDIAL meeting
on Discourse and Dialogue, pages 209?212.
H. Cuaya?huitl, S. Renals, O. Lemon, and H. Shimodaira.
2010. Evaluation of a hierarchical reinforcement
learning spoken dialogue system. Computer Speech
& Language, 24:395?429.
H. Cuaya?huitl. 2011. Learning Dialogue Agents with
Bayesian Relational State Representations. In Pro-
ceedings of the IJCAI Workshop on Knowledge and
Reasoning in Practical Dialogue Systems (IJCAI-
KRPDS), Barcelona, Spain.
H. Erdogan, R. Sarikaya, Y. Gao, and M. Picheny. 2002.
Semantic structured language models. In Proceedings
of the 7th International Conference on Spoken Lan-
guage Processing (ICSLP), Denver, USA.
M. Eskenazi. 2009. An overview of spoken language
technology for education. Speech Commununications,
51:832?844.
M. Frampton and O. Lemon. 2009. Recent research ad-
vances in reinforcement learning in spoken dialogue
systems. Knowledge Engineering Review, 24(4):375?
408.
L. Getoor and B. Taskar. 2007. Introduction to Statistical
Relational Learning. The MIT Press.
M. Hauskrecht, N. Meuleau, L. P. Kaelbling, T. Dean, and
C. Boutilier. 1998. Hierarchical solution of markov
decision processes using macro-actions. In Proceed-
ings of the 14th Conference on Uncertainty in Artifi-
cial Intelligence (UAI), pages 220?229.
Y. He and S. Young. 2005. Semantic processing using
the hidden vector state model. Computer Speech &
Language, 19(1):85?106.
J. Henderson, O. Lemon, and K. Georgila. 2008. Hybrid
reinforcement/supervised learning of dialogue poli-
cies from fixed data sets. Computational Linguistics,
34:487?511.
M. Jaeger. 2001. Complex probabilistic modeling with
recursive relational bayesian networks. Annals of
Mathematics and Artificial Intelligence, 32(1-4):179?
220.
D. Koller and N. Friedman. 2009. Probabilistic Graphi-
cal Models: Principles and Techniques. MIT Press.
G.-J. M. Kruijff, P. Lison, T. Benjamin, H. Jacobsson,
Hendrik Zender, and Ivana Kruijff-Korbayova?, 2010.
Situated Dialogue Processing for Human-Robot Inter-
action, chapter 8. Springer Verlag, Heidelberg, Ger-
many.
K. C. Lan, K. S. Ho, R. W. Pong Luk, and H. Va Leong.
2008. Dialogue act recognition using maximum en-
tropy. Journal of the American Society for Information
Science and Technology (JASIST), pages 859?874.
T. Lang and M. Toussaint. 2010. Planning with noisy
probabilistic relational rules. Journal of Artificial In-
telligence Research, 39:1?49.
S. Larsson and D. R. Traum. 2000. Information state and
dialogue management in the TRINDI dialogue move
engine toolkit. Natuarl Language Engineering, 6(3-
4):323?340, September.
M. Lease, M. Johnson, and E. Charniak. 2006. Rec-
ognizing disfluencies in conversational speech. IEEE
Transactions on Audio, Speech & Language Process-
ing, 14(5):1566?1573.
O. Lemon and O. Pietquin. 2007. Machine Learning for
Spoken Dialogue Systems. In Proceedings of the 10th
European Conference on Speech Communication and
Technologies (Interspeech?07), pages 2685?2688.
O. Lemon. 2011. Learning what to say and how to say
it: Joint optimisation of spoken dialogue management
and natural language generation. Computer Speech &
Language, 25:210?221.
D. J. Litman and S. Silliman. 2004. ITSPOKE: an in-
telligent tutoring spoken dialogue system. In Proceed-
ings of the Conference of the North American Chapter
of the Association of Computational Linguistics (HLT-
NAACL 2004), pages 5?8.
A. Nguyen. 2005. An agent-based approach to dialogue
management in personal assistants. In Proceedings of
the 2005 International conference on Intelligent User
Interfaces (IUI), pages 137?144. ACM Press.
187
A. Oh and A. I. Rudnicky. 2002. Stochastic natural
language generation for spoken dialog systems. Com-
puter Speech & Language, 16(3-4):387?407.
M. Otterlo. 2012. Solving relational and first-order log-
ical markov decision processes: A survey. In Rein-
forcement Learning, volume 12 of Adaptation, Learn-
ing, and Optimization, pages 253?292. Springer Berlin
Heidelberg.
J. Pineau. 2004. Tractable Planning Under Uncertainty:
Exploiting Structure. Ph.D. thesis, Robotics Institute,
Carnegie Mellon University, Pittsburgh, USA.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning, 62:107?136.
S. Ross, J. Pineau, B. Chaib-draa, and P. Kreitmann.
2011. A Bayesian Approach for Learning and Plan-
ning in Partially Observable Markov Decision Pro-
cesses. Journal of Machine Learning Research,
12:1729?1770.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and
S. Young. 2007. Agenda-based user simulation for
bootstrapping a POMDP dialogue system. In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics, NAACL 2007, pages 149?152.
M. Steedman and R. P. A. Petrick. 2007. Planning dialog
actions. In Proceedings of the 8th SIGDIAL Meeting
on Discourse and Dialogue, pages 265?272, Antwerp,
Belgium.
A. Stolcke, N. Coccaro, R. Bates, P. Taylor, C. Van Ess-
Dykema, K. Ries, E. Shriberg, D. Jurafsky, R. Mar-
tin, and M. Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computional Linguistics, 26:339?373.
V. Thomson and S. Young. 2010. Bayesian update
of dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech & Language,
24:562?588, October.
J. D. Williams. 2008. The best of both worlds: Unify-
ing conventional dialog systems and POMDPs. In In-
ternational Conference on Speech and Language Pro-
cessing (ICSLP 2008), Brisbane, Australia.
J. D. Williams. 2010. Incremental partition recombi-
nation for efficient tracking of multiple dialog states.
In Proceedings of the IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP),
pages 5382?5385.
S. Young, M. Gas?ic?, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson, and K. Yu. 2010. The hidden
information state model: A practical framework for
pomdp-based spoken dialogue management. Com-
puter Speech & Language, 24:150?174.
H. Zen, K. Tokuda, and A. W. Black. 2009. Review:
Statistical parametric speech synthesis. Speech Com-
munications, 51:1039?1064.
A Example of interaction
We present below two recorded excerpts of Wizard-
of-Oz interactions. The user utterances are simpli-
fied to show only the hypothesis with highest score
(indicated in parenthesis) in the N-best list.
Transcript A.1 Wizard-of-Oz interaction 1
HUMAN: Hi Lenny (0.74)
ROBOT: Hello there!
HUMAN: Move your left arm up (0.41)
ROBOT: Sure!
[executing movement, waits a few seconds]
ROBOT: Was that correct?
HUMAN: Yes (0.88)
[registers movement as confirmed]
HUMAN: Move your right arm up at the same
time (0.38)
ROBOT: Let?s see
[executing movement]
ROBOT: Was that correct?
HUMAN: No (0.50)
[goes back to previous position]
ROBOT: What should I do then?
[...]
Transcript A.2 Wizard-of-Oz interaction 2
HUMAN: Stand up (0.41)
ROBOT: Let?s see
[executing movement]
HUMAN: Move your right arm forward (0.35)
[last move is implicitly grounded]
ROBOT: Let?s see
[executing movement]
HUMAN: Lower the left arm (0.34)
ROBOT: Sorry, could you repeat?
HUMAN: Lift your left arm (0.37)
ROBOT: Sure
[executing movement, waits a few seconds]
ROBOT: Should I do it like this?
[...]
188
