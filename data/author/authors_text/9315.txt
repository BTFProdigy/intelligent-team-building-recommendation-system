Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 562?571,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A Tale of Two Parsers: investigating and combining graph-based and
transition-based dependency parsing using beam-search
Yue Zhang and Stephen Clark
Oxford University Computing Laboratory
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
{yue.zhang,stephen.clark}@comlab.ox.ac.uk
Abstract
Graph-based and transition-based approaches
to dependency parsing adopt very different
views of the problem, each view having its
own strengths and limitations. We study both
approaches under the framework of beam-
search. By developing a graph-based and a
transition-based dependency parser, we show
that a beam-search decoder is a competitive
choice for both methods. More importantly,
we propose a beam-search-based parser that
combines both graph-based and transition-
based parsing into a single system for train-
ing and decoding, showing that it outper-
forms both the pure graph-based and the pure
transition-based parsers. Testing on the En-
glish and Chinese Penn Treebank data, the
combined system gave state-of-the-art accura-
cies of 92.1% and 86.2%, respectively.
1 Introduction
Graph-based (McDonald et al, 2005; McDon-
ald and Pereira, 2006; Carreras et al, 2006) and
transition-based (Yamada and Matsumoto, 2003;
Nivre et al, 2006) parsing algorithms offer two dif-
ferent approaches to data-driven dependency pars-
ing. Given an input sentence, a graph-based algo-
rithm finds the highest scoring parse tree from all
possible outputs, scoring each complete tree, while
a transition-based algorithm builds a parse by a se-
quence of actions, scoring each action individually.
The terms ?graph-based? and ?transition-based?
were used by McDonald and Nivre (2007) to de-
scribe the difference between MSTParser (McDon-
ald and Pereira, 2006), which is a graph-based parser
with an exhaustive search decoder, and MaltParser
(Nivre et al, 2006), which is a transition-based
parser with a greedy search decoder. In this paper,
we do not differentiate graph-based and transition-
based parsers by their search algorithms: a graph-
based parser can use an approximate decoder while
a transition-based parser is not necessarily determin-
istic. To make the concepts clear, we classify the two
types of parser by the following two criteria:
1. whether or not the outputs are built by explicit
transition-actions, such as ?Shift? and ?Reduce?;
2. whether it is dependency graphs or transition-
actions that the parsing model assigns scores to.
By this classification, beam-search can be applied
to both graph-based and transition-based parsers.
Representative of each method, MSTParser and
MaltParser gave comparable accuracies in the
CoNLL-X shared task (Buchholz and Marsi, 2006).
However, they make different types of errors, which
can be seen as a reflection of their theoretical differ-
ences (McDonald and Nivre, 2007). MSTParser has
the strength of exact inference, but its choice of fea-
tures is constrained by the requirement of efficient
dynamic programming. MaltParser is deterministic,
yet its comparatively larger feature range is an ad-
vantage. By comparing the two, three interesting re-
search questions arise: (1) how to increase the flex-
ibility in defining features for graph-based parsing;
(2) how to add search to transition-based parsing;
and (3) how to combine the two parsing approaches
so that the strengths of each are utilized.
In this paper, we study these questions under one
framework: beam-search. Beam-search has been
successful in many NLP tasks (Koehn et al, 2003;
562
Inputs: training examples (xi, yi)
Initialization: set ~w = 0
Algorithm:
// R training iterations; N examples
for t = 1..R, i = 1..N :
zi = argmaxy?GEN(xi) ?(y) ? ~w
if zi 6= yi:
~w = ~w + ?(yi)? ?(zi)
Outputs: ~w
Figure 1: The perceptron learning algorithm
Collins and Roark, 2004), and can achieve accuracy
that is close to exact inference. Moreover, a beam-
search decoder does not impose restrictions on the
search problem in the way that an exact inference
decoder typically does, such as requiring the ?op-
timal subproblem? property for dynamic program-
ming, and therefore enables a comparatively wider
range of features for a statistical system.
We develop three parsers. Firstly, using the same
features as MSTParser, we develop a graph-based
parser to examine the accuracy loss from beam-
search compared to exact-search, and the accuracy
gain from extra features that are hard to encode
for exact inference. Our conclusion is that beam-
search is a competitive choice for graph-based pars-
ing. Secondly, using the transition actions from
MaltParser, we build a transition-based parser and
show that search has a positive effect on its accuracy
compared to deterministic parsing. Finally, we show
that by using a beam-search decoder, we are able
to combine graph-based and transition-based pars-
ing into a single system, with the combined system
significantly outperforming each individual system.
In experiments with the English and Chinese Penn
Treebank data, the combined parser gave 92.1% and
86.2% accuracy, respectively, which are comparable
to the best parsing results for these data sets, while
the Chinese accuracy outperforms the previous best
reported by 1.8%. In line with previous work on de-
pendency parsing using the Penn Treebank, we fo-
cus on projective dependency parsing.
2 The graph-based parser
Following MSTParser (McDonald et al, 2005; Mc-
Donald and Pereira, 2006), we define the graph-
Variables: agenda ? the beam for state items
item ? partial parse tree
output ? a set of output items
index, prev ? word indexes
Input: x ? POS-tagged input sentence.
Initialization: agenda = [??]
Algorithm:
for index in 1..x.length():
clear output
for item in agenda:
// for all prev words that can be linked with
// the current word at index
prev = index ? 1
while prev 6= 0: // while prev is valid
// add link making prev parent of index
newitem = item // duplicate item
newitem.link(prev, index) // modify
output.append(newitem) // record
// if prev does not have a parent word,
// add link making index parent of prev
if item.parent(prev) == 0:
item.link(index, prev) // modify
output.append(item) // record
prev = the index of the first word before
prev whose parent does not exist
or is on its left; 0 if no match
clear agenda
put the best items from output to agenda
Output: the best item in agenda
Figure 2: A beam-search decoder for graph-based pars-
ing, developed from the deterministic Covington algo-
rithm for projective parsing (Covington, 2001).
based parsing problem as finding the highest scoring
tree y from all possible outputs given an input x:
F (x) = argmax
y?GEN(x)
Score(y)
where GEN(x) denotes the set of possible parses for
the input x. To repeat our earlier comments, in this
paper we do not consider the method of finding the
argmax to be part of the definition of graph-based
parsing, only the fact that the dependency graph it-
self is being scored, and factored into scores at-
tached to the dependency links.
The score of an output parse y is given by a linear
model:
Score(y) = ?(y) ? ~w
563
where ?(y) is the global feature vector from y and
~w is the weight vector of the model.
We use the discriminative perceptron learning al-
gorithm (Collins, 2002; McDonald et al, 2005) to
train the values of ~w. The algorithm is shown in Fig-
ure 1. Averaging parameters is a way to reduce over-
fitting for perceptron training (Collins, 2002), and is
applied to all our experiments.
While the MSTParser uses exact-inference (Eis-
ner, 1996), we apply beam-search to decoding. This
is done by extending the deterministic Covington
algorithm for projective dependency parsing (Cov-
ington, 2001). As shown in Figure 2, the decoder
works incrementally, building a state item (i.e. par-
tial parse tree) word by word. When each word is
processed, links are added between the current word
and its predecessors. Beam-search is applied by
keeping the B best items in the agenda at each pro-
cessing stage, while partial candidates are compared
by scores from the graph-based model, according to
partial graph up to the current word.
Before decoding starts, the agenda contains an
empty sentence. At each processing stage, existing
partial candidates from the agenda are extended in
all possible ways according to the Covington algo-
rithm. The top B newly generated candidates are
then put to the agenda. After all input words are pro-
cessed, the best candidate output from the agenda is
taken as the final output.
The projectivity of the output dependency trees
is guaranteed by the incremental Covington process.
The time complexity of this algorithm is O(n2),
where n is the length of the input sentence.
During training, the ?early update? strategy of
Collins and Roark (2004) is used: when the correct
state item falls out of the beam at any stage, parsing
is stopped immediately, and the model is updated
using the current best partial item. The intuition is
to improve learning by avoiding irrelevant informa-
tion: when all the items in the current agenda are
incorrect, further parsing steps will be irrelevant be-
cause the correct partial output no longer exists in
the candidate ranking.
Table 1 shows the feature templates from the
MSTParser (McDonald and Pereira, 2006), which
are defined in terms of the context of a word, its
parent and its sibling. To give more templates, fea-
tures from templates 1 ? 5 are also conjoined with
1 Parent word (P) Pw; Pt; Pwt
2 Child word (C) Cw; Ct; Cwt
3 P and C PwtCwt; PwtCw;
PwCwt; PwtCt;
PtCwt; PwCw; PtCt
4 A tag Bt PtBtCt
between P, C
5 Neighbour words PtPLtCtCLt;
of P, C, PtPLtCtCRt;
left (PL/CL) PtPRtCtCLt;
and right (PR/CR) PtPRtCtCRt;
PtPLtCLt; PtPLtCRt;
PtPRtCLt; PtPRtCRt;
PLtCtCLt; PLtCtCRt;
PRtCtCLt; PRtCtCRt;
PtCtCLt; PtCtCRt;
PtPLtCt; PtPRtCt
6 sibling (S) of C CwSw; CtSt;
CwSt; CtSw;
PtCtSt;
Table 1: Feature templates from MSTParser
w ? word; t ? POS-tag.
1 leftmost (CLC) and PtCtCLCt;
rightmost (CRC) PtCtCRCt
children of C
2 left (la) and right (ra) Ptla; Ptra;
arity of P Pwtla; Pwtra
Table 2: Additional feature templates for the graph-based
parser
the link direction and distance, while features from
template 6 are also conjoined with the direction and
distance between the child and its sibling. Here
?distance? refers to the difference between word in-
dexes. We apply all these feature templates to the
graph-based parser. In addition, we define two extra
feature templates (Table 2) that capture information
about grandchildren and arity (i.e. the number of
children to the left or right). These features are not
conjoined with information about direction and dis-
tance. They are difficult to include in an efficient
dynamic programming decoder, but easy to include
in a beam-search decoder.
564
Figure 3: Feature context for the transition-based algo-
rithm
3 The transition-based parser
We develop our transition-based parser using the
transition model of the MaltParser (Nivre et al,
2006), which is characterized by the use of a stack
and four transition actions: Shift, ArcRight, ArcLeft
and Reduce. An input sentence is processed from
left to right, with an index maintained for the current
word. Initially empty, the stack is used throughout
the parsing process to store unfinished words, which
are the words before the current word that may still
be linked with the current or a future word.
The Shift action pushes the current word to the
stack and moves the current index to the next word.
The ArcRight action adds a dependency link from
the stack top to the current word (i.e. the stack top
becomes the parent of the current word), pushes the
current word on to the stack, and moves the current
index to the next word. The ArcLeft action adds a
dependency link from the current word to the stack
top, and pops the stack. The Reduce action pops the
stack. Among the four transition actions, Shift and
ArcRight push a word on to the stack while ArcLeft
and Reduce pop the stack; Shift and ArcRight read
the next input word while ArcLeft and ArcRight add
a link to the output. By repeated application of these
actions, the parser reads through the input and builds
a parse tree.
The MaltParser works deterministically. At each
step, it makes a single decision and chooses one of
the four transition actions according to the current
context, including the next input words, the stack
and the existing links. As illustrated in Figure 3, the
contextual information consists of the top of stack
(ST), the parent (STP) of ST, the leftmost (STLC) and
rightmost child (STRC) of ST, the current word (N0),
the next three words from the input (N1, N2, N3) and
the leftmost child of N0 (N0LC). Given the context
s, the next action T is decided as follows:
T (s) = argmax
T?ACTION
Score(T, s)
where ACTION = {Shift, ArcRight, ArcLeft,
Reduce}.
One drawback of deterministic parsing is error
propagation, since once an incorrect action is made,
the output parse will be incorrect regardless of the
subsequent actions. To reduce such error propa-
gation, a parser can keep track of multiple candi-
date outputs and avoid making decisions too early.
Suppose that the parser builds a set of candidates
GEN(x) for the input x, the best output F (x) can
be decided by considering all actions:
F (x) = argmax
y?GEN(x)
?
T ??act(y) Score(T ?, sT ?)
Here T ? represents one action in the sequence
(act(y)) by which y is built, and sT ? represents the
corresponding context when T ? is taken.
Our transition-based algorithm keeps B different
sequences of actions in the agenda, and chooses the
one having the overall best score as the final parse.
Pseudo code for the decoding algorithm is shown
in Figure 4. Here each state item contains a partial
parse tree as well as a stack configuration, and state
items are built incrementally by transition actions.
Initially the stack is empty, and the agenda contains
an empty sentence. At each processing stage, one
transition action is applied to existing state items as
a step to build the final parse. Unlike the MaltParser,
which makes a decision at each stage, our transition-
based parser applies all possible actions to each ex-
isting state item in the agenda to generate new items;
then from all the newly generated items, it takes the
B with the highest overall score and puts them onto
the agenda. In this way, some ambiguity is retained
for future resolution.
Note that the number of transition actions needed
to build different parse trees can vary. For exam-
ple, the three-word sentence ?A B C? can be parsed
by the sequence of three actions ?Shift ArcRight
ArcRight? (B modifies A; C modifies B) or the
sequence of four actions ?Shift ArcLeft Shift Ar-
cRight? (both A and C modifies B). To ensure that
all final state items are built by the same number
of transition actions, we require that the final state
565
Variables: agenda ? the beam for state items
item ? (partial tree, stack config)
output ? a set of output items
index ? iteration index
Input: x ? POS-tagged input sentence.
Initialization: agenda = [(??, [])]
Algorithm:
for index in 1 .. 2? x.length() ?1:
clear output
for item in agenda:
// when all input words have been read, the
// parse tree has been built; only pop.
if item.length() == x.length():
if item.stacksize() > 1:
item.Reduce()
output.append(item)
// when some input words have not been read
else:
if item.lastaction() 6= Reduce:
newitem = item
newitem.Shift()
output.append(newitem)
if item.stacksize() > 0:
newitem = item
newitem.ArcRight()
output.append(newitem)
if (item.parent(item.stacktop())==0):
newitem = item
newitem.ArcLeft()
output.append(newitem)
else:
newitem = item
newitem.Reduce()
output.append(newitem)
clear agenda
transfer the best items from output to agenda
Output: the best item in agenda
Figure 4: A beam-search decoding algorithm for
transition-based parsing
items must 1) have fully-built parse trees; and 2)
have only one root word left on the stack. In this
way, popping actions should be made even after a
complete parse tree is built, if the stack still contains
more than one word.
Now because each word excluding the root must
be pushed to the stack once and popped off once
during the parsing process, the number of actions
Inputs: training examples (xi, yi)
Initialization: set ~w = 0
Algorithm:
// R training iterations; N examples
for t = 1..R, i = 1..N :
zi = argmaxy?GEN(xi)
?
T ??act(yi) ?(T
?, c?) ? ~w
if zi 6= yi:
~w = ~w + ?T ??act(yi) ?(T
?, cT ?)
??T ??act(zi) ?(T
?, cT ?)
Outputs: ~w
Figure 5: the perceptron learning algorithm for the
transition-based parser
1 stack top STwt; STw; STt
2 current word N0wt; N0w; N0t
3 next word N1wt; N1w; N1t
4 ST and N0 STwtN0wt; STwtN0w;
STwN0wt; STwtN0t;
STtN0wt; STwN0w; STtN0t
5 POS bigram N0tN1t
6 POS trigrams N0tN1tN2t; STtN0tN1t;
STPtSTtN0t; STtSTLCtN0t;
STtSTRCtN0t; STtN0tN0LCt
7 N0 word N0wN1tN2t; STtN0wN1t;
STPtSTtN0w; STtSTLCtN0w;
STtSTRCtN0w; STtN0wN0LCt
Table 3: Feature templates for the transition-based parser
w ? word; t ? POS-tag.
needed to parse a sentence is always 2n ? 1, where
n is the length of the sentence. Therefore, the de-
coder has linear time complexity, given a fixed beam
size. Because the same transition actions as the
MaltParser are used to build each item, the projec-
tivity of the output dependency tree is ensured.
We use a linear model to score each transition ac-
tion, given a context:
Score(T, s) = ?(T, s) ? ~w
?(T, s) is the feature vector extracted from the ac-
tion T and the context s, and ~w is the weight vec-
tor. Features are extracted according to the templates
shown in Table 3, which are based on the context in
Figure 3. Note that our feature definitions are sim-
ilar to those used by MaltParser, but rather than us-
ing a kernel function with simple features (e.g. STw,
566
N0t, but not STwt or STwN0w), we combine features
manually.
As with the graph-based parser, we use the dis-
criminative perceptron (Collins, 2002) to train the
transition-based model (see Figure 5). It is worth
noticing that, in contrast to MaltParser, which trains
each action decision individually, our training algo-
rithm globally optimizes all action decisions for a
parse. Again, ?early update? and averaging parame-
ters are applied to the training process.
4 The combined parser
The graph-based and transition-based approaches
adopt very different views of dependency parsing.
McDonald and Nivre (2007) showed that the MST-
Parser and MaltParser produce different errors. This
observation suggests a combined approach: by using
both graph-based information and transition-based
information, parsing accuracy can be improved.
The beam-search framework we have developed
facilitates such a combination. Our graph-based
and transition-based parsers share many similarities.
Both build a parse tree incrementally, keeping an
agenda of comparable state items. Both rank state
items by their current scores, and use the averaged
perceptron with early update for training. The key
differences are the scoring models and incremental
parsing processes they use, which must be addressed
when combining the parsers.
Firstly, we combine the graph-based and the
transition-based score models simply by summation.
This is possible because both models are global and
linear. In particular, the transition-based model can
be written as:
ScoreT(y) =
?
T ??act(y) Score(T ?, sT ?)
= ?T ??act(y) ?(T ?, sT ?) ? ~wT
= ~wT ?
?
T ??act(y) ?(T ?, sT ?)
If we take
?
T ??act(y) ?(T ?, sT ?) as the global fea-
ture vector ?T(y), we have:
ScoreT(y) = ?T(y) ? ~wT
which has the same form as the graph-based model:
ScoreG(y) = ?G(y) ? ~wG
Sections Sentences Words
Training 2?21 39,832 950,028
Dev 22 1,700 40,117
Test 23 2,416 56,684
Table 4: The training, development and test data from
PTB
We therefore combine the two models to give:
ScoreC(y) = ScoreG(y) + ScoreT(y)
= ?G(y) ? ~wG + ?T(y) ? ~wT
Concatenating the feature vectors ?G(y) and ?T(y)
to give a global feature vector ?C(y), and the weight
vectors ~wG and ~wT to give a weight vector ~wC, the
combined model can be written as:
ScoreC(y) = ?C(y) ? ~wC
which is a linear model with exactly the same form
as both sub-models, and can be trained with the per-
ceptron algorithm in Figure 1. Because the global
feature vectors from the sub models are concate-
nated, the feature set for the combined model is the
union of the sub model feature sets.
Second, the transition-based decoder can be used
for the combined system. Both the graph-based de-
coder in Figure 2 and the transition-based decoder in
Figure 4 construct a parse tree incrementally. How-
ever, the graph-based decoder works on a per-word
basis, adding links without using transition actions,
and so is not appropriate for the combined model.
The transition-based algorithm, on the other hand,
uses state items which contain partial parse trees,
and so provides all the information needed by the
graph-based parser (i.e. dependency graphs), and
hence the combined system.
In summary, we build the combined parser by
using a global linear model, the union of feature
templates and the decoder from the transition-based
parser.
5 Experiments
We evaluate the parsers using the English and Chi-
nese Penn Treebank corpora. The English data
is prepared by following McDonald et al (2005).
Bracketed sentences from the Penn Treebank (PTB)
3 are split into training, development and test sets
567
Figure 6: The influence of beam size on the transition-
based parser, using the development data
X-axis: number of training iterations
Y-axis: word precision
as shown in Table 4, and then translated into depen-
dency structures using the head-finding rules from
Yamada and Matsumoto (2003).
Before parsing, POS tags are assigned to the in-
put sentence using our reimplementation of the POS-
tagger from Collins (2002). Like McDonald et al
(2005), we evaluate the parsing accuracy by the
precision of lexical heads (the percentage of input
words, excluding punctuation, that have been as-
signed the correct parent) and by the percentage
of complete matches, in which all words excluding
punctuation have been assigned the correct parent.
5.1 Development experiments
Since the beam size affects all three parsers, we
study its influence first; here we show the effect on
the transition-based parser. Figure 6 shows different
accuracy curves using the development data, each
with a different beam size B. The X-axis represents
the number of training iterations, and the Y-axis the
precision of lexical heads.
The parsing accuracy generally increases as the
beam size increases, while the quantity of increase
becomes very small when B becomes large enough.
The decoding times after the first training iteration
are 10.2s, 27.3s, 45.5s, 79.0s, 145.4s, 261.3s and
469.5s, respectively, when B = 1, 2, 4, 8, 16, 32, 64.
Word Complete
MSTParser 1 90.7 36.7
Graph [M] 91.2 40.8
Transition 91.4 41.8
Graph [MA] 91.4 42.5
MSTParser 2 91.5 42.1
Combined [TM] 92.0 45.0
Combined [TMA] 92.1 45.4
Table 5: Accuracy comparisons using PTB 3
In the rest of the experiments, we set B = 64 in
order to obtain the highest possible accuracy.
When B = 1, the transition-based parser be-
comes a deterministic parser. By comparing the
curves when B = 1 and B = 2, we can see that,
while the use of search reduces the parsing speed, it
improves the quality of the output parses. Therefore,
beam-search is a reasonable choice for transition-
based parsing.
5.2 Accuracy comparisons
The test accuracies are shown in Table 5, where each
row represents a parsing model. Rows ?MSTParser
1/2? show the first-order (using feature templates 1 ?
5 from Table 1) (McDonald et al, 2005) and second-
order (using all feature templates from Table 1)
(McDonald and Pereira, 2006) MSTParsers, as re-
ported by the corresponding papers. Rows ?Graph
[M]? and ?Graph [MA]? represent our graph-based
parser using features from Table 1 and Table 1 + Ta-
ble 2, respectively; row ?Transition? represents our
transition-based parser; and rows ?Combined [TM]?
and ?Combined [TMA]? represent our combined
parser using features from Table 3 + Table 1 and Ta-
ble 3 + Table 1 + Table 2, respectively. Columns
?Word? and ?Complete? show the precision of lexi-
cal heads and complete matches, respectively.
As can be seen from the table, beam-search re-
duced the head word accuracy from 91.5%/42.1%
(?MSTParser 2?) to 91.2%/40.8% (?Graph [M]?)
with the same features as exact-inference. How-
ever, with only two extra feature templates from
Table 2, which are not conjoined with direction or
distance information, the accuracy is improved to
91.4%/42.5% (?Graph [MA]?). This improvement
can be seen as a benefit of beam-search, which al-
lows the definition of more global features.
568
Sections Sentences Words
Training 001?815; 16,118 437,859
1001?1136
Dev 886?931; 804 20,453
1148?1151
Test 816?885; 1,915 50,319
1137?1147
Table 6: Training, development and test data from CTB
Non-root Root Comp.
Graph [MA] 83.86 71.38 29.82
Duan 2007 84.36 73.70 32.70
Transition 84.69 76.73 32.79
Combined [TM] 86.13 77.04 35.25
Combined [TMA] 86.21 76.26 34.41
Table 7: Test accuracies with CTB 5 data
The combined parser is tested with various sets
of features. Using only graph-based features in Ta-
ble 1, it gave 88.6% accuracy, which is much lower
than 91.2% from the graph-based parser using the
same features (?Graph [M]?). This can be explained
by the difference between the decoders. In particu-
lar, the graph-based model is unable to score the ac-
tions ?Reduce? and ?Shift?, since they do not mod-
ify the parse tree. Nevertheless, the score serves as a
reference for the effect of additional features in the
combined parser.
Using both transition-based features and graph-
based features from the MSTParser (?Combined
[TM]?), the combined parser achieved 92.0% per-
word accuracy, which is significantly higher than the
pure graph-based and transition-based parsers. Ad-
ditional graph-based features further improved the
accuracy to 92.1%/45.5%, which is the best among
all the parsers compared.1
5.3 Parsing Chinese
We use the Penn Chinese Treebank (CTB) 5 for ex-
perimental data. Following Duan et al (2007), we
1A recent paper, Koo et al (2008) reported parent-prediction
accuracy of 92.0% using a graph-based parser with a different
(larger) set of features (Carreras, 2007). By applying separate
word cluster information, Koo et al (2008) improved the accu-
racy to 93.2%, which is the best known accuracy on the PTB
data. We excluded these from Table 5 because our work is not
concerned with the use of such additional knowledge.
split the corpus into training, development and test
data as shown in Table 6, and use the head-finding
rules in Table 8 in the Appendix to turn the bracketed
sentences into dependency structures. Most of the
head-finding rules are from Sun and Jurafsky (2004),
while we added rules to handle NN and FRAG, and
a default rule to use the rightmost node as the head
for the constituent that are not listed.
Like Duan et al (2007), we use gold-standard
POS-tags for the input. The parsing accuracy is eval-
uated by the percentage of non-root words that have
been assigned the correct head, the percentage of
correctly identified root words, and the percentage
of complete matches, all excluding punctuation.
The accuracies are shown in Table 7. Rows
?Graph [MA]?, ?Transition?, ?Combined [TM]? and
?Combined [TMA]? show our models in the same
way as for the English experiments from Section 5.2.
Row ?Duan 2007? represents the transition-based
model from Duan et al (2007), which applies beam-
search to the deterministic model from Yamada and
Matsumoto (2003), and achieved the previous best
accuracy on the data.
Our observations on parsing Chinese are essen-
tially the same as for English. Our combined parser
outperforms both the pure graph-based and the pure
transition-based parsers. It gave the best accuracy
we are aware of for dependency parsing using CTB.
6 Related work
Our graph-based parser is derived from the work
of McDonald and Pereira (2006). Instead of per-
forming exact inference by dynamic programming,
we incorporated the linear model and feature tem-
plates from McDonald and Pereira (2006) into our
beam-search framework, while adding new global
features. Nakagawa (2007) and Hall (2007) also
showed the effectiveness of global features in im-
proving the accuracy of graph-based parsing, us-
ing the approximate Gibbs sampling method and a
reranking approach, respectively.
Our transition-based parser is derived from the
deterministic parser of Nivre et al (2006). We
incorporated the transition process into our beam-
search framework, in order to study the influence
of search on this algorithm. Existing efforts to
add search to deterministic parsing include Sagae
569
and Lavie (2006b), which applied best-first search
to constituent parsing, and Johansson and Nugues
(2006) and Duan et al (2007), which applied beam-
search to dependency parsing. All three methods es-
timate the probability of each transition action, and
score a state item by the product of the probabilities
of all its corresponding actions. But different from
our transition-based parser, which trains all transi-
tions for a parse globally, these models train the
probability of each action separately. Based on the
work of Johansson and Nugues (2006), Johansson
and Nugues (2007) studied global training with an
approximated large-margin algorithm. This model
is the most similar to our transition-based model,
while the differences include the choice of learning
and decoding algorithms, the definition of feature
templates and our application of the ?early update?
strategy.
Our combined parser makes the biggest contribu-
tion of this paper. In contrast to the models above,
it includes both graph-based and transition-based
components. An existing method to combine mul-
tiple parsing algorithms is the ensemble approach
(Sagae and Lavie, 2006a), which was reported to
be useful in improving dependency parsing (Hall et
al., 2007). A more recent approach (Nivre and Mc-
Donald, 2008) combined MSTParser and MaltParser
by using the output of one parser for features in the
other. Both Hall et al (2007) and Nivre and McDon-
ald (2008) can be seen as methods to combine sep-
arately defined models. In contrast, our parser com-
bines two components in a single model, in which
all parameters are trained consistently.
7 Conclusion and future work
We developed a graph-based and a transition-based
projective dependency parser using beam-search,
demonstrating that beam-search is a competitive
choice for both parsing approaches. We then com-
bined the two parsers into a single system, using dis-
criminative perceptron training and beam-search de-
coding. The appealing aspect of the combined parser
is the incorporation of two largely different views of
the parsing problem, thus increasing the information
available to a single statistical parser, and thereby
significantly increasing the accuracy. When tested
using both English and Chinese dependency data,
the combined parser was highly competitive com-
pared to the best systems in the literature.
The idea of combining different approaches to
the same problem using beam-search and a global
model could be applied to other parsing tasks, such
as constituent parsing, and possibly other NLP tasks.
Acknowledgements
This work is supported by the ORS and Clarendon
Fund. We thank the anonymous reviewers for their
detailed comments.
Appendix
Constituent Rules
ADJP r ADJP JJ AD; r
ADVP r ADVP AD CS JJ NP PP P VA VV; r
CLP r CLP M NN NP; r
CP r CP IP VP; r
DNP r DEG DNP DEC QP; r
DP r M; l DP DT OD; l
DVP r DEV AD VP; r
FRAG r VV NR NN NT; r
IP r VP IP NP; r
LCP r LCP LC; r
LST r CD NP QP; r
NP r NP NN IP NR NT; r
NN r NP NN IP NR NT; r
PP l P PP; l
PRN l PU; l
QP r QP CLP CD; r
UCP l IP NP VP; l
VCD l VV VA VE; l
VP l VE VC VV VNV VPT VRD VSB
VCD VP; l
VPT l VA VV; l
VRD l VVI VA; l
VSB r VV VE; r
default r
Table 8: Head-finding rules to extract dependency data
from CTB
570
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164, New York
City, USA, June.
Xavier Carreras, Mihai Surdeanu, and Lluis Marquez.
2006. Projective dependency parsing with perceptron.
In Proceedings of CoNLL, New York City, USA, June.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP/CoNLL, pages
957?961, Prague, Czech Republic, June.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL, pages 111?118, Barcelona, Spain, July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8, Philadelphia, USA, July.
Michael A. Covington. 2001. A fundamental algorithm
for dependency parsing. In Proceedings of the ACM
Southeast Conference, Athens, Georgia, March.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilis-
tic models for action-based chinese dependency pars-
ing. In Proceedings of ECML/ECPPKDD, Warsaw,
Poland, September.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
of COLING, pages 340?345, Copenhagen, Denmark,
August.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen Eryigit,
Bea?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? a study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP/CoNLL, pages 933?
939, Prague, Czech Republic, June.
Keith Hall. 2007. K-best spanning tree parsing. In Pro-
ceedings of ACL, Prague, Czech Republic, June.
Richard Johansson and Pierre Nugues. 2006. Investigat-
ing multilingual dependency parsing. In Proceedings
of CoNLL, pages 206?210, New York City, USA, June.
Richard Johansson and Pierre Nugues. 2007. Incremen-
tal dependency parsing using online learning. In Pro-
ceedings of the CoNLL/EMNLP, pages 1134?1138,
Prague, Czech Republic.
Philip Koehn, Franz Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings of
NAACL/HLT, Edmonton, Canada, May.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL/HLT, pages 595?603, Columbus,
Ohio, June.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP/CoNLL, pages 122?
131, Prague, Czech Republic, June.
R McDonald and F Pereira. 2006. Online learning of ap-
proximate dependency parsing algorithms. In In Proc.
of EACL, pages 81?88, Trento, Italy, April.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL, pages 91?98, Ann
Arbor, Michigan, June.
Tetsuji Nakagawa. 2007. Multilingual dependency
parsing using global features. In Proceedings of the
CoNLL Shared Task Session of EMNLP/CoNLL, pages
952?956, Prague, Czech Republic, June.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL/HLT, pages 950?958, Colum-
bus, Ohio, June.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?en Eryig?it,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector ma-
chines. In Proceedings of CoNLL, pages 221?225,
New York City, USA, June.
K Sagae and A Lavie. 2006a. Parser combination by
reparsing. In In Proc. HLT/NAACL, pages 129?132,
New York City, USA, June.
Kenji Sagae and Alon Lavie. 2006b. A best-first prob-
abilistic shift-reduce parser. In Proceedings of COL-
ING/ACL (poster), pages 691?698, Sydney, Australia,
July.
Honglin Sun and Daniel Jurafsky. 2004. Shallow
semantic parsing of Chinese. In Proceedings of
NAACL/HLT, Boston, USA, May.
H Yamada and Y Matsumoto. 2003. Statistical depen-
dency analysis using support vector machines. In Pro-
ceedings of IWPT, Nancy, France, April.
571
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 840?847,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Chinese Segmentation with a Word-Based Perceptron Algorithm
Yue Zhang and Stephen Clark
Oxford University Computing Laboratory
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
{yue.zhang,stephen.clark}@comlab.ox.ac.uk
Abstract
Standard approaches to Chinese word seg-
mentation treat the problem as a tagging
task, assigning labels to the characters in
the sequence indicating whether the char-
acter marks a word boundary. Discrimina-
tively trained models based on local char-
acter features are used to make the tagging
decisions, with Viterbi decoding finding the
highest scoring segmentation. In this paper
we propose an alternative, word-based seg-
mentor, which uses features based on com-
plete words and word sequences. The gener-
alized perceptron algorithm is used for dis-
criminative training, and we use a beam-
search decoder. Closed tests on the first and
second SIGHAN bakeoffs show that our sys-
tem is competitive with the best in the litera-
ture, achieving the highest reported F-scores
for a number of corpora.
1 Introduction
Words are the basic units to process for most NLP
tasks. The problem of Chinese word segmentation
(CWS) is to find these basic units for a given sen-
tence, which is written as a continuous sequence of
characters. It is the initial step for most Chinese pro-
cessing applications.
Chinese character sequences are ambiguous, of-
ten requiring knowledge from a variety of sources
for disambiguation. Out-of-vocabulary (OOV) words
are a major source of ambiguity. For example, a
difficult case occurs when an OOV word consists
of characters which have themselves been seen as
words; here an automatic segmentor may split the
OOV word into individual single-character words.
Typical examples of unseen words include Chinese
names, translated foreign names and idioms.
The segmentation of known words can also be
ambiguous. For example, ???b? should be ??
? (here)b (flour)? in the sentence ???b?s?
5? (flour and rice are expensive here) or ?? (here)
?b (inside)? in the sentence ???b??? (it?s
cold inside here). The ambiguity can be resolved
with information about the neighboring words. In
comparison, for the sentences ?=Proceedings of ACL-08: HLT, pages 888?896,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Joint Word Segmentation and POS Tagging using a Single Perceptron
Yue Zhang and Stephen Clark
Oxford University Computing Laboratory
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
{yue.zhang,stephen.clark}@comlab.ox.ac.uk
Abstract
For Chinese POS tagging, word segmentation
is a preliminary step. To avoid error propa-
gation and improve segmentation by utilizing
POS information, segmentation and tagging
can be performed simultaneously. A challenge
for this joint approach is the large combined
search space, which makes efficient decod-
ing very hard. Recent research has explored
the integration of segmentation and POS tag-
ging, by decoding under restricted versions of
the full combined search space. In this paper,
we propose a joint segmentation and POS tag-
ging model that does not impose any hard con-
straints on the interaction between word and
POS information. Fast decoding is achieved
by using a novel multiple-beam search algo-
rithm. The system uses a discriminative sta-
tistical model, trained using the generalized
perceptron algorithm. The joint model gives
an error reduction in segmentation accuracy of
14.6% and an error reduction in tagging ac-
curacy of 12.2%, compared to the traditional
pipeline approach.
1 Introduction
Since Chinese sentences do not contain explicitly
marked word boundaries, word segmentation is a
necessary step before POS tagging can be performed.
Typically, a Chinese POS tagger takes segmented in-
puts, which are produced by a separate word seg-
mentor. This two-step approach, however, has an
obvious flaw of error propagation, since word seg-
mentation errors cannot be corrected by the POS tag-
ger. A better approach would be to utilize POS in-
formation to improve word segmentation. For ex-
ample, the POS-word pattern ?number word? + ??
(a common measure word)? can help in segmenting
the character sequence ??|? into the word se-
quence ? (one) ? (measure word) | (person)?
instead of ? (one) ?| (personal; adj)?. More-
over, the comparatively rare POS pattern ?number
word? + ?number word? can help to prevent seg-
menting a long number word into two words.
In order to avoid error propagation and make use
of POS information for word segmentation, segmen-
tation and POS tagging can be viewed as a single
task: given a raw Chinese input sentence, the joint
POS tagger considers all possible segmented and
tagged sequences, and chooses the overall best out-
put. A major challenge for such a joint system is
the large search space faced by the decoder. For a
sentence with n characters, the number of possible
output sequences is O(2n?1 ? Tn), where T is the
size of the tag set. Due to the nature of the com-
bined candidate items, decoding can be inefficient
even with dynamic programming.
Recent research on Chinese POS tagging has
started to investigate joint segmentation and tagging,
reporting accuracy improvements over the pipeline
approach. Various decoding approaches have been
used to reduce the combined search space. Ng and
Low (2004) mapped the joint segmentation and POS
tagging task into a single character sequence tagging
problem. Two types of tags are assigned to each
character to represent its segmentation and POS. For
example, the tag ?b NN? indicates a character at
the beginning of a noun. Using this method, POS
features are allowed to interact with segmentation.
888
Since tagging is restricted to characters, the search
space is reduced to O((4T )n), and beam search de-
coding is effective with a small beam size. How-
ever, the disadvantage of this model is the difficulty
in incorporating whole word information into POS
tagging. For example, the standard ?word + POS
tag? feature is not explicitly applicable. Shi and
Wang (2007) introduced POS information to seg-
mentation by reranking. N -best segmentation out-
puts are passed to a separately-trained POS tagger,
and the best output is selected using the overall POS-
segmentation probability score. In this system, the
decoding for word segmentation and POS tagging
are still performed separately, and exact inference
for both is possible. However, the interaction be-
tween POS and segmentation is restricted by rerank-
ing: POS information is used to improve segmenta-
tion only for the N segmentor outputs.
In this paper, we propose a novel joint model
for Chinese word segmentation and POS tagging,
which does not limiting the interaction between
segmentation and POS information in reducing the
combined search space. Instead, a novel multiple
beam search algorithm is used to do decoding effi-
ciently. Candidate ranking is based on a discrimina-
tive joint model, with features being extracted from
segmented words and POS tags simultaneously. The
training is performed by a single generalized percep-
tron (Collins, 2002). In experiments with the Chi-
nese Treebank data, the joint model gave an error
reduction of 14.6% in segmentation accuracy and
12.2% in the overall segmentation and tagging accu-
racy, compared to the traditional pipeline approach.
In addition, the overall results are comparable to the
best systems in the literature, which exploit knowl-
edge outside the training data, even though our sys-
tem is fully data-driven.
Different methods have been proposed to reduce
error propagation between pipelined tasks, both in
general (Sutton et al, 2004; Daume? III and Marcu,
2005; Finkel et al, 2006) and for specific problems
such as language modeling and utterance classifica-
tion (Saraclar and Roark, 2005) and labeling and
chunking (Shimizu and Haas, 2006). Though our
model is built specifically for Chinese word segmen-
tation and POS tagging, the idea of using the percep-
tron model to solve multiple tasks simultaneously
can be generalized to other tasks.
1 word w
2 word bigram w1w2
3 single-character word w
4 a word of length l with starting character c
5 a word of length l with ending character c
6 space-separated characters c1 and c2
7 character bigram c1c2 in any word
8 the first / last characters c1 / c2 of any word
9 word w immediately before character c
10 character c immediately before word w
11 the starting characters c1 and c2 of two con-
secutive words
12 the ending characters c1 and c2 of two con-
secutive words
13 a word of length l with previous word w
14 a word of length l with next word w
Table 1: Feature templates for the baseline segmentor
2 The Baseline System
We built a two-stage baseline system, using the per-
ceptron segmentation model from our previous work
(Zhang and Clark, 2007) and the perceptron POS tag-
ging model from Collins (2002). We use baseline
system to refer to the system which performs seg-
mentation first, followed by POS tagging (using the
single-best segmentation); baseline segmentor to re-
fer to the segmentor from (Zhang and Clark, 2007)
which performs segmentation only; and baseline
POStagger to refer to the Collins tagger which per-
forms POS tagging only (given segmentation). The
features used by the baseline segmentor are shown in
Table 1. The features used by the POS tagger, some
of which are different to those from Collins (2002)
and are specific to Chinese, are shown in Table 2.
The word segmentation features are extracted
from word bigrams, capturing word, word length
and character information in the context. The word
length features are normalized, with those more than
15 being treated as 15.
The POS tagging features are based on contex-
tual information from the tag trigram, as well as the
neighboring three-word window. To reduce overfit-
ting and increase the decoding speed, templates 4, 5,
6 and 7 only include words with less than 3 charac-
ters. Like the baseline segmentor, the baseline tag-
ger also normalizes word length features.
889
1 tag t with word w
2 tag bigram t1t2
3 tag trigram t1t2t3
4 tag t followed by word w
5 word w followed by tag t
6 word w with tag t and previous character c
7 word w with tag t and next character c
8 tag t on single-character word w in charac-
ter trigram c1wc2
9 tag t on a word starting with char c
10 tag t on a word ending with char c
11 tag t on a word containing char c (not the
starting or ending character)
12 tag t on a word starting with char c0 and
containing char c
13 tag t on a word ending with char c0 and
containing char c
14 tag t on a word containing repeated char cc
15 tag t on a word starting with character cat-
egory g
16 tag t on a word ending with character cate-
gory g
Table 2: Feature templates for the baseline POS tagger
Templates 15 and 16 in Table 2 are inspired by the
CTBMorph feature templates in Tseng et al (2005),
which gave the most accuracy improvement in their
experiments. Here the category of a character is
the set of tags seen on the character during train-
ing. Other morphological features from Tseng et al
(2005) are not used because they require extra web
corpora besides the training data.
During training, the baseline POS tagger stores
special word-tag pairs into a tag dictionary (Ratna-
parkhi, 1996). Such information is used by the de-
coder to prune unlikely tags. For each word occur-
ring more than N times in the training data, the de-
coder can only assign a tag the word has been seen
with in the training data. This method led to im-
provement in the decoding speed as well as the out-
put accuracy for English POS tagging (Ratnaparkhi,
1996). Besides tags for frequent words, our base-
line POS tagger also uses the tag dictionary to store
closed-set tags (Xia, 2000) ? those associated only
with a limited number of Chinese words.
3 Joint Segmentation and Tagging Model
In this section, we build a joint word segmentation
and POS tagging model that uses exactly the same
source of information as the baseline system, by ap-
plying the feature templates from the baseline word
segmentor and POS tagger. No extra knowledge is
used by the joint model. However, because word
segmentation and POS tagging are performed simul-
taneously, POS information participates in word seg-
mentation.
3.1 Formulation of the joint model
We formulate joint word segmentation and POS tag-
ging as a single problem, which maps a raw Chi-
nese sentence to a segmented and POS tagged output.
Given an input sentence x, the output F (x) satisfies:
F (x) = argmax
y?GEN(x)
Score(y)
where GEN(x) represents the set of possible outputs
for x.
Score(y) is computed by a feature-based linear
model. Denoting the global feature vector for the
tagged sentence y with ?(y), we have:
Score(y) = ?(y) ? ~w
where ~w is the parameter vector in the model. Each
element in ~w gives a weight to its corresponding el-
ement in ?(y), which is the count of a particular
feature over the whole sentence y. We calculate the
~w value by supervised learning, using the averaged
perceptron algorithm (Collins, 2002), given in Fig-
ure 1. 1
We take the union of feature templates from the
baseline segmentor (Table 1) and POS tagger (Ta-
ble 2) as the feature templates for the joint system.
All features are treated equally and processed to-
gether according to the linear model, regardless of
whether they are from the baseline segmentor or tag-
ger. In fact, most features from the baseline POS
tagger, when used in the joint model, represent seg-
mentation patterns as well. For example, the afore-
mentioned pattern ?number word? + ???, which is
1In order to provide a comparison for the perceptron algo-
rithm we also tried SVMstruct (Tsochantaridis et al, 2004) for
parameter estimation, but this training method was prohibitively
slow.
890
Inputs: training examples (xi, yi)
Initialization: set ~w = 0
Algorithm:
for t = 1..T , i = 1..N
calculate zi = argmaxy?GEN(xi) ?(y) ? ~w
if zi 6= yi
~w = ~w + ?(yi) ? ?(zi)
Outputs: ~w
Figure 1: The perceptron learning algorithm
useful only for the POS ?number word? in the base-
line tagger, is also an effective indicator of the seg-
mentation of the two words (especially ???) in the
joint model.
3.2 The decoding algorithm
One of the main challenges for the joint segmenta-
tion and POS tagging system is the decoding algo-
rithm. The speed and accuracy of the decoder is
important for the perceptron learning algorithm, but
the system faces a very large search space of com-
bined candidates. Given the linear model and feature
templates, exact inference is very hard even with dy-
namic programming.
Experiments with the standard beam-search de-
coder described in (Zhang and Clark, 2007) resulted
in low accuracy. This beam search algorithm pro-
cesses an input sentence incrementally. At each
stage, the incoming character is combined with ex-
isting partial candidates in all possible ways to gen-
erate new partial candidates. An agenda is used to
control the search space, keeping only the B best
partial candidates ending with the current charac-
ter. The algorithm is simple and efficient, with a
linear time complexity of O(BTn), where n is the
size of input sentence, and T is the size of the tag
set (T = 1 for pure word segmentation). It worked
well for word segmentation alone (Zhang and Clark,
2007), even with an agenda size as small as 8, and
a simple beam search algorithm also works well for
POS tagging (Ratnaparkhi, 1996). However, when
applied to the joint model, it resulted in a reduction
in segmentation accuracy (compared to the baseline
segmentor) even with B as large as 1024.
One possible cause of the poor performance of the
standard beam search method is the combined nature
of the candidates in the search space. In the base-
Input: raw sentence sent ? a list of characters
Variables: candidate sentence item ? a list of
(word, tag) pairs;
maximum word-length record
maxlen for each tag;
the agenda list agendas;
the tag dictionary tagdict;
start index for current word;
end index for current word
Initialization: agendas[0] = [??],
agendas[i] = [] (i! = 0)
Algorithm:
for end index = 1 to sent.length:
foreach tag:
for start index =
max(1, end index ? maxlen[tag] + 1)
to end index:
word = sent[start index..end index]
if (word, tag) consistent with tagdict:
for item ? agendas[start index ? 1]:
item1 = item
item1.append((word,tag))
agendas[end index].insert(item1)
Outputs: agendas[sent.length].best item
Figure 2: The decoding algorithm for the joint word seg-
mentor and POS tagger
line POS tagger, candidates in the beam are tagged
sequences ending with the current word, which can
be compared directly with each other. However, for
the joint problem, candidates in the beam are seg-
mented and tagged sequences up to the current char-
acter, where the last word can be a complete word or
a partial word. A problem arises in whether to give
POS tags to incomplete words. If partial words are
given POS tags, it is likely that some partial words
are ?justified? as complete words by the current POS
information. On the other hand, if partial words are
not given POS tag features, the correct segmentation
for long words can be lost during partial candidate
comparison (since many short completed words with
POS tags are likely to be preferred to a long incom-
plete word with no POS tag features).2
2We experimented with both assigning POS features to par-
tial words and omitting them; the latter method performed better
but both performed significantly worse than the multiple beam
search method described below.
891
Another possible cause is the exponential growth
in the number of possible candidates with increasing
sentence size. The number increases from O(Tn)
for the baseline POS tagger to O(2n?1Tn) for the
joint system. As a result, for an incremental decod-
ing algorithm, the number of possible candidates in-
creases exponentially with the current word or char-
acter index. In the POS tagging problem, a new in-
coming word enlarges the number of possible can-
didates by a factor of T (the size of the tag set).
For the joint problem, however, the enlarging fac-
tor becomes 2T with each incoming character. The
speed of search space expansion is much faster, but
the number of candidates is still controlled by a sin-
gle, fixed-size beam at any stage. If we assume
that the beam is not large enough for all the can-
didates at at each stage, then, from the newly gen-
erated candidates, the baseline POS tagger can keep
1/T for the next processing stage, while the joint
model can keep only 1/2T , and has to discard the
rest. Therefore, even when the candidate compar-
ison standard is ignored, we can still see that the
chance for the overall best candidate to fall out of
the beam is largely increased. Since the search space
growth is exponential, increasing the fixed beam size
is not effective in solving the problem.
To solve the above problems, we developed a mul-
tiple beam search algorithm, which compares candi-
dates only with complete tagged words, and enables
the size of the search space to scale with the input
size. The algorithm is shown in Figure 2. In this
decoder, an agenda is assigned to each character in
the input sentence, recording the B best segmented
and tagged partial candidates ending with the char-
acter. The input sentence is still processed incremen-
tally. However, now when a character is processed,
existing partial candidates ending with any previous
characters are available. Therefore, the decoder enu-
merates all possible tagged words ending with the
current character, and combines each word with the
partial candidates ending with its previous charac-
ter. All input characters are processed in the same
way, and the final output is the best candidate in the
final agenda. The time complexity of the algorithm
is O(WTBn), with W being the maximum word
size, T being the total number of POS tags and n the
number of characters in the input. It is also linear
in the input size. Moreover, the decoding algorithm
gives competent accuracy with a small agenda size
of B = 16.
To further limit the search space, two optimiza-
tions are used. First, the maximum word length
for each tag is recorded and used by the decoder
to prune unlikely candidates. Because the major-
ity of tags only apply to words with length 1 or
2, this method has a strong effect. Development
tests showed that it improves the speed significantly,
while having a very small negative influence on the
accuracy. Second, like the baseline POS tagger, the
tag dictionary is used for Chinese closed set tags and
the tags for frequent words. To words outside the tag
dictionary, the decoder still tries to assign every pos-
sible tag.
3.3 Online learning
Apart from features, the decoder maintains other
types of information, including the tag dictionary,
the word frequency counts used when building the
tag dictionary, the maximum word lengths by tag,
and the character categories. The above data can
be collected by scanning the corpus before training
starts. However, in both the baseline tagger and the
joint POS tagger, they are updated incrementally dur-
ing the perceptron training process, consistent with
online learning.3
The online updating of word frequencies, max-
imum word lengths and character categories is
straightforward. For the online updating of the tag
dictionary, however, the decision for frequent words
must be made dynamically because the word fre-
quencies keep changing. This is done by caching
the number of occurrences of the current most fre-
quent word M , and taking all words currently above
the threshold M/5000 + 5 as frequent words. 5000
is a rough figure to control the number of frequent
words, set according to Zipf?s law. The parameter
5 is used to force all tags to be enumerated before a
word is seen more than 5 times.
4 Related Work
Ng and Low (2004) and Shi and Wang (2007) were
described in the Introduction. Both models reduced
3We took this approach because we wanted the whole train-
ing process to be online. However, for comparison purposes,
we also tried precomputing the above information before train-
ing and the difference in performance was negligible.
892
the large search space by imposing strong restric-
tions on the form of search candidates. In particu-
lar, Ng and Low (2004) used character-based POS
tagging, which prevents some important POS tag-
ging features such as word + POS tag; Shi and Wang
(2007) used an N -best reranking approach, which
limits the influence of POS tagging on segmentation
to the N -best list. In comparison, our joint model
does not impose any hard limitations on the inter-
action between segmentation and POS information.4
Fast decoding speed is achieved by using a novel
multiple-beam search algorithm.
Nakagawa and Uchimoto (2007) proposed a hy-
brid model for word segmentation and POS tagging
using an HMM-based approach. Word information is
used to process known-words, and character infor-
mation is used for unknown words in a similar way
to Ng and Low (2004). In comparison, our model
handles character and word information simultane-
ously in a single perceptron model.
5 Experiments
The Chinese Treebank (CTB) 4 is used for the exper-
iments. It is separated into two parts: CTB 3 (420K
characters in 150K words / 10364 sentences) is used
for the final 10-fold cross validation, and the rest
(240K characters in 150K words / 4798 sentences)
is used as training and test data for development.
The standard F-scores are used to measure both
the word segmentation accuracy and the overall seg-
mentation and tagging accuracy, where the overall
accuracy is TF = 2pr/(p + r), with the precision
p being the percentage of correctly segmented and
tagged words in the decoder output, and the recall r
being the percentage of gold-standard tagged words
that are correctly identified by the decoder. For di-
rect comparison with Ng and Low (2004), the POS
tagging accuracy is also calculated by the percentage
of correct tags on each character.
5.1 Development experiments
The learning curves of the baseline and joint models
are shown in Figure 3, Figure 4 and Figure 5, respec-
tively. These curves are used to show the conver-
4Apart from the beam search algorithm, we do impose some
minor limitations on the search space by methods such as the tag
dictionary, but these can be seen as optional pruning methods
for optimization.
0.88
0.89
0.9
0.91
0.92
1 2 3 4 5 6 7 8 9 10
Number of training iterations
F-
sc
o
re
Figure 3: The learning curve of the baseline segmentor
0.86
0.87
0.88
0.89
0.9
1 2 3 4 5 6 7 8 9 10
Number of training iterations
F-
sc
o
re
Figure 4: The learning curve of the baseline tagger
0.8
0.82
0.84
0.86
0.88
0.9
0.92
1 2 3 4 5 6 7 8 9 10
Number of training iterations
F-
sc
o
re
segmentation accuracy
overall accuracy
Figure 5: The learning curves of the joint system
gence of perceptron and decide the number of train-
ing iterations for the test. It should be noticed that
the accuracies from Figure 4 and Figure 5 are not
comparable because gold-standard segmentation is
used as the input for the baseline tagger. Accord-
ing to the figures, the number of training iterations
893
Tag Seg NN NR VV AD JJ CD
NN 20.47 ? 0.78 4.80 0.67 2.49 0.04
NR 5.95 3.61 ? 0.19 0.04 0.07 0
VV 12.13 6.51 0.11 ? 0.93 0.56 0.04
AD 3.24 0.30 0 0.71 ? 0.33 0.22
JJ 3.09 0.93 0.15 0.26 0.26 ? 0.04
CD 1.08 0.04 0 0 0.07 0 ?
Table 3: Error analysis for the joint model
for the baseline segmentor, POS tagger, and the joint
system are set to 8, 6, and 7, respectively for the re-
maining experiments.
There are many factors which can influence the
accuracy of the joint model. Here we consider the
special character category features and the effect of
the tag dictionary. The character category features
(templates 15 and 16 in Table 2) represent a Chinese
character by all the tags associated with the charac-
ter in the training data. They have been shown to im-
prove the accuracy of a Chinese POS tagger (Tseng
et al, 2005). In the joint model, these features also
represent segmentation information, since they con-
cern the starting and ending characters of a word.
Development tests showed that the overall tagging
F-score of the joint model increased from 84.54% to
84.93% using the character category features. In the
development test, the use of the tag dictionary im-
proves the decoding speed of the joint model, reduc-
ing the decoding time from 416 seconds to 256 sec-
onds. The overall tagging accuracy also increased
slightly, consistent with observations from the pure
POS tagger.
The error analysis for the development test is
shown in Table 3. Here an error is counted when
a word in the standard output is not produced by the
decoder, due to incorrect segmentation or tag assign-
ment. Statistics about the six most frequently mis-
taken tags are shown in the table, where each row
presents the analysis of one tag from the standard
output, and each column gives a wrongly assigned
value. The column ?Seg? represents segmentation
errors. Each figure in the table shows the percentage
of the corresponding error from all the errors.
It can be seen from the table that the NN-VV and
VV-NN mistakes were the most commonly made by
the decoder, while the NR-NN mistakes are also fre-
Baseline Joint
# SF TF TA SF TF TA
1 96.98 92.91 94.14 97.21 93.46 94.66
2 97.16 93.20 94.34 97.62 93.85 94.79
3 95.02 89.53 91.28 95.94 90.86 92.38
4 95.51 90.84 92.55 95.92 91.60 93.31
5 95.49 90.91 92.57 96.06 91.72 93.25
6 93.50 87.33 89.87 94.56 88.83 91.14
7 94.48 89.44 91.61 95.30 90.51 92.41
8 93.58 88.41 90.93 95.12 90.30 92.32
9 93.92 89.15 91.35 94.79 90.33 92.45
10 96.31 91.58 93.01 96.45 91.96 93.45
Av. 95.20 90.33 92.17 95.90 91.34 93.02
Table 4: The accuracies by 10-fold cross validation
SF ? segmentation F-score,
TF ? overall F-score,
TA ? tagging accuracy by character.
quent. These three types of errors significantly out-
number the rest, together contributing 14.92% of all
the errors. Moreover, the most commonly mistaken
tags are NN and VV, while among the most frequent
tags in the corpus, PU, DEG and M had compara-
tively less errors. Lastly, segmentation errors con-
tribute around half (51.47%) of all the errors.
5.2 Test results
10-fold cross validation is performed to test the ac-
curacy of the joint word segmentor and POS tagger,
and to make comparisons with existing models in the
literature. Following Ng and Low (2004), we parti-
tion the sentences in CTB 3, ordered by sentence ID,
into 10 groups evenly. In the nth test, the nth group
is used as the testing data.
Table 4 shows the detailed results for the cross
validation tests, each row representing one test. As
can be seen from the table, the joint model outper-
forms the baseline system in each test.
Table 5 shows the overall accuracies of the base-
line and joint systems, and compares them to the rel-
evant models in the literature. The accuracy of each
model is shown in a row, where ?Ng? represents the
models from Ng and Low (2004) and ?Shi? repre-
sents the models from Shi and Wang (2007). Each
accuracy measure is shown in a column, including
the segmentation F-score (SF ), the overall tagging
894
Model SF TF TA
Baseline+ (Ng) 95.1 ? 91.7
Joint+ (Ng) 95.2 ? 91.9
Baseline+* (Shi) 95.85 91.67 ?
Joint+* (Shi) 96.05 91.86 ?
Baseline (ours) 95.20 90.33 92.17
Joint (ours) 95.90 91.34 93.02
Table 5: The comparison of overall accuracies by 10-fold
cross validation using CTB
+ ? knowledge about sepcial characters,
* ? knowledge from semantic net outside CTB.
F-score (TF ) and the tagging accuracy by characters
(TA). As can be seen from the table, our joint model
achieved the largest improvement over the baseline,
reducing the segmentation error by 14.58% and the
overall tagging error by 12.18%.
The overall tagging accuracy of our joint model
was comparable to but less than the joint model of
Shi and Wang (2007). Despite the higher accuracy
improvement from the baseline, the joint system did
not give higher overall accuracy. One likely reason
is that Shi and Wang (2007) included knowledge
about special characters and semantic knowledge
from web corpora (which may explain the higher
baseline accuracy), while our system is completely
data-driven. However, the comparison is indirect be-
cause our partitions of the CTB corpus are different.
Shi and Wang (2007) also chunked the sentences be-
fore doing 10-fold cross validation, but used an un-
even split. We chose to follow Ng and Low (2004)
and split the sentences evenly to facilitate further
comparison.
Compared with Ng and Low (2004), our baseline
model gave slightly better accuracy, consistent with
our previous observations about the word segmen-
tors (Zhang and Clark, 2007). Due to the large ac-
curacy gain from the baseline, our joint model per-
formed much better.
In summary, when compared with existing joint
word segmentation and POS tagging systems in the
literature, our proposed model achieved the best ac-
curacy boost from the cascaded baseline, and com-
petent overall accuracy.
6 Conclusion and Future Work
We proposed a joint Chinese word segmentation and
POS tagging model, which achieved a considerable
reduction in error rate compared to a baseline two-
stage system.
We used a single linear model for combined word
segmentation and POS tagging, and chose the gen-
eralized perceptron algorithm for joint training. and
beam search for efficient decoding. However, the
application of beam search was far from trivial be-
cause of the size of the combined search space. Mo-
tivated by the question of what are the compara-
ble partial hypotheses in the space, we developed
a novel multiple beam search decoder which effec-
tively explores the large search space. Similar tech-
niques can potentially be applied to other problems
involving joint inference in NLP.
Other choices are available for the decoding of
a joint linear model, such as exact inference with
dynamic programming, provided that the range of
features allows efficient processing. The baseline
feature templates for Chinese segmentation and POS
tagging, when added together, makes exact infer-
ence for the proposed joint model very hard. How-
ever, the accuracy loss from the beam decoder, as
well as alternative decoding algorithms, are worth
further exploration.
The joint system takes features only from the
baseline segmentor and the baseline POS tagger to
allow a fair comparison. There may be additional
features that are particularly useful to the joint sys-
tem. Open features, such as knowledge of numbers
and European letters, and relationships from seman-
tic networks (Shi and Wang, 2007), have been re-
ported to improve the accuracy of segmentation and
POS tagging. Therefore, given the flexibility of the
feature-based linear model, an obvious next step is
the study of open features in the joint segmentor and
POS tagger.
Acknowledgements
We thank Hwee-Tou Ng and Mengqiu Wang for
their helpful discussions and sharing of experimen-
tal data, and the anonymous reviewers for their sug-
gestions. This work is supported by the ORS and
Clarendon Fund.
895
References
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the EMNLP conference, pages 1?8, Philadelphia, PA.
Hal Daume? III and Daniel Marcu. 2005. Learning as
search optimization: Approximate large margin meth-
ods for structured prediction. In Proceedings of the
ICML Conference, pages 169?176, Bonn, Germany.
Jenny Rose Finkel, Christopher D. Manning, and An-
drew Y. Ng. 2006. Solving the problem of cascading
errors: Approximate Bayesian inference for linguistic
annotation pipelines. In Proceedings of the EMNLP
Conference, pages 618?626, Sydney, Australia.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A
hybrid approach to word segmentation and pos tag-
ging. In Proceedings of ACL Demo and Poster Ses-
sion, pages 217?220, Prague, Czech Republic.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese
part-of-speech tagging: One-at-a-time or all-at-once?
Word-based or character-based? In Proceedings of
the EMNLP Conference, pages 277?284, Barcelona,
Spain.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of the
EMNLP Conference, pages 133?142, Philadelphia,
PA.
Murat Saraclar and Brian Roark. 2005. Joint discrimi-
native language modeling and utterance classification.
In Proceedings of the ICASSP Conference, volume 1,
Philadelphia, USA.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer CRF
based joint decoding method for cascade segmentation
and labelling tasks. In Proceedings of the IJCAI Con-
ference, Hyderabad, India.
Nobuyuki Shimizu and Andrew Haas. 2006. Exact de-
coding for jointly labeling and chunking sequences. In
Proceedings of the COLING/ACL Conference, Poster
Sessions, Sydney, Australia.
Charles Sutton, Khashayar Rohanimanesh, and Andrew
McCallum. 2004. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. In Proceedings of the
ICML Conference, Banff, Canada.
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morphological features help POS tagging
of unknown words across language varieties. In Pro-
ceedings of the Fourth SIGHAN Workshop, Jeju Island,
Korea.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In Proceedings of
the ICML Conference, Banff, Canada.
Fei Xia. 2000. The part-of-speech tagging guidelines for
the Chinese Treebank (3.0). IRCS Report, University
of Pennsylvania.
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-based perceptron algorithm. In
Proceedings of the ACL Conference, pages 840?847,
Prague, Czech Republic.
896
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 162?171,
Paris, October 2009. c?2009 Association for Computational Linguistics
Transition-Based Parsing of the Chinese Treebank using a Global
Discriminative Model
Yue Zhang
Oxford University
Computing Laboratory
yue.zhang@comlab.ox.ac.uk
Stephen Clark
Cambridge University
Computer Laboratory
stephen.clark@cl.cam.ac.uk
Abstract
Transition-based approaches have shown
competitive performance on constituent
and dependency parsing of Chinese. State-
of-the-art accuracies have been achieved
by a deterministic shift-reduce parsing
model on parsing the Chinese Treebank 2
data (Wang et al, 2006). In this paper,
we propose a global discriminative model
based on the shift-reduce parsing process,
combined with a beam-search decoder, ob-
taining competitive accuracies on CTB2.
We also report the performance of the
parser on CTB5 data, obtaining the highest
scores in the literature for a dependency-
based evaluation.
1 Introduction
Transition-based statistical parsing associates
scores with each decision in the parsing process,
selecting the parse which is built by the highest
scoring sequence of decisions (Briscoe and Car-
roll, 1993; Nivre et al, 2006). The parsing algo-
rithm is typically some form of bottom-up shift-
reduce algorithm, so that scores are associated
with actions such as shift and reduce. One ad-
vantage of this approach is that the parsing can be
highly efficient, for example by pursuing a greedy
strategy in which a single action is chosen at each
decision point.
The alternative approach, exemplified by
Collins (1997) and Charniak (2000), is to use
a chart-based algorithm to build the space of
possible parses, together with pruning of low-
probability constituents and the Viterbi algorithm
to find the highest scoring parse. For English de-
pendency parsing, the two approaches give similar
results (McDonald et al, 2005; Nivre et al, 2006).
For English constituent-based parsing using the
Penn Treebank, the best performing transition-
based parser lags behind the current state-of-the-
art (Sagae and Lavie, 2005). In contrast, for Chi-
nese, the best dependency parsers are currently
transition-based (Duan et al, 2007; Zhang and
Clark, 2008). For constituent-based parsing using
the Chinese Treebank (CTB), Wang et al (2006)
have shown that a shift-reduce parser can give
competitive accuracy scores together with high
speeds, by using an SVM to make a single decision
at each point in the parsing process.
In this paper we describe a global discrimina-
tive model for Chinese shift-reduce parsing, and
compare it with Wang et al?s approach. We ap-
ply the same shift-reduce procedure as Wang et
al. (2006), but instead of using a local classifier
for each transition-based action, we train a gener-
alized perceptron model over complete sequences
of actions, so that the parameters are learned in
the context of complete parses. We apply beam
search to decoding instead of greedy search. The
parser still operates in linear time, but the use of
beam-search allows the correction of local deci-
sion errors by global comparison. Using CTB2,
our model achieved Parseval F-scores comparable
to Wang et al?s approach. We also present accu-
racy scores for the much larger CTB5, using both
a constituent-based and dependency-based evalu-
ation. The scores for the dependency-based eval-
uation were higher than the state-of-the-art depen-
dency parsers for the CTB5 data.
2 The Shift-Reduce Parsing Process
The shift-reduce process used by our beam-search
decoder is based on the greedy shift-reduce parsers
of Sagae and Lavie (2005) and Wang et al (2006).
162
The process assumes binary-branching trees; sec-
tion 2.1 explains how these are obtained from the
arbitrary-branching trees in the Chinese Treebank.
The input is assumed to be segmented and POS
tagged, and the word-POS pairs waiting to be pro-
cessed are stored in a queue. A stack holds the
partial parse trees that are built during the parsing
process. A parse state is defined as a ?stack,queue?
pair. Parser actions, including SHIFT and various
kinds of REDUCE, define functions from states to
states by shifting word-POS pairs onto the stack
and building partial parse trees.
The actions used by the parser are:
? SHIFT, which pushes the next word-POS pair
in the queue onto the stack;
? REDUCE?unary?X, which makes a new
unary-branching node with label X; the stack
is popped and the popped node becomes the
child of the new node; the new node is pushed
onto the stack;
? REDUCE?binary?{L/R}?X, which makes a
new binary-branching node with label X; the
stack is popped twice, with the first popped
node becoming the right child of the new
node and the second popped node becoming
the left child; the new node is pushed onto the
stack;
? TERMINATE, which pops the root node off
the stack and ends parsing. This action
is novel in our parser. Sagae and Lavie
(2005) and Wang et al (2006) only used the
first three transition actions, setting the fi-
nal state as all incoming words having been
processed, and the stack containing only one
node. However, there are a small number of
sentences (14 out of 3475 from the training
data) that have unary-branching roots. For
these sentences, Wang?s parser will be unable
to produce the unary-branching roots because
the parsing process terminates as soon as the
root is found. We define a separate action to
terminate parsing, allowing unary reduces to
be applied to the root item before parsing fin-
ishes.
The trees built by the parser are lexicalized, us-
ing the head-finding rules from Zhang and Clark
(2008). The left (L) and right (R) versions of the
REDUCE-binary rules indicate whether the head of
for node Y = X1..Xm ? T :
if m > 2 :
find the head node Xk(1 ? k ? m) of Y
m? = m
while m? > k and m? > 2 :
new node Y ? = X1..Xm??1
Y ? Y ?Xm?
m? = m? ? 1
n? = 1
while n? < k and k ? n? > 1 :
new node Y ? = Xn? ..Xk
Y ? Xn?Y ?
n? = n? + 1
Figure 2: the binarization algorithm with input T
the new node is to be taken from the left or right
child. Note also that, since the parser is building
binary trees, the X label in the REDUCE rules can
be one of the temporary constituent labels, such
as NP?, which are needed for the binarization pro-
cess described in Section 2.1. Hence the number
of left and right binary reduce rules is the number
of constituent labels in the binarized grammar.
Wang et al (2006) give a detailed example
showing how a segmented and POS-tagged sen-
tence can be incrementally processed using the
shift-reduce actions to produce a binary tree. We
show this example in Figure 1.
2.1 The binarization process
The algorithm in Figure 2 is used to map CTB
trees into binarized trees, which are required by
the shift-reduce parsing process. For any tree node
with more than two child nodes, the algorithm
works by first finding the head node, and then pro-
cessing its right-hand-side and left-hand-side, re-
spectively. The head-finding rules are taken from
Zhang and Clark (2008). Y = X1..Xm represents
a tree node Y with child nodes X1...Xm(m ? 1).
The label of the newly generated node Y ? is
based on the constituent label of the original node
Y , but marked with an asterix. Hence binariza-
tion enlarges the set of constituent labels. We
call the constituents marked with ? temporary con-
stituents. The binarization process is reversible, in
that output from the shift-reduce parser can be un-
binarized into CTB format, which is required for
evaluation.
163
Figure 1: An example shift-reduce parsing process, adopted from Wang et al (2006)
2.2 Restrictions on the sequence of actions
Not all sequences of actions produce valid bina-
rized trees. In the deterministic parser of Wang et
al. (2006), the highest scoring action predicted by
the classifier may prevent a valid binary tree from
being built. In this case, Wang et al simply return
a partial parse consisting of all the subtrees on the
stack.
In our parser a set of restrictions is applied
which guarantees a valid parse tree. For example,
two simple restrictions are that a SHIFT action can
only be applied if the queue of incoming words
164
Variables: state item item = (S,Q), where
S is stack and Q is incoming queue;
the agenda agenda;
list of state items next;
Algorithm:
for item ? agenda:
if item.score = agenda.bestScore and
item.isFinished:
rval = item
break
next = []
for move ? item.legalMoves:
next.push(item.TakeAction(move))
agenda = next.getBBest()
Outputs: rval
Figure 3: the beam-search decoding algorithm
is non-empty, and the binary reduce actions can
only be performed if the stack contains at least two
nodes. Some of the restrictions are more complex
than this; the full set is listed in the Appendix.
3 Decoding with Beam Search
Our decoder is based on the incremental shift-
reduce parsing process described in Section 2. We
apply beam-search, keeping the B highest scoring
state items in an agenda during the parsing pro-
cess. The agenda is initialized with a state item
containing the starting state, i.e. an empty stack
and a queue consisting of all word-POS pairs from
the sentence.
At each stage in the decoding process, existing
items from the agenda are progressed by applying
legal parsing actions. From all newly generated
state items, the B highest scoring are put back on
the agenda. The decoding process is terminated
when the highest scored state item in the agenda
reaches the final state. If multiple state items have
the same highest score, parsing terminates if any
of them are finished. The algorithm is shown in
Figure 3.
4 Model and Learning Algorithm
We use a linear model to score state items. Recall
that a parser state is a ?stack,queue? pair, with the
stack holding subtrees and the queue holding in-
coming words waiting to be processed. The score
Inputs: training examples (xi, yi)
Initialization: set ~w = 0
Algorithm:
for t = 1..T , i = 1..N :
zi = parse(xi, ~w)
if zi 6= yi:
~w = ~w +?(yi)? ?(zi)
Outputs: ~w
Figure 4: the perceptron learning algorithm
for state item Y is defined by:
Score(Y ) = ~w ? ?(Y ) =?
i
?i fi(Y )
where ?(Y ) is the global feature vector from Y ,
and ~w is the weight vector defined by the model.
Each element from ?(Y ) represents the global
count of a particular feature from Y . The feature
set consists of a large number of features which
pick out various configurations from the stack and
queue, based on the words and subtrees in the state
item. The features are described in Section 4.1.
The weight values are set using the generalized
perceptron algorithm (Collins, 2002).
The perceptron algorithm is shown in Figure 4.
It initializes weight values as all zeros, and uses
the current model to decode training examples (the
parse function in the pseudo-code). If the output
is correct, it passes on to the next example. If
the output is incorrect, it adjusts the weight val-
ues by adding the feature vector from the gold-
standard output and subtracting the feature vector
from the parser output. Weight values are updated
for each example (making the process online) and
the training data is iterated over T times. In or-
der to avoid overfitting we used the now-standard
averaged version of this algorithm (Collins, 2002).
We also apply the early update modification
from Collins and Roark (2004). If the agenda, at
any point during the decoding process, does not
contain the correct partial parse, it is not possible
for the decoder to produce the correct output. In
this case, decoding is stopped early and the weight
values are updated using the highest scoring par-
tial parse on the agenda.
4.1 Feature set
Table 1 shows the set of feature templates for the
model. Individual features are generated from
165
Description Feature templates
Unigrams S0tc, S0wc, S1tc, S1wc,
S2tc, S2wc, S3tc, S3wc,
N0wt, N1wt, N2wt, N3wt,
S0lwc, S0rwc, S0uwc,
S1lwc, S1rwc, S1uwc,
Bigrams S0wS1w, S0wS1c, S0cS1w, S0cS1c,
S0wN0w, S0wN0t, S0cN0w, S0cN0t,
N0wN1w, N0wN1t, N0tN1w, N0tN1t
S1wN0w, S1wN0t, S1cN0w, S1cN0t,
Trigrams S0cS1cS2c, S0wS1cS2c,
S0cS1wS2c, S0cS1cS2w,
S0cS1cN0t, S0wS1cN0t,
S0cS1wN0t, S0cS1cN0w
Bracket S0wb, S0cb
S0wS1cb, S0cS1wb, S0cS1cb
S0wN0tb, S0cN0wb, S0cN0tb
Separator S0wp, S0wcp, S0wq, S0wcq,
S1wp, S1wcp, S1wq, S1wcq
S0cS1cp, S0cS1cq
Table 1: Feature templates
these templates by first instantiating a template
with particular labels, words and tags, and then
pairing the instantiated template with a particu-
lar action. In the table, the symbols S0, S1, S2,
and S3 represent the top four nodes on the stack,
and the symbols N0, N1, N2 and N3 represent the
first four words in the incoming queue. S0L, S0R
and S0U represent the left and right child for bi-
nary branching S0, and the single child for unary
branching S0, respectively; w represents the lex-
ical head token for a node; c represents the label
for a node. When the corresponding node is a ter-
minal, c represents its POS-tag, whereas when the
corresponding node is non-terminal, c represents
its constituent label; t represents the POS-tag for a
word.
The context S0, S1, S2, S3 and N0, N1, N2, N3
for the feature templates is taken from Wang et al
(2006). However, Wang et al (2006) used a poly-
nomial kernel function with an SVM and did not
manually create feature combinations. Since we
used the linear perceptron algorithm we manually
combined Unigram features into Bigram and Tri-
gram features.
The ?Bracket? row shows bracket-related fea-
tures, which were inspired by Wang et al (2006).
Here brackets refer to left brackets including ???,
??? and ??? and right brackets including ???,
??? and ???. In the table, b represents the
matching status of the last left bracket (if any)
on the stack. It takes three different values:
1 (no matching right bracket has been pushed
onto stack), 2 (a matching right bracket has been
pushed onto stack) and 3 (a matching right bracket
has been pushed onto stack, but then popped off).
The ?Separator? row shows features that in-
clude one of the separator punctuations (i.e. ???,
???, ??? and ???) between the head words of
S0 and S1. These templates apply only when
the stack contains at least two nodes; p repre-
sents a separator punctuation symbol. Each unique
separator punctuation between S0 and S1 is only
counted once when generating the global feature
vector. q represents the count of any separator
punctuation between S0 and S1.
Whenever an action is being considered at each
point in the beam-search process, templates from
Table 1 are matched with the context defined by
the parser state and combined with the action to
generate features. Negative features, which are the
features from incorrect parser outputs but not from
any training example, are included in the model.
There are around a million features in our experi-
ments with the CTB2 dataset.
Wang et al (2006) used a range of other fea-
tures, including rhythmic features of S0 and S1
(Sun and Jurafsky, 2003), features from the most
recently found node that is to the left or right of S0
and S1, the number of words and the number of
punctuations in S0 and S1, the distance between
S0 and S1 and so on. We did not include these
features in our parser, because they did not lead to
improved performance during development exper-
iments.
5 Experiments
The experiments were performed using the Chi-
nese Treebank 2 and Chinese Treebank 5 data.
Standard data preparation was performed before
the experiments: empty terminal nodes were re-
moved; any non-terminal nodes with no children
were removed; any unary X ? X nodes resulting
from the previous steps were collapsed into one X
node.
For all experiments, we used the EVALB tool1
for evaluation, and used labeled recall (LR), la-
beled precision (LP ) and F1 score (which is the
1http://nlp.cs.nyu.edu/evalb/
166
Figure 5: The influence of beam-size
Sections Sentences Words
Training 001?270 3475 85,058
Development 301?325 355 6,821
Test 271?300 348 8,008
Table 2: The standard split of CTB2 data
harmonic mean of LR and LP ) to measure pars-
ing accuracy.
5.1 The influence of beam-size
Figure 5 shows the accuracy curves using differ-
ent beam-sizes for the decoder. The number of
training iterations is on the x-axis with F -score
on the y-axis. The tests were performed using
the development test data and gold-standard POS-
tags. The figure shows the benefit of using a beam
size greater than 1, with comparatively little accu-
racy gain being obtained beyond a beam size of 8.
Hence we set the beam size to 16 for the rest of the
experiments.
5.2 Test results on CTB2
The experiments in this section were performed
using CTB2 to allow comparison with previous
work, with the CTB2 data extracted from Chinese
Treebank 5 (CTB5). The data was split into train-
ing, development test and test sets, as shown in Ta-
ble 2, which is consistent with Wang et al (2006)
and earlier work. The tests were performed us-
ing both gold-standard POS-tags and POS-tags au-
tomatically assigned by a POS-tagger. We used our
Model LR LP F1
Bikel Thesis 80.9% 84.5% 82.7%
Wang 2006 SVM 87.2% 88.3% 87.8%
Wang 2006 Stacked 88.3% 88.1% 88.2%
Our parser 89.4% 90.1% 89.8%
Table 3: Accuracies on CTB2 with gold-standard
POS-tags
own implementation of the perceptron-based tag-
ger from Collins (2002).
The results of various models measured using
sentences with less than 40 words and using gold-
standard POS-tags are shown in Table 3. The
rows represent the model from Bikel and Chiang
(2000), Bikel (2004), the SVM and ensemble mod-
els from Wang et al (2006), and our parser, re-
spectively. The accuracy of our parser is competi-
tive using this test set.
The results of various models using automati-
cally assigned POS-tags are shown in Table 4. The
rows in the table represent the models from Bikel
and Chiang (2000), Levy and Manning (2003),
Xiong et al (2005), Bikel (2004), Chiang and
Bikel (2002), the SVM model from Wang et al
(2006) and the ensemble system from Wang et
al. (2006), and the parser of this paper, respec-
tively. Our parser gave comparable accuracies to
the SVM and ensemble models from Wang et al
(2006). However, comparison with Table 3 shows
that our parser is more sensitive to POS-tagging er-
rors than some of the other models. One possible
reason is that some of the other parsers, e.g. Bikel
(2004), use the parser model itself to resolve tag-
ging ambiguities, whereas we rely on a POS tag-
ger to accurately assign a single tag to each word.
In fact, for the Chinese data, POS tagging accu-
racy is not very high, with the perceptron-based
tagger achieving an accuracy of only 93%. The
beam-search decoding framework we use could
accommodate joint parsing and tagging, although
the use of features based on the tags of incom-
ing words complicates matters somewhat, since
these features rely on tags having been assigned to
all words in a pre-processing step. We leave this
problem for future work.
In a recent paper, Petrov and Klein (2007) re-
ported LR and LP of 85.7% and 86.9% for sen-
tences with less than 40 words and 81.9% and
84.8% for all sentences on the CTB2 test set, re-
167
? 40 words ? 100 words Unlimited
LR LP F1 POS LR LP F1 POS LR LP F1 POS
Bikel 2000 76.8% 77.8% 77.3% - 73.3% 74.6% 74.0% - - - - -
Levy 2003 79.2% 78.4% 78.8% - - - - - - - - -
Xiong 2005 78.7% 80.1% 79.4% - - - - - - - - -
Bikel Thesis 78.0% 81.2% 79.6% - 74.4% 78.5% 76.4% - - - - -
Chiang 2002 78.8% 81.1% 79.9% - 75.2% 78.0% 76.6% - - - - -
Wang 2006 SVM 78.1% 81.1% 79.6% 92.5% 75.5% 78.5% 77.0% 92.2% 75.0% 78.0% 76.5% 92.1%
Wang 2006 Stacked 79.2% 81.1% 80.1% 92.5% 76.7% 78.4% 77.5% 92.2% 76.2% 78.0% 77.1% 92.1%
Our parser 80.2% 80.5% 80.4% 93.5% 76.5% 77.7% 77.1% 93.1% 76.1% 77.4% 76.7% 93.0%
Table 4: Accuracies on CTB2 with automatically assigned tags
? 40 words Unlimited
LR LP F1 POS LR LP F1 POS
87.9% 87.5% 87.7% 100% 86.9% 86.7% 86.8% 100%
80.2% 79.1% 79.6% 94.1% 78.6% 78.0% 78.3% 93.9%
Table 5: Accuracies on CTB5 using gold-standard and automatically assigned POS-tags
Sections Sentences Words
Set A 001?270 3,484 84,873
Set B Set A; 400?699 6,567 161,893
Set C Set B; 700?931 9,707 236,051
Table 6: Training sets with different sizes
spectively. These results are significantly better
than any model from Table 4. However, we did
not include their scores in the table because they
used a different training set from CTB5, which is
much larger than the CTB2 training set used by all
parsers in the table. In order to make a compari-
son, we split the data in the same way as Petrov
and Klein (2007) and tested our parser using auto-
matically assigned POS-tags. It gave LR and LP
of 82.0% and 80.9% for sentences with less than
40 words and 77.8% and 77.4% for all sentences,
significantly lower than Petrov and Klein (2007),
which we partly attribute to the sensitivity of our
parser to pos tag errors (see Table 5).
5.3 The effect of training data size
CTB2 is a relatively small corpus, and so we in-
vestigated the effect of adding more training data
from CTB5. Intuitively, more training data leads
to higher parsing accuracy. By using increased
amount of training sentences (Table 6) from CTB5
with the same development test data (Table 2),
we draw the accuracy curves with different num-
ber of training iterations (Figure 6). This exper-
iment confirmed that the accuracy increases with
the amount of training data.
Figure 6: The influence of the size of training data
Another motivation for us to use more training
data is to reduce overfitting. We invested consid-
erable effort into feature engineering using CTB2,
and found that a small variation of feature tem-
plates (e.g. changing the feature template S0cS1c
from Table 1 to S0tcS1tc) can lead to a compar-
atively large change (up to 1%) in the accuracy.
One possible reason for this variation is the small
size of the CTB2 training data. When performing
experiments using the larger set B from Table 6,
we observed improved stability relative to small
feature changes.
168
Sections Sentences Words
Training 001?815; 16,118 437,8591001?1136
Dev 886?931; 804 20,4531148?1151
Test 816?885; 1,915 50,3191137?1147
Table 7: Standard split of CTB5 data
Non-root Root Complete
Zhang 2008 86.21% 76.26% 34.41%
Our parser 86.95% 79.19% 36.08%
Table 8: Comparison with state-of-the-art depen-
dency parsing using CTB5 data
5.4 Test accuracy using CTB5
Table 5 presents the performance of the parser on
CTB5. We adopt the data split from Zhang and
Clark (2008), as shown in Table 7. We used the
same parser configurations as Section 5.2.
As an additional evaluation we also produced
dependency output from the phrase-structure
trees, using the head-finding rules, so that we
can also compare with dependency parsers, for
which the highest scores in the literature are cur-
rently from our previous work in Zhang and Clark
(2008). We compare the dependencies read off our
constituent parser using CTB5 data with the depen-
dency parser from Zhang and Clark (2008). The
same measures are taken and the accuracies with
gold-standard POS-tags are shown in Table 8. Our
constituent parser gave higher accuracy than the
dependency parser. It is interesting that, though
the constituent parser uses many fewer feature
templates than the dependency parser, the features
do include constituent information, which is un-
available to dependency parsers.
6 Related work
Our parser is based on the shift-reduce parsing
process from Sagae and Lavie (2005) and Wang
et al (2006), and therefore it can be classified
as a transition-based parser (Nivre et al, 2006).
An important difference between our parser and
the Wang et al (2006) parser is that our parser
is based on a discriminative learning model with
global features, whilst the parser from Wang et al
(2006) is based on a local classifier that optimizes
each individual choice. Instead of greedy local de-
coding, we used beam search in the decoder.
An early work that applies beam search to con-
stituent parsing is Ratnaparkhi (1999). The main
difference between our parser and Ratnaparkhi?s is
that we use a global discriminative model, whereas
Ratnaparkhi?s parser has separate probabilities of
actions chained together in a conditional model.
Both our parser and the parser from Collins and
Roark (2004) use a global discriminative model
and an incremental parsing process. The major
difference is the use of different incremental pars-
ing processes. To achieve better performance for
Chinese parsing, our parser is based on the shift-
reduce parsing process. In addition, we did not in-
clude a generative baseline model in the discrimi-
native model, as did Collins and Roark (2004).
Our parser in this paper shares similarity
with our transition-based dependency parser from
Zhang and Clark (2008) in the use of a discrimina-
tive model and beam search. The main difference
is that our parser in this paper is for constituent
parsing. In fact, our parser is one of only a few
constituent parsers which have successfully ap-
plied global discriminative models, certainly with-
out a generative baseline as a feature, whereas
global models for dependency parsing have been
comparatively easier to develop.
7 Conclusion
The contributions of this paper can be summarized
as follows. First, we defined a global discrimina-
tive model for Chinese constituent-based parsing,
continuing recent work in this area which has fo-
cused on English (Clark and Curran, 2007; Car-
reras et al, 2008; Finkel et al, 2008). Second, we
showed how such a model can be applied to shift-
reduce parsing and combined with beam search,
resulting in an accurate linear-time parser. In stan-
dard tests using CTB2 data, our parser achieved
comparable Parseval F-score to the state-of-the-
art systems. Moreover, we observed that more
training data lead to improvements on both accu-
racy and stability against feature variations, and
reported performance of the parser using CTB5
data. By converting constituent-based output to
dependency relations using standard head-finding
rules, our parser also obtained the highest scores
for a CTB5 dependency evaluation in the literature.
Due to the comparatively low accuracy for Chi-
nese POS-tagging, the parsing accuracy dropped
169
significantly when using automatically assigned
POS-tags rather than gold-standard POS-tags. In
our further work, we plan to investigate possible
methods of joint POS-tagging and parsing under
the discriminative model and beam-search frame-
work.
A discriminative model allows consistent train-
ing of a wide range of different features. We
showed in Zhang and Clark (2008) that it was pos-
sible to combine graph and transition-based de-
pendency parser into the same global discrimina-
tive model. Our parser framework in this paper
allows the same integration of graph-based fea-
tures. However, preliminary experiments with fea-
tures based on graph information did not show
accuracy improvements for our parser. One pos-
sible reason is that the transition actions for the
parser in this paper already include graph infor-
mation, such as the label of the newly gener-
ated constituent, while for the dependency parser
in Zhang and Clark (2008), transition actions do
not contain graph information, and therefore the
use of transition-based features helped to make
larger improvements in accuracy. The integration
of graph-based features for our shift-reduce con-
stituent parser is worth further study.
The source code of our parser is publicly avail-
able at http://www.sourceforge.net/projects/zpar.2
Appendix
The set of restrictions which ensures a valid binary
tree is shown below. The restriction on the num-
ber of consecutive unary rule applications is taken
from Sagae and Lavie (2005); it prevents infinite
running of the parser by repetitive use of unary re-
duce actions, and ensures linear time complexity
in the length of the sentence.
? the shift action can only be performed when
the queue of incoming words is not empty;
? when the node on top of the stack is tempo-
rary and its head word is from the right child,
no shift action can be performed;
? the unary reduce actions can be performed
only when the stack is not empty;
? a unary reduce with the same constituent la-
bel (Y ? Y ) is not allowed;
? no more than three unary reduce actions can
be performed consecutively;
2The make target for the parser in this paper is chi-
nese.conparser.
? the binary reduce actions can only be per-
formed when the stack contains at least two
nodes, with at least one of the two nodes on
top of stack (with R being the topmost and L
being the second) being non-temporary;
? if L is temporary with label X?, the result-
ing node must be labeled X or X? and left-
headed (i.e. to take the head word from L);
similar restrictions apply when R is tempo-
rary;
? when the incoming queue is empty and the
stack contains only two nodes, binary reduce
can be applied only if the resulting node is
non-temporary;
? when the stack contains only two nodes, tem-
porary resulting nodes from binary reduce
must be left-headed;
? when the queue is empty and the stack con-
tains more than two nodes, with the third
node from the top being temporary, binary re-
duce can be applied only if the resulting node
is non-temporary;
? when the stack contains more than two nodes,
with the third node from the top being tempo-
rary, temporary resulting nodes from binary
reduce must be left-headed;
? the terminate action can be performed when
the queue is empty, and the stack size is one.
170
References
Daniel M. Bikel and David Chiang. 2000. Two sta-
tistical parsing models applied to the Chinese Tree-
bank. In Proceedings of SIGHAN Workshop, pages
1?6, Morristown, NJ, USA.
Daniel M. Bikel. 2004. On the Parameter Space of
Generative Lexicalized Statistical Parsing Models.
Ph.D. thesis, University of Pennsylvania.
Ted Briscoe and John Carroll. 1993. Generalized prob-
abilistic LR parsing of natural language (corpora)
with unification-based grammars. Computational
Linguistics, 19(1):25?59.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. Tag, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of CoNLL, pages 9?16, Manchester, England,
August.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL, pages
132?139, Seattle, WA.
David Chiang and Daniel M. Bikel. 2002. Recovering
latent information in treebanks. In Proceedings of
COLING.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL, pages 111?118, Barcelona, Spain, July.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of
the 35th Meeting of the ACL, pages 16?23, Madrid,
Spain.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1?8, Philadelphia, USA, July.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic models for action-based Chinese dependency
parsing. In Proceedings of ECML/ECPPKDD, War-
saw, Poland, September.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, con-
ditional random field parsing. In Proceedings of
ACL/HLT, pages 959?967, Columbus, Ohio, June.
Association for Computational Linguistics.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese Treebank?
In Proceedings of ACL, pages 439?446, Sapporo,
Japan, July.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, pages 91?
98, Ann Arbor, Michigan, June.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?en
Eryig?it, and Svetoslav Marinov. 2006. Labeled
pseudo-projective dependency parsing with support
vector machines. In Proceedings of CoNLL, pages
221?225, New York City, June.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
HLT/NAACL, pages 404?411, Rochester, New York,
April. Association for Computational Linguistics.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1-3):151?175.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of IWPT, pages 125?132, Vancouver, British
Columbia, October.
Honglin Sun and Daniel Jurafsky. 2003. The effect of
rhythm on structural disambiguation in Chinese. In
Proceedings of SIGHAN Workshop.
Mengqiu Wang, Kenji Sagae, and Teruko Mitamura.
2006. A fast, accurate deterministic parser for Chi-
nese. In Proceedings of COLING/ACL, pages 425?
432, Sydney, Australia.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin,
and Yueliang Qian. 2005. Parsing the Penn Chinese
Treebank with semantic knowledge. In Proceedings
of IJCNLP.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of EMNLP, pages
562?571, Hawaii, USA, October.
171
Coling 2010: Poster Volume, pages 1471?1479,
Beijing, August 2010
Chart Pruning for Fast Lexicalised-Grammar Parsing
Yue Zhanga? Byung-Gyu Ahn b? Stephen Clarka? Curt Van Wyk c
James R. Currand Laura Rimella
Computer Laboratorya Computer Scienceb Computer Sciencec School of ITd
Cambridge Johns Hopkins Northwestern College Sydney
{yue.zhang,stephen.clark}@cl.cam.ac.uka? bahn@jhu.edu b?
Abstract
Given the increasing need to process mas-
sive amounts of textual data, efficiency of
NLP tools is becoming a pressing concern.
Parsers based on lexicalised grammar for-
malisms, such as TAG and CCG, can be
made more efficient using supertagging,
which for CCG is so effective that every
derivation consistent with the supertagger
output can be stored in a packed chart.
However, wide-coverage CCG parsers still
produce a very large number of deriva-
tions for typical newspaper or Wikipedia
sentences. In this paper we investigate
two forms of chart pruning, and develop a
novel method for pruning complete cells
in a parse chart. The result is a wide-
coverage CCG parser that can process al-
most 100 sentences per second, with lit-
tle or no loss in accuracy over the baseline
with no pruning.
1 Introduction
Many NLP tasks and applications require the pro-
cessing of massive amounts of textual data. For
example, knowledge acquisition efforts can in-
volve processing billions of words of text (Cur-
ran, 2004). Also, the increasing need to process
large amounts of web data places an efficiency
demand on existing NLP tools. TextRunner, for
example, is a system that performs open infor-
mation extraction on the web (Lin et al, 2009).
However, the text processing that is performed by
TextRunner, in particular the parsing, is rudimen-
tary: finite-state shallow parsing technology that
is now decades old. TextRunner uses this technol-
ogy largely for efficiency reasons.
Many of the popular wide-coverage parsers
available today operate at around one newspa-
per sentence per second (Collins, 1999; Charniak,
2000; Petrov and Klein, 2007). There are de-
pendency parsers that operate orders of magni-
tude faster, by exploiting the fact that accurate
dependency parsing can be achieved by using a
shift-reduce linear-time process which makes a
single decision at each point in the parsing pro-
cess (Nivre and Scholz, 2004).
In this paper we focus on the Combinatory Cat-
egorial Grammar (CCG) parser of Clark and Cur-
ran (2007). One advantage of the CCG parser is
that it is able to assign rich structural descriptions
to sentences, from a variety of representations,
e.g. CCG derivations, CCG dependency structures,
grammatical relations (Carroll et al, 1998), and
first-order logical forms (Bos et al, 2004). One
of the properties of the grammar formalism is
that it is lexicalised, associating CCG lexical cate-
gories, or CCG supertags, with the words in a sen-
tence (Steedman, 2000). Clark and Curran (2004)
adapt the technique of supertagging (Bangalore
and Joshi, 1999) to CCG, using a standard max-
imum entropy tagger to assign small sets of su-
pertags to each word. The reduction in ambiguity
resulting from the supertagging stage results in a
surprisingly efficient parser, given the rich struc-
tural output, operating at tens of newspaper sen-
tences per second.
In this paper we demonstrate that the CCG
parser can be made more than twice as fast, with
little or no loss in accuracy. A noteworthy feature
of the CCG parser is that, after the supertagging
1471
stage, the parser builds a complete packed chart,
storing all sentences consistent with the assigned
supertags and the parser?s CCG combinatory rules,
with no chart pruning whatsoever. The use of
chart pruning techniques, typically some form of
beam search, is essential for practical parsing us-
ing Penn Treebank parsers (Collins, 1999; Petrov
and Klein, 2007; Charniak and Johnson, 2005), as
well as practical parsers based on linguistic for-
malisms, such as HPSG (Ninomiya et al, 2005)
and LFG (Kaplan et al, 2004). However, in the
CCG case, the use of the supertagger means that
enough ambiguity has already been resolved to al-
low the complete chart to be represented.
Despite the effectiveness of the supertagging
stage, the number of derivations stored in a packed
chart can still be enormous for typical newspa-
per sentences. Hence it is an obvious question
whether chart pruning techniques can be prof-
itably applied to the CCG parser. Some previous
work (Djordjevic et al, 2007) has investigated this
question but with little success.
In this paper we investigate two types of chart
pruning: a standard beam search, similar to that
used in the Collins parser (Collins, 1999), and a
more aggressive strategy in which complete cells
are pruned, following Roark and Hollingshead
(2009). Roark and Hollingshead use a finite-state
tagger to decide which words in a sentence can
end or begin constituents, from which whole cells
in the chart can be removed. We develop a novel
extension to this approach, in which a tagger is
trained to infer the maximum length constituent
that can begin or end at a particular word. These
lengths can then be used in a more agressive prun-
ing strategy which we show to be significantly
more effective than the basic approach.
Both beam search and cell pruning are highly
effective, with the resulting CCG parser able to
process almost 100 sentences per second using
a single CPU, for both newspaper and Wikipedia
data, with little or no loss in accuracy.
2 The CCG Parser
The parser is described in detail in Clark and Cur-
ran (2007). It is based on CCGbank, a CCG ver-
sion of the Penn Treebank developed by Hocken-
maier and Steedman (2007).
The stages in the parsing pipeline are as fol-
lows. First, a POS tagger assigns a single POS tag
to each word in a sentence. Second, a CCG su-
pertagger assigns lexical categories to the words
in the sentence. Third, the parsing stage combines
the categories, using CCG?s combinatory rules,
and builds a packed chart representation contain-
ing all the derivations which can be built from
the lexical categories. Finally, the Viterbi algo-
rithm finds the highest scoring derivation from
the packed chart, using the normal-form log-linear
model described in Clark and Curran (2007).
Sometimes the parser is unable to build an anal-
ysis which spans the whole sentence. When this
happens the parser and supertagger interact us-
ing the adaptive supertagging strategy described
in Clark and Curran (2004): the parser effectively
asks the supertagger to provide more lexical cate-
gories for each word. This potentially continues
for a number of iterations until the parser does
create a spanning analysis, or else it gives up and
moves to the next sentence.
The parser uses the CKY algorithm (Kasami,
1965; Younger, 1967) described in Steedman
(2000) to create a packed chart. The CKY al-
gorithm applies naturally to CCG since the gram-
mar is binary. It builds the chart bottom-up, start-
ing with two-word constituents (assuming the su-
pertagging phase has been completed), incremen-
tally increasing the span until the whole sentence
is covered. The chart is packed in the standard
sense that any two equivalent constituents created
during the parsing process are placed in the same
equivalence class, with pointers to the children
used in the creation. Equivalence is defined in
terms of the category and head of the constituent,
to enable the Viterbi algorithm to efficiently find
the highest scoring derivation.1 A textbook treat-
ment of CKY applied to statistical parsing is given
in Jurafsky and Martin (2000).
3 Data and Evaluation Metrics
We performed efficiency and accuracy tests on
newspaper and Wikipedia data. For the newspa-
per data, we used the standard test sections from
1Use of the Viterbi algorithm in this way requires the fea-
tures in the parser model to be local to a single rule applica-
tion; Clark and Curran (2007) has more discussion.
1472
(ncmod num hundred 1 Seven 0)
(conj and 2 sixty-one 3)
(conj and 2 hundred 1)
(dobj in 6 total 7)
(ncmod made 5 in 6)
(aux made 5 were 4)
(ncsubj made 5 and 2 obj)
(passive made 5)
Seven hundred and sixty-one were made in
total.
Figure 1: Example Wikipedia test sentence anno-
tated with grammatical relations.
CCGbank. Following Clark and Curran (2007) we
used the CCG dependencies for accuracy evalua-
tion, comparing those output by the parser with
the gold-standard dependencies in CCGbank. Un-
like Clark and Curran, we calculated recall scores
over all sentences, including those for which the
parser did not find an analysis. For the WSJ data
the parser fails on a small number of sentences
(less than 1%), but the chart pruning has the effect
of reducing this failure rate further, and we felt
that this should be factored into the calculation of
recall and hence F-score.
In order to test the parser on Wikipedia text,
we created two test sets. The first, Wiki 300, for
testing accuracy, consists of 300 sentences man-
ually annotated with grammatical relations (GRs)
in the style of Briscoe and Carroll (2006). An
example sentence is given in Figure 1. The data
was created by manually correcting the output of
the parser on these sentences, with the annotation
being performed by Clark and Rimell, including
checks on a subset of these cases to ensure con-
sistency across the two annotators. For the ac-
curacy evaluation, we calculated precision, recall
and balanced F-measure over the GRs in the stan-
dard way.
For testing speed on Wikipedia, we used a cor-
pus of 2500 randomly chosen sentences, Wiki
2500. For all speed tests we measured the num-
ber of sentences per second, using a single CPU
and standard hardware.
4 Beam Search
The beam search approach used in our exper-
iments prunes all constituents in a cell having
scores below a multiple (?) of the score of the
? Speed Gain F-score Gain
Baseline 43.0 85.55
0.001 48.6 13% 85.82 0.27
0.002 54.2 26% 85.88 0.33
0.005 59.0 37% 85.73 0.18
0.01 66.7 55% 85.53 -0.02
Table 1: Accuracy and speed results using differ-
ent beam values ?.
? Speed Gain F-score Gain
Baseline 43.0 85.55
10 60.1 39% 85.55 0.00
20 70.6 64% 85.66 0.11
30 72.3 68% 85.65 0.10
40 76.4 77% 85.63 0.08
50 76.7 78% 85.62 0.07
60 74.5 73% 85.71 0.16
80 68.4 59% 85.71 0.16
100 62.0 44% 85.73 0.18
None 59.0 37% 85.73 0.18
Table 2: Accuracy and speed results for different
values of ? where ? = 0.005.
highest scoring constituent for that cell.2 The
scores for a constituent are calculated using the
same model used to find the highest scoring
derivation. We consider two scores: the Viterbi
score, which is the score of the highest scoring
sub-derivation for that constituent; and the inside
score, which is the sum over all sub-derviations
for that constituent. We investigated the follow-
ing: the trade-off between the aggressiveness of
the beam search and accuracy; the comparison be-
tween the Viterbi and inside scores; and whether
applying the beam to only certain cells in the chart
can improve performance.
Table 1 shows results on Section 00 of CCG-
bank, using the Viterbi score to prune. As ex-
pected, the parsing speed increases as the value
of ? increases, since more constituents are pruned
with a higher ? value. The pruning is effective,
with a ? value of 0.01 giving a 55% speed increase
with neglible loss in accuracy.3
2One restriction we apply in practice is that only con-
stituents resulting from the application of a CCG binary rule,
rather than a unary rule, are pruned.
3The small accuracy increase for some ? values could be
attributable to two factors: one, the parser may select a lower
1473
Speed F-score
Dataset Baseline Beam Gain Baseline Beam Gain
WSJ 00 43.0 76.4 77% 85.55 85.63 0.08
WSJ 02-21 53.4 99.4 86% 93.61 93.27 -0.34
WSJ 23 55.0 107.0 94% 87.12 86.90 -0.22
Wiki 300 35.5 80.3 126% 84.23 85.06 0.83
Wiki 2500 47.6 90.3 89%
Table 4: Beam search results on WSJ 00, 02-21, 23 and Wikipedia texts with ? = 0.005 and ? = 40.
? ? Speed F-score
Baseline 24.7 85.55
inside scores
0.01 37.7 85.52
0.001 25.3 85.79
0.005 10 33.4 85.54
0.005 20 39.5 85.64
0.005 50 42.9 85.58
Viterbi scores
0.01 38.1 85.53
0.001 28.2 85.82
0.005 10 33.6 85.55
0.005 20 39.4 85.66
0.005 50 43.1 85.62
Table 3: Comparison between using Viterbi scores
and inside scores as beam scores.
We also studied the effect of the beam search
at different levels of the chart. We applied a selec-
tive beam in which pruning is only applied to con-
stituents of length less than or equal to a threshold
?. For example, if ? = 20, pruning is applied only
to constituents spanning 20 words or less. The re-
sults are shown in Table 2. The selective beam
is also highly effective, showing speed gains over
the baseline (which does not use a beam) with no
loss in F-score. For a ? value of 50 the speed in-
crease is 78% with no loss in accuracy.
Note that for ? greater than 50, the speed re-
duces. We believe that this is due to the cost
of calculating the beam scores and the reduced
effectiveness of pruning for cells with longer
spans (since pruning shorter constituents early in
the chart-parsing process prevents the creation of
many larger, low-scoring constituents later).
Table 3 shows the comparison between the in-
scoring but more accurate derivation; and two, a possible in-
crease in recall, discussed in Section 3, can lead to a higher
F-score.
side and Viterbi scores. The results are similar,
with Viterbi marginally outperforming the inside
score in most cases. The interesting result from
these experiments is that the summing used in cal-
culating the inside score does not improve perfor-
mance over the max operator used by Viterbi.
Table 4 gives results on Wikipedia text, com-
pared with a number of sections from CCGbank.
(Sections 02-21 provide the training data for the
parser which explains the high accuracy results
on these sections.) Despite the fact that the prun-
ing model is derived from CCGbank and based on
WSJ text, the speed improvements for Wikipedia
were even greater than for WSJ text, with param-
eters ? = 0.005 and ? = 40 leading to almost a
doubling of speed on the Wiki 2500 set, with the
parser operating at 90 sentences per second.
5 Cell Pruning
Whole cells can be pruned from the chart by tag-
ging words in a sentence. Roark and Hollingshead
(2009) used a binary tagging approach to prune a
CFG CKY chart, where tags are assigned to input
words to indicate whether they can be the start or
end of multiple-word constituents. We adapt their
method to CCG chart pruning. We also show the
limitation of binary tagging, and propose a novel
tagging method which leads to increased speeds
and accuracies over the binary taggers.
5.1 Binary tagging
Following Roark and Hollingshead (2009), we as-
sign the binary begin and end tags separately us-
ing two independent taggers. Given the input
?We like playing cards together?, the pruning ef-
fects of each type of tag on the CKY chart are
shown in Figure 2. In this chart, rows repre-
1474
XWe like playing cards together
1 2 3 4 5
1
2
4
5
3
1 1 1 0 0
X X
X
We like playing cards together
1 2 3 4 5
1
2
4
5
3
0 0 0 1 1
Figure 2: The pruning effect of begin (top) and
end (bottom) tags; X indicates a removed cell.
sent consituent sizes and columns represent initial
words of constituents. No cell in the first row of
the chart is pruned, since these cells correspond
to single words, and are necessary for finding a
parse. The begin tag for the input word ?cards? is
0, which means that it cannot begin a multi-word
constituent. Therefore, no cell in column 4 can
contain any constituent. The pruning effect of a
binary begin tag is to cross out a column of chart
cells (ignoring the first row) when the tag value
is zero. Similarly, the end tag of the word ?play-
ing? is 0, which means that it cannot be the end
of a multi-word constituent. Consequently cell (2,
2), which contains constituents for ?like playing?,
and cell (1, 3), which contains constituents for
?We like playing?, must be empty. The pruning
effect of a binary end tag is to cross out a diagonal
of cells (ignoring the first row) when the tag value
is zero.
We use a maximum entropy trigram tagger
(Ratnaparkhi, 1996; Curran and Clark, 2003) to
Model Speed F-score
baseline 25.10 84.89
begin only 27.49 84.71
end only 30.33 84.56
both 33.90 84.60
oracle 33.60 85.67
Table 5: Accuracy and speed results for the binary
taggers on Section 00 of CCGbank.
assign the begin and end tags. Features based on
the words and POS in a 5-word window, plus the
two previously assigned tags, are extracted from
the trigram ending with the current tag and the
five-word window with the current word in the
middle. In our development experiments, both the
begin and the end taggers gave a per-word accu-
racy of around 96%, similar to the accuracy re-
ported in Roark and Hollingshead (2009).
Table 5 shows accuracy and speed results for
the binary taggers.4 Using begin or end tags alone,
the parser achieved speed increases with a small
loss in accuracy. When both begin and end tags
are applied, the parser achieved further speed in-
creases, with no loss in accuracy compared to the
end tag alone. Row ?oracle? shows what happens
using the perfect begin and end taggers, by using
gold-standard constituent information from CCG-
bank. The F-score is higher, since the parser is
being guided away from incorrect derivations, al-
though the speed is no higher than when using au-
tomatically assigned tags.
5.2 Level tagging
A binary tag cannot take effect when there is any
chart cell in the corresponding column or diagonal
that contains constituents. For example, the begin
tag for the word ?card? in Figure 3 cannot be 0 be-
cause ?card? begins a two-word constituent ?card
games?. Hence none of the cells in the column can
be pruned using the binary begin tag, even though
all the cells from the third row above are empty.
We propose what we call a level tagging approach
to address this problem.
Instead of taking a binary value that indicates
4The baseline differs slightly to the previous section be-
cause gold-standard POS tags were used for the beam-search
experiments.
1475
1 2 3 4 5
1
2
4
5
3
Playing card games is fun
Figure 3: The limitation of binary begin tags.
whether a whole column or diagonal of cells can
be pruned, a level tag (begin or end) takes an in-
teger value which indicates the row from which
a column or diagonal can be pruned in the up-
ward direction. For example, a level begin tag
with value 2 allows the column of chart cells for
the word ?card? in Figure 3 to be pruned from the
third row upwards. A level tag (begin or end) with
value 1 prunes the corresponding row or diago-
nal from the second row upwards; it has the same
pruning effect as a binary tag with value 0. For
convenience, value 0 for a level tag means that the
corresponding word can be the beginning or end
of any constituent, which is the same as a binary
tag value 1.
A comparison of the pruning effect of binary
and level tags for the sentence ?Playing card
games is fun? is shown in Figure 4. With a level
begin tag, more cells can be pruned from the col-
umn for ?card?. Therefore, level tags are poten-
tially more powerful for pruning.
We now need a method for assigning level tags
to words in a sentence. However, we cannot
achieve this with a straighforward classifier since
level tags are related; for example, a level tag (be-
gin or end) with value 2 implies level tags with
values 3 and above. We develop a novel method
for calculating the probability of a level tag for
a particular word. Our mechanism for calculat-
ing these probabilities uses what we call maxspan
tags, which can be assigned using a maximum en-
tropy tagger.
Maxspan tags take the same values as level tags.
However, the meanings of maxspan tags and level
X
XX
X
1 2 3 4 5
1
2
4
5
3
Playing card games is fun
X
XX
X
Playing card games is fun
1 2 3 4 5
1
2
4
5
3
Figure 4: The pruning effect of binary (top) and
level (bottom) tags.
tags are different. While a level tag indicates the
row from which a column or diagonal of cells is
pruned, a maxspan tag represents the size of the
largest constituent a word begins or ends. For ex-
ample, in Figure 3, the level end tag for the word
?games? has value 3, since the largest constituent
this words ends spans ?playing card games?.
We use the standard maximum entropy trigram
tagger for maxspan tagging, where features are
extracted from tag trigrams and surrounding five-
word windows, as for the binary taggers. Parse
trees can be turned directly into training data for
a maxspan tagger. Since the level tag set is fi-
nite, we a require a maximum value N that a level
tag can take. We experimented with N = 2 and
N = 4, which reflects the limited range of the
features used by the taggers.5
During decoding, the maxspan tagger uses the
forward-backward algorithm to compute the prob-
ability of maxspan tag values for each word in the
5Higher values of N did not lead to improvements during
development experiments.
1476
Model Speed F-score
baseline 25.10 84.89
binary 33.90 84.60
binary oracle 33.60 85.67
level N = 2 32.79 84.92
level N = 4 34.91 84.95
level N = 4 oracle 47.45 86.49
Table 6: Accuracy and speed results for the level
taggers on Section 00 of CCGbank.
input. Then for each word, the probability of its
level tag tl having value x is the sum of the prob-
abilities of its maxspan tm tag having values 1..x:
P (tl = x) =
x?
i=1
P (tm = i)
Maxspan tag values i from 1 to x represent dis-
joint events in which the largest constituent that
the corresponding word begins or ends has size i.
Summing the probabilities of these disjoint events
gives the probability that the largest constituent
the word begins or ends has a size between 1 and
x, inclusive. That is also the probability that all
the constituents the word begins or ends are in the
range of cells from rows 1 to row x in the corre-
sponding column or diagonal. And therefore that
is also the probability that the chart cells above
row x in the corresponding column or diagonal
do not contain any constituents, which means that
the column and diagonal can be pruned from row
x upward. Therefore, it is also the probability of a
level tag with value x.
The probability of a level tag having value x
increases as x increases from 1 to N . We set a
probability threshold Q and choose the smallest
level tag value x with probability P (tl = x) ? Q
as the level tag for a word. If P (tl = N) < Q, we
set the level tag to 0 and do not prune the column
or diagonal. The threshold value determines a bal-
ance between pruning power and accuracy, with a
higher value pruning more cells but increasing the
risk of incorrectly pruning a cell. During devel-
opment we arrived at a threshold value of 0.8 as
providing a suitable compromise between pruning
power and accuracy.
Table 6 shows accuracy and speed results for
the level tagger, using a threshold value of 0.8.
Model Speed F-score
baseline 36.64 84.23
binary gold 49.59 84.36
binary self 40K 48.79 83.64
binary self 200K 51.51 83.71
binary self 1M 47.78 83.75
level gold 58.23 84.12
level self 40K 54.76 83.83
level self 200K 48.57 83.39
level self 1M 52.54 83.71
Table 7: Accuracy tests on Wiki 300 comparing
gold training (gold) with self training (self) for
different sizes of parser output for self-training.
We compare the effect of the binary tagger and
level taggers with N = 2 and N = 4. The accu-
racies with the level taggers are higher than those
with the binary tagger; they are also higher than
the baseline parsing accuracy. The parser achieves
the highest speed and accuracy when pruned with
the N = 4 level tagger. Comparing the oracle
scores, the level taggers lead to higher speeds than
the binary tagger, reflecting the increased pruning
power of the level taggers compared with the bi-
nary taggers.
5.2.1 Final experiments using gold training
and self training
In this section we report our final tests using
Wikipedia data. We used two methods to derive
training data for the taggers. The first is the stan-
dard method, which is to transform gold-standard
parse trees into begin and end tag sequences. This
method is the method that we used for all previ-
ous experiments, and we call it ?gold training?.
In addition to gold training, we also investigate
an alternative method, which is to obtain training
data for the taggers from the output of the parser
itself, in a form of self-training (McClosky et al,
2006). The intuition is that the tagger will learn
what constituents a trained parser will eventually
choose, and as long as the constituents favoured
by the parsing model are not pruned, no reduction
in accuracy can occur. There is the potential for
an increase in speed, however, due to the pruning
effect.
For gold training, we used sections 02-21 of
1477
Model Speed
baseline 47.6
binary gold 80.8
binary 40K 75.5
binary 200K 77.4
binary 1M 78.6
level gold 93.7
level 40K 92.8
level 200K 92.5
level 1M 96.6
Table 8: Speed tests with gold and self-training on
Wiki 2500.
CCGBank (which consists of about 40K training
sentences) to derive training data. For self train-
ing, we trained the parser on sections 02-21 of
CCGBank, and used the parser to parse 40 thou-
sand, 200 thousand and 1 million sentences from
Wikipedia, respectively. Then we derive three sets
of self training data from the three sets of parser
outputs. We then used our Wiki 300 set to test the
accuracy, and the Wiki 2500 set to test the speed
of the parser.
The results are shown in Tables 7 and 8, where
each row represents a training data set. Rows ?bi-
nary gold? and ?level gold? represent binary and
level taggers trained using gold training. Rows
?binary self X? and ?level self X? represent bi-
nary and level taggers trained using self training,
with the size of the training data being X sen-
tences.
It can be seen from the Tables that the accuracy
loss with self-trained binary or level taggers was
not large (in the worst case, the accuracy dropped
from 84.23% to 83.39%), while the speed was
significantly improved. Using binary taggers, the
largest speed improvement was from 47.6 sen-
tences per second to 80.8 sentences per second
(a 69.7% relative increase). Using level taggers,
the largest speed improvement was from 47.6 sen-
tences per second to 96.6 sentences per second (a
103% relative increase).
A potential advantage of self-training is the
availability of large amounts of training data.
However, our results are somewhat negative in
this regard, in that we find training the tagger on
more than 40,000 parsed sentences (the size of
CCGbank) did not improve the self-training re-
sults. We did see the usual speed improvements
from using the self-trained taggers, however, over
the baseline parser with no pruning.
6 Conclusion
Using our novel method of level tagging for prun-
ing complete cells in a CKY chart, the CCG parser
was able to process almost 100 Wikipedia sen-
tences per second, using both CCGbank and the
output of the parser to train the taggers, with little
or no loss in accuracy. This was a 103% increase
over the baseline with no pruning.
We also demonstrated that standard beam
search is highly effective in increasing the speed
of the CCG parser, despite the fact that the su-
pertagger has already had a significant pruning
effect. In future work we plan to investigate the
gains that can be achieved from combining the
two pruning methods, as well as other pruning
methods such as the self-training technique de-
scribed in Kummerfeld et al (2010) which re-
duces the number of lexical categories assigned
by the supertagger (leading to a speed increase).
Since these methods are largely orthogonal, we
expect to achieve further gains, leading to a re-
markably fast wide-coverage parser outputting
complex linguistic representations.
Acknowledgements
This work was largely carried out at the Johns
Hopkins University Summer Workshop and (par-
tially) supported by National Science Founda-
tion Grant Number IIS-0833652. Yue Zhang and
Stephen Clark are supported by the European
Union Seventh Framework Programme (FP7-ICT-
2009-4) under grant agreement no. 247762.
References
Bangalore, Srinivas and Aravind Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237?265.
Bos, Johan, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of COLING-04, pages 1240?
1246, Geneva, Switzerland.
1478
Briscoe, Ted and John Carroll. 2006. Evaluating
the accuracy of an unlexicalized statistical parser on
the PARC DepBank. In Proceedings of the Poster
Session of COLING/ACL-06, pages 41?48, Sydney,
Australia.
Carroll, John, Ted Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: a survey and a new pro-
posal. In Proceedings of the 1st LREC Conference,
pages 447?454, Granada, Spain.
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-fine N-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Meeting of
the ACL, pages 173?180, Michigan, Ann Arbor.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Meeting
of the NAACL, pages 132?139, Seattle, WA.
Clark, Stephen and James R. Curran. 2004. The im-
portance of supertagging for wide-coverage CCG
parsing. In Proceedings of COLING-04, pages 282?
288, Geneva, Switzerland.
Clark, Stephen and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Collins, Michael. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Curran, James R. and Stephen Clark. 2003. Inves-
tigating GIS and smoothing for maximum entropy
taggers. In Proceedings of the 10th Meeting of the
EACL, pages 91?98, Budapest, Hungary.
Curran, James R. 2004. From Distributional to Se-
mantic Similarity. Ph.D. thesis, University of Edin-
burgh.
Djordjevic, Bojan, James R. Curran, and Stephen
Clark. 2007. Improving the efficiency of a wide-
coverage CCG parser. In Proceedings of IWPT-07,
pages 39?47, Prague, Czech Republic.
Hockenmaier, Julia and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Jurafsky, Daniel and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics and Speech Recognition. Prentice Hall, New
Jersey.
Kaplan, Ron, Stefan Riezler, Tracy H. King, John
T. Maxwell III, Alexander Vasserman, and Richard
Crouch. 2004. Speed and accuracy in shallow and
deep stochastic parsing. In Proceedings of HLT-
NAACL?04, Boston, MA.
Kummerfeld, Jonathan K., Jessika Roesner, Tim
Dawborn, James Haggerty, James R. Curran, and
Stephen Clark. 2010. Faster parsing by supertag-
ger adaptation. In Proceedings of ACL-10, Uppsala,
Sweden.
Lin, Thomas, Oren Etzioni, and James Fogarty. 2009.
Identifying interesting assertions from the web. In
Proceedings of the 18th Conference on Information
and Knowledge Management (CIKM 2009), Hong
Kong.
McClosky, David, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of NAACL-06, pages 152?159, Brook-
lyn, NY.
Ninomiya, Takashi, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of beam
thresholding, unification filtering and hybrid pars-
ing in probabilistic HPSG parsing. In Proceedings
of IWPT-05, pages 103?114, Vancouver, Canada.
Nivre, J. and M. Scholz. 2004. Deterministic depen-
dency parsing of English text. In Proceedings of
COLING-04, pages 64?70, Geneva, Switzerland.
Petrov, Slav and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
the HLT/NAACL conference, Rochester, NY.
Ratnaparkhi, Adwait. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP-96, pages 133?142, Somerset, New Jer-
sey.
Roark, Brian and Kristy Hollingshead. 2009. Lin-
ear complexity context-free parsing pipelines via
chart constraints. In Proceedings of HLT/NAACL-
09, pages 647?655, Boulder, Colorado.
Steedman, Mark. 2000. The Syntactic Process. The
MIT Press, Cambridge, MA.
1479
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 257?268, Dublin, Ireland, August 23-29 2014.
Multi-view Chinese Treebanking
Likun Qiu
1,2,3
, Yue Zhang
1
, Peng Jin
4
and Houfeng Wang
2
1
Singapore University of Technology and Design, Singapore
2
Institute of Computational Linguistics, Peking University, China
3
School of Chinese Language and Literature, Ludong University, China
4
Lab of Intelligent Information Processing and Application, Leshan Normal University, China
{qiulikun,jandp,wanghf}@pku.edu.cn, yue zhang@sutd.edu.sg
Abstract
We present a multi-view annotation framework for Chinese treebanking, which uses dependen-
cy structures as the base view and supports conversion into phrase structures with minimal loss
of information. A multi-view Chinese treebank was built under the proposed framework, and
the first release (PMT 1.0) containing 14,463 sentences is be made freely available. To verify
the effectiveness of the multi-view framework, we implemented an arc-standard transition-based
dependency parser and added phrase structure features produced by the phrase structure view.
Experimental results show the effectiveness of additional features for dependency parsing. Fur-
ther, experiments on dependency-to-string machine translation show that our treebank and parser
could achieve similar results compared to the Stanford Parser trained on CTB 7.0.
1 Introduction
Phrase structures (PS) and dependency structures (DS) are two of the most popular grammar formalisms
for statistical parsing (Collins, 2003; Charniak, 2000; McDonald et al., 2005; Nivre, 2006; Petrov and
Klein, 2007; Zhang and Clark, 2008). While DS trees emphasize the grammatical relation between heads
and dependents, PS trees stress the hierarchical constituent structures of sentences. Several researchers
have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong,
2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al.,
2013), showing that the two types of information complement each other for NLP tasks.
Most existing Chinese and English treebanks fall into the phrase structure category, and much work
has been done to convert PS into DS (Magerman, 1994; Collins et al., 1999; Collins, 2003; Sun and
Jurafsky, 2004; Johansson and Nugues, 2007; Duan et al., 2007; Zhang and Clark, 2008). Research on
statistical dependency parsing has frequently used dependency treebanks converted from phrase structure
treebanks, such as the Penn Treebank (PTB) (Marcus et al., 1993) and Penn Chinese Treebank (CTB)
(Xue et al., 2000). However, previous research shows that dependency categories in converted tree-
banks are simplified (Johansson and Nugues, 2007), and the widely used head-table PS to DS conversion
approach encounters ambiguities and uncertainty, especially for complex coordination structures (Xue,
2007). The main reason is that the PS treebanks were designed without consideration of DS conversion,
leading to inherent ambiguities in the mapping, and loss of information in the resulting DS treebanks. To
minimize information loss during treebank conversions, a treebank could be designed by considering PS
and DS information simultaneously; such treebanks have been proposed as multi-view treebanks (Xia et
al., 2009). We develop a multi-view treebank for Chinese, which treats PS and DS as different views of
the same internal structures of a sentence.
We choose the DS view as the base view, from which PS would be derived. Our choice is based on the
effectiveness of information transfer rather than convenience of annotation (Rambow, 2010; Bhatt and
Xia, 2012). Research on Chinese syntax (Zhu, 1982; Chen, 1999; Chen, 2009) shows that the phrasal
category of a constituent can be derived from the phrasal categories of its immediate subconstituents and
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
257
PKU POS Our POS
Ag, a, ad, ia, ja, la a (adjective)
Bg,b, ib, jb, jm, lb b (distinguishing words)
Dg, d, dc, df, id, jd, ld d (adverb)
m, mq m(number)
n, an, in, jn, ln, Ng, vn, nr, kn n (noun)
Qg,q, qb, qc, qd, qe, qj, ql, qr, qt, qv, qz q (measure word)
Rg,r, rr, ry, ryw, rz, rzw r (pronoun)
Tg, t, tt t (temporal noun)
u, ud, ue, ui, ul, uo, us, uz, Ug u (auxiliary word)
v, iv, im, jv, lv, Vg, vd, vi, vl, vq,vu, vx, vt,kv v (verb)
w, wd, wf, wj, wk, wky, wkz, wm,wp, ws, wt, wu, ww, wy, wyy, wyz w (punctuation)
Table 1: Mapping from PKU POS to our POS.
the dependency categories between them (for terminal words, parts-of-speech can be used as phrasal cat-
egories). Consequently, in Chinese, the canonical PS, containing information of constituent hierarchies
and phrasal categories, can be derived naturally from the canonical DS. As Xia et al. (2009) stated, a rich
set of dependency categories should be designed to ensure lossless conversion from DS to PS. When the
information of PS has been represented in DS explicitly or implicitly, we can convert DS to PS without
ambiguity (Rambow et al., 2002).
Given our framework, a multi-view Chinese treebank, containing 14,463 sentences and 336K words,
is constructed. This main corpus is based on the Peking University People?s Daily Corpus. We name our
treebank the Peking University Multi-view Chinese Treebank (PMT) release 1.0. To verify the useful-
ness of the treebank for statistical NLP, a transition-based dependency parser is implemented to include
PS features produced in the derivation process of phrasal categories. We perform a set of empirical
evaluations, with experimental results on both dependency parsing and dependency-to-string machine
translation showing the effectiveness of the proposed annotation framework and treebank. We make the
treebank, the DS to PS conversion script and the parser freely available.
2 Annotation Framework
2.1 Part-of-speech Tagset
Our part-of-speech (POS) tagset is based on the Peking University (PKU) People?s Daily corpus, which
consists of over 100 tags (Yu et al., 2003). We simplify the PKU tagset by syntactic distribution. The
simplified tagset contains 33 POS tags. The mapping from the original PKU POS to our simplified POS is
shown in Table 1. For instance, Ag (adjective morpheme), ad (adjective acting as an adverb), ia (adjective
idioms), ja (adjective abbreviation) and la (temporary phrase acting as an adjective) are all mapped to one
tag a (adjective). A set of basic PKU POS tags, including c (conjunction), e (interjection), f (localizer),
g (morpheme), h (prefix), i (idiom), j (abbreviation), k (suffix), l (temporary phrase), nr (personal name),
nrf (family name), nrg (surname), ns (toponym), nt (organization name), nx (non-Chinese noun), nz
(other proper noun), o (onomonopeia), p (preposition), q (measure word), r (pronoun), s (locative), x
(other non-Chinese word), y (sentence final particle), z (state adjective), are left unchanged.
2.2 Dependency Category Tagset
In a DS, the modifier is tagged with a dependency category, which denotes the role the modifier plays
with regard to its head. The root word of a sentence is dependent on a virtual root node R and tagged
with the dependency category HED. Table 2 lists the 32 dependency categories used in our annotation
guideline. These categories are designed in consideration of PS conversion with minimal ambiguities,
and can be classified according to the following criteria:
(1) whether the head dominates a compound clause (i.e. has an IC modifier) in the PS view. Accord-
ing to this, dependency categories can be cross-clause or in-clause. For instance, in Figure 1, the last
punctuation (") is labeled with the cross-clause tag PUS, and its head dominates an IC modifier. (2) the
relative position of the modifier to the head. According to this, dependency categories can be left, right
or free. For instance, the LAD, SBV, ADV, COS, DE and ATT labels in Figure 1 are all left. The VOB label
258
Tag Description Tag Description
ACT action object LAD left additive
ADV adverbial MT modality and time
APP appositive element NUM number
ATT attribute POB propositional object
CMP complement PUN punctuation
COO other coordination element PUS cross-clause punctuation
COS share-right-child coordination element QUC post-positional quantity
DE de (modifier of(special function word)) QUCC non-shared post-positional quantity
DEI dei (modifier ofProceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 816?826, Dublin, Ireland, August 23-29 2014.
Feature Embedding for Dependency Parsing
Wenliang Chen
?
, Yue Zhang
?
, and Min Zhang
??
?
School of Computer Science and Technology, Soochow University, China
?
Singapore University of Technology and Design, Singapore
{wlchen, mzhang}@suda.edu.cn
yue zhang@sutd.edu.sg
Abstract
In this paper, we propose an approach to automatically learning feature embeddings to address
the feature sparseness problem for dependency parsing. Inspired by word embeddings, feature
embeddings are distributed representations of features that are learned from large amounts of
auto-parsed data. Our target is to learn feature embeddings that can not only make full use of
well-established hand-designed features but also benefit from the hidden-class representations
of features. Based on feature embeddings, we present a set of new features for graph-based
dependency parsing models. Experiments on the standard Chinese and English data sets show
that the new parser achieves significant performance improvements over a strong baseline.
1 Introduction
Discriminative models have become the dominant approach for dependency parsing (Nivre et al., 2007;
Zhang and Clark, 2008; Hatori et al., 2011). State-of-the-art accuracies have been achieved by the use of
rich features in discriminative models (Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010; Zhang and
Nivre, 2011). While lexicalized features extracted from non-local contexts enhance the discriminative
power of parsers, they are relatively sparse. Given a limited set of training data (typically less than 50k
sentences for dependency parsing), the chance of a feature occurring in the training data but not in the
test data can be high.
Another limitation on features is that many are typically derived by (manual) combination of atomic
features. For example, given the head word (w
h
) and part-of-speech tag (p
h
), dependent word (w
d
)
and part-of-speech tag (p
d
), and the label (l) of a dependency arc, state-of-the-art dependency parsers
can have the combined features: [w
h
; p
h
], [w
h
; p
h
;w
d
; p
d
], [w
h
; p
h
;w
d
], and so on, in addition to the
atomic features: [w
h
], [p
h
], etc. Such combination is necessary for high accuracies because the dominant
approach uses linear models, which can not capture complex correlations between atomic features.
We tackle the above issues by borrowing solutions from word representations, which have been in-
tensely studied in the NLP community (Turian et al., 2010). In particular, distributed representations of
words have been used for many NLP problems, which represent a word by information from the words
it frequently co-occurs with (Lin, 1997; Curran, 2005; Collobert et al., 2011; Bengio, 2009; Mikolov
et al., 2013b). The representation can be learned from large amounts of raw sentences, and hence used
to reduce OOV rates in test data. In addition, since the representation of each word carries information
about its context words, it can also be used to calculate word similarity (Mikolov et al., 2013a), or used
as additional semantic features (Koo et al., 2008).
In this paper, we show that a distributed representation can be learned for features also. Learned
from large amount of automatically parsed data, the representation of each feature can be defined on the
?
Corresponding author
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
816
features it frequently co-occurs with. Similar to words, the feature representation can be used to reduce
the rate of unseen features in test data, and to capture inherent correlations between features. Borrowing
terminologies from word embeddings, we call the feature representation feature embeddings.
Compared with the task of learning word embeddings, the task of learning feature embeddings is
more difficult because the size of features is much larger than the vocabulary size and tree structures
are more complex than word sequences. This requires us to find an effective embedding format and an
efficient inference algorithm. Traditional LSA and RNN (Collobert et al., 2011; Bengio, 2009) models
turn out to be very slow for feature embeddings. Recently, Mikolov et al. (2013a) and Mikolov et al.
(2013b) introduce efficient models to learn high-quality word embeddings from extremely large amounts
of raw text, which offer a possible solution to the efficiency issue of learning feature embeddings. We
adapt their approach for learning feature embeddings, showing how an unordered feature context can
be used to learn the representation of a set of complex features. Using this method, a large number
of embeddings are trained from automatically parsed texts, based on which a set of new features are
designed and incorporated into a graph-based parsing model (McDonald and Nivre, 2007).
We conduct experiments on the standard data sets of the Penn English Treebank and the Chinese Tree-
bank V5.1. The results indicate that our proposed approach significantly improves parsing accuracies.
2 Background
In this section, we introduce the background of dependency parsing and build a baseline parser based on
the graph-based parsing model proposed by McDonald et al. (2005).
2.1 Dependency parsing
Given an input sentence x = (w
0
, w
1
, ..., w
i
, ..., w
m
), where w
0
is ROOT and w
i
(i ?= 0) refers to a
word, the task of dependency parsing is to find y
?
which has the highest score for x,
y
?
= argmax
y?Y (x)
score(x, y)
where Y (x) is the set of all the valid dependency trees for x. There are two major models (Nivre
and McDonald, 2008): the transition-based model and graph-based model, which showed comparable
accuracies for a wide range of languages (Nivre et al., 2007; Bohnet, 2010; Zhang and Nivre, 2011;
Bohnet and Nivre, 2012). We apply feature embeddings to a graph-based model in this paper.
2.2 Graph-based parsing model
We use an ordered pair (w
i
, w
j
) ? y to define a dependency relation in tree y from word w
i
to word w
j
(w
i
is the head and w
j
is the dependent), and G
x
to define a graph that consists of a set of nodes V
x
=
{w
0
, w
1
, ..., w
i
, ..., w
m
} and a set of arcs (edges) E
x
= {(w
i
, w
j
)|i ?= j, w
i
? V
x
, w
j
? (V
x
? {w
0
})}.
The parsing model of McDonald et al. (2005) searches for the maximum spanning tree (MST) in G
x
.
We denote Y (G
x
) as the set of all the subgraphs of G
x
that are valid spanning trees (McDonald and
Nivre, 2007). The score of a dependency tree y ? Y (G
x
) is the sum of the scores of its subgraphs,
score(x, y) =
?
g?y
score(x, g) =
?
g?y
f(x, g) ? w (1)
where g is a spanning subgraph of y, which can be a single arc or adjacent arcs, f(x, g) is a high-
dimensional feature vector based on features defined over g and x, and w refers to the weights for the
features. In this paper we assume that a dependency tree is a spanning projective tree.
2.3 Baseline parser
We use the decoding algorithm proposed by Carreras (2007) and use the Margin Infused Relaxed Al-
gorithm (MIRA) (Crammer and Singer, 2003; McDonald et al., 2005) to train feature weights w. We
use the feature templates of Bohnet (2010) as our base feature templates, which produces state-of-the-art
accuracies. We further extend the features by introducing more lexical features to the base features. The
817
First-order
[wp]
h
, [wp]
d
, d(h, d)
[wp]
h
, d(h, d)
w
d
, p
d
, d(h, d)
[wp]
d
, d(h, d)
w
h
, p
h
, w
d
, p
d
, d(h, d)
p
h
, w
h
, p
d
, d(h, d)
w
h
, w
d
, p
d
, d(h, d)
w
h
, p
h
, [wp]
d
, d(h, d)
p
h
, p
b
, p
d
, d(h, d)
p
h
, p
h+1
, p
d?1
, p
d
, d(h, d)
p
h?1
, p
h
, p
d?1
, p
d
, d(h, d)
p
h
, p
h+1
, p
d
, p
d+1
, d(h, d)
p
h?1
, p
h
, p
d
, p
d+1
, d(h, d)
Second-order
p
h
, p
d
, p
c
, d(h, d, c)
w
h
, w
d
, c
w
, d(h, d, c)
p
h
, [wp]
c
, d(h, d, c)
p
d
, [wp]
c
, d(h, d, c)
Second-order (continue)
w
h
, [wp]
c
, d(h, d, c)
w
d
, [wp]
c
, d(h, d, c)
[wp]
h
, [wp]
h+1
, [wp]
c
, d(h, d, c)
[wp]
h?1
, [wp]
h
, [wp]
c
, d(h, d, c)
[wp]
h
, [wp]
c?1
, [wp]
c
, d(h, d, c)
[wp]
h
, [wp]
c
, [wp]
c+1
, d(h, d, c)
[wp]
h?1
, [wp]
h
, [wp]
c?1
, [wp]
c
, d(h, d, c)
[wp]
h
, [wp]
h+1
, [wp]
c?1
, [wp]
c
, d(h, d, c)
[wp]
h?1
, [wp]
h
, [wp]
c
, [wp]
c+1
, d(h, d, c)
[wp]
h
, [wp]
h+1
, [wp]
c
, [wp]
c+1
, d(h, d, c)
[wp]
d
, [wp]
d+1
, [wp]
c
, d(h, d, c)
[wp]
d?1
, [wp]
d
, [wp]
c
, d(h, d, c)
[wp]
d
, [wp]
c?1
, [wp]
c
, d(h, d, c)
[wp]
d
, [wp]
c
, [wp]
c+1
, d(h, d, c)
[wp]
d
, [wp]
d+1
, [wp]
c?1
, [wp]
c
, d(h, d, c)
[wp]
d
, [wp]
d+1
, [wp]
c
, [wp]
c+1
, d(h, d, c)
[wp]
d?1
, [wp]
d
, [wp]
c?1
, [wp]
c
, d(h, d, c)
[wp]
d?1
, [wp]
d
, [wp]
c
, [wp]
c+1
, d(h, d, c)
Table 1: Base feature templates.
base feature templates are listed in Table 1, where h and d refer to the head, the dependent, respectively,
c refers to d?s sibling or child, b refers to the word between h and d, +1 (?1) refers to the next (previous)
word, w and p refer to the surface word and part-of-speech tag, respectively, [wp] refers to the surface
word or part-of-speech tag, d(h, d) is the direction of the dependency relation between h and d, and
d(h, d, c) is the directions of the relation among h, d, and c.
We train a parser with the base features and use it as the Baseline parser. Defining f
b
(x, g) as the base
features and w
b
as the corresponding weights, the scoring function becomes,
score(x, g) = f
b
(x, g) ? w
b
(2)
3 Feature Embeddings
Our goal is to reduce the sparseness of rich features by learning a distributed representation of features,
which is dense and low dimensional. We call the distributed feature representation feature embeddings.
In the representation, each dimension represents a hidden-class of the features and is expected to capture
a type of similarities or share properties among the features.
The key to learn embeddings is making use of information from a local context, and to this end
various methods have been proposed for learning word embeddings. Lin (1997) and Curran (2005) use
the count of words in a surrounding word window to represent distributed meaning of words. Brown
et al. (1992) uses bigrams to cluster words hierarchically. These methods have been shown effective
on words. However, the number of features is much larger than the vocabulary size, which makes it
infeasible to apply them on features. Another line of research induce word embeddings using neural
language models (Bengio, 2008). However, the training speed of neural language models is too slow for
the high dimensionality of features. Mikolov et al. (2013b) and Mikolov et al. (2013a) introduce efficient
methods to directly learn high-quality word embeddings from large amounts of unstructured raw text.
Since the methods do not involve dense matrix multiplications, the training speed is extremely fast.
We adapt the models of Mikolov et al. (2013b) and Mikolov et al. (2013a) for learning feature embed-
dings from large amounts of automatically parsed dependency trees. Since feature embeddings have a
high computational cost, we also use Negative sampling technique in the learning stage (Mikolov et al.,
2013b). Different from word embeddings, the input of our approach is features rather than words, and
the feature representations are generated from tree structures instead of word sequences. Consequently,
818
Figure 1: An example of one-step context. Figure 2: One-step surrounding features.
we give a definition of unordered feature contexts and adapt the algorithms of Mikolov et al. (2013b) for
feature embeddings.
3.1 Surrounding feature context
The most important difference between features and words is the contextual structure. Given a sentence
x =w
1
, w
2
, ..., w
n
and its dependency tree y, we define the M -step context as a set of relations reachable
within M steps from the current relation. Here one step refers to one dependency arc. For instance, the
one-step context includes the surrounding relations that can be reached in one arc, as shown in Figure 1.
In the figure, for the relation between ?with? and ?fork?, the relation between ?ate? and ?with? is in the
one-step context, while the relation between ?He? and ?ate? is in the two-step context because it can be
reached via the arc between ?ate? and ?with?. A larger M results in more contextual features and thus
might lead to a more accurate embedding, but at the expense of training speed.
Based on the M -step context, we use surrounding features to represent the features on the current
dependency relations. The surrounding features are defined on the relations in the M -step context. Take
1-step context as an example. Figure 2 shows the representations for the current relation between ?with?
and ?fork? in Figure 1. Given the current relation and the relations in its one-step context, we generate
the features based on the base feature templates. In Figure 2 the current feature ?f1:with, fork, R?
can be represented by the surrounding features ?cf1:ate, with, R? and ?cf1: fork, a, L? based on the
template ?T1:w
h
, w
d
, d(h, d)?. Similarly, all the features on the current relation are represented by the
features on the relations in the one-step context. To reduce computational cost, we generate for every
feature its contextual features based on the same feature template. As a result, the embeddings for each
feature template is trained separately. In the experiments, we use one-step context for learning feature
embeddings.
3.2 Feature Embedding model
We adapt the models of Mikolov et al. (2013b) and Mikolov et al. (2013a) to infer feature embeddings
(FE). Based on the representation of surrounding context, the input to the learning models is a set of
features and the output is feature embeddings as shown in Figure 3. For each dependency tree in large
amounts of auto-parsed data, we generate the base features and associate them with their surrounding
contextual features. Then all the base features are put into a set, which is used as the training instances
for learning models.
In the embedding model, we use the features on the current dependency arc to predict the surround-
ing features, as shown in Figure 4. Given sentences and their corresponding dependency trees Y , the
objective of the model is to maximize the conditional log-likelihood of context features,
?
y?Y
?
f?F
y
?
cf?CF
f
log(p(cf |f)) (3)
819
Figure 3: Input feature set. Figure 4: The feature embedding model.
where F
y
is a set of features generated from tree y, CF
f
is the set of surrounding features in the M -step
context of feature f . p(cf |f) can be computed by using the softmax function (Mikolov et al., 2013b),
for which the input is f and the output is cf ,
p(cf |f) =
exp(v
?
cf
T
v
f
)
?
F
i=1
exp(v
?
cf
i
T
v
f
)
(4)
where v
f
and v
?
f
are the input and output vector representations of f , and F is the number of features in
the feature table. The formulation is impractical for large data because the number of features is large
(in the millions) and the computational cost is too high.
To compute the probabilities efficiently, we use the Negative sampling method proposed by Mikolov
et al. (2013b), which approximates the probability by the correct example and K negative samples for
each instance. The formulation to compute log(p(cf |f)) is,
log ?(v
?
cf
T
v
f
) +
K
?
k=1
E
cf
k
?P (cf)
[log ?(?v
?
cf
k
T
v
f
)] (5)
where ?(z) = 1/(1 + exp(?z)) and P (f) is the noise distribution on the data. Following the setting of
Mikolov et al. (2013b), we set K to 5 in our experiments.
We predict the set of features one by one. Stochastic gradient ascent is used to perform the following
iterative update after predicting the i
th
feature,
? ? ? + ?(
?
?
cf
log(p(cf
i
|f)
??
) (6)
where ? is the learning rate and ? includes the parameters of the model and the vector representations
of features. The initial value of ? is 0.025. If the log-likelihood does not improve significantly after one
update, the rate is halved (Mikolov et al., 2009).
3.3 Distributed representation
Based on the proposed surrounding context, we use the feature embedding model with the help of the
Negative sampling method to learn feature embeddings. For each base template T
i
, the distributed rep-
resentations are stored in a matrixM
i
? R
d?|F
i
|
, where d is the number of dimensions (to be chosen
in the experiments) and |F
i
| is the size of the features F
i
for T
i
. For each feature f ? F
i
, its vector is
v
f
= [v
1
, ..., v
d
].
820
< j : T (f) ? ?(v
j
) > for j ? [1, d]
< j : T (f) ? ?(v
j
), w
h
> for j ? [1, d]
Table 2: FE-based templates.
4 Parsing with feature embeddings
In this section, we discuss how to apply the feature embeddings to dependency parsing.
4.1 FE-based feature templates
The base parsing model contains only binary features, while the values in the feature embedding repre-
sentation are real numbers that are not in a bounded range. If the range of the values is too large, they will
exert much more influence than the binary features. To solve this problem, we define a function ?(v
i
)
(details are given in Section 4.3) to convert real values to discrete values. The vector v
f
= [v
1
, ..., v
d
] is
converted into v
N
f
= [?(v
1
), ...,?(v
d
)].
We define a set of new feature templates for the parsing models, capturing feature embedding infor-
mation. Table 2 shows the new templates, where T (f) refers to the base template type of feature f . We
remove any new feature related to the surface form of the head if the word is not one of the Top-N most
frequent words in the training data. We used N=1000 for the experiments, which reduces the size of the
feature sets.
4.2 FE parser
We combine the base features with the new features by a new scoring function,
score(x, g) = f
b
(x, g) ? w
b
+ f
e
(x, g) ? w
e
(7)
where f
b
(x, g) refers to the base features, f
e
(x, g) refers to the FE-based features, and w
b
and w
e
are
their corresponding weights, respectively. The feature weights are learned during training using MIRA
(Crammer and Singer, 2003; McDonald et al., 2005).
We use the same decoding algorithm in the new parser as in the Baseline parser. The new parser is
referred to as the FE Parser.
4.3 Discretization functions
There are various functions to convert the real values in the vectors into discrete values. Here, we use a
simple method. First, for the i
th
base template, the values in the j
th
dimension are sorted in decreasing
order L
ij
. We divide the list into two parts for positive (L
ij+
) and negative (L
ij?
), respectively, and
define two functions. The first function is,
?
1
(v
j
) =
?
?
?
?
?
?
?
+B1 if v
j
is in top 50% in L
ij+
+B2 if v
j
is in bottom 50% in L
ij+
?B1 if v
j
is in top 50% in L
ij?
?B2 if v
j
is in bottom 50% in L
ij?
The second function is,
?
2
(v
j
) =
{
+B1 if v
j
is in top 50% in L
ij+
?B2 if v
j
is in bottom 50% in L
ij?
In ?
2
, we only consider the values (?+B1? and ?-B2?), which have strong values (positive or negative)
on each dimension, and omit the values which are close to zero. We refer the systems with ?
1
as M1
and the ones with ?
2
as M2. We also tried the original continuous values and the scaled values as used
by Turian et al. (2010), but the results were negative.
5 Experiments
We conducted experiments on English and Chinese data, respectively.
821
train dev test
PTB 2-21 22 23
CTB5 001-815 886-931 816-885
1001-1136 1148-1151 1137-1147
Table 3: Data sets of PTB and CTB5.
# of words # of sentences
BLLIP WSJ 43.4M 1.8M
Gigaword Xinhua 272.3M 11.7M
Table 4: Information of raw data.
 89.4
 89.6
 89.8
 90
 90.2
 90.4
 90.6
 90.8
 91
2 5 10 20 50
U
A
S
Sizes of dimensions
BaselineM1M2
Figure 5: Effect of different sizes of embeddings on the development data.
5.1 Data sets
We used the Penn Treebank (PTB) to generate the English data sets, and the Chinese Treebank version 5.1
(CTB5) to generate the Chinese data sets. ?Penn2Malt?
1
was used to convert the data into dependency
structures with the English head rules of Yamada and Matsumoto (2003) and the Chinese head rules
of Zhang and Clark (2008). The details of data splits are listed in Table 3, where the data partition of
Chinese were chosen to match previous work (Duan et al., 2007; Li et al., 2011b; Hatori et al., 2011).
Following the work of Koo et al. (2008), we used a tagger trained on the training data to provide
part-of-speech (POS) tags for the development and test sets, and used 10-way jackknifing to generate
part-of-speech tags for the training set. For English we used the MXPOST (Ratnaparkhi, 1996) tagger
and for Chinese we used a CRF-based tagger with the feature templates defined in Zhang and Clark
(2008). We used gold-standard segmentation in the CTB5 experiments. The accuracies of part-of-speech
tagging are 97.32% for English and 93.61% for Chinese on the test sets, respectively.
To obtain feature contexts, we processed raw data to obtain dependency trees. For English, we used the
BLLIP WSJ Corpus Release 1.
2
For Chinese, we used the Xinhua portion of Chinese Gigaword
3
Version
2.0 (LDC2009T14). The statistical information of raw data sets is listed in Table 4. The MXPOST part-
of-speech tagger and the Baseline dependency parser trained on the training data were used to process
the sentences of the BLLIP WSJ corpus. For Chinese, we need to perform word segmentation and part-
of-speech tagging before parsing. The MMA system (Kruengkrai et al., 2009) trained on the training
data was used to perform word segmentation and tagging, and the Baseline parser was used to parse the
sentences in the Gigaword corpus.
We report the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of tokens
(excluding all punctuation tokens) with the correct HEAD. We also report the scores on complete depen-
dency tree matches (COMP).
1
http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
2
We excluded the texts of PTB from the BLLIP WSJ Corpus.
3
We excluded the texts of CTB5 from the Gigaword data.
822
UAS COMP
Baseline 92.78 48.08
Baseline+BrownClu 93.37 49.26
M2 93.74 50.82
Koo and Collins (2010) 93.04 N/A
Zhang and Nivre (2011) 92.9 48.0
Koo et al. (2008) 93.16 N/A
Suzuki et al. (2009) 93.79 N/A
Chen et al. (2009) 93.16 47.15
Zhou et al. (2011) 92.64 46.61
Suzuki et al. (2011) 94.22 N/A
Chen et al. (2013) 93.77 51.36
Table 5: Results on English data.
N/A=Not Available.
POS UAS COMP
Baseline 93.61 81.04 29.73
M2 93.61 82.94 31.72
Li et al. (2011a) 93.08 80.74 29.11
Hatori et al. (2011) 93.94 81.33 29.90
Li et al. (2012) 94.51 81.21 N/A
Chen et al. (2013) N/A 83.08 32.21
Table 6: Results on Chinese data.
N/A=Not Available.
5.2 Development experiments
In this section, we use the English development data to investigate the effects of different vector sizes
of feature embeddings, and compare the systems with the discretization functions ?
1
(M1) and ?
2
(M2)
(defined in Section 4.3), respectively. To reduce the training time, we used 10% of the labeled training
data to train the parsing models.
Turian et al. (2010) reported that the optimal size of word embedding dimensions was task-specific for
NLP tasks. Here, we investigated the effect of different sizes of embedding dimensions on dependency
parsing. Figure 5 shows the effect on UAS scores as we varied the vector sizes. The systems with FE-
based features always outperformed the Baseline. The curve of M2 was almost flat and we found that M1
performed worse as the sizes increased. Overall, M2 performed better than M1. For M2, 10-dimensional
embeddings achieved the highest score among all the systems. Based on the above observations, we
chose the following settings for further evaluations: 10-dimensional embeddings for M2.
5.3 Final results on English data
We trained the M2 model on the full training data and evaluated it on the English testing data. The
results are shown in Table 5. The parser using the FE-based features outperformed the Baseline. We
obtained absolute improvements of 0.96 UAS points. As for the COMP scores, M2 achieved absolute
improvement of 2.74 over the Baseline. The improvements were significant in McNemar?s Test (p <
10
?7
) (Nivre et al., 2004).
We listed the performance of the related systems in Table 5. We also added the cluster-based features
of Koo et al. (2008) to our baseline system listed as ?Baseline+BrownClu? in Table 5. From the table,
we found that our FE parsers obtained comparable accuracies with the previous state-of-the-art systems.
Suzuki et al. (2011) reported the best result by combining their method with the method of Koo et al.
(2008). We believe that the performance of our parser can be further enhanced by integrating their
methods.
5.4 Final results on Chinese data
We also evaluated the systems on the testing data for Chinese. The results are shown in Table 6. Sim-
ilar to the results on English, the parser using the FE-based features outperformed the Baseline. The
improvements were significant in McNemar?s Test (p < 10
?8
) (Nivre et al., 2004).
We listed the performance of the related systems
4
on Chinese in Table 6. From the table, we found
that the scores of our FE parser was higher than most of the related systems and comparable with the
results of Chen2013, which was the best reported scores so far.
4
We did not include the result (83.96) of Wu et al. (2013) because their part-of-speech tagging accuracy is 97.7%, much
higher than ours and other work. Their tagger includes rich external resources.
823
6 Related work
Learning feature embeddings are related to two lines of research: deep learning models for NLP, and
semi-supervised dependency parsing.
Recent studies used deep learning models in a variety of NLP tasks. Turian et al. (2010) applied
word embeddings to chunking and Named Entity Recognition (NER). Collobert et al. (2011) designed
a unified neural network to learn distributed representations that were useful for part-of-speech tagging,
chunking, NER, and semantic role labeling. They tried to avoid task-specific feature engineering. Socher
et al. (2013) proposed a Compositional Vector Grammar, which combined PCFGs with distributed word
representations. Zheng et al. (2013) investigated Chinese character embeddings for Chinese word seg-
mentation and part-of-speech tagging. Wu et al. (2013) directly applied word embeddings to Chinese
dependency parsing. In most cases, words or characters were the inputs to the learning systems and
word/character embeddings were used for the tasks. Our work is different in that we explore distributed
representations at the feature level and we can make full use of well-established hand-designed features.
We use large amounts of raw data to infer feature embeddings. There are several previous studies
relevant to using raw data for dependency parsing. Koo et al. (2008) used the Brown algorithm to learn
word clusters from a large amount of unannotated data and defined a set of word cluster-based features
for dependency parsing models. Suzuki et al. (2009) adapted a Semi-supervised Structured Conditional
Model (SS-SCM) to dependency parsing. Suzuki et al. (2011) reported the best results so far on the
standard test sets of PTB using a condensed feature representation combined with the word cluster-based
features of Koo et al. (2008). Chen et al. (2013) mapped the base features into predefined types using
the information of frequencies counted in large amounts of auto-parsed data. The work of Suzuki et al.
(2011) and Chen et al. (2013) were to perform feature clustering. Ando and Zhang (2005) presented
a semi-supervised learning algorithm named alternating structure optimization for text chunking. They
used a large projection matrix to map sparse base features into a small number of high level features over
a large number of auxiliary problems. One of the advantages of our approach is that it is simpler and
more general than that of Ando and Zhang (2005). Our approach can easily be applied to other tasks by
defining new feature contexts.
7 Conclusion
In this paper, we have presented an approach to learning feature embeddings for dependency parsing from
large amounts of raw data. Based on the feature embeddings, we represented a set of new features, which
was used with the base features in a graph-based model. When tested on both English and Chinese, our
method significantly improved the performance over the baselines and provided comparable accuracy
with the best systems in the literature.
Acknowledgments
Wenliang Chen was supported by the National Natural Science Foundation of China (Grant No.
61203314) and Yue Zhang was supported by MOE grant 2012-T2-2-163. We would also thank the
anonymous reviewers for their detailed comments, which have helped us to improve the quality of this
work.
References
R.K. Ando and T. Zhang. 2005. A high-performance semi-supervised learning method for text chunking. ACL.
Yoshua Bengio. 2008. Neural net language models. In Scholarpedia, page 3881.
Yoshua Bengio. 2009. Learning deep architectures for AI. Foundations and trends
R
? in Machine Learning,
2(1):1?127.
Bernd Bohnet and Joakim Nivre. 2012. A transition-based system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In Proceedings of EMNLP-CoNLL 2012, pages 1455?1465. Association
for Computational Linguistics.
824
Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd
International Conference on Computational Linguistics (Coling 2010), pages 89?97, Beijing, China, August.
Coling 2010 Organizing Committee.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, T. J. Watson, Vincent J. Della Pietra, and Jenifer C. Lai. 1992.
Class-Based n-gram Models of Natural Language. Computational Linguistics.
Xavier Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 957?961, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto, and Kentaro Torisawa. 2009. Improving dependency
parsing with subtrees from auto-parsed data. In Proceedings of EMNLP 2009, pages 570?579, Singapore,
August.
Wenliang Chen, Min Zhang, and Yue Zhang. 2013. Semi-supervised feature transformation for dependency
parsing. In Proceedings of EMNLP 2013, pages 1303?1313, Seattle, Washington, USA, October. Association
for Computational Linguistics.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493?2537.
Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. J. Mach.
Learn. Res., 3:951?991.
James Curran. 2005. Supersense tagging of unknown nouns using semantic similarity. In Proceedings of the
43rd Annual Meeting of the Association for Computational Linguistics (ACL?05), pages 26?33, Ann Arbor,
Michigan, June. Association for Computational Linguistics.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilistic models for action-based chinese dependency parsing.
In Proceedings of ECML/ECPPKDD, Warsaw, Poland.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2011. Incremental joint pos tagging and
dependency parsing in chinese. In Proceedings of 5th International Joint Conference on Natural Language
Processing, pages 1216?1224, Chiang Mai, Thailand, November. Asian Federation of Natural Language Pro-
cessing.
Terry Koo and Michael Collins. 2010. Efficient third-order dependency parsers. In Proceedings of ACL 2010,
pages 1?11, Uppsala, Sweden, July. Association for Computational Linguistics.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of
ACL-08: HLT, Columbus, Ohio, June.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara.
2009. An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging. In
Proceedings of ACL-IJCNLP2009, pages 513?521, Suntec, Singapore, August. Association for Computational
Linguistics.
Zhenghua Li, Wanxiang Che, and Ting Liu. 2011a. Improving chinese pos tagging with dependency parsing. In
Proceedings of 5th International Joint Conference on Natural Language Processing, pages 1447?1451, Chiang
Mai, Thailand, November. Asian Federation of Natural Language Processing.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wenliang Chen, and Haizhou Li. 2011b. Joint models for
chinese pos tagging and dependency parsing. In Proceedings of EMNLP 2011, UK, July.
Zhenghua Li, Min Zhang, Wanxiang Che, and Ting Liu. 2012. A separately passive-aggressive training algo-
rithm for joint pos tagging and dependency parsing. In Proceedings of the 24rd International Conference on
Computational Linguistics (Coling 2012), Mumbai, India. Coling 2012 Organizing Committee.
Dekang Lin. 1997. Using syntactic dependency as local context to resolve word sense ambiguity. In Proceedings
of the 35th Annual Meeting of the Association for Computational Linguistics, pages 64?71, Madrid, Spain, July.
Association for Computational Linguistics.
R. McDonald and J. Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In Pro-
ceedings of EMNLP-CoNLL, pages 122?131.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency
parsers. In Proceedings of ACL 2005, pages 91?98. Association for Computational Linguistics.
825
Tomas Mikolov, Jiri Kopecky, Lukas Burget, Ondrej Glembek, and Jan Cernocky. 2009. Neural network based
language models for highly inflective languages. In Proceedings of ICASSP 2009, pages 4725?4728. IEEE.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations
in vector space. arXiv preprint arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013b. Distributed representations of
words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, pages
3111?3119.
J. Nivre and R. McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceed-
ings of ACL-08: HLT, Columbus, Ohio, June.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based dependency parsing. In Proc. of CoNLL 2004, pages 49?56.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 shared
task on dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages
915?932.
Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133?142.
Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013. Parsing with compositional vector
grammars. In Proceedings of ACL 2013. Citeseer.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing. In Proceedings of EMNLP2009, pages 551?560, Singa-
pore, August. Association for Computational Linguistics.
Jun Suzuki, Hideki Isozaki, and Masaaki Nagata. 2011. Learning condensed feature representations from large
unsupervised data sets for supervised learning. In Proceedings of ACL2011, pages 636?641, Portland, Oregon,
USA, June. Association for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method
for semi-supervised learning. In Proceedings of ACL 2010, pages 384?394. Association for Computational
Linguistics.
Xianchao Wu, Jie Zhou, Yu Sun, Zhanyi Liu, Dianhai Yu, Hua Wu, and Haifeng Wang. 2013. Generalization of
words for chinese dependency parsing. In Proceedings of IWPT 2013, pages 73?81.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In
Proceedings of IWPT 2003, pages 195?206.
Y. Zhang and S. Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transition-based
dependency parsing. In Proceedings of EMNLP 2008, pages 562?571, Honolulu, Hawaii, October.
Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Pro-
ceedings of ACL-HLT2011, pages 188?193, Portland, Oregon, USA, June. Association for Computational Lin-
guistics.
Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013. Deep learning for chinese word segmentation and pos
tagging. In Proceedings of EMNLP 2013, pages 647?657. Association for Computational Linguistics.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011. Exploiting web-derived selectional preference to improve
statistical dependency parsing. In Proceedings of ACL-HLT2011, pages 1556?1565, Portland, Oregon, USA,
June. Association for Computational Linguistics.
826
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 843?852,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Fast Decoder for Joint Word Segmentation and POS-Tagging Using a
Single Discriminative Model
Yue Zhang and Stephen Clark
University of Cambridge Computer Laboratory
William Gates Building,
15 JJ Thomson Avenue,
Cambridge CB3 0FD, UK
{yue.zhang, stephen.clark}@cl.cam.ac.uk
Abstract
We show that the standard beam-search al-
gorithm can be used as an efficient decoder
for the global linear model of Zhang and
Clark (2008) for joint word segmentation and
POS-tagging, achieving a significant speed im-
provement. Such decoding is enabled by:
(1) separating full word features from par-
tial word features so that feature templates
can be instantiated incrementally, according to
whether the current character is separated or
appended; (2) deciding the POS-tag of a poten-
tial word when its first character is processed.
Early-update is used with perceptron training
so that the linear model gives a high score to a
correct partial candidate as well as a full out-
put. Effective scoring of partial structures al-
lows the decoder to give high accuracy with a
small beam-size of 16. In our 10-fold cross-
validation experiments with the Chinese Tree-
bank, our system performed over 10 times as
fast as Zhang and Clark (2008) with little ac-
curacy loss. The accuracy of our system on
the standard CTB 5 test was competitive with
the best in the literature.
1 Introduction and Motivation
Several approaches have been proposed to solve
word segmentation and POS-tagging jointly, includ-
ing the reranking approach (Shi and Wang, 2007;
Jiang et al, 2008b), the hybrid approach (Nakagawa
and Uchimoto, 2007; Jiang et al, 2008a), and the
single-model approach (Ng and Low, 2004; Zhang
and Clark, 2008; Kruengkrai et al, 2009). These
methods led to accuracy improvements over the tra-
ditional, pipelined segmentation and POS-tagging
baseline by avoiding segmentation error propagation
and making use of part-of-speech information to im-
prove segmentation.
The single-model approach to joint segmentation
and POS-tagging offers consistent training of all in-
formation, concerning words, characters and parts-
of-speech. However, exact inference with dynamic
programming can be infeasible if features are de-
fined over a large enough range of the output, such
as over a two-word history. In our previous work
(Zhang and Clark, 2008), which we refer to as
Z&C08 from now on, we used an approximate de-
coding algorithm that keeps track of a set of partially
built structures for each character, which can be seen
as a dynamic programming chart which is greatly re-
duced by pruning.
In this paper we follow the line of single-model
research, in particular the global linear model of
Z&C08. We show that effective decoding can be
achieved with standard beam-search, which gives
significant speed improvements compared to the de-
coding algorithm of Z&C08, and achieves accura-
cies that are competitive with the state-of-the-art.
Our research is also in line with recent research on
improving the speed of NLP systems with little or
no accuracy loss (Charniak et al, 2006; Roark and
Hollingshead, 2008).
Our speed improvement is achieved by the use
of a single-beam decoder. Given an input sentence,
candidate outputs are built incrementally, one char-
acter at a time. When each character is processed,
it is combined with existing candidates in all possi-
ble ways to generate new candidates, and an agenda
is used to keep the N -best candidate outputs from
843
the begining of the sentence to the current character.
Compared to the multiple-beam search algorithm of
Z&C08, the use of a single beam can lead to an order
of magnitude faster decoding speed.
1.1 The processing of partial words
An important problem that we solve in this paper
is the handling of partial words with a single beam
decoder for the global model. As we pointed out
in Z&C08, it is very difficult to score partial words
properly when they are compared with full words,
although such comparison is necessary for incre-
mental decoding with a single-beam. To allow com-
parisons with full words, partial words can either be
treated as full words, or handled differently.
We showed in Z&C08 that a naive single-beam
decoder which treats partial words in the same way
as full words failed to give a competitive accu-
racy. An important reason for the low accuracy is
over-segmentation during beam-search. Consider
the three characters ???? (tap water)?. The first
two characters do not make sense when put together
as a single word. Rather, when treated as two single-
character words, they can make sense in a sentence
such as ?? (please)? (self)? (come)? (take)?.
Therefore, when using single-beam search to pro-
cess ???? (tap water)?, the two-character word
candidate ???? is likely to have been thrown off
the agenda before the third character ??? is con-
sidered, leading to an unrecoverable segmentation
error.
This problem is even more severe for a joint seg-
mentor and POS-tagger than for a pure word seg-
mentor, since the POS-tags and POS-tag bigram of
??? and ??? further supports them being separated
when ??? is considered. The multiple-beam search
decoder we proposed in Z&C08 can be seen as a
means to ensure that the three characters ?????
always have a chance to be considered as a single
word. It explores candidate segmentations from the
beginning of the sentence until each character, and
avoids the problem of processing partial words by
considering only full words. However, since it ex-
plores a larger part of the search space than a single-
beam decoder, its time complexity is correspond-
ingly higher.
In this paper, we treat partial words differently
from full words, so that in the previous example,
the decoder can take the first two characters in ??
?? (tap water)? as a partial word, and keep it
in the beam before the third character is processed.
One challenge is the representation of POS-tags for
partial words. The POS of a partial word is unde-
fined without the corresponding full word informa-
tion. Though a partial word can make sense with
a particular POS-tag when it is treated as a com-
plete word, this POS-tag is not necessarily the POS of
the full word which contains the partial word. Take
the three-character sequence ????? as an exam-
ple. The first character ??? represents a single-
character word ?below?, for which the POS can be
LC or VV. The first two characters ???? repre-
sent a two-character word ?rain?, for which the POS
can be VV. Moreover, all three characters when put
together make the word ?rainy day?, for which the
POS is NN. As discussed above, assigning POS tags
to partial words as if they were full words leads to
low accuracy.
An obvious solution to the above problem is not to
assign a POS to a partial word until it becomes a full
word. However, lack of POS information for partial
words makes them less competitive compared to full
words in the beam, since the scores of full words are
futher supported by POS and POS ngram informa-
tion. Therefore, not assigning POS to partial words
potentially leads to over segmentation. In our exper-
iments, this method did not give comparable accura-
cies to our Z&C08 system.
In this paper, we take a different approach, and
assign a POS-tag to a partial word when its first char-
acter is separated from the final character of the pre-
vious word. When more characters are appended to
a partial word, the POS is not changed. The idea is
to use the POS of a partial word as the predicted POS
of the full word it will become. Possible predictions
are made with the first character of the word, and the
likely ones will be kept in the beam for the next pro-
cessing steps. For example, with the three characters
?????, we try to keep two partial words (besides
full words) in the beam when the first word ??? is
processed, with the POS being VV and NN, respec-
tively. The first POS predicts the two-character word
?????and the second the three-character word
?????. Now when the second character is pro-
cessed, we still need to maintain the possible POS
NN in the agenda, which predicts the three-character
844
word ?????.
As a main contribution of this paper, we show that
the mechanism of predicting the POS at the first char-
acter gives competitive accuracy. This mechanism
can be justified theoretically. Unlike alphabetical
languages, each Chinese character represents some
specific meanings. Given a character, it is natural for
a human speaker to know immediately what types
of words it can start. The allows the knowledge of
possible POS-tags of words that a character can start,
using information about the character from the train-
ing data. Moreover, the POS of the previous words to
the current word are also useful in deciding possible
POS for the word.1
The mechanism of first-character decision of POS
also boosts the efficiency, since the enumeration of
POS is unecessary when a character is appended to
the end of an existing word. As a result, the com-
plexity of each processing step is reduce by half
compared to a method without POS prediction.
Finally, an intuitive way to represent the status of
a partial word is using a flag explicitly, which means
an early decision of the segmentation of the next in-
coming character. We take a simpler alternative ap-
proach, and treat every word as a partial word un-
til the next incoming character is separated from the
last character of this word. Before a word is con-
firmed as a full word, we only apply to it features
that represent its current partial status, such as char-
acter bigrams, its starting character and its part-of-
speech, etc. Full word features, including the first
and last characters of a word, are applied immedi-
ately after a word is confirmed as complete.
An important component for our proposed system
is the training process, which needs to ensure that
the model scores a partial word with predicted POS
properly. We use the averaged perceptron (Collins,
2002) for training, together with the ?early update?
mechanism of Collins and Roark (2004). Rather
than updating the parameters after decoding is com-
plete, the modified algorithm updates parameters at
any processing step if the correct partial candidate
falls out of the beam.
In our experiments using the Chinese Treebank
1The next incoming characters are also a useful source
of information for predicting the POS. However, our system
achieved competitive accuracy with Z&C08 without such char-
acter lookahead features.
data, our system ran an order of magnitude faster
than our Z&C08 system with little loss of accuracy.
The accuracy of our system was competitive with
other recent models.
2 Model and Feature Templates
We use a linear model to score both partial and full
candidate outputs. Given an input x, the score of a
candidate output y is computed as:
Score(y) = ?(y) ? ~w,
where ?(y) is the global feature vector extracted
from y, and ~w is the parameter vector of the model.
Figure 1 shows the feature templates for the
model, where templates 1 ? 14 contain only seg-
mentation information and templates 15 ? 29 contain
both segmentation and POS information. Each tem-
plate is instantiated according to the current charac-
ter in the decoding process. Row ?For? shows the
conditions for template instantiation, where ?s? in-
dicates that the corresponding template is instanti-
ated when the current character starts a new word,
and ?a? indicates that the corresponding template is
instantiated when the current character does not start
a new word. In the row for feature templates, w, t
and c are used to represent a word, a POS-tag and
a character, respectively. The subscripts are based
on the current character, where w?1 represents the
first word to the left of the current character, and
p?2 represents the POS-tag on the second word to
the left of the current character, and so on. As an
example, feature template 1 is instantiated when the
current character starts a new word, and the resulting
feature value is the word to the left of this charac-
ter. start(w), end(w) and len(w) represent the first
character, the last character and the length of word
w, respectively. The length of a word is normalized
to 16 if it is larger than 16. cat(c) represents the POS
category of character c, which is the set of POS-tags
seen on character c, as we used in Z&C08.
Given a partial or complete candidate y, its global
feature vector ?(y) is computed by instantiating all
applicable feature templates from Table 1 for each
character in y, according to whether or not the char-
acter is separated from the previous character.
The feature templates are mostly taken from, or
inspired by, the feature templates of Z&C08. Tem-
plates 1, 2, 3, 4, 5, 8, 10, 12, 13, 14, 15, 19, 20,
845
Feature template For
1 w?1 s
2 w?1w?2 s
3 w?1, where len(w?1) = 1 s
4 start(w?1)len(w?1) s
5 end(w?1)len(w?1) s
6 end(w?1)c0 s
7 c?1c0 a
8 begin(w?1)end(w?1) s
9 w?1c0 s
10 end(w?2)w?1 s
11 start(w?1)c0 s
12 end(w?2)end(w?1) s
13 w?2len(w?1) s
14 len(w?2)w?1 s
15 w?1t?1 s
16 t?1t0 s
17 t?2t?1t0 s
18 w?1t0 s
19 t?2w?1 s
20 w?1t?1end(w?2) s
21 w?1t?1c0 s
22 c?2c?1c0t?1, s
where len(w?1) = 1
23 start(w0)t0 s
24 t?1start(w?1) s
25 t0c0 s, a
26 c0t0start(w0) a
27 ct?1end(w?1), s
where c ? w?1 and c 6= end(w?1)
28 c0t0cat(start(w0)) s
29 ct?1cat(end(w?1)), s
where c ? w?1 and c 6= end(w?1)
30 c0t0c?1t?1 s
31 c0t0c?1 a
Table 1: Feature templates.
24, 27 and 29 concern complete word information,
and they are used in the model to differentiate cor-
rect and incorrect output structures in the same way
as our Z&C08 model. Templates 6, 7, 9, 16, 17,
18, 21, 22, 23, 25, 26 and 28 concern partial word
information, whose role in the model is to indicate
the likelihood that the partial word including the cur-
rent character will become a correct full word. They
act as guidance for the action to take for the cur-
function DECODE(sent, agenda):
CLEAR(agenda)
ADDITEM(agenda, ??)
for index in [0..LEN(sent)]:
for cand in agenda:
new? APPEND(cand, sent[index])
ADDITEM(agenda, new)
for pos in TAGSET():
new? SEP(cand, sent[index], pos)
ADDITEM(agenda, new)
agenda? N-BEST(agenda)
return BEST(agenda)
Figure 1: The incremental beam-search decoder.
rent character according to the context, and are the
crucial reason for the effectiveness of the algorithm
with a small beam-size.
2.1 Decoding
The decoding algorithm builds an output candidate
incrementally, one character at a time. Each char-
acter can either be attached to the current word or
separated as the start a new word. When the current
character starts a new word, a POS-tag is assigned to
the new word. An agenda is used by the decoder to
keep the N -best candidates during the incremental
process. Before decoding starts, the agenda is ini-
tialized with an empty sentence. When a character is
processed, existing candidates are removed from the
agenda and extended with the current character in all
possible ways, and the N -best newly generated can-
didates are put back onto the agenda. After all input
characters have been processed, the highest-scored
candidate from the agenda is taken as the output.
Pseudo code for the decoder is shown in Figure
1. CLEAR removes all items from the agenda, AD-
DITEM adds a new item onto the agenda, N-BEST
returns the N highest-scored items from the agenda,
and BEST returns the highest-scored item from the
agenda. LEN returns the number of characters in a
sentence, and sent[i] returns the ith character from
the sentence. APPEND appends a character to the
last word in a candidate, and SEP joins a character
as the start of a new word in a candidate, assigning
a POS-tag to the new word.
846
Both our decoding algorithm and the decoding al-
gorithm of Z&C08 run in linear time. However, in
order to generate possible candidates for each char-
acter, Z&C08 uses an extra loop to search for pos-
sible words that end with the current character. A
restriction to the maximum word length is applied
to limit the number of iterations in this loop, with-
out which the algorithm would have quadratic time
complexity. In contrast, our decoder does not search
backword for the possible starting character of any
word. Segmentation ambiguities are resolved by bi-
nary choices between the actions append or sepa-
rate for each character, and no POS enumeration is
required when the character is appended. This im-
proves the speed by a significant factor.
2.2 Training
The learning algorithm is based on the generalized
perceptron (Collins, 2002), but parameter adjust-
ments can be performed at any character during the
decoding process, using the ?early update? mecha-
nism of Collins and Roark (2004).
The parameter vector of the model is initialized as
all zeros before training, and used to decode training
examples. Each training example is turned into the
raw input format, and processed in the same way as
decoding. After each character is processed, partial
candidates in the agenda are compared to the cor-
responding gold-standard output for the same char-
acters. If none of the candidates in the agenda are
correct, the decoding is stopped and the parameter
vector is updated by adding the global feature vector
of the gold-standard partial output and subtracting
the global feature vector of the highest-scored par-
tial candidate in the agenda. The training process
then moves on to the next example. However, if any
item in the agenda is the same as the correspond-
ing gold-standard, the decoding process moves to
the next character, without any change to the pa-
rameter values. After all characters are processed,
the decoder prediction is compared with the training
example. If the prediction is correct, the parame-
ter vector is not changed; otherwise it is updated by
adding the global feature vector of the training ex-
ample and subtracting the global feature vector of
the decoder prediction, just as the perceptron algo-
rithm does. The same training examples can be used
to train the model for multiple iterations. We use
the averaged parameter vector (Collins, 2002) as the
final model.
Pseudocode for the training algorithm is shown in
Figure 2. It is based on the decoding algorithm in
Figure 1, and the main differences are: (1) the train-
ing algorithm takes the gold-standard output and the
parameter vector as two additional arguments; (2)
the training algorithm does not return a prediction,
but modifies the parameter vector when necessary;
(3) lines 11 to 20 are additional lines of code for pa-
rameter updates.
Without lines 11 to 16, the training algorithm is
exactly the same as the generalized perceptron al-
gorithm. These lines are added to ensure that the
agenda contains highly probable candidates during
the whole beam-search process, and they are crucial
to the high accuracy of the system. As stated earlier,
the decoder relies on proper scoring of partial words
to maintain a set of high quality candidates in the
agenda. Updating the value of the parameter vector
for partial outputs can be seen as a means to ensure
correct scoring of partial candidates at any character.
2.3 Pruning
We follow Z&C08 and use several pruning methods,
most of which serve to to improve the accuracy by
removing irrelevant candidates from the beam. First,
the system records the maximum number of charac-
ters that a word with a particular POS-tag can have.
For example, from the Chinese Treebank that we
used for our experiments, most POS are associated
with only with one- or two-character words. The
only POS-tags that are seen with words over ten char-
acters long are NN (noun), NR (proper noun) and
CD (numbers). The maximum word length informa-
tion is initialized as all ones, and updated according
to each training example before it is processed.
Second, a tag dictionary is used to record POS-
tags associated with each word. During decoding,
frequent words and words with ?closed set? tags2
are only allowed POS-tags according to the tag dic-
tionary, while other words are allowed every POS-tag
to make candidate outputs. Whether a word is a fre-
quent word is decided by the number of times it has
been seen in the training process. Denoting the num-
2
?Closed set? tags are the set of POS-tags which are only
associated with a fixed set of words, according to the Penn Chi-
nese Treebank specifications (Xia, 2000).
847
function TRAIN(sent, agenda, gold-standard, ~w):
01: CLEAR(agenda)
02: ADDITEM(agenda, ??)
03: for index in [0..LEN(sent)]:
04: for cand in agenda:
05: new? APPEND(cand, sent[index])
06: ADDITEM(agenda, new)
07: for pos in TAGSET():
08: new? SEP(cand, sent[index], pos)
09: ADDITEM(agenda, new)
10: agenda? N-BEST(agenda)
11: for cand in agenda:
12: if cand = gold-standard[0:index]:
13: CONTINUE
14: ~w? ~w + ?(gold-standard[0:index])
15: ~w? ~w - ?(BEST(agenda))
16: return
17: if BEST(agenda) 6= gold-standard:
18: ~w? ~w + ?(gold-standard)
19: ~w? ~w - ?(BEST(agenda))
20: return
21: return
Figure 2: The incremental learning function.
ber of times the most frequent word has been seen
with M , a word is a frequent word if it has been
seen more than M/5000 + 5 times. The threshold
value is taken from Z&C08, and we did not adjust
it during development. Word frequencies are initial-
ized as zeros and updated according to each training
example before it is processed; the tag dictionary is
initialized as empty and updated according to each
training example before it is processed.
Third, we make an additional record of the initial
characters for words with ?closed set? tags. During
decoding, when the current character is added as the
start of a new word, ?closed set? tags are only as-
signed to the word if it is consistent with the record.
This type of pruning is used in addition to the tag
dictionary to prune invalid partial words, while the
tag dictionary is used to prune complete words. The
record for initial character and POS is initially empty,
and udpated according to each training example be-
fore it is processed.
Finally, at any decoding step, we group partial
candidates that are generated by separating the cur-
rent character as the start of a new word by the sig-
nature p0p?1w?1, and keep only the best among
those having the same p0p?1w?1. The signature
p0p?1w?1 is decided by the feature templates we
use: it can be shown that if two candidates cand1
and cand2 generated at the same step have the same
signature, and the score of cand1 is higher than the
score of cand2, then at any future step, the highest
scored candidate generated from cand1 will always
have a higher score than the highest scored candidate
generated from cand2.
From the above pruning methods, only the third
was not used by Z&C08. It can be seen as an extra
mechanism to help keep likely partial words in the
agenda and improve the accuracy, but which does
not give our system a speed advantage over Z&C08.
3 Experiments
We used the Chinese Treebank (CTB) data to per-
form one set of development tests and two sets of fi-
848
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 5  10  15  20  25  30
F-
m
ea
su
re
Training iteration
beam=1
beam=2
beam=4
beam=8
beam=16
beam=32
Figure 3: The influence of beam-sizes, and the conver-
gence of the perceptron.
nal tests. The CTB 4 was split into two parts, with the
CTB 3 being used for a 10-fold cross validation test
to compare speed and accuracies with Z&C08, and
the rest being used for development. The CTB 5 was
used to perform the additional set of experiments to
compare accuracies with other recent work.
We use the standard F-measure to evaluate output
accuracies. For word segmentation, precision is de-
fined as the number of correctly segmented words
divided by the total number of words in the output,
and recall is defined as the number of correctly seg-
mented words divided by the total number of words
in the gold-standard output. For joint segmentation
and POS-tagging, precision is defined as the num-
ber of correctly segmented and POS-tagged words
divided by the total number of words from the out-
put, and recall is defined as the correctly segmented
and POS-tagged words divided by the total number
of words in the gold-standard output.
All our experiments were performed on a Linux
platform, and a single 2.66GHz Intel Core 2 CPU.
3.1 Development tests
Our development data consists of 150K words in
4798 sentences. 80% of the data were randomly
chosen as the development training data, while the
rest were used as the development test data. Our de-
velopment tests were mainly used to decide the size
of the beam, the number of training iterations, the ef-
fect of partial features in beam-search decoding, and
the effect of incremental learning (i.e. early update).
Figure 3 shows the accuracy curves for joint seg-
mentation and POS-tagging by the number of train-
ing iterations, using different beam sizes. With the
size of the beam increasing from 1 to 32, the accura-
cies generally increase, while the amount of increase
becomes small when the size of the beam becomes
16. After the 10th iteration, a beam size of 32 does
not always give better accuracies than a beam size
of 16. We therefore chose 16 as the size of the beam
for our system.
The testing times for each beam size between 1
and 32 are 7.16s, 11.90s, 18.42s, 27.82s, 46.77s
and 89.21s, respectively. The corresponding speeds
in the number of sentences per second are 111.45,
67.06, 43.32, 28.68, 17.06 and 8.95, respectively.
Figure 3 also shows that the accuracy increases
with an increased number of training iterations, but
the amount of increase becomes small after the 25th
iteration. We chose 29 as the number of iterations to
train our system.
The effect of incremental training: We compare
the accuracies by incremental training using early
update and normal perceptron training. In the nor-
mal perceptron training case, lines 11 to 16 are taken
out of the training algorithm in Figure 2. The algo-
rithm reached the best performance at the 22nd iter-
ation, with the segmentation F-score being 90.58%
and joint F-score being 83.38%. In the incremental
training case, the algorithm reached the best accu-
racy at the 30th training iteration, obtaining a seg-
mentation F-score of 91.14% and a joint F-score of
84.06%.
3.2 Final tests using CTB 3
CTB 3 consists of 150K words in 10364 sentences.
We follow Z&C08 and split it into 10 equal-sized
parts. In each test, one part is taken as the test
data and the other nine are combined together as
the training data. We compare the speed and accu-
racy with the joint segmentor and tagger of Z&C08,
which is publicly available as the ZPar system, ver-
sion 0.23.
The results are shown in Table 2, where each row
shows one cross validation test. The column head-
ings ?sf?, ?jf?, ?time? and ?speed? refer to segmen-
tation F-measure, joint F-measure, testing time (in
3http://www.sourceforge.net/projects/zpar
849
Z&C08 this paper
# sf jf time speed sf jf time speed
1 97.18 93.27 557.97 1.86 97.25 93.51 44.20 23.44
2 97.65 93.81 521.63 1.99 97.66 93.97 42.07 24.26
3 96.08 91.04 444.69 2.33 95.55 90.65 39.23 26.41
4 96.31 91.93 431.04 2.40 96.37 92.15 39.54 26.20
5 96.35 91.94 508.39 2.04 95.84 91.51 43.30 23.93
6 94.48 88.63 482.78 2.15 94.25 88.53 43.77 23.67
7 95.27 90.52 361.95 2.86 95.10 90.42 41.76 24.81
8 94.98 90.01 418.54 2.47 94.87 90.30 39.81 26.02
9 95.23 90.84 471.3 2.20 95.21 90.55 42.03 26.65
10 96.49 92.11 500.72 2.08 96.33 92.12 43.12 24.03
average 96.00 91.41 469.90 2.24 95.84 91.37 41.88 24.94
Table 2: Speed and acccuracy comparisons with Z&C08 by 10-fold cross validation.
seconds) and testing speed (in the number of sen-
tences per second), respectively.
Our system gave a joint segmentation and POS-
tagging F-score of 91.37%, which is only 0.04%
lower than that of ZPar 0.2. The speed of our system
was over 10 times as fast as ZPar 0.2.
3.3 Final tests using CTB 5
We follow Kruengkrai et al (2009) and split the CTB
5 into training, development testing and testing sets,
as shown in Table 3. We ignored the development
test data since our system had been developed in pre-
vious experiments.
Kruengkrai et al (2009) made use of character
type knowledge for spaces, numerals, symbols, al-
phabets, Chinese and other characters. In the previ-
ous experiments, our system did not use any knowl-
edge beyond the training data. To make the compar-
ison fairer, we included knowledge of English let-
ters and Arabic numbers in this experiment. During
both training and decoding, English letters and Ara-
bic numbers are segmented using simple rules, treat-
ing consecutive English letters or Arabic numbers as
a single word.
The results are shown in Table 4, where row
?N07? refers to the model of Nakagawa and Uchi-
moto (2007), rows ?J08a? and ?b? refer to the mod-
els of Jiang et al (2008a) and Jiang et al (2008b),
and row ?K09? refers to the models of Kruengkrai et
al. (2009). Columns ?sf? and ?jf? refer to segmen-
tation and joint accuracies, respectively. Our system
Sections Sentences Words
Training 1?270 18,085 493,892
400?931
1001?1151
Dev 301?325 350 6,821
Test 271?300 348 8,008
Table 3: Training, development and test data on CTB 5.
sf jf
K09 (error-driven) 97.87 93.67
our system 97.78 93.67
K09 (baseline) 97.79 93.60
J08a 97.85 93.41
J08b 97.74 93.37
N07 97.83 93.32
Table 4: Accuracy comparisons with recent studies on
CTB 5.
gave comparable accuracies to these recent works,
obtaining the best (same as the error-driven version
of K09) joint F-score.
4 Related Work
The effectiveness of our beam-search decoder
showed that the joint segmentation and tagging
problem may be less complex than previously per-
ceived (Zhang and Clark, 2008; Jiang et al, 2008a).
At the very least, the single model approach with a
simple decoder achieved competitive accuracies to
what has been achieved so far by the reranking (Shi
850
and Wang, 2007; Jiang et al, 2008b) models and
an ensemble model using machine-translation tech-
niques (Jiang et al, 2008a). This may shed new light
on joint segmentation and POS-tagging methods.
Kruengkrai et al (2009) and Zhang and Clark
(2008) are the most similar to our system among
related work. Both systems use a discriminatively
trained linear model to score candidate outputs. The
work of Kruengkrai et al (2009) is based on Nak-
agawa and Uchimoto (2007), which separates the
processing of known words and unknown words,
and uses a set of segmentation tags to represent the
segmentation of characters. In contrast, our model
is conceptually simpler, and does not differentiate
known words and unknown words. Moreover, our
model is based on our previous work, in line with
Zhang and Clark (2007), which does not treat word
segmentation as character sequence labeling.
Our learning and decoding algorithms are also
different from Kruengkrai et al (2009). While Kru-
engkrai et al (2009) perform dynamic programming
and MIRA learning, we use beam-search to perform
incremental decoding, and the early-update version
of the perceptron algorithm to train the model. Dy-
namic programming is exact inference, for which
the time complexity is decided by the locality of
feature templates. In contrast, beam-search is ap-
proximate and can run in linear time. The param-
eter updating for our algorithm is conceptually and
computationally simpler than MIRA, though its per-
formance can be slightly lower. However, the early-
update mechanism we use is consistent with our in-
cremental approach, and improves the learning of
the beam-search process.
5 Conclusion
We showed that a simple beam-search decoding al-
gorithm can be effectively applied to the decoding
problem for a global linear model for joint word
segmentation and POS-tagging. By guiding search
with partial word information and performing learn-
ing for partial candidates, our system achieved sig-
nificantly faster speed with little accuracy loss com-
pared to the system of Z&C08.
The source code of our joint segmentor and POS-
tagger can be found at:
www.sourceforge.net/projects/zpar, version 0.4.
Acknowledgements
We thank Canasai Kruengkrai for discussion on effi-
ciency issues, and the anonymous reviewers for their
suggestions. Yue Zhang and Stephen Clark are sup-
ported by the European Union Seventh Framework
Programme (FP7-ICT-2009-4) under grant agree-
ment no. 247762.
References
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph
Austerweil, David Ellis, Isaac Haxton, Catherine Hill,
R. Shrivaths, Jeremy Moore, Michael Pozar, and
Theresa Vu. 2006. Multilevel coarse-to-fine PCFG
parsing. In Proceedings of HLT/NAACL, pages 168?
175, New York City, USA, June. Association for Com-
putational Linguistics.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL, pages 111?118, Barcelona, Spain, July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8, Philadelphia, USA, July.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008a. A cascaded linear model for joint Chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL/HLT, pages 897?904, Columbus,
Ohio, June.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word
lattice reranking for Chinese word segmentation and
part-of-speech tagging. In Proceedings of COLING,
pages 385?392, Manchester, UK, August.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of ACL/AFNLP, pages 513?
521, Suntec, Singapore, August.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A
hybrid approach to word segmentation and POS tag-
ging. In Proceedings of ACL Demo and Poster Ses-
sion, Prague, Czech Republic, June.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Proceedings of EMNLP,
Barcelona, Spain.
Brian Roark and Kristy Hollingshead. 2008. Classify-
ing chart cells for quadratic complexity context-free
inference. In Proceedings of COLING, pages 745?
752, Manchester, UK, August. Coling 2008 Organiz-
ing Committee.
851
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer CRF
based joint decoding method for cascade segmentation
and labelling tasks. In Proceedings of IJCAI, Hyder-
abad, India.
Fei Xia, 2000. The part-of-speech tagging guidelines for
the Chinese Treebank (3.0).
Yue Zhang and Stephen Clark. 2007. Chinese segmenta-
tion with a word-based perceptron algorithm. In Pro-
ceedings of ACL, pages 840?847, Prague, Czech Re-
public, June.
Yue Zhang and Stephen Clark. 2008. Joint word segmen-
tation and POS tagging using a single perceptron. In
Proceedings of ACL/HLT, pages 888?896, Columbus,
Ohio, June.
852
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1147?1157,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Syntax-Based Grammaticality Improvement using CCG and Guided Search
Yue Zhang
University of Cambridge
Computer Laboratory
yue.zhang@cl.cam.ac.uk
Stephen Clark
University of Cambridge
Computer Laboratory
stephen.clark@cl.cam.ac.uk
Abstract
Machine-produced text often lacks grammat-
icality and fluency. This paper studies gram-
maticality improvement using a syntax-based
algorithm based on CCG. The goal of the
search problem is to find an optimal parse tree
among all that can be constructed through se-
lection and ordering of the input words. The
search problem, which is significantly harder
than parsing, is solved by guided learning for
best-first search. In a standard word order-
ing task, our system gives a BLEU score of
40.1, higher than the previous result of 33.7
achieved by a dependency-based system.
1 Introduction
Machine-produced text, such as SMT output, often
lacks grammaticality and fluency, especially when
using n-gram language modelling (Knight, 2007).
Recent efforts have been made to improve grammat-
icality using local language models (Blackwood et
al., 2010) and global dependency structures (Wan et
al., 2009). We study grammaticality improvement
using a syntax-based system.
The task is effectively a text-to-text generation
problem where the goal is to produce a grammati-
cal sentence from an ungrammatical and fragmen-
tary input. The input can range from a bag-of-
words (Wan et al, 2009) to a fully-ordered sentence
(Blackwood et al, 2010). A general form of the
problem is to construct a grammatical sentence from
a set of un-ordered input words. However, in cases
where the base system produces fluent subsequences
within the sentence, constraints on the choice and
order of certain words can be fed to the grammati-
cality improvement system. The input may also in-
clude words beyond the output of the base system,
e.g. extra words from the SMT lattice, so that con-
tent word insertion and deletion can be performed
implicity via word selection.
We study the above task using CCG (Steedman,
2000). The main challenge is the search problem,
which is to find an optimal parse tree among all that
can be constructed with any word choice and order
from the set of input words. We use an approximate
best-first algorithm, guided by learning, to tackle
the more-than-factorial complexity. Beam-search is
used to control the volume of accepted hypotheses,
so that only a very small portion of the whole search
space is explored. The search algorithm is guided by
perceptron training, which ensures that the explored
path in the search space consists of highly proba-
ble hypotheses. This framework of best-first search
guided by learning is a general contribution of the
paper, which could be applied to problems outside
grammaticality improvement.
We evaluate our system using the generation task
of word-order recovery, which is to recover the orig-
inal word order of a fully scrambled input sentence
(Bangalore et al, 2000; Wan et al, 2009). This
problem is an instance of our general task formu-
lation, but without any input constraints, or con-
tent word selection (since all input words are used).
It is straightforward to use this task to evaluate
our system and compare with existing approaches.
Our system gave 40.1 BLEU score, higher than the
dependency-based system of Wan et al (2009), for
which a BLEU score of 33.7 was reported.
1147
2 The Grammar
Combinatory Categorial Grammar (CCG; Steedman
(2000)) is a lexicalized grammar formalism, which
associates words with lexical categories. Lexical
categories are detailed grammatical labels, typically
expressing subcategorisation information. CCG, and
parsing with CCG, has been described in detail
elsewhere (Clark and Curran, 2007; Hockenmaier,
2003); here we provide only a short description.
During CCG parsing, adjacent categories are com-
bined using CCG?s combinatory rules. For example,
a verb phrase in English (S\NP ) can combine with
an NP to its left:
NP S\NP ? S
In addition to binary rule instances, such as the
one above, there are also unary rules which operate
on a single category in order to change its type. For
example, forward type-raising can change a subject
NP into a complex category looking to the right for
a verb phrase:
NP ? S/(S\NP)
Following Hockenmaier (2003), we extract the
grammar by reading rule instances directly from the
derivations in CCGbank (Hockenmaier and Steed-
man, 2007), rather than defining the combinatory
rule schema manually as in Clark and Curran (2007).
3 The Search Algorithm
The input to the search algorithm is a set of words,
each word having a count that specifies the maxi-
mum number of times it can appear in the output.
Typically, most input words can occur only once in
the output. However, punctuation marks and func-
tion words can be given a higher count. Depending
on the fluency of the base output (e.g. the output
of the base SMT system), some constraints can be
given to specific input words, limiting their order or
identifying them as an atomic phrase, for example.
The goal of the search algorithm is to find an op-
timal parse tree (including the surface string) among
all that can be constructed via selecting and ordering
a subset of words from the input multiset. The com-
plexity of this problem is much higher than a typical
parsing problem, since there is an exponential num-
ber of word choices for the output sentence, each
with a factorial number of orderings. Moreover, dy-
namic programming packing for parsers, such as a
CKY chart, is not applicable, because of the lack of
a fixed word order.
We perform approximate search using a best-
first algorithm. Starting from single words, candi-
date parses are constructed bottom-up. Similar to a
best-first parser (Caraballo and Charniak, 1998), the
highest scored hypothesis is expanded first. A hy-
pothesis is expanded by applying CCG unary rules
to the hypothesis, or by combining the hypothesis
with existing hypotheses using CCG binary rules.
We use beam search to control the number of ac-
cepted hypotheses, so that the computational com-
plexity of expanding each hypothesis is linear in the
size of the beam. Since there is no guarantee that a
goal hypothesis will be found in polynomial time,
we apply a robustness mechanism (Riezler et al,
2002; White, 2004), and construct a default output
when no goal hypothesis is found within a time limit.
3.1 Data Structures
Edges are the basic structures that represent hy-
potheses. Each edge is a CCG constituent, spanning
a sequence of words. Similar to partial parses in a
typical chart parser, edges have recursive structures.
Depending on the number of subedges, edges can
be classified into leaf edges, unary edges and binary
edges. Leaf edges, which represent input words,
are constructed first in the search process. Existing
edges are expanded to generate new edges via unary
and binary CCG rules. An edge that meets the output
criteria is called a goal edge. In the experiments of
this paper, we define a goal edge as one that includes
all input words the correct number of times.
The signature of an edge consists of the cate-
gory label, surface string and head word of the con-
stituent. Two edges are equivalent if they share
the same signature. Given our feature definitions,
a lower scoring edge with the same signature as a
higher scoring edge cannot be part of the highest
scoring derivation.
The number of words in the surface string of an
edge is called the size of the edge. Other important
substructures of an edge include a bitvector and an
array, which stores the indices of the input words
that the edge contains. Before two edges are com-
bined using a binary CCG rule, an input check is per-
1148
formed to make sure that the total count for a word
from the two edges does not exceed the count for
that word in the input. Intuitively, an edge can record
the count of each unique input word it contains,
and perform the input check in linear time. How-
ever, since most input words typically occur once,
they can be indexed and represented by a bitvector,
which allows a constant time input check. The few
multiple-occurrence words are stored in a count ar-
ray.
In the best-first process, edges to be expanded are
ordered by their scores, and stored in an agenda.
Edges that have been expanded are stored in a chart.
There are many ways in which edges could be or-
dered and compared. Here the chart is organised as
a set of beams, each containing a fixed number of
edges with a particular size. This is similar to typical
decoding algorithms for phrase-based SMT (Koehn,
2010). In each beam, edges are ordered by their
scores, and low score edges are pruned. In addition
to pruning by the beam, only the highest scored edge
is kept among all that share the same signature.
3.2 The Search Process
Figure 1 shows pseudocode for the search algorithm.
During initialization, the agenda (a) and chart (c)
are cleared. All candidate lexical categories are as-
signed to each input word, and the resulting leaf
edges are put onto the agenda.
In the main loop, the best edge (e) is popped from
the agenda. If e is a goal hypothesis, it is appended
to a list of goals (goal), and the loop is continued
without e being expanded. If e or any equivalent
edge e? of e is already in the chart, the loop continues
without expanding e. It can be proved that any edge
in the chart must have been combined with e?, and
therefore the expansion of e is unnecessary.
Edge e is first expanded by applying unary rules,
and any new edges are put into a list (new). Next, e
is matched against each existing edge e? in the chart.
e and e? can be combined if they pass the input check,
and there is a binary rule in which the constituents
are combined. e and e? are combined in both possible
orders, and any resulting edge is added to new.
At the end of each loop, edges from new are added
to the agenda, and new is cleared. The loop contin-
ues until a stopping criterion is met. A typical stop-
ping condition is that goal contains N goal edges.
a? INITAGENDA(input)
c? INITCHART()
new? []
goal? []
while not STOP(goal, time):
e? POPBEST(a)
if GOALTEST(e)
APPEND(goal, e)
continue
for e? in c:
if EQUIV(e?, e):
continue
for e? in UNARY(e, grammar):
APPEND(new, e?)
for e? in c:
if CANCOMBINE(e, e?):
e? ? BINARY(e, e?, grammar)
APPEND(new, e?)
if CANCOMBINE(e?, e):
e? ? BINARY(e?, e, grammar)
APPEND(new, e?)
for e? in new:
ADD(a, e?)
ADD(c, e)
new? []
Figure 1: The search algorithm.
We set N to 1 in our experiments. For practical
reasons we also include a timeout stopping condi-
tion. If no goal edges are found before the timeout
is reached, a default output is constructed by the fol-
lowing procedure. First, if any two edges in the chart
pass the input check, and the words they contain
constitute the full input set, they are concatenated to
form an output string. Second, when no two edges in
the chart meet the above condition, the largest edge
e? in the chart is chosen. Then edges in the chart are
iterated over in the larger first order, with any edge
that passes the input check with e? concatenated with
e? and e? updated. The final e?, which can be shorter
than the input, is taken as the default output.
4 Model and Features
We use a discriminative linear model to score edges,
where the score of an edge e is calculated using the
global feature vector ?(e) and the parameter vector
1149
~w of the model.
SCORE(e) = ?(e) ? ~w
?(e) represents the counts of individual features
of e. It is computed incrementally as the edge is
built. At each constituent level, the incremental fea-
ture vector is extracted according to the feature tem-
plates from Table 1, and we use the term constituent
level vector ? to refer to it. So for any edge e, ?(e)
consists of features from the top rule of the hierar-
chical structure of e. ?(e) can be written as the sum
of ?(e?) of all recursive subedges e? of e, including
e itself:
?(e) =
?
e??e
?(e?)
The parameter update in Section 5 is in terms of con-
stituent level vectors.
The features in Table 1 represent patterns in-
cluding the constituent label; the head word of the
constituent; the size of the constituent; word, POS
and lexical category N-grams resulting from a bi-
nary combination; and the unary and binary rules
by which the constituent is constructed. They can
be classified roughly into ?parsing? features (those
about the parse structure, such as the binary rule)
and ?generation features? (those about the surface
string, such as word bigrams), although some fea-
tures, such as ?rule + head word + non-head word?,
contain both types of information.
5 The Learning Algorithm
The statistical model plays two important roles in
our system. First, as in typical statistical systems, it
is expected to give a higher score to a more correct
hypothesis. Second, it is also crucial to the speed of
the search algorithm, since the best-first mechanism
relies on a model to find goal hypotheses efficiently.
As an indication of the impact of the model on effi-
ciency, if the model parameters are set to all zeros,
the search algorithm cannot find a result for the first
sentence in the development data within two hours.
We perform training on a corpus of CCG deriva-
tions, where constituents in a gold-standard deriva-
tion serve as gold edges. The training algorithm
runs the decoder on each training example, updat-
ing the model when necessary, until the gold goal
condition feature
constituent + size
all edges constituent + head word
constituent + size + head word
constituent + head POS
constituent + leftmost word
constituent + rightmost word
size > 1 consti. + leftmost POS bigram
consti. + rightmost POS bigram
consti. + lmost POS + rmost POS
the binary rule
the binary rule + head word
rule + head word + non-head word
bigrams resulting from combination
binary POS bigrams resulting from combi.
edges word trigrams resulting from combi.
POS trigrams resulting from combi.
resulting lexical categary trigrams
resulting word + POS bigrams
resulting POS + word bigrams
resulting POS + word + POS trigrams
unary unary rule
edges unary rule + headw
Table 1: Feature template definitions.
edge is recovered. We use the perceptron (Rosen-
blatt, 1958) to perform parameter updates. The tra-
ditional perceptron has been adapted to structural
prediction (Collins, 2002) and search optimization
problems (Daume? III and Marcu, 2005; Shen et al,
2007). Our training algorithm can be viewed as an
adaptation of the perceptron to our best-first frame-
work for search efficiency and accuracy.
We choose to update parameters as soon as the
best edge from the agenda is not a gold-standard
edge. The intuition is that all gold edges are forced
to be above all non-gold edges on the agenda. This
is a strong precondition for parameter updates. An
alternative is to update when a gold-standard edge
falls off the chart, which corresponds to the pre-
condition for parameter updates of Daume? III and
Marcu (2005). However, due to the complexity of
our search task, we found that reasonable training
efficiency cannot be achieved by the weaker alterna-
tives. Our updates lead both to correctness (edges in
the chart are correct) and efficiency (correct edges
are found at the first possible opportunity).
1150
During a perceptron update, an incorrect predic-
tion, corresponding to the current best edge in the
agenda, is penalized, and the corresponding gold
edge is rewarded. However, in our scenario it is not
obvious what the corresponding gold edge should
be, and there are many ways in which the gold
edge could be defined. We investigated a number
of alternatives, for example trying to find the ?best
match? for the incorrect prediction. In practice we
found that the simple strategy of selecting the lowest
scored gold-standard edge in the agenda was effec-
tive, and the results presented in this paper are based
on this method.
After an update, there are at least two alterna-
tive methods to continue. The first is to reinitial-
ize the agenda and chart using the new model, and
continue until the current training example is cor-
rectly predicted. This method is called aggressive
training (Shen et al, 2007). In order to achieve
reasonable efficiency, we adopt a second approach,
which is to continue training without reinitializing
the agenda and chart. Instead, only edges from the
top of the agenda down to the lowest-scoring gold-
standard edge are given new scores according to the
new parameters.
Figure 2 shows pseudocode for the learning al-
gorithm applied to one training example. The ini-
tialization is identical to the test search, except that
the list of goal edges is not maintained. In the main
loop, the best edge e is popped off the agenda. If it
is the gold goal edge, the training for this sentence
finishes. If e is not a gold edge, parameter updates
are performed and the loop is continued with e be-
ing discarded. Only gold edges are pushed onto the
chart throughout the training process.
When updating parameters, the current non-gold
edge (e) is used as the negative example, and the
smallest gold edge in the agenda (minGold) is used
as the corresponding positive example. The model
parameters are updated by adding the constituent
level feature vector (see Section 4) of minGold, and
subtracting the constituent level feature vector of e.
Note that we do not use the global feature vector in
the update, since only the constituent level param-
eter vectors are compatible for edges with different
sizes. After a parameter update, edges are rescored
from the top of the agenda down to minGold.
The training algorithm iterates through all train-
a? INITAGENDA(input)
c? INITCHART()
new? []
while true:
e? POPBEST(a)
if GOLD(e) and GOALTEST(e):
return
if not GOLD(e):
popped? []
n? 0
while n < GOLDCOUNT(a):
e?? POPBEST(a)
APPEND(popped, e?)
if GOLD(e?):
minGold? e?
n? n+ 1
~w? ~w ? ?(e) + ?(minGold)
for e? in popped:
RECOMPUTESCORE(e?)
ADD(a, e?)
for e? in c:
RECOMPUTESCORE(e?)
continue
for e? in UNARY(e, grammar):
APPEND(new, e?)
for e? in c:
if CANCOMBINE(e, e?):
e? ? BINARY(e, e?, grammar)
APPEND(new, e?)
if CANCOMBINE(e?, e):
e? ? BINARY(e?, e, grammar)
APPEND(new, e?)
for e? in new:
ADD(a, e?)
ADD(c, e)
new? []
Figure 2: The learning algorithm.
ing examples N times, and the final parameter vec-
tor is used as the model. In our experiments, N is
chosen according to results on development data.
6 Experiments
We use CCGBank (Hockenmaier and Steedman,
2007) for experimental data. CCGbank is the CCG
version of the Penn Treebank. Sections 02?21 are
1151
used for training, section 00 is used for development
and section 23 for the final test.
Original sentences from CCGBank are trans-
formed into bags of words, with sequence informa-
tion removed, and passed to our system as input
data. The system outputs are compared to the orig-
inal sentences for evaluation. Following Wan et al
(2009), we use the BLEU metric (Papineni et al,
2002) for string comparison. Whilst BLEU is not
an ideal measure of fluency or grammaticality, be-
ing based on n-gram precision, it is currently widely
used for automatic evaluation and allows us to com-
pare directly with existing work (Wan et al, 2009).
In addition to the surface string, our system also
produces the CCG parse given an input bag of words.
The quality of the parse tree can reflect both the
grammaticality of the surface string and the quality
of the trained grammar model. However, there is
no direct way to automatically evaluate parse trees
since output word choice and order can be differ-
ent from the gold-standard. Instead, we indirectly
measure parse quality by calculating the precision of
CCG lexical categories. Since CCG lexical categories
contain so much syntactic information, they provide
a useful measure of parse quality. Again because the
word order can be different, we turn both the output
and the gold-standard into a bag of word/category
pairs, and calculate the percentage of matched pairs
as the lexical category precision.
For fair comparison with Wan et al (2009), we
keep base NPs as atomic units when preparing the
input. Wan et al (2009) used base NPs from Penn
Treebank annotation, while we extract base NPs
from the CCGBbank by taking as base NPs the NPs
that do not recursively contain other NPs. These
base NPs mostly correspond to the base NPs from
the Penn Treebank. In the training data, there are
242,813 Penn Treebank base NPs with an average
size of 1.09, and 216,670 CCGBank base NPs with
an average size of 1.19.
6.1 Development Tests
Table 2 shows a set of development experiment re-
sults after one training iteration. Three different
methods of assigning lexical categories are used.
The first (?dictionary?) is to assign all possible lex-
ical categories to each input word from the dictio-
nary. The lexical category dictionary is built using
Length Timeout
Method Timeout BLEU ratio ratio
0.5s 34.98 84.02 62.26
1s 35.40 85.66 57.87
dictionary 5s 36.27 89.05 45.79
10s 36.45 89.13 42,13
50s 37.07 92.52 32.41
0.5s 36.54 84.26 66.07
1s 37.50 86.69 58.22
? = 0.0001 5s 38.75 90.15 43.23
10s 39.14 91.35 38.36
50s 39.58 93.09 30.53
0.5s 40.87 85.66 61.27
1s 42.04 87.99 53.11
? = 0.075 5s 43.99 91.20 40.30
10s 44.23 92.14 35.70
50s 45.08 93.70 29.43
Table 2: Development tests using various levels of lexical
categories and timeouts, after one training iteration.
the training sections of CCGBank. For each word
occurring more than 20 times in the corpus, the dic-
tionary has an entry with all lexical categories the
word has been seen with. For the rest of the words,
the dictionary maintains an entry for each POS which
contains all lexical categories it has been seen with.
There are on average 26.8 different categories for
each input word by this method.
In practice, it is often unnecessary to leave lexi-
cal category disambiguation completely to the gram-
maticality improvement system. When it is reason-
able to assume that the input sentence for the gram-
maticality improvement system is sufficiently fluent,
a list of candidate lexical categories can be assigned
automatically to each word via supertagging (Clark
and Curran, 2007) on the input sequence. We use
the C&C supertagger1 to assign a set of probable
lexical categories to each input word using the gold-
standard order. When the input is noisy, the accuracy
of a supertagger tends to be lower than when the in-
put is grammatical. One way to address this problem
is to allow the supertagger to produce a larger list
of possible supertags for each input word, and leave
the ambiguity to the grammatical improvement sys-
tem. We simulate the noisy input situation by using
1http://svn.ask.it.usyd.edu.au/trac/candc/wiki/Download.
1152
Precision
dictionary 58.5%
? = 0.0001 59.7%
? = 0.075% 77.0%
Table 3: Lexical category accuracies. Timeout = 5s. 1
training iteration.
a small probability cutoff (?) value in the supertag-
ger, and supertag correctly ordered input sentences
before breaking them into bags of words. With a ?
value of 0.0001, there are 5.4 lexical categories for
each input word in the development test (which is
smaller than the dictionary case).
The average number of lexical categories per
word drops to 1.3 when ? equals 0.075, which is the
value used for parsing newspaper text in Clark and
Curran (2007). We include this ? in our experiments
to compare the effect of different ? levels.
The table shows that the BLEU score of the gram-
maticality improvement system is higher when a su-
per tagger is used, and the higher the ? value, the
better the BLEU score. In practice, the ? value
should be set in accordance with the lack of gram-
maticality and fluency in the input. The dictionary
method can be used when the output is extremely
unreliable, while a small beta value can be used if
the output is almost fluent.
Due to the default output mechanism on timeout,
the system can sometimes fail to produce sentences
that cover all input words. We choose five different
timeout settings between 0.5s to 50s, and compare
the speed/quality tradeoff. In addition to BLEU, we
report the percentage of timeouts and the ratio of the
sum of all output sentence lengths to the sum of all
input sentence lengths.
When the timeout value increases, the BLEU
score generally increases. The main effect of a larger
timeout is the increased possibility of a complete
sentence being found. As the time increases from
0.5s to 50s using the dictionary method, for exam-
ple, the average output sentence length increases
from 84% of the input length to 93%.
Table 3 shows the lexical category accuracies us-
ing the dictionary, and supertagger with different ?
levels. The timeout limit is set to 5 seconds. As
the lexical category ambiguity decreases, the accu-
Length dictionary ? = 0.0001 ? = 0.075
? 5 75.65 89.42 92.64
? 10 57.74 66.00 78.54
? 20 42.44 48.89 58.23
? 40 37.48 40.32 46.00
? 80 36.50 39.01 44.26
all 36.27 38.75 43.99
Table 4: BLEU scores measured on different lengths on
development data. Timeout = 5s. 1 training iteration.
racy increases. The best lexical category accuracy
of 77% is achieved when using a supertagger with
a ? level 0.075, the level for which the least lexical
category disambiguation is required. However, com-
pared to the 93% lexical category accuracy of a CCG
parser (Clark and Curran, 2007), which also uses a ?
level of 0.075 for the majority of sentences, the ac-
curacy of our grammaticality improvement system
is much lower. The lower score reflects the lower
quality of the parse trees produced by our system.
Besides the difference in the algorithms themselves,
one important reason is the much higher complexity
of our search problem.
Table 4 shows the BLEU scores measured by dif-
ferent sizes of input. We also give some example
output sentences in Figure 3. It can be seen from
the table that the BLEU scores are higher when the
size of input is smaller. For sentences shorter than
20 words, our system generally produces reason-
ably fluent and grammatical outputs. For longer sen-
tences, the grammaticality drops. There are three
possible reasons. First, larger constituents require
more steps to construct. The model and search algo-
rithm face many more ambiguities, and error propa-
gation is more severe. Second, the search algorithm
often fails to find a goal hypothesis before timeout,
and a default output that is less grammatical than
a complete constituent is constructed. Long sen-
tences have comparatively more input words uncov-
ered in the output. Third, the upper bound is not 100,
and presumably lower for longer sentences, because
there are many ways to generate a grammatical sen-
tence given a bag of words. For example, the bag
{ cats, chase, dogs } can produce two equally fluent
and grammatical sentences.
The relatively low score for long sentences is un-
1153
(dictionary) our products There is no asbestos in now .
(? = 0.0001) in our products now There is no asbestos .
(? = 0.075) There is no asbestos in our products now .
(dictionary) No price for the new shares has been set .
(both ?) No price has been set for the new shares .
(all) Federal Data Corp. got a $ 29.4 million Air Force contract for
intelligence data handling .
(dictionary) was a nonexecutive director of Rudolph Agnew and former chairman
of Consolidated Gold Fields PLC , this British industrial
conglomerate , 55 years old . named
(? = 0.0001) old Consolidated Gold Fields PLC , was named 55 years , former
chairman of Rudolph Agnew and a nonexecutive director of this
British industrial conglomerate .
(? = 0.075) Consolidated Gold Fields PLC , 55 years old , was named former
chairman of Rudolph Agnew and a nonexecutive director of this
British industrial conglomerate .
(dictionary) McDermott International Inc. said its Babcock & Wilcox unit
completed the sale of its Bailey Controls Operations for
Finmeccanica S.p . A. to $ 295 million .
(? = 0.0001) $ 295 million McDermott International Inc. for the sale of
its Babcock & Wilcox unit said its Bailey Controls Operations
completed to Finmeccanica S.p . A. .
(? = 0.075) McDermott International Inc. said its Bailey Controls
Operations completed the sale of Finmeccanica S.p . A. for its
Babcock & Wilcox unit to $ 295 million .
Figure 3: Example outputs on development data.
likely to be such a problem in practice, because
the base system (e.g. an SMT system) is likely to
produce sentences with locally fluent subsequences.
When fluent local phrases in the input are treated as
atomic units, the effective sentence length is shorter.
All the above development experiments were per-
formed using only one training iteration. Figure 4
shows the effect of different numbers of training it-
erations. For the final test, based on the graphs in
Figure 4, we chose the training iterations to be 8, 6
and 4 for the dictionary, ? = 0.0001 and ? = 0.075
methods, respectively.
6.2 Final Accuracies
Table 5 shows the final results of our system, to-
gether with the MST-based (?Wan 2009 CLE?)
and assignment-based (?Wan 2009 AB?) systems
of Wan et al (2009). Our system outperforms the
BLEU
Wan 2009 CLE 26.8
Wan 2009 AB 33.7
This paper dictionary 40.1
This paper ? = 0.0001 43.2
This paper ? = 0.075 50.1
Table 5: Final accuracies.
dependency grammar-based systems, and using a
supertagger with small ? value produces the best
BLEU. Note that through the use of a supertagger,
we are no longer assuming that the input is a bag of
words without any order, and therefore only the dic-
tionary results are directly comparable with Wan et
al. (2009)2.
2We also follow Wan et al (2009) by assuming each word is
associated with its POS tag.
1154
 0.34
 0.36
 0.38
 0.4
 0.42
 0.44
 0.46
 0.48
 1  2  3  4  5  6  7  8
BL
EU
training iteration
0.075
0.0001
dictionary
Figure 4: The effect of training iterations.
7 Related Work
Both Wan et al (2009) and our system use approx-
imate search to solve the problem of input word or-
dering. There are three differences. First, Wan et
al. use a dependency grammar to model grammati-
cality, while we use CCG. Compared to dependency
trees, CCG has stronger category constraints on the
parse structure. Moreover, CCG allows us to reduce
the ambiguity level of the search algorithm through
the assignment of possible lexical categories to input
words, which is useful when the input has a basic
degree of fluency, as is often the case in a grammat-
icality improvement task.
Second, we use learning to optimise search in or-
der to explore a large search space. In contrast, Wan
et al break the search problem into a sequence of
sub tasks and use greedy search to connect them.
Finally, in addition to ordering, our algorithm fur-
ther allows word selection. This gives our system
the flexibility to support word insertion and deletion.
White (2004) describes a system that performs
CCG realization using best-first search. The search
process of our algorithm is similar to his work.
The problem we solve is different from realization,
which takes an input in logical form and produces
a corresponding sentence. Without constraints, the
word order ambiguities can be much larger with a
bag of words, and we use learning to guide our
search algorithm. Espinosa et al (2008) apply hy-
pertagging to logical forms to assign lexical cate-
gories for realization. White and Rajkumar (2009)
further use perceptron reranking on N-best outputs
to improve the quality.
The use of perceptron learning to improve search
has been proposed in guided learning for easy-first
search (Shen et al, 2007) and LaSO (Daume? III and
Marcu, 2005). LaSO is a general framework for
various search strategies. Our learning algorithm is
similar to LaSO with best-first inference, but the pa-
rameter updates are different. In particular, LaSO
updates parameters when all correct hypotheses are
lost, but our algorithm makes an update as soon as
the top item from the agenda is incorrect. Our algo-
rithm updates the parameters using a stronger pre-
condition, because of the large search space. Given
an incorrect hypothesis, LaSO finds the correspond-
ing gold hypothesis for perceptron update by con-
structing its correct sibling. In contrast, our algo-
rithm takes the lowest scored gold hypothesis cur-
rently in the agenda to avoid updating parameters
for hypotheses that may have not been constructed.
Our parameter update strategy is closer to the
guided learning mechanism for the easy-first algo-
rithm of Shen et al (2007), which maintains a queue
of hypotheses during search, and performs learning
to ensure that the highest scored hypothesis in the
queue is correct. However, in easy-first search, hy-
potheses from the queue are ranked by the score of
their next action, rather than the hypothesis score.
Moreover, Shen et al use aggressive learning and
regenerate the queue after each update, but we per-
form non-agressive learning, which is faster and is
more feasible for our complex search space. Similar
methods to Shen et al (2007) have also been used
in Shen and Joshi (2008) and Goldberg and Elhadad
(2010).
8 Conclusion
We proposed a grammaticality improvement system
using CCG, and evaluated it using a standard input
word ordering task. Our system gave higher BLEU
scores than the dependency-based system of Wan et
al. (2009). We showed that the complex search prob-
lem can be solved effectively using guided learning
for best-first search.
Potential improvements to our system can be
made in several areas. First, a large scale lan-
guage model can be incorporated into our model in
the search algorithm, or through reranking. Sec-
ond, a heuristic future cost (e.g. Varges and Mel-
1155
lish (2010)) can be considered for each hypothesis
so that it also considers the words that have not been
used, leading to better search. Future work also in-
cludes integration with an SMT system, where con-
tent word selection will be applicable.
Acknowledgements
We thank Graeme Blackwood, Bill Byrne, Adria` de
Gispert, Stephen Wan and the anonymous reviewers
for their discussions and suggestions. Yue Zhang
and Stephen Clark are supported by the European
Union Seventh Framework Programme (FP7-ICT-
2009-4) under grant agreement no. 247762.
References
Srinivas Bangalore, Owen Rambow, and Steve Whittaker.
2000. Evaluation metrics for generation. In Proceed-
ings of the First International Natural Language Gen-
eration Conference (INLG2000), Mitzpe, pages 1?8.
Graeme Blackwood, Adria` de Gispert, and William
Byrne. 2010. Fluency constraints for minimum bayes-
risk decoding of statistical machine translation lattices.
In Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010), pages
71?79, Beijing, China, August. Coling 2010 Organiz-
ing Committee.
Sharon A. Caraballo and Eugene Charniak. 1998. New
figures of merit for best-first probabilistic chart pars-
ing. Comput. Linguist., 24:275?298, June.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493?552.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8, Philadelphia, USA.
Hal Daume? III and Daniel Marcu. 2005. Learning as
search optimization: approximate large margin meth-
ods for structured prediction. In ICML, pages 169?
176.
Dominic Espinosa, Michael White, and Dennis Mehay.
2008. Hypertagging: Supertagging for surface real-
ization with CCG. In Proceedings of ACL-08: HLT,
pages 183?191, Columbus, Ohio, June. Association
for Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 742?750, Los Angeles, California, June.
Association for Computational Linguistics.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Compu-
tational Linguistics, 33(3):355?396.
Julia Hockenmaier. 2003. Parsing with generative mod-
els of predicate-argument structure. In Proceedings of
the 41st Meeting of the ACL, pages 359?366, Sapporo,
Japan.
Kevin Knight. 2007. Automatic language translation
generation help needs badly. In MT Summit XI Work-
shop on Using Corpora for NLG: Keynote Address.
Phillip Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. III Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using
a lexical-functional grammar and discriminative esti-
mation techniques. In Proceedings of 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 271?278, Philadelphia, Pennsylvania,
USA, July. Association for Computational Linguistics.
F. Rosenblatt. 1958. The perceptron: A probabilistic
model for information storage and organization in the
brain. Psychological Review, 65:386?408.
Libin Shen and Aravind Joshi. 2008. LTAG dependency
parsing with bidirectional incremental construction.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 495?
504, Honolulu, Hawaii, October. Association for Com-
putational Linguistics.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In Proceedings of ACL, pages 760?767, Prague,
Czech Republic, June.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, Mass.
Sebastian Varges and Chris Mellish. 2010. Instance-
based natural language generation. Natural Language
Engineering, 16(3):309?346.
Stephen Wan, Mark Dras, Robert Dale, and Ce?cile Paris.
2009. Improving grammaticality in statistical sen-
tence generation: Introducing a dependency spanning
tree algorithm with an argument satisfaction model.
1156
In Proceedings of the 12th Conference of the Euro-
pean Chapter of the ACL (EACL 2009), pages 852?
860, Athens, Greece, March. Association for Compu-
tational Linguistics.
Michael White and Rajakrishnan Rajkumar. 2009. Per-
ceptron reranking for CCG realization. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 410?419, Singapore,
August. Association for Computational Linguistics.
Michael White. 2004. Reining in CCG chart realization.
In Proc. INLG-04, pages 182?191.
1157
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1303?1313,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Semi-supervised Feature Transformation for Dependency Parsing
Wenliang Chen?, Min Zhang??, and Yue Zhang?
?School of Computer Science and Technology, Soochow University, China
?Singapore University of Technology and Design, Singapore
{wlchen, mzhang}@suda.edu.cn
yue zhang@sutd.edu.sg
Abstract
In current dependency parsing models, con-
ventional features (i.e. base features) defined
over surface words and part-of-speech tags
in a relatively high-dimensional feature space
may suffer from the data sparseness problem
and thus exhibit less discriminative power on
unseen data. In this paper, we propose a
novel semi-supervised approach to address-
ing the problem by transforming the base fea-
tures into high-level features (i.e. meta fea-
tures) with the help of a large amount of au-
tomatically parsed data. The meta features are
used together with base features in our final
parser. Our studies indicate that our proposed
approach is very effective in processing un-
seen data and features. Experiments on Chi-
nese and English data sets show that the fi-
nal parser achieves the best-reported accuracy
on the Chinese data and comparable accuracy
with the best known parsers on the English
data.
1 Introduction
In recent years, supervised learning models have
achieved lots of progress in the dependency pars-
ing task, as can be found in the CoNLL shared
tasks (Buchholz and Marsi, 2006; Nivre et al,
2007). The supervised models take annotated data
as training data, utilize features defined over surface
words, part-of-speech tags, and dependency trees,
and learn the preference of features via adjusting
feature weights.
?Corresponding author
In the supervised learning scenarios, many previ-
ous studies explore rich feature representation that
leads to significant improvements. McDonald and
Pereira (2006) and Carreras (2007) define second-
order features over two adjacent arcs in second-
order graph-based models. Koo and Collins (2010)
use third-order features in a third-order graph-based
model. Bohnet (2010) considers information of
more surrounding words for the graph-based mod-
els, while Zhang and Nivre (2011) define a set
of rich features including the word valency and
the third-order context features for transition-based
models. All these models utilize richer and more
complex feature representations and achieve better
performance than the earlier models that utilize the
simpler features (McDonald et al, 2005; Yamada
and Matsumoto, 2003; Nivre and Scholz, 2004).
However, the richer feature representations result in
a high-dimensional feature space. Features in such a
space may suffer from the data sparseness problem
and thus have less discriminative power on unseen
data. If input sentences contain unknown features
that are not included in training data, the parsers can
usually give lower accuracy.
Several methods have been proposed to alleviate
this problem by using large amounts of unannotated
data, ranging from self-training and co-training (Mc-
Closky et al, 2006; Sagae and Tsujii, 2007) to more
complex methods that collect statistical information
from unannotated sentences and use them as addi-
tional features (Koo et al, 2008; Chen et al, 2009).
In this paper, we propose an alternative approach
to semi-supervised dependency parsing via feature
transformation (Ando and Zhang, 2005). More
1303
specifically, we transform base features to a higher-
level space. The base features defined over surface
words, part-of-speech tags, and dependency trees
are high dimensional and have been explored in the
above previous studies. The higher-level features,
which we call meta features, are low dimensional,
and newly defined in this paper. The key idea be-
hind is that we build connections between known
and unknown base features via the meta features.
From another viewpoint, we can also interpret the
meta features as a way of doing feature smoothing.
Our feature transfer method is simpler than that of
Ando and Zhang (2005), which is based on splitting
the original problem into multiple auxiliary prob-
lems. In our approach, the base features are grouped
and each group relates to a meta feature. In the first
step, we use a baseline parser to parse a large amount
of unannotated sentences. Then we collect the base
features from the parse trees. The collected features
are transformed into predefined discrete values via a
transformation function. Based on the transformed
values, we define a set of meta features. Finally, the
meta features are incorporated directly into parsing
models.
To demonstrate the effectiveness of the proposed
approach, we apply it to the graph-based parsing
models (McDonald and Nivre, 2007). We conduct
experiments on the standard data split of the Penn
English Treebank (Marcus et al, 1993) and the Chi-
nese Treebank Version 5.1 (Xue et al, 2005). The
results indicate that the approach significantly im-
proves the accuracy. In summary, we make the fol-
lowing contributions:
? We define a simple yet useful transformation
function to transform base features to meta fea-
tures automatically. The meta features build
connections between known and unknown base
features, and relieve the data sparseness prob-
lem.
? Compared to the base features, the number of
meta features is remarkably small.
? We build semi-supervised dependency parsers
that achieve the best accuracy on the Chinese
data and comparable accuracy with the best
known systems on the English data.
The rest of this paper is organized as follows. Sec-
tion 2 introduces the graph-based parsing model.
Section 3 describes the meta features and meta
parser. Section 4 describes the experiment settings
and reports the experimental results on English and
Chinese data sets. Section 5 discusses related work.
Finally, in Section 6 we summarize the proposed ap-
proach.
2 Baseline parser
In this section, we introduce a graph-based pars-
ing model proposed by McDonald et al (2005) and
build a baseline parser.
2.1 Graph-based parsing model
Given an input sentence, dependency parsing is
to build a dependency tree. We define X as
the set of possible input sentences, Y as the
set of possible dependency trees, and D =
(x1, y1), ..., (xi, yi), ..., (xn, yn) as a training set of
n pairs of xi ? X and yi ? Y . A sentence is de-
noted by x = (w0, w1, ..., wi, ..., wm), where w0 is
ROOT and does not depend on any other word and
wi refers to a word.
In the graph-based model, we define ordered pair
(wi, wj) ? y as a dependency relation in tree y from
word wi to word wj (wi is the head and wj is the
dependent), Gx as a graph that consists of a set of
nodes Vx = {w0, w1, ..., wi, ..., wm} and a set of
arcs (edges) Ex = {(wi, wj)|i ?= j, wi ? Vx, wj ?
(Vx?{w0})}. The parsing model of McDonald et al
(2005) is to search for the maximum spanning tree
(MST) in graph Gx. We denote Y (Gx) as the set
of all the subgraphs of Gx that are valid dependency
trees (McDonald and Nivre, 2007) for sentence x.
We define the score of a dependency tree y ?
Y (Gx) to be the sum of the subgraph scores,
score(x, y) =
?
g?y
score(x, g) (1)
where g is a spanning subgraph of y, which can be a
single arc or adjacent arcs. In this paper we assume
the dependency tree to be a spanning projective tree.
The model scores each subgraph using a linear rep-
resentation. Then scoring function score(x, g) is,
score(x, g) = f(x, g) ? w (2)
where f(x, g) is a high-dimensional feature vector
based on features defined over g and x and w refers
to the weights for the features.
1304
The maximum spanning tree is the highest scoring
tree in Y (Gx). The task of decoding algorithms in
the parsing model for an input sentence x is to find
y?, where
y? = argmax
y?Y (Gx)
score(x, y)
= argmax
y?Y (Gx)
?
g?y
score(x, g)
= argmax
y?Y (Gx)
?
g?y
f(x, g) ? w (3)
In our system, we use the decoding algorithm
proposed by Carreras (2007), which is a second-
order CKY-style algorithm (Eisner, 1996) and fea-
ture weights w are learned during training using the
Margin Infused Relaxed Algorithm (MIRA) (Cram-
mer and Singer, 2003; McDonald et al, 2005).
2.2 Base features
Previous studies have defined different sets of fea-
tures for the graph-based parsing models, such as
the first-order features defined in McDonald et al
(2005), the second-order parent-siblings features de-
fined in McDonald and Pereira (2006), and the
second-order parent-child-grandchild features de-
fined in Carreras (2007). Bohnet (2010) explorers
a richer set of features than the above sets. We fur-
ther extend the features defined by Bohnet (2010)
by introducing more lexical features as the base fea-
tures. The base feature templates are listed in Table
1, where h, d refer to the head, the dependent re-
spectively, c refers to d?s sibling or child, b refers
to the word between h and d, +1 (?1) refers to the
next (previous) word, w and p refer to the surface
word and part-of-speech tag respectively, [wp] refers
to the surface word or part-of-speech tag, d(h, d) is
the direction of the dependency relation between h
and d, and d(h, d, c) is the directions of the relation
among h, d, and c. We generate the base features
based on the above templates.
2.3 Baseline parser
We train a parser with the base features as the Base-
line parser. We define fb(x, g) as the base features
and wb as the corresponding weights. The scoring
function becomes,
score(x, g) = fb(x, g) ? wb (4)
3 Meta features
In this section, we propose a semi-supervised ap-
proach to transform the features in the base feature
space (FB) to features in a higher-level space (FM )
with the following properties:
? The features in FM are able to build connec-
tions between known and unknown features in
FB and therefore should be highly informative.
? The transformation should be learnable based
on a labeled training set and an automatically
parsed data set, and automatically computable
for the test sentences.
The features in FM are referred to as meta fea-
tures. In order to perform the feature transformation,
we choose to define a simple yet effective mapping
function. Based on the mapped values, we define
feature templates for generating the meta features.
Finally, we build a new parser with the base and
meta features.
3.1 Template-based mapping function
We define a template-based function for mapping
the base features to predefined discrete values. We
first put the base features into several groups and
then perform mapping.
We have a set of base feature templates TB . For
each template Ti ? TB , we can generate a set of
base features Fi from dependency trees in the parsed
data, which is automatically parsed by the Baseline
parser. We collect the features and count their fre-
quencies. The collected features are sorted in de-
creasing order of frequencies. The mapping function
for a base feature fb of Fi is defined as follows,
?(fb) =
?
?
?
?
?
?
?
Hi if R(fb) ? TOP10
Mi if TOP10 < R(fb) ? TOP30
Li if TOP30 < R(fb)
Oi Others
where R(fb) is the position number of fb in the
sorted list, ?Others? is defined for the base features
that are not included in the list, and TOP10 and TOP
30 refer to the position numbers of top 10% and top
30% respectively. The numbers, 10% and 30%, are
tuned on the development sets in the experiments.
For a base feature generated from template Ti, we
have four possible values: Hi, Mi, Li, and Oi. In
1305
(a) First-order standard
h[wp], d[wp], d(h,d)
h[wp], d(h,d)
dw, dp, d(h,d)
d[wp], d(h,d)
hw, hp, dw, dp, d(h,d)
hp, hw, dp, d(h,d)
hw, dw, dp, d(h,d)
hw, hp, d[wp], d(h,d)
(b) First-order Linear
hp, bp, dp, d(h,d)
hp, h+1p, d?1p, dp, d(h,d)
h?1p, hp, d?1p, dp, d(h,d)
hp, h+1p, dp, d+1p, d(h,d)
h?1p, hp, dp, d+1p, d(h,d)
(c) Second-order standard
hp, dp, cp, d(h,d,c)
hw, dw, cw, d(h,d,c)
hp, c[wp], d(h,d,c)
dp, c[wp], d(h,d,c)
hw, c[wp], d(h,d,c)
dw, c[wp], d(h,d,c)
(d) Second-order Linear
h[wp], h+1[wp], c[wp], d(h,d,c)
h?1[wp], h[wp], c[wp], d(h,d,c)
h[wp], c?1[wp], c[wp], d(h,d,c)
h[wp], c[wp], c+1[wp], d(h,d,c)
h?1[wp], h[wp], c?1[wp], c[wp], d(h,d,c)
h[wp], h+1[wp], c?1[wp], c[wp], d(h,d,c)
h?1[wp], h[wp], c[wp], c+1[wp], d(h,d,c)
h[wp], h+1[wp], c[wp], c+1[wp], d(h,d,c)
d[wp], d+1[wp], c[wp], d(h,d,c)
d?1[wp], d[wp], c[wp], d(h,d,c)
d[wp], c?1[wp], c[wp], d(h,d,c)
d[wp], c[wp], c+1[wp], d(h,d,c)
d[wp], d+1[wp], c?1[wp], c[wp], d(h,d,c)
d[wp], d+1[wp], c[wp], c+1[wp], d(h,d,c)
d?1[wp], d[wp], c?1[wp], c[wp], d(h,d,c)
d?1[wp], d[wp], c[wp], c+1[wp], d(h,d,c)
Table 1: Base feature templates
total, we have 4?N(TB) possible values for all the
base features, where N(TB) refers to the number of
the base feature templates, which is usually small.
We can obtain the mapped values of all the collected
features via the mapping function.
3.2 Meta feature templates
Based on the mapped values, we define meta fea-
ture templates in FM for dependency parsing. The
meta feature templates are listed in Table 2, where
fb is a base feature of FB , hp refers to the part-
of-speech tag of the head and hw refers to the sur-
face word of the head. Of the table, the first tem-
plate uses the mapped value only, the second and
third templates combine the value with the head in-
formation. The number of the meta features is rel-
atively small. It has 4 ? N(TB) for the first type,
4 ? N(TB) ? N(POS) for the second type, and
4 ?N(TB) ?N(WORD) for the third one, where
N(POS) refers to the number of part-of-speech
tags, N(WORD) refers to the number of words.
We remove any feature related to the surface form
if the word is not one of the Top-N most frequent
words in the training data. We used N=1000 for the
experiments for this paper. This method can reduce
the size of the feature sets. The empirical statistics
of the feature sizes at Section 4.2.2 shows that the
size of meta features is only 1.2% of base features.
[?(fb)]
[?(fb)], hp
[?(fb)], hw
Table 2: Meta feature templates
3.3 Generating meta features
We use an example to demonstrate how to gener-
ate the meta features based on the meta feature tem-
plates in practice. Suppose that we have sentence ?I
ate the meat with a fork.? and want to generate the
meta features for the relation among ?ate?, ?meat?,
and ?with?, where ?ate? is the head, ?meat? is the
dependent, and ?with? is the closest left sibling of
?meat?. Figure 1 shows the example.
We demonstrate the generating procedure using
template Tk = ?hw, dw, cw, d(h, d, c)? (the second
template of Table 1-(c) ), which contains the sur-
face forms of the head, the dependent, its sibling,
and the directions of the dependencies among h,
d, and c. We can have a base feature ?ate, meat,
with, RIGHTSIB?, where ?RIGHTSIB? refers to the
parent-siblings structure with the right direction. In
the auto-parsed data, this feature occurs 200 times
and ranks between TOP10 and TOP30. Accord-
1306
I ate the meat with a fork!!!! !!!! !!!! !!!! !!!! !!!! !!!!.
Tk:!hw,!dw,!cw,!d(h,d,c)
Fb:!ate,!meat,!with,!RIGHTSIB
" (fb)=Mk
[Mk];![Mk],!VV;![Mk],!ate
Figure 1: An example of generating meta features
ing to the mapping function, we obtain the mapped
value Mk. Finally, we have the three meta features
?[Mk]?, ?[Mk], V V ?, and ?[Mk], ate?, where V V is
the part-of-speech tag of word ?ate?. In this way,
we can generate all the meta features for the graph-
based model.
3.4 Meta parser
We combine the base features with the meta features
by a new scoring function,
score(x, g) = fb(x, g) ? wb + fm(x, g) ? wm (5)
where fb(x, g) refers to the base features, fm(x, g)
refers to the meta features, and wb and wm are
their corresponding weights respectively. The fea-
ture weights are learned during training using MIRA
(Crammer and Singer, 2003; McDonald et al,
2005). Note that wb is also retrained here.
We use the same decoding algorithm in the new
parser as in the Baseline parser. The new parser is
referred to as the meta parser.
4 Experiments
We evaluated the effect of the meta features for the
graph-based parsers on English and Chinese data.
4.1 Experimental settings
In our experiments, we used the Penn Treebank
(PTB) (Marcus et al, 1993) for English and the
Chinese Treebank version 5.1 (CTB5) (Xue et al,
2005) for Chinese. The tool ?Penn2Malt?1 was used
1http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
to convert the data into dependency structures with
the English head rules of Yamada and Matsumoto
(2003) and the Chinese head rules of Zhang and
Clark (2008). We followed the standard data splits
as shown in Table 3. Following the work of Koo et
al. (2008), we used a tagger trained on training data
to provide part-of-speech (POS) tags for the devel-
opment and test sets, and used 10-way jackknifing to
generate part-of-speech tags for the training set. We
used the MXPOST (Ratnaparkhi, 1996) tagger for
English and the CRF-based tagger for Chinese. We
used gold standard segmentation in the CTB5. The
data partition of Chinese were chosen to match pre-
vious work (Duan et al, 2007; Li et al, 2011; Hatori
et al, 2011).
train dev test
PTB 2-21 22 23
(sections)
CTB5 001-815 886-931 816-885
(files) 1001-1136 1148-1151 1137-1147
Table 3: Standard data splits
For the unannotated data in English, we used the
BLLIP WSJ corpus (Charniak et al, 2000) contain-
ing about 43 million words.2 We used the MXPOST
tagger trained on the training data to assign part-of-
speech tags and used the Baseline parser to process
the sentences of the Brown corpus. For the unanno-
tated data in Chinese, we used the Xinhua portion
of Chinese Gigaword3 Version 2.0 (LDC2009T14)
(Huang, 2009), which has approximately 311 mil-
lion words. We used the MMA system (Kruengkrai
et al, 2009) trained on the training data to perform
word segmentation and POS tagging and used the
Baseline parser to parse the sentences in the Giga-
word data.
In collecting the base features, we removed the
features which occur only once in the English data
and less than four times in the Chinese data. The
feature occurrences of one time and four times are
based on the development data performance.
We measured the parser quality by the unlabeled
attachment score (UAS), i.e., the percentage of to-
2We ensured that the text used for building the meta features
did not include the sentences of the Penn Treebank.
3We excluded the sentences of the CTB data from the Giga-
word data.
1307
kens (excluding all punctuation tokens) with the cor-
rect HEAD.We also reported the scores on complete
dependency trees evaluation (COMP).
4.2 Feature selection on development sets
We evaluated the parsers with different settings on
the development sets to select the meta features.
4.2.1 Different models vs meta features
In this section, we investigated the effect of dif-
ferent types of meta features for the models trained
on different sizes of training data on English.
There are too many base feature templates to test
one by one. We divided the templates into several
categories. Of Table 1, some templates are only re-
lated to part-of-speech tags (P), some are only re-
lated to surface words (W), and the others contain
both part-of-speech tags and surfaces (M). Table 4
shows the categories, where numbers [1?4] refer to
the numbers of words involved in templates. For ex-
ample, the templates of N3WM are related to three
words and contain the templates of W and M. Based
on different categories of base templates, we have
different sets of meta features.4
Category Example
N1P hp, d(h, d)
N1WM hw, d(h, d); hw, hp, d(h, d)
N2P hp, dp, d(h, d)
N2WM hw, dw, d(h, d);
hw, dp, d(h, d)
N3P hp, dp, cp, d(h, d, c)
N3WM hw, dw, cw, d(h, d, c);
dw, d+1p, cp, d(h, d, c)
N4P hp, h+1p, cp, c+1p, d(h, d, c)
N4WM hw, h+1w, cw, c+1w, d(h, d, c);
hw, h+1p, cp, c+1p, d(h, d, c)
Table 4: Categories of base feature templates
We randomly selected 1% and 10% of the sen-
tences respectively from the training data. We
trained the POS taggers and Baseline parsers on
these small training data and used them to process
the unannotated data. Then, we generated the meta
features based on the newly auto-parsed data. The
4We also tested the settings of dividing WM into two sub-
types: W and M. The results showed that both two sub-types
provided positive results. To simplify, we merged W and M
into one category WM.
meta parsers were trained on the different subsets
of the training data with different sets of meta fea-
tures. Finally, we have three meta parsers: MP1,
MP10, MPFULL, which were trained on 1%, 10%
and 100% of the training data.
MP1 MP10 MPFULL
Baseline 82.22 89.50 93.01
+N1P 82.42 89.48 93.08
+N1WM 82.80 89.42 93.19
+N2P 81.29 89.01 93.02
+N2WM 82.69 90.10 93.23
+N3P 83.32 89.73 93.05
+N3WM 84.47 90.75 93.80
+N4P 82.73 89.48 93.01
+N4WM 84.07 90.42 93.67
OURS 85.11 91.14 93.91
Table 5: Effect of different categories of meta features
Table 5 shows the results, where we add each cat-
egory of Table 4 individually. From the table, we
found that the meta features that are only related to
part-of-speech tags did not always help, while the
ones related to the surface words were very helpful.
We also found that MP1 provided the largest relative
improvement among the three settings. These sug-
gested that the more sparse the base features were,
the more effective the corresponding meta features
were. Thus, we built the final parsers by adding
the meta features of N1WM, N2WM, N3WM, and
N4WM. The results showed that OURS achieved
better performance than the systems with individual
sets of meta features.
4.2.2 Different meta feature types
In Table 2, there are three types of meta feature
templates. Here, the results of the parsers with dif-
ferent settings are shown in Table 6, where CORE
refers to the first type, WithPOS refers to the sec-
ond one, and WithWORD refers to the third one.
The results showed that with all the types the parser
(OURS) achieved the best. We also counted the
numbers of the meta features. Only 327,864 (or
1.2%) features were added into OURS. Thus, we
used all the three types of meta features in our final
meta parsers.
1308
System NumOfFeat UAS
Baseline 27,119,354 93.01
+CORE +498 93.84
+WithPOS +14,993 93.82
+WithWORD +312,373 93.27
OURS +327,864 93.91
Table 6: Numbers of meta features
4.3 Main results on test sets
We then evaluated the meta parsers on the English
and Chinese test sets.
4.3.1 English
The results are shown in Table 7, where Meta-
Parser refers to the meta parser. We found that the
meta parser outperformed the baseline with an ab-
solute improvement of 1.01 points (UAS). The im-
provement was significant in McNemar?s Test (p
< 10?7 ).
UAS COMP
Baseline 92.76 48.05
MetaParser 93.77 51.36
Table 7: Main results on English
4.3.2 Chinese
UAS COMP
Baseline 81.01 29.71
MetaParser 83.08 32.21
Table 8: Main results on Chinese
The results are shown in Table 8. As in the ex-
periment on English, the meta parser outperformed
the baseline. We obtained an absolute improvement
of 2.07 points (UAS). The improvement was signif-
icant in McNemar?s Test (p < 10?8 ).
In summary, Tables 7 and 8 convincingly show
the effectiveness of our proposed approach.
4.4 Different sizes of unannotated data
Here, we considered the improvement relative to the
sizes of the unannotated data used to generate the
meta features. We randomly selected the 0.1%, 1%,
and 10% of the sentences from the full data. Table
English Chinese
Baseline 92.76 81.01
TrainData 91.93 80.40
P0.1 92.82 81.58
P1 93.14 82.23
P10 93.48 82.81
FULL 93.77 83.08
Table 9: Effect of different sizes of auto-parsed data
9 shows the results, where P0.1, P1, and P10 corre-
spond to 0.1%, 1%, and 10% respectively. From the
table, we found that the parsers obtained more ben-
efits as we used more raw sentences. We also tried
generating the meta features from the training data
only, shown as TrainData in Table 9. However, the
results shows that the parsers performed worse than
the baselines. This is not surprising because only
the known base features are included in the training
data.
4.5 Comparison with previous work
4.5.1 English
Table 10 shows the performance of the previ-
ous systems that were compared, where McDon-
ald06 refers to the second-order parser of McDon-
ald and Pereira (2006), Koo10 refers to the third-
order parser with model1 of Koo and Collins (2010),
Zhang11 refers to the parser of Zhang and Nivre
(2011), Li12 refers to the unlabeled parser of Li et
al. (2012), Koo08 refers to the parser of Koo et al
(2008), Suzuki09 refers to the parser of Suzuki et al
(2009), Chen09 refers to the parser of Chen et al
(2009), Zhou11 refers to the parser of Zhou et al
(2011), Suzuki11 refers to the parser of Suzuki et al
(2011), and Chen12 refers to the parser of Chen et
al. (2012).
The results showed that our meta parser out-
performed most of the previous systems and ob-
tained the comparable accuracy with the best result
of Suzuki11 (Suzuki et al, 2011) which combined
the clustering-based word representations of Koo et
al. (2008) and a condensed feature representation.
However, our approach is much simpler than theirs
and we believe that our meta parser can be further
improved by combining their methods.
1309
Type System UAS COMP
Sup
McDonald06 91.5
Koo10 93.04 -
Zhang11 92.9 48.0
Li12 93.12 -
Our Baseline 92.76 48.05
Semi
Koo08 93.16
Suzuki09 93.79
Chen09 93.16 47.15
Zhou11 92.64 46.61
Suzuki11 94.22 -
Chen12 92.76 -
MetaParser 93.77 51.36
Table 10: Relevant results for English. Sup denotes the
supervised parsers, Semi denotes the parsers with semi-
supervised methods.
4.5.2 Chinese
Table 11 shows the comparative results, where
Li11 refers to the parser of Li et al (2011), Hatori11
refers to the parser of Hatori et al (2011), and Li12
refers to the unlabeled parser of Li et al (2012). The
reported scores on this data were produced by the
supervised learning methods and our Baseline (su-
pervised) parser provided the comparable accuracy.
We found that the score of our meta parser for this
data was the best reported so far and significantly
higher than the previous scores. Note that we used
the auto-assigned POS tags in the test set to match
the above previous studies.
System UAS COMP
Li11 80.79 29.11
Hatori11 81.33 29.90
Li12 81.21 -
Our Baseline 81.01 29.71
MetaParser 83.08 32.21
Table 11: Relevant results for Chinese
4.6 Analysis
Here, we analyzed the effect of the meta features on
the data sparseness problem.
We first checked the effect of unknown features
on the parsing accuracy. We calculated the number
of unknown features in each sentence and computed
the average number per word. The average num-
bers were used to eliminate the influence of varied
sentence sizes. We sorted the test sentences in in-
creasing orders of these average numbers, and di-
vided equally into five bins. BIN 1 is assigned the
sentences with the smallest numbers and BIN 5 is
with the largest ones. Figure 2 shows the average
accuracy scores of the Baseline parsers against to
the bins. From the figure, we found that for both
two languages the Baseline parsers performed worse
while the sentences contained more unknown fea-
tures.
 70
 75
 80
 85
 90
 95
 100
1 2 3 4 5
A
c
c
u
r
a
c
y
BIN
EnglishChinese
Figure 2: Accuracies relative to numbers of unknown fea-
tures (average per word) by Baseline parsers
Then, we investigated the effect of the meta fea-
tures. We calculated the average number of ac-
tive meta features per word that were transformed
from the unknown features for each sentence. We
sorted the sentences in increasing order of the av-
erage numbers of active meta features and divided
them into five bins. BIN 1 is assigned the sen-
tences with the smallest numbers and BIN 5 is with
the largest ones. Figures 3 and 4 show the results,
where ?Better? is for the sentences where the meta
parsers provided better results than the baselines and
?Worse? is for those where the meta parsers pro-
vided worse results. We found that the gap between
?Better? and ?Worse? became larger while the sen-
tences contain more active meta features for the un-
known features. The gap means performance im-
provement. This indicates that the meta features are
very effective in processing the unknown features.
5 Related work
Our approach is to use unannotated data to generate
the meta features to improve dependency parsing.
1310
 0
 10
 20
 30
 40
 50
1 2 3 4 5
P
e
r
c
e
n
t
a
g
e
BIN
BetterWorse
Figure 3: Improvement relative to numbers of active meta
features on English (average per word)
 0
 10
 20
 30
 40
 50
1 2 3 4 5
P
e
r
c
e
n
t
a
g
e
BIN
BetterWorse
Figure 4: Improvement relative to numbers of active meta
features on Chinese (average per word)
Several previous studies relevant to our approach
have been conducted.
Koo et al (2008) used a word clusters trained on a
large amount of unannotated data and designed a set
of new features based on the clusters for dependency
parsing models. Chen et al (2009) extracted sub-
tree structures from a large amount of data and rep-
resented them as the additional features to improve
dependency parsing. Suzuki et al (2009) extended a
Semi-supervised Structured Conditional Model (SS-
SCM) of Suzuki and Isozaki (2008) to the depen-
dency parsing problem and combined their method
with the word clustering feature representation of
Koo et al (2008). Chen et al (2012) proposed an ap-
proach to representing high-order features for graph-
based dependency parsing models using a depen-
dency language model and beam search. In future
work, we may consider to combine their methods
with ours to improve performance.
Several previous studies used co-training/self-
training methods. McClosky et al (2006) presented
a self-training method combined with a reranking al-
gorithm for constituency parsing. Sagae and Tsujii
(2007) applied the standard co-training method for
dependency parsing. In their approaches, some au-
tomatically parsed sentences were selected as new
training data, which was used together with the orig-
inal labeled data to retrain a new parser. We are able
to use their approaches on top of the output of our
parsers.
With regard to feature transformation, the work
of Ando and Zhang (2005) is similar in spirit to our
work. They studied semi-supervised text chunking
by using a large projection matrix to map sparse base
features into a small number of high level features.
Their project matrix was trained by transforming the
original problem into a large number of auxiliary
problems, obtaining training data for the auxiliary
problems by automatically labeling raw data and us-
ing alternating structure optimization to estimate the
matrix across all auxiliary tasks. In comparison with
their approach, our method is simpler in the sense
that we do not request any intermediate step of split-
ting the prediction problem, and obtain meta fea-
tures directly from self-annotated data. The training
of our meta feature values is highly efficient, requir-
ing the collection of simple statistics over base fea-
tures from huge amount of data. Hence our method
can potentially be useful to other tasks also.
6 Conclusion
In this paper, we have presented a simple but effec-
tive semi-supervised approach to learning the meta
features from the auto-parsed data for dependency
parsing. We build a meta parser by combining the
meta features with the base features in a graph-based
model. The experimental results show that the pro-
posed approach significantly improves the accuracy.
Our meta parser achieves comparable accuracy with
the best known parsers on the English data (Penn
English Treebank) and the best accuracy on the Chi-
nese data (Chinese Treebank Version 5.1) so far.
Further analysis indicate that the meta features are
very effective in processing the unknown features.
The idea described in this paper is general and can
be applied to other NLP applications, such as part-
1311
of-speech tagging and Chinese word segmentation,
in future work.
Acknowledgments
This study was started when Wenliang Chen and
Min Zhang were members of the Department of
Human Language Technology, Institute for Info-
comm Research, Singapore. Wenliang Chen was
funded partially by the National Science Founda-
tion of China (61203314) and Yue Zhang was sup-
ported by MOE grant 2012-T2-2-163. We would
also thank the anonymous reviewers for their de-
tailed comments, which have helped us to improve
the quality of this work.
References
R.K. Ando and T. Zhang. 2005. A high-performance
semi-supervised learning method for text chunking.
ACL.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 89?97, Beijing, China,
August. Coling 2010 Organizing Committee.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL-X. SIGNLL.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 957?961, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 2000. BLLIP 1987-
89 WSJ Corpus Release 1, LDC2000T43. Linguistic
Data Consortium.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving dependency
parsing with subtrees from auto-parsed data. In Pro-
ceedings of EMNLP 2009, pages 570?579, Singapore,
August.
Wenliang Chen, Min Zhang, and Haizhou Li. 2012. Uti-
lizing dependency language models for graph-based
dependency parsing models. In Proceedings of ACL
2012, Korea, July.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilis-
tic models for action-based chinese dependency pars-
ing. In Proceedings of ECML/ECPPKDD, Warsaw,
Poland.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of
COLING1996, pages 340?345.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2011. Incremental joint pos tag-
ging and dependency parsing in chinese. In Proceed-
ings of 5th International Joint Conference on Natu-
ral Language Processing, pages 1216?1224, Chiang
Mai, Thailand, November. Asian Federation of Natu-
ral Language Processing.
Chu-Ren Huang. 2009. Tagged Chinese Gigaword Ver-
sion 2.0, LDC2009T14. Linguistic Data Consortium.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL
2010, pages 1?11, Uppsala, Sweden, July. Association
for Computational Linguistics.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceedings
of ACL-08: HLT, Columbus, Ohio, June.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of ACL-IJCNLP2009, pages
513?521, Suntec, Singapore, August. Association for
Computational Linguistics.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-
liang Chen, and Haizhou Li. 2011. Joint models for
chinese pos tagging and dependency parsing. In Pro-
ceedings of EMNLP 2011, UK, July.
Zhenghua Li, Min Zhang, Wanxiang Che, and Ting Liu.
2012. A separately passive-aggressive training algo-
rithm for joint pos tagging and dependency parsing.
In Proceedings of the 24rd International Conference
on Computational Linguistics (Coling 2012), Mumbai,
India. Coling 2012 Organizing Committee.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguisticss, 19(2):313?330.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proceedings of Coling-ACL, pages 337?344.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proceedings of EMNLP-CoNLL, pages 122?131.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL 2006, pages 81?88.
1312
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL 2005, pages 91?98.
Association for Computational Linguistics.
Joakim Nivre and Mario Scholz. 2004. Determinis-
tic dependency parsing of English text. In Proc. of
the 20th Intern. Conf. on Computational Linguistics
(COLING), pages 64?70.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 915?932.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133?142.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with LR models and parser ensem-
bles. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 1044?1050.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using Giga-word
scale unlabeled data. In Proceedings of ACL-08: HLT,
pages 665?673, Columbus, Ohio, June. Association
for Computational Linguistics.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, andMichael
Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing.
In Proceedings of EMNLP2009, pages 551?560, Sin-
gapore, August. Association for Computational Lin-
guistics.
Jun Suzuki, Hideki Isozaki, and Masaaki Nagata. 2011.
Learning condensed feature representations from large
unsupervised data sets for supervised learning. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 636?641, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. Building a Large Annotated Chinese
Corpus: the Penn Chinese Treebank. Journal of Natu-
ral Language Engineering, 11(2):207?238.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT 2003, pages 195?206.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing. In Proceedings of EMNLP
2008, pages 562?571, Honolulu, Hawaii, October.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL-HLT2011, pages 188?193, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011.
Exploiting web-derived selectional preference to im-
prove statistical dependency parsing. In Proceedings
of ACL-HLT2011, pages 1556?1565, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
1313
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 177?182,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Syntactic SMT Using a Discriminative Text Generation Model
Yue Zhang Kai Song? Linfeng Song?
SUTD, Singapore NEU, China ICT/CAS, China
yue zhang@sutd.edu.sg songkai.sk@alibaba-inc.com songlinfeng@ict.ac.cn
Jingbo Zhu Qun Liu
NEU, China CNGL, Ireland and ICT/CAS, China
zhujingbo@mail.neu.edu.cn qliu@computing.dcu.ie
Abstract
We study a novel architecture for syntactic
SMT. In contrast to the dominant approach
in the literature, the system does not rely
on translation rules, but treat translation
as an unconstrained target sentence gen-
eration task, using soft features to cap-
ture lexical and syntactic correspondences
between the source and target languages.
Target syntax features and bilingual trans-
lation features are trained consistently in
a discriminative model. Experiments us-
ing the IWSLT 2010 dataset show that the
system achieves BLEU comparable to the
state-of-the-art syntactic SMT systems.
1 Introduction
Translation rules have been central to hierarchi-
cal phrase-based and syntactic statistical machine
translation (SMT) (Galley et al., 2004; Chiang,
2005; Liu et al., 2006; Quirk et al., 2005; Marcu et
al., 2006; Shen and Joshi, 2008; Xie et al., 2011).
They are attractive by capturing the recursiveness
of languages and syntactic correspondences be-
tween them. One important advantage of trans-
lation rules is that they allow efficient decoding
by treating MT as a statistical parsing task, trans-
forming a source sentence to its translation via re-
cursive rule application.
The efficiency takes root in the fact that target
word orders are encoded in translation rules. This
fact, however, also leads to rule explosion, noise
and coverage problems (Auli et al., 2009), which
can hurt translation quality. Flexibility of function
word usage, rich morphology and paraphrasing all
add to the difficulty of rule extraction. In addition,
restricting target word orders by hard translation
rules can also hurt output fluency.
?
* Work done while visiting Singapore University of
Technology and Design (SUTD)
Figure 1: Overall system architecture.
A potential solution to the problems above is to
treat translation as a generation task, represent-
ing syntactic correspondences using soft features.
Both adequacy and fluency can potentially be im-
proved by giving full flexibility to target synthe-
sis, and leaving all options to the statistical model.
The main challenge to this method is a signifi-
cant increase in the search space (Knight, 1999).
To this end, recent advances in tackling complex
search tasks for text generation offer some so-
lutions (White and Rajkumar, 2009; Zhang and
Clark, 2011).
In this short paper, we present a preliminary in-
vestigation on the possibility of building a syn-
tactic SMT system that does not use hard transla-
tion rules, by utilizing recent advances in statisti-
cal natural language generation (NLG). The over-
all architecture is shown in Figure 1. Translation
is performed by first parsing the source sentence,
then transferring source words and phrases to their
target equivalences, and finally synthesizing the
target output.
We choose dependency grammar for both the
source and the target syntax, and adapt the syntac-
tic text synthesis system of Zhang (2013), which
performs dependency-based linearization. The
linearization task for MT is different from the
monolingual task in that not all translation options
are used to build the output, and that bilingual cor-
respondences need to be taken into account dur-
177
ing synthesis. The algorithms of Zhang (2013) are
modified to perform word selection as well as or-
dering, using two sets of features to control trans-
lation adequacy and fluency, respectively.
Preliminary experiments on the IWSLT1 2010
data show that the system gives BLEU compara-
ble to traditional tree-to-string and string-to-tree
translation systems. It demonstrates the feasibility
of leveraging statistical NLG techniques for SMT,
and the possibility of building a statistical transfer-
based MT system.
2 Approach
The main goal being proof of concept, we keep
the system simple by utilizing existing methods
for the main components, minimizing engineer-
ing efforts. Shown in Figure 1, the end-to-end
system consists of two main components: lexical
transfer and synthesis. The former provides can-
didate translations for (overlapping) source words
and phrases. Although lexicons and rules can
be used for this step, we take a simple statisti-
cal alignment-based approach. The latter searches
for a target translation by constructing dependency
trees bottom-up. The process can be viewed as
a syntax-based generation process from a bag of
overlapping translation options.
2.1 Lexical transfer
We perform word alignment using IBM model 4
(Brown et al., 1993), and then extract phrase pairs
according to the alignment and automatically-
annotated target syntax. In particular, consistent
(Och et al., 1999) and cohesive (Fox, 2002) phrase
pairs are extracted from intersected alignments in
both directions: the target side must form a pro-
jective span, with a single root, and the source side
must be contiguous. A resulting phrase pair con-
sists of the source phrase, its target translation, as
well as the head position and head part-of-speech
(POS) of the target span, which are useful for tar-
get synthesis. We further restrict that neither the
source nor the target side of a valid phrase pair
contains over s words.
Given an input source sentence, the lexical
transfer unit finds all valid target translation op-
tions for overlapping source phrases up to size s,
and feeds them as inputs to the target synthesis de-
coder. The translation options with a probability
1International Workshop on Spoken Language Transla-
tion, http://iwslt2010.fbk.eu
below ? ? P
max
are filtered out, where P
max
is the
probability of the most probable translation. Here
the probability of a target translation is calculated
as the count of the translation divided by the count
of all translations of the source phrase.
2.2 Synthesis
The synthesis module is based on the monolingual
text synthesis algorithm of Zhang (2013), which
constructs an ordered dependency tree given a bag
of words. In the bilingual setting, inputs to the al-
gorithm are translation options, which can be over-
lapping and mutually exclusive, and not necessar-
ily all of which are included in the output. As a
result, the decoder needs to perform word selec-
tion in addition to word ordering. Another differ-
ence between the bilingual and monolingual set-
tings is that the former requires translation ade-
quacy in addition to output fluency.
We largely rely on the monolingual system for
MT decoding. To deal with overlapping transla-
tion options, a source coverage vector is used to
impose mutual exclusiveness on input words and
phrases. Each element in the coverage vector is
a binary value that indicates whether a particular
source word has been translated in the correspond-
ing target hypothesis. For translation adequacy,
we use a set of bilingual features on top of the set
of monolingual features for text synthesis.
2.2.1 Search
The search algorithm is the best-first algorithm of
Zhang (2013). Each search hypothesis is a par-
tial or full target-language dependency tree, and
hypotheses are constructed bottom-up from leaf
nodes, which are translation options. An agenda
is used to maintain a list of search hypothesis to
be expanded, and a chart is used to record a set
of accepted hypotheses. Initially empty, the chart
is a beam of size k ? n, where n is the number
of source words and k is a positive integer. The
agenda is a priority queue, initialized with all leaf
hypotheses (i.e. translation options). At each step,
the highest-scored hypothesis e is popped off the
agenda, and expanded by combination with all hy-
potheses on the chart in all possible ways, with
the set of newly generated hypotheses e
1
, e
2
, ...e
N
being put onto the agenda, and e being put onto
the chart. When two hypotheses are combined,
they can be put in two different orders, and in each
case different dependencies can be constructed be-
tween their head words, leading to different new
178
dependency syntax
WORD(h) ? POS(h) ? NORM(size) ,
WORD(h) ? NORM(size), POS(h) ? NORM(size)
POS(h) ? POS(m) ? POS(b) ? dir
POS(h) ? POS(h
l
) ? POS(m) ? POS(m
r
) ? dir (h > m),
POS(h) ? POS(h
r
) ? POS(m) ? POS(m
l
) ? dir (h < m)
WORD(h) ? POS(m) ? POS(m
l
) ? dir ,
WORD(h) ? POS(m) ? POS(m
r
) ? dir
POS(h) ? POS(m) ? POS(m
1
) ? dir ,
POS(h) ? POS(m
1
) ? dir , POS(m) ? POS(m
1
) ? dir
WORD(h) ? POS(m) ? POS(m
1
) ? POS(m
2
) ? dir ,
POS(h) ? POS(m) ? POS(m
1
) ? POS(m
2
) ? dir ,
...
dependency syntax for completed words
WORD(h) ? POS(h) ? WORD(h
l
) ? POS(h
l
),
POS(h) ? POS(h
l
),
WORD(h) ? POS(h) ? POS(h
l
),
POS(h) ? WORD(h
l
) ? POS(h
l
) ,
WORD(h) ? POS(h) ? WORD(h
r
) ? POS(h
r
),
POS(h) ? POS(h
r
),
...
surface string patterns (B?bordering index)
WORD(B ? 1) ? WORD(B), POS(B ? 1) ? POS(B),
WORD(B ? 1) ? POS(B), POS(B ? 1) ? WORD(B),
WORD(B ? 1) ? WORD(B) ? WORD(B + 1),
WORD(B ? 2) ? WORD(B ? 1) ? WORD(B),
POS(B ? 1) ? POS(B) ? POS(B + 1),
...
surface string patterns for complete sentences
WORD(0), WORD(0) ? WORD(1),
WORD(size ? 1),
WORD(size ? 1) ? WORD(size ? 2),
POS(0), POS(0) ? POS(1),
POS(0) ? POS(1) ? POS(2),
...
Table 1: Monolingual feature templates.
hypotheses. The decoder expands a fixed number
L hypotheses, and then takes the highest-scored
chart hypothesis that contains over ? ? n words as
the output, where ? is a real number near 1.0.
2.2.2 Model and training
A scaled linear model is used by the decoder to
score search hypotheses:
Score(e) =
~
? ? ?(e)
|e|
,
where ?(e) is the global feature vector of the hy-
pothesis e, ~? is the parameter vector of the model,
and |e| is the number of leaf nodes in e. The
scaling factor |e| is necessary because hypothe-
ses with different numbers of words are compared
with each other in the search process to capture
translation equivalence.
While the monolingual features of Zhang
(2013) are applied (example feature templates
from the system are shown in Table 1), an addi-
tional set of bilingual features is defined, shown
phrase translation features
PHRASE(m) ? PHRASE(t), P (trans),
bilingual syntactic features
POS(th) ? POS(tm) ? dir ? LEN(path),
WORD(th) ? POS(tm) ? dir ? LEN(path),
POS(th) ? WORD(tm) ? dir ? LEN(path),
WORD(th) ? WORD(tm) ? dir ? LEN(path),
WORD(sh) ? WORD(sm) ? dir ? LEN(path),
WORD(sh) ? WORD(th) ? dir ? LEN(path),
WORD(sm) ? WORD(tm) ? dir ? LEN(path),
bilingual syntactic features (LEN(path) ? 3)
POS(th) ? POS(tm) ? dir ? LABELS(path),
WORD(th) ? POS(tm) ? dir ? LABELS(path),
POS(th) ? WORD(tm) ? dir ? LABELS(path),
WORD(th) ? WORD(tm) ? dir ? LABELS(path),
WORD(sh) ? WORD(sm) ? dir ? LABELS(path),
WORD(sh) ? WORD(th) ? dir ? LABELS(path),
WORD(sm) ? WORD(tm) ? dir ? LABELS(path),
POS(th) ? POS(tm) ? dir ? LABELSPOS(path),
WORD(th) ? POS(tm) ? dir ? LABELSPOS(path),
POS(th) ? WORD(tm) ? dir ? LABELSPOS(path),
WORD(th) ? WORD(tm) ? dir ? LABELSPOS(path),
WORD(sh) ? WORD(sm) ? dir ? LABELSPOS(path),
WORD(sh) ? WORD(th) ? dir ? LABELSPOS(path),
WORD(sm) ? WORD(tm) ? dir ? LABELSPOS(path),
Table 2: Bilingual feature templates.
in Table 2. In the tables, s and t represent the
source and target, respectively; h and m repre-
sent the head and modifier in a dependency arc,
respectively; h
l
and h
r
represent the neighboring
words on the left and right of h, respectively; m
l
and m
r
represent the neighboring words on the left
and right of m, respectively; m
1
and m
2
repre-
sent the closest and second closest sibling of m on
the side of h, respectively. dir represents the arc
direction (i.e. left or right); PHRASE represents
a lexical phrase; P(trans) represents the source-
to-target translation probability from the phrase-
table, used as a real-valued feature; path repre-
sents the shortest path in the source dependency
tree between the two nodes that correspond to the
target head and modifier, respectively; LEN(path)
represents the number of arcs on path, normalized
to bins of [5, 10, 20, 40+]; LABELS(path) repre-
sents the array of dependency arc labels on path;
LABELSPOS(path) represents the array of depen-
dency arc labels and source POS on path. In addi-
tion, a real-valued four-gram language model fea-
ture is also used, with four-grams extracted from
the surface boundary when two hypothesis are
combined.
We apply the discriminative learning algorithm
of Zhang (2013) to train the parameters ~?. The al-
gorithm requires training examples that consist of
full target derivations, with leaf nodes being input
translation options. However, the readily available
179
training examples are automatically-parsed target
derivations, with leaf nodes being the reference
translation. As a result, we apply a search pro-
cedure to find a derivation process, through which
the target dependency tree is constructed from a
subset of input translation options. The search
procedure can be treated as a constrained decod-
ing process, where only the oracle tree and its sub
trees can be constructed. In case the set of transla-
tion options cannot lead to the oracle tree, we ig-
nore the training instance.2 Although the ignored
training sentence pairs cannot be utilized for train-
ing the discriminative synthesizer, they are never-
theless used for building the phrase table and train-
ing the language model.
3 Experiments
We perform experiments on the IWSLT 2010
Chinese-English dataset, which consists of train-
ing sentence pairs from the dialog task (dialog)
and Basic Travel and Expression Corpus (BTEC).
The union of dialog and BTEC are taken as our
training set, which contains 30,033 sentence pairs.
For system tuning, we use the IWSLT 2004 test set
(also released as the second development test set
of IWSLT 2010), which contains 500 sentences.
For final test, we use the IWSLT 2003 test set (also
released as the first development test set of IWSLT
2010), which contains 506 sentences.
The Chinese sentences in the datasets are seg-
mented using NiuTrans3 (Xiao et al., 2012), while
POS-tagging of both English and Chinese is per-
formed using ZPar4 version 0.5 (Zhang and Clark,
2011). We train the English POS-tagger using the
WSJ sections of the Penn Treebank (Marcus et al.,
1993), turned into lower-case. For syntactic pars-
ing of both English and Chinese, we use the de-
fault models of ZPar 0.5.
We choose three baseline systems: a string-to-
tree (S2T) system, a tree-to-string (T2S) system
and a tree-to-tree (T2T) system (Koehn, 2010).
The Moses release 1.0 implementations of all
three systems are used, with default parameter set-
tings. IRSTLM5 release 5.80.03 (Federico et al.,
2008) is used to train a four-gram language models
2This led to the ignoring of over 40% of the training sen-
tence pairs. For future work, we will consider substitute or-
acles from reachable target derivations by using maximum
sentence level BLEU approximation (Nakov et al., 2012) or
METEOR (Denkowski and Lavie, 2011) as selection criteria.
3http://www.nlplab.com/NiuPlan/NiuTrans.ch.html
4http://sourceforge.net/projects/zpar/
5http://sourceforge.net/apps/mediawiki/irstlm
System T2S S2T T2T OURS
BLEU 32.65 36.07 28.46 34.24
Table 3: Final results.
SOURCE:?????????
REF: I have a terrible headache .
OURS: now , I have a headache .
SOURCE:??????????
REF: I ?d like a twin room with a bath please .
OURS: a twin room , I ?ll find a room with a bath .
SOURCE:??????????
REF: can you change yen into dollars ?
OURS: please change yen into dollars .
SOURCE:????? ?
REF: roast chicken , please .
OURS: please have roast chicken .
SOURCE:?????????
REF: take two tablets after every meal .
OURS: please eat after each meal .
SOURCE:????
REF: check , please .
OURS: I have to check - out , please .
SOURCE:?????????????
REF: yes , well , that ?s our specialty .
OURS: ah , the food that ?s right .
SOURCE:?????
REF: my air conditioner is n?t working .
OURS: the air - conditioner does n?t work .
Table 4: Sample output sentences.
over the English training data, which is applied to
the baseline systems and our system. Kneser-Ney
smoothing is used to train the language model.
We use the tuning set to determine the optimal
number of training iterations. The translation op-
tion filter ? is set to 0.1; the phrase size limit s is
set to 5 in order to verify the effectiveness of syn-
thesis; the number of expanded nodes L is set to
200; the chart factor k is set to 16 for a balance be-
tween efficiency and accuracy; the goal parameter
? is set to 0.8.
The final scores of our system and the baselines
are shown in Table 3. Our system gives a BLEU
of 34.24, which is comparable to the baseline sys-
tems. Some example outputs are shown in Table 4.
Manual comparison does not show significant dif-
ferences in overall translation adequacy or fluency
between the outputs of the four systems. However,
an observation is that, while our system can pro-
duce more fluent outputs, the choice of translation
options can be more frequently incorrect. This
suggests that while the target synthesis component
is effective under the bilingual setting, a stronger
lexical selection component may be necessary for
better translation quality.
180
4 Related work
As discussed in the introduction, our work is
closely related to previous studies on syntactic
MT, with the salient difference that we do not rely
on hard translation rules, but allow free target syn-
thesis. The contrast can be summarized as ?trans-
lation by parsing? vs ?translation by generation?.
There has been a line of research on genera-
tion for translation. Soricut and Marcu (2006) use
a form of weighted IDL-expressions (Nederhof
and Satta, 2004) for generation. Bangalore et al.
(2007) treats MT as a combination of global lex-
ical transfer and word ordering; their generation
component does not perform lexical selection, re-
lying on an n-gram language model to order target
words. Goto et al. (2012) use a monotonic phrase-
based system to perform target word selection, and
treats target ordering as a post-processing step.
More recently, Chen et al. (2014) translate source
dependencies arc-by-arc to generate pseudo target
dependencies, and generate the translation by re-
ordering of arcs. In contrast with these systems,
our system relies more heavily on a syntax-based
synthesis component, in order to study the useful-
ness of statistical NLG on SMT.
With respect to syntax-based word ordering,
Chang and Toutanova (2007) and He et al. (2009)
study a simplified word ordering problem by as-
suming that the un-ordered target dependency tree
is given. Wan et al. (2009) and Zhang and Clark
(2011) study the ordering of a bag of words, with-
out input syntax. Zhang et al. (2012), Zhang
(2013) and Song et al. (2014) further extended this
line of research by adding input syntax and allow-
ing joint inflection and ordering. de Gispert et al.
(2014) use a phrase-structure grammer for word
ordering. Our generation system is based on the
work of Zhang (2013), but further allows lexical
selection.
Our work is also in line with the work of Liang
et al. (2006), Blunsom et al. (2008), Flanigan et
al. (2013) and Yu et al. (2013) in that we build a
discriminative model for SMT.
5 Conclusion
We investigated a novel system for syntactic ma-
chine translation, treating MT as an unconstrained
generation task, solved by using a single discrim-
inative model with both monolingual syntax and
bilingual translation features. Syntactic corre-
spondence is captured by using soft features rather
than hard translation rules, which are used by most
syntax-based statistical methods in the literature.
Our results are preliminary in the sense that
the experiments were performed using a relatively
small dataset, and little engineering effort was
made on fine-tuning of parameters for the base-
line and proposed models. Our Python imple-
mentation gives the same level of BLEU scores
compared with baseline syntactic SMT systems,
but is an order of magnitude slower than Moses.
However, the results demonstrate the feasibility of
leveraging text generation techniques for machine
translation, directly connecting the two currently
rather separated research fields. The system is not
strongly dependent on the specific generation al-
gorithm, and one potential of the SMT architec-
ture is that it can directly benefit from advances in
statistical NLG technology.
Acknowledgement
The work has been supported by the Singa-
pore Ministration of Education Tier 2 project
T2MOE201301 and the startup grant SRG ISTD
2012 038 from SUTD. We thank the anonymous
reviewers for their constructive comments.
References
Michael Auli, Adam Lopez, Hieu Hoang, and Philipp
Koehn. 2009. A systematic analysis of translation
model search spaces. In Proc. WMT, pages 224?
232.
Srinivas Bangalore, Patrick Haffner, and Stephan Kan-
thak. 2007. Statistical machine translation through
global lexical selection and sentence reconstruction.
In Proc. ACL, pages 152?159.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. ACL, pages 200?208.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Pi-Chuan Chang and Kristina Toutanova. 2007. A dis-
criminative syntactic word order model for machine
translation. In Proc. ACL, pages 9?16.
Hongshen Chen, Jun Xie, Fandong Meng, Wenbin
Jiang, and Qun Liu. 2014. A dependency edge-
based transfer model for statistical machine transla-
tion. In Proc. COLING 2014, pages 1103?1113.
181
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
ACL, pages 263?270.
Adria` de Gispert, Marcus Tomalin, and Bill Byrne.
2014. Word ordering with phrase-based grammars.
In Proc. EACL, pages 259?268.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Proc.
WMT, pages 85?91.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Proc. In-
terspeech, pages 1618?1621.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell.
2013. Large-scale discriminative training for statis-
tical machine translation using held-out line search.
In Proc. NAACL, pages 248?258.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proc. EMNLP, pages 304?311.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. HLT-NAACL, pages 273?280.
Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012.
Post-ordering by parsing for Japanese-English sta-
tistical machine translation. In Proc. ACL, pages
311?316.
Wei He, Haifeng Wang, Yuqing Guo, and Ting Liu.
2009. Dependency based Chinese sentence realiza-
tion. In Proc. ACL/AFNLP, pages 809?816.
Kevin Knight. 1999. Squibs and Discussions: Decod-
ing Complexity in Word-Replacement Translation
Models. Computational Linguistics, 25(4):607?
615.
Phillip Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to
machine translation. In Proc. COLING/ACL, pages
761?768.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. COLING/ACL, pages 609?616.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proc. EMNLP, pages 44?52.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The penn treebank. Com-
putational linguistics, 19(2):313?330.
Preslav Nakov, Francisco Guzman, and Stephan Vo-
gel. 2012. Optimizing for sentence-level BLEU+1
yields short translations. In Proc. Coling, pages
1979?1994.
Mark-Jan Nederhof and Giorgio Satta. 2004. Idl-
expressions: a formalism for representing and pars-
ing finite languages in natural language processing.
J. Artif. Intell. Res.(JAIR), 21:287?317.
Franz Josef Och, Christoph Tillmann, and Hermann
Ney. 1999. Improved alignment models for statis-
tical machine translation. In Proc. EMNLP, pages
20?28.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proc. ACL, pages 271?279.
Libin Shen and Aravind Joshi. 2008. LTAG depen-
dency parsing with bidirectional incremental con-
struction. In Proc. EMNLP, pages 495?504.
Linfeng Song, Yue Zhang, Kai Song, and Qun Liu.
2014. Joint morphological generation and syntactic
linearization. In Proc. AAAI, pages 1522?1528.
Radu Soricut and Daniel Marcu. 2006. Stochastic lan-
guage generation using widl-expressions and its ap-
plication in machine translation and summarization.
In Proc. ACL, pages 1105?1112.
Stephen Wan, Mark Dras, Robert Dale, and Ce?cile
Paris. 2009. Improving grammaticality in statisti-
cal sentence generation: Introducing a dependency
spanning tree algorithm with an argument satisfac-
tion model. In Proc. EACL, pages 852?860.
Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for CCG realization. In Proc.
the EMNLP, pages 410?419.
Tong Xiao, Jingbo Zhu, Hao Zhang, and Qiang Li.
2012. NiuTrans: An open source toolkit for phrase-
based and syntax-based machine translation. In
Proc. ACL Demos, pages 19?24.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Proc. EMNLP, pages 216?226.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable MT training. In Proc. EMNLP,
pages 1112?1123.
Yue Zhang and Stephen Clark. 2011. Syntax-based
grammaticality improvement using CCG and guided
search. In Proc. EMNLP, pages 1147?1157.
Yue Zhang, Graeme Blackwood, and Stephen Clark.
2012. Syntax-based word ordering incorporating a
large-scale language model. In Proc. EACL, pages
736?746.
Yue Zhang. 2013. Partial-tree linearization: General-
ized word ordering for text synthesis. In Proc. IJ-
CAI, pages 2232?2238.
182
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 864?874,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Domain Adaptation for CRF-based Chinese Word Segmentation using
Free Annotations
Yijia Liu ??, Yue Zhang ?, Wanxiang Che ?, Ting Liu ?, Fan Wu ?
?Singapore University of Technology and Design
?Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{yjliu,car,tliu}@ir.hit.edu.cn {yue zhang,fan wu}@sutd.edu.sg
Abstract
Supervised methods have been the domi-
nant approach for Chinese word segmen-
tation. The performance can drop signif-
icantly when the test domain is different
from the training domain. In this paper,
we study the problem of obtaining par-
tial annotation from freely available data
to help Chinese word segmentation on dif-
ferent domains. Different sources of free
annotations are transformed into a unified
form of partial annotation and a variant
CRF model is used to leverage both fully
and partially annotated data consistently.
Experimental results show that the Chi-
nese word segmentation model benefits
from free partially annotated data. On the
SIGHAN Bakeoff 2010 data, we achieve
results that are competitive to the best re-
ported in the literature.
1 Introduction
Statistical Chinese word segmentation gains high
accuracies on newswire (Xue and Shen, 2003;
Zhang and Clark, 2007; Jiang et al., 2009; Zhao
et al., 2010; Sun and Xu, 2011). However, man-
ually annotated training data mostly come from
the news domain, and the performance can drop
severely when the test data shift from newswire
to blogs, computer forums and Internet literature
(Liu and Zhang, 2012).
Several methods have been proposed for solv-
ing the domain adaptation problem for segmenta-
tion, which include the traditional token- and type-
supervised methods (Song et al., 2012; Zhang et
al., 2014). While token-supervised methods rely
on manually annotated target-domain sentences,
type-supervised methods leverage manually as-
sembled domain-specific lexicons to improve
target-domain segmentation accuracies. Both
. ? ? ? ? ? ? ? ? ?
b b b b b b b b b
m m m m m m m m m
e e e e e e e e e
s s s s s s s s s
Figure 1: The segmentation problem, illustrated
using the sentence ??? (Pudong) ?? (devel-
opment) ? (and) ?? (legal) ?? (construc-
tion)?. Possible segmentation labels are drawn un-
der each character, where b, m, e, s stand for the
beginning, middle, end of a multi-character word,
and a single character word, respectively. The path
shows the correct segmentation by choosing one
label for each character.
methods are competitive given the same amount of
annotation effects (Garrette and Baldridge, 2012;
Zhang et al., 2014). However, obtaining manually
annotated data can be expensive.
On the other hand, there are free data which
contain limited but useful segmentation informa-
tion over the Internet, including large-scale un-
labeled data, domain-specific lexicons and semi-
annotated web pages such as Wikipedia. In the
last case, word-boundary information is contained
in hyperlinks and other markup annotations. Such
free data offer a useful alternative for improving
the segmentation performance, especially on do-
mains that are not identical to newswire, and for
which little annotation is available.
In this paper, we investigate techniques for
adopting freely available data to help improve the
performance on Chinese word segmentation. We
propose a simple but robust method for construct-
ing partial segmentation from different sources
of free data, including unlabeled data and the
Wikipedia. There has been work on making use
of both unlabeled data (Sun and Xu, 2011; Wang
et al., 2011) and Wikipedia (Jiang et al., 2013)
864
to improve segmentation. However, no empiri-
cal results have been reported on a unified ap-
proach to deal with different types of free data.
We use a conditional random fields (Lafferty et al.,
2001; Tsuboi et al., 2008) variant that can lever-
age the partial annotations obtained from different
sources of free annotation. Training is achieved by
a modification to the learning objective, incorpo-
rating partial annotation likelihood, so that a single
model can be trained consistently with a mixture
of full and partial annotation.
Experimental results show that our method of
using partially annotated data can consistently im-
proves cross-domain segmentation performance.
We obtain results which are competitive to the
best reported in the literature. Our segmentor
is freely released at https://github.com/
ExpResults/partial-crfsuite.
2 Obtaining Partially Annotated Data
We model the Chinese word segmentation task as
a character sequence tagging problem, which is to
give each character in a sentence a word-boundary
tag (Xue and Shen, 2003). We adopt four tags, b,
m, e and s, which represent the beginning, middle,
end of a multi-character word, and a single char-
acter word, respectively. A manually segmented
sentence can be represented as a tag sequence, as
shown in Figure 1.
We investigate two major sources of freely-
available annotations: lexicons and natural anno-
tation, both with the help of unannotated data.
To make use of the first source of informa-
tion, we incorporate words from a lexicon into
unannotated sentences by matching of character
sequences, resulting in partially annotated sen-
tences, as shown in Figure 2a. In this example,
the word ???? (the Huqi Mountain)? in the
unannotated sentence matches an item in the lex-
icon. As a result, we obtain a partially-annotated
sentence, in which the segmentation ambiguity of
the characters ?? (fox)?, ?? (brandy road)? and
?? (mountain)? are resolved (??? being the be-
ginning, ??? being the middle and ??? being the
end of the same word). At the same time, the seg-
mentation ambiguity of the surrounding characters
?? (at)? and ?? (save)? are reduced (??? be-
ing either a single-character word or the end of
a multi-character word, and ??? being either a
single-character word or the beginning of a multi-
character word).
. ? ? ? ? ? ? ? ? ?
b b b b b b b b b
m m m m m m m m m
e e e e e e e e e
s s s s s s s s s
(a) ?? (at) ??? (Huqi Mountain) ? ? (save) ?
? (Biyao)?, where ????? matches a lexicon word.
. ? ? ? ? ? ? ? ? ?
b b b b b b b b b
m m m m m m m m m
e e e e e e e e e
s s s s s s s s s
(b) ?? (e.g.)???? (lysozyme)? ??? (lactoferrin)?,
where ?????? is a hyperlink.
Figure 2: Examples of partially annotated data.
The paths show possible correct segmentations.
Natural annotation, which refers to word
boundaries that can be inferred from URLs, fonts
or colors on web pages, also result in partially-
annotated sentences. Taking a web page shown
in Figure 2b for example. It can be inferred from
the URL tags on ?????? that ??? should be
either the beginning of a multi-character word or
a single-character word, and ??? should be either
the end a multi-character word or single-character
word. Similarly, possible tags of the surrounding
character ??? and ??? can also be inferred.
We turn both lexicons and natural annotation
into the same form of partial annotation with
same unresolved ambiguities, as shown in Figure
2, and use them together with available full anno-
tation (Figure 1) as the training data for the seg-
mentor. In this section, we describe in detail how
to obtain partially annotated sentences from each
resource, respectively.
2.1 Lexicons
In this scenario, we assume that there are unla-
beled sentences along with a lexicon for the target
domain. We obtain partially segmented sentences
by extracting word boundaries from the unlabeled
sentences with the help of the lexicon. Previous
matching methods (Wu and Tseng, 1993; Wong
and Chan, 1996) for Chinese word segmentation
largely rely on the lexicons, and are generally con-
sidered being weak in ambiguity resolution (Gao
865
People?s
Daily
?? (saw)?? (Hainan)??? (tourist industry)?? (full)?? (hope)
saw tourist industry in Hainan is full of hope
Wikipedia
??(mainly)?(is)?? (tourist)? (industry)?(and)?? (software)??(industry)
mainly is tourist industry and software industry
(a) Case of incompatible annotation on ????(tourist industry)? between People?s Daily and Wikipedia.
Literature
????? (Shuo Wen Jie Zi, a book)?(segmented)?(annotated)?
the segmented and annotated version of Shuo Wen Jie Zi
Computer
??(each)??(record)?(is)??(splitted)?(into)?? (fields)
each record is splitted into several fields
(b) Similar subsequence ???(field)? is segmented differently under different domains in Wikipedia.
Table 1: Examples natural annotation from Wikipedia. Underline marks annotated words.
et al., 2005). But for obtaining the partial labeled
data with lexicon, the matching method can still be
a solution. Since we do not aim to recognize every
word from sentence, we can select a lexicon with
smaller coverage but less ambiguity to achieve rel-
atively precise matching result.
In this paper, we apply two matching schemes
to the same raw sentences to obtain partially an-
notated sentences. The first is a simple forward-
maximum matching (FMM) scheme, which is
very close to the forward maximum matching al-
gorithm ofWu and Tseng (1993) for Chinese word
segmentation. This scheme scans the input sen-
tence from left to right. At each position, it at-
tempts to find the longest subsequence of Chi-
nese characters that matches a lexicon entry. If
such an entry is found, the subsequence is tagged
with the corresponding tags, and its surrounding
characters are also constrained to a smaller set of
tags. If no subsequence is found in the lexicon, the
character is left with all the possible tags. Taking
the sentence in Figure 2a for example. When the
algorithm scans the second character, ???, and
finds the entry ????? in the lexicon, the sub-
sequence of characters is recognized as a word,
and tagged with b, m and e, respectively. At the
same time, the previous character ??? can be in-
ferred as only end of a multi-character word (e) or
a single-character word (s). The second matching
scheme is backward maximum matching, which
can be treated as the application of FMM on the
reverse of unlabeled sentences using a lexicon of
reversed words.
To mitigate the errors resulting from one single
matching scheme, we combine the two matching
results by agreement. The basic idea is that if a
subsequence of sentence is recognized as word by
multiple matching results, it can be considered as a
more precise annotation. Our algorithm reads par-
tial segmentation by different methods and selects
the subsequences that are identified as word by all
methods as annotated words.
2.2 Natural Annotation
We use the Chinese Wikipedia for natural anno-
tation. Partially annotated sentences are readily
formed in Wikipedia by markup syntax, such as
URLs. However, some subtle issues exist if the
sentences are used directly. One problem is in-
compatibility of segmentation standards between
the annotated training data and Wikipedia. Jiang
et al. (2009) discuss this incompatibility problem
between two corpora ? the CTB and the Peo-
ple?s Daily; the problem is even more severe on
Wikipedia because it can be edited by any user.
Table 1a shows a case of incompatible annota-
tion between the People?s Daily data and natural
annotation in Wikipedia, where the three charac-
ters ????? are segmented differently. Both can
be treated as correct, although they have different
segmentation granularities.
Another problem is the intrinsic ambiguity of
segmentation. The same character sequence can
be segmented into different words under differ-
ent contexts. If the training and test data contain
different contexts, the learned model can give in-
correct results on the test data. This is particu-
larly true across different domains. Table 1b gives
such an example, where the character sequence
???? is segmented differently in two of our test
domains, but both cases exist in Wikipedia.
In summary, Wikipedia introduces both use-
ful information for domain adaptation and harm-
ful noise with negative effects on the model. To
866
achieve better performance of domain adaptation
using Wikipedia, one intuitive approach is to se-
lect more domain-related data and less irrelevant
data to minimize the risks that result from incom-
patible annotation and domain difference.
To this end, we assume that there are some raw
sentences on the target domain, which can be used
to evaluate the relevance between Wikipedia and
target domain test data. We assume that URL-
tagged entries reflect the segmentation standards
of Wikipedia sentence, and use them to match
Wikipedia sentences with the raw target domain
data. If the character sequence of any URL-tagged
entry in a Wikipedia sentence matches the target
domain data, the Wikipedia sentence is selected
for training. Another advantage of such data se-
lection is that the training time consumption can
be reduced by reducing the size of training data.
3 CRF for Word Segmentation
We follow the work of Zhao et al. (2010) and Sun
and Xu (2011), and adopt the Conditional Random
Fields (CRF) model (Lafferty et al., 2001) for the
sequence labeling problem of word segmentation.
Given an input characters sequence, the task is to
assign one segmentation label from {b,m, e, s} on
each character. Let x = (x
1
, x
2
, ..., x
T
) be the
sequence of characters in sentence whose length
is T , and y = (y
1
, y
2
, ..., y
T
) be the correspond-
ing label sequence, where y
i
? Y . The linear-
chain conditional random field for Chinese word
segmentation can be formalized as
p(y|x) =
1
Z
exp
T
?
t=1
?
k
?
k
f
k
(y
t
, y
t?1
,x) (1)
where ?
k
are the model parameters, f
k
are the fea-
ture functions and Z is the probability normalizer.
Z =
?
y
exp
T
?
t=1
?
k
?
k
f
k
(y
t
, y
t?1
,x) (2)
We follow Sun and Xu (2011) and use the fea-
ture templates shown in Table 2 to model the seg-
mented task. For ith character in the sentence, the
n-gram features represent the surrounding charac-
ters of this character; Type categorizes the charac-
ter it into digit, punctuation, english and other;
Identical indicates whether the input character is
the same with its surrounding characters. This
feature captures repetition patterns such as ??
? (try)? or ??? (stroll)?.
Type Template
unigram C
s
(i? 3 < s < i + 3)
bigram C
s
C
s+1
(i? 3 < s < i + 2)
C
s
C
s+2
(i? 3 < s < i + 1)
type Type(C
i
)
Type(C
s
)Type(C
s+1
)
(i? 1 < s < i + 2)
identical Identical(C
s
, C
s+1
) (i ? 3 <
s < i + 1)
Identical(C
s
, C
s+2
) (i ? 3 <
s < i)
Table 2: Feature templates for the ith character.
For fully-annotated training data, the learning
problem of conditional random fields is to maxi-
mize the log likelihood over all the training data
(Lafferty et al., 2001)
L =
N
?
n=1
log p(y
(n)
|x
(n)
)
Here N is the number of training sentences. Both
the likelihood p(y
(n)
|x
(n)
) and its gradient can be
calculated by performing the forward-backward
algorithm (Baum and Petrie, 1966) on the se-
quence, and several optimization algorithm can be
adopted to learn parameters from data, including
L-BFGS (Liu and Nocedal, 1989) and SGD (Bot-
tou, 1991).
4 Training a CRF with partially
annotated data
For word segmentation with partially annotated
data, some characters in a sentence can have
a definite segmentation label, while some can
have multiple labels with ambiguities remain-
ing. Taking the partially annotated sentence
in Figure 2a for example, the corresponding
potential label sequence for ??????? is
{(e, s), (b), (m), (e), (b, s)}, where the characters
???, ??? and ??? have fixed labels but for ???
and ???, some ambiguities exist. Note that the
full annotation in Figure 1 can be regarded as a
special case of partial annotation, where the num-
ber of potential labels for each character is one.
We follow Tsuboi et al. (2008) and model
marginal probabilities over partially annotated
data. Define the possible labels that correspond
to the partial annotation as L = (L
1
, L
2
, ..., L
T
),
where each L
i
is a non-empty subset of Y that cor-
responds to the set of possible labels for x
i
. Let
867
YL
be the set of all possible label sequences where
?y ? Y
L
, y
i
? L
i
. The marginal probability of
Y
L
can be modeled as
p(Y
L
|x) =
1
Z
?
y?Y
L
exp
T
?
t=1
?
k
?
k
f
k
(y
t
, y
t?1
,x) (3)
Defining the unnormalized marginal probability as
Z
Y
L
=
?
y?Y
L
exp
T
?
t=1
?
k
?
k
f
k
(y
t
, y
t?1
,x),
and the normalizer Z being the same as Equation
2, the log marginal probability of Y
L
over N par-
tially annotated training examples can be formal-
ized as
L
Y
L
=
N
?
n=1
log p(Y
L
|x) =
N
?
n=1
(logZ
Y
L
? logZ)
The gradient of the likelihood can be written as
?L
Y
L
??
k
=
N
?
n=1
T
?
t=1
?
y
Y
L
?L
t
,
y
?
Y
L
?L
t?1
f
k
(y
Y
L
, y
?
Y
L
,x)p
Y
L
(y
Y
L
, y
?
Y
L
|x)
?
N
?
n=1
T
?
t=1
?
y,y
?
f
k
(y, y
?
,x)p(y, y
?
|x)
Both Z
Y
L
and its gradient are similar in form to
Z. By introducing a modification to the forward-
backward algorithm, Z
Y
L
and L
Y
L
can be calcu-
lated. Define the forward variable for partially an-
notated data ?
Y
L
,t
(j) = p
Y
L
(x
?1,...,t?
, y
t
= j). A
modification on the forward algorithm can be for-
malized as
?
Y
L
,t
(j) =
{
0 j /? L
t
?
i?L
t?1
?
t
(j, i, x
t
)?
Y
L
,t?1
(i) j ? L
t
where?
t
(j, i, x) is a potential function that equals
?
k
?
k
f
k
(y
t
= j, y
t?1
= i, x
t
). Similarly, for the
backward variable ?
Y
L
,t
,
?
Y
L
,t
(i) =
{
0 i /? L
t
?
j?L
t+1
?
t
(j, i, x
t+1
)?
Y
L
,t+1
(j) i ? L
t
Z
Y
L
can be calculated by ?
Y
L
(T ),
and p
Y
L
(y, y
?
|x) can be calculated by
?
Y
L
,t?1
(y
?
)?
t
(y, y
?
, x
t
)?
Y
L
,t
(y).
Note that if each element in Y
L
is constrained
to one single label, the CRF model in Equation 3
degrades into Equation 1. So we can train a unified
model with both fully and partially annotated data.
We implement this CRF model based on a open
source toolkit CRFSuite.
1
In our experiments, we
use the L-BFGS (Liu and Nocedal, 1989) algo-
rithm to learn parameters from both fully and par-
tially annotated data.
5 Experiments
We perform our experiments on the domain adap-
tation test data from SIGHANBakeoff 2010 (Zhao
et al., 2010), adapting annotated training sentences
from People?s Daily (PD) (Yu et al., 2001) to
different test domains. The fully annotated data
is selected from the People?s Daily newspaper
in January of 1998, and the four test domains
from the SIGHAN Bakeoff 2010 include finance,
medicine, literature and computer. Sample seg-
mented data in the computer domain from this
bakeoff is used as development set. Statistics of
the data are shown in first half of Table 3. We
use wikidump20140419
2
for the Wikipedia data.
All the traditional Chinese pages in Wikipedia are
converted to simplified Chinese. After filtering
functional pages like redirection and removing du-
plication, 5.45 million sentences are reserved.
For comparison with related work on using a
lexicon to improve segmentation, another set of
test data is chosen for this setting. We use the Chi-
nese Treebank (CTB) as the source domain data,
and Zhuxian (a free Internet novel, also named as
?Jade dynasty?, referred to as ZX henceforth) as
the target domain data.
3
The ZX data are written
in a different style from newswire, and contains
many out-of-vocabulary words. This setting has
been used by Liu and Zhang (2012) and Zhang et
al. (2014) for domain adaptation of segmentation
and POS-tagging. We use the standard training,
development and test split. Statistics of the test
data annotated by Zhang et al. (2014) are shown
in the second half of Table 3.
The data preparation method in Section 2 and
the CRF method in Section 4 are used for all
the experiments. Both recall of out-of-vocabulary
words (R
oov
) and F-score are used to evaluate the
1
http://www.chokkan.org/software/
crfsuite/
2
http://dumps.wikimedia.org/zhwiki/
20140419/
3
Annotated target domain test data and lexicon are avail-
able from http://ir.hit.edu.cn/
?
mszhang/
eacl14mszhang.zip.
868
P
D
?
S
I
G
H
A
N
Data set Train Development Test
PD Computer Finance Medicine Literature Computer
# sent. 19,056 1,000 560 1,308 670 1,329
# words 1,109,734 21,398 33,035 31,499 35,735 35,319
OOV 0.1766 0.0874 0.1102 0.0619 0.1522
C
T
B
5
?
Z
X
Data set Train Development Test Unlabeled
W
i
k
i
p
e
d
i
a
Unlabeled
CTB5 ZX
# sent. 18,086 788 1,394 32,023 5,456,151
# words 493,934 20,393 34,355
OOV 0.1377 0.1550
Table 3: Statistics of data used in this paper.
segmentation performance. There is a mixture of
Chinese characters, English words and numeric
expression in the test data from SIGHAN Bakeoff
2010. To test the influence of Wikipedia data on
Chinese word segmentation alone, we apply reg-
ular expressions to detect English words and nu-
meric expressions, so that they are marked as not
segmented. After performing this preprocessing
step, cleaned test input data are fed to the CRF
model to give a relatively strong baseline.
5.1 Free Lexicons
5.1.1 Obtaining lexicons
For domain adaption from CTB to ZX, we use
a lexicon released by Zhang et al. (2014). The
lexicon is crawled from a online encyclopedia
4
,
and contains the names of 159 characters and ar-
tifacts in the Zhuxian novel. We follow Zhang et
al. (2014) and name it NR for convenience of fur-
ther discussion. The NR lexicon can be treated
as a strongly domain-related, high quality but rel-
atively small lexicon. It?s a typical example of
freely available lexicon over the Internet.
For domain adaptation from PD to medicine and
computer, we collect a list of page titles under
the corresponding categories in Wikipedia. For
medicine, entries under essential medicines, bi-
ological system and diseases are collected. For
computer, entries under computer network, Mi-
crosoft Windows and software widgets are se-
lected. These lexicons are typical freely available
lexicons that we can access to.
5.1.2 Obtaining Unlabeled Sentences
For ZX, partially annotated sentences are obtained
using the NR lexicon and unlabeled ZX sentences
by applying the matching scheme described in
4
http://baike.baidu.com/view/18277.htm
90.1
90.2
90.3
90.4
1 2 4 8 16 32# of sentences * 1000
F sc
ore 
on d
eve
lopm
ent
Figure 3: F-score on the development data when
using different numbers of unlabeled data.
Section 2. The CTB5 training data and the par-
tially annotated data are mixed as the final train-
ing data. Different amounts of unlabeled data are
applied to the development test set, and results are
shown in Figure 3. From this figure we can see
that incorporating 16K sentences gives the high-
est accuracy, and adding more partial labeled data
does not change the accuracy significantly. So for
the ZX experiments, we choose the 16K sentences
as the unlabeled data.
For the medicine and computer experiments, we
selected domain-specific sentences by matching
with the domain-specific lexicons. About 46K out
of the 5.45 million wiki sentences contain subse-
quences in the medicine lexicon and 22K in the
case of the computer domain. We randomly se-
lect 16K sentences as the unlabeled data for each
domain, respectively.
5.1.3 Final results
We incorporate the partially annotated data ob-
tained with the help of lexicon for each of the
test domain. For adaptation from CTB to ZX, we
trained our baseline model on the CTB5 training
data with the feature templates in Table 2. For
adaptation from PD to medicine and computer, we
869
Domain ZX Medicine Computer
F Roov F Roov F Roov
Baseline 87.50 73.65 91.36 72.95 93.16 84.02
Baseline+Lexicon Feature 90.36 80.69 91.60 74.39 93.14 84.27
Baseline+PA (Lex) 90.63 84.88 91.68 74.99 93.47 85.63
Zhang et al. (2014) 88.34 - - - - -
Table 4: Final result for adapting CTB to Zhuxian and adapting PD to the medicine and computer
domains, using partially annotated data (referred to as PA) obtained from unlabeled data and lexicons.
trained our baseline model on the PD training data
with the same feature template setting.
Previous research makes use of a lexicon by
adding lexicon features directly into a model (Sun
and Xu, 2011; Zhang et al., 2014), rather than
transforming them into partially annotated sen-
tences. To make a comparison, we follow Sun and
Xu (2011) and add three lexicon features to repre-
sent whether c
i
is located at the beginning, middle
or the end of a word in the lexicon, respectively.
For each test domain, the lexicon for the lexi-
con feature model consists of the most frequent
words in the source domain training data (about
6.7K for CTB5 and 8K for PD, respectively) and
the domain-specific lexicon we obtained in Sec-
tion 5.1.1.
The results are shown in Table 4, where the first
row shows the performance of the baseline mod-
els and the second row shows the performance
of the model incorporating lexicon feature. The
third row shows our method using partial anno-
tation. On the ZX test set, our method outper-
forms the baseline by more than 3 absolute per-
centage. The model with partially annotated data
performs better than the one with additional lexi-
con features. Similar conclusion is obtained when
adapting from PD to medicine and computer. By
incorporating the partially annotated data, the seg-
mentation of lexicon words, along with the con-
text, is learned.
We also compare our method with the work of
Zhang et al. (2014), who reported results only on
the ZX test data. We use the same lexicon settings.
Our method gives better result than Zhang et al.
(2014), showing that the combination of a lexicon
and unannotated sentence into partially annotated
data can lead to better performance than using a
dictionary alone in type-supervision. Given that
we only explore the use of free resource, combin-
ing a lexicon with unannotated sentences is a bet-
ter option than using the lexicon directly. Zhang
et al.?s concern, on the other hand, is to compare
Method
Com. Dev
F Roov
Baseline 93.56 83.75
Baseline+PA (Random 160K) 94.29 86.58
Baseline+PA (Selected) 95.00 88.28
Table 5: The performance of data selection on the
development set of the computer domain.
type- and token-annotation. Our partial annota-
tion can thus be treated as a compromise to obtain
some pseudo partial token-annotations when full
token annotations are unavailable. Another thing
to note is that the model of Zhang et al. (2014) is
a joint model for segmentation and POS-tagging,
which is generally considered stronger than a sin-
gle segmentation model.
5.2 Free Natural Annotation
When extracting word boundaries from Wikipedia
sentences, we ignore natural annotations on En-
glish words and digits because these words are rec-
ognized by the preprocessor. Following Jiang et
al. (2013), we also recognize a naturally annotated
two-character subsequence as a word.
5.2.1 Effect of data selection
To make better use of more domain-specific data,
and to alleviate noise in partial annotation, we ap-
ply the selection method proposed in Section 2
to the Wikipedia data. On the computer domain
development test data, this selection method re-
sults in 9.4K computer-related sentences with par-
tial annotation. A model is trained with both the
PD training data and the partially annotated com-
puter domain Wikipedia data. For comparison, we
also trained a model with 160K randomly selected
Wikipedia sentences. The experimental result is
shown in Table 5. The model incorporating se-
lected data achieves better performance compared
to the model with randomly sampled data, demon-
strating that data selection is helpful to improving
870
Method
Finance Medicine Literature Computer
Avg-FF Roov F Roov F Roov F Roov
Baseline 95.20 86.90 91.36 72.90 92.27 73.61 93.16 83.48 93.00
Baseline+PA (Ran-
dom 160K)
95.16 87.60 92.41 78.13 92.17 75.30 93.91 83.48 93.41
Baseline+PA
(Selected)
95.54 88.53 92.47 78.28 92.49 76.84 93.93 87.53 93.61
+0.34 +1.11 +0.22 +0.77
Jiang et al. (2013) 93.16 93.34 93.53 91.19 92.80
Table 6: Experimental results on the SIGHAN Bakeoff 2010 data.
the domain adaption accuracy.
5.2.2 Final Result
The final results on the four test domains are
shown in Table 6. From this table, we can see
that significant improvements are achieved with
the help of the partially annotated Wikipedia data,
when compared to the baseline. The models
trained with selected partial annotation perform
better than those trained with random partial an-
notation. Our F-scores are competitive to those re-
ported by Jiang et al. (2013). However, since their
model is trained on a different source domain, the
results are not directly comparable.
5.2.3 Analysis
In this section, we study the effect of Wikipedia on
domain adaptation when no data selection is per-
formed, in order to analyze the effect of partially
annotated data. We randomly sample 10K, 20K,
40K, 80K and 160K sentences from the 5.45 mil-
lion Wikipedia sentences, and incorporate them
into the training process, respectively. Five models
are obtained adding the baseline, and we test their
performances on the four test domains. Figure 4
shows the results.
From the figure we can see that for the medicine
and computer domains, where the OOV rate is rel-
atively high, the F-score generally increases when
more data from Wikipedia are used. The trends
of F-score and OOV recall against the volume of
Wikipedia data are almost identical. However, for
the finance and literature domains, which have low
OOV rates, such a relation between data size and
accuracy is not witnessed. For the literature do-
main, even an opposite trends is shown.
We can draw the following conclusions: (1)
Natural annotation on Wikipedia data contributes
to the recognition of OOV words on domain adap-
tation; (2) target domains with more OOV words
benefit more from Wikipedia data. (3) along with
Method
Med. Com.
F F
Baseline 91.36 93.16
Baseline+PA (Lex) 91.68 93.47
Baseline+PA (Natural) 92.47 93.93
Baseline+PA (Lex+Natural) 92.63 94.07
Table 7: Results by combining different sources of
free annotation.
the positive effect on OOV recognition, Wikipedia
data can also introduce noise, and hence data se-
lection can be useful.
5.3 Combining Lexicon and Natural
Annotation
To make the most use of free annotation, we com-
bine available free lexicon and natural annotation
resources by joining the partially annotated sen-
tences derived using each resource, training our
CRF model with these partially annotated sen-
tences and the fully annotated PD sentences. The
tests are performed on medicine and computer do-
mains. Table 7 shows the results, where further
improvements are made on both domains when the
two types of resources are combined.
6 Related Work
There has been a line of research on making use of
unlabeled data for word segmentation. Zhao and
Kit (2008) improve segmentation performance by
mutual information between characters, collected
from large unlabeled data; Li and Sun (2009) use
punctuation information in a large raw corpus to
learn a segmentation model, and achieve better
recognition of OOVwords; Sun and Xu (2011) ex-
plore several statistical features derived from un-
labeled data to help improve character-based word
segmentation. These investigations mainly focus
on in-domain accuracies. Liu and Zhang (2012)
871
0.9516
0.9519
0.9522
0.9525
0 50 100 150
0.8700
0.8725
0.8750
0.8775
(a) Finance
0.9150
0.9175
0.9200
0.9225
0 50 100 150
0.73
0.74
0.75
0.76
0.77
0.78
(b) Medicine
0.9216
0.9219
0.9222
0.9225
0 50 100 150 0.735
0.740
0.745
0.750
(c) Literature
0.934
0.936
0.938
0 50 100 150
0.84
0.85
0.86
0.87
(d) Computer
Figure 4: Performance of the model incorporating difference sizes of Wikipedia data. The solid line
represents the F-score and dashed line represents the recall of OOV words.
study domain adaptation using an unsupervised
self-training method. In contrast to their work,
we make use of not only unlabeled data, but also
leverage any free annotation to achieve better re-
sults for domain adaptation.
There has also been work on making use of a
dictionary and natural annotation for segmenta-
tion. Zhang et al. (2014) study type-supervised do-
main adaptation for Chinese segmentation. They
categorize domain difference into two types: dif-
ferent vocabulary and different POS distributions.
While the first type of difference can be effec-
tively resolved by using lexicon for each domain,
the second type of difference needs to be resolved
by using annotated sentences. They found that
given the same manual annotation time, a com-
bination of the lexicon and sentence is the most
effective. Jiang et al. (2013) use 160K Wikipedia
sentences to improves segmentation accuracies on
several domains. Both Zhang et al. (2014) and
Jiang et al. (2013) work on discriminative mod-
els using the structure perceptron (Collins, 2002),
although they study two different sources of infor-
mation. In contrast to their work, we unify both
types of information under the CRF framework.
CRF has been used for Chinese word segmenta-
tion (Tseng, 2005; Shi and Wang, 2007; Zhao and
Kit, 2008; Wang et al., 2011). However, most pre-
vious work train a CRF by using full annotation
only. In contrast, we study CRF based segmenta-
tion by using both full and partial annotation.
Several other variants of CRF model has been
proposed in the machine learning literature, such
as the generalized expectation method (Mann and
McCallum, 2008), which introduce knowledge by
incorporating a manually annotated feature dis-
tribution into the regularizer, and the JESS-CM
(Suzuki and Isozaki, 2008), which use a EM-like
method to iteratively optimize the parameter on
both the annotated data and unlabeled data. In
contrast, we directly incorporate the likelihood of
partial annotation into the objective function. The
work that is the most similar to ours is Tsuboi et
al. (2008), who modify the CRF learning objec-
tive for partial data. They focus on Japanese lexi-
cal analysis using manually collected partial data,
while we investigate the effect of partial annota-
tion from freely available sources for Chinese seg-
mentation.
7 Conclusion
In this paper, we investigated the problem of do-
main adaptation for word segmentation, by trans-
ferring various sources of free annotations into a
consistent form of partially annotated data and ap-
plying a variant of CRF that can be trained using
fully- and partially-annotated data simultaneously.
We performed a large set of experiments to study
the effectness of free data, finding that they are
useful for improving segmentation accuracy. Ex-
periments also show that proper data selection can
further benefit the model?s performance.
872
Acknowledgments
We thank the anonymous reviewers for their con-
structive comments. This work was supported
by the National Key Basic Research Program of
China via grant 2014CB340503 and the National
Natural Science Foundation of China (NSFC) via
grant 61133012 and 61370164, the Singapore
Ministry of Education (MOE) AcRF Tier 2 grant
T2MOE201301 and SRG ISTD 2012 038 from
Singapore University of Technology and Design.
References
Leonard E Baum and Ted Petrie. 1966. Statistical
inference for probabilistic functions of finite state
markov chains. The annals of mathematical statis-
tics, pages 1554?1563.
L?eon Bottou. 1991. Stochastic gradient learning in
neural networks. In Proceedings of Neuro-N??mes
91, Nimes, France. EC2.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1?8. Associ-
ation for Computational Linguistics, July.
Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning
Huang. 2005. Chinese word segmentation and
named entity recognition: A pragmatic approach.
Comput. Linguist., 31(4):531?574, December.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden markovmodels for part-of-speech
tagging with incomplete tag dictionaries. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 821?
831, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging ? a case study.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 522?530, Suntec, Singapore,
August. Association for Computational Linguistics.
Wenbin Jiang, Meng Sun, Yajuan L?u, Yating Yang, and
Qun Liu. 2013. Discriminative learning with natu-
ral annotations: Word segmentation as a case study.
In Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 761?769, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Zhongguo Li and Maosong Sun. 2009. Punctuation as
implicit annotations for chinese word segmentation.
Comput. Linguist., 35(4):505?512, December.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Math.
Program., 45(3):503?528, December.
Yang Liu and Yue Zhang. 2012. Unsupervised domain
adaptation for joint segmentation and POS-tagging.
In Proceedings of COLING 2012: Posters, pages
745?754, Mumbai, India, December. The COLING
2012 Organizing Committee.
Gideon S. Mann and Andrew McCallum. 2008.
Generalized expectation criteria for semi-supervised
learning of conditional random fields. In Proceed-
ings of ACL-08: HLT, pages 870?878, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
crfs based joint decoding method for cascaded seg-
mentation and labeling tasks. In Proceedings of the
20th International Joint Conference on Artifical In-
telligence, IJCAI?07, pages 1707?1712, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Yan Song, Prescott Klassen, Fei Xia, and Chunyu Kit.
2012. Entropy-based training data selection for do-
main adaptation. In Proceedings of COLING 2012:
Posters, pages 1191?1200, Mumbai, India, Decem-
ber. The COLING 2012 Organizing Committee.
Weiwei Sun and Jia Xu. 2011. Enhancing chinese
word segmentation using unlabeled data. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 970?
979, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proceedings of ACL-
08: HLT, pages 665?673, Columbus, Ohio, June.
Association for Computational Linguistics.
Huihsin Tseng. 2005. A conditional random field word
segmenter. In In Fourth SIGHAN Workshop on Chi-
nese Language Processing.
Yuta Tsuboi, Hisashi Kashima, Shinsuke Mori, Hiroki
Oda, and Yuji Matsumoto. 2008. Training condi-
tional random fields using incomplete annotations.
In Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
897?904, Manchester, UK, August. Coling 2008 Or-
ganizing Committee.
873
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmentation
and pos tagging with semi-supervised methods using
large auto-analyzed data. In Proceedings of 5th In-
ternational Joint Conference on Natural Language
Processing, pages 309?317, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.
Pak-kwong Wong and Chorkin Chan. 1996. Chinese
word segmentation based on maximum matching
and word binding force. In Proceedings of the 16th
Conference on Computational Linguistics - Volume
1, COLING ?96, pages 200?203, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Zimin Wu and Gwyneth Tseng. 1993. Chinese text
segmentation for text retrieval: Achievements and
problems. J. Am. Soc. Inf. Sci., 44(9):532?542, Oc-
tober.
Nianwen Xue and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In Proceedings of the
Second SIGHAN Workshop on Chinese Language
Processing - Volume 17, SIGHAN ?03, pages 176?
179, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming
Duan, Shiyong Kang, Honglin Sun, Hui Wang,
Qiang Zhao, and Weidong Zhan. 2001. Processing
norms of modern chinese corpus. Technical report.
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-based perceptron algorithm. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 840?
847, Prague, Czech Republic, June. Association for
Computational Linguistics.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2014. Type-supervised domain adaptation for
joint segmentation and pos-tagging. In Proceed-
ings of the 14th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 588?597, Gothenburg, Sweden, April. Asso-
ciation for Computational Linguistics.
Hai Zhao and Chunyu Kit. 2008. An empirical com-
parison of goodness measures for unsupervised chi-
nese word segmentation with a unified framework.
In In: The Third International Joint Conference on
Natural Language Processing (IJCNLP-2008.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2010. A unified character-based tagging frame-
work for chinese word segmentation. 9(2):5:1?5:32,
June.
874
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1415?1425,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Using Structured Events to Predict Stock Price Movement:
An Empirical Investigation
Xiao Ding
??
, Yue Zhang
?
, Ting Liu
?
, Junwen Duan
?
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{xding, tliu, jwduan}@ir.hit.edu.cn
?
Singapore University of Technology and Design
yue zhang@sutd.edu.sg
Abstract
It has been shown that news events influ-
ence the trends of stock price movements.
However, previous work on news-driven
stock market prediction rely on shallow
features (such as bags-of-words, named
entities and noun phrases), which do not
capture structured entity-relation informa-
tion, and hence cannot represent complete
and exact events. Recent advances in
Open Information Extraction (Open IE)
techniques enable the extraction of struc-
tured events from web-scale data. We
propose to adapt Open IE technology for
event-based stock price movement pre-
diction, extracting structured events from
large-scale public news without manual
efforts. Both linear and nonlinear mod-
els are employed to empirically investigate
the hidden and complex relationships be-
tween events and the stock market. Large-
scale experiments show that the accuracy
of S&P 500 index prediction is 60%, and
that of individual stock prediction can be
over 70%. Our event-based system out-
performs bags-of-words-based baselines,
and previously reported systems trained on
S&P 500 stock historical data.
1 Introduction
Predicting stock price movements is of clear in-
terest to investors, public companies and govern-
ments. There has been a debate on whether the
market can be predicted. The Random Walk The-
ory (Malkiel, 1973) hypothesizes that prices are
determined randomly and hence it is impossible to
outperform the market. However, with advances
of AI, it has been shown empirically that stock
?
This work was done while the first author was visiting
Singapore University of Technology and Design
Figure 1: Example news for Apple Inc. and
Google Inc.
price movement is predictable (Bondt and Thaler,
1985; Jegadeesh, 1990; Lo and MacKinlay, 1990;
Jegadeesh and Titman, 1993). Recent work (Das
and Chen, 2007; Tetlock, 2007; Tetlock et al.,
2008; Si et al., 2013; Xie et al., 2013; Wang and
Hua, 2014) has applied Natural Language Process-
ing (NLP) techniques to help analyze the effect of
web texts on stock market prediction, finding that
events reported in financial news are important ev-
idence to stock price movement prediction.
As news events affect human decisions and the
volatility of stock prices is influenced by human
trading, it is reasonable to say that events can influ-
ence the stock market. Figure 1 shows two pieces
of financial news about Apple Inc. and Google
Inc., respectively. Shares of Apple Inc. fell as trad-
ing began in New York on Thursday morning, the
day after its former CEO Steve Jobs passed away.
Google?s stock fell after grim earnings came out.
Accurate extraction of events from financial news
may play an important role in stock market pre-
diction. However, previous work represents news
documents mainly using simple features, such as
bags-of-words, noun phrases, and named entities
(Lavrenko et al., 2000; Kogan et al., 2009; Luss
and d?Aspremont, 2012; Schumaker and Chen,
2009). With these unstructured features, it is dif-
ficult to capture key events embedded in financial
news, and even more difficult to model the impact
of events on stock market prediction. For exam-
ple, representing the event ?Apple has sued Sam-
sung Electronics for copying ?the look and feel?
1415
of its iPad tablet and iPhone smartphone.? using
term-level features {?Apple?, ?sued?, ?Samsung?,
?Electronics?, ?copying?, ...} alone, it can be dif-
ficult to accurately predict the stock price move-
ments of Apple Inc. and Samsung Inc., respec-
tively, as the unstructured terms cannot indicate
the actor and object of the event.
In this paper, we propose using structured in-
formation to represent events, and develop a pre-
diction model to analyze the relationship between
events and the stock market. The problem is im-
portant because it provides insights into under-
standing the underlying mechanisms of the influ-
ence of events on the stock market. There are two
main challenges to this method. On the one hand,
how to obtain structured event information from
large-scale news streams is a challenging problem.
We propose to apply Open Information Extraction
techniques (Open IE; Banko et al. (2007); Et-
zioni et al. (2011); Fader et al. (2011)), which
do not require predefined event types or manu-
ally labeled corpora. Subsequently, two ontolo-
gies (i.e. VerbNet and WordNet) are used to gen-
eralize structured event features in order to reduce
their sparseness. On the other hand, the problem
of accurately predicting stock price movement us-
ing structured events is challenging, since events
and the stock market can have complex relations,
which can be influenced by hidden factors. In ad-
dition to the commonly used linear models, we
build a deep neural network model, which takes
structured events as input and learn the potential
relationships between events and the stock market.
Experiments on large-scale financial news
datasets from Reuters
1
(106,521 documents)
and Bloomberg
2
(447,145 documents) show that
events are better features for stock market predic-
tion than bags-of-words. In addition, deep neu-
ral networks achieve better performance than lin-
ear models. The accuracy of S&P 500 index pre-
diction by our approach outperforms previous sys-
tems, and the accuracy of individual stock predic-
tion can be over 70% on the large-scale data.
Our system can be regarded as one step towards
building an expert system that exploits rich knowl-
edge for stock market prediction. Our results are
helpful for automatically mining stock price re-
lated news events, and for improving the accuracy
of algorithm trading systems.
1
http://www.reuters.com/
2
http://www.bloomberg.com/
2 Method
2.1 Event Representation
We follow the work of Kim (1993) and design a
structured representation scheme that allows us to
extract events and generalize them. Kim defines
an event as a tuple (O
i
, P , T ), where O
i
? O is
a set of objects, P is a relation over the objects
and T is a time interval. We propose a representa-
tion that further structures the event to have roles
in addition to relations. Each event is composed
of an action P , an actor O
1
that conducted the
action, and an object O
2
on which the action was
performed. Formally, an event is represented as
E = (O
1
, P, O
2
, T ), where P is the action, O
1
is the actor,O
2
is the object and T is the timestamp
(T is mainly used for aligning stock data with
news data). For example, the event ?Sep 3, 2013
- Microsoft agrees to buy Nokia?s mobile phone
business for $7.2 billion.? is modeled as: (Actor =
Microsoft, Action = buy, Object = Nokia?s mobile
phone business, Time = Sep 3, 2013).
Previous work on stock market prediction rep-
resents events as a set of individual terms (Fung
et al., 2002; Fung et al., 2003; Hayo and Ku-
tan, 2004; Feldman et al., 2011). For example,
?Microsoft agrees to buy Nokia?s mobile phone
business for $7.2 billion.? can be represented by
{?Microsoft?, ?agrees?, ?buy?, ?Nokia?s?, ?mo-
bile?, ...} and ?Oracle has filed suit against Google
over its ever-more-popular mobile operating sys-
tem, Android.? can be represented by {?Oracle?,
?has?, ?filed?, ?suit?, ?against?, ?Google?, ...}.
However, terms alone might fail to accurately pre-
dict the stock price movement ofMicrosoft, Nokia,
Oracle and Google, because they cannot indicate
the actor and object of the event. To our knowl-
edge, no effort has been reported in the literature
to empirically investigate structured event repre-
sentations for stock market prediction.
2.2 Event Extraction
A main contribution of our work is to extract and
use structured events instead of bags-of-words in
prediction models. However, structured event ex-
traction can be a costly task, requiring predefined
event types and manual event templates (Ji and Gr-
ishman, 2008; Li et al., 2013). Partly due to this,
the bags-of-words-based document representation
has been the mainstream method for a long time.
To tackle this issue, we resort to Open IE, extract-
ing event tuples from wide-coverage data with-
1416
out requiring any human input (e.g. templates).
Our system is based on the system of Fader et al.
(2011) and the work of Ding et al. (2013); it does
not require predefined target event types and la-
beled training examples. Given a natural language
sentence obtained from news texts, the following
procedure is used to extract structured events:
1. Event Phrase Extraction. We extract the
predicate verb P of a sentence based on
the dependency parser of Zhang and Clark
(2011), and then find the longest sequence of
words P
v
, such that P
v
starts at P and satis-
fies the syntactic and lexical constraints pro-
posed by Fader et al. (2011). The content of
these two constraints are as follows:
? Syntactic constraint: every multi-word
event phrase must begin with a verb, end
with a preposition, and be a contiguous
sequence of words in the sentence.
? Lexical constraint: an event phrase
should appear with at least a minimal
number of distinct argument pairs in a
large corpus.
2. Argument Extraction. For each event
phrase P
v
identified in the step above, we find
the nearest noun phrase O
1
to the left of P
v
in the sentence, and O
1
should contain the
subject of the sentence (if it does not contain
the subject of P
v
, we find the second near-
est noun phrase). Analogously, we find the
nearest noun phrase O
2
to the right of P
v
in
the sentence, and O
2
should contain the ob-
ject of the sentence (if it does not contain the
object of P
v
, we find the second nearest noun
phrase).
An example of the extraction algorithm is as fol-
lows. Consider the sentence,
Instant view: Private sector adds 114,000 jobs
in July: ADP.
The predicate verb is identified as ?adds?, and
its subject and object ?sector? and ?jobs?, respec-
tively. The structured event is extracted as (Private
sector, adds, 114,000 jobs).
2.3 Event Generalization
Our goal is to train a model that is able to make
predictions based on various expressions of the
same event. For example, ?Microsoft swallows
Nokia?s phone business for $7.2 billion? and ?Mi-
crosoft purchases Nokia?s phone business? report
the same event. To improve the accuracy of our
prediction model, we should endow the event ex-
traction algorithm with generalization capacity.
To this end, we leverage knowledge from two
well-known ontologies, WordNet (Miller, 1995)
and VerbNet (Kipper et al., 2006). The pro-
cess of event generalization consists of two steps.
First, we construct a morphological analysis tool
based on the WordNet stemmer to extract lemma
forms of inflected words. For example, in ?In-
stant view: Private sector adds 114,000 jobs in
July.?, the words ?adds? and ?jobs? are trans-
formed to ?add? and ?job?, respectively. Second,
we generalize each verb to its class name in Verb-
Net. For example, ?add? belongs to the multi-
ply class. After generalization, the event (Private
sector, adds, 114,000 jobs) becomes (private sec-
tor, multiply class, 114,000 job). Similar methods
on event generalization have been investigated in
Open IE based event causal prediction (Radinsky
and Horvitz, 2013).
2.4 Prediction Models
1. Linear model. Most previous work uses linear
models to predict the stock market (Fung et al.,
2002; Luss and d?Aspremont, 2012; Schumaker
and Chen, 2009; Kogan et al., 2009; Das and
Chen, 2007; Xie et al., 2013). To make direct com-
parisons, this paper constructs a linear prediction
model by using Support Vector Machines (SVMs),
a state-of-the-art classification model. Given a
training set (d
1
, y
1
), (d
2
, y
2
), ..., (d
N
, y
N
),
where n ? [1, N ], d
n
is a news document and
y
i
? {+1, ?1} is the output class. d
n
can be
news titles, news contents or both. The output
Class +1 represents that the stock price will in-
crease the next day/week/month, and the output
Class -1 represents that the stock price will de-
crease the next day/week/month. The features
can be bag-of-words features or structured event
features. By SVMs, y = argmax{Class +
1, Class ? 1} is determined by the linear func-
tion w ??(d
n
, y
n
), where w is the feature weight
vector, and ?(d
n
, y
n
) is a function that maps d
n
into a M-dimensional feature space. Feature tem-
plates will be discussed in the next subsection.
2. Nonlinear model. Intuitively, the relationship
between events and the stock market may be more
complex than linear, due to hidden and indirect
1417
? 
News documents 
?1 
Class +1 The polarity of the stock price movement is positive 
Class -1 The polarity of the stock price movement is negative 
Input Layer 
Output Layer 
Hidden Layers ? 
? 
?2 ?3 ?M 
Figure 2: Structure of the deep neural network
model
relationships. We exploit a deep neural network
model, the hidden layers of which is useful for
learning such hidden relationships. The structure
of the model with two hidden layers is illustrated
in Figure 2. In all layers, the sigmoid activation
function ? is used.
Let the values of the neurons of the output layer
be y
cls
(cls ? {+1,?1}), its input be net
cls
, and
y
2
be the value vector of the neurons of the last
hidden layer; then:
y
cls
= f(net
cls
) = ?(wcls ? y2) (1)
where wcls is the weight vector between the neu-
ron cls of the output layer and the neurons of the
last hidden layer. In addition,
y
2k
= ?(w
2k ? y1) (k ? [1, |y2|])
y
1j
= ?(w
1j ??(dn)) (j ? [1, |y1|])
(2)
Here y
1
is the value vector of the neu-
rons of the first hidden layer, w
2k =
(w
2k1
, w
2k2
, ..., w
2k|y
1
|
), k ? [1, |y
2
|] and
w
1j = (w1j1, w1j2, ..., w1jM ), j ? [1, |y1|].
w
2kj
is the weight between the kth neuron of
the last hidden layer and the jth neuron of the
first hidden layer; w
1jm
is the weight between
the jth neuron of the first hidden layer and the
mth neuron of the input layer m ? [1, M ]; d
n
is a news document and ?(d
n
) maps d
n
into a
M-dimensional features space. News documents
and features used in the nonlinear model are the
same as those in the linear model, which will be
introduced in details in the next subsection. The
standard back-propagation algorithm (Rumelhart
et al., 1985) is used for supervised training of the
neural network.
train dev test
number of
instances
1425 178 179
number of
events
54776 6457 6593
time inter-
val
02/10/2006
-
18/16/2012
19/06/2012
-
21/02/2013
22/02/2013
-
21/11/2013
Table 1: Dataset splitting
2.5 Feature Representation
In this paper, we use the same features (i.e. docu-
ment representations) in the linear and nonlinear
prediction models, including bags-of-words and
structured events.
(1) Bag-of-words features. We use the clas-
sic ?TFIDF? score for bag-of-words features. Let
L be the vocabulary size derived from the train-
ing data (introduced in the next section), and
freq(t
l
) denote the number of occurrences of
the lth word in the vocabulary in document d.
TF
l
=
1
|d|
freq(t
l
), ?l ? [1 , L], where |d| is
the number of words in the document d (stop
words are removed). TFIDF
l
=
1
|d|
freq(t
l
) ?
log(
N
|{d :freq(t
l
)>0}|
), where N is the number of
documents in the training set. The feature vector
? can be represented as? = (?
1
, ?
2
, ..., ?
M
) =
(TFIDF
1
, TFIDF
2
, ..., TFIDF
M
). The TFIDF
feature representation has been used by most pre-
vious studies on stock market prediction (Kogan et
al., 2009; Luss and d?Aspremont, 2012).
(2) Event features. We represent an event
tuple (O
1
, P, O
2
, T ) by the combination of
elements (except for T) (O
1
, P , O
2
, O
1
+ P ,
P + O
2
, O
1
+ P + O
2
). For example, the
event tuple (Microsoft, buy, Nokia?s mobile phone
business) can be represented as (#arg1=Microsoft,
#action=get class, #arg2=Nokia?s mobile phone
business, #arg1 action=Microsoft get class,
#action arg2=get class Nokia?s mobile phone
business, #arg1 action arg2=Microsoft get class
Nokia?s mobile phone business). Structured
events are more sparse than words, and we reduce
sparseness by two means. First, verb classes
(Section 2.3) are used instead of verbs for P. For
example, ?get class? is used instead of the verb
?buy?. Second, we use back-off features, such
as O
1
+ P (?Microsoft get class?) and P + O
2
(?get class Nokia?s mobile phone business?), to
address the sparseness ofO
1
andO
2
. Note that the
order of O
1
and O
2
is important for our task since
they indicate the actor and object, respectively.
1418
 0.52
 0.53
 0.54
 0.55
 0.56
 0.57
 0.58
 0.59
 0.6
1day 1week 1month
Ac
cu
rac
y
Time span
bow+svmbow+deep neural network
event+svm
event+deep neural network
(a) Accuarcy
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
1day 1week 1month
MC
C
Time span
bow+svmbow+deep neural network
event+svm
event+deep neural network
(b) MCC
Figure 3: Overall development experiment results
3 Experiments
Our experiments are carried out on three differ-
ent time intervals: short term (1 day), medium
term (1 week) and long term (1 month). We test
the influence of events on predicting the polarity
of stock change for each time interval, comparing
the event-based news representation with bag-of-
words-based news representations, and the deep
neural network model with the SVM model.
3.1 Data Description
We use publicly available financial news from
Reuters and Bloomberg over the period from Oc-
tober 2006 to November 2013. This time span
witnesses a severe economic downturn in 2007-
2010, followed by a modest recovery in 2011-
2013. There are 106,521 documents in total
from Reuters News and 447,145 from Bloomberg
News. News titles and contents are extracted from
HTML. The timestamps of the news are also ex-
tracted, for alignment with stock price informa-
tion. The data size is larger than most previous
work in the literature.
We mainly focus on predicting the change of the
Standard & Poor?s 500 stock (S&P 500) index
3
,
obtaining indices and stock price data from Yahoo
Finance. To justify the effectiveness of our predic-
tion model, we also predict price movements of
fifteen individual shares from different sectors in
S&P 500. We automatically align 1,782 instances
of daily trading data with news titles and contents
from the previous day/the day a week before the
stock price data/the day a month before the stock
price data, 4/5 of which are used as the training
3
Standard & Poor?s 500 is a stock market index based
on the market capitalizations of 500 large companies having
common stock listed on the NYSE or NASDAQ.
data, 1/10 for development testing and 1/10 for
testing. As shown in Table 1, the training, devel-
opment and test set are split temporally, with the
data from 02/10/2006 to 18/16/2012 for training,
the data from 19/06/2012 to 21/02/2013 for de-
velopment testing, and the data from 22/02/2013
to 21/11/2013 for testing. There are about 54,776
events in the training set, 6,457 events in the de-
velopment set and 6,593 events in the test set.
3.2 Evaluation Metrics
We use two assessment metrics. First, a standard
and intuitive approach to measuring the perfor-
mance of classifiers is accuracy. However, this
measure is very sensitive to data skew: when a
class has an overwhelmingly high frequency, the
accuracy can be high using a classifier that makes
prediction on the majority class. Previous work
(Xie et al., 2013) uses an additional evaluation
metric, which relies on the Matthews Correlation
Cofficient (MCC) to avoid bias due to data skew
(our data are rather large and not severely skewed,
but we also use MCC for comparison with previ-
ous work). MCC is a single summary value that
incorporates all 4 cells of a 2*2 confusion matrix
(True Positive, False Positive, True Negative and
False Negative, respectively). GivenTP ,TN , FP
and FN :
MCC =
TP ?TN?FP ?FN
?
(TP+FP)(TP+FN )(TN +FP)(TN +FN )
(3)
3.3 Overall Development Results
We evaluate our four prediction methods (i.e.
SVM with bag-of-word features (bow), deep neu-
ral network with bag-of-word features (bow),
1419
1 day 1 week 1 month
1 layer
Accuracy 58.94% 57.73% 55.76%
MCC 0.1249 0.0916 0.0731
2 layers
Accuracy 59.60% 57.73% 56.19%
MCC 0.1683 0.1215 0.0875
Table 2: Different numbers of hidden layers
title content content +
title
bloomberg
title + title
Acc 59.60% 54.65% 56.83% 59.64%
MCC 0.1683 0.0627 0.0852 0.1758
Table 3: Different amounts of data
SVM with event features and deep neural network
with event features) on three time intervals (i.e.
1 day, 1 week and 1 month, respectively) on the
development dataset, and show the results in Fig-
ure 3. We find that:
(1) Structured event is a better choice for rep-
resenting news documents. Given the same pre-
diction model (SVM or deep neural network), the
event-based method achieves consistently better
performance than the bag-of-words-based method
over all three time intervals. This is likely due
to the following two reasons. First, being an ex-
traction of predicate-argument structures, events
carry the most essential information of the docu-
ment. In contrast, bag-of-words can contain more
irrelevant information. Second, structured events
can directly give the actor and object of the action,
which is important for predicting stock market.
(2) The deep neural network model achieves
better performance than the SVM model, partly by
learning hidden relationships between structured
events and stock prices. We give analysis to these
relationships in the next section.
(3) Event information is a good indicator for
short-term volatility of stock prices. As shown in
Figure 3, the performance of daily prediction is
better than weekly and monthly prediction. Our
experimental results confirm the conclusion of
Tetlock, Saar-Tsechansky, and Macskassy (2008)
that there is a one-day delay between the price
response and the information embedded in the
news. In addition, we find that some events may
cause immediate changes of stock prices. For ex-
ample, former Microsoft CEO Steve Ballmer an-
nounced he would step down within 12 months
on 23/08/2013. Within an hour, Microsoft shares
jumped as much as 9 percent. This fact indicates
that it may be possible to predict stock price move-
ment on a shorter time interval than one day. How-
Google Inc.
Company News Sector News All News
Acc MCC Acc MCC Acc MCC
67.86% 0.4642 61.17% 0.2301 55.70% 0.1135
Boeing Company
Company News Sector News All News
Acc MCC Acc MCC Acc MCC
68.75% 0.4339 57.14% 0.1585 56.04% 0.1605
Wal-Mart Stores
Company News Sector News All News
Acc MCC Acc MCC Acc MCC
70.45% 0.4679 62.03% 0.2703 56.04% 0.1605
Table 4: Individual stock prediction results
ever, we cannot access fine-grained stock price
historical data, and this investigation will be left
as future work.
3.4 Experiments with Different Numbers of
Hidden Layers of the Deep Neural
Network Model
Cybenko (1989) states that when every processing
element utilizes the sigmoid activation function,
one hidden layer is enough to solve any discrim-
inant classification problem, and two hidden lay-
ers are capable to parse arbitrary output functions
of input pattern. Here we conduct a development
experiment by different number of hidden layers
for the deep neural network model. As shown in
Table 2, the performance of two hidden layers is
better than one hidden layer, which is consistent
with the experimental results of Sharda and De-
len (2006) on the task of movie box-office predic-
tion. It indicates that more hidden layers can ex-
plain more complex relations (Bengio, 2009). In-
tuitively, three or more hidden layers may achieve
better performance. However, three hidden lay-
ers mean that we construct a five-layer deep neu-
ral network, which is difficult to train (Bengio et
al., 1994). We did not obtain improved accuracies
using three hidden layers, due to diminishing gra-
dients. A deep investigation of this problem is out
of the scope of this paper.
3.5 Experiments with Different Amounts of
Data
We conduct a development experiment by extract-
ing news titles and contents from Reuters and
Bloomberg, respectively. While titles can give the
central information about the news, contents may
provide some background knowledge or details.
Radinsky et al. (2012) argued that news titles are
more helpful for prediction compared to news con-
1420
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0  100  200  300  400  500
Ac
cu
rac
y
Company Ranking
Wal-Mart
GoogleBoeing
Nike
Qualcomm
Apache
Starbucks
Avon
Visa
Symantec
Hershey
Mattel
Actavis GannettSanDisk
individual stock
(a) Accuarcy
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0  100  200  300  400  500
MC
C
Company Ranking
Wal-MartGoogle
Boeing
Nike Qualcomm
Apache
Starbucks
Avon
Visa
Symantec
Hershey
Mattel
Actavis Gannett
SanDisk
individual stock
(b) MCC
Figure 4: Individual stock prediction experiment results
tents, and this paper mainly uses titles. Here we
design a comparative experiment to analyze the ef-
fectiveness of news titles and contents. First, we
use Reuters news to compare the effectiveness of
news titles and contents, and then add Bloomberg
news titles to investigate whether the amounts of
data matters. Table 3 shows that using only news
titles achieves the best performance. A likely rea-
son is that we may extract some irrelevant events
from news contents.
With the additional Bloomberg data, the results
are not dramatically improved. This is intuitively
because most events are reported by both Reuters
news and Bloomberg news. We randomly se-
lect about 9,000 pieces of news documents from
Reuters and Bloomberg and check the daily over-
lap manually, finding that about 60% of the news
are reported by both Reuters and Bloomberg. The
overlap of important news (news related to S&P
500 companies) is 80% and the overlap of unim-
portant news is 40%.
3.6 Individual Stock Prediction
In addition to predicting the S&P 500 index, we
also investigate the effectiveness of our approach
on the problem of individual stock prediction us-
ing the development dataset. We select three well-
known companies, Google Inc., Boeing Company
and Wal-Mart Stores from three different sec-
tors (i.e. Information Technology, Industrials and
Consumer Staples, respectively) classified by the
Global Industry Classification Standard (GICS).
We use company news, sector news and all news to
predict individual stock price movement, respec-
tively. The experimental results are listed in Ta-
ble 4.
The result of individual stock prediction by us-
ing only company news dramatically outperforms
the result of S&P 500 index prediction. The main
reason is that company-related events can directly
affect the volatility of company shares. There is
a strong correlation between company events and
company shares. Table 4 also shows that the result
of individual stock prediction by using sector news
or all news does not achieve a good performance,
probably because there are many irrelevant events
in all news, which would reduce the performance
of our prediction model.
The fact that the accuracy of these well-known
stocks are higher than the index may be because
there is relatively more news events dedicated to
the relevant companies. To gain a better under-
standing of the behavior of the model on more
individual stocks, we randomly select 15 compa-
nies (i.e. Google Inc., Boeing Company, Wal-Mart
Stores, Nike Inc., QUALCOMM Inc., Apache Cor-
poration, Starbucks Corp., Avon Products, Visa
Inc., Symantec Corp., The Hershey Company,
Mattel Inc., Actavis plc, Gannett Co. and SanDisk
Corporation) from S&P 500 companies. More
specifically, according to the Fortune ranking of
S&P 500 companies
4
, we divide the ranked list
into five parts, and randomly select three compa-
nies from each part. The experimental results are
shown in Figure 4. We find that:
(1) All 15 individual stocks can be predicted
with accuracies above 50%, while 60% of the
stocks can be predicted with accuracies above
60%. It shows that the amount of company-related
events has strong relationship with the volatility of
4
http://money.cnn.com/magazines/fortune/fortune500/.
The amount of company-related news is correlated to the
fortune ranking of companies. However, we find that the
trade volume does not have such a correlation with the
ranking.
1421
S&P 500 Index Prediction
Individual Stock Prediction
Google Inc. Boeing Company Wal-Mart Stores
Accuracy MCC Accuracy MCC Accuracy MCC Accuracy MCC
dev 59.60% 0.1683 67.86% 0.4642 68.75% 0.4339 70.45% 0.4679
test 58.94% 0.1649 66.97% 0.4435 68.03% 0.4018 69.87% 0.4456
Table 5: Final experimental results on the test dataset
company shares.
(2) With decreasing company fortune rankings,
the accuracy and MCC decrease. This is mainly
because there is not as much daily news about low-
ranking companies, and hence one cannot extract
enough structured events to predict the volatility
of these individual stocks.
3.7 Final Results
The final experimental results on the test dataset
are shown in Table 5 (as space is limited, we show
the results on the time interval of one day only).
The experimental results on the development and
test datasets are consistent, which indicate that our
approach has good robustness. The following con-
clusions obtained from development experiments
also hold on the test dataset:
(1) Structured events are more useful represen-
tations compared to bags-of-words for the task of
stock market prediction.
(2) A deep neural network model can be more
accurate on predicting the stock market compared
to the linear model.
(3) Our approach can achieve stable experiment
results on S&P 500 index prediction and individ-
ual stock prediction over a large amount of data
(eight years of stock prices and more than 550,000
pieces of news).
(4) The quality of information is more impor-
tant than the quantity of information on the task
of stock market prediction. That is to say that the
most relevant information (i.e. news title vs news
content, individual company news vs all news) is
better than more, but less relevant information.
3.8 Analysis and Discussion
We use Figure 5 to demonstrate our analysis to
the development experimental result of Google
Inc. stock prediction, which directly shows the
relationship between structured events and the
stock market. The links between each layer show
the magnitudes of feature weights in the model
learned using the training set.
Three events, (Google, says bought stake in,
China?s XunLei), (Google, reveals stake in, Chi-
?1 ?2 ?3 ?4 ?5 ?6 ?7 ?8 ?M ?1: (Google, says bought stake in, China?s XunLei) ?4: (Google, reveals stake in, Chinese social website) ?6: (Capgemini, partners, Google apps software)  ?2: (Oracle, sues, Google) ?5: (Google map, break, privacy law) ?8: (Google, may pull out of, China) 
? 
? 
Figure 5: Prediction of Google Inc. (we only show
structured event features since backoff features are
less informative)
nese social website) and (Capgemini, partners,
Google apps software), have the highest link
weights to the first hidden node (from the left).
These three events indicate that Google constantly
makes new partners and expands its business area.
The first hidden node has high-weight links to
Class +1, showing that Google?s positive coopera-
tion can lead to the rise of its stock price.
Three other events, (Oracle, sues, Google),
(Google map, break, privacy law) and (Google,
may pull out of, China), have high-weight links
to the second hidden node. These three events
show that Google was suffering questions and
challenges, which could affect its reputation and
further pull down its earnings. Correspondingly,
the second hidden node has high-weight links to
Class -1. These suggest that our method can au-
tomatically and directly reveal complex relation-
ships between structured events and the stock mar-
ket, which is very useful for investors, and can fa-
cilitate the research of stock market prediction.
Note that the event features used in our predic-
tion model are generalized based on the algorithm
introduced in Section 2.5. Therefore, though a
specific event in the development test set might
have never happened, its generalized form can be
found in the training set. For example, ?Google
acquired social marketing company Wildfire In-
1422
teractive? is not in the training data, but ?Google
get class? (?get? is the class name of ?acquire?
and ?buy? in VerbNet) can indeed be found in the
training set, such as ?Google bought stake in Xun-
Lei? on 04/01/2007. Hence although the full spe-
cific event feature does not fire, its back-offs fire
for a correct prediction. For simplicity of showing
the event, we did not include back-off features in
Figure 5.
4 Related Work
Stock market prediction has attracted a great deal
of attention across the fields of finance, computer
science and other research communities in the
past. The literature of stock market prediction
was initiated by economists (Keynes, 1937). Sub-
sequently, the influential theory of Efficient Mar-
ket Hypothesis (EMH) (Fama, 1965) was estab-
lished, which states that the price of a security re-
flects all of the information available and that ev-
eryone has a certain degree of access to the infor-
mation. EMH had a significant impact on security
investment, and can serve as the theoretical basis
of event-based stock price movement prediction.
Various studies have found that financial news
can dramatically affect the share price of a se-
curity (Chan, 2003; Tetlock et al., 2008). Cul-
ter et al. (1998) was one of the first to investi-
gate the relationship between news coverage and
stock prices, since which empirical text analysis
technology has been widely used across numerous
disciplines (Lavrenko et al., 2000; Kogan et al.,
2009; Luss and d?Aspremont, 2012). These stud-
ies primarily use bags-of-words to represent finan-
cial news documents. However, as Schumaker and
Chen (2009) and Xie et al. (2013) point out, bag-
of-words features are not the best choice for pre-
dicting stock prices. Schumaker and Chen (2009)
extract noun phrases and named entities to aug-
ment bags-of-words. Xie et al. (2013) explore a
rich feature space that relies on frame semantic
parsing. Wang et al. (2014) use the same fea-
tures as Xie et al. (2013), but they perform non-
parametric kernel density estimation to smooth out
the distribution of features. These can be regarded
as extensions to the bag-of-word method. The
drawback of these approaches, as discussed in the
introduction, is that they do not directly model
events, which have structured information.
There has been efforts to model events more di-
rectly (Fung et al., 2002; Hayo and Kutan, 2005;
Feldman et al., 2011). Fung, Yu, and Lam (2002)
use a normalized word vector-space to model
event. Feldman et al. (2011) extract 9 prede-
fined categories of events based on heuristic rules.
There are two main problems with these efforts.
First, they cannot extract structured event (e.g. the
actor of the event and the object of the event). Sec-
ond, Feldman et al. (2011) can obtain only lim-
ited categories of events, and hence the scalabil-
ity of their work is not strong. In contrast, we
extract structured events by leveraging Open In-
formation Extraction technology (Open IE; Yates
et al. (2007); Etzioni et al. (2011); Faber et al.
(2011)) without predefined event types, which can
effectively solve the two problems above.
Apart from events, sentiment analysis is another
perspective to the problem of stock prediction
(Das and Chen, 2007; Tetlock, 2007; Tetlock et
al., 2008; Bollen et al., 2011; Si et al., 2013). Tet-
lock (2007) examines how qualitative information
(i.e. the fraction of negative words in a particular
news column) is incorporated in aggregate market
valuations. Tetlock, Saar-Tsechansky, and Mac-
skassy (2008) extend that analysis to address the
impact of negative words in all Wall Street Joural
(WSJ) and Dow Jones News Services (DJNS) sto-
ries about individual S&P500 firms from 1980 to
2004. Bollen and Zeng (2011) study whether the
large-scale collective emotion on Twitter is cor-
related with the volatility of Dow Jones Indus-
trial Average (DJIA). From the experimental re-
sults, they find that changes of the public mood
match shifts in the DJIA values that occur 3 to 4
days later. Sentiment-analysis-based stock mar-
ket prediction focuses on investigating the influ-
ence of subjective emotion. However, this paper
puts emphasis on the relationship between objec-
tive events and the stock price movement, and is
orthogonal to the study of subjectivity. As a result,
our model can be combined with the sentiment-
analysis-based method.
5 Conclusion
In this paper, we have presented a framework for
event-based stock price movement prediction. We
extracted structured events from large-scale news
based on Open IE technology and employed both
linear and nonlinear models to empirically investi-
gate the complex relationships between events and
the stock market. Experimental results showed
that events-based document representations are
1423
better than bags-of-words-based methods, and
deep neural networks can model the hidden and in-
directed relationship between events and the stock
market. For further comparisons, we freely release
our data at http://ir.hit.edu.cn/?xding/data.
Acknowledgments
We thank the anonymous reviewers for their
constructive comments, and gratefully acknowl-
edge the support of the National Basic Re-
search Program (973 Program) of China via Grant
2014CB340503, the National Natural Science
Foundation of China (NSFC) via Grant 61133012
and 61202277, the Singapore Ministry of Educa-
tion (MOE) AcRF Tier 2 grant T2MOE201301
and SRG ISTD 2012 038 from Singapore Univer-
sity of Technology and Design. We are very grate-
ful to Ji Ma for providing an implementation of the
neural network algorithm.
References
Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction for the web. In IJCAI,
volume 7, pages 2670?2676.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
dient descent is difficult. Neural Networks, IEEE
Transactions on, 5(2):157?166.
Yoshua Bengio. 2009. Learning deep architectures for
ai. Foundations and trends
R
? in Machine Learning,
2(1):1?127.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1?8.
Werner FM Bondt and Richard Thaler. 1985. Does
the stock market overreact? The Journal of finance,
40(3):793?805.
Wesley S Chan. 2003. Stock price reaction to news and
no-news: Drift and reversal after headlines. Journal
of Financial Economics, 70(2):223?260.
David M Cutler, James M Poterba, and Lawrence H
Summers. 1998. What moves stock prices? Bern-
stein, Peter L. and Frank L. Fabozzi, pages 56?63.
George Cybenko. 1989. Approximation by superposi-
tions of a sigmoidal function. Mathematics of con-
trol, signals and systems, 2(4):303?314.
Sanjiv R Das and Mike Y Chen. 2007. Yahoo! for
amazon: Sentiment extraction from small talk on the
web. Management Science, 53(9):1375?1388.
Xiao Ding, Bing Qin, and Ting Liu. 2013. Building
chinese event type paradigm based on trigger clus-
tering. In Proc. of IJCNLP, pages 311?319, Octo-
ber.
Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, and Mausam Mausam. 2011.
Open information extraction: The second genera-
tion. In Proceedings of the Twenty-Second inter-
national joint conference on Artificial Intelligence-
Volume Volume One, pages 3?10. AAAI Press.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1535?1545. Association for Computational
Linguistics.
Eugene F Fama. 1965. The behavior of stock-market
prices. The journal of Business, 38(1):34?105.
Ronen Feldman, Benjamin Rosenfeld, Roy Bar-Haim,
and Moshe Fresko. 2011. The stock sonarsentiment
analysis of stocks based on a hybrid approach. In
Twenty-Third IAAI Conference.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, and Wai
Lam. 2002. News sensitive stock trend prediction.
In Advances in Knowledge Discovery and Data Min-
ing, pages 481?493. Springer.
Bernd Hayo and Ali M Kutan. 2005. The impact of
news, oil prices, and global market developments
on russian financial markets1. Economics of Tran-
sition, 13(2):373?393.
Narasimhan Jegadeesh and Sheridan Titman. 1993.
Returns to buying winners and selling losers: Im-
plications for stock market efficiency. The Journal
of Finance, 48(1):65?91.
Narasimhan Jegadeesh. 1990. Evidence of predictable
behavior of security returns. The Journal of Fi-
nance, 45(3):881?898.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In ACL,
pages 254?262.
John Maynard Keynes. 1937. The general theory of
employment. The Quarterly Journal of Economics,
51(2):209?223.
Jaegwon Kim. 1993. Supervenience and mind: Se-
lected philosophical essays. Cambridge University
Press.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2006. Extending verbnet with novel
verb classes. In Proceedings of LREC, volume 2006,
page 1.
Shimon Kogan, Dimitry Levin, Bryan R. Routledge,
Jacob S. Sagi, and Noah A. Smith. 2009. Predicting
risk from financial reports with regression. In Proc.
NAACL, pages 272?280, Boulder, Colorado, June.
Association for Computational Linguistics.
1424
Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul
Ogilvie, David Jensen, and James Allan. 2000.
Mining of concurrent text and time series. In KDD-
2000 Workshop on Text Mining, pages 37?44.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proc. of ACL (Volume 1: Long Papers),
pages 73?82, August.
Andrew W Lo and Archie Craig MacKinlay. 1990.
When are contrarian profits due to stock mar-
ket overreaction? Review of Financial studies,
3(2):175?205.
Ronny Luss and Alexandre d?Aspremont. 2012.
Predicting abnormal returns from news using text
classification. Quantitative Finance, pp.1?14,
doi:10.1080/14697688.2012.672762.
Burton G. Malkiel. 1973. A Random Walk Down Wall
Street. W. W. Norton, New York.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Kira Radinsky and Eric Horvitz. 2013. Mining the
web to predict future events. In Proceedings of the
sixth ACM international conference on Web search
and data mining, pages 255?264. ACM.
Kira Radinsky, Sagie Davidovich, and Shaul
Markovitch. 2012. Learning causality for
news events prediction. In Proc. of WWW, pages
909?918. ACM.
David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1985. Learning internal representations
by error propagation. Technical report, DTIC Doc-
ument.
Robert P Schumaker and Hsinchun Chen. 2009.
Textual analysis of stock market prediction using
breaking financial news: The azfin text system.
ACM Transactions on Information Systems (TOIS),
27(2):12.
Ramesh Sharda and Dursun Delen. 2006. Predict-
ing box-office success of motion pictures with neu-
ral networks. Expert Systems with Applications,
30(2):243?254.
Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li,
Huayi Li, and Xiaotie Deng. 2013. Exploiting
topic based twitter sentiment for stock prediction. In
Proc. of ACL (Volume 2: Short Papers), pages 24?
29, Sofia, Bulgaria, August. Association for Compu-
tational Linguistics.
Paul C Tetlock, Maytal Saar-Tsechansky, and Sofus
Macskassy. 2008. More than words: Quantifying
language to measure firms? fundamentals. The Jour-
nal of Finance, 63(3):1437?1467.
Paul C Tetlock. 2007. Giving content to investor sen-
timent: The role of media in the stock market. The
Journal of Finance, 62(3):1139?1168.
William Yang Wang and Zhenhao Hua. 2014. A
semiparametric gaussian copula regression model
for predicting financial risks from earnings calls. In
Proc. of ACL, June.
Boyi Xie, Rebecca J. Passonneau, Leon Wu, and
Germ?an G. Creamer. 2013. Semantic frames to pre-
dict stock price movement. In Proc. of ACL (Volume
1: Long Papers), pages 873?883, August.
Alexander Yates, Michael Cafarella, Michele Banko,
Oren Etzioni, Matthew Broadhead, and Stephen
Soderland. 2007. Textrunner: open information ex-
traction on the web. In Proc. of NAACL: Demonstra-
tions, pages 25?26. Association for Computational
Linguistics.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
1425
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1870?1880,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
ZORE: A Syntax-based System for Chinese Open Relation Extraction
Likun Qiu and Yue Zhang
Singapore University of Technology and Design, Singapore
qiulikun@gmail.com, yue zhang@sutd.edu.sg
Abstract
Open Relation Extraction (ORE) over-
comes the limitations of traditional IE
techniques, which train individual extrac-
tors for every single relation type. Sys-
tems such as ReVerb, PATTY, OLLIE, and
Exemplar have attracted much attention
on English ORE. However, few studies
have been reported on ORE for languages
beyond English. This paper presents a
syntax-based Chinese (Zh) ORE system,
ZORE, for extracting relations and seman-
tic patterns from Chinese text. ZORE
identifies relation candidates from auto-
matically parsed dependency trees, and
then extracts relations with their semantic
patterns iteratively through a novel double
propagation algorithm. Empirical results
on two data sets show the effectiveness of
the proposed system.
1 Introduction
Traditional Information Extraction (IE) system-
s train extractors for pre-specified relations (Kim
and Moldovan, 1993). This approach cannot scale
to the web, where target relations are not defined
in advance. Open Relation Extraction (ORE) at-
tempts to solve this problem by shallow-parsing-
based, syntax-based or semantic-role-based pat-
tern matching without pre-defined relation types,
and has achieved great success on open-domain
corpora ranging from news to Wikipedia (Banko
et al., 2007; Wu and Weld, 2010; Nakashole et
al., 2012; Etzioni et al., 2011; Moro and Nav-
igli, 2013). Many NLP and IR applications, in-
cluding selectional preference learning, common-
sense knowledge and entailment rule mining, have
benefited from ORE (Ritter et al., 2010). Howev-
er, most existing ORE systems focus on English,
and little research has been reported on other lan-
guages. In addition, existing ORE techniques are
mainly concerned with the extraction of textual re-
lations, without trying to give semantic analysis,
which is the advantage of traditional IE.
Our goal in this paper is to present a syntax-
based Chinese (Zh) ORE system, ZORE, which
extracts relations by using syntactic dependen-
cy patterns, while associating them with explic-
it semantic information. An example is shown
is Figure 1, where the relation (cn? (Oba-
ma)o? (President) , Pred[.? (graduate)],M
? (Harvard) {? (Law School)) is extract-
ed from the given sentence ?cn? (Obama) o
? (President) .? (graduate) u (from) M?
(Harvard) {? (Law School)?, and general-
ized into the syntactic-semantic pattern {nsubj-
NR(Af) Pred[.? (graduate)] prep-u (from)
pobj-NN(Di)}. Here, Af and Di stand for human
and institution, respectively, according to a Chi-
nese taxonomy Extended Cilin (Che et al., 2010).
Rather than extracting binary relations and then
generalizing them into semantic patterns, which
most previous work does (Mausam et al., 2012;
Nakashole et al., 2012; Moro and Navigli, 2012;
Moro and Navigli, 2013), we develop a novel
method that extracts relations and patterns simul-
taneously. A double propagation algorithm is used
to make relation and pattern information reinforce
each other, so that negative effects from automatic
syntactic and semantic analysis errors can be miti-
gated. In this way, semantic pattern information is
leveraged to improve relation extraction.
We manually annotate two sets of data, from
news text and Wikipedia, respectively. Experi-
ments on both data sets show that the double prop-
agation algorithm gives better precision and recall
compared to the baseline. To our knowledge, we
are one of the first to report empirical results on
Chinese ORE. The ZORE system, together with
the two sets of test data we annotated, and the sets
of 5 million relations and 344K semantic patterns
extracted from news and Wikipedia, is freely re-
1870
Figure 1: A sample sentence analyzed by ZORE.
leased
1
.
2 Basic Definitions for Open Information
Extraction
ZORE is applied to web text to extract general re-
lations and their semantic types. Our definition
of relations follow previous work on ORE (Moro
and Navigli, 2013), but with language-specific ad-
justments. In this section, we use the sentence in
Figure 1 as an instance to describe the basic defi-
nitions for ZORE.
Definition 1 (predicate phrase) A predicate
phrase is a sequence of words that contains at least
one verb or copula, and governs one or more noun
phrases syntactically. For instance, a predicate
phrase for the sentence in Figure 1 is ?.? (grad-
uate)?. Following Fader et al. (2011), Mausam et
al. (2012) and Nakashole et al. (2012), in case of
light verb constructions, the verb and its direct ob-
ject jointly serve as predicate phrase. We do not
include prepositions into the predicate phrases.
Definition 2 (argument) An argument is a base
noun phrase governed by a predicate phrase direct-
ly or indirectly with a preposition. For instance,
?cn? (Obama) o? (President)? and ?M?
(Harvard) {? (Law School)? are two argu-
ments of the predicate phrase ?.? (graduate)?.
Definition 3 (relation) A binary relation is a
triple that consists of the predicate phrase Pred
and its two arguments x and y. Accordingly, an
n-ary relation contains n arguments. For instance,
the sentence in Figure 1 contains the binary rela-
tion (cn? (Obama) o? (President), Pred[.
? (graduate)], M? (Harvard) {? (Law
School)). In English, the two arguments of a bi-
nary relation are usually positioned on the left and
right of Pred, respectively. Hence, shallow pat-
terns are highly useful for English relation extrac-
1
https://sourceforge.net/projects/zore/
tion (Banko et al., 2007). In Chinese, however, the
two arguments can be both on the left, both on the
right or one on the left and one on the right of the
predicate, and the resulting binary relation can be
either (x, y, Pred), (Pred, x, y) and (x, Pred, y), de-
pending on the sentence. This makes the detection
of relation phrases more complicated.
Definition 4 (syntactic pattern) A syntactic pat-
tern is the syntactic abstraction of a relation. A re-
lation can be generalized into the combination of
words, POS-tags and syntactic dependency labels
(Nakashole et al., 2012). For instance, the syntac-
tic pattern of the sentence in Figure 1 is {nsubj-
NR(A) Pred[.?] prep-u pobj-NN(A)}. It con-
sists of four sub-patterns. The first, nsubj-NR(A),
denotes that the current phrase acts as the sub-
ject of the predicate phrase with the POS-tag NR
(proper nouns). Here, ?(A)? means that the phrase
is an argument of the extracted relation. The sec-
ond sub-pattern denotes that the predicate phrase
of the example is ?.? (graduate)?. Note that
the words between the predicate and arguments
(e.g., prep-u) are included into the pattern direct-
ly (Nakashole et al., 2012; Mausam et al., 2012).
Definition 5 (semantic signature) The seman-
tic signature of a relation consists of the semantic
categories of the arguments. The semantic signa-
ture of Figure 1 is (Af, Di), where Af and Di de-
notes human and institute, respectively.
Definition 6 (semantic pattern) A semantic pat-
tern is the semantic abstraction of a relation. It
is the combination of a syntactic pattern and a
semantic signature. For instance, the syntactic
pattern {nsubj-NR(A) Pred[.?] prep-u pobj-
NN(A)}, combined with the semantic signature
(Af, Di), results in the semantic pattern {nsubj-
NR(Af) Pred[.?] prep-u pobj-NN(Di)}.
1871
Figure 2: Architecture of ZORE.
Figure 3: Parsing result of the example sentence
in Figure 1, in Stanford dependencies.
3 ZORE
The architecture of ZORE is shown in Figure 2. It
consists of three components. The first is a relation
candidate extractor, which consumes input tex-
t and performs sentence segmentation, word seg-
mentation, POS tagging, syntactic parsing, base
NP extraction, light verb structure (LVC) detection
and relation candidate extraction. The output is a
set of relation candidates. The second component
tags relations and extracts semantic patterns by a
double propagation algorithm. In the third compo-
nent, extracted patterns are grouped into synsets,
and relations are filtered by confidence scores.
3.1 Extracting Relation Candidates
3.1.1 Parsing and Base NP Extraction
ZORE analyzes the syntactic structures of input
texts by applying a pipeline of NLP tools. Each
sentence is segmented into a list of words by using
the Stanford segmenter (Chang et al., 2008), and
parsed by using ZPar (Zhang and Clark, 2011),
with POS tags and constituent structures by the
CTB standard (Xue et al., 2005). The result-
ing constituent trees are transformed into projec-
tive trees with Stanford dependencies by using the
Stanford parser (Chang et al., 2009). Figure 4
shows the parse tree of the sentence in Figure 1.
Next, base noun phrases (NPs) are extracted
from the dependency tree. Here a base NP is a
maximum phrase whose words can only have POS
from the first row of Table 1. The head word of a
base NP can be either a noun, a pronoun, a num-
ber or a measure word (the second row of Table
1). The dependency labels within a base NP can
only be from the third row of Table 1. Obviously,
a base NP does not contain other base NPs, and is
also not contained by any other base NP.
3.1.2 Detecting Light Verb Constructions
In linguistics, a light verb is a verb that has little
semantic content of its own, and typically form-
s a predicate with a noun (Butt, 2003). Exam-
ple predicates by light verb constructions (LVC)
include ?is a capital of? and ?claim responsibil-
ity for?, where ?is? and ?claim? are light verbs.
Improper handling of LVC can cause a significan-
t problem by uninformative extractions (Etzioni et
al., 2011). For example, if ?is? and ?claim? are ex-
tracted as predicates, the resulting relations (such
as (Hamas, claimed, responsibility) from the sen-
tence ?Hamas claimed responsibility for the Gaza
attack?) might not bare useful information. Re-
Verb (Etzioni et al., 2011) handles this problem by
hard syntactic constraints, taking the noun phrase
(e.g., responsibility) between a verb phrase (e.g.,
?claim?) and a preposition (e.g., ?for?) as a part of
the predicate phrase rather than an argument, lead-
ing to the relation (Hamas, claimed responsibility
for, the Gaza attack).
In Chinese, LVCs are highly frequent and
should be handled properly in order to ensure that
the extracted relations are informative. Howev-
er, the syntactic constraints in ReVerb can not be
transferred to Chinese directly, because the word
orders of English and Chinese are quite different.
1872
Labels
Base NP modifier NN (common noun), M (measure word), CD (cardinal number), OD (ordinal number), PN (pronoun), NR
(proper noun), NT (temporal noun), JJ (other noun-modifier), or PU (punctuation)
Base NP head NN (common noun), M (measure word), CD (cardinal number), OD (ordinal number), PN (pronoun), NR
(proper noun), NT (temporal noun)
Labels in base NPs nn (noun compound modifier), conj (conjunct), nummod (number modifier), cc (coordinating conjunction),
clf (classifier modifier), det (determiner), ordmod (ordinal number modifier), punct (punctuation), dep (other
dependencies), or amod (adjectival modifier)
Labels from base
NPs to predicate
phrase
nsubj (nominal subject), conj (conjunct), dobj (direct object), advmod (adverbial modifier), prep (preposi-
tional modifier), pobj (prepositional object), lobj (localizer object), range (dative object that is a quantifier
phrase), tmod (temporal modifier), plmod (localizer modifier of a preposition), attr (attributive), loc (local-
izer), top (topic), xsubj (controlling subject), ba (?ba? construction), nsubjpass (nominal passive subject)
Table 1: Constraints on POS-tags and dependency labels. Labels in the top three rows are used for base
NP extraction, while labels in the last row for traversing from a base NP to the predicate phrase.
In Chinese, prepositions acting as the modifier of
a verb can be on both the left and right of the ver-
b. For instance, the sentence ?cn? (Obama)o
? (President) u (from) M? (Harvard) {?
(Law School).? (graduate)? is a paraphrase of
the sentence in Figure 1, with the preposition u
(from) on the left of the predicate phrase.
Chinese LVCs can be classified into two types,
which we refer to as dummy-LVCs and common
LVCs, respectively. For the first type, the predi-
cate is a dummy verb such as ??1 (do)? and ??
? (give)?, which has a noun phrase as its object.
Since dummy verbs in Chinese are a closed set, we
detect this type of LVCs (such as ??1 (do)?!
(talk)?) by finding the dummy verb from a lexi-
con. For the second type of LVCs, the predicate is
a common verb, which has a nominalized structure
or a common noun as its object. For instance, ??
m (launch) N (investigation)? belongs to this
type of construction.
Common LVCs are more difficult to detect than
dummy-LVCs. We detect common LVCs by the
context. Besides the NPs in the LVC itself, a com-
mon LVC typically governs two NPs, with the lat-
ter being connected to the predicate phrase by an
LVC-related preposition such as ?? (for), ?u
(for), ? (for), ? (to), ? (with), ? (with), ?
(with) ?. Based on the observation, a basic idea of
identifying common LVCs is to find verb-object
structures that frequently co-occur with a LVC-
related preposition in a large-scale corpus parsed
automatically. For a given verb-object v, let f
v
and
f
p
denote the frequency of v and the frequency of v
co-occurring with an LVC-related preposition, re-
spectively. We define the statistical strength of v
to be an LVC as the ratio f
p
/f
v
. If the statistical
strength of v exceeds a threshold t
lvc
, we identify v
as a LVC. Table 2 illustrates some high-frequency
LVCs extracted by the method automatically.
3.1.3 Extracting Relation Candidates
ZORE tries to extract relation candidates from
sentences that contain two or more base NPs. Giv-
en two base NPs, we traverse the dependency tree
to obtain the shortest path that connects them. The
path can contain only dependency labels in the
fourth row of Table 1, and should contain at least
one of the labels from ?nsubj? and ?dobj? to en-
sure that a predicate phrase is included in the path.
If such a path is acquired, other base NPs governed
by the same predicate phrase are included into the
target relation, resulting in a n-ary relation candi-
dates with each base NP being an argument. Ac-
cording to the predicate phrase, relation candidates
can be classified into the following classes.
Common and dummy LVC relations. In this
type of relations, the predicate phrase of the path
is an LVC (e.g., a light verb and a nominal ob-
ject). The two base NPs can be the subject or
prepositional object of the light verb. For instance,
in the sentence ??&Z (Houdini) ? (to) ?
(my)?? (career)k (have)?? (big)K? (in-
fluence)?, ?k (have)?and ?K? (influence)? are
combined into a common LVC and taken as the
predicate phrase, resulting the relation (?&Z
(Houdini), Pred[k (have)K? (influence)],?
(my)?? (career)). In the corresponding English
sentence, the predicate phrase ?be a big influence
in? is also an LVC structure.
Verb relations. In this type of relations, a verb
acts as the predicate phrase. For instance, the rela-
tion (cn? (Obama) o? (President), Pred[.
? (graduate)], M? (Harvard) {? (Law
School)) extracted from the sentence in Figure 1
is a typical verb relation.
Relative-clause relations. In an relative-clause
1873
Verb Noun
?1 (do) (*) u1 (distribution),?? (analysis),?8 (collection),?U (modification),?? (visit),?v (punishment)
k (have) (*) K?(effect), z (contribution),, (interest),?? (help),@? (understanding),?" (expectation)
) (generate) (**) K? (effect),, (interest),~?(doubt),?? (shock),?a (good feeling),?? (fear)
E? (cause) (**) K? (effect),??(destruction),?? (harm),% (threat),?? (pressure),Z6 (distraction)
L? (express) (**) ?? (satisfaction),?H (welcome),?? (respect),?b (worry),H (mourning),a (gratitude)
?m (launch) (**) N (investigation),?? (attack),?? (offensive),1? (criticism),1 (negotiation),?z (lawsuit)
Table 2: Instances of dummy-LVCs (*) and common LVCs (**). A verb in the left column is combined
with a noun in the right column to form an LVC, which serves as the predicate phrase.
relation, the head word is a noun, modified by
an relative clause, but acting as an argument of
the predicate of the relative clause semantically.
The sentence ?.? (graduate) u (from) M?
(Harvard) {? (Law School)  (de, an aux-
iliary word) cn? (Obama) o? (president)?
is a paraphrase of the sentence in Figure 1, with
the same predicate phrase and arguments. How-
ever, the relation extracted from this phrase is an
relative-clause relation (Pred[.? (graduate)],
M? (Harvard) {? (Law School), cn?
(Obama) o? (president)), which belongs to the
same pattern synset as the relation of Figure 1.
3.2 Semantic Tagging by Double Propagation
The basic idea of our approach is to identify rela-
tions and patterns iteratively through semantical-
ly tagging the head words of arguments in rela-
tion candidates. Given a set of relation candidates
and a semantic taxonomy, the propagation consist-
s of three steps. In Step 1, monosemic arguments
in candidate relations are tagged with a seman-
tic category, such as Af and Di, to obtain seman-
tic patterns. In Step 2 and Step 3, untagged am-
biguous and unknown words are tagged by perfect
matching and partial matching, respectively. In the
end of each step, semantic patterns are generalized
from extracted and tagged relations, and then used
to help relation tagging in the next step. Because
of the two-way information exchange, we call this
method double propagation. The method can also
be treated as similar to bootstrapping (Yangarber
et al., 2000; Qiu et al., 2009).
3.2.1 Step 1: Tagging Monosemic Arguments
Each argument in a relation candidate is a base N-
P. Since base NPs are endocentric, we can take the
semantic category of the head word of a base NP
as the semantic category of the base NP. In a tax-
onomy, each word is associated with one or more
semantic categories. In this step, however, only
monosemic words are tagged, while both ambigu-
ous words and unknown words are left untagged.
Most named entities are not included in the
taxonomy. However, after POS-tagging, most of
them are detected as NR (proper noun). As a re-
sult, they are taken as ambiguous words that can
be person names, organization names or location
names. The named entities that are not included in
the taxonomy are tagged in Steps 2 and 3.
After this step, all the arguments in some re-
lation candidates have been tagged with semantic
categories. We refer to these relation candidates as
tagged relation candidates, and the remaining re-
lation candidates as untagged relation candidates.
Tagged relation candidate are generalized into se-
mantic patterns, consisting of syntactic patterns
and semantic signatures, as illustrated in Figure 1
and Section 2. We call the set of resulting seman-
tic patterns Set
SemPat
.
3.2.2 Step 2: Tagging by Perfect Pattern
Matching
In this step, the arguments in the untagged relation
candidates are tagged by semantic pattern match-
ing. Given an untagged relation candidate r, we
acquire a set of possible semantic categories for
each argument with an ambiguous head word. For
the arguments with unknown head words, we ac-
quire a set of possible semantic categories accord-
ing to their characters. Qiu et al. (2011) demon-
strate that 98% Chinese words have at least one
synonym, which shares at least one character. For
Chinese nouns, the set of synonyms usually shares
the last one or two characters. According to this,
our strategy for acquiring possible semantic cate-
gories for an unknown word is as follows.
Given an unknown word w
u
, if we find a known
word w
k
that shares the last two character with w
u
,
the semantic categories of w
k
will be used as the
possible semantic categories of w
u
. Otherwise, if
we find a known word w
k
that share the last one
character with w
u
, the semantic categories of w
k
will be used as the possible categories of w
u
.
1874
We then acquire possible semantic signatures of
untagged relation candidates, of which all the ar-
guments are tagged with possible semantic cate-
gories. As in Step 1, we generalize relation r in-
to a syntactic pattern pat
syn
, and then combine
pat
syn
with each possible semantic signature of
r to generate possible semantic patterns. In case
one or more possible semantic patterns of r ex-
ist in Set
SemPat
, if the highest frequency of these
patterns is above a threshold t
sem
, the correspond-
ing pattern will be taken as the semantic pattern
of r, from which we infer the semantic signature
for r and then the semantic category for the head
word of each argument of r. After this step, the
frequency of each semantic pattern in Set
SemPat
is updated according to the newly tagged relation
candidates.
3.2.3 Step 3: Tagging by Partial Pattern
Matching
In this step, we tag the ambiguous and unknown
words by partial matching rather than perfec-
t matching of the whole semantic pattern. This
can be treated as a back-off of the last step.
We first split n-ary semantic patterns in
Set
SemPat
into binary semantic patterns, and cal-
culate their frequencies. Second, we split each
untagged relation candidate r into several binary
sub-relations and then search for the correspond-
ing semantic patterns as in Step 2 ? for each bi-
nary sub-relation, we obtain a binary semantic sig-
nature with the highest frequency. By combining
the binary semantic signatures, we obtain one n-
ary semantic signature for r, based on which all
the unknown and ambiguous words can be tagged
with a semantic categories. If all the arguments
of a relation candidate r are tagged, r is treated as
tagged. Finally, according to the newly tagged re-
lations, statistics in Set
SemPat
are updated.
3.3 Grouping Patterns into Synsets
In this step, we group semantic patterns from
Set
SemPat
into pattern synsets, based on a
single-pass clustering process (Papka and Allan,
1998). Given two semantic patterns SemPat
i
and
SemPat
j
, we refer to their corresponding syntac-
tic pattern, semantic signature and predicate phras-
es as SynPat
i
and SynPat
j
, SemSig
i
and SemSig
j
,
Pred
i
and Pred
j
, respectively. Not taking the pred-
icate phrase into account, SynPat
i
and SynPat
j
are
identical, and we call them loosely identical (?).
The algorithm in Figure 4 is used to group
Figure 4: Algorithm for pattern synset grouping.
Type Feature Weight
Base r covers all words in c 0.96
Base There are commas within r -0.47
Base LENGTH(r)<10 words 0.35
Base 10 words[LENGTH(r)<20 words 0.11
Base 20 words[LENGTH(r) -1.06
Base COUNT(arguments)=2 0.14
Base COUNT(arguments)=3 0.33
Base COUNT(arguments)=4 -0.60
Base COUNT(arguments)> 4 -0.46
SemPat Being tagged in Step 3 0.87
SemPat Being tagged before Step 3 0.75
SemPat 50[SIZE(SemPat) and untagged -0.05
SemPat 50[SIZE(SemPat) and tagged 0.65
SemPat 10[SIZE(SemPat)<50 and untagged -0.16
SemPat 10[SIZE(SemPat)<50 and tagged 0.39
SemPat 5[SIZE(SemPat)<10 and untagged -0.22
SemPat 5[SIZE(SemPat)<10 and tagged 0.36
SemPat SIZE(SemPat) <5 and untagged -0.92
SemPat SIZE(SemPat) <5 and tagged -0.64
Table 3: Features of the logistic regression classi-
fier with weights trained on Wiki-500 dataset.
patterns, where ARGCOUNT(SynPat
j
) denotes the
number of arguments in SemPat
i
, SEMCAT(arg
1
)
indicates the semantic category of the first ar-
gument, and ISSYNONYM (Pred
i
, Pred
j
) return-
s whether two predicates are synonyms. In
similarity-based single-pass clustering, the topic
excursion problem is common (Papka and Allan,
1998). But since our similarity measure is sym-
metric, we do not suffer from this problem.
3.4 Computing the Confidence for Relations
Without filtering, the extraction algorithm in the
previous sections may yield false relations. Fol-
lowing previous ORE systems, we make a balance
between recall and precision by using a confidence
threshold (Fader et al., 2011). A logistic regres-
sion classifier is used to give a confidence score
to each relation, with features shown in Table 3.
In the table, c, r, arguments and SemPat denote
1875
Dataset Source #Sen #Rel
Wiki-500 Chinese Wikipedia 500 561
Sina-500 Sina News 500 707
Table 4: Annotated relation datasets.
clause, relation, arguments in a relation, and se-
mantic pattern, respectively. LENGTH(r), COUN-
T(arguments) and SIZE(SemPat) indicate the num-
ber of words in r, the number of arguments in
r, and the number of relations that belong to the
same semantic pattern SemPat as r. Because se-
mantic patterns from the double propagation al-
gorithm are used as features in the classifier, they
participate in relation extraction also. Their effect
on relation extraction can directly demonstrate the
effectiveness of double propagation.
4 Experiments
4.1 Experimental Setup
We run ZORE on two difference corpora: the Chi-
nese edition of Wikipedia (Wiki), which contain-
s 4.3 million sentences (as of March 29, 2014),
and a corpus from the Sina News archive (Sina
News), which includes 6.1 million sentences from
January 2013 to May 2013. The sentences that do
not end with punctuations are filtered. The Chi-
nese taxonomy Extended Cilin
2
(Cilin) (Che et al.,
2010) is used to give semantic categories for each
word. Cilin contains 77,492 Chinese words, or-
ganized into a five-level hierarchy. There are 12
categories in the top level, 94 in the second and
1492 in the third. In this paper, the second level
is used for semantic categories. We create two test
sets, containing 500 sentences from Wiki and 500
sentences from Sina News, respectively (see Table
4), annotated by two independent annotators us-
ing the annotation strategy of Fader et al. (2011).
The thresholds t
lvc
and t
sem
for pattern matching
are set as 0.4 and 5, tuned on 100 sentences from
Wiki-500 dataset, respectively.
4.2 Evaluation of Relation Extraction
First, we compare ZORE with a baseline system
to illustrate the effectiveness of the double prop-
agation algorithm. The baseline system does not
have the double propagation tagging component
in Figure 2, using the logistic regression classifier
in Section 3.4.1 with the 9 base features to filter
extracted relation candidates. It is similar to the
architecture of ReVerb (Fader et al., 2011). We
2
http://ir.hit.edu.cn/demo/ltp/Sharing Plan.htm
Figure 5: Performance on Wiki.
Figure 6: Performance on Sina News.
measure the precision and recall of the extracted
relations. An extracted relation is considered cor-
rect only when the predicate phrase and all the ar-
guments match the the gold set. On each data set,
we perform 5-fold cross-validation test and take
the average as the final precision and recall.
Figures 5 and 6 show the comparison of the two
systems on Wiki and Sina News, respectively. On
Wiki, ZORE has higher precision than the baseline
at all levels of recall. When the recall is 0.3, the
precision of ZORE is 0.77, 0.11 higher than the
baseline. The result on Sina News is similar. The
second column of Table 3 shows the weights of all
features trained on the Wiki data set, which indi-
cates that the semantic pattern features can give a
positive effect on relation filtering.
Second, we compare the intermediate results at
Steps 1, 2, and 3 in Section 3.2, respectively. The
precision, recall and F1 of the three steps with d-
ifferent numbers of Wiki sentences (from 10K to
5M sentences) are shown in Table 5. This figure
shows that Step 2 achieves higher precision than
Step 1 at all levels of recall, indicating that the
word sense tagging method in step 2 is useful for
1876
Sentences Step 1 Step 2 Step 3
P R F1 P R F1 P R F1
10K 0.947 0.032 0.062 0.960 0.043 0.082 0.933 0.075 0.139
50K 0.894 0.075 0.138 0.922 0.105 0.189 0.907 0.139 0.241
100K 0.897 0.093 0.169 0.924 0.130 0.228 0.909 0.160 0.272
200K 0.901 0.114 0.202 0.926 0.157 0.268 0.892 0.191 0.315
500K 0.891 0.146 0.251 0.909 0.196 0.322 0.860 0.230 0.363
1M 0.860 0.164 0.275 0.885 0.219 0.351 0.842 0.248 0.383
2M 0.797 0.182 0.296 0.819 0.250 0.383 0.788 0.278 0.411
3M 0.784 0.187 0.302 0.802 0.253 0.385 0.778 0.282 0.414
4M 0.739 0.178 0.287 0.801 0.258 0.390 0.778 0.287 0.419
5M 0.779 0.189 0.304 0.798 0.260 0.392 0.768 0.289 0.420
Table 5: Accuracies on different numbers Wiki sentences.
a significant boost of recall, together with a lit-
tle improvement in precision. In particular, Step
2 can extract about 20% relations with relatively
high precision (about 90%). The result of Step 3
is better to that of Step 2 in terms of F1-measure,
with the highest F1-measure achieved by this step.
4.3 Evaluation of Patterns
ZORE acquires 122K and 222K patterns from Wi-
ki and Sina News, clustered into 59K and 118K
pattern synsets, respectively. The frequency distri-
bution of the Wiki patterns is shown in Figure 7,
which conforms to Zipf?s law.
To assess the accuracy of pattern extraction, we
rank the extracted patterns by the size, and eval-
uated the precision of the top 100 and a random
set of 100 pattern synsets. Two annotators were
shown a pattern synset with its semantic signature
and a few example relations, and then asked to
judge whether it indicates a valid semantic rela-
tion or not. The results are shown in Table 6. The
averaged precision is 92% for the top 100 set, and
85% for the random 100 set.
The patterns in a pattern synset can be taken
as paraphrases (Barzilay and Lee, 2003). We ob-
serve that two synonymous patterns might differ
in three aspects. First, two patterns can differ
by the predicates, which are synonyms. For in-
stance, the verbs ???, , ?, ??, ?, ??
are synonyms, meaning ?to hold the appointment
of?. Second, two patterns in the same synset can
belong to different syntactic patterns, and there-
fore are paraphrases in the syntactic level. For in-
stance, the semantic patterns of the two sentences
?.? (graduate) u (from) M? (Harvard) {?
 (Law School) (de, an auxiliary word) cn
? (Obama)o? (president)? and ?cn? (Oba-
ma)o? (president)l(from)M? (Harvard){
? (Law School).? (graduate)? are both syn-
onymous to that of the sentence in Figure 1; al-
l the three patterns are found in the same synset
obtained by ZORE. Third, two patterns can dif-
fer only by the POS-tag. For instance, ?cn?
(Obama) l(from) M? (Harvard) {? (Law
School).? (graduate)? and ?@? (That) ??
(attorney) l(from) M? (Harvard) {? (Law
School) .? (graduate)? are synonyms with d-
ifferent POS-tags for the first argument (i.e. N-
R and NN). According to the grouping algorithm
in Section 3.3, all the three types of paraphrases
are grouped in a pattern synset, which makes some
synsets very large. The largest synset contains 110
patterns, while the top 100 synsets contain more
than 20 patterns.
4.4 Error analysis
We analyze the incorrect extractions (precision
loss) and missed correct relations (recall loss) re-
turned by Step 2, running on 500K sentences. Ta-
ble 7 summarizes the types of correct relations that
are missed by ZORE. 40% missed relations are
due to the minimum frequency constraint on se-
mantic patterns, which is used for a balance be-
tween precision and recall. Another main source
of failure is the incorrect identification of the pred-
icate phrase due to parsing errors, which account
for 37% of the total errors. Other sources of fail-
ures include redundant arguments and segmenta-
tion errors. Most redundant arguments are related
to prepositions such as ?U? (according to)? and
??? (on the basis of)?. For instance, in the sen-
tence ?U? (according to)?? (the)*: (point
of view) ? (,) ? (fundamental) ?K (prob-
lem) ? (is)?, an incorrect binary relation (??
(the)*: (point of view),? (fundamental)?
K (problem), Pred[? (is)]) is extracted, because
the prepositional object ??? (the) *: (point
of view)? is tagged as an argument of the predi-
cate phrase ?? (is)?.
Table 8 summarizes the major types of incor-
1877
Corpus Patterns Synsets Top100 Random100
Wiki 122,723 59,298 0.93 0.87
Sina 222,773 118,923 0.91 0.83
Table 6: Precision of pattern synsets.
40% Relations filtered by semantic pattern constraint
37% Could not identify correct predicates because of
preprocessing errors
12% Too many arguments because of parsing errors
11% Segmentation and POS tagging errors
Table 7: Relations missed by ZORE.
rect relations, 56% of which were caused by pars-
ing errors, and 34% of which were due to word
segmentation and POS tagging errors. Although
many errors have been filtered by ZORE, the
biggest source of errors is still syntactic analysis,
which is very important for high quality of ORE.
5 Related Work
English has been the major language on which
ORE research has been conducted. Previous
work on English ORE has evolved from shallow-
syntactic (Banko et al., 2007; Fader et al., 2011;
Merhav et al., 2012) to full-syntactic (Nakashole
et al., 2012; Mausam et al., 2012; Moro and Nav-
igli, 2013; Xu et al., 2013) and semantic (Johans-
son and Nugues, 2008) systems.
It has been shown that a full-syntactic system
based on dependency grammar can give signif-
icantly better results than shallow syntactic sys-
tems based on surface POS-patterns, yet enjoy
higher efficiency compared with semantic system-
s (Mesquita et al., 2013). Our investigation on
Chinese ORE takes root in full dependency syntax
and is hence able to identify patterns that involve
long-range dependencies. Considering the charac-
teristics of the Chinese language, such as the lack
of morphology and function words, and the high
segmentation and word sense ambiguities, we in-
corporate semantic ontology information into the
design of the system to improve the output quality
without sacrificing efficiency.
The state-of-the-art systems most closely relat-
ed to our approach are PATTY (Nakashole et al.,
2012) and the system of Moro and Navigli (2013).
Both, however, extract relations first, and then de-
fines patterns based on extracted relations. This
paper differs in that patterns and relations are ex-
tracted in a simultaneous process and so they can
improve each other. Previous studies show that
pattern generalization benefit from relation extrac-
56% Parsing errors
17% Segmentation errors
17% POS tagging errors
6% Redundant arguments
6% Other, including base NP extraction errors
Table 8: Incorrect extractions by ZORE.
Figure 7: The frequency distribution of patterns
extracted from Wiki. Size and Count denote the
number of relations that belong to a semantic pat-
tern and the logarithmic number of semantic pat-
terns that have the same size, respectively.
tion (Nakashole et al., 2012; Moro and Navigli,
2013), and relation extraction can benefit from
pattern generalization (Mausam et al., 2012). By
using double propagation, not only can we make
relation and pattern extraction benefit from each
other, but we can also tag relations and patterns
with semantic categories in a joint process.
There has been a line of research on Chinese re-
lation extraction, where both feature-based (Zhou
et al., 2005; Li et al., 2008) and kernel-based
(Zhang et al., 2006; Che et al., 2005) methods have
been applied. In addition, semantic ontologies
such as Extended Cilin have been shown useful
for Chinese relation extraction (Liu et al., 2013).
However, these studies have focused on tradition-
al IE, with pre-defined relations. In contrast, we
investigate ORE for Chinese, finding that seman-
tic ontologies useful for this task also. Tseng et al.
(2014) is the only previous research focusing on
Chinese ORE. Their system can be considered as
a pipeline of word segmentation, POS-tagging and
parsing, while our work gives semantic interpreta-
tion and explicitly deals with statistical errors in
parsing by a novel double propagation algorithm
between patterns and relations.
1878
6 Conclusion and Future Work
We presented a Chinese ORE system that inte-
grates relation extraction with semantic pattern
generalization by double propagation. Experimen-
tal results on two datasets demonstrated the ef-
fectiveness of the proposed algorithm. We make
the ZORE system, together with the large scale
relations and pattern synsets extracted by ZORE,
freely available at (https://sourceforge.
net/projects/zore/). Another version of
ZORE (ZORE-PMT), which is based on the de-
pendency tagset from PMT1.0 (Qiu et al., 2014),
is also provided.
Our error analysis demonstrates that the quali-
ty of syntactic parsing is crucial to the accuracy of
syntax-based Chinese ORE. Improvements to syn-
tactic analysis is likely to lead to improved ORE.
In addition, the idea of double propagation can be
generalized into information propagation between
relation extraction and syntactic analysis. We plan
to investigate the use of ORE in improving syntac-
tic analysis in future work.
Acknowledgments
We gratefully acknowledge the invaluable assis-
tance of Ji Ma, Wanxiang Che and Yijia Liu. We
also thank the anonymous reviewers for their con-
structive comments, and gratefully acknowledge
the support of the Singapore Ministry of Educa-
tion (MOE) AcRF Tier 2 grant T2MOE201301,
the start-up grant SRG ISTD 2012 038 from
Singapore University of Technology and Design,
the National Natural Science Foundation of Chi-
na (No. 61103089), National High Technolo-
gy Research and Development Program of Chi-
na (863 Program) (No. 2012AA011101), Ma-
jor National Social Science Fund of China (No.
12&ZD227), Scientific Research Foundation of
Shandong Province Outstanding Young Scientist
Award (No. BS2013DX020) and Humanities and
Social Science Projects of Ludong University (No.
WY2013003).
References
Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction for the web. In IJCAI,
volume 7, pages 2670?2676.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach using
multiple-sequence alignment. In Proceedings of the
2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology-Volume 1, pages 16?
23. Association for Computational Linguistics.
Miriam Butt. 2003. The light verb jungle. In Work-
shop on Multi-Verb Constructions, pages 1?28.
Pi-Chuan Chang, Michel Galley, and Christopher D
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 224?232. Association for
Computational Linguistics.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D Manning. 2009. Discriminative re-
ordering with Chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation, pages
51?59. Association for Computational Linguistics.
WX Che, Jianmin Jiang, Zhong Su, Yue Pan, and T-
ing Liu. 2005. Improved-edit-distance kernel for
Chinese relation extraction. In IJCNLP, pages 132?
137.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:
A Chinese language technology platform. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics: Demonstrations, pages
13?16. Association for Computational Linguistics.
Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, and Mausam Mausam. 2011.
Open information extraction: The second genera-
tion. In Proceedings of the Twenty-Second inter-
national joint conference on Artificial Intelligence-
Volume Volume One, pages 3?10. AAAI Press.
Anthony Fader, Stephen Soderland, and Oren Etzion-
i. 2011. Identifying relations for open informa-
tion extraction. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1535?1545. Association for Compu-
tational Linguistics.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of prop-
bank. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
69?78. Association for Computational Linguistics.
Jun-Tae Kim and Dan I Moldovan. 1993. Acquisition
of semantic patterns for information extraction from
corpora. In Artificial Intelligence for Application-
s, 1993. Proceedings., Ninth Conference on, pages
171?176. IEEE.
Wenjie Li, Peng Zhang, Furu Wei, Yuexian Hou, and
Qin Lu. 2008. A novel feature-based approach
to Chinese entity relation extraction. In Proceed-
ings of the 46th Annual Meeting of the Association
for Computational Linguistics on Human Language
Technologies: Short Papers, pages 89?92. Associa-
tion for Computational Linguistics.
1879
Dandan Liu, Zhiwei Zhao, Yanan Hu, and Longhua
Qian. 2013. Incorporating lexical semantic simi-
larity to tree kernel-based Chinese relation extrac-
tion. In Chinese Lexical Semantics, pages 11?21.
Springer.
Michael Schmitz Mausam, Robert Bart, Stephen
Soderland, and Oren Etzioni. 2012. Open language
learning for information extraction. pages 523?534.
Yuval Merhav, Filipe Mesquita, Denilson Barbosa,
Wai Gen Yee, and Ophir Frieder. 2012. Extracting
information networks from the blogosphere. ACM
Transactions on the Web (TWEB), 6(3):11.
Filipe Mesquita, Jordan Schmidek, and Denilson Bar-
bosa. 2013. Effectiveness and efficiency of open
relation extraction. New York Times, 500:150.
Andrea Moro and Roberto Navigli. 2012. Wisenet:
Building a wikipedia-based semantic network with
ontologized relations. In Proceedings of the 21st
ACM international conference on Information and
knowledge management, pages 1672?1676. ACM.
Andrea Moro and Roberto Navigli. 2013. Integrating
syntactic and semantic analysis into the open infor-
mation extraction paradigm. In Proceedings of the
Twenty-Third international joint conference on Arti-
ficial Intelligence, pages 2148?2154. AAAI Press.
Ndapandula Nakashole, Gerhard Weikum, and Fabi-
an Suchanek. 2012. PATTY: a taxonomy of rela-
tional patterns with semantic types. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1135?1145. As-
sociation for Computational Linguistics.
Ron Papka and James Allan. 1998. On-line new event
detection using single pass clustering. University of
Massachusetts, Amherst.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In IJCAI, volume 9, pages
1199?1204.
Likun Qiu, Yunfang Wu, and Yanqiu Shao. 2011.
Combining contextual and structural information for
supersense tagging of Chinese unknown words. In
Computational Linguistics and Intelligent Text Pro-
cessing, pages 15?28. Springer.
Likun Qiu, Yue Zhang, Peng Jin, and Houfeng Wang.
2014. Multi-view Chinese treebanking. In Proceed-
ings of COLING 2014, pages 257?268.
Alan Ritter, Oren Etzioni, et al. 2010. A latent dirich-
let allocation method for selectional preferences. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 424?
434. Association for Computational Linguistics.
Yuen-Hsien Tseng, Lung-Hao Lee, Shu-Yen Lin, Bo-
Shun Liao, Mei-Jun Liu, Hsin-Hsi Chen, Oren Et-
zioni, and Anthony Fader. 2014. Chinese open rela-
tion extraction for knowledge acquisition. In EACL
2014, pages 12?16.
Fei Wu and Daniel S Weld. 2010. Open information
extraction using wikipedia. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 118?127. Association for
Computational Linguistics.
Ying Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel,
and Denilson Barbosa. 2013. Open information
extraction with tree kernels. In Proceedings of
NAACL-HLT, pages 868?877.
Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn Chinese treebank: Phrase
structure annotation of a large corpus. Natural lan-
guage engineering, 11(2):207?238.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisition
of domain knowledge for information extraction. In
Proceedings of the 18th conference on Computation-
al linguistics-Volume 2, pages 940?946. Association
for Computational Linguistics.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 825?832. Association for Computa-
tional Linguistics.
GuoDong Zhou, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd annual meeting
on association for computational linguistics, pages
427?434. Association for Computational Linguistic-
s.
1880
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 736?746,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Syntax-Based Word Ordering Incorporating a Large-Scale Language
Model
Yue Zhang
University of Cambridge
Computer Laboratory
yz360@cam.ac.uk
Graeme Blackwood
University of Cambridge
Engineering Department
gwb24@eng.cam.ac.uk
Stephen Clark
University of Cambridge
Computer Laboratory
sc609@cam.ac.uk
Abstract
A fundamental problem in text generation
is word ordering. Word ordering is a com-
putationally difficult problem, which can
be constrained to some extent for particu-
lar applications, for example by using syn-
chronous grammars for statistical machine
translation. There have been some recent
attempts at the unconstrained problem of
generating a sentence from a multi-set of
input words (Wan et al 2009; Zhang and
Clark, 2011). By using CCG and learn-
ing guided search, Zhang and Clark re-
ported the highest scores on this task. One
limitation of their system is the absence
of an N-gram language model, which has
been used by text generation systems to
improve fluency. We take the Zhang and
Clark system as the baseline, and incor-
porate an N-gram model by applying on-
line large-margin training. Our system sig-
nificantly improved on the baseline by 3.7
BLEU points.
1 Introduction
One fundamental problem in text generation is
word ordering, which can be abstractly formu-
lated as finding a grammatical order for a multi-
set of words. The word ordering problem can also
include word choice, where only a subset of the
input words are used to produce the output.
Word ordering is a difficult problem. Finding
the best permutation for a set of words accord-
ing to a bigram language model, for example, is
NP-hard, which can be proved by linear reduction
from the traveling salesman problem. In prac-
tice, exploring the whole search space of permu-
tations is often prevented by adding constraints.
In phrase-based machine translation (Koehn et al
2003; Koehn et al 2007), a distortion limit is
used to constrain the position of output phrases.
In syntax-based machine translation systems such
as Wu (1997) and Chiang (2007), synchronous
grammars limit the search space so that poly-
nomial time inference is feasible. In fluency
improvement (Blackwood et al 2010), parts of
translation hypotheses identified as having high
local confidence are held fixed, so that word or-
dering elsewhere is strictly local.
Some recent work attempts to address the fun-
damental word ordering task directly, using syn-
tactic models and heuristic search. Wan et al
(2009) uses a dependency grammar to solve word
ordering, and Zhang and Clark (2011) uses CCG
(Steedman, 2000) for word ordering and word
choice. The use of syntax models makes their
search problems harder than word permutation us-
ing an N -gram language model only. Both meth-
ods apply heuristic search. Zhang and Clark de-
veloped a bottom-up best-first algorithm to build
output syntax trees from input words, where
search is guided by learning for both efficiency
and accuracy. The framework is flexible in allow-
ing a large range of constraints to be added for
particular tasks.
We extend the work of Zhang and Clark (2011)
(Z&C) in two ways. First, we apply online large-
margin training to guide search. Compared to the
perceptron algorithm on ?constituent level fea-
tures? by Z&C, our training algorithm is theo-
retically more elegant (see Section 3) and con-
verges more smoothly empirically (see Section 5).
Using online large-margin training not only im-
proves the output quality, but also allows the in-
corporation of an N -gram language-model into
736
the system. N -gram models have been used as a
standard component in statistical machine trans-
lation, but have not been applied to the syntac-
tic model of Z&C. Intuitively, an N -gram model
can improve local fluency when added to a syntax
model. Our experiments show that a four-gram
model trained using the English GigaWord cor-
pus gave improvements when added to the syntax-
based baseline system.
The contributions of this paper are as follows.
First, we improve on the performance of the Z&C
system for the challenging task of the general
word ordering problem. Second, we develop a
novel method for incorporating a large-scale lan-
guage model into a syntax-based generation sys-
tem. Finally, we analyse large-margin training in
the context of learning-guided best-first search,
offering a novel solution to this computationally
hard problem.
2 The statistical model and decoding
algorithm
We take Z&C as our baseline system. Given
a multi-set of input words, the baseline system
builds a CCG derivation by choosing and ordering
words from the input set. The scoring model is
trained using CCGBank (Hockenmaier and Steed-
man, 2007), and best-first decoding is applied. We
apply the same decoding framework in this paper,
but apply an improved training process, and incor-
porate an N -gram language model into the syntax
model. In this section, we describe and discuss
the baseline statistical model and decoding frame-
work, motivating our extensions.
2.1 Combinatory Categorial Grammar
CCG, and parsing with CCG, has been described
elsewhere (Clark and Curran, 2007; Hockenmaier
and Steedman, 2002); here we provide only a
short description.
CCG (Steedman, 2000) is a lexicalized gram-
mar formalism, which associates each word in a
sentence with a lexical category. There is a small
number of basic lexical categories, such as noun
(N), noun phrase (NP), and prepositional phrase
(PP). Complex lexical categories are formed re-
cursively from basic categories and slashes, which
indicate the directions of arguments. The CCG
grammar used by our system is read off the deriva-
tions in CCGbank, following Hockenmaier and
Steedman (2002), meaning that the CCG combina-
tory rules are encoded as rule instances, together
with a number of additional rules which deal with
punctuation and type-changing. Given a sentence,
its CCG derivation can be produced by first assign-
ing a lexical category to each word, and then re-
cursively applying CCG rules bottom-up.
2.2 The decoding algorithm
In the decoding algorithm, a hypothesis is an
edge, which corresponds to a sub-tree in a CCG
derivation. Edges are built bottom-up, starting
from leaf edges, which are generated by assigning
all possible lexical categories to each input word.
Each leaf edge corresponds to an input word with
a particular lexical category. Two existing edges
can be combined if there exists a CCG rule which
combines their category labels, and if they do not
contain the same input word more times than its
total count in the input. The resulting edge is as-
signed a category label according to the combi-
natory rule, and covers the concatenated surface
strings of the two sub-edges in their order or com-
bination. New edges can also be generated by ap-
plying unary rules to a single existing edge. Start-
ing from the leaf edges, the bottom-up process is
repeated until a goal edge is found, and its surface
string is taken as the output.
This derivation-building process is reminiscent
of a bottom-up CCG parser in the edge combina-
tion mechanism. However, it is fundamentally
different from a bottom-up parser. Since, for
the generation problem, the order of two edges
in their combination is flexible, the search prob-
lem is much harder than that of a parser. With
no input order specified, no efficient dynamic-
programming algorithm is available, and less con-
textual information is available for disambigua-
tion due to the lack of an input string.
In order to combat the large search space, best-
first search is applied, where candidate hypothe-
ses are ordered by their scores, and kept in an
agenda, and a limited number of accepted hy-
potheses are recorded in a chart. Here the chart
is essentially a set of beams, each of which con-
tains the highest scored edges covering a particu-
lar number of words. Initially, all leaf edges are
generated and scored, before they are put onto the
agenda. During each step in the decoding process,
the top edge from the agenda is expanded. If it is
a goal edge, it is returned as the output, and the
737
Algorithm 1 The decoding algorithm.
a? INITAGENDA( )
c? INITCHART( )
while not TIMEOUT( ) do
new? []
e? POPBEST(a)
if GOALTEST(e) then
return e
end if
for e? ? UNARY(e, grammar) do
APPEND(new, e)
end for
for e? ? c do
if CANCOMBINE(e, e?) then
e?? BINARY(e, e?, grammar)
APPEND(new, e?)
end if
if CANCOMBINE(e?, e) then
e?? BINARY(e?, e, grammar)
APPEND(new, e?)
end if
end for
for e? ? new do
ADD(a, e?)
end for
ADD(c, e)
end while
decoding finishes. Otherwise it is extended with
unary rules, and combined with existing edges in
the chart using binary rules to produce new edges.
The resulting edges are scored and put onto the
agenda, while the original edge is put onto the
chart. The process repeats until a goal edge is
found, or a timeout limit is reached. In the latter
case, a default output is produced using existing
edges in the chart.
Pseudocode for the decoder is shown as Algo-
rithm 1. Again it is reminiscent of a best-first
parser (Caraballo and Charniak, 1998) in the use
of an agenda and a chart, but is fundamentally dif-
ferent due to the fact that there is no input order.
2.3 Statistical model and feature templates
The baseline system uses a linear model to score
hypotheses. For an edge e, its score is defined as:
f(e) = ?(e) ? ?,
where ?(e) represents the feature vector of e and
? is the parameter vector of the model.
During decoding, feature vectors are computed
incrementally. When an edge is constructed, its
score is computed from the scores of its sub-edges
and the incrementally added structure:
f(e) = ?(e) ? ?
=
(
(
?
es?e
?(es)
)
+ ?(e)
)
? ?
=
(
?
es?e
?(es) ? ?
)
+ ?(e) ? ?
=
(
?
es?e
f(es)
)
+ ?(e) ? ?
In the equation, es ? e represents a sub-edge of
e. Leaf edges do not have any sub-edges. Unary-
branching edges have one sub-edge, and binary-
branching edges have two sub-edges. The fea-
ture vector ?(e) represents the incremental struc-
ture when e is constructed over its sub-edges.
It is called the ?constituent-level feature vector?
by Z&C. For leaf edges, ?(e) includes informa-
tion about the lexical category label; for unary-
branching edges, ?(e) includes information from
the unary rule; for binary-branching edges, ?(e)
includes information from the binary rule, and ad-
ditionally the token, POS and lexical category bi-
grams and trigrams that result from the surface
string concatenation of its sub-edges. The score
f(e) is therefore the sum of f(es) (for all es ? e)
plus ?(e) ??. The feature templates we use are the
same as those in the baseline system.
An important aspect of the scoring model is that
edges with different sizes are compared with each
other during decoding. Edges with different sizes
can have different numbers of features, which can
make the training of a discriminative model more
difficult. For example, a leaf edge with one word
can be compared with an edge over the entire in-
put. One way of reducing the effect of the size dif-
ference is to include the size of the edge as part of
feature definitions, which can improve the compa-
rability of edges of different sizes by reducing the
number of features they have in common. Such
features are applied by Z&C, and we make use of
them here. Even with such features, the question
of whether edges with different sizes are linearly
separable is an empirical one.
3 Training
The efficiency of the decoding algorithm is de-
pendent on the statistical model, since the best-
738
first search is guided to a solution by the model,
and a good model will lead to a solution being
found more quickly. In the ideal situation for the
best-first decoding algorithm, the model is perfect
and the score of any gold-standard edge is higher
than the score of any non-gold-standard edge. As
a result, the top edge on the agenda is always a
gold-standard edge, and therefore all edges on the
chart are gold-standard before the gold-standard
goal edge is found. In this oracle procedure, the
minimum number of edges is expanded, and the
output is correct. The best-first decoder is perfect
in not only accuracy, but also speed. In practice
this ideal situation is rarely met, but it determines
the goal of the training algorithm: to produce the
perfect model and hence decoder.
If we take gold-standard edges as positive ex-
amples, and non-gold-standard edges as negative
examples, the goal of the training problem can be
viewed as finding a large separating margin be-
tween the scores of positive and negative exam-
ples. However, it is infeasible to generate the full
space of negative examples, which is factorial in
the size of input. Like Z&C, we apply online
learning, and generate negative examples based
on the decoding algorithm.
Our training algorithm is shown as Algo-
rithm 2. The algorithm is based on the decoder,
where an agenda is used as a priority queue of
edges to be expanded, and a set of accepted edges
is kept in a chart. Similar to the decoding algo-
rithm, the agenda is intialized using all possible
leaf edges. During each step, the top of the agenda
e is popped. If it is a gold-standard edge, it is ex-
panded in exactly the same way as the decoder,
with the newly generated edges being put onto
the agenda, and e being inserted into the chart.
If e is not a gold-standard edge, we take it as a
negative example e?, and take the lowest scored
gold-standard edge on the agenda e+ as a positive
example, in order to make an udpate to the model
parameter vector ?. Our parameter update algo-
rithm is different from the baseline perceptron al-
gorithm, as will be discussed later. After updating
the parameters, the scores of agenda edges above
and including e?, together with all chart edges,
are updated, and e? is discarded before the start
of the next processing step. By not putting any
non-gold-standard edges onto the chart, the train-
ing speed is much faster; on the other hand a wide
range of negative examples is pruned. We leave
Algorithm 2 The training algorithm.
a? INITAGENDA( )
c? INITCHART( )
while not TIMEOUT( ) do
new? []
e? POPBEST(a)
if GOLDSTANDARD(e) and GOALTEST(e)
then return e
end if
if not GOLDSTANDARD(e) then
e?? e
e+?MINGOLD(a)
UPDATEPARAMETERS(e+ , e?)
RECOMPUTESCORES(a, c)
continue
end if
for e? ? UNARY(e, grammar) do
APPEND(new, e)
end for
for e? ? c do
if CANCOMBINE(e, e?) then
e?? BINARY(e, e?, grammar)
APPEND(new, e?)
end if
if CANCOMBINE(e?, e) then
e?? BINARY(e?, e, grammar)
APPEND(new, e?)
end if
end for
for e? ? new do
ADD(a, e?)
end for
ADD(c, e)
end while
for further work possible alternative methods to
generate more negative examples during training.
Another way of viewing the training process is
that it pushes gold-standard edges towards the top
of the agenda, and crucially pushes them above
non-gold-standard edges. This is the view de-
scribed by Z&C. Given a positive example e+ and
a negative example e?, they use the perceptron
algorithm to penalize the score for ?(e?) and re-
ward the score of ?(e+), but do not update pa-
rameters for the sub-edges of e+ and e?. An argu-
ment for not penalizing the sub-edge scores for e?
is that the sub-edges must be gold-standard edges
(since the training process is constructed so that
only gold-standard edges are expanded). From
739
the perspective of correctness, it is unnecessary
to find a margin between the sub-edges of e+ and
those of e?, since both are gold-standard edges.
However, since the score of an edge not only
represents its correctness, but also affects its pri-
ority on the agenda, promoting the sub-edge of
e+ can lead to ?easier? edges being constructed
before ?harder? ones (i.e. those that are less
likely to be correct), and therefore improve the
output accuracy. This perspective has been ob-
served by other works of learning-guided-search
(Shen et al 2007; Shen and Joshi, 2008; Gold-
berg and Elhadad, 2010). Intuitively, the score
difference between easy gold-standard and harder
gold-standard edges should not be as great as the
difference between gold-standard and non-gold-
standard edges. The perceptron update cannot
provide such control of separation, because the
amount of update is fixed to 1.
As described earlier, we treat parameter update
as finding a separation between correct and incor-
rect edges, in which the global feature vectors ?,
rather than ?, are considered. Given a positive ex-
ample e+ and a negative example e?, we make a
minimum update so that the score of e+ is higher
than that of e? with some margin:
? ? argmin
??
? ????0 ?, s.t.?(e+)????(e?)?? ? 1
where ?0 and ? denote the parameter vectors be-
fore and after the udpate, respectively. The up-
date is similar to the update of online large-margin
learning algorithms such as 1-best MIRA (Cram-
mer et al 2006), and has a closed-form solution:
? ? ?0+
f(e?)? f(e+) + 1
? ?(e+)? ?(e?) ?2
(
?(e+)??(e?)
)
In this update, the global feature vectors ?(e+)
and ?(e?) are used. Unlike Z&C, the scores
of sub-edges of e+ and e? are also udpated, so
that the sub-edges of e? are less prioritized than
those of e+. We show empirically that this train-
ing algorithm significantly outperforms the per-
ceptron training of the baseline system in Sec-
tion 5. An advantage of our new training algo-
rithm is that it enables the accommodation of a
separately trained N -gram model into the system.
4 Incorporating an N-gram language
model
Since the seminal work of the IBM models
(Brown et al 1993), N -gram language models
have been used as a standard component in statis-
tical machine translation systems to control out-
put fluency. For the syntax-based generation sys-
tem, the incorporation of an N -gram language
model can potentially improve the local fluency
of output sequences. In addition, the N -gram
language model can be trained separately using
a large amount of data, while the syntax-based
model requires manual annotation for training.
The standard method for the combination of
a syntax model and an N -gram model is linear
interpolation. We incorporate fourgram, trigram
and bigram scores into our syntax model, so that
the score of an edge e becomes:
F (e) = f(e) + g(e)
= f(e) + ? ? gfour(e) + ? ? gtri(e) + ? ? gbi(e),
where f is the syntax model score, and g is the
N -gram model score. g consists of three com-
ponents, gfour, gtri and gbi, representing the log-
probabilities of fourgrams, trigrams and bigrams
from the language model, respectively. ?, ? and
? are the corresponding weights.
During decoding, F (e) is computed incremen-
tally. Again, denoting the sub-edges of e as es,
F (e) = f(e) + g(e)
=
(
?
es?e
F (es)
)
+ ?(e)? + g?(e)
Here g?(e) = ? ?g?four(e)+? ?g?tri(e)+? ?g?bi(e)
is the sum of log-probabilities of the new N -
grams resulting from the construction of e. For
leaf edges and unary-branching edges, no new N -
grams result from their construction (i.e. g? = 0).
For a binary-branching edge, new N -grams result
from the surface-string concatenation of its sub-
edges. The sum of log-probabilities of the new
fourgrams, trigrams and bigrams contribute to g?
with weights ?, ? and ?, respectively.
For training, there are at least three methods to
tune ?, ?, ? and ?. One simple method is to train
the syntax model ? independently, and select ?,
?, and ? empirically from a range of candidate
values according to development tests. We call
this method test-time interpolation. An alterna-
tive is to select ?, ? and ? first, initializing the
vector ? as all zeroes, and then run the training
algorithm for ? taking into account the N -gram
language model. In this process, g is considered
when finding a separation between positive and
740
negative examples; the training algorithm finds a
value of ? that best suits the precomputed ?, ?
and ? values, together with the N -gram language
model. We call this method g-precomputed in-
terpolation. Yet another method is to initialize ?,
?, ? and ? as all zeroes, and run the training al-
gorithm taking into account the N -gram language
model. We call this method g-free interpolation.
The incorporation of an N -gram language
model into the syntax-based generation system is
weakly analogous to N -gram model insertion for
syntax-based statistical machine translation sys-
tems, both of which apply a score from the N -
gram model component in a derivation-building
process. As discussed earlier, polynomial-time
decoding is typically feasible for syntax-based
machine translation systems without an N -gram
language model, due to constraints from the
grammar. In these cases, incorporation of N -
gram language models can significantly increase
the complexity of a dynamic-programming de-
coder (Bar-Hillel et al 1961). Efficient search
has been achieved using chart pruning (Chiang,
2007) and iterative numerical approaches to con-
strained optimization (Rush and Collins, 2011).
In contrast, the incorporation of an N -gram lan-
guage model into our decoder is more straightfor-
ward, and does not add to its asymptotic complex-
ity, due to the heuristic nature of the decoder.
5 Experiments
We use sections 2?21 of CCGBank to train our
syntax model, section 00 for development and
section 23 for the final test. Derivations from
CCGBank are transformed into inputs by turn-
ing their surface strings into multi-sets of words.
Following Z&C, we treat base noun phrases (i.e.
NPs that do not recursively contain other NPs) as
atomic units for the input. Output sequences are
compared with the original sentences to evaluate
their quality. We follow previous work and use
the BLEU metric (Papineni et al 2002) to com-
pare outputs with references.
Z&C use two methods to construct leaf edges.
The first is to assign lexical categories according
to a dictionary. There are 26.8 lexical categories
for each word on average using this method, cor-
responding to 26.8 leaf edges. The other method
is to use a pre-processing step ? a CCG supertag-
ger (Clark and Curran, 2007) ? to prune can-
didate lexical categories according to the gold-
CCGBank Sentences Tokens
training 39,604 929,552
development 1,913 45,422
GigaWord v4 Sentences Tokens
AFP 30,363,052 684,910,697
XIN 15,982,098 340,666,976
Table 1: Number of sentences and tokens by language
model source.
standard sequence, assuming that for some prob-
lems the ambiguities can be reduced (e.g. when
the input is already partly correctly ordered).
Z&C use different probability cutoff levels (the
? parameter in the supertagger) to control the
pruning. Here we focus mainly on the dictionary
method, which leaves lexical category disam-
biguation entirely to the generation system. For
comparison, we also perform experiments with
lexical category pruning. We chose ? = 0.0001,
which leaves 5.4 leaf edges per word on average.
We used the SRILM Toolkit (Stolcke, 2002)
to build a true-case 4-gram language model es-
timated over the CCGBank training and develop-
ment data and a large additional collection of flu-
ent sentences in the Agence France-Presse (AFP)
and Xinhua News Agency (XIN) subsets of the
English GigaWord Fourth Edition (Parker et al
2009), a total of over 1 billion tokens. The Gi-
gaWord data was first pre-processed to replicate
the CCGBank tokenization. The total number
of sentences and tokens in each LM component
is shown in Table 1. The language model vo-
cabulary consists of the 46,574 words that oc-
cur in the concatenation of the CCGBank train-
ing, development, and test sets. The LM proba-
bilities are estimated using modified Kneser-Ney
smoothing (Kneser and Ney, 1995) with interpo-
lation of lower n-gram orders.
5.1 Development experiments
A set of development test results without lexical
category pruning (i.e. using the full dictionary) is
shown in Table 2. We train the baseline system
and our systems under various settings for 10 iter-
ations, and measure the output BLEU scores after
each iteration. The timeout value for each sen-
tence is set to 5 seconds. The highest score (max
BLEU) and averaged score (avg. BLEU) of each
system over the 10 training iterations are shown
in the table.
741
Method max BLEU avg. BLEU
baseline 38.47 37.36
margin 41.20 39.70
margin +LM (g-precomputed) 41.50 40.84
margin +LM (? = 0, ? = 0, ? = 0) 40.83 ?
margin +LM (? = 0.08, ? = 0.016, ? = 0.004) 38.99 ?
margin +LM (? = 0.4, ? = 0.08, ? = 0.02) 36.17 ?
margin +LM (? = 0.8, ? = 0.16, ? = 0.04) 34.74 ?
Table 2: Development experiments without lexical category pruning.
The first three rows represent the baseline sys-
tem, our largin-margin training system (margin),
and our system with the N -gram model incorpo-
rated using g-precomputed interpolation. For in-
terpolation we manually chose ? = 0.8, ? = 0.16
and ? = 0.04, respectively. These values could
be optimized by development experiments with
alternative configurations, which may lead to fur-
ther improvements. Our system with large-margin
training gives higher BLEU scores than the base-
line system consistently over all iterations. The
N -gram model led to further improvements.
The last four rows in the table show results
of our system with the N -gram model added us-
ing test-time interpolation. The syntax model is
trained with the optimal number of iterations, and
different ?, ?, and ? values are used to integrate
the language model. Compared with the system
using no N -gram model (margin), test-time inter-
polation did not improve the accuracies.
The row with ?, ?, ? = 0 represents our system
with the N -gram model loaded, and the scores
gfour , gtri and gbi computed for each N -gram
during decoding, but the scores of edges are com-
puted without using N -gram probabilities. The
scoring model is the same as the syntax model
(margin), but the results are lower than the row
?margin?, because computing N -gram probabil-
ities made the system slower, exploring less hy-
potheses under the same timeout setting.1
The comparison between g-precomputed inter-
polation and test-time interpolation shows that the
system gives better scores when the syntax model
takes into consideration the N -gram model during
1More decoding time could be given to the slower N -
gram system, but we use 5 seconds as the timeout setting
for all the experiments, giving the methods with the N -gram
language model a slight disadvantage, as shown by the two
rows ?margin? and ?margin +LM (?, ?, ? = 0).
 37
 38
 39
 40
 41
 42
 43
 44
 45
 1  2  3  4  5  6  7  8  9  10
BL
EU
training iteration
baseline
margin
margin +LM
Figure 1: Development experiments with lexical cate-
gory pruning (? = 0.0001).
training. One question that arises is whether g-
free interpolation will outperform g-precomputed
interpolation. g-free interpolation offers the free-
dom of ?, ? and ? during training, and can poten-
tially reach a better combination of the parameter
values. However, the training algorithm failed to
converge with g-free interpolation. One possible
explanation is that real-valued features from the
language model made our large-margin training
harder. Another possible reason is that our train-
ing process with heavy pruning does not accom-
modate this complex model.
Figure 1 shows a set of development experi-
ments with lexical category pruning (with the su-
pertagger parameter ? = 0.0001). The scores
of the three different systems are calculated by
varying the number of training iterations. The
large-margin training system (margin) gave con-
sistently better scores than the baseline system,
and adding a language model (margin +LM) im-
proves the scores further.
Table 3 shows some manually chosen examples
for which our system gave significant improve-
ments over the baseline. For most other sentences
the improvements are not as obvious. For each
742
baseline margin margin +LM
as a nonexecutive director Pierre Vinken
, 61 years old , will join the board . 29
Nov.
61 years old , the board will join as a
nonexecutive director Nov. 29 , Pierre
Vinken .
as a nonexecutive director Pierre Vinken
, 61 years old , will join the board Nov.
29 .
Lorillard nor smokers were aware of the
Kent cigarettes of any research on the
workers who studied the researchers
of any research who studied Neither the
workers were aware of smokers on the
Kent cigarettes nor the researchers
Neither Lorillard nor any research on the
workers who studied the Kent cigarettes
were aware of smokers of the researchers
.
you But 35 years ago have to recognize
that these events took place .
recognize But you took place that these
events have to 35 years ago .
But you have to recognize that these
events took place 35 years ago .
investors to pour cash into money funds
continue in Despite yields recent declines
Despite investors , yields continue to
pour into money funds recent declines in
cash .
Despite investors , recent declines in
yields continue to pour cash into money
funds .
yielding The top money funds are cur-
rently well over 9 % .
The top money funds currently are yield-
ing well over 9 % .
The top money funds are yielding well
over 9 % currently .
where A buffet breakfast , held in the mu-
seum was food and drinks to . everyday
visitors banned
everyday visitors are banned to where
A buffet breakfast was held , food and
drinks in the museum .
A buffet breakfast , everyday visitors are
banned to where food and drinks was
held in the museum .
A Commonwealth Edison spokesman
said an administrative nightmare would
be tracking down the past 3 12 years that
the two million customers have . whose
changed
tracking A Commonwealth Edison
spokesman said that the two million cus-
tomers whose addresses have changed
down during the past 3 12 years would
be an administrative nightmare .
an administrative nightmare whose ad-
dresses would be tracking down A Com-
monwealth Edison spokesman said that
the two million customers have changed
during the past 3 12 years .
The $ 2.5 billion Byron 1 plant , Ill. , was
completed . near Rockford in 1985
The $ 2.5 billion Byron 1 plant was near
completed in Rockford , Ill. , 1985 .
The $ 2.5 billion Byron 1 plant near
Rockford , Ill. , was completed in 1985 .
will ( During its centennial year , The
Wall Street Journal report events of the
past century that stand as milestones of
American business history . )
as The Wall Street Journal ( During its
centennial year , milestones stand of
American business history that will re-
port events of the past century . )
During its centennial year events will re-
port , The Wall Street Journal that stand
as milestones of American business his-
tory ( of the past century ) .
Table 3: Some chosen examples with significant improvements (supertagger parameter ? = 0.0001).
method, the examples are chosen from the devel-
opment output with lexical category pruning, af-
ter the optimal number of training iterations, with
the timeout set to 5s. We also tried manually se-
lecting examples without lexical category prun-
ing, but the improvements were not as obvious,
partly because the overall fluency was lower for
all the three systems.
Table 4 shows a set of examples chosen ran-
domly from the development test outputs of our
system with the N -gram model. The optimal
number of training iterations is used, and a time-
out of 1 minute is used in addition to the 5s time-
out for comparison. With more time to decode
each input, the system gave a BLEU score of
44.61, higher than 41.50 with the 5s timout.
While some of the outputs we examined are
reasonably fluent, most are to some extent frag-
mentary.2 In general, the system outputs are
still far below human fluency. Some samples are
2Part of the reason for some fragmentary outputs is the
default output mechanism: partial derivations from the chart
are greedily put together when timeout occurs before a goal
hypothesis is found.
syntactically grammatical, but are semantically
anomalous. For example, person names are often
confused with company names, verbs often take
unrelated subjects and objects. The problem is
much more severe for long sentences, which have
more ambiguities. For specific tasks, extra infor-
mation (such as the source text for machine trans-
lation) can be available to reduce ambiguities.
6 Final results
The final results of our system without lexical cat-
egory pruning are shown in Table 5. Row ?W09
CLE? and ?W09 AB? show the results of the
maximum spanning tree and assignment-based al-
gorithms of Wan et al(2009); rows ?margin?
and ?margin +LM? show the results of our large-
margin training system and our system with the
N -gram model. All these results are directly com-
parable since we do not use any lexical category
pruning for this set of results. For each of our
systems, we fix the number of training iterations
according to development test scores. Consis-
tent with the development experiments, our sys-
743
timeout = 5s timeout = 1m
drooled the cars and drivers , like Fortune 500 executives . over
the race
After schoolboys drooled over the cars and drivers , the race
like Fortune 500 executives .
One big reason : thin margins . One big reason : thin margins .
You or accountants look around ... and at an eye blinks . pro-
fessional ballplayers
blinks nobody You or accountants look around ... and at an eye
. professional ballplayers
most disturbing And of it , are educators , not students , for the
wrongdoing is who .
And blamed for the wrongdoing , educators , not students who
are disturbing , much of it is most .
defeat coaching aids the purpose of which is , He and other
critics say can to . standardized tests learning progress
gauge coaching aids learning progress can and other critics say
the purpose of which is to defeat , standardized tests .
The federal government of government debt because Congress
has lifted the ceiling on U.S. savings bonds suspended sales
The federal government suspended sales of government debt
because Congress has n?t lifted the ceiling on U.S. savings
bonds .
Table 4: Some examples chosen at random from development test outputs without lexical category pruning.
System BLEU
W09 CLE 26.8
W09 AB 33.7
Z&C11 40.1
margin 42.5
margin +LM 43.8
Table 5: Test results without lexical category pruning.
System BLEU
Z&C11 43.2
margin 44.7
margin +LM 46.1
Table 6: Test results with lexical category pruning (su-
pertagger parameter ? = 0.0001).
tem outperforms the baseline methods. The acu-
racies are significantly higher when the N -gram
model is incorporated.
Table 6 compares our system with Z&C using
lexical category pruning (? = 0.0001) and a 5s
timeout for fair comparison. The results are sim-
ilar to Table 5: our large-margin training systems
outperforms the baseline by 1.5 BLEU points, and
adding the N -gram model gave a further 1.4 point
improvement. The scores could be significantly
increased by using a larger timeout, as shown in
our earlier development experiments.
7 Related Work
There is a recent line of research on text-to-
text generation, which studies the linearization of
dependency structures (Barzilay and McKeown,
2005; Filippova and Strube, 2007; Filippova and
Strube, 2009; Bohnet et al 2010; Guo et al
2011). Unlike our system, and Wan et al(2009),
input dependencies provide additional informa-
tion to these systems. Although the search space
can be constrained by the assumption of projec-
tivity, permutation of modifiers of the same head
word makes exact inference for tree lineariza-
tion intractable. The above systems typically ap-
ply approximate inference, such as beam-search.
While syntax-based features are commonly used
by these systems for linearization, Filippova and
Strube (2009) apply a trigram model to control
local fluency within constituents. A dependency-
based N-gram model has also been shown effec-
tive for the linearization task (Guo et al 2011).
The best-first inference and timeout mechanism
of our system is similar to that of White (2004), a
surface realizer from logical forms using CCG.
8 Conclusion
We studied the problem of word-ordering using
a syntactic model and allowing permutation. We
took the model of Zhang and Clark (2011) as the
baseline, and extended it with online large-margin
training and an N -gram language model. These
extentions led to improvements in the BLEU eval-
uation. Analyzing the generated sentences sug-
gests that, while highly fluent outputs can be pro-
duced for short sentences (? 10 words), the sys-
tem fluency in general is still way below human
standard. Future work remains to apply the sys-
tem as a component for specific text generation
tasks, for example machine translation.
Acknowledgements
Yue Zhang and Stephen Clark are supported by the Eu-
ropean Union Seventh Framework Programme (FP7-
ICT-2009-4) under grant agreement no. 247762.
744
References
Yehoshua Bar-Hillel, M. Perles, and E. Shamir. 1961.
On formal properties of simple phrase structure
grammars. Zeitschrift fu?r Phonetik, Sprachwis-
senschaft und Kommunikationsforschung, 14:143?
172. Reprinted in Y. Bar-Hillel. (1964). Language
and Information: Selected Essays on their Theory
and Application, Addison-Wesley 1964, 116?150.
Regina Barzilay and Kathleen McKeown. 2005. Sen-
tence fusion for multidocument news summariza-
tion. Computational Linguistics, 31(3):297?328.
Graeme Blackwood, Adria` de Gispert, and William
Byrne. 2010. Fluency constraints for minimum
Bayes-risk decoding of statistical machine trans-
lation lattices. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(Coling 2010), pages 71?79, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Bernd Bohnet, Leo Wanner, Simon Mill, and Alicia
Burga. 2010. Broad coverage multilingual deep
sentence generation with a stochastic multi-level re-
alizer. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling
2010), pages 98?106, Beijing, China, August. Col-
ing 2010 Organizing Committee.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Sharon A. Caraballo and Eugene Charniak. 1998.
New figures of merit for best-first probabilistic chart
parsing. Comput. Linguist., 24:275?298, June.
David Chiang. 2007. Hierarchical Phrase-
based Translation. Computational Linguistics,
33(2):201?228.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
Katja Filippova and Michael Strube. 2007. Gener-
ating constituent order in german clauses. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 320?
327, Prague, Czech Republic, June. Association for
Computational Linguistics.
Katja Filippova and Michael Strube. 2009. Tree lin-
earization in english: Improving language model
based approaches. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Short Papers, pages 225?228, Boulder, Colorado,
June. Association for Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 742?750, Los Angeles, California,
June. Association for Computational Linguistics.
Yuqing Guo, Deirdre Hogan, and Josef van Genabith.
2011. Dcu at generation challenges 2011 surface
realisation track. In Proceedings of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation, pages 227?229,
Nancy, France, September. Association for Compu-
tational Linguistics.
Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with Combi-
natory Categorial Grammar. In Proceedings of the
40th Meeting of the ACL, pages 335?342, Philadel-
phia, PA.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In International
Conference on Acoustics, Speech, and Signal Pro-
cessing, 1995. ICASSP-95, volume 1, pages 181?
184.
Philip Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of NAACL/HLT, Edmonton, Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword Fourth
Edition, Linguistic Data Consortium.
Alexander M. Rush and Michael Collins. 2011. Exact
decoding of syntactic translation models through la-
745
grangian relaxation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
72?82, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Libin Shen and Aravind Joshi. 2008. LTAG depen-
dency parsing with bidirectional incremental con-
struction. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 495?504, Honolulu, Hawaii, Octo-
ber. Association for Computational Linguistics.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classi-
fication. In Proceedings of ACL, pages 760?767,
Prague, Czech Republic, June.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, Mass.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing, pages 901?904.
Stephen Wan, Mark Dras, Robert Dale, and Ce?cile
Paris. 2009. Improving grammaticality in statisti-
cal sentence generation: Introducing a dependency
spanning tree algorithm with an argument satisfac-
tion model. In Proceedings of the 12th Conference
of the European Chapter of the ACL (EACL 2009),
pages 852?860, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Michael White. 2004. Reining in CCG chart realiza-
tion. In Proc. INLG-04, pages 182?191.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
Yue Zhang and Stephen Clark. 2011. Syntax-
based grammaticality improvement using CCG and
guided search. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 1147?1157, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
746
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 588?597,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Type-Supervised Domain Adaptation for Joint Segmentation and
POS-Tagging
Meishan Zhang
?
, Yue Zhang
?
, Wanxiang Che
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{mszhang, car, tliu}@ir.hit.edu.cn
?
Singapore University of Technology and Design
yue zhang@sutd.edu.sg
Abstract
We report an empirical investigation on
type-supervised domain adaptation for
joint Chinese word segmentation and
POS-tagging, making use of domain-
specific tag dictionaries and only un-
labeled target domain data to improve
target-domain accuracies, given a set of
annotated source domain sentences. Pre-
vious work on POS-tagging of other lan-
guages showed that type-supervision can
be a competitive alternative to token-
supervision, while semi-supervised tech-
niques such as label propagation are
important to the effectiveness of type-
supervision. We report similar findings
using a novel approach for joint Chinese
segmentation and POS-tagging, under a
cross-domain setting. With the help of un-
labeled sentences and a lexicon of 3,000
words, we obtain 33% error reduction in
target-domain tagging. In addition, com-
bined type- and token-supervision can lead
to improved cost-effectiveness.
1 Introduction
With accuracies of over 97%, POS-tagging of
WSJ can be treated as a solved problem (Man-
ning, 2011). However, performance is still well
below satisfactory for many other languages and
domains (Petrov et al., 2012; Christodoulopoulos
et al., 2010). There has been a line of research on
using a tag-dictionary for POS-tagging (Merialdo,
1994; Toutanova and Johnson, 2007; Ravi and
Knight, 2009; Garrette and Baldridge, 2012). The
idea is compelling: on the one hand, a list of lex-
icons is often available for special domains, such
as bio-informatics; on the other hand, compiling a
?
Corresponding author.
lexicon of word-tag pairs appears to be less time-
consuming than annotating full sentences.
However, success in type-supervised POS-
tagging turns out to depend on several subtle fac-
tors. For example, recent research has found that
the quality of the tag-dictionary is crucial to the
success of such methods (Banko and Moore, 2004;
Goldberg et al., 2008; Garrette and Baldridge,
2012). Banko and Moore (2004) found that the
accuracies can drop from 96% to 77% when a
hand-crafted tag dictionary is replaced with a raw
tag dictionary gleaned from data, without any hu-
man intervention. These facts indicate that careful
considerations need to be given for effective type-
supervision. In addition, significant manual work
might be required to ensure the quality of lexicons.
To compare type- and token-supervised tagging,
Garrette and Baldridge (2013) performed a set of
experiments by conducting each type of annota-
tion for two hours. They showed that for low-
resource languages, a tag-dictionary can be rea-
sonably effective if label propagation (Talukdar
and Crammer, 2009) and model minimizations
(Ravi and Knight, 2009) are applied to expand and
filter the lexicons. Similar findings were reported
in Garrette et al. (2013).
Do the above findings carry over to the Chi-
nese language? In this paper, we perform an
empirical study on the effects of tag-dictionaries
for domain adaptation of Chinese POS-tagging.
We aim to answer the following research ques-
tions: (a) Is domain adaptation feasible with only
a target-domain lexicon? (b) Can we further im-
prove type-supervised domain adaptation using
unlabeled target-domain sentences? (c) Is craft-
ing a tag dictionary for domain adaptation more
effective than manually annotating target domain
sentences, given similar efforts?
Our investigations are performed under two
Chinese-specific settings. First, unlike low-
resource languages, large amounts of annotation
588
are available for Chinese. For example, the Chi-
nese Treebank (CTB) (Xue et al., 2005) contains
over 50,000 manually tagged news sentences.
Hence rather than studying purely type-supervised
POS-tagging, we make use of CTB as the source
domain, and study domain adaptation to the Inter-
net literature.
Second, one uniqueness of Chinese POS-
tagging, in contrast to the POS-tagging of alpha-
betical languages, is that word segmentation can
be performed jointly to avoid error propagation
(Ng and Low, 2004; Zhang and Clark, 2008; Kru-
engkrai et al., 2009; Zhang and Clark, 2010). We
adopt this approach for a strong baseline. Previous
studies showed that unsupervised domain adap-
tation can give moderate improvements (Liu and
Zhang, 2012). We show that accuracies can be
much more significantly improved by using target-
domain knowledge in the form of lexicons.
Both token-supervised and type-supervised do-
main adaptation rely on a set of source-domain
annotations; while the former makes additional
use of a small set of target annotations, the lat-
ter leverages a target-domain lexicon. We take
a feature-based method, analogous to that of
Daume III (2007), which tunes domain-dependent
versions of features using domain-specific data.
Our method tunes a set of lexicon-based features,
so that domain-dependent models are derived from
inserting domain-specific lexicons.
The conceptually simple method worked highly
effectively on a test set of 1,394 sentences from
the Internet novel ?Zhuxian?. Combined with
the use of unlabeled data, a tag lexicon of 3,000
words gave a 33% error reduction when com-
pared with a strong baseline system trained using
CTB data. We observe that joint use of type- and
token-supervised domain adaptation is more cost-
effective than pure type- or token-supervision.
With 10 hours of annotation, the best error reduc-
tion reaches 47%, with F-score increasing from
80.81% to 89.84%.
2 Baseline
We take as the baseline system a discriminative
joint segmentation and tagging model, proposed
by Zhang and Clark (2010), together with simple
self-training (Liu and Zhang, 2012). While the
baseline discriminative model gives state-of-the-
art joint segmentation and tagging accuracies on
CTB data, the baseline self-training makes use of
unlabeled target domain data to find improved tar-
get domain accuracies over bare CTB training.
2.1 The Baseline Discriminative Chinese
POS-Tagging Model
The baseline discriminative model performs
segmentation and POS-tagging simultaneously.
Given an input sentence c
1
? ? ? c
n
(c
i
refers to the
ith character in the sentence), it operates incre-
mentally, from left to right. At each step, the cur-
rent character can either be appended to the last
word of the existing partial output, or seperated as
the start of a new word with tag p. A beam is used
to maintain the N-best partial results at each step
during decoding. At step i (0 ? i < n), each
item in the beam corresponds to a segmentation
and POS-tagging hypothesis for the first i?1 char-
acters, with the last word being associated with a
POS, but marked as incomplete. When the next
character c
i
is processed, it is combined with all
the partial results from the beam to generate new
partial results, using two types of actions: (1) Ap-
pend, which appends c
i
to the last (partial) word
in a partial result; (2) Separate(p), which makes
the last word in the partial result as completed and
adds c
i
as a new partial word with a POS tag p.
Partial results in the beam are scored globally
over all actions used to build them, so that the N-
best can be put back to the agenda for the next step.
For each action, features are extracted differently.
We use the features from Zhang and Clark (2010).
Discriminative learning with early-update (Collins
and Roark, 2004; Zhang and Clark, 2011) is used
to train the model with beam-search.
2.2 Baseline Unsupervised Adaptation by
Self-Training
A simple unsupervised approach for POS-tagging
with unlabeled data is EM. For a generative model
such as HMM, EM can locally maximize the like-
lihood of training data. Given a good start, EM
can result in a competitive HMM tagging model
(Goldberg et al., 2008).
For discriminative models with source-domain
training examples, an initial model can be trained
using the source-domain data, and self-training
can be applied to find a locally-optimized model
using raw target domain sentences. The training
process is sometimes associated with the EM al-
gorithm. Liu and Zhang (2012) used perplexities
of character trigrams to order unlabeled sentences,
and applied self-training to achieve a 6.3% error
589
Common
Lexicon
Source
Lexicon
Source
Corpus
Training
Model
Target
Sentences
Target
Lexicon
Common
Lexicon
Tagging
Tagging
Results
Training Tagging
Figure 1: Architecture of our lexicon-based model for domain adaptation.
reduction on target-domain data when compared
with source domain training. Their method is sim-
ple to implement, and we take it as our baseline.
3 Type-Supervised Domain Adaptation
To give a formal definition of the domain adap-
tation tasks, we denote by C
s
a set of anno-
tated source-domain sentences, C
t
a set of anno-
tated target-domain sentences, and L
t
an anno-
tated target-domain lexicon. The form of L
t
is a
list of target-domain words, each associated with
a set of POS tags. Token-supervised domain adap-
tation is the task of making use of C
s
and C
t
to
improve target-domain performances, while type-
supervised domain adaptation is to make use of C
s
and L
t
instead for the same purpose.
As described in the introduction, type-
supervised domain adaptation is useful when
annotated sentences are absent, but lexicons are
available. In addition, it is an interesting question
which type of annotation is more cost-effective
when neither is available. We empirically com-
pare the two approaches by proposing a novel
method for type-supervised domain adaptation of
a discriminate tagging model, showing that it can
be a favourable choice in practical situation.
In particular, we split Chinese words into
domain-independent and domain-specific cate-
gories, and define unlexicalized features for
domain-specific words. We train lexicalized
domain-independent and unlexicalized domain-
specific features using the source domain anno-
tated sentences and a source-domain lexicon, and
then apply the resulting model to the target do-
main by replacing the source-domain lexicon with
a target domain lexicon. Combined with unsu-
pervised learning with unlabeled target-domain
of sentences, the conceptually simple method
worked highly effectively. Following Garrette and
Baldridge (2013), we address practical questions
on type-supervised domain adaptation by compar-
ison with token-supervised methods under similar
human annotation efforts.
3.1 System Architecture
Our method is based on the intuition that domain-
specific words of certain types (e.g. proper names)
can behave similarly across domains. For exam-
ple, consider the source-domain sentence ???
?|NR (Jiang Zemin) ??|AD (afterwards) ?
?|VV (visit) ??|NR (Shanghai Automobiles
Corp.)? and the target-domain sentence ??
?|NR (Biyao) ??|AD (afterwards) ??|VV
(arrive) ???|NR (the Bamboo Mountains)?.
???? (Jiang Zemin)? and ??? (Biyao)? are
person names in the two domains, respectively,
whereas ??? (Shanghai Automobiles Corp.)?
and ???? (the Bamboo Mountains)? are loca-
tion names in the two domains, respectively. If the
four words are simply treated as domain-specific
nouns, the two sentences both have the pattern
??domain-NR? AD VV ?domain-NR??, and hence
source domain training data can be useful in train-
ing the distributions of the lexicon-based features
for both domains.
Further, we assume that the syntax structures
and the usage of function words do not vary sig-
nificantly across domains. For example, verbs, ad-
jectives or proper nouns can be different from do-
main to domain, but the subject-verb-object sen-
tence structure does not change. In addition, the
usage of closed-set function words remains sta-
ble across different domains. In the CTB tagset,
closed-set POS tags are the vast majority. Under
this assumption, we introduce a set of unlexical-
ized features into the discriminative model, in or-
der to capture the distributions of domain-specific
dictionary words. Unlexicalized features trained
for source domain words can carry over to the tar-
get domain. The overall architecture of our sys-
590
Action Lexicon Feature templates
Separate in-lex(w
?1
), l(w
?1
) ? in-lex(w
?1
),
in-lex(w
?1
, t
?1
), l(w
?1
) ? in-lex(w
?1
, t
?1
)
Table 1: Dictionary features of the type-
supervised model, where w
?1
and t
?1
denote the
last word and POS tag of a partial result, re-
spectively; l(w) denotes the length of the word
w; in-lex(w, t) denotes whether the word-tag pair
(w, t) is in the lexicon.
tem is shown in Figure 1, where lexicons can be
treated as ?plugins? to the model for different do-
mains, and one model trained from the source do-
main can be applied to many different target do-
mains, as long as a lexicon is available.
The method can be the most effective
when there is a significant amount of domain-
independent words in the data, which provide rich
lexicalized contexts for estimating unlexicalized
features for domain-specific words. For scientific
domains (e.g. the biomedical domain) which
share a significant proportion of common words
with the news domain, and have most domain
specific words being nouns (e.g. ???? (dia-
betes)?), the method can be the most effective.
We choose a comparatively difficult domain pair
(e.g. modern news v.s. ancient style novel),
for which the use of many word types are quite
different. Results on this data can be relatively
more indicative of the usefulness of the method.
3.2 Lexicon-Based Features
Table 1 shows the set of new unlexicalized fea-
tures for the domain-specific lexicons. In addition
to words and POS tags, length information is also
encoded in the features, to capture different dis-
tributions of different word sizes. For example,
a one-character word in the dictionary might not
be identified as confidently using the lexicon as a
three-character word in the dictionary.
To acquire a domain-specific lexicon for the
source domain, we use HowNet (Dong and
Dong, 2006) to classify CTB words into domain-
independent and domain-specific categories. Con-
sisting of semantic information for nearly 100,000
common Chinese words, HowNet can serve as a
resource of domain-independent Chinese words.
We choose out of all words in the source domain
training data those that also occur in HowNet for
domain-independent words, and out of the remain-
ing words those that occur more than 3 times for
words specific to the source domain. We assume
that the domain-independent lexicon applies to all
target domains also. For some target domains,
we can obtain domain-specific terminologies eas-
ily from the Internet. However, this can be a very
small portion depending on the domain. Thus, it
may still be necessary to obtain new lexicons by
manual annotation.
3.3 Lexicon and Self-Training
The lexicon-based features can be combined with
unsupervised learning to further improve target-
domain accuracies. We apply self-training on top
of the lexicon-based features in the following way:
we train a lexicon-based model M using a lexi-
con L
s
of the source domain, and then apply M
together with a target-domain lexicon L
t
to auto-
matically label a set of target domain sentences.
We combine the automatically labeled target sen-
tences with the source-domain training data to ob-
tain an extended set of training data, and train a
final model M
self
, using the lexicon L
s
and L
t
for
source- and target-domain data, respectively.
Different numbers of target domain sentences
can be used for self-training. Liu and Zhang
(2012) showed that an increased amount of tar-
get sentences do not constantly lead to improved
development accuracies. They use character per-
plexity to order target domain sentences, taking
the top K sentences for self-training. They eval-
uate the optimal development accuracies using a
range of different Kvalues, and select the best K
for a final model. This method gave better results
than using sentences in the internet novel in their
original order (Liu and Zhang, 2012). We follow
this method in ranking target domain sentences.
4 Experiments
4.1 Setting
We use annotated sentences from the CTB5 for
source-domain training, splitting the corpus into
training, development and test sections in the same
way as previous work (Kruengkrai et al., 2009;
Zhang and Clark, 2010; Sun, 2011).
Following Liu and Zhang (2012), we use the
free Internet novel ?Zhuxian? (henceforth referred
to as ZX; also known as ?Jade dynasty?) as our tar-
get domain data. The writing style of the novel is
in the literature genre, with the style of Ming and
Qing novels, very different from news in CTB. Ex-
591
CTB sentences ZX sentences
?????????? ??????????????????????
(Qiaoshi meets the Russian delegates.) (The world was big. It held everything. There were fascinating
?????????????? landscapes. There were haunting ghosts.)
(Lipeng stressed on speeding the reform of official regulations.) ??????????????
?????????????? (No time left. Let me call out Zhuxian, the ancient sword.)
(Chinese chemistry industry increases the pace of opening up.) ???????????????(There came suddenly
a gust of wind, out of which was laughters and magic flashes.)
Table 2: Example sentences from CTB and ZX to illustrate the differences between news and novel.
Data Set Chap. IDs # sents # words
CTB5
Train 1-270, 400-931, 10,086 493,930
1001-1151
Devel 301-325 350 6,821
Test 271-300 348 8,008
ZX
Train 6.6-6.10, 2,373 67,648
7.6-7.10, 19
Devel 6.1-6.5 788 20,393
Test 7.1-7.5 1,394 34,355
Table 3: Corpus statistics.
ample sentences from the two corpora are shown
in Table 2. Liu and Zhang (2012) manually anno-
tated 385 sentences as development and test data,
which we download from their website.
1
These
data follow the same annotation guidelines as the
Chinese Treebank (Xue et al., 2000).
To gain more reliable statistics in our results,
we extend their annotation work to a total 4,555
sentences, covering the sections 6, 7 and 19 of the
novel. The annotation work is based on the auto-
matically labeled sentences by our baseline model
trained with CTB5 corpus. It took an experienced
native speaker 80 hours, about one minute on av-
erage to annotate one sentence. We use chapters
1-5 of section 6 as the development data, chap-
ters 1-5 of section 7 as the test data, and the re-
maining data for target-domain training,
2
in order
to compare type-supervised methods with token-
supervised methods. Under permission from the
author of the novel, we release our annotation for
future reference. Statistics of both the source and
the target domain data are shown in Table 3. The
rest of the novel is treated as unlabeled sentences,
used for type-annotation and self-training.
We perform the standard evaluation, using F-
scores for both the segmentation accuracy and the
1
http://faculty.sutd.edu.sg/?yue zhang/emnlp12yang.zip
2
We only use part of the training sentences in our experi-
ments, and the remaining can be used for further research.
overall segmentation and POS tagging accuracy.
4.2 Baseline Performances
The baseline discriminative model can achieve
state-of-the-art performances on the CTB5, with
a 97.62% segmentation accuracy and a 93.85% on
overall segmentation and tagging accuracy. Using
the CTB model, the performance on ZX drops sig-
nificantly, to a 87.71% segmentation accuracy and
a 80.81% overall accuracy. Applying self-training,
the segmentation and overall F-scores can be im-
proved to 88.62% and 81.94% respectively.
4.3 Development Experiments
In this section, we study type-supervised domain
adaptation by conducting a series of experiments
on the development data, addressing the follow-
ing questions. First, what is the influence of tag-
dictionaries through lexicon-based features? Sec-
ond, what is the effect of type-supervised domain
adaptation in contrast to token-supervised domain
adaptation under the same annotation cost? Third,
what is the interaction between tag-dictionary and
self-training? Finally, what is the combined effect
of type- and token-supervised domain adaptation?
4.3.1 The Influence of The Tag Dictionary
We investigate the effects of two different tag dic-
tionaries. The first dictionary contains names of
characters (e.g. ?? (Guili)) and artifacts (e.g.
swords such as?? (Dragonslayer)) in the novel,
which are obtained from an Internet Encyclope-
dia,
3
and requires little human effort. We ex-
tracted 159 words from this page, verified them,
and put them into a tag dictionary. We associate
every word in this tag dictionary with the POS
?NR (proper noun)?, and name the lexicon by NR.
The second dictionary was constructed man-
ually, by first employing our baseline tagger to
tag the unlabeled ZX sentences automatically,
3
http://baike.baidu.com/view/18277.htm
592
Model
Target-Domain
Cost
Supervised +Self-Training
Resources SEG POS SEG POS ER
Baseline ? 0 89.77 82.92 90.35 83.95 6.03
Type-Supervision
NR(T) 0 89.84 83.91 91.18 85.22 8.14
3K(T) 5h 91.93 86.53 92.86 87.67 8.46
ORACLE(T) ? 93.10 88.87 94.00 89.91 9.34
Token-Supervision
300(S) 5h 92.59 86.86 93.33 87.85 7.53
600(S) 10h 93.19 88.13 93.81 89.01 7.41
900(S) 15h 93.53 88.53 94.15 89.33 6.97
Combined 3K(T) + 300(S) 10h 93.49 88.54 94.00 89.21 5.85
Type- and Token-Supervision 3K(T) + 600(S) 15h 93.98 89.27 94.61 89.87 5.59
Table 4: Development test results, where Cost denotes the cost of type- or token-annotation measured
by person hours, ER denotes the error reductions of overall performances brought by self-training, T
denotes type-annotation and S denotes token-annotation.
and then randomly selecting the words that are
not domain-independent for an experienced native
speaker to annotate. To facilitate comparison with
token-supervision, we spent about 5 person hours
in annotating 3,000 word-tag pairs, at about the
same cost as annotating 300 sentences. Finally we
conjoined the 3,000 word-tag pairs with the NR
lexicon, and name the resulting lexicon by 3K.
For the target domain, we mark the words from
both NR and 3K as the domain-specific lexicons.
In all experiments, we use the same domain-
independent lexicon, which is extracted from the
source domain training data by HowNet matching.
The accuracies are shown in Table 4, where
the NR lexicon improved the overall F-score
slightly over the baseline, and the larger lexicon
3K brought more significant improvements. These
experiments agree with the intuition that the size
and the coverage of the tag dictionary is impor-
tant to the accuracies. To understand the extent to
which a lexicon can improve the accuracies, we
perform an oracle test, in which lexicons in the
gold-standard test outputs are included in the dic-
tionary. The accuracy is 88.87%.
4.3.2 Comparing Type-Supervised and
Token-Supervised Domain Adaptation
Table 4 shows that the accuracy improvement by
3,000 annotated word-tag pairs (86.53%) is close
to that by 300 annotated sentences (86.86%). This
suggest that using our method, type-supervised
domain adaptation can be a competitive choice to
the token-supervised methods.
The fact that the token-supervised model gives
slightly better results than our type-annotation
method under similar efforts can probably be ex-
0.6 0.7 0.8 0.9 1
0.6
0.7
0.8
0.9
1
Token-Supervision with 300(S)
T
y
p
e
-
S
u
p
e
r
v
i
s
i
o
n
w
i
t
h
3
K
(
T
)
Figure 2: Sentence accuracy comparisons for
type- and token-supervision with equal cost.
plained by the nature of domain differences. Texts
in the Internet novel are different with CTB news
in not only the vocabulary, but also POS n-gram
distributions. The latter cannot be transferred from
the source-domain training data directly. Texts
from domains such as modern-style novels and
scientific articles might have more similar POS
distributions to the CTB data, and can potentially
benefit more from pure lexicons. We leave the ver-
ification of this intuition to future work.
4.3.3 Making Use of Unlabeled Sentences
Both type- and token-supervised domain adapta-
tion methods can be further improved via unla-
beled target sentences. We apply self-training to
both methods, and find improved results across the
board in Table 4. The results indicate that unla-
beled data is useful in further improving both type-
and token-supervised domain adaptation.
593
Interestingly, the effects of the two methods
on self-training are slightly different. The er-
ror reduction by self-training improves from 6.0%
(baseline) to averaged 7.3% and 8.6% for token-
and type-supervised adaptation, respectively. The
better effect for the type-supervised method may
result from comparatively more uniform cover-
age of the lexicon on sentences, since the target-
domain lexicon is annotated by selecting words
from much more than 300 sentences.
4.3.4 Combined Model of Type- and
Token-Supervision
Figure 2 shows the F-scores of each development
test sentence by type- and token-supervised do-
main adaptation with 5 person hours, respectively.
It indicates that the two methods make different
types of errors, and can potentially be used jointly
for better improvements. We conduct a set of ex-
periments as shown in Table 4, finding that the
combined type- and token-supervised model with
lexicon 3K and 300 labeled sentences achieves
an overall accuracy of 88.54%, exceeding the ac-
curacies of both the type-supervised model with
lexicon 3K and the token-supervised model with
300 labeled sentences. Similar observation can
be found for the combined model with lexicon 3K
and 600 labeled sentences. If combined with self-
training, the same fact can be observed.
More interestingly, the combined model also
exceeds pure type- and token-supervised mod-
els with the same annotation cost. For exam-
ple, the combined model with 3K and 300 la-
beled sentences gives a better accuracy than the
token-supervised model with 600 sentences, with
or without self-training. Similar observations hold
between the combined model with 3K and 600 la-
beled sentences and the token-supervised model
with 900 sentences. The results suggest that the
most cost-effective approach for domain adapta-
tion can be combined type- and token-supervision:
after annotating a set of raw sentences, one could
stop to annotate some words, rather than continu-
ing sentence annotation.
4.4 Final Results
Table 5 shows the final results on test corpus
within ten person hours? annotation. With five per-
son hours (lexicon 3K), the type-supervised model
gave an error reduction of 32.99% compared with
the baseline. The best result was obtained by the
combined type- and token-supervised model, with
SEG POS ER Time
Baseline 87.71 80.81 0.00 0
Baseline+Self-Training 88.62 81.94 5.89 0
Type-Supervision
NR(T) 88.34 82.54 9.02 0
NR(T)+ Self-Training 89.52 83.93 16.26 0
3K(T) 91.11 86.04 27.25 5h
3K(T)+Self-Training 92.11 87.14 32.99 5h
Token-Supervision
300(S) 92.44 86.87 31.58 5h
300(S)+Self-Training 93.24 87.48 34.76 5h
600(S) 93.09 88.05 37.73 10h
600(S)+Self-Training 93.77 88.78 41.53 10h
Combined Type- and Token-Supervision
3K(T)+300(S) 93.27 89.03 42.83 10h
3K(T)+300(S)+Self-Training 93.98 89.84 47.06 10h
Table 5: Final results on test set within ten per-
son hours? annotation, where ER denotes the over-
all error reductions compared with the baseline
model, Time denotes the cost of type- or token-
annotation measured by person hours, T denotes
type-annotation and S denotes token-annotation.
an error reduction of 47.06%, higher than that the
token-supervised model with the same cost under
the same setting (the model of 600 labeled sen-
tences with an error reduction of 41.53%). The
results confirm that the type-supervised model
is a competitive alternative for joint segmenta-
tion and POS-tagging under the cross-domain set-
ting. Combined type- and token-supervised model
yields better results than single models.
5 Related Work
As mentioned in the introduction, tag dictionaries
have been applied to type-supervised POS tagging
of English (Toutanova and Johnson, 2007; Gold-
water and Griffiths, 2007; Ravi and Knight, 2009;
Garrette and Baldridge, 2012), Hebrew (Goldberg
et al., 2008), Kinyarwanda and Malagasy (Gar-
rette and Baldridge, 2013; Garrette et al., 2013),
and other languages (T?ackstr?om et al., 2013).
These methods assume that lexicon can be ob-
tained by manual annotation or semi-supervised
learning, and use the lexicon to induce tag se-
quences on unlabeled sentences. We study type-
supervised Chinese POS-tagging, but under the
setting of domain adaptation. The problem is
how to leverage a target domain lexicon and an
available annotated resources in a different source
domain to improving POS-tagging. Consistent
594
with Garrette et al. (2013), we also find that the
type-supervised method is a competitive choice to
token-supervised adaptation.
There has been a line of work on using graph-
based label propagation to expand tag-lexicons for
POS-tagging (Subramanya et al., 2010; Das and
Petrov, 2011). Similar methods have been ap-
plied to character-level Chinese tagging (Zeng et
al., 2013). We found that label propagation from
neither the source domain nor auto-labeled target
domain sentences can improve domain adaptation.
The main reason could be significant domain dif-
ferences. Due to space limitations, we omit this
negative result in our experiments.
With respect to domain adaptation, existing
methods can be classified into three categories.
The first category does not explicitly model dif-
ferences between the source and target domains,
but use standard semi-supervised learning meth-
ods with labeled source domain data and unla-
beled target domain data (Dai et al., 2007; Raina
et al., 2007). The baseline self-training ap-
proach (Liu and Zhang, 2012) belongs to this cat-
egory. The second considers the differences in the
two domains in terms of features (Blitzer et al.,
2006; Daume III, 2007), classifying features into
domain-independent source domain and target do-
main groups and training these types consistently.
The third considers differences between the dis-
tributions of instances in the two domains, treat-
ing them differently (Jiang and Zhai, 2007). Our
type-supervised method is closer to the second cat-
egory. However, rather than splitting features into
domain-independent and domain-specific types,
we use domain-specific dictionaries to capture do-
main differences, and train a model on the source
domain only. Our method can be treated as an ap-
proach specific to the POS-tagging task.
With respect to Chinese lexical analysis, lit-
tle previous work has been reported on using a
tag dictionary to improve joint segmentation and
POS-tagging. There has been work on using a
lexicon in improving segmentation in a Chinese
analysis pipeline. Peng et al. (2004) used fea-
tures from a set of Chinese words and characters
to improve CRF-based segmentation; Low et al.
(2005) extracted features based on a Chinese lex-
icon from Peking University to help a maximum
segmentor; Sun (2011) collected 12,992 idioms
from Chinese dictionaries, and used them for rule-
based pre-segmentation; Hatori et al. (2012) col-
lected Chinese words from HowNet and the Chi-
nese Wikipedia to enhance segmentation accura-
cies of their joint dependency parsing systems. In
comparison with their work, our lexicon contain
additional POS information, and are used for word
segmentation and POS-tagging simultaneously. In
addition, we separate domain-dependent lexicons
for the source and target lexicons, and use a novel
framework to perform domain adaptation.
Wang et al. (2011) collect word-tag statistics
from automatically labeled texts, and use them as
features to improve POS-tagging. Their word-tag
statistics can be treated as a type of lexicon. How-
ever, their efforts differ from ours in several as-
pects: (1) they focus on in-domain POS-tagging,
while our concern is cross-domain tagging; (2)
they study POS-tagging on segmented sentences,
while we investigate joint segmentation and POS-
tagging for Chinese; (3) their tag-dictionaries are
not tag-dictionaries literally, but statistics of word-
tag associations.
6 Conclusions
We performed an empirical study on the use of
tag-dictionaries for the domain adaptation of joint
Chinese segmentation and POS-tagging, showing
that type-supervised methods can be a compet-
itive alternative to token-supervised methods
in cost-effectiveness. In addition, combination
of the two methods gives the best cost-effect.
Finally, we release our annotation of over 4,000
sentences in the Internet literature domain on-
line at http://faculty.sutd.edu.sg/
?
yue_zhang/eacl14meishan.zip as a
free resource for Chinese POS-tagging.
Acknowledgments
We thank the anonymous reviewers for their con-
structive comments. We gratefully acknowl-
edge the support of the National Key Basic
Research Program (973 Program) of China via
Grant 2014CB340503 and the National Natural
Science Foundation of China (NSFC) via Grant
61133012 and 61370164, the National Basic Re-
search Program (973 Program) of China via Grant
2014CB340503, the Singaporean Ministration of
Education Tier 2 grant T2MOE201301 and SRG
ISTD 2012 038 from Singapore University of
Technology and Design.
595
References
Michele Banko and Robert C. Moore. 2004. Part-of-
speech tagging in context. In COLING.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 120?128, Sydney, Australia, July.
Association for Computational Linguistics.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsu-
pervised POS induction: How far have we come?
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
575?584, Cambridge, MA, October. Association for
Computational Linguistics.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 111?118, Barcelona, Spain, July.
Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong
Yu. 2007. Transferring Naive Bayes Classifiers for
Text Classification. In AAAI, pages 540?545.
Dipanjan Das and Slav Petrov. 2011. Unsuper-
vised part-of-speech tagging with bilingual graph-
based projections. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
600?609, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Zhendong Dong and Qiang Dong. 2006. Hownet And
the Computation of Meaning. World Scientific Pub-
lishing Co., Inc., River Edge, NJ, USA.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden markov models for part-of-speech
tagging with incomplete tag dictionaries. In
EMNLP-CoNLL, pages 821?831.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 138?147, Atlanta, Georgia, June. Association
for Computational Linguistics.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-world semi-supervised learning of pos-
taggers for low-resource languages. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 583?592, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-taggers
(when given a good start). In Proceedings of ACL-
08: HLT, pages 746?754, Columbus, Ohio, June.
Association for Computational Linguistics.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 744?751, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2012. Incremental joint approach
to word segmentation, pos tagging, and dependency
parsing in chinese. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1045?
1053, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 513?521,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Yang Liu and Yue Zhang. 2012. Unsupervised domain
adaptation for joint segmentation and POS-tagging.
In Proceedings of COLING 2012: Posters, pages
745?754, Mumbai, India, December. The COLING
2012 Organizing Committee.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A maximum entropy approach to chinese word seg-
mentation. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, pages
161?164.
Christopher D. Manning. 2011. Part-of-speech tag-
ging from 97% to 100%: is it time for some linguis-
tics? In Proceeding of CICLing?11.
Bernard Merialdo. 1994. Tagging english text with
a probabilistic model. Computational Linguistics,
20(2).
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 277?284, Barcelona, Spain, July. Association
for Computational Linguistics.
596
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. In Proceedings
of Coling 2004, pages 562?568, Geneva, Switzer-
land, Aug 23?Aug 27. COLING.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC, May.
Rajat Raina, Alexis Battle, Honglak Lee, Benjamin
Packer, and Andrew Y. Ng. 2007. Self-taught learn-
ing: transfer learning from unlabeled data. In ICML,
pages 759?766.
Sujith Ravi and Kevin Knight. 2009. Minimized
models for unsupervised part-of-speech tagging. In
ACL/IJCNLP, pages 504?512.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-
supervised learning of structured tagging models.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
167?176, Cambridge, MA, October. Association for
Computational Linguistics.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1385?
1394, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, McDon-
ald Ryan, and Joakim Nivre. 2013. Token and type
constraints for cross-lingual part-of-speech tagging.
In Transactions of the ACL. Association for Compu-
tational Linguistics, March.
Partha Pratim Talukdar and Koby Crammer. 2009.
New regularized algorithms for transductive learn-
ing. In ECML/PKDD (2), pages 442?457.
Kristina Toutanova and Mark Johnson. 2007. A
bayesian lda-based model for semi-supervised part-
of-speech tagging. In NIPS.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmentation
and pos tagging with semi-supervised methods using
large auto-analyzed data. In Proceedings of 5th In-
ternational Joint Conference on Natural Language
Processing, pages 309?317, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.
Nianwen Xue, Fei Xia, Shizhe Huang, and Tony Kroch.
2000. The bracketing guidelines for the chinese
treebank. Technical report, University of Pennsyl-
vania.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and Is-
abel Trancoso. 2013. Graph-based semi-supervised
model for joint chinese word segmentation and part-
of-speech tagging. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 770?
779, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of ACL-08: HLT, pages 888?
896, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 843?852, Cambridge,
MA, October. Association for Computational Lin-
guistics.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
597
Syntactic Processing Using the Generalized
Perceptron and Beam Search
Yue Zhang?
University of Cambridge
Stephen Clark??
University of Cambridge
We study a range of syntactic processing tasks using a general statistical framework that consists
of a global linear model, trained by the generalized perceptron together with a generic beam-
search decoder. We apply the framework to word segmentation, joint segmentation and POS-
tagging, dependency parsing, and phrase-structure parsing. Both components of the framework
are conceptually and computationally very simple. The beam-search decoder only requires the
syntactic processing task to be broken into a sequence of decisions, such that, at each stage in
the process, the decoder is able to consider the top-n candidates and generate all possibilities
for the next stage. Once the decoder has been defined, it is applied to the training data, using
trivial updates according to the generalized perceptron to induce a model. This simple framework
performs surprisingly well, giving accuracy results competitive with the state-of-the-art on all
the tasks we consider.
The computational simplicity of the decoder and training algorithm leads to significantly
higher test speeds and lower training times than their main alternatives, including log-linear
and large-margin training algorithms and dynamic-programming for decoding. Moreover, the
framework offers the freedom to define arbitrary features which can make alternative training
and decoding algorithms prohibitively slow. We discuss how the general framework is applied to
each of the problems studied in this article, making comparisons with alternative learning and
decoding algorithms. We also show how the comparability of candidates considered by the beam
is an important factor in the performance. We argue that the conceptual and computational sim-
plicity of the framework, together with its language-independent nature, make it a competitive
choice for a range of syntactic processing tasks and one that should be considered for comparison
by developers of alternative approaches.
1. Introduction
In this article we study a range of syntactic processing tasks using a general framework
for structural prediction that consists of the generalized perceptron (Collins 2002) and
? University of Cambridge Computer Laboratory, William Gates Building, 15 JJ Thomson Avenue,
Cambridge, UK. E-mail: yue.zhang@cl.cam.ac.uk.
?? University of Cambridge Computer Laboratory, William Gates Building, 15 JJ Thomson Avenue,
Cambridge, UK. E-mail: stephen.clark@cl.cam.ac.uk.
Submission received: 10 November 2009; revised submission received: 12 August 2010; accepted for
publication: 20 September 2010.
? 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 1
beam-search. We show that the framework, which is conceptually and computationally
simple, is practically effective for structural prediction problems that can be turned into
an incremental process, allowing accuracies competitive with the state-of-the-art to be
achieved for all the problems we consider.
The framework is extremely flexible and easily adapted to each task. One advan-
tage of beam-search is that it does not impose any requirements on the structure of
the problem, for example, the optimal sub-problem property required for dynamic-
programming, and can easily accommodate non-local features. The generalized per-
ceptron is equally flexible, relying only on a decoder for each problem and using a
trivial online update procedure for each training example. An advantage of the linear
perceptron models we use is that they are global models, assigning a score to a complete
hypothesis for each problem rather than assigning scores to parts which are then com-
bined under statistical independence assumptions. Here we are following a recent line
of work applying global discriminative models to tagging and wide-coverage parsing
problems (Lafferty, McCallum, and Pereira 2001; Collins 2002; Collins and Roark 2004;
McDonald, Crammer, and Pereira 2005; Clark and Curran 2007; Carreras, Collins, and
Koo 2008; Finkel, Kleeman, and Manning 2008).
The flexibility of our framework leads to competitive accuracies for each of the tasks
we consider. For word segmentation, we show how the framework can accommodate
a word-based approach, rather than the standard and more restrictive character-based
tagging approaches. For POS-tagging, we consider joint segmentation and POS-tagging,
showing that a single beam-search decoder can be used to achieve a significant accuracy
boost over the pipeline baseline. For Chinese and English dependency parsing, we
show how both graph-based and transition-based algorithms can be implemented as
beam-search, and then combine the two approaches into a single model which out-
performs both in isolation. Finally, for Chinese phrase-structure parsing, we describe a
global model for a shift-reduce parsing algorithm, in contrast to current deterministic
approaches which use only local models at each step of the parsing process. For all these
tasks we present results competitive with the best results in the literature.
In Section 2 we describe our general framework of the generic beam-search algo-
rithm and the generalized perceptron. Then in the subsequent sections we describe
each task in turn, based on conference papers including Zhang and Clark (2007, 2008a,
2008b, 2009, 2010), presented in our single coherent framework. We give an updated
set of results, plus a number of additional experiments which probe further into the
advantages and disadvantages of our framework. For the segmentation task, we also
compare our beam-search framework with alternative decoding algorithms including
an exact dynamic-programming method, showing that the beam-search method is sig-
nificantly faster with comparable accuracy. For the joint segmentation and POS-tagging
task, we present a novel solution using the framework in this article, and show that
it gives comparable accuracies to our previous work (Zhang and Clark 2008a), while
being more than an order of magnitude faster.
In Section 7 we provide further discussion of the framework based on the studies
of the individual tasks. We present the main advantages of the framework, and give an
analysis of the main reasons for the high speeds and accuracies achieved. We also dis-
cuss how this framework can be applied to a potential new task, and show that the com-
parability of candidates in the incremental process is an important factor to consider.
In summary, we study a general framework for incremental structural prediction,
showing how the framework can be tailored to a range of syntactic processing problems
to produce results competitive with the state-of-the-art. The conceptual and compu-
tational simplicity of the framework, together with its language-independent nature,
106
Zhang and Clark Syntactic Processing
make it a competitive choice that should be considered for comparison by developers
of alternative approaches.
2. The Decoding and Training Framework
The framework we study in this article addresses the general structural prediction
problem of mapping an input structure x ? X onto an output structure y ? Y, where
X is the set of possible inputs, and Y is the set of possible outputs. For example, for
the problem of Chinese word segmentation, X is the set of raw Chinese sentences and
Y is the set of all possible segmented Chinese sentences. For the problem of English
dependency parsing, X is the set of all English sentences and Y is the set of all possible
English dependency trees.
Given an input sentence x, the output F(x) is defined as the highest scored among
the possible output structures for x:
F(x) = arg max
y?GEN(x)
Score(y) (1)
where GEN(x) denotes the set of possible outputs for an input sentence x, and Score(y)
is some real-valued function on Y.
To compute Score(y), the output structure y is mapped into a global feature vector
?(y) ? N d. Here a feature is a count of the occurrences of a certain pattern in an output
structure, extracted according to a set of feature templates, and d is the total number of
features. The term global feature vector is used by Collins (2002) to distinguish between
feature counts for whole sequences and the local feature vectors in maximum entropy
tagging models, which are boolean-valued vectors containing the indicator features for
one element in the sequence (Ratnaparkhi 1998). Having defined the feature vector,
Score(y) is computed using a linear model:
Score(y) = ?(y) ? ~w (2)
where ~w ? Rd is the parameter vector of the model, the value of which is defined by
supervised learning using the generalized perceptron.
For the general framework we study in this article, the output y is required to be
built through an incremental process. Suppose that K incremental steps are taken in
total to build y. The incremental change at the ith step (0 < i ? K) can be written as
?(y, i). For word segmentation, ?(y, i) can be an additional character added to the output;
for shift-reduce parsing, ?(y, i) can be an additional shift-reduce action. Denoting the
change to the global feature vector at the incremental step as ?(?(y, i)), the global feature
vector ?(y) can be written as ?(y) =?Ki=1 ?(?(y, i)). Hence, Score(y) can be computed
incrementally by
Score(y) =
K?
i=1
?(?(y, i)) ? ~w (3)
In the following sections, we describe the two major components of the gen-
eral framework, that is, the general beam-search algorithm for finding F(x) =
arg maxy?GEN(x) Score(y) for a given x, and the generalized perceptron for training ~w.
107
Computational Linguistics Volume 37, Number 1
2.1 Beam-Search Decoding
Given an input x, the output structure y is built incrementally. At each step, an incre-
mental sub-structure is added to the partially built output. Due to structural ambiguity,
different sub-structures can be built. Taking POS-tagging for example, the incremental
sub-structure for each processing step can be a POS-tag assigned to the next input word.
Due to structural ambiguity, different POS-tags can be assigned to a word, and the
decoding algorithm searches for the particular path of incremental steps which builds
the highest scored output.
We present a generic beam-search algorithm for our decoding framework, which
uses an agenda to keep the B-best partial outputs at each incremental step. The partially
built structures, together with useful additional information, are represented as a set of
state items. Additional information in a state item is used by the decoder to organize
the current structures or keep a record of the incremental process. For POS-tagging
it includes the remaining input words yet to be assigned POS-tags; for a shift-reduce
parser, it includes the stack structure for the shift-reduce process and the incoming
queue of unanalyzed words.
The agenda is initialized as empty, and the state item that corresponds to the initial
structure is put onto it before decoding starts. At each step during decoding, each state
item from the agenda is extended with one incremental step. When there are multiple
choices to extend one state item, multiple new state items are generated. The new state
items generated at a particular step are ranked by their scores, and the B-best are put
back onto the agenda. The process iterates until a stopping criterion is met, and the
current best item from the agenda is taken as the output.
Pseudo code for the generic beam-search algorithm is given in Figure 1, where the
variable problem represents a particular task, such as word segmentation or dependency
parsing, and the variable candidate represents a state item, which has a different defini-
tion for each task. For example, for the segmentation task, a candidate is a pair, consisting
of the partially segmented sentence and the remaining character sequence yet to be
segmented. The agenda is an ordered list, used to keep all the state items generated at
each stage, ordered by score. The variable candidates is the set of state items that can be
used to generate new state items, that is, the B-best state items from the previous stage.
B is the number of state items retained at each stage.
function BEAM-SEARCH(problem, agenda, candidates, B)
candidates ? {STARTITEM(problem)}
agenda ? CLEAR(agenda)
loop do
for each candidate in candidates
agenda ? INSERT(EXPAND(candidate, problem), agenda)
best ? TOP(agenda)
if GOALTEST(problem, best)
then return best
candidates ? TOP-B(agenda, B)
agenda ? CLEAR(agenda)
Figure 1
The generic beam-search algorithm.
108
Zhang and Clark Syntactic Processing
STARTITEM initializes the start state item according to the problem; for example,
for the segmentation task, the start state item is a pair consisting of an empty seg-
mented sentence and the complete sequence of characters waiting to be segmented.
CLEAR removes all items from the agenda. INSERT puts one or more state items onto
the agenda. EXPAND represents an incremental processing step, which takes a state
item and generates new state items from it in all possible ways; for example, for the
segmentation task, EXPAND takes the partially segmented sentence in a state item,
and extends it in all possible ways using the first character in the remaining character
sequence in the state item. TOP returns the highest scoring state item on the agenda.
GOALTEST checks whether the incremental decoding process is completed; for example,
for the segmentation task, the process is completed if the state item consists of a fully
segmented sentence and an empty remaining character sequence. TOP-B returns the B-
highest scoring state items on the agenda, which are used for the next incremental step.
State items in the agenda are ranked by their scores. Suppose that K incremental
steps are taken in total to build an output y. At the ith step (0 < i ? K), a state item in
the agenda can be written as candidatei, and we have
Score(candidatei) =
i?
n=1
?(?(candidatei, n)) ? ~w
Features for a state item can be based on both the partially built structure and the
additional information we mentioned earlier.
The score of a state item can be computed incrementally as the item is built. The
score of the start item is 0. At the ith step (0 < i ? K), a state item candidatei is generated
by extending an existing state item candidatei?1 on the agenda with ?(candidatei, i). In
this case, we have
Score(candidatei) = Score(candidatei?1) +?(?(candidatei, i)) ? ~w
Therefore, when a state item is extended, its score can be updated by adding the
incremental score of the step ?(?(candidatei, i)) ? ~w. The nature of the scoring function
means that, given appropriately defined features, it can be computed efficiently for both
the incremental decoding and training processes.
Because the correct item can fall out of the agenda during the decoding process, the
general beam-search framework is an approximate decoding algorithm. Nevertheless,
empirically this algorithm gives competitive results on all the problems in this article.
2.2 The Generalized Perceptron
The perceptron learning algorithm is a supervised training algorithm. It initializes the
parameter vector as all zeros, and updates the vector by decoding the training examples.
For each example, the output structure produced by the decoder is compared with
the correct structure. If the output is correct, no update is performed. If the output
is incorrect, the parameter vector is updated by adding the global feature vector of
the training example and subtracting the global feature vector of the decoder output.
Intuitively, the training process is effectively coercing the decoder to produce the correct
output for each training example. The algorithm can perform multiple passes over the
same training sentences. In all experiments, we decide the number of training iterations
using a set of development test data, by choosing the number that gives the highest
109
Computational Linguistics Volume 37, Number 1
Inputs: training examples (xi, yi)
Initialization: set ~w = 0
Algorithm:
for r = 1..P, i = 1..N
calculate zi = decode(xi)
if zi 6= yi
~w = ~w +?(yi) ? ?(zi)
Outputs: ~w
Figure 2
The generalized perceptron algorithm, adapted from Collins (2002).
development test accuracy as the final number in testing. Figure 2 gives the algorithm,
where N is the number of training sentences and P is the number of passes over the data.
The averaged perceptron algorithm (Collins 2002) is a standard way of reducing
overfitting on the training data. It was motivated by the voted-perceptron algorithm
(Freund and Schapire 1999) and has been shown to give improved accuracy over the
non-averaged perceptron on a number of tasks. Let N be the number of training sen-
tences, P the number of training iterations, and ~wi,r the parameter vector immediately
after the ith sentence in the rth iteration. The averaged parameter vector ~? ? Rd is
defined as
~? = 1PN
?
i=1..N,r=1..P
~wi,r
and it is used instead of ~w as the model parameters. We use the averaged perceptron for
all the tasks we consider.
We also use the early-update strategy of Collins and Roark (2004), which is a
modified version of the perceptron algorithm specifically for incremental decoding
using beam search. At any step during the decoding process to calculate zi, if all partial
candidates in the agenda are incorrect, decoding is stopped and the parameter vector is
updated according to the current best candidate in the agenda and the corresponding
gold-standard partial output. To perform early-update, the decoder needs to keep a
version of the correct partial output for each incremental step, so that the parameter
values are adjusted as soon as the beam loses track of the correct state item. The intuition
is to force the beam to keep the correct state item at every incremental step, rather than
learning only the correct overall structure. This strategy has been shown to improve the
accuracy over the original perceptron for beam-search decoding.
In summary, our general framework consists of a global linear model, which is
trained by the averaged perceptron, and a beam-search decoder. When applied to a
particular task, the structure of a state item as well as some of the functions in the
decoder need to be instantiated. In the following sections, we show how the general
framework can be applied to Chinese word segmentation, joint segmentation and POS-
tagging, Chinese and English dependency parsing, and Chinese constituent parsing.
3. Word Segmentation
Chinese word segmentation (CWS) is the problem of finding word boundaries for
Chinese sentences, which are written as continuous character sequences. Other lan-
guages, including Japanese and Thai, also have the problem of word segmentation, and
typical statistical models for CWS can also be applied to them.
110
Zhang and Clark Syntactic Processing
Word segmentation is a problem of ambiguity resolution, often requiring knowl-
edge from a variety of sources. Out-of-vocabulary (OOV) words are a major source of
ambiguity. For example, a difficult case occurs when an OOV word consists of characters
which have themselves been seen as words; here an automatic segmentor may split the
OOV word into individual single-character words. Typical examples of unseen words
include Chinese names, translated foreign names, and idioms.
The segmentation of known words can also be ambiguous. For example,
should be (here) (flour) in the sentence (flour and rice are ex-
pensive here) or (here) (inside) in the sentence (it?s cold inside
here). The ambiguity can be resolved with information about the neighboring words. In
comparison, for the sentence , possible segmentations include (the
discussion) (will) (very) (be successful) and (the discussion meeting)
(very) (be successful). The ambiguity can only be resolved with contextual infor-
mation outside the sentence. Human readers often use semantics, contextual informa-
tion about the document, and world knowledge to resolve segmentation ambiguities.
There is no fixed standard for Chinese word segmentation. Experiments have
shown that there is only about 75% agreement among native speakers regarding the
correct word segmentation (Sproat et al 1996). Also, specific NLP tasks may require
different segmentation criteria. For example, could be treated as a single
word (Bank of Beijing) for machine translation, although it is more naturally segmented
into (Beijing) (bank) for tasks such as text-to-speech synthesis. Therefore,
supervised learning with specifically defined training data has become the dominant
approach.
Following Xue (2003), the standard approach for building a statistical CWS model
is to treat CWS as a sequence labeling task. A tag is assigned to each character in the
input sentence, indicating whether the character is a single-character word or the start,
middle, or end of a multi-character word. The context for disambiguation is normally a
five-character window with the current character in the middle. We call these methods
character-based word segmentation. The advantage of character-based segmentation is
that well-known tagging approaches can be applied directly to the CWS problem.
There are various character-based models in the literature. They differ mainly in the
learning algorithm and the features used. Several discriminative learning algorithms
have been applied to the character-based systems. Examples include Xue (2003), Peng,
Feng, and McCallum (2004), and Wang et al (2006), which use maximum entropy and
conditional random field models, and Jiang et al (2008), which uses the perceptron
model. The standard feature set is that defined by Ng and Low (2004), though other
feature sets are reported to improve the accuracy (Zhao, Huang, and Li 2006). Zhao,
Huang, and Li (2006) also showed that the best accuracy for conditional random field
(CRF) models is given by using a set of six character segmentation tags, rather than
the standard set {beginning, middle, end, single} shown previously. Standard search
algorithms for sequence tagging have been applied to the decoding process, such as
the dynamic-programming algorithm and beam-search.
A disadvantage of character-based models is the use of limited contextual infor-
mation. For these methods, context is confined to the neighboring characters. Other
contextual information, in particular the surrounding words, is not included. Consider
the sentence , which can be from (among which) (foreign)
(companies), or (in China) (foreign companies) (business). Note that the
five-character window surrounding is the same in both cases, making the tagging
decision for that character difficult given the local window. The correct decision can be
made, however, by comparing the two three-word windows containing this character.
111
Computational Linguistics Volume 37, Number 1
In Zhang and Clark (2007) we proposed a word-based approach to segmentation,
which provides a direct solution to the problem. In comparison with the character-based
approach, our segmentor does not map the CWS problem into sequence labeling. By
using a global linear model, it addresses the segmentation problem directly, extracting
word-based features from the output segmented structure. Hence we call our word
segmentation model the word-based approach. In fact, word-based segmentors can be
seen as a generalization of character-based segmentors, because any character-based
features can be defined in a word-based model.
In the following sections, we describe a word-based segmentor using the general
framework of this article, which is slightly different from the original system we pro-
posed in Zhang and Clark (2007). We compare the accuracies of this segmentor and
the 2007 segmentor, and report a set of improved results for our 2007 segmentor using
a better method to optimize the number of training iterations. We then study alterna-
tive decoders to the general framework, including a Viterbi inference algorithm and a
multiple-beam search algorithm, and provide discussion on the general framework and
word-based segmentation.
3.1 Instantiating the General Framework
In this section we formulate our word-based segmentor as an instance of the general
framework of this article. Our segmentor builds a candidate segmentation incremen-
tally, one character at a time. When each character is processed, it is either combined
with the last word of the partial candidate that has been built so far, or added to the
candidate as the start of a new word. The same process repeats for each input character,
and therefore runs in linear time.
For ambiguity resolution, we use a beam-search decoding algorithm to explore the
search space. Initially containing only an empty sentence, an agenda is used to keep a
set of candidate items for each processing step. When an input character is processed,
it is combined with each candidate in the agenda in the two aforementioned ways, and
two new candidates are generated. At the end of each step, the B-best newly generated
candidates are kept in the agenda for the next processing step. When the input sentence
is exhausted, the top candidate from the agenda is taken as the output.
This decoding process can be expressed as an instance of the generic algorithm in
Figure 1. For the word segmentation problem, a state item in the algorithm is a pair
?S, Q?, where S contains part of the input that has been segmented, and Q contains
the rest of the input sentence as a queue of incoming characters. The initial state item
STARTITEM(word segmentation) contains an empty sentence, and an incoming queue of
the whole input sentence. EXPAND(candidate, word segmentation) pops the first character
from the incoming queue, and adds it to the partial segmented sentence in candidate in
two different ways to generate two new state items: It either appends the character to
the last word in the sentence or joins it as the start of a new word in the sentence. Finally,
GOALTEST(word segmentation, best) returns true if best contains a fully segmented input
sentence, and therefore an empty incoming queue, and false otherwise.
The score of a segmented sentence is computed by the global linear model in
Equation (2), where the parameter vector ~w for the model is computed by the early-
update version of the perceptron training algorithm described in Section 2.2. Our word
segmentor computes the global feature vector ?(y) incrementally according to Equa-
tion (3), where for the ith character, ?(?(y, i)) is computed using the feature templates
in Table 1, according to whether the character is appended to or separated from its
previous character.
112
Zhang and Clark Syntactic Processing
Table 1
Feature templates for the word segmentor.
Feature template When c0 is
1 w?1 separated
2 w?1w?2 separated
3 w?1, where len(w?1) = 1 separated
4 start(w?1)len(w?1) separated
5 end(w?1)len(w?1) separated
6 end(w?1)c0 separated
7 c?1c0 appended
8 begin(w?1)end(w?1) separated
9 w?1c0 separated
10 end(w?2)w?1 separated
11 start(w?1)c0 separated
12 end(w?2)end(w?1) separated
13 w?2len(w?1) separated
14 len(w?2)w?1 separated
w = word; c = character. The index of the current character is 0.
3.2 Comparisons with Zhang and Clark (2007)
Both the segmentor of this article and our segmentor of Zhang and Clark (2007) use
a global linear model trained discriminatively using the perceptron. However, when
comparing state items in the agenda, our 2007 segmentor treated full words in the
same way as partial words, scoring them using the same feature templates. This scoring
mechanism can potentially have a negative effect on the accuracy. In this article, we take
a different strategy and apply full-word feature templates only when the next input
character is separated from the word. In fact, most of the feature templates in Table 1
are related to full word information, and are applied when separating the next character.
This method thus gives a clear separation of partial word and full word information. We
also applied early-update in this article, so that the training process is closely coupled
with beam-search decoding. In Zhang and Clark (2007) we performed the standard
global discriminative learning.
3.3 Combining Word-Based and Character-Based Segmentation
As stated earlier, a character-based segmentor maps the segmentation problem into
a sequence labeling problem, where labels are assigned to each input character to
represent its segmentation. Our word-based approach does not map the segmentation
problem into a labeling task but solves it directly. In this article, we further show that the
flexibility of the word-based approach, enabled by our general framework, allows the
combination of a character-based sub-system into our word-based system. The intuition
is simple: Both perceptron learning and beam-search decoding allow arbitrary features,
and therefore features from a typical character-based system can be incorporated into
our segmentor to provide further information. Though character-based segmentors can
also leverage word-level features indirectly, the labeling nature prevents them from
direct use of word information.
We follow the convention of character-based segmentation, and define the set of
segmentation tags as {B, E, M, S}. The tags B, E, M represent the character being the
113
Computational Linguistics Volume 37, Number 1
beginning, end, and middle of a multiple-character word, respectively, and the tag S
represents the character being a single-character word.
The character-based features that we incorporate into our segmentor are shown
in Table 2, which consist of unigram, bigram, and trigram information in the three-
character window surrounding the current character, paired with the segmentation tag
of the current character. To distinguish this system from our system without combina-
tion of character-based information, we call our segmentor in Section 3.1 the pure word-
based segmentor and the segmentor that uses character-based features the combined
segmentor in our experimental sections.
3.4 Experiments
We performed two sets of experiments. In the first set of experiments, we used the
Chinese Treebank (CTB) data to study the speed/accuracy tradeoff by varying the
size of the beam. In the second set of experiments, we used training and testing sets
from the first and second international Chinese word segmentation bakeoffs (Sproat
and Emerson 2003; Emerson 2005) to compare the accuracies to other models in the
literature, including our segmentor of Zhang and Clark (2007).
F-score is used as the accuracy measure: 2pr/(p + r), where precision p is the per-
centage of words in the decoder output that are segmented correctly, and recall r is the
percentage of gold-standard output words that are correctly segmented by the decoder.
CWS systems are evaluated by two types of tests. The closed tests require that the
system is trained only with a designated training corpus. Any extra knowledge is not
allowed, including common surnames, Chinese and Arabic numbers, European letters,
lexicons, parts-of-speech, semantics, and so on. The open tests do not impose such
restrictions. Open tests measure a model?s capability to utilize extra information and
domain knowledge, which can lead to improved performance, but because this extra
information is not standardized, direct comparison between open test results is less
informative. In this article, we focus only on the closed test.
3.4.1 Speed/Accuracy Tradeoff. We split CTB5 into training, development test, and test
sets as shown in Table 3, where the development test data are used to determine the
number of training iterations, which are used to obtain the final accuracies on the test
data. We measure the accuracies on the test data with various beam-sizes, and plot
the speed/accuracy tradeoff graph in Figure 3. Each point in the figure, from right to
left, corresponds to beam size B = 1, 2, 4, 8, 16, 32, and 64, respectively. Speed is mea-
sured in the number of thousand characters per second, and accuracy is calculated using
F-score.
As the size of the beam increases, the speed of the segmentor decreases. Because
a larger part of the search is explored with an increased beam size, the accuracy of
Table 2
Feature templates of a typical character-based word segmentor.
Feature template When c0 is
1 cis0, i ? {?1, 0, 1} separated, appended
2 ci?1cis0, i ? 0, 1 separated, appended
3 c?1c0c1s0 separated, appended
c = character; s = segmentation tag. The index of the current character is 0.
114
Zhang and Clark Syntactic Processing
Table 3
Training, development, and test data for word segmentation on CTB5.
Sections Sentences Words
Training 1?270, 400?931, 1001?1151 18,085 493,892
Dev 301?325 350 6,821
Test 271?300 348 8,008
the decoder has the potential to increase. This explains the increased accuracies when
B increases from 1 to 16. However, the amount of increase drops when the beam size
increases.
3.4.2 Closed Test on the SIGHAN Bakeoffs. Four training and testing corpora were used in
the first bakeoff (Sproat and Emerson 2003), including the Academia Sinica Corpus (AS),
the Penn Chinese Treebank Corpus (CTB), the Hong Kong City University Corpus (CU),
and the Peking University Corpus (PU). However, because the testing data from the
Penn Chinese Treebank Corpus is currently unavailable to us, we excluded this corpus
from our experiments. The corpora are encoded in GB (PU, CTB) and BIG5 (AS, CU).
In order to test them consistently in our system, they are all converted to UTF8 without
loss of information.
The results are shown in Table 4. We follow the format from Peng, Feng, and
McCallum (2004), where each row represents a CWS model. The first three columns
represent tests with the AS, CU, and PU corpora, respectively. The best score in each
column is shown in bold. The last two columns represent the average accuracy of each
model over the tests it participated in (SAV), and our average over the same tests (OAV),
respectively. The first eight rows represent models from Sproat and Emerson (2003) that
participated in at least one closed test from the table, row ?Peng? represents the CRF
model from Peng, Feng, and McCallum (2004), row ?Zhang 2007? represents our model
as reported in Zhang and Clark (2007), and the last two rows represent our model in
this article, using only word-based features in Table 1 and combined features in Tables 1
plus 2, respectively.
In Zhang and Clark (2007) we fixed the number of training iterations to six for
all experiments, according to a separate set of development data. An alternative way
to decide the number of training iterations is to set apart 10% from the training data
Figure 3
Speed/accuracy tradeoff of the segmentor.
115
Computational Linguistics Volume 37, Number 1
Table 4
The accuracies of various word segmentors over the first SIGHAN bakeoff data.
AS CU PU SAV OAV
S01 93.8 90.1 95.1 93.0 95.5
S04 93.9 93.9 94.8
S05 94.2 89.4 91.8 95.9
S06 94.5 92.4 92.4 93.1 95.5
S08 90.4 93.6 92.0 94.8
S09 96.1 94.6 95.4 95.9
S10 94.7 94.7 94.8
S12 95.9 91.6 93.8 95.9
Peng 95.6 92.8 94.1 94.2 95.5
Zhang 2007 96.5 94.6 94.0 95.0 95.5
Zhang 2007* 96.9 94.6 94.1 95.2 95.5
this article pure 97.0 94.6 94.6 95.4 95.5
this article combined 96.9 94.8 94.8
The best score in each column and the best average in each row is in boldface.
*Zhang 2007 with the (Carreras, Surdeanu, and Marquez 2006) method applied (see text for
details).
as development test data, and use the rest for development training. For testing, all
training data are used for training, with the number of training iterations set to be the
number which gave the highest accuracy during the development experiments. This
method was used by Carreras, Surdeanu, and Marquez (2006) in their parsing model.
We apply it to our segmentor model in this article. Moreover, we also use this method
to decide the number of training iterations for our system of Zhang and Clark (2007),
and show the accuracies in row ?Zhang 2007*?.
For each row the best average is shown in bold. We achieved the best accuracy in all
three corpora, and better overall accuracy than all the other models using the method
of this article. Our new method to decide the number of training iterations also gave
improved accuracies compared to our 2007 model. The combination of character-based
features and our original word-based features gave slight improvement in the overall
accuracy.
Four training and testing corpora were used in the second bakeoff (Emerson 2005),
including the Academia Sinica corpus (AS), the Hong Kong City University Corpus
(CU), the Peking University Corpus (PK), and the Microsoft Research Corpus (MR).
Different encodings were provided, and the UTF8 data for all four corpora were used
in our experiments.
Following the format of Table 4, the results for this bakeoff are shown in Table 5. We
chose the three models that achieved at least one best score in the closed tests from
Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita
(2006) for comparison. Row ?Zh-a? and ?Zh-b? represent the pure sub-word CRF model
and the confidence-based combination of the CRF and rule-based models, respectively.
Again, our model achieved better overall accuracy than all the other models. The
combination of character-based features improved the accuracy slightly again.
116
Zhang and Clark Syntactic Processing
Table 5
The accuracies of various word segmentors over the second SIGHAN bakeoff data.
AS CU PK MR SAV OAV
S14 94.7 94.3 95.0 96.4 95.1 95.6
S15b 95.2 94.1 94.1 95.8 94.8 95.6
S27 94.5 94.0 95.0 96.0 94.9 95.6
Zh-a 94.7 94.6 94.5 96.4 95.1 95.6
Zh-b 95.1 95.1 95.1 97.1 95.6 95.6
Zhang 2007 94.6 95.1 94.5 97.2 95.4 95.6
Zhang 2007* 95.0 95.1 94.6 97.3 95.5 95.6
This article pure 95.1 95.2 94.4 97.3 95.5 95.6
This article combined 95.4 95.1 94.4 97.3
The best score in each column and the best average in each row is in boldface.
*Zhang 2007 with the (Carreras, Surdeanu, and Marquez 2006) method applied (see text for
details).
3.5 Alternative Decoding Algorithms
Besides the general framework of this article, there are various alternative learning and
decoding algorithms for a discriminative linear model applied to the word segmenta-
tion problem, using the same feature templates we defined. In this section, we study
two alternative decoding algorithms to the beam-search decoder, including a multiple-
beam search algorithm, which can be viewed as an alternative decoder specifically
designed for the word segmentation and joint segmentation and tagging problems, and
a dynamic-programming algorithm. Both algorithms explore a larger part of the search
space than the single beam-search algorithm, and we compare the accuracy and speed
of these algorithms within the generalized perceptron learning framework.
3.5.1 A Multiple-Beam Search Decoder. In Zhang and Clark (2008a) we proposed a
multiple-beam decoder for the problem of joint word segmentation and POS-tagging,
in which state items only contain complete words. This algorithm can be naturally
adapted for word segmentation. Compared with the single-beam decoder, it explores a
larger fraction of the search space. Moreover, the multiple-beam decoder does not have
the problem of comparing partial words with full words in a single agenda, which our
segmentor of Zhang and Clark (2007) has. We implement this decoder for segmentation
and compare its accuracies with our single-beam decoder.
Instead of a single agenda, the multiple-beam algorithm keeps an agenda for each
character in the input sentence, recording the best partial candidates ending with the
character. Like the single beam decoder, the input sentence is processed incremen-
tally. However, at each stage, partial sequence candidates are available at all previous
characters. Therefore, the decoder can examine all candidate words ending with the
current character. These possible words are combined with the relevant partial candi-
dates from the previous agendas to generate new candidates, which are then inserted
into the agenda for the current character. The output of the decoder is the top candidate
in the last agenda, representing the best segmentation for the whole sentence. The
117
Computational Linguistics Volume 37, Number 1
multiple-beam search decoder explores a larger number of candidate outputs compared
to the single-beam search. To improve the running speed, a maximum word length
record is kept to limit the length of candidate words.
Because the multiple-beam decoder can also be applied to the joint segmentation
and POS-tagging problem in Section 4, we describe this algorithm by extending the
generic beam-search algorithm in Figure 1. Three modifications are made to the original
algorithm, shown in Figure 4. First, the B-best candidates generated in each processing
step are kept in prev topBs. Here prev topBs can be seen as a list of the candidates in the
original search algorithm, with prev topBs[k] containing the current items with size k,
and prev topBs[0] containing only the start item. An extra loop is used to enumerate all
state items in prev topBs for the generation of new state items. Second, variable k is used
to represent the size of the state items to be generated, and EXPAND generates only state
items with size k, taking k as an extra parameter. Third, the B-best newly generated state
items are appended to the back of prev topBs, without removing previous state items in
prev topBs. The algorithm thereby keeps track of kB state items instead of B at the kth
processing stage, and explores a larger subset of the exponential search space.
The rest of the algorithm is the same as the original algorithm in Figure 1. To in-
stantiate this generic algorithm for word segmentation, STARTITEM(word segmentation)
consists of an empty sentence S and a queue Q containing the full input sentence;
EXPAND(candidate, k, word segmentation) generates a single new state item by popping
characters on the incoming queue from the front until the kth character (k is the index in
the input sentence rather than the queue itself), and appending them as a new word
to candidate; and GOALTEST(word segmentation, best) returns true if best consists of a
complete segmented sentence and an empty incoming queue.
As before, the linear model from Section 2 is applied directly to score state items,
and the model parameters are trained with the averaged perceptron algorithm. The
features for a state item are extracted according to the feature templates in Table 1.
3.5.2 A Dynamic-Programming Decoder. Given the feature templates that we define,
a dynamic-programming algorithm can be used to explore the whole search space
in cubic time. The idea is to reduce the search task into overlapping sub-problems.
function MULTIPLE-BEAM-SEARCH(problem, agenda, prev topBs, B)
prev topBs ? {{STARTITEM(problem)}}
agenda ? CLEAR(agenda)
k ? 0
loop do
k ? k + 1
for each candidates in prev topBs
for each candidate in candidates
agenda ? INSERT(EXPAND(candidate, k, problem), agenda)
if GOALTEST(problem, agenda)
then return TOP(agenda)
candidates ? TOP-B(agenda, B)
prev topBs ? APPEND(prev topBs, candidates)
agenda ? CLEAR(agenda)
Figure 4
The extended generic beam-search algorithm with multiple beams.
118
Zhang and Clark Syntactic Processing
Suppose that the input has n characters; one way to find the highest-scored segmenta-
tion is to first find the highest-scored segmentations with the last word being characters
b..n ? 1, where b ? 0..n ? 1, respectively, and then choose the highest-scored one from
these segmentations. In order to find the highest-scored segmentation with the last
word being characters b, ..n ? 1, the last word needs to be combined with all different
segmentations of characters 0..b ? 1 so that the highest scored can be selected. However,
because the largest-range feature templates span only over two words (see Table 1),
the highest scored among the segmentations of characters 0..b ? 1 with the last word
being characters b?..b ? 1 will also give the highest score when combined with the word
b..n ? 1. As a result, the highest-scored segmentation with the last word being charac-
ters b..n ? 1 can be found as long as the highest-scored segmentations of 0..b ? 1 with
the last word being b?..b ? 1 are found, where b? ? 0..b ? 1. With the same reasoning, the
highest-scored segmentation of characters 0..b ? 1 with the last word being b?..b ? 1 can
be found by choosing the highest-scored one among the highest-scored segmentations
of 0..b? ? 1 with the last word being b??..b? ? 1, where b?? ? 0..b? ? 1. In this way, the
search task is reduced recursively into smaller problems, where in the simplest case
the highest-scored segmentation of characters 0..e with the last word being characters
0..e(e ? 0..n ? 1) are known. And the final highest-scored segmentation can be found by
incrementally finding the highest-scored segmentations of characters 0..e(e ? 0..n ? 1)
with the last word being b..e(b ? 0..e).
The pseudo code for this algorithm is shown in Figure 5. It works by building
an n by n table chart, where n is the number of characters in the input sentence sent.
chart[b, e] records the highest scored segmentation from the beginning to character e,
with the last word starting from character b and ending at character e. chart[0, e] can be
computed directly for e = 0..n ? 1, whereas chart[b, e] needs to be built by combing the
best segmentation on sent[0, b ? 1] and sent[b, e], for b > 0. The final output is the best
among chart[b, n ? 1], with b = 0..n ? 1. The reason for recording partial segmentations
with different final words separately (leading to cubic running time) is the word bigram
feature template. Note that with a larger feature range, exact inference with dynamic-
programming can become prohibitively slow.
Inputs: raw sentence sent (length n)
Variables: an n by n table chart, where chart[b, e] stores the best scored (partial)
segmentation of the characters from the begining of the sentence to character
e, with the last word spanning over the characters from b until e;
character index b for the start of word;
character index e for the end of word;
character index p for the start of the previous word.
Initialization:
for e = 0..n ? 1:
chart[0, e] ? a single word sent[0..e]
Algorithm:
for e = 0..n ? 1:
for b = 1..e:
chart[b, e] ? the highest scored segmentation among those derived by combining
chart[p, b ? 1] with sent[b, e], for p = 0..b ? 1
Outputs: the highest scored segmentation among chart[b, n ? 1], for b = 0..n ? 1
Figure 5
A dynamic-programming algorithm for word segmentation.
119
Computational Linguistics Volume 37, Number 1
Table 6
Comparison between three different decoders for word segmentation.
Bakeoff 1 Bakeoff 2
AS CU PU AS CU PU MS Average
SB F-measure 96.9 94.6 94.1 95.0 95.1 94.6 97.3 95.4
SB sent/sec 212 145 202 358 115 177 105 188
SB char/sec 3,054 6,846 4,808 5,263 5,333 5,870 4,963 5,162
SB # features 4.0M 0.5M 1.5M 3.9M 1.8M 1.5M 2.7M 2.3M
MB F-measure 97.0 94.5 94.1 95.0 95.0 94.4 97.3 95.3
MB sent/sec 147 7 13 167 7 11 5 51
MB char/sec 2,118 331 187 2,455 325 365 236 859
MB # features 4.0M 0.6M 1.5M 3.9M 1.8M 1.5M 2.7M 2.3M
DP F-measure 97.1 94.6 94.3 95.0 95.0 94.5 97.2 95.4
DP sent/sec 131 3 6 142 4 4 2 42
DP char/sec 1,887 142 86 2,087 185 133 95 659
DP # features 4.0M 0.5M 1.5M 3.9M 1.7M 1.4M 2.7M 2.3M
3.5.3 Experiments. Table 6 shows the comparison between the single-beam (SB), multiple-
beam (MB), and dynamic-programming (DP) decoders by F-score and speed.1 Speed is
measured by the number of sentences (sent) and characters (char) per second (excluding
model loading time). We also include the size of the models in each case. The slight
difference in model size between different methods is due to different numbers of
negative features generated during training. The single-beam search algorithm achieved
significantly higher speed than both the multiple-beam and the dynamic-programming
algorithms, whereas the multiple-beam search algorithm ran slightly faster than the
dynamic-programming algorithm. Though addressing the comparability issue and
exploring a larger number of candidate output segmentations, neither multiple-beam
search nor dynamic programming gave higher accuracy than the single-beam search
algorithm overall. One of the possible reasons is that the perceptron algorithm adjusts
its parameters according to the mistakes the decoder makes: Although the single-beam
might make more mistakes than the multiple-beam given the same model, it does not
necessarily perform worse with a specifically tailored model.
4. Joint Segmentation and Part-of-Speech Tagging
Joint word segmentation and POS-tagging is the problem of solving word segmen-
tation and POS-tagging simultaneously. Traditionally, Chinese word segmentation and
POS-tagging are performed in a pipeline. The output from the word segmentor is taken
as the input for the POS-tagger. A disadvantage of pipelined segmentation and POS-
tagging is that POS-tag information, which is potentially useful for segmentation, is
not used during the segmentation step. In addition, word segmentation errors are
propagated to the POS-tagger, leading to lower quality of the overall segmented and
1 The experiments were performed using the Zhang and Clark (2007) feature set and single-beam decoder,
and our new way to decide the number of training iterations in this article. The single-beam results
correspond to ?Zhang 2007*? in Tables 4 and 5.
120
Zhang and Clark Syntactic Processing
POS-tagged output. Joint word segmentation and POS-tagging is a method that ad-
dresses these problems. In Zhang and Clark (2008a) we proposed a joint word segmen-
tor and POS-tagger using a multiple-beam decoder, and showed that it outperformed a
pipelined baseline. We recently showed that comparable accuracies can be achieved by a
single-beam decoder, which runs an order of magnitude faster (Zhang and Clark 2010).
In this section, we describe our single-beam system using our general framework, and
provide a detailed comparison with our multiple-beam and baseline systems of Zhang
and Clark (2008a).
4.1 Instantiating the General Framework
Given an input sentence, our joint segmentor and POS-tagger builds an output incre-
mentally, one character at a time. When a character is processed, it is either concatenated
with the last word in the partially built output, or taken as a new word. In the latter
case, a POS-tag is assigned to the new word. When more characters are concatenated to
a word, the POS-tag of the word remains unchanged.
For the decoding problem, an agenda is used to keep B different candidates at
each incremental step. Before decoding starts, the agenda is initialized with an empty
sentence. When a character is processed, existing candidates are removed from the
agenda and extended with the current character in all possible ways, and the B-best
newly generated candidates are put back onto the agenda. After all the input characters
have been processed, the highest-scored candidate from the agenda is taken as output.
Expressed as an instance of the generic algorithm in Figure 1, a state item is a pair
?S, Q?, with S being a segmented and tagged sentence and Q being a queue of the next in-
coming characters. STARTITEM(joint tagging) contains an empty sentence and the whole
input sentence as incoming characters; EXPAND(candidate, joint tagging) pops the first
character from the incoming queue, adds it to candidate, and assigns POS-tags in the
aforementioned way to generate a set of new state items; and GOALTEST( joint tagging,
best) returns true if best contains a complete segmented and POS-tagged output and an
empty queue.
The linear model from Section 2 is applied to score state items, differentiating
partial words from full words in the aforementioned ways, and the model parameters
are trained with the averaged perceptron. The features for a state item are extracted
incrementally according to Equation (3), where for the ith character, ?(?(y, i)) is com-
puted according to the feature templates in both Table 1, which are related to word
segmentation, and Table 7, which are related to POS-tagging. During training, the early-
update method of Collins and Roark (2004), as described in Section 2, is used. It ensures
that state items on the beam are highly probable at each incremental step, and is crucial
to the high accuracy given by a single-beam.
4.2 Pruning
We use several pruning methods from Zhang and Clark (2008a), most of which serve
to improve the accuracy by removing irrelevant candidates from the beam. First, the
system records the maximum number of characters that a word with a particular POS-
tag can have. For example, from the Chinese Treebank that we used for our experiments,
most POS are associated only with one- or two-character words. The only POS-tags that
are seen with words over ten characters long are NN (noun), NR (proper noun), and
CD (numbers). The maximum word length information is initialized as all ones, and
updated according to each training example before it is processed.
121
Computational Linguistics Volume 37, Number 1
Table 7
POS feature templates for the joint segmentor and POS-tagger.
Feature template when c0 is
1 w?1t?1 separated
2 t?1t0 separated
3 t?2t?1t0 separated
4 w?1t0 separated
5 t?2w?1 separated
6 w?1t?1end(w?2) separated
7 w?1t?1c0 separated
8 c?2c?1c0t?1, where len(w?1) = 1 separated
9 c0t0 separated
10 t?1start(w?1) separated
11 t0c0 separated or appended
12 c0t0start(w0) appended
13 ct?1end(w?1), where c ? w?1 and c 6= end(w?1) separated
14 c0t0cat(start(w0)) separated
15 ct?1cat(end(w?1)), where c ? w?1 and c 6= end(w?1) appended
16 c0t0c?1t?1 separated
17 c0t0c?1 appended
w = word; c = character; t = POS-tag. The index of the current character is 0.
Second, a tag dictionary is used to record POS-tags associated with each word.
During decoding, frequent words and words with ?closed set? tags2 are only assigned
POS-tags according to the tag dictionary, while other words are assigned every POS-
tag to make candidate outputs. Whether a word is a frequent word is decided by the
number of times it has been seen in the training process. Denoting the number of times
the most frequent word has been seen by M, a word is a frequent word if it has been
seen more than M/5, 000 + 5 times. The threshold value is taken from Zhang and Clark
(2008a), and we did not adjust it during development. Word frequencies are initialized
as zeros and updated according to each training example before it is processed; the
tag dictionary is initialized as empty and updated according to each training example
before it is processed.
Third, we make an additional record of the initial characters for words with ?closed
set? tags. During decoding, when the current character is added as the start of a new
word, ?closed set? tags are only assigned to the word if it is consistent with the record.
This type of pruning is used in addition to the tag dictionary to prune invalid partial
words, while the tag dictionary is used to prune complete words. The record for initial
character and POS is initially empty, and updated according to each training example
before it is processed.
Finally, at any decoding step, we group partial candidates that are generated by
separating the current character as the start of a new word by the signature p0p?1w?1,
and keep only the best among those having the same p0p?1w?1. The signature p0p?1w?1
is decided by the feature templates we use: it can be shown that if two candidates cand1
and cand2 generated at the same step have the same signature, and the score of cand1
is higher than the score of cand2, then at any future step, the highest scored candidate
2 ?Closed set? tags are the set of POS-tags which are only associated with a fixed set of words, according to
the Penn Chinese Treebank specifications (Xia 2000).
122
Zhang and Clark Syntactic Processing
generated from cand1 will always have a higher score than the highest scored candidate
generated from cand2.
From these four pruning methods, only the third was not used by our multiple-
beam system (Zhang and Clark 2008a). This was designed to help keep likely partial
words in the agenda and improve the accuracy, and does not give our system a speed
advantage over our multiple-beam system.
4.3 Comparison with Multiple-Beam Search (Zhang and Clark 2008a)
Our system of Zhang and Clark (2008a) was based on the perceptron and a multiple-
beam decoder. That decoder can be seen as a slower alternative of our decoder in this ar-
ticle, but one which explores a larger part of the search space. The comparison between
our joint segmentation and tagging systems of Zhang and Clark (2008a) and this article
is similar to the comparison between our segmentors in sections 3.5.1 and 3.1. In Zhang
and Clark (2008a), we argued that the straightforward implementation of the single-
beam decoder cannot give competitive accuracies to the multiple-beam decoder, and
the main difficulties for a single-beam decoder are in the representing of partial words,
and the handling of an exponentially large combined search space using one beam. In
this section, we give a description of our system of Zhang and Clark (2008a), and discuss
the reason we can achieve competitive accuracies using a single beam in this article.
4.3.1 The Multiple-Beam System of Zhang and Clark (2008a). The decoder of our multiple-
beam system can be formulated as an instance of the multiple-beam decoder described
in Section 3.5.1.
Similar to the multiple-beam search decoder for word segmentation, the decoder
compares candidates only with complete tagged words, and enables the size of the
search space to scale with the input size. A set of state items is kept for each character
to record possible segmented and POS-tagged sentences ending with the character.
Just as with the single-beam decoder, the input sentence is processed incrementally.
However, when a character is processed, the number of previously built state items is
increased from B to kB, where B is the beam-size and k is the number of characters that
have been processed. Moreover, partial candidates ending with any previous character
are available. The decoder thus generates all possible tagged words ending with the cur-
rent character, concatenating each with existing partial sentences ending immediately
before the word, and putting the resulting sentence onto the agenda. After the character
is processed, the B-best items in the agenda are kept as the corresponding state items
for the character, and the agenda is cleared for the next character. All input characters
are processed in the same way, and the final output is the best state item for the last
character.
To instantiate the generic multiple-beam algorithm in Figure 4 for joint segmenta-
tion and POS-tagging, STARTITEM( joint tagging) consists of an empty sentence S and
a queue Q containing the full input sentence; EXPAND(candidate, k, joint tagging) pops
characters on the incoming queue from the front until the kth character (k is the index
in the input sentence rather than the queue itself), appending them as a new word
to candidate, and assigning to the new word all possible POS-tags to generate a set of
new items; and GOALTEST(joint tagging, best) returns true if best consists of a complete
segmented and POS-tagged sentence and an empty incoming queue.
The linear model from Section 2 is again applied directly to score state items, and
the model parameters are trained with the averaged perceptron algorithm. The features
123
Computational Linguistics Volume 37, Number 1
for a state item are extracted according to the union of the feature templates for the
baseline segmentor and the baseline POS-tagger.
4.3.2 Discussion. An important problem that we solve for a single-beam decoder for the
global model is the handling of partial words. As we pointed out in Zhang and Clark
(2008a), it is very difficult to score partial words properly when they are compared with
full words, although such comparison is necessary for incremental decoding with a
single-beam. To allow comparisons with full words, partial words can either be treated
as full words, or handled differently.
We showed in Zhang and Clark (2008a) that a naive single-beam decoder which
treats partial words in the same way as full words failed to give a competitive ac-
curacy. An important reason for the low accuracy is over-segmentation during beam-
search. Consider the three characters (tap water). The first two characters do not
make sense when put together as a single word. Rather, when treated as two single-
character words, they can make sense in a sentence such as (please) (self) (come)
(take). Therefore, when using single-beam search to process (tap water), the
two-character word candidate is likely to have been thrown off the agenda before
the third character is considered, leading to an unrecoverable segmentation error.
This problem is even more severe for a joint segmentor and POS-tagger than for
a pure word segmentor, because the POS-tags and POS-tag bigram of and fur-
ther supports them being separated when is considered. The multiple-beam search
decoder we proposed in Zhang and Clark (2008a) can be seen as a means to ensure
that the three characters always have a chance to be considered as a single
word. It explores candidate segmentations from the beginning of the sentence until each
character, and avoids the problem of processing partial words by considering only full
words. However, because it explores a larger part of the search space than a single-beam
decoder, its time complexity is correspondingly higher.
In our single-beam system, we treat partial words differently from full words,
so that in the previous example, the decoder can take the first two characters in
(tap water) as a partial word, and keep it in the beam before the third character is
processed. One challenge is the representation of POS-tags for partial words. The POS of
a partial word is undefined without the corresponding full word information. Though
a partial word can make sense with a particular POS-tag when it is treated as a complete
word, this POS-tag is not necessarily the POS of the full word which contains the partial
word. Take the three-character sequence as an example. The first character
represents a single-character word ?below?, for which the POS can be LC or VV. The
first two characters represent a two-character word ?rain?, for which the POS can
be VV. Moreover, all three characters when put together make the word ?rainy day?, for
which the POS is NN. As discussed earlier, assigning POS tags to partial words as if they
were full words leads to low accuracy.
An obvious solution to this problem is not to assign a POS to a partial word until it
becomes a full word. However, lack of POS information for partial words makes them
less competitive compared to full words in the beam, because the scores of full words
are further supported by POS and POS n-gram information. Therefore, not assigning POS
to partial words potentially leads to over segmentation. In our experiments, this method
did not give comparable accuracies to our multiple-beam system.
We take a different approach, and assign a POS-tag to a partial word when its
first character is separated from the final character of the previous word. When more
characters are appended to a partial word, the POS is not changed. The idea is to use
the POS of a partial word as the predicted POS of the full word it will become. Possible
124
Zhang and Clark Syntactic Processing
predictions are made with the first character of the word, and the likely ones will be
kept in the beam for the next processing steps. For example, with the three characters
, we try to keep two partial words (besides full words) in the beam when the
first word is processed, with the POS being VV and NN, respectively. The first POS
predicts the two-character word , and the second the three-character word .
Now when the second character is processed, we still need to maintain the possible POS
NN in the agenda, which predicts the three-character word .
We show that the mechanism of predicting the POS at the first character gives
competitive accuracy. This mechanism can be justified theoretically. Unlike alphabetical
languages, each Chinese character represents some specific meanings. Given a character,
it is natural for a human speaker to know immediately what types of words it can start.
This allows the knowledge of possible POS-tags of words that a character can start, using
information about the character from the training data. Moreover, the POS of the previ-
ous words to the current word are also useful in deciding possible POS for the word.3
The mechanism of first-character decision of POS also boosts the efficiency, because
the enumeration of POS is unnecessary when a character is appended to the end of an
existing word. As a result, the complexity of each processing step is reduced by half
compared to a method without POS prediction.
Finally, an intuitive way to represent the status of a partial word is using a flag
explicitly, which means an early decision of the segmentation of the next incoming
character. We take a simpler alternative approach, and treat every word as a partial
word until the next incoming character is separated from the last character of this word.
Before a word is confirmed as a full word, we only apply to it features that represent its
current partial status, such as character bigrams, its starting character, its part-of-speech,
and so forth. Full word features, including the first and last characters of a word, are
applied immediately after a word is confirmed as complete.
An important component for our proposed system is the training process, which
needs to ensure that the model scores a partial word with predicted POS properly. We
apply the general framework and use the averaged perceptron for training, together
with the ?early update? mechanism.
4.4 Experiments
We performed two sets of experiments, using the Chinese Treebank 4 and 5, respectively.
In the first set of experiments, CTB4 was separated into two parts: CTB3 (420K characters
in 150K words/10, 364 sentences) was used for the final 10-fold cross validation, and
the rest (240K characters in 150K words/4, 798 sentences) was used as training and test
data for development. The second set of experiments was performed to compare with
relevant systems on CTB5 data.
The standard F-scores are used to measure both the word segmentation accuracy
and the overall segmentation and tagging accuracy, where the overall accuracy is
JF = 2pr/(p + r)
with the precision p being the percentage of correctly segmented and tagged words
in the decoder output, and the recall r being the percentage of gold-standard tagged
3 The next incoming characters are also a useful source of information for predicting the POS. However, our
system achieved competitive accuracy compared to our multiple-beam system without such character
lookahead features.
125
Computational Linguistics Volume 37, Number 1
words that are correctly identified by the decoder. For direct comparison with Ng and
Low (2004), the POS-tagging accuracy is also calculated by the percentage of correct tags
on each character.
4.4.1 Development Experiments. Our development data consists of 150K words in 4, 798
sentences. Eighty percent (80%) of the data were randomly chosen as the development
training data, and the rest were used as the development test data. Our development
tests were mainly used to decide the size of the beam, the number of training iterations,
and to observe the effect of early update.
Figure 6 shows the accuracy curves for joint segmentation and POS-tagging by the
number of training iterations, using different beam sizes. With the size of the beam
increasing from 1 to 32, the accuracies generally increase, although the amount of
increase becomes small when the size of the beam becomes 16. After the tenth iteration,
a beam size of 32 does not always give better accuracies than a beam size of 16. We
therefore chose 16 as the size of the beam for our system.
The testing times for each beam size between 1 and 32 are 7.16 sec, 11.90 sec,
18.42 sec, 27.82 sec, 46.77 sec, and 89.21 sec, respectively. The corresponding speeds
in the number of sentences per second are 111.45, 67.06, 43.32, 28.68, 17.06 and 8.95,
respectively.
Figure 6 also shows that the accuracy increases with an increased number of train-
ing iterations, but the amount of increase becomes small after the 25th iteration. We
chose 29 as the number of iterations to train our system.
The effect of early update: We compare the accuracies by early update and normal
perceptron training. In the normal perceptron training case, the system reached the best
performance at the 22nd iteration, with a segmentation F-score of 90.58% and joint
Figure 6
The influence of beam-sizes, and the convergence of the perceptron for the joint segmentor and
POS-tagger.
126
Zhang and Clark Syntactic Processing
Table 8
The accuracies of joint segmentation and POS-tagging by 10-fold cross validation.
Baseline (pipeline) Joint single-beam Joint multiple-beam
# SF JF JA SF JF JA SF JF JA
Av. 95.2 90.3 92.2 95.8 91.4 93.0 95.9 91.3 93.0
SF = segmentation F-score; JF = overall segmentation and POS-tagging F-score; JA = tagging
accuracy by character.
F-score of 83.38%. When using early update, the algorithm reached the best accuracy
at the 30th training iteration, obtaining a segmentation F-score of 91.14% and a joint
F-score of 84.06%.
4.4.2 Cross-Validation Results. Ten-fold cross validation is performed to test the accuracy
of the joint word segmentor and POS-tagger, and to make comparisons with existing
models in the literature. Following Ng and Low (2004), we partition the sentences in
CTB3, ordered by sentence ID, into 10 groups evenly. In the nth test, the nth group is
used as the testing data.
Table 8 shows the cross-validation accuracies of the pipeline baseline system and
the joint system using single and multiple beam decoders. SF, JF and JA represent
segmentation F-score, tagging F-score, and tagging accuracy, respectively. The joint
segmentor and tagger systems outperformed the baseline consistently, while the single
beam-search decoder in this article gave comparable accuracies to our multiple-beam
algorithm of Zhang and Clark (2008a).
Speed comparisons of the three systems using the same 10-fold cross-validation are
shown in Table 9. TE, ML, and SP represents the testing time (seconds), model loading
time (seconds), and speed (number of sentences per second), respectively. Speed is cal-
culated as number of test sentences divided by the test time (excluding model loading).
For the baseline system, test time and model loading time for both the segmentor and
the POS-tagger are recorded. The joint system using a single beam decoder was over
10 times faster than the multiple-beam system, and the baseline system was more than
three times as fast as the single-beam joint system. These tests were performed on a Mac
OSX platform with a 2.13GHz CPU and a gcc 4.0.1 compiler.
Table 10 shows the overall accuracies of the baseline and joint systems, and com-
pares them to two relevant models in the literature. The accuracy of each model is
Table 9
The speeds of joint word segmentation and POS-tagging by 10-fold cross validation.
Baseline (pipeline) Joint single-beam Joint multiple-beam
# TE (s+p=total) ML (s+p=total) SP TE ML SP TE ML SP
Av. 8.8+10.4=19.2 2.9+3.7=6.6 82.2 58.6 12.1 22.4 575.0 9.5 1.9
TE = testing time (seconds); ML = model loading time (seconds); SP = speed by the number of
sentences per second, excluding loading time; (s) = segmentation in baseline; (p) = POS-tagging in
baseline.
127
Computational Linguistics Volume 37, Number 1
Table 10
The comparison of overall accuracies of various joint segmentor and POS-taggers by 10-fold
cross validation using CTB.
Model SF JF JA
Baseline+ (Ng) 95.1 ? 91.7
Joint+ (Ng) 95.2 ? 91.9
Baseline+* (Shi) 95.85 91.67 ?
Joint+* (Shi) 96.05 91.86 ?
Baseline (ours) 95.20 90.33 92.17
Joint (our multiple-beam) 95.90 91.34 93.02
Joint (our single-beam) 95.84 91.37 93.01
SF = segmentation F-score; JF = joint segmentation and POS-tagging F-score; JA = tagging accuracy
by character.
+ Knowledge about special characters.
* Knowledge from semantic net outside CTB.
shown in a row, where Ng represents the models from Ng and Low (2004), which
applies a character tagging approach to perform word segmentation and POS-tagging
simultaneously, and Shi represents the models from Shi and Wang (2007), which is a
reranking system for segmentation and POS-tagging. These two models are described
in more detail in the related work section. Each accuracy measure is shown in a col-
umn, including the segmentation F-score (SF), the overall tagging F-score ( JF), and the
tagging accuracy by character ( JA). As can be seen from the table, our joint models
achieved the largest improvement over the baseline, reducing the segmentation error
by more than 14% and the overall tagging error by over 12%.
The overall tagging accuracy of our joint model was comparable to but less than
the joint model of Shi and Wang (2007). Despite the higher accuracy improvement
from the baseline, the joint system did not give higher overall accuracy. One possi-
ble reason is that Shi and Wang (2007) included knowledge about special characters
and semantic knowledge from Web corpora (which may explain the higher baseline
accuracy), whereas our system is completely data-driven. However, the comparison is
indirect because our partitions of the CTB corpus are different. Shi and Wang (2007) also
chunked the sentences before doing 10-fold cross validation, but used an uneven split.
We chose to follow Ng and Low (2004) and split the sentences evenly to facilitate further
comparison.
Compared with Ng and Low (2004), our baseline model gave slightly better accu-
racy, consistent with our previous observations about the word segmentors in Section 3.
Due to the large accuracy gain from the baseline, our joint model performed much
better.
In summary, when compared with existing joint word segmentation and POS-
tagging systems using cross-validation tests, our proposed model achieved the best
accuracy boost from the pipelined baseline, and competitive overall accuracy. Our sys-
tem based on the general framework of this article gave comparable accuracies to our
multiple-beam system in Zhang and Clark (2008a), and a speed that is over an order of
magnitude higher than the multiple-beam algorithm.
4.4.3 Test Results Using CTB5. We follow Kruengkrai et al (2009) and split the CTB5 into
training, development testing, and testing sets, as shown in Table 11. The data are used
128
Zhang and Clark Syntactic Processing
Table 11
Training, development, and test data from CTB5 for joint word segmentation and POS-tagging.
Sections Sentences Words
Training 1?270, 400?931, 1001?1151 18,085 493,892
Dev 301?325 350 6,821
Test 271?300 348 8,008
to compare the accuracies of our joint system with models in the literature, and to draw
the speed/accuracy tradeoff graph.
Kruengkrai et al (2009) made use of character type knowledge for spaces, numerals,
symbols, alphabets, and Chinese and other characters. In the previous experiments, our
system did not use any knowledge beyond the training data. To make the comparison
fairer, we included knowledge of English letters and Arabic numbers in this experiment.
During both training and decoding, English letters and Arabic numbers are segmented
using rules, treating consecutive English letters or Arabic numbers as a single word.
The results are shown in Table 12, where row N07 refers to the model of Nakagawa
and Uchimoto (2007), rows J08a and J08b refer to the models of Jiang et al (2008) and
Jiang, Mi, and Liu (2008), and row K09 refers to the models of Kruengkrai et al (2009).
Columns SF and JF refer to segmentation and joint segmentation and tagging accuracies,
respectively. Our system gave comparable accuracies to these recent works, obtaining
the best (same as the error-driven version of K09) joint F-score.
The accuracy/speed tradeoff graphs for the joint segmentor and POS-taggers, to-
gether with the baseline pipeline system, are shown in Figure 7. For each point in each
curve, the development test data were used to decide the number of training iterations,
and then the speed and accuracy were measured using test data. No character knowl-
edge is used in any system. The baseline curve was drawn with B = 16 for the baseline
segmentor, because the baseline segmentation accuracy did not improve beyond B = 16
in our experiments. Each point in this curve corresponds to a different beam size of
the baseline POS-tagger, which are 2, 4, 8, 16, 32, 64, 128, and 256, respectively, from
right to left.
When the speed is over 2.5 thousand characters per second, the baseline system
performed better than the joint single-beam and multiple-beam systems, due to the
higher segmentation accuracy brought by the fixed beam segmentor. However, as the
Table 12
Accuracy comparisons between various joint segmentors and POS-taggers on CTB5.
SF JF
K09 (error-driven) 97.87 93.67
our system 97.78 93.67
Zhang 2008 97.82 93.62
K09 (baseline) 97.79 93.60
J08a 97.85 93.41
J08b 97.74 93.37
N07 97.83 93.32
SF = segmentation F-score; JF = joint segmentation and POS-tagging F-score.
129
Computational Linguistics Volume 37, Number 1
Figure 7
The accuracy/speed tradeoff graph for the joint segmentor and POS-taggers and the two-stage
baseline.
tagger beam size further increased, the accuracy of the baseline system did not improve.
The highest F-score of the baseline (92.83%) was achieved when the beam size of the
baseline POS-tagger was 32. In fact, we have shown in Section 3.5 that the accuracy
of the baseline segmentor was similar to using a Viterbi decoder when the beam size
was 16. This was also true for the baseline POS-tagger, according to our experiments.
Therefore, though being the most accurate when the speed is high, the baseline system
reaches the highest F-score at 2.63 thousand characters per second, and cannot further
improve the accuracy on this curve.
The points in the single-beam curve correspond to beam sizes of 2, 4, 8, 16, and 32,
respectively, from right to left. When the speed is roughly between 0.5 and 2.0 thou-
sand characters per second, the single-beam system gave the best F-score. This is
because the multiple-beam system did not reach such high speeds, and the baseline
system could not produce a higher accuracy than 92.83%. The single-beam joint system
gave the highest accuracy of 93.50% when the beam size was 16 and the speed was
1.01 thousand characters per second, and the F-score dropped slightly when the beam
further increased to 32.
The points in the multiple-beam curve correspond to beam sizes of 1, 2, 4, 8, and 16,
respectively, from right to left. The multiple-beam system gave the highest F-score of
93.62% when the beam size was 16, but the speed was down to 0.06 thousand sentences
per second.
4.5 Related Work
Ng and Low (2004) mapped the joint segmentation and POS-tagging task into a
single character sequence tagging problem. Two types of tags are assigned to each
character to represent its segmentation and POS. For example, the tag b NN indicates a
character at the beginning of a noun, and the tag e VV indicates a character at the end
of a verb. Using this method, POS features are allowed to interact with segmentation.
Because tagging is restricted to characters, the search space is reduced to O((4T)n),
where 4 is the number of segmentation tags and T is the size of the tag set. Beam-search
decoding is effective with a small beam-size. However, the disadvantage of this
model is the difficulty of incorporating whole word information into POS-tagging. For
example, the standard word + POS-tag feature is not applicable.
130
Zhang and Clark Syntactic Processing
Shi and Wang (2007) introduced POS information into segmentation by reranking.
B-best segmentation outputs are passed to a separately-trained POS-tagger, and the best
output is selected using the overall POS-segmentation probability score. In this system,
the decoding for word segmentation and POS-tagging are still performed separately, and
exact inference for both is possible. However, the interaction between POS and segmen-
tation is restricted by reranking: POS information is used to improve segmentation only
for the B segmentor outputs. In comparison to the two systems described here, our joint
system does not impose any hard constraints on the interaction between segmentation
and POS information.
Jiang, Mi, and Liu (2008) proposed a reranking system that builds a pruned word
lattice instead of generating a B-best list by the segmentor. The advantage of this
reranking method compared to Shi and Wang?s (2007) method is that more ambiguity is
kept in the reranking stage. The reranking algorithm used a similar model to our joint
segmentor and POS-tagger.
Nakagawa and Uchimoto (2007) proposed a hybrid model for word segmentation
and POS tagging using an HMM-based approach. Word information is used to process
known words, and character information is used for unknown words in a similar
way to Ng and Low (2004). In comparison, our model handles character and word
information simultaneously in a single perceptron model. Recently, Kruengkrai et al
(2009) developed this hybrid model further by scoring characters and words in the same
model. Their idea is similar to our joint segmentor and POS-tagger in Zhang and Clark
(2008a).
5. Dependency Parsing
Dependency parsing is the problem of producing the syntactic structure for an input
sentence according to dependency grammar. The output structure of a dependency
parser is called a dependency graph; in this article, only dependency trees are con-
sidered. As can be seen from Figure 8, a dependency tree consists of a set of vertices
and directed arcs. Each arc represents the relationship between a pair of words in the
sentence; it points from the head word to its modifier. For example, in the arc between
the word (I) and the word (like), (like) is the head word and (I) is the
subject that modifies (like); in the arc between the word (like) and the word
(reading), (like) is the head word and (reading) is the object that modifies
(like). In a dependency tree, there is only one word that does not modify any other
word, and it is called the head word of the sentence. The other words each modify
exactly one word, and no word can modify itself.
A dependency tree is called projective if there are no crossing arcs when the
sentence is represented linearly, in word order. Though almost all languages are non-
projective to some degree, the majority of sentences in most languages are projective. In
Figure 8
An example Chinese dependency tree.
131
Computational Linguistics Volume 37, Number 1
the CoNLL shared tasks on dependency parsing (Buchholz and Marsi 2006; Nivre et al
2007) most data sets contain only 1?2% non-projective arcs, and projective dependency
parsing models can give reasonable accuracy in these tasks (Carreras, Surdeanu, and
Marquez 2006). Although non-projective dependency parsing can be solved directly by
using a different model from projective dependency parsing (McDonald et al 2005), it
can also be solved using a projective parsing model, with the help of a reversible trans-
formation procedure between non-projective and projective dependency trees (Nivre
et al 2006). In this article we focus on projective dependency parsing.
An unlabeled dependency tree is a dependency tree without dependency labels
such as Subj and Obj in Figure 8. The same techniques used by unlabeled dependency
parsers can be applied to labeled dependency parsing. For example, a shift-reduce
unlabeled dependency parser can be extended to perform labeled dependency parsing
by splitting a single reduce action into a set of reduce actions each associated with a
dependency label. Here we focus on unlabeled dependency parsing.
Graph-based (McDonald, Crammer, and Pereira 2005; Carreras, Surdeanu, and
Marquez 2006; McDonald and Pereira 2006) and transition-based (Yamada and
Matsumoto 2003; Nivre et al 2006) parsing algorithms offer two different approaches
to data-driven dependency parsing. Given an input sentence, a graph-based algorithm
finds the highest scoring parse tree from all possible outputs, scoring each complete
tree, while a transition-based algorithm builds a parse by a sequence of actions, scoring
each action individually. Although graph-based and transition-based parsers can be
differentiated in various ways, we prefer to think in terms of the features used in the
two approaches as the differentiating factor.
In Zhang and Clark (2008b) we proposed a graph-based parser and a transition-
based parser using the generalized perceptron and beam-search, showing that beam-
search is a competitive choice for both graph-based and transition-based dependency
parsing. In the same paper we used a single discriminative model to combine graph-
based and transition-based parsing, showing that the combined parser outperforms
both graph-based and transition-based parsers individually. All three parsers can be
expressed by our general framework. Here we describe the transition-based and com-
bined parsers, which share the same decoding process.
5.1 Instantiating the General Framework
Our dependency parser uses the incremental parsing process of MaltParser (Nivre
et al 2006), which is characterized by the use of a stack and four transition actions:
SHIFT, ARCRIGHT, ARCLEFT, and REDUCE. An input sentence is formed into a queue
of incoming words and processed from left to right. Initially empty, the stack is used
throughout the parsing process to store unfinished words, which are the words before
the current word that may still be linked with the current or a future word.
The SHIFT action pops the first word from the queue and pushes it onto the stack.
The ARCRIGHT action pops the first word from the queue, adds a dependency link
from the stack top to the word (i.e., the stack top becomes the parent of the word), and
pushes the word onto the stack. The ARCLEFT action adds a dependency link from the
first word on the queue to the stack top, and pops the stack. The REDUCE action pops
the stack. Among the four transition actions, SHIFT and ARCRIGHT push a word onto
the stack, and ARCLEFT and REDUCE pop the stack; SHIFT and ARCRIGHT read the next
input word, and ARCLEFT and ARCRIGHT add a link to the output.
The initial state contains an empty stack and the whole input sentence as incoming
words. The final state contains a stack holding only the head word of the sentence and
132
Zhang and Clark Syntactic Processing
an empty queue, with the input words having all been processed. The incremental pars-
ing process starts from the initial state, and builds a candidate parse tree by repeatedly
applying one transition action out of the four, until the final state is reached.
For the decoding problem, an agenda is used to find the output parse tree from
different possible candidates. Starting with an initial state item, a set of candidate
state items is used to generate new state items for each step. At each step, each existing
state item is extended by applying all applicable actions from the four, and the generated
items are put onto the agenda. After each step, the best B items are taken from the
agenda to generate new state items for the next step, and the agenda is cleared. After all
incoming words have been processed, the corresponding parse of the top item from the
agenda is taken as the output.
The decoding process is an instance of the generalized algorithm in Figure 1. A state
item is a pair ?S, Q?, where S represents the stack with the partial parse, and Q represents
the queue of incoming words. The initial state item STARTITEM(dependency parsing)
consists of an empty stack, and an incoming queue of the whole input. The function
EXPAND(candidate, dependency parsing) applies all possible actions to candidate and gen-
erates a set of new state items. GOALTEST(dependency parsing, best) returns true if best
has reached the final state, and false otherwise.
The score of a parse tree is computed by the global linear model in Equation (2),
where the parameter vector ~w for the model is computed by the averaged perceptron
training algorithm described in Section 2.2. During training of the dependency parser,
the early-update strategy of Collins and Roark (2004) is used. The intuition is to improve
learning by avoiding irrelevant information, as discussed earlier: When all the items in
the current agenda are incorrect, further parsing steps will be irrelevant because the
correct partial output no longer exists in the candidate ranking.
Both the transition-based and the combined parsers use this training and decod-
ing framework. The main difference between the two parsers is in the definition of
the feature templates. Whereas the transition-based parser uses only transition-based
features, the combined parser applies features from the graph-based parser in addition
to transition-based features. Table 13 shows the templates for transition-based features.
Individual features for a parse are extracted from each transition action that is used to
build the parse, by first instantiating the templates according to the local context, and
then pairing the instantiated template with the transition action. Shown in Figure 9, the
contextual information consists of the top of the stack (ST), the parent (STP) of ST, the
leftmost (STLC) and rightmost child (STRC) of ST, the current word (N0), the next three
Table 13
Transition-based feature templates for the dependency parser.
1 stack top STwt; STw; STt
2 current word N0wt; N0w; N0t
3 next word N1wt; N1w; N1t
4 ST and N0 STwtN0wt; STwtN0w; STwN0wt; STwtN0t; STtN0wt; STwN0w; STtN0t
5 POS bigram N0tN1t
6 POS trigrams N0tN1tN2t; STtN0tN1t; STPtSTtN0t;
STtSTLCtN0t; STtSTRCtN0t; STtN0tN0LCt
7 N0 word N0wN1tN2t; STtN0wN1t; STPtSTtN0w;
STtSTLCtN0w; STtSTRCtN0w; STtN0wN0LCt
w = word; t = POS-tag.
133
Computational Linguistics Volume 37, Number 1
Figure 9
Transition-based feature context for the dependency parser.
words from the input (N1, N2, N3), and the leftmost child of N0 (N0LC). Word and POS
information from the context are manually combined, and the combination of feature
templates is decided by development tests.
Table 14 shows the templates used to extract graph-based features from partial
parses. Templates 1?6 are taken from MSTParser (McDonald and Pereira 2006), a
second-order graph-based parser. They are defined in the context of a word, its parent
and its sibling. To give more templates, features from templates 1?5 are also conjoined
with the arc direction and distance, whereas features from template 6 are also conjoined
with the direction and distance between the child and its sibling. Here ?distance? refers
to the difference between word indexes. Templates 7?8 are two extra feature templates
that capture information about grandchildren and arity (i.e., the number of children
to the left or right). They are difficult to include in an efficient dynamic-programming
decoder, but easy to include in a beam-search decoder. During decoding, the graph-
based feature templates are instantiated at the earliest possible situation. For example,
the first-order arc and second-order sibling feature templates are instantiated when
ARCLEFT or ARCRIGHT is performed, with sibling information for the newly added
link. The arity features are added as soon as the left or right arity of a word becomes
fixed, the left arity templates being instantiated when ARCLEFT or SHIFT is performed,
with the right arity templates being instantiated when ARCLEFT or REDUCE is per-
formed. Similarly, the leftmost grandchild features are instantiated when ARCLEFT or
ARCRIGHT is performed, and the rightmost grandchild features are instantiated when
ARCLEFT or REDUCE is performed.
Table 14
Graph-based feature templates for the dependency parser.
1 Parent word (P) Pw; Pt; Pwt
2 Child word (C) Cw; Ct; Cwt
3 P and C PwtCwt; PwtCw; PwCwt; PwtCt; PtCwt; PwCw; PtCt
4 Tag Bt between P, C PtBtCt
5 Neighbor words PtPLtCtCLt; PtPLtCtCRt; PtPRtCtCLt; PtPRtCtCRt;
of P and C, left (L) PtPLtCLt; PtPLtCRt; PtPRtCLt; PtPRtCRt; PLtCtCLt; PLtCtCRt;
and right (R) PRtCtCLt; PRtCtCRt; PtCtCLt; PtCtCRt; PtPLtCt; PtPRtCt
6 sibling (S) of C CwSw; CtSt; CwSt; CtSw; PtCtSt
7 leftmost (CLC) and PtCtCLCt; PtCtCRCt
rightmost (CRC)
children of C
8 left (la) and right (ra) Ptla; Ptra; Pwtla; Pwtra
arity of P
w = word; t = POS-tag.
134
Zhang and Clark Syntactic Processing
5.1.1 The Comparability of State Items. Our dependency parsers are based on the incre-
mental shift-reduce parsing process. During decoding, state items built with the same
number of transition actions are put onto the agenda and compared with each other. To
build any full parse tree, each word in the input sentence must be put onto the stack
once, and each word except the root of the sentence must be popped off the stack
once. Because the four transition actions are either stack-pushing or stack-popping,
each full parse must be built with 2n ? 1 transition actions, where n denotes the size
of the input. Therefore, the decoding process consists of 2n ? 1 steps, and in each step
all state items in the agenda have been built using the same number of actions. Our
experiments showed that start-of-the-art accuracy can be achieved by this intuitive
method of candidate comparison.
5.2 Experiments for English
We used Penn Treebank 3 for our experiments, which was separated into the training,
development, and test sets in the same way as McDonald, Crammer, and Pereira
(2005), shown in Table 15. Bracketed sentences from the Treebank were translated
into dependency structures using the head-finding rules from Yamada and Matsumoto
(2003).
Before parsing, POS-tags are assigned to the input sentence using our baseline
POS-tagger of Zhang and Clark (2008a), which can be seen as the perceptron tagger
of Collins (2002) with beam-search. Like McDonald, Crammer, and Pereira (2005), we
evaluated the parsing accuracy by the precision of lexical heads (the percentage of input
words, excluding punctuation, that have been assigned the correct parent) and by the
percentage of complete matches, in which all words excluding punctuation have been
assigned the correct parent.
A set of development tests, including the convergence of the perceptron, can be
found in Zhang and Clark (2008a). In this article, we report only the final test accu-
racies and a set of additional speed/accuracy tradeoff results. The accuracies of our
transition-based and combined parsers on English data are shown together with other
systems in Table 16. In the table, each row represents a parsing model. Rows Yamada
2003 and MSTParser represent Yamada and Matsumoto (2003), and MSTParser with
templates 1?6 from Table 14 (McDonald and Pereira 2006), respectively. Rows Transition
and Combined represent our pure transition-based and combined parsers, respectively.
Row Huang 2010 shows the recent work of Huang and Sagae (2010), which applies
dynamic-programming packing to transition-based dependency parsing. Rows Koo
2008 and Chen 2009 represent the models of Koo, Carreras, and Collins (2008) and
Chen et al (2009), which perform semi-supervised learning by word-clustering and
self-training, respectively. Columns Word and Complete show the precision of lexical
Table 15
The training, development, and test data for English dependency parsing.
Sections Sentences Words
Training 2?21 39,832 950,028
Development 22 1,700 40,117
Test 23 2,416 56,684
135
Computational Linguistics Volume 37, Number 1
Table 16
Accuracy comparisons between various dependency parsers on English data.
Word Complete
Yamada 2003 90.3 38.4
Transition 91.4 41.8
MSTParser 91.5 42.1
Combined 92.1 45.4
Huang 2010 92.1 ?
Koo 2008 93.2 ?
Chen 2009 93.2 47.2
See text for details.
Figure 10
The accuracy/speed tradeoff graph for the transition-based and combined dependency parsers.
heads and complete matches, respectively. The combined parser achieved 92.1% per-
word accuracy, which was significantly higher than the pure transition-based parser.4
These results showed the effectiveness of utilizing both graph-based and transition-
based information in a single model. Represented by features that are not available in a
pure transition-based system, graph-based information helped the combined parser to
achieve higher accuracy.
As in the previous sections, we plot the speed/accuracy tradeoff for the transition-
based and combined parsers. For each point in each curve in Figure 10, we run the
development test to decide the number of training iterations, and read the speed and
accuracy from the final test. Each point in the transition curve corresponds to B = 1, 2, 4,
8, 16, 32, 64, and 128, respectively, and each point in the combined curve corresponds to
B = 1, 2, 4, 8, 16, 32, and 64, respectively. When the speed was above 100 sentences per
second, the pure transition-based parser outperformed the combined parser with the
same speed. However, as the size of the beam increases, the accuracy of the combined
parser increased more rapidly. The combined parser gave higher accuracies at the same
speed when the speed was below 50 sentences per second. The accuracy of the pure
4 In Zhang and Clark (2008a) we showed that the combined parser also significantly outperformed the
graph-based parser.
136
Zhang and Clark Syntactic Processing
Table 17
Training, development, and test data for Chinese dependency parsing.
Sections Sentences Words
Training 001?815 16,118 437,859
1,001?1,136
Dev 886?931 804 20,453
1,148?1,151
Test 816?885 1,915 50,319
1,137?1,147
transition parser did not increase beyond B = 64. Our experiments were performed on
a Linux platform with a 2.0GHz CPU and a gcc 4.0.1 compiler.
When B = 1, the transition-based parser was similar to MaltParser, because our
transition-based parser is built upon the arc-each transition process of MaltParser,
and uses similar feature sets. The main difference is that our transition-based parser
performs global training using the perceptron algorithm, whereas MaltParser performs
local training using a support vector machine (SVM) with a polynomial kernel. Because
global training optimizes the model for full sequences of transition actions, a small beam
can potentially have a negative impact on learning, and hence the overall performance.
5.3 Experiments for Chinese
We used the Penn Chinese Treebank 5 for our experiments. Following Duan, Zhao, and
Xu (2007), we split the corpus into training, development, and test data as shown in
Table 17. We used a set of head-finding rules to turn the bracketed sentences into depen-
dency structures, and they can be found in Zhang and Clark (2008a). Like Duan, Zhao,
and Xu (2007), we used gold-standard POS-tags for the input. The parsing accuracy
was evaluated by the percentage of non-root words that have been assigned the correct
head, the percentage of correctly identified root words, and the percentage of complete
matches, all excluding punctuation.
The accuracies are shown in Table 18. Rows Transition and Combined show our
models in the same way as for the English experiments from Section 5.2. Row Duan 2007
represents the transition-based model from Duan, Zhao, and Xu (2007), which applied
beam-search to the deterministic model from Yamada and Matsumoto (2003). Row
Table 18
Test accuracies of various dependency parsers on CTB5 data.
Non-root Root Complete
Duan 2007 84.36 73.70 32.70
Transition 84.69 76.73 32.79
Huang 2010 85.52 78.32 33.72
Combined 86.21 76.26 34.41
See text for details.
137
Computational Linguistics Volume 37, Number 1
Huang 2010 represents the model of Huang and Sagae (2010), which applies dynamic-
programming packing to transition-based parsing. The observations were similar to the
English tests. The combined parser outperformed the transition-based parsers. It gave
the best accuracy we are aware of for supervised dependency parsing using the CTB.
One last question we investigate for this article is the overall performance when the
parser is pipelined with a POS-tagger, or with the joint segmentation and POS-tagging
algorithm in Section 4, forming a complete pipeline for Chinese inputs. The results are
shown in Table 19. For these experiments, we tune the pipelined POS-tagger and the
joint segmentor and POS-tagger on the CTB5 corpus in Table 17, using the development
test data to decide the number of training iterations and reporting the final test accuracy.
The overall accuracy is calculated in F-score: Defining nc as the number of output words
that have been correctly segmented and assigned the correctly segmented head word,
no as the number of words in the output, and nr the number of words in the reference,
precision is p = nc/no and recall is r = nc/nr. When pipelined with a pure POS-tagger
using gold-standard segmentation, the pipelined system gave 93.89% POS accuracy
and 81.21% joint tagging and parsing F-score for non-root words. When combined
with the joint segmentation and POS-tagging system, the segmentation F-score, joint
segmentation and POS-tagging F-score were 95.00% and 90.17%, respectively, and the
joint segmentation and parsing F-score for non-root words (excluding punctuations)
was 75.09%, where the corresponding accuracy with gold-standard segmented and POS-
tagged input was 86.21%, as shown in Table 18.
5.4 Related Work
MSTParser (McDonald and Pereira 2006) is a graph-based parser with an exhaustive
search decoder, and MaltParser (Nivre et al 2006) is a transition-based parser with
a greedy search decoder. Representative of each method, MSTParser and MaltParser
gave comparable accuracies in the CoNLL-X shared task (Buchholz and Marsi 2006).
However, they make different types of errors, which can be seen as a reflection of their
theoretical differences (McDonald and Nivre 2007). By combining graph-based and
transition-based information, our dependency parser achieved higher accuracy than
both graph-based and transition-based baselines. The combination of information is
enabled by our general framework. Our global model does not impose any limitation
on the kind of features that can be used, and therefore allows both graph-based
and transition-based features. Moreover, beam-search frees the decoder from locality
restrictions such as the ?optimal sub-problem? requirement for dynamic-programming,
which limits the range of features in order to achieve reasonable decoding speed.
Compared to the greedy local search decoder for MaltParser, beam-search can reduce
error propagation by keeping track of a set of candidate items.
Table 19
The combined segmentation, POS-tagging, and dependency parsing F-scores using different
pipelined systems.
Seg F Tag F Dep F
Gold seg tag 100.00 100.00 86.21
Gold seg auto tag 100.00 93.89 81.21
Auto seg tag 95.00 90.17 75.09
138
Zhang and Clark Syntactic Processing
Our transition-based parser is based on the arc-eager parsing process of MaltParser
(Nivre et al 2006), although our parser is different from MaltParser in two aspects.
First, we applied beam-search in decoding, which helps to prevent error propagation
of local search. Johansson and Nugues (2007) also use beam search. Second, we use the
perceptron to train whole sequences of transition actions globally, whereas MaltParser
uses SVM to train each transition action locally. Our global training corresponds to beam-
search decoding, which searches for a globally optimal sequence of transition actions
rather than an optimal action at each step.
An existing method to combine multiple parsing algorithms is the ensemble ap-
proach (Sagae and Lavie 2006), which was reported to be useful in improving depen-
dency parsing (Hall et al 2007). A more recent approach (Nivre and McDonald 2008)
combined MSTParser and MaltParser by using the output of one parser for features
in the other in a stacking framework. Both Hall et al (2007) and Nivre and McDonald
(2008) can be seen as methods to combine separately defined models. In contrast, our
parser combines two components in a single model, in which all parameters are trained
consistently.
6. Phrase-Structure Parsing
Phrase-structure parsing is the problem of producing the syntactic structure of an
input sentence according to a phrase-structure grammar. An example phrase-structure
parse tree is shown in Figure 11. Similar to dependency parsing, dominant approaches
to phrase-structure parsing include the transition-based method (Briscoe and Carroll
1993), which builds an output parse tree by choosing a series of transition actions such as
SHIFT and REDUCE, and the graph-based method (Collins 1999; Charniak 2000), which
explores the search space of possible parse trees to find the best output according to
graph-based scores.
For English constituent parsing using the Penn Treebank, the best performing
transition-based parser lags behind the current state-of-the-art (Sagae and Lavie 2005).
However, for Chinese constituent parsing using the Chinese Treebank, Wang et al
(2006) have shown that a shift-reduce parser can give competitive accuracy scores to-
gether with high speeds by using an SVM to make a single decision at each point in the
parsing process.
In Zhang and Clark (2009) we proposed a transition-based constituent parser for
Chinese, which is based on the transition process of Wang et al (2006). Rather than
making a single decision at each processing step, our parser uses a global linear model
Figure 11
An example Chinese lexicalized phrase-structure parse tree.
139
Computational Linguistics Volume 37, Number 1
and beam-search decoding, and achieved competitive accuracy. This phrase-structure
parser can be expressed as an instance of our general framework.
6.1 Instantiating the General Framework
The incremental parsing process of our parser is based on the shift-reduce parsers of
Sagae and Lavie (2005) and Wang et al (2006), with slight modifications. The input
is assumed to be segmented and POS-tagged, and the word?POS pairs waiting to be
processed are stored in a queue. A stack holds the partial parse trees that are built during
the parsing process. The output of this process is a binarized parse tree. The four types
of action used to build a parse are:
r SHIFT, which pushes the next word-POS pair in the queue onto the stack.
r REDUCE?unary?X, which makes a new unary-branching node with
label X; the stack is popped and the popped node becomes the child of
the new node; the new node is pushed onto the stack.
r REDUCE?binary?{L/R}?X, which makes a new binary-branching node
with label X; the stack is popped twice, with the first popped node
becoming the right child of the new node and the second popped node
becoming the left child; the new node is pushed onto the stack. The left (L)
and right (R) versions of the rules indicate whether the head of the new
node is to be taken from the left or right child.
r TERMINATE, which pops the root node off the stack and ends parsing. This
action can only be applied when the input queue is empty, and the stack
contains only one item. The popped node is taken as the output.
Defining the start state as the stack being empty and the queue containing the input
sentence, and the final state as the state immediately after a TERMINATE action, the
incremental process builds a parse tree by repeatedly applying actions from the start
state until the final state is reached. Note that not all sequences of actions produce valid
binarized trees. In the deterministic parser of Wang et al (2006), the highest scoring
action predicted by the classifier may prevent a valid binary tree from being built. In
this case, Wang et al simply return a partial parse consisting of all the subtrees on
the stack. In our parser a set of restrictions is applied which guarantees a valid parse
tree. For example, two simple restrictions are that a SHIFT action can only be applied
if the queue of incoming words is non-empty, and the binary reduce actions can only
be performed if the stack contains at least two nodes. Some of the restrictions are more
complex than this; the full set can be found in Zhang and Clark (2009). Wang et al
(2006) give a detailed example showing how a segmented and POS-tagged sentence can
be incrementally processed using the shift-reduce actions to produce a binary tree. We
show this example in Figure 12.
For the decoding problem, our parser performs beam-search using an agenda to
find the output parse from possible candidates. Initially containing a start state item, a
set of state items is used to generate new state items for each processing step. At each
step, each of the state items is extended using all applicable actions, generating a set of
new state items, which are put onto the agenda. After each step, the B-best items from
the agenda are taken for the generation of new state items in the next step. The same
140
Zhang and Clark Syntactic Processing
Figure 12
An example shift-reduce parsing process.
process repeats until the highest scored item from the agenda is in the final state, and it
is taken as the final parse.
This decoding process can be seen as an instance of the generic algorithm in Fig-
ure 1. Here a state item is a pair ?S, Q?, where S represents the stack with partial parses,
and Q represents the incoming queue. The initial state item STARTITEM(constituent
parsing) is the start state item, where the stack is empty and the queue contains the
141
Computational Linguistics Volume 37, Number 1
whole input sentence, the function EXPAND(candidate, constituent parsing) extends can-
didate by using all applicable actions to generate a set of new state items, and
GOALTEST (constituent parsing, best) returns true if best reaches the final state, and false
otherwise.
The score of a parse tree is computed by the global linear model in Equation (2),
where the parameter vector ~w for the model is computed by the averaged perceptron
training algorithm described in Section 2.2. As in the training of the dependency parser,
the early-update strategy of Collins and Roark (2004) is used.
Table 20 shows the set of feature templates for the model. Individual features are
generated from these templates by first instantiating a template with particular labels,
words, and tags, and then pairing the instantiated template with a particular action.
In the table, the symbols S0, S1, S2, and S3 represent the top four nodes on the stack,
and the symbols N0, N1, N2, and N3 represent the first four words in the incoming
queue. S0L, S0R, and S0U represent the left and right child for binary branching S0,
and the single child for unary branching S0, respectively; w represents the lexical head
token for a node; and c represents the label for a node. When the corresponding node
is a terminal, c represents its POS-tag, whereas when the corresponding node is a non-
terminal, c represents its constituent label; t represents the POS-tag for a word.
The context S0, S1, S2, S3, N0, N1, N2, N3 for the feature templates is taken from Wang
et al (2006). However, Wang et al (2006) used a polynomial kernel function with an
SVM and did not manually create feature combinations. Because we used the linear per-
ceptron algorithm we manually combined Unigram features into Bigram and Trigram
features.
The Bracket row shows bracket-related features, which were inspired by Wang et al
(2006). Here brackets refer to left brackets including ??, ?fi?, and ?
? and right brackets
including ?	?, ?fl?, and ??. In the table, b represents the matching status of the last
left bracket (if any) on the stack. It takes three different values: 1 (no matching right
bracket has been pushed onto stack), 2 (a matching right bracket has been pushed
onto stack), and 3 (a matching right bracket has been pushed onto stack, but then
popped off).
Table 20
Feature templates for the phrase-structure parser.
Description Feature templates
Unigrams S0tc, S0wc, S1tc, S1wc, S2tc, S2wc, S3tc, S3wc,
N0wt, N1wt, N2wt, N3wt,
S0lwc, S0rwc, S0uwc, S1lwc, S1rwc, S1uwc,
Bigrams S0wS1w, S0wS1c, S0cS1w, S0cS1c,
S0wN0w, S0wN0t, S0cN0w, S0cN0t,
N0wN1w, N0wN1t, N0tN1w, N0tN1t
S1wN0w, S1wN0t, S1cN0w, S1cN0t,
Trigrams S0cS1cS2c, S0wS1cS2c, S0cS1wS2c, S0cS1cS2w,
S0cS1cN0t, S0wS1cN0t, S0cS1wN0t, S0cS1cN0w
Bracket S0wb, S0cb
S0wS1cb, S0cS1wb, S0cS1cb, S0wN0tb, S0cN0wb, S0cN0tb
Separator S0wp, S0wcp, S0wq, S0wcq, S1wp, S1wcp, S1wq, S1wcq
S0cS1cp, S0cS1cq
w = word; c = constituent label; t = POS-tag.
142
Zhang and Clark Syntactic Processing
Table 21
The standard split of CTB2 data for phrase-structure parsing.
Sections Sentences Words
Training 001?270 3,475 85,058
Development 301?325 355 6,821
Test 271?300 348 8,008
The Separator row shows features that include one of the separator punctuations
(i.e., ??, ??, ??, and ?ff?) between the head words of S0 and S1. These templates apply
only when the stack contains at least two nodes; p represents a separator punctuation
symbol. Each unique separator punctuation between S0 and S1 is only counted once
when generating the global feature vector. q represents the count of any separator
punctuation between S0 and S1.
6.1.1 The Comparability of State Items. Just as in our dependency parsers, the phrase-
structure parser is based on an incremental shift-reduce parsing process, and state
items built with the same number of transition actions are compared with each other
during decoding. However, due to the possibility of unary-reduce actions, the number
of actions used to build full parse trees can vary given an input sentence. This makes the
comparison of state items more difficult than for dependency parsing, because when
some state items on the agenda are completed (i.e., in the final state), others on the
agenda may still need more actions to complete. We choose to push completed state
items back onto the agenda during the decoding process, without changing them. The
decoding continues until the highest scored state item on the agenda is completed.
When decoding stops, the highest scored state item on the agenda is taken as the final
output. In this process, completed state items that do not have the highest score in the
agenda are kept unchanged and compared with incomplete state items, even though
they may have been built with different numbers of actions. Our experiments showed
that this approach gave reasonable accuracies.
6.2 Experiments
The experiments were performed using the Chinese Treebank 2 (Table 21) and Chinese
Treebank 5 data. Standard data preparation was performed before the experiments:
Empty terminal nodes were removed; any non-terminal nodes with no children were
removed; any unary X ? X nodes resulting from the previous steps were collapsed
into one X node.
For all experiments, we used the EVALB tool5 for evaluation, and used labeled recall
(LR), labeled precision (LP) and F1 score (which is the harmonic mean of LR and LP) to
measure parsing accuracy.
6.2.1 Test Results on CTB2. The following tests were performed using both gold-standard
POS-tags and POS-tags automatically assigned by a POS-tagger. We used our base-
line POS-tagger in Section 4 for automatic POS-tagging. The results of various models
5 http://nlp.cs.nyu.edu/evalb/.
143
Computational Linguistics Volume 37, Number 1
Table 22
Accuracies of various phrase-structure parsers on CTB2 with gold-standard POS-tags.
Model LR LP F1
Bikel Thesis 80.9% 84.5% 82.7%
Wang 2006 SVM 87.2% 88.3% 87.8%
Wang 2006 Stacked 88.3% 88.1% 88.2%
Our parser 89.4% 90.1% 89.8%
See text for details.
evaluated on sentences with less than 40 words and using gold-standard POS-tags are
shown in Table 22. The rows represent the model from Bikel and Chiang (2000) and
Bikel (2004), the SVM and ensemble models from Wang et al (2006), and our parser,
respectively. The accuracy of our parser is competitive using this test set.
The results of various models using automatically assigned POS-tags are shown in
Table 23. The rows in the table represent the models from Bikel and Chiang (2000), Levy
and Manning (2003), Xiong et al (2005), Bikel (2004), Chiang and Bikel (2002), the SVM
model from Wang et al (2006), the ensemble system from Wang et al (2006), and the
parser of this article, respectively. Our parser gave comparable accuracies to the SVM
and ensemble models from Wang et al (2006). However, comparison with Table 22
shows that our parser is more sensitive to POS-tagging errors than some of the other
models. One possible reason is that some of the other parsers (e.g., Bikel 2004) use the
parser model itself to resolve tagging ambiguities, whereas we rely on a POS-tagger to
accurately assign a single tag to each word. In fact, for the Chinese data, POS-tagging
accuracy is not high, with the perceptron-based tagger achieving an accuracy of only
93%. The beam-search decoding framework we use could accommodate joint parsing
and tagging, although the use of features based on the tags of incoming words com-
plicates matters somewhat, because these features rely on tags having been assigned to
all words in a pre-processing step. One possible solution would be generating multiple
POS-tags for each word during tagging, and incorporating tag information into the shift
action, so that the parser will resolve the POS-tag ambiguity. We leave this problem for
future work.
Table 23
Accuracies of various phrase-structure parsers on CTB2 with automatically assigned tags.
? 40 words ? 100 words Unlimited
LR LP F1 POS LR LP F1 POS LR LP F1 POS
Bikel 2000 76.8% 77.8% 77.3% - 73.3% 74.6% 74.0% - - - - -
Levy 2003 79.2% 78.4% 78.8% - - - - - - - - -
Xiong 2005 78.7% 80.1% 79.4% - - - - - - - - -
Bikel Thesis 78.0% 81.2% 79.6% - 74.4% 78.5% 76.4% - - - - -
Chiang 2002 78.8% 81.1% 79.9% - 75.2% 78.0% 76.6% - - - - -
W06 SVM 78.1% 81.1% 79.6% 92.5% 75.5% 78.5% 77.0% 92.2% 75.0% 78.0% 76.5% 92.1%
W06 Stacked 79.2% 81.1% 80.1% 92.5% 76.7% 78.4% 77.5% 92.2% 76.2% 78.0% 77.1% 92.1%
Our parser 80.2% 80.5% 80.4% 93.5% 76.5% 77.7% 77.1% 93.1% 76.1% 77.4% 76.7% 93.0%
See text for details.
144
Zhang and Clark Syntactic Processing
Figure 13
The accuracy/speed tradeoff graph for the phrase-structure parser.
Petrov and Klein (2007) reported LR and LP of 85.7% and 86.9% for sentences
with less than 40 words and 81.9% and 84.8% for all sentences on the CTB2 test set,
respectively. These results are significantly better than any model from Table 23.
However, we did not include their scores in the table because they used a different
training set from CTB5, which is much larger than the CTB2 training set used by all
parsers in the table. In order to make a comparison, we split the data in the same way
as Petrov and Klein and tested our parser using automatically assigned POS-tags. It
gave LR and LP of 82.0% and 80.9% for sentences with less than 40 words and 77.8%
and 77.4% for all sentences, significantly lower than Petrov and Klein. Possible reasons
include the sensitivity of our parser to POS-tag errors, and perhaps the use of a latent
variable model by Petrov and Klein.
As in the previous sections, we plot the speed/accuracy tradeoff for our phrase-
structure parser. For each point in each curve in Figure 13, we run the development
test to decide the number of training iterations, and draw the point with speed and
accuracy from the final test. Each point in the curve corresponds to B = 1, 2, 4, 8, 16,
and 32, respectively. The accuracies increased when the beam increased from 1 to 4, but
fluctuated when the beam increased beyond 4. In contrast to the development tests, the
accuracy reached its maximum when the beam size was 4 rather than 16. However,
the general trend of increased accuracy as the speed decreases can still be observed,
and the amount of increase diminishes as the speed decreases. These experiments were
performed on a Linux platform with a 2.0GHz CPU and a gcc 4.0.1 compiler.
6.2.2 Test Accuracy Using CTB5. Table 24 presents the performance of the parser on CTB5.
We adopt the data split from the previous section, as shown in Table 17. We used the
same parser configurations as Section 6.2.1.
Table 24
Accuracies of our phrase-structure parser on CTB5 using gold-standard and automatically
assigned POS-tags.
? 40 words Unlimited
LR LP F1 POS LR LP F1 POS
87.9% 87.5% 87.7% 100% 86.9% 86.7% 86.8% 100%
80.2% 79.1% 79.6% 94.1% 78.6% 78.0% 78.3% 93.9%
145
Computational Linguistics Volume 37, Number 1
As an additional evaluation we also produced dependency output from the phrase-
structure trees, using the head-finding rules, so that we can compare with dependency
parsers. We compare the dependencies read off our constituent parser using CTB5 data
with the dependency parser from Section 5, which currently gives the best dependency
parsing accuracy on CTB5. The same measures are taken and the accuracies with gold-
standard POS-tags are shown in Table 25. Our constituent parser gave higher accuracy
than the combined dependency parser. It is interesting that, though the constituent
parser uses many fewer feature templates than the dependency parser, the features do
include constituent information, which is unavailable to the dependency parser.
6.3 Related Work
Our parser is based on the shift-reduce parsing process from Sagae and Lavie (2005)
and Wang et al (2006), and therefore it can be classified as a transition-based parser
(Nivre et al 2006). An important difference between our parser and the Wang et al
(2006) parser is that our parser is based on a discriminative learning model with global
features, whereas the parser from Wang et al (2006) is based on a local classifier that
optimizes each individual choice. Instead of greedy local decoding, we used beam-
search in the decoder.
An early work that applies beam-search to constituent parsing is Ratnaparkhi
(1999). The main difference between our parser and Ratnaparkhi?s is that we use a
global discriminative model, whereas Ratnaparkhi?s parser has separate probabilities
of actions chained together in a conditional model.
Both our parser and the parser from Collins and Roark (2004) use a global discrim-
inative model and an incremental parsing process. The major difference is that Collins
and Roark, like Roark (2001), follow a top?down derivation strategy, whereas we chose
to use a shift-reduce process which has been shown to give state-of-the-art accuracies
for Chinese (Wang et al 2006). In addition, we did not include a generative baseline
model in the discriminative model, as did Collins and Roark (2004).
7. Discussion
We have demonstrated in the previous sections that accuracies competitive with the
state-of-the-art can be achieved by our general framework for Chinese word segmen-
tation, joint word segmentation and POS-tagging, Chinese and English dependency
parsing, and Chinese phrase-structure parsing. Besides these tasks, our baseline POS-
tagger in Section 4 is also implemented in the framework. When it is applied to English
POS-tagging using feature templates from Collins (2002), it gave similar accuracy to the
Table 25
Comparison of dependency accuracies between phrase-structure parsing and dependency
parsing using CTB5 data.
Non-root Root Complete
Dependency parser 86.21% 76.26% 34.41%
Constituent parser 86.95% 79.19% 36.08%
146
Zhang and Clark Syntactic Processing
dynamic-programming decoder of Collins (2002). All these experiments suggest that the
general yet efficient framework provides a competitive solution for structural prediction
problems with an incremental output-building process. In this section, we discuss the
main reasons for the effectiveness of the general framework, as well as its prerequisites,
advantages, and limitations when applied to a general task.
One of the main reasons for the high accuracies achieved by this framework is the
freedom in using arbitrary features to capture statistical patterns, including those that
lead to impractical time complexity with alternative learning and decoding frameworks
and algorithms such as CRFs and dynamic programming. This freedom was exploited
by our effort to incorporate larger sources of statistical information for the improve-
ment of accuracy. For example, our word-based segmentor extends the character-based
approach by including word information; our joint word segmentor and POS-tagger
utilizes POS information for word segmentation; our combined dependency parser
includes statistical information from two different methods in a single, consistently
trained model. The models gave state-of-the-art accuracies in these problems, demon-
strating the advantage of using flexible information covering large parts of the output
structure.
Compared to alternative discriminative training algorithms such as structural SVM
(Tsochantaridis et al 2004) and CRFs (Lafferty, McCallum, and Pereira 2001; Clark and
Curran 2007), the perceptron has a simple parameter update process, which is often
efficient in both memory usage and running time depending on the decoding algorithm.
Consider joint word segmentation and POS-tagging for example; we found in our
experiments that practical training times can be achieved by the perceptron algorithm,
but not with structural SVM. MIRA (Crammer and Singer 2003) and its simplifications
(McDonald, Crammer, and Pereira 2005; Crammer et al 2006) are also commonly used
learning algorithms to train a global linear model. They can be treated as slower but
potentially more accurate alternatives to the perceptron for the general framework. We
experimented with these for joint segmentation and tagging but did not improve upon
the perceptron.
Beam-search enables training to be performed efficiently for extremely large com-
plex search spaces, for which dynamic programming algorithms may be impractical.
For the joint word segmentation and POS-tagging problem, a dynamic-programming
decoder is prohibitively slow but a beam-search decoder runs in reasonable time.
A more important advantage of beam-search compared to dynamic-programming
is that beam-search allows arbitrary features, whereas the efficiency of a dynamic-
programming decoder is restricted by the range of features, due to its requirement
for optimal substructure. For our combined dependency parser, the feature set makes
a dynamic-programming decoder infeasibly slow. From this perspective, beam-search
is in line with other recent research on the improvement of accuracies by incorporat-
ing non-local features via approximation, such as belief propagation for dependency
parsing (Smith and Eisner 2008), integer linear programming for dependency parsing
(Martins, Smith, and Xing 2009), and forest reranking for phrase-structure parsing
(Huang 2008).
The only prerequisite of the framework is an incremental process, which consumes
the input sentence and builds the output structure using a sequence of actions. All
four problems studied in the article were first turned into an incremental process, and
then solved by applying the framework. The number of distinct actions for a problem
is dependent on the complexity of the output. For word segmentation, there are only
two actions (append or separate). For transition-based unlabeled dependency parsing,
there are four actions (shift, arc-left, arc-right, and reduce). For joint segmentation and
147
Computational Linguistics Volume 37, Number 1
POS-tagging and constituent parsing, there are many more distinct actions according
to the set of labels. For example, the number of distinct unary-reduce actions for con-
stituent parsing is equal to the number of distinct constituent labels that form unary-
branching nodes. The incremental processes for all four problems have linear time
complexity, and a larger number of distinct actions leads to a slower decoder.
One of the most important issues in using beam-search is the comparability of par-
tially built structures at each incremental step during decoding. For word segmentation
and joint segmentation and POS-tagging, we compare partially built sentences that have
the same number of characters. For word segmentation, partial words can be treated in
the same way as full words, without losing much accuracy. The same approach was
not effective when applied to joint segmentation and POS-tagging, for which partial
words must be treated differently, or avoided by alternative inference such as using
multiple-beams. For dependency parsing, we compared partial outputs that have been
built using the same number of transition actions. Because all parses for a sentence with
size n are built using exactly 2n ? 1 transition actions, this comparison is consistent
throughout the decoding process. For constituent parsing, we initially compare partial
outputs that have been built using the same number of actions. However, because
different output parse trees can contain a different number of unary-reduce actions,
some candidate outputs will be completed earlier than others. When this happens, we
choose to retain fully built outputs in the agenda while continuing the decoding process,
until the highest scored item in the agenda is a complete parse. Therefore, there are
situations when different parses in the agenda have a different number of actions. This
did not turn out to be a significant problem. We believe that the most effective way to
organize output comparison is largely an empirical question.
Finally, alternative components can be used to replace the learning or decoding
algorithms of the framework to give higher accuracies. Huang and Sagae (2010) have
recently applied dynamic-programming to dependency parsing to pack items that have
the same signature in the beam, and obtained higher accuracy than our transition-based
dependency parser in Section 5.
8. Conclusion
We explored word segmentation, joint word segmentation and POS-tagging, depen-
dency parsing, and phrase-structure parsing using a general framework of a global
linear model, trained by the perceptron algorithm and decoded with beam-search.
We have chosen to focus on Chinese; the framework itself and our algorithms for the
specific problems are language-independent, however. In Section 5 we reported results
on English dependency parsing. Despite the rather simple nature of the decoding and
training processes, the framework achieved accuracies competitive with the state-of-
the-art for all the tasks we considered, by making use of a large range of statistical
information. As further evidence of the effectiveness of our framework, we have re-
cently adapted our phrase-structure parser in Section 6 to parsing with a lexicalized
grammar formalism, Combinatory Categorial Grammar (CCG), and achieved higher
F-scores than the state-of-the-art C&C CCG parser (Clark and Curran 2007). The range of
problems that we have studied suggests that our framework is a simple yet competitive
option for structural prediction problems in general.
Our source code can be found at www.sourceforge.net/projects/zpar. It contains
the general framework, our implementations of the four tasks addressed in this article,
and other tools for syntactic processing.
148
Zhang and Clark Syntactic Processing
Acknowledgments
This work was largely carried out while
Yue Zhang was a DPhil student at the
Oxford University Computing Laboratory,
where he was supported by an ORS
Scholarship and the Clarendon Fund.
Stephen Clark was supported by EPSRC
grant EP/E035698/1.
References
Bikel, Daniel M. 2004. On the Parameter Space
of Generative Lexicalized Statistical Parsing
Models. Ph.D. thesis, University of
Pennsylvania.
Bikel, Daniel M. and David Chiang. 2000.
Two statistical parsing models applied
to the Chinese Treebank. In Proceedings of
SIGHAN Workshop, pages 1?6, Hong Kong.
Briscoe, Ted and John Carroll. 1993.
Generalized probabilistic LR parsing
of natural language (corpora) with
unification-based grammars.
Computational Linguistics, 19(1):25?59.
Buchholz, Sabine and Erwin Marsi. 2006.
aonll-X shared task on multilingual
dependency parsing. In Proceedings of
CoNLL, pages 149?164, New York, NY.
Carreras, Xavier, Michael Collins, and Terry
Koo. 2008. Tag, dynamic programming,
and the perceptron for efficient,
feature-rich parsing. In Proceedings of
CoNLL, pages 9?16, Manchester.
Carreras, Xavier, Mihai Surdeanu, and Lluis
Marquez. 2006. Projective dependency
parsing with perceptron. In Proceedings of
CoNLL, pages 181?185, New York, NY.
Charniak, Eugene. 2000. A
maximum-entropy-inspired parser. In
Proceedings of NAACL, pages 132?139,
Seattle, WA.
Chen, Wenliang, Jun?ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009.
Improving dependency parsing with
subtrees from auto-parsed data. In
Proceedings of EMNLP, pages 570?579,
Singapore.
Chiang, David and Daniel M. Bikel. 2002.
Recovering latent information in
treebanks. In Proceedings of COLING,
pages 183?198, Taipei.
Clark, Stephen and James R. Curran.
2007. Wide-coverage efficient statistical
parsing with CCG and log-linear
models. Computational Linguistics,
33(4):493?552.
Collins, Michael. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania.
Collins, Michael. 2002. Discriminative
training methods for hidden Markov
models: Theory and experiments with
perceptron algorithms. In Proceedings of
EMNLP, pages 1?8, Philadelphia, PA.
Collins, Michael and Brian Roark. 2004.
Incremental parsing with the perceptron
algorithm. In Proceedings of ACL,
pages 111?118, Barcelona.
Crammer, Koby, Ofer Dekel, Joseph Keshet,
Shai Shalev-Shwartz, and Yoram Singer.
2006. Online passive-aggressive
algorithms. Journal of Machine Learning
Research, 7:551?585.
Crammer, Koby and Yoram Singer. 2003.
Ultraconservative online algorithms for
multiclass problems. Journal of Machine
Learning Research, 3:951?991.
Duan, Xiangyu, Jun Zhao, and Bo Xu. 2007.
Probabilistic models for action-based
Chinese dependency parsing. In
Proceedings of ECML/ECPPKDD,
pages 559?566, Warsaw.
Emerson, Thomas. 2005. The second
international Chinese word segmentation
bakeoff. In Proceedings of SIGHAN
Workshop, pages 123?133, Jeju.
Finkel, Jenny Rose, Alex Kleeman, and
Christopher D. Manning. 2008. Efficient,
feature-based, conditional random field
parsing. In Proceedings of ACL/HLT,
pages 959?967, Columbus, OH.
Freund, Y. and R. Schapire. 1999. Large
margin classification using the perceptron
algorithm. In Rob Holte, editor, Machine
Learning. Kluwer, Boston, MA,
pages 277?296.
Hall, Johan, Jens Nilsson, Joakim Nivre,
Gu?lsen Eryigit, Bea?ta Megyesi, Mattias
Nilsson, and Markus Saers. 2007. Single
malt or blended? A study in multilingual
parser optimization. In Proceedings of the
CoNLL Shared Task Session of EMNLP/
CoNLL, pages 933?939, Prague.
Huang, Liang. 2008. Forest reranking:
Discriminative parsing with non-local
features. In Proceedings of ACL/HLT,
pages 586?594, Columbus, OH.
Huang, Liang and Kenji Sagae. 2010.
Dynamic programming for linear-time
incremental parsing. In Proceedings of ACL,
pages 1077?1086, Uppsala.
Jiang, Wenbin, Liang Huang, Qun Liu, and
Yajuan Lu?. 2008. A cascaded linear model
for joint Chinese word segmentation and
part-of-speech tagging. In Proceedings of
ACL/HLT, pages 897?904, Columbus, OH.
Jiang, Wenbin, Haitao Mi, and Qun Liu. 2008.
Word lattice reranking for Chinese word
149
Computational Linguistics Volume 37, Number 1
segmentation and part-of-speech tagging.
In Proceedings of COLING, pages 385?392,
Manchester.
Johansson, Richard and Pierre Nugues. 2007.
Incremental dependency parsing using
online learning. In Proceedings of the
CoNLL/EMNLP, pages 1134?1138, Prague.
Koo, Terry, Xavier Carreras, and Michael
Collins. 2008. Simple semi-supervised
dependency parsing. In Proceedings of
ACL/HLT, pages 595?603, Columbus, OH.
Kruengkrai, Canasai, Kiyotaka Uchimoto,
Jun?ichi Kazama, Yiou Wang, Kentaro
Torisawa, and Hitoshi Isahara. 2009. An
error-driven word-character hybrid model
for joint Chinese word segmentation and
POS tagging. In Proceedings of ACL/AFNLP,
pages 513?521, Suntec.
Lafferty, J., A. McCallum, and F. Pereira.
2001. Conditional random fields:
Probabilistic models for segmenting and
labeling sequence data. In Proceedings of
ICML, pages 282?289, Williamstown, MA.
Levy, Roger and Christopher D. Manning.
2003. Is it harder to parse Chinese, or the
Chinese treebank? In Proceedings of ACL,
pages 439?446, Sapporo.
Martins, Andre, Noah Smith, and Eric Xing.
2009. Concise integer linear programming
formulations for dependency parsing. In
Proceedings of ACL/AFNLP, pages 342?350,
Suntec.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Online
large-margin training of dependency
parsers. In Proceedings of ACL, pages 91?98,
Ann Arbor, MI.
McDonald, Ryan and Joakim Nivre. 2007.
Characterizing the errors of data-driven
dependency parsing models. In Proceedings
of EMNLP/CoNLL, pages 122?131, Prague.
McDonald, Ryan and Fernando Pereira.
2006. Online learning of approximate
dependency parsing algorithms. In
Proceedings of EACL, pages 81?88, Trento.
McDonald, Ryan, Fernando Pereira,
Kiril Ribarov, and Jan Hajic. 2005.
Non-projective dependency parsing
using spanning tree algorithms. In
Proceedings of HLT/EMNLP, pages 523?530,
Vancouver.
Nakagawa, Tetsuji and Kiyotaka Uchimoto.
2007. A hybrid approach to word
segmentation and POS tagging. In
Proceedings of ACL Demo and Poster
Session, Prague.
Ng, Hwee Tou and Jin Kiat Low. 2004.
Chinese part-of-speech tagging:
One-at-a-time or all-at-once? Word-based
or character-based? In Proceedings of
EMNLP, pages 227?284, Barcelona.
Nivre, Joakim, Johan Hall, Sandra Ku?bler,
Ryan McDonald, Jens Nilsson, Sebastian
Riedel, and Deniz Yuret. 2007. The CoNLL
2007 shared task on dependency parsing.
In Proceedings of the CoNLL Shared Task
Session of EMNLP/CoNLL, pages 915?932,
Prague.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Gu?ls?en Eryig?it, and Svetoslav Marinov.
2006. Labeled pseudo-projective
dependency parsing with support vector
machines. In Proceedings of CoNLL,
pages 221?225, New York, NY.
Nivre, Joakim and Ryan McDonald.
2008. Integrating graph-based and
transition-based dependency parsers. In
Proceedings of ACL/HLT, pages 950?958,
Columbus, OH.
Peng, F., F. Feng, and A. McCallum. 2004.
Chinese segmentation and new word
detection using conditional random fields.
In Proceedings of COLING, pages 562?568,
Geneva.
Petrov, Slav and Dan Klein. 2007. Improved
inference for unlexicalized parsing. In
Proceedings of HLT/NAACL, pages 404?411,
Rochester, NY.
Ratnaparkhi, Adwait. 1998. Maximum
Entropy Models for Natural Language
Ambiguity Resolution. Ph.D. thesis,
University of Pennsylvania.
Ratnaparkhi, Adwait. 1999. Learning to
parse natural language with maximum
entropy models. Machine Learning,
34(1-3):151?175.
Roark, Brian. 2001. Probabilistic top?down
parsing and language modeling.
Computational Linguistics, 27:249?276.
Sagae, Kenji and Alon Lavie. 2005. A
classifier-based parser with linear
run-time complexity. In Proceedings of
IWPT, pages 125?132, Vancouver.
Sagae, Kenji and Alon Lavie. 2006. Parser
combination by reparsing. In Proceedings
of HLT/NAACL, Companion Volume: Short
Papers, pages 129?132, New York, NY.
Shi, Yanxin and Mengqiu Wang. 2007. A
dual-layer CRF based joint decoding
method for cascade segmentation and
labelling tasks. In Proceedings of IJCAI,
pages 1707?1712, Hyderabad.
Smith, David and Jason Eisner. 2008.
Dependency parsing by belief
propagation. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 145?156,
Honolulu, HI.
150
Zhang and Clark Syntactic Processing
Sproat, R., C. Shih, W. Gail, and N. Chang.
1996. A stochastic finite-state
word-segmentation algorithm for Chinese.
Computational Linguistics, 22(3):377?404.
Sproat, Richard and Thomas Emerson. 2003.
The first international Chinese word
segmentation bakeoff. In Proceedings of The
Second SIGHAN Workshop, pages 282?289,
Sapporo.
Tsochantaridis, I., T. Hofmann, T. Joachims,
and Y. Altun. 2004. Support vector
machine learning for interdependent and
structured output spaces. In Proceedings
of ICML, pages 102?114, Banff.
Wang, Xinhao, Xiaojun Lin, Dianhai Yu,
Hao Tian, and Xihong Wu. 2006. Chinese
word segmentation with maximum
entropy and n-gram language model.
In Proceedings of SIGHAN Workshop,
pages 138?141, Sydney.
Xia, Fei. 2000. The Part-of-Speech Tagging
Guidelines for the Chinese Treebank (3.0),
University of Pennsylvania.
Xiong, Deyi, Shuanglong Li, Qun Liu,
Shouxun Lin, and Yueliang Qian. 2005.
Parsing the Penn Chinese Treebank with
semantic knowledge. In Proceedings of
IJCNLP, pages 70?81, Jeju.
Xue, N. 2003. Chinese word segmentation as
character tagging. International Journal of
Computational Linguistics and Chinese
Language Processing, 8(1):29?48.
Yamada, H. and Y. Matsumoto. 2003.
Statistical dependency analysis using
support vector machines. In Proceedings
of IWPT, pages 195?206, Nancy.
Zhang, Ruiqiang, Genichiro Kikui, and
Eiichiro Sumita. 2006. Subword-based
tagging by conditional random
fields for Chinese word segmentation.
In Proceedings of the HLT/NAACL,
Companion, volume Short Papers,
pages 193?196, New York, NY.
Zhang, Yue and Stephen Clark. 2007.
Chinese segmentation with a word-based
perceptron algorithm. In Proceedings of
ACL, pages 840?847, Prague.
Zhang, Yue and Stephen Clark. 2008a. Joint
word segmentation and POS tagging using
a single perceptron. In Proceedings of
ACL/HLT, pages 888?896, Columbus, OH.
Zhang, Yue and Stephen Clark. 2008b.
A tale of two parsers: Investigating
and combining graph-based and
transition-based dependency parsing
using beam-search. In Proceedings of
EMNLP, pages 562?571, Honolulu, HI.
Zhang, Yue and Stephen Clark. 2009.
Transition-based parsing of the Chinese
Treebank using a global discriminative
model. In Proceedings of IWPT,
pages 162?171, Paris.
Zhang, Yue and Stephen Clark. 2010. A fast
decoder for joint word segmentation and
POS-tagging using a single discriminative
model. In Proceedings of EMNLP,
pages 843?852, Cambridge, MA.
Zhao, Hai, Chang-Ning Huang, and Mu Li.
2006. An improved Chinese word
segmentation system with conditional
random field. In Proceedings of SIGHAN
Workshop, pages 162?165, Sydney.
151
152
Proceedings of the NAACL HLT 2010: Tutorial Abstracts, pages 7?8,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Recent Advances in Dependency Parsing
Qin Iris Wang, AT&T Interactive
Yue Zhang, Oxford
   Data-driven (statistical) approaches have been playing an
increasingly prominent role in parsing since the 1990s. In recent
years, there has been a growing interest in dependency-based as
opposed to constituency-based approaches to syntactic parsing, with
application to a wide range of research areas and different languages.
Graph-based and transition-based methods are the two dominant
data-driven approaches to dependency parsing. In a graph-based model,
it defines a space of candidate dependency trees for a given sentence.
Each candidate tree is scored via a local or global scoring function.
The parser (usually uses dynamic programming) outputs the
highest-scored tree. In contrast, in a transition-based model, it
defines a transition system for mapping a sentence to its dependency
tree.  It induces a model for predicting the next state transition,
given the transition history. Given the induced model, the output
parse tree is built deterministically upon the construction of the
optimal transition sequence.
   Both Graph-based and transition-based approaches have been used to
achieve state-of-the-art dependency parsing results for a wide range
of languages. Some researchers have used the combination of the two
models and it shows the performance of the combined model is
significantly better than the individual models. Another recent trend
is to apply online training to shift-reduce parsing in the
transition-based models. In this tutorial, we first introduce the two
main-stream data-driven dependency parsing models--- graph-based and
transition-based models. After comparing the differences between them,
we show how these two models can be combined in various ways to
achieve better results.
Outline
Part A: Introduction to Dependency Parsing
Part B: Graph-based Dependency Parsing Models
- Learning Algorithms (Local Learning vs. Global Learning)
- Parsing Algorithms (Dynamic Programming)
- Features (Static Features vs. Dynamic Features)
7
Part C: Transition-based Dependency Parsing Models
- Learning Algorithms (Local Learning vs. online Learning)
- Parsing Algorithms (Shift-reduce Parsing)
- Features
Part D: The Combined Models
- The stacking Method
- The ensemble Method
- Single-model Combination
Part E: Other Recent Trends in Dependency Parsing
- Integer Linear Programming
- Fast Non-Projective Parsing
Presenters
Qin Iris Wang
Email: qiniriswang@gmail.com
Qin Iris Wang is currently a Research Scientist at AT&T Interactive
(San Francisco). Qin obtained her PhD in 2008 from the University of
Alberta under Dekang Lin and Dale Schuurmans. Qin's research interests
include NLP (in particular dependency parsing), machine learning,
information retrieval, text mining and large scale data processing.
Qin's PhD studies was focused on Learning Structured Classifiers for
Statistical Dependency Parsing. Before joined AT&T, she was a research
scientist at Yahoo Labs. Qin was a teaching assistant for two years
during her PhD studies. In 2009, Qin organized a workshop on "
Semi-supervised Learning for Natural Language Processing" at
NAACL-HLT.
Yue Zhang
Email: yue.zhang@comlab.ox.ac.uk
Yue Zhang just defended his PhD thesis at the University of Oxford.
Yue's research interests include natural language processing (word
segmentation, parsing, machine translation), machine learning, etc.
More specifically, his research area is the syntactic analysis of the
Chinese language, using discriminative machine-learning approaches. He
has worked on word segmentation, joint word segmentation and
POS-tagging, phrase-structure parsing and dependency parsing. Yue
worked on Chinese-English machine-translation during MSc studies in
Oxford, and parallel computing during undergrad studies in Tsinghua
University.
8
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 683?692,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Shift-Reduce CCG Parsing
Yue Zhang
University of Cambridge
Computer Laboratory
yue.zhang@cl.cam.ac.uk
Stephen Clark
University of Cambridge
Computer Laboratory
stephen.clark@cl.cam.ac.uk
Abstract
CCGs are directly compatible with binary-
branching bottom-up parsing algorithms, in
particular CKY and shift-reduce algorithms.
While the chart-based approach has been the
dominant approach for CCG, the shift-reduce
method has been little explored. In this paper,
we develop a shift-reduce CCG parser using
a discriminative model and beam search, and
compare its strengths and weaknesses with the
chart-based C&C parser. We study different
errors made by the two parsers, and show that
the shift-reduce parser gives competitive accu-
racies compared to C&C. Considering our use
of a small beam, and given the high ambigu-
ity levels in an automatically-extracted gram-
mar and the amount of information in the CCG
lexical categories which form the shift actions,
this is a surprising result.
1 Introduction
Combinatory Categorial Grammar (CCG; Steedman
(2000)) is a lexicalised theory of grammar which has
been successfully applied to a range of problems in
NLP, including treebank creation (Hockenmaier and
Steedman, 2007), syntactic parsing (Hockenmaier,
2003; Clark and Curran, 2007), logical form con-
struction (Bos et al, 2004) and surface realization
(White and Rajkumar, 2009). From a parsing per-
spective, the C&C parser (Clark and Curran, 2007)
has been shown to be competitive with state-of-the-
art statistical parsers on a variety of test suites, in-
cluding those consisting of grammatical relations
(Clark and Curran, 2007), Penn Treebank phrase-
structure trees (Clark and Curran, 2009), and un-
bounded dependencies (Rimell et al, 2009).
The binary branching nature of CCG means that
it is naturally compatible with bottom-up parsing al-
gorithms such as shift-reduce and CKY (Ades and
Steedman, 1982; Steedman, 2000). However, the
parsing work by Clark and Curran (2007), and also
Hockenmaier (2003) and Fowler and Penn (2010),
has only considered chart-parsing. In this paper we
fill a gap in the CCG literature by developing a shift-
reduce parser for CCG.
Shift-reduce parsers have become popular for de-
pendency parsing, building on the initial work of Ya-
mada and Matsumoto (2003) and Nivre and Scholz
(2004). One advantage of shift-reduce parsers is that
the scoring model can be defined over actions, al-
lowing highly efficient parsing by using a greedy
algorithm in which the highest scoring action (or a
small number of possible actions) is taken at each
step. In addition, high accuracy can be maintained
by using a model which utilises a rich set of features
for making each local decision (Nivre et al, 2006).
Following recent work applying global discrim-
inative models to large-scale structured prediction
problems (Collins and Roark, 2004; Miyao and
Tsujii, 2005; Clark and Curran, 2007; Finkel et
al., 2008), we build our shift-reduce parser using a
global linear model, and compare it with the chart-
based C&C parser. Using standard development
and test sets from CCGbank, our shift-reduce parser
gives a labeled F-measure of 85.53%, which is com-
petitive with the 85.45% F-measure of the C&C
parser on recovery of predicate-argument dependen-
cies from CCGbank. Hence our work shows that
683
transition-based parsing can be successfully applied
to CCG, improving on earlier attempts such as Has-
san et al (2008). Detailed analysis shows that our
shift-reduce parser yields a higher precision, lower
recall and higher F-score on most of the common
CCG dependency types compared to C&C.
One advantage of the shift-reduce parser is that
it easily handles sentences for which it is difficult
to find a spanning analysis, which can happen with
CCG because the lexical categories at the leaves of a
derivation place strong contraints on the set of possi-
ble derivations, and the supertagger which provides
the lexical categories sometimes makes mistakes.
Unlike the C&C parser, the shift-reduce parser nat-
urally produces fragmentary analyses when appro-
priate (Nivre et al, 2006), and can produce sensible
local structures even when a full spanning analysis
cannot be found.1
Finally, considering this work in the wider pars-
ing context, it provides an interesting comparison
between heuristic beam search using a rich set of
features, and optimal dynamic programming search
where the feature range is restricted. We are able to
perform this comparison because the use of the CCG
supertagger means that the C&C parser is able to
build the complete chart, from which it can find the
optimal derivation, with no pruning whatsoever at
the parsing stage. In contrast, the shift-reduce parser
uses a simple beam search with a relatively small
beam. Perhaps surprisingly, given the ambiguity lev-
els in an automatically-extracted grammar, and the
amount of information in the CCG lexical categories
which form the shift actions, the shift-reduce parser
using heuristic beam search is able to outperform the
chart-based parser.
2 CCG Parsing
CCG, and the application of CCG to wide-coverage
parsing, is described in detail elsewhere (Steedman,
2000; Hockenmaier, 2003; Clark and Curran, 2007).
Here we provide only a short description.
During CCG parsing, adjacent categories are com-
bined using CCG?s combinatory rules. For example,
a verb phrase in English (S\NP ) can combine with
1See e.g. Riezler et al (2002) and Zhang et al (2007) for chart-
based parsers which can produce fragmentary analyses.
an NP to its left using function application:
NP S\NP ? S
Categories can also combine using function
composition, allowing the combination of ?may?
((S\NP)/(S\NP)) and ?like? ((S\NP)/NP) in
coordination examples such as ?John may like but
may detest Mary?:
(S\NP)/(S\NP) (S\NP)/NP ? (S\NP)/NP
In addition to binary rules, such as function appli-
cation and composition, there are also unary rules
which operate on a single category in order to
change its type. For example, forward type-raising
can change a subject NP into a complex category
looking to the right for a verb phrase:
NP ? S/(S\NP)
An example CCG derivation is given in Section 3.
The resource used for building wide-coverage
CCG parsers of English is CCGbank (Hockenmaier
and Steedman, 2007), a version of the Penn Tree-
bank in which each phrase-structure tree has been
transformed into a normal-form CCG derivation.
There are two ways to extract a grammar from this
resource. One approach is to extract a lexicon,
i.e. a mapping from words to sets of lexical cat-
egories, and then manually define the combinatory
rule schemas, such as functional application and
composition, which combine the categories together.
The derivations in the treebank are then used to pro-
vide training data for the statistical disambiguation
model. This is the method used in the C&C parser.2
The second approach is to read the complete
grammar from the derivations, by extracting combi-
natory rule instances from the local trees consisting
of a parent category and one or two child categories,
and applying only those instances during parsing.
(These rule instances also include rules to deal with
punctuation and unary type-changing rules, in addi-
tion to instances of the combinatory rule schemas.)
This is the method used by Hockenmaier (2003) and
is the method we adopt in this paper.
Fowler and Penn (2010) demonstrate that the sec-
ond extraction method results in a context-free ap-
proximation to the grammar resulting from the first
2Although the C&C default mode applies a restriction for effi-
ciency reasons in which only rule instances seen in CCGbank
can be applied, making the grammar of the second type.
684
method, which has the potential to produce a mildly-
context sensitive grammar (given the existence of
certain combinatory rules) (Weir, 1988). However,
it is important to note that the advantages of CCG, in
particular the tight relationship between syntax and
semantic interpretation, are still maintained with the
second approach, as Fowler and Penn (2010) argue.
3 The Shift-reduce CCG Parser
Given an input sentence, our parser uses a stack of
partial derivations, a queue of incoming words, and
a series of actions?derived from the rule instances
in CCGbank?to build a derivation tree. Following
Clark and Curran (2007), we assume that each input
word has been assigned a POS-tag (from the Penn
Treebank tagset) and a set of CCG lexical categories.
We use the same maximum entropy POS-tagger and
supertagger as the C&C parser. The derivation tree
can be transformed into CCG dependencies or gram-
matical relations by a post-processing step, which
essentially runs the C&C parser deterministically
over the derivation, interpreting the derivation and
generating the required output.
The configuration of the parser, at each step of
the parsing process, is shown in part (a) of Figure 1,
where the stack holds the partial derivation trees that
have been built, and the queue contains the incoming
words that have not been processed. In the figure,
S(H) represents a category S on the stack with head
word H, while Qi represents a word in the incoming
queue.
The set of action types used by the parser is as
follows: {SHIFT, COMBINE, UNARY, FINISH}.
Each action type represents a set of possible actions
available to the parser at each step in the process.
The SHIFT-X action pushes the next incoming
word onto the stack, and assigns the lexical category
X to the word (Figure 1(b)). The label X can be any
lexical category from the set assigned to the word
being shifted by the supertagger. Hence the shift ac-
tion performs lexical category disambiguation. This
is in contrast to a shift-reduce dependency parser in
which a shift action typically just pushes a word onto
the stack.
The COMBINE-X action pops the top two nodes
off the stack, and combines them into a new node,
which is pushed back on the stack. The category of
Figure 1: The parser configuration and set of actions.
the new node is X. A COMBINE action corresponds
to a combinatory rule in the CCG grammar (or one of
the additional punctuation or type-changing rules),
which is applied to the categories of the top two
nodes on the stack.
The UNARY-X action pops the top of the stack,
transforms it into a new node with category X, and
pushes the new node onto the stack. A UNARY ac-
tion corresponds to a unary type-changing or type-
raising rule in the CCG grammar, which is applied to
the category on top of the stack.
The FINISH action terminates the parsing pro-
cess; it can be applied when all input words have
been shifted onto the stack. Note that the FINISH
action can be applied when the stack contains more
than one node, in which case the parser produces
a set of partial derivation trees, each corresponding
to a node on the stack. This sometimes happens
when a full derivation tree cannot be built due to su-
pertagging errors, and provides a graceful solution
to the problem of producing high-quality fragmen-
tary parses when necessary.
685
Figure 2: An example parsing process.
Figure 2 shows the shift-reduce parsing process
for the example sentence ?IBM bought Lotus?. First
the word ?IBM? is shifted onto the stack as an NP;
then ?bought? is shifted as a transitive verb look-
ing for its object NP on the right and subject NP on
the left ((S[dcl]\NP)/NP); and then ?Lotus? is shifted
as an NP. Then ?bought? is combined with its ob-
ject ?Lotus? resulting in a verb phrase looking for its
subject on the left (S[dcl]\NP). Finally, the resulting
verb phrase is combined with its subject, resulting in
a declarative sentence (S[dcl]).
A key difference with previous work on shift-
reduce dependency (Nivre et al, 2006) and CFG
(Sagae and Lavie, 2006b) parsing is that, for CCG,
there are many more shift actions ? a shift action for
each word-lexical category pair. Given the amount
of syntactic information in the lexical categories, the
choice of correct category, from those supplied by
the supertagger, is often a difficult one, and often
a choice best left to the parsing model. The C&C
parser solves this problem by building the complete
packed chart consistent with the lexical categories
supplied by the supertagger, leaving the selection of
the lexical categories to the Viterbi algorithm. For
the shift-reduce parser the choice is also left to the
parsing model, but in contrast to C&C the correct
lexical category could be lost at any point in the
heuristic search process. Hence it is perhaps sur-
prising that we are able to achieve a high parsing ac-
curacy of 85.5%, given a relatively small beam size.
4 Decoding
Greedy local search (Yamada and Matsumoto, 2003;
Sagae and Lavie, 2005; Nivre and Scholz, 2004)
has typically been used for decoding in shift-reduce
parsers, while beam-search has recently been ap-
plied as an alternative to reduce error-propagation
(Johansson and Nugues, 2007; Zhang and Clark,
2008; Zhang and Clark, 2009; Huang et al, 2009).
Both greedy local search and beam-search have lin-
ear time complexity. We use beam-search in our
CCG parser.
To formulate the decoding algorithm, we define a
candidate item as a tuple ?S,Q,F ?, where S repre-
sents the stack with partial derivations that have been
built, Q represents the queue of incoming words that
have not been processed, and F is a boolean value
that represents whether the candidate item has been
finished. A candidate item is finished if and only if
the FINISH action has been applied to it, and no
more actions can be applied to a candidate item af-
ter it reaches the finished status. Given an input sen-
tence, we define the start item as the unfinished item
with an empty stack and the whole input sentence as
the incoming words. A derivation is built from the
start item by repeated applications of actions until
the item is finished.
To apply beam-search, an agenda is used to hold
the N -best partial (unfinished) candidate items at
each parsing step. A separate candidate output is
686
function DECODE(input, agenda, list, N ,
grammar, candidate output):
agenda.clear()
agenda.insert(GETSTARTITEM(input))
candidate output = NONE
while not agenda.empty():
list.clear()
for item in agenda:
for action in grammar.getActions(item):
item? = item.apply(action)
if item?.F == TRUE:
if candidate output == NONE or
item?.score > candidate output.score:
candidate output = item?
else:
list.append(item?)
agenda.clear()
agenda.insert(list.best(N ))
Figure 3: The decoding algorithm; N is the agenda size
used to record the current best finished item that has
been found, since candidate items can be finished at
different steps. Initially the agenda contains only the
start item, and the candidate output is set to none. At
each step during parsing, each candidate item from
the agenda is extended in all possible ways by apply-
ing one action according to the grammar, and a num-
ber of new candidate items are generated. If a newly
generated candidate is finished, it is compared with
the current candidate output. If the candidate output
is none or the score of the newly generated candi-
date is higher than the score of the candidate output,
the candidate output is replaced with the newly gen-
erated item; otherwise the newly generated item is
discarded. If the newly generated candidate is un-
finished, it is appended to a list of newly generated
partial candidates. After all candidate items from the
agenda have been processed, the agenda is cleared
and the N -best items from the list are put on the
agenda. Then the list is cleared and the parser moves
on to the next step. This process repeats until the
agenda is empty (which means that no new items
have been generated in the previous step), and the
candidate output is the final derivation. Pseudocode
for the algorithm is shown in Figure 3.
feature templates
1 S0wp, S0c, S0pc, S0wc,
S1wp, S1c, S1pc, S1wc,
S2pc, S2wc,
S3pc, S3wc,
2 Q0wp, Q1wp, Q2wp, Q3wp,
3 S0Lpc, S0Lwc, S0Rpc, S0Rwc,
S0Upc, S0Uwc,
S1Lpc, S1Lwc, S1Rpc, S1Rwc,
S1Upc, S1Uwc,
4 S0wcS1wc, S0cS1w, S0wS1c, S0cS1c,
S0wcQ0wp, S0cQ0wp, S0wcQ0p, S0cQ0p,
S1wcQ0wp, S1cQ0wp, S1wcQ0p, S1cQ0p,
5 S0wcS1cQ0p, S0cS1wcQ0p, S0cS1cQ0wp,
S0cS1cQ0p, S0pS1pQ0p,
S0wcQ0pQ1p, S0cQ0wpQ1p, S0cQ0pQ1wp,
S0cQ0pQ1p, S0pQ0pQ1p,
S0wcS1cS2c, S0cS1wcS2c, S0cS1cS2wc,
S0cS1cS2c, S0pS1pS2p,
6 S0cS0HcS0Lc, S0cS0HcS0Rc,
S1cS1HcS1Rc,
S0cS0RcQ0p, S0cS0RcQ0w,
S0cS0LcS1c, S0cS0LcS1w,
S0cS1cS1Rc, S0wS1cS1Rc.
Table 1: Feature templates.
5 Model and Training
We use a global linear model to score candidate
items, trained discriminatively with the averaged
perceptron (Collins, 2002). Features for a (finished
or partial) candidate are extracted from each ac-
tion that have been applied to build the candidate.
Following Collins and Roark (2004), we apply the
?early update? strategy to perceptron training: at any
step during decoding, if neither the candidate out-
put nor any item in the agenda is correct, decoding
is stopped and the parameters are updated using the
current highest scored item in the agenda or the can-
didate output, whichever has the higher score.
Table 1 shows the feature templates used by the
parser. The symbols S0, S1, S2 and S3 in the ta-
ble represent the top four nodes on the stack (if ex-
istent), and Q0, Q1, Q2 and Q3 represent the front
four words in the incoming queue (if existent). S0H
and S1H represent the subnodes of S0 and S1 that
have the lexical head of S0 and S1, respectively. S0L
represents the left subnode of S0, when the lexical
head is from the right subnode. S0R and S1R rep-
resent the right subnode of S0 and S1, respectively,
687
when the lexical head is from the left subnode. If S0
is built by a UNARY action, S0U represents the only
subnode of S0. The symbols w, p and c represent the
word, the POS, and the CCG category, respectively.
These rich feature templates produce a large num-
ber of features: 36 million after the first training it-
eration, compared to around 0.5 million in the C&C
parser.
6 Experiments
Our experiments were performed using CCGBank
(Hockenmaier and Steedman, 2007), which was
split into three subsets for training (Sections 02?21),
development testing (Section 00) and the final test
(Section 23). Extracted from the training data, the
CCG grammar used by our parser consists of 3070
binary rule instances and 191 unary rule instances.
We compute F-scores over labeled CCG depen-
dencies and also lexical category accuracy. CCG de-
pendencies are defined in terms of lexical categories,
by numbering each argument slot in a complex cat-
egory. For example, the first NP in a transitive verb
category is a CCG dependency relation, correspond-
ing to the subject of the verb. Clark and Curran
(2007) gives a more precise definition. We use the
generate script from the C&C tools3 to transform
derivations into CCG dependencies.
There is a mismatch between the grammar that
generate uses, which is the same grammar as the
C&C parser, and the grammar we extract from CCG-
bank, which contains more rule instances. Hence
generate is unable to produce dependencies for
some of the derivations our shift-reduce parser pro-
duces. In order to allow generate to process all
derivations from the shift-reduce parser, we repeat-
edly removed rules that the generate script can-
not handle from our grammar, until all derivations
in the development data could be dealt with. In
fact, this procedure potentially reduces the accuracy
of the shift-reduce parser, but the effect is compar-
atively small because only about 4% of the devel-
opment and test sentences contain rules that are not
handled by the generate script.
All experiments were performed using automati-
3Available at http://svn.ask.it.usyd.edu.au/trac/candc/wiki; we
used the generate and evaluate scripts, as well as the
C&C parser, for evaluation and comparison.
cally assigned POS-tags, with 10-fold cross valida-
tion used to assign POS-tags and lexical categories
to the training data. At the supertagging stage, mul-
tiple lexical categories are assigned to each word in
the input. For each word, the supertagger assigns all
lexical categories whose forward-backward proba-
bility is above ? ? max, where max is the highest
lexical category probability for the word, and ? is a
threshold parameter. To give the parser a reasonable
freedom in lexical category disambiguation, we used
a small ? value of 0.0001, which results in 3.6 lexi-
cal categories being assigned to each word on aver-
age in the training data. For training, but not testing,
we also added the correct lexical category to the list
of lexical categories for a word in cases when it was
not provided by the supertagger.
Increasing the size of the beam in the parser beam
search leads to higher accuracies but slower running
time. In our development experiments, the accu-
racy improvement became small when the beam size
reached 16, and so we set the size of the beam to 16
for the remainder of the experiments.
6.1 Development test accuracies
Table 2 shows the labeled precision (lp), recall (lr),
F-score (lf), sentence-level accuracy (lsent) and lex-
ical category accuracy (cats) of our parser and the
C&C parser on the development data. We ran the
C&C parser using the normal-form model (we re-
produced the numbers reported in Clark and Cur-
ran (2007)), and copied the results of the hybrid
model from Clark and Curran (2007), since the hy-
brid model is not part of the public release.
The accuracy of our parser is much better when
evaluated on all sentences, partly because C&C
failed on 0.94% of the data due to the failure to pro-
duce a spanning analysis. Our shift-reduce parser
does not suffer from this problem because it pro-
duces fragmentary analyses for those cases. When
evaluated on only those sentences that C&C could
analyze, our parser gave 0.29% higher F-score. Our
shift-reduce parser also gave higher accuracies on
lexical category assignment. The sentence accuracy
of our shift-reduce parser is also higher than C&C,
which confirms that our shift-reduce parser produces
reasonable sentence-level analyses, despite the pos-
sibility for fragmentary analysis.
688
lp. lr. lf. lsent. cats. evaluated on
shift-reduce 87.15% 82.95% 85.00% 33.82% 92.77% all sentences
C&C (normal-form) 85.22% 82.52% 83.85% 31.63% 92.40% all sentences
shift-reduce 87.55% 83.63% 85.54% 34.14% 93.11% 99.06% (C&C coverage)
C&C (hybrid) ? ? 85.25% ? ? 99.06% (C&C coverage)
C&C (normal-form) 85.22% 84.29% 84.76% 31.93% 92.83% 99.06% (C&C coverage)
Table 2: Accuracies on the development test data.
 60
 65
 70
 75
 80
 85
 90
 0  5  10  15  20  25  30
pr
ec
isi
on
 %
dependency length (bins of 5)
Precision comparison by dependency length
this paper
C&C
 50
 55
 60
 65
 70
 75
 80
 85
 90
 0  5  10  15  20  25  30
re
ca
ll 
%
dependency length (bins of 5)
Recall comparison by dependency length
this paper
C&C
Figure 4: P & R scores relative to dependency length.
6.2 Error comparison with C&C parser
Our shift-reduce parser and the chart-based C&C
parser offer two different solutions to the CCG pars-
ing problem. The comparison reported in this sec-
tion is similar to the comparison between the chart-
based MSTParser (McDonald et al, 2005) and shift-
reduce MaltParser (Nivre et al, 2006) for depen-
dency parsing. We follow McDonald and Nivre
(2007) and characterize the errors of the two parsers
by sentence and dependency length and dependency
type.
We measured precision, recall and F-score rel-
ative to different sentence lengths. Both parsers
performed better on shorter sentences, as expected.
Our shift-reduce parser performed consistently bet-
ter than C&C on all sentence lengths, and there
was no significant difference in the rate of perfor-
mance degradation between the parsers as the sen-
tence length increased.
Figure 4 shows the comparison of labeled preci-
sion and recall relative to the dependency length (i.e.
the number of words between the head and depen-
dent), in bins of size 5 (e.g. the point at x=5 shows
the precision or recall for dependency lengths 1 ? 5).
This experiment was performed using the normal-
form version of the C&C parser, and the evaluation
was on the sentences for which C&C gave an anal-
ysis. The number of dependencies drops when the
dependency length increases; there are 141, 180 and
124 dependencies from the gold-standard, C&C out-
put and our shift-reduce parser output, respectively,
when the dependency length is between 21 and 25,
inclusive. The numbers drop to 47, 56 and 36 when
the dependency length is between 26 and 30. The
recall of our parser drops more quickly as the de-
pendency length grows beyond 15. A likely reason
is that the recovery of longer-range dependencies re-
quires more processing steps, increasing the chance
of the correct structure being thrown off the beam.
In contrast, the precision did not drop more quickly
than C&C, and in fact is consistently higher than
C&C across all dependency lengths, which reflects
the fact that the long range dependencies our parser
managed to recover are comparatively reliable.
Table 3 shows the comparison of labeled precision
(lp), recall (lr) and F-score (lf) for the most common
CCG dependency types. The numbers for C&C are
for the hybrid model, copied from Clark and Curran
(2007). While our shift-reduce parser gave higher
precision for almost all categories, it gave higher re-
call on only half of them, but higher F-scores for all
but one dependency type.
6.3 Final results
Table 4 shows the accuracies on the test data. The
numbers for the normal-form model are evaluated
by running the publicly available parser, while those
for the hybrid dependency model are from Clark
and Curran (2007). Evaluated on all sentences, the
accuracies of our parser are much higher than the
C&C parser, since the C&C parser failed to produce
any output for 10 sentences. When evaluating both
689
category arg lp. (o) lp. (C) lr. (o) lr. (C) lf. (o) lf. (C) freq.
N/N 1 95.77% 95.28% 95.79% 95.62% 95.78% 95.45% 7288
NP/N 1 96.70% 96.57% 96.59% 96.03% 96.65% 96.30% 4101
(NP\NP)/NP 2 83.19% 82.17% 89.24% 88.90% 86.11% 85.40% 2379
(NP\NP)/NP 1 82.53% 81.58% 87.99% 85.74% 85.17% 83.61% 2174
((S\NP)\(S\NP))/NP 3 77.60% 71.94% 71.58% 73.32% 74.47% 72.63% 1147
((S\NP)\(S\NP))/NP 2 76.30% 70.92% 70.60% 71.93% 73.34% 71.42% 1058
((S[dcl]\NP)/NP 2 85.60% 81.57% 84.30% 86.37% 84.95% 83.90% 917
PP/NP 1 73.76% 75.06% 72.83% 70.09% 73.29% 72.49% 876
((S[dcl]\NP)/NP 1 85.32% 81.62% 82.00% 85.55% 83.63% 83.54% 872
((S\NP)\(S\NP)) 2 84.44% 86.85% 86.60% 86.73% 85.51% 86.79% 746
Table 3: Accuracy comparison on the most common CCG dependency types. (o) ? our parser; (C) ? C&C (hybrid)
lp. lr. lf. lsent. cats. evaluated
shift-reduce 87.43% 83.61% 85.48% 35.19% 93.12% all sentences
C&C (normal-form) 85.58% 82.85% 84.20% 32.90% 92.84% all sentences
shift-reduce 87.43% 83.71% 85.53% 35.34% 93.15% 99.58% (C&C coverage)
C&C (hybrid) 86.17% 84.74% 85.45% 32.92% 92.98% 99.58% (C&C coverage)
C&C (normal-form) 85.48% 84.60% 85.04% 33.08% 92.86% 99.58% (C&C coverage)
F&P (Petrov I-5)* 86.29% 85.73% 86.01% ? ? ? (F&P? C&C coverage; 96.65% on dev. test)
C&C hybrid* 86.46% 85.11% 85.78% ? ? ? (F&P? C&C coverage; 96.65% on dev. test)
Table 4: Comparison with C&C; final test. * ? not directly comparable.
parsers on the sentences for which C&C produces an
analysis, our parser still gave the highest accuracies.
The shift-reduce parser gave higher precision, and
lower recall, than C&C; it also gave higher sentence-
level and lexical category accuracy.
The last two rows in the table show the accuracies
of Fowler and Penn (2010) (F&P), who applied the
CFG parser of Petrov and Klein (2007) to CCG, and
the corresponding accuracies for the C&C parser on
the same test sentences. F&P can be treated as an-
other chart-based parser; their evaluation is based
on the sentences for which both their parser and
C&C produced dependencies (or more specifically
those sentences for which generate could pro-
duce dependencies), and is not directly comparable
with ours, especially considering that their test set is
smaller and potentially slightly easier.
The final comparison is parser speed. The shift-
reduce parser is linear-time (in both sentence length
and beam size), and can analyse over 10 sentences
per second on a 2GHz CPU, with a beam of 16,
which compares very well with other constituency
parsers. However, this is no faster than the chart-
based C&C parser, although speed comparisons
are difficult because of implementation differences
(C&C uses heavily engineered C++ with a focus on
efficiency).
7 Related Work
Sagae and Lavie (2006a) describes a shift-reduce
parser for the Penn Treebank parsing task which
uses best-first search to allow some ambiguity into
the parsing process. Differences with our approach
are that we use a beam, rather than best-first, search;
we use a global model rather than local models
chained together; and finally, our results surpass
the best published results on the CCG parsing task,
whereas Sagae and Lavie (2006a) matched the best
PTB results only by using a parser combination.
Matsuzaki et al (2007) describes similar work
to ours but using an automatically-extracted HPSG,
rather than CCG, grammar. They also use the gen-
eralised perceptron to train a disambiguation model.
One difference is that Matsuzaki et al (2007) use an
approximating CFG, in addition to the supertagger,
to improve the efficiency of the parser.
690
Ninomiya et al (2009) (and Ninomiya et al
(2010)) describe a greedy shift-reduce parser for
HPSG, in which a single action is chosen at each
parsing step, allowing the possibility of highly ef-
ficient parsing. Since the HPSG grammar has rela-
tively tight constraints, similar to CCG, the possibil-
ity arises that a spanning analysis cannot be found
for some sentences. Our approach to this problem
was to allow the parser to return a fragmentary anal-
ysis; Ninomiya et al (2009) adopt a different ap-
proach based on default unification.
Finally, our work is similar to the comparison of
the chart-based MSTParser (McDonald et al, 2005)
and shift-reduce MaltParser (Nivre et al, 2006) for
dependency parsing. MSTParser can perform ex-
haustive search, given certain feature restrictions,
because the complexity of the parsing task is lower
than for constituent parsing. C&C can perform ex-
haustive search because the supertagger has already
reduced the search space. We also found that ap-
proximate heuristic search for shift-reduce parsing,
utilising a rich feature space, can match the perfor-
mance of the optimal chart-based parser, as well as
similar error profiles for the two CCG parsers com-
pared to the two dependency parsers.
8 Conclusion
This is the first work to present competitive results
for CCG using a transition-based parser, filling a gap
in the CCG parsing literature. Considered in terms
of the wider parsing problem, we have shown that
state-of-the-art parsing results can be obtained using
a global discriminative model, one of the few pa-
pers to do so without using a generative baseline as a
feature. The comparison with C&C also allowed us
to compare a shift-reduce parser based on heuristic
beam search utilising a rich feature set with an opti-
mal chart-based parser whose features are restricted
by dynamic programming, with favourable results
for the shift-reduce parser.
The complementary errors made by the chart-
based and shift-reduce parsers opens the possibil-
ity of effective parser combination, following sim-
ilar work for dependency parsing.
The parser code can be downloaded at
http://www.sourceforge.net/projects/zpar,
version 0.5.
Acknowledgements
We thank the anonymous reviewers for their sugges-
tions. Yue Zhang and Stephen Clark are supported
by the European Union Seventh Framework Pro-
gramme (FP7-ICT-2009-4) under grant agreement
no. 247762.
References
A. E. Ades and M. Steedman. 1982. On the order of
words. Linguistics and Philosophy, pages 517 ? 558.
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG parser. In Pro-
ceedings of COLING-04, pages 1240?1246, Geneva,
Switzerland.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark and James R. Curran. 2009. Comparing
the accuracy of CCG and Penn Treebank parsers. In
Proceedings of ACL-2009 (short papers), pages 53?
56, Singapore.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL, pages 111?118, Barcelona, Spain.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8, Philadelphia, USA.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Feature-based, conditional random
field parsing. In Proceedings of the 46th Meeting of
the ACL, pages 959?967, Columbus, Ohio.
Timothy A. D. Fowler and Gerald Penn. 2010. Ac-
curate context-free parsing with Combinatory Catego-
rial Grammar. In Proceedings of ACL-2010, Uppsala,
Sweden.
H. Hassan, K. Sima?an, and A. Way. 2008. A syntactic
language model based on incremental CCG parsing.
In Proceedings of the Second IEEE Spoken Language
Technology Workshop, Goa, India.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Compu-
tational Linguistics, 33(3):355?396.
Julia Hockenmaier. 2003. Data and Models for Statis-
tical Parsing with Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
691
parsing. In Proceedings of the 2009 EMNLP Confer-
ence, pages 1222?1231, Singapore.
Richard Johansson and Pierre Nugues. 2007. Incre-
mental dependency parsing using online learning. In
Proceedings of the CoNLL/EMNLP Conference, pages
1134?1138, Prague, Czech Republic.
Takuya Matsuzaki, Yusuke Miyao, and Jun ichi Tsu-
jii. 2007. Efficient HPSG parsing with supertagging
and CFG-filtering. In Proceedings of IJCAI-07, pages
1671?1676, Hyderabad, India.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP/CoNLL, pages 122?
131, Prague, Czech Republic.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Meeting of the
ACL, pages 91?98, Michigan, Ann Arbor.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage HPSG pars-
ing. In Proceedings of the 43rd meeting of the ACL,
pages 83?90, University of Michigan, Ann Arbor.
Takashi Ninomiya, Takuya Matsuzaki, Nobuyuki
Shimizu, and Hiroshi Nakagawa. 2009. Deterministic
shift-reduce parsing for unification-based grammars
by using default unification. In Proceedings of
EACL-09, pages 603?611, Athens, Greece.
Takashi Ninomiya, Takuya Matsuzaki, Nobuyuki
Shimizu, and Hiroshi Nakagawa. 2010. Deter-
ministic shift-reduce parsing for unification-based
grammars. Journal of Natural Language Engineering,
DOI:10.1017/S1351324910000240.
J. Nivre and M. Scholz. 2004. Deterministic dependency
parsing of English text. In Proceedings of COLING-
04, pages 64?70, Geneva, Switzerland.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?en Eryig?it,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector ma-
chines. In Proceedings of CoNLL, pages 221?225,
New York, USA.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
HLT/NAACL, pages 404?411, Rochester, New York,
April.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell III, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Meet-
ing of the ACL, pages 271?278, Philadelphia, PA.
Laura Rimell, Stephen Clark, and Mark Steedman. 2009.
Unbounded dependency recovery for parser evalua-
tion. In Proceedings of EMNLP-09, pages 813?821,
Singapore.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of IWPT, pages 125?132, Vancouver, Canada.
Kenji Sagae and Alon Lavie. 2006a. A best-first
probabilistic shift-reduce parser. In Proceedings of
COLING/ACL poster session, pages 691?698, Sydney,
Australia, July.
Kenji Sagae and Alon Lavie. 2006b. Parser combination
by reparsing. In Proceedings of HLT/NAACL, Com-
panion Volume: Short Papers, pages 129?132, New
York, USA.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, Mass.
David Weir. 1988. Characterizing Mildly Context-
Sensitive Grammar Formalisms. Ph.D. thesis, Univer-
sity of Pennsylviania.
Michael White and Rajakrishnan Rajkumar. 2009. Per-
ceptron reranking for CCG realization. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 410?419, Singapore.
H Yamada and Y Matsumoto. 2003. Statistical depen-
dency analysis using support vector machines. In Pro-
ceedings of IWPT, Nancy, France.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-based
and transition-based dependency parsing using beam-
search. In Proceedings of EMNLP-08, Hawaii, USA.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the Chinese Treebank using a global dis-
criminative model. In Proceedings of IWPT, Paris,
France, October.
Yi Zhang, Valia Kordoni, and Erin Fitzgerald. 2007. Par-
tial parse selection for robust deep processing. In Pro-
ceedings of the ACL 2007 Workshop on Deep Linguis-
tic Processing, Prague, Czech Republic.
692
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 188?193,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Transition-based Dependency Parsing with Rich Non-local Features
Yue Zhang
University of Cambridge
Computer Laboratory
yue.zhang@cl.cam.ac.uk
Joakim Nivre
Uppsala University
Department of Linguistics and Philology
joakim.nivre@lingfil.uu.se
Abstract
Transition-based dependency parsers gener-
ally use heuristic decoding algorithms but can
accommodate arbitrarily rich feature represen-
tations. In this paper, we show that we can im-
prove the accuracy of such parsers by consid-
ering even richer feature sets than those em-
ployed in previous systems. In the standard
Penn Treebank setup, our novel features im-
prove attachment score form 91.4% to 92.9%,
giving the best results so far for transition-
based parsing and rivaling the best results
overall. For the Chinese Treebank, they give a
signficant improvement of the state of the art.
An open source release of our parser is freely
available.
1 Introduction
Transition-based dependency parsing (Yamada and
Matsumoto, 2003; Nivre et al, 2006b; Zhang and
Clark, 2008; Huang and Sagae, 2010) utilize a deter-
ministic shift-reduce process for making structural
predictions. Compared to graph-based dependency
parsing, it typically offers linear time complexity
and the comparative freedom to define non-local fea-
tures, as exemplified by the comparison between
MaltParser and MSTParser (Nivre et al, 2006b; Mc-
Donald et al, 2005; McDonald and Nivre, 2007).
Recent research has addressed two potential dis-
advantages of systems like MaltParser. In the
aspect of decoding, beam-search (Johansson and
Nugues, 2007; Zhang and Clark, 2008; Huang et
al., 2009) and partial dynamic-programming (Huang
and Sagae, 2010) have been applied to improve upon
greedy one-best search, and positive results were re-
ported. In the aspect of training, global structural
learning has been used to replace local learning on
each decision (Zhang and Clark, 2008; Huang et al,
2009), although the effect of global learning has not
been separated out and studied alone.
In this short paper, we study a third aspect in a
statistical system: feature definition. Representing
the type of information a statistical system uses to
make predictions, feature templates can be one of
the most important factors determining parsing ac-
curacy. Various recent attempts have been made
to include non-local features into graph-based de-
pendency parsing (Smith and Eisner, 2008; Martins
et al, 2009; Koo and Collins, 2010). Transition-
based parsing, by contrast, can easily accommodate
arbitrarily complex representations involving non-
local features. Complex non-local features, such as
bracket matching and rhythmic patterns, are used
in transition-based constituency parsing (Zhang and
Clark, 2009; Wang et al, 2006), and most transition-
based dependency parsers incorporate some non-
local features, but current practice is nevertheless to
use a rather restricted set of features, as exemplified
by the default feature models in MaltParser (Nivre et
al., 2006a). We explore considerably richer feature
representations and show that they improve parsing
accuracy significantly.
In standard experiments using the Penn Treebank,
our parser gets an unlabeled attachment score of
92.9%, which is the best result achieved with a
transition-based parser and comparable to the state
of the art. For the Chinese Treebank, our parser gets
a score of 86.0%, the best reported result so far.
188
2 The Transition-based Parsing Algorithm
In a typical transition-based parsing process, the in-
put words are put into a queue and partially built
structures are organized by a stack. A set of shift-
reduce actions are defined, which consume words
from the queue and build the output parse. Recent
research have focused on action sets that build pro-
jective dependency trees in an arc-eager (Nivre et
al., 2006b; Zhang and Clark, 2008) or arc-standard
(Yamada and Matsumoto, 2003; Huang and Sagae,
2010) process. We adopt the arc-eager system1, for
which the actions are:
? Shift, which removes the front of the queue
and pushes it onto the top of the stack;
? Reduce, which pops the top item off the stack;
? LeftArc, which pops the top item off the
stack, and adds it as a modifier to the front of
the queue;
? RightArc, which removes the front of the
queue, pushes it onto the stack and adds it as
a modifier to the top of the stack.
Further, we follow Zhang and Clark (2008) and
Huang et al (2009) and use the generalized percep-
tron (Collins, 2002) for global learning and beam-
search for decoding. Unlike both earlier global-
learning parsers, which only perform unlabeled
parsing, we perform labeled parsing by augmenting
the LeftArc and RightArc actions with the set
of dependency labels. Hence our work is in line with
Titov and Henderson (2007) in using labeled transi-
tions with global learning. Moreover, we will see
that label information can actually improve link ac-
curacy.
3 Feature Templates
At each step during a parsing process, the
parser configuration can be represented by a tuple
?S,N,A?, where S is the stack, N is the queue of
incoming words, and A is the set of dependency
arcs that have been built. Denoting the top of stack
1It is very likely that the type of features explored in this
paper would be beneficial also for the arc-standard system, al-
though the exact same feature templates would not be applicable
because of differences in the parsing order.
from single words
S0wp; S0w; S0p; N0wp; N0w; N0p;
N1wp; N1w; N1p; N2wp; N2w; N2p;
from word pairs
S0wpN0wp; S0wpN0w; S0wN0wp; S0wpN0p;
S0pN0wp; S0wN0w; S0pN0p
N0pN1p
from three words
N0pN1pN2p; S0pN0pN1p; S0hpS0pN0p;
S0pS0lpN0p; S0pS0rpN0p; S0pN0pN0lp
Table 1: Baseline feature templates.
w ? word; p ? POS-tag.
distance
S0wd; S0pd; N0wd; N0pd;
S0wN0wd; S0pN0pd;
valency
S0wvr; S0pvr; S0wvl; S0pvl; N0wvl; N0pvl;
unigrams
S0hw; S0hp; S0l; S0lw; S0lp; S0ll;
S0rw; S0rp; S0rl;N0lw; N0lp; N0ll;
third-order
S0h2w; S0h2p; S0hl; S0l2w; S0l2p; S0l2l;
S0r2w; S0r2p; S0r2l; N0l2w; N0l2p; N0l2l;
S0pS0lpS0l2p; S0pS0rpS0r2p;
S0pS0hpS0h2p; N0pN0lpN0l2p;
label set
S0wsr; S0psr; S0wsl; S0psl; N0wsl; N0psl;
Table 2: New feature templates.
w ? word; p ? POS-tag; vl, vr ? valency; l ?
dependency label, sl, sr ? labelset.
with S0, the front items from the queue with N0,
N1, and N2, the head of S0 (if any) with S0h, the
leftmost and rightmost modifiers of S0 (if any) with
S0l and S0r, respectively, and the leftmost modifier
of N0 (if any) with N0l, the baseline features are
shown in Table 1. These features are mostly taken
from Zhang and Clark (2008) and Huang and Sagae
(2010), and our parser reproduces the same accura-
cies as reported by both papers. In this table, w and
p represents the word and POS-tag, respectively. For
example, S0pN0wp represents the feature template
that takes the word and POS-tag of N0, and com-
bines it with the word of S0.
189
In this short paper, we extend the baseline feature
templates with the following:
Distance between S0 and N0
Direction and distance between a pair of head and
modifier have been used in the standard feature
templates for maximum spanning tree parsing (Mc-
Donald et al, 2005). Distance information has
also been used in the easy-first parser of (Goldberg
and Elhadad, 2010). For a transition-based parser,
direction information is indirectly included in the
LeftArc and RightArc actions. We add the dis-
tance between S0 and N0 to the feature set by com-
bining it with the word and POS-tag of S0 and N0,
as shown in Table 2.
It is worth noticing that the use of distance in-
formation in our transition-based model is different
from that in a typical graph-based parser such as
MSTParser. The distance between S0 and N0 will
correspond to the distance between a pair of head
and modifier when an LeftArc action is taken, for
example, but not when a Shift action is taken.
Valency of S0 and N0
The number of modifiers to a given head is used
by the graph-based submodel of Zhang and Clark
(2008) and the models of Martins et al (2009) and
Sagae and Tsujii (2007). We include similar infor-
mation in our model. In particular, we calculate the
number of left and right modifiers separately, call-
ing them left valency and right valency, respectively.
Left and right valencies are represented by vl and vr
in Table 2, respectively. They are combined with the
word and POS-tag of S0 and N0 to form new feature
templates.
Again, the use of valency information in our
transition-based parser is different from the afore-
mentioned graph-based models. In our case,
valency information is put into the context of the
shift-reduce process, and used together with each
action to give a score to the local decision.
Unigram information for S0h, S0l, S0r and N0l
The head, left/rightmost modifiers of S0 and the
leftmost modifier of N0 have been used by most
arc-eager transition-based parsers we are aware of
through the combination of their POS-tag with infor-
mation from S0 and N0. Such use is exemplified by
the feature templates ?from three words? in Table 1.
We further use their word and POS-tag information
as ?unigram? features in Table 2. Moreover, we
include the dependency label information in the
unigram features, represented by l in the table. Uni-
gram label information has been used in MaltParser
(Nivre et al, 2006a; Nivre, 2006).
Third-order features of S0 and N0
Higher-order context features have been used by
graph-based dependency parsers to improve accura-
cies (Carreras, 2007; Koo and Collins, 2010). We
include information of third order dependency arcs
in our new feature templates, when available. In
Table 2, S0h2, S0l2, S0r2 and N0l2 refer to the head
of S0h, the second leftmost modifier and the second
rightmost modifier of S0, and the second leftmost
modifier of N0, respectively. The new templates
include unigram word, POS-tag and dependency
labels of S0h2, S0l2, S0r2 and N0l2, as well as
POS-tag combinations with S0 and N0.
Set of dependency labels with S0 and N0
As a more global feature, we include the set of
unique dependency labels from the modifiers of S0
and N0. This information is combined with the word
and POS-tag of S0 and N0 to make feature templates.
In Table 2, sl and sr stands for the set of labels on
the left and right of the head, respectively.
4 Experiments
Our experiments were performed using the Penn
Treebank (PTB) and Chinese Treebank (CTB) data.
We follow the standard approach to split PTB3, using
sections 2 ? 21 for training, section 22 for develop-
ment and 23 for final testing. Bracketed sentences
from PTB were transformed into dependency for-
mats using the Penn2Malt tool.2 Following Huang
and Sagae (2010), we assign POS-tags to the training
data using ten-way jackknifing. We used our imple-
mentation of the Collins (2002) tagger (with 97.3%
accuracy on a standard Penn Treebank test) to per-
form POS-tagging. For all experiments, we set the
beam size to 64 for the parser, and report unlabeled
and labeled attachment scores (UAS, LAS) and un-
labeled exact match (UEM) for evaluation.
2http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
190
feature UAS UEM
baseline 92.18% 45.76%
+distance 92.25% 46.24%
+valency 92.49% 47.65%
+unigrams 92.89% 48.47%
+third-order 93.07% 49.59%
+label set 93.14% 50.12%
Table 3: The effect of new features on the development
set for English. UAS = unlabeled attachment score; UEM
= unlabeled exact match.
UAS UEM LAS
Z&C08 transition 91.4% 41.8% ?
H&S10 91.4% ? ?
this paper baseline 91.4% 42.5% 90.1%
this paper extended 92.9% 48.0% 91.8%
MSTParser 91.5% 42.5% ?
K08 standard 92.0% ? ?
K&C10 model 1 93.0% ? ?
K&C10 model 2 92.9% ? ?
Table 4: Final test accuracies for English. UAS = unla-
beled attachment score; UEM = unlabeled exact match;
LAS = labeled attachment score.
4.1 Development Experiments
Table 3 shows the effect of new features on the de-
velopment test data for English. We start with the
baseline features in Table 1, and incrementally add
the distance, valency, unigram, third-order and label
set feature templates in Table 2. Each group of new
feature templates improved the accuracies over the
previous system, and the final accuracy with all new
features was 93.14% in unlabeled attachment score.
4.2 Final Test Results
Table 4 shows the final test results of our
parser for English. We include in the table
results from the pure transition-based parser of
Zhang and Clark (2008) (row ?Z&C08 transition?),
the dynamic-programming arc-standard parser of
Huang and Sagae (2010) (row ?H&S10?), and graph-
based models including MSTParser (McDonald and
Pereira, 2006), the baseline feature parser of Koo et
al. (2008) (row ?K08 baeline?), and the two models
of Koo and Collins (2010). Our extended parser sig-
nificantly outperformed the baseline parser, achiev-
UAS UEM LAS
Z&C08 transition 84.3% 32.8% ?
H&S10 85.2% 33.7% ?
this paper extended 86.0% 36.9% 84.4%
Table 5: Final test accuracies for Chinese. UAS = unla-
beled attachment score; UEM = unlabeled exact match;
LAS = labeled attachment score.
ing the highest attachment score reported for a
transition-based parser, comparable to those of the
best graph-based parsers.
Our experiments were performed on a Linux plat-
form with a 2GHz CPU. The speed of our baseline
parser was 50 sentences per second. With all new
features added, the speed dropped to 29 sentences
per second.
As an alternative to Penn2Malt, bracketed sen-
tences can also be transformed into Stanford depen-
dencies (De Marneffe et al, 2006). Our parser gave
93.5% UAS, 91.9% LAS and 52.1% UEM when
trained and evaluated on Stanford basic dependen-
cies, which are projective dependency trees. Cer et
al. (2010) report results on Stanford collapsed de-
pendencies, which allow a word to have multiple
heads and therefore cannot be produced by a reg-
ular dependency parser. Their results are relevant
although not directly comparable with ours.
4.3 Chinese Test Results
Table 5 shows the results of our final parser, the pure
transition-based parser of Zhang and Clark (2008),
and the parser of Huang and Sagae (2010) on Chi-
nese. We take the standard split of CTB and use gold
segmentation and POS-tags for the input. Our scores
for this test set are the best reported so far and sig-
nificantly better than the previous systems.
5 Conclusion
We have shown that enriching the feature repre-
sentation significantly improves the accuracy of our
transition-based dependency parser. The effect of
the new features appears to outweigh the effect of
combining transition-based and graph-based mod-
els, reported by Zhang and Clark (2008), as well
as the effect of using dynamic programming, as in-
Huang and Sagae (2010). This shows that feature
definition is a crucial aspect of transition-based pars-
191
ing. In fact, some of the new feature templates in this
paper, such as distance and valency, are among those
which are in the graph-based submodel of Zhang
and Clark (2008), but not the transition-based sub-
model. Therefore our new features to some extent
achieved the same effect as their model combina-
tion. The new features are also hard to use in dy-
namic programming because they add considerable
complexity to the parse items.
Enriched feature representations have been stud-
ied as an important factor for improving the accu-
racies of graph-based dependency parsing also. Re-
cent research including the use of loopy belief net-
work (Smith and Eisner, 2008), integer linear pro-
gramming (Martins et al, 2009) and an improved
dynamic programming algorithm (Koo and Collins,
2010) can be seen as methods to incorporate non-
local features into a graph-based model.
An open source release of our parser, together
with trained models for English and Chinese, are
freely available.3
Acknowledgements
We thank the anonymous reviewers for their useful
comments. Yue Zhang is supported by the Euro-
pean Union Seventh Framework Programme (FP7-
ICT-2009-4) under grant agreement no. 247762.
References
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP/CoNLL, pages
957?961, Prague, Czech Republic.
Daniel Cer, Marie-Catherine de Marneffe, Dan Juraf-
sky, and Chris Manning. 2010. Parsing to stan-
ford dependencies: Trade-offs between speed and ac-
curacy. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10).
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8, Philadelphia, USA.
Marie-catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC.
3http://www.sourceforge.net/projects/zpar. version 0.5.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Porceedings of HLT/NAACL, pages
742?750, Los Angeles, California, June.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In Pro-
ceedings of ACL, pages 1077?1086, Uppsala, Sweden,
July.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP, pages 1222?1231,
Singapore.
Richard Johansson and Pierre Nugues. 2007. Incre-
mental dependency parsing using online learning. In
Proceedings of CoNLL/EMNLP, pages 1134?1138,
Prague, Czech Republic.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL,
pages 1?11, Uppsala, Sweden, July.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL/HLT, pages 595?603, Columbus,
Ohio, June.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of ACL/IJCNLP,
pages 342?350, Suntec, Singapore, August.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP/CoNLL, pages 122?
131, Prague, Czech Republic.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL, pages 81?88, Trento,
Italy, April.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL, pages 91?98, Ann
Arbor, Michigan, June.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006a.
Maltparser: A data-driven parser-generator for depen-
dency parsing. pages 2216?2219.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?en Eryig?it,
and Svetoslav Marinov. 2006b. Labeled pseudo-
projective dependency parsing with support vector ma-
chines. In Proceedings of CoNLL, pages 221?225,
New York, USA.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 1044?1050,
192
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
David Smith and Jason Eisner. 2008. Dependency pars-
ing by belief propagation. In Proceedings of EMNLP,
pages 145?156, Honolulu, Hawaii, October.
Ivan Titov and James Henderson. 2007. A latent variable
model for generative dependency parsing. In Proceed-
ings of IWPT, pages 144?155, Prague, Czech Repub-
lic, June.
Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, and
Xihong Wu. 2006. Chinese word segmentation with
maximum entropy and n-gram language model. In
Proceedings of SIGHAN Workshop, pages 138?141,
Sydney, Australia, July.
H Yamada and Y Matsumoto. 2003. Statistical depen-
dency analysis using support vector machines. In Pro-
ceedings of IWPT, Nancy, France.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-based
and transition-based dependency parsing using beam-
search. In Proceedings of EMNLP, Hawaii, USA.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the Chinese Treebank using a global dis-
criminative model. In Proceedings of IWPT, Paris,
France, October.
193
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 125?134,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Chinese Parsing Exploiting Characters
Meishan Zhang?, Yue Zhang??, Wanxiang Che?, Ting Liu?
?Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{mszhang, car, tliu}@ir.hit.edu.cn
?Singapore University of Technology and Design
yue zhang@sutd.edu.sg
Abstract
Characters play an important role in the
Chinese language, yet computational pro-
cessing of Chinese has been dominated
by word-based approaches, with leaves in
syntax trees being words. We investigate
Chinese parsing from the character-level,
extending the notion of phrase-structure
trees by annotating internal structures of
words. We demonstrate the importance
of character-level information to Chinese
processing by building a joint segmen-
tation, part-of-speech (POS) tagging and
phrase-structure parsing system that inte-
grates character-structure features. Our
joint system significantly outperforms a
state-of-the-art word-based baseline on the
standard CTB5 test, and gives the best
published results for Chinese parsing.
1 Introduction
Characters play an important role in the Chinese
language. They act as basic phonetic, morpho-
syntactic and semantic units in a Chinese sentence.
Frequently-occurring character sequences that ex-
press certain meanings can be treated as words,
while most Chinese words have syntactic struc-
tures. For example, Figure 1(b) shows the struc-
ture of the word ???? (construction and build-
ing industry)?, where the characters ?? (construc-
tion)? and ?? (building)? form a coordination,
and modify the character ?? (industry)?.
However, computational processing of Chinese
is typically based on words. Words are treated
as the atomic units in syntactic parsing, machine
translation, question answering and other NLP
tasks. Manually annotated corpora, such as the
Chinese Treebank (CTB) (Xue et al, 2005), usu-
ally have words as the basic syntactic elements
?Email correspondence.
?? ???
??
? ??
NR NN
VV
JJ NN
NP NP
NPADJP
NP
VPNP
IP
NP NP
NPADJP
NP
VPNP
IP
NN
?
NR-e
?
NR-b
JJ
?
JJ-s
VV
?
VV-e
?
VV-bNN
?
NN-e
?
NN-m
?
NN-b
NR
?
NR-eNR-b
?
NP NP
NPADJP
NP
VPNP
IP
NN-c
?
NR-i
?
NR-b
JJ-t
?
JJ-b
VV-c
?
VV-i
?
VV-bNR-r
?
NR-iNR-b
?
NR-t
NN-r
?
NN-i
?
NN-i
?
NN-b
NN-c
NN-t
VV-t
NN-t
(a) CTB-style word-based syntax tree for ??? (China) ?
?? (architecture industry) ?? (show) ? (new) ??
(pattern)?.
?? ???
??
? ??
NR NN
VV
JJ NN
NP NP
NPADJP
NP
VPNP
IP
NP NP
NPADJP
NP
VPNP
IP
NN
?
NR-e
?
NR-b
JJ
?
JJ-s
VV
?
VV-e
?
VV-bNN
?
NN-e
?
NN-m
?
NN-b
NR
?
NR-eNR-b
?
NP NP
NPADJP
NP
VPNP
IP
NN-c
?
NN-i
?
NN-b
JJ-t
?
JJ-b
VV-c
?
VV-i
?
VV-bNR-r
?
NR-iNR-b
?
NR-t
NN-r
?
NN-i
?
NN-i
?
NN-b
NN-c
NN-t
VV-t
NN-t
(b) character-level syntax tree with hierarchal word structures
for ?? (middle) ? (nation) ? (construction) ? (building)
? (industry) ? (present) ? (show) ? (new) ? (style) ?
(situation)?.
Figure 1: Word-based and character-level phrase-
structure trees for the sentence ????????
??? (China?s architecture industry shows new
patterns)?, where ?l?, ?r?, ?c? denote the direc-
tions of head characters (see section 2).
(Figure 1(a)). This form of annotation does not
give character-level syntactic structures for words,
a source of linguistic information that is more fun-
damental and less sparse than atomic words.
In this paper, we investigate Chinese syn-
tactic parsing with character-level information
by extending the notation of phrase-structure
125
(constituent) trees, adding recursive structures of
characters for words. We manually annotate the
structures of 37,382 words, which cover the entire
CTB5. Using these annotations, we transform
CTB-style constituent trees into character-level
trees (Figure 1(b)). Our word structure corpus,
together with a set of tools to transform CTB-style
trees into character-level trees, is released at
https://github.com/zhangmeishan/wordstructures.
Our annotation work is in line with the work of
Vadas and Curran (2007) and Li (2011), which
provide extended annotations of Penn Treebank
(PTB) noun phrases and CTB words (on the
morphological level), respectively.
We build a character-based Chinese parsing
model to parse the character-level syntax trees.
Given an input Chinese sentence, our parser pro-
duces its character-level syntax trees (Figure 1(b)).
With richer information than word-level trees, this
form of parse trees can be useful for all the afore-
mentioned Chinese NLP applications.
With regard to task of parsing itself, an impor-
tant advantage of the character-level syntax trees is
that they allow word segmentation, part-of-speech
(POS) tagging and parsing to be performed jointly,
using an efficient CKY-style or shift-reduce algo-
rithm. Luo (2003) exploited this advantage by
adding flat word structures without manually an-
notation to CTB trees, and building a generative
character-based parser. Compared to a pipeline
system, the advantages of a joint system include
reduction of error propagation, and the integration
of segmentation, POS tagging and syntax features.
With hierarchical structures and head character in-
formation, our annotated words are more informa-
tive than flat word structures, and hence can bring
further improvements to phrase-structure parsing.
To analyze word structures in addition to phrase
structures, our character-based parser naturally
performs joint word segmentation, POS tagging
and parsing jointly. Our model is based on the
discriminative shift-reduce parser of Zhang and
Clark (2009; 2011), which is a state-of-the-art
word-based phrase-structure parser for Chinese.
We extend their shift-reduce framework, adding
more transition actions for word segmentation and
POS tagging, and defining novel features that cap-
ture character information. Even when trained
using character-level syntax trees with flat word
structures, our joint parser outperforms a strong
pipelined baseline that consists of a state-of-the-
NN-c
NN-iNN-b
?
(science)
?
(technology)
VV-l
VV-iVV-b
?
(burn)
?
(up)
NN-r
NN-iNN-b
?
(repository)
?
(saving)
NN-l
VV-iVV-b
?
(investigate)
?
(ancient)
NN-r
NN-iNN-b
?
(bad)
?
(kind)
AD-l
AD-iAD-b
?
(vain)
?
(so)
NN-r
NN-iNN-b
?
(crouching)
?
(tiger)
NN-r
NN-iNN-i
?
(hidden)
?
(dragon)
NN-c
VV-r
VV-iVV-b
?
(fiercely)
?
(sweep)
VV-r
VV-iVV-i
?
(thousands)
?
(troops)
VV-l
NN-c
NN-iNN-b
?
(teach)
?
(education)
NN-i
?
(field)
NN-r
NN
NN-fNN-f
??
(education)
?
(field)
NN-c
NN-iNN-b
?
(friend)
?
(friend)
NN-i
?
(plural)
NN-l
NN
NN-fNN-f
??
(friend)
?
(plural)
(a) subject-predicate.
NN-c
NN-iNN-b
?
(science)
?
(technology)
VV-l
VV-iVV-b
?
(burn)
?
(up)
NN-r
NN-iNN-b
?
(repository)
?
(saving)
NN-l
VV-iVV-b
?
(investigate)
?
(ancient)
NN-r
NN-iNN-b
?
(bad)
?
(kind)
AD-l
AD-iAD-b
?
(vain)
?
(so)
NN-r
NN-iNN-b
?
(crouching)
?
(tiger)
NN-r
NN-iNN-i
?
(hidden)
?
(dragon)
NN-c
VV-r
VV-iVV-b
?
(fiercely)
?
(sweep)
VV-r
VV-iVV-i
?
(thousands)
?
(troops)
VV-l
NN-c
NN-iNN-b
?
(teach)
?
(education)
NN-i
?
(field)
NN-r
NN
NN-fNN-f
??
(education)
?
(field)
NN-c
NN-iNN-b
?
(friend)
?
(friend)
NN-i
?
(plural)
NN-l
NN
NN-fNN-f
??
(friend)
?
(plural)
(b) verb-object.
NN-c
NN-iNN-b
?
(science)
?
(technology)
VV-l
VV-iVV-b
?
(burn)
?
(up)
NN-r
NN-iNN-b
?
(repository)
?
(saving)
NN-l
VV-iVV-b
?
(investigate)
?
(ancient)
NN-r
NN-iNN-b
?
(bad)
?
(kind)
AD-l
AD-iAD-b
?
(vain)
?
(so)
NN-r
NN-iNN-b
?
(crouching)
?
(tiger)
NN-r
NN-iNN-i
?
(hidden)
?
(dragon)
NN-c
VV-r
VV-iVV-b
?
(fiercely)
?
(sweep)
VV-r
VV-iVV-i
?
(thousands)
?
(troops)
VV-l
NN-c
NN-iNN-b
?
(teach)
?
(education)
NN-i
?
(field)
NN-r
NN
NN-fNN-f
??
(education)
?
(field)
NN-c
NN-iNN-b
?
(friend)
?
(friend)
NN-i
?
(plural)
NN-l
NN
NN-fNN-f
??
(friend)
?
(plural)
(c) coordination.
NN-c
NN-iNN-b
?
(science)
?
(technology)
VV-l
VV-iVV-b
?
(burn)
?
(up)
NN-r
NN-iNN-b
?
(repository)
?
(saving)
NN-l
VV-iVV-b
?
(investigate)
?
(ancient)
NN-r
NN-iNN-b
?
(bad)
?
(kind)
AD-l
AD-iAD-b
?
(vain)
?
(so)
NN-r
NN-iNN-b
?
(crouching)
?
(tiger)
NN-r
NN-iNN-i
?
(hidden)
?
(dragon)
NN-c
VV-r
VV-iVV-b
?
(fiercely)
?
(sweep)
VV-r
VV-iVV-i
?
(thousands)
?
(troops)
VV-l
NN-c
NN-iNN-b
?
(teach)
?
(education)
NN-i
?
(field)
NN-r
NN
NN-fNN-f
??
(education)
?
(field)
NN-c
NN-iNN-b
?
(friend)
?
(friend)
NN-i
?
(plural)
NN-l
NN
NN-fNN-f
??
(friend)
?
(plural)
(d) modifier-noun.
Figure 2: Inner word structures of ??? (reper-
tory)?,??? (archaeology)?, ??? (science and
technology)? and ??? (degenerate)?.
art joint segmenter and POS tagger, and our base-
line word-based parser. Our word annotations lead
to further improvements to the joint system, espe-
cially for phrase-structure parsing accuracy.
Our parser work falls in line with recent work
of joint segmentation, POS tagging and parsing
(Hatori et al, 2012; Li and Zhou, 2012; Qian
and Liu, 2012). Compared with related work,
our model gives the best published results for
joint segmentation and POS tagging, as well as
joint phrase-structure parsing on standard CTB5
evaluations. With linear-time complexity, our
parser is highly efficient, processing over 30 sen-
tences per second with a beam size of 16. An
open release of the parser is freely available at
http://sourceforge.net/projects/zpar/, version 0.6.
2 Word Structures and Syntax Trees
The Chinese language is a character-based lan-
guage. Unlike alphabetical languages, Chinese
characters convey meanings, and the meaning of
most Chinese words takes roots in their charac-
ter. For example, the word ???? (computer)? is
composed of the characters ?? (count)?, ?? (cal-
culate)? and ?? (machine)?. An informal name of
?computer? is ????, which is composed of ??
(electronic)? and ?? (brain)?.
Chinese words have internal structures (Xue,
2001; Ma et al, 2012). The way characters inter-
act within words can be similar to the way words
interact within phrases. Figure 2 shows the struc-
tures of the four words ??? (repertory)?, ???
126
NN-c
NN-iNN-b
?
(science)
?
(technology)
VV-l
VV-iVV-b
?
(burn)
?
(up)
NN-r
NN-iNN-b
?
(repository)
?
(saving)
NN-l
VV-iVV-b
?
(investigate)
?
(ancient)
NN-r
NN-iNN-b
?
(bad)
?
(kind)
AD-l
AD-iAD-b
?
(vain)
?
(so)
NN-r
NN-iNN-b
?
(crouching)
?
(tiger)
NN-r
NN-iNN-i
?
(hidden)
?
(dragon)
NN-c
VV-r
VV-iVV-b
?
(fiercely)
?
(sweep)
VV-r
VV-iVV-i
?
(thousands)
?
(troops)
VV-l
NN-c
NN-iNN-b
?
(teach)
?
(education)
NN-i
?
(field)
NN-r
NN
NN-fNN-f
??
(education)
?
(field)
NN-c
NN-iNN-b
?
(friend)
?
(friend)
NN-i
?
(plural)
NN-l
NN
NN-fNN-f
??
(friend)
?
(plural)
Figure 3: Character-level word structure of ???
?? (crouching tiger hidden dragon)?.
(archaeology)?, ??? (science and technology)?
and ??? (degenerate)?, which demonstrate
four typical syntactic structures of two-character
words, including subject-predicate, verb-object,
coordination and modifier-noun structures. Multi-
character words can also have recursive syntac-
tic structures. Figure 3 illustrates the structure
of the word ????? (crouching tiger hidden
dragon)?, which is composed of two subwords ??
? (crouching tiger)? and ??? (hidden dragon)?,
both having a modifier-noun structure.
The meaning of characters can be a useful
source of information for computational process-
ing of Chinese, and some recent work has started
to exploit this information. Zhang and Clark
(2010) found that the first character in a Chinese
word is a useful indicator of the word?s POS. They
made use of this information to help joint word
segmentation and POS tagging.
Li (2011) studied the morphological structures
of Chinese words, showing that 35% percent of
the words in CTB5 can be treated as having mor-
phemes. Figure 4(a) illustrates the morphological
structures of the words ? ??? (friends)? and
???? (educational world)?, in which the char-
acters ?? (plural)? and ?? (field)? can be treated
as suffix morphemes. They studied the influence
of such morphology to Chinese dependency pars-
ing (Li and Zhou, 2012).
The aforementioned work explores the influ-
ence of particular types of characters to Chinese
processing, yet not the full potentials of complete
word structures. We take one step further in this
line of work, annotating the full syntactic struc-
tures of 37,382 Chinese words in the form of Fig-
ure 2 and Figure 3. Our annotation covers the
entire vocabulary of CTB5. In addition to dif-
ference in coverage (100% vs 35%), our annota-
tion is structurally more informative than that of
Li (2011), as illustrated in Figure 4(b).
Our annotations are binarized recursive word
NN-c
NN-iNN-b
?
(science)
?
(technology)
VV-l
VV-iVV-b
?
(burn)
?
(up)
NN-r
NN-iNN-b
?
(repository)
?
(saving)
NN-l
VV-iVV-b
?
(investigate)
?
(ancient)
NN-r
NN-iNN-b
?
(bad)
?
(kind)
AD-l
AD-iAD-b
?
(vain)
?
(so)
NN-r
NN-iNN-b
?
(crouching)
?
(tiger)
NN-r
NN-iNN-i
?
(hidden)
?
(dragon)
NN-c
VV-r
VV-iVV-b
?
(fiercely)
?
(sweep)
VV-r
VV-iVV-i
?
(thousands)
?
(troops)
VV-l
NN-c
NN-iNN-b
?
(teach)
?
(education)
NN-i
?
(field)
NN-r
NN
NN-fNN-f
??
(education)
?
(field)
NN-c
NN-iNN-b
?
(friend)
?
(friend)
NN-i
?
(plural)
NN-l
NN
NN-fNN-f
??
(friend)
?
(plural)
(a) morphological-level word structures, where ?f? de-
notes a special mark for fine-grained words.
NN-c
NN-iNN-b
?(science) ?(technology)
VV-l
VV-iVV-b
?(burn) ?(up)
NN-r
NN-iNN-b
?(repository) ?(saving)
NN-l
VV-iVV-b
?(investigate) ?(ancient)
NN-r
NN-iNN-b
?(bad) ?(kind)
AD-l
AD-iAD-b
?(vain) ?(so)
NN-r
NN-iNN-b
?(crouching) ?(tiger)
NN-r
NN-iNN-i
?(hidden) ?(dragon)
NN-c
VV-r
VV-iVV-b
?(fiercely) ?(sweep)
VV-r
VV-iVV-i
?(thousands) ?(troops)
VV-l
NN-c
NN-iNN-b
?(teach) ?(education)
NN-i
?(field)
NN-r
NN
NN-fNN-f
??(education) ?(field)
NN-c
NN-iNN-b
?(friend) ?(friend)
NN-i
?(plural)
NN-l
NN
NN-fNN-f
??(friend) ?(plural)
(b) character-level word structures.
Figure 4: Comparison between character-level and
morphological-level word structures.
structures. For each word or subword, we spec-
ify its POS and head direction. We use ?l?, ?r?
and ?c? to indicate the ?left?, ?right? and ?coordi-
nation? head directions, respectively. The ?coor-
dination? direction is mostly used in coordination
structures, while a very small number of translit-
eration words, such as ???? (Obama)? and ??
?? (Los Angeles)?, have flat structures, and we
use ?coordination? for their left binarization. For
leaf characters, we follow previous work on word
segmentation (Xue, 2003; Ng and Low, 2004), and
use ?b? and ?i? to indicate the beginning and non-
beginning characters of a word, respectively.
The vast majority of words do not have struc-
tural ambiguities. However, the structures of some
words may vary according to different POS. For
example, ???? means ?dominate? when it is
tagged as a verb, of which the head is the left char-
acter; the same word means ?uniform dress? when
tagged as a noun, of which the head is the right
character. Thus the input of the word structure
annotation is a word together with its POS. The
annotation work was conducted by three persons,
with one person annotating the entire corpus, and
the other two checking the annotations.
Using our annotations, we can extend CTB-
style syntax trees (Figure 1(a)) into character-
level trees (Figure 1(b)). In particular, we mark
the original nodes that represent POS tags in CTB-
style trees with ?-t?, and insert our word structures
as unary subnodes of the ?-t? nodes. For the rest
of the paper, we refer to the ?-t? nodes as full-word
nodes, all nodes above full-word nodes as phrase
127
nodes, and all nodes below full-word nodes as sub-
word nodes.
Our character-level trees contain additional syn-
tactic information, which are potentially useful to
Chinese processing. For example, the head char-
acters of words can be populated up to phrase-
level nodes, and serve as an additional source of
information that is less sparse than head words. In
this paper, we build a parser that yields character-
level trees from raw character sequences. In addi-
tion, we use this parser to study the effects of our
annotations to character-based statistical Chinese
parsing, showing that they are useful in improving
parsing accuracies.
3 Character-based Chinese Parsing
To produce character-level trees for Chinese
NLP tasks, we develop a character-based parsing
model, which can jointly perform word segmen-
tation, POS tagging and phrase-structure parsing.
To our knowledge, this is the first work to develop
a transition-based system that jointly performs the
above three tasks. Trained using annotated word
structures, our parser also analyzes the internal
structures of Chinese words.
Our character-based Chinese parsing model is
based on the work of Zhang and Clark (2009),
which is a transition-based model for lexicalized
constituent parsing. They use a beam-search de-
coder so that the transition action sequence can be
globally optimized. The averaged perceptron with
early-update (Collins and Roark, 2004) is used to
train the model parameters. Their transition sys-
tem contains four kinds of actions: (1) SHIFT,
(2) REDUCE-UNARY, (3) REDUCE-BINARY and
(4) TERMINATE. The system can provide bina-
rzied CFG trees in Chomsky Norm Form, and they
present a reversible conversion procedure to map
arbitrary CFG trees into binarized trees.
In this work, we remain consistent with their
work, using the head-finding rules of Zhang and
Clark (2008), and the same binarization algo-
rithm.1 We apply the same beam-search algorithm
for decoding, and employ the averaged perceptron
with early-update to train our model.
We make two extensions to their work to en-
able joint segmentation, POS tagging and phrase-
structure parsing from the character level. First,
we modify the actions of the transition system for
1We use a left-binarization process for flat word structures
that contain more than two characters.
S2
sta ck
. . .
. . .
q u eu e
Q0 Q1 . . .S1
S1l S1r
. . . . . .
S0
S0l S0r
. . . . . .
Figure 5: A state in a transition-based model.
parsing the inner structures of words. Second, we
extend the feature set for our parsing problem.
3.1 The Transition System
In a transition-based system, an input sentence is
processed in a linear left-to-right pass, and the
output is constructed by a state-transition pro-
cess. We learn a model for scoring the transi-
tion Ai from one state STi to the next STi+1. As
shown in Figure 5, a state ST consists of a stack
S and a queue Q, where S = (? ? ? , S1, S0) con-
tains partially constructed parse trees, and Q =
(Q0, Q1, ? ? ? , Qn?j) = (cj , cj+1, ? ? ? , cn) is the
sequence of input characters that have not been
processed. The candidate transition action A at
each step is defined as follows:
? SHIFT-SEPARATE(t): remove the head
character cj from Q, pushing a subword node
S?
cj
2 onto S, assigning S?.t = t. Note that the
parse tree S0 must correspond to a full-word
or a phrase node, and the character cj is the
first character of the next word. The argu-
ment t denotes the POS of S?.
? SHIFT-APPEND: remove the head character
cj from Q, pushing a subword node S?cj onto
S. cj will eventually be combined with all the
subword nodes on top of S to form a word,
and thus we must have S?.t = S0.t.
? REDUCE-SUBWORD(d): pop the top two
nodes S0 and S1 off S, pushing a new sub-
word node S?S1 S0 onto S. The argument ddenotes the head direction of S?, of which
the value can be ?left?, ?right? or ?coordi-
nation?.3 Both S0 and S1 must be subword
nodes and S?.t = S0.t = S1.t.
2We use this notation for a compact representation of a
tree node, where the numerator represents a father node, and
the denominator represents the children.
3For the head direction ?coordination?, we extract the
head character from the left node.
128
Category Feature templates When to Apply
Structure S0ntl S0nwl S1ntl S1nwl S2ntl S2nwl S3ntl S3nwl, All
features Q0c Q1c Q2c Q3c Q0c ?Q1c Q1c ?Q2c Q2c ?Q3c,
S0ltwl S0rtwl S0utwl S1ltwl S1rtwl S1utwl,
S0nw ? S1nw S0nw ? S1nl S0nl ? S1nw S0nl ? S1nl,
S0nw ?Q0c S0nl ?Q0c S1nw ?Q0c S1nlQ0c,
S0nl ? S1nl ? S2nl S0nw ? S1nl ? S2nl S0nl ? S1nw ? S2nl S0nl ? S1nl ? S2nw,
S0nw ? S1nl ?Q0c S0nl ? S1nw ?Q0c S0nl ? S1nl ?Q0c,
S0ncl S0nct S0nctl S1ncl S1nct S1nctl,
S2ncl S2nct S2nctl S3ncl S3nct S3nctl,
S0nc ? S1nc S0ncl ? S1nl S0nl ? S1ncl S0ncl ? S1ncl,
S0nc ? Q0c S0nl ? Q0c S1nc ? Q0c S1nl ? Q0c,
S0nc ? S1nc ? Q0c S0nc ? S1nc ? Q0c ? Q1c
start(S0w) ? start(S1w) start(S0w) ? end(S1w), REDUCE-SUBWORD
indict(S1wS0w) ? len(S1wS0w) indict(S1wS0w, S0t) ? len(S1wS0w)
String t?1 ? t0 t?2 ? t?1t0 w?1 ? t0 c0 ? t0 start(w?1) ? t0 c?1 ? c0 ? t?1 ? t0, SHIFT-SEPARATE
features w?1 w?2 ? w?1 w?1,where len(w?1) = 1 end(w?1) ? c0, REDUCE-WORD
start(w?1) ? len(w?1) end(w?1) ? len(w?1) start(w?1) ? end(w?1),
w?1 ? c0 end(w?2) ? w?1 start(w?1) ? c0 end(w?2) ? end(w?1),
w?1 ? len(w?2) w?2 ? len(w?1) w?1 ? t?1 w?1 ? t?2 w?1 ? t?1 ? c0,
w?1 ? t?1 ? end(w?2) c?2 ? c?1 ? c0 ? t?1,where len(w?1) = 1 end(w?1) ? t?1,
c ? t?1 ? end(w?1),where c ? w?1 and c 6= end(w?1)
c0 ? t?1 c?1 ? c0 start(w?1) ? c0t?1 c?1 ? c0 ? t?1 SHIFT-APPEND
Table 1: Feature templates for the character-level parser. The function start(?), end(?) and len(?) denote
the first character, the last character and the length of a word, respectively.
? REDUCE-WORD: pop the top node S0 off S,
pushing a full-word node S?S0 onto S. This re-duce action generates a full-word node from
S0, which must be a subword node.
? REDUCE-BINARY(d, l): pop the top two
nodes S0 and S1 off S, pushing a binary
phrase node S?S1 S0 onto S. The argument ldenotes the constituent label of S?, and the ar-
gument d specifies the lexical head direction
of S?, which can be either ?left? or ?right?.
Both S0 and S1 must be a full-word node or
a phrase node.
? REDUCE-UNARY(l): pop the top node S0
off S, pushing a unary phrase node S?S0 onto
S. l denotes the constituent label of S?.
? TERMINATE: mark parsing complete.
Compared to set of actions in our baseline
transition-based phrase-structure parser, we have
made three major changes. First, we split the orig-
inal SHIFT action into SHIFT-SEPARATE(t)
and SHIFT-APPEND, which jointly perform the
word segmentation and POS tagging tasks. Sec-
ond, we add an extra REDUCE-SUBWORD(d) op-
eration, which is used for parsing the inner struc-
tures of words. Third, we add REDUCE-WORD,
which applies a unary rule to mark a completed
subword node as a full-word node. The new node
corresponds to a unary ?-t? node in Figure 1(b).
3.2 Features
Table 1 shows the feature templates of our model.
The feature set consists of two categories: (1)
structure features, which encode the structural in-
formation of subwords, full-words and phrases.
(2) string features, which encode the information
of neighboring characters and words.
For the structure features, the symbols S0, S1,
S2, S3 represent the top four nodes on the stack;
Q0, Q1, Q2, Q3 denote the first four characters
in the queue; S0l, S0r, S0u represent the left,
right child for a binary branching S0, and the sin-
gle child for a unary branching S0, respectively;
S1l, S1r, S1u represent the left, right child for
a binary branching S1, and the single child for
a unary branching S1, respectively; n represents
the type for a node; it is a binary value that indi-
cates whether the node is a subword node; c, w,
t and l represent the head character, word (or sub-
word), POS tag and constituent label of a node, re-
spectively. The structure features are mostly taken
129
from the work of Zhang and Clark (2009). The
feature templates in bold are novel, are designed
to encode head character information. In particu-
lar, the indict function denotes whether a word is
in a tag dictionary, which is collected by extract-
ing all multi-character subwords that occur more
than five times in the training corpus.
For string features, c0, c?1 and c?2 represent
the current character and its previous two charac-
ters, respectively; w?1 and w?2 represent the pre-
vious two words to the current character, respec-
tively; t0, t?1 and t?2 represent the POS tags of
the current word and the previous two words, re-
spectively. The string features are used for word
segmentation and POS tagging, and are adapted
from a state-of-the-art joint segmentation and tag-
ging model (Zhang and Clark, 2010).
In summary, our character-based parser con-
tains the word-based features of constituent parser
presented in Zhang and Clark (2009), the word-
based and shallow character-based features of
joint word segmentation and POS tagging pre-
sented in Zhang and Clark (2010), and addition-
ally the deep character-based features that encode
word structure information, which are the first pre-
sented by this paper.
4 Experiments
4.1 Setting
We conduct our experiments on the CTB5 cor-
pus, using the standard split of data, with sections
1?270,400?931 and 1001?1151 for training, sec-
tions 301?325 for system development, and sec-
tions 271?300 for testing. We apply the same pre-
processing step as Harper and Huang (2011), so
that the non-terminal yield unary chains are col-
lapsed to single unary rules.
Since our model can jointly process word seg-
mentation, POS tagging and phrase-structure pars-
ing, we evaluate our model for the three tasks, re-
spectively. For word segmentation and POS tag-
ging, standard metrics of word precision, recall
and F-score are used, where the tagging accuracy
is the joint accuracy of word segmentation and
POS tagging. For phrase-structure parsing, we
use the standard parseval evaluation metrics on
bracketing precision, recall and F-score. As our
constituent trees are based on characters, we fol-
low previous work and redefine the boundary of
a constituent span by its start and end characters.
In addition, we evaluate the performance of word
6570
7580
8590
95
0 10 20 30 40
64b16b4b1b
(a) Joint segmentation and
POS tagging F-scores.
3040
5060
7080
90
0 10 20 30 40
64b16b4b1b
(b) Joint constituent parsing
F-scores.
Figure 6: Accuracies against the training epoch
for joint segmentation and tagging as well as joint
phrase-structure parsing using beam sizes 1, 4, 16
and 64, respectively.
structures, using the word precision, recall and F-
score metrics. A word structure is correct only if
the word and its internal structure are both correct.
4.2 Development Results
Figure 6 shows the accuracies of our model using
different beam sizes with respect to the training
epoch. The performance of our model increases
as the beam size increases. The amount of in-
creases becomes smaller as the size of the beam
grows larger. Tested using gcc 4.7.2 and Fedora
17 on an Intel Core i5-3470 CPU (3.20GHz), the
decoding speeds are 318.2, 98.0, 30.3 and 7.9 sen-
tences per second with beam size 1, 4, 16 and 64,
respectively. Based on this experiment, we set the
beam size 64 for the rest of our experiments.
The character-level parsing model has the ad-
vantage that deep character information can be ex-
tracted as features for parsing. For example, the
head character of a word is exploited in our model.
We conduct feature ablation experiments to eval-
uate the effectiveness of these features. We find
that the parsing accuracy decreases about 0.6%
when the head character related features (the bold
feature templates in Table 1) are removed, which
demonstrates the usefulness of these features.
4.3 Final Results
In this section, we present the final results of our
model, and compare it to two baseline systems, a
pipelined system and a joint system that is trained
with automatically generated flat words structures.
The baseline pipelined system consists of the
joint segmentation and tagging model proposed by
130
Task P R F
Pipeline Seg 97.35 98.02 97.69
Tag 93.51 94.15 93.83
Parse 81.58 82.95 82.26
Flat word Seg 97.32 98.13 97.73
structures Tag 94.09 94.88 94.48
Parse 83.39 83.84 83.61
Annotated Seg 97.49 98.18 97.84
word structures Tag 94.46 95.14 94.80
Parse 84.42 84.43 84.43
WS 94.02 94.69 94.35
Table 2: Final results on test corpus.
Zhang and Clark (2010), and the phrase-structure
parsing model of Zhang and Clark (2009). Both
models give state-of-the-art performances, and are
freely available.4 The model for joint segmen-
tation and POS tagging is trained with a 16-
beam, since it achieves the best performance. The
phrase-structure parsing model is trained with a
64-beam. We train the parsing model using the
automatically generated POS tags by 10-way jack-
knifing, which gives about 1.5% increases in pars-
ing accuracy when tested on automatic segmented
and POS tagged inputs.
The joint system trained with flat word struc-
tures serves to test the effectiveness of our joint
parsing system over the pipelined baseline, since
flat word structures do not contain additional
sources of information over the baseline. It is also
used to test the usefulness of our annotation in im-
proving parsing accuracy.
Table 2 shows the final results of our model
and the two baseline systems on the test data.
We can see that both character-level joint mod-
els outperform the pipelined system; our model
with annotated word structures gives an improve-
ment of 0.97% in tagging accuracy and 2.17% in
phrase-structure parsing accuracy. The results also
demonstrate that the annotated word structures are
highly effective for syntactic parsing, giving an ab-
solute improvement of 0.82% in phrase-structure
parsing accuracy over the joint model with flat
word structures.
Row ?WS? in Table 2 shows the accuracy of
hierarchical word-structure recovery of our joint
system. This figure can be useful for high-level ap-
plications that make use of character-level trees by
4http://sourceforge.net/projects/zpar/, version 0.5.
our parser, as it reflects the capability of our parser
in analyzing word structures. In particular, the per-
formance of parsing OOV word structure is an im-
portant metric of our parser. The recall of OOV
word structures is 60.43%, while if we do not con-
sider the influences of segmentation and tagging
errors, counting only the correctly segmented and
tagged words, the recall is 87.96%.
4.4 Comparison with Previous Work
In this section, we compare our model to previous
systems on the performance of joint word segmen-
tation and POS tagging, and the performance of
joint phrase-structure parsing.
Table 3 shows the results. Kruengkrai+ ?09
denotes the results of Kruengkrai et al (2009),
which is a lattice-based joint word segmentation
and POS tagging model; Sun ?11 denotes a sub-
word based stacking model for joint segmenta-
tion and POS tagging (Sun, 2011), which uses a
dictionary of idioms; Wang+ ?11 denotes a semi-
supervised model proposed by Wang et al (2011),
which additionally uses the Chinese Gigaword
Corpus; Li ?11 denotes a generative model that
can perform word segmentation, POS tagging and
phrase-structure parsing jointly (Li, 2011); Li+
?12 denotes a unified dependency parsing model
that can perform joint word segmentation, POS
tagging and dependency parsing (Li and Zhou,
2012); Li ?11 and Li+ ?12 exploited annotated
morphological-level word structures for Chinese;
Hatori+ ?12 denotes an incremental joint model
for word segmentation, POS tagging and depen-
dency parsing (Hatori et al, 2012); they use exter-
nal dictionary resources including HowNet Word
List and page names from the Chinese Wikipedia;
Qian+ ?12 denotes a joint segmentation, POS tag-
ging and parsing system using a unified frame-
work for decoding, incorporating a word segmen-
tation model, a POS tagging model and a phrase-
structure parsing model together (Qian and Liu,
2012); their word segmentation model is a combi-
nation of character-based model and word-based
model. Our model achieved the best performance
on both joint segmentation and tagging as well as
joint phrase-structure parsing.
Our final performance on constituent parsing is
by far the best that we are aware of for the Chinese
data, and even better than some state-of-the-art
models with gold segmentation. For example, the
un-lexicalized PCFG model of Petrov and Klein
131
System Seg Tag Parse
Kruengkrai+ ?09 97.87 93.67 ?
Sun ?11 98.17* 94.02* ?
Wang+ ?11 98.11* 94.18* ?
Li ?11 97.3 93.5 79.7
Li+ ?12 97.50 93.31 ?
Hatori+ ?12 98.26* 94.64* ?
Qian+ ?12 97.96 93.81 82.85
Ours pipeline 97.69 93.83 82.26
Ours joint flat 97.73 94.48 83.61
Ours joint annotated 97.84 94.80 84.43
Table 3: Comparisons of our final model with
state-of-the-art systems, where ?*? denotes that
external dictionary or corpus has been used.
(2007) achieves 83.45%5 in parsing accuracy on
the test corpus, and our pipeline constituent pars-
ing model achieves 83.55% with gold segmenta-
tion. They are lower than the performance of our
character-level model, which is 84.43% without
gold segmentation. The main differences between
word-based and character-level parsing models are
that character-level model can exploit character
features. This further demonstrates the effective-
ness of characters in Chinese parsing.
5 Related Work
Recent work on using the internal structure of
words to help Chinese processing gives impor-
tant motivations to our work. Zhao (2009) stud-
ied character-level dependencies for Chinese word
segmentation by formalizing segmentsion task in
a dependency parsing framework. Their results
demonstrate that annotated word dependencies
can be helpful for word segmentation. Li (2011)
pointed out that the word?s internal structure is
very important for Chinese NLP. They annotated
morphological-level word structures, and a unified
generative model was proposed to parse the Chi-
nese morphological and phrase-structures. Li and
Zhou (2012) also exploited the morphological-
level word structures for Chinese dependency
parsing. They proposed a unified transition-based
model to parse the morphological and depen-
dency structures of a Chinese sentence in a unified
framework. The morphological-level word struc-
5We rerun the parser and evaluate it using the publicly-
available code on http://code.google.com/p/berkeleyparser
by ourselves, since we have a preprocessing step for the
CTB5 corpus.
tures concern only prefixes and suffixes, which
cover only 35% of entire words in CTB. Accord-
ing to their results, the final performances of their
model on word segmentation and POS tagging are
below the state-of-the-art joint segmentation and
POS tagging models. Compared to their work,
we consider the character-level word structures
for Chinese parsing, presenting a unified frame-
work for segmentation, POS tagging and phrase-
structure parsing. We can achieve improved seg-
mentation and tagging performance.
Our character-level parsing model is inspired
by the work of Zhang and Clark (2009), which
is a transition-based model with a beam-search
decoder for word-based constituent parsing. Our
work is based on the shift-reduce operations of
their work, while we introduce additional opera-
tions for segmentation and POS tagging. By such
an extension, our model can include all the fea-
tures in their work, together with the features for
segmentation and POS tagging. In addition, we
propose novel features related to word structures
and interactions between word segmentation, POS
tagging and word-based constituent parsing.
Luo (2003) was the first work to introduce the
character-based syntax parsing. They use it as
a joint framework to perform Chinese word seg-
mentation, POS tagging and syntax parsing. They
exploit a generative maximum entropy model for
character-based constituent parsing, and find that
POS information is very useful for Chinese word
segmentation, but high-level syntactic information
seems to have little effect on segmentation. Com-
pared to their work, we use a transition-based dis-
criminative model, which can benefit from large
amounts of flexible features. In addition, in-
stead of using flat structures, we manually anno-
tate hierarchal tree structures of Chinese words
for converting word-based constituent trees into
character-based constituent trees.
Hatori et al (2012) proposed the first joint work
for the word segmentation, POS tagging and de-
pendency parsing. They used a single transition-
based model to perform the three tasks. Their
work demonstrates that a joint model can improve
the performance of the three tasks, particularly
for POS tagging and dependency parsing. Qian
and Liu (2012) proposed a joint decoder for word
segmentation, POS tagging and word-based con-
stituent parsing, although they trained models for
the three tasks separately. They reported better
132
performances when using a joint decoder. In our
work, we employ a single character-based dis-
criminative model to perform segmentation, POS
tagging and phrase-structure parsing jointly, and
study the influence of annotated word structures.
6 Conclusions and Future Work
We studied the internal structures of more than
37,382 Chinese words, analyzing their structures
as the recursive combinations of characters. Using
these word structures, we extended the CTB into
character-level trees, and developed a character-
based parser that builds such trees from raw char-
acter sequences. Our character-based parser per-
forms segmentation, POS tagging and parsing
simultaneously, and significantly outperforms a
pipelined baseline. We make both our annotations
and our parser available online.
In summary, our contributions include:
? We annotated the internal structures of Chi-
nese words, which are potentially useful
to character-based studies of Chinese NLP.
We extend CTB-style constituent trees into
character-level trees using our annotations.
? We developed a character-based parsing
model that can produce our character-level
constituent trees. Our parser jointly performs
word segmentation, POS tagging and syntac-
tic parsing.
? We investigated the effectiveness of our joint
parser over pipelined baseline, and the effec-
tiveness of our annotated word structures in
improving parsing accuracies.
Future work includes investigations of our
parser and annotations on Chinese NLP tasks.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
61133012, the National ?863? Major Projects
via grant 2011AA01A207, the National ?863?
Leading Technology Research Project via grant
2012AA011102, and SRG ISTD 2012 038 from
Singapore University of Technology and Design.
References
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 111?118, Barcelona, Spain, July.
Mary Harper and Zhongqiang Huang. 2011. Chinese
statistical parsing. Handbook of Natural Language
Processing and Machine Translation.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2012. Incremental joint approach
to word segmentation, pos tagging, and dependency
parsing in chinese. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1045?
1053, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 513?521,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Zhongguo Li and Guodong Zhou. 2012. Unified de-
pendency parsing of chinese morphological and syn-
tactic structures. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1445?1454, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Zhongguo Li. 2011. Parsing the internal structure of
words: A new paradigm for chinese word segmen-
tation. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies, pages 1405?1414,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Xiaoqiang Luo. 2003. A maximum entropy Chi-
nese character-based parser. In Michael Collins and
Mark Steedman, editors, Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 192?199.
Jianqiang Ma, Chunyu Kit, and Dale Gerdemann.
2012. Semi-automatic annotation of chinese word
structure. In Proceedings of the Second CIPS-
SIGHAN Joint Conference on Chinese Language
Processing, pages 9?17, Tianjin, China, December.
Association for Computational Linguistics.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 277?284, Barcelona, Spain, July. Association
for Computational Linguistics.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
133
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Association for Computational Linguistics.
Xian Qian and Yang Liu. 2012. Joint chinese word
segmentation, pos tagging and parsing. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
501?511, Jeju Island, Korea, July. Association for
Computational Linguistics.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1385?
1394, Portland, Oregon, USA, June. Association for
Computational Linguistics.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the penn treebank. In Proceed-
ings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 240?247,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmentation
and pos tagging with semi-supervised methods using
large auto-analyzed data. In Proceedings of 5th In-
ternational Joint Conference on Natural Language
Processing, pages 309?317, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Nianwen Xue. 2001. Defining and Automatically
Identifying Words in Chinese. Ph.D. thesis, Univer-
sity of Delaware.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. International Journal of Compu-
tational Linguistics and Chinese Language Process-
ing, 8(1).
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562?
571, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Yue Zhang and Stephen Clark. 2009. Transition-
based parsing of the chinese treebank using a global
discriminative model. In Proceedings of the 11th
International Conference on Parsing Technologies
(IWPT?09), pages 162?171, Paris, France, October.
Association for Computational Linguistics.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 843?852, Cambridge,
MA, October. Association for Computational Lin-
guistics.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
Hai Zhao. 2009. Character-level dependencies in chi-
nese: Usefulness and learning. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 879?887, Athens, Greece,
March. Association for Computational Linguistics.
134
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434?443,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Fast and Accurate Shift-Reduce Constituent Parsing
Muhua Zhu?, Yue Zhang?, Wenliang Chen?, Min Zhang? and Jingbo Zhu?
?Natural Language Processing Lab., Northeastern University, China
?Singapore University of Technology and Design, Singapore
? Soochow University, China and Institute for Infocomm Research, Singapore
zhumuhua@gmail.com yue zhang@sutd.edu.sg
chenwenliang@gmail.com mzhang@i2r.a-star.edu.sg
zhujingbo@mail.neu.edu.cn
Abstract
Shift-reduce dependency parsers give
comparable accuracies to their chart-
based counterparts, yet the best shift-
reduce constituent parsers still lag behind
the state-of-the-art. One important reason
is the existence of unary nodes in phrase
structure trees, which leads to different
numbers of shift-reduce actions between
different outputs for the same input. This
turns out to have a large empirical impact
on the framework of global training and
beam search. We propose a simple yet
effective extension to the shift-reduce
process, which eliminates size differences
between action sequences in beam-search.
Our parser gives comparable accuracies
to the state-of-the-art chart parsers. With
linear run-time complexity, our parser is
over an order of magnitude faster than the
fastest chart parser.
1 Introduction
Transition-based parsers employ a set of shift-
reduce actions and perform parsing using a se-
quence of state transitions. The pioneering mod-
els rely on a classifier to make local decisions, and
search greedily for a transition sequence to build a
parse tree. Greedy, classifier-based parsers have
been developed for both dependency grammars
(Yamada and Matsumoto, 2003; Nivre et al, 2006)
and phrase-structure grammars (Sagae and Lavie,
2005). With linear run-time complexity, they were
commonly regarded as a faster but less accurate
alternative to graph-based chart parsers (Collins,
1997; Charniak, 2000; McDonald et al, 2005).
Various methods have been proposed to address
the disadvantages of greedy local parsing, among
which a framework of beam-search and global
discriminative training have been shown effective
for dependency parsing (Zhang and Clark, 2008;
Huang and Sagae, 2010). While beam-search
reduces error propagation compared with greedy
search, a discriminative model that is globally op-
timized for whole sequences of transition actions
can avoid local score biases (Lafferty et al, 2001).
This framework preserves the most important ad-
vantage of greedy local parsers, including linear
run-time complexity and the freedom to define ar-
bitrary features. With the use of rich non-local fea-
tures, transition-based dependency parsers achieve
state-of-the-art accuracies that are comparable to
the best-graph-based parsers (Zhang and Nivre,
2011; Bohnet and Nivre, 2012). In addition, pro-
cessing tens of sentences per second (Zhang and
Nivre, 2011), these transition-based parsers can be
a favorable choice for dependency parsing.
The above global-learning and beam-search
framework can be applied to transition-based
phrase-structure (constituent) parsing also (Zhang
and Clark, 2009), maintaining all the afore-
mentioned benefits. However, the effects were
not as significant as for transition-based depen-
dency parsing. The best reported accuracies of
transition-based constituent parsers still lag behind
the state-of-the-art (Sagae and Lavie, 2006; Zhang
and Clark, 2009). One difference between phrase-
structure parsing and dependency parsing is that
for the former, parse trees with different numbers
of unary rules require different numbers of actions
to build. Hence the scoring model needs to disam-
biguate between transitions sequences with differ-
ent sizes. For the same sentence, the largest output
can take twice as many as actions to build as the
434
smallest one. This turns out to have a significant
empirical impact on parsing with beam-search.
We propose an extension to the shift-reduce pro-
cess to address this problem, which gives signifi-
cant improvements to the parsing accuracies. Our
method is conceptually simple, requiring only one
additional transition action to eliminate size dif-
ferences between different candidate outputs. On
standard evaluations using both the Penn Tree-
bank and the Penn Chinese Treebank, our parser
gave higher accuracies than the Berkeley parser
(Petrov and Klein, 2007), a state-of-the-art chart
parser. In addition, our parser runs with over 89
sentences per second, which is 14 times faster than
the Berkeley parser, and is the fastest that we are
aware of for phrase-structure parsing. An open
source release of our parser (version 0.6) is freely
available on the Web. 1
In addition to the above contributions, we apply
a variety of semi-supervised learning techniques to
our transition-based parser. These techniques have
been shown useful to improve chart-based pars-
ing (Koo et al, 2008; Chen et al, 2012), but little
work has been done for transition-based parsers.
We therefore fill a gap in the literature by report-
ing empirical results using these methods. Experi-
mental results show that semi-supervised methods
give a further improvement of 0.9% in F-score on
the English data and 2.4% on the Chinese data.
Our Chinese results are the best that we are aware
of on the standard CTB data.
2 Baseline parser
We adopt the parser of Zhang and Clark (2009) for
our baseline, which is based on the shift-reduce
process of Sagae and Lavie (2005), and employs
global perceptron training and beam search.
2.1 Vanilla Shift-Reduce
Shift-reduce parsing is based on a left-to-right
scan of the input sentence. At each step, a tran-
sition action is applied to consume an input word
or construct a new phrase-structure. A stack
is used to maintain partially constructed phrase-
structures, while the input words are stored in a
buffer. The set of transition actions are
? SHIFT: pop the front word from the buffer,
and push it onto the stack.
1http://sourceforge.net/projects/zpar/
Axioms [?, 0, false,0]
Goal [S, n, true, C]
Inference Rules:
[S, i, false, c]
SHIFT [S|w, i + 1, false, c + cs]
[S|s1s0, i, false, c]
REDUCE-L/R-X [S|X, i, false, c + cr]
[S|s0, i, false, c]
UNARY-X [S|X, i, false, c + cu]
[S, n, false, c]
FINISH [S, n, true, c + cf ]
Figure 1: Deduction system of the baseline shift-
reduce parsing process.
? REDUCE-L/R-X: pop the top two con-
stituents off the stack, combine them into a
new constituent with label X, and push the
new constituent onto the stack.
? UNARY-X: pop the top constituent off the
stack, raise it to a new constituent with la-
bel X, and push the new constituent onto the
stack.
? FINISH: pop the root node off the stack and
ends parsing.
The deduction system for the process is shown
in Figure 1, where the item is formed as ?stack,
buffer front index, completion mark, score?, and
cs, cr , and cu represent the incremental score of
the SHIFT, REDUCE, and UNARY parsing steps,
respectively; these scores are calculated according
to the context features of the parser state item. n
is the number of words in the input.
2.2 Global Discriminative Training and
Beam-Search
For a given input sentence, the initial state has an
empty stack and a buffer that contains all the input
words. An agenda is used to keep the k best state
items at each step. At initialization, the agenda
contains only the initial state. At each step, every
state item in the agenda is popped and expanded
by applying a valid transition action, and the top
k from the newly constructed state items are put
back onto the agenda. The process repeats until
the agenda is empty, and the best completed state
item (recorded as candidate output) is taken for
435
Description Templates
unigrams s0tc, s0wc, s1tc, s1wc, s2tc
s2wc, s3tc, s3wc, q0wt, q1wt
q2wt, q3wt, s0lwc, s0rwc
s0uwc, s1lwc, s1rwc, s1uwc
bigrams s0ws1w, s0ws1c, s0cs1w, s0cs1c,
s0wq0w, s0wq0t, s0cq0w, s0cq0t,
q0wq1w, q0wq1t, q0tq1w, q0tq1t,
s1wq0w, s1wq0t, s1cq0w, s1cq0t
trigrams s0cs1cs2c, s0ws1cs2c, s0cs1wq0t
s0cs1cs2w, s0cs1cq0t, s0ws1cq0t
s0cs1wq0t, s0cs1cq0w
Table 1: A summary of baseline feature templates,
where si represents the ith item on the stack S and
qi denotes the ith item in the queue Q. w refers to
the head lexicon, t refers to the head POS, and c
refers to the constituent label.
the output.
The score of a state item is the total score of the
transition actions that have been applied to build
the item:
C(?) =
N?
i=1
?(ai) ? ~?
Here ?(ai) represents the feature vector for the ith
action ai in state item ?. It is computed by apply-
ing the feature templates in Table 1 to the context
of ?. N is the total number of actions in ?.
The model parameter ~? is trained with the aver-
aged perceptron algorithm, applied to state items
(sequence of actions) globally. We apply the early
update strategy (Collins and Roark, 2004), stop-
ping parsing for parameter updates when the gold-
standard state item falls off the agenda.
2.3 Baseline Features
Our baseline features are adopted from Zhang and
Clark (2009), and are shown in Table 1 Here si
represents the ith item on the top of the stack S
and qi denotes the ith item in the front end of the
queue Q. The symbol w denotes the lexical head
of an item; the symbol c denotes the constituent
label of an item; the symbol t is the POS of a lex-
ical head. These features are adapted from Zhang
and Clark (2009). We remove Chinese specific
features and make the baseline parser language-
independent.
3 Improved hypotheses comparison
Unlike dependency parsing, constituent parse
trees for the same sentence can have different
numbers of nodes, mainly due to the existence
of unary nodes. As a result, completed state
NP
NN
address
NNS
issues
VP
VB
address
NP
NNS
issues
Figure 2: Example parse trees of the same sen-
tence with different numbers of actions.
items for the same sentence can have different
numbers of unary actions. Take the phrase ?ad-
dress issues? for example, two possible parses
are shown in Figure 2 (a) and (b), respectively.
The first parse corresponds to the action sequence
[SHIFT, SHIFT, REDUCE-R-NP, FINISH], while
the second parse corresponds to the action se-
quence [SHIFT, SHIFT, UNARY-NP, REDUCE-L-
VP, FINISH], which consists of one more action
than the first case. In practice, variances between
state items can be much larger than the chosen ex-
ample. In the extreme case where a state item does
not contain any unary action, the number of ac-
tions is 2n, where n is the number of words in
the sentence. On the other hand, if the maximum
number of consequent unary actions is 2 (Sagae
and Lavie, 2005; Zhang and Clark, 2009), then the
maximum number of actions a state item can have
is 4n.
The significant variance in the number of ac-
tions N can have an impact on the linear sepa-
rability of state items, for which the feature vec-
tors are
?N
i=1 ? (ai). This turns out to have a sig-
nificant empirical influence on perceptron training
with early-update, where the training of the model
interacts with search (Daume III, 2006).
One way of improving the comparability of
state items is to reduce the differences in their
sizes, and we use a padding method to achieve
this. The idea is to extend the set of actions by
adding an IDLE action, so that completed state
items can be further expanded using the IDLE ac-
tion. The action does not change the state itself,
but simply adds to the number of actions in the
sequence. A feature vector is extracted for the
IDLE action according to the final state context,
in the same way as other actions. Using the IDLE
action, the transition sequence for the two parses
in Figure 2 can be [SHIFT, SHIFT, REDUCE-
NP, FINISH, IDLE] and [SHIFT, SHIFT, UNARY-
NP, REDUCE-L-VP, FINISH], respectively. Their
436
Axioms [?, 0, false, 0, 0]
Goal [S, n, true, m : 2n ? m ? 4n, C]
Inference Rules:
[S, i, false, k,c]
SHIFT [S|w, i + 1, false, k + 1, c + cs]
[S|s1s0, i, false, k, c]
REDUCE-L/R-X [S|X, i, false, k + 1, c + cr]
[S|s0, i, false, k, c]
UNARY-X [S|X, i, false, k + 1, c + cu]
[S, n, false, k, c]
FINISH [S, n, true, k + 1, c + cf ]
[S,n, true, k, c]
IDLE [S, n, true, k + 1, c + ci]
Figure 3: Deductive system of the extended tran-
sition system.
corresponding feature vectors have about the same
sizes, and are more linearly separable. Note that
there can be more than one action that are padded
to a sequence of actions, and the number of IDLE
actions depends on the size difference between the
current action sequence and the largest action se-
quence without IDLE actions.
Given this extension, the deduction system is
shown in Figure 3. We add the number of actions
k to an item. The initial item (Axioms) has k = 0,
while the goal item has 2n ? k ? 4n. Given this
process, beam-search decoding can be made sim-
pler than that of Zhang and Clark (2009). While
they used a candidate output to record the best
completed state item, and finish decoding when
the agenda contains no more items, we can sim-
ply finish decoding when all items in the agenda
are completed, and output the best state item in
the agenda. With this new transition process, we
experimented with several extended features,and
found that the templates in Table 2 are useful to
improve the accuracies further. Here sill denotes
the left child of si?s left child. Other notations can
be explained in a similar way.
4 Semi-supervised Parsing with Large
Data
This section discusses how to extract informa-
tion from unlabeled data or auto-parsed data to
further improve shift-reduce parsing accuracies.
We consider three types of information, including
s0llwc, s0lrwc, s0luwc
s0rlwc, s0rrwc, s0ruwc
s0ulwc, s0urwc, s0uuwc
s1llwc, s1lrwc, s1luwc
s1rlwc, s1rrwc, s1ruwc
Table 2: New features for the extended parser.
paradigmatic relations, dependency relations, and
structural relations. These relations are captured
by word clustering, lexical dependencies, and a
dependency language model, respectively. Based
on the information, we propose a set of novel fea-
tures specifically designed for shift-reduce con-
stituent parsing.
4.1 Paradigmatic Relations: Word
Clustering
Word clusters are regarded as lexical intermedi-
aries for dependency parsing (Koo et al, 2008)
and POS tagging (Sun and Uszkoreit, 2012). We
employ the Brown clustering algorithm (Liang,
2005) on unannotated data (word segmentation is
performed if necessary). In the initial state of clus-
tering, each word in the input corpus is regarded
as a cluster, then the algorithm repeatedly merges
pairs of clusters that cause the least decrease in
the likelihood of the input corpus. The clustering
results are a binary tree with words appearing as
leaves. Each cluster is represented as a bit-string
from the root to the tree node that represents the
cluster. We define a function CLU(w) to return the
cluster ID (a bit string) of an input word w.
4.2 Dependency Relations: Lexical
Dependencies
Lexical dependencies represent linguistic relations
between words: whether a word modifies another
word. The idea of exploiting lexical dependency
information from auto-parsed data has been ex-
plored before for dependency parsing (Chen et al,
2009) and constituent parsing (Zhu et al, 2012).
To extract lexical dependencies, we first run the
baseline parser on unlabeled data. To simplify
the extraction process, we can convert auto-parsed
constituency trees into dependency trees by using
Penn2Malt. 2 From the dependency trees, we ex-
tract bigram lexical dependencies ?w1, w2, L/R?
where the symbol L (R) means that w1 (w2) is the
head of w2 (w1). We also extract trigram lexical
2http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
437
dependencies ?w1, w2, w3, L/R?, where L means
that w1 is the head of w2 and w3, meanwhile w2
and w3 are required to be siblings.
Following the strategy of Chen et al (2009),
we assign categories to bigram and trigram items
separately according to their frequency counts.
Specifically, top-10% most frequent items are as-
signed to the category of High Frequency (HF);
otherwise if an item is among top 20%, we assign
it to the category of Middle Frequency (MF); oth-
erwise the category of Low Frequency (LF). Here-
after, we refer to the bigram and trigram lexical
dependency lists as BLD and TLD, respectively.
4.3 Structural Relations: Dependency
Language Model
The dependency language model is proposed by
Shen et al (2008) and is used as additional in-
formation for graph-based dependency parsing in
Chen et al (2012). Formally, given a depen-
dency tree y of an input sentence x, we can
denote by H(y) the set of words that have at
least one dependent. For each xh ? H(y), we
have a corresponding dependency structure Dh =
(xLk, . . . xL1, xh, xR1, . . . , xRm). The probability
P (Dh) is defined to be
P (Dh) = PL(Dh) ? PR(Dh)
where PL(Dh) can be in turn defined as:
PL(Dh) ? P (xL1|xh)
?P (xL2|xL1, xh)
? . . .
?P (xLk|xLk?1, . . . , xLk?N+1, xh)
PR(Dh) can be defined in a similar way.
We build dependency language models on auto-
parsed data. Again, we convert constituency trees
into dependency trees for the purpose of simplic-
ity. From the dependency trees, we build a bigram
and a trigram language model, which are denoted
by BLM and TLM, respectively. The following
are the templates of the records of the dependency
language models.
(1) ?xLi, xh, P (xLi|xh)?
(2) ?xRi, xh, P (xRi|xh)?
(3) ?xLi, xLi?1, xh, P (xLi|xLi?1, xh)?
(4) ?xRi, xRi?1, xh, P (xRi|xRi?1, xh)?
Here the templates (1) and (2) belong to BLM
and the templates (3) and (4) belong to TLM. To
Stat Train Dev Test Unlabeled
EN # sent 39.8k 1.7k 2.4k 3,139.1k# word 950.0k 40.1k 56.7k 76,041.4k
CH # sent 18.1k 350 348 11,810.7k# word 493.8k 8.0k 6.8k 269,057.2k
Table 4: Statistics on sentence and word numbers
of the experimental data.
use the dependency language models, we employ
a map function ?(r) to assign a category to each
record r according to its probability, as in Chen et
al. (2012). The following is the map function.
?(r) =
?
??
??
HP if P (r) ? top?10%
MP else if P (r) ? top?30%
LP otherwise
4.4 Semi-supervised Features
We design a set of features based on the infor-
mation extracted from auto-parsed data or unan-
notated data. The features are summarized in Ta-
ble 3. Here CLU returns a cluster ID for a word.
The functions BLDl/r(?), TLDl/r(?), BLMl/r(?),
and TLMl/r(?) check whether a given word com-
bination can be found in the corresponding lists.
For example, BLDl(s1w, s0w) returns a category
tag (HF, MF, or LF) if ?s1w, s0w,L? exits in the
list BLD, else it returns NONE.
5 Experiments
5.1 Set-up
Labeled English data employed in this paper were
derived from the Wall Street Journal (WSJ) corpus
of the Penn Treebank (Marcus et al, 1993). We
used sections 2-21 as labeled training data, section
24 for system development, and section 23 for fi-
nal performance evaluation. For labeled Chinese
data, we used the version 5.1 of the Penn Chinese
Treebank (CTB) (Xue et al, 2005). Articles 001-
270 and 440-1151 were used for training, articles
301-325 were used as development data, and arti-
cles 271-300 were used for evaluation.
For both English and Chinese data, we used ten-
fold jackknifing (Collins, 2000) to automatically
assign POS tags to the training data. We found that
this simple technique could achieve an improve-
ment of 0.4% on English and an improvement of
2.0% on Chinese. For English POS tagging, we
adopted SVMTool, 3 and for Chinese POS tagging
3http://www.lsi.upc.edu/?nlp/SVMTool/
438
Word Cluster Features
CLU(s1w) CLU(s0w) CLU(q0w)
CLU(s1w)s1t CLU(s0w)s0t CLU(q0w)q0w
Lexical Dependency Features
BLDl(s1w, s0w) BLDl(s1w, s0w)?s1t?s0t BLDr(s1w, s0w)
BLDr(s1w, s0w)?s1t?s0t BLDl(s1w, q0w)?s1t?q0t BLDl(s1w, q0w)
BLDr(s1w, q0w) BLDr(s1w, q0w)?s1t?q0t BLDl(s0w, q0w)
BLDl(s0w, q0w)?s0t?q0t BLDr(s0w, q0w)?s0t?q0t BLDr(s0w, q0w)
TLDl(s1w, s1rdw, s0w) TLDl(s1w, s1rdw, s0w)?s1t?s0t TLDr(s1w, s0ldw, s0w)
TLDr(s1w, s0ldw, s0w)?s1t?s0t TLDl(s0w, s0rdw, q0w)?s0t?q0t TLDl(s0w, s0rdw, q0w)
TLDr(s0w,NONE, q0w) TLDr(s0w,NONE, q0w)?s0t?q0t
Dependency Language Model Features
BLMl(s1w, s0w) BLMl(s1w, s0w)?s1t?s0t BLMr(s1w, s0w)
BLMr(s1w, s0w)?s1t?s0t BLMl(s0w, q0w) BLMl(s0w, q0w)?s0t?q0t
BLMr(s0w, q0w)?s0t?q0t BLMr(s0w, q0w) TLMl(s1w, s1rdw, s0w)
TLMl(s1w, s1rdw, s0w)?s1t?s0t TLMr(s1w, s0ldw, s0w) TLMr(s1w, s0ldw, s0w)?s1t?s0t
Table 3: Semi-supervised features designed on the base of word clusters, lexical dependencies, and
dependency language models. Here the symbol si denotes a stack item, qi denotes a queue item, w
represents a word, and t represents a POS tag.
Lan. System LR LP F1
EN
G Baseline 88.4 88.7 88.6
+padding 88.8 89.5 89.1
+features 89.0 89.7 89.3
CH
N Baseline 85.6 86.3 86.0
+padding 85.5 87.2 86.4
+features 85.5 87.6 86.5
Table 5: Experimental results on the English and
Chinese development sets with the padding tech-
nique and new supervised features added incre-
mentally.
we employed the Stanford POS tagger. 4
We took the WSJ articles from the TIPSTER
corpus (LDC93T3A) as unlabeled English data. In
addition, we removed from the unlabeled English
data the sentences that appear in the WSJ corpus
of the Penn Treebank. For unlabeled Chinese data,
we used Chinese Gigaword (LDC2003T09), on
which we conducted Chinese word segmentation
by using a CRF-based segmenter. Table 4 summa-
rizes data statistics on sentence and word numbers
of the data sets listed above.
We used EVALB to evaluate parser perfor-
mances, including labeled precision (LP), labeled
recall (LR), and bracketing F1. 5 For significance
tests, we employed the randomized permutation-
based tool provided by Daniel Bikel. 6
In both training and decoding, we set the beam
size to 16, which achieves a good tradeoff be-
tween efficiency and accuracy. The optimal iter-
ation number of perceptron learning is determined
4http://nlp.stanford.edu/software/tagger.shtml
5http://nlp.cs.nyu.edu/evalb
6http://www.cis.upenn.edu/?dbikel/software.html#comparator
Lan. Features LR LP F1
EN
G +word cluster 89.3 90.0 89.7
+lexical dependencies 89.7 90.3 90.0
+dependency LM 90.0 90.6 90.3
CH
N +word cluster 85.7 87.5 86.6
+lexical dependencies 87.2 88.6 87.9
+dependency LM 87.2 88.7 88.0
Table 6: Experimental results on the English and
Chinese development sets with different types of
semi-supervised features added incrementally to
the extended parser.
on the development sets. For word clustering, we
set the cluster number to 50 for both the English
and Chinese experiments.
5.2 Results on Development Sets
Table 5 reports the results of the extended parser
(baseline + padding + supervised features) on the
English and Chinese development sets. We inte-
grated the padding method into the baseline parser,
based on which we further incorporated the super-
vised features in Table 2. From the results we find
that the padding method improves the parser accu-
racies by 0.5% and 0.4% on English and Chinese,
respectively. Incorporating the supervised features
in Table 2 gives further improvements of 0.2% on
English and 0.1% on Chinese.
Based on the extended parser, we experimented
different types of semi-supervised features by
adding the features incrementally. The results are
shown in Table 6. By comparing the results in Ta-
ble 5 and the results in Table 6 we can see that the
semi-supervised features achieve an overall im-
provement of 1.0% on the English data and an im-
439
Type Parser LR LP F1
SI
Ratnaparkhi (1997) 86.3 87.5 86.9
Collins (1999) 88.1 88.3 88.2
Charniak (2000) 89.5 89.9 89.5
Sagae & Lavie (2005)? 86.1 86.0 86.0
Sagae & Lavie (2006)? 87.8 88.1 87.9
Baseline 90.0 89.9 89.9
Petrov & Klein (2007) 90.1 90.2 90.1
Baseline+Padding 90.2 90.7 90.4
Carreras et al (2008) 90.7 91.4 91.1
RE Charniak & Johnson (2005) 91.2 91.8 91.5Huang (2008) 92.2 91.2 91.7
SE
Zhu et al (2012)? 90.4 90.5 90.4
Baseline+Padding+Semi 91.1 91.5 91.3
Huang & Harper (2009) 91.1 91.6 91.3
Huang et al (2010)? 91.4 91.8 91.6
McClosky et al (2006) 92.1 92.5 92.3
Table 7: Comparison of our parsers and related
work on the English test set. ? Shift-reduce
parsers. ? The results of self-training with a sin-
gle latent annotation grammar.
Type Parser LR LP F1
SI
Charniak (2000)? 79.6 82.1 80.8
Bikel (2004)? 79.3 82.0 80.6
Baseline 82.1 83.1 82.6
Baseline+Padding 82.1 84.3 83.2
Petrov & Klein (2007) 81.9 84.8 83.3
RE Charniak & Johnson (2005)? 80.8 83.8 82.3
SE Zhu et al (2012) 80.6 81.9 81.2Baseline+Padding+Semi 84.4 86.8 85.6
Table 8: Comparison of our parsers and related
work on the test set of CTB5.1.? Huang (2009)
adapted the parsers to Chinese parsing on CTB5.1.
? We run the parser on CTB5.1 to get the results.
provement of 1.5% on the Chinese data.
5.3 Final Results
Here we report the final results on the English and
Chinese test sets. We compared the final results
with a large body of related work. We grouped the
parsers into three categories: single parsers (SI),
discriminative reranking parsers (RE), and semi-
supervised parsers (SE). Table 7 shows the com-
parative results on the English test set and Table 8
reports the comparison on the Chinese test set.
From the results we can see that our extended
parser (baseline + padding + supervised features)
outperforms the Berkeley parser by 0.3% on En-
glish, and is comparable with the Berkeley parser
on Chinese (?0.1% less). Here +padding means
the padding technique and the features in Table 2.
After integrating semi-supervised features, the
parsing accuracy on English is improved to 91.3%.
We note that the performance is on the same level
Parser #Sent/Second
Ratnaparkhi (1997) Unk
Collins (1999) 3.5
Charniak (2000) 5.7
Sagae & Lavie (2005)? 3.7?
Sagae & Lavie (2006)? 2.2?
Petrov & Klein (2007) 6.2
Carreras et al (2008) Unk
This Paper
Baseline 100.7
Baseline+Padding 89.5
Baseline+Padding+Semi 46.8
Table 9: Comparison of running times on the En-
glish test set, where the time for loading models
is excluded. ? The results of SVM-based shift-
reduce parsing with greedy search. ? The results of
MaxEnt-based shift-reduce parser with best-first
search. ? Times reported by authors running on
different hardware.
as the performance of self-trained parsers, except
for McClosky et al (2006), which is based on the
combination of reranking and self-training. On
Chinese, the final parsing accuracy is 85.6%. To
our knowledge, this is by far the best reported per-
formance on this data set.
The padding technique, supervised features,
and semi-supervised features achieve an overall
improvement of 1.4% over the baseline on En-
glish, which is significant on the level of p <
10?5. The overall improvement on Chinese is
3.0%, which is also significant on the level of
p < 10?5.
5.4 Comparison of Running Time
We also compared the running times of our parsers
with the related single parsers. We ran timing tests
on an Intel 2.3GHz processor with 8GB mem-
ory. The comparison is shown in Table 9. From
the table, we can see that incorporating semi-
supervised features decreases parsing speed, but
the semi-supervised parser still has the advantage
of efficiency over other parsers. Specifically, the
semi-supervised parser is 7 times faster than the
Berkeley parser. Note that Sagae & Lavie (2005)
and Sagae & Lavie (2006) are also shift-reduce
parsers, and their running times were evaluated on
different hardwares. In practice, the running times
of the shift-reduce parsers should be much shorter
than the reported times in the table.
5.5 Error Analysis
We conducted error analysis for the three sys-
tems: the baseline parser, the extended parser with
440
 86
 88
 90
 92
 94
1 2 3 4 5 6 7 8
F 
Sc
or
e
Span Length
Baseline
Extended
Semi-supervised
Figure 5: Comparison of parsing accuracies of
the baseline, extended parser, and semi-supervised
parsers on spans of different lengths.
the padding technique, and the semi-supervised
parser, focusing on the English test set. The analy-
sis was performed in four dimensions: parsing ac-
curacies on different phrase types, on constituents
of different span lengths, on different sentence
lengths, and on sentences with different numbers
of unknown words.
5.5.1 Different Phrase Types
Table 10 shows the parsing accuracies of the base-
line, extended parser, and semi-supervised parser
on different phrase types. Here we only consider
the nine most frequent phrase types in the English
test set. In the table, the phrase types are ordered
from left to right in the descending order of their
frequencies. We also show the improvements of
the semi-supervised parser over the baseline parser
(the last row in the table). As the results show, the
extended parser achieves improvements on most
of the phrase types with two exceptions: Preposi-
tion Prase (PP) and Quantifier Phrase (QP). Semi-
supervised features further improve parsing accu-
racies over the extended parser (QP is an excep-
tion). From the last row, we can see that improve-
ments of the semi-supervised parser over the base-
line on VP, S, SBAR, ADVP, and ADJP are above
the average improvement (1.4%).
5.5.2 Different Span Lengths
Figure 5 shows a comparison of the three parsers
on spans of different lengths. Here we consider
span lengths up to 8. As the results show, both
the padding extension and semi-supervised fea-
tures are more helpful on relatively large spans:
the performance gaps between the three parsers
are enlarged with increasing span lengths.
 82
 84
 86
 88
 90
 92
 94
10 20 30 40 50 60 70
F 
Sc
or
e
Sentence Length
Baseline
Extended
Semi-supervised
Figure 6: Comparison of parsing accuracies of
the baseline, extended parser, and semi-supervised
parser on sentences of different lengths.
5.5.3 Different Sentence Lengths
Figure 6 shows a comparison of parsing accura-
cies of the three parsers on sentences of different
lengths. Each number on the horizontal axis repre-
sents the sentences whose lengths are between the
number and its previous number. For example, the
number 30 refers to the sentences whose lengths
are between 20 and 30. From the results we can
see that semi-supervised features improve parsing
accuracy on both short and long sentences. The
points at 70 are exceptions. In fact, sentences with
lengths between 60 and 70 have only 8 instances,
and the statistics on such a small number of sen-
tences are not reliable.
5.5.4 Different Numbers of Unknown Words
Figure 4 shows a comparison of parsing accura-
cies of the baseline, extended parser, and semi-
supervised parser on sentences with different num-
bers of unknown words. As the results show,
the padding method is not very helpful on sen-
tences with large numbers of unknown words,
while semi-supervised features help significantly
on this aspect. This conforms to the intuition that
semi-supervised methods reduce data sparseness
and improve the performance on unknown words.
6 Conclusion
In this paper, we addressed the problem of dif-
ferent action-sequence lengths for shift-reduce
phrase-structure parsing, and designed a set of
novel non-local features to further improve pars-
ing. The resulting supervised parser outperforms
the Berkeley parser, a state-of-the-art chart parser,
in both accuracies and speeds. In addition, we in-
corporated a set of semi-supervised features. The
441
System NP VP S PP SBAR ADVP ADJP WHNP QP
Baseline 91.9 90.1 89.8 88.1 85.7 84.6 72.1 94.8 89.3
Extended 92.1 90.7 90.2 87.9 86.6 84.5 73.6 95.5 88.6
Semi-supervised 93.2 92.0 91.5 89.3 88.2 86.8 75.1 95.7 89.1
Improvements +1.3 +1.9 +1.7 +1.2 +2.5 +2.2 +3.0 +0.9 -0.2
Table 10: Comparison of parsing accuracies of the baseline, extended parser, and semi-supervised parsers
on different phrase types.
0 1 2 3 4 5 6 7
70
80
90
100
91
.9
8
89
.7
3
88
.8
7
87
.9
6
85
.9
5
83
.7
81
.4
2
82
.7
4
92
.1
7
90
.5
3
89
.5
1
87
.9
9
88
.6
6
87
.3
3
83
.8
9
80
.4
9
92
.8
8
91
.2
6
90
.4
3
89
.8
8
90
.3
5
86
.3
9 90
.6
8
90
.2
4
F-
sc
o
re
(%
)
Baseline Extended Semi-supervised
Figure 4: Comparison of parsing accuracies of the baseline, extended parser, and semi-supervised parser
on sentences of different unknown words.
final parser reaches an accuracy of 91.3% on En-
glish and 85.6% on Chinese, by far the best re-
ported accuracies on the CTB data.
Acknowledgements
We thank the anonymous reviewers for their valu-
able comments. Yue Zhang and Muhua Zhu
were supported partially by SRG-ISTD-2012-038
from Singapore University of Technology and De-
sign. Muhua Zhu and Jingbo Zhu were funded
in part by the National Science Foundation of
China (61073140; 61272376), Specialized Re-
search Fund for the Doctoral Program of Higher
Education (20100042110031), and the Fundamen-
tal Research Funds for the Central Universities
(N100204002). Wenliang Chen was funded par-
tially by the National Science Foundation of China
(61203314).
References
Daniel M. Bikel. 2004. On the parameter space
of generative lexicalized statistical parsing models.
Ph.D. thesis, University of Pennsylvania.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of EMNLP, pages 12?14, Jeju Island, Ko-
rea.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. Tag, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of CoNLL, pages 9?16, Manchester, England.
Eugune Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL, pages 173?180.
Eugune Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL, pages
132?139, Seattle, Washington, USA.
Wenliang Chen, Junichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving depen-
dency parsing with subtrees from auto-parsed data.
In Proceedings of EMNLP, pages 570?579, Singa-
pore.
Wenliang Chen, Min Zhang, and Haizhou Li. 2012.
Utilizing dependency language models for graph-
based dependency. In Proceedings of ACL, pages
213?222, Jeju, Republic of Korea.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL, Stroudsburg, PA, USA.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of
ACL, Madrid, Spain.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Michael Collins. 2000. Discriminative reranking
for natural language processing. In Proceedings of
ICML, pages 175?182, Stanford, CA, USA.
Hal Daume III. 2006. Practical Structured Learn-
ing for Natural Language Processing. Ph.D. thesis,
USC.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
442
across languages. In Proceedings of EMNLP, pages
832?841, Singapore.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of ACL, pages 1077?1086, Uppsala,
Sweden.
Zhongqiang Huang, Mary Harper, and Slav Petrov.
2010. Self-training with products of latent variable
grammars. In Proceedings of EMNLP, pages 12?22,
Massachusetts, USA.
Liang Huang. 2008. Forest reranking: discriminative
parsing with non-local features. In Proceedings of
ACL, pages 586?594, Ohio, USA.
Liang-Ya Huang. 2009. Improve Chinese parsing with
Max-Ent reranking parser. In Master Project Re-
port, Brown University.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings of ICML, pages 282?289, Massachusetts, USA,
June.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Master?s thesis, Massachusetts Insti-
tute of Technology.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewiz. 1993. Building a large anno-
tated corpus of English. Computational Linguistics,
19(2):313?330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the HLT/NAACL, Main Conference,
pages 152?159, New York City, USA, June.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, pages 91?
98, Ann Arbor, Michigan, June.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: a data-driven parser-generator for de-
pendency parsing. In Proceedings of LREC, pages
2216?2219.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
HLT/NAACL, pages 404?411, Rochester, New York,
April.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models.
In Proceedings of EMNLP, Rhode Island, USA.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of IWPT, pages 125?132, Vancouver, Canada.
Kenji Sagae and Alon Lavie. 2006. Parser combina-
tion by reparsing. In Proceedings of HLT/NAACL,
Companion Volume: Short Papers, pages 129?132,
New York, USA.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL, pages 577?585, Ohio, USA.
Weiwei Sun and Hans Uszkoreit. 2012. Capturing
paradigmatic and syntagmatic lexical relations: to-
wards accurate Chinese part-of-speech tagging. In
Proceedings of ACL, Jeju, Republic of Korea.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of IWPT, pages 195?206,
Nancy, France.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of ACL/HLT, pages 888?896,
Columbus, Ohio.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the Chinese Treebank using a global dis-
criminative model. In Proceedings of IWPT, Paris,
France, October.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL, pages 188?193, Portland, Ore-
gon, USA.
Muhua Zhu, Jingbo Zhu, and Huizhen Wang. 2012.
Exploiting lexical dependencies from large-scale
data for better shift-reduce constituency parsing. In
Proceedings of COLING, pages 3171?3186, Mum-
bai, India.
443
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 352?357,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning to Prune: Context-Sensitive Pruning for Syntactic MT
Wenduan Xu
Computer Laboratory
University of Cambridge
wenduan.xu@cl.cam.ac.uk
Yue Zhang
Singapore University of
Technology and Design
yue zhang@sutd.edu.sg
Philip Williams and Philipp Koehn
School of Informatics
University of Edinburgh
p.j.williams-2@sms.ed.ac.uk
pkoehn@inf.ed.ac.uk
Abstract
We present a context-sensitive chart prun-
ing method for CKY-style MT decoding.
Source phrases that are unlikely to have
aligned target constituents are identified
using sequence labellers learned from the
parallel corpus, and speed-up is obtained
by pruning corresponding chart cells. The
proposed method is easy to implement, or-
thogonal to cube pruning and additive to
its pruning power. On a full-scale English-
to-German experiment with a string-to-
tree model, we obtain a speed-up of more
than 60% over a strong baseline, with no
loss in BLEU.
1 Introduction
Syntactic MT models suffer from decoding effi-
ciency bottlenecks introduced by online n-gram
language model integration and high grammar
complexity. Various efforts have been devoted to
improving decoding efficiency, including hyper-
graph rescoring (Heafield et al, 2013; Huang and
Chiang, 2007), coarse-to-fine processing (Petrov
et al, 2008; Zhang and Gildea, 2008) and gram-
mar transformations (Zhang et al, 2006). For
more expressive, linguistically-motivated syntac-
tic MT models (Galley et al, 2004; Galley et
al., 2006), the grammar complexity has grown
considerably over hierarchical phrase-based mod-
els (Chiang, 2007), and decoding still suffers from
efficiency issues (DeNero et al, 2009).
In this paper, we study a chart pruning method
for CKY-style MT decoding that is orthogonal to
cube pruning (Chiang, 2007) and additive to its
pruning power. The main intuition of our method
is to find those source phrases (i.e. any sequence
of consecutive words) that are unlikely to have any
consistently aligned target counterparts according
to the source context and grammar constraints. We
show that by using highly-efficient sequence la-
belling models learned from the bitext used for
translation model training, such phrases can be ef-
fectively identified prior to MT decoding, and cor-
responding chart cells can be excluded for decod-
ing without affecting translation quality.
We call our method context-sensitive pruning
(CSP); it can be viewed as a bilingual adap-
tation of similar methods in monolingual pars-
ing (Roark and Hollingshead, 2008; Zhang et al,
2010) which improve parsing efficiency by ?clos-
ing? chart cells using binary classifiers. Our con-
tribution is that we demonstrate such methods can
be applied to synchronous-grammar parsing by la-
belling the source-side alone. This is achieved
through a novel training scheme where the la-
belling models are trained over the word-aligned
bitext and gold-standard pruning labels are ob-
tained by projecting target-side constituents to the
source words. To our knowledge, this is the first
work to apply this technique to MT decoding.
The proposed method is easy to implement
and effective in practice. Results on a full-scale
English-to-German experiment show that it gives
more than 60% speed-up over a strong cube prun-
ing baseline, with no loss in BLEU. While we use
a string-to-tree model in this paper, the approach
can be adapted to other syntax-based models.
352
but we need that reform process .
TOP
S-TOP
KON
denn
NP-OA
PDAT
diesen
NN
Reformproze?
VVFIN
brauchen
NP-SB
PPER
wir
PUNC.
.
r1 KON ? ? but, denn ?
r2 NP-SB ? ? we, wir ?
r3 NP-OA ? ? that reform process,
diesen Reformproze? ?
r4 TOP ? ? X1 . , S-TOP1 . ?
r5 S-TOP ? ? but X1 need X2,
denn NP-OA2 brauchen NP-SB1 ?
Figure 1: A selection of grammar rules extractable
from an example word-aligned sentence pair.
2 The Baseline String-to-Tree Model
Our baseline translation model uses the rule ex-
traction algorithm of Chiang (2007) adapted to a
string-to-tree grammar. After extracting phrasal
pairs using the standard approach of Koehn et al
(2003), all pairs whose target phrases are not ex-
haustively dominated by a constituent of the parse
tree are removed and each remaining pair, ?f, e?,
together with its constituent label, C, forms a lex-
ical grammar rule: C ? ?f, e?. The rules r1, r2,
and r3 in Figure 1 are lexical rules. Non-lexical
rules are generated by eliminating one or more
pairs of terminal substrings from an existing rule
and substituting non-terminals. This process pro-
duces the example rules r4 and r5.
Our decoding algorithm is a variant of CKY
and is similar to other algorithms tailored for spe-
cific syntactic translation grammars (DeNero et
al., 2009; Hopkins and Langmead, 2010). By tak-
ing the source-side of each rule, projecting onto it
the non-terminal labels from the target-side, and
weighting the grammar according to the model?s
local scoring features, decoding is a straightfor-
ward extension of monolingual weighted chart
parsing. Non-local features, such as n-gram lan-
guage model scores, are incorporated through
cube pruning (Chiang, 2007).
3 Chart Pruning
3.1 Motivations
The abstract rules and large non-terminal sets of
many syntactic MT grammars cause translation
productstheofvalue
NP-TOP
NP-AG
NN
Produkte
ART
der
NN
Wert
(a) en-de
productstheofvalue
NP
NN
??
NP
DEG
?
NN
??
(b) en-jp
Figure 2: Two example alignments. In (a) ?the
products? does not have a consistent alignment on
the target side, while it does in (b).
overgeneration at the span level and render decod-
ing inefficient. Prior work on monolingual syn-
tactic parsing has demonstrated that by exclud-
ing chart cells that are likely to violate constituent
constraints, decoding efficiency can be improved
with no loss in accuracy (Roark and Hollingshead,
2008). We consider a similar mechanism for syn-
tactic MT decoding by prohibiting subtranslation
generation for chart cells violating synchronous-
grammar constraints.
A motivating example is shown in Figure 2a,
where a segment of an English-German sentence
pair from the training data, along with its word
alignment and target-side parse tree is depicted.
The English phrases ?value of? and ?the products?
do not have corresponding German translations in
this example. Although the grammar may have
rules to translate these two phrases, they can be
safely pruned for this particular sentence pair.
In contrast to chart pruning for monolingual
parsing, our pruning decisions are based on the
source context, its target translation and the map-
ping between the two. This distinction is impor-
tant since the syntactic correspondence between
different language pairs is different. Suppose that
we were to translate the same English sentence
into Japanese (Figure 2a); unlike the English to
German example, the English phrase ?the prod-
ucts? will be a valid phrase that has a Japanese
translation under a target constituent, since it is
syntactically aligned to ???? (Figure 2b).
The key question to consider is how to inject
target syntax and word alignment information into
our labelling models, so that pruning decisions can
be based on the source alone, we address this in the
following two sections.
3.2 Pruning by Labelling
We use binary tags to indicate whether a source
word can start or end a multi-word phrase that has
353
1 	
 0 	
 1 	
 1 	
 1 	

(a) b-tags
1 	
 1 	
 1 	
 0 	
 1 	

(b) e-tags
Figure 3: The pruning effects of two types of bi-
nary tags. The shaded cells are pruned and two
types of tags are assigned independently.
a consistently aligned target constituent. We call
these two types the b-tag and the e-tag, respec-
tively, and use the set of values {0, 1} for both.
Under this scheme, a b-tag value of 1 indi-
cates that a source word can be the start of a
source phrase that has a consistently aligned target
phrase; similarly an e-tag of 0 indicates that a word
cannot end a source phrase. If either the b-tag or
the e-tag of an input phrase is 0, the correspond-
ing chart cells will be pruned. The pruning effects
of the two types of tags are illustrated in Figure 3.
In general, 0-valued b-tags prune a whole column
of chart cells and 0-valued e-tags prune a whole
diagonal of cells; and the chart cells on the first
row and the top-most cell are always kept so that
complete translations can always be found.
We build a separate labeller for each tag type us-
ing gold-standard b- and e-tags, respectively. We
train the labellers with maximum-entropy models
(Curran and Clark, 2003; Ratnaparkhi, 1996), us-
ing features similar to those used for suppertag-
ging for CCG parsing (Clark and Curran, 2004).
In each case, features for a pruning tag consist
of word and POS uni-grams extracted from the 5-
word window with the current word in the middle,
POS trigrams ending with the current word, as well
as two previous tags as a bigram and two separate
uni-grams. Our pruning labellers are highly effi-
cient, run in linear time and add little overhead to
decoding. During testing, in order to prevent over-
pruning, a probability cutoff value ? is used. A tag
value of 0 is assigned to a word only if its marginal
probability is greater than ?.
3.3 Gold-standard Pruning Tags
Gold-standard tags are extracted from the word-
aligned bitext used for translation model train-
ing, respecting rule extraction constraints, which
is crucial for the success of our method.
For each training sentence pair, gold-standard
b-tags and e-tags are assigned separately to the
Algorithm 1 Gold-standard Labelling Algorithm
Input forward alignment Ae?f , backward align-
ment A?f?e and 1-best parse tree ? for f
Output Tag sequences b and e for e
1: procedure TAG(e, f , ?,A, A?)
2: l? |e|
3: for i? 0 to l ? 1 do
4: b[i]? 0, e[i]? 0
5: for f [i?, j?] in ? do
6: s? {A?[k] | k ? [i?, j?]}
7: if |s| ? 1 then continue
8: i? min(s), j ? max(s)
9: if CONSISTENT(i, j, i?, j?) then
10: b[i?]? 1, e[j?]? 1
11: procedure CONSISTENT(i, j, i?, j?)
12: t? {A[k] | k ? [i, j]}
13: return min(t) ? i? and max(t) ? j?
source words. First, we initialize both tags of each
source word to 0s. Then, we iterate through all tar-
get constituent spans, and for each span, we find
its corresponding source phrase, as determined by
the word alignment. If a constituent exists for the
phrase pair, the b-tag of the first word and the e-tag
of the last word in the source phrase are set to 1s,
respectively. Pseudocode is shown in Algorithm 1.
Note that our definition of the gold-standard al-
lows source-side labels to integrate bilingual in-
formation. On line 6, the target-side syntax is
projected to the source; on line 9, consistency is
checked against word alignment.
Consider again the alignment in Figure 2a. Tak-
ing the target constituent span covering ?der Pro-
dukte? as an example, the source phrase under a
consistent word alignment is ?of the products?.
Thus, the b-tag of ?of? and the e-tag of ?prod-
ucts? are set to 1s. After considering all target
constituent spans, the complete b- and e-tag se-
quences for the source-side phrase in Figure 2a
are [1, 1, 0, 0] and [0, 0, 1, 1], respectively. Note
that, since we never prune single-word spans, we
ignore source phrases under consistent one-to-one
or one-to-many alignments.
From the gold standard data, we found 73.69%
of the 54M words do not begin a multi-word
aligned phrase and 77.71% do not end a multi-
word aligned phrase; the 1-best accuracies of the
two labellers tested on a held-out 20K sentences
are 82.50% and 88.78% respectively.
354
 0.146
 0.1465
 0.147
 0.1475
 0.148
 0.1485
 0.149
 0  2  4  6  8  10  12  14  16  18  20
BLE
U
CPU seconds/sentence
csp
cube pruning
(a) time vs. BLEU
 0.146
 0.1465
 0.147
 0.1475
 0.148
 0.1485
 0.149
 0  0.5  1  1.5  2  2.5
BLE
U
Hypothesis Count (x106)
csp
cube pruning
(b) hypo count vs. BLEU
Figure 4: Translation quality comparison with the cube pruning baseline.
4 Experiments
4.1 Setup
A Moses (Koehn et al, 2007) string-to-tree sys-
tem is used as our baseline. The training cor-
pus consists of the English-German sections of
the Europarl (Koehn, 2005) and the News Com-
mentary corpus. Discarding pairs without target-
side parses, the final training data has 2M sen-
tence pairs, with 54M and 52M words on the
English and German sides, respectively. Word-
alignments are obtained by running GIZA++ (Och
and Ney, 2000) in both directions and refined
with ?grow-diag-final-and? (Koehn et al, 2003).
For all experiments, a 5-gram language model
with Kneser-Ney smoothing (Chen and Goodman,
1996) built with the SRILM Toolkit (Stolcke and
others, 2002) is used.
The development and test sets are the 2008
WMT newstest (2,051 sentences) and 2009 WMT
newstest (2,525 sentences) respectively. Feature
weights are tuned with MERT (Och, 2003) on
the development set and output is evaluated us-
ing case-sensitive BLEU (Papineni et al, 2002).
For both rule extraction and decoding, up to seven
terminal/non-terminal symbols on the source-side
are allowed. For decoding, the maximum span-
length is restricted to 15, and the grammar is pre-
filtered to match the entire test set for both the
baseline system and the chart pruning decoder.
We use two labellers to perform b- and e-tag la-
belling independently prior to decoding. Training
of the labelling models is able to complete in un-
der 2.5 hours and the whole test set is labelled in
under 2 seconds. A standard perceptron POS tag-
ger (Collins, 2002) trained on Wall Street Journal
sections 2-21 of the Penn Treebank is used to as-
sign POS tags for both our training and test data.
4.2 Results
Figures 4a and 4b compare CSP with the cube
pruning baseline in terms of BLEU. Decoding
speed is measured by the average decoding time
and average number of hypotheses generated per
sentence. We first run the baseline decoder un-
der various beam settings (b = 100 - 2500) un-
til no further increase in BLEU is observed. We
then run the CSP decoder with a range of ? val-
ues (? = 0.91 ? 0.99), at the default beam size
of 1000 of the baseline decoder. The CSP de-
coder, which considers far fewer chart cells and
generates significantly fewer subtranslations, con-
sistently outperforms the slower baseline. It ulti-
mately achieves a BLEU score of 14.86 at a proba-
bility cutoff value of 0.98, slightly higher than the
highest score of the baseline.
At all levels of comparable translation quality,
our decoder is faster than the baseline. On aver-
age, the speed-up gained is 63.58% as measured
by average decoding time, and comparing on a
point-by-point basis, our decoder always runs over
60% faster. At the ? value of 0.98, it yields a
speed-up of 57.30%, compared with a beam size
of 400 for the baseline, where both achieved the
highest BLEU.
Figures 5a and 5b demonstrate the pruning
power of CSP (? = 0.95) in comparison with the
baseline (beam size = 300); across all the cutoff
values and beam sizes, the CSP decoder considers
54.92% fewer translation hypotheses on average
and the minimal reduction achieved is 46.56%.
Figure 6 shows the percentage of spans of dif-
ferent lengths pruned by CSP (? = 0.98). As ex-
355
 0
 1
 2
 3
 4
 5
 6
 7
 0  20  40  60  80  100  120  140
Hyp
oth
esis
 Co
unt
 (x1
06 )
Sentence Length
csp
cube pruning
(a) sentence length vs. hypo count
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
 9000
 0  20  40  60  80  100  120  140
Cha
rt C
ell C
oun
t
Sentence Length
csp
cube pruning
(b) sentence length vs. cell count
Figure 5: Search space comparison with the cube pruning baseline.
 0
 10
 20
 30
 40
 50
 60
 0  20  40  60  80  100  120
Pe
rce
nta
ge
 Pr
un
ed
 (%
)
Span Length
Figure 6: Percentage of spans of different lengths pruned at ? = 0.98.
pected, longer spans are pruned more often, as
they are more likely to be at the intersections of
cells pruned by the two types of pruning labels,
thus can be pruned by either type.
We also find CSP does not improve search qual-
ity and it leads to slightly lower model scores,
which shows that some higher scored translation
hypotheses are pruned. This, however, is perfectly
desirable. Since our pruning decisions are based
on independent labellers using contextual infor-
mation, with the objective of eliminating unlikely
subtranslations and rule applications. It may even
offset defects of the translation model (i.e. high-
scored bad translations). The fact that the output
BLEU did not decrease supports this reasoning.
Finally, it is worth noting that our string-to-tree
model does not force complete target parses to be
built during decoding, which is not required in our
pruning method either. We do not use any other
heuristics (other than keeping singleton and the
top-most cells) to make complete translation al-
ways possible. The hypothesis here is that good
labelling models should not affect the derivation
of complete target translations.
5 Conclusion
We presented a novel sequence labelling based,
context-sensitive pruning method for a string-to-
tree MT model. Our method achieves more than
60% speed-up over a state-of-the-art baseline on
a full-scale translation task. In future work, we
plan to adapt our method to models with differ-
ent rule extraction algorithms, such as Hiero and
forest-based translation (Mi and Huang, 2008).
Acknowledgements
We thank the anonymous reviewers for comments.
The first author is fully supported by the Carnegie
Trust and receives additional support from the
Cambridge Trusts. Yue Zhang is supported by
SUTD under the grant SRG ISTD 2012-038.
Philip Williams and Philipp Koehn are supported
under EU-FP7-287658 (EU BRIDGE).
356
References
S.F. Chen and J. Goodman. 1996. An empirical study
of smoothing techniques for language modeling. In
Proc. ACL, pages 310?318.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201?228.
S. Clark and J.R. Curran. 2004. The importance of su-
pertagging for wide-coverage ccg parsing. In Proc.
COLING, page 282.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proc. EMNLP,
pages 1?8.
J.R. Curran and S. Clark. 2003. Investigating gis and
smoothing for maximum entropy taggers. In Proc.
EACL, pages 91?98.
John DeNero, Mohit Bansal, Adam Pauls, and Dan
Klein. 2009. Efficient parsing for transducer gram-
mars. In Proc. NAACL-HLT, pages 227?235.
M. Galley, M. Hopkins, K. Knight, and D. Marcu.
2004. What?s in a translation rule. In Proc. HLT-
NAACL, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
COLING and ACL, pages 961?968.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2013. Grouping language model boundary words to
speed k?best extraction from hypergraphs. In Proc.
NAACL.
Mark Hopkins and Greg Langmead. 2010. SCFG
decoding without binarization. In Proc. EMNLP,
pages 646?655, October.
L. Huang and D. Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proc. ACL, volume 45, page 144.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
NAACL-HLT, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
ACL Demo Sessions, pages 177?180.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT Summit, volume 5.
H. Mi and L. Huang. 2008. Forest-based translation
rule extraction. In Proc. EMNLP, pages 206?214.
F. J. Och and H. Ney. 2000. Improved statistical
alignment models. In Proc. ACL, pages 440?447,
Hongkong, China, October.
F.J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. ACL, pages 160?
167.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. ACL, pages 311?318.
S. Petrov, A. Haghighi, and D. Klein. 2008. Coarse-
to-fine syntactic machine translation using language
projections. In Proc. ACL, pages 108?116.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. EMNLP, volume 1,
pages 133?142.
Brian Roark and Kristy Hollingshead. 2008. Classify-
ing chart cells for quadratic complexity context-free
inference. In Proc. COLING, pages 745?751.
Brian Roark and Kristy Hollingshead. 2009. Linear
complexity context-free parsing pipelines via chart
constraints. In Proc. NAACL, pages 647?655.
A. Stolcke et al 2002. Srilm-an extensible language
modeling toolkit. In Proc. ICSLP, volume 2, pages
901?904.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free gram-
mars. In Proc. ACL.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proc. NAACL, pages 256?263.
Y. Zhang, B.G. Ahn, S. Clark, C. Van Wyk, J.R. Cur-
ran, and L. Rimell. 2010. Chart pruning for fast
lexicalised-grammar parsing. In Proc. COLING,
pages 1471?1479.
357
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 144?154,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Tagging The Web: Building A Robust Web Tagger with Neural Network
Ji Ma
?
, Yue Zhang
?
and Jingbo Zhu
?
?
Northeastern University, China
?
Singapore University of Technology and Design
majineu@gmail.com
yue zhang@sutd.edu.sg
zhujingbo@mail.neu.edu.cn
Abstract
In this paper, we address the problem of
web-domain POS tagging using a two-
phase approach. The first phase learns rep-
resentations that capture regularities un-
derlying web text. The representation is
integrated as features into a neural network
that serves as a scorer for an easy-first POS
tagger. Parameters of the neural network
are trained using guided learning in the
second phase. Experiment on the SANCL
2012 shared task show that our approach
achieves 93.15% average tagging accu-
racy, which is the best accuracy reported
so far on this data set, higher than those
given by ensembled syntactic parsers.
1 Introduction
Analysing and extracting useful information from
the web has become an increasingly important re-
search direction for the NLP community, where
many tasks require part-of-speech (POS) tag-
ging as a fundamental preprocessing step. How-
ever, state-of-the-art POS taggers in the literature
(Collins, 2002; Shen et al, 2007) are mainly opti-
mized on the the Penn Treebank (PTB), and when
shifted to web data, tagging accuracies drop sig-
nificantly (Petrov and McDonald, 2012).
The problem we face here can be considered
as a special case of domain adaptation, where we
have access to labelled data on the source domain
(PTB) and unlabelled data on the target domain
(web data). Exploiting useful information from
the web data can be the key to improving web
domain tagging. Towards this end, we adopt the
idea of learning representations which has been
demonstrated useful in capturing hidden regular-
ities underlying the raw input data (web text, in
our case).
Our approach consists of two phrases. In the
pre-training phase, we learn an encoder that con-
verts the web text into an intermediate represen-
tation, which acts as useful features for prediction
tasks. We integrate the learned encoder with a set
of well-established features for POS tagging (Rat-
naparkhi, 1996; Collins, 2002) in a single neural
network, which is applied as a scorer to an easy-
first POS tagger. We choose the easy-first tagging
approach since it has been demonstrated to give
higher accuracies than the standard left-to-right
POS tagger (Shen et al, 2007; Ma et al, 2013).
In the fine-tuning phase, the parameters of the
network are optimized on a set of labelled train-
ing data using guided learning. The learned model
preserves the property of preferring to tag easy
words first. To our knowledge, we are the first to
investigate guided learning for neural networks.
The idea of learning representations from un-
labelled data and then fine-tuning a model with
such representations according to some supervised
criterion has been studied before (Turian et al,
2010; Collobert et al, 2011; Glorot et al, 2011).
While most previous work focus on in-domain se-
quential labelling or cross-domain classification
tasks, we are the first to learn representations for
web-domain structured prediction. Previous work
treats the learned representations either as model
parameters that are further optimized in super-
vised fine-tuning (Collobert et al, 2011) or as
fixed features that are kept unchanged (Turian et
al., 2010; Glorot et al, 2011). In this work,
we investigate both strategies and give empirical
comparisons in the cross-domain setting. Our re-
sults suggest that while both strategies improve
in-domain tagging accuracies, keeping the learned
representation unchanged consistently results in
better cross-domain accuracies.
We conduct experiments on the official data set
provided by the SANCL 2012 shared task (Petrov
and McDonald, 2012). Our method achieves a
93.15% average accuracy across the web-domain,
which is the best result reported so far on this data
144
set, higher than those given by ensembled syntac-
tic parsers. Our code will be publicly available at
https://github.com/majineu/TWeb.
2 Learning from Web Text
Unsupervised learning is often used for training
encoders that convert the input data to abstract rep-
resentations (i.e. encoding vectors). Such repre-
sentations capture hidden properties of the input,
and can be used as features for supervised tasks
(Bengio, 2009; Ranzato et al, 2007). Among the
many proposed encoders, we choose the restricted
Boltzmann machine (RBM), which has been suc-
cessfully used in many tasks (Lee et al, 2009b;
Hinton et al, 2006). In this section, we give some
background on RBMs and then show how they can
be used to learn representations of the web text.
2.1 Restricted Boltzmann Machine
The RBM is a type of graphical model that con-
tains two layers of binary stochastic units v ?
{0, 1}
V
and h ? {0, 1}
H
, corresponding to a set
of visible and hidden variables, respectively. The
RBM defines the joint probability distribution over
v and h by an energy function
E(v,h) = ?c
?
h? b
?
v ? h
?
Wv, (1)
which is factorized by a visible bias b ? R
V
, a
hidden bias c ? R
H
and a weight matrix W ?
R
H?V
. The joint distribution P (v,h) is given by
P (v,h) =
1
Z
exp(E(v,h)), (2)
where Z is the partition function.
The affine form of E with respect to v and h
implies that the visible variables are conditionally
independent with each other given the hidden layer
units, and vice versa. This yields the conditional
distribution:
P (v|h) =
V
?
j=1
P (v
j
|h) P (h|v) =
H
?
i=1
P (h
i
|v)
P (v
j
= 1|h) = ?(b
j
+W
?j
h) (3)
P (h
i
= 1|v) = ?(c
j
+W
i?
v) (4)
Here ? denotes the sigmoid function. Parameters
of RBMs ? = {b, c,W} can be trained efficiently
using contrastive divergence learning (CD), see
(Hinton, 2002) for detailed descriptions of CD.
2.2 Encoding Web Text with RBM
Most of the indicative features for POS disam-
biguation can be found from the words and word
combinations within a local context (Ratnaparkhi,
1996; Collins, 2002). Inspired by this observa-
tion, we apply the RBM to learn feature repre-
sentations from word n-grams. More specifically,
given the i
th
word w
i
of a sentence, we apply
RBMs to model the joint distribution of the n-gram
(w
i?l
, ? ? ? , w
i+r
), where l and r denote the left
and right window, respectively. Note that the vis-
ible units of RBMs are binary. While in our case,
each visible variable corresponds to a word, which
may take on tens-of-thousands of different values.
Therefore, the RBM need to be re-factorized to
make inference tractable.
We utilize the Word Representation RBM (WR-
RBM) factorization proposed by Dahl et al
(2012). The basic idea is to share word representa-
tions across different positions in the input n-gram
while using position-dependent weights to distin-
guish between different word orders.
Let w
k
be the k-th entry of lexicon L, and w
k
be its one-hot representation (i.e., only the k-th
component of w
k
is 1, and all the others are 0).
Let v
(j)
represents the j-th visible variable of the
WRRBM, which is a vector of length |L|. Then
v
(j)
= w
k
means that the j-th word in the n-gram
is w
k
. Let D ? R
D?|L|
be a projection matrix,
then Dw
k
projects w
k
into a D-dimensional real
value vector (embedding). For each position j,
there is a weight matrix W
(j)
? R
H?D
, which
is used to model the interaction between the hid-
den layer and the word projection in position j.
The visible biases are also shared across different
positions (b
(j)
= b ?j) and the energy function is:
E(v,h) = ?c
?
h?
n
?
j=1
(b
?
v
(j)
+ h
?
W
(j)
Dv
(j)
),
(5)
which yields the conditional distributions:
P (v|h) =
n
?
j=1
P (v
(j)
|h) P (h|v) =
?
i=1
P (h
i
|v)
P (h
i
= 1|v) = ?(c
i
+
n
?
j=1
W
(j)
i?
Dv
(j)
) (6)
P (v
(j)
= w
k
|h) =
1
Z
exp(b
?
w
k
+ h
?
W
(j)
Dw
k
)
(7)
145
Again Z is the partition function.
The parameters {b, c,D,W
(1)
, . . . ,W
(n)
}
can be trained using a Metropolis-Hastings-based
CD variant and the learned word representations
also capture certain syntactic information; see
Dahl et al (2012) for more details.
Note that one can stack standard RBMs on top
of a WRRBM to construct a Deep Belief Network
(DBN). By adopting greedy layer-wise training
(Hinton et al, 2006; Bengio et al, 2007), DBNs
are capable of modelling higher order non-linear
relations between the input, and has been demon-
strated to improve performance for many com-
puter vision tasks (Hinton et al, 2006; Bengio et
al., 2007; Lee et al, 2009a). However, in this work
we do not observe further improvement by em-
ploying DBNs. This may partly be due to the fact
that unlike computer vision tasks, the input struc-
ture of POS tagging or other sequential labelling
tasks is relatively simple, and a single non-linear
layer is enough to model the interactions within
the input (Wang and Manning, 2013).
3 Neural Network for POS
Disambiguation
We integrate the learned WRRBM into a neural
network, which serves as a scorer for POS dis-
ambiguation. The main challenge to designing
the neural network structure is: on the one hand,
we hope that the model can take the advantage
of information provided by the learned WRRBM,
which reflects general properties of web texts, so
that the model generalizes well in the web domain;
on the other hand, we also hope to improve the
model?s discriminative power by utilizing well-
established POS tagging features, such as those of
Ratnaparkhi (1996).
Our approach is to leverage the two sources of
information in one neural network by combining
them though a shared output layer, as shown in
Figure 1. Under the output layer, the network
consists of two modules: the web-feature mod-
ule, which incorporates knowledge from the pre-
trained WRRBM, and the sparse-feature module,
which makes use of other POS tagging features.
3.1 The Web-Feature Module
The web-feature module, shown in the lower left
part of Figure 1, consists of a input layer and two
hidden layers. The input for the this module is the
word n-gram (w
i?l
, . . . , w
i+r
), the form of which
Figure 1: The proposed neural network. The web-
feature module (lower left) and sparse-feature
module (lower right) are combined by a shared
output layer (upper).
is identical to the training data of the pre-trained
WRRBM.
The first layer is a linear projection layer, where
each word in the input is projected into a D-
dimensional real value vector using the projection
operation described in Section 2.2. The output of
this layer o
1
w
is the concatenation of the projec-
tions of w
i?l
, . . . , w
i+r
:
o
1
w
=
?
?
?
M
1
w
w
i?l
.
.
.
M
1
w
w
i+r
?
?
?
(8)
Here M
1
w
denotes the parameters of the first layer
of the web-feature module, which is a D ? |L|
projection matrix.
The second layer is a sigmoid layer to model
non-linear relations between the word projections:
o
2
w
= ?(M
2
w
o
1
w
+ b
2
w
) (9)
Parameters of this layer include: a bias vector
b
2
w
? R
H
and a weight matrix M
2
w
? R
H?nD
.
The web-feature module enables us to explore
the learned WRRBM in various ways. First, it al-
lows us to investigate knowledge from the WR-
RBM incrementally. We can choose to use only
the word representations of the learned WRRBM.
This can be achieved by initializing only the first
layer of the web module with the projection matrix
D of the learned WRRBM:
M
1
w
? D. (10)
Alternatively, we can choose to use the hidden
states of the WRRBM, which can be treated as the
146
representations of the input n-gram. This can be
achieved by also initializing the parameters of the
second layer of the web-feature module using the
position-dependent weight matrix and hidden bias
of the learned WRRBM:
b
2
w
? c (11)
M
2
w
? (W
(1)
, . . . ,W
(n)
) (12)
Second, the web-feature module also allows us
to make a comparison between whether or not to
further adjust the pre-trained representation in the
supervised fine-tuning phase, which corresponds
to the supervised learning strategies of Turian et al
(2010) and Collobert et al (2011), respectively. To
our knowledge, no investigations have been pre-
sented in the literature on this issue.
3.2 The Sparse-Feature Module
The sparse-feature module, as shown in the lower
right part of Figure 1, is designed to incorporate
commonly-used tagging features. The input for
this module is a vector of boolean values ?(x) =
(f
1
(x), . . . , f
k
(x)), where x denotes the partially
tagged input sentence and f
i
(x) denotes a fea-
ture function, which returns 1 if the correspond-
ing feature fires and 0 otherwise. The first layer of
this module is a linear transformation layer, which
converts the high dimensional sparse vector into a
fixed-dimensional real value vector:
o
s
= M
s
?(x) + b
s
(13)
Depending on the specific task being considered,
the output of this layer can be further fed to other
non-linear layers, such as a sigmoid or hyperbolic
tangent layer, to model more complex relations.
For POS tagging, we found that a simple linear
layer yields satisfactory accuracies.
The web-feature and sparse-feature modules are
combined by a linear output layer, as shown in the
upper part of Figure 1. The value of each unit in
this layer denotes the score of the corresponding
POS tag.
o
o
= M
o
(
o
w
o
s
)
+ b
o
(14)
In some circumstances, probability distribution
over POS tags might be a more preferable form
of output. Such distribution can be easily obtained
by adding a soft-max layer on top of the output
layer to perform a local normalization, as done by
Collobert et al (2011).
Algorithm 1 Easy-first POS tagging
Input: x a sentence of m words w
1
, . . . , w
m
Output: tag sequence of x
1: U? [w
1
, . . . , w
m
] // untagged words
2: while U 6= [] do
3: (w?,
?
t)? arg max
(w,t)?U?T
S(w, t)
4: w?.t?
?
t
5: U? U/[w?] // remove w? from U
6: end while
7: return [w
1
.t, . . . , w
m
.t]
4 Easy-first POS tagging with Neural
Network
The neural network proposed in Section 3 is used
for POS disambiguation by the easy-first POS tag-
ger. Parameters of the network are trained using
guided learning, where learning and search inter-
act with each other.
4.1 Easy-first POS tagging
Pseudo-code of easy-first tagging is shown in Al-
gorithm 1. Rather than tagging a sentence from
left to right, easy-first tagging is based on a deter-
ministic process, repeatedly selecting the easiest
word to tag. Here ?easiness? is evaluated based
on a statistical model. At each step, the algorithm
adopts a scorer, the neural network in our case,
to assign a score to each possible word-tag pair
(w, t), and then selects the highest score one (w?,
?
t)
to tag (i.e., tag w? with
?
t). The algorithm repeats
until all words are tagged.
4.2 Training
The training algorithm repeats for several itera-
tions over the training data, which is a set of sen-
tences labelled with gold standard POS tags. In
each iteration, the procedure shown in Algorithm
2 is applied to each sentence in the training set.
At each step during the processing of a training
example, the algorithm calculates a margin loss
based on two word-tag pairs (w, t) and (w?,
?
t) (line
4 ? line 6). (w, t) denotes the word-tag pair that
has the highest model score among those that are
inconsistent with the gold standard, while (w?,
?
t)
denotes the one that has the highest model score
among those that are consistent with the gold stan-
dard. If the loss is zero, the algorithm continues to
process the next untagged word. Otherwise, pa-
rameters are updated using back-propagation.
The standard back-propagation algorithm
147
(Rumelhart et al, 1988) cannot be applied
directly. This is because the standard loss is
calculated based on a unique input vector. This
condition does not hold in our case, because w?
and w may refer to different words, which means
that the margin loss in line 6 of Algorithm 2 is
calculated based on two different input vectors,
denoted by ?w?? and ?w?, respectively.
We solve this problem by decomposing the mar-
gin loss in line 6 into two parts:
? 1 + nn(w, t), which is associated with ?w?;
? ?nn(w?,
?
t), which is associated with ?w??.
In this way, two separate back-propagation up-
dates can be used to update the model?s parameters
(line 8 ? line 11). For the special case where w?
and w do refer to the same word w, it can be easily
verified that the two separate back-propagation up-
dates equal to the standard back-propagation with
a loss 1 + nn(w, t)? nn(w,
?
t) on the input ?w?.
The algorithm proposed here belongs to a gen-
eral framework named guided learning, where
search and learning interact with each other. The
algorithm learns not only a local classifier, but also
the inference order. While previous work (Shen et
al., 2007; Zhang and Clark, 2011; Goldberg and
Elhadad, 2010) apply guided learning to train a
linear classifier by using variants of the percep-
tron algorithm, we are the first to combine guided
learning with a neural network, by using a margin
loss and a modified back-propagation algorithm.
5 Experiments
5.1 Setup
Our experiments are conducted on the data set
provided by the SANCL 2012 shared task, which
aims at building a single robust syntactic anal-
ysis system across the web-domain. The data
set consists of labelled data for both the source
(Wall Street Journal portion of the Penn Treebank)
and target (web) domains. The web domain data
can be further classified into five sub-domains, in-
cluding emails, weblogs, business reviews, news
groups and Yahoo!Answers. While emails and
weblogs are used as the development sets, reviews,
news groups and Yahoo!Answers are used as the
final test sets. Participants are not allowed to use
web-domain labelled data for training. In addi-
tion to labelled data, a large amount of unlabelled
data on the web domain is also provided. Statistics
Algorithm 2 Training over one sentence
Input: (x, t) a tagged sentence, neural net nn
Output: updated neural net nn
?
1: U? [w
1
, . . . , w
m
] // untagged words
2: R? [(w
1
, t
1
), . . . , (w
m
, t
m
)] // reference
3: while U 6= [] do
4: (w, t)? arg max
(w,t)?(U?T/R)
nn(w, t)
5: (w?,
?
t)? arg max
(w,t)?R
nn(w, t)
6: loss? max(0, 1 + nn(w, t)? nn(w?,
?
t))
7: if loss > 0 then
8: e?? nn.BackPropErr(?w??,?nn(w?,
?
t))
9: e? nn.BackPropErr(?w?, 1+nn(w, t))
10: nn.Update(?w??, e?)
11: nn.Update(?w?, e)
12: else
13: U? U/{w?}, R? R/(w?,
?
t)
14: end if
15: end while
16: return nn
about labelled and unlabelled data are summarized
in Table 1 and Table 2, respectively.
The raw web domain data contains much noise,
including spelling error, emotions and inconsis-
tent capitalization. Following some participants
(Le Roux et al, 2012), we conduct simple prepro-
cessing steps to the input of the development and
the test sets
1
? Neutral quotes are transformed to opening or
closing quotes.
? Tokens starting with ?www.?, ?http.? or end-
ing with ?.org?, ?.com? are converted to a
?#URL? symbol
? Repeated punctuations such as ?!!!!? are col-
lapsed into one.
? Left brackets such as ?<?,?{? and ?[? are
converted to ?-LRB-?. Similarly, right brack-
ets are converted to ?-RRB-?
? Upper cased words that contain more than 4
letters are lowercased.
? Consecutive occurrences of one or more dig-
its within a word are replaced with ?#DIG?
We apply the same preprocessing steps to all the
unlabelled data. In addition, following Dahl et
1
The preprocessing steps make use of no POS knowledge,
and does not bring any unfair advantages to the participants.
148
Training set Dev set Test set
WSJ-Train Emails Weblogs WSJ-dev Answers Newsgroups Reviews WSJ-test
#Sen 30060 2,450 1,016 1,336 1,744 1,195 1,906 1,640
#Words 731,678 29,131 24,025 32,092 28,823 20,651 28,086 35,590
#Types 35,933 5,478 4,747 5,889 4,370 4,924 4,797 6,685
Table 1: Statistics of the labelled data. #Sen denotes number of sentences. #Words and #Types denote
number of words and unique word types, respectively.
Emails Weblogs Answers Newsgroups Reviews
#Sen 1,194,173 524,834 27,274 1,000,000 1,965,350
#Words 17,047,731 10,365,284 424,299 18,424,657 29,289,169
#Types 221,576 166,515 33,325 357,090 287,575
Table 2: Statistics of the raw unlabelled data.
features templates
unigram H(w
i
), C(w
i
), L(w
i
), L(w
i?1
), L(w
i+1
), t
i?2
, t
i?1
, t
i+1
, t
i+2
bigram L(w
i
) L(w
i?1
), L(w
i
) L(w
i+1
), t
i?2
 t
i?1
, t
i?1
 t
i+1
, t
i+1
 t
i+2
,
L(w
i
) t
i?2
, L(w
i
) t
i?1
, L(w
i
) t
i+1
, L(w
i
) t
i+2
trigram L(w
i
) t
i?2
 t
i?1
, L(w
i
) t
i?1
 t
i+1
, L(w
i
) t
i+1
 t
i+2
Table 3: Feature templates, where w
i
denotes the current word. H(w) and C(w) indicates whether w
contains hyphen and upper case letters, respectively. L(w) denotes a lowercased w.
al. (2012) and Turian et al (2010), we also low-
ercased all the unlabelled data and removed those
sentences that contain less than 90% a-z letters.
The tagging performance is evaluated accord-
ing to the official evaluation metrics of SANCL
2012. The tagging accuracy is defined as the per-
centage of words (punctuations included) that are
correctly tagged. The averaged accuracies are cal-
culated across the web domain data.
We trained the WRRBM on web-domain data
of different sizes (number of sentences). The data
sets are generated by first concatenating all the
cleaned unlabelled data, then selecting sentences
evenly across the concatenated file.
For each data set, we investigate an extensive set
of combinations of hyper-parameters: the n-gram
window (l, r) in {(1, 1), (2, 1), (1, 2), (2, 2)}; the
hidden layer size in {200, 300, 400}; the learning
rate in {0.1, 0.01, 0.001}. All these parameters are
selected according to the averaged accuracy on the
development set.
5.2 Baseline
We reimplemented the greedy easy-first POS tag-
ger of Ma et al (2013), which is used for all the
experiments. While the tagger of Ma et al (2013)
utilizes a linear scorer, our tagger adopts the neural
network as its scorer. The neural network of our
baseline tagger only contains the sparse-feature
module. We use this baseline to examine the per-
formance of a tagger trained purely on the source
domain. Feature templates are shown in Table 3,
which are based on those of Ratnaparkhi (1996)
and Shen et al (2007).
Accuracies of the baseline tagger are shown in
the upper part of Table 6. Compared with the
performance of the official baseline (row 4 of Ta-
ble 6), which is evaluated based on the output of
BerkeleyParser (Petrov et al, 2006; Petrov and
Klein, 2007), our baseline tagger achieves com-
parable accuracies on both the source and target
domain data. With data preprocessing, the aver-
age accuracy boosts to about 92.02 on the test set
of the target domain. This is consistent with pre-
vious work (Le Roux et al, 2011), which found
that for noisy data such as web domain text, data
cleaning is a effective and necessary step.
5.3 Exploring the Learned Knowledge
As mentioned in Section 3.1, the knowledge
learned from the WRRBM can be investigated
incrementally, using word representation, which
corresponds to initializing only the projection
layer of web-feature module with the projection
matrix of the learned WRRBM, or ngram-level
representation, which corresponds to initializing
both the projection and sigmoid layers of the web-
feature module by the learned WRRBM. In each
case, there can be two different training strate-
gies depending on whether the learned representa-
tions are further adjusted or kept unchanged dur-
ing the fine-turning phrase. Experimental results
under the 4 combined settings on the development
sets are illustrated in Figure 2, 3 and 4, where the
149
96.5
96.6
96.7
96.8
96.9
200 400 600 800 1000
Acc
ura
cy
Number of unlabelled sentences (k)
WSJ
word-fixedword-adjustngram-fixedngram-adjust
Figure 2: Tagging accuracies on the source-
domain data. ?word? and ?ngram? denote using
word representations and n-gram representations,
respectively. ?fixed? and ?adjust? denote that the
learned representation are kept unchanged or fur-
ther adjusted in supervised learning, respectively.
89.8
90
90.2
90.4
90.6
90.8
91
200 400 600 800 1000
Acc
ura
cy
Number of unlabelled sentences (k)
Email
word-fixedword-adjustngram-fixedngram-adjust
Figure 3: Accuracies on the email domain.
94.8
95
95.2
95.4
95.6
95.8
200 400 600 800 1000
Acc
ura
cy
Number of unlabelled sentences (k)
Weblog
word-fixedword-adjustngram-fixedngram-adjust
Figure 4: Accuracies on the weblog domain.
x-axis denotes the size of the training data and y-
axis denotes tagging accuracy.
5.3.1 Effect of the Training Strategy
From Figure 2 we can see that when knowl-
edge from the pre-trained WRRBM is incorpo-
method all non-oov oov
baseline 89.81 92.42 65.64
word-adjust +0.09 ?0.05 +1.38
word-fix +0.11 +0.13 +1.73
ngram-adjust +0.53 +0.52 +0.53
ngram-fix +0.69 +0.60 +2.30
Table 4: Performance on the email domain.
rated, both the training strategies (?word-fixed?
vs ?word-adjusted?, ?ngram-fixed? vs ?ngram-
adjusted?) improve accuracies on the source do-
main, which is consistent with previous findings
(Turian et al, 2010; Collobert et al, 2011). In
addition, adjusting the learned representation or
keeping them fixed does not result in too much dif-
ference in tagging accuracies.
On the web-domain data, shown in Figure 3 and
4, we found that leaving the learned representation
unchanged (?word-fixed?, ?ngram-fixed?) yields
consistently higher performance gains. This re-
sult is to some degree expected. Intuitively, unsu-
pervised pre-training moves the parameters of the
WRRBM towards the region where properties of
the web domain data are properly modelled. How-
ever, since fine-tuning is conducted with respect
to the source domain, adjusting the parameters
of the pre-trained representation towards optimiz-
ing source domain tagging accuracies would dis-
rupt its ability in modelling the web domain data.
Therefore, a better idea is to keep the representa-
tion unchanged so that we can learn a function that
maps the general web-text properties to its syntac-
tic categories.
5.3.2 Word and N-gram Representation
From Figures 2, 3 and 4, we can see that
adopting the ngram-level representation consis-
tently achieves better performance compared with
using word representations only (?word-fixed?
vs ?ngram-fixed?, ?word-adjusted? vs ?ngram-
adjusted?). This result illustrates that the ngram-
level knowledge captures more complex interac-
tions of the web text, which cannot be recovered
by using only word embeddings. Similar result
was reported by Dahl et al (2012), who found
that using both the word embeddings and the hid-
den units of a tri-gram WRRBM as additional fea-
tures for a CRF chunker yields larger improve-
ments than using word embeddings only.
Finally, more detailed accuracies under the 4
settings on the email domain are shown in Table
4. We can see that the improvement of using word
150
RBM-E RBM-W RBM-M
+acc%
Emails +0.73 +0.37 +0.69
Weblog +0.31 +0.52 +0.54
cov%
Emails 95.24 92.79 93.88
Weblog 90.21 97.74 94.77
Table 5: Effect of unlabelled data. ?+acc? denotes
improvement in tagging accuracy and ?cov? de-
notes the lexicon coverages.
representations mainly comes from better accu-
racy of out-of-vocabulary (oov) words. By con-
trast, using n-gram representations improves the
performance on both oov and non-oov.
5.4 Effect of Unlabelled Domain Data
In some circumstances, we may know beforehand
that the target domain data belongs to a certain
sub-domain, such as the email domain. In such
cases, it might be desirable to train WRRBM using
data only on that domain. We conduct experiments
to test whether using the target domain data to
train the WRRBM yields better performance com-
pared with using mixed data from all sub-domains.
We trained 3 WRRBMs using the email do-
main data (RBM-E), weblog domain data (RBM-
W) and mixed domain data (RBM-M), respec-
tively, with each data set consisting of 300k sen-
tences. Tagging performance and lexicon cover-
ages of each data set on the development sets are
shown in Table 5. We can see that using the target
domain data achieves similar improvements com-
pared with using the mixed data. However, for the
email domain, RBM-W yields much smaller im-
provement compared with RBM-E, and vice versa.
From the lexicon coverages, we can see that the
sub-domains varies significantly. The results sug-
gest that using mixed data can achieve almost as
good performance as using the target sub-domain
data, while using mixed data yields a much more
robust tagger across all sub-domains.
5.5 Final Results
The best result achieved by using a 4-gram WR-
RBM, (w
i?2
, . . . , w
i+1
), with 300 hidden units
learned on 1,000k web domain sentences are
shown in row 3 of Table 6. Performance of the
top 2 systems of the SANCL 2012 task are also
shown in Table 6. Our greedy tagger achieves 93%
tagging accuracy, which is significantly better than
the baseline?s 92.02% accuracy (p < 0.05 by Mc-
Nemar?s test). Moreover, we achieve the high-
est tagging accuracy reported so far on this data
set, surpassing those achieved using parser combi-
nations based on self-training (Tang et al, 2012;
Le Roux et al, 2012). In addition, different from
Le Roux et al (2012), we do not use any external
resources in data cleaning.
6 Related Work
Learning representations has been intensively
studied in computer vision tasks (Bengio et al,
2007; Lee et al, 2009a). In NLP, there is also
much work along this line. In particular, Col-
lobert et al (2011) and Turian et al (2010) learn
word embeddings to improve the performance of
in-domain POS tagging, named entity recogni-
tion, chunking and semantic role labelling. Yang
et al (2013) induce bi-lingual word embeddings
for word alignment. Zheng et al (2013) investi-
gate Chinese character embeddings for joint word
segmentation and POS tagging. While those ap-
proaches mainly explore token-level representa-
tions (word or character embeddings), using WR-
RBM is able to utilize both word and n-gram rep-
resentations.
Titov (2011) and Glorot et al (2011) propose
to learn representations from the mixture of both
source and target domain unlabelled data to im-
prove cross-domain sentiment classification. Titov
(2011) also propose a regularizer to constrain the
inter-domain variability. In particular, their reg-
ularizer aims to minimize the Kullback-Leibler
(KL) distance between the marginal distributions
of the learned representations on the source and
target domains.
Their work differs from ours in that their ap-
proaches learn representations from the feature
vectors for sentiment classification, which might
be of thousands of dimensions. Such high di-
mensional input gives rise to high computational
cost and it is not clear whether those approaches
can be applied to large scale unlabelled data, with
hundreds of millions of training examples. Our
method learns representations from only word n-
grams with n ranging from 3 to 5, which can
be easily applied to large scale-data. In addition,
while Titov (2011) and Glorot et al (2011) use the
learned representation to improve cross-domain
classification tasks, we are the first to apply it to
cross-domain structured prediction.
Blitzer et al (2006) propose to induce shared
representations for domain adaptation, which is
based on the alternating structure optimization
151
System Answer Newsgroup Review WSJ-t Avg
baseline-raw 89.79 91.36 89.96 97.09 90.31
baseline-clean 91.35 92.06 92.92 97.09 92.02
best-clean 92.37 93.59 93.62 97.44 93.15
baseline-offical 90.20 91.24 89.33 97.08 90.26
Le Roux et al(2011) 91.79 93.81 93.11 97.29 92.90
Tang et al (2012) 91.76 92.91 91.94 97.49 92.20
Table 6: Main results. ?baseline-raw? and ?baseline-clean? denote performance of our baseline tagger
on the raw and cleaned data, respectively. ?best-clean? is best performance achieved using a 4-gram
WRRBM. The lower part shows accuracies of the official baseline and that of the top 2 participants.
(ASO) method of Ando and Zhang (2005). The
idea is to project the original feature representa-
tions into low dimensional representations, which
yields a high-accuracy classifier on the target do-
main. The new representations are induced based
on the auxiliary tasks defined on unlabelled data
together with a dimensionality reduction tech-
nique. Such auxiliary tasks can be specific to the
supervised task. As pointed out by Plank (2009),
for many NLP tasks, defining the auxiliary tasks is
a non-trivial engineering problem. Compared with
Blitzer et al (2006), the advantage of using RBMs
is that it learns representations in a pure unsuper-
vised manner, which is much simpler.
Besides learning representations, another line
of research addresses domain-adaptation by in-
stance re-weighting (Bickel et al, 2007; Jiang
and Zhai, 2007) or feature re-weighting (Satpal
and Sarawagi, 2007). Those methods assume that
each example x that has a non-zero probability on
the source domain must have a non-zero proba-
bility on the target domain, and vice-versa. As
pointed out by Titov (2011), such an assumption
is likely to be too restrictive since most NLP tasks
adopt word-based or lexicon-based features that
vary significantly across different domains.
Regarding using neural networks for sequential
labelling, our approach shares similarity with that
of Collobert et al (2011). In particular, we both
use a non-linear layer to model complex relations
underling word embeddings. However, our net-
work differs from theirs in the following aspects.
Collobert et al (2011) model the dependency be-
tween neighbouring tags in a generative manner,
by employing a transition score A
ij
. Training the
score involves a forward process of complexity
O(nT
2
), where T denotes the number of tags. Our
model captures such a dependency in a discrimina-
tive manner, by just adding tag-related features to
the sparse-feature module. In addition, Collobert
et al (2011) train their network by maximizing the
training set likelihood, while our approach is to
minimize the margin loss using guided learning.
7 Conclusion
We built a web-domain POS tagger using a
two-phase approach. We used a WRRBM to
learn the representation of the web text and
incorporate the representation in a neural net-
work, which is trained using guided learning
for easy-first POS tagging. Experiment showed
that our approach achieved significant improve-
ment in tagging the web domain text. In ad-
dition, we found that keeping the learned repre-
sentations unchanged yields better performance
compared with further optimizing them on the
source domain data. We release our tools at
https://github.com/majineu/TWeb.
For future work, we would like to investigate
the two-phase approach to more challenging tasks,
such as web domain syntactic parsing. We be-
lieve that high-accuracy web domain taggers and
parsers would benefit a wide range of downstream
tasks such as machine translation
2
.
8 Acknowledgements
We would like to thank Hugo Larochelle for his
advices on re-implementing WRRBM. We also
thank Nan Yang, Shujie Liu and Tong Xiao for
the fruitful discussions, and three anonymous re-
viewers for their insightful suggestions. This re-
search was supported by the National Science
Foundation of China (61272376; 61300097), the
research grant T2MOE1301 from Singapore Min-
istry of Education (MOE) and the start-up grant
SRG ISTD2012038 from SUTD.
References
Rie Ando and Tong Zhang. 2005. A high-performance
semi-supervised learning method for text chunk-
2
This work is done while the first author is visiting SUTD.
152
ing. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(ACL?05), pages 1?9, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
Hugo Larochelle. 2007. Greedy layer-wise train-
ing of deep networks. In B. Sch?olkopf, J. Platt, and
T. Hoffman, editors, Advances in Neural Informa-
tion Processing Systems 19, pages 153?160. MIT
Press, Cambridge, MA.
Yoshua Bengio. 2009. Learning deep architectures for
AI. Foundations and Trends in Machine Learning,
2(1):1?127. Also published as a book. Now Pub-
lishers, 2009.
Steffen Bickel, Michael Brckner, and Tobias Scheffer.
2007. Discriminative learning for differing training
and test distributions. In Proc of ICML 2007, pages
81?88. ACM Press.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 120?128, Sydney, Australia, July.
Association for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10, EMNLP
?02, pages 1?8, Stroudsburg, PA, USA. Association
for Computational Linguistics.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:2493?2537.
George E. Dahl, Ryan P. Adams, and Hugo Larochelle.
2012. Training restricted boltzmann machines on
word observations. In John Langford and Joelle
Pineau, editors, Proceedings of the 29th Interna-
tional Conference on Machine Learning (ICML-12),
ICML ?12, pages 679?686, New York, NY, USA,
July. Omnipress.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Proc of
ICML 2011, pages 513?520.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 742?750, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye
Teh. 2006. A fast learning algorithm for deep belief
nets. Neural Comput., 18(7):1527?1554, July.
Geoffrey E. Hinton. 2002. Training products of ex-
perts by minimizing contrastive divergence. Neural
Comput., 14(8):1771?1800, August.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in nlp. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 264?271,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Joseph Le Roux, Jennifer Foster, Joachim Wagner, Ra-
sul Samad Zadeh Kaljahi, and Anton Bryl. 2012.
DCU-Paris13 Systems for the SANCL 2012 Shared
Task. In Proceedings of the NAACL 2012 First
Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL), pages 1?4, Montr?eal, Canada,
June.
Honglak Lee, Roger Grosse, Rajesh Ranganath, and
Andrew Y. Ng. 2009a. Convolutional deep belief
networks for scalable unsupervised learning of hi-
erarchical representations. In Proc of ICML 2009,
pages 609?616.
Honglak Lee, Peter Pham, Yan Largman, and Andrew
Ng. 2009b. Unsupervised feature learning for audio
classification using convolutional deep belief net-
works. In Y. Bengio, D. Schuurmans, J. Lafferty,
C. K. I. Williams, and A. Culotta, editors, Advances
in Neural Information Processing Systems 22, pages
1096?1104.
Ji Ma, Jingbo Zhu, Tong Xiao, and Nan Yang. 2013.
Easy-first pos tagging and dependency parsing with
beam search. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 110?114,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Association for Computational Linguistics.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. Notes of
the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433?440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Barbara Plank. 2009. Structural correspondence learn-
ing for parse disambiguation. In Alex Lascarides,
153
Claire Gardent, and Joakim Nivre, editors, EACL
(Student Research Workshop), pages 37?45. The As-
sociation for Computer Linguistics.
Marc?Aurelio Ranzato, Christopher Poultney, Sumit
Chopra, and Yann LeCun. 2007. Efficient learn-
ing of sparse representations with an energy-based
model. In B. Sch?olkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Process-
ing Systems 19, pages 1137?1144. MIT Press, Cam-
bridge, MA.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1988. Neurocomputing: Foundations
of research. chapter Learning Representations
by Back-propagating Errors, pages 696?699. MIT
Press, Cambridge, MA, USA.
Sandeepkumar Satpal and Sunita Sarawagi. 2007. Do-
main adaptation of conditional probability models
via feature subsetting. In PKDD, volume 4702 of
Lecture Notes in Computer Science, pages 224?235.
Springer.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 760?767, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Buzhou Tang, Min Jiang, and Hua Xu. 2012.
Varderlibt?s systems for sancl2012 shared task. In
Proceedings of the NAACL 2012 First Workshop
on Syntactic Analysis of Non-Canonical Language
(SANCL), Montr?eal, Canada, June.
Ivan Titov. 2011. Domain adaptation by constraining
inter-domain variability of latent feature representa-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 62?71, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 384?394, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Mengqiu Wang and Christopher D. Manning. 2013.
Effect of non-linear deep architecture in sequence la-
beling. In Proceedings of the 6th International Joint
Conference on Natural Language Processing (IJC-
NLP).
Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Neng-
hai Yu. 2013. Word alignment modeling with con-
text dependent deep neural network. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 166?175, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Yue Zhang and Stephen Clark. 2011. Syntax-based
grammaticality improvement using ccg and guided
search. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1147?1157, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013. Deep learning for Chinese word segmenta-
tion and POS tagging. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 647?657, Seattle, Wash-
ington, USA, October. Association for Computa-
tional Linguistics.
154
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 218?227,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Shift-Reduce CCG Parsing with a Dependency Model
Wenduan Xu
University of Cambridge
Computer Laboratory
wx217@cam.ac.uk
Stephen Clark
University of Cambridge
Computer Laboratory
sc609@cam.ac.uk
Yue Zhang
Singapore University
of Technology and Design
yue zhang@sutd.edu.sg
Abstract
This paper presents the first dependency
model for a shift-reduce CCG parser. Mod-
elling dependencies is desirable for a num-
ber of reasons, including handling the
?spurious? ambiguity of CCG; fitting well
with the theory of CCG; and optimizing
for structures which are evaluated at test
time. We develop a novel training tech-
nique using a dependency oracle, in which
all derivations are hidden. A challenge
arises from the fact that the oracle needs
to keep track of exponentially many gold-
standard derivations, which is solved by
integrating a packed parse forest with the
beam-search decoder. Standard CCGBank
tests show the model achieves up to 1.05
labeled F-score improvements over three
existing, competitive CCG parsing models.
1 Introduction
Combinatory Categorial Grammar (CCG; Steed-
man (2000)) is able to derive typed dependency
structures (Hockenmaier, 2003; Clark and Curran,
2007), providing a useful approximation to the un-
derlying predicate-argument relations of ?who did
what to whom?. To date, CCG remains the most
competitive formalism for recovering ?deep? de-
pendencies arising from many linguistic phenom-
ena such as raising, control, extraction and coordi-
nation (Rimell et al, 2009; Nivre et al, 2010).
To achieve its expressiveness, CCG exhibits
so-called ?spurious? ambiguity, permitting many
non-standard surface derivations which ease the
recovery of certain dependencies, especially those
arising from type-raising and composition. But
this raises the question of what is the most suit-
able model for CCG: should we model the deriva-
tions, the dependencies, or both? The choice for
some existing parsers (Hockenmaier, 2003; Clark
and Curran, 2007) is to model derivations directly,
restricting the gold-standard to be the normal-form
derivations (Eisner, 1996) from CCGBank (Hock-
enmaier and Steedman, 2007).
Modelling dependencies, as a proxy for the se-
mantic interpretation, fits well with the theory of
CCG, in which Steedman (2000) argues that the
derivation is merely a ?trace? of the underlying
syntactic process, and that the structure which
is built, and predicated over when applying con-
straints on grammaticality, is the semantic inter-
pretation. The early dependency model of Clark
et al (2002), in which model features were defined
over only dependency structures, was partly moti-
vated by these theoretical observations.
More generally, dependency models are desir-
able for a number of reasons. First, modelling
dependencies provides an elegant solution to the
spurious ambiguity problem (Clark and Curran,
2007). Second, obtaining training data for de-
pendencies is likely to be easier than for syn-
tactic derivations, especially for incomplete data
(Schneider et al, 2013). Clark and Curran (2006)
show how the dependency model from Clark and
Curran (2007) extends naturally to the partial-
training case, and also how to obtain dependency
data cheaply from gold-standard lexical category
sequences alone. And third, it has been argued that
dependencies are an ideal representation for parser
evaluation, especially for CCG (Briscoe and Car-
roll, 2006; Clark and Hockenmaier, 2002), and so
optimizing for dependency recovery makes sense
from an evaluation perspective.
In this paper, we fill a gap in the literature by
developing the first dependency model for a shift-
reduce CCG parser. Shift-reduce parsing applies
naturally to CCG (Zhang and Clark, 2011), and the
left-to-right, incremental nature of the decoding
fits with CCG?s cognitive claims. The discrimina-
tive model is global and trained with the structured
perceptron. The decoder is based on beam-search
218
(Zhang and Clark, 2008) with the advantage of
linear-time decoding (Goldberg et al, 2013).
A main contribution of the paper is a novel tech-
nique for training the parser using a dependency
oracle, in which all derivations are hidden. A
challenge arises from the potentially exponential
number of derivations leading to a gold-standard
dependency structure, which the oracle needs to
keep track of. Our solution is an integration of
a packed parse forest, which efficiently stores all
the derivations, with the beam-search decoder at
training time. The derivations are not explicitly
part of the data, since the forest is built from the
gold-standard dependencies. We also show how
perceptron learning with beam-search (Collins and
Roark, 2004) can be extended to handle the ad-
ditional ambiguity, by adapting the ?violation-
fixing? perceptron of Huang et al (2012).
Results on the standard CCGBank tests show
that our parser achieves absolute labeled F-score
gains of up to 0.5 over the shift-reduce parser of
Zhang and Clark (2011); and up to 1.05 and 0.64
over the normal-form and hybrid models of Clark
and Curran (2007), respectively.
2 Shift-Reduce with Beam-Search
This section describes how shift-reduce tech-
niques can be applied to CCG, following Zhang
and Clark (2011). First we describe the determin-
istic process which a parser would follow when
tracing out a single, correct derivation; then we
describe how a model of normal-form derivations
? or, more accurately, a sequence of shift-reduce
actions leading to a normal-form derivation ?
can be used with beam-search to develop a non-
deterministic parser which selects the highest scor-
ing sequence of actions. Note this section only de-
scribes a normal-form derivation model for shift-
reduce parsing. Section 3 explains how we extend
the approach to dependency models.
The shift-reduce algorithm adapted to CCG is
similar to that of shift-reduce dependency parsing
(Yamada and Matsumoto, 2003; Nivre and Mc-
Donald, 2008; Zhang and Clark, 2008; Huang and
Sagae, 2010). Following Zhang and Clark (2011),
we define each item in the parser as a pair ?s, q?,
where q is a queue of remaining input, consisting
of words and a set of possible lexical categories for
each word (with q
0
being the front word), and s is
the stack that holds subtrees s
0
, s
1
, ... (with s
0
at
the top). Subtrees on the stack are partial deriva-
step stack (s
n
, ..., s
1
, s
0
) queue (q
0
, q
1
, ..., q
m
) action
0 Mr. President visited Paris
1 N/N President visited Paris SHIFT
2 N/N N visited Paris SHIFT
3 N visited Paris REDUCE
4 NP visited Paris UNARY
5 NP (S [dcl]\NP)/NP Paris SHIFT
6 NP (S [dcl]\NP)/NP N SHIFT
7 NP (S [dcl]\NP)/NP NP UNARY
8 NP S [dcl]\NP REDUCE
9 S [dcl] REDUCE
Figure 1: Deterministic example of shift-reduce
CCG parsing (lexical categories omitted on queue).
tions that have been built as part of the shift-reduce
process. SHIFT, REDUCE and UNARY are the three
types of actions that can be applied to an item. A
SHIFT action shifts one of the lexical categories
of q
0
onto the stack. A REDUCE action combines
s
0
and s
1
according to a CCG combinatory rule,
producing a new category on the top of the stack.
A UNARY action applies either a type-raising or
type-changing rule to the stack-top category s
0
.
1
Figure 1 shows a deterministic example for the
sentence Mr. President visited Paris, giving a sin-
gle sequence of shift-reduce actions which pro-
duces a correct derivation (i.e. one producing the
correct set of dependencies). Starting with the ini-
tial item ?s, q?
0
(row 0), which has an empty stack
and a full queue, a total of nine actions are applied
to produce the complete derivation.
Applying beam-search to a statistical shift-
reduce parser is a straightforward extension to the
deterministic example. At each step, a beam is
used to store the top-k highest-scoring items, re-
sulting from expanding all items in the previous
beam. An item becomes a candidate output once it
has an empty queue, and the parser keeps track of
the highest scored candidate output and returns the
best one as the final output. Compared with greedy
local-search (Nivre and Scholz, 2004), the use of
a beam allows the parser to explore a larger search
space and delay difficult ambiguity-resolving de-
cisions by considering multiple items in parallel.
We refer to the shift-reduce model of Zhang and
Clark (2011) as the normal-form model, where
the oracle for each sentence specifies a unique se-
quence of gold-standard actions which produces
the corresponding normal-form derivation. No de-
pendency structures are involved at training and
test time, except for evaluation. In the next sec-
tion, we describe a dependency oracle which con-
siders all sequences of actions producing a gold-
standard dependency structure to be correct.
1
See Hockenmaier (2003) and Clark and Curran (2007)
for a description of CCG rules.
219
Mr. President visited Paris
N /N N (S [dcl ]\NP)/NP NP
> >
N S [dcl ]\NP
>TC
NP
<
S [dcl ]
(a)
Mr. President visited Paris
N /N N (S [dcl ]\NP)/NP NP
>
N
>TC
NP
>T
S [dcl ]/(S [dcl ]\NP)
>B
S [dcl ]/NP
>
S [dcl ]
(b)
Figure 2: Two derivations leading to the same dependency structure. TC denotes type-changing.
3 The Dependency Model
Categories in CCG are either basic (such as NP
and PP ) or complex (such as (S [dcl ]\NP)/NP ).
Each complex category in the lexicon defines one
or more predicate-argument relations, which can
be realized as a predicate-argument dependency
when the corresponding argument slot is con-
sumed. For example, the transitive verb category
above defines two relations: one for the subject
NP and one for the object NP . In this paper a
CCG predicate-argument dependency is a 4-tuple:
?h
f
, f, s, h
a
? where h
f
is the lexical item of the
lexical category expressing the relation; f is the
lexical category; s is the argument slot; and h
a
is
the head word of the argument. Since the lexical
items in a dependency are indexed by their sen-
tence positions, all dependencies for a sentence
form a set, which is referred to as a CCG depen-
dency structure. Clark and Curran (2007) contains
a detailed description of dependency structures.
Fig. 2 shows an example demonstrating spu-
rious ambiguity in relation to a CCG depen-
dency structure. In both derivations, the first
two lexical categories are combined using for-
ward application (>) and the following depen-
dency is realized: ?Mr.,N /N
1
, 1,President?. In
the normal-form derivation (a), the dependency
?visited, (S\NP
1
)/NP
2
, 2,Paris? is created by com-
bining the transitive verb category with the ob-
ject NP using forward application. One final de-
pendency, ?visited, (S\NP
1
)/NP
2
, 1,President?, is re-
alized when the root node S [dcl ] is produced
through backward application (<).
Fig. 2(b) shows a non-normal-form derivation
which uses type-raising (T) and composition (B)
(which are not required to derive the correct de-
pendency structure). In this alternative derivation,
the dependency ?visited, (S\NP
1
)/NP
2
, 1,President?
is realized using forward composition (B), and
?visited, (S\NP
1
)/NP
2
, 2,Paris? is realized when the
S [dcl ] root is produced.
The chart-based dependency model of Clark
and Curran (2007) treats all derivations as hid-
den, and defines a probabilistic model for a de-
pendency structure by summing probabilities of
all derivations leading to a particular structure.
Features are defined over both derivations and
CCG predicate-argument dependencies. We fol-
low a similar approach, but rather than define
a probabilistic model (which requires summing),
we define a linear model over sequences of shift-
reduce actions, as for the normal-form shift-reduce
model. However, the difference compared to the
normal-form model is that we do not assume a sin-
gle gold-standard sequence of actions.
Similar to Goldberg and Nivre (2012), we de-
fine an oracle which determines, for a gold-
standard dependency structure, G, what the valid
transition sequences are (i.e. those sequences cor-
responding to derivations leading to G). More
specifically, the oracle can determine, givenG and
an item ?s, q?, what the valid actions are for that
item (i.e. what actions can potentially lead to G,
starting with ?s, q? and the dependencies already
built on s). However, there can be exponentially
many valid action sequences for G, which we rep-
resent efficiently using a packed parse forest. We
show how the forest can be used, during beam-
search decoding, to determine the valid actions
for a parse item (Section 3.2). We also show, in
Section 3.3, how perceptron training with early-
update (Collins and Roark, 2004) can be used in
this setting.
3.1 The Oracle Forest
A CCG parse forest efficiently represents an
exponential number of derivations. Following
Clark and Curran (2007) (which builds on Miyao
and Tsujii (2002)), and using the same nota-
tion, we define a CCG parse forest ? as a tuple
?C,D,R, ?, ??, where C is a set of conjunctive
220
Algorithm 1 (Clark and Curran, 2007)
Input: A packed forest ?C,D,R, ?, ??, with dmax(c)
and dmax(d) already computed
1: function MAIN
2: for each d
r
? R s.t. dmax
.
(d
r
) = |G| do
3: MARK(d
r
)
4: procedure MARK(d)
5: mark d as a correct node
6: for each c ? ?(d) do
7: if dmax(c) == dmax(d) then
8: mark c as a correct node
9: for each d
?
? ?(c) do
10: if d
?
has not been visited then
11: MARK(d
?
)
nodes and D is a set of disjunctive nodes.
2
Con-
junctive nodes are individual CCG categories in ?,
and are either obtained from the lexicon, or by
combining two disjunctive nodes using a CCG rule,
or by applying a unary rule to a disjunctive node.
Disjunctive nodes are equivalence classes of con-
junctive nodes. Two conjunctive nodes are equiv-
alent iff they have the same category, head and un-
filled dependencies (i.e. they will lead to the same
derivation, and produce the same dependencies, in
any future parsing). R ? D is a set of root dis-
junctive nodes. ? : D ? 2
C
is the conjunctive
child function and ? : C ? 2
D
is the disjunctive
child function. The former returns the set of all
conjunctive nodes of a disjunctive node, and the
latter returns the disjunctive child nodes of a con-
junctive node.
The dependency model requires all the conjunc-
tive and disjunctive nodes of ? that are part of the
derivations leading to a gold-standard dependency
structure G. We refer to such derivations as cor-
rect derivations and the packed forest containing
all these derivations as the oracle forest, denoted
as ?
G
, which is a subset of ?. It is prohibitive to
enumerate all correct derivations, but it is possible
to identify, from ?, all the conjunctive and dis-
junctive nodes that are part of ?
G
. Clark and Cur-
ran (2007) gives an algorithm for doing so, which
we use here. The main intuition behind the algo-
rithm is that a gold-standard dependency structure
decomposes over derivations; thus gold-standard
dependencies realized at conjunctive nodes can be
counted when ? is built, and all nodes that are part
of ?
G
can then be marked out of ? by traversing
it top-down. A key idea in understanding the algo-
2
Under the hypergraph framework (Gallo et al, 1993;
Huang and Chiang, 2005), a conjunctive node corresponds to
a hyperedge and a disjunctive node corresponds to the head
of a hyperedge or hyperedge bundle.
rithm is that dependencies are created when dis-
junctive nodes are combined, and hence are asso-
ciated with, or ?live on?, conjunctive nodes in the
forest.
Following Clark and Curran (2007), we also
define the following three values, where the first
decomposes only over local rule productions,
while the other two decompose over derivations:
cdeps(c) =
{
? if ? ? ? deps(c), ? /? G
|deps(c)| otherwise
dmax(c) =
?
??
??
? if cdeps(c) == ?
? if dmax(d) == ? for some d ? ?(c)
?
d??(c)
dmax(d) + cdeps(c) otherwise
dmax(d) = max{dmax(c) | c ? ?(d)}
deps(c) is the set of all dependencies on con-
junctive node c, and cdeps(c) counts the number
of correct dependencies on c. dmax(c) is the max-
imum number of correct dependencies over any
sub-derivation headed by c and is calculated re-
cursively; dmax(d) returns the same value for a
disjunctive node. In all cases, a special value ?
indicates the presence of incorrect dependencies.
To obtain the oracle forest, we first pre-compute
dmax(c) and dmax(d) for all d and c in ? when ?
is built using CKY, which are then used by Algo-
rithm 1 to identify all the conjunctive and disjunc-
tive nodes in ?
G
.
3.2 The Dependency Oracle Algorithm
We observe that the canonical shift-reduce algo-
rithm (as demonstrated in Fig. 1) applied to a sin-
gle parse tree exactly resembles bottom-up post-
order traversal of that tree. As an example, con-
sider the derivation in Fig. 2a, where the corre-
sponding sequence of actions is: sh N /N , sh N ,
re N , un NP , sh (S [dcl ]\NP)/NP , sh NP ,
re S [dcl ]\NP , re S [dcl ].
3
The order of traversal
is left-child, right-child and parent. For a single
parse, the corresponding shift-reduce action se-
quence is unique, and for a given item this canoni-
cal order restricts the possible derivations that can
be formed using further actions. We now extend
this observation to the more general case of an
oracle forest, where there may be more than one
gold-standard action for a given item.
Definition 1. Given a gold-standard dependency
3
The derivation is ?upside down?, following the conven-
tion used for CCG, where the root is S [dcl ]. We use sh, re
and un to denote the three types of shift-reduce action.
221
Mr. President visited Paris
N /N N (S [dcl ]\NP)/NP NP
> >
N S[dcl]\NP
(a)
Mr. President visited Paris
N/N N (S [dcl ]\NP)/NP NP
>
S[dcl]\NP
(b)
Figure 3: Example subtrees on two stacks, with two subtrees in (a) and three in (b); roots of subtrees are
in bold.
structure G, an oracle forest ?
G
, and an item
?s, q?, we say s is a realization of G, denoted
s ' G, if |s| = 1, q is empty and the single deriva-
tion on s is correct. If |s| > 0 and the subtrees on
s can lead to a correct derivation in ?
G
using fur-
ther actions, we say s is a partial-realization of
G, denoted as s ? G. And we define s ? G for
|s| = 0.
As an example, assume that ?
G
contains only
the derivation in Fig. 2a; then a stack containing
the two subtrees in Fig. 3a is a partial-realization,
while a stack containing the three subtrees in
Fig. 3b is not. Note that each of the three sub-
trees in Fig. 3b is present in ?
G
; however, these
subtrees cannot be combined into the single cor-
rect derivation, since the correct sequence of shift-
reduce actions must first combine the lexical cat-
egories for Mr. and President before shifting the
lexical category for visited.
We denote an action as a pair (x, c), where
x ? {SHIFT, REDUCE, UNARY} and c is the root
of the subtree resulting from that action. For all
three types of actions, c also corresponds to a
unique conjunctive node in the complete forest ?;
and we use c
s
i
to denote the conjunctive node in
? corresponding to subtree s
i
on the stack. Let
?s
?
, q
?
? = ?s, q? ? (x, c) be the resulting item from
applying the action (x, c) to ?s, q?; and let the
set of all possible actions for ?s, q? be X
?s,q?
=
{(x, c) | (x, c) is applicable to ?s, q?}.
Definition 2. Given ?
G
and an item ?s, q? s.t. s ?
G, we say an applicable action (x, c) for the item
is valid iff s
?
? G or s
?
' G, where ?s
?
, q
?
? =
?s, q? ? (x, c).
Definition 3. Given ?
G
, the dependency oracle
function f
d
is defined as:
f
d
(?s, q?, (x, c),?
G
) =
{
true if s
?
? G or s
?
' G
false otherwise
where (x, c) ? X
?s,q?
and ?s
?
, q
?
? = ?s, q? ? (x, c).
The pseudocode in Algorithm 2 implements f
d
.
It determines, for a given item, whether an appli-
cable action is valid in ?
G
.
It is trivial to determine the validity of a SHIFT
action for the initial item, ?s, q?
0
, since the SHIFT
action is valid iff its category matches the gold-
standard lexical category of the first word in
the sentence. For any subsequent SHIFT action
(SHIFT, c) to be valid, the necessary condition is
c ? c
lex
0
, where c
lex
0
denotes the gold-standard
lexical category of the front word in the queue, q
0
(line 3). However, this condition is not sufficient;
a counterexample is the case where all the gold-
standard lexical categories for the sentence in Fig-
ure 2 are shifted in succession. Hence, in general,
the conditions under which an action is valid are
more complex than the trivial case above.
First, suppose there is only one correct deriva-
tion in ?
G
. A SHIFT action (SHIFT, c
lex
0
) is valid
whenever c
s
0
(the conjunctive node in ?
G
cor-
responding to the subtree s
0
on the stack) and
c
lex
0
(the conjunctive node in ?
G
corresponding
to the next gold-standard lexical category from
the queue) are both dominated by the conjunctive
node parent p of c
s
0
in ?
G
.
4
A REDUCE action
(REDUCE, c) is valid if c matches the category of
the conjunctive node parent of c
s
0
and c
s
1
in ?
G
.
A UNARY action (UNARY, c) is valid if c matches
the conjunctive node parent of c
s
0
in ?
G
. We now
generalize the case where ?
G
contains a single
correct parse to the case of an oracle forest, where
each parent p is replaced by a set of conjunctive
nodes in ?
G
.
Definition 4. The left parent set p
L
(c) of con-
junctive node c ? ?
G
is the set of all parent con-
junctive nodes of c in ?
G
, which have the disjunc-
tive node d containing c (i.e. c ? ?(d)) as a left
child.
Definition 5. The ancestor set A(c) of conjunc-
tive node c ? ?
G
is the set of all reachable ances-
tor conjunctive nodes of c in ?
G
.
Definition 6. Given an item ?s, q?, if |s| = 1 we
say s is a frontier stack.
4
Strictly speaking, the conjunctive node parent is a parent
of the disjunctive node containing the conjunctive node c
s
0
.
We will continue to use this shorthand for parents of conjunc-
tive nodes throughout the paper.
222
Algorithm 2 The Dependency Oracle Function f
d
Input: ?
G
, an item ?s, q? s.t. s ? G, (x, c) ? X
?s,q?
Let s
?
be the stack of ?s
?
, q
?
? = ?s, q? ? (x, c)
1: function MAIN(?s, q?, (x, c), ?
G
)
2: if x is SHIFT then
3: if c 6? c
lex
0
then . c not gold lexical category
4: return false
5: else if c ? c
lex
0
and |s| = 0 then . the initial item
6: return true
7: else if c ? c
lex
0
and |s| 6= 0 then
8: computeR(c
s
?
1
, c
s
?
0
)
9: returnR(c
s
?
1
, c
s
?
0
) 6= ?
10: if x is REDUCE then . s is non-frontier
11: if c ? R(c
s
1
, c
s
0
) then
12: computeR(c
s
?
1
, c
s
?
0
)
13: return true
14: else return false
15: if x is UNARY then
16: if |s| = 1 then . s is frontier
17: return c ? ?
G
18: if |s| 6= 1 and c ? ?
G
then . s is non-frontier
19: computeR(c
s
?
1
, c
s
?
0
)
20: returnR(c
s
?
1
, c
s
?
0
) 6= ?
A key to defining the dependency oracle func-
tion is the notion of a shared ancestor set. In-
tuitively, shared ancestor sets are built up through
shift actions, and contain sets of nodes which can
potentially become the results of reduce or unary
actions. A further intuition is that shared ances-
tor sets define the space of possible correct deriva-
tions, and nodes in these sets are ?ticked off? when
reduce and unary actions are applied, as a single
correct derivation is built through the shift-reduce
process (corresponding to a bottom-up post-order
traversal of the derivation). The following defi-
nition shows how the dependency oracle function
builds shared ancestor sets for each action type.
Definition 7. Let ?s, q? be an item and let
?s
?
, q
?
? = ?s, q? ? (x, c). We define the shared an-
cestor setR(c
s
?
1
, c
s
?
0
) of c
s
?
0
, after applying action
(x, c), as:
? {c
?
| c
?
? p
L
(c
s
0
) ? A(c)}, if s is frontier and x =
SHIFT
? {c
?
| c
?
? p
L
(c
s
0
) ? A(c) and there is some c
??
?
R(c
s
1
, c
s
0
) s.t. c
??
? A(c
?
)}, if s is non-frontier and
x = SHIFT
? {c
?
| c
?
? R(c
s
2
, c
s
1
) ? A(c)}, if x = REDUCE
? {c
?
| c
?
? R(c
s
1
, c
s
0
) ? A(c)}, if s is non-frontier
and x = UNARY
? R(, c
0
s
0
) = ? where c
0
s
0
is the conjunctive node cor-
responding to the gold-standard lexical category of the
first word in the sentence ( is a dummy symbol indi-
cating the bottom of stack).
The base case for Definition 7 is when the gold-
standard lexical category of the first word in the
sentence has been shifted, which creates an empty
shared ancestor set. Furthermore, the shared an-
cestor set is always empty when the stack is a fron-
tier stack.
The dependency oracle algorithm checks the va-
lidity of applicable actions. A SHIFT action is
valid if R(c
s
?
1
, c
s
?
0
) 6= ? for the resulting stack
s
?
. A valid REDUCE action consumes s
1
and
s
0
. For the new node, its shared ancestor set is
the subset of the conjunctive nodes in R(c
s
2
, c
s
1
)
which dominate the resulting conjunctive node of
a valid REDUCE action. The UNARY case for a
frontier stack is trivial: any UNARY action ap-
plicable to s in ?
G
is valid. For a non-frontier
stack, the UNARY case is similar to REDUCE ex-
cept the resulting shared ancestor set is a subset of
R(c
s
1
, c
s
0
).
We now turn to the problem of finding the
shared ancestor sets. In practice, we do not do this
by traversing ?
G
top-down from the conjunctive
nodes in p
L
(c
s
0
) on-the-fly to find each member of
R. Instead, when we build ?
G
in bottom-up topo-
logical order, we pre-compute the set of reachable
disjunctive nodes of each conjunctive node c in
?
G
as:
D(c) = ?(c) ? (?
c
?
??(d),d??(c)
(D(c
?
)))
Each D is implemented as a hash map, which
allows us to test the membership of one potential
conjunctive node in O(1) time. For example, a
conjunctive node c ? p
L
(c
s
0
) is reachable from
c
lex
0
if there is a disjunctive node d ? D(c) s.t.
c
lex
0
? ?(d). With this implementation, the com-
plexity of checking each valid SHIFT action is then
O(|p
L
(c
s
0
)|).
3.3 Training
We use the averaged perceptron (Collins, 2002)
to train a global linear model and score each ac-
tion. The normal-form model of Zhang and Clark
(2011) uses an early update mechanism (Collins
and Roark, 2004), where decoding is stopped to
update model weights whenever the single gold
action falls outside the beam. In our parser, there
can be multiple gold items in a beam. One option
would be to apply early update whenever at least
223
Algorithm 3 Dependency Model Training
Input: (y,G) and beam size k
1: w? 0; B
0
? ?; i? 0
2: B
0
.push(?s, q?
0
) . the initial item
3: cand? ? . candidate output priority queue
4: gold? ? . gold output priority queue
5: while B
i
6= ? do
6: for each ?s, q? ? B
i
do
7: if |q| = 0 then . candidate output
8: cand.push(?s, q?)
9: if s ' G then . s is a realization of G
10: gold.push(?s, q?)
11: expand ?s, q? into B
i+1
12: B
i+1
? B
i+1
[1 : k] . apply beam
13: if ?
G
6= ?, ?
G
? B
i+1
= ? and cand[0] 6' G then
14: w? w + ?(?
G
[0])? ?(B
i+1
[0]) . early update
15: return
16: i? i+ 1 . continue to next step
17: if cand[0] 6' G then . final update
18: w? w + ?(gold[0])? ?(cand[0])
one of these gold items falls outside the beam.
However, this may not be a true violation of the
gold-standard (Huang et al, 2012). Thus, we use a
relaxed version of early update, in which all gold-
standard actions must fall outside the beam before
an update is performed. This update mechanism is
provably correct under the violation-fixing frame-
work of Huang et al (2012).
Let (y,G) be a training sentence paired with its
gold-standard dependency structure and let ?
?s,q?
be the following set for an item ?s, q?:
{?s, q? ? (x, c) | f
d
(?s, q?, (x, c),?
G
) = true}
?
?s,q?
contains all correct items at step i + 1 ob-
tained by expanding ?s, q?. Let the set of all cor-
rect items at a step i+ 1 be:
5
?
G
=
?
?s,q??B
i
?
?s,q?
Algorithm 3 shows the pseudocode for training
the dependency model with early update for one
input (y,G). The score of an item ?s, q? is calcu-
lated as w ? ?(?s, q?) with respect to the current
model w, where ?(?s, q?) is the feature vector for
the item. At step i, all items are expanded and
added onto the next beam B
i+1
, and the top-k re-
tained. Early update is applied when all gold items
first fall outside the beam, and any candidate out-
put is incorrect (line 14). Since there are poten-
tially many gold items, and one gold item is re-
quired for the perceptron update, a decision needs
5
In Algorithm 3 we abuse notation by using ?
G
[0] to de-
note the highest scoring gold item in the set.
to be made regarding which gold item to update
against. We choose to reward the highest scoring
gold item, in line with the violation-fixing frame-
work; and penalize the highest scoring incorrect
item, using the standard perceptron update. A fi-
nal update is performed if no more expansions are
possible but the final output is incorrect.
4 Experiments
We implement our shift-reduce parser on top of the
core C&C code base (Clark and Curran, 2007) and
evaluate it against the shift-reduce parser of Zhang
and Clark (2011) (henceforth Z&C) and the chart-
based normal-form and hybrid models of Clark
and Curran (2007). For all experiments, we use
CCGBank with the standard split: sections 2-21
for training (39,604 sentences), section 00 for de-
velopment (1,913 sentences) and section 23 (2,407
sentences) for testing.
The way that the CCG grammar is implemented
in C&C has some implications for our parser.
First, unlike Z&C, which uses a context-free cover
(Fowler and Penn, 2010) and hence is able to use
all sentences in the training data, we are only able
to use 36,036 sentences. The reason is that the
grammar in C&C does not have complete cover-
age of CCGBank, due to the fact that e.g. not
all rules in CCGBank conform to the combinatory
rules of CCG. Second, our parser uses the unifica-
tion mechanism from C&C to output dependencies
directly, and hence does not need a separate post-
processing step to convert derivations into CCG de-
pendencies, as required by Z&C.
The feature templates of our model consist of
all of those in Z&C, except the ones which re-
quire lexical heads to come from either the left or
right child, as such features are incompatible with
the head passing mechanism used by C&C. Each
Z&C template is defined over a parse item, and
captures various aspects of the stack and queue
context. For example, one template returns the
top category on the stack plus its head word, to-
gether with the first word and its POS tag on the
queue. Another template returns the second cat-
egory on the stack, together with the POS tag of
its head word. Every Z&C feature is defined as
a pair, consisting of an instantiated context tem-
plate and a parse action. In addition, we use all
the CCG predicate-argument dependency features
from Clark and Curran (2007), which contribute to
the score of a REDUCE action when dependencies
224
LP % LR % LF % LSent. % CatAcc. % coverage %
this parser 86.29 84.09 85.18 34.40 92.75 100
Z&C 87.15 82.95 85.00 33.82 92.77 100
C&C (normal-form) 85.22 82.52 83.85 31.63 92.40 100
this parser 86.76 84.90 85.82 34.72 93.20 99.06 (C&C coverage)
Z&C 87.55 83.63 85.54 34.14 93.11 99.06 (C&C coverage)
C&C (hybrid) ? ? 85.25 ? ? 99.06 (C&C coverage)
C&C (normal-form) 85.22 84.29 84.76 31.93 92.83 99.06 (C&C coverage)
Table 1: Accuracy comparison on Section 00 (auto POS).
 60
 65
 70
 75
 80
 85
 90
 0  5  10  15  20  25  30
Pre
cisi
on %
Dependency length (bins of 5)
C&CZ&Cthis parser
(a) precision vs. dependency length
 50
 55
 60
 65
 70
 75
 80
 85
 90
 0  5  10  15  20  25  30
Rec
all %
Dependency length (bins of 5)
C&CZ&Cthis parser
(b) recall vs. dependency length
Figure 4: Labeled precision and recall relative to dependency length on the development set. C&C
normal-form model is used.
are realized. Detailed descriptions of all the tem-
plates in our model can be found in the respective
papers. We run 20 training iterations and the re-
sulting model contains 16.5M features with a non-
zero weight.
We use 10-fold cross validation for POS tagging
and supertagging the training data, and automat-
ically assigned POS tags for all experiments. A
probability cut-off value of 0.0001 for the ? pa-
rameter in the supertagger is used for both train-
ing and testing. The ? parameter determines how
many lexical categories are assigned to each word;
? = 0.0001 is a relatively small value which al-
lows in a large number of categories, compared to
the default value used in Clark and Curran (2007).
For training only, if the gold-standard lexical cat-
egory is not supplied by the supertagger for a par-
ticular word, it is added to the list of categories.
4.1 Results and Analysis
The beam size was tuned on the development set,
and a value of 128 was found to achieve a rea-
sonable balance of accuracy and speed; hence this
value was used for all experiments. Since C&C al-
ways enforces non-fragmentary output (i.e. it can
only produce spanning analyses), it fails on some
sentences in the development and test sets, and
thus we also evaluate on the reduced sets, follow-
ing Clark and Curran (2007). Our parser does not
fail on any sentences because it permits fragmen-
tary output (those cases where there is more than
one subtree left on the final stack). The results for
Z&C, and the C&C normal-form and hybrid mod-
els, are taken from Zhang and Clark (2011).
Table 1 shows the accuracies of all parsers on
the development set, in terms of labeled precision
and recall over the predicate-argument dependen-
cies in CCGBank. On both the full and reduced
sets, our parser achieves the highest F-score. In
comparison with C&C, our parser shows signif-
icant increases across all metrics, with 0.57%
and 1.06% absolute F-score improvements over
the hybrid and normal-form models, respectively.
Another major improvement over the other two
parsers is in sentence level accuracy, LSent, which
measures the number of sentences for which the
dependency structure is completely correct.
Table 1 also shows that our parser has improved
recall over Z&C at some expense of precision. To
probe this further we compare labeled precision
and recall relative to dependency length, as mea-
sured by the distance between the two words in a
dependency, grouped into bins of 5 values. Fig. 4
shows clearly that Z&C favors precision over re-
call, giving higher precision scores for almost all
dependency lengths compared to our parser. In
225
category LP % (o) LP % (z) LP % (c) LR % (o) LR % (z) LR % (c) LF % (o) LF % (z) LF % (c) freq.
N /N 95.53 95.77 95.28 95.83 95.79 95.62 95.68 95.78 95.45 7288
NP/N 96.53 96.70 96.57 97.12 96.59 96.03 96.83 96.65 96.30 4101
(NP\NP)/NP 81.64 83.19 82.17 90.63 89.24 88.90 85.90 86.11 85.40 2379
(NP\NP)/NP 81.70 82.53 81.58 88.91 87.99 85.74 85.15 85.17 83.61 2174
((S\NP)\(S\NP))/NP 77.64 77.60 71.94 72.97 71.58 73.32 75.24 74.47 72.63 1147
((S\NP)\(S\NP))/NP 75.78 76.30 70.92 71.27 70.60 71.93 73.45 73.34 71.42 1058
((S [dcl ]\NP)/NP 83.94 85.60 81.57 86.04 84.30 86.37 84.98 84.95 83.90 917
PP/NP 77.06 73.76 75.06 73.63 72.83 70.09 75.31 73.29 72.49 876
((S [dcl ]\NP)/NP 82.03 85.32 81.62 83.26 82.00 85.55 82.64 83.63 83.54 872
((S\NP)\(S\NP)) 86.42 84.44 86.85 86.19 86.60 86.73 86.31 85.51 86.79 746
Table 2: Accuracy comparison on most frequent dependency types, for our parser (o), Z&C (z) and C&C
hybrid model (c). Categories in bold indicate the argument slot in the relation.
LP % LR % LF % LSent. % CatAcc. % coverage %
our parser 87.03 85.08 86.04 35.69 93.10 100
Z&C 87.43 83.61 85.48 35.19 93.12 100
C&C (normal-form) 85.58 82.85 84.20 32.90 92.84 100
our parser 87.04 85.16 86.09 35.84 93.13 99.58 (C&C coverage)
Z&C 87.43 83.71 85.53 35.34 93.15 99.58 (C&C coverage)
C&C (hybrid) 86.17 84.74 85.45 32.92 92.98 99.58 (C&C coverage)
C&C (normal-form) 85.48 84.60 85.04 33.08 92.86 99.58 (C&C coverage)
Table 3: Accuracy comparison on section 23 (auto POS).
terms of recall (Fig. 4b), our parser outperforms
Z&C over all dependency lengths, especially for
longer dependencies (x ? 20). When compared
with C&C, the recall of the Z&C parser drops
quickly for dependency lengths over 10. While
our parser also suffers from this problem, it is
less severe and is able to achieve higher recall at
x ? 30.
Table 2 compares our parser with Z&C and the
C&C hybrid model, for the most frequent depen-
dency relations. While our parser achieved lower
precision than Z&C, it is more balanced and gives
higher recall for all of the dependency relations ex-
cept the last one, and higher F-score for over half
of them.
Table 3 presents the final test results on Section
23. Again, our parser achieves the highest scores
across all metrics (for both the full and reduced
test sets), except for precision and lexical category
assignment, where Z&C performed better.
5 Conclusion
We have presented a dependency model for a shift-
reduce CCG parser, which fully aligns CCG parsing
with the left-to-right, incremental nature of a shift-
reduce parser. Our work is in part inspired by the
dependency models of Clark and Curran (2007)
and, in the use of a dependency oracle, is close
in spirit to that of Goldberg and Nivre (2012). The
difference is that the Goldberg and Nivre parser
builds, and scores, dependency structures directly,
whereas our parser uses a unification mechanism
to create dependencies, and scores the CCG deriva-
tions, allowing great flexibility in terms of what
dependencies can be realized. Another related
work is Yu et al (2013), which introduced a sim-
ilar technique to deal with spurious ambiguity in
MT. Finally, there may be potential to integrate the
techniques of Auli and Lopez (2011), which cur-
rently represents the state-of-the-art in CCGBank
parsing, into our parser.
Acknowledgements
We thank the anonymous reviewers for their help-
ful comments. Wenduan Xu is fully supported by
the Carnegie Trust and receives additional fund-
ing from the Cambridge Trusts. Stephen Clark
is supported by ERC Starting Grant DisCoTex
(306920) and EPSRC grant EP/I037512/1. Yue
Zhang is supported by Singapore MOE Tier2 grant
T2MOE201301.
References
Michael Auli and Adam Lopez. 2011. A compari-
son of loopy belief propagation and dual decompo-
sition for integrated CCG supertagging and parsing.
In Proc. ACL 2011, pages 470?480, Portland, OR.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on the
226
PARC DepBank. In Proc. of COLING/ACL, pages
41?48, Sydney, Australia.
Stephen Clark and James R. Curran. 2006. Partial
training for a lexicalized-grammar parser. In Proc.
NAACL-06, pages 144?151, New York, USA.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark and Julia Hockenmaier. 2002. Evalu-
ating a wide-coverage CCG parser. In Proc. of the
LREC 2002 Beyond Parseval Workshop, pages 60?
66, Las Palmas, Spain.
Stephen Clark, Julia Hockenmaier, and Mark Steed-
man. 2002. Building deep dependency structures
with a wide-coverage CCG parser. In Proc. ACL,
pages 327?334, Philadelphia, PA.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proc. of
ACL, pages 111?118, Barcelona, Spain.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In Proc. of
EMNLP, pages 1?8, Philadelphia, USA.
Jason Eisner. 1996. Efficient normal-form parsing for
Combinatory Categorial Grammar. In Proc. ACL,
pages 79?86, Santa Cruz, CA.
Timothy AD Fowler and Gerald Penn. 2010. Accu-
rate context-free parsing with Combinatory Catego-
rial Grammar. In Proc. ACL, pages 335?344, Upp-
sala, Sweden.
Giorgio Gallo, Giustino Longo, Stefano Pallottino,
and Sang Nguyen. 1993. Directed hypergraphs
and applications. Discrete applied mathematics,
42(2):177?201.
Yoav Goldberg and Joakim Nivre. 2012. A dynamic
oracle for arc-eager dependency parsing. In Proc.
COLING, Mumbai, India.
Yoav Goldberg, Kai Zhao, and Liang Huang. 2013.
Efficient implementation for beam search incremen-
tal parsers. In Proceedings of the Short Papers of
ACL, Sofia, Bulgaria.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Julia Hockenmaier. 2003. Data and Models for Sta-
tistical Parsing with Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
Liang Huang and David Chiang. 2005. Better k-
best parsing. In Proceedings of the Ninth Interna-
tional Workshop on Parsing Technology, pages 53?
64, Vancouver, Canada.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proc. ACL, pages 1077?1086, Uppsala, Sweden.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proc.
NAACL, pages 142?151, Montreal, Canada.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proceed-
ings of the Human Language Technology Confer-
ence, San Diego, CA.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proc. of ACL/HLT, pages 950?958,
Columbus, Ohio.
J. Nivre and M Scholz. 2004. Deterministic depen-
dency parsing of English text. In Proceedings of
COLING 2004, pages 64?70, Geneva, Switzerland.
Joakim Nivre, Laura Rimell, Ryan McDonald, and Car-
los Gomez-Rodriguez. 2010. Evaluation of depen-
dency parsers on unbounded dependencies. In Proc.
of COLING, Beijing, China.
Laura Rimell, Stephen Clark, and Mark Steedman.
2009. Unbounded dependency recovery for parser
evaluation. In Proc. EMNLP, pages 813?821, Edin-
burgh, UK.
Nathan Schneider, Brendan O?Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A. Smith,
Chris Dyer, and Jason Baldridge. 2013. A frame-
work for (under)specifying dependency syntax with-
out overloading annotators. In Proc. of the 7th Lin-
guistic Annotation Workshop and Interoperability
with Discourse, Sofia, Bulgaria.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, Mass.
H Yamada and Y Matsumoto. 2003. Statistical depen-
dency analysis using support vector machines. In
Proc. of IWPT, Nancy, France.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable mt training. In Proc. EMNLP, Seat-
tle, Washington, USA.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proc. of EMNLP, Hawaii, USA.
Yue Zhang and Stephen Clark. 2011. Shift-reduce
CCG parsing. In Proc. ACL 2011, pages 683?692,
Portland, OR.
227
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1326?1336,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Character-Level Chinese Dependency Parsing
Meishan Zhang
?
, Yue Zhang
?
, Wanxiang Che
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{mszhang, car, tliu}@ir.hit.edu.cn
?
Singapore University of Technology and Design
yue zhang@sutd.edu.sg
Abstract
Recent work on Chinese analysis has led
to large-scale annotations of the internal
structures of words, enabling character-
level analysis of Chinese syntactic struc-
tures. In this paper, we investigate the
problem of character-level Chinese depen-
dency parsing, building dependency trees
over characters. Character-level infor-
mation can benefit downstream applica-
tions by offering flexible granularities for
word segmentation while improving word-
level dependency parsing accuracies. We
present novel adaptations of two ma-
jor shift-reduce dependency parsing algo-
rithms to character-level parsing. Exper-
imental results on the Chinese Treebank
demonstrate improved performances over
word-based parsing methods.
1 Introduction
As a light-weight formalism offering syntactic
information to downstream applications such as
SMT, the dependency grammar has received in-
creasing interest in the syntax parsing commu-
nity (McDonald et al, 2005; Nivre and Nilsson,
2005; Carreras et al, 2006; Duan et al, 2007; Koo
and Collins, 2010; Zhang and Clark, 2008; Nivre,
2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi
and McCallum, 2013). Chinese dependency trees
were conventionally defined over words (Chang et
al., 2009; Li et al, 2012), requiring word segmen-
tation and POS-tagging as pre-processing steps.
Recent work on Chinese analysis has embarked
on investigating the syntactic roles of characters,
leading to large-scale annotations of word internal
structures (Li, 2011; Zhang et al, 2013). Such an-
notations enable dependency parsing on the char-
acter level, building dependency trees over Chi-
nese characters. Figure 1(c) shows an example of
?
Corresponding author.
??? ??? ? ? ??
forestry administration deputy director meeting in make a speech
(a) a word-based dependency tree
? ? ? ? ? ? ? ? ? ?
woods industry office deputy office manager meeting in make speech
(b) a character-level dependency tree by Zhao (2009) with
real intra-word and pseudo inter-word dependencies
? ? ? ? ? ? ? ? ? ?
woods industry office deputy office manager meeting in make speech
(c) a character-level dependency tree investigated in this pa-
per with both real intra- and inter-word dependencies
Figure 1: An example character-level dependency
tree. ????????????? (The deputy
director of forestry administration make a speech
in the meeting)?.
a character-level dependency tree, where the leaf
nodes are Chinese characters.
Character-level dependency parsing is interest-
ing in at least two aspects. First, character-level
trees circumvent the issue that no universal stan-
dard exists for Chinese word segmentation. In the
well-known Chinese word segmentation bakeoff
tasks, for example, different segmentation stan-
dards have been used by different data sets (Emer-
son, 2005). On the other hand, most disagreement
on segmentation standards boils down to disagree-
ment on segmentation granularity. As demon-
strated by Zhao (2009), one can extract both fine-
grained and coarse-grained words from character-
level dependency trees, and hence can adapt to
flexible segmentation standards using this formal-
ism. In Figure 1(c), for example, ???? (deputy
1326
director)? can be segmented as both ?? (deputy)
| ?? (director)? and ???? (deputy direc-
tor)?, but not ?? (deputy) ? (office) | ? (man-
ager)?, by dependency coherence. Chinese lan-
guage processing tasks, such as machine transla-
tion, can benefit from flexible segmentation stan-
dards (Zhang et al, 2008; Chang et al, 2008).
Second, word internal structures can also be
useful for syntactic parsing. Zhang et al (2013)
have shown the usefulness of word structures in
Chinese constituent parsing. Their results on the
Chinese Treebank (CTB) showed that character-
level constituent parsing can bring increased per-
formances even with the pseudo word structures.
They further showed that better performances can
be achieved when manually annotated word struc-
tures are used instead of pseudo structures.
In this paper, we make an investigation of
character-level Chinese dependency parsing using
Zhang et al (2013)?s annotations and based on
a transition-based parsing framework (Zhang and
Clark, 2011). There are two dominant transition-
based dependency parsing systems, namely the
arc-standard and the arc-eager parsers (Nivre,
2008). We study both algorithms for character-
level dependency parsing in order to make a com-
prehensive investigation. For direct comparison
with word-based parsers, we incorporate the tra-
ditional word segmentation, POS-tagging and de-
pendency parsing stages in our joint parsing mod-
els. We make changes to the original transition
systems, and arrive at two novel transition-based
character-level parsers.
We conduct experiments on three data sets, in-
cluding CTB 5.0, CTB 6.0 and CTB 7.0. Exper-
imental results show that the character-level de-
pendency parsing models outperform the word-
based methods on all the data sets. Moreover,
manually annotated intra-word dependencies can
give improved word-level dependency accuracies
than pseudo intra-word dependencies. These re-
sults confirm the usefulness of character-level
syntax for Chinese analysis. The source codes
are freely available at http://sourceforge.
net/projects/zpar/, version 0.7.
2 Character-Level Dependency Tree
Character-level dependencies were first proposed
by Zhao (2009). They show that by annotat-
ing character dependencies within words, one can
adapt to different segmentation standards. The
dependencies they study are restricted to intra-
word characters, as illustrated in Figure 1(b). For
inter-word dependencies, they use a pseudo right-
headed representation.
In this study, we integrate inter-word syntactic
dependencies and intra-word dependencies using
large-scale annotations of word internal structures
by Zhang et al (2013), and study their interac-
tions. We extract unlabeled dependencies from
bracketed word structures according to Zhang et
al.?s head annotations. In Figure 1(c), the depen-
dencies shown by dashed arcs are intra-word de-
pendencies, which reflect the internal word struc-
tures, while the dependencies with solid arcs are
inter-word dependencies, which reflect the syntac-
tic structures between words.
In this formulation, a character-level depen-
dency tree satisfies the same constraints as the
traditional word-based dependency tree for Chi-
nese, including projectivity. We differentiate intra-
word dependencies and inter-word dependencies
by the arc type, so that our work can be com-
pared with conventional word segmentation, POS-
tagging and dependency parsing pipelines under a
canonical segmentation standard.
The character-level dependency trees hold to a
specific word segmentation standard, but are not
limited to it. We can extract finer-grained words
of different granulities from a coarse-grained word
by taking projective subtrees of different sizes. For
example, taking all the intra-word modifier nodes
of ?? (manager)? in Figure 1(c) results in the
word ???? (deputy director)?, while taking the
first modifier node of ?? (manager)? results in the
word ??? (director)?. Note that ??? (deputy
office)? cannot be a word because it does not form
a projective span without ?? (manager)?.
Inner-word dependencies can also bring bene-
fits to parsing word-level dependencies. The head
character can be a less sparse feature compared
to a word. As intra-word dependencies lead to
fine-grained subwords, we can also use these sub-
words for better parsing. In this work, we use
the innermost left/right subwords as atomic fea-
tures. To extract the subwords, we find the inner-
most left/right modifiers of the head character, re-
spectively, and then conjoin them with all their de-
scendant characters to form the smallest left/right
subwords. Figure 2 shows an example, where the
smallest left subword of ???? (chief lawyer)?
is ??? (lawyer)?, and the smallest right subword
1327
? ? ?
big law officer
(a) smallest left subword
? ? ?
agree with law ize
(b) smallest right subword
Figure 2: An example to illustrate the innermost
left/right subwords.
of ???? (legalize)? is ??? (legal)?.
3 Character-Level Dependency Parsing
A transition-based framework with global learn-
ing and beam search decoding (Zhang and Clark,
2011) has been applied to a number of natural lan-
guage processing tasks, including word segmen-
tation, POS-tagging and syntactic parsing (Zhang
and Clark, 2010; Huang and Sagae, 2010; Bohnet
and Nivre, 2012; Zhang et al, 2013). It models
a task incrementally from a start state to an end
state, where each intermediate state during decod-
ing can be regarded as a partial output. A num-
ber of actions are defined so that the state ad-
vances step by step. To learn the model param-
eters, it usually uses the online perceptron algo-
rithm with early-update under the inexact decod-
ing condition (Collins, 2002; Collins and Roark,
2004). Transition-based dependency parsing can
be modeled under this framework, where the state
consists of a stack and a queue, and the set of ac-
tions can be either the arc-eager (Zhang and Clark,
2008) or the arc-standard (Huang et al, 2009)
transition systems.
When the internal structures of words are an-
notated, character-level dependency parsing can
be treated as a special case of word-level depen-
dency parsing, with ?words? being ?characters?.
A big weakness of this approach is that full words
and POS-tags cannot be used for feature engineer-
ing. Both are crucial to well-established features
for word segmentation, POS-tagging and syntactic
parsing. In this section, we introduce novel exten-
sions to the arc-standard and the arc-eager tran-
sition systems, so that word-based and character-
based features can be used simultaneously for
character-level dependency parsing.
3.1 The Arc-Standard Model
The arc-standard model has been applied to joint
segmentation, POS-tagging and dependency pars-
ing (Hatori et al, 2012), but with pseudo word
structures. For unified processing of annotated
word structures and fair comparison between
character-level arc-eager and arc-standard sys-
tems, we define a different arc-standard transition
system, consistent with our character-level arc-
eager system.
In the word-based arc-standard model, the tran-
sition state includes a stack and a queue, where
the stack contains a sequence of partially-parsed
dependency trees, and the queue consists of un-
processed input words. Four actions are defined
for state transition, including arc-left (AL, which
creates a left arc between the top element s
0
and
the second top element s
1
on the stack), arc-right
(AR, which creates a right arc between s
0
and s
1
),
pop-root (PR, which defines the root node of a de-
pendency tree when there is only one element on
the stack and no element in the queue), and the last
shift (SH, which shifts the first element q
0
of the
queue onto the stack).
For character-level dependency parsing, there
are two types of dependencies: inter-word depen-
dencies and intra-word dependencies. To parse
them with both character and word features, we
extend the original transition actions into two cat-
egories, for inter-word dependencies and intra-
word dependencies, respectively. The actions for
inter-word dependencies include inter-word arc-
left (AL
w
), inter-word arc-right (AR
w
), pop-root
(PR) and inter-word shift (SH
w
). Their definitions
are the same as the word-based model, with one
exception that the inter-word shift operation has
a parameter denoting the POS-tag of the incoming
word, so that POS disambiguation is performed by
the SH
w
action.
The actions for intra-word dependencies in-
clude intra-word arc-left (AL
c
), intra-word arc-
right (AR
c
), pop-word (PW) and inter-word shift
(SH
c
). The definitions of AL
c
, AR
c
and SH
c
are
the same as the word-based arc-standard model,
while PW changes the top element on the stack
into a full-word node, which can only take inter-
word dependencies. One thing to note is that, due
to variable word sizes in character-level parsing,
the number of actions can vary between differ-
ent sequences of actions corresponding to differ-
ent analyses. We use the padding method (Zhu
et al, 2013), adding an IDLE action to finished
transition action sequences, for better alignments
between states in the beam.
In the character-level arc-standard transition
1328
step action stack queue dependencies
0 - ? ? ? ? ? ? ?
1 SH
w
(NR) ?/NR ? ? ? ? ? ?
2 SH
c
?/NR ?/NR ? ? ? ? ? ?
3 AL
c
?/NR ? ? ? ? ? A
1
= {?x?}
4 SH
c
?/NR ?/NR ? ? ? ? ? A
1
5 AL
c
?/NR ? ? ? ? ? A
2
= A
1
?
{?x?}
6 PW ???/NR ? ? ? ? ? A
2
7 SH
w
(NN) ???/NR ?/NN ? ? ? ? ? A
2
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
12 PW ???/NR ???/NN ? ? ? ? ? A
i
13 AL
w
???/NN ? ? ? ? ? A
i+1
= A
i
?
{???/NRx???/NN}
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
(a) character-level dependency parsing using the arc-standard algorithm
step action stack deque queue dependencies
0 - ? ? ? ? ? ?
1 SH
c
(NR) ? ?/NR ? ? ? ? ? ?
2 AL
c
? ? ?/NR ? ? ? ? A
1
= {?x?}
3 SH
c
? ?/NR ? ? ? ? ? A
1
4 AL
c
? ? ?/NR ? ? ? ? A
2
= A
1
?
{?x?}
5 SH
c
? ?/NR ? ? ? ? ? A
2
6 PW ? ???/NR ? ? ? ? ? A
2
7 SH
w
???/NR ? ? ? ? ? ? A
2
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
13 PW ???/NR ???/NN ? ? ? ? ? A
i
14 AL
w
? ???/NN ? ? ? ? ? A
i+1
= A
i
?
{???/NRx???/NN}
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
(b) character-level dependency parsing using the arc-eager algorithm, t = 1
Figure 3: Character-level dependency parsing of the sentence in Figure 1(c).
system, each word is initialized by the action SH
w
with a POS tag, before being incrementally mod-
ified by a sequence of intra-word actions, and fi-
nally being completed by the action PW. The inter-
word actions can be applied when all the elements
on the stack are full-word nodes, while the intra-
word actions can be applied when at least the top
element on the stack is a partial-word node. For
the actions AL
c
and AR
c
to be valid, the top two
elements on the stack are both partial-word nodes.
For the action PW to be valid, only the top ele-
ment on the stack is a partial-word node. Figure
3(a) gives an example action sequence.
There are three types of features. The first two
types are traditionally established features for the
dependency parsing and joint word segmentation
and POS-tagging tasks. We use the features pro-
posed by Hatori et al (2012). The word-level
dependency parsing features are added when the
inter-word actions are applied, and the features
for joint word segmentation and POS-tagging are
added when the actions PW, SH
w
and SH
c
are ap-
plied. Following the work of Hatori et al (2012),
we have a parameter ? to adjust the weights for
joint word segmentation and POS-tagging fea-
tures. We apply word-based dependency pars-
ing features to intra-word dependency parsing as
well, by using subwords (the conjunction of char-
acters spanning the head node) to replace words in
word features. The third type of features is word-
structure features. We extract the head charac-
ter and the smallest subwords containing the head
character from the intra-word dependencies (Sec-
tion 2). Table 1 summarizes the features.
3.2 The Arc-Eager Model
Similar to the arc-standard case, the state of a
word-based arc-eager model consists of a stack
and a queue, where the stack contains a sequence
of partial dependency trees, and the queue con-
sists of unprocessed input words. Unlike the arc-
standard model, which builds dependencies on the
top two elements on the stack, the arc-eager model
builds dependencies between the top element of
the stack and the first element of the queue. Five
actions are defined for state transformation: arc-
left (AL, which creates a left arc between the top
element of the stack s
0
and the first element in
the queue q
0
, while popping s
0
off the stack),
arc-right (AR, which creates a right arc between
1329
Feature templates
Lc, Lct, Rc, Rct, L
lc1
c, L
rc1
c, R
lc1
c,
Lc ?Rc, L
lc1
ct, L
rc1
ct, R
lc1
ct,
Lc ?Rw, Lw ?Rc, Lct ?Rw,
Lwt ?Rc, Lw ?Rct, Lc ?Rwt,
Lc ?Rc ? L
lc1
c, Lc ?Rc ? L
rc1
c,
Lc ?Rc ? L
lc2
c, Lc ?Rc ? L
rc2
c,
Lc ?Rc ?R
lc1
c, Lc ?Rc ?R
lc2
c,
Llsw, Lrsw, Rlsw, Rrsw, Llswt,
Lrswt, Rlswt, Rrswt, Llsw ?Rw,
Lrsw ?Rw, Lw ?Rlsw, Lw ?Rrsw
Table 1: Feature templates encoding intra-word
dependencies. L and R denote the two elements
over which the dependencies are built; the sub-
scripts lc1 and rc1 denote the left-most and right-
most children, respectively; the subscripts lc2 and
rc2 denote the second left-most and second right-
most children, respectively; w denotes the word;
t denotes the POS tag; c denotes the head charac-
ter; lsw and rsw denote the smallest left and right
subwords respectively, as shown in Figure 2.
s
0
and q
0
, while shifting q
0
from the queue onto
the stack), pop-root (PR, which defines the ROOT
node of the dependency tree when there is only
one element on the stack and no element in the
queue), reduce (RD, which pops s
0
off the stack),
and shift (SH, which shifts q
0
onto the stack).
There is no previous work that exploits the
arc-eager algorithm for jointly performing POS-
tagging and dependency parsing. Since the first
element of the queue can be shifted onto the stack
by either SH or AR, it is more difficult to assign
a POS tag to each word by using a single action.
In this work, we make a change to the configu-
ration state, adding a deque between the stack and
the queue to save partial words with intra-word de-
pendencies. We divide the transition actions into
two categories, one for inter-word dependencies
(AR
w
, AL
w
, SH
w
, RD
w
and PR) and the other
for intra-word dependencies (AR
c
, AL
c
, SH
c
, RD
c
and PW), requiring that the intra-word actions be
operated between the deque and the queue, while
the inter-word actions be operated between the
stack and the deque.
For character-level arc-eager dependency pars-
ing, the inter-word actions are the same as the
word-based methods. The actions AL
c
and AR
c
are the same as AL
w
and AR
w
, except that they
operate on characters, but the SH
c
operation has a
parameter to denote the POS tag of a word. The
PW action recognizes a full-word. We also have
an IDLE action, for the same reason as the arc-
standard model.
In the character-level arc-eager transition sys-
tem, a word is formed in a similar way with that
of character-level arc-standard algorithm. Each
word is initialized by the action SH
c
with a POS
tag, and then incrementally changed a sequence of
intra-word actions, before being finalized by the
action PW. All these actions operate between the
queue and deque. For the action PW, only the
first element in the deque (close to the queue) is
a partial-word node. For the actions AR
c
and AL
c
to be valid, the first element in the deque must be
a partial-word node. The action SH
c
have a POS
tag when shifting the first character of a word,but
does not have such a parameter when shifting the
next characters of a word. For the action SH
c
with
a POS tag to be valid, the first element in the deque
must be a full-word node. Different from the arc-
standard model, at any stage we can choose either
the action SH
c
with a POS tag to initialize a new
word on the deque, or the inter-word actions on
the stack. In order to eliminate the ambiguity, we
define a new parameter t to limit the max size of
the deque. If the deque is full with t words, inter-
word actions are performed; otherwise intra-word
actions are performed. All the inter-word actions
must be applied on full-word nodes between the
stack an the deque. Figure 3(b) gives an example
action sequence.
Similar to the arc-standard case, there are three
types of features, with the first two types being
traditionally established features for dependency
parsing and joint word segmentation and POS-
tagging. The dependency parsing features are
taken from the work of Zhang and Nivre (2011),
and the features for joint word segmentation and
POS-tagging are taken from Zhang and Clark
(2010)
1
. The word-level dependency parsing fea-
tures are triggered when the inter-word actions are
applied, while the features of joint word segmenta-
tion and POS-tagging are added when the actions
SH
c
, AR
c
and PW are applied. Again we use a pa-
rameter ? to adjust the weights for joint word seg-
mentation and POS-tagging features. The word-
level features for dependency parsing are applied
to intra-word dependency parsing as well, by us-
ing subwords to replace words. The third type of
features is word-structure features, which are the
1
Since Hatori et al (2012) also use Zhang and Clark
(2010)?s features, the arc-standard and arc-eager character-
level dependency parsing models have the same features for
joint word segmentation and POS-tagging.
1330
CTB50 CTB60 CTB70
Training
#sent 18k 23k 31k
#word 494k 641k 718k
Development
#sent 350 2.1k 10k
#word 6.8k 60k 237k
#oov 553 3.3k 13k
Test
#sent 348 2.8k 10k
#word 8.0k 82k 245k
#oov 278 4.6k 13k
Table 2: Statistics of datasets.
same as those of the character-level arc-standard
model, shown in Table 1.
4 Experiments
4.1 Experimental Settings
We use the Chinese Penn Treebank 5.0, 6.0 and 7.0
to conduct the experiments, splitting the corpora
into training, development and test sets according
to previous work. Three different splitting meth-
ods are used, namely CTB50 by Zhang and Clark
(2010), CTB60 by the official documentation of
CTB 6.0, and CTB70 by Wang et al (2011). The
dataset statistics are shown in Table 2. We use
the head rules of Zhang and Clark (2008) to con-
vert phrase structures into dependency structures.
The intra-word dependencies are extracted from
the annotations of Zhang et al (2013)
2
.
The standard measures of word-level precision,
recall and F1 score are used to evaluate word seg-
mentation, POS-tagging and dependency parsing,
following Hatori et al (2012). In addition, we use
the same measures to evaluate intra-word depen-
dencies, which indicate the performance of pre-
dicting word structures. A word?s structure is cor-
rect only if all the intra-word dependencies are all
correctly recognized.
4.2 Baseline and Proposed Models
For the baseline, we have two different pipeline
models. The first consists of a joint segmentation
and POS-tagging model (Zhang and Clark, 2010)
and a word-based dependency parsing model us-
ing the arc-standard algorithm (Huang et al,
2009). We name this model STD (pipe). The
second consists of the same joint segmentation
and POS-tagging model and a word-based depen-
dency parsing model using the arc-eager algorithm
2
https://github.com/zhangmeishan/
wordstructures; their annotation was conducted
on CTB 5.0, while we made annotations of the remainder of
the CTB 7.0 words. We also make the annotations publicly
available at the same site.
(Zhang and Nivre, 2011). We name this model
EAG (pipe). For the pipeline models, we use a
beam of size 16 for joint segmentation and POS-
tagging, and a beam of size 64 for dependency
parsing, according to previous work.
We study the following character-level depen-
dency parsing models:
? STD (real, pseudo): the arc-standard model
with annotated intra-word dependencies and
pseudo inter-word dependencies;
? STD (pseudo, real): the arc-standard model
with pseudo intra-word dependencies and
real inter-word dependencies;
? STD (real, real): the arc-standard model with
annotated intra-word dependencies and real
inter-word dependencies;
? EAG (real, pseudo): the arc-eager model
with annotated intra-word dependencies and
pseudo inter-word dependencies;
? EAG (pseudo, real): the arc-eager model
with pseudo intra-word dependencies and
real inter-word dependencies;
? EAG (real, real): the arc-eager model with
annotated intra-word dependencies and real
inter-word dependencies.
The annotated intra-word dependencies refer to
the dependencies extracted from annotated word
structures, while the pseudo intra-word depen-
dencies used in the above models are similar
to those of Hatori et al (2012). For a given
word w = c
1
c
2
? ? ? c
m
, the intra-word depen-
dency structure is c
x
1
c
x
2
? ? ?
x
c
m
3
. The real inter-
word dependencies refer to the syntactic word-
level dependencies by head-finding rules from
CTB, while the pseudo inter-word dependencies
refer to the word-level dependencies used by Zhao
(2009) (w
x
1
w
x
2
? ? ?
x
w
n
). The character-level
models with annotated intra-word dependencies
and pseudo inter-word dependencies are compared
with the pipelines on word segmentation and POS-
tagging accuracies, and are compared with the
character-level models with annotated intra-word
dependencies and real inter-word dependencies
on word segmentation, POS-tagging and word-
structure predicating accuracies. All the proposed
3
We also tried similar structures with right arcs, which
gave lower accuracies.
1331
STD (real, real) SEG POS DEP WS
? = 1 95.85 91.60 76.96 95.14
? = 2 96.09 91.89 77.28 95.29
? = 3 96.02 91.84 77.22 95.23
? = 4 96.10 91.96 77.49 95.29
? = 5 96.07 91.90 77.31 95.21
Table 3: Development test results of the character-
level arc-standard model on CTB60.
EAG (real, real) SEG POS DEP WS
? = 1
t = 1 96.00 91.66 74.63 95.49
t = 2 95.93 91.75 76.60 95.37
t = 3 95.93 91.74 76.94 95.36
t = 4 95.91 91.71 76.82 95.33
t = 5 95.95 91.73 76.84 95.40
t = 3
? = 1 95.93 91.74 76.94 95.36
? = 2 96.11 91.99 77.17 95.56
? = 3 96.16 92.01 77.48 95.62
? = 4 96.11 91.93 77.40 95.53
? = 5 96.00 91.84 77.10 95.43
Table 4: Development test results of the character-
level arc-eager model on CTB60.
models use a beam of size 64 after considering
both speeds and accuracies.
4.3 Development Results
Our development tests are designed for two pur-
poses: adjusting the parameters for the two pro-
posed character-level models and testing the effec-
tiveness of the novel word-structure features. Tun-
ing is conducted by maximizing word-level depen-
dency accuracies. All the tests are conducted on
the CTB60 data set.
4.3.1 Parameter Tuning
For the arc-standard model, there is only one pa-
rameter ? that needs tuning. It adjusts the weights
of segmentation and POS-tagging features, be-
cause the number of feature templates is much less
for the two tasks than for parsing. We set the value
of ? to 1 ? ? ? 5, respectively. Table 3 shows the
accuracies on the CTB60 development set. Ac-
cording to the results, we use ? = 4 for our final
character-level arc-standard model.
For the arc-eager model, there are two parame-
ters t and ?. t denotes the deque size of the arc-
eager model, while ? shares the same meaning as
the arc-standard model. We take two steps for pa-
rameter tuning, first adjusting the more crucial pa-
rameter t and then adjusting ? on the best t. Both
parameters are assigned the values of 1 to 5. Ta-
SEG POS DEP WS
STD (real, real) 96.10 91.96 77.49 95.29
STD (real, real)/wo 95.99 91.79 77.19 95.35
? -0.11 -0.17 -0.30 +0.06
EAG (real, real) 96.16 92.01 77.48 95.62
EAG (real, real)/wo 96.09 91.82 77.12 95.56
? -0.07 -0.19 -0.36 -0.06
Table 5: Feature ablation tests for the novel word-
structure features, where ?/wo? denotes the corre-
sponding models without the novel intra-word de-
pendency features.
ble 4 shows the results. According to results, we
set t = 3 and ? = 3 for the final character-level
arc-eager model, respectively.
4.3.2 Effectiveness of Word-Structure
Features
To test the effectiveness of our novel word-
structure features, we conduct feature ablation ex-
periments on the CTB60 development data set for
the proposed arc-standard and arc-eager models,
respectively. Table 5 shows the results. We can
see that both the two models achieve better accu-
racies on word-level dependencies with the novel
word-structure features, while the features do not
affect word-structure predication significantly.
4.4 Final Results
Table 6 shows the final results on the CTB50,
CTB60 and CTB70 data sets, respectively. The
results demonstrate that the character-level depen-
dency parsing models are significantly better than
the corresponding word-based pipeline models,
for both the arc-standard and arc-eager systems.
Similar to the findings of Zhang et al (2013), we
find that the annotated word structures can give
better accuracies than pseudo word structures. An-
other interesting finding is that, although the arc-
eager algorithm achieves lower accuracies in the
word-based pipeline models, it obtains compara-
tive accuracies in the character-level models.
We also compare our results to those of Hatori
et al (2012), which is comparable to STD (pseudo,
real) since similar arc-standard algorithms and
features are used. The major difference is the
set of transition actions. We rerun their system
on the three datasets
4
. As shown in Table 6, our
arc-standard system with pseudo word structures
4
http://triplet.cc/. We use a different
constituent-to-dependency conversion scheme in com-
parison with Hatori et al (2012)?s work.
1332
Model
CTB50 CTB60 CTB70
SEG POS DEP WS SEG POS DEP WS SEG POS DEP WS
The arc-standard models
STD (pipe) 97.53 93.28 79.72 ? 95.32 90.65 75.35 ? 95.23 89.92 73.93 ?
STD (real, pseudo) 97.78 93.74 ? 97.40 95.77
?
91.24
?
? 95.08 95.59
?
90.49
?
? 94.97
STD (pseudo, real) 97.67 94.28
?
81.63
?
? 95.63
?
91.40
?
76.75
?
? 95.53
?
90.75
?
75.63
?
?
STD (real, real) 97.84 94.62
?
82.14
?
97.30 95.56
?
91.39
?
77.09
?
94.80 95.51
?
90.76
?
75.70
?
94.78
Hatori+ ?12 97.75 94.33 81.56 ? 95.26 91.06 75.93 ? 95.27 90.53 74.73 ?
The arc-eager models
EAG (pipe) 97.53 93.28 79.59 ? 95.32 90.65 74.98 ? 95.23 89.92 73.46 ?
EAG (real, pseudo) 97.75 93.88 ? 97.45 95.63
?
91.07
?
? 95.06 95.50
?
90.36
?
? 95.00
EAG (pseudo, real) 97.76 94.36
?
81.70
?
? 95.63
?
91.34
?
76.87
?
? 95.39
?
90.56
?
75.56
?
?
EAG (real, real) 97.84 94.36
?
82.07
?
97.49 95.71
?
91.51
?
76.99
?
95.16 95.47
?
90.72
?
75.76
?
94.94
Table 6: Main results, where the results marked with ? denote that the p-value is less than 0.001 compared
with the pipeline word-based models using pairwise t-test.
brings consistent better accuracies than their work
on all the three data sets.
Both the pipelines and character-level mod-
els with pseudo inter-word dependencies perform
word segmentation and POS-tagging jointly, with-
out using real word-level syntactic information. A
comparison between them (STD/EAG (pipe) vs.
STD/EAG (real, pseudo)) reflects the effectiveness
of annotated intra-word dependencies on segmen-
tation and POS-tagging. We can see that both the
arc-standard and arc-eager models with annotated
intra-word dependencies can improve the segmen-
tation accuracies by 0.3% and the POS-tagging ac-
curacies by 0.5% on average on the three datasets.
Similarly, a comparison between the character-
level models with pseudo inter-word dependen-
cies and the character-level models with real inter-
word dependencies (STD/EAG (real, pseudo) vs.
STD/EAG (real, real)) can reflect the effectiveness
of annotated inter-word structures on morphology
analysis. We can see that improved POS-tagging
accuracies are achieved using the real inter-word
dependencies when jointly performing inner- and
inter-word dependencies. However, we find that
the inter-word dependencies do not help the word-
structure accuracies.
4.5 Analysis
To better understand the character-level parsing
models, we conduct error analysis in this section.
All the experiments are conducted on the CTB60
test data sets. The new advantage of the character-
level models is that one can parse the internal
word structures of intra-word dependencies. Thus
we are interested in their capabilities of predict-
ing word structures. We study the word-structure
accuracies in two aspects, including OOV, word
length, POS tags and the parsing model.
4.5.1 OOV
The word-structure accuracy of OOV words re-
flects a model?s ability of handling unknown
words. The overall recalls of OOV word structures
are 67.98% by STD (real, real) and 69.01% by
EAG (real, real), respectively. We find that most
errors are caused by failures of word segmenta-
tion. We further investigate the accuracies when
words are correctly segmented, where the accura-
cies of OOV word structures are 87.64% by STD
(real, real) and 89.07% by EAG (real, real). The
results demonstrate that the structures of Chinese
words are not difficult to predict, and confirm the
fact that Chinese word structures have some com-
mon syntactic patterns.
4.5.2 Parsing Model
From the above analysis in terms of OOV, word
lengths and POS tags, we can see that the EAG
(real, real) model and the STD (real, real) mod-
els behave similarly on word-structure accuracies.
Here we study the two models more carefully,
comparing their word accuracies sentence by sen-
tence. Figure 4 shows the results, where each
point denotes a sentential comparison between
STD (real, real) and EAG (real, real), the x-axis
denotes the sentential word-structure accuracy of
STD (real, real), and the y-axis denotes that of
EAG (real, real). The points at the diagonal show
the same accuracies by the two models, while oth-
ers show that the two models perform differently
on the corresponding sentences. We can see that
most points are beyond the diagonal line, indicat-
1333
0.6 0.7 0.8 0.9 1
0.6
0.7
0.8
0.9
1
STD (real, real)
E
A
G
(
r
e
a
l
,
r
e
a
l
)
Figure 4: Sentential word-structure accuracies of
STD (real, real) and EAG (real, real).
ing that the two parsing models can be comple-
mentary in parsing intra-word dependencies.
5 Related Work
Zhao (2009) was the first to study character-level
dependencies; they argue that since no consistent
word boundaries exist over Chinese word segmen-
tation, dependency-based representations of word
structures serve as a good alternative for Chinese
word segmentation. Thus their main concern is
to parse intra-word dependencies. In this work,
we extend their formulation, making use of large-
scale annotations of Zhang et al (2013), so that the
syntactic word-level dependencies can be parsed
together with intra-word dependencies.
Hatori et al (2012) proposed a joint model
for Chinese word segmentation, POS-tagging and
dependency parsing, studying the influence of
joint model and character features for parsing,
Their model is extended from the arc-standard
transition-based model, and can be regarded as
an alternative to the arc-standard model of our
work when pseudo intra-word dependencies are
used. Similar work is done by Li and Zhou (2012).
Our proposed arc-standard model is more concise
while obtaining better performance than Hatori et
al. (2012)?s work. With respect to word structures,
real intra-word dependencies are often more com-
plicated, while pseudo word structures cannot be
used to correctly guide segmentation.
Zhao (2009), Hatori et al (2012) and our
work all study character-level dependency pars-
ing. While Zhao (2009) focus on word internal
structures using pseudo inter-word dependencies,
Hatori et al (2012) investigate a joint model using
pseudo intra-word dependencies. We use manual
dependencies for both inner- and inter-word struc-
tures, studying their influences on each other.
Zhang et al (2013) was the first to perform Chi-
nese syntactic parsing over characters. They ex-
tended word-level constituent trees by annotated
word structures, and proposed a transition-based
approach to parse intra-word structures and word-
level constituent structures jointly. For Hebrew,
Tsarfaty and Goldberg (2008) investigated joint
segmentation and parsing over characters using a
graph-based method. Our work is similar in ex-
ploiting character-level syntax. We study the de-
pendency grammar, another popular syntactic rep-
resentation, and propose two novel transition sys-
tems for character-level dependency parsing.
Nivre (2008) gave a systematic description of
the arc-standard and arc-eager algorithms, cur-
rently two popular transition-based parsing meth-
ods for word-level dependency parsing. We extend
both algorithms to character-level joint word seg-
mentation, POS-tagging and dependency parsing.
To our knowledge, we are the first to apply the arc-
eager system to joint models and achieve compar-
ative performances to the arc-standard model.
6 Conclusions
We studied the character-level Chinese depen-
dency parsing, by making novel extensions to
two commonly-used transition-based dependency
parsing algorithms for word-based dependency
parsing. With both pseudo and annotated word
structures, our character-level models obtained
better accuracies than previous work on seg-
mentation, POS-tagging and word-level depen-
dency parsing. We further analyzed some im-
portant factors for intra-word dependencies, and
found that two proposed character-level pars-
ing models are complementary in parsing intra-
word dependencies. We make the source code
publicly available at http://sourceforge.
net/projects/zpar/, version 0.7.
Acknowledgments
We thank the anonymous reviewers for their
constructive comments, and gratefully acknowl-
edge the support of the National Basic Re-
search Program (973 Program) of China via Grant
2014CB340503, the National Natural Science
Foundation of China (NSFC) via Grant 61133012
and 61370164, the Singapore Ministry of Educa-
tion (MOE) AcRF Tier 2 grant T2MOE201301
and SRG ISTD 2012 038 from Singapore Univer-
sity of Technology and Design.
1334
References
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of the EMNLP-CONLL, pages 1455?1465,
Jeju Island, Korea, July.
Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Proceed-
ings of the 23rd COLING, number August, pages
89?97.
Xavier Carreras, Mihai Surdeanu, and Llu??s M`arquez.
2006. Projective dependency parsing with per-
ceptron. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning
(CoNLL-X), pages 181?185, New York City, June.
Pi-Chuan Chang, Michel Galley, and Chris Manning.
2008. Optimizing chinese word segmentation for
machine translation performance. In ACL Workshop
on Statistical Machine Translation.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, , and
Christopher D. Manning. 2009. Discriminative
reordering with chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation.
Jinho D. Choi and Andrew McCallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In Proceedings of ACL, pages
1052?1062, August.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 111?118, Barcelona, Spain, July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the 7th EMNLP.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic models for action-based chinese dependency
parsing. In Proceedings of ECML/ECPPKDD, vol-
ume 4701 of Lecture Notes in Computer Science,
pages 559?566.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings
of the Second SIGHAN Workshop on Chinese Lan-
guage Processing, pages 123?133.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2012. Incremental joint approach
to word segmentation, pos tagging, and dependency
parsing in chinese. In Proceedings of the 50th ACL,
pages 1045?1053, Jeju Island, Korea, July.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th ACL, pages 1077?1086, Up-
psala, Sweden, July.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3-Volume 3, pages 1222?1231. Asso-
ciation for Computational Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the ACL, pages 1?11.
Zhongguo Li and Guodong Zhou. 2012. Unified de-
pendency parsing of chinese morphological and syn-
tactic structures. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1445?1454, Jeju Island, Ko-
rea, July.
Zhenghua Li, Ting Liu, and Wanxiang Che. 2012. Ex-
ploiting multiple treebanks for parsing with quasi-
synchronous grammars. In Proceedings of the 50th
ACL, pages 675?684, Jeju Island, Korea, July.
Zhongguo Li. 2011. Parsing the internal structure of
words: A new paradigm for chinese word segmenta-
tion. In Proceedings of the 49th ACL, pages 1405?
1414, Portland, Oregon, USA, June.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, number
June, pages 91?98, Morristown, NJ, USA.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
ACL.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513?553.
Reut Tsarfaty and Yoav Goldberg. 2008. Word-based
or morpheme-based? annotation strategies for mod-
ern hebrew clitics. In LREC. European Language
Resources Association.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmenta-
tion and pos tagging with semi-supervised methods
using large auto-analyzed data. In Proceedings of
5th IJCNLP, pages 309?317, Chiang Mai, Thailand,
November.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: Investigating and combining graph-
based and transition-based dependency parsing. In
Proceedings of EMNLP, pages 562?571, Honolulu,
Hawaii, October.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of the
EMNLP, pages 843?852, Cambridge, MA, October.
1335
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th ACL, pages 188?193, Port-
land, Oregon, USA, June.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008. Chinese word segmentation and statistical
machine translation. IEEE Transactions on Signal
Processing, 5(2).
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2013. Chinese parsing exploiting characters.
In Proceedings of the 51st ACL, pages 125?134,
Sofia, Bulgaria, August.
Hai Zhao. 2009. Character-level dependencies in chi-
nese: Usefulness and learning. In Proceedings of
the EACL, pages 879?887, Athens, Greece, March.
Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of the
51st ACL, pages 434?443, Sofia, Bulgaria, August.
1336
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 649?655,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
On WordNet Semantic Classes and Dependency Parsing
Kepa Bengoetxea?, Eneko Agirre?, Joakim Nivre?,
Yue Zhang*, Koldo Gojenola?
?University of the Basque Country UPV/EHU / IXA NLP Group
?Uppsala University / Department of Linguistics and Philology
? Singapore University of Technology and Design
kepa.bengoetxea@ehu.es, e.agirre@ehu.es,
joakim.nivre@lingfil.uu.se, yue zhang@sutd.edu.sg,
koldo.gojenola@ehu.es
Abstract
This paper presents experiments with
WordNet semantic classes to improve de-
pendency parsing. We study the effect
of semantic classes in three dependency
parsers, using two types of constituency-
to-dependency conversions of the English
Penn Treebank. Overall, we can say that
the improvements are small and not sig-
nificant using automatic POS tags, con-
trary to previously published results using
gold POS tags (Agirre et al, 2011). In
addition, we explore parser combinations,
showing that the semantically enhanced
parsers yield a small significant gain only
on the more semantically oriented LTH
treebank conversion.
1 Introduction
This work presents a set of experiments to investi-
gate the use of lexical semantic information in de-
pendency parsing of English. Whether semantics
improve parsing is one interesting research topic
both on parsing and lexical semantics. Broadly
speaking, we can classify the methods to incor-
porate semantic information into parsers in two:
systems using static lexical semantic repositories,
such as WordNet or similar ontologies (Agirre et
al., 2008; Agirre et al, 2011; Fujita et al, 2010),
and systems using dynamic semantic clusters au-
tomatically acquired from corpora (Koo et al,
2008; Suzuki et al, 2009).
Our main objective will be to determine
whether static semantic knowledge can help pars-
ing. We will apply different types of semantic in-
formation to three dependency parsers. Specifi-
cally, we will test the following questions:
? Does semantic information in WordNet help
dependency parsing? Agirre et al (2011)
found improvements in dependency parsing
using MaltParser on gold POS tags. In this
work, we will investigate the effect of seman-
tic information using predicted POS tags.
? Is the type of semantic information related
to the type of parser? We will test three
different parsers representative of successful
paradigms in dependency parsing.
? How does the semantic information relate to
the style of dependency annotation? Most ex-
periments for English were evaluated on the
Penn2Malt conversion of the constituency-
based Penn Treebank. We will also examine
the LTH conversion, with richer structure and
an extended set of dependency labels.
? How does WordNet compare to automati-
cally obtained information? For the sake of
comparison, we will also perform the experi-
ments using syntactic/semantic clusters auto-
matically acquired from corpora.
? Does parser combination benefit from seman-
tic information? Different parsers can use se-
mantic information in diverse ways. For ex-
ample, while MaltParser can use the semantic
information in local contexts, MST can in-
corporate them in global contexts. We will
run parser combination experiments with and
without semantic information, to determine
whether it is useful in the combined parsers.
After introducing related work in section 2, sec-
tion 3 describes the treebank conversions, parsers
and semantic features. Section 4 presents the re-
sults and section 5 draws the main conclusions.
2 Related work
Broadly speaking, we can classify the attempts to
add external knowledge to a parser in two sets:
using large semantic repositories such as Word-
Net and approaches that use information automat-
ically acquired from corpora. In the first group,
Agirre et al (2008) trained two state-of-the-art
constituency-based statistical parsers (Charniak,
649
2000; Bikel, 2004) on semantically-enriched in-
put, substituting content words with their seman-
tic classes, trying to overcome the limitations of
lexicalized approaches to parsing (Collins, 2003)
where related words, like scissors and knife, can-
not be generalized. The results showed a signi-
cant improvement, giving the first results over both
WordNet and the Penn Treebank (PTB) to show
that semantics helps parsing. Later, Agirre et al
(2011) successfully introduced WordNet classes in
a dependency parser, obtaining improvements on
the full PTB using gold POS tags, trying different
combinations of semantic classes. MacKinlay et
al. (2012) investigate the addition of semantic an-
notations in the form of word sense hypernyms, in
HPSG parse ranking, reducing error rate in depen-
dency F-score by 1%, while some methods pro-
duce substantial decreases in performance. Fu-
jita et al (2010) showed that fully disambiguated
sense-based features smoothed using ontological
information are effective for parse selection.
On the second group, Koo et al (2008) pre-
sented a semisupervised method for training de-
pendency parsers, introducing features that incor-
porate word clusters automatically acquired from
a large unannotated corpus. The clusters include
strongly semantic associations like {apple, pear}
or {Apple, IBM} and also syntactic clusters like
{of, in}. They demonstrated its effectiveness in
dependency parsing experiments on the PTB and
the Prague Dependency Treebank. Suzuki et al
(2009), Sagae and Gordon (2009) and Candito
and Seddah (2010) also experiment with the same
cluster method. Recently, T?ackstr?om et al (2012)
tested the incorporation of cluster features from
unlabeled corpora in a multilingual setting, giving
an algorithm for inducing cross-lingual clusters.
3 Experimental Framework
In this section we will briefly describe the PTB-
based datasets (subsection 3.1), followed by the
data-driven parsers used for the experiments (sub-
section 3.2). Finally, we will describe the different
types of semantic representation that were used.
3.1 Treebank conversions
Penn2Malt
1
performs a simple and direct conver-
sion from the constituency-based PTB to a depen-
dency treebank. It obtains projective trees and has
been used in several works, which allows us to
1
http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
compare our results with related experiments (Koo
et al, 2008; Suzuki et al, 2009; Koo and Collins,
2010). We extracted dependencies using standard
head rules (Yamada and Matsumoto, 2003), and a
reduced set of 12 general dependency tags.
LTH
2
(Johansson and Nugues, 2007) presents
a conversion better suited for semantic process-
ing, with a richer structure and a more fine-grained
set of dependency labels (42 different dependency
labels), including links to handle long-distance
phenomena, giving a 6.17% of nonprojective sen-
tences. The results from parsing the LTH output
are lower than those for Penn2Malt conversions.
3.2 Parsers
We have made use of three parsers representative
of successful paradigms in dependency parsing.
MaltParser (Nivre et al, 2007) is a determinis-
tic transition-based dependency parser that obtains
a dependency tree in linear-time in a single pass
over the input using a stack of partially analyzed
items and the remaining input sequence, by means
of history-based feature models. We added two
features that inspect the semantic feature at the top
of the stack and the next input token.
MST
3
represents global, exhaustive graph-
based parsing (McDonald et al, 2005; McDon-
ald et al, 2006) that finds the highest scoring di-
rected spanning tree in a graph. The learning pro-
cedure is global since model parameters are set
relative to classifying the entire dependency graph,
in contrast to the local but richer contexts used
by transition-based parsers. The system can be
trained using first or second order models. The
second order projective algorithm performed best
on both conversions, and we used it in the rest of
the evaluations. We modified the system in or-
der to add semantic features, combining them with
wordforms and POS tags, on the parent and child
nodes of each arc.
ZPar
4
(Zhang and Clark, 2008; Zhang and
Nivre, 2011) performs transition-based depen-
dency parsing with a stack of partial analysis
and a queue of remaining inputs. In contrast to
MaltParser (local model and greedy deterministic
search) ZPar applies global discriminative learn-
ing and beam search. We extend the feature set of
ZPar to include semantic features. Each set of se-
mantic information is represented by two atomic
2
http://nlp.cs.lth.se/software/treebank converter
3
http://mstparser.sourceforge.net
4
www.sourceforge.net/projects/zpar
650
Base WordNet WordNet Clusters
line SF SS
Malt 88.46 88.49 (+0.03) 88.42 (-0.04) 88.59 (+0.13)
MST 90.55 90.70 (+0.15) 90.47 (-0.08) 90.88 (+0.33)?
ZPar 91.52 91.65 (+0.13) 91.70 (+0.18)? 91.74 (+0.22)
Table 1: LAS results with several parsing algo-
rithms, Penn2Malt conversion (?: p <0.05, ?: p
<0.005). In parenthesis, difference with baseline.
feature templates, associated with the top of the
stack and the head of the queue, respectively. ZPar
was directly trained on the Penn2Malt conversion,
while we applied the pseudo-projective transfor-
mation (Nilsson et al, 2008) on LTH, in order to
deal with non-projective arcs.
3.3 Semantic information
Our aim was to experiment with different types of
WordNet-related semantic information. For com-
parison with automatically acquired information,
we will also experiment with bit clusters.
WordNet. We will experiment with the seman-
tic representations used in Agirre et al (2008) and
Agirre et al (2011), based on WordNet 2.1. Word-
Net is organized into sets of synonyms, called
synsets (SS). Each synset in turn belongs to a
unique semantic file (SF). There are a total of 45
SFs (1 for adverbs, 3 for adjectives, 15 for verbs,
and 26 for nouns), based on syntactic and seman-
tic categories. For example, noun SFs differen-
tiate nouns denoting acts or actions, and nouns
denoting animals, among others. We experiment
with both full SSs and SFs as instances of fine-
grained and coarse-grained semantic representa-
tion, respectively. As an example, knife in its
tool sense is in the EDGE TOOL USED AS A
CUTTING INSTRUMENT singleton synset, and
also in the ARTIFACT SF along with thousands
of words including cutter. These are the two ex-
tremes of semantic granularity in WordNet. For
each semantic representation, we need to deter-
mine the semantics of each occurrence of a target
word. Agirre et al (2011) used i) gold-standard
annotations from SemCor, a subset of the PTB, to
give an upper bound performance of the semantic
representation, ii) first sense, where all instances
of a word were tagged with their most frequent
sense, and iii) automatic sense ranking, predicting
the most frequent sense for each word (McCarthy
et al, 2004). As we will make use of the full PTB,
we only have access to the first sense information.
Clusters. Koo et al (2008) describe a semi-
Base WordNet WordNet Clusters
line SF SS
Malt 84.95 85.12 (+0.17) 85.08 (+0.16) 85.13 (+0.18)
MST 85.06 85.35 (+0.29)? 84.99 (-0.07) 86.18 (+1.12)?
ZPar 89.15 89.33 (+0.18) 89.19 (+0.04) 89.17 (+0.02)
Table 2: LAS results with several parsing algo-
rithms in the LTH conversion (?: p <0.05, ?: p
<0.005). In parenthesis, difference with baseline.
supervised approach that makes use of cluster fea-
tures induced from unlabeled data, providing sig-
nificant performance improvements for supervised
dependency parsers on the Penn Treebank for En-
glish and the Prague Dependency Treebank for
Czech. The process defines a hierarchical cluster-
ing of the words, which can be represented as a
binary tree where each node is associated to a bit-
string, from the more general (root of the tree) to
the more specific (leaves). Using prefixes of vari-
ous lengths, it can produce clusterings of different
granularities. It can be seen as a representation of
syntactic-semantic information acquired from cor-
pora. They use short strings of 4-6 bits to represent
parts of speech and the full strings for wordforms.
4 Results
In all the experiments we employed a baseline fea-
ture set using word forms and parts of speech, and
an enriched feature set (WordNet or clusters). We
firstly tested the addition of each individual se-
mantic feature to each parser, evaluating its contri-
bution to the parser?s performance. For the combi-
nations, instead of feature-engineering each parser
with the wide array of different possibilities for
features, as in Agirre et al (2011), we adopted
the simpler approach of combining the outputs of
the individual parsers by voting (Sagae and Lavie,
2006). We will use Labeled Attachment Score
(LAS) as our main evaluation criteria. As in pre-
vious work, we exclude punctuation marks. For
all the tests, we used a perceptron POS-tagger
(Collins, 2002), trained on WSJ sections 2?21, to
assign POS tags automatically to both the training
(using 10-way jackknifing) and test data, obtaining
a POS tagging accuracy of 97.32% on the test data.
We will make use of Bikel?s randomized parsing
evaluation comparator to test the statistical signi-
cance of the results. In all of the experiments the
parsers were trained on sections 2-21 of the PTB
and evaluated on the development set (section 22).
Finally, the best performing system was evaluated
on the test set (section 23).
651
Parsers LAS UAS
Best baseline (ZPar) 91.52 92.57
Best single parser (ZPar + Clusters) 91.74 (+0.22) 92.63
Best combination (3 baseline parsers) 91.90 (+0.38) 93.01
Best combination of 3 parsers:
3 baselines + 3 SF extensions 91.93 (+0.41) 92.95
Best combination of 3 parsers:
3 baselines + 3 SS extensions 91.87 (+0.35) 92.92
Best combination of 3 parsers:
3 baselines + 3 cluster extensions 91.90 (+0.38) 92.90
Table 3: Parser combinations on Penn2Malt.
Parsers LAS UAS
Best baseline (ZPar) 89.15 91.81
Best single parser (ZPar + SF) 89.33 (+0.15) 92.01
Best combination (3 baseline parsers) 89.15 (+0.00) 91.81
Best combination of 3 parsers:
3 baselines + 3 SF extensions 89.56 (+0.41)? 92.23
Best combination of 3 parsers:
3 baselines + 3 SS extensions 89.43 (+0.28) 93.12
Best combination of 3 parsers:
3 baselines + 3 cluster extensions 89.52 (+0.37)? 92.19
Table 4: Parser combinations on LTH (?: p <0.05,
?: p <0.005).
4.1 Single Parsers
We run a series of experiments testing each indi-
vidual semantic feature, also trying different learn-
ing configurations for each one. Regarding the
WordNet information, there were 2 different fea-
tures to experiment with (SF and SS). For the bit
clusters, there are different possibilities, depend-
ing on the number of bits used. For Malt and MST,
all the different lengths of bit strings were used.
Given the computational requirements and the pre-
vious results on Malt and MST, we only tested all
bits in ZPar. Tables 1 and 2 show the results.
Penn2Malt. Table 1 shows that the only signifi-
cant increase over the baseline is for ZPar with SS
and for MST with clusters.
LTH. Looking at table 2, we can say that the dif-
ferences in baseline parser performance are accen-
tuated when using the LTH treebank conversion,
as ZPar clearly outperforms the other two parsers
by more than 4 absolute points. We can see that
SF helps all parsers, although it is only significant
for MST. Bit clusters improve significantly MST,
with the highest increase across the table.
Overall, we see that the small improvements
do not confirm the previous results on Penn2Malt,
MaltParser and gold POS tags. We can also con-
clude that automatically acquired clusters are spe-
cially effective with the MST parser in both tree-
bank conversions, which suggests that the type of
semantic information has a direct relation to the
parsing algorithm. Section 4.3 will look at the de-
tails by each knowledge type.
4.2 Combinations
Subsection 4.1 presented the results of the base al-
gorithms and their extensions based on semantic
features. Sagae and Lavie (2006) report improve-
ments over the best single parser when combining
three transition-based models and one graph-based
model. The same technique was also used by the
winning team of the CoNLL 2007 Shared Task
(Hall et al, 2007), combining six transition-based
parsers. We used MaltBlender
5
, a tool for merging
the output of several dependency parsers, using the
Chu-Liu/Edmonds directed MST algorithm. After
several tests we noticed that weighted voting by
each parser?s labeled accuracy gave good results,
using it in the rest of the experiments. We trained
different types of combination:
? Base algorithms. This set includes the 3 base-
line algorithms, MaltParser, MST, and ZPar.
? Extended parsers, adding semantic informa-
tion to the baselines. We include the three
base algorithms and their semantic exten-
sions (SF, SS, and clusters). It is known (Sur-
deanu and Manning, 2010) that adding more
parsers to an ensemble usually improves ac-
curacy, as long as they add to the diver-
sity (and almost regardless of their accuracy
level). So, for the comparison to be fair, we
will compare ensembles of 3 parsers, taken
from sets of 6 parsers (3 baselines + 3 SF,
SS, and cluster extensions, respectively).
In each experiment, we took the best combina-
tion of individual parsers on the development set
for the final test. Tables 3 and 4 show the results.
Penn2Malt. Table 3 shows that the combina-
tion of the baselines, without any semantic infor-
mation, considerably improves the best baseline.
Adding semantics does not give a noticeable in-
crease with respect to combining the baselines.
LTH (table 4). Combining the 3 baselines does
not give an improvement over the best baseline, as
ZPar clearly outperforms the other parsers. How-
ever, adding the semantic parsers gives an increase
with respect to the best single parser (ZPar + SF),
which is small but significant for SF and clusters.
4.3 Analysis
In this section we analyze the data trying to under-
stand where and how semantic information helps
most. One of the obstacles of automatic parsers
is the presence of incorrect POS tags due to auto-
5
http://w3.msi.vxu.se/users/jni/blend/
652
LAS on sentences LAS on sentences
POS tags Parser LAS test set without POS errors with POS errors
Gold ZPar 90.45 91.68 89.14
Automatic ZPar 89.15 91.62 86.51
Automatic Best combination of 3 parsers: 89.56 (+0.41) 91.90 (+0.28) 87.06 (+0.55)
3 baselines + 3 SF extensions
Automatic Best combination of 3 parsers: 89.43 (+0.28) 91.95 (+0.33) 86.75 (+0.24)
3 baselines + 3 SS extensions
Automatic Best combination of 3 parsers: 89.52 (+0.37) 91.92 (+0.30) 86.96 (+0.45)
3 baselines + 3 cluster extensions
Table 5: Differences in LAS (LTH) for baseline and extended parsers with sentences having cor-
rect/incorrect POS tags (the parentheses show the difference w.r.t ZPar with automatic POS tags).
matic tagging. For example, ZPar?s LAS score on
the LTH conversion drops from 90.45% with gold
POS tags to 89.12% with automatic POS tags. We
will examine the influence of each type of seman-
tic information on sentences that contain or not
POS errors, and this will clarify whether the incre-
ments obtained when using semantic information
are useful for correcting the negative influence of
POS errors or they are orthogonal and constitute
a source of new information independent of POS
tags. With this objective in mind, we analyzed the
performance on the subset of the test corpus con-
taining the sentences which had POS errors (1,025
sentences and 27,300 tokens) and the subset where
the sentences had (automatically assigned) correct
POS tags (1,391 sentences and 29,386 tokens).
Table 5 presents the results of the best single
parser on the LTH conversion (ZPar) with gold
and automatic POS tags in the first two rows. The
LAS scores are particularized for sentences that
contain or not POS errors. The following three
rows present the enhanced (combined) parsers
that make use of semantic information. As the
combination of the three baseline parsers did not
give any improvement over the best single parser
(ZPar), we can hypothesize that the gain coming
from the parser combinations comes mostly from
the addition of semantic information. Table 5 sug-
gests that the improvements coming from Word-
Net?s semantic file (SF) are unevenly distributed
between the sentences that contain POS errors and
those that do not (an increase of 0.28 for sentences
without POS errors and 0.55 for those with er-
rors). This could mean that a big part of the in-
formation contained in SF helps to alleviate the
errors performed by the automatic POS tagger. On
the other hand, the increments are more evenly
distributed for SS and clusters, and this can be
due to the fact that the semantic information is
orthogonal to the POS, giving similar improve-
ments for sentences that contain or not POS errors.
We independently tested this fact for the individ-
ual parsers. For example, with MST and SF the
gains almost doubled for sentences with incorrect
POS tags (+0.37 with respect to +0.21 for sen-
tences with correct POS tags) while the gains of
adding clusters? information for sentences without
and with POS errors were similar (0.91 and 1.33,
repectively). This aspect deserves further inves-
tigation, as the improvements seem to be related
to both the type of semantic information and the
parsing algorithm.We did an initial exploration but
it did not give any clear indication of the types of
improvements that could be expected using each
parser and semantic data.
5 Conclusions
This work has tried to shed light on the contribu-
tion of semantic information to dependency pars-
ing. The experiments were thorough, testing two
treebank conversions and three parsing paradigms
on automatically predicted POS tags. Compared
to (Agirre et al, 2011), which used MaltParser on
the LTH conversion and gold POS tags, our results
can be seen as a negative outcome, as the improve-
ments are very small and non-significant in most
of the cases. For parser combination, WordNet
semantic file information does give a small sig-
nificant increment in the more fine-grained LTH
representation. In addition we show that the im-
provement of automatic clusters is also weak. For
the future, we think tdifferent parsers, eitherhat a
more elaborate scheme is needed for word classes,
requiring to explore different levels of generaliza-
tion in the WordNet (or alternative) hierarchies.
Acknowledgments
This research was supported by the the Basque
Government (IT344- 10, S PE11UN114), the Uni-
versity of the Basque Country (GIU09/19) and
the Spanish Ministry of Science and Innovation
(MICINN, TIN2010-20218).
653
References
Eneko Agirre, Timothy Baldwin, and David Martinez.
2008. Improving parsing and PP attachment per-
formance with sense information. In Proceedings
of ACL-08: HLT, pages 317?325, Columbus, Ohio,
June. Association for Computational Linguistics.
Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, and
Joakim Nivre. 2011. Improving dependency pars-
ing with semantic classes. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 699?703, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Daniel M. Bikel. 2004. Intricacies of collins? parsing
model. Computational Linguistics, 30(4):479?511.
Marie Candito and Djam?e Seddah. 2010. Pars-
ing word clusters. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?84, Los
Angeles, CA, USA, June. Association for Computa-
tional Linguistics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, NAACL 2000, pages
132?139, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1?8. Associ-
ation for Computational Linguistics, July.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Lin-
guistics, 29(4):589?637, December.
Sanae Fujita, Francis Bond, Stephan Oepen, and
Takaaki Tanaka. 2010. Exploiting semantic infor-
mation for hpsg parse selection. Research on Lan-
guage and Computation, 8(1):122.
Johan Hall, Jens Nilsson, Joakim Nivre, Glsen Eryigit,
Beta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? a study in multi-
lingual parser optimization. In Proceedings of the
CoNLL Shared Task EMNLP-CoNLL.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proceedings of NODALIDA 2007, pages
105?112, Tartu, Estonia, May 25-26.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1?11, Uppsala, Sweden,
July. Association for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT, pages 595?603,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Andrew MacKinlay, Rebecca Dridan, Diana McCarthy,
and Timothy Baldwin. 2012. The effects of seman-
tic annotations on precision parse ranking. In First
Joint Conference on Lexical and Computational Se-
mantics (*SEM), page 228236, Montreal, Canada,
June. Association for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses
in untagged text. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 279?286, Barcelona,
Spain, July.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
R. McDonald, K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proceedings of CoNLL 2006.
Jens Nilsson, Joakim Nivre, and Johan Hall. 2008.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of the 45th Con-
ference of the ACL.
Joakim Nivre, Johan Hall, Jens Nilsson, Chanev A.,
Glsen Eryiit, Sandra Kbler, Marinov S., and Edwin
Marsi. 2007. Maltparser: A language-independent
system for data-driven dependency parsing. Natural
Language Engineering.
Kenji Sagae and Andrew Gordon. 2009. Clustering
words by syntactic similarity improves dependency
parsing of predicate-argument structures. In Pro-
ceedings of the Eleventh International Conference
on Parsing Technologies.
Kenji Sagae and Alon Lavie. 2006. Parser com-
bination by reparsing. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Proceedings of the North Ameri-
can Chapter of the Association for Computational
Linguistics Conference (NAACL-2010), Los Ange-
les, CA, June.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of semi-
supervised structured conditional models for depen-
dency parsing. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 551?560, Singapore, August. As-
sociation for Computational Linguistics.
654
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 477?
487, Montr?eal, Canada, June. Association for Com-
putational Linguistics.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of the 8th IWPT, pages 195?
206. Association for Computational Linguistics.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562?
571, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188?193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
655
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 791?796,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Punctuation Processing for Projective Dependency Parsing
?
Ji Ma
?
, Yue Zhang
?
and Jingbo Zhu
?
?
?
Northeastern University, Shenyang, China
?
Singapore University of Technology and Design, Singapore
?
Hangzhou YaTuo Company, 358 Wener Rd., Hangzhou, China, 310012
majineu@gmail.com
yue zhang@sutd.edu.sg
zhujingbo@mail.neu.edu.cn
Abstract
Modern statistical dependency parsers as-
sign lexical heads to punctuations as well
as words. Punctuation parsing errors lead
to low parsing accuracy on words. In this
work, we propose an alternative approach
to addressing punctuation in dependency
parsing. Rather than assigning lexical
heads to punctuations, we treat punctu-
ations as properties of their neighbour-
ing words, used as features to guide the
parser to build the dependency graph. In-
tegrating our method with an arc-standard
parser yields a 93.06% unlabelled attach-
ment score, which is the best accuracy by
a single-model transition-based parser re-
ported so far.
1 Introduction
The task of dependency parsing is to identify the
lexical head of each of the tokens in a string.
Modern statistical parsers (McDonald et al, 2005;
Nivre et al, 2007; Huang and Sagae, 2010; Zhang
and Nivre, 2011) treat all the tokens equally, as-
signing lexical heads to punctuations as well as
words. Punctuations arguably play an important
role in syntactic analysis. However, there are a
number of reasons that it is not necessary to parse
punctuations:
First, the lexical heads of punctuations are not
as well defined as those of words. Consequently,
punctuations are not as consistently annotated in
treebanks as words, making it harder to parse
punctuations. For example, modern statistical
parsers achieve above 90% unlabelled attachment
score (UAS) on words. However, the UAS on
punctuations are generally below 85%.
?
This work was done while the first author was visiting
SUTD
Moreover, experimental results showed that
parsing accuracy of content words drops on sen-
tences which contain higher ratios of punctuations.
One reason for this result is that projective de-
pendency parsers satisfy the ?no crossing links?
constraint, and errors in punctuations may pre-
vent correct word-word dependencies from being
created (see section 2). In addition, punctuations
cause certain type of features inaccurate. Take va-
lency features for example, previous work (Zhang
and Nivre, 2011) has shown that such features are
important to parsing accuracy, e.g., it may inform
the parser that a verb already has two objects at-
tached to it. However, such information might
be inaccurate when the verb?s modifiers contain
punctuations.
Ultimately, it is the dependencies between
words that provide useful information for real
world applications. Take machine translation or
information extraction for example, most systems
take advantage of the head-modifier relationships
between word pairs rather than word-punctuation
pairs to make better predictions. The fact that most
previous work evaluates parsing accuracies with-
out taking punctuations into account is also largely
due to this reason.
Given the above reasons, we propose an alterna-
tive approach to punctuation processing for depen-
dency parsing. In this method, punctuations are
not associated with lexical heads, but are treated
as properties of their neighbouring words.
Our method is simple and can be easily incor-
porated into state-of-the-art parsers. In this work,
we report results on an arc-standard transition-
based parser. Experiments show that our method
achieves about 0.90% UAS improvement over the
greedy baseline parser on the standard Penn Tree-
bank test set. Although the improvement becomes
smaller as the beam width grows larger, we still
achieved 93.06% UAS with a beam of width 64,
which is the best result for transition-based parsers
791
Length 1 ? 20 21? 40 41? 60
Punc % 0 ? 15 15 ? 30 > 30 0 ? 15 15 ? 30 > 30 0 ? 15 15 ? 30 > 30
E-F 94.56 92.88 87.67 91.84 91.82 83.87 89.83 88.01 ?
A-S 93.87 92.00 90.05 90.81 90.15 75.00 88.06 88.89 ?
A-S-64 95.28 94.43 88.15 92.96 92.63 76.61 90.78 88.76 ?
MST 94.90 93.55 88.15 92.45 93.11 77.42 90.89 89.77 ?
Table 2: Parsing accuracies vs punctuation ratios, on the development set
System E-F A-S A-S-64 MST
Dev UAS 91.83 90.71 93.02 92.56
Test UAS 91.75 90.34 92.84 92.10
Dev UAS-p 83.20 79.69 84.80 84.42
Test UAS-p 84.67 79.64 87.80 85.67
Dev
?
UAS 90.64 89.55 91.87 90.11
Test
?
UAS 90.40 89.33 91.75 89.82
Table 1: Parsing accuracies. ?E-F? and ?MST? de-
note easy-first parser and MSTparser, respectively.
?A-S? and ?A-S 64? denote our arc-standard parser
with beam width 1 and 64, respectively. ?UAS?
and ?UAS-p? denote word and punctuation unla-
belled attachment score, respectively. ?
?
? denotes
the data set with punctuations removed.
reported so far. Our code will be available at
https://github.com/majineu/Parser/Punc/A-STD.
2 Influence of Punctuations on Parsing
In this section, we conduct a set of experiments to
show the influence of punctuations on dependency
parsing accuracies.
2.1 Setup
We use the Wall Street Journal portion of the Penn
Treebank with the standard splits: sections 02-21
are used as the training set; section 22 and sec-
tion 23 are used as the development and test set,
respectively. Penn2Malt is used to convert brack-
eted structures into dependencies. We use our own
implementation of the Part-Of-Speech (POS) tag-
ger proposed by Collins (2002) to tag the devel-
opment and test sets. Training set POS tags are
generated using 10-fold jack-knifing. Parsing ac-
curacy is evaluated using unlabelled attachment
score (UAS), which is the percentage of words that
are assigned the correct lexical heads.
To show that the influence of punctuations
on parsing is independent of specific pars-
ing algorithms, we conduct experiments us-
ing three parsers, each representing a different
parsing methodology: the open source MST-
Parser
1
(McDonald and Pereira, 2006), our own
re-implementation of an arc-standard transition-
based parser (Nivre, 2008), which is trained us-
ing global learning and beam-search (Zhang and
Clark, 2008) with a rich feature set (Zhang and
Nivre, 2011)
2
, and our own re-implementation of
the easy-first parser (Goldberg and Elhadad, 2010)
with an extended feature set (Ma et al, 2013).
2.2 Punctuations and Parsing Accuracy
Our first experiment is to show that, compared
with words, punctuations are more difficult to
parse and to learn. To see this, we evaluate the
parsing accuracies of the selected parsers on words
and punctuations, separately. Results are listed
in Table 1, where row 2 and row 3 list the UAS
of words (all excluding punctuations) on the de-
velopment and test set, respectively. Row 4 and
row 5 list accuracies of punctuations (all excluding
words) on the development and test set, respec-
tively. We can see that although all the parsers
achieve above 90% UAS on words, the UAS on
punctuations are mostly below 85%.
As for learning, we calculate the percentage of
parameter updates that are caused by associating
punctuations with incorrect heads during training
of the easy-first parser
3
. The result is that more
than 31% of the parameter updates are caused due
to punctuations, though punctuations account for
only 11.6% of the total tokens in the training set.
The fact that parsers achieve low accuracies on
punctuations is to some degree expected, because
the head of a punctuation mark is linguistically
less well-defined. However, a related problem is
1
We trained a second order labelled parser with all the
configurations set to the default value. The code is publicly
available at http://sourceforge.net/projects/mstparser/
2
Some feature templates in Zhang and Nivre (2011) in-
volve head word and head POS tags which are not avail-
able for an arc-standard parser. Interestingly, without those
features our arc-standard parser still achieves 92.84% UAS
which is comparable to the 92.90% UAS obtained by the arc-
eager parser of Zhang and Nivre (2011)
3
For the greedy easy-first parser, whether a parameter up-
date is caused by punctuation error can be determined with
no ambiguity.
792
Figure 1: Illustration of processing paired punctuation. The property of a word is denoted by the punc-
tuation below that word.
that parsing accuracy on words tends to drop on
the sentences which contain high ratio of punc-
tuations. To see this, we divide the sentences in
the development set into sub-sets according the
punctuation ratio (percentage of punctuations that
a sentence contains), and then evaluate parsing ac-
curacies on the sub-sets separately.
The results are listed in Table 2. Since long
sentences are inherently more difficult to parse,
to make a fair comparison, we further divide the
development set according to sentence lengths as
shown in the first row
4
. We can see that most of the
cases, parsing accuracies drop on sentences with
higher punctuation ratios. Note that this negative
effect on parsing accuracy might be overlooked
since most previous work evaluates parsing accu-
racy without taking punctuations into account.
By inspecting the parser outputs, we found that
error propagation caused by assigning incorrect
head to punctuations is one of the main reason that
leads to this result. Take the sentence shown in
Figure 1 (a) for example, the word Mechanisms
is a modifier of entitled according to the gold ref-
erence. However, if the quotation mark, ?, is in-
correctly recognized as a modifier of was, due to
the ?no crossing links? constraint, the arc between
Mechanisms and entitled can never be created.
A natural question is whether it is possible to
reduce such error propagation by simply remov-
ing all punctuations from parsing. Our next ex-
periment aims at answering this question. In this
experiment, we first remove all punctuations from
the original data and then modify the dependency
arcs accordingly in order to maintain word-word
dependencies in the original data. We re-train the
parsers on the modified training set and evaluate
4
1694 out of 1700 sentences on the development set with
length no larger than 60 tokens
parsing accuracies on the modified data.
Results are listed in row 6 and row 7 of Table 1.
We can see that parsing accuracies on the modified
data drop significantly compared with that on the
original data. The result indicates that by remov-
ing punctuations, we lose some information that is
important for dependency parsing.
3 Punctuation as Properties
In our method, punctuations are treated as prop-
erties of its neighbouring words. Such properties
are used as additional features to guide the parser
to construct the dependency graph.
3.1 Paired Punctuation
Our method distinguishes paired punctuations
from other punctuations. Here paired punctuations
include brackets and quotations marks, whose
Penn Treebank POS tags are the following four:
-LRB- -RRB- ? ?
The characteristics of paired punctuations include:
(1) they typically exist in pairs; (2) they serve as
boundaries that there is only one dependency arc
between the words inside the boundaries and the
words outside. Take the sentence in Figure 1 (a)
for example, the only arc cross the boundary is
(Mechanisms, entitled) where entitled is the head.
To utilize such boundary information, we fur-
ther classify paired punctuations into two cate-
gories: those that serve as the beginning of the
boundary, whose POS tags are either -LRB- or ?,
denoted by BPUNC; and those that serve as the end
of the boundary, denoted by EPUNC.
Before parsing starts, a preprocessing step is
used to first attach the paired punctuations as
properties of their neighbouring words, and then
remove them from the sentence. In particular,
793
unigram for p in ?
0
, ?
1
, ?
2
, ?
3
, ?
0
, ?
1
, ?
2
p
punc
for p in ?
0
, ?
1
, ?
2
, ?
0
, ?
1
p
punc
 p
w
, p
punc
 p
t
bigram for p, q in (?
0
, ?
0
), (?
0
, ?
1
), (?
0
, ?
2
), (?
0
, ?
1
), (?
0
, ?
2
) p
punc
 q
punc
, p
punc
 q
t
, p
punc
 q
w
for p, q in (?
2
, ?
0
), (?
1
, ?
0
), (?
2
, ?
0
) p
punc
 q
t
, p
punc
 p
t
 q
t
for p, q in (?
2
, ?
0
), (?
1
, ?
0
), (?
0
, ?
0
) d
pq
 p
punc
 p
t
 q
t
Table 3: Feature templates. For an element p either on ? or ? of an arc-standard parser, we use p
punc
,
p
w
and p
t
to denote the punctuation property, head word and head tag of p, respectively. d
pq
denotes the
distance between the two elements p and q.
we attach BPUNCs to their right neighbours and
EPUNCs to their left neighbours, as shown in Fig-
ure 1 (b). Note that in Figure 1 (a), the left neigh-
bour of ? is also a punctuation. In such cases, we
simply remove these punctuations since the exis-
tence of paired punctuations already indicates that
there should be a boundary.
During parsing, when a dependency arc with
lexical head w
h
is created, the property of w
h
is
updated by the property of its left (or right) most
child to keep track whether there is a BPUNC (or
EPUNC) to the left (or right) side of the sub-tree
rooted at w
h
, as shown in Figure 1 (c). When
BPUNCs and EPUNCs meet each other at w
h
, a
PAIRED property is assigned to w
h
to capture that
the words within the paired punctuations form a
sub-tree, rooted at w
h
. See Figure 1 (d).
3.2 Practical Issues
It is not uncommon that two BPUNCS appear ad-
jacent to each other. For example,
(?Congress?s Environmental Buccaneers,?
Sept. 18).
In our implementation, BPUNC or EPUNC prop-
erties are implemented using flags. In the exam-
ple, we set two flags ? and ( on the word Con-
grees?s. When BPUNC and EPUNC meet each
other, the corresponding flags are turned off. In
the example, when Congrees?s is identified as a
modifier of Buccaneers, the ? flag of Buccaneers
is turned off. However, we do not assign a PAIRED
property to Buccaneers since its ( flag is still on.
The PAIRED property is assigned only when all
the flags are turned off.
3.3 Non-Paired Punctuations
Though some types of non-paired punctuations
may capture certain syntactic patterns, we do not
make further distinctions between them, and treat
these punctuations uniformly for simplicity.
Before parsing starts and after the preprocessing
step for paired punctuations, our method employs
a second preprocessing step to attach non-paired
punctuations to their left neighbouring words. It
is guaranteed that the property of the left neigh-
bouring words of non-paired punctuations must be
empty. Otherwise, it means the non-paired punc-
tuation is adjacent to a paired punctuation. In
such cases, the non-paired punctuation would be
removed in the first processing step.
During parsing, non-paired punctuations are
also passed bottom-up: the property of w
h
is up-
dated by its right-most dependent to keep track
whether there is a punctuation to the right side
of the tree rooted at w
h
. The only special case is
that ifw
h
already contains a BPUNC property, then
our method simply ignores the non-paired prop-
erty since we maintain the boundary information
with the highest priority.
3.4 Features
We incorporate our method into the arc-standard
transition-based parser, which uses a stack ? to
maintain partially constructed trees and a buffer ?
for the incoming words (Nivre, 2008). We design
a set of features to exploit the potential of using
punctuation properties for the arc-standard parser.
The feature templates are listed in Table 3.
In addition to the features designed for paired
punctuations, such as bigram punctuation features
listed in line 3 of Table 3, we also design features
for non-paired punctuations. For example, the dis-
tance features in line 5 of Table 3 is used to capture
the pattern that if a word w with comma property
is the left modifier of a noun or a verb, the distance
between w and its lexical head is often larger than
1. In other words, they are not adjacent.
4 Results
Our first experiment is to investigate the effect of
processing paired punctuations on parsing accu-
racy. In this experiment, the method introduced
in Section 3.1 is used to process paired punctua-
tions, and the non-paired punctuations are left un-
794
s Baseline Paired All
1 90.76 91.25 91.47
2 91.88 92.06 92.34
4 92.50 92.61 92.70
8 92.73 92.76 92.82
16 92.90 92.94 92.99
64 92.99 93.04 93.10
Table 4: Parsing accuracies on the development
set. s denotes the beam width.
touched. Feature templates used in this experi-
ment are those listed in the top three rows of Ta-
ble 3 together with those used for the baseline arc-
standard parser.
Results on the development set are shown in the
second column of Table 4. We can see that when
the beam width is set to 1, our method achieves an
0.49 UAS improvement. By comparing the out-
puts of the two parsers, two types of errors made
by the baseline parser are effectively corrected.
The first is that our method is able to cap-
ture the pattern that there is only one depen-
dency arc between the words within the paired-
punctuations and the words outside, while the
baseline parser sometimes creates more depen-
dency arcs that cross the boundary.
The second is more interesting. Our method is
able to capture that the root, w
h
, of the sub-tree
within the paired-punctuation, such as ?Mecha-
nisms? in Figure 1, generally serves as a modifier
of the words outside, while the baseline parser oc-
casionally make w
h
as the head of the sentence.
As we increase the beam width, the improve-
ment of our method over the baseline becomes
smaller. This is as expected, since beam search
also has the effect of reducing error propagation
(Zhang and Nivre, 2012), thereby alleviating the
errors caused by punctuations.
In the last experiment, we examine the effect
of incorporating all punctuations using the method
introduced in Section 2. In this experiment, we
use all the feature templates in Table 3 and those
in the baseline parser. Results are listed in the
fourth column of Table 4, which shows that pars-
ing accuracies can be further improved by also
processing non-paired punctuations. The overall
accuracy improvement when the beam width is 1
reaches 0.91%. The extra improvements mainly
come from better accuracies on the sentences with
comma. However, the exact type of errors that
are corrected by using non-paired punctuations is
more difficult to summarize.
system UAS Comp Root
Baseline 90.38 37.71 89.45
All-Punc 91.32 41.35 92.43
Baseline-64 92.84 46.90 95.57
All-Punc-64 93.06 48.55 95.53
Huang 10 92.10 ? ?
Zhang 11 92.90 48.00 91.80
Choi 13 92.96 ? ?
Bohnet 12 93.03 ? ?
Table 5: Final result on the test set.
The final results on the test set are listed in Ta-
ble 5
5
. Table 5 also lists the accuracies of state-
of-the-art transition-based parsers. In particular,
?Huang 10? and ?Zhang 11? denote Huang and
Sagae (2010) and Zhang and Nivre (2011), re-
spectively. ?Bohnet 12? and ?Choi 13? denote
Bohnet and Nivre (2012) and Choi and Mccal-
lum (2013), respectively. We can see that our
method achieves the best accuracy for single-
model transition-based parsers.
5 Conclusion and Related Work
In this work, we proposed to treat punctuations
as properties of context words for dependency
parsing. Experiments with an arc-standard parser
showed that our method effectively improves pars-
ing performance and we achieved the best accu-
racy for single-model transition-based parser.
Regarding punctuation processing for depen-
dency parsing, Li et al (2010) proposed to uti-
lize punctuations to segment sentences into small
fragments and then parse the fragments separately.
A similar approach is proposed by Spitkovsky et
al. (2011) which also designed a set of constraints
on the fragments to improve unsupervised depen-
dency parsing.
Acknowledgements
We highly appreciate the anonymous reviewers
for their insightful suggestions. This research
was supported by the National Science Founda-
tion of China (61272376; 61300097; 61100089),
the Fundamental Research Funds for the Cen-
tral Universities (N110404012), the research grant
T2MOE1301 from Singapore Ministry of Ed-
ucation (MOE) and the start-up grant SRG
ISTD2012038 from SUTD.
5
The number of training iteration is determined using the
development set.
795
References
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL ?12, pages 1455?1465, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jinho D. Choi and Andrew Mccallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10, EMNLP
?02, pages 1?8, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 742?750, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Jan Hajic, Sandra Carberry, and Stephen Clark, ed-
itors, ACL, pages 1077?1086. The Association for
Computer Linguistics.
Zhenghua Li, Wanxiang Che, and Ting Liu. 2010. Im-
proving dependency parsing using punctuation. In
Minghui Dong, Guodong Zhou, Haoliang Qi, and
Min Zhang, editors, IALP, pages 53?56. IEEE Com-
puter Society.
Ji Ma, Jingbo Zhu, Tong Xiao, and Nan Yang. 2013.
Easy-first pos tagging and dependency parsing with
beam search. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 110?114,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-2006)), volume 6, pages
81?88.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?05, pages 91?98, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513?553.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2011. Punctuation: Making a point in un-
supervised dependency parsing. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning (CoNLL-2011).
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562?
571, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188?193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Yue Zhang and Joakim Nivre. 2012. Analyzing
the effect of global learning and beam-search on
transition-based dependency parsing. In Proceed-
ings of COLING 2012: Posters, pages 1391?1400,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
796
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials, pages 13?15,
Baltimore, Maryland, USA, 22 June 2014.
c?2014 Association for Computational Linguistics
Incremental Structured Prediction Using a Global Learning and
Beam-Search Framework
Yue Zhang
?
, Meishan Zhang
?
, Ting Liu
?
?
Singapore University of Technology and Design
yue zhang@sutd.edu.sg
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{mszhang, tliu}@ir.hit.edu.cn
Abstract
This tutorial discusses a framework for in-
cremental left-to-right structured predica-
tion, which makes use of global discrimi-
native learning and beam-search decoding.
The method has been applied to a wide
range of NLP tasks in recent years, and
achieved competitive accuracies and effi-
ciencies. We give an introduction to the
algorithms and efficient implementations,
and discuss their applications to a range of
NLP tasks.
1 Introduction
This tutorial discusses a framework of online
global discriminative learning and beam-search
decoding for syntactic processing (Zhang and
Clark, 2011b), which has recently been applied
to a wide variety of natural language processing
(NLP) tasks, including word segmentation (Zhang
and Clark, 2007), dependency parsing (Zhang and
Clark, 2008b; Huang and Sagae, 2010; Zhang and
Nivre, 2011; Bohnet and Kuhn, 2012), context
free grammar (CFG) parsing (Collins and Roark,
2004; Zhang and Clark, 2009; Zhu et al., 2013),
combinational categorial grammar (CCG) parsing
(Zhang and Clark, 2011a; Xu et al., 2014) and
machine translation (Liu, 2013), achieving state-
of-the-art accuracies and efficiencies. In addition,
due to its high efficiencies, it has also been ap-
plied to a range of joint structural problems, such
as joint segmentation and POS-tagging (Zhang
and Clark, 2008a; Zhang and Clark, 2010), joint
POS-tagging and dependency parsing (Hatori et
al., 2011; Bohnet and Nivre, 2012), joint mor-
phological analysis, POS-tagging and dependency
parsing (Bohnet et al., 2013), and joint segmenta-
tion, POS-tagging and parsing (Zhang et al., 2013;
Zhang et al., 2014).
In addition to the aforementioned tasks, the
framework can be applied to all structural pre-
diction tasks for which the output can be con-
structed using an incremental process. The advan-
tage of this framework is two-fold. First, beam-
search enables highly efficient decoding, which
typically has linear time complexity, depending on
the incremental process. Second, free from DP-
style constraints and Markov-style independence
assumptions, the framework allows arbitrary fea-
tures to be defined to capture structural patterns.
In addition to feature advantages, the high accura-
cies of this framework are also enabled by direct
interactions between learning and search (Daum?e
III and Marcu, 2005; Huang et al., 2012; Zhang
and Nivre, 2012).
2 Tutorial Overview
In this tutorial, we make an introduction to the
framework, illustrating how it can be applied to
a range of NLP problems, giving theoretical dis-
cussions and demonstrating a software implemen-
tation. We start with a detailed introduction of
the framework, describing the averaged percep-
tron algorithm (Collins, 2002) and its efficient im-
plementation issues (Zhang and Clark, 2007), as
well as beam-search and the early-update strategy
(Collins and Roark, 2004). We then illustrate how
the framework can be applied to NLP tasks, in-
cluding word segmentation, joint segmentation &
POS-tagging, labeled and unlabeled dependency
parsing, joint POS-tagging and dependency pars-
ing, CFG parsing, CCG parsing, and joint segmen-
tation, POS-tagging and parsing. In each case, we
illustrate how the task is turned into an incremen-
tal left-to-right output-building process, and how
rich features are defined to give competitive accu-
racies. These examples can serve as guidance in
applying the framework to other structural predic-
tion tasks.
In the second part of the tutorial, we give
some analysis on why the framework is effective.
We discuss several alternative learning algorithms,
13
and compare beam-search with greedy search on
dependency parsing. We show that accuracy bene-
fits from interaction between learning and search.
Finally, the tutorial concludes with an introduction
to ZPar, an open source toolkit that provides op-
timized C++ implementations of of all the above
tasks.
3 Outline
1 Introduction (0.5 hours)
1.1 An overview of the syntactic processing
framework and its applications
1.2 An introduction to the beam-search
framework and comparison to dynamic
programming
1.3 Algorithm in details
1.3.1 Online discriminative learning using
the perceptron
1.3.2 Beam-search decoding
1.3.3 The integrated framework
2 Applications (1.25 hours)
2.1 Overview
2.2 Word segmentation
2.3 Joint segmentation and POS-tagging
2.4 Dependency parsing
2.5 Context free grammar parsing
2.6 Combinatory categorial grammar pars-
ing
2.7 Joint segmentation, POS-tagging and
parsing
3 Analysis of the framework (0.75 hours)
3.1 The influence of global learning
3.2 The influence of beam-search
3.3 Benefits from the combination
3.4 Related discussions
4 The ZPar software tool (0.5 hours)
4 About the Presenters
Yue Zhang is an Assistant Professor at Singapore
University of Technology and Design (SUTD).
Before joining SUTD in 2012, he worked as a
postdoctoral research associate at University of
Cambridge. He received his PhD and MSc degrees
from University of Oxford, and undergraduate de-
gree from Tsinghua University, China. Dr Zhang?s
research interest includes natural language pars-
ing, natural language generation, machine trans-
lation and machine learning.
Meishan Zhang is a fifth-year Phd candidate at
Research Center for Social Computing and Infor-
mation Retrieval, Harbin Institute of Technology,
China (HIT-SCIR). His research interest includes
Chinese morphological and syntactic parsing, se-
mantic representation and parsing, joint modelling
and machine learning.
Ting Liu is a professor at HIT-SCIR. His re-
search interest includes social computing, infor-
mation retrieval and natural language processing.
References
Bernd Bohnet and Jonas Kuhn. 2012. The best of
bothworlds ? a graph-based completion model for
transition-based parsers. In Proceedings of EACL,
pages 77?87, Avignon, France, April. Association
for Computational Linguistics.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of EMNLP, pages 1455?1465, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky,
Richard Farkas, Filip Ginter, and Jan Hajic. 2013.
Joint morphological and syntactic analysis for richly
inflected languages. Transactions of the Association
for Computational Linguistics, 1:415?428.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL 2004, Main Volume, pages 111?118,
Barcelona, Spain, July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of EMNLP, pages 1?8. Association for Computa-
tional Linguistics, July.
Hal Daum?e III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
methods for structured prediction. In International
Conference on Machine Learning (ICML), Bonn,
Germany.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2011. Incremental joint pos tagging
and dependency parsing in chinese. In Proceedings
of IJCNLP, pages 1216?1224, Chiang Mai, Thai-
land, November. Asian Federation of Natural Lan-
guage Processing.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of ACL 2010, pages 1077?1086, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
14
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of NAACL 2012, pages 142?151, Montr?eal,
Canada, June. Association for Computational Lin-
guistics.
Yang Liu. 2013. A shift-reduce parsing algorithm for
phrase-based string-to-dependency translation. In
Proceedings of the ACL, pages 1?10, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Wenduan Xu, Yue Zhang, and Stephen Clark. 2014.
Shift-reduce ccg parsing with a dependency model.
In Proceedings of the ACL.
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-based perceptron algorithm. In
Proceedings of ACL 2007, pages 840?847, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Yue Zhang and Stephen Clark. 2008a. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of ACL-08: HLT, pages 888?
896, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Yue Zhang and Stephen Clark. 2008b. A tale of
two parsers: Investigating and combining graph-
based and transition-based dependency parsing. In
Proceedings of EMNLP, pages 562?571, Honolulu,
Hawaii, October. Association for Computational
Linguistics.
Yue Zhang and Stephen Clark. 2009. Transition-
based parsing of the chinese treebank using a global
discriminative model. In Proceedings of IWPT?09,
pages 162?171, Paris, France, October. Association
for Computational Linguistics.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging us-
ing a single discriminative model. In Proceedings
of EMNLP 2010, pages 843?852, Cambridge, MA,
October. Association for Computational Linguistics.
Yue Zhang and Stephen Clark. 2011a. Shift-reduce
ccg parsing. In Proceedings of ACL 2011, pages
683?692, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Yue Zhang and Stephen Clark. 2011b. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL 2011, pages 188?193, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Yue Zhang and Joakim Nivre. 2012. Analyzing
the effect of global learning and beam-search on
transition-based dependency parsing. In Proceed-
ings of COLING 2012: Posters, pages 1391?1400,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2013. Chinese parsing exploiting characters.
In Proceedings of ACL 2013.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2014. Character-level chinese dependency
parsing. In Proceedings of the ACL.
Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of ACL
2013.
15
