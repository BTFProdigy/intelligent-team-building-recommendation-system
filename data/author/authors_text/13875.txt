Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 195?200,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Applying morphological decomposition to statistical machine translation
Sami Virpioja and Jaakko Va?yrynen and Andre? Mansikkaniemi and Mikko Kurimo
Aalto University School of Science and Technology
Department of Information and Computer Science
PO BOX 15400, 00076 Aalto, Finland
{svirpioj,jjvayryn,ammansik,mikkok}@cis.hut.fi
Abstract
This paper describes the Aalto submission
for the German-to-English and the Czech-
to-English translation tasks of the ACL
2010 Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR.
Statistical machine translation has focused
on using words, and longer phrases con-
structed from words, as tokens in the sys-
tem. In contrast, we apply different mor-
phological decompositions of words using
the unsupervised Morfessor algorithms.
While translation models trained using the
morphological decompositions did not im-
prove the BLEU scores, we show that the
Minimum Bayes Risk combination with
a word-based translation model produces
significant improvements for the German-
to-English translation. However, we did
not see improvements for the Czech-to-
English translations.
1 Introduction
The effect of morphological variation in languages
can be alleviated by using word analysis schemes,
which may include morpheme discovery, part-of-
speech tagging, or other linguistic information.
Words are very convenient and even efficient rep-
resentation in statistical natural language process-
ing, especially with English, but morphologically
rich languages can benefit from more fine-grained
information. For instance, statistical morphs dis-
covered with unsupervised methods result in bet-
ter performance in automatic speech recognition
for highly-inflecting and agglutinative languages
(Hirsima?ki et al, 2006; Kurimo et al, 2006).
Virpioja et al (2007) applied morph-based
models in statistical machine translation (SMT)
between several language pairs without gaining
improvement in BLEU score, but obtaining re-
ductions in out-of-vocabulary rates. They uti-
lized morphs both in the source and in the tar-
get language. Later, de Gispert et al (2009)
showed that Minimum Bayes Risk (MBR) com-
bination of word-based and morph-based trans-
lation models improves translation with Arabic-
to-English and Finnish-to-English language pairs,
where only the source language utilized morph-
based models. Similar results have been shown for
Finnish-to-English and Finnish-to-German in per-
formance evaluation of various unsupervised mor-
pheme analysis algorithms in Morpho Challenge
2009 competition (Kurimo et al, 2009).
We continue the research described above and
examine how the level of decomposition affects
both the individual morph-based systems and
MBR combinations with the baseline word-based
model. Experiments are conducted with the
WMT10 shared task data for German-to-English
and Czech-to-English language pairs.
2 Methods
In this work, morphological analyses are con-
ducted on the source language data, and each dif-
ferent analysis is applied to create a unique seg-
mentation of words into morphemes. Translation
systems are trained with the Moses toolkit (Koehn
et al, 2007) from each differently segmented ver-
sion of the same source language to the target lan-
guage. Evaluation with BLEU is performed on
both the individual systems and system combina-
tions, using different levels of decomposition.
2.1 Morphological models for words
Morfessor (Creutz and Lagus, 2002; Creutz and
Lagus, 2007, etc.) is a family of methods for
unsupervised morphological segmentation. Mor-
fessor does not limit the number of morphemes
for each word, making it suitable for agglutina-
tive and compounding languages. An analysis of a
single word is a list of non-overlapping segments,
195
morphs, stored in the model lexicon. We use both
the Morfessor Baseline (Creutz and Lagus, 2005b)
and the Morfessor Categories-MAP (Creutz and
Lagus, 2005a) algorithms.1 Both are formulated
in a maximum a posteriori (MAP) framework, i.e.,
the learning algorithm tries to optimize the prod-
uct of the model prior and the data likelihood.
The generative model applied by Morfessor
Baseline assumes that the morphs are independent.
The resulting segmentation can be influenced by
using explicit priors for the morph lengths and
frequencies, but their effect is usually minimal.
The training data has a larger effect on the re-
sults: A larger data set alows a larger lexicon,
and thus longer morphs and less morphs per word
(Creutz and Lagus, 2007). Moreover, the model
can be trained with or without taking into account
the word frequencies. If the frequencies are in-
cluded, the more frequent words are usually un-
dersegmented compared to a linguistic analysis,
whereas the rare words are oversegmented (Creutz
and Lagus, 2005b). An easy way to control the
amount of segmentation is to weight the training
data likelihood by a positive factor ?. If ? > 1,
the increased likelihood results in longer morphs.
If ? < 1, the morphs will be shorter and the words
more segmented.
Words that are not present in the training data
can be segmented using an algorithm similar to
Viterbi. The algorithm can be modified to allow
new morphs types to be used by using an approx-
imative cost of adding them into the lexicon (Vir-
pioja and Kohonen, 2009). The modification pre-
vents oversegmentation of unseen word forms. In
machine translation, this is important especially
for proper nouns, for which there is usually no
need for translation.
The Morfessor Categories-MAP algorithm ex-
tends the model by imposing morph categories of
stems, prefixes and suffixes, as well as transition
probabilities between them. In addition, it applies
a hierarchical segmentation model that allows it to
construct new stems from smaller pieces of ?non-
morphemes? (Creutz and Lagus, 2007). Due to
these features, it can provide reasonable segmen-
tations also for those words that contain new mor-
phemes. The drawback of the more sophisticated
model is the slower and more complex training al-
gorithm. In addition, the amount of the segmenta-
1The respective software is available at http://www.
cis.hut.fi/projects/morpho/
tion is harder to control.
Morfessor Categories-MAP was applied to sta-
tistical machine translation by Virpioja et al
(2007) and de Gispert et al (2009). However,
Kurimo et al (2009) report that Morfessor Base-
line outperformed Categories-MAP in Finnish-to-
English and German-to-English tasks both with
and without MBR combination, although the dif-
ferences were not statistically significant. In all
the previous cases, the models were trained on
word types, i.e., without using their frequencies.
Here, we also test models trained on word tokens.
2.2 Statistical machine translation
We utilize the Moses toolkit (Koehn et al, 2007)
for statistical machine translation. The default pa-
rameter values are used except with the segmented
source language, where the maximum sentence
length is increased from 80 to 100 tokens to com-
pensate for the larger number of tokens in text.
2.3 Morphological model combination
For combining individual models, we apply Min-
imum Bayes Risk (MBR) system combination
(Sim et al, 2007). N-best lists from multiple
SMT systems trained with different morpholog-
ical analysis methods are merged; the posterior
distributions over the individual lists are interpo-
lated to form a new distribution over the merged
list. MBR hypotheses selection is then performed
using sentence-level BLEU score (Kumar and
Byrne, 2004).
In this work, the focus of the system combina-
tion is not to combine different translation systems
(e.g., Moses and Systran), but to combine systems
trained with the same translation algorithm using
the same source language data with with different
morphological decompositions.
3 Experiments
The German-to-English and Czech-to-English
parts of the ACL WMT10 shared task data were
investigated. Vanilla SMT models were trained
with Moses using word tokens for MBR combi-
nation and comparison purposes. Several different
morphological segmentation models for German
and Czech were trained with Morfessor. Each seg-
mentation model corresponds to a morph-based
SMT model trained with Moses. The word-based
vanilla Moses model is compared to each morph-
based model as well as to several MBR com-
196
binations between word-based translation models
and morph-based translation models. Quantitative
evaluation is carried out using the BLEU score
with re-cased and re-tokenized translations.
4 Data
The data used in the experiments consisted
of Czech-to-English (CZ-EN) and German-to-
English (DE-EN) parallel language data from
ACL WMT10. The data was divided into distinct
training, development, and evaluation sets. Statis-
tics and details are shown in Table 1.
Aligned data from Europarl v5 and News
Commentary corpora were included in training
German-to-English SMT models. The English
part from the same data sets was used for train-
ing a 5-gram language model, which was used in
all translation tasks. The Czech-to-English trans-
lation model was trained with CzEng v0.9 (train-
ing section 0) and News Commentary data. The
monolingual German and Czech parts of the train-
ing data sets were used for training the morph seg-
mentation models with Morfessor.
The data sets news-test2009, news-
syscomb2009 and news-syscombtune2010
from the ACL WMT 2009 and WMT 2010,
were used for development. The news-test2008,
news-test2010, and news-syscombtest2010 data
sets were used for evaluation.
4.1 Preprocessing
All data sets were preprocessed before use. XML-
tags were removed, text was tokenized and char-
acters were lowercased for every training, devel-
opment and evaluation set.
Morphological models for German and Czech
were trained using a corpus that was a combina-
tion of the respective training sets. Then the mod-
els were used for segmenting all the data sets, in-
cluding development and evaluation sets, with the
Viterbi algorithm discussed in Section 2.1. The
modification of allowing new morph types for out-
of-vocabulary words was not applied.
The Moses cleaning script performed additional
filtering on the parallel language training data.
Specifically, sentences with over 80 words were
removed from the vanilla Moses word-based mod-
els. For morph-based models the limit was set
to 100 morphs, which is the maximum limit of
the Giza++ alignment tool. After filtering with a
threshold of 100 tokens, the different morph seg-
mentations for DE-EN training data from com-
bined Europarl and News Commentary data sets
ranged from 1 613 556 to 1 624 070 sentences.
Similarly, segmented CZ-EN training data ranged
from 896 163 to 897 744 sentences. The vanilla
words-based model was trained with 1 609 998
sentences for DE-EN and 897 497 sentences for
CZ-EN.
5 Results
The details of the ACL WMT10 submissions are
shown in Table 2. The results of experiments with
different morphological decompositions and MBR
system combinations are shown in Table 3. The
significances of the differences in BLEU scores
between the word-based model (Words) and mod-
els with different morphological decompositions
was measured by dividing each evaluation data set
into 49 subsets of 41?51 sentences, and using the
one-sided Wilcoxon signed rank test (p < 0.05).
5.1 Segmentation
We created several word segmentations with Mor-
fessor baseline and Morfessor Categories-MAP
(CatMAP). Statistics for the different segmenta-
tions are given in Table 3. The amount of seg-
mentation was measured as the average number of
morphs per word (m/w) and as the percentage of
segmented words (s-%) in the training data. In-
creasing the data likelihood weight ? in Morfes-
sor Baseline increases the amount of segmentation
for both languages. However, it had little effect
on the proportion of segmented words in the three
evaluation data sets: The proportion of segmented
word tokens was 10?11 % for German and 8?9 %
for Czech, whereas the out-of-vocabulary rate was
7.5?7.8 % for German and 4.8?5.6 % for Czech.
Disregarding the word frequency information
in Morfessor Baseline (nofreq) produced more
morphs per word type and segmented nearly
all words in the training data. The Morfessor
CatMAP algorithm created segmentations with the
largest number of morphs per word, but did not
segment as many words as the Morfessor Baseline
without the frequencies.
5.2 Morph-based translation systems
The models with segmented source language per-
formed worse individually than the word-based
models. The change in the BLEU score was statis-
tically significant in almost all segmentations and
197
Data set Statistics Training Development Evaluation
Sentences Words per sentence SM LM TM
DE CZ EN DE CZ EN DE-EN CZ-EN {DE,CZ}-EN {DE,CZ}-EN
Europarl v5 1 540 549 23.2 25.2 x x x
News Commentary 100 269 21.9 18.9 21.5 x x x x x
CzEng v0.9 (training section 0) 803 286 8.3 9.9 x x
news-test2009 2 525 21.7 18.8 23.2 x
news-syscomb2009 502 19.7 17.2 21.1 x
news-syscombtune2010 455 20.2 17.3 21.0 x
news-test2008 2 051 20.3 17.8 21.7 x
news-test2010 2 489 21.7 18.4 22.3 x
news-syscombtest2010 2 034 22.0 18.6 22.6 x
Table 1: Data sets for the Czech-to-English and German-to-English SMT experiments, including the
number of aligned sentences and the average number of words per sentence in each language. The data
sets used for model training, development and evaluation are marked. Training is divided into German
(DE) and Czech (CZ) segmentation model (SM) training, English (EN) language model (LM) training
and German-to-English (DE-EN) and Czech-to-English (CZ-EN) translation model (TM) training.
Submission Segmentation model for source language BLEU-cased
(news-test2010)
aalto DE-EN WMT10 Morfessor Baseline (? = 0.5) 17.0
aalto DE-EN WMT10 CatMAP Morfessor Categories-MAP 16.5
aalto CZ-EN WMT10 Morfessor Baseline (? = 0.5) 16.2
aalto CZ-EN WMT10 CatMAP Morfessor Categories-MAP 15.9
Table 2: Our submissions for the ACL WMT10 shared task in translation. The translation models are
trained from the segmented source language into unsegmented target language with Moses.
all evaluation sets. Morfessor Baseline (? = 0.5)
was the best individual segmented model for both
German and Czech in the sense that it had the
lowest number of significant decreases the BLEU
score compared to the word-based model. Remov-
ing word frequency information with Morfessor
Baseline and using Morfessor CatMAP gave the
lowest BLEU scores with both source languages.
5.3 Translation system combination
For the DE-EN language pair, all MBR system
combinations between each segmented model and
the word-based model had slightly higher BLUE
scores than the individual word-based model.
Nearly all improvements were statistically signifi-
cant.
The BLEU scores for the MBR combinations
in the CZ-EN language pair were mostly not sig-
nificantly different from the individual word-based
model. Two scores were significantly lower.
6 Discussion
We have applied concatenative morphological
analysis, in which each original word token is seg-
mented into one or more non-overlapping morph
tokens. Our results with different levels of seg-
mentation with Morfessor suggest that the optimal
level of segmentation is language pair dependent
in machine translation.
Our approach for handling rich morphology has
not been able to directly improve the translation
quality. We assume that improvements might still
be possible by carefully tuning the amount of seg-
mentation. The experiments in this paper with
different values of the ? parameter for Morfes-
sor Baseline were conducted with the word fre-
quencies. The parameter had little effect on the
proportion of segmented words in the evaluation
data sets, as frequent words were not segmented
at all, and out-of-vocabulary words were likely to
be oversegmented by the Viterbi algorithm. Fu-
ture work includes testing a larger range of val-
ues for ?, also for models trained without the
word frequencies, and using the modification of
the Viterbi algorithm proposed in Virpioja and Ko-
honen (2009).
It might also be helpful to only segment selected
words, where the selection would be based on the
potential benefit in the translation process. In gen-
eral, the direct segmentation of words into morphs
is problematic because it increases the number
of tokens in the text and directly increases both
model training and decoding complexity. How-
ever, an efficient segmentation decreases the num-
ber of types and the out-of-vocabulary rate (Virpi-
oja et al, 2007).
We have replicated here the result that an MBR
combination of a morph-based MT system with
198
Segmentation (DE) Statistics (DE) BLEU-cased (DE-EN)
news-test2008 news-test2010 news-syscombtest2010
m/w s-% No MBR MBR with No MBR No MBR MBR with
Words Words
Words 1.00 0.0% 16.37 - 17.28 13.22 -
Morfessor Baseline (? = 0.5) 1.82 72.4% 15.19? 16.47+ 17.04? 13.28? 13.70+
Morfessor Baseline (? = 1.0) 1.65 61.0% 15.14? 16.54+ 16.87? 11.95? 13.66+
Morfessor Baseline (? = 5.0) 1.24 23.7% 15.04? 16.44? 16.63? 11.78? 13.43+
Morfessor CatMAP 2.25 67.5% 14.21? 16.42? 16.53? 11.15? 13.61+
Morfessor Baseline nofreq 2.24 91.6% 13.98? 16.47+ 16.36? 10.66? 13.58+
Segmentation (CZ) Statistics (CZ) BLEU-cased (CZ-EN)
news-test2008 news-test2010 news-syscombtest2010
m/w s-% No MBR MBR with No MBR No MBR MBR with
Words Words
Words 1.00 0.0% 14.91 - 16.73 12.75 -
Morfessor Baseline (? = 0.5) 1.19 17.7% 13.22? 14.87? 16.01? 12.60? 12.53?
Morfessor Baseline (? = 1.0) 1.09 8.1% 13.33? 14.88? 16.10? 11.29? 12.84?
Morfessor Baseline (? = 5.0) 1.03 2.9% 13.53? 14.83? 15.92? 11.17? 12.85?
Morfessor CatMAP 2.29 71.9% 11.93? 14.86? 15.79? 10.12? 10.79?
Morfessor Baseline nofreq 2.18 90.3% 12.43? 14.96? 15.82? 10.13? 12.89?
Table 3: Results for German-to-English (DE-EN) and Czech-to-English (CZ-EN) translation models.
The source language is segmented with the shown algorithms. The amount of segmentation in the train-
ing data is measured with the average number of morphs per word (m/w) and as proportion of segmented
words (s-%) against the word-based model (Words). The trained translation systems are evaluated in-
dependently (No MBR) and in Minimum Bayes Risk system combination of word-based translation
systems (MBR). Unchanged (?), significantly higher (+) and lower (?) BLEU scores compared to the
word-based translation model (Words) are marked. The best morph-based model for each column is
emphasized.
a word-based MT system can produce a BLEU
score that is higher than from either of the indi-
vidual systems (de Gispert et al, 2009; Kurimo
et al, 2009). With the DE-EN language pair, the
improvement was statistically significant with all
tested segmentation models. However, the im-
provements were not as large as those obtained
before and the results for the CZ-EN language
pair were not significantly different in most cases.
Whether this is due to the different languages,
training data sets, the domain of the evaluation
data sets, or some problems in the model training,
is currently uncertain.
One very different approach for applying dif-
ferent levels of linguistic analysis is factor mod-
els for SMT (Koehn and Hoang, 2007), where
pre-determined factors (e.g., surface form, lemma
and part-of-speech) are stored as vectors for each
word. This provides better integration of mor-
phosyntactic information and more control of the
process, but the translation models are more com-
plex and the number and factor types in each word
must be fixed.
Our submissions to the ACL WMT10 shared
task utilize unsupervised morphological decompo-
sition models in a straightforward manner. The
individual morph-based models trained with the
source language words segmented into morphs
did not improve the vanilla word-based models
trained with the unsegmented source language.
We have replicated the result for the German-
to-English language pair that an MBR combina-
tion of a word-based and a segmented morph-
based model gives significant improvements to the
BLEU score. However, we did not see improve-
ments for the Czech-to-English translations.
Acknowledgments
This work was supported by the Academy of
Finland in the project Adaptive Informatics, the
Finnish graduate school in Language Technology,
and the IST Programme of the European Commu-
nity, under the FP7 project EMIME (213845).
References
Mathias Creutz and Krista Lagus. 2002. Unsuper-
vised discovery of morphemes. In Proceedings of
the Workshop on Morphological and Phonological
Learning of ACL?02, pages 21?30, Philadelphia,
Pennsylvania, USA.
Mathias Creutz and Krista Lagus. 2005a. Inducing the
morphological lexicon of a natural language from
unannotated text. In Proceedings of the AKRR?05,
Espoo, Finland.
199
Mathias Creutz and Krista Lagus. 2005b. Unsu-
pervised morpheme segmentation and morphology
induction from text corpora using Morfessor 1.0.
Technical Report A81, Publications in Computer
and Information Science, Helsinki University of
Technology.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4(1), January.
Adria` de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes risk com-
bination of translation hypotheses from alternative
morphological decompositions. In Proceedings of
Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, Com-
panion Volume: Short Papers, pages 73?76, Boul-
der, USA, June. Association for Computational Lin-
guistics.
Teemu Hirsima?ki, Mathias Creutz, Vesa Siivola, Mikko
Kurimo, Sami Virpioja, and Janne Pylkko?nen.
2006. Unlimited vocabulary speech recognition
with morph language models applied to Finnish.
Computer Speech and Language, 20(4):515?541.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the EMNLP 2007,
pages 868?876, Prague, Czech Republic, June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Annual Meeting of ACL, demonstration ses-
sion, pages 177?180, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the HLT-NAACL 2004, pages
169?176.
Mikko Kurimo, Antti Puurula, Ebru Arisoy, Vesa Si-
ivola, Teemu Hirsima?ki, Janne Pylkko?nen, Tanel
Aluma?e, and Murat Saraclar. 2006. Unlimited vo-
cabulary speech recognition for agglutinative lan-
guages. In Proceedings of the HLT-NAACL 2006,
pages 487?494, New York, USA.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2009.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.
K. C. Sim, W. J. Byrne, M. J. F. Gales, H. Sahbi, and
P. C. Woodl. 2007. Consensus network decoding
for statistical machine translation system combina-
tion. In IEEE Int. Conf. on Acoustics, Speech, and
Signal Processing.
Sami Virpioja and Oskar Kohonen. 2009. Unsuper-
vised morpheme analysis with Allomorfessor. In
Working notes for the CLEF 2009 Workshop, Corfu,
Greece.
Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz,
and Markus Sadeniemi. 2007. Morphology-aware
statistical machine translation based on morphs in-
duced in an unsupervised manner. In Proceedings
of the Machine Translation Summit XI, pages 491?
498, Copenhagen, Denmark, September.
200
NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 37?40,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Vocabulary Adaptation for Morph-based Language Models
Andre? Mansikkaniemi and Mikko Kurimo
Aalto University School of Science
Department of Information and Computer Science
PO BOX 15400, 00076 Aalto, Finland
{andre.mansikkaniemi,mikko.kurimo}@aalto.fi
Abstract
Modeling of foreign entity names is an im-
portant unsolved problem in morpheme-based
modeling that is common in morphologically
rich languages. In this paper we present an
unsupervised vocabulary adaptation method
for morph-based speech recognition. Foreign
word candidates are detected automatically
from in-domain text through the use of letter
n-gram perplexity. Over-segmented foreign
entity names are restored to their base forms in
the morph-segmented in-domain text for eas-
ier and more reliable modeling and recogni-
tion. The adapted pronunciation rules are fi-
nally generated with a trainable grapheme-to-
phoneme converter. In ASR performance the
unsupervised method almost matches the abil-
ity of supervised adaptation in correctly rec-
ognizing foreign entity names.
1 Introduction
Foreign entity names (FENs) are difficult to rec-
ognize correctly in automatic speech recognition
(ASR). Pronunciation rules that cover native words
usually give incorrect pronunciation for foreign
words. More often the foreign entity names encoun-
tered in speech are out-of-vocabulary words, previ-
ously unseen words not present in neither the lexicon
nor background language model (LM).
An in-domain LM trained on a smaller corpus re-
lated to the topic of the speech, can be used to adapt
the background LM to give more suitable probabil-
ities to rare or unseen foreign words. Proper pro-
nunciation rules for foreign entity names are needed
to increase the probability of their correct recogni-
tion. These can either be obtained from a hand-made
lexicon or by generating pronunciation rules auto-
matically using for example a trainable grapheme-
to-phoneme (G2P) converter.
In morph-based speech recognition words are
segmented into sub-word units called morphemes.
When using statistical morph-segmentation algo-
rithms such as Morfessor (Creutz and Lagus, 2005)
new foreign entity names encountered in in-domain
text corpora are often over-segmented (e.g. mcdow-
ell ? mc do well). To guarantee reliable pronuncia-
tion modeling, it?s preferable to keep the lemma in-
tact. Restoring over-segmented foreign entity names
back in to their base forms is referred to as mor-
pheme adaptation in this paper.
This work describes an unsupervised approach to
language and pronunciation modeling of foreign en-
tity names in morph-based speech recognition. We
will study an adaptation framework illustrated below
in Figure 1.
In-domain text
Find FENs
Stemmer
Morpheme
adaptation
LM adaptation
Morph
segmentation
Background
corpus
Adapted LM
FEN lexicon
G2P converter
Adapted
lexicon
Figure 1: Adaptation framework.
37
The adaptation framework is centered around the
following automated steps: 1. Find foreign words in
adaptation texts, 2. Convert foreign word candidates
into their base forms, 3. Generate pronunciation
variants for the retrieved foreign entity name can-
didates using a G2P converter. Additionally, to fa-
cilitate easier and more reliable pronunciation adap-
tation, the foreign entity names are restored to their
base forms in the segmented in-domain text.
The adaptation framework will be compared to a
supervised method where the adaptation steps are
done manually. The evaluation will be done on
Finnish radio news segments.
2 Methods
2.1 Foreign Word Detection
Unsupervised detection of foreign words in text
has previously been implemented for English using
word n-ngram models (Ahmed, 2005).
Finnish has a rich morphology and using word n-
gram models or dictionaries for the detection of for-
eign words would not be practical. Many of the for-
eign words occurring in written Finnish texts could
be identified from unusual letter sequences that are
not common in native words. A letter n-gram model
trained on Finnish words could be used to identify
foreign words by calculating the average perplexity
of the letter sequence in a word normalized by its
length.
A two-step algorithm is implemented for the au-
tomatic detection of foreign words. First, all words
starting in uppercase letters in the unprocessed adap-
tation text are held out as potential foreign entity
names. The perplexity for each foreign word candi-
date is calculated using a letter-ngram model trained
on Finnish words. Words with the highest perplexity
values are the most probable foreign entity names. A
percentage threshold T for the top perplexity words
can be determined from prior information.
The most likely foreign words are fi-
nally converted into their base forms using
a Finnish stemming algorithm (Snowball -
http://snowball.tartarus.org/).
2.2 Lexicon Adaptation
For Finnish ASR systems the pronunciation dictio-
nary can easily be constructed for arbitrary words
by mapping letters directly to phonemes. Foreign
names are often pronounced according to their orig-
inal languages, which can have more complicated
pronunciation rules. These pronunciation rules
can either be manually added to a lookup dictio-
nary or generated automatically with a grapheme-
to-phoneme converter. Constructing a foreign word
lexicon through manual input involves a lot of te-
dious work and it will require a continuous effort to
keep it updated.
In this work Sequitur G2P is used, a data-
driven grapheme-to-phoneme converter based on
joint-sequence models (Bisani and Ney, 2008). A
pronunciation model is trained on a manually con-
structed foreign word lexicon consisting of 2000 for-
eign entity names with a manually given pronuncia-
tion hand-picked from a Finnish newswire text col-
lection. The linguistic origins of the foreign words
are mixed but Germanic and Slavic languages are
the most common.
The pronunciation model is used to generate the
most probable pronunciation variants for the foreign
entity name candidates found in the adaptation text.
2.3 Morpheme Adaptation
In current state of the art Finnish language model-
ing words are segmented into sub-word units (mor-
phemes) (Hirsima?ki et. al, 2009). This allows the
system to cover a large number of words which re-
sult from the highly agglutinative word morphology.
Over-segmentation usually occurs for previously
unseen words found in adaptation texts. To en-
sure reliable pronunciation modeling of foreign
entity names it?s preferable to keep the lemma
intact. Mapping a whole word pronunciation
rule onto separate morphemes is a non-trivial
task for non-phonetic languages such as English.
The morphemes in the in-domain corpus will be
adapted such that all foreign words are restored
into their base forms and the base forms are added
to the morpheme vocabulary. Below is an exam-
ple. Word boundaries are labeled with the <w>-tag.
<w> oilers <w> ha?visi <w> edmonton in <w> com mon
we al th <w> sta dium illa <w>
?
<w> oilers <w> ha?visi <w> edmonton in <w> com-
monwealth <w> stadium illa <w>
38
2.4 Language Model Adaptation
The in-domain adaptation text is segmented differ-
ently depending on the foreign entity name can-
didates that are included. A separate in-domain
LM Pi(w|h) is trained for each segmentation of the
text. Linear interpolation is used to the adapt the
background LM PB(w|h) with the in-domain LM
Pi(w|h).
Padapi(w|h) = ?Pi(w|h) + (1? ?)PB(w|h) (1)
3 Experiments
3.1 Speech Data
Evaluation data consisted of two sets of Finnish ra-
dio news segments in 16 kHz audio. All of the
recordings were collected in 2011-2012 from YLE
Radio Suomi news and sports programs.
The first data set consisted of 32 general news
segments. The total transcription length was 8271
words. 4.8% of the words were categorized as for-
eign entity names (FEN). The second data set con-
sisted of 43 sports news segments. The total tran-
scription length 6466 was words. 7.9% of the words
were categorized as foreign entity names.
3.2 System and Models
All speech recognition experiments were run on the
Aalto speech recognizer (Hirsima?ki et. al, 2009).
The background LM was trained on the Kieli-
pankki corpus (70 million words). A lexicon of 30k
morphs and a model of morph segmentation was
learnt from the same corpus as the LM using Mor-
fessor (Creutz and Lagus, 2005). The baseline lex-
icon was adapted with a manually transcribed pro-
nunciation dictionary of 2000 foreign entity names
found in Finnish newswire texts. A Kneser-Ney
smoothed varigram LM (n=12) was trained on the
segmented corpus with the variKN language model-
ing toolkit (Siivola et al, 2007).
LM adaptation data was manually collected from
the Web. On average 2-3 articles were gathered per
topic featured in the evaluation data sets. 120 000
words of text were gathered for LM adaptation on
the general news set. 60 000 words were gathered
for LM adaptation on the sports news set.
The foreign word detection algorithm and a letter
trigram model trained on the Kielipankki word list
were used to automatically find foreign entity names
in the adaptation texts and convert them into their
base forms. Different values were used as percent-
age threshold T (30, 60, and 100%).
The adaptation texts were segmented into morphs
with the segmentation model learnt from the back-
ground corpus. Morpheme adaptation was per-
formed by restoring the foreign entity name candi-
dates into their base forms. Separate in-domain vari-
gram LMs (n=6) were trained for adaptation data
segmented into morphs using each choice of T in
the foreign name detection. The background LM
was adapted with each in-domain LM separately us-
ing linear interpolation with weight ? = 0.1 chosen
based on preliminary experiments.
A pronunciation model was trained with Sequitur
G2P on the manually constructed foreign word lexi-
con. The number of the most probable pronunciation
variants m for one word to be used in lexicon adap-
tation, was tested with different values (1, 4, and 8).
4 Results
The word error rate (WER), letter error rate (LER),
and the foreign entity name error rate (FENER) are
reported in the results. All the results are presented
in Table 1.
The first experiment was run on the baseline sys-
tem. The average WER is 21.7% for general news
and 34.0% for sports. The average FENER is signif-
icantly higher for both (76.6% and 80.7%).
Supervised vocabulary adaptation was imple-
mented by manually retrieving the foreign entity
names from the adaptation text and adding their pro-
nunciation rules to the lexicon. Morpheme adapta-
tion was also applied. Compared to only using linear
interpolation (? = 0.1) supervised vocabulary adap-
tation reduces WER by 4% (general news) and 6%
(sports news). Recognition of foreign entity names
is also improved with FENER reductions of 18%
and 24%.
Unsupervised vocabulary adaptation was imple-
mented through automatic retrieval and pronuncia-
tion generation of foreign entity names. The pa-
rameters of interest are the foreign name percentage
threshold T, determining how many foreign word
candidates are included for lexicon and morpheme
adaptation and m, the number of pronunciation vari-
39
Adaptation method Results
LM Lexicon General News Sports NewsAdaptation T[%] m WER[%] LER[%] FENER[%] WER[%] LER[%] FENER[%]
Background Baseline 21.7 5.7 76.6 34.0 11.4 80.7
Background + Adaptation
Baseline 20.6 5.3 67.8 32.0 10.7 69.4
Supervised - 1 19.8 5.0 55.7 30.1 9.8 53.1
Unsupervised
30
1 20.4 5.2 64.0 31.5 10.4 64.1
4 20.2 5.2 58.7 31.6 10.4 60.4
8 20.4 5.3 56.9 31.5 10.4 56.8
60
1 20.7 5.3 63.7 32.3 10.4 63.7
4 20.7 5.3 59.4 31.1 9.9 59.8
8 21.1 5.5 58.2 31.0 9.9 55.6
100
1 21.1 5.4 62.7 33.2 10.7 66.1
4 21.2 5.5 58.2 32.6 10.4 60.7
8 22.1 5.9 59.2 33.2 10.6 57.0
Table 1: Results of adaptation experiments on the two test sets. Linear interpolation is tested with supervised and
unsupervised vocabulary adaptation. T is the top percentage of foreign entity name candidates used in unsupervised
vocabulary adaptation, and m is the number of pronunciation variants for each word.
ants generated for each word. The best performance
is reached on the general news set with T = 30% and
m = 4 (WER = 20.2%, FENER = 58.7%), and on the
sports news set with T = 60% and m = 8 (WER =
31.0%, FENER = 55.6%).
5 Conclusion and Discussion
In this work we presented an unsupervised approach
to pronunciation and language modeling of foreign
entity names in morph-based speech recognition.
In the context of LM adaptation, foreign en-
tity name candidates were retrieved from in-domain
texts using a foreign word detection algorithm. Pro-
nunciation variants were generated for the foreign
word candidates using a grapheme-to-phoneme con-
verter. Morpheme adaptation was also applied by
restoring the foreign entity names into their base
forms in the morph-segmented adaptation texts.
The results indicate that unsupervised pronun-
ciation and language modeling of foreign entity
names is feasible. The unsupervised approach al-
most matches supervised adaptation in correctly rec-
ognizing foreign entity names. Average WER is also
very close to the supervised adaptation one despite
the increased acoustic confusability when introduc-
ing more pronunciation variants. The percentage of
foreign word candidates included for adaptation af-
fects performance of the algorithm. Including all
words starting in uppercase letters significantly de-
grades ASR results. The optimal threshold value
is dependent on the adaptation text and its foreign
word frequency and similarity to the evaluation data.
The composition of likely pronunciations of for-
eign names by Finnish speakers is not a straight-
forward task. While the native pronunciation of the
name is the favored one, the origin of the name is not
always clear, nor the definition of the pronunciation.
Additionally, the mapping of the native pronuncia-
tion to the phoneme set used by the Finnish ASR
system can only be an approximation, as well as the
pronunciations that the Finnish speakers are able to
produce. In future work we will study new methods
to model the pronunciation of the foreign names and
perform evaluations also in speech retrieval where
the recognition of names have particular importance.
References
B. Ahmed. 2005. Detection of Foreign Words and Names
in Written Text. Doctoral thesis, Pace University.
M. Bisani and H. Ney. 2008. Joint-Sequence Models for
Grapheme-to-Phoneme Conversion. Speech Commu-
nication, vol. 50, Issue 5, pp. 434-451.
M. Creutz and K. Lagus. 2005. Unsupervised Mor-
pheme Segmentation and Morphology Induction from
Text Corpora using Morfessor 1.0. Technical Report
A81, Publications in Computer and Information Sci-
ence, Helsinki University of Technology.
T. Hirsima?ki, J. Pylkko?nen, and M. Kurimo 2009. Im-
portance of High-order N-gram Models in Morph-
based Speech Recognition. IEEE Trans. Audio,
Speech and Lang., pp. 724-732, vol. 17.
V. Siivola, T. Hirsima?ki and S. Virpioja. 2007. On Grow-
ing and Pruning Kneser-Ney Smoothed N-Gram Mod-
els. IEEE Trans. Audio, Speech and Lang., Vol. 15,
No. 5.
40
