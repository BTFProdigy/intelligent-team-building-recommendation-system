Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 737?744
Manchester, August 2008
Translating Queries into Snippets for Improved Query Expansion
Stefan Riezler and Yi Liu and Alexander Vasserman
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043
{riezler,yliu,avasserm}@google.com
Abstract
User logs of search engines have recently
been applied successfully to improve var-
ious aspects of web search quality. In this
paper, we will apply pairs of user queries
and snippets of clicked results to train a
machine translation model to bridge the
?lexical gap? between query and document
space. We show that the combination of
a query-to-snippet translation model with
a large n-gram language model trained
on queries achieves improved contextual
query expansion compared to a system
based on term correlations.
1 Introduction
In recent years, user logs of search engines have at-
tracted considerable attention in research on query
clustering, query suggestions, query expansion, or
general web search. Besides the sheer size of these
data sets, the main attraction of user logs lies in
the possibility to capitalize on users? input, either
in form of user-generated query reformulations, or
in form of user clicks on presented search results.
However noisy, sparse, incomplete, and volatile
these data may be, recent research has presented
impressive results that are based on simply taking
the majority vote of user clicks as a signal for the
relevance of results.
In this paper we will apply user logs to the prob-
lem of the ?word mismatch? or ?lexical chasm?
(Berger et al, 2000) between user queries and
documents. The standard solution to this prob-
lem, query expansion, attempts to overcome this
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
mismatch in query and document vocabularies by
adding terms with similar statistical properties to
those in the original query. This will increase the
chances of matching words in relevant documents
and also decrease the ambiguity of the overall
query that is inherent to natural language. A suc-
cessful approach to this problem is local feed-
back, or pseudo-relevance feedback (Xu and Croft,
1996), where expansion terms are extracted from
the top-most documents that were retrieved in an
initial retrieval round. Because of irrelevant results
in the initial retrieval, caused by ambiguous terms
or retrieval errors, this technique may cause expan-
sion by unrelated terms, leading to query drift. Fur-
thermore, the requirement of two retrieval steps is
computationally expensive.
Several approaches have been presented that de-
ploy user query logs to remedy these problems.
One set of approaches focuses on user reformu-
lations of queries that differ only in one segment
(Jones et al, 2006; Fonseca et al, 2005; Huang
et al, 2003). Such segments are then identified
as candidate expansion terms, and filtered by var-
ious signals such as cooccurrence in similar ses-
sions or log-likelihood ratio of original and ex-
pansion phrases. Other approaches focus on the
relation of queries and retrieval results, either by
deploying the graph induced by queries and user
clicks in calculating query similarity (Beeferman
and Berger, 2000; Wen et al, 2002; Baeza-Yates
and Tiberi, 2007), or by leveraging top results from
past queries to provide greater context in find-
ing related queries (Raghavan and Sever, 1995;
Fitzpatrick and Dent, 1997; Sahami and Heilman,
2006). Cui et al (2002) present an all together dif-
ferent way to deploy user clickthrough data by ex-
tracting expansion terms directly from clicked re-
sults. They claim significant improvements over
737
the local feedback technique of Xu and Croft
(1996).
Cui et al?s (2002) work is the closest to ours.
We follow their approach in extracting expansion
terms directly from clicked results, however, with a
focus on high precision of query expansion. While
expansion from the domain of document terms has
the advantage that expansion terms are guaranteed
to be in the search domain, expansion precision
may suffer from the noisy and indirect ?approval?
of retrieval results by user clicks. Thus expansion
terms from the document domain are more likely
to be generalizations, specifications, or otherwise
related terms, than terms extracted from query sub-
stitutions that resemble synonyms more closely.
Furthermore, if the model that learns to correlate
document terms to query terms is required to ig-
nore context in order to generalize, finding appro-
priate expansions for ambiguous query terms is
difficult.
Our approach is to look at the ?word mismatch?
problem as a problem of translating from a source
language of queries into a target language of docu-
ments, represented as snippets. Since both queries
and snippets are arguably natural language, sta-
tistical machine translation technology (SMT) is
readily applicable to this task. In previous work,
this has been done successfully for question an-
swering tasks (Riezler et al, 2007; Soricut and
Brill, 2006; Echihabi and Marcu, 2003; Berger et
al., 2000), but not for web search in general. Cui et
al.?s (2002) model is to our knowledge the first to
deploy query-document relations for direct extrac-
tion of expansion terms for general web retrieval.
Our SMT approach has two main advantages over
Cui et al?s model: Firstly, Cui et al?s model re-
lates document terms to query terms by using sim-
ple term frequency counts in session data, with-
out considering smoothing techniques. Our ap-
proach deploys a sophisticated machine learning
approach to word alignment, including smooth-
ing techniques, to map query phrases to snippet
phrases. Secondly, Cui et al?s model only indi-
rectly uses context information to disambiguate
expansion terms. This is done by calculating the
relationship of an expansion term to the whole
query by multiplying its contributions to all query
terms. In our SMT approach, contextual disam-
biguation is done by deploying an n-gram lan-
guage model trained on queries to decide about the
appropriateness of an expansion term in the con-
text of the rest of the query terms. As shown in
an experimental evaluation, together the orthogo-
nal information sources of a translation model and
a language model provide significantly better con-
textual query expansion than Cui et al?s (2002)
correlation-based approach.
In the following, we recapitulate the essentials
of Cui et al?s (2002) model, and contrast it with
our SMT-based query expansion system. Further-
more, we will present a detailed comparison of the
two systems on a real-world query expansion task.
2 Query-Document Term Correlations
The query expansion model of Cui et al (2002)
is based on the principle that if queries containing
one term often lead to the selection of documents
containing another term, then a strong relationship
between the two terms is assumed. Query terms
and document terms are linked via clicked docu-
ments in user sessions. Formally, Cui et al (2002)
compute the following probability distribution of
document words w
d
given query words w
q
from
counts over clicked documents D:
P (w
d
|w
q
) =
?
D
P (w
d
|D)P (D|w
q
) (1)
The first term in the righthandside of equation 1 is
a normalized tfidf weight of the the document term
in the clicked document, and the second term is the
relative cooccurrence of document and query term
in sessions.
Since equation 1 calculates expansion probabil-
ities for each term separately, Cui et al (2002)
introduce the following cohesion formula that re-
spects the whole query Q by aggregating the ex-
pansion probabilities for each query term:
CoWeight
Q
(w
d
) = ln(
?
w
q
?Q
P (w
d
|w
q
) + 1) (2)
In contrast to local feedback techniques (Xu
and Croft, 1996), Cui et al?s algorithm allows to
precompute term correlations offline by collecting
counts from query logs. This reliance on pure fre-
quency counting is both a blessing and a curse: On
the one hand it allows for efficient non-iterative es-
timation, on the other hand it makes the implicit
assumption that data sparsity will be overcome by
counting from huge datasets. The only attempt at
smoothing that is made in this approach is a recur-
rence to words in query context, using equation 2,
when equation 1 assigns zero probability to unseen
pairs.
738
3 Query-Snippet Translation
The SMT system deployed in our approach is
an implementation of the alignment template ap-
proach of Och and Ney (Och and Ney, 2004). The
basic features of the model consist of a translation
model and a language model which go back to the
noisy channel formulation of machine translation
in Brown et al (1993). Their ?fundamental equa-
tion of machine translation? defines the job of a
translation system as finding the English string
?
e
that is a translation of a foreign string f such that
?
e = argmax
e
P (e|f)
= argmax
e
P (f |e)P (e) (3)
Equation 3 allows for a separation of a language
model P (e), and a translation model P (f |e). Och
and Ney (2004) reformulate equation 3 as a lin-
ear combination of feature functions h
m
(e, f) and
weights ?
m
, including feature functions for trans-
lation models h
i
(e, f) = P (f |e) and language
models h
j
(e) = P (e):
?
e = argmax
e
M
?
m=1
?
m
h
m
(e, f) (4)
The translation model used in our approach is
based on the sequence of alignment models de-
scribed in Och and Ney (2003). The relationship of
translation model and alignment model for source
language string f = f
J
1
and target string e = e
I
1
is via a hidden variable describing an alignment
mapping from source position j to target position
a
j
:
P (f
J
1
|e
I
1
) =
?
a
J
1
P (f
J
1
, a
J
1
|e
I
1
) (5)
The alignment a
J
1
contains so-called null-word
alignments a
j
= 0 that align source words to the
empty word. The different alignment models de-
scribed in Och and Ney (2003) each parameter-
ize equation 5 differently so as to capture differ-
ent properties of source and target mappings. All
models are based on estimating parameters ? by
maximizing the likelihood of training data con-
sisting of sentence-aligned, but not word-aligned
strings {(f
s
, e
s
) : s = 1, . . . , S}. Since each sen-
tence pair is linked by a hidden alignment variable
a = a
J
1
, the optimal
?
? is found using unlabeled-
data log-likelihood estimation techniques such as
the EM algorithm (Dempster et al, 1977):
?
? = argmax
?
S
?
s=1
?
a
p
?
(f
s
,a|e
s
) (6)
The final translation model is calculated from rel-
ative frequencies of phrases, i.e. consecutive se-
quences of words occurring in text. Phrases are
extracted via various heuristics as larger blocks of
aligned words from best word alignments, as de-
scribed in Och and Ney (2004).
Language modeling in our approach deploys an
n-gram language model that assigns the following
probability to a string w
L
1
of words (see Brants et
al. (2007)):
P (w
L
1
) =
L
?
i=1
P (w
i
|w
i?1
1
) (7)
?
L
?
i=1
P (w
i
|w
i?1
i?n+1
) (8)
Estimation of n-gram probabilities is done by
counting relative frequencies of n-grams in a cor-
pus of user queries. Remedies against sparse data
problems are achieved by various smoothing tech-
niques, as described in Brants et al (2007).
For applications of the system to translate un-
seen queries, a standard dynamic-programming
beam-search decoder (Och and Ney, 2004) that
tightly integrates translation model and language
model is used. Expansion terms are taken from
those terms in the 5-best translations of the query
that have not been seen in the original query string.
In our opinion, the advantages of using an
alignment-based translation model to correlate
document terms with query terms, instead of rely-
ing on a term frequency counts as in equation 1, are
as follows. The formalization of translation mod-
els as involving a hidden alignment variable allows
us to induce a probability distribution that assigns
some probability of being translated into a target
word to every source word. This is a crucial step
towards solving the problem of the ?lexical gap?
described above. Furthermore, various additional
smoothing techniques are employed in alignment
to avoid overfitting and improved coping with rare
words (see Och and Ney (2003)). Lastly, estima-
tion of hidden-variable models can be based on
the well-defined framework of statistical estima-
tion via the EM algorithm.
Similar arguments hold for the language model:
N-gram language modeling is a well-understood
739
sentence source target
pairs words words
tokens 3 billion 8 billion 25 billion
avg. length - 2.6 8.3
Table 1: Statistics of query-snippet training data
for translation model.
problem, with a host of well-proven smoothing
techniques to avoid data sparsity problems (see
Brants et al (2007).)
In combination, translation model and language
model provide orthogonal sources of information
to the overall translation quality. While the trans-
lation model induces a smooth probability distri-
bution that relates source to target words, the lan-
guage model deploys probabilities of target lan-
guage strings to assess the adequacy of a target
word as a translation in context. Reliance on or-
dering information of the words in the context of a
source word is a huge advantage over the bag-of-
words aggregation of context information in Cui et
al?s (2002) model. Furthermore, in the SMT model
used in our approach, translation model and lan-
guage model are efficiently integrated in a beam-
search decoder.
In our application of SMT to query expansion,
queries are considered as source language sen-
tences and snippets of clicked result documents
as target sentences. A parallel corpus of sentence-
aligned data is created by pairing each query with
each snippet of its clicked results. Further adjust-
ments to system parameters were applied in or-
der to adapt the training procedure to this special
data set. For example, in order to account for the
difference in sentence length between queries and
snippets, we set the null-word probability to 0.9.
This allows us to improve precision of alignment
of noisy data by concentrating the alignment to a
small number of key words. Furthermore, extrac-
tion of phrases in our approach is restricted to the
intersection of alignments from both translation di-
rections, thus favoring precision over recall also in
phrase extraction. The only major adjustment of
the language model to the special case of query-
snippet translation is the fact that we train our n-
gram model on queries taken from user logs, in-
stead of on standard English text.
1-grams 2-grams 3-grams
9 million 1.5 billion 5 billion
Table 2: Statistics of unique query n-grams in lan-
guage model.
items disagreements
w/ agreement included
# items 102 125
mean item score 0.333 0.279
95% conf. int. [0.216, 0.451] [0.176, 0.381]
Table 3: Comparison of SMT-based expan-
sion with correlation-based expansion on 7-point
Likert-type scale.
4 Experimental Evaluation
4.1 Data
The training data for the translation model and
the correlation-based model consist of pairs of
queries and snippets for clicked results taken from
anonymized query logs. Using snippets instead of
full documents makes iterative training feasible
and also reduces noise considerably. This parallel
corpus of query-snippet pairs is fed into a standard
SMT training pipeline (modulo the adjustments to
word and phrase alignment discussed above). The
parallel corpus consists of 3 billion query-snippet
pairs that are input to training of word and phrase
alignment models. The resulting phrase translation
table that builds the basis of the translation model
consists 700 million query-snippet phrase transla-
tions. A collection of data statistics for the training
data is shown in table 1.
The language model used in our experiment is a
trigram language model trained on English queries
in user logs. N-grams were cut off at a minimum
frequency of 4. Data statistics for resulting unique
n-grams are shown in table 2.
4.2 Experimental Comparison
Our experimental setup for query expansion de-
ploys a real-world search engine, google.com, for
a comparison of expansions from the SMT-based
system and the correlation-based system. The ex-
perimental evaluation was done as direct compari-
son of search results for queries where both exper-
imental systems suggested expansion terms. Since
expansions from both experimental systems are
done on top of the same underlying search engine,
this allows us to abstract away from interactions
with the underlying system. The queries used for
evaluation were extracted randomly from 3+ word
740
query SMT-based expansions corr-based expansions score
applying U.S. passport passport - visa applying - home -1.0
configure debian to use dhcp debian - linux configure - configuring -1.0
configure - install
how many episodes of 30 rock? episodes - season how many episodes - tv -0.83
episodes - series many episodes - wikipedia
lampasas county sheriff department department - office department - home -0.83
sheriff - office
weakerthans cat virtue chords chords - guitar cat - tabs -0.83
chords - lyrics chords - tabs
chords - tab
Henry VIII Menu Portland, Maine menu - restaurant portland - six 1.3
menu - restaurants menu - england
ladybug birthday parties parties - ideas ladybug - kids 1.3
parties - party
political cartoon calvin coolidge cartoon - cartoons political cartoon - encyclopedia 1.3
top ten dining, vancouver dining - restaurants dining vancouver - 10 1.3
international communication communication - communications international communication - college 1.3
in veterinary medicine communication - skills
Table 4: SMT-based versus correlation-based expansions with mean item score.
queries in user logs in order to allow the systems
to deploy context information for expansion.
In order to evaluate Cui et al?s (2002)
correlation-based system in this setup, we required
the system to assign expansion terms to particu-
lar query terms. This could be achieved by using
a linear interpolation of scores in equation 2 and
equation 1. Equation 1 thus introduces a prefer-
ence for a particular query term to the whole-query
score calculated by equation 2. Our reimplementa-
tion uses unigram and bigram phrases in queries
and expansions. Furthermore, we use Okapi BM25
instead of tfidf in the calculation of equation 1 (see
Robertson et al (1998)).
Query expansion for the SMT-based system is
done by extracting terms introduced in the 5-best
list of query translations as expansion terms for the
respective query terms.
The evaluation was performed by three in-
dependent raters. The raters were given task-
specific rating guidelines, and were shown queries
and 10-best search results from both systems,
anonymized, and presented randomly on left or
right sides. The raters? task was to evaluate the re-
sults on a 7-point Likert-type
1
scale, defined as:
-1.5: much worse
-1.0: worse
-0.5: slightly worse
1
Likert?s (1932) original rating system is a 5-point scale
using integer scores 1 through 5. Our system uses average
scores over three raters for each item, and uses a 7-point in-
stead of a 5-point scale. See Dawes (2008) on the compara-
bility of 5-, 7-, or 10-point scales.
0: about the same
0.5: slightly better
1.0: better
1.5: much better
Results on 125 queries where both systems sug-
gested expansion terms are shown in table 3. For
each query, rating scores are averaged over the
scores assigned by three raters. The overall mean
item score for a comparison of SMT-based ex-
pansion against correlation-based expansion was
0.333 for 102 items with rater agreement, and
0.279 for 125 items including rater disagreements.
All result differences are statistically significant.
Examples for SMT-based and correlation-based
expansions are given in table 4. The first five ex-
amples are losses for the SMT-based system. In
the first example, passport is replaced by the re-
lated, but not synonymous term visa in the SMT-
based expansion. The second example is a loss for
SMT-based expansion because of a replacement of
the specific term debian by the more general term
linux. The correlation-based expansions tv 30 rock
in the third example, lampasas county sheriff home
in the fourth example, and weakerthans tabs in the
fifth example directly hit the title of relevant web
pages, while the SMT-based expansion terms do
not improve retrieval results. However, even from
these negative examples it becomes apparent that
the SMT-based expansion terms are clearly related
to the query terms, and for a majority cases this
has a positive effect. Such examples are shown in
741
(herbs , herbs) ( for , for) ( chronic , chronic) ( constipation , constipation)
(herbs , herb) ( for , for) ( chronic , chronic) ( constipation , constipation)
(herbs , remedies) ( for , for) ( chronic , chronic) ( constipation , constipation)
(herbs , medicine) ( for , for) ( chronic , chronic) ( constipation , constipation)
(herbs , supplements) ( for , for) ( chronic , chronic) ( constipation , constipation)
(herbs , herbs) ( for , for) ( mexican , mexican) ( cooking , cooking)
(herbs , herbs) ( for , for) ( cooking , cooking) ( mexican , mexican)
(herbs , herbs) ( for , for) ( mexican , mexican) ( cooking , food)
(mexican , mexican) ( herbs , herbs) ( for , for) ( cooking , cooking)
(herbs , spices) ( for , for) ( mexican , mexican) ( cooking , cooking)
Table 5: Unique 5-best phrase-level translations of queries herbs for chronic constipation and herbs for
mexican cooking.
query terms n-best expansions
herbs com treatment encyclopedia
chronic interpret treating com
constipation interpret treating com
herbs for medicinal support women
for chronic com gold encyclopedia
chronic constipation interpret treating
herbs cooks recipes com
mexican recipes com cooks
cooking cooks recipes com
herbs for medicinal women support
for mexican cooks com allrecipes
Table 6: Correlation-based expansions for queries herbs for chronic constipation and herbs for mexican
cooking.
the second set of expansions. SMT-based expan-
sions such as henry viii restaurant portland, maine,
or ladybug birthday ideas, or top ten restaurants,
vancouver achieve a change in retrieval results that
does not result in a query drift, but rather in im-
proved retrieval results. In contrast, the terms in-
troduced by the correlation-based system are either
only vaguely related or noise.
5 Discussion
We attribute the experimental result of a signif-
icant preference for SMT-based expansions over
correlation-based expansions to the fruitful com-
bination of translation model and language model
provided by the SMT system. The SMT approach
can be viewed as a combined system that proposes
candidate expansion via the translation model, and
filters them by the language model. Thus we may
find a certain amount of non-sensical expansion
candidates at the phrase translation level. This can
be seen from inspecting table 7 which shows the
most probable phrase translations that are applica-
ble to the queries herbs for chronic constipation
and herbs for mexican cooking. The phrase table
includes identity translations and closely related
terms as most probable translations for nearly ev-
ery phrase, however, it also clearly includes noisy
and non-related terms. More importantly, an ex-
traction of expansion terms from the phrase table
alone would not allow to choose the appropriate
term for the given query context. This can be at-
tained by combining the phrase translations with a
language model: As shown in table 5, the 5-best
translations of the full queries attain a proper dis-
ambiguation of the senses of herbs by replacing
the term by remedies, medicine, and supplements
for the first query, and with spices for the second
query. Expansion terms highlighted in bold face.
The fact that the most probable translation for
the whole query mostly is the identity translation
can be seen as a feature, not as a bug, of the SMT-
based approach: By the option to prefer identity
translations or word reorderings over translations
of source words, the SMT model effectively can
choose not to generate any expansion terms. This
will happen if none of the candidate phrase trans-
lations fit with high enough probability into the
context of the whole query, as assessed by the lan-
guage model.
In contrast to the SMT model, the correlation-
based model cannot fall back onto the ordering in-
formation of the language model, but aggregates
information for the whole query from a bag-of-
words of query terms. Table 6 shows the top three
742
correlation-based expansion terms assigned to uni-
grams and bigrams in the queries herbs for chronic
constipation and herbs for mexican cooking. Ex-
pansion terms are chosen by overall highest weight
and shown in bold face. Relevant expansion terms
such as treatment or recipes that would disam-
biguate the meaning of herbs are in fact proposed
by the correlation-based model, however, the cohe-
sion score also promotes terms such as interpret or
com as best whole-query expansions, thus leading
to query drift.
6 Conclusion
We presented an approach to contextual query ex-
pansion that deploys natural language technology
in form of statistical machine translation. The key
idea of our approach is to consider the problem
of the ?lexical gap? between queries and docu-
ments from a linguistic point of view, and at-
tempt to bridge this gap by translating from the
query language into the document language. Us-
ing search engine user logs, we could extract large
amounts of parallel data of queries and snippets
from clicked documents. These data were used to
train an alignment-based translation model, and
an n-gram based language model. The same data
were used to train a reimplementation of Cui
et al?s (2002) term-correlation based query ex-
pansion system. An experimental comparison of
the two systems showed a considerable prefer-
ence for SMT-based expansions over correlation-
based expansion. Our explanation for this result
is the fruitful combination of the orthogonal in-
formation sources from translation model and lan-
guage model. While in the SMT approach expan-
sion candidates proposed by the translation model
are effectively filtered by ordering information
on the query context from the language model,
the correlation-based approach resorts to an in-
ferior bag-of-word aggregation of scores for the
whole query. Furthermore, each component of the
SMT model takes great care to avoid sparse data
problems by various sophisticated smoothing tech-
niques. In contrast, the correlation-based model re-
lies on pure counts of term frequencies.
An interesting task for future work is to dis-
sect the contributions of translation model and
language model, for example, by combining a
correlation-based system with a language model
filter. The challenge here is a proper integration of
n-gram lookup into correlation-based expansion.
References
Baeza-Yates, Ricardo and Alessandro Tiberi. 2007.
Extracting semantic relations from query logs. In
Proceedings of the 13th ACM SIGKDD Confer-
ence on Knowledge Discovery and Data Mining
(KDD?07), San Jose, CA.
Beeferman, Doug and Adam Berger. 2000. Agglom-
erative clustering of a search engine query log. In
Proceedings of the 6th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing (KDD?00), Boston, MA.
Berger, Adam L., Rich Caruana, David Cohn, Dayne
Freitag, and Vibhu Mittal. 2000. Bridging the lexi-
cal chasm: Statistical approaches to answer-finding.
In Proceedings of SIGIR?00, Athens, Greece.
Brants, Thorsten, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?07), Prague, Czech Re-
public.
Brown, Peter F., Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263?311.
Cui, Hang, Ji-Rong Wen, Jian-Yun Nie, and Wei-Ying
Ma. 2002. Probabilistic query expansion using
query logs. In Proceedings of WWW 2002, Hon-
olulu, Hawaii.
Dawes, John. 2008. Do data characteristics change ac-
cording to the number of scale points used? An ex-
periment using 5-point, 7-point and 10-point scales.
International Journal of Market Research, 50(1):61?
77.
Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical So-
ciety, 39(B):1?38.
Echihabi, Abdessamad and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics (ACL?03),
Sapporo, Japan.
Fitzpatrick, Larry and Mei Dent. 1997. Automatic
feedback using past queries: Social searching? In
Proceedings of SIGIR?97, Philadelphia, PA.
Fonseca, Bruno M., Paulo Golgher, Bruno Possas,
Berthier Ribeiro-Neto, and Nivio Ziviani. 2005.
Concept-based interactive query expansion. In Pro-
ceedings of the 14th Conference on Information and
Knowledge Management (CIKM?05), Bremen, Ger-
many.
743
Huang, Chien-Kang, Lee-Feng Chien, and Yen-Jen
Oyang. 2003. Relevant term suggestion in interac-
tive web search based on contextual information in
query session logs. Journal of the American Society
for Information Science and Technology, 54(7):638?
649.
Jones, Rosie, Benjamin Rey, Omid Madani, and Wi-
ley Greiner. 2006. Generating query substitutions.
In Proceedings of the 15th International World Wide
Web conference (WWW?06), Edinburgh, Scotland.
Likert, Rensis. 1932. A technique for the measurement
of attitudes. Archives of Psychology, 140:5?55.
Och, Franz Josef and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Och, Franz Josef and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Raghavan, Vijay V. and Hayri Sever. 1995. On the
reuse of past optimal queries. In Proceedings of SI-
GIR?95, Seattle, WA.
Riezler, Stefan, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007.
Statistical machine translation for query expansion
in answer retrieval. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL?07), Prague, Czech Republic.
Robertson, Stephen E., Steve Walker, and Micheline
Hancock-Beaulieu. 1998. Okapi at TREC-7. In
Proceedings of the Seventh Text REtrieval Confer-
ence (TREC-7), Gaithersburg, MD.
Sahami, Mehran and Timothy D. Heilman. 2006. A
web-based kernel function for measuring the sim-
ilarity of short text snippets. In Proceedings of
the 15th International World Wide Web conference
(WWW?06), Edinburgh, Scotland.
Soricut, Radu and Eric Brill. 2006. Automatic question
answering using the web: Beyond the factoid. Jour-
nal of Information Retrieval - Special Issue on Web
Information Retrieval, 9:191?206.
Wen, Ji-Rong, Jian-Yun Nie, and Hong-Jiang Zhang.
2002. Query clustering using user logs. ACM Trans-
actions on Information Systems, 20(1):59?81.
Xu, Jinxi and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In Pro-
ceedings of SIGIR?96, Zurich, Switzerland.
herbs herbs
herbal
medicinal
spices
supplements
remedies
herbs for herbs for
herbs
herbs and
with herbs
herbs for chronic herbs for chronic
and herbs for chronic
herbs for
for for
for chronic for chronic
chronic
of chronic
for chronic constipation for chronic constipation
chronic constipation
for constipation
chronic chronic
acute
patients
treatment
chronic constipation chronic constipation
of chronic constipation
with chronic constipation
constipation constipation
bowel
common
symptoms
for mexican for mexican
mexican
the mexican
of mexican
for mexican cooking mexican food
mexican food and
mexican glossary
mexican mexican
mexico
the mexican
mexican cooking mexican cooking
mexican food
mexican
cooking
cooking cooking
culinary
recipes
cook
food
recipe
Table 7: Phrase translations applicable to source
strings herbs for chronic constipation and herbs
for mexican cooking.
744
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 464?471,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Statistical Machine Translation for Query Expansion in Answer Retrieval
Stefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal and Yi Liu
Google Inc., 1600 Amphitheatre Parkway, Mountain View, CA 94043
{riezler|avasserm|ioannis|vibhu|yliu}@google.com
Abstract
We present an approach to query expan-
sion in answer retrieval that uses Statisti-
cal Machine Translation (SMT) techniques
to bridge the lexical gap between ques-
tions and answers. SMT-based query ex-
pansion is done by i) using a full-sentence
paraphraser to introduce synonyms in con-
text of the entire query, and ii) by trans-
lating query terms into answer terms us-
ing a full-sentence SMT model trained on
question-answer pairs. We evaluate these
global, context-aware query expansion tech-
niques on tfidf retrieval from 10 million
question-answer pairs extracted from FAQ
pages. Experimental results show that SMT-
based expansion improves retrieval perfor-
mance over local expansion and over re-
trieval without expansion.
1 Introduction
One of the fundamental problems in Question An-
swering (QA) has been recognized to be the ?lexi-
cal chasm? (Berger et al, 2000) between question
strings and answer strings. This problem is mani-
fested in a mismatch between question and answer
vocabularies, and is aggravated by the inherent am-
biguity of natural language. Several approaches have
been presented that apply natural language process-
ing technology to close this gap. For example, syn-
tactic information has been deployed to reformu-
late questions (Hermjakob et al, 2002) or to re-
place questions by syntactically similar ones (Lin
and Pantel, 2001); lexical ontologies such as Word-
net1 have been used to find synonyms for question
words (Burke et al, 1997; Hovy et al, 2000; Prager
et al, 2001; Harabagiu et al, 2001), and statisti-
cal machine translation (SMT) models trained on
question-answer pairs have been used to rank can-
didate answers according to their translation prob-
abilities (Berger et al, 2000; Echihabi and Marcu,
2003; Soricut and Brill, 2006). Information retrieval
(IR) is faced by a similar fundamental problem of
?term mismatch? between queries and documents.
A standard IR solution, query expansion, attempts to
increase the chances of matching words in relevant
documents by adding terms with similar statistical
properties to those in the original query (Voorhees,
1994; Qiu and Frei, 1993; Xu and Croft, 1996).
In this paper we will concentrate on the task of
answer retrieval from FAQ pages, i.e., an IR prob-
lem where user queries are matched against docu-
ments consisting of question-answer pairs found in
FAQ pages. Equivalently, this is a QA problem that
concentrates on finding answers given FAQ docu-
ments that are known to contain the answers. Our
approach to close the lexical gap in this setting at-
tempts to marry QA and IR technology by deploy-
ing SMT methods for query expansion in answer
retrieval. We present two approaches to SMT-based
query expansion, both of which are implemented in
the framework of phrase-based SMT (Och and Ney,
2004; Koehn et al, 2003).
Our first query expansion model trains an end-
to-end phrase-based SMT model on 10 million
question-answer pairs extracted from FAQ pages.
1http://wordnet.princeton.edu
464
The goal of this system is to learn lexical correla-
tions between words and phrases in questions and
answers, for example by allowing for multiple un-
aligned words in automatic word alignment, and dis-
regarding issues such as word order. The ability to
translate phrases instead of words and the use of a
large language model serve as rich context to make
precise decisions in the case of ambiguous transla-
tions. Query expansion is performed by adding con-
tent words that have not been seen in the original
query from the n-best translations of the query.
Our second query expansion model is based on
the use of SMT technology for full-sentence para-
phrasing. A phrase table of paraphrases is extracted
from bilingual phrase tables (Bannard and Callison-
Burch, 2005), and paraphrasing quality is improved
by additional discriminative training on manually
created paraphrases. This approach utilizes large
bilingual phrase tables as information source to ex-
tract a table of para-phrases. Synonyms for query
expansion are read off from the n-best paraphrases
of full queries instead of from paraphrases of sep-
arate words or phrases. This allows the model to
take advantage of the rich context of a large n-gram
language model when adding terms from the n-best
paraphrases to the original query.
In our experimental evaluation we deploy a
database of question-answer pairs extracted from
FAQ pages for both training a question-answer
translation model, and for a comparative evalua-
tion of different systems on the task of answer re-
trieval. Retrieval is based on the tfidf framework
of Jijkoun and de Rijke (2005), and query expan-
sion is done straightforwardly by adding expansion
terms to the query for a second retrieval cycle. We
compare our global, context-aware query expansion
techniques with Jijkoun and de Rijke?s (2005) tfidf
model for answer retrieval and a local query expan-
sion technique (Xu and Croft, 1996). Experimen-
tal results show a significant improvement of SMT-
based query expansion over both baselines.
2 Related Work
QA has approached the problem of the lexical gap
by various techniques for question reformulation,
including rule-based syntactic and semantic refor-
mulation patterns (Hermjakob et al, 2002), refor-
mulations based on shared dependency parses (Lin
and Pantel, 2001), or various uses of the Word-
Net ontology to close the lexical gap word-by-word
(Hovy et al, 2000; Prager et al, 2001; Harabagiu
et al, 2001). Another use of natural language pro-
cessing has been the deployment of SMT models on
question-answer pairs for (re)ranking candidate an-
swers which were either assumed to be contained
in FAQ pages (Berger et al, 2000) or retrieved by
baseline systems (Echihabi and Marcu, 2003; Sori-
cut and Brill, 2006).
IR has approached the term mismatch problem by
various approaches to query expansion (Voorhees,
1994; Qiu and Frei, 1993; Xu and Croft, 1996).
Inconclusive results have been reported for tech-
niques that expand query terms separately by adding
strongly related terms from an external thesaurus
such as WordNet (Voorhees, 1994). Significant
improvements in retrieval performance could be
achieved by global expansion techniques that com-
pute corpus-wide statistics and take the entire query,
or query concept (Qiu and Frei, 1993), into account,
or by local expansion techniques that select expan-
sion terms from the top ranked documents retrieved
by the original query (Xu and Croft, 1996).
A similar picture emerges for query expansion
in QA: Mixed results have been reported for word-
by-word expansion based on WordNet (Burke et
al., 1997; Hovy et al, 2000; Prager et al, 2001;
Harabagiu et al, 2001). Considerable improvements
have been reported for the use of the local context
analysis model of Xu and Croft (1996) in the QA
system of Ittycheriah et al (2001), or for the sys-
tems of Agichtein et al (2004) or Harabagiu and
Lacatusu (2004) that use FAQ data to learn how to
expand query terms by answer terms.
The SMT-based approaches presented in this pa-
per can be seen as global query expansion tech-
niques in that our question-answer translation model
uses the whole question-answer corpus as informa-
tion source, and our approach to paraphrasing de-
ploys large amounts of bilingual phrases as high-
coverage information source for synonym finding.
Furthermore, both approaches take the entire query
context into account when proposing to add new
terms to the original query. The approaches that
are closest to our models are the SMT approach of
Radev et al (2001) and the paraphrasing approach
465
web pages FAQ pages QA pairs
count 4 billion 795,483 10,568,160
Table 1: Corpus statistics of QA pair data
of Duboue and Chu-Carroll (2006). None of these
approaches defines the problem of the lexical gap
as a query expansion problem, and both approaches
use much simpler SMT models than our systems,
e.g., Radev et al (2001) neglect to use a language
model to aid disambiguation of translation choices,
and Duboue and Chu-Carroll (2006) use SMT as
black box altogether.
In sum, our approach differs from previous work
in QA and IR in the use SMT technology for query
expansion, and should be applicable in both areas
even though experimental results are only given for
the restricted domain of retrieval from FAQ pages.
3 Question-Answer Pairs from FAQ Pages
Large-scale collection of question-answer pairs has
been hampered in previous work by the small sizes
of publicly available FAQ collections or by restricted
access to retrieval results via public APIs of search
engines. Jijkoun and de Rijke (2005) nevertheless
managed to extract around 300,000 FAQ pages
and 2.8 million question-answer pairs by repeatedly
querying search engines with ?intitle:faq?
and ?inurl:faq?. Soricut and Brill (2006) could
deploy a proprietary URL collection of 1 billion
URLs to extract 2.3 million FAQ pages contain-
ing the uncased string ?faq? in the url string. The
extraction of question-answer pairs amounted to a
database of 1 million pairs in their experiment.
However, inspection of the publicly available Web-
FAQ collection provided by Jijkoun and de Rijke2
showed a great amount of noise in the retrieved
FAQ pages and question-answer pairs, and yet the
indexed question-answer pairs showed a serious re-
call problem in that no answer could be retrieved for
many well-formed queries. For our experiment, we
decided to prefer precision over recall and to attempt
a precision-oriented FAQ and question-answer pair
extraction that benefits the training of question-
answer translation models.
2http://ilps.science.uva.nl/Resources/WazDah/
As shown in Table 1, the FAQ pages used in our
experiment were extracted from a 4 billion page
subset of the web using the queries ?inurl:faq?
and ?inurl:faqs? to match the tokens ?faq? or
?faqs? in the urls. This extraction resulted in 2.6
million web pages (0.07% of the crawl). Since not
all those pages are actually FAQs, we manually la-
beled 1,000 of those pages to train an online passive-
aggressive classificier (Crammer et al, 2006) in a
10-fold cross validation setup. Training was done
using 20 feature functions on occurrences question
marks and key words in different fields of web
pages, and resulted in an F1 score of around 90%
for FAQ classification. Application of the classifier
to the extracted web pages resulted in a classification
of 795,483 pages as FAQ pages.
The extraction of question-answer pairs from this
database of FAQ pages was performed again in a
precision-oriented manner. The goal of this step
was to extract url, title, question, and answers fields
from the question-answer pairs in FAQ pages. This
was achieved by using feature functions on punc-
tuations, HTML tags (e.g., <p>, <BR>), listing
markers (e.g., Q:, (1)), and lexical cues (e.g.,
What, How), and an algorithm similar to Joachims
(2003) to propagate initial labels across similar text
pieces. The result of this extraction step is a database
of about 10 million question answer pairs (13.3
pairs per FAQ page). A manual evaluation of 100
documents, containing 1,303 question-answer pairs,
achieved a precision of 98% and a recall of 82% for
extracting question-answer pairs.
4 SMT-Based Query Expansion
Our SMT-based query expansion techniques are
based on a recent implementation of the phrase-
based SMT framework (Koehn et al, 2003; Och and
Ney, 2004). The probability of translating a foreign
sentence f into English e is defined in the noisy chan-
nel model as
argmax
e
p(e|f) = argmax
e
p(f|e)p(e) (1)
This allows for a separation of a language model
p(e), and a translation model p(f|e). Translation
probabilities are calculated from relative frequencies
of phrases, which are extracted via various heuris-
tics as larger blocks of aligned words from best word
466
alignments. Word alignments are estimated by mod-
els similar to Brown et al (1993). For a sequence of
I phrases, the translation probability in equation (1)
can be decomposed into
p(f Ii |e
I
i ) =
I?
i=1
p(fi|ei) (2)
Recent SMT models have shown significant im-
provements in translation quality by improved mod-
eling of local word order and idiomatic expressions
through the use of phrases, and by the deployment
of large n-gram language models to model fluency
and lexical choice.
4.1 Question-Answer Translation
Our first approach to query expansion treats the
questions and answers in the question-answer cor-
pus as two distinct languages. That is, the 10 million
question-answer pairs extracted from FAQ pages are
fed as parallel training data into an SMT training
pipeline. This training procedure includes various
standard procedures such as preprocessing, sentence
and chunk alignment, word alignment, and phrase
extraction. The goal of question-answer translation
is to learn associations between question words and
synonymous answer words, rather than the trans-
lation of questions into fluent answers. Thus we
did not conduct discriminative training of feature
weights for translation probabilities or language
model probabilities, but we held out 4,000 question-
answer pairs for manual development and testing of
the system. For example, the system was adjusted
to account for the difference in sentence length be-
tween questions and answers by setting the null-
word probability parameter in word alignment to
0.9. This allowed us to concentrate the word align-
ments to a small number of key words. Furthermore,
extraction of phrases was based on the intersection
of alignments from both translation directions, thus
favoring precision over recall also in phrase align-
ment.
Table 2 shows unique translations of the query
?how to live with cat allergies? on the phrase-level,
with corresponding source and target phrases shown
in brackets. Expansion terms are taken from phrase
terms that have not been seen in the original query,
and are highlighted in bold face.
4.2 SMT-Based Paraphrasing
Our SMT-based paraphrasing system is based on the
approach presented in Bannard and Callison-Burch
(2005). The central idea in this approach is to iden-
tify paraphrases or synonyms at the phrase level by
pivoting on another language. For example, given
a table of Chinese-to-English phrase translations,
phrasal synonyms in the target language are defined
as those English phrases that are aligned to the same
Chinese source phrases. Translation probabilities for
extracted para-phrases can be inferred from bilin-
gual translation probabilities as follows: Given an
English para-phrase pair (trg, syn), the probability
p(syn|trg) that trg translates into syn is defined
as the joint probability that the English phrase trg
translates into the foreign phrase src, and that the
foreign phrase src translates into the English phrase
syn. Under an independence assumption of those
two events, this probability and the reverse transla-
tion direction p(trg|syn) can be defined as follows:
p(syn|trg) = max
src
p(src|trg)p(syn|src) (3)
p(trg|syn) = max
src
p(src|syn)p(trg|src)
Since the same para-phrase pair can be obtained
by pivoting on multiple foreign language phrases, a
summation or maximization over foreign language
phrases is necessary. In order not to put too much
probability mass onto para-phrase translations that
can be obtained from multiple foreign language
phrases, we maximize instead of summing over src.
In our experiments, we employed equation (3)
to infer for each para-phrase pair translation model
probabilities p?(syn|trg) and p??(trg|syn) from
relative frequencies of phrases in bilingual tables.
In contrast to Bannard and Callison-Burch (2005),
we applied the same inference step to infer also
lexical translation probabilities pw(syn|trg) and
pw?(trg|syn) as defined in Koehn et al (2003) for
para-phrases. Furthermore, we deployed features for
the number of words lw, number of phrases c? , a
reordering score pd , and a score for a 6-gram lan-
guage model pLM trained on English web data. The
final model combines these features in a log-linear
model that defines the probability of paraphrasing a
full sentence, consisting of a sequence of I phrases
467
qa-translation (how, how) (to, to) (live, live) (with, with) (cat, pet) (allergies, allergies)
(how, how) (to, to) (live, live) (with, with) (cat, cat) (allergies, allergy)
(how, how) (to, to) (live, live) (with, with) (cat, cat) (allergies, food)
(how, how) (to, to) (live, live) (with, with) (cat, cats) (allergies, allergies)
paraphrasing (how, how) (to live, to live) (with cat, with cat) (allergies, allergy)
(how, ways) (to live, to live) (with cat, with cat) (allergies, allergies)
(how, how) (to live with, to live with) (cat, feline) (allergies, allergies)
(how to, how to) (live, living) (with cat, with cat) (allergies, allergies)
(how to, how to) (live, life) (with cat, with cat) (allergies, allergies)
(how, way) (to live, to live) (with cat, with cat) (allergies, allergies)
(how, how) (to live, to live) (with cat, with cat) (allergies, allergens)
(how, how) (to live, to live) (with cat, with cat) (allergies, allergen)
Table 2: Unique n-best phrase-level translations of query ?how to live with cat allergies?.
as follows:
p(synI1|trg
I
1) = (
I?
i=1
p?(syni|trgi)
?? (4)
? p??(trgi|syni)
???
? pw(syni|trgi)
?w
? pw?(trgi|syni)
?w?
? pd(syni, trgi)
?d)
? lw(syn
I
1)
?l
? c?(syn
I
1)
?c
? pLM (syn
I
1)
?LM
For estimation of the feature weights ~? defined
in equation (4) we employed minimum error rate
(MER) training under the BLEU measure (Och,
2003). Training data for MER training were taken
from multiple manual English translations of Chi-
nese sources from the NIST 2006 evaluation data.
The first of four reference translations for each Chi-
nese sentence was taken as source paraphrase, the
rest as reference paraphrases. Discriminative train-
ing was conducted on 1,820 sentences; final evalua-
tion on 2,390 sentences. A baseline paraphrase table
consisting of 33 million English para-phrase pairs
was extracted from 1 billion phrase pairs from three
different languages, at a cutoff of para-phrase prob-
abilities of 0.0025.
Query expansion is done by adding terms intro-
duced in n-best paraphrases of the query. Table 2
shows example paraphrases for the query ?how to
live with cat allergies? with newly introduced terms
highlighted in bold face.
5 Experimental Evaluation
Our baseline answer retrieval system is modeled af-
ter the tfidf retrieval model of Jijkoun and de Ri-
jke (2005). Their model calculates a linear com-
bination of vector similarity scores between the
user query and several fields in the question-answer
pair. We used the cosine similarity metric with
logarithmically weighted term and document fre-
quency weights in order to reproduce the Lucene3
model used in Jijkoun and de Rijke (2005). For
indexing of fields, we adopted the settings that
were reported to be optimal in Jijkoun and de
Rijke (2005). These settings comprise the use of
8 question-answer pair fields, and a weight vec-
tor ?0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.2, 0.3? for fields or-
dered as follows: (1) full FAQ document text, (2)
question text, (3) answer text, (4) title text, (5)-(8)
each of the above without stopwords. The second
field thus takes takes wh-words, which would typ-
ically be filtered out, into account. All other fields
are matched without stopwords, with higher weight
assigned to document and question than to answer
and title fields. We did not use phrase-matching or
stemming in our experiments, similar to Jijkoun and
de Rijke (2005), who could not find positive effects
for these features in their experiments.
Expansion terms are taken from those terms
in the n-best translations of the query that have
not been seen in the original query string. For
paraphrasing-based query expansion, a 50-best list
of paraphrases of the original query was used.
For the noisier question-answer translation, expan-
sion terms and phrases were extracted from a 10-
3http://lucene.apache.org
468
S2@10 S2@20 S1,2@10 S1,2@20
baseline tfidf 27 35 58 65
local expansion 30 (+ 11.1) 40 (+ 14.2) 57 (- 1) 63 (- 3)
SMT-based expansion 38 (+ 40.7) 43 (+ 22.8) 58 65
Table 3: Success rate at 10 or 20 results for retrieval of adequate (2) or material (1) answers; relative change
in brackets.
best list of query translations. Terms taken from
query paraphrases were matched with the same field
weight vector ?0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.2, 0.3? as
above. Terms taken from question-answer trans-
lation were matched with the weight vector
?0.0, 1.0, 0.0, 0.0, 0.5, 0.2, 0.5, 0.3?, preferring an-
swer fields over question fields. After stopword
removal, the average number of expansion terms
produced was 7.8 for paraphrasing, and 3.1 for
question-answer translation.
The local expansion technique used in our exper-
iments follows Xu and Croft (1996) in taking ex-
pansion terms from the top n answers that were re-
trieved by the baseline tfidf system, and by incorpo-
rating cooccurrence information with query terms.
This is done by calculating term frequencies for ex-
pansion terms by summing up the tfidf weights of
the answers in which they occur, thus giving higher
weight to terms that occur in answers that receive
a higher similarity score to the original query. In
our experiments, expansion terms are ranked accord-
ing to this modified tfidf calculation over the top 20
answers retrieved by the baseline retrieval run, and
matched a second time with the field weight vector
?0.0, 1.0, 0.0, 0.0, 0.5, 0.2, 0.5, 0.3? that prefers an-
swer fields over question fields. After stopword re-
moval, the average number of expansion terms pro-
duced by the local expansion technique was 9.25.
The test queries we used for retrieval are taken
from query logs of the MetaCrawler search en-
gine4 and were provided to us by Valentin Jijk-
oun. In order to maximize recall for the comparative
evaluation of systems, we selected 60 queries that
were well-formed natural language questions with-
out metacharacters and spelling errors. However, for
one third of these well-formed queries none of the
five compared systems could retrieve an answer. Ex-
amples are ?how do you make a cornhusk doll?,
4http://www.metacrawler.com
?what is the idea of materialization?, or ?what does
8x certified mean?, pointing to a severe recall prob-
lem of the question-answer database.
Evaluation was performed by manual labeling of
top 20 answers retrieved for each of 60 queries for
each system by two independent judges. For the sake
of consistency, we chose not to use the assessments
provided by Jijkoun and de Rijke. Instead, the judges
were asked to find agreement on the examples on
which they disagreed after each evaluation round.
The ratings together with the question-answer pair
id were stored and merged into the retrieval results
for the next system evaluation. In this way consis-
tency across system evaluations could be ensured,
and the effort of manual labeling could be substan-
tially reduced. The quality of retrieval results was
assessed according to Jijkoun and de Rijke?s (2005)
three point scale:
? adequate (2): answer is contained
? material (1): no exact answer, but important in-
formation given
? unsatisfactory (0): user?s information need is
not addressed
The evaluation measure used in Jijkoun and de
Rijke (2005) is the success rate at 10 or 20 an-
swers, i.e., S2@n is the percentage of queries with
at least one adequate answer in the top n retrieved
question-answer pairs, and S1,2@n is the percentage
of queries with at least one adequate or material an-
swer in the top n results. This evaluation measure ac-
counts for improvements in coverage, i.e., it rewards
cases where answers are found for queries that did
not have an adequate or material answer before. In
contrast, the mean reciprocal rank (MRR) measure
standardly used in QA can have the effect of prefer-
ring systems that find answers only for a small set
of queries, but rank them higher than systems with
469
(1) query: how to live with cat allergies
local expansion (-): allergens allergic infections filter plasmacluster rhinitis introduction effective replacement
qa-translation (+): allergy cats pet food
paraphrasing (+): way allergens life allergy feline ways living allergen
(2) query: how to design model rockets
local expansion (-): models represented orientation drawings analysis element environment different structure
qa-translation (+): models rocket
paraphrasing (+): missiles missile rocket grenades arrow designing prototype models ways paradigm
(3) query: what is dna hybridization
local expansion (-): instructions individual blueprint characteristics chromosomes deoxyribonucleic information biological
genetic molecule
qa-translation (+): slides clone cdna sitting sequences
paraphrasing (+): hibridization hybrids hybridation anything hibridacion hybridising adn hybridisation nothing
(4) query: how to enhance competitiveness of indian industries
local expansion (+): resources production quality processing established investment development facilities institutional
qa-translation (+): increase industry
paraphrasing (+): promote raise improve increase industry strengthen
(5) query: how to induce labour
local expansion (-): experience induction practice imagination concentration information consciousness different meditation
relaxation
qa-translation (-): birth industrial induced induces
paraphrasing (-): way workers inducing employment ways labor working child work job action unions
Table 4: Examples for queries and expansion terms yielding improved (+), decreased (-), or unchanged (0)
retrieval performance compared to retrieval without expansion.
higher coverage. This makes MRR less adequate for
the low-recall setup of FAQ retrieval.
Table 3 shows success rates at 10 and 20 retrieved
question-answer pairs for five different systems. The
results for the baseline tfidf system, following Jijk-
oun and de Rijke (2005), are shown in row 2. Row
3 presents results for our variant of local expansion
by pseudo-relevance feedback (Xu and Croft, 1996).
Results for SMT-based expansion are given in row 4.
A comparison of success rates for retrieving at least
one adequate answer in the top 10 results shows rel-
ative improvements over the baseline of 11.1% for
local query expansion, and of 40.7% for combined
SMT-based expansion. Success rates at top 20 re-
sults show similar relative improvements of 14.2%
for local query expansion, and of 22.8% for com-
bined SMT-based expansion. On the easier task of
retrieving a material or adequate answer, success
rates drop by a small amount for local expansion,
and stay unchanged for SMT-based expansion.
These results can be explained by inspecting a few
sample query expansions. Examples (1)-(3) in Ta-
ble 4 illustrate cases where SMT-based query expan-
sion improves results over baseline performance, but
local expansion decreases performance by introduc-
ing irrelevant terms. In (4) retrieval performance is
improved over the baseline for both expansion tech-
niques. In (5) both local and SMT-based expansion
introduce terms that decrease retrieval performance
compared to retrieval without expansion.
6 Conclusion
We presented two techniques for query expansion in
answer retrieval that are based on SMT technology.
Our method for question-answer translation uses a
large corpus of question-answer pairs extracted from
FAQ pages to learn a translation model from ques-
tions to answers. SMT-based paraphrasing utilizes
large amounts of bilingual data as a new informa-
tion source to extract phrase-level synonyms. Both
SMT-based techniques take the entire query context
into account when adding new terms to the orig-
inal query. In an experimental comparison with a
baseline tfidf approach and a local query expansion
technique on the task of answer retrieval from FAQ
pages, we showed a significant improvement of both
SMT-based query expansion over both baselines.
Despite the small-scale nature of our current ex-
perimental results, we hope to apply the presented
techniques to general web retrieval in future work.
Another task for future work is to scale up the ex-
traction of question-answer pair data in order to
provide an improved resource for question-answer
translation.
470
References
Eugene Agichtein, Steve Lawrence, and Luis Gravano.
2004. Learning to find answers to questions on
the web. ACM Transactions on Internet Technology,
4(2):129?162.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of (ACL?05), Ann Arbor, MI.
Adam L. Berger, Rich Caruana, David Cohn, Dayne Fre-
itag, and Vibhu Mittal. 2000. Bridging the lexical
chasm: Statistical approaches to answer-finding. In
Proceedings of SIGIR?00, Athens, Greece.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311.
Robin B. Burke, Kristian J. Hammond, and Vladimir A.
Kulyukin. 1997. Question answering from
frequently-asked question files: Experiences with the
FAQ finder system. AI Magazine, 18(2):57?66.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yo-ram Singer. 2006. Online passive-
agressive algorithms. Machine Learning, 7:551?585.
Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006. An-
swering the question you wish they had asked: The im-
pact of paraphrasing for question answering. In Pro-
ceedings of (HLT-NAACL?06), New York, NY.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of (ACL?03), Sapporo, Japan.
Sanda Harabagiu and Finley Lacatusu. 2004. Strategies
for advanced question answering. In Proceedings of
the HLT-NAACL?04 Workshop on Pragmatics of Ques-
tion Answering, Boston, MA.
Sanda Harabagiu, Dan Moldovan, Marius Pas?ca, Rada
Mihalcea, Mihai Surdeanu, Ra?zvan Bunescu, Roxana
G??rju, Vasile Rus, and Paul Mora?rescu. 2001. The
role of lexico-semantic feedback in open-domain tex-
tual question-answering. In Proceedings of (ACL?01),
Toulouse, France.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformulation
resource and web exploitation for question answering.
In Proceedings of TREC-11, Gaithersburg, MD.
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Michael
Junk, and Chin-Yew Lin. 2000. Question answering
in webclopedia. In Proceedings of TREC 9, Gaithers-
burg, MD.
Abraham Ittycheriah, Martin Franz, and Salim Roukos.
2001. IBM?s statistical question answering system. In
Proceedings of TREC 10, Gaithersburg, MD.
Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving
answers from frequently asked questions pages on the
web. In Proceedings of the Tenth ACM Conference on
Information and Knowledge Management (CIKM?05),
Bremen, Germany.
Thorsten Joachims. 2003. Transductive learning
via spectral graph partitioning. In Proceedings of
ICML?03, Washington, DC.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of (HLT-NAACL?03), Edmonton, Cananda.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Journal of Natural
Language Engineering, 7(3):343?360.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
(HLT-NAACL?03), Edmonton, Cananda.
John Prager, Jennifer Chu-Carroll, and Krysztof Czuba.
2001. Use of wordnet hypernyms for answering what-
is questions. In Proceedings of TREC 10, Gaithers-
burg, MD.
Yonggang Qiu and H. P. Frei. 1993. Concept based query
expansion. In Proceedings of SIGIR?93, Pittsburgh,
PA.
Dragomir R. Radev, Hong Qi, Zhiping Zheng, Sasha
Blair-Goldensohn, Zhu Zhang, Weigo Fan, and John
Prager. 2001. Mining the web for answers to natu-
ral language questions. In Proceedings of (CIKM?01),
Atlanta, GA.
Radu Soricut and Eric Brill. 2006. Automatic question
answering using the web: Beyond the factoid. Journal
of Information Retrieval - Special Issue on Web Infor-
mation Retrieval, 9:191?206.
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of SI-
GIR?94, Dublin, Ireland.
Jinxi Xu and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In Proceed-
ings of SIGIR?96, Zurich, Switzerland.
471
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 368?375,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Automated Vocabulary Acquisition and Interpretation
in Multimodal Conversational Systems
Yi Liu Joyce Y. Chai Rong Jin
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824, USA
{liuyi3, jchai, rongjin}@cse.msu.edu
Abstract
Motivated by psycholinguistic findings that
eye gaze is tightly linked to human lan-
guage production, we developed an unsuper-
vised approach based on translation models
to automatically learn the mappings between
words and objects on a graphic display dur-
ing human machine conversation. The ex-
perimental results indicate that user eye gaze
can provide useful information to establish
such mappings, which have important impli-
cations in automatically acquiring and inter-
preting user vocabularies for conversational
systems.
1 Introduction
To facilitate effective human machine conversation,
it is important for a conversational system to have
knowledge about user vocabularies and understand
how these vocabularies are mapped to the internal
entities for which the system has representations.
For example, in a multimodal conversational system
that allows users to converse with a graphic inter-
face, the system needs to know what vocabularies
users tend to use to describe objects on the graphic
display and what (type of) object(s) a user is attend-
ing to when a particular word is expressed. Here,
we use acquisition to refer to the process of acquir-
ing relevant vocabularies describing internal entities,
and interpretation to refer to the process of automat-
ically identifying internal entities given a particular
word. Both acquisition and interpretation have been
traditionally approached by either knowledge engi-
neering (e.g., manually created lexicons) or super-
vised learning from annotated data. In this paper,
we describe an unsupervised approach that relies
on naturally co-occurred eye gaze and spoken utter-
ances during human machine conversation to auto-
matically acquire and interpret vocabularies.
Motivated by psycholinguistic studies (Just and
Carpenter, 1976; Griffin and Bock, 2000; Tenenhaus
et al, 1995) and recent investigations on computa-
tional models for language acquisition and ground-
ing (Siskind, 1995; Roy and Pentland, 2002; Yu
and Ballard, 2004), we are particularly interested in
two unique questions related to multimodal conver-
sational systems: (1) In a multimodal conversation
that involves more complex tasks (e.g., both user
initiated tasks and system initiated tasks), is there
a reliable temporal alignment between eye gaze and
spoken references so that the coupled inputs can be
used for automated vocabulary acquisition and inter-
pretation? (2) If such an alignment exists, how can
we model this alignment and automatically acquire
and interpret the vocabularies?
To address the first question, we conducted an
empirical study to examine the temporal relation-
ships between eye fixations and their correspond-
ing spoken references. As shown later in section 4,
although a larger variance (compared to the find-
ings from psycholinguistic studies) exists in terms of
how eye gaze is linked to speech production during
human machine conversation, eye fixations and the
corresponding spoken references still occur in a very
close vicinity to each other. This natural coupling
between eye gaze and speech provides an opportu-
nity to automatically learn the mappings between
368
words and objects without any human supervision.
Because of the larger variance, it is difficult to
apply rule-based approaches to quantify this align-
ment. Therefore, to address the second question,
we developed an approach based on statistical trans-
lation models to explore the co-occurrence patterns
between eye fixated objects and spoken references.
Our preliminary experiment results indicate that the
translation model can reliably capture the mappings
between the eye fixated objects and the correspond-
ing spoken references. Given an object, this model
can provide possible words describing this object,
which represents the acquisition process; given a
word, this model can also provide possible objects
that are likely to be described, which represents the
interpretation process.
In the following sections, we first review some re-
lated work and introduce the procedures used to col-
lect eye gaze and speech data during human machine
conversation. We then describe our empirical study
and the unsupervised approach based on translation
models. Finally, we present experiment results and
discuss their implications in natural language pro-
cessing applications.
2 Related Work
Our work is motivated by previous work in the fol-
lowing three areas: psycholinguistics studies, multi-
modal interactive systems, and computational mod-
eling of language acquisition and grounding.
Previous psycholinguistics studies have shown
that the direction of gaze carries information about
the focus of the user?s attention (Just and Carpenter,
1976). Specifically, in human language processing
tasks, eye gaze is tightly linked to language produc-
tion. The perceived visual context influences spo-
ken word recognition and mediates syntactic pro-
cessing (Tenenhaus et al, 1995). Additionally, be-
fore speaking a word, the eyes usually move to the
objects to be mentioned (Griffin and Bock, 2000).
These psycholinguistics findings have provided a
foundation for our investigation.
In research on multimodal interactive systems, re-
cent work indicates that the speech and gaze inte-
gration patterns can be modeled reliably for indi-
vidual users and therefore be used to improve mul-
timodal system performances (Kaur et al, 2003).
Studies have also shown that eye gaze has a poten-
tial to improve resolution of underspecified referring
expressions in spoken dialog systems (Campana et
al., 2001) and to disambiguate speech input (Tanaka,
1999). In contrast to these earlier studies, our work
focuses on a different goal of using eye gaze for au-
tomated vocabulary acquisition and interpretation.
The third area of research that influenced our
work is computational modeling of language acqui-
sition and grounding. Recent studies have shown
that multisensory information (e.g., through vision
and language processing) can be combined to effec-
tively acquire words to their perceptually grounded
objects in the environment (Siskind, 1995; Roy and
Pentland, 2002; Yu and Ballard, 2004). Especially in
(Yu and Ballard, 2004), an unsupervised approach
based on a generative correspondence model was
developed to capture the mapping between spoken
words and the occurring perceptual features of ob-
jects. This approach is most similar to the transla-
tion model used in our work. However, compared
to this work where multisensory information comes
from vision and language processing, our work fo-
cuses on a different aspect. Here, instead of applying
vision processing on objects, we are interested in eye
gaze behavior when users interact with a graphic dis-
play. Eye gaze is an implicit and subconscious input
modality during human machine interaction. Eye
gaze data inevitably contain a significant amount of
noise. Therefore, it is the goal of this paper to exam-
ine whether this modality can be utilized for vocab-
ulary acquisition for conversational systems.
3 Data Collection
We used a simplified multimodal conversational sys-
tem to collect synchronized speech and eye gaze
data. A room interior scene was displayed on a com-
puter screen, as shown in Figure 1. While watching
the graphical display, users were asked to communi-
cate with the system on topics about the room dec-
orations. A total of 28 objects (e.g., multiple lamps
and picture frames, a bed, two chairs, a candle, a
dresser, etc., as marked in Figure 1) are explicitly
modeled in this scene. The system is simplified in
the sense that it only supports 14 tasks during human
machine interaction. These tasks are designed to
cover both open-ended utterances (e.g., the system
369
Figure 1: The room interior scene for user studies.
For easy reference, we give each object an ID. These
IDs are hidden from the system users.
asks users to describe the room) and more restricted
utterances (e.g., the system asks the user whether
he/she likes the bed) that are commonly supported in
conversational systems. Seven human subjects par-
ticipated in our study.
User speech inputs were recorded using the Au-
dacity software1, with each utterance time-stamped.
Eye movements were recorded using an EyeLink II
eye tracker sampled at 250Hz. The eye tracker au-
tomatically saved two-dimensional coordinates of a
user?s eye fixations as well as the time-stamps when
the fixations occurred.
The collected raw gaze data is extremely noisy.
To refine the gaze data, we further eliminated in-
valid and saccadic gaze points (known as ?saccadic
suppression? in vision studies). Since eyes do not
stay still but rather make small, frequent jerky move-
ments, we also smoothed the data by averaging
nearby gaze locations to identify fixations.
4 Empirical Study on Speech-Gaze
Alignment
Based on the data collected, we investigated the tem-
poral alignment between co-occurred eye gaze and
spoken utterances. In particular, we examined the
temporal alignment between eye gaze fixations and
the corresponding spoken references (i.e., the spo-
ken words that are used to refer to the objects on the
graphic display).
According to the time-stamp information, we can
1http://audacity.sourceforge.net/
measure the length of time gap between a user?s eye
fixation falling on an object and the corresponding
spoken reference being uttered (which we refer to
as ?length of time gap? for brevity). Also, we can
count the number of times that user fixations hap-
pen to change their target objects during this time
gap (which we refer to as ?number of fixated object
changes? for brevity). The nine most frequently oc-
curred spoken references in utterances from all users
(as shown in Table 1) are chosen for this empirical
study. For each of those spoken references, we use
human judgment to decide which object is referred
to. Then, from both before and after the onset of
the spoken reference, we find the closest occurrence
of the fixation falling on that particular object. Al-
together we have 96 such speech-gaze pairs. In 54
pairs, the eye gaze fixation occurred before the cor-
responding speech reference was uttered; and in the
other 42 pairs, the eye fixation occurred after the
corresponding speech reference was uttered. This
observation suggests that in human machine conver-
sation, eye fixation on an object does not necessarily
always proceed the utterance of the corresponding
speech reference.
Further, we computed the average absolute length
of the time gap and the average number of fixated
object changes, as well as their variances for each of
5 selected users2 as shown in Table 1. From Table 1,
it is easy to observe that: (I) A spoken reference al-
ways appears within a short period of time (usually
1-2 seconds) before or after the corresponding eye
gaze fixation. But, the exact length of the period is
far from constant. (II) It is not necessary for a user
to utter the corresponding spoken reference imme-
diately before or after the eye gaze fixation falls on
that particular object. Eye gaze fixations may move
back and forth. Between the time an object is fixated
and the corresponding spoken reference is uttered, a
user?s eye gaze may fixate on a few other objects
(reflected by the average number of eye fixated ob-
ject changes shown in the table). (III) There is a
large variance in both the length of time gap and the
number of fixated object changes in terms of 1) the
same user and the same spoken reference at differ-
ent time-stamps, 2) the same user but different spo-
2The other two users are not selected because the nine se-
lected words do not appear frequently in their utterances.
370
Spoken Average Absolute Length of Time Gap (in seconds) Average Number of Eye Fixated Object Changes
Reference User 1 User 2 User 3 User 4 User 5 User 1 User 2 User 3 User 4 User 5
bed 1.27? 1.40 1.02? 0.65 0.32? 0.21 0.59? 0.77 2.57? 3.25 2.1? 3.2 2.1? 2.2 0.4? 0.5 1.4? 2.2 5.3? 7.9
tree - 0.24? 0.24 - - - - 0.0? 0.0 - - -
window - 0.67? 0.74 - - 1.95? 3.20 - 0.0? 0.0 - - 3.3? 5.9
mirror - 1.04? 1.36 - - - - 1.0? 1.4 - - -
candle - - 3.64? 0.59 - - - - 8.5? 2.1 - -
waterfall 1.80? 1.12 - - - - 5.5? 4.9 - - - -
painting 0.10? 0.10 - - - - 0.2? 0.4 - - - -
lamp 0.74? 0.54 1.70? 0.99 0.26? 0.35 1.98? 1.72 2.84? 2.42 1.3? 1.3 1.8? 1.5 0.3? 0.6 4.8? 4.3 2.7? 2.2
door 2.47? 0.84 - - 2.49? 1.90 6.36? 2.29 5.0? 2.6 - - 6.7? 5.5 13.3? 6.7
Table 1: The average absolute length of time and the number of eye fixated object changes within the time
gap of eye gaze and corresponding spoken references. Variances are also listed. Some of the entries are not
available because the spoken references were never or rarely used by the corresponding users.
ken references, and 3) the same spoken reference but
different users. We believe this is due to the different
dialog scenarios and user language habits.
To summarize our empirical study, we find that
in human machine conversation, there still exists a
natural temporal coupling between user speech and
eye gaze, i.e. the spoken reference and the corre-
sponding eye fixation happen within a close vicinity
of each other. However, a large variance is also ob-
served in terms of these temporal vicinities, which
indicates an intrinsically more complex gaze-speech
pattern. Therefore, it is hard to directly quantify
the temporal or ordering relationship between spo-
ken references and corresponding eye fixated objects
(for example, through rules).
To better handle the complexity in the gaze-
speech pattern, we propose to use statistical transla-
tion models. Given a time window of enough length,
a speech input that contains a list of spoken refer-
ences (e.g., definite noun phrases) is always accom-
panied by a list of naturally occurred eye fixations
and therefore a list of objects receiving those fixa-
tions. All those pairs of speech references and cor-
responding fixated objects could be viewed as paral-
lel, i.e. they co-occur within the time window. This
situation is very similar to the training process of
translation models in statistical machine translation
(Brown et al, 1993), where parallel corpus is used to
find the mappings between words from different lan-
guages by exploiting their co-occurrence patterns.
The same idea can be borrowed here: by exploring
the co-occurrence statistics, we hope to uncover the
exact mapping between those eye fixated objects and
spoken references. The intuition is that, the more of-
ten a fixation is found to exclusively co-occur with a
spoken reference, the more likely a mapping should
be established between them.
5 Translation Models for Vocabulary
Acquisition and Interpretation
Formally, we denote the set of observations by
D = {wi,oi}Ni=1 where wi and oi refers to
the i-th speech utterance (i.e., a list of words
of spoken references) and the i-th corresponding
eye gaze pattern (i.e., a list of eye fixated ob-
jects) respectively. When we study the prob-
lem of mapping given objects to words (for vo-
cabulary acquisition), the parameter space ? =
{Pr(wj |ok), 1 ? j ? mw, 1 ? k ? mo} consists of
the mapping probabilities of an arbitrary word wj
to an arbitrary object ok, where mw and mo repre-
sent the total number of unique words and objects
respectively. Those mapping probabilities are sub-
ject to constraints ?mwj=1 Pr(wj |ok) = 1. Note that
Pr(wj |ok) = 0 if the corresponding word wj and ok
never co-occur in any observed list pair (wi,oi).
Let lwi and loi denote the length of lists wi and
oi respectively. To distinguish with the notations
wj and ok whose subscripts are indices for unique
words and objects respectively, we use w?i,j to de-
note the word in the j-th position of the list wi and
o?i,k to denote the object in the k-th position of the
list oi. In translation models, we assume that any
word in the list wi is mapped to an object in the cor-
responding list oi or a null object (we reserve the
position 0 for it in every object list). To denote all
the word-object mappings in the i-th list pair, we in-
troduce an alignment vector ai, whose element ai,j
takes the value k if the word w?i,j is mapped to o?i,k.
Then, the likelihood of the observations given the
371
parameters can be computed as follows
Pr(D;?) =
N
?
i=1
Pr(wi|oi) =
N
?
i=1
?
ai
Pr(wi,ai|oi)
=
N
?
i=1
?
ai
Pr(lwi |oi)
(loi + 1)l
w
i
lwi
?
j=1
Pr(w?i,j |o?ai,j )
=
N
?
i=1
Pr(lwi |oi)
(loi + 1)l
w
i
?
ai
lwi
?
j=1
Pr(w?i,j |o?ai,j )
Note that the following equation holds:
lwi
?
j=1
loi
?
k=0
Pr(w?i,j |o?i,k) =
loi
?
ai,1=1
? ? ?
loi
?
ai,lwi =1
lwi
?
j=1
Pr(w?i,j |o?ai,j )
where the right-hand side is actually the expansion
of
?
ai
?lwi
j Pr(w?i,j |o?ai,j ). Therefore, the likelihood
can be simplified as
Pr(D;?) =
N
?
i=1
Pr(lwi |oi)
(loi + 1)l
w
i
lwi
?
j=1
loi
?
k=0
Pr(w?i,j |o?i,k)
Switching to the notations wj and ok, we have
Pr(D;?)=
N
?
i=1
Pr(lwi |oi)
(loi + 1)l
w
i
mw
?
j=1
[ mo
?
k=0
Pr(wj |ok)?oi,k
]?wi,j
where ?wi,j = 1 if w?i,j ? wi and ?wi,j = 0 otherwise,
and ?oi,k = 1 if o?i,k ? oi and ?oi,k = 0 otherwise.
Finally, the translation model can be formalized
as the following optimization problem
arg max? log Pr(D;?)
s.t.
mw
?
j=1
Pr(wj |ok) = 1,?k
This optimization problem can be solved by the EM
algorithm (Brown et al, 1993).
The above model is developed in the con-
text of mapping given objects to words, i.e., its
solution yields a set of conditional probabilities
{Pr(wj |ok),?j} for each object ok, indicating how
likely every word is mapped to it. Similarly, we
can develop the model in the context of mapping
given words to objects (for vocabulary interpreta-
tion), whose solution leads to another set of prob-
abilities {Pr(ok|wj),?k} for each word wj indicat-
ing how likely every object is mapped to it. In our
experiments, both models are implemented and we
will present the results later.
6 Experiments
We experimented our proposed statistical translation
model on the collected data mentioned in Section 3.
6.1 Preprocessing
The main purpose of preprocessing is to create a
?parallel corpus? for training a translation model.
Here, the ?parallel corpus? refers to a series of
speech-gaze pairs, each of them consisting of a list
of words from the spoken references in the user ut-
terances and a list of objects that are fixated upon
within the same time window.
Specifically, we first transcribed the user speech
into scripts by automatic speech recognition soft-
ware and then refined them manually. A time-stamp
was associated with each word in the speech script.
Further, we detected long pauses in the speech script
as splitting points to create time windows, since a
long pause usually marks the start of a sentence
that indicates a user?s attention shift. In our exper-
iment, we set the threshold of judging a long pause
to be 1 second. From all the data gathered from 7
users, we get 357 such time windows (which typi-
cally contain 10-20 spoken words and 5-10 fixated
object changes).
Given a time window, we then found the objects
being fixated upon by eye gaze (represented by their
IDs as shown in Figure 1). Considering that eye gaze
fixation could occur during the pauses in speech, we
expanded each time window by a fixed length at both
its start and end to find the fixations. In our experi-
ments, the expansion length is set to 0.5 seconds.
Finally, we applied a part-of-speech tagger to
each sentence in the user script and only singled out
nouns as potential spoken references in the word list.
The Porter stemming algorithm was also used to get
the normalized forms of those nouns.
The translation model was trained based on this
preprocessed parallel data.
6.2 Evaluation Metrics
As described in Section 5, by using a statistical
translation model we can get a set of translation
probabilities, either from any given spoken word to
all the objects, or from any given object to all the
spoken words. To evaluate the two sets of trans-
lation probabilities, we use precision and recall as
372
#Rank Precision Recall #Rank Precision Recall
1 0.6667 0.2593 6 0.2302 0.5370
2 0.4524 0.3519 7 0.2041 0.5556
3 0.3810 0.4444 8 0.1905 0.5926
4 0.3095 0.4815 9 0.1799 0.6296
5 0.2667 0.5185 10 0.1619 0.6296
Table 2: Average precision/recall of mapping given
objects to words (i.e., acquisition)
#Rank Precision Recall #Rank Precision Recall
1 0.7826 0.3214 6 0.3043 0.7500
2 0.5870 0.4821 7 0.2671 0.7679
3 0.4638 0.5714 8 0.2446 0.8036
4 0.3804 0.6250 9 0.2293 0.8393
5 0.3478 0.7143 10 0.2124 0.8571
Table 3: Average precision/recall of mapping given
words to objects.(i.e., interpretation)
evaluation metrics.
Specifically, for a given object ok the trans-
lation model will yield a set of probabilities
{Pr(wj |ok),?j}. We can sort the probabilities and
get a ranked list. Let us assume that we have the
ground truth about all the spoken words to which
the given object should be mapped. Then, at a given
number n of top ranked words, the precision of map-
ping the given object ok to words is defined as
# words that ok is correctly mapped to
# words that ok is mapped to
and the recall is defined as
# words that ok is correctly mapped to
# words that ok should be mapped to
All the counting above is done within the top n rank.
Therefore, we can get different precision/recall at
different ranks. At each rank, the overall perfor-
mance can be evaluated by averaging the preci-
sion/recall for all the given objects. Human judg-
ment is used to decide whether an object-word map-
ping is correct or not, as ground truth for evaluation.
Similarly, based on the set of probabilities of map-
ping a given object with spoken words, we can
find a ranked list of objects for a given word, i.e.
{Pr(ok|wj),?k}. Thus, at a given rank the preci-
sion and recall of mapping a given word wj to ob-
jects can be measured.
6.3 Experiment Results
Vocabulary acquisition is the process of finding
the appropriate word(s) for any given object. For
the sake of statistical significance, our evaluation is
done on 21 objects that were mentioned at least 3
times by the users.
Table 2 gives the average precision/recall evalu-
ated at the top 10 ranks. As we can see, if we use
the most probable word acquired for each object,
about 66.67% of them are appropriate. With the
rank increasing, more and more appropriate words
can be acquired. About 62.96% of all the appropri-
ate words are included within the top 10 probable
words found. The results indicate that by using a
translation model, we can obtain the words that are
used by the users to describe the objects with rea-
sonable accuracy.
Table 4 presents the top 3 most probable words
found for each object. It shows that although there
may be more than one word appropriate to describe
a given object, those words with highest probabil-
ities always suggest the most popular way of de-
scribing the corresponding object among the users.
For example, for the object with ID 26, the word
candle gets a higher probability than the word
candlestick, which is in accordance with our
observation that in our user study, on most occasions
users tend to use the word candle rather than the
word candlestick.
Vocabulary interpretation is the process of find-
ing the appropriate object(s) for any given spoken
word. Out of 176 nouns in the user vocabulary,
we only evaluate those used at least three times for
statistical significance concerns. Further, abstract
words (such as reason, position) and general
words (such as room, furniture) are not eval-
uated since they do not refer to any particular objects
in the scene. Finally, 23 nouns remain for evalua-
tion.
We manually enumerated all the object(s) that
those 23 nouns refer to as the ground truth in our
evaluation. Note that a given noun can possibly
be used to refer to multiple objects, such as lamp,
since we have several lamps (with object ID 3, 8, 17,
and 23) in the experiment setting, and bed, since
bed frame, bed spread, and pillows (with object ID
19, 21, and 20 respectively) are all part of a bed.
Also, an object can be referred to by multiple nouns.
For example, the words painting, picture,
or waterfall can all be used to refer to the ob-
ject with ID 15.
373
Object Rank 1 Rank 2 Rank 3
1 paint (0.254) * wall (0.191) left (0.150)
2 pictur (0.305) * girl (0.122) niagara (0.095) *
3 wall (0.109) lamp (0.093) * floor (0.084)
4 upsid (0.174) * left (0.151) * paint (0.149) *
5 pictur (0.172) window (0.157) * wall (0.116)
6 window (0.287) * curtain (0.115) pictur (0.076)
7 chair (0.287) * tabl (0.088) bird (0.083)
9 mirror (0.161) * dresser (0.137) bird (0.098) *
12 room (0.131) lamp (0.127) left (0.069)
14 hang (0.104) favourit (0.085) natur (0.064)
15 thing (0.066) size (0.059) queen (0.057)
16 paint (0.211) * pictur (0.116) * forest (0.076) *
17 lamp (0.354) * end (0.154) tabl (0.097)
18 bedroom (0.158) side (0.128) bed (0.104)
19 bed (0.576) * room (0.059) candl (0.049)
20 bed (0.396) * queen (0.211) * size (0.176)
21 bed (0.180) * chair (0.097) orang (0.078)
22 bed (0.282) door (0.235) * chair (0.128)
25 chair (0.215) * bed (0.162) candlestick (0.124)
26 candl (0.145) * chair (0.114) candlestick (0.092) *
27 tree (0.246) * chair (0.107) floor (0.096)
Table 4: Words found for given objects. Each row
lists the top 3 most probable spoken words (being
stemmed) for the corresponding given object, with
the mapping probabilities in parentheses. Asterisks
indicate correctly identified spoken words. Note
that some objects are heavily overlapped, so the cor-
responding words are considered correct for all the
overlapping objects, such as bed being considered
correct for objects with ID 19, 20, and 21.
Word Rank 1 Rank 2 Rank 3 Rank 4
curtain 6 (0.305) * 5 (0.305) * 7 (0.133) 1 (0.121)
candlestick 25 (0.147) * 28 (0.135) 24 (0.131) 22 (0.117)
lamp 22 (0.126) 12 (0.094) 17 (0.093) * 25 (0.093)
dresser 12 (0.298) * 9 (0.294) * 13 (0.173) * 7 (0.104)
queen 20 (0.187) * 21 (0.182) * 22 (0.136) 19 (0.136) *
door 22 (0.200) * 27 (0.124) 25 (0.108) 24 (0.106)
tabl 9 (0.152) * 12 (0.125) * 13 (0.112) * 22 (0.107)
mirror 9 (0.251) * 12 (0.238) 8 (0.109) 13 (0.081)
girl 2 (0.173) 22 (0.128) 16 (0.099) 10 (0.074)
chair 22 (0.132) 25 (0.099) * 28 (0.085) 24 (0.082)
waterfal 6 (0.226) 5 (0.215) 1 (0.118) 9 (0.083)
candl 19 (0.156) 22 (0.139) 28 (0.134) 24 (0.131)
niagara 4 (0.359) * 2 (0.262) * 1 (0.226) 7 (0.045)
plant 27 (0.230) * 22 (0.181) 23 (0.131) 28 (0.117)
tree 27 (0.352) * 22 (0.218) 26 (0.100) 13 (0.062)
upsid 4 (0.204) * 12 (0.188) 9 (0.153) 1 (0.104) *
bird 9 (0.142) * 10 (0.138) 12 (0.131) 7 (0.121)
desk 12 (0.170) * 9 (0.141) * 19 (0.118) 8 (0.118)
bed 19 (0.207) * 22 (0.141) 20 (0.111) * 28 (0.090)
upsidedown 4 (0.243) * 3 (0.219) 6 (0.203) 5 (0.188)
paint 4 (0.188) * 16 (0.148) * 1 (0.137) * 15 (0.118) *
window 6 (0.305) * 5 (0.290) * 3 (0.085) 22 (0.065)
lampshad 3 (0.223) * 7 (0.137) 11 (0.137) 10 (0.137)
Table 5: Objects found for given words. Each row
lists the 4 most probable object IDs for the corre-
sponding given words (being stemmed), with the
mapping probabilities in parentheses. Asterisks in-
dicate correctly identified objects. Note that some
objects are heavily overlapped, such as the candle
(with object ID 26) and the chair (with object ID
25), and both were considered correct for the re-
spective spoken words.
Table 3 gives the average precision/recall evalu-
ated at the top 10 ranks. As we can see, if we use the
most probable object found for each speech word,
about 78.26% of them are appropriate. With the rank
increasing, more and more appropriate objects can
be found. About 85.71% of all the appropriate ob-
jects are included within the top 10 probable objects
found. The results indicate that by using a trans-
lation model, we can predict the objects from user
spoken words with reasonable accuracy.
Table 5 lists the top 4 probable objects found for
each spoken word being evaluated. A close look re-
veals that in general, the top ranked objects tend to
gather around the correct object for a given spoken
word. This is consistent with the fact that eye gaze
tends to move back and forth. It also indicates that
the mappings established by the translation model
can effectively find the approximate area of the cor-
responding fixated object, even if it cannot find the
object due to the noisy and jerky nature of eye gaze.
The precision/recall in vocabulary acquisition is
not as high as that in vocabulary interpretation, par-
tially due to the relatively small scale of our exper-
iment data. For example, with only 7 users? speech
data on 14 conversational tasks, some words were
only spoken a few times to refer to an object, which
prevented them from getting a significant portion of
probability mass among all the words in the vocab-
ulary. This degrades both precision and recall. We
believe that in large scale experiments or real-world
applications, the performance will be improved.
7 Discussion and Conclusion
Previous psycholinguistic findings have shown that
eye gaze is tightly linked with human language pro-
duction. During human machine conversation, our
study shows that although a larger variance is ob-
served on how eye fixations are exactly linked with
corresponding spoken references (compared to the
psycholinguistic findings), eye gaze in general is
closely coupled with corresponding referring ex-
pressions in the utterances. This close coupling na-
ture between eye gaze and speech utterances pro-
vides an opportunity for the system to automatically
374
acquire different words related to different objects
without any human supervision. To further explore
this idea, we developed a novel unsupervised ap-
proach using statistical translation models.
Our experimental results have shown that this ap-
proach can reasonably uncover the mappings be-
tween words and objects on the graphical display.
The main advantages of this approach include: 1) It
is an unsupervised approach with minimum human
inference; 2) It does not need any prior knowledge to
train a statistical translation model; 3) It yields prob-
abilities that indicate the reliability of the mappings.
Certainly, our current approach is built upon sim-
plified assumptions. It is quite challenging to in-
corporate eye gaze information since it is extremely
noisy with large variances. Recent work has shown
that the effect of eye gaze in facilitating spoken lan-
guage processing varies among different users (Qu
and Chai, 2007). In addition, visual properties of
the interface also affect user gaze behavior and thus
influence the predication of attention (Prasov et al,
2007) based on eye gaze. Our future work will de-
velop models to address these variations.
Nevertheless, the results from our current work
have several important implications in building ro-
bust conversational interfaces. First of all, most
conversational systems are built with static knowl-
edge space (e.g., vocabularies) and can only be up-
dated by the system developers. Our approach can
potentially allow the system to automatically ac-
quire knowledge and vocabularies based on the nat-
ural interactions with the users without human in-
tervention. Furthermore, the automatically acquired
mappings between words and objects can also help
language interpretation tasks such as reference res-
olution. Given the recent advances in eye track-
ing technology (Duchowski, 2002), integrating non-
intrusive and high performance eye trackers with
conversational interfaces becomes feasible. The
work reported here can potentially be integrated in
practical systems to improve the overall robustness
of human machine conversation.
Acknowledgment
This work was supported by funding from National
Science Foundation (IIS-0347548, IIS-0535112,
and IIS-0643494) and Disruptive Technology Of-
fice. The authors would like to thank Zahar Prasov
for his contribution to data collection.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
E. Campana, J. Baldridge, J. Dowding, B. A. Hockey,
R. Remington, and L. S. Stone. 2001. Using eye
movements to determine referents in a spoken dialog
system. In Proceedings of PUI?01.
A. T. Duchowski. 2002. A breath-first survey of eye
tracking applications. Behavior Research methods, In-
struments, and Computers, 33(4).
Z. M. Griffin and K. Bock. 2000. What the eyes say
about speaking. Psychological Science, 11:274?279.
M. A. Just and P. A. Carpenter. 1976. Eye fixations and
cognitive processes. Cognitive Psychology, 8:441?
480.
M. Kaur, M. Tremaine, N. Huang, J. Wilder, Z. Gacovski,
F. Flippo, and C. S. Mantravadi. 2003. Where is ?it??
Event synchronization in gaze-speech input systems.
In Proceedings of ICMI?03, pages 151?157.
Z. Prasov, J. Y. Chai, and H. Jeong. 2007. Eye gaze
for attention prediction in multimodal human-machine
conversation. In 2007 Spring Symposium on Inter-
action Challenges for Artificial Assistants, Palo Alto,
California, March.
S. Qu and J. Y. Chai. 2007. An exploration of eye gaze
in spoken language processing for multimodal con-
versational interfaces. In NAACL?07, pages 284?291,
Rochester, New York, April.
D. Roy and A. Pentland. 2002. Learning words from
sights and sounds, a computational model. Cognitive
Science, 26(1):113?1146.
J. M. Siskind. 1995. Grounding language in perception.
Artificial Intelligence Review, 8:371?391.
K. Tanaka. 1999. A robust selection system using real-
time multi-modal user-agent interactions. In Proceed-
ings of IUI?99, pages 105?108.
M. K. Tenenhaus, M. Sivey-Knowlton, E. Eberhard, and
J. Sedivy. 1995. Integration of visual and linguistic
information during spoken language comprehension.
Science, 268:1632?1634.
C. Yu and D. H. Ballard. 2004. On the integration of
grounding language and learning objects. Proceedings
of AAAI?04.
375
Query Rewriting Using Monolingual
Statistical Machine Translation
Stefan Riezler?
Google
Yi Liu??
Google
Long queries often suffer from low recall in Web search due to conjunctive term matching. The
chances of matching words in relevant documents can be increased by rewriting query terms into
new terms with similar statistical properties. We present a comparison of approaches that deploy
user query logs to learn rewrites of query terms into terms from the document space. We show
that the best results are achieved by adopting the perspective of bridging the ?lexical chasm?
between queries and documents by translating from a source language of user queries into a
target language of Web documents. We train a state-of-the-art statistical machine translation
model on query-snippet pairs from user query logs, and extract expansion terms from the query
rewrites produced by the monolingual translation system. We show in an extrinsic evaluation in
a real-world Web search task that the combination of a query-to-snippet translation model with
a query language model achieves improved contextual query expansion compared to a state-of-
the-art query expansion model that is trained on the same query log data.
1. Introduction
Information Retrieval (IR) applications have been notoriously resistant to improvement
attempts by Natural Language Processing (NLP). With a few exceptions for specialized
tasks,1 the contribution of part-of-speech taggers, syntactic parsers, or ontologies of
nouns or verbs has been inconclusive. In this article, instead of deploying NLP tools
or ontologies, we apply NLP ideas to IR problems. In particular, we take a viewpoint
that looks at the problem of the word mismatch between queries and documents in
Web search as a problem of translating from a source language of user queries into a
target language of Web documents. We concentrate on the task of query expansion by
query rewriting. This task consists of adding expansion terms with similar statistical
properties to the original query in order to increase the chances of matching words in
relevant documents, and also to decrease the ambiguity of the query that is inherent
in natural language. We focus on a comparison of models that learn to generate query
? Brandschenkestrasse 110, 8002 Zu?rich, Switzerland. E-mail: riezler@gmail.com.
?? 1600 Amphitheatre Parkway, Mountain View, CA. E-mail: yliu@google.com.
1 See for example Sable, McKeown, and Church (2002), who report improvements in text categorization by
using tagging and parsing for the task of categorizing captioned images.
Submission received: 19 June 2009; revised submission received: 4 March 2010; accepted for publication:
12 May 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
rewrites from large amounts of user query logs, and use query expansion in Web search
for an extrinsic evaluation of the produced rewrites. The experimental query expansion
setup used in this article is simple and direct: For a given set of randomly selected
queries, n-best rewrites are produced. From the changes introduced by the rewrites,
expansion terms are extracted and added as alternative terms to the query, leaving the
ranking function untouched.
Figure 1 shows expansions of the queries herbs for chronic constipation and herbs for
mexican cooking using AND and OR operators. Conjunctive matching of all query terms
is the default, and indicated by the AND operator. Expansion terms are added using
the OR operator. The example in Figure 1 illustrates the key requirements to successful
query expansion, namely, to find appropriate expansions in the context of the query.
While remedies, medicine, or supplement are appropriate expansions in the context of the
first query, they would cause a severe query drift if used in the second query. In the
context of the second query, spices is an appropriate expansion for herbs, whereas this
expansion would again not work for the first query.
The central idea behind our approach is to combine the orthogonal information
sources of the translation model and the language model to expand query terms in
context. The translation model proposes expansion candidates, and the query language
model performs a selection in the context of the surrounding query terms. Thus, in
combination, the incessant problems of term ambiguity and query drift can be solved.
One of the goals of this article is to show that existing SMT technology is readily
applicable to this task. We apply SMT to large parallel data of queries on the source
side, and snippets of clicked search results on the target side. Snippets are short text
fragments that represent the parts of the result pages that are most relevant to the
queries, for example, in terms of query term matches. Although the use of snippets
instead of the full documents makes our approach efficient, it introduces noise because
text fragments are used instead of full sentences. However, we show that state-of-the-
art statistical machine translation (SMT) technology is in fact robust and flexible enough
to capture the peculiarities of the language pair of user queries and result snippets.
We evaluate our system in a comparative, extrinsic evaluation in a real-world Web
search task. We compare our approach to the expansion system of Cui et al (2002)
that is trained on the same user logs data and has been shown to produce significant
improvements over the local feedback technique of Xu and Croft (1996) in a standard
evaluation on TREC data. Our extrinsic evaluation is done by embedding the expansion
systems into a real-world search engine, and comparing the two systems based on
the search results that are triggered by the respective query expansions. Our results
show that the combination of translation and language model of a state-of-the-art
SMT model produces high-quality rewrites and outperforms the expansion model of
Cui et al (2002).
In the following, we will discuss related work (Section 2) and quickly sketch
Cui et al (2002)?s approach (Section 3). Then we will recapitulate the essentials of
Figure 1
Search queries herbs for chronic constipation and herbs for mexican cooking integrating expansion
terms into OR-nodes in conjunctive matching.
570
Riezler and Liu Query Rewriting Using Monolingual Statistical Machine Translation
state-of-the-art SMT and describe how to adapt an SMT system to the query expansion
task (Section 4). Results of the extrinsic experimental evaluation are presented in Sec-
tion 5. The presented results are based on earlier results presented in Riezler, Liu, and
Vasserman (2008), and extended by deeper analyses and further experiments.
2. Related Work
Standard query expansion techniques such as local feedback, or pseudo-relevance feed-
back, extract expansion terms from the topmost documents retrieved in an initial re-
trieval round (Xu and Croft 1996). The local feedback approach is costly and can lead to
query drift caused by irrelevant results in the initial retrieval round. Most importantly,
though, local feedback models do not learn from data, in contrast to the approaches
described in this article.
Recent research in the IR community has increasingly focused on deploying user
query logs for query reformulations (Huang, Chien, and Oyang 2003; Fonseca et al
2005; Jones et al 2006), query clustering (Beeferman and Berger 2000; Wen, Nie, and
Zhang 2002; Baeza-Yates and Tiberi 2007), or query similarity (Raghavan and Sever
1995; Fitzpatrick and Dent 1997; Sahami and Heilman 2006). The advantage of these ap-
proaches is that user feedback is readily available in user query logs and can efficiently
be precomputed. Similarly to this recent work, our approach uses data from user query
logs, but as input to a monolingual SMT model for learning query rewrites.
The SMT viewpoint has been introduced to the field of IR by Berger and Lafferty
(1999) and Berger et al (2000), who proposed to bridge the ?lexical chasm? by a retrieval
model based on IBM Model 1 (Brown et al 1993). Since then, ranking models based
on monolingual SMT have seen various applications, especially in areas like Question
Answering where a large lexical gap between questions and answers has to be bridged
(Berger et al 2000; Echihabi and Marcu 2003; Soricut and Brill 2006; Riezler et al 2007;
Surdeanu, Ciaramita, and Zaragoza 2008; Xue, Jeon, and Croft 2008). Whereas most
applications of SMT ideas to IR problems used translation system scores for (re)ranking
purposes, only a few approaches use SMT to generate actual query rewrites (Riezler,
Liu, and Vasserman 2008). Similarly to Riezler, Liu, and Vasserman (2008), we use SMT
to produce actual rewrites rather than for (re)ranking, and evaluate the rewrites in a
query expansion task that leaves the ranking model of the search engine untouched.
Lastly, monolingual SMT has been established in the NLP community as a useful
expedient for paraphrasing, that is, the task of reformulating phrases or sentences into
semantically similar strings (Quirk, Brockett, and Dolan 2004; Bannard and Callison-
Burch 2005). Although the use of the SMT in paraphrasing goes beyond pure ranking to
actual rewriting, SMT-based paraphrasing has to our knowledge not yet been applied
to IR tasks.
3. Query Expansion by Query?Document Term Correlations
The query expansion model of Cui et al (2002) is based on the principle that if queries
containing one term often lead to the selection of documents containing another term,
then a strong relationship between the two terms can be assumed. Query terms and
document terms are linked via sessions in which users click on documents in the
retrieval result for the query. Cui et al define a session as follows:
session := <query text>[clicked document]*
571
Computational Linguistics Volume 36, Number 3
According to this definition, a link is established if at least one user clicks on a document
in the retrieval results for a query. Because query logs contain sessions from different
users, an aggregation of clicks over sessions will reflect the preferences of multiple users.
Cui et al (2002) compute the following probability distribution of document words wd
given query words wq from counts over clicked documents D aggregated over sessions:
P(wd|wq) =
?
D
P(wd|D)P(D|wq) (1)
The first term in Equation (1) is a normalized tfidf weight of the document term in
the clicked document, and the second term is the relative cooccurrence of the clicked
document and query term.
Because Equation (1) calculates expansion probabilities for each term separately,
Cui et al (2002) introduce the following cohesion formula that respects the whole query
Q by aggregating the expansion probabilities for each query term:
CoWeightQ(w
d) = ln(
?
wq?Q
P(wd|wq) + 1) (2)
In contrast to local feedback techniques (Xu and Croft 1996), Cui et al (2002)?s
algorithm allows us to precompute term correlations off-line by collecting counts from
query logs. This reliance on pure frequency counting is both a blessing and a curse:
On the one hand it allows for efficient non-iterative estimation, but on the other hand
it makes the implicit assumption that data sparsity will be overcome by counting
from huge data sets. The only attempt at smoothing that is made in this approach is
shifting the burden to words in the query context, using Equation (2), when Equation (1)
assigns zero probability to unseen pairs. Nonetheless, Cui et al (2002) show significant
improvements over the local feedback technique of Xu and Croft (1996) in an evaluation
on TREC data.
4. Query Expansion Using Monolingual SMT
4.1 Linear Models for SMT
The job of a translation system is defined in Och and Ney (2004) as finding the English
string e? that is a translation of a foreign string f using a linear combination of feature
functions hm(e, f) and weights ?m as follows:
e? = arg max
e
M
?
m=1
?mhm(e, f)
As is now standard in SMT, several complex features such as lexical translation models
and phrase translation models, trained in source-target and target-source directions, are
combined with language models and simple features such as phrase and word counts.
In the linear model formulation, SMT can be thought of as a general tool for computing
string similarities or for string rewriting.
572
Riezler and Liu Query Rewriting Using Monolingual Statistical Machine Translation
4.2 Word Alignment
The relationship of translation model and alignment model for source language string
f = f J1 and target string e = e
I
1 is via a hidden variable describing an alignment mapping
from source position j to target position aj:
P( f J1|eI1) =
?
aJ1
P( f J1, a
J
1|eI1)
The alignment aJ1 contains so-called null-word alignments aj = 0 that align source words
to the empty word.
In our approach, ?sentence aligned? parallel training data are prepared by pairing
user queries with snippets of search results clicked for the respective queries. The
translation models used are based on a sequence of word alignment models, whereas in
our case three Model-1 iterations and three HMM iterations were performed. Another
important adjustment in our approach is the setting of the null-word alignment proba-
bility to 0.9 in order to account for the difference in sentence length between queries and
snippets. This setting improves alignment precision by filtering out noisy alignments
and instead concentrating on alignments with high support in the training data.
4.3 Phrase Extraction
Statistical estimation of alignment models is done by maximum-likelihood estimation
of sentence-aligned strings {(fs, es) : s = 1, . . . , S}. Because each sentence pair is linked
by a hidden alignment variable a = aJ1, the optimal ?? is found using unlabeled-data log-
likelihood estimation techniques such as the EM algorithm:
?? = arg max
?
S
?
s=1
?
a
p?(fs, a|es)
The (Viterbi-)alignment a?J1 that has the highest probability under a model is defined as
follows:
a?J1 = arg max
aJ1
p??( f
J
1, a
J
1|eI1)
Because a source?target algnment does not allow a source word to be aligned with two
or more target words, source?target and target?source alignments can be combined via
various heuristics to improve both recall and precision of alignments.
In our application, it is crucial to remove noise in the alignments of queries to
snippets. In order to achieve this, we symmetrize Viterbi alignments for source?target
and target?source directions by intersection only. That is, given two Viterbi alignments
A1 = {(aj, j)| aj > 0} and A2 = {(i, bi)| bi > 0}, the alignments in the intersection are de-
fined as A = A1 ? A2. Phrases are extracted as larger blocks of aligned words from the
alignments in the intersection, as described in Och and Ney (2004).
573
Computational Linguistics Volume 36, Number 3
4.4 Language Modeling
Language modeling in our approach deploys an n-gram language model that assigns
the following probability to a string wL1 of words:
P(wL1 ) =
L
?
i=1
P(wi|wi?11 )
?
L
?
i=1
P(wi|wi?1i?n+1)
Estimation of n-gram probabilities is done by counting relative frequencies of n-grams
in a corpus of user queries. Remedies for sparse data problems are achieved by various
smoothing techniques, as described in Brants et al (2007).
The most important departure of our approach from standard SMT is the use of a
language model trained on queries. Although this approach may seem counterintuitive
from the standpoint of the noisy-channel model for SMT (Brown et al 1993), it fits
perfectly into the linear model. Whereas in the first view a query language model
would be interpreted as a language model on the source language, in the linear model
directionality of translation is not essential. Furthermore, the ultimate task of a query
language model in our approach is to select appropriate phrase translations in the
context of the original query for query expansion. This is achieved perfectly by an
SMT model that assigns the identity translation as most probable translation to each
phrase. Descending the n-best list of translations, in effect the language model picks
alternative non-identity translations for a phrase in context of identity-translations of
the other phrases.
Another advantage of using identity translations and word reordering in our ap-
proach is the fact that, by preferring identity translations or word reorderings over
non-identity translations of source phrases, the SMT model can effectively abstain from
generating any expansion terms. This will happen if none of the candidate phrase
translations fits with high enough probability in the context of the whole query, as
assessed by the language model.
5. Evaluating Query Expansion in a Web Search Task
5.1 Data
The training data for the translation model and the correlation-based model consist of
pairs of queries and snippets for clicked results taken from query logs. Representing
documents by snippets makes it possible to create a parallel corpus that contains data of
roughly the same ?sentence? length. Furthermore, this makes iterative training feasible.
Queries and snippets are linked via clicks on result pages, where a parallel sentence pair
is introduced for each query and each snippet of its clicked results. This yields a data set
of 3 billion query?snippet pairs from which a phrase-table of 700 million query?snippet
phrase translations is extracted. A collection of data statistics for the training data is
shown in Table 1. The language model used in our experiment is a trigram language
model trained on English queries in user logs. n-grams were cut off at a minimum
frequency of 4. Data statistics for resulting unique n-grams are shown in Table 2.
574
Riezler and Liu Query Rewriting Using Monolingual Statistical Machine Translation
Table 1
Statistics of query?snippet training data.
query?snippet query snippet
pairs words words
tokens 3 billion 8 billion 25 billion
avg. length - 2.6 8.3
Table 2
Statistics of unique n-grams in language model.
1-grams 2-grams 3-grams
9 million 1.5 billion 5 billion
5.2 Query Expansion Setup
The setup for our extrinsic evaluation deploys a real-world search engine, google.com,
for a comparison of expansions from the SMT-based system, the correlation-based sys-
tem, and the correlation-based system using the language model as additional filter. All
expansion systems are trained on the same set of parallel training data. SMT modules
such as the language model and the translation models in source?target and target?
source directions are combined in a uniform manner in order to give the SMT and
correlation-based models the same initial conditions.
The expansion terms used in our experiments were extracted as follows: Firstly,
a set of 150,000 randomly extracted 3+ word queries was rewritten by each of the
systems. For each system, expansion terms were extracted from the 5-best rewrites, and
stored in a table that maps source phrases to target phrases in the context of the full
queries. For example, Table 3 shows unique 5-best translations of the SMT system for the
queries herbs for chronic constipation and herbs for mexican cooking. Phrases that are newly
introduced in the translations are highlighted in boldface. These phrases are extracted
for expansion and stored in a table that maps source phrases to target phrases in the
context of the query from which they were extracted. When applying the expansion
Table 3
Unique 5-best phrase-level translations of queries herbs for chronic constipation and herbs for
mexican cooking. Terms extracted for expansion are highlighted in boldface.
(herbs , herbs) (for , for) (chronic , chronic) (constipation , constipation)
(herbs , herb) (for , for) (chronic , chronic) (constipation , constipation)
(herbs , remedies) (for , for) (chronic , chronic) (constipation , constipation)
(herbs , medicine) (for , for) (chronic , chronic) (constipation , constipation)
(herbs , supplements) (for , for) (chronic , chronic) (constipation , constipation)
(herbs , herbs) (for , for) (mexican , mexican) (cooking , cooking)
(herbs , herbs) (for , for) (cooking , cooking) (mexican , mexican)
(herbs , herbs) (for , for) (mexican , mexican) (cooking , food)
(mexican , mexican) (herbs , herbs) (for , for) (cooking , cooking)
(herbs , spices) (for , for) (mexican , mexican) (cooking , cooking)
575
Computational Linguistics Volume 36, Number 3
table to the same 150,000 queries that were input to the translation, expansion phrases
are included in the search query via an OR-operation. An example search query that
uses the SMT-based expansions from Table 3 is shown in Figure 1.
In order to evaluate Cui et al (2002)?s correlation-based system in this setup, we
required the system to assign expansion terms to particular query terms. The best results
were achieved by using a linear interpolation of scores in Equation (2) and Equation (1).
Equation (1) thus introduces a preference for a particular query term to the whole-
query score calculated by Equation (2). Our reimplementation uses unigram and bigram
phrases in queries and expansions. Furthermore, we use Okapi BM25 instead of tfidf in
the calculation of Equation (1) (see Robertson, Walker, and Hancock-Beaulieu 1998).
In addition to SMT and correlation-based expansion, we evaluate a system that uses
the query language model to rescore the rewrites produced by the correlation-based
model. The intended effect is to filter correlation-based expansions by a more effective
context model than the cohesion model proposed by Cui et al (2002).
Because expansions from all experimental systems are done on top of the same
underlying search engine, we can abstract away from interactions with the underlying
system. Rewrite scores or translation probabilities were only used to create n-best lists
for the respective systems; the ranking function of the underlying search engine was left
untouched.
5.3 Experimental Evaluation
The evaluation was performed by three independent raters. The raters were presented
with queries and 10-best search results from two systems, anonymized, and presented
randomly on left or right sides. The raters? task was to evaluate the results on a 7-point
Likert scale, defined as:
?1.5: much worse
?1.0: worse
?0.5: slightly worse
0: about the same
0.5: slightly better
1.0: better
1.5: much better
Table 4 shows evaluation results for all pairings of the three expansion systems.
For each pairwise comparison, a set of 200 queries that has non-empty, different re-
sult lists for both systems is randomly selected from the basic set of 150,000 queries.
The mean item score (averaged over queries and raters) for the experiment that com-
pares the correlation-based model with language model filtering (corr+lm) against
the correlation-based model (corr) shows a clear win for the experimental system.
Table 4
Comparison of query expansion systems on the Web search task with respect to a 7-point
Likert scale.
experiment corr+lm SMT SMT
baseline corr corr corr+lm
mean item score 0.264 ? 0.095 0.254 ? 0.09125 0.093 ? 0.0850
576
Riezler and Liu Query Rewriting Using Monolingual Statistical Machine Translation
An experiment that compares SMT-based expansion (SMT) against correlation-based
expansions (corr) results in a clear preference for the SMT model. An experiment that
compares the SMT-based expansions (SMT) against the correlation-based expansions
filtered by the language model (corr+lm) shows a smaller, but still statistically signifi-
cant, preference for the SMT model. Statistical significance of result differences has been
computed with a paired t-test (Cohen 1995), yielding statistical significance at the 95%
level for the first two columns in Table 4, and statistical significance at the 90% level for
the last column in Table 4.
Examples for SMT-based and correlation-based expansions are given in Table 5.
The first five examples show the five biggest wins in terms of mean item score for the
SMT system over the correlation-based system. The second set of examples shows the
five biggest losses of the SMT system compared to the correlation-based system. On
inspection of the first set, we see that SMT-based expansions such as henry viii restaurant
portland, maine, or ladybug birthday ideas, or top ten restaurants, vancouver, achieve a
change in retrieval results that does not result in a query drift, but rather in improved
retrieval results. The first and fifth result are wins for the SMT system because of non-
sensical expansions by the baseline correlation-based system. A closer inspection of the
second set of examples shows that the SMT-based expansion terms are all clearly related
to the source terms, but not synonymous. In the first example, shutdown is replaced
by reboot or restart which causes a demotion of the top result that matches the query
exactly. In the second example, passport is replaced by the related term visa in the SMT-
based expansion. The third example is a loss for SMT-based expansion because of a
replacement of the specific term debian by the more general term linux. The correlation-
based expansions how many tv 30 rock in the fourth example, and lampasas county sheriff
Table 5
5-best and 5-worst expansions from SMT system and corr system with mean item score.
query SMT expansions corr expansions score
broyhill conference - broyhill - welcome; 1.5
center boone boone - welcome
Henry VIII Menu menu - restaurant, portland - six; 1.3
Portland, Maine restaurants menu - england
ladybug birthday parties - ideas, ladybug - kids 1.3
parties party
top ten dining, dining - restaurants dining - 10 1.3
vancouver
international communication - international 1.3
communication in communications, skills communication -
veterinary medicine college
SCRIPT TO SHUTDOWN SHUTDOWN - - ?1.0
NT 4.0 shutdown, reboot, restart
applying U.S. passport passport - visa applying - home ?1.0
configure debian debian - linux; configure - ?1.0
to use dhcp configure - install configuring
how many episodes episodes - season, episodes - tv; ?0.83
of 30 rock? series many episodes -
wikipedia
lampasas county department - office department - home ?0.83
sheriff department
577
Computational Linguistics Volume 36, Number 3
home in the fifth example directly hit the title of relevant Web pages, while the SMT-
based expansion terms do not improve retrieval results. However, even from these
negative examples it becomes apparent that the SMT-based expansion terms are clearly
related to the query terms, and for a majority of cases this has a positive effect. In
contrast, the terms introduced by the correlation-based system are either only vaguely
related or noise.
Similar results are shown in Table 6 where the five best and five worst examples
for the comparison of the SMT model with the corr+lm model are listed. The wins for
the SMT system are achieved by synonymous or closely related terms (make - build,
create; layouts - backgrounds; contractor - contractors) or terms that properly disambiguate
ambiguous query terms: For example, the term vet in the query dr. tim hammond, vet
is expanded by the appropriate term veterinarian in the SMT-based expansion, whereas
the correlation-based expansion to vets does not match the query context. The losses
of the SMT-based system are due to terms that are only marginally related. Furthermore,
the expansions of the correlation-based model are greatly improved by language model
filtering. This can be seen more clearly in Table 7, which shows the five best and worst
results from the comparison of correlation-based models with and without language
model filtering. Here the wins by the filtered model are due to filtering non-sensical
expansions or too general expansions by the unfiltered correlation-based model rather
than promoting new useful expansions.
We attribute the experimental result of a significant preference for SMT-based ex-
pansions over correlation-based expansions to the fruitful combination of translation
model and language model provided by the SMT system. The SMT approach can be
viewed as a combined system that proposes already reasonable candidate expansions
via the translation model, and filters them by the language model. We may find a
certain amount of non-sensical expansion candidates at the phrase translation level of
the SMT system. However, a comparison with unfiltered correlation-based expansions
shows that the candidate pool of phrase translations of the SMT model is of higher
quality, yielding overall better results after language model filtering. This can be seen
Table 6
5-best and 5-worst expansions from SMT system and corr+lm system with mean item score.
query SMT expansions corr+lm expansions score
how to make bombs make - build, create make - book 1.5
dominion power va - dominion - virginia 1.3
purple myspace layouts - backgrounds purple - free 1.167
layouts myspace - free
dr. tim hammond, vet vet - veterinarian, vet - vets 1.167
veterinary, hospital
tci general contractor contractor - contractors - 1.167
health effects of tea - coffee - ?1.5
drinking too much tea
tomahawk wis - wis - wisconsin ?1.0
bike rally
apprentice tv show - tv - com ?1.0
super nes roms roms - emulator nes - nintendo ?1.0
family guy family - genealogy clips - video ?1.0
clips hitler
578
Riezler and Liu Query Rewriting Using Monolingual Statistical Machine Translation
Table 7
5-best and 5-worst expansions from corr system and corr+lm system with mean item score.
query corr+lm expansions corr expansions score
outer cape - cape - home; 1.5
health services health - home;
services - home
Henry VII Menu - menu - england; 1.5
Portland, Maine portland - six
easing to relieve gallbladder - gallstone gallbladder - disease, 1.333
gallbladder pain gallstones, gallstone
guardian angel picture - picture - lyrics 1.333
view full episodes episodes - watch naruto - tv 1.333
of naruto
iditarod 2007 schedule iditarod 2007 - race - ?1.5
40 inches plus inches plus - review inches - calculator ?1.333
Lovell sisters review lovell sisters - website - ?1.333
smartparts ion Review smartparts ion - reviews review - pbreview ?1.167
canon eos rebel epinion - com - ?1.167
xt slr + epinion
from inspecting Table 8 which shows the most probable phrase translations that are
applicable to the queries herbs for chronic constipation and herbs for mexican cooking. The
phrase tables include identity translations and closely related terms as most probable
translations for nearly every phrase. However, they also clearly include noisy and non-
related terms. Thus an extraction of expansion terms from the phrase table alone would
not allow the choice of the appropriate term for the given query context. This can be
attained by combining the phrase translations with a language model: As shown in
Table 3, the 5-best translations of the full queries attain a proper disambiguation of the
senses of herbs by replacing the term with remedies, medicine, and supplements for the first
Table 8
Phrase translations for source strings herbs for chronic constipation and herbs for mexican cooking.
herbs herbs, herbal, medicinal, spices, supplements, remedies
herbs for herbs for, herbs, herbs and, with herbs
herbs for chronic herbs for chronic, and herbs for chronic, herbs for
for chronic for chronic, chronic, of chronic
for chronic constipation for chronic constipation, chronic constipation, for constipation
chronic chronic, acute, patients, treatment
chronic constipation chronic constipation, of chronic constipation,
with chronic constipation
constipation constipation, bowel, common, symptoms
for mexican for mexican, mexican, the mexican, of mexican
for mexican cooking mexican food, mexican food and, mexican glossary
mexican mexican, mexico, the mexican
mexican cooking mexican cooking, mexican food, mexican, cooking
cooking cooking, culinary, recipes, cook, food, recipe
579
Computational Linguistics Volume 36, Number 3
Table 9
Correlation-based expansions for queries herbs for chronic constipation and herbs for mexican
cooking.
query terms n-best expansions
herbs com treatment encyclopedia
chronic interpret treating com
constipation interpret treating com
herbs for medicinal support women
for chronic com gold encyclopedia
chronic constipation interpret treating
herbs cooks recipes com
mexican recipes com cooks
cooking cooks recipes com
herbs for medicinal women support
for mexican cooks com allrecipes
query, and with spices for the second query. Table 9 shows the top three correlation-
based expansion terms assigned to unigrams and bigrams in the queries herbs for
chronic constipation and herbs for mexican cooking. Expansion terms are chosen by overall
highest weight and shown in boldface. Relevant expansion terms such as treatment
or recipes that would disambiguate the meaning of herbs are in fact in the candidate
list; however, the cohesion score promotes general terms such as interpret or com as
best whole-query expansions. Although language model filtering greatly improves the
quality of correlation-based expansions, overall the combination of phrase translations
and language model produces better results than the combination of correlation-based
expansions and language model. This is confirmed by the pairwise comparison of the
SMT and corr+lm systems shown in Table 4.
6. Conclusion
We presented a view of the term mismatch problem between queries and Web doc-
uments as a problem of translating from a source language of user queries to a tar-
get language of Web documents. We showed that a state-of-the-art SMT model can
be applied to parallel data of user queries and snippets for clicked Web documents,
and showed improvements over state-of-the-art probabilistic query expansion. Our
experimental evaluation showed firstly that state-of-the-art SMT is robust and flexible
enough to capture the peculiarities of query?snippet translation, thus questioning the
need for special-purpose models to control noisy translations as suggested by Lee et al
(2008). Furthermore, we showed that the combination of translation model and lan-
guage model significantly outperforms the combination of correlation-based model and
language model. We chose to take advantage of the access the google.com search engine
to evaluate the query rewrite systems by query expansion embedded in a real-word
search task. Although this conforms with recent appeals for more extrinsic evaluations
(Belz 2009), it decreases the reproducability of the evaluation experiment.
In future work, we hope to apply SMT-based rewriting to other rewriting tasks such
as query suggestions. Also, we hope that our successful application of SMT to query
580
Riezler and Liu Query Rewriting Using Monolingual Statistical Machine Translation
expansion might serve as an example and perhaps open the doors for new applications
and extrinsic evaluations of related NLP approaches such as paraphrasing.
References
Baeza-Yates, Ricardo and Alessandro Tiberi.
2007. Extracting semantic relations from
query logs. In Proceedings of the 13th
ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD?07),
San Jose, CA, pages 76?85.
Bannard, Colin and Chris Callison-Burch.
2005. Paraphrasing with bilingual parallel
corpora. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL?05), Ann Arbor, MI,
pages 597?604.
Beeferman, Doug and Adam Berger. 2000.
Agglomerative clustering of a search
engine query log. In Proceedings of the 6th
ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining
(KDD?00), Boston, MA, pages 407-416.
Belz, Anja. 2009. That?s nice ... what can you
do with it? Computational Linguistics,
35(1):111?118.
Berger, Adam and John Lafferty. 1999.
Information retrieval as statistical
translation. In Proceedings of the 22nd
ACM SIGIR Conference on Research and
Development in Information Retrieval
(SIGIR?99), Berkeley, CA, pages 222-229.
Berger, Adam L., Rich Caruana, David Cohn,
Dayne Freitag, and Vibhu Mittal. 2000.
Bridging the lexical chasm: Statistical
approaches to answer-finding. In
Proceedings of the 23rd ACM SIGIR
Conference on Research and Development
in Information Retrieval (SIGIR?00),
Athens, Greece, 192?199.
Brants, Thorsten, Ashok C. Popat, Peng Xu,
Franz J. Och, and Jeffrey Dean. 2007. Large
language models in machine translation.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP?07), Prague Czech Republic,
pages 858?867.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Cohen, Paul R. 1995. Empirical Methods for
Artificial Intelligence. The MIT Press,
Cambridge, MA.
Cui, Hang, Ji-Rong Wen, Jian-Yun Nie,
and Wei-Ying Ma. 2002. Probabilistic
query expansion using query logs.
In Proceedings of the 11th International
World Wide Web conference (WWW?02),
Honolulu, HI, pages 325?332.
Echihabi, Abdessamad and Daniel Marcu.
2003. A noisy-channel approach to
question answering. In Proceedings of the
41st Annual Meeting of the Association
for Computational Linguistics (ACL?03),
Sapporo, Japan, pages 16?23.
Fitzpatrick, Larry and Mei Dent. 1997.
Automatic feedback using past queries:
Social searching? In Proceedings of the 20th
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR?97), Philadelphia, PA,
pages 306?313.
Fonseca, Bruno M., Paulo Golgher, Bruno
Possas, Berthier Ribeiro-Neto, and Nivio
Ziviani. 2005. Concept-based interactive
query expansion. In Proceedings of the 14th
Conference on Information and Knowledge
Management (CIKM?05), Bremen, Germany,
pages 696?703.
Huang, Chien-Kang, Lee-Feng Chien, and
Yen-Jen Oyang. 2003. Relevant term
suggestion in interactive Web search based
on contextual information in query session
logs. Journal of the American Society for
Information Science and Technology,
54(7):638?649.
Jones, Rosie, Benjamin Rey, Omid Madani,
and Wiley Greiner. 2006. Generating query
substitutions. In Proceedings of the 15th
International World Wide Web conference
(WWW?06), Edinburgh, Scotland,
pages 387?396.
Lee, Jung-Tae, Sang-Bum Kim, Young-In
Song, and Hae-Chang Rim. 2008. Bridging
lexical gaps between queries and questions
on large online QA collections with
compact translation models. In Proceedings
of the Conference on Empirical Methods in
Natural Language Processing (EMNLP?08),
Honolulu, HI, pages 410?418.
Och, Franz Josef and Hermann Ney. 2004.
The alignment template approach
to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Quirk, Chris, Chris Brockett, and William
Dolan. 2004. Monolingual machine
translation for paraphrase generation. In
Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics
(ACL?04), Barcelona, Spain, pages 142?149.
Raghavan, Vijay V. and Hayri Sever. 1995.
On the reuse of past optimal queries. In
Proceedings of the 18th Annual International
581
Computational Linguistics Volume 36, Number 3
ACM SIGIR Conference on Research and
Development in Information Retrieval
(SIGIR?95), Seattle, WA, pages 344?350.
Riezler, Stefan, Yi Liu, and Alexander
Vasserman. 2008. Translating queries into
snippets for improved query expansion.
In Proceedings of the 22nd International
Conference on Computational Linguistics
(COLING?08), Manchester, England,
pages 737?744.
Riezler, Stefan, Alexander Vasserman,
Ioannis Tsochantaridis, Vibhu Mittal, and
Yi Liu. 2007. Statistical machine translation
for query expansion in answer retrieval. In
Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics
(ACL?07), Prague Czech Republic, Vol. 1,
pages 464?471.
Robertson, Stephen E., Steve Walker, and
Micheline Hancock-Beaulieu. 1998.
Okapi at TREC-7. In Proceedings of the
Seventh Text REtrieval Conference
(TREC-7), Gaithersburg, MD,
pages 253?264.
Sable, Carl, Kathleen McKeown, and
Kenneth W. Church. 2002. NLP found
helpful (at least for one text categorization
task). In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language
Processing (EMNLP?02), Philadelphia, PA,
pages 172?179.
Sahami, Mehran and Timothy D. Heilman.
2006. A Web-based kernel function for
measuring the similarity of short text
snippets. In Proceedings of the 15th
International World Wide Web conference
(WWW?06), Edinburgh, Scotland,
pages 377-386.
Soricut, Radu and Eric Brill. 2006. Automatic
question answering using the Web:
Beyond the factoid. Journal of Information
Retrieval - Special Issue on Web Information
Retrieval, 9:191?206.
Surdeanu, M., M. Ciaramita, and
H. Zaragoza. 2008. Learning to rank
answers on large online QA collections. In
Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics
(ACL?08), Columbus, OH, pages 719?727.
Wen, Ji-Rong, Jian-Yun Nie, and Hong-Jiang
Zhang. 2002. Query clustering using user
logs. ACM Transactions on Information
Systems, 20(1):59?81.
Xu, Jinxi and W. Bruce Croft. 1996.
Query expansion using local and
global document analysis. In Proceedings
of the 30th Annual International ACM
SIGIR Conference on Research and
Development in Information Retrieval
(SIGIR?07), Zurich, Switzerland,
pages 4?11.
Xue, Xiaobing, Jiwoon Jeon, and Bruce Croft.
2008. Retrieval models for question and
answer archives. In Proceedings of the 31st
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR?08), Singapore,
pages 475?482.
582
