Event-based Information Extraction for the biomedical domain: the Caderige project 
 
Erick Alphonse**, Sophie Aubin*, Philippe Bessi?res**, Gilles Bisson****, Thierry Hamon*, 
Sandrine Lagarrigue***, Adeline Nazarenko*, Alain-Pierre Manine**, Claire N?dellec**, 
Mohamed Ould Abdel Vetah**, Thierry Poibeau*, Davy Weissenbacher* 
 
*Laboratoire d?Informatique de Paris-Nord  
CNRS UMR 7030 
Av. J.B. Cl?ment 93430 F-Villetaneuse 
{firstname.lastname}@lipn.univ-paris13.fr 
**Laboratoire Math?matique, Informatique et G?nome (MIG), 
INRA,  
Domaine de Vilvert, 78352 F-Jouy-en-Josas 
{firstname.lastname}@jouy.inra.fr 
***Laboratoire de G?n?tique Animale,  
INRA-ENSAR 
Route de Saint Brieuc, 35042 Rennes Cedex 
lagarrig@roazhon.inra.fr 
****Laboratoire Leibniz ? UMR CNRS 5522  
46 Avenue F?lix Viallet - 38031 F-Grenoble Cedex 
Gilles.Bisson@imag.fr 
 
 Abstract  
This paper gives an overview of the 
Caderige project. This project involves 
teams from different areas (biology, 
machine learning, natural language 
processing) in order to develop high-
level analysis tools for extracting 
structured information from biological 
bibliographical databases, especially 
Medline. The paper gives an overview 
of the approach and compares it to the 
state of the art.  
1 Introduction 
Developments in biology and biomedicine are 
reported in large bibliographical databases 
either focused on a specific species (e.g. 
Flybase, specialized on Drosophilia 
Menogaster) or not (e.g. Medline). This type 
of  information sources is crucial for biologists 
but there is a lack of tools to explore them and 
extract relevant information. While recent 
named entity recognition tools have gained a 
certain success on these domains, event-based 
Information Extraction (IE) is still a challenge.  
The Caderige project aims at designing and 
integrating Natural Language Processing 
(NLP) and Machine Learning (ML) techniques 
to explore, analyze and extract targeted 
information in biological textual databases. We 
promote a corpus-based approach focusing on 
text pre-analysis and normalization: it is 
intended to drain out the linguistic variation 
dimension, as most as possible. Actually, the 
MUC (1995) conferences have demonstrated 
that extraction is more efficient when 
performed on normalized texts. The extraction 
patterns are thus easier to acquire or learn, 
more abstract and easier to maintain 
Beyond extraction patterns, it is also possible 
to acquire from the corpus, via ML methods, a 
part of the knowledge necessary for text 
normalization as shown here.  
This paper gives an overview of current 
research activities and achievements of the 
Caderige project. The paper first presents our 
approach and compares it with the one 
developed in the framework of a similar 
project called Genia (Collier et al 1999). We 
then propose an account of Caderige 
techniques on various filtering and 
normalization tasks, namely, sentence filtering, 
resolution of named entity synonymy, 
syntactic parsing, and ontology learning. 
Finally, we show how extraction patterns can 
be learned from normalized and annotated 
documents, all applied to biological texts.  
2 Description of our approach 
In this section, we give some details about the 
motivations and choices of implementation. 
We then briefly compare our approach with the 
one of the Genia project. 
43
2.1 Project organization 
The Caderige project is a multi disciplinary 
French research project on the automatic 
mining of textual data from the biomedical 
domain and is mainly exploratory orientated. It 
involved biology teams (INRA), computer 
science teams (LIPN, INRA and Leibniz-
IMAG) and NLP teams (LIPN) as major 
partners, plus LRI and INRIA from 2000 to 
2003. 
2.2 Project motivations 
Biologists can search bibliographic databases 
via the Internet, using keyword queries that 
retrieve a large superset of relevant papers. 
Alternatively, they can navigate through 
hyperlinks between genome databanks and 
referenced papers. To extract the requisite 
knowledge from the retrieved papers, they 
must identify the relevant abstracts or 
paragraphs. Such manual processing is time 
consuming and repetitive, because of the 
bibliography size, the relevant data sparseness, 
and the database continuous updating. From 
the Medline database, the focused query 
?Bacillus subtilis and transcription? which 
returned 2,209 abstracts in 2002, retrieves 
2,693 of them today. We chose this example 
because Bacillus subtilis is a model bacterium 
and transcription is a central phenomenon in 
functional genomics involved in genic 
interaction, a popular IE problem. 
GerE stimulates cotD transcription and 
inhibits cotA transcription in vitro by 
sigma K RNA polymerase, as expected from 
in vivo studies, and, unexpectedly, 
profoundly inhibits in vitro 
transcription of the gene (sigK) that 
encode sigma K. 
Figure 1: A sentence describing a genic interaction 
Once relevant abstracts have been retrieved, 
templates should be filled by hand since there 
is no available IE tool operational in genomics  
Type: positive 
Agent: GerE 
 
Interaction 
Target: transcription of the 
gene sigK 
Figure 2: A template describing a genic 
interaction. 
Still, applying IE ? la MUC to genomics and 
more generally to biology is not an easy task 
because IE systems require deep analysis 
methods to locate relevant fragments. As 
shown in the example in Figures 1 and 2, 
retrieving that GerE is the agent of the 
inhibition of the transcription of the gene sigK 
requires at least syntactic dependency analysis 
and coordination processing. In most of the 
genomics IE tasks (function, localization, 
homology) the methods should then combine 
the semantic-conceptual analysis of text 
understanding methods with IE through pattern 
matching. 
2.3 Comparison with the Genia project 
Our approach is very close to the one of the 
Genia project (Collier et al, 1999). Both 
projects rely on precise high-level linguistic 
analysis to be able to perform IE. The kind of 
information being searched is similar, 
concerning mainly gene and protein interaction 
as most of the research in this domain. The 
Genia corpus (Ohtae et al 2001) is not 
specialized on a specific species whereas ours 
is based on Bacillus Subtilis.  
Both projects develop annotation tools and 
Document Type Definition (DTD), which are, 
for the most part, compatible. The aim here is 
to build training corpus to which various 
techniques of NLP and ML are applied in 
order to acquire efficient event-based 
extraction patterns. The choice of ML and 
NLP methods differs but their aim is similar to 
our: normalizing text with predicate-arguments 
structures for learning better patterns. For 
example, Genia uses a combination of parsers 
to finally perform an HPSG-like analysis. The 
Caderige syntactic analysis is based on the 
specialization of the Link Parser (Sleator and 
Temperley, 1993 see section 4) to the 
biological domain.  
 In the following two sections, we detail our 
text filtering and normalization methods. 
Filtering aims at pruning the irrelevant part of 
the corpus while normalization aims at 
building an abstract representation of the 
relevant text. Section 4 is devoted to the 
acquisition of extraction patterns from the 
filtered and normalized text. 
3 Text filtering 
IR and text filtering are a prerequisite step to 
IE, as IE methods (including normalization and 
learning) cannot be applied to large and 
irrelevant corpora (they are not robust enough 
and they are computationally expensive). IR 
here is done through Medline interface by 
keyword queries for filtering the appropriate 
44
document subset. Then, text filtering, reduces 
the variability of textual data with the 
following assumptions: 
? desired information is local to sentences ; 
? relevant sentences contain at least two gene 
names. 
These hypotheses may lead to miss some genic 
interactions, but we assume that information 
redundancy is such that at least one instance of 
each interaction is contained into a single 
sentence in the corpus. The documents 
retrieved are thus segmented into sentences 
and the sentences with at least two gene names 
are selected. 
To identify the only relevant sentences among 
thoses,  classical supervised ML methods have 
been applied to a Bacillus Subtilis corpus in 
which relevant and irrelevant sentences had 
been annotated by a biological expert. Among 
SVMs, Na?ve Bayes (NB) methods, Neural 
Networks, decision trees (Marcotte et al, 
2001;  Nedellec et al, 2001), (Nedellec et al 
2001) demonstrates that  simple NB methods 
coupled with feature selection seem to perform 
well by yielding around 85 % precision and 
recall. Moreover, our first experiments show 
that the linguistic-based representation changes 
such as the use of lemmatization, terminology 
and named entities, do not lead to significant 
improvements. The relevant sentences filtered 
at this step are then used as input of the next 
tasks, normalization and IE. 
4 Normalization 
This section briefly presents three text 
normalization tasks: normalization of entity 
names, normalization of relations between text 
elements through syntactic dependency parsing 
and semantic labeling. The normalization 
process, by providing an abstract 
representation of the sentences, allows the 
identification of regularities that simplify the 
acquisition or learning of pattern rules. 
4.1 Entity names normalization 
Named Entity recognition is a critical point in 
biological text analysis, and a lot of work was 
previously done to detect gene names in text 
(Proux and al., 1998), (Fukuda and al., 1998). 
So, in Caderige, we do not develop any 
original NE extraction tool. We focus on a less 
studied problem that is synonyms recognition.  
Beyond typographical variations and 
abbreviations, biological entities often have 
several different names. Synonymy of gene 
names is a well-known problem, partly due to 
the huge amount of data manipulated (43.238 
references registered in Flybase for 
Drosophilia Melanogaster for example). Genes 
are often given a temporary name by a 
biologist. This name is then changed according 
to information on the concerned gene: for 
example SYGP-ORF50 is a gene name 
temporarily attributed by a sequencing project 
to the PMD1 yeast gene. We have shown that, 
in addition to available data in genomic 
database (GenBank, SwissProt,?), it is 
possible to acquire many synonymy relations 
with good precision through text analysis. By 
focusing on synonymy trigger phrases such as 
"also called" or "formerly", we can extract text 
fragments of that type :  gene trigger gene. 
However, the triggers themselves are subject to 
variation and the arguments of the synonymy 
relation must be precisely identified. We have 
shown that it is possible to define patterns to 
recognize synonymy expressions. These 
patterns have been trained on a representative 
set of sentences from Medline and then tested 
on a new corpus made of 106 sentences 
containing the keyword formerly. Results on 
the test corpus are the following: 97.5% 
precision, 75% recall. We chose to have a high 
precision since the acquired information must 
be valid for further acquisition steps 
(Weissenbacher, 2004).  
The approach that has been developed is very 
modular since abstract patterns like gene 
trigger gene (the trigger being a linguistic 
marker or a simple punctuation) can be 
instantiated by various linguistic items. A 
score can be computed for each instantiation of 
the pattern, during a learning phase on a large 
representative corpus. The use of a reduced 
tagged corpus and of a large untagged corpus 
justify the use of semi-supervised learning 
techniques.  
4.2  Sentence parsing 
The extraction of structured information from 
texts requires precise sentence parsing tools 
that exhibit relevant relation between domain 
entities. Contrary to (Akane et al 2001), we 
chose a partial parsing approach: the analysis 
is focused on relevant parts of texts and, from 
these chunks, on specific relations. Several 
reasons motivate this choice: among others, the 
fact that relevant information generally appears 
in predefined syntactic patterns and, moreover, 
45
the fact that we want to learn domain 
knowledge ontologies from specific syntactic 
relations (Faure and Nedellec, 2000 ; Bisson et 
al. 2000). 
First experiments have been done on several 
shallow parsers. It appeared that constituent 
based parsers are efficient to segment the text 
in syntactic phrases but fail to extract relevant 
functional relationships betweens phrases. 
Dependency grammars are more adequate 
since they try to establish links between heads 
of syntactic phrases. In addition, as described 
in Schneider (1998), dependency grammars are 
looser on word order, which is an advantage 
when working on  a domain specific language.  
Two dependency-based syntactic parsers have 
been tested (Aubin 2003): a hybrid commercial 
parser (henceforth HCP) that combines 
constituent and dependency analysis, and a 
pure dependency analyzer: the Link Parser.   
Prasad and Sarkar (2000) promote a twofold 
evaluation for parsers: on the one hand the use 
of a representative corpus and, on the other 
hand, the use of specific manually elaborated 
sentences. The idea is to evaluate analyzers on 
real data (corpus evaluation) and then to check 
the performance on specific syntactic 
phenomena. In this experiment, we chose to 
have only one corpus, made of sentences 
selected from the Medline corpus depending 
on their syntactic particularity. This strategy 
ensures representative results on real data. 
A set of syntactic relations was then selected 
and manually evaluated. This led to the results 
presented for major relations only in table 1. 
For each analyzer and relation, we compute a 
recall and precision score (recall = # relevant 
found relations / # relations to be found; 
precision = # relevant found relations / # 
relations found by the system).  
The Link Parser generally obtains better results 
than HCP. One reason is that a major 
particularity of our corpus (Medline abstracts) 
is that sentences are often (very) long (27 
words on average) and contain several clauses. 
The dependency analyzer is more accurate to 
identify relevant relationships between 
headwords whereas the constituent parser is 
lost in the sentence complexity. We finally 
opted for the Link Parser. Another advantage 
of the Link Parser is the possibility to modify 
its set of rules (see next subsection). The Link 
parser is currently used in INRA to extract 
syntactic relationships from texts in order to 
learn domain ontologies on the basis of a 
distributional analysis (Harris 1951, Faure and 
N?dellec, 1999).  
4.3 Recycling a general parser for biology 
During the evaluation tests, we noticed that 
some changes had to be applied either to the 
parser or to the text itself to improve the 
syntactic analysis of our biomedical corpus. 
The corpus needs to be preprocessed: sentence 
segmentation, named entities and terms 
recognition are thus performed using generic 
modules tuned for the biology domain
1
. Term 
recognition allows the removing of numerous 
structure ambiguities, which clearly benefits 
the parsing quality and execution time.  
                                                     
1
 A term analyser is currently being built at LIPN 
using existing term resources like Gene Ontology 
(see Hamon and Aubin, 2004). 
  Link Parser HCP 
Rel nbRel relOK R. RelTot P. RelOK R RelTot P. 
Subject 
18 13 0.72 19 0.68 14 0.78 20 0.65 
Object 
18 16 0.89 17 0.94 9 0.5 13 0.69 
Prep 
48 25 0.52 55 0.45 20 0.42 49 0.41 
V-GP1 
14 13 0.93 15 0.87 9 0.64 23 0.39 
O-GP 
16 7 0.43 12 0.58 12 0.75 28 0.43 
NofN 
16 13 0.81 15 0.87 14 0.87 26 0.54 
VtoV 
10 9 0.9 9 1 7 0.7 7 1 
VcooV 
10 8 0.8 9 0.89 6 0.6 6 1 
NcooN 
10 8 0.7 10 0.8 4 0.4 6 0.67 
nV-Adj 
10 8 0.8 9 0.89 0 0 0 1 
PaSim 
18 17 0.94 18 0.94 17 0.94 22 0.77 
PaRel 
12 11 0.92 11 1 8 0.67 11 0.73 
Table 1: Evaluation of two parsers on various syntactic relations 
Relations meaning: subject = subject-verb, Object = verb-object, Prep = prepositional phrase, V-GP = verb-prep. 
phrase, O-GP = Object- prep. phrase, NofN = Noun of noun, VtoV = Verb to Verb, VcooV = Verb coord. Verb, 
NcooN = Noun coord. Noun, nV-Adj = not + Verb or adjective, PaSim = passive form, PaRel = passive relative 
46
Concerning the Link Parser, we have manually 
introduced new rules and lexicon to allow the 
parsing of syntactic structures specific to the 
domain. For instance, the Latin-derived Noun 
Adjective phrase "Bacillus subtilis" has a 
structure inverse to the canonical English noun 
phrase (Adjective Noun). Another major task 
was to loosen the rules constraints because 
Medline abstracts are written by biologists 
who express themselves in sometimes broken 
English. A typical error is the omission of the 
determinant before some nouns that require 
one. We finally added words unknown to the 
original parser. 
4.4 Semantic labelling 
Asium software is used to semi-automatically 
acquire relevant semantic categories by 
distributional semantic analysis of parsed 
corpus. These categories contribute to text 
normalization at two levels, disambiguating 
syntactic parsing and typing entities and 
actions for IE. Asium is based on an original 
ascendant hierarchical clustering method that 
builds a hierarchy of semantic classes from the 
syntactic dependencies parsed in the training 
corpus. Manual validation is required in order 
to distinguish between different meanings 
expressed by identical syntactic structures. 
5 Extraction pattern learning 
Extraction pattern learning requires a training 
corpus from which the relevant and 
discriminant regularities can be automatically 
identified. This relies on two processes: text 
normalization that is domain-oriented but not 
task-oriented (as described in previous 
sections), and task-oriented annotation by the 
expert of the task.  
5.1 Annotation procedure 
The Caderige annotation language is based on 
XML and a specific DTD (Document Type 
Definition that can be used to annotate both 
prokaryote and eukaryote organisms by 50 
tags with up to 8 attributes. Such a precision is 
required for learning feasibility and extraction 
efficiency. Practically, each annotation aims at 
highlighting the set of words in the sentence 
describing: 
? Agents (A): the entities activating or 
controlling the interaction 
? Targets (T): the entities that are produced 
or controlled 
? Interaction (I): the kind of control 
performed during the interaction 
? Confidence (C): the confidence level in this 
interaction. 
The annotation of ?A low level of GerE 
activated transcription of CotD by GerE RNA 
polymerase in vitro ...? is given below. The 
attributes associated to the tag <GENIC-
INTERACTION> express the fact that the 
interaction is a transcriptional activation and 
that it is certain. The other tags (<IF>, 
<AF1>, ?) mark  the agent (AF1 and AF2), the 
target (TF1) and the interaction (IF). 
 
<GENIC-INTERACTION 
 id=?1?  
 type=?transcriptional?  
 assertion=?exist?  
 regulation=?activate?  
 uncertainty=?certain?  
 self-contained=?yes?  
 text-clarity=?good?> 
  <IF>A<I> low level </I>of</IF>     
  <AF1><A1  
     type=protein  
        role=modulate  
        direct=yes> GerE 
  </A1></AF1>,  
  <IF><I>activated</I> transcription  
      of</IF>    
     <TF1><T1 type=protein> CotD </T1>           
         </TF1> by   
     <AF2><A2  
           type=protein  
         role=required> 
       GerE RNA polymerase 
   </A2></AF2>,  
   <CF>but<C>in vitro</C></CF> 
</GENIC-INTERACTION> 
5.2 The annotation editor2 
Annotations cannot be processed in text form 
by biologists. The annotation framework 
developed by Caderige provide a general XML 
editor with a graphic interface for creating, 
checking and revising annotated documents. 
For instance, it displays the text with graphic 
attributes as defined in the editor XML style 
sheet, it allows to add the tags without strong 
constraint on the insertion order and it 
automatically performs some checking. 
The editor interface is composed of four main 
parts (see Figure 3). The editable text zone for 
annotation, the list of XML tags that can be 
used at a given time, the attributes zone to edit 
the values of the selected tag, and the XML 
                                                     
2
 Contact one of the authors if you are interested to 
use this annotation tool in a research project 
47
code currently generated. In the text zone, the 
above sentence is displayed as follows: 
A low level of GerE activated 
transcription of CotD by GerE RNA 
polymerase but in vitro 
This editor is currently used by some of the 
Caderige project partners and at SIB (Swiss 
Institute of BioInformatics) with another DTD, 
in the framework of the European BioMint 
project. Several corpora on various species 
have been annotated using this tool, mainly by 
biologists from INRA.  
5.3 Learning 
The vast majority of approaches relies on 
hand-written pattern rules that are based on 
shallow representations of the sentences (e.g. 
Ono et al, 2001). In Caderige, the deep 
analysis methods increase the complexity of 
the sentence representation, and thus of the IE 
patterns. ML techniques appear therefore very 
appealing to automate the process of rule 
acquisition (Freitag, 1998; Califf et al, 1998; 
Craven et al, 1999).  
Learning IE rules is seen as a discrimination  
task, where the concept to learn is a n-ary 
relation between arguments which correspond 
to the template fields. For example, the 
template in figure 2 can be filled by learning a 
ternary relation genic-interaction(X,Y,Z), 
where X,Y and Z are the type, the agent and 
the target of the interaction. The learning 
algorithm is provided with a set of positive and 
negative examples built from the sentences 
annotated and normalized. We use the 
relational learning algorithm, Propal (Alphonse 
et al, 2000). The appeal of using a relational 
method for this task is that it can naturally 
represent the relational structure of the 
syntactic dependencies in the normalized 
sentences and the background knowledge if 
needed, such as for instance semantic relations.  
For instance, the IE rules learned by Propal 
extract, from the following sentence :"In this 
mutant, expression of the spoIIG gene, whose 
transcription depends on both sigA and the 
phosphorylated Spo0A protein, Spo0AP, a 
major transcription factor during early stages 
of sporulation, was greatly reduced at 43 
degrees C.", successfully extract the two 
relations genic-interaction(positive, sigA, 
spoIIG) and genic-interaction(positive, 
Spo0AP, spoIIG). As preliminary experiments, 
we selected a subset of sentences as learning 
dataset, similar to this one. The performance of 
the learner evaluated by ten-fold cross-
validation is 69?6.5% of recall and 86?3.2% 
of precision. This result is encouraging, 
showing that the normalization process 
provides a good representation for learning IE 
rules with both high recall and high precision. 
6 Conclusion 
We have presented in this paper some results 
from the Caderige project. Two major issues 
are the development of a specific annotation 
editor for domain specialists and a set of 
machine learning and linguistic processing 
tools tuned for the biomedical domain.  
Current developments focus on the use of 
learning methods in the extraction process. 
These methods are introduced at different 
levels in the system architecture. A first use is 
Figure 3: the Caderige annotation editor 
48
the acquisition of domain knowledge to 
enhance the extraction phase. A second use 
concerns a dynamic adaptation of existing 
modules during the analysis according to 
specific features in a text or to specific text 
genres.  
7 References 
E. Agichtein and H. Yu (2003). Extracting 
synonymous gene and protein terms from 
biological literature. Bioinformatics, vol. 19 
Suppl.1, Oxford Press. 
E. Alphonse and C. Rouveirol (2000). Lazy 
propositionalisation for Relational  
Learning. In 14th European Conference on 
Artificial Intelligence (ECAI?00, W. Horn ed.), 
Berlin, pp. 256-260.  
S. Aubin (2003). ?valuation comparative de deux 
analyseurs produisant des relations syntaxiques. 
In workshop TALN and multilinguism. Batz-sur-
Mer. 
Y. Akane, Y. Tateisi, Y. Miyao and J. Tsujii. 
(2001). Event extraction from biomedical papers 
using a full parser. In Proceedings of the sixth 
Pacific Symposium on Biocomputing (PSB 2001). 
Hawaii, U.S.A.. pp. 408-419.  
G. Bisson, C. Nedellec, L. Ca?amero 2000. 
Designing clustering methods for ontology 
building: The Mo?K workbench. In Proceedings 
of Ontology Learning workshop (ECAI 2000), 
Berlin, 22 ao?t 2000.  
M. E. Califf, 1998. Relational Learning Techniques 
for Natural Language Extraction. Ph.D. 
Disseration, Computer Science Department, 
University of Texas, Austin, TX. AI Technical 
Report 98-276. 
N. Collier, Hyun Seok Park, Norihiro Ogata, Yuka 
Tateisi, Chikashi Nobata, Takeshi Sekimizu, 
Hisao Imai and Jun'ichi Tsujii. (1999). The 
GENIA project: corpus-based knowledge 
acquisition and information extraction from 
genome research papers. In Proceedings of the 
European Association for Computational 
Linguistics (EACL 1999). 
M. Craven et al, 1999. Constructing Biological 
Knowledge Bases by Extracting Information 
from Text Sources. ISMB 1999: 77-86 
D. Faure and C. Nedellec (1999). Knowledge 
acquisition of predicate argument structures from 
technical texts using Machine Learning: the 
system ASIUM. In EKAW'99, pp. 329-334, 
Springer-Verlag.  
D. Freitag, 1998, Multistrategy learning for 
information extraction. In Proceedings of the 
Fifteenth International Conference on Machine 
Learning, 161-169. Madison, WI: Morgan 
Kaufmann 
T. Hamon and S. Aubin (2004). Evaluating 
terminological resource coverage for relevant 
sentence selection and semantic class building. 
LIPN internal report. 
K. Fukuda, T. Tsunoda, A. Tamura, T. Takagi 
(1998). Toward information extraction : 
identifying protein names from biological papers. 
Proceedings of the Pacific Symposium of 
Biocomputing, pp. 707-718. 
Z. Harris (1951). Methods in Structural Linguistics. 
Chicago. University of Chicago Press.  
E.M. Marcotte, I. Xenarios I., and D. Eisenberg 
(2001). Mining litterature for protein-protein 
interactions. In Bioinformatics, vo. 17 n? 4, 
pp. 359-363. 
MUC (1995) Proceeding of the 6
th
 Message 
understanding Conference. Morgan Kaufmann. 
Palo Alto.  
C. N?dellec, M. Ould Abdel Vetah and P. Bessi?res 
(2001). Sentence Filtering for Information 
Extraction in Genomics: A Classification 
Problem. In Proceedings of the International 
Conference on Practical Knowledge Discovery in 
Databases (PKDD?2001), pp. 326?338. Springer 
Verlag, LNAI 2167, Freiburg. 
T. Ohta, Yuka Tateisi, Jin-Dong Kim, Hideki Mima 
and Jun'ichi Tsujii. (2001). Ontology Based 
Corpus Annotation and Tools. In Proceedings of 
the 12th Genome Informatics 2001. pp. 469--470. 
T. Ono, H. Hishigaki, A. Tanigami and T. Takagi 
(2001). Automated extraction of information on 
protein-protein interactions from the biological  
literature. Bioinformatics. vol 17, n? 2, pp. 155-
161, Oxford Press. 
B. Prasad and A. Sarkar (2000) Comparing Test-
suite based evaluation and Corpus-based 
evaluation of a wide-coverage grammar for 
English. In Using Evaluation within Human 
Language Technology. LREC. Athens.  
D. Proux, F. Rechenmann, L. Julliard, V. Pillet, B. 
Jacq (1998). Detecting gene symbols and names 
in biological texts : a first step toward pertinent 
information extraction. In Genome Informatics, 
vol. 9, pp. 72-80. 
G. Schneider (1998). A Linguistic Comparison of 
Constituency, Dependency and Link Grammar. 
PhD thesis, Institut f?r Informatik der Universit?t 
Z?rich, Switzerland. 
D. Sleator and D. Temperley (1993). Parsing 
English with a Link Grammar. In Third 
International Workshop on Parsing 
Technologies. Tilburg. Netherlands. 
D. Weissenbacher (2004). La relation de 
synonymie en g?nomique. In Recital conference. 
Fes. 
49
Automatic extraction of paraphrastic phrases
from medium size corpora
Thierry Poibeau
Laboratoire d?Informatique de Paris-Nord ? CNRS UMR 7030
Av. J.B. Cl?ment ? F-93430 Villetaneuse
thierry.poibeau@lipn.univ-paris13.fr
Abstract
This paper presents a versatile system
intended to acquire paraphrastic phrases
from a representative corpus. In order to
decrease the time spent on the elaboration of
resources for NLP system (for example
Information Extraction, IE hereafter), we
suggest to use a knowledge acquisition
module that helps extracting new
information despite linguistic variation
(textual entailment). This knowledge is
automatically derived from the text
collection, in interaction with a large
semantic network.
1 Introduction
Recent researches in NLP have promoted a
now widely-accepted shallow-based analysis
framework that has proven to be efficient for a
number of tasks, including information
extraction and question answering. However,
this approach often leads to over-simplified
solutions to complex problems. For example,
the bag-of-words approach fails in examples
such as: Lee Harvey Oswald, the gunman who
assassinated President John F. Kennedy, was
later shot and killed by Jack Ruby (example
taken from Lin and Katz, 2003). In this case, it
is essential to keep track of the argument
structure of the verb, to be able to infer that it
is Jack Ruby and not John Kennedy who is the
murderer of Lee Harvey Oswald. A wrong
result would be obtained considering too
shallow analysis techniques or heuristics,
based for example of the proximity between
two person names in the sentence.
Several studies have recently proposed
some approaches based on the redundancy of
the web to acquire extraction patterns and
semantic structures. However, these methods
cannot be applied to medium size corpora.
Moreover, existing structured knowledge
contained in dictionaries, thesauri or semantic
networks can boost the learning process by
providing clear intuition over text units.
In this paper, we propose a knowledge rich
approach to paraphrase acquisition. We will
firstly describe some related work for the
acquisition of knowledge, especially
paraphrases, from texts. We then describe how
semantic similarity between words can be
inferred from large semantic networks. We
present an acquisition process, in which the
semantic network is projected on the corpus to
derive extraction patterns. This mechanism can
be seen as a dynamic lexical tuning of
information contained in the semantic network
in order to generate paraphrases of an original
pattern. In the last section, we propose an
evaluation and some perspectives.
2 Related work
This section presents some related works
for the acquisition of extraction patterns and
paraphrases from texts.
2.1 IE and resource acquisition
IE is known to have established a now
widely accepted linguistic architecture based
on cascading automata and domain-specific
knowledge (Appelt et al 1993). However,
several studies have outlined the problem of
the definition of the resources. For example,
E. Riloff (1995) says that about 1500 hours are
necessary to define the resources for a text
classification system on terrorism
1
. Most of
these resources are variants of extraction
patterns, which have to be manually
established.
                                                       
1
 We estimate that the development of resources for
IE is at least as long as for text classification.
To address this problem of portability, a
recent research effort focused on using
machine learning throughout the IE process
(Muslea, 1999). A first trend was to directly
apply machine learning methods to replace IE
components. For example, statistical methods
have been successfully applied to the named-
entity task. Among others, (Bikel et a., 1997)
learns names by using a variant of hidden
Markov models.
2.2 Extraction pattern learning
Another research area trying to avoid the
time-consuming task of elaborating IE
resources is concerned with the generalization
of extraction patterns from examples.  (Muslea,
1999) gives an extensive description of the
different approaches of that problem. Autoslog
(Riloff, 1993) was one of the very first systems
using a simple form of learning to build a
dictionary of extraction patterns. Ciravegna
(2001) demonstrates the interest of
independent acquisition of left and right
boundaries of extraction patterns during the
learning phase. In general, the left part of a
pattern is easier to acquire than the right part
and some heuristics can be applied to infer the
right boundary from the left one. The same
method can be applied for argument
acquisition: each argument can be acquired
independently from the others since the
argument structure of a predicate in context is
rarely complete.
Collins and Singer (1999) demonstrate how
two classifiers operating on disjoint features
sets recognize named entities with very little
supervision. The method is interesting in that
the analyst only needs to provide some seed
examples to the system in order to learn
relevant information. However, these
classifiers must be made interactive in order
not to diverge from the expected result, since
each error is transmitted and amplified by
subsequent processing stages. Contrary to this
approach, partially reproduced by Duclaye et
al. (2003) for paraphrase learning, we prefer a
slightly supervised method with clear
interaction steps with the analyst during the
acquisition process, to ensure the solution is
converging.
3 Overview of the approach
Argument structure acquisition is a
complex task since the argument structure is
rarely complete. To overcome this problem, we
propose an acquisition process in which all the
arguments are acquired separately.
Figure 1 presents an outline of the overall
paraphrase acquisition strategy. The process is
made of automatic steps and manual validation
stages. The process is weakly supervised since
the analyst only has to provide one example to
the system. However, we observed that the
quality of the acquisition process highly
depends from this seed example, so that
several experiments has to be done for the
acquisition of an argument structure, in order
to be sure to obtain an accurate coverage of a
domain.
From the seed pattern, a set of paraphrases
is automatically acquired, using similarity
measures between words and a shallow
syntactic analysis of the found patterns, in
order to ensure they describe a predicative
sequence. All these stages are described below,
after the description of similarity measures
allowing to calculate the semantic proximity
between words.
 
Seed pattern selection  
 
 
Automatic step  
Paraphrase acquisition  
Syntactic expansion  
Semantic expansion  
Semantic  
net 
Corpus  
Validation 
Figure 1: Outline of the acquisition process
End-user input
Interaction with
the end-user
4 Similarity measures
Several studies have recently proposed
measures to calculate the semantic proximity
between words. Different measures have been
proposed, which are not easy to evaluate (see
(Lin and Pantel, 2002) for proposals). The
methods proposed so far are automatic or
manual and generally imply the evaluation of
word clusters in different contexts (a word
cluster is close to another one if the words it
contains are interchangeable in some linguistic
contexts).
Budanitsky and Hirst (2001) present the
evaluation of 5 similarity measures based on
the structure of Wordnet. All the algorithms
they examine are based on the hypernym-
hyponym relation which structures the
classification of clusters inside Wordnet (the
synsets). They sometimes obtain unclear
conclusions about the reason of the
performances of the different algorithms (for
example, comparing Jiang and Conrath?s
measure (1997) with Lin?s one (1998): ?It
remains unclear, however, just why it
performed so much better than Lin?s measure,
which is but a different arithmetic combination
of the same terms?). However, the authors
emphases on the fact that the use of the sole
hyponym relation is insufficient to capture the
complexity of meaning: ?Nonetheless, it
remains a strong intuition that hyponymy is
only one part of semantic relatedness;
meronymy, such as whee l?ca r, is most
definitely an indicator of semantic relatedness,
and, a fortiori, semantic relatedness can arise
from little more than common or stereotypical
associations or statistical co-occurrence in real
life (for example, penguin?Antarc t i ca;
birthday?candle; sleep?pajamas)?.
In this paper, we propose to use the
semantic distance described in (Dutoit et al,
2002) which is based on a knowledge-rich
semantic net encoding a large variety of
semantic relationships between set of words,
including meronymy and stereotypical
associations.
The semantic distance between two words A
and B  is based on the notion of  nearest
common ancestors (NCA) between A and B .
NCA is defined as the set of nodes that are
daughters of c(A) ? c(B) and that are not
ancestors in c(A) ? c(B). The activation
measure d
_
 is equal to the mean of the weight
of each NCA calculated from A and  B?:
d
?
 (A, B)
 
= 
?
=
+
n
1i
ii ))NCA,B(d)NCA,A(d(
n
1
Please, refer to  (Dutoit and Poibeau, 2002) for
more details and examples. However, this
measure is sensitive enough to give valuable
results for a wide variety of applications,
including text filtering and information
extraction (Poibeau et al, 2002).
5 The acquisition process
The process begins as the end-user provides
a predicative linguistic structure to the system
along with a representative corpus. The system
tries to discover relevant parts of text in the
corpus based on the presence of plain words
closely related to the ones of the seed pattern.
A syntactic analysis of the sentence is then
done to verify that these plain words
correspond to a paraphrastic structure. The
method is close to the one of Morin and
Jacquemin (1999), who first try to locate
couples of relevant terms and then apply
relevant patterns to analyse the nature of their
relationship. However, Morin and Jacquemin
only focus on term variations whereas we are
interested in predicative structures, being either
verbal or nominal. The syntactic variations we
have to deal with are then different and, for a
part, more complex than the ones examined by
Morin and Jacquemin.
The detail algorithm is described below:
1. The head noun of the example pattern is
compared with the head noun of the
candidate pattern using the proximity
measure from (Dutoit et al, 2002). This
result of the measure must be under a
threshold fixed by the end-user.
2. The same condition must be filled by the
?expansion? element (possessive phrase
or verb complement in the candidate
pattern).
3. The structure must be predicative (either
a nominal or a verbal predicate, the
algorithm does not make any difference
at this level).
The following schema (Figure 2) resumes the
acquisition process.
Figure 2: paraphrase acquisition
Finally, this process is formalized throughout
the algorithm 1. Note that the predicative form
is acquired together with its arguments, as in a
co-training process.
P ? pattern to be found
S ? Sentence to analyze
C ? Phrases(S)
W ? Plain_words(S)
Result ? empty list
head ? Head word of the pattern P
exp ? Expansion word of the pattern P
Threshold ?  threshold fixed by the
analyst
For every word w
i
 from W do
 Prox
1
 = d?
?
(head, w
i
)
 If (Prox
1
 <= Threshold) then
   w
i+1
 ? Next element from W (if end of
sentence then exit)
  Prox
2
 = d?
?
(exp, w
i+1
)
  If (Prox
2
 <= Threshold) then
    If there is c ? C so that (w
i
 ? c) and
     (w
i+1
 ? c) then
       Result ? Add (w
i
, w
i+1
)
    End_if
   End_if
  End_if
End_for
  Algorithm 1?: Paraphrastic phrases
acquisition
The result of this analysis is a table
representing predicative structures, which are
semantically equivalent to the initial example
pattern. The process uses the corpus and the
semantic net as two different complementary
knowledge sources:
? The semantic net provides information
about lexical semantics and relations
between words
? The corpus attests possible expressions
and filter irrelevant ones.
We performed an evaluation on different
French corpora, given that the semantic net is
especially rich for this language. We take the
expression cession de soci?t?  (company
transfer) as an initial pattern. The system then
discovered the following expressions, each of
them being semantic paraphrases of the initial
seed pattern:
reprise des activit?s
rachat d?activit?
acqu?rir des magasins
racheter *c-company*
cession de *c-company*?
The result must be manually validated. Some
structures are found even if they are irrelevant,
due to the activation of irrelevant links. It is the
case of the expression renoncer ? se porter
acqu?reur (to give up buying sthg), which is
not relevant. In this case, there was a spurious
link between to give up and company in the
semantic net.
5.1 Dealing with syntactic variations
The previous step extract semantically
related predicative structures from a corpus.
These structures are found in the corpus in
various linguistic structures, but we want the
system to be able to find this information even
if it appears in other kind of linguistic
sequences. That is the reason why we associate
some meta-graphs with the linguistic
structures, so that different transformations can
be recognized. This strategy is based on Harris
theory of sublanguages (1991). These
transformations concern the syntactic level,
either on the head (H) or on the expansion part
(E) of the linguistic structure.
Semantic
link
Acquisition 
Head
Company 
Exp
Enterprise
Factory
Holding?
Acquire
Buy
Seizure?
Predicative  link
Semantic
link
The meta-graphs encode transformations
concerning the following structures:
? Subject ? verb,
? Verb ? direct object,
? Verb ? indirect object (especially when
introduced by the French preposition ?
or de),
? Noun ? possessive phrase.
These meta-graphs encode the major part of
the linguistic structures we are concern with in
the process of IE.
The graph on Figure 4 recognizes the
following sequences (in brackets we underline
the couple of words previously extracted from
the corpus):
Reprise des activit?s charter? (H:
reprise, E: activit?)
Reprendre les activit?s charter?
(H: reprendre, E: activit?)
Reprise de l?ensemble des magasins
suisse? (H: reprise, E: magasin)
Reprendre l?ensemble des magasins
suisse? (H: reprendre, E: magasin)
Racheter les diff?rentes activit?s?
(H: racheter, E: activit?)
Rachat des diff?rentes activit?s?
(H: rachat, E: activit?)
This kind of graph is not easy to read. It
includes at the same time some linguistic tags
and some applicability constraints. For
example, the first box contains a reference to
the @A  column in the table of identified
structures. This column contains a set of binary
constraints, expressed by some signs +  or - .
The sign + means that the identified pattern is
of type verb-direct object: the graph can then
be applied to deal with passive structures. In
other words, the graph can only be applied in a
sign + appears in the @ A column of the
constraints table. The constraints are removed
from the instantiated graph. Even if the
resulting graph is normally not visible (the
compilation process directly produced a graph
in a binary format), we give an image of a part
of that graph on Figure 4.
This mechanism using constraint tables and
meta-graph has been implemented in the finite-
state toolbox INTEX (Silberztein, 1993). 26
meta-graphs have been defined modeling
linguistic variation for the 4 predicative
structures defined above. The phenomena
mainly concern the insertion of modifiers (with
the noun or the verb), verbal transformations
(passive) and phrasal structures (relative
clauses like ?Vivendi, qui a rachet?
Universal?Vivendi, that bought Universal).
The compilation of the set of meta-graphs
produces a graph made of 317 states and 526
Figure 4: a syntactic meta-graph
Figure 3: the linguistic constraint table
relations. These graphs are relatively abstract
but the end-user is not intended to directly
manipulate them. They generate instantiated
graphs, that is to say graphs in which the
abstract variables have been replaced linguistic
information as modeled in the constraint
tables. This method associates a couple of
elements with a set of transformation that
covers more examples than the one of the
training corpus. This generalization process is
close to the one imagined by Morin and
Jacquemin (1999) for terminology analysis but,
as we already said, we cover sequences that are
not only nominal ones.
6 Evaluation
The evaluation concerned the extraction of
information from a French financial corpus,
about companies buying other companies. The
corpus is made of 300 texts (200 texts for the
training corpus, 100 texts for the test corpus).
A system was first manually developed and
evaluated. We then tried to perform the same
task with automatically developed resources,
so that a comparison is possible. The corpus is
firstly normalized. For example, all the
company names are replaced by a variable *c-
company* thanks to the named entity
recognizer. In the semantic network, *c-
company* is introduced as a synonym of
company, so that all the sequences with a
proper name corresponding to a company
could be extracted.
For the slot corresponding to the company
that is being bought, 6 seed example patterns
were given to semantic expansion module.
This module acquired from the corpus 25 new
validated patterns. Each example pattern
generated 4.16 new patterns on average. For
example, from the pattern rachat de
*c-company* we obtain the following list:
reprise de *c-company*
achat de *c-company*
acqu?rir *c-company*
racheter *c-company*
cession de *c-company*
This set of paraphrastic patterns includes
nominal phrases (reprise de *c-company*)
and verbal phrases (racheter *c-company*).
The acquisition process concerns at the same
time, the head and the expansion. The
simultaneous acquisition of different semantic
classes can also be found in the co-training
algorithm proposed for this kind of task by E.
Riloff and R. Jones (Riloff et Jones, 1999).
The proposed patterns must be filtered and
validated by the end-user. We estimate that
generally 25% of the acquired pattern should
be rejected. However, this validation process is
very rapid: a few minutes only were necessary
to check the 31 proposed patterns and retain 25
of them.
We then compared these results with the
ones obtained with the manually elaborated
system. The evaluation concerned the three
slots that necessitate a syntactic and semantic
analysis: the company that is buying another
one (arg1) the company that is being bought
(arg2), the company that sells (arg3). These
slots imply nominal phrases, they can be
complex and a functional analysis is most of
the time necessary (is the nominal phrase the
subject or the direct object of the sentence?).
We thus chose to perform an operational
evaluation: what is evaluated is the ability of a
given phrase or pattern to fill a given slot (also
called textual entailment by Dagan and
Glickman [2004]). This kind of evaluation
avoids, as far as possible, the bias of human
judgment on possibly ambiguous expressions.
An overview of the results is given below
(P refers to precision, R to recall, F to the
harmonic mean between P and R):
Arg 1 Arg 2 Arg 3
P: 100
R: 90
P: 100
R: 91.6
P: 99
R: 92
Human
annotators
F: 94.7 F: 95.6 F: 94.2
P: 79.6
R: 62.6
P: 93.4
R: 73
P: 88.4
R: 70
Automaticall
y acquired
resources
F: 70 F: 81.9 F: 77
We observed that the system running with
automatically defined resources is about 10%
less efficient than the one with manually
defined resources. The decrease of
performance may vary in function of the slot
(the decrease is less important for the arg2 than
for arg1 or arg3). Two kind of errors are
observed: Certain sequences are not found
because a relation between words is missing in
the semantic net. Some sequences are extracted
by the semantic analysis but do not correspond
to a transformation registered in the syntactic
variation management module.
7 Conclusion
In this paper, we have shown an efficient
algorithm to semi-automatically acquire
paraphrastic phrases from a semantic net and a
corpus. We have shown that this approach is
highly relevant in the framework of IE
systems. Even if the performance decrease
when the resources are automatically defined,
the gain in terms of development time is
sufficiently significant to ensure the usability
of the method.
8 References
Appelt D.E, Hobbs J., Bear J., Israel D., Kameyana
M. and Tyson M. (1993) FASTUS: a finite-state
processor for information extraction from real-
world text. Proceedings of IJCAI?93, Chamb?ry,
France, pp. 1172?1178.
Bikel D., Miller S., Schwartz R. and Weischedel R.
(1997) Nymble: a high performance learning
name-finder. Proceeding of the 5
th
 ANLP
Conference, Washington, USA.
Budanitsky A. and Hirst G. (2001) Semantic
distance in WordNet: An experimental,
application-oriented evaluation of five measures.
Workshop on WordNet and Other Lexical
Resources, in NAACL 2001, Pittsburgh.
Ciravegna F. (2001) Adaptive Information
Extraction from Text by Rule Induction and
Generalisation. Proceedings of the 17
th
International Joint Conference on Artificial
Intelligence (IJCAI?2001), Seattle, pp.
1251?1256.
Collins M. and Singer Y. (1999) Unsupervised
m o d e l s  for named entity classification.
Proceedings of EMNLP-WVLC?99, College
Park, pp. 100?110.
Dagan I. and Glickman O. (2004) Probabilistic
Textual Entailment: Generic Applied Modeling
of Language Variability. Workshop Learning
Methods for Text Understanding and Mining.
Grenoble, France.
Duclaye F., Yvon F. and Collin O. (2003) Learning
paraphrases to improve a question answering
system. Proceeding of the EACL Workshop
?NLP for Question Answering?, Budapest,
Hungary.
Dutoit D. and Poibeau T. (2002) Deriving
knowledge from a large semantic network,
Proceedings of COLING?2002, Taipei, Taiwan,
pp. 232?238.
Fellbaum C. (1998) WordNet : An Electronic
Lexical Database, edited by Fellbaum, MIT
press.
Grefenstette G. (1998) Evaluating the adequancy of
a multilingual transfer dictionary for the Cross
Language Information Retrieval, LREC 1998.
Harris Z. (1991) A theory of language and
information: a mathematical approach. Oxford
University Press. Oxford.
Jiang J. and Conrath D. (1997) Semantic similarity
based on corpus statistics and lexical taxonomy.
Proceedings of International Conference on
Research in Computational Linguistics, Taiwan.
Jones R., McCallum A., Nigam K. and Riloff E.
(1999) Bootstrapping for Text Learning Tasks.
Proceedings of the IJCAI?99 Workshop on Text
Mining: Foundations, Techniques and
Applications, Stockholm, 1999, pp. 52?63.
Lin D. (1998) An information-theoretic definition
of similarity. Proceedings of the 15th International
Conference on Machine Learning, Madison, WI.
Lin D. and Pantel P. (2002) Concept Discovery
from Text. Proceedings of COLING?2002,
Taipei, Taiwan, pp. 577?583.
Lin J. and Katz B. (2003) Q/A techniques for
WWW. Tutorial. 10
th
 Meeting of the European
Association for Computational Linguistics
(EACL?03), Budapest, 2003.
Morin E. and Jacquemin C. (1999) Projecting
corpus-based semantic links on a thesaurus.
Proceedings of the 37th ACL, pp. 389?396.
Muslea I. (1999) Extraction patterns for
Information Extraction tasks: a survey, AAAI?99
(avai lable  a t  the  fol lowing URL:
http://www.isi.edu/~muslea/ RISE/ML4IE/)
Pazienza M.T, ed. (1997) Information extraction.
Springer Verlag  (Lecture Notes in computer
Science), Heidelberg, Germany.
Poibeau T.,?Dutoit D., Bizouard S.? (2002)
Evaluating resource acquisition tools for
Information Extraction. Proceeding of the
International Language Resource and Evaluation
Conference (LREC 2002), Las Palmas.
Riloff E. (1993) Automatically constructing a
dictionary for formation extraction tasks,
AAAI?93, Stanford, USA, pp. 811?816.
Riloff E. (1995) Little Words Can Make a Big
Difference for Text Classification, Proceedings of
the SIGIR'95, Seattle, USA, pp. 130?136.
Riloff E. et Jones R.? (1999) Learning Dictionaries
for Information Extraction by Multi-Level
Bootstrapping. Proceedings of the 16th National
Conference on Artificial Intelligence (AAAI?99),
Orlando, 1999, pp. 474?479.
Silberztein M. (1993) Dictionnaires ?lectroniques
et analyse automatique des textes, Masson, Paris,
France.
BioNLP 2007: Biological, translational, and clinical language processing, pages 113?120,
Prague, June 2007. c?2007 Association for Computational Linguistics
Automatically Restructuring Practice Guidelines using the GEM DTD  
Amanda Bouffier                       Thierry Poibeau 
Laboratoire d?Informatique de Paris-Nord 
Universit? Paris 13 and CNRS UMR 7030 
99, av. J.-B. Cl?ment ? F-93430 Villetaneuse 
firstname.lastname@lipn.univ-paris13.fr 
 
 
Abstract 
This paper describes a system capable of 
semi-automatically filling an XML template 
from free texts in the clinical domain (prac-
tice guidelines). The XML template includes 
semantic information not explicitly encoded 
in the text (pairs of conditions and ac-
tions/recommendations). Therefore, there is 
a need to compute the exact scope of condi-
tions over text sequences expressing the re-
quired actions. We present a system devel-
oped for this task. We show that it yields 
good performance when applied to the 
analysis of French practice guidelines.  
1 Introduction  
During the past years, clinical practices have con-
siderably evolved towards standardization and ef-
fectiveness. A major improvement is the develop-
ment of practice guidelines (Brownson et al, 2003). 
However, even if widely distributed to hospitals, 
doctors and other medical staff, clinical practice 
guidelines are not routinely fully exploited1. There 
is now a general tendency to transfer these guide-
lines to electronic devices (via an appropriate XML 
format). This transfer is justified by the assumption 
that electronic documents are easier to browse than 
paper documents.  
However, migrating a collection of texts to XML 
requires a lot of re-engineering. More precisely, it 
means analyzing the full set of textual documents 
so that they can fit with strict templates, as required 
either by XML schemas or DTD (document type 
definition). Unfortunately, most of the time, the 
                                                 
1 See (Kolata, 2004). This newspaper article is a good example 
of the huge social impact of this research area. 
semantic blocks of information required by the 
XML model are not explicitly marked in the origi-
nal text. These blocks of information correspond to 
discourse structures. 
This problem has thus renewed the interest for 
the recognition and management of discourse struc-
tures, especially for technical domains. In this 
study, we show how technical documents belong-
ing to a certain domain (namely, clinical practice 
guidelines) can be semi-automatically structured 
using NLP techniques. Practice guidelines describe 
best practices with the aim of guiding decisions and 
criteria in specific areas of healthcare, as defined 
by an authoritative examination of current evidence 
(evidence-based medicine, see Wikipedia or 
Brownson et al, 2003).  
The Guideline Elements Model (GEM) is an 
XML-based guideline document model that can 
store and organize the heterogeneous information 
contained in practice guidelines (Schiffman, 2000). 
It is intended to facilitate translation of natural lan-
guage guideline documents into a format that can 
be processed by computers. The main element of 
GEM, knowledge component, contains the most 
useful information, especially sequences of condi-
tions and recommendations. Our aim is thus to 
format these documents which have been written 
manually without any precise model, according to 
the GEM DTD (see annex A).  
The organization of the paper is as follows: first, 
we present the task and some previous approaches 
(section 2). We then describe the different process-
ing steps (section 3) and the implementation (sec-
tion 4). We finish with the presentation of some 
results (section 5), before the conclusion (section 6). 
113
2 Document Restructuring: the Case of 
Practice Guidelines 
As we have previously seen, practice guidelines are 
not routinely fully exploited. One reason is that 
they are not easily accessible to doctors during 
consultation. Moreover, it can be difficult for the 
doctor to find relevant pieces of information from 
these guides, even if they are not very long. To 
overcome these problems, national health agencies 
try to promote the electronic distribution of these 
guidelines (so that a doctor could check recom-
mendations directly from his computer).  
2.1 Previous Work 
Several attempts have already been made to im-
prove the use of practice guidelines: for example 
knowledge-based diagnostic aids can be derived 
from them (e.g. S?roussi et al, 2001).  
GEM is an intermediate document model, be-
tween pure text (paper practice guidelines) and 
knowledge-based models like GLIF (Peleg et al, 
2000) or EON (Tu and Musen, 2001). GEM is thus 
an elegant solution, independent from any theory or 
formalisms, but compliant with other frameworks. 
GEM Cutter (http://gem.med.yale.edu/) is a 
tool aimed at aiding experts to fill the GEM DTD 
from texts. However, this software is only an inter-
face allowing the end-user to perform the task 
through a time-consuming cut-and-paste process. 
The overall process described in Shiffman et al 
(2004) is also largely manual, even if it is an at-
tempt to automate and regularize the translation 
process.  
The main problem in the automation of the 
translation process is to identify that a list of rec-
ommendations expressed over several sentences is 
under the scope of a specific condition (conditions 
may refer to a specific pathology, a specific kind of 
patients, temporal restrictions, etc.). However, pre-
vious approaches have been based on the analysis 
of isolated sentences. They do not compute the ex-
act scope of conditional sequences (Georg and 
Jaulent, 2005): this part of the work still has to be 
done by hand.  
Our automatic approach relies on work done in 
the field of discourse processing. As we have seen 
in the introduction, the most important sequences 
of text to be tagged correspond to discourse struc-
tures (conditions, actions ?). Although most re-
searchers agree that a better understanding of text 
structure and text coherence could help extract 
knowledge, descriptive frameworks like the one 
developed by Halliday and Hasan2 are poorly for-
malized and difficult to apply in practice.  
Some recent works have proposed more opera-
tional descriptions of discourse structures (P?ry-
Woodley, 1998). Several authors (Halliday and 
Matthiessen, 2004; Charolles, 2005) have investi-
gated the use of non-lexical cues for discourse 
processing (e.g temporal adverbials like ?in 1999?). 
These adverbials introduce situation frames in a 
narrative discourse, that is to say a ?period? in the 
text which is dependent from the adverbial.  
We show in this study that condition sequences 
play the same role in practice guidelines: their 
scope may run over several dependent clauses 
(more precisely, over a set of several recommenda-
tions). Our plan is to automatically recognize these 
using surface cues and processing rules.  
2.2 Our Approach 
Our aim is to semi-automatically fill a GEM tem-
plate from existing guidelines: the algorithm is 
fully automatic but the result needs to be validated 
by experts to yield adequate accuracy. Our system 
tries to compute the exact scope of conditional se-
quences. In this paper we apply it to the analysis of 
several French practice guidelines.  
The main aim of the approach is to go from a 
textual document to a GEM based document, as 
shown on Figure 1 (see also annex A). We focus on 
conditions (including temporal restrictions) and 
recommendations since these elements are of 
paramount importance for the task. They are espe-
cially difficult to deal with since they require to 
accurately compute the scope of conditions.  
The example on figure 1 is complex since it con-
tains several levels of overlapping conditions. We 
observe a first opposition (Chez le sujet non immu-
nod?prim? / chez le sujet immunod?prim?? Con-
cerning the non-immuno-depressed patient / Con-
cerning the immuno-depressed patient?) but a sec-
ond condition interferes in the scope of this first 
level (En cas d?aspect normal de la muqueuse il?-
ale? If the ileal mucus seems normal?). The task 
involves recognizing these various levels of condi-
tions in the text and explicitly representing them 
through the GEM DTD.   
                                                 
2 See ?the text-forming component in the linguistic system? in 
Halliday and Hasan (1976:23). 
114
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. From the text to GEM  
 
What is obtained in the end is a tree where the 
leaves are recommendations and the branching 
nodes correspond to the constraints on conditions. 
2.3 Data  
We analyzed 18 French practice guidelines pub-
lished by French national health agency (ANAES, 
Agence Nationale d?Accr?ditation et d?Evaluation 
en Sant? and AFSSAPS, Agence Francaise de S?-
curit? Sanitaire des Produits de Sant?) between 
2000 and 2005. These practice guidelines focus on 
different pathologies (e.g. diabetes, high blood 
pressure, asthma etc.) as well as with clinical 
examination processes (e.g. digestive endoscopy). 
amination processes (e.g. digestive endoscopy). 
The data are thus homogeneous, and is about 250 
pages long (150,000+ words). Most of these prac-
tice guidelines are publicly available at: 
http://www.anaes.fr or http://affsaps.sante 
.fr. Similar documents have been published in 
English and other languages; the GEM DTD is 
language independent.  
3 Processing Steps 
Segmenting a guideline to fill an XML template is 
a complex process involving several steps. We de-
scribe here in detail the most important steps 
(mainly the way the scope of conditional sequences 
is computed), and will only give a brief overview 
of the pre-processing stages.  
3.1 Overview 
A manual study of several French practice guide-
lines revealed a number of trends in the data. We 
observed that there is a default structure in these 
guidelines that may help segmenting the text accu-
rately. This default segmentation corresponds to a 
highly conventionalized writing style used in the 
document (a norm). For example, the location of 
conditions is especially important: if a condition 
occurs at the opening of a sequence (a paragraph, a 
section?), its scope is by default the entire follow-
ing text sequence. If the condition is included in the 
sequence (inside a sentence), its default scope is 
restricted to the current sentence (Charolles, 2005 
for similar observations on different text types).  
This default segmentation can be revised if some 
linguistic cues suggest another more accurate seg-
mentation (violation of the norm). We make use of 
Halliday?s theory of text cohesion (Halliday and 
Hasan, 1976). According to this theory, some ?co-
hesion cues? suggest extending the default segmen-
tation while some others suggest limiting the scope 
of the conditional sequence (see section 3.4).  
3.2 Pre-processing (Cue Identification) 
The pre-processing stage concerns the analysis of 
relevant linguistic cues. These cues vary in nature: 
they can be based either on the material structure or 
the content of texts. We chose to mainly focus on 
task-independent knowledge so that the method is 
portable, as far as possible (we took inspiration 
from Halliday and Matthiessen?s introduction to 
functional grammar, 2004). Some of these cues 
 
<recommandation> 
<decision.variable>Chez le sujet non immunod?prim? 
</decsion.variable> 
<decision.variable>en cas d'aspect macroscopique nor-
mal de la muqueuse colique </decison.variable> 
<action> des biopsies coliques nombreuses et ?tag?es 
sont recommand?es (?) </action> 
<action>Les biopsies isol?es sont insuffisantes(..)      
</action> 
<action>L?exploration de l?il?on terminal est ?gale-
ment recommand?e</action> 
</recommandation> 
 
<recommandation> 
<decision.variable>Chez le sujet non immunod?prim? 
</decsion.variable> 
<decision.variable>en cas d'aspect macroscopique nor-
mal de la muqueuse colique </decison.variable> 
<decision.variable>En cas d'aspect normal de la mu-
queuse il?ale</decision.variable> 
<action>la r?alisation de biospsies n'est pas syst?ma-
tique</action> 
</recommandation> 
 
<recommandation 
<decision.variable>Chez le sujet immunod?pri-
m?</decision.variable> 
<action> il est n?cessaire de r?aliser des biopsies 
syst?matiques(?)</action> 
</recommandation> 
 
Chez le sujet non immunod?prim?, en cas d'as-
pect macroscopique normal de la muqueuse co-
lique, des biopsies coliques nombreuses et ?tag?es 
sont recommand?es (?). Les biopsies isol?es sont 
insuffisantes (?). 
L'exploration de l'il?on terminal est ?galement re-
command?e (grade C). En cas d'aspect normal de 
la muqueuse il?ale (?), la r?alisation de biospsies 
n'est pas syst?matique (accord professionnel). 
 
Chez le sujet immunod?prim?, il est n?cessaire de 
r?aliser des biopsies syst?matiques (?) 
 
115
(especially connectors and lexical cues) can be 
automatically captured by machine learning meth-
ods.  
Material structure cues. These features include the 
recognition of titles, section, enumerations and 
paragraphs. 
Morpho-syntactic cues. Recommendations are not 
expressed in the same way as conditions from a 
morpho-syntactic point of view. We take the fol-
lowing features into account: 
? Part of speech tags. For example recommand? 
should be a verb and not a noun, even if the 
form is ambiguous in French; 
? Tense and mood of the verb. Present and future 
tenses are relevant, as well as imperative and 
conditional moods. Imperative and future al-
ways have an injunctive value in the texts. In-
junctive verbs (see lexical cues) lose their in-
junctive property when used in a past tense. 
Anaphoric cues. A basic and local analysis of ana-
phoric elements is performed. We especially fo-
cused on expressions such as dans ce cas, dans les 
N cas pr?c?dents (in this case, in the n preceding 
cases?) which are very frequent in clinical docu-
ments. The recognition of such expressions is 
based on a limited set of possible nouns that oc-
curred in context, together with specific constraints 
(use of demonstrative pronouns, etc).   
Conjunctive cues (discourse connectors). Condi-
tions are mainly expressed through conjunctive 
cues. The following forms are especially interest-
ing: forms prototypically expressing conditions (si, 
en cas de, dans le cas o?? if, in case of?); Forms 
expressing the locations of some elements (chez, en 
pr?sence de... in presence of?); Forms expressing 
a temporal frame (lorsque, au moment o?, avant 
de? when, before?) 
Lexical cues. Recommendations are mainly ex-
pressed through lexical cues. We have observed 
forms prototypically expressing recommendations 
(recommander, prescrire, ? recommend, pre-
scribe), obligations (devoir, ? shall) or options 
(pouvoir, ?  can). Most of these forms are highly 
ambiguous but can be automatically acquired from 
an annotated corpus. Some expressions from the 
medical domains can be automatically extracted 
using a terminology extractor (we use Yatea, see 
section 4, ?Implementation?).  
3.3 Basic Segmentation 
A basic segment corresponds to a text sequence 
expressing either a condition or a recommendation. 
It is most of the time a sentence, or a proposition 
inside a sentence.  
Some of the features described in the previous 
section may be highly ambiguous. For this reason 
basic segmentation is rarely done according to a 
single feature, but most of the time according to a 
bundle of features acquired from a representative 
corpus. For example, if a text sequence contains an 
injunctive verb with an infinitive form at the begin-
ning of a sentence, the whole sequence is typed as 
action. The relevant sets of co-occurring features 
are automatically derived from a set of annotated 
practice guidelines, using the chi-square test to cal-
culate the dissimilarity of distributions.  
After this step, the text is segmented into typed 
basic sequences expressing either a recommenda-
tion or a condition (the rest of the text is left 
untagged).  
3.4 Computing Frames and Scopes 
As for quantifiers, a conditional element may have 
a scope (a frame) that extends over several basic 
segments. It has been shown by several authors 
(Halliday and Matthiessen, 2004; Charolles, 2005) 
working on different types of texts that conditions 
detached from the sentence have most of the time a 
scope beyond the current sentence whereas condi-
tions included in a sentence (but not in the begin-
ning of a sentence) have a scope which is limited to 
the current sentence. Accordingly we propose a 
two-step strategy: 1) the default segmentation is 
done, and 2) a revision process is used to correct 
the main errors caused by the default segmentation 
(corresponding to the norm). 
Default Segmentation  
We propose a strategy which makes use of the no-
tion of default. By default: 
1. Scope of a heading goes up to the next head-
ing; 
2. Scope of an enumeration?s header covers all 
the items of the enumeration ; 
3. If a conditional sequence is detached (in the 
beginning of a paragraph or a sentence), its 
scope is the whole paragraph; 
4. If the conditional sequence is included in a 
sentence, its scope is equal to the current 
sentence.  
116
Cases 3 and 4 cover 50-80% of all the cases, de-
pending on the practice guidelines used. However, 
this default segmentation is revised and modified 
when a linguistic cue is a continuation mark within 
the text or when the default segmentation seems to 
contradict some cohesion cue.  
Revising the Default Segmentation 
There are two cases which require revising the de-
fault segmentation: 1) when a cohesion mark indi-
cates that the scope is larger than the default unit; 
2) when a rupture mark indicates that the scope is 
smaller. We only have room for two examples, 
which, we hope, give a broad idea of this process. 
1) Anaphoric relations are strong cues of text 
coherence: they usually indicate the continuation of 
a frame after the end of its default boundaries.   
Figure 2. The last sentence introduced by dans les 
deux cas is under the scope of the conditions intro-
duced by lorsque3. 
In Figure 2, the expression dans les deux cas (in 
the two cases?) is an anaphoric mark referring to 
the two previous utterances. The scope of the con-
ditional segment introduced by lorsque (that would 
normally be limited to the sentence it appears in) is 
thus extended accordingly.  
2) Other discourse cues are strong indicators 
that a frame must be closed before its default 
boundaries. These cues may indicate some contras-
tive, corrective or adversative information (cepen-
dant, en revanche? however). Justifications cues 
(en effet, en fait ? in effect) also pertain to this 
class since a justification is not part of the action 
element of the GEM DTD.  
Figure 3 is a typical example. The linguistic cue 
en effet (in effect) closes the frame introduced by  
 
                                                 
3 In figures 2 and 3, bold and grey background are used only 
for sake of clarity; actual documents are made of text without 
any formatting. 
Figure 3. The last sentence contains a justification cue 
(en effet) which limits the scope of the condition in the 
preceding sentence.  
Chez les patients ayant initialement...(<1g/l) since 
this sequence should fill the explanation element 
of the GEM DTD and is not an action element. 
4 Implementation 
Accurate discourse processing requires a lot of in-
formation ranging from lexical cues to complex co- 
occurrence of different features. We chose to im-
plement these in a classic blackboard architecture 
(Englemore and Morgan, 1988). The advantages of 
this architecture for our problem are easy to grasp: 
each linguistic phenomenon can be treated as an 
independent agent; inference rules can also be 
coded as specific agents, and a facilitator controls 
the overall process. 
Basic linguistic information is collected by a set 
of modules called ?linguistic experts?.  Each mod-
ule is specialized in a specific phenomenon (text 
structure recognition, part-of-speech tagging, term 
spotting, etc.). The text structure and text format-
ting elements are recognized using Perl scripts. 
Linguistic elements are encoded in local grammars, 
mainly implemented as finite-state transducers 
(Unitex 4 ). Other linguistic features are obtained 
using publicly available software packages, e.g. a 
part-of-speech tagger (Tree Tagger5) and a term 
extractor (Yatea6), etc. Each linguist expert is en-
capsulated and produces annotations that are stored 
in the database of facts, expressed in Prolog (we 
thus avoid the problem of overlapping XML tags, 
which are frequent at this stage). These annotations 
are indexed according to the textual clause they 
appear in, but linear ordering of the text is not cru-
                                                 
4 http://www-igm.univ-mlv.fr/~unitex/ 
5 http://www.ims.uni-stuttgart.de/projekte/cor 
plex/TreeTagger/DecisionTreeTagger.html 
6 http://www-lipn.univ-paris13.fr/~hamon/YaTeA 
Chez les patients ayant initialement une concentra-
tion tr?s ?lev?e de LDL-cholest?rol, et notamment 
chez les patients ? haut risque dont la cible th?ra-
peutique est basse (<1g/l), le prescripteur doit garder 
? l?esprit que la prescription de statine ? fortes doses ou 
en association n?cessite une prise en compte au cas par 
cas du rapport b?n?fice/risque et ne doit jamais ?tre sys-
t?matique. En effet, les fortes doses de statines et les 
bith?rapies n?ont pas fait l?objet ? ce jour d?une ?valuation 
suffisante dans ces situations. 
 
(Prise en charge th?rapeutique du patient dyslipid?mique, 2005, 
p4) 
L?indication d?une insulinoth?rapie est recommand?e 
lorsque l?HbA1c est > 8%, sur deux contr?les suc-
cessifs sous l?association de sulfamides/metformine 
? posologie optimale. Elle est laiss?e ? l?appr?ciation par 
le clinicien du rapport b?n?fices/inconv?nients de 
l?insulinoth?rapie lorsque l?HbA1c est comprise entre 
6,6% et 8% sous la m?me association. Dans les deux 
cas, la di?t?tique aura au pr?alable ?t? r??valu?e et un 
facteur intercurrent de d?compensation aura ?t? recher-
ch?e (accord professionnel). 
 
Strat?gie de prise en charge du patient diab?tique de type 2 ? 
l?exclusion de la prise en charge des complications (2000) 
117
cial for further processing steps since the system 
mainly looks for co-occurrences of different cues. 
The resulting set of annotations constitutes the 
?working memory? of the system.  
Another set of experts then combine the initial 
disseminated knowledge to recognize basic seg-
ments (section 3.3) and to compute scopes and 
frames (section 3.4). These experts form the ?infer-
ence engine? which analyzes information stored in 
the working memory and adds new knowledge to 
the database. Even when linear order is irrelevant 
for the inference process new information is in-
dexed with textual clauses, to enable the system to 
produce the original text along with annotation.  
 A facilitator helps to determine which expert 
has the most information needed to solve the prob-
lem. It is the facilitator that controls, for example, 
the application of default rules and the revision of 
the default segmentation. It controls the chalk, me-
diating among experts competing to write on the 
blackboard. Finally, an XML output is produced 
for the document, corresponding to a candidate 
GEM version of the document (no XML tags over-
lap in the output since we produce an instance of 
the GEM DTD; all potential remaining conflicts 
must have been solved by the supervisor). To 
achieve optimal accuracy this output is validated 
and possibly modified by domain experts.  
5 Evaluation 
The study is based on a corpus of 18 practice 
guidelines in French (several hundreds of frames), 
with the aid of domain experts. We evaluated the 
approach on a subset of the corpus that has not 
been used for training.  
5.1 Evaluation Criteria 
In our evaluation, a sequence is considered correct 
if the semantics of the sequence is preserved. For 
example Chez l?ob?se non diab?tique (accord 
professionnel) (In the case of an obese person 
without any diabetes (professional approval)), 
recognition is correct even if professional approval 
is not stricto sensu part of the condition. On the 
other hand, Chez l?ob?se (In the case of an obese 
person) is incorrect. The same criteria are applied 
for recommendations. 
We evaluate the scope of condition sequences by 
measuring whether each recommendation is linked 
with the appropriate condition sequence or not.  
5.2 Manual Annotation and Inter-annotator 
Agreement 
The data is evaluated against practice guidelines 
manually annotated by two annotators: a domain 
expert (a doctor) and a linguist. In order to evaluate 
inter-annotator agreement, conditions and actions 
are first extracted from the text. The task of the 
human annotators is then to (manually) build a tree, 
where each action has to be linked with a condi-
tion. The output can be represented as a set of cou-
ples (condition ? actions). In the end, we calculate 
accuracy by comparing the outputs of the two an-
notators (# of common couples). 
Inter-annotator agreement is high (157 nodes out 
of 162, i.e. above .96 agreement). This degree of 
agreement is encouraging. It differs from previous 
experiments, usually done using more heterogene-
ous data, for example, narrative texts. Temporals 
(like ?in 1999?) are known to open a frame but 
most of the time this frame has no clear boundary. 
Practice guidelines should lead to actions by the 
doctor and the scope of conditions needs to be clear 
in the text. 
In our experiment, inter-annotator agreement is 
high, especially considering that we required an 
agreement between an expert and non-expert. We 
thus make the simplified assumption that the scope 
of conditions is expressed through linguistic cues 
which do not require, most of the time, domain-
specific or expert knowledge. Yet the very few 
cases where the annotations were in disagreement 
were clearly due to a lack of domain knowledge by 
the non-expert.  
5.3 Evaluation of the Automatic Recognition 
of Basic Sequences 
The evaluation of basic segmentation gives the fol-
lowing results for the condition and the recommen-
dation sequences. In the table, P is precision; R is 
recall; P&R is the harmonic mean of precision and 
recall  (P&R = (2*P*R) / (P+R), corresponding to a 
F-measure with a ? factor equal to 1). 
 
Conditions: 
 
 Without domain 
knowledge 
With domain 
knowledge 
P 1 1 
R .83 .86 
P&R .91 .92 
118
Recommendations: 
 
 Without domain 
knowledge 
With domain 
knowledge 
P 1 1 
R .94 .95 
P&R .97 .97 
 
Results are high for both conditions and recom-
mendations.  
The benefit of domain knowledge is not evident 
from overall results. However, this information is 
useful for the tagging of titles corresponding to 
pathologies. For example, the title Hypertension 
art?rielle (high arterial blood pressure) is equiva-
lent to a condition introduced by in case of? It is 
thus important to recognize and tag it accurately, 
since further recommendations are under the scope 
of this condition. This cannot be done without do-
main-specific knowledge. 
The number of titles differs significantly from 
one practice guideline to another. When the num-
ber is high, the impact on the performance can be 
strong. Also, when several recommendations are 
dependent on the same condition, the system may 
fail to recognize the whole set of recommendations.  
Finally, we observed that not all conditions and 
recommendations have the same importance from a 
medical point of view ? however, it is difficult to 
quantify this in the evaluation.  
5.4 Evaluation of the Automatic Recognition 
of the Scope of Conditions 
The scope of conditions is recognized with accu-
racy above .7 (we calculated this score using the 
same method as for inter-annotator agreement, see 
section 5.2).  
This result is encouraging, especially consider-
ing the large number of parameters involved in dis-
course processing. In most of successful cases the 
scope of a condition is recognized by the default 
rule (default segmentation, see section 3.4). How-
ever, some important cases are solved due to the 
detection of cohesion or boundary cue (especially 
titles).  
The system fails to recognize extended scopes 
(beyond the default boundary) when the cohesion 
marks correspond to lexical items which are related 
(synonyms, hyponyms or hypernyms) or to com-
plex anaphora structures (nominal anaphora; hypo-
nyms and hypernyms can be considered as a spe-
cial case of nominal anaphora). Resolving these 
rarer complex cases would require ?deep? domain 
knowledge which is difficult to implement using 
state-of-art techniques.  
6 Conclusion 
We have presented in this paper a system capable 
of performing automatic segmentation of clinical 
practice guidelines. Our aim was to automatically 
fill an XML DTD from textual input. The system is 
able to process complex discourse structures and to 
compute the scope of conditional segments span-
ning several propositions or sentences. We show 
that inter-annotator agreement is high for this task 
and that the system performs well compared to 
previous systems. Moreover, our system is the first 
one capable of resolving the scope of conditions 
over several recommendations.  
As we have seen, discourse processing is diffi-
cult but fundamental for intelligent information 
access. We plan to apply our model to other lan-
guages and other kinds of texts in the future. The 
task requires at least adapting the linguistic com-
ponents of our system (mainly the pre-processing 
stage). More generally, the portability of discourse-
based systems across languages is a challenging 
area for the future. 
References 
R.C. Brownson, E.A. Baker, T.L. Leet, K.N. Gillespie. 
2003. Evidence-based public health. Oxford Univer-
sity Press. Oxford, UK. 
M. Charolles. 2005. ?Framing adverbials and their role 
in discourse cohesion: from connexion to forward 
labeling?. Papers of the Symposium on the 
Exploration and Modelling of Meaning (Sem?05), 
Biarritz. France.  
R. Englemore and T. Morgan. 1988. Blackboard Sys-
tems. Addison-Wesley, USA.  
G. Georg and M.-C. Jaulent. 2005. ?An Environment for 
Document Engineering of Clinical Guidelines?. Pro-
ceedings of the American Medical Informatics Asso-
ciation. Washington DC. USA. pp. 276?280. 
M.A.K. Halliday and R. Hasan. 1976. Cohesion in Eng-
lish. Longman. Harlow, UK.  
M.A.K. Halliday and C. Matthiessen. 2004. Introduction 
to functional grammar (3rd ed.). Arnold. London, UK.  
G. Kolata. 2004. ?Program Coaxes Hospitals to See 
Treatments Under Their Noses?. The New York 
Times. December 25, 2004. 
119
M. Peleg, A. Boxwala, O. Ogunyemi, Q. Zeng, S. Tu, R. 
Lacson, E. Bernstam, N. Ash, P. Mork, L. Ohno-
Machado, E. Shortliffe and R. Greenes. 2000. 
?GLIF3: The Evolution of a Guideline Representa-
tion Format?. In Proceedings of the American Medi-
cal Informatics Association. pp. 645?649. 
M-P. P?ry-Woodley. 1998. ?Signalling in written text: a 
corpus-based approach?. In M. Stede, L. Wanner & 
E. Hovy (Eds.), Proceeding of te Coling ?98 Work-
shop on Discourse Relations and Discourse Markers, 
pp. 79?85 
B. S?roussi, J.  Bouaud, H. Dr?au., H. Falcoff., C. Riou., 
M. Joubert., G. Simon, A. Venot. 2001. ?ASTI :  A 
Guideline-based drug-ordering system for primary 
care?. In Proceedings MedInfo. pp.  528?532. 
R.N. Shiffman, B.T. Karras, A. Agrawal, R. Chen, L. 
Marenco, S. Nath. 2000. ?GEM: A proposal for a 
more comprehensive guideline document model us-
ing XML?. Journal of the American Medical Infor-
matics Assoc.  n?7(5). pp. 488?498. 
R.N. Shiffman, M. George, M.G. Essaihi and E. Thorn-
quist. 2004. ?Bridging the guideline implementation 
gap: a systematic, document-centered approach to 
guideline implementation?. In Journal of the Ameri-
can Medical Informatics Assoc. n?11(5). pp. 418?
426. 
S. Tu and M. Musen. 2001. ?Modeling data and knowl-
edge in the EON Guideline Architecture?. In 
Medinfo. n?10(1). pp. 280?284. 
 
Annex A. Screenshots of the system 
 
 
Figure A1. A practice guideline once analyzed by the system (Traitement m?dicamenteux du diab?te de 
type 2, AFSSAPS-HAS, nov. 2006) 
 
            
Figures A2 and A3. The original text, an the XML GEM template instanciated from the text 
120
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 418?421,
Prague, June 2007. c?2007 Association for Computational Linguistics
UP13: Knowledge-poor Methods (Sometimes) Perform Poorly 
Thierry Poibeau 
Laboratoire d?Informatique de Paris-Nord 
CNRS UMR 7030 et universit? Paris 13 
99, avenue J.-B. Cl?ment F-93430 Villetaneuse 
thierry.poibeau@lipn.univ-paris13.fr 
 
 
Abstract 
This short paper presents a system developed at 
the Universit? Paris 13 for the Semeval 2007 
Metonymy Resolution Task (task #08, location 
name track; see Markert and Nissim, 2007). 
The system makes use of plain word forms 
only. In this paper, we evaluate the accuracy of 
this minimalist approach, compare it to a more 
complex one which uses both syntactic and 
semantic features, and discuss its usefulness for 
metonymy resolution in general. 
1 Introduction 
This short paper presents the system developed at 
the Universit? Paris 13 for the Metonymy 
resolution task, during Semeval 2007 (Markert and 
Nissim, 2007). Two sub-tasks were proposed, 
concerning 1) country names and 2) company 
names. We only participated in the first task 
(country names). We developed a simple approach 
which we present and thoroughly evaluate in this 
paper. We discuss the relevance of this approach 
and compare it to more complex ones.  
2 Motivation 
We participated in the metonymy task with a very 
basic system. The idea was to investigate the 
efficiency of a minimalist (though, not Chomskian) 
system. This system tags entities on the basis of 
discriminative (plain) word forms occurring in a 
given window only. Our aim was to find out which 
word forms are discriminative enough to be 
considered as parameters. 
In the past, we developed a system for 
metonymy resolution for French, evaluated in the 
framework of the ESTER evaluation (Gravier, 
2004). This system, described in Poibeau (2006), 
uses various kinds of information, among others: 
plain word forms, part-of-speech tags, and 
syntactic and semantic tags (conceptual word 
classes).  
The usefulness of complex linguistic features 
(especially syntactic and semantic tags) is 
questionable: they may be hard to compute, error-
prone and their contribution is not clear. We 
therefore developed a new version of the system 
mainly based on 1) a distributional analysis (on 
surface word forms) along with 2) a filtering 
process. The latter restricted metonymic readings 
to country and capital names (as opposed to other 
location names), since they include a vast majority 
of the metonymic readings (this proved to be 
efficient but is of course a harsh pragmatic over-
simplification without real linguistic basis). We 
nevertheless obtained a highly versatile system, 
performing reasonably well, compared to our 
previous, much more complex implementation 
(F-score was .58 instead of .63; we computed 
F-score with ?=1).  
In the framework of the Semeval evaluation, the 
filtering process is irrelevant since only country 
names are considered as entities. However, we 
thought that it would be interesting to develop a 
very basic system, to evaluate the performance one 
can obtain using plain word forms only.  
3 A (too) Lazy Approach 
We chose not to use any part-of-speech tagger or 
syntactic or semantic analyzer; we did not use any 
external knowledge or any other annotated corpus 
than the one provided for the training phase. Since 
no NLP tool was used, we had to duplicate most of 
the words in order to get the singular and the plural 
form. Our system is thus very simple compared to 
418
the state-of-art in this domain (e.g. Nissim and 
Markert, 2003). 
We used discriminative plain words only. These 
are gathered as follows: all the words in a given 
window (here we use a 7 word window, before and 
after the target entity since it gave the best results 
on the training data) are extracted and associated 
with two classes (literal vs. non literal). We thus 
consider the most discriminative words, i.e. words 
that appear frequently in some contexts but not in 
others (literal vs. non-literal readings). 
Discriminative words are elements that are 
abnormally frequent or rare in one corpus 
compared to another one.  
Characteristic features are selected based on 
their probabilities. Probability levels measure the 
significance of the differences between the relative 
frequency of an expression or a feature within a 
group (or a category) with its global relative 
frequency calculated over the entire corpus (Lafon, 
1980). They are calculated under the hypothesis of 
a random distribution of the forms. The smaller the 
probability levels, the more characteristic the 
corresponding forms (Lebart and Salem, 1997). 
We thus obtained 4 lists of discriminative words 
(literal vs. non-literal ? before vs. after the target 
entity). As the result, some semantic families 
emerged, especially for words appearing before 
literal readings: lists of prepositions (in, at, 
within?) and geographical items (east, west, 
western?). Some lists were manually completed, 
when a ?natural? series appeared to be incomplete 
(for example, if we got east, west, north, we 
completed the word series with south).  
3.1 Reducing the Size of the Search Space 
The approach described so far may seems a bit 
simplistic (and, indeed, it is!), but nevertheless it 
yielded highly discriminative features. For 
example, if we only tag country names 
immediately preceded by the preposition in as 
?literal?, we obtain the results presented in table 1 
(in the following tables, precision is the most 
relevant issue; coverage gives an idea of the 
percentage of tagged entities by the considered 
feature, compared to the total number of entities to 
be tagged). Figure 1 shows that detecting the 
preposition in in front of a location name 
discriminates almost perfectly 23% of the literal 
readings. 
 Training Test 
Precision 1 .98 
Coverage .23 .23 
Table 1. Results for the pattern in + LOC 
(result tag = literal) 
A simple discriminative analysis of the training 
corpus produces the following list of prepositions 
and geographical discriminative features: "at", 
"within", "in", "into", "from", "coast", 
"land", "area", "southern", "south", "east", 
"north", "west", "western", "eastern", etc 1 . 
Table 2 presents the results obtained from this list 
of words (occurring in a 7 word window, on the 
left of the target word): 
 
 Training Test 
Precision .91 .88 
Coverage .60 .55 
Table 2. Results for the pattern <at+within+?> 
+ LOC (note that table 1 is contained in table 2) 
Another typical feature was the use of the entity in 
a genitive construction (e.g. in Iran's official 
commitment, Iran is considered as a literal 
reading). The presence of 's on the right side of the 
target entity is highly discriminative (table 3): 
 
 Training Test 
Precision .87 .89 
Coverage .15 .17 
Table 3. Results for the pattern LOC?s  
(result tag = literal) 
This strategy may seem strange, since the task is to 
find metonymic readings rather than literal ones 
(the baseline is to tag all the target entities as 
literal). However, it is useful in reducing the size 
of the search space by approximately 50%. This 
means that more than 70% of the entities with a 
literal meaning can be tagged with a confidence 
around 90% using this technique, thus reducing the 
number of problematic cases. The resulting file is 
relatively balanced: it contains about 50-60% of 
literal meaning and 40-50% of metaphorical 
meaning (instead of a classical ratio 80% vs. 20%).  
                                                 
1 The list also contains nouns and verbs like: "enter", 
"entered", "fly", "flown", "went", "go", "come", 
"land", "country", "mountain"? 
419
3.2 Looking for Metonymy, Desperately ? 
We used the same strategy for metonymic 
readings. We have observed in the past that word 
forms are much more efficient for literal readings 
than for metonymic readings. However, the fact 
that the location name is followed by a verb like 
"has", "should", "was", "would", "will" 
seemed to be discriminative on the training corpus. 
Unfortunately, this feature did not work well on 
the test corpus (table 4). 
 
 Training Test 
Precision .6 .3 
Coverage .1 .04 
Table 4. Results for the pattern LOC + <was, 
should?> (result tag = metonymic) 
This simply means that a syntactic analysis would 
be useful to discriminate between the sentences 
where the target entity is the subject of the 
following verb (in this context, the entity is most of 
the time used with a metaphoric reading; to go 
further, one needs to filter the verb according to 
semantic classes).  
Another point that was clear from the task 
guidelines was that sport?s teams correspond to 
metonymic readings.  The list of characteristic 
words for this class, obtained from the training 
corpus was the following: "player", "team", 
"defender", "plays", "role", "score", 
"scores", "scored", "win", "won", "cup", "v"2, 
"against", "penalty", "goal", "goals", 
"champion", "champions", etc. But, bad luck! 
This list did not work well on the test corpus either: 
 
 Training Test 
Precision .64 .32 
Coverage .13 .05 
Table 5. Results for the pattern LOC + 
<player, team?>  (result tag = metonymic) 
Table 5 shows that coverage as well as precision 
are very low.  
Yet another category included words related to 
the political role of countries, which entails a 
metonymic reading: "role", "institution", 
"preoccupation", "attitude", "ally", 
"allies", "institutions", "initiative", 
                                                 
2 v for versus, especially in sports: Arsenal-MU  3 v 2. 
"according", "authority"? All these categories 
had low coverage on the test corpus. This is not so 
surprising and is related to our knowledge-poor 
strategy: the training corpus is relatively small and 
it was foreseeable that we would miss most of the 
relevant contexts. However, we wanted to maintain 
precision above .5 (i.e. relevant contexts should 
remain relevant), but failed in this, as one can see 
from the overall results. 
4 Overall Evaluation 
We mainly discuss here the results of the coarse 
evaluation, where only literal vs non-literal 
meanings were targeted. We did not develop any 
specific strategy for the other tracks (medium and 
fine) since there were too few examples in the 
training data. We just transferred non-literal 
readings to the most probable class according to 
the training corpus (metonymic for medium, 
place-for-people for fine). However, the 
performance of our system (i.e. accuracy) is 
relatively stable between these three tracks, since 
the distribution of examples between the different 
classes is very unequally distributed.  
Before giving the results, recall that our purpose 
was to investigate a knowledge-poor strategy, in 
order to establish how far one can get using only 
surface indicators. Thus, unsurprisingly, our results 
for the training corpus were already lower than 
those obtained using a more sophisticated system 
(Nissim and Markert, 2003). They are however a 
good indicator of performance when one uses only 
surface features.  
The accuracy on the training corpus was .815. 
Precision and recall are presented in the table 6.  
 
 Literal Non-lit. 
Precision .88 .54 
Recall .88 .57 
P&R .88 .55 
Table 6. Overall results on the training corpus 
Accuracy on the test corpus is .754 only. Table 7 
shows the results obtained for the different kinds of 
location names. The result is obvious: there is a 
significant drop in both recall and precision, 
compared to the results on the training corpus. 
 
 
420
 Literal Non lit. 
Precision .83 .38 
Recall .86 .31 
P&R .84 .34 
Table 7. Overall results on the test corpus 
5 Discussion 
Metonymy is a complex linguistic phenomenon 
and it is thus no surprise that such a basic system 
performed badly, even if the drop in precision 
between training and test set was disappointing. 
The main conclusion of this approach is that 
surface forms can be used to reduce the size of the 
search space with a relatively good accuracy. A 
large part of the literal readings can be tagged 
using surface forms only. For the remaining cases, 
the use of more sophisticated linguistic information 
(both syntactic and semantic) is necessary.  
During this work, we discovered some 
problematic target entities whose annotation is 
challenging. For instance, we tagged the following 
example as metonymic (because of the keywords 
?role? and ?above?), whereas it is tagged as 
literal in the gold standard: 
This two-track approach was seen (?) as 
reflecting continued manoeuvring over 
the role of the <annot> <location 
reading="literal"> United States 
</location> </annot> in the alliance, ?  
See also the following example (tagged by our 
system as metonymic because of the keyword 
?relations?, but assumed to be literal in the gold 
standard): 
Relations with China and <annot> 
<location reading="literal"> Singapore 
</location></annot> ? 
On the other hand, the following example was 
tagged as literal by our system (due to the 
preposition in) instead of metonymic.  
After their European Championship 
victory (?), Holland will be expected 
to do well in <annot> <location 
reading="metonymic" metotype="place-
for-event"> Italy </location></annot>.  
If Italy is assumed to refer to the World Cup 
occurring in Italy, we think that the literal reading 
is not completely irrelevant (a paraphrase could be: 
??to do well during their stay in Italy? which is 
clearly literal).  
Metonymy is a form of figurative speech ?in 
which one expression is used to refer to the 
referent of a related one? (Markert and Nissim, 
2007). The phenomenon corresponds to a semantic 
shift in interpretation (?a profile shift?) that 
appears to be a function of salience (Cruse and 
Croft, 2004). We assume that this semantic shift 
does not completely erase the original referent: it 
rather puts the focus on a specific feature of the 
content (?the profile?) of the standard referent. If 
we adopt this theory, we can explain why it may be 
difficult to tag some examples, since both readings 
may co-exist.  
6 Conclusion 
In this paper, we presented a (minimalist) system 
for metonymy resolution and evaluated its 
usefulness for the task. The system worked well 
for reducing the size of the search space but 
performed badly for the recognition of metonymic 
readings themselves. It should be used in 
combination with more complex features, 
especially syntactic and semantic information.  
References 
A. Cruse and W. Croft. 2004. Meaning in language, an 
introduction to semantics and pragmatics. Oxford 
University Press, Oxford. 
G. Gravier, J.-F. Bonastre, E. Geoffrois, S. Galliano, K. 
Mc Tait and K. Choukri. 2004. The ESTER 
evaluation campaign for the rich transcription of 
French broadcast news?. Proceedings of LREC?04. 
Lisbon, Portugal. pp. 885?888. 
P. Lafon. 1980. Sur la variabilit? de la fr?quence des 
formes dans un corpus. Mots. 1. pp. 127?165. 
L. Lebart and A. Salem. 1997. Exploring Textual Data. 
Springer. Berlin. 
K. Markert and M. Nissim. 2007. Task08: Metonymy 
Resolution at Semeval 2007. Proceedings of Semeval 
2007. Prague, Czech Rep. 
M. Nissim  and K. Markert. 2003. Syntactic Features 
and Word Similarity for supervised Metonymy 
Resolution.  Proceedings of ACL?03. Sapporo, Japan. 
pp. 56?63. 
T. Poibeau. 2006. Dealing with Metonymic Readings of 
Named Entities. Proceedings of COGSCI?06. 
Vancouver, Canada. pp. 1962?1968. 
421
155
156
157
158
155
156
157
158
Inferring knowledge from a large semantic network 
 
Dominique Dutoit Thierry Poibeau 
Memodata et CRISCO 
17, rue Dumont d?Urville 
F-14000 Caen 
Thales and LIPN 
Domaine de Corbeville 
F-91404 Orsay 
memodata@wanadoo.fr thierry.poibeau@thalesgroup.com 
 
 
Abstract  
In this paper, we present a rich semantic 
network based on a differential analysis. 
We then detail implemented measures 
that take into account common and 
differential features between words. In a 
last section, we describe some industrial 
applications. 
1 Introduction: textual and 
differential semantics 
In textual analysis, each lexical item from a 
text is broken down in a list of semantic 
features. Features are intended to differentiate 
one word from another: a naive example would 
be a feature back that could express the 
difference between a chair and a stool. Of 
course, most of the time, features are not so 
easy to define. Some feature typologies have 
been provided, but there are still much 
discussions about the nature of a feature in a 
text. Most of the studies concerning differential 
semantics are based on a human approach to 
texts (this can lead to different problems, see 
below). Textual Semantics, also called 
differential semantics, is revisiting the 
concepts of continental structuralism like 
decomponential semantics (Cavazza, 1998). 
 The problem is then to have a lexical 
formalism that allows, for a lexical item, a 
simple description and some other features 
which could be dynamically inferred from the 
text. For example, the dictionary should 
mention that a ?door? is an aperture, but it is 
more questionable to mention in the dictionary 
that ?one can walk through a door?. However, 
it can be an important point for the interpretation 
of a sentence in context.  
 That is the reason why Pustejovsky 
introduced in the nineties the notion of 
?generative lexicon? (Pustejovsky, 1991) 
(Pustejovsky, 1995). His analysis has to deal 
with the notion of context: he proposes to 
associate to a word a core semantic description 
(the fact that a ?door? is an ?aperture?) and to 
add some additional features, which can be 
activated in context (?walk-through? is the telic 
role of a ?door?). However, Pustejovsky does 
not take into account important notions such as 
lexical chains and text coherence. He proposes 
an abstract model distant from real texts. 
 Semantic features can be used to check out 
text coherence through the notion of ?isotopy?. 
This notion is ?the recurrence within a given text 
section (regardless of sentence boundaries) of 
the same semantic feature through different 
words? (Cavazza, 1998). The recurrences of 
these features throughout a text allows to extract 
the topic of interest and some other points which 
are marginally tackled in the text. It provides 
interesting ways to glance at the text without a 
full reading of it; it also helps the interpretation. 
 In this paper, we present a rich semantic 
network based on a differential analysis. We 
then detail implemented measures that take into 
account common and differential features 
between words. In a last section, we describe 
some industrial applications.  
  
2 The semantic network 
The semantic network used in this experiment is 
a multilingual network providing information for 
5 European languages. We quickly describe the 
network and then give some detail about its 
overall structure. 
2.1 Overall organisation 
The semantic network we use is called The 
Integral Dictionary. This database is basically 
structured as a merging of three semantic 
models available for five languages. The 
maximal coverage is given for the French 
language, with 185.000 word-meanings 
encoded in the database. English Language 
appears like the second language in term of 
coverage with 79.000 word-meanings. Three 
additional languages (Spanish, Italian and 
German) are present for about 39.500 senses.  
 These smallest dictionaries, with universal 
identifiers to ensure the translation, define the 
Basic Multilingual Dictionary available from 
the ELRA. Grefenstette (1998) has done a 
corpus coverage evaluation for the Basic 
Multilingual Dictionary. The newspapers 
corpora defined by the US-government-
sponsored Text Retrieval Conference (TREC, 
2000) has been used as a test corpus. The result 
was that the chance of pulling a random noun 
out of the different corpus was on average 
92%. This statistic is given for the Basic 
Multilingual Dictionary and, of course, the 
French Integral Dictionary reaches the highest 
coverage.  
 This semantic network is richer than 
Wordnet (Bagga et al, 1997) (Fellbaum, 
1998): it has got a larger number of links and is 
based on a componential lexical analysis. 
Because words are highly interconnected, the 
semantic network is easily tunable for a new 
corpus (see section 2.3). 
2.2 Measure of distance between words 
We propose an original way to measure the 
semantic proximity between two words. This 
measure takes into account the similarity 
between words (their common features) but 
also their differences.  
 Let?s take the following example:  
 
 
 
 
 
 
\Universe 
 
\Person    \Sell    \Flower 
 
 
sell  seller florist    flower 
Figure 1: An example of semantic graph 
The comparison between two words is based on 
the structure of the graph: the algorithm 
calculates a score taking into account the 
common ancestors but also the different ones. 
Let?s take the example of seller and florist. They 
have two common ancestors: \Person and 
\Sell, but also one differential element: the 
concept \Flower that dominates florist but 
not seller. 
 The notion of ?nearest common ancestor? is 
classical in graph theory. We extend this notion 
to distinguish between ?symmetric nearest 
common ancestor? (direct common ancestor for 
both nodes) and ?asymmetric nearest common 
ancestor? (common ancestor, indirect at least for 
one node). 
Definition: Distance between two nodes in a 
graph 
We note d the distance between two nodes A 
and B in a graph. This distance is equivalent to 
the number of intervals between two nodes A 
and B. We have d(A, B) = d(B,A). 
Example: We have d(sell, \Sell) = 1 and 
d(sell, \Universe) = 2, from Figure 1. Note 
that d(sell, \Sell) = d(\Sell, sell) = 1.  
 
Given: 
h(f) = the set of ancestors of f . 
c(f) = the set of arcs between a daughter f and 
the graph?s root. 
 
We have: 
h(seller) = {\Sell, \Person, 
\Universe} 
c(seller) = { (seller, \Sell), 
(seller, \Person), (\Sell, 
\Universe), (\Person, \Universe)} 
 
etc. 
 Definition: Nearest common ancestors 
(NCA) 
The nearest common ancestors between two 
words A and B are the set of nodes that are 
daughters of c(A) ? c(B) and that are not 
ancestors in c(A) ? c(B).  
Example: From Figure 1, we have: 
c(seller) ? c(florist) = { (\Sell, 
\Universe), (\Person, \Universe) } 
DaughterNodes(c(seller) ? 
c(florist)) = { \Sell, \Person } 
AncestorNodes (c(seller) ? 
c(florist)) = { \Universe } 
 
The NCA is equal to the set of nodes in the set 
DaughterNodes (c(seller) ? c(florist)) 
but not in AncestorNodes (c(seller) ? 
c(florist)). Given that no element from 
AncestorNodes (c(seller) ? c(florist)) appears 
in DaughterNodes(c(seller) ? c(florist)), we 
have:  
NCA(seller, florist) = { \Sell, 
\Person } 
We then propose a measure to calculate the 
similarity between two words. The measure is 
called activation and only takes into account 
the common features between two nodes in the 
graph. An equal weight is attributed to each 
NCA. This weight corresponds to the minimal 
distance between the NCA and each of the two 
concerned nodes. 
Definition: activation (d
?
) 
The activation measure d  is equal to the mean 
of the weight of each NCA calculated from A 
and  B : 
d
?
 (A, B) = ?
=
+
n
1i
ii ))NCA,B(d)NCA,A(d(
n
1
 
The activation measure has the following 
properties:  
? d
?
 
(A, A) = 0, because A is the unique 
NCA of A 
?
 A.  
? d
?
 
(A, B) = d
?
 
(B, A)
  
(symmetry) 
? d
?
 
(A, B) + d
?
 (B, C)
 
>= d
?
 
(A, C)  
(euclidianity) 
Example : According to Figure 1, we have 
NCA(seller, florist) = { \Sell, 
\Person}. Consequently, if we assign a weight 
equal to 1 to each link, we have: 
 
d
?
(seller, florist) = (d(seller, 
\Sell)+d(\Sell, florist) +   
d(seller, \Person)+ d(\Person, 
florist)) / 2 
d
?
 
(seller, florist)
 
 = (1 + 1 + 1 + 
1) / 2 
d
?
 
(seller, florist)
 
 = 2 
 
We can verify that: 
d
?
 
(florist, seller)
 
 = d
?
 
(seller, 
florist)
 
 = 2 
The set of NCA takes into account the common 
features between two nodes A et B. We then 
need another measure to take into account their 
differences. To be able to do that, we must 
define the notion of asymmetric nearest common 
ancestor.  
 
Definition: Asymmetric nearest common 
ancestor (ANCA) 
The asymmetric nearest common ancestors from 
a node A to a node B is contained into the set of 
ancestors of c(B) ? c(A) which have a direct 
node belonging to h(A) but not to h(B).  
 
Example: According to Figure 1, we have: 
AncestorNodesNotNCA (c (seller) ? 
c(florist)) = { \Universe } 
The concept \Universe does not have any 
daughter that is a member of h(seller) but not 
of h(florist). As a consequence, we have: 
ANCA(seller, florist) = ? 
On the other hand, the concept \Universe has a 
daughter \Flower that belongs to h(florist) 
but not to h(seller). As a consequence, we 
have: 
ANCA(florist, seller) = {\Universe} 
It is now possible to measure the distance 
between two words from their differences. A 
weight is allocated to each link going from node 
Ni, asymmetric nearest common ancestor, to A 
and B. The weight is equal to the length of the 
minimal length of the path going from A to Ni 
and from B to Ni.  
 
Definition: proximity (d?) 
The proximity measure takes into account the 
common features but also the differences 
between two elements A and B and is defined 
by the following function: 
d?(A,B)= d
?
(A,B)+ 
?
=
+
n
1i
ii ))ANCA,B(d)ANCA,A(d(
n
1
 
Because the set of ANCA from a node A to a 
node B is not the same as the one from a node B 
to a node A, the proximity measure has the 
following properties:  
? d? (A, A) = 0, because ANCA(A, A) = 
?.  
? d
 ?
 
(A, B) ? d?
 
(B, A)
 
 if the set of 
ANCA is not empty (antisymmetry) 
? d
 
?
 
(A, B) + d?
 
(B, C) >= d?
 
(A, C)
 
 (euclidianity) 
The proximity measure is dependent from the 
structure of the network. However, one must 
notice that this measure is a relative one: if the 
semantic network evolves, all the proximity 
measures between nodes are changed but the 
relations between nodes can stay relatively 
stable (note that the graph presented on Figure 
1 is extremely simplified: the real network is 
largely more connected).  
Example: Let?s calculate the semantic 
proximity between seller and florist: d? 
(seller, florist). We will then be able to 
see that the proximity between florist and 
seller does not produce the same result 
(antisymmetry). 
 
Given that ANCA(seller, florist) = ?, the 
second element of the formula based on the set 
of ANCA is equal to 0. We then have: 
 
d? (seller, florist)  =  
d
?
 
(seller, florist) + 0 
d? (seller, florist)  = 2 + 0 
d? (seller, florist)  = 2 
 
ANCA(seller, florist) is the set 
containing the concept \Universe, 
because the concept \Flower is an ancestor of 
florist
 but not of seller. We then have: 
d?(florist, seller) = d
?
 (florist, 
seller) + (d(seller, \Universe) + 
d(\Universe, florist)) / 1 
d?(florist, seller) = 2 + ( 2 + 2 ) / 
1 
d?(florist, seller) = 6 
 
To sum up, we have:  
d
?
 
(florist, seller)
 
 
= 2 
d
?
 
(seller, florist)
 
 = 2 
d?
 
(seller, florist)
 
 
= 2 
d? (florist, seller) = 6 
 
The proximity measure discriminates florist 
from seller, whereas the activation measure is 
symmetric. The componential analysis of the 
semantic network reflects some weak semantic 
differences between words. 
2.3 Link weighting 
All the links in the semantic network are typed 
so that a weight can be allocated to each link, 
given its type. This mechanism allows to very 
precisely adapt the network to the task: one does 
not use the same weighting to perform lexical 
acquisition as to perform word-sense 
disambiguation. This characteristic makes the 
network highly adaptive and appropriate to 
explore some kind of lexical tuning. 
3 Experiment and evaluation 
through an information filtering 
task 
In this section we propose to evaluate the 
semantic network and the measures that have 
been implemented through a set of NLP 
applications related to information filtering. To 
help the end-user focus on relevant information 
in texts, it is necessary to provide filtering tools. 
The idea is that the end-user defines a ?profile? 
describing his research interests (van Rijsbergen, 
1979) (Voorhees, 1999). 
 A profile is a set of words, describing the 
user?s domain of interest. Unfortunately the 
measures we have described are only concerned 
with simple words, not with set of words.  
 We first need to slightly modify the 
activation measure, so that it accepts to compare 
two sets of words, and not only two simple 
words1. We propose to aggregate the set of 
nodes in the graphs corresponding to the set of 
words in the profile. This node has the 
following properties: 

n
1i
)m(h)M(h i
=
=  

n
1i
)m(c)M(c i
=
=  
where h(M) is the set of ancestors of M and 
c(M)
 the set of links between M and the root of 
the graph. It is then possible to compare two 
set of words, and not only two simple words. 
 In the framework of an Information 
Extraction task, we want to filter texts to focus 
on sentences that are of possible interest for the 
extraction process (sentences that could allow 
to fill a given slot). We then need a very 
precise filtering process performing at the 
sentence level2. We used the activation 
measure for the filtering task. A sentence is 
kept if the activation score between the 
filtering profile and the sentence is above a 
certain threshold (empirically defined by the 
end-user). A filtering profile is a set of words 
in relation with the domain or the slot to be fill, 
defined by the end-user.  
 We made a set of experiments on a French 
financial newswire corpus. The topic was the 
same as in the MUC-6 conference (1995): 
companies purchasing other companies. We 
made the experiment on a set of 100 news 
stories (no training phase).  
 The filtering profile was composed of the 
following words: rachat, cession, 
enterprise (buy, purchase, company). 
The corpus has been manually processed to 
identify relevant sentences (the reference 
corpus). We then compare the result of the 
filtering task with the reference corpus.  
                                                     
1
 This measure allows to compare two set of words, 
or two sentences. For a sentence, it is first necessary 
to delete empty words, to obtain a set of full words 
2
 This is original since most of the systems so far 
are concern with texts filtering, not sentence 
filtering.   
 In the different experiments we made, we 
modified different parameters such as the 
filtering threshold (the percentage of sentences 
to be kept). We obtained the following results: 
  10% 20% 30% 40% 50% 
Precision .72 .54 .41 .33 .28 
Recall .43 .64 .75 .81 .85 
We also tried to normalize the corpus, that is to 
say to replace entities by their type, to improve 
the filtering process. We used a state-of-the-art 
named entity recogniser that was part of a larger 
toolbox for named entity recognition. 
 10% 20% 30% 40% 50% 
Precision .75 .56 .43 .34 .29 
Recall .49 .71 .82 .89 .94 
We notice that we obtain, from 10% of the 
corpus, a 75% precision ratio (3 sentences out of 
4 are relevant) and nearly a 50% recall ratio. The 
main interest of this process is to help the end-
user directly focus on relevant pieces of text. 
This strategy is very close from the EXDISCO 
system developed by R. Yangarber at NYU 
(2000), even if the algorithms we use are 
different.  
4 Application services overview 
In this section, we detail some of the 
applications developed from the semantic 
network described above.  All of these 
applications are available through java API. 
They are part of the applicative part of the 
network called the Semiograph3. Most of the 
examples will be given in French. 
4.1 Query expansion 
This application gives a help to the users who 
query the web through a search engine. In this 
framework, the Semiograph has to determinate 
                                                     
3
 Part of Speech tagging, syntactic analysis for 
French and Word Sense Disambiguation are also 
APIs of the Semiograph. 
the sense of the query and generate (or 
suggest) an expansion of the query in 
accordance to the semantic and syntactic 
properties of the source. 
The Semiograph links independent 
mechanisms of expansion defined by the user. 
Eight mechanisms are available : 
? Alias: to get the graphics variant 
? Synonyms: to get synonyms for a 
meaning 
? Hypernyms: to get hypernyms for a 
meaning 
? Hyponyms: to get hyponyms for a 
meaning 
? Inflected forms : to get the inflected for a 
meaning 
? Derived forms: to get correct lexical 
functions in accordance or not with the 
syntactical proposition 
? Geographical belonging: to get toponyms  
? Translation (language parameter) : to get a 
translation of the query. 
 
 
Figure 2: Query expansion 
4.2 Word sense disambiguation and 
Term spotting 
 Lexical semantics provides an original 
approach for the term spotting task.  Generally 
speaking, the main topics addressed by a 
document are expressed by ambiguous words. 
Most of the time, these words can be 
disambiguated from the context. If a document 
treats of billiards, the context of billiards is 
necessarily saturated by terms of larger topics 
like games, competition, dexterity... and terms in 
dependence with billiard like ball, cue, cannon...  
 Using this property, lexical topics are found 
by measuring the semantic proximity of each 
plain word of a text with the text itself. Terms 
that have the minimal semantic proximity are the 
best descriptors.  
 Note that this property may be used to verify 
the relevance of keywords manually given by a 
writer. An application may be the  struggle to 
the spamming of search engine. To give an 
example of result of lexical summary, the 
algorithm applied to this paper provides in the 
20 best words the terms : lexicon, dictionary, 
semantic network, semantics, measures and 
disambiguation. All these terms are highly 
relevant. 
4.3 Emails sorting and answering 
In this application, we have to classify a flow of 
documents according to a set of existing 
profiles. Most systems execute this task after a 
learning phase. A learning phase causes a 
problem because it needs a costly preliminary 
manual tagging of documents. It is then 
attractive to see if a complex lexicon could 
perform an accurate classification without any 
learning phase. 
 In our experiments the end-user must have to 
define profiles that correspond to his domains of 
interest. The formalism is very light: firstly, we 
define an identifier for each profile; secondly we 
define a definition of this profile (a set of 
relevant terms according to the domain). On the 
following examples, identifiers are given 
between parentheses and definitions are given 
after. 
 
[guerre du Kosovo] guerre du Kosovo 
[tabac et jeunesse] tabac et jeunesse 
[alcoolisme et Bretagne] alcoolisme et 
Bretagne 
[investissement immobilier en Ile-de-
France] achat, vente et march? 
immobilier en ?le-de-France 
 
The definitions may be given in English with the 
exactly same result. The following text : 
Les loyers stagnent ? Paris mais la baisse de la 
TVA sur les d?penses de r?paration de l?habitat 
devrait soutenir le march? de l?ancien 
gives in term of semantic proximity: 
 
[guerre du Kosovo]  135 
[tabac et jeunesse]  140 
[alcoolisme et Bretagne]  129 
[investissement immobilier en 
Ile-de-France]  9 
 
We observe that differences between the 
mailboxes are very marked (the best score is 
the lowest one). Note that this approach may 
be used to help the classifying of web sites that 
is today entirely manually carry out. 
5 Conclusion 
In this paper, we have shown an efficient 
algorithm to semi-automatically acquire 
knowledge from a semantic network and a 
corpus. A set of basic services are also 
available through java APIs developed above 
the semantic network. We have shown that this 
set of elements offers a versatile toolbox for a 
large variety of NLP applications. 
6 References 
Bagga A., Chai J.Y. et Biermann A. The Role of 
WORDNET in the Creation of a Trainable Message 
Understanding System. In Proceedings of the 14th 
National Conference on Artificial Intelligence 
and the Ninth Conference on the Innovative 
Applications of Artificial Intelligence 
(AAAI/IAAI?97), Rhode Island, 1997, pp. 941?
948.  
Basili R., Catizone R., Pazienza M.T., 
Stevenson M., Velardi P., Vindigni M. and  
Wilks Y. (1998) An empirical approach to 
Lexical Tuning. Workshop on Adapting lexical 
and corpus resources to sublanguages and 
applications, LREC (Grenada). 
Cavazza M. (1998) Textual semantics and corpus-
specific lexicons. Workshop on Adapting lexical 
and corpus resources to sublanguages and 
applications, LREC (Grenada). 
Fellbaum C. (1998) WordNet : An Electronic 
Lexical Database, edited by Fellbaum, M.I.T. 
press. 
Grefenstette G. (1998) Evaluating the adequancy of a 
multilingual transfer dictionary for the Cross 
Language Information Retrieval, LREC 1998. 
MUC-6 (1995) Proceedings Sixth Message 
Understanding Conference (DARPA), Morgan 
Kaufmann Publishers, San Francisco. 
Pustejovsky J. (1991) The generative lexicon.  
Computational Linguistics, 17(4). 
Pustejovsky J. (1995) The generative lexicon, MIT 
Press, Cambridge. 
TREC (2000)  The Ninth Text REtrieval Conference 
(TREC 9). Gaithersburg, 2000. 
http://trec.nist.gov/pubs/trec9/t9_pro
ceedings.html. 
van Rijsbergen C.J. (1979) Information Retrieval. 
Butterworths, Londres. 
Voorhees, E.M. (1999) Natural language processing 
and information retrieval. In M.T. PAZIENZA (?d.), 
Information extraction, toward scalable, adaptable 
systems, Springer Verlag (Lecture Notes in 
computer Science), Heidelberg, pp. 32?48. 
Yangarber R. (2000) Scenario Customization for 
Information Extraction. PhD Thesis, New York 
University. 
 
Generating extraction patterns  
from a large semantic network and an untagged corpus 
 
 
 
Thierry POIBEAU 
Thales and LIPN  
Domaine de Corbeville 
91404 Orsay, France 
Thierry.Poibeau@thalesgroup.com 
Dominique DUTOIT 
Memodata and CRISCO 
17, rue Dumont d?Urville 
Caen, France 
memodata@wanadoo.fr  
 
 
Abstract  
This paper presents a module dedicated 
to the elaboration of linguistic resources 
for a versatile Information Extraction 
system. In order to decrease the time 
spent on the elaboration of resources for 
the IE system and guide the end-user in 
a new domain, we suggest to use a 
machine learning system that helps 
defining new templates and associated 
resources. This knowledge is 
automatically derived from the text 
collection, in interaction with a large 
semantic network. 
1 Introduction 
Information Extraction (IE) is a technology 
dedicated to the extraction of structured 
information from texts. This technique is used 
to highlight relevant sequences in the original 
text or to fill pre-defined templates (Pazienza, 
1997).  
 Even if IE seems to be now a relatively 
mature technology, it suffers from a number of 
yet unsolved problems that limit its 
dissemination through industrial applications. 
Among these limitations, we can consider the 
fact that systems are not really portable from 
one domain to another. Even if the system is 
using some generic components, most of its 
knowledge resources are domain-dependent. 
Moving from one domain to another means re-
developing some resources, which is a boring 
and time-consuming task (for example Riloff 
(1995) mentions a 1500 hours development). 
Several recent works propose to overcome these 
limitations by using annotated corpora as a 
reservoir of knowledge. However, annotated 
corpora are rarely present in companies, and to a 
certain extent solutions based on corpora seem 
to be inappropriate. 
In this paper, we propose an approach based 
on a rich semantic network. We will firstly 
describe this network and a set of original 
measures we have implemented to calculate 
similarities between words. We will then present 
the acquisition process, in which the semantic 
network is projected on the corpus to derive 
extraction patterns. This mechanism can be seen 
as a dynamic lexical tuning of information 
contained in the semantic network. In the last 
section, we propose an evaluation and some 
perspectives.  
2 Related work 
The bases of IE as defined in the introduction 
are exposed in (Pazienza, 1997). IE is known to 
have established a now widely accepted 
linguistic architecture based on cascading 
automata and domain-specific knowledge 
(Appelt et al 1993). However, several studies 
have outlined the problem of the definition of 
the resources, see E. Riloff (1995). 
 To address this problem of portability, a 
recent research effort focused on using machine 
learning throughout the IE process (Muslea, 
1999). A first trend was to directly apply 
machine learning methods to replace IE 
components. For instance, statistical methods 
have been successfully applied to the named-
entity task. Among others, (Bikel et a., 1997) 
learns names by using a variant of hidden 
Markov models.  
 Another research area trying to avoid the 
time-consuming task of elaborating IE 
resources is concerned with the generalization 
of extraction patterns from examples.  (Muslea, 
1999) gives an extensive description of the 
different approaches of that problem. Autoslog 
(Riloff, 1993) was one of the very first systems 
using a simple form of learning to build a 
dictionary of extraction patterns. Successors of 
AutoSlog like Crystal (Soderland et al, 1995) 
mainly use decision trees and relational 
learning techniques to learn set of rules during 
their extraction step. More recently, the SrV 
system (Freitag, 1998) and the Pinocchio 
system (Ciravegna, 2001) use a combination of 
relational and basic statistical methods inspired 
from Na?ve Bayes for IE tasks. 
 These approaches acquire knowledge from 
texts but they must be completed with a 
semantic expansion module. Several authors 
have presented experiments based on Wordnet 
(Bagga et al, 1996).  
 Our approach is original given that it 
consists in an integrated system, using both a 
semantic network and a corpus to acquire 
knowledge and overcome the limitations of 
both knowledge sources. On the one hand, the 
fact that we use a semantic network allows us  
to obtain a broader coverage than if we only 
used a training corpus (contrary Ciravegna? 
system for example). On the other hand, the 
corpus ensures that the acquired resources are 
quite adapted to the task (contrary Bagga? 
system for example). The performance of the 
system will demonstrate this point (see below 
section 5).  
3 The semantic net 
The semantic network used in this experiment 
is a multilingual net providing information for 
five European languages. We quickly describe 
the network and then give some detail about its 
overall structure. 
3.1 Overall description 
 The semantic network we use is called The 
Integral Dictionary. This database is basically 
structured as a merging of three semantic 
models available for five languages. The 
maximal coverage is given for the French 
language, with 185.000 word-meanings encoded 
in the database. English Language appears like 
the second language in term of coverage with 
79.000 word-meanings. Three additional 
languages (Spanish, Italian and German) are 
present for about 39.500 senses.  
 These smallest dictionaries, with universal 
identifiers to ensure the translation, define the 
Basic Multilingual Dictionary available from the 
ELRA. Grefenstette (1998) has done a corpus 
coverage evaluation for the Basic Multilingual 
Dictionary. The newspapers corpora defined by 
the US-government-sponsored Text Retrieval 
Conference (TREC) have been used as a test 
corpus. The result was that the chance of pulling 
a random noun out of the different corpora was 
on average 92%1. This statistic is given for the 
Basic Multilingual Dictionary and, of course, the 
French Integral Dictionary reaches the highest 
coverage.  
3.2 Semantic links 
 The links in the semantic network can 
connect word-senses together, but also classes 
and concepts. Up to now, more than 100 
different kinds of links have been definded. All 
these links are typed so that a weight can be 
allocated to each link, given its type. This 
mechanism allows to very precisely adapt the 
network to the task: one does not use the same 
weighting to perform lexical acquisition as to 
perform word-sense disambiguation. This 
characteristic makes the network highly adaptive 
and appropriate to explore some kind of lexical 
tuning. 
 This network includes original strategies to 
measure the semantic proximity between two 
words. These measures take into account the 
similarity between words (their common 
features) but also their differences. The 
comparison between two words is based on the 
structure of the graph: the algorithm calculates a 
score taken into account the common ancestors 
but also the different ones. 
   
                                                     
1
 This means that for a target English text, one can 
assume that 92% of the tokens will be in the semantic 
net. 
 Figure 1: A table of linguistic constraints 
 
 
We will not detail here the different measures 
that have been implemented to calculate 
similarities between words. Please refer to 
(Dutoit and Poibeau, 2002) for more details. 
4 Acquisition of semantically 
equivalent predicative structures  
For IE applications, defining an appropriate set 
of extraction pattern is crucial. That is why we 
want to validate the proposed measures to 
extend an initial set of extraction patterns. 
4.1 The acquisition process 
The process begins when the end-user provides 
a predicative linguistic structure to the system 
along with a representative corpus. The system 
tries to discover relevant parts of text in the 
corpus based on the presence of plain words 
closely related to the ones of the example 
pattern. A syntactic analysis of the sentence is 
then done to verify that these plain words 
correspond to a predicative structure. The 
method is close to the one of E. Morin et C. 
Jacquemin (1999), who first locate couples of 
relevant terms and then try to apply relevant 
patterns to analyse the nature of their 
relationship. The detail algorithm is described 
below: 
1. The head noun of the example pattern is 
compared with the head noun of the 
candidate pattern using the proximity 
measure. This result of the measure must 
be under a threshold fixed by the end-
user. 
2. The same condition must be filled by the 
?expansion? element (the complement of 
the noun or of the verb of the candidate 
pattern). 
3. The structure must be predicative (either a 
nominal or a verbal predicate, the 
algorithm does not make any difference at 
this level).  
 
The result of this analysis is a table that 
represent predicative structures equivalent to the 
initial example pattern. The process uses the 
corpus and the semantic net as two different 
complementary knowledge sources:  
? The semantic net provides information 
about lexical semantics and relations 
between words 
? The corpus attests possible expressions 
and filter irrelevant ones. 
We performed some evaluation on different 
French corpora, given that the semantic net is 
especially rich for this language. We take the 
expression cession de soci?t?  (company 
transfer) as an initial pattern. The system then 
discovered the following expressions, each of 
them being semantically related to the initial 
pattern : 
reprise des activit?s 
rachat d?activit? 
acqu?rir des magasins 
racheter *c-company* 
cession de *c-company*? 
This result includes some phase with 
*c-company*: the corpus has been previously 
preprocessed so that each named entity is 
replaced by its type. This process normalizes 
the corpus so that the learning process can 
achieve better performance. 
The result must be manually validated. Some 
structures are found even if they are irrelevant, 
due to the activation of irrelevant links. It is the 
case of the expression renoncer ? se porter 
acqu?reur (to give up buying sthg), which is 
not relevant. In this case, there was a spurious 
link between to give up and company in the 
semantic net. 
4.2 Dealing with syntactic variations 
The previous step extract semantically 
related predicative structures from a corpus. 
These structures are found in the corpus in a 
certain linguistic structure, but we want the 
system to be able to find this information even 
if it appears in other kind of linguistic 
sequences. That is the reason why we associate 
some meta-graphs with these linguistic 
structures, so that different transformation can 
be recognized2. This transformation concerns 
the syntactic level, either on the head (H) or on 
the expansions (E) of the linguistic structure.  
The meta-graphs encode transformations 
concerning the following structures: 
? Subject ? verb, 
? Verb ? direct object, 
                                                     
2
 A meta-graph corresponds to a non-lexicalized 
graph. A meta-graph is then a kind of abstract 
grammar (see also the notion of metagrammar in 
the TAG theory (Candito, 1999) 
? Verb ? direct object (especially when 
introduced by the French preposition ? or 
de), 
? Noun ? noun complement. 
These meta-graphs encode the major part of the 
linguistic structures we are concern with in the 
process of IE.  
 The graph on Figure 2 recognizes the 
following sequences (in brackets we underline 
the couple of words previously extracted from 
the corpus): 
Reprise des activit?s charter? (H: 
reprise, E: activit?) 
Reprendre les activit?s charter?  (H: 
reprendre, E: activit?) 
Reprise de l?ensemble des magasins 
suisse? (H: reprise, E: magasin) 
Reprendre l?ensemble des magasins 
suisse? (H: reprendre, E: magasin) 
Racheter les diff?rentes activit?s? 
(H: racheter, E: activit?) 
Rachat des diff?rentes activit?s? (H: 
rachat, E: activit?) 
 
 This kind of graph is not easy to read. It 
includes at the same time some linguistic tags 
and some applicability constraints. For example, 
the first box contains a reference to the @A 
column in the table of identified structures. This 
column contains a set of binary constraints, 
expressed by some signs + or -. The sign + 
means that the identified pattern is of type verb-
direct object: the graph can then be applied to 
deal with passive structures. In other words, the 
graph can only be applied in a sign + appears in 
the @A column of the constraints table. The 
constraints are removed from the instantiated 
graph3. Even if the resulting graph is normally 
not visible (the compilation process directly 
                                                     
3
 In other words, an abstract graph is a non-
lexicalized graph and an instantiated graph is a 
lexicalized graph. 
Figure 2: A meta-graph encoding syntactic variations 
produced a graph in a binary format), we can 
give an equivalent graph. 
This mechanism using constraint tables and 
meta-graph has been implemented in the finite-
state toolbox INTEX (Silberztein, 1993). 26 
meta-graphs have been defined modelling 
linguistic variation for the 4 predicative 
structures defined above. The phenomena 
mainly concern the insertion of modifiers (with 
the noun or the verb), verbal transformations 
(passive) and phrasal structures (relative 
clauses like ?Vivendi, qui a rachet? 
Universal?Vivendi, that bought Universal).  
The compilation of the set of meta-graphs 
produces a graph made of 317 states and 526 
relations. These graphs are relatively abstract 
but the end-user is not intended to directly 
manipulate them. They generate instantiated 
graphs, that is to say graphs in which the 
abstract variables have been replaced linguistic 
information as modeled in the constraint 
tables. 
This method associates a couple of 
elements with a set of transformation that 
covers more examples than the one of the 
training corpus. This generalization process is 
close to the one imagined by Morin and 
Jacquemin (1999) for terminology analysis. 
5 Evaluation 
The evaluation concerned the extraction of 
information from a French financial corpus, 
about companies buying other companies. The 
corpus is made of 300 texts (200 texts for the 
training corpus, 100 texts for the test corpus).  
A system was first manually developed and 
evaluated. We then tried to perform the same 
task with automatically developed resources, 
so that a comparison is possible. At the 
beginning, the end-user must provide a set of 
relevant pattern to the acquisition system. We 
have developed a filtering tool to help the end 
user focus on relevant portion of text. Due to 
lack of place, we will not describe this filtering 
tool, which is very close in its conception to 
the EXDISCO system developed by R. 
Yangarber at NYU.  
First of all, the corpus is normalized. For 
example, all the company names are replaced by 
a variable *c-company* thanks to the named 
entity recognizer. In the semantic network, *c-
company*
 is introduced as a synonym of 
company, so that all the sequences with a proper 
name corresponding to a company could be 
extracted. 
For the slot corresponding to the company 
that is being bought, 6 seed patterns were given 
to semantic expansion module. This module 
acquired from the corpus 25 new validated 
patterns. Each example pattern generated 4.16 
new patterns on average. For example, from the 
pattern rachat de *c-company* we obtain the 
following list:  
reprise de *c-company* 
achat de *c-company* 
acqu?rir *c-company* 
racheter *c-company* 
cession de *c-company* 
 
This set of pattern includes nominal phrases 
(reprise de *c-company*) and verbal phrases 
(racheter *c-company*). The acquisition 
process concerns at the same time, the head and 
the expansion. This technique is very close to 
the co-training algorithm proposed for this kind 
of task by E. Riloff and R. Jones (Riloff et 
Jones, 1999) (Jones et al, 1999). 
 The proposed patterns must be filtered and 
validated by the end-user. We estimate that 
generally 25% of the acquired pattern should be 
rejected. However, this validation process is 
very rapid: a few minutes only were necessary to 
check the 31 proposed patterns and retain 25 of 
them. 
 We then compared these results with the ones 
obtained with the manually elaborated system. 
The evaluation concerned the two slots that 
necessitate a syntactic and semantic analysis: the 
company that is buying another one (slot 1) and 
the company that is being bought (slot 2). These 
slots imply nominal phrases, they can be 
complex and a functional analysis is most of the 
time necessary (is the nominal phrase the subject 
or the direct object of the sentence?). An 
overview of the results is given below (P is for 
precision, R for recall; P&R is the combined 
ratio of P and R): 
 Slot 1 Slot 2 
P: 100 
R: 90 
P: 100 
R: 91.6 
Human 
annotators 
P&R : 94.7 P&R : 95.6 
P: 79.6 
R: 62.6 
P: 93.4 
R: 73 
INTEX +  
manual 
resources  P&R : 70 P&R : 81.9 
P: 65.8 
R: 58.7 
P: 77 
R: 65.3 
INTEX +  
SemTex 
P&R: 62 P&R: 70.7 
 
The system running with automatically 
defined resources is about 10% less efficient 
than the one with manually defined resources. 
The decrease of performance may vary in 
function of the slot (the decrease is less 
important for the slot 1 than for the slot 2). 
Two kind of errors are observed: 
Certain sequences are not found because a 
relation between words is missing in the 
semantic net. This is the case for some 
idiomatic expressions that were not registered 
in the network like tomber dans l?escarcelle de  
which  means to acquire.  
Some sequences are extracted by the 
semantic analysis but do not correspond to a 
transformation registered in the syntactic 
variation management module. For example 
the sequence: 
*c-company* renforce son activit? 
communication ethnique en prenant 
une participation dans *c-company* 4 
is not completely recognized. The pattern  
(prendre <DET>) participation dans *c-
company* correctly identifies the company 
that is being bought. But the pattern *c-
company*
 (prendre <DET>) participation 
cannot apply because the subject is too far 
from the verb.  
 Lastly, we can mention that some patterns 
that were not found manually are identified by 
the automatic procedure. The gain concerning 
development time is very significant (50 h 
were necessary to manually define the 
                                                     
4
 *c-company* reinforces its activity in 
ethnic communication by taking some 
interest in *c-company* 
 
resources, only 10 h with the semi-automatic 
process). 
 Even if the decrease of performance is 
significant (10%), it can be reduced using more 
linguistic knowledge. For example, we know 
that nominalizations are not correctly handled by 
the system at the moment. Some more 
information could be used from the semantic 
network (that also includes morphological and 
syntactic information) to enhance the 
performances of the overall system.  
 Experiments have been made on different 
corpora and on different MUC-like tasks. They 
have all proved the efficiency of the strategy 
described in this paper. Moreover, it is possible 
to adapt the system so that it has a better 
precision, or a better recall, given user needs 
(Poibeau, 2001). For example, people working 
on large genomic textual databases are facing a 
huge amount of redundant information. They 
generally want some very precise information to 
be extracted. On the other hand, human 
operators monitoring critical situation generally 
want to be able to have access to all the 
available information. Our system is versatile 
and could be easily adapted to these different 
contexts. 
6 Conclusion 
In this paper, we have shown an efficient 
algorithm to semi-automatically acquire 
extraction patterns from a semantic network and 
a corpus. Even if the performance decrease 
when the resource are automatically defined, the 
gain in development time is sufficiently 
significant to ensure the usability of the 
approach. 
7 References 
Appelt D.E, Hobbs J., Bear J., Israel D., Kameyana 
M. and Tyson M. (1993) FASTUS: a finite-state 
processor for information extraction from real-
world text. Proceedings of IJCAI?93, Chamb?ry, 
France, pp. 1172?1178.  
Bagga A., Chai J.Y. et Biermann A. The Role of 
WORDNET in the Creation of a Trainable Message 
Understanding System. In Proceedings of the 14th 
National Conference on Artificial Intelligence and 
the Ninth Conference on the Innovative 
Applications of Artificial Intelligence 
(AAAI/IAAI?97), Rhode Island, 1997, pp. 941?
948.  
Bikel D., Miller S., Schwartz R. and Weischedel R. 
(1997) Nymble: a high performance learning 
name-finder. Proceeding of the fifth Conference 
on Applied Language Processing, Washington, 
USA. 
Candito, M.-H. Organisation modulaire et 
param?trable de grammaires ?lectroniques 
lexicalis?es. PhD Thesis, University Paris 7, 
1999. 
Ciravegna F. Adaptive Information Extraction from 
Text by Rule Induction and Generalisation. In 
Proceedings of the 17th International Joint 
Conference on Artificial Intelligence 
(IJCAI?2001), Seattle, 2001, pp. 1251?1256. 
Dutoit D. and Poibeau T. (2002) Inferring 
knowledge from a large semantic network. In 
Proceedings of COLING?2002, Ta?pei. 
Fellbaum C. (1998) WordNet : An Electronic 
Lexical Database, edited by Fellbaum, M.I.T. 
press. 
Freitag D. (1998) Machine learning for Information 
Extraction in Informal Domains, Thesis, Carnegie 
Mellon University, USA. 
Grefenstette G. (1998) Evaluating the adequancy of 
a multilingual transfer dictionary for the Cross 
Language Information Retrieval, LREC 1998. 
Jones R., McCallum A., Nigam K. and Riloff E. 
(1999) Bootstrapping for Text Learning Tasks. 
Proceedings of the IJCAI?99 Workshop on Text 
Mining: Foundations, Techniques and 
Applications, Stockholm, 1999, pp. 52?63.  
Morin E. and Jacquemin C. (1999) Projecting 
corpus-based semantic links on a thesaurus. 
Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics 
(ACL?99), Maryland, 1999, pp. 389?396. 
Muslea I. (1999) Extraction patterns for 
Information Extraction tasks: a survey, AAAI?99 
(available at the following URL: 
http://www.isi.edu/~muslea/ RISE/ML4IE/)  
Pazienza M.T, ed. (1997) Information extraction. 
Springer Verlag  (Lecture Notes in computer 
Science), Heidelberg, Germany. 
Poibeau T. (2001) ? ? Deriving a multi-domain 
information extraction system from a rough 
ontology. Proceeding of the 17th International 
Conference on Artificial Intelligence 
(IJCAI?2001), Seattle, 2001, pp. 1264?1270. 
Riloff E. (1993) Automatically constructing a 
dictionary for formation extraction tasks, 
AAAI?93, Stanford, USA, pp. 811?816. 
Riloff E. (1995) Little Words Can Make a Big 
Difference for Text Classification , Proceedings of 
the SIGIR'95, Seattle, USA, pp. 130?136. 
Riloff E. et Jones R.  (1999) Learning Dictionaries 
for Information Extraction by Multi-Level 
Bootstrapping. Proceedings of the 16th National 
Conference on Artificial Intelligence (AAAI?99), 
Orlando, 1999, pp. 474?479. 
Silberztein M. (1993) Dictionnaires ?lectroniques et 
analyse automatique des textes, Masson, Paris, 
France. 
Soderland S., Fisher D., Aseltine J. and Lenhert W. 
(1995) Crystal: inducing a conceptual dictionary, 
Proceedings of IJCAI?95, Montr?al, Canada, 
pp. 1314?1319. 
Yangarber R. (2000) Scenario Customization for 
Information Extraction. PhD Thesis, New York 
University. 
 
 
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1056?1064,
Beijing, August 2010
Investigating the cross-linguistic potential of VerbNet -style classification
Lin Sun and Anna Korhonen
Computer Laboratory
University of Cambridge
ls418,alk23@cl.cam.ac.uk
Thierry Poibeau
LaTTiCe, UMR8094
CNRS & ENS
thierry.poibeau@ens.fr
Ce?dric Messiant
LIPN, UMR7030
CNRS & U. Paris 13
cedric.messiant@lipn.fr
Abstract
Verb classes which integrate a wide range
of linguistic properties (Levin, 1993) have
proved useful for natural language pro-
cessing (NLP) applications. However,
the real-world use of these classes has
been limited because for most languages,
no resources similar to VerbNet (Kipper-
Schuler, 2005) are available. We apply
a verb clustering approach developed for
English to French ? a language for which
no such experiment has been conducted
yet. Our investigation shows that not only
the general methodology but also the best
performing features are transferable be-
tween the languages, making it possible
to learn useful VerbNet style classes for
French automatically without language-
specific tuning.
1 Introduction
A number of verb classifications have been built to
support natural language processing (NLP) tasks
(Grishman et al, 1994; Miller, 1995; Baker et al,
1998; Palmer et al, 2005; Kipper-Schuler, 2005;
Hovy et al, 2006). These include both syntactic
and semantic classifications, as well as ones which
integrate aspects of both. Classifications which in-
tegrate a wide range of linguistic properties can
be particularly useful for NLP applications suffer-
ing from data sparseness. One such classification
is VerbNet (Kipper-Schuler, 2005). Building on
the taxonomy of Levin (1993), VerbNet groups
verbs (e.g. deliver, post, dispatch) into classes
(e.g. SEND) on the basis of their shared mean-
ing components and syntactic behaviour, identi-
fied in terms of meaning preserving diathesis al-
ternations. Such classes can be identified across
the entire lexicon, and they may also apply across
languages, since their meaning components are
said to be cross-linguistically applicable (Jack-
endoff, 1990).
Offering a powerful tool for generalization, ab-
straction and prediction, VerbNet classes have
been used to support many important NLP
tasks, including e.g. computational lexicography,
parsing, word sense disambiguation, semantic
role labeling, information extraction, question-
answering, and machine translation (Swier and
Stevenson, 2004; Dang, 2004; Shi and Mihalcea,
2005; Abend et al, 2008). However, to date their
exploitation has been limited because for most
languages, no Levin style classification is avail-
able.
Since manual classification is costly (Kipper
et al, 2008) automatic approaches have been pro-
posed recently which could be used to learn novel
classifications in a cost-effective manner (Joanis
et al, 2008; Li and Brew, 2008; O? Se?aghdha
and Copestake, 2008; Vlachos et al, 2009; Sun
and Korhonen, 2009). However, most work on
Levin type classification has focussed on English.
Large-scale research on other languages such as
German (Schulte im Walde, 2006) and Japanese
(Suzuki and Fukumoto, 2009) has focussed on se-
mantic classification. Although the two classifica-
tion systems have shared properties, studies com-
paring the overlap between VerbNet and WordNet
(Miller, 1995) have reported that the mapping is
only partial and many to many due to fine-grained
nature of classes based on synonymy (Shi and Mi-
halcea, 2005; Abend et al, 2008).
Only few studies have been conducted on Levin
style classification for languages other than En-
glish. In their experiment involving 59 verbs and
three classes, Merlo et al (2002) applied a su-
pervised approach developed for English to Ital-
ian, obtaining high accuracy (86.3%). In an-
other experiment with 60 verbs and three classes,
1056
they showed that features extracted from Chinese
translations of English verbs can improve English
classification. These results are promising, but
those from a later experiment by Ferrer (2004)
are not. Ferrer applied a clustering approach de-
veloped for English to Spanish, and evaluated it
against the manual classification of Va?zquez et al
(2000), constructed using criteria similar (but not
identical) to Levin?s. This experiment involving
514 verbs and 31 classes produced results only
slightly better than the random baseline.
In this paper, we investigate the cross-linguistic
potential of Levin style classification further. In
past years, verb classification techniques ? in par-
ticular unsupervised ones ? have improved con-
siderably, making investigations for a new lan-
guage more feasible. We take a recent verb clus-
tering approach developed for English (Sun and
Korhonen, 2009) and apply it to French ? a ma-
jor language for which no such experiment has
been conducted yet. Basic NLP resources (cor-
pora, taggers, parsers and subcategorization ac-
quisition systems) are now sufficiently developed
for this language for the application of a state-of-
the-art verb clustering approach to be realistic.
Our investigation reveals similarities between
the English and French classifications, support-
ing the linguistic hypothesis (Jackendoff, 1990)
and the earlier result of Merlo et al (2002)
that Levin classes have a strong cross-linguistic
basis. Not only the general methodology but
also best performing features are transferable be-
tween the languages, making it possible to learn
useful classes for French automatically without
language-specific tuning.
2 French Gold Standard
The development of an automatic verb classifi-
cation approach requires at least an initial gold
standard. Some syntactic (Gross, 1975) and se-
mantic (Vossen, 1998) verb classifications exist
for French, along with ones which integrate as-
pects of both (Saint-Dizier, 1998). Since none of
these resources offer classes similar to Levins?,
we followed the idea of Merlo et al (2002) and
translated a number of Levin classes from English
to French. As our aim was to to investigate the
cross-linguistic applicability of classes, we took
an English gold standard which has been used to
evaluate several recent clustering works ? that of
Sun et al (2008). This resource includes 17 fine-
grained Levin classes. Each class has 12 member
verbs whose predominant sense in English (ac-
cording to WordNet) belongs to that class.
Member verbs were first translated to French.
Where several relevant translations were identi-
fied, each of them was considered. For each can-
didate verb, subcategorization frames (SCFs) were
identified and diathesis alternations were consid-
ered using the criteria of Levin (1993): alterna-
tions must result in the same or extended verb
sense. Only verbs sharing diathesis alternations
were kept in the class.
For example, the gold standard class 31.1
AMUSE includes the following English verbs:
stimulate, threaten, shock, confuse, upset, over-
whelm, scare, disappoint, delight, exhaust, in-
timidate and frighten. Relevant French transla-
tions were identified for all of them: abattre,
accabler, briser, de?primer, consterner, ane?antir,
e?puiser, exte?nuer, e?craser, ennuyer, e?reinter, inon-
der. The majority of these verbs take similar SCFs
and diathesis alternations, e.g. Cette affaire e?crase
Marie (de chagrin), Marie est e?crase?e par le cha-
grin, Le chagrin e?crase Marie. However, stim-
uler (stimulate) and menacer (threaten) do not,
and they were therefore removed.
40% of translations were discarded from
classes because they did not share the same aler-
nations. The final version of the gold stan-
dard (shown in table 1) includes 171 verbs in 16
classes. Each class is named according to the
original Levin class. The smallest class (30.3) in-
cludes 7 verbs and the largest (37.3) 16. The aver-
age number of verbs per class is 10.7.
3 Verb Clustering
We performed an experiment where we
? took a French corpus and a SCF lexicon au-
tomatically extracted from that corpus,
? extracted from these resources a range of fea-
tures (lexical, syntactic and semantic) ? a
representative sample of those employed in
recent English experiments,
1057
Class No Class Verbs
9.1 PUT accrocher, de?poser, mettre, placer, re?partir, re?inte?grer, empiler, emporter, enfermer,
inse?rer, installer
10.1 REMOVE o?ter, enlever, retirer, supprimer, retrancher, de?barrasser, soustraire, de?compter, e?liminer
11.1 SEND envoyer, lancer, transmettre, adresser, porter, expe?dier, transporter, jeter, renvoyer, livrer
13.5.1 GET acheter, prendre, saisir, re?server, conserver, garder, pre?server, maintenir, retenir, louer,
affre?ter
18.1 HIT cogner, heurter, battre, frapper, fouetter, taper, rosser, brutaliser, e?reinter, maltraiter,
corriger,
22.2 AMALGAMATE incorporer, associer, re?unir, me?langer, me?ler, unir, assembler, combiner, lier, fusionner
29.2 CHARACTERIZE appre?hender, concevoir, conside?rer, de?crire, de?finir, de?peindre, de?signer, envisager,
identifier, montrer, percevoir, repre?senter, ressentir
30.3 PEER regarder, e?couter, examiner, conside?rer, voir, scruter, de?visager
31.1 AMUSE abattre, accabler, briser, de?primer, consterner, ane?antir, e?puiser, exte?nuer, e?craser, en-
nuyer, e?reinter, inonder,
36.1 CORRESPOND coope?rer, participer, collaborer, concourir, contribuer, prendre part, s?associer, travaille
37.3 MANNER OF
SPEAKING
ra?ler, gronder, crier, ronchonner, grogner, bougonner, maugre?er, rouspe?ter, grommeler,
larmoyer, ge?mir, geindre, hurler, gueuler, brailler, chuchoter
37.7 SAY dire, re?ve?ler, de?clarer, signaler, indiquer, montrer, annoncer, re?pondre, affirmer, certifier,
re?pliquer
43.1 LIGHT EMIS-
SION
briller, e?tinceler, flamboyer, luire, resplendir, pe?tiller, rutiler, rayonner., scintiller
45.4 CHANGE OF
STATE
me?langer, fusionner, consolider, renforcer, fortifier, adoucir, polir, atte?nuer, tempe?rer,
pe?trir, fac?onner, former
47.3 MODES OF BE-
ING
trembler, fre?mir, osciller, vaciller, vibrer, tressaillir, frissonner, palpiter, gre?siller, trem-
bloter, palpiter
51.3.2 RUN voyager, aller, se promener, errer, circuler, se de?placer, courir, bouger, naviguer, passer
Table 1: A Levin style gold standard for French
? clustered the features using a method which
has proved promising in both English and
German experiments: spectral clustering,
? evaluated the clusters both quantitatively (us-
ing the gold standard) and qualitatively,
? and compared the performance to that re-
cently obtained for English in order to gain
a better understanding of the cross-linguistic
and language-specific properties of verb clas-
sification
This work is described in the subsequent sections.
3.1 Data: the LexSchem Lexicon
We extracted the features for clustering from
LexSchem (Messiant et al, 2008). This large sub-
categorization lexicon provides SCF frequency in-
formation for 3,297 French verbs. It was acquired
fully automatically from Le Monde newspaper
corpus (200M words from years 1991-2000) us-
ing ASSCI ? a recent subcategorization acquisi-
tion system for French (Messiant, 2008). Systems
similar to ASSCI have been used in recent verb
classification works e.g. (Schulte im Walde, 2006;
Li and Brew, 2008; Sun and Korhonen, 2009).
Like these other systems, ASSCI takes raw corpus
data as input. The data is first tagged and lemma-
tized using the Tree-Tagger and then parsed us-
ing Syntex (Bourigault et al, 2005). Syntex is
a shallow parser which employs a combination
of statistics and heuristics to identify grammati-
cal relations (GRs) in sentences. ASSCI considers
GRs where the target verbs occur and constructs
SCFs from nominal, prepositional and adjectival
phrases, and infinitival and subordinate clauses.
When a verb has no dependency, its SCF is con-
sidered as intransitive. ASSCI assumes no pre-
defined list of SCFs but almost any combination
of permitted constructions can appear as a candi-
date SCF. The number of automatically generated
SCF types in LexSchem is 336.
Many candidate SCFs are noisy due to process-
ing errors and the difficulty of argument-adjunct
distinction. Most SCF systems assume that true
arguments occur in argument positions more fre-
quently than adjuncts. Many systems also inte-
grate filters for removing noise from system out-
put. When LexSchem was evaluated after filter-
1058
ing its F-measure was 69 ? which is similar to
that of other current SCF systems (Messiant et al,
2008) We used the unfiltered version of the lexi-
con because English experiments have shown that
information about adjuncts can help verb cluster-
ing (Sun et al, 2008).
4 Features
Lexical entries in LexSchem provide a variety of
material for verb clustering. Using this material,
we constructed a range of features for experimen-
tation. The first three include basic information
about SCFs:
F1: SCFs and their relative frequencies with indi-
vidual verbs. SCFs abstract over particles and
prepositions.
F2: F1, with SCFs parameterized for the tense
(the POS tag) of the verb.
F3: F2, with SCFs parameterized for prepositions
(PP).
The following six features include informa-
tion about the lexical context (co-occurrences)
of verbs. We adopt the best method of Li and
Brew (2008) where collocations (COs) are ex-
tracted from the window of words immediately
preceding and following a lemmatized verb. Stop
words are removed prior to extraction.
F4, F6, F8: COs are extracted from the window
of 4, 6 and 8 words, respectively. The relative
word position is ignored.
F5, F7, F9: F4, F6 and F8 with the relative word
position recorded.
The next four features include information
about lexical preferences (LP) of verbs in argu-
ment head positions of specific GRs associated
with the verb:
F10: LP(PREP): the type and frequency of prepo-
sitions in the preposition (PREP) relation.
F11: LP(SUBJ): the type and frequency of nouns
in the subject (SUBJ) relation.
F12: LP(IOBJ): the type and frequency of nouns
in the object (OBJ) and indirect object (IOBJ)
relation.
F13: LP(ALL): the combination of F10-F13.
The final two features refine SCF features with
LPs and semantic information about verb selec-
tional preferences (SP):
F14-F16: F1-F3 parameterized for LPs.
F17: F3 refined with SPs.
We adopt a fully unsupervised approach to SP
acquisition using the method of Sun and Korho-
nen (2009), with the difference that we determine
the optimal number of SP clusters automatically
following Zelnik-Manor and Perona (2004). The
method is introduced in the following section. The
approach involves (i) taking the GRs (SUBJ, OBJ,
IOBJ) associated with verbs, (ii) extracting all the
argument heads in these GRs, and (iii) clustering
the resulting N most frequent argument heads into
M classes. The empirically determined N 200
was used. The method produced 40 SP clusters.
5 Clustering Methods
Spectral clustering (SPEC) has proved promising
in previous verb clustering experiments (Brew
and Schulte im Walde, 2002; Sun and Korho-
nen, 2009) and other similar NLP tasks involv-
ing high dimensional feature space (Chen et al,
2006). Following Sun and Korhonen (2009) we
used the MNCut spectral clustering (Meila and
Shi, 2001) which has a wide applicability and
a clear probabilistic interpretation (von Luxburg,
2007; Verma and Meila, 2005). However, we ex-
tended the method to determine the optimal num-
ber of clusters automatically using the technique
proposed by (Zelnik-Manor and Perona, 2004).
Clustering groups a given set of verbs V =
{vn}Nn=1 into a disjoint partition of K classes.
SPEC takes a similarity matrix as input. All our
features can be viewed as probabilistic distribu-
tions because the combination of different fea-
tures is performed via parameterization. Thus we
use the Jensen-Shannon divergence (JSD) to con-
struct the similarity matrix. The JSD between
1059
two feature vectors v and v? is djsd(v, v?) =
1
2D(v||m)+ 12D(v?||m) where D is the Kullback-Leibler divergence, and m is the average of the v
and v?.
The similarity matrix W is constructed where
Wij = exp(?djsd(v, v?)). In SPEC, the simi-
larities Wij are viewed as the connection weight
ij of a graph G over V . The similarity matrix
W is thus the adjacency matrix for G. The de-
gree of a vertex i is di = ?Nj=1 wij . A cut be-
tween two partitions A and A? is defined to be
Cut(A,A?) =?m?A,n?A? Wmn.
The similarity matrix W is normalized into a
stochastic matrix P .
P = D?1W (1)
The degree matrix D is a diagonal matrix where
Dii = di.
It was shown by Meila and Shi (2001) that if P
has the K leading eigenvectors that are piecewise
constant1 with respect to a partition I? and their
eigenvalues are not zero, then I? minimizes the
multiway normalized cut(MNCut):
MNCut(I) = K ??Kk=1 Cut(Ik,Ik)Cut(Ik,I)
Pmn can be interpreted as the transition proba-
bility between vertices m,n. The criterion can
thus be expressed as MNCut(I) = ?Kk=1(1 ?
P (Ik ? Ik|Ik)) (Meila, 2001), which is the sum
of transition probabilities across different clusters.
This criterion finds the partition where the random
walks are most likely to happen within the same
cluster. In practice, the leading eigenvectors of P
are not piecewise constant. But we can extract the
partition by finding the approximately equal ele-
ments in the eigenvectors using a clustering algo-
rithm like K-Means.
As the value of K is not known beforehand, we
use Zelnik-Manor and Perona (2004)?s method to
estimate it. This method finds the optimal value
by minimizing a cost function based on the eigen-
vector structure of W .
Like Brew and Schulte im Walde (2002), we
compare SPEC against a K-Means baseline. We
used the Matlab implementation with euclidean
distance as the distance measure.
1The eigenvector v is piecewise constant with respect to
I if v(i) = v(j)?i, j ? Ik and k ? 1, 2...K
6 Experimental Evaluation
6.1 Data and Pre-processing
The SCF-based features (F1-F3 and F14-F17)
were extracted directly from LexSchem. The CO
(F4-F9) and LP features (F10-F13) were extracted
from the raw and parsed corpus sentences, respec-
tively, which were used for creating the lexicon.
Features that only appeared once were removed.
Feature vectors were normalized by the sum of the
feature values before clustering. Since our clus-
tering algorithms have an element of randomness,
we repeated clustering multiple times. We report
the results that minimize the distortion (the dis-
tance to cluster centroid).
6.2 Evaluation Measures
We employ the same measures for evaluation as
previously employed e.g. by O? Se?aghdha and
Copestake (2008) and Sun and Korhonen (2009).
The first measure is modified purity (mPUR) ?
a global measure which evaluates the mean preci-
sion of clusters. Each cluster is associated with its
prevalent class. The number of verbs in a cluster
K that take this class is denoted by nprevalent(K).
Verbs that do not take it are considered as errors.
Clusters where nprevalent(K) = 1 are disregarded
as not to introduce a bias towards singletons:
mPUR =
?
nprevalent(ki)>2
nprevalent(ki)
number of verbs
The second measure is weighted class accuracy
(ACC): the proportion of members of dominant
clusters DOM-CLUSTi within all classes ci.
ACC =
?C
i=1 verbs in DOM-CLUSTi
number of verbs
mPUR and ACC can be seen as a measure of pre-
cision(P) and recall(R) respectively. We calculate
F measure as the harmonic mean of P and R:
F = 2 ? mPUR ? ACCmPUR + ACC
The random baseline (BL) is calculated as fol-
lows: BL = 1/number of classes
7 Evaluation
7.1 Quantitative Evaluation
In our first experiment, we evaluated 116 verbs ?
those which appeared in LexSchem the minimum
1060
of 150 times. We did this because English exper-
iments had shown that due to the Zipfian nature
of SCF distributions, 150 corpus occurrences are
typically needed to obtain a sufficient number of
frames for clustering (Sun et al, 2008).
Table 2 shows F-measure results for all the fea-
tures. The 4th column of the table shows, for com-
parison, the results of Sun and Korhonen (2009)
obtained for English when they used the same fea-
tures as us, clustered them using SPEC, and evalu-
ated them against the English version of our gold
standard, also using F-measure2.
As expected, SPEC (the 2nd column) outper-
forms K-Means (the 3rd column). Looking at the
basic SCF features F1-F3, we can see that they per-
form significantly better than the BL method. F3
performs the best among the three features both
in French (50.6 F) and in English (63.3 F). We
therefore use F3 as the SCF feature in F14-F17
(the same was done for English).
In French, most CO features (F4-F9) outper-
form SCF features. The best result is obtained
with F7: 55.1 F. This is clearly better than the
best SCF result 50.6 (F3). This result is interesting
since SCFs correspond better than COs with fea-
tures used in manual Levin classification. Also,
SCFs perform considerably better than COs in the
English experiment (we only have the result for F4
available, but it is considerably lower than the re-
sult for F3). However, earlier English studies have
reported contradictory results (e.g. Li and Brew
(2008) showed that CO performs better than SCF
in supervised verb classification), indicating that
the role of CO features in verb classification re-
quires further investigation.
Looking at the LP features, F13 produces the
best F (52.7) for French which is slightly better
than the best SCF result for the language. Also
in English, F13 performs the best in this feature
group and yields a higher result than the best SCF-
based feature F3.
Parameterizing the best SCF feature F3 with LPs
(F14-16) and SPs (F17) yields better performance
2Note that the results for the two languages are not mu-
tually comparable due to differences in test sets, data sizes,
and feature extraction systems (see Section 8 for discussion).
The results for English are included so that we can compare
the relative performance of individual features in the two lan-
guages in question.
in French. F15 and F17 have the F of 54.5 and
54.6, respectively. These results are so close to
the result of the best CO feature F7 (55.1 ? which
is the highest result in this experiment) that the
differences are not statistically significant. In En-
glish, the results of F14-F17 are similarly good;
however, only F17 beats the already high perfor-
mance of F13.
On the basis of this experiment, it is difficult to
tell whether shallow CO features or more sophisti-
cated SCF-based features are better for French. In
the English experiment sophisticated features per-
formed better (the SCF-SP feature was the best).
However, the English experiment employed a
much larger dataset. These more sophisticated
features may suffer from data sparseness in our
French experiment since although we required the
minimum of 150 occurrences per verb, verb clus-
tering performance tends to improve when more
data is available, and given the fine-grained nature
of LexShem SCFs it is likely that more data is re-
quired for optimal performance.
We therefore performed another experiment
with French on the full set of 147 verbs, using
SPEC, where we investigated the effect of instance
filtering on the performance of the best features
from each feature group: F3, F7, F13 and F17.
The results shown in Table 3 reveal that the perfor-
mance of the features remains fairly similar until
the instance threshold of 1000. When 2000 occur-
rences per verb are used, the differences become
clearer, until at the threshold of 4000, it is obvious
that the most sophisticated SCF-SP feature F17 is
by far the best feature for French (65.4 F) and the
SCF feature F3 the second best (60.5 F). The CO-
feature F7 and the LP feature F13 are not nearly as
good (53.4 and 51.0 F).
Although the results at different thresholds are
not comparable due to the different number of
verbs and classes (see columns 2-3), the results
for features at the same threshold are. Those re-
sults suggest that when 2000 or more occurrences
per verb are used, most features perform like they
performed for English in the experiment of Sun
and Korhonen (2009), with CO being the least in-
formative3 and SCF-SP being the most informa-
3However, it is worth noting that CO is not a useless fea-
ture. As table 3 shows, when 150 or fewer occurrences are
1061
SPEC K Eng.
BL 6.7 6.7 6.7
F1 SCF 42.4 39.3 57.8
F2 SCF(POS) 45.9 40.3 46.7
F3 SCF(PP) 50.6 36.9 63.3
F4 CO(4) 50.3 38.2 40.9
F5 CO(4+loc) 48.8 26.3 -
F6 CO(6) 52.7 29.2 -
F7 CO(6+loc) 55.1 33.8 -
F8 CO(8) 54.2 36.4 -
F9 CO(8+loc) 54.6 37.2 -
F10 LP(PREP) 35.5 32.8 49.0
F11 LP(SUBJ) 33.7 23.6 -
F12 LP(OBJ) 50.1 33.3 -
F13 LP(ALL) 52.7 40.1 74.6
F14 SCF+LP(SUBJ) 50.3 40.1 71.7
F15 SCF+LP(OBJ) 54.5 35.6 74.0
F16 SCF+LP(SUBJ+OBJ) 53.4 36.2 73.0
F17 SCF+SP 54.6 39.8 80.4
Table 2: Results for all the features for French
(SPEC and K-means) and English (SPEC)
THR Verbs Cls F3 F7 F13 F17
0 147 15 43.7 57.5 43.3 50.1
50 137 15 47.9 56.1 44.8 49.1
100 125 15 49.2 54.3 44.8 49.5
150 116 15 50.6 55.1 52.7 54.6
200 110 15 54.9 52.9 49.7 52.5
400 96 15 52.7 52.9 43.9 53.2
1000 71 15 51.4 54.0 44.8 54.5
2000 59 12 52.3 45.9 42.7 53.5
3000 51 12 55.7 49.0 46.8 59.2
4000 43 10 60.5 53.4 51.0 65.4
Table 3: The effect of verb frequency
tive feature. The only exception is the LP feature
which performed better than CO in English.
7.2 Qualitative Evaluation
We conducted qualitative analysis of the clusters
for French: those created using SPEC with F17
and F3. Verbs in the gold standard classes 29.2,
36.1, 37.3, 37.7 and 47.3 (Table 1) performed
particularly well, with the majority of member
verbs found in the same cluster. These verbs
are ideal for clustering because they have distinc-
tive syntactic-semantic characteristics. For exam-
ple, verbs in 29.2 CHARACTERIZE class (e.g. con-
cevoir, conside?rer, de?peindre) not only have a very
specific meaning but they also take high frequency
SCFs involving the preposition comme (Eng. as)
available for a verb, CO outperforms all the other features in
French, compensating for data sparseness.
which is not typical to many other classes. Inter-
estingly, Levin classes 29.2, 36.1, 37.3, and 37.7
were among the best performing classes also in
the supervised verb classification experiment of
Sun et al (2008) because these classes have dis-
tinctive characteristics also in English.
The benefit of sophisticated features which
integrate also semantic (SP) information (F17)
is particularly evident for classes with non-
distinctive syntactic characteristics. For example,
the intransitive verbs in 43.1 LIGHT EMISSION
class (e.g. briller, e?tinceler, flamboyer) are diffi-
cult to cluster based on syntax only, but semantic
features work because the verbs pose strong SPs
on their subjects (entities capable of light emis-
sion). In the experiment of Sun et al (2008), 43.1
was the worst performing class, possibly because
no semantic features were used in the experiment.
The most frequent source of error is syntac-
tic idiosyncracy. This is particularly evident
for classes 10.1 REMOVE and 45.4 CHANGE OF
STATE. Although verbs in these classes can take
similar SCFs and alternations, only some of them
are frequent in data. For example, the SCF o?ter X
a` Y is frequent for verbs in 10.1, but not o?ter X
de Y. Although class 10.1 did not suffer from this
problem in the English experiment of Sun et al
(2008), class 45.4 did. Class 45.4 performs par-
ticularly bad in French also because its member
verbs are low in frequency.
Some errors are due to polysemy, caused partly
by the fact that the French version of the gold stan-
dard was not controlled for this factor. Some verbs
have their predominant senses in classes which are
missing in the gold standard, e.g. the most fre-
quent sense of retenir is memorize, not keep as in
the gold standard class 13.5.1. GET.
Finally, some errors are not true errors but
demonstrate the capability of clustering to learn
novel information. For example, the CHANGE
OF STATE class 45.4 includes many antonyms
(e.g. weaken vs. strenghten). Clustering (us-
ing F17) separates these antonyms, so that verbs
adoucir, atte?nuer and tempe?rer appear in one clus-
ter and consolider and renforcer in another. Al-
though these verbs share the same alternations,
their SPs are different. The opposite effect can be
observed when clustering maps together classes
1062
which are semantically and syntactically related
(e.g. 36.1 CORRESPOND and 37.7 SPEAK). Such
classes are distinct in Levin and VerbNet, al-
though should ideally be related. Cases such as
these show the potential of clustering in discover-
ing novel valuable information in data.
8 Discussion and Conclusion
When sufficient corpus data is available, there is
a strong correlation between the types of features
which perform the best in English and French.
When the best features are used, many individ-
ual Levin classes have similar performance in the
two languages. Due to differences in data sets
direct comparison of performance figures for En-
glish and French is not possible. When consid-
ering the general level of performance, our best
performance for French (65.4 F) is lower than the
best performance for English in the experiment of
Sun and Korhonen (2009). However, it does com-
pare favourably to the performance of other state-
of-the-art (even supervised) English systems (Joa-
nis et al, 2008; Li and Brew, 2008; O? Se?aghdha
and Copestake, 2008; Vlachos et al, 2009). This
is impressive considering that we experimented
with a fully unsupervised approach originally de-
veloped for another language.
When aiming to improve performance further,
employing larger data is critical. Most recent ex-
periments on English have employed bigger data
sets, and unlike us, some of them have only con-
sidered the predominant senses of medium-high
frequency verbs. As seen in section 7.1, such dif-
ferences in data can have significant impact on
performance. However, parser and feature ex-
traction performance can also play a big role in
overall accuracy, and should therefore be inves-
tigated further (Sun and Korhonen, 2009). The
relatively low performance of basic LP features
in French suggests that at least some of the cur-
rent errors are due to parsing. Future research
should investigate the source of error at different
stages of processing. In addition, it would be in-
teresting to investigate whether language-specific
tuning (e.g. using language specific features such
as auxiliary classes) can further improve perfor-
mance on French.
Earlier works most closely related to ours are
those of Merlo et al (2002) and Ferrer (2004).
Our results contrast with those of Ferrer who
showed that a clustering approach does not trans-
fer well from English to Spanish. However, she
used basic SCF and named entity features only,
and a clustering algorithm less suitable for high
dimensional data. Like us, Merlo et al (2002) cre-
ated a gold standard by translating Levin classes
to another language (Italian). They also applied a
method developed for English to Italian, and re-
ported good overall performance using features
developed for English. Although the experiment
was small (focussing on three classes and a few
features only) and involved supervised classifica-
tion, the results agree with ours.
These experiments support the linguistic hy-
pothesis that Levin style classification can be
cross-linguistically applicable. A clustering tech-
nique such as the one presented here could be used
as a tool for investigating whether classifications
are similar across a wider range of more diverse
languages. From the NLP perspective, the fact that
an unsupervised technique developed for one lan-
guage can be applied to another language with-
out the need for substantial tuning means that au-
tomatic techniques could be used to hypothesise
useful Levin style classes for further languages.
This, in turn, could facilitate the creation of mul-
tilingual VerbNets in the future.
9 Acknowledgement
Our work was funded by the Royal Society Uni-
versity Research Fellowship (AK), the Dorothy
Hodgkin Postgraduate Award (LS), the EPSRC
grants EP/F030061/1 and EP/G051070/1 (UK)
and the EU FP7 project ?PANACEA?.
References
Omri Abend, Roi Reichart, and Ari Rappoport. A
supervised algorithm for verb disambiguation into
VerbNet classes. In Proc. of COLING, pages 9?16,
2008.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
The Berkeley FrameNet Project. In COLING-ACL,
pages 86?90, 1998.
Didier Bourigault, Marie-Paule Jacques, Ce?cile Fabre,
Ce?cile Fre?rot, and Sylwia Ozdowska. Syntex,
analyseur syntaxique de corpus. In Actes des
1063
12e`mes journe?es sur le Traitement Automatique des
Langues Naturelles, 2005.
Chris Brew and Sabine Schulte im Walde. Spectral
clustering for German verbs. In Proc. of EMNLP,
pages 117?124, 2002.
Jinxiu Chen, Dong-Hong Ji, Chew Lim Tan, and
Zheng-Yu Niu. Unsupervised relation disambigua-
tion using spectral clustering. In Proc. of COL-
ING/ACL, pages 89?96, 2006.
Hoa Trang Dang. Investigations into the Role of Lexi-
cal Semantics in Word Sense Disambiguation. PhD
thesis, CIS, University of Pennsylvania, 2004.
Eva Esteve Ferrer. Towards a semantic classification of
Spanish verbs based on subcategorisation informa-
tion. In Proc. of ACL Student Research Workshop,
2004.
Ralph Grishman, Catherine Macleod, and Adam Mey-
ers. Comlex syntax: building a computational lexi-
con. In Proc. of COLING, pages 268?272, 1994.
Maurice Gross. Me?thodes en syntaxe. Hermann, Paris,
1975.
Eduard Hovy, Mitch Marcus, Martha Palmer,
L. Ramshaw, and R. Weischedel. Ontonotes: The
90% solution. In HLT/NAACL, 2006.
Ray Jackendoff. Semantic Structures. The MIT Press,
Cambridge, MA, 1990.
Eric Joanis, Suzanne Stevenson, and David James. A
general feature space for automatic verb classifica-
tion. Nat. Lang. Eng., 14(3):337?367, 2008.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. A large-scale classification of En-
glish verbs. Language Resources and Evaluation,
42:21?40, 2008.
Karin Kipper-Schuler. VerbNet: A broad-coverage,
comprehensive verb lexicon. University of Pennsyl-
vania, PA, 2005.
Beth. Levin. English verb classes and alternations: A
preliminary investigation. Chicago, IL, 1993.
Jianguo Li and Chris Brew. Which Are the Best Fea-
tures for Automatic Verb Classification. In Proc. of
ACL, pages 434?442, 2008.
Marina. Meila. The multicut lemma. Technical report,
University of Washington, 2001.
Marina Meila and Jianbo Shi. A random walks view of
spectral segmentation. In AISTATS, 2001.
Paola Merlo, Suzanne Stevenson, Vivian Tsang, and
Gianluca Allaria. A multilingual paradigm for auto-
matic verb classification. In Proc. of ACL, 2002.
Ce?dric Messiant. ASSCI : A subcategorization frames
acquisition system for French. In Proc. of ACL Stu-
dent Research Workshop, pages 55?60, 2008.
Ce?dric Messiant, Thierry Poibeau, and Anna Korho-
nen. LexSchem: a Large Subcategorization Lexicon
for French Verbs. In Proc. of LREC, 2008.
George A. Miller. WordNet: a lexical database for En-
glish. Communications of the ACM, 1995.
Diarmuid O? Se?aghdha and Ann Copestake. Semantic
classification with distributional kernels. In Proc. of
COLING, pages 649?656, 2008.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
The proposition bank: An annotated corpus of se-
mantic roles. Computational Linguistics, 3(1):71?
106, 2005.
Patrick Saint-Dizier. Verb Semantic Classes Based on
?alternations? and WordNet-like criteria . In P. Saint-
Dizier, editor, Predicative Forms in Natural lan-
guage and lexical Knowledge Bases , pages 247?
279. Kluwer Academic, 1998.
Sabine Schulte im Walde. Experiments on the Auto-
matic Induction of German Semantic Verb Classes.
Computational Linguistics, 2006.
Lei Shi and Rada Mihalcea. Putting pieces together:
Combining FrameNet, VerbNet and WordNet for ro-
bust semantic parsing. In Proc. of CICLing, pages
100?111, 2005.
Lin Sun and Anna Korhonen. Improving verb cluster-
ing with automatically acquired selectional prefer-
ences. In Proc. of EMNLP, pages 638?647, 2009.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
Verb class discovery from rich syntactic data. LNCS,
4919:16, 2008.
Yoshimi Suzuki and Fumiyo Fukumoto. Classify-
ing Japanese Polysemous Verbs based on Fuzzy C-
means Clustering. In Proc. of TextGraphs-4, pages
32?40, 2009.
Robert Swier and Suzanne Stevenson. Unsupervised
semantic role labelling. In Proc. of EMNLP, 2004.
Gloria Va?zquez, Ana Ferna?ndez, Irene Castello?n, and
M. Antonia Mart??. Clasificacio?n verbal: Alternan-
cias de dia?tesis. In Quaderns de Sintagma. Univer-
sitat de Lleida, 2000.
Deepak Verma and Marina Meila. A comparison of
spectral clustering algorithms. Technical report, De-
partment of CSE University of Washington Seattle,
2005.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. Unsupervised and Constrained Dirich-
let Process Mixture Models for Verb Clustering. In
Proc. of the Workshop on on GEMS, pages 74?82,
2009.
Ulrike von Luxburg. A tutorial on spectral clustering.
STAT COMPUT, 17:395 ? 416, 2007.
Piek Vossen. EuroWordNet: A Multilingual Database
with Lexical Semantic Networks. Kluwer Academic
Publishers, Dordrecht, 1998.
Lihi Zelnik-Manor and Pietro Perona. Self-tuning
spectral clustering. NIPS, 17(1601-1608):16, 2004.
1064
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 273?283,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Weakly-supervised Approach to Argumentative Zoning of Scientific
Documents
Yufan Guo
Computer Laboratory
University of Cambridge, UK
yg244@cam.ac.uk
Anna Korhonen
Computer Laboratory
University of Cambridge, UK
alk23@cam.ac.uk
Thierry Poibeau
LaTTiCe, UMR8094
CNRS & ENS, France
thierry.poibeau@ens.fr
Abstract
Argumentative Zoning (AZ) ? analysis of the
argumentative structure of a scientific paper ?
has proved useful for a number of informa-
tion access tasks. Current approaches to AZ
rely on supervised machine learning (ML).
Requiring large amounts of annotated data,
these approaches are expensive to develop and
port to different domains and tasks. A poten-
tial solution to this problem is to use weakly-
supervised ML instead. We investigate the
performance of four weakly-supervised clas-
sifiers on scientific abstract data annotated for
multiple AZ classes. Our best classifier based
on the combination of active learning and self-
training outperforms our best supervised clas-
sifier, yielding a high accuracy of 81% when
using just 10% of the labeled data. This re-
sult suggests that weakly-supervised learning
could be employed to improve the practical
applicability and portability of AZ across dif-
ferent information access tasks.
1 Introduction
Many practical tasks require accessing specific types
of information in scientific literature. For example,
a reader of scientific literature may be looking for
information about the objective of the study in ques-
tion, the methods used in the study, the results ob-
tained, or the conclusions drawn by authors. Sim-
ilarly, many Natural Language Processing (NLP)
tasks focus on the extraction of specific types of in-
formation in documents only.
To date, a number of approaches have been pro-
posed for sentence-based classification of scien-
tific literature according to categories of information
structure (or discourse, rhetorical, argumentative or
conceptual structure, depending on the framework
in question). Some of these classify sentences ac-
cording to typical section names seen in scientific
documents (Lin et al, 2006; Hirohata et al, 2008),
while others are based e.g. on argumentative zones
(Teufel and Moens, 2002; Mizuta et al, 2006; Teufel
et al, 2009), qualitative dimensions (Shatkay et al,
2008) or conceptual structure (Liakata et al, 2010)
of documents.
The best of current approaches have yielded
promising results and proved useful for information
retrieval, information extraction and summarization
tasks (Teufel and Moens, 2002; Mizuta et al, 2006;
Tbahriti et al, 2006; Ruch et al, 2007). How-
ever, relying on fully supervised machine learning
(ML) and a large body of annotated data, existing
approaches are expensive to develop and port to dif-
ferent scientific domains and tasks.
A potential solution to this bottleneck is to de-
velop techniques based on weakly-supervised ML.
Relying on a small amount of labeled data and
a large pool of unlabeled data, weakly-supervised
techniques (e.g. semi-supervision, active learning,
co/tri-training, self-training) aim to keep the advan-
tages of fully supervised approaches. They have
been applied to a wide range of NLP tasks, includ-
ing named-entity recognition, question answering,
information extraction, text classification and many
others (Abney, 2008), yielding performance levels
similar or equivalent to those of fully supervised
techniques.
To the best of our knowledge, such techniques
273
have not yet been applied to the analysis of infor-
mation structure of scientific documents by afore-
mentioned approaches. Recent experiments have
demonstrated the usefulness of weakly-supervised
learning for classifying discourse relations in scien-
tific texts, e.g. (Hernault et al, 2011). However, fo-
cusing on local (rather than global) structure of doc-
uments and being much more fine-grained in nature,
this related task differs from ours considerably.
In this paper, we investigate the potential of
weakly-supervised learning for Argumentative Zon-
ing (AZ) of scientific abstracts. AZ is an approach to
information structure which provides an analysis of
the rhetorical progression of the scientific argument
in a document (Teufel and Moens, 2002). It has
been used to analyze scientific texts in various disci-
plines ? including computational linguistics (Teufel
and Moens, 2002), law, (Hachey and Grover, 2006),
biology (Mizuta et al, 2006) and chemistry (Teufel
et al, 2009) ? and has proved useful for NLP tasks
such as summarization (Teufel and Moens, 2002).
Although the basic scheme is said to be discipline-
independent (Teufel et al, 2009), its application to
different domains has resulted in various modifica-
tions and laborious annotation exercises. This sug-
gests that a weakly-supervised approach would be
more practical than a fully supervised one for the
real-world application of AZ.
Taking two supervised classifiers as a comparison
point ? Support Vector Machines (SVM) and Con-
ditional Random Fields (CRF) ? we investigate the
performance of four weakly-supervised classifiers
on the AZ task: two based on semi-supervised learn-
ing (transductive SVM and semi-supervised CRF)
and two on active learning (Active SVM alone and
in combination with self-training).
The results are promising. Our best weakly-
supervised classifier (Active SVM with self-
training) outperforms the best supervised classifier
(SVM), yielding high accuracy of 81% when using
just 10% of the labeled data. When using just one
third of the labeled data, it performs equally well as
a fully supervised SVM which uses 100% of the la-
beled data. Our investigation suggests that weakly-
supervised learning could be employed to improve
the practical applicability and portability of AZ to
different information access tasks.
2 Data
We used in our experiments the recent dataset of
(Guo et al, 2010). Guo et al (2010) provide a cor-
pus of 1000 biomedical abstracts (consisting of 7985
sentences and 225785 words) annotated according
to three schemes of information structure ? those
based on section names (Hirohata et al, 2008), AZ
(Mizuta et al, 2006) and Core Scientific Concepts
(CoreSC) (Liakata et al, 2010). We focus here on
AZ only, because it subsumes all the categories of
the simple section name -based scheme, and accord-
ing to the inter-annotator agreement and ML experi-
ments reported by Guo et al (2010) it performs bet-
ter on this data than the fairly fine-grained CoreSC
scheme.
AZ is a scheme which provides an analysis of
the rhetorical progression of the scientific argument,
following the knowledge claims made by authors.
(Teufel and Moens, 2002) introduced AZ and ap-
plied it first to computational linguistics papers.
(Hachey and Grover, 2006) applied the scheme later
to legal texts and (Mizuta et al, 2006) modified it for
biology papers. More recently, (Teufel et al, 2009)
introduced a refined version of AZ and applied it to
chemistry papers.
The biomedical dataset of (Guo et al, 2010) has
been annotated according to the version of AZ de-
veloped for biology papers (Mizuta et al, 2006)
(with only minor modifications concerning zone
names). Seven categories of this scheme (out of the
10 possible) actually appear in abstracts and in the
resulting corpus. These are shown and explained
in Table 1. For example, the Method zone (METH)
is for sentences which describe a way of doing re-
search, esp. according to a defined and regular
plan; a special form of procedure or characteristic
set of procedures employed in a field of study as a
mode of investigation and inquiry.
An example of a biomedical abstract annotated
according to AZ is shown in Figure 1, with different
zones highlighted in different colors. For example,
the RES zone is highlighted in lemon green.
Table 2 shows the distribution of sentences per
scheme category in the corpus: Results (RES) is
by far the most frequent zone (accounting for 40%
of the corpus), while Background (BKG), Objective
(OBJ), Method (METH) and Conclusion (CON) cover
274
Table 1: Categories of AZ appearing in the corpus of (Guo et al, 2010)
Category Abbr. Definition
Background BKG The circumstances pertaining to the current work, situation, or its causes, history, etc.
Objective OBJ A thing aimed at or sought, a target or goal
Method METH A way of doing research, esp. according to a defined and regular plan; a special form
of procedure or characteristic set of procedures employed in a field of study as a mode
of investigation and inquiry
Result RES The effect, consequence, issue or outcome of an experiment; the quantity, formula,
etc. obtained by calculation
Conclusion CON A judgment or statement arrived at by any reasoning process; an inference, deduction,
induction; a proposition deduced by reasoning from other propositions; the result of
a discussion, or examination of a question, final determination, decision, resolution,
final arrangement or agreement
Related work REL A comparison between the current work and the related work
Future work FUT The work that needs to be done in the future
Figure 1: An example of an annotated abstract
B
ut
ad
ie
ne
(B
D
)m
et
ab
ol
is
m
sh
ow
s
ge
nd
er
,s
pe
ci
es
an
d
co
nc
en
tr
at
io
n
de
pe
nd
en
cy
,m
ak
in
g
th
e
ex
tr
ap
ol
at
io
n
of
an
im
al
re
su
lts
to
hu
m
an
s
co
m
pl
ex
.B
D
is
m
et
ab
ol
iz
ed
m
ai
nl
y
by
cy
to
ch
ro
m
e
P4
50
2E
1
to
th
re
e
ep
ox
id
es
,1
,2
-e
po
xy
-3
-b
ut
en
e
(E
B
),
1,
2;
3,
4-
di
ep
ox
yb
ut
an
e
(D
E
B
)a
nd
1,
2-
ep
ox
y-
bu
ta
ne
di
ol
(E
B
-d
io
l).
Fo
ra
cc
ur
at
e
ris
k
as
se
ss
m
en
ti
ti
s
im
po
rt
an
tt
o
el
uc
id
at
e
sp
ec
ie
s
di
ff
er
en
ce
s
in
th
e
in
te
rn
al
fo
rm
at
io
n
of
th
e
in
di
vi
du
al
ep
ox
id
es
in
or
de
rt
o
as
si
gn
th
e
re
la
tiv
e
ris
ks
as
so
ci
at
ed
w
ith
th
ei
rd
iff
er
en
tm
ut
ag
en
ic
po
te
nc
ie
s.
A
na
ly
si
s
of
N
-t
er
m
in
al
gl
ob
in
ad
du
ct
s
is
a
co
m
m
on
ap
pr
oa
ch
fo
rm
on
ito
rin
g
th
e
in
te
rn
al
fo
rm
at
io
n
of
B
D
de
riv
ed
ep
ox
id
es
.O
ur
lo
ng
te
rm
st
ra
te
gy
is
to
de
ve
lo
p
an
L
C
-M
S/
M
S
m
et
ho
d
fo
rs
im
ul
ta
ne
ou
s
de
te
ct
io
n
of
al
lt
hr
ee
B
D
he
m
og
lo
bi
n
ad
du
ct
s.
Th
is
ap
pr
oa
ch
is
m
od
el
ed
af
te
rt
he
re
ce
nt
ly
re
po
rt
ed
im
m
un
oa
ff
in
ity
L
C
-M
S/
M
S
m
et
ho
d
fo
rt
he
cy
cl
ic
N
,N
-(
2,
3-
di
hy
dr
ox
y-
1,
4-
bu
ta
dy
il)
-v
al
in
e
(p
yr
-V
al
,d
er
iv
ed
fr
om
D
E
B
).
W
e
re
po
rt
he
re
in
th
e
an
al
ys
is
of
th
e
E
B
-d
er
iv
ed
2-
hy
dr
ox
yl
-3
-b
ut
en
yl
-v
al
in
e
pe
pt
id
e
(H
B
-V
al
).
Th
e
pr
oc
ed
ur
e
ut
ili
ze
s
tr
yp
si
n
hy
dr
ol
ys
is
of
gl
ob
in
an
d
im
m
un
oa
ff
in
ity
(I
A
)p
ur
ifi
ca
tio
n
of
al
ky
la
te
d
he
pt
ap
ep
tid
es
.Q
ua
nt
ita
tio
n
is
ba
se
d
on
L
C
-M
S/
M
S
m
on
ito
rin
g
of
th
e
tr
an
si
tio
n
fr
om
th
e
si
ng
ly
ch
ar
ge
d
m
ol
ec
ul
ar
io
n
of
H
B
-V
al
(1
-7
)t
o
th
e
a(
1)
fr
ag
m
en
t.
H
um
an
H
B
-V
al
(1
-1
1)
w
as
sy
nt
he
si
ze
d
an
d
us
ed
fo
ra
nt
ib
od
y
pr
od
uc
tio
n.
A
s
in
te
rn
al
st
an
da
rd
,t
he
la
be
le
d
ra
t-
[(
13
)C
(5
)(
15
)N
]-
V
al
(1
-1
1)
w
as
pr
ep
ar
ed
th
ro
ug
h
di
re
ct
al
ky
la
tio
n
of
th
e
co
rr
es
po
nd
in
g
pe
pt
id
e
w
ith
E
B
.S
ta
nd
ar
ds
w
er
e
ch
ar
ac
te
riz
ed
an
d
qu
an
tif
ie
d
by
L
C
-M
S/
M
S
an
d
L
C
-U
V
.T
he
m
et
ho
d
w
as
va
lid
at
ed
w
ith
di
ff
er
en
ta
m
ou
nt
s
of
hu
m
an
H
B
-V
al
st
an
da
rd
.T
he
re
co
ve
ry
w
as
>7
5%
an
d
co
ef
fic
ie
nt
of
va
ria
tio
n
<2
5%
.T
he
L
O
Q
w
as
se
tt
o
10
0
fm
ol
/in
je
ct
io
n.
Fo
ra
pr
oo
fo
fp
rin
ci
pa
le
xp
er
im
en
t,
gl
ob
in
sa
m
pl
es
fr
om
m
al
e
an
d
fe
m
al
e
ra
ts
ex
po
se
d
to
10
00
pp
m
B
D
fo
r9
0
da
ys
w
er
e
an
al
yz
ed
.T
he
am
ou
nt
s
of
H
B
-V
al
pr
es
en
tw
er
e
26
8.
2+
/-
56
an
d
35
0+
/-
70
pm
ol
/g
(m
ea
n+
/-
S.
D
.)
fo
rm
al
es
an
d
fe
m
al
es
,r
es
pe
ct
iv
el
y.
N
o
H
B
-V
al
w
as
de
te
ct
ed
in
co
nt
ro
ls
.T
he
se
da
ta
ar
e
m
uc
h
lo
w
er
co
m
pa
re
d
to
pr
ev
io
us
ly
re
po
rt
ed
va
lu
es
m
ea
su
re
d
by
G
C
-M
S/
M
S.
Th
e
di
ff
er
en
ce
m
ay
be
du
e
hi
gh
er
sp
ec
ifi
ci
ty
of
th
e
L
C
-M
S/
M
S
m
et
ho
d
to
th
e
N
-t
er
m
in
al
pe
pt
id
e
fr
om
th
e
al
ph
a-
ch
ai
n
ve
rs
us
de
riv
at
iz
at
io
n
of
bo
th
al
ph
a-
an
d
be
ta
-c
ha
in
by
E
dm
an
de
gr
ad
at
io
n,
an
d
po
ss
ib
le
in
st
ab
ili
ty
of
H
B
-V
al
ad
du
ct
s
du
rin
g
lo
ng
te
rm
st
or
ag
e
(a
bo
ut
10
ye
ar
s)
be
tw
ee
n
th
e
an
al
ys
es
.T
he
se
di
ff
er
en
ce
s
w
ill
be
re
so
lv
ed
by
ex
am
in
in
g
re
ce
nt
ly
co
lle
ct
ed
sa
m
pl
es
,u
si
ng
th
e
sa
m
e
in
te
rn
al
st
an
da
rd
fo
rp
ar
al
le
la
na
ly
si
s
by
G
C
-M
S/
M
S
an
d
L
C
-M
S/
M
S.
B
as
ed
on
ou
re
xp
er
ie
nc
e
w
ith
py
r-
V
al
ad
du
ct
as
sa
y
w
e
an
tic
ip
at
e
th
at
th
is
as
sa
y
w
ill
be
su
ita
bl
e
fo
r
ev
al
ua
tio
n
of
H
B
-V
al
in
m
ul
tip
le
sp
ec
ie
s.
Ba
ck
gr
ou
nd
O
bj
ec
tiv
e
M
et
ho
d
Re
su
lt
Co
nc
lu
si
on
Re
la
te
d 
w
or
k
Fu
tu
re
 w
or
k
Table 2: Distribution of sentences in the AZ-annotated
corpus
BKG OBJ METH RES CON REL FUT
Word 36828 23493 41544 89538 30752 2456 1174
Sentence 1429 674 1473 3185 1082 95 47
Sentence 18% 8% 18% 40% 14% 1% 1%
8-18% of the corpus each. Two categories are very
low in frequency, only covering 1% of the corpus
each: Related work (REL) and Future work (FUT).
Guo et al (2010) report the inter-annotator agree-
ment between their three annotators: one linguist,
one computational linguist and one domain expert.
According to Cohen?s kappa (Cohen, 1960) the
agreement is relatively high: ? = 0.85.
3 Automatic identification of AZ
3.1 Features and feature extraction
Guo et al (2010) used a variety of features in
their fully supervised ML experiments on different
schemes of information structure. Since their fea-
ture types cover the best performing feature types in
earlier works e.g. (Teufel and Moens, 2002; Lin et
al., 2006; Mullen et al, 2005; Hirohata et al, 2008;
Merity et al, 2009) we re-implemented and used
them in our experiment1. However, being aware
of the fact that some of these features may not be
optimal for weakly-supervised learning (i.e. when
learning from smaller data), we evaluate their per-
formance and suitability for the task later in sec-
tion 4.3.
? Location. Zones tend to appear in typical po-
sitions in abstracts. Each abstract was there-
1The only exception is the history feature which was left out
because it cannot be applied to all of our methods
275
fore divided into ten parts (1-10, measured by
the number of words), and the location was de-
fined by the parts where the sentence begins
and ends.
? Word. All the words in the corpus.
? Bi-gram. Any combination of two adjacent
words in the corpus.
? Verb. All the verbs in the corpus.
? Verb Class. 60 verb classes appearing in
biomedical journal articles.
? Part-of-Speech ? POS. The POS tag of each
verb in the corpus.
? Grammatical Relation ? GR. Subject (nc-
subj), direct object (dobj), indirect object (iobj)
and second object (obj2) relations in the cor-
pus. e.g. (ncsubj observed 14 difference 5
obj). The value of this feature equals 1 if it
occurs in a particular sentence (and 0 if not).
? Subj and Obj. The subjects and objects ap-
pearing with any verbs in the corpus (extracted
from above GRs).
? Voice. The voice of verbs (active or passive) in
the corpus.
These features were extracted from the corpus us-
ing a number of tools. A tokenizer was used to detect
the boundaries of sentences and to separate punctu-
ation from adjacent words e.g. in complex biomed-
ical terms such as 2-amino-3,8-diethylimidazo[4,5-
f]quinoxaline. The C&C tools (Curran et al, 2007)
trained on biomedical literature were employed for
POS tagging, lemmatization and parsing. The
lemma output was used for creating Word, Bi-gram
and Verb features. The GR output was used for cre-
ating the GR, Subj, Obj and Voice features. The
?obj? marker in a subject relation indicates passive
voice (e.g. (ncsubj observed 14 difference 5 obj)).
The verb classes were acquired automatically from
the corpus using the unsupervised spectral cluster-
ing method of (Sun and Korhonen, 2009). To con-
trol the number of features we lemmatized the lexi-
cal items for all the features, and removed the words
and GRs with fewer than 2 occurrences and bi-grams
with fewer than 5 occurrences.
3.2 Machine learning methods
Support Vector Machines (SVM) and Conditional
Random Fields (CRF) have proved the best perform-
ing fully supervised methods in most recent works
on information structure, e.g. (Teufel and Moens,
2002; Mullen et al, 2005; Hirohata et al, 2008; Guo
et al, 2010). We therefore implemented these meth-
ods as well as weakly supervised variations of them:
active SVM with and without self-training, transduc-
tive SVM and semi-supervised CRF.
3.2.1 Supervised methods
SVM constructs hyperplanes in a multidimen-
sional space to separate data points of different
classes. Good separation is achieved by the hyper-
plane that has the largest distance from the nearest
data points of any class. The hyperplane has the
form w ? x ? b = 0, where w is its normal vec-
tor. We want to maximize the distance from the hy-
perplane to the data points, or the distance between
two parallel hyperplanes each of which separates the
data. The parallel hyperplanes can be written as:
w ? x ? b = 1 and w ? x ? b = ?1, and the dis-
tance between them is 2|w| . The problem reduces to:
Minimize |w| (in w, b)
Subject to
w ? x? b ? 1 for x of one class,
w ? x? b ? ?1 for x of the other,
which can be solved by using the SMO algorithm
(Platt, 1999b). We used Weka software (Hall et al,
2009) (employing its linear kernel) for SVM experi-
ments.
CRF is an undirected graphical model which de-
fines a probability distribution over the hidden states
(e.g. label sequences) given the observations. The
probability of a label sequence y given an observa-
tion sequence x can be written as:
p(y|x, ?) = 1Z(x)exp(
?
j ?jFj(y, x)),
where Fj(y, x) is a real-valued feature function of
the states and the observations; ?j is the weight of
Fj , and Z(x) is a normalization factor. The ? pa-
rameters can be learned using the L-BFGS algorithm
(Nocedal, 1980). We used Mallet software (McCal-
lum, 2002) for CRF experiments.
3.2.2 Weakly-supervised methods
Active SVM (ASVM) starts with a small amount of
labeled data, and iteratively chooses a proportion of
276
unlabeled data for which SVM has less confidence
to be labeled (the labels can be restored from the
original corpus) and used in the next round of learn-
ing, i.e. active learning. Query strategies based on
the structure of SVM are frequently employed (Tong
and Koller, 2001; Novak et al, 2006). For exam-
ple, it is often assumed that the data points close to
the separating hyperplane are those that the SVM is
uncertain about. Unlike these methods, our learn-
ing algorithm compares the posterior probabilities
of the best estimate given each unlabeled instance,
and queries those with the lowest probabilities for
the next round of learning. The probabilities can be
obtained by fitting a Sigmoid after the standard SVM
(Platt, 1999a), and combined using a pairwise cou-
pling algorithm (Hastie and Tibshirani, 1998) in the
multi-class case. We used the SVM linear kernel in
Weka for classification, and the -M flag in Weka for
calculating the posterior probabilities.
Active SVM with self-training (ASSVM) is an ex-
tension of ASVM where each round of training has
two steps: (i) training on the labeled, and testing
on the unlabeled data, and querying; (ii) training on
both labeled and unlabeled/machine-labeled data by
using the estimates from step (i). The idea of ASSVM
is to make the best use of the labeled data, and to
make the most use of the unlabeled data.
Transductive SVM (TSVM) is an extension of
SVM which takes advantage of both labeled and un-
labeled data (Vapnik, 1998). Similar to SVM, the
problem is defined as:
Minimize |w| (in w, b, y(u))
Subject to
y(l)(w ? x(l) ? b) ? 1,
y(u)(w ? x(u) ? b) ? 1 ,
y(u) ? {?1, 1},
where x(u) is unlabeled data and y(u) the estimate
of its label. The problem can be solved by using
the CCCP algorithm (Collobert et al, 2006). We
used UniverSVM software (Sinz, 2011) for TSVM
experiments.
Semi-supervised CRF (SSCRF) can be imple-
mented with entropy regularization (ER). It ex-
tends the objective function on Labeled data?
L log p(y(l)|x(l), ?) with an additional term?
U
?
Y p(y|x(u), ?) log p(y|x(u), ?) to minimize
the conditional entropy of the model?s predictions on
Unlabeled data (Jiao et al, 2006; Mann and Mccal-
lum, 2007). We used Mallet software (McCallum,
2002) for SSCRF experiments.
4 Experimental evaluation
4.1 Evaluation methods
We evaluated the ML results in terms of accuracy,
precision, recall, and F-measure against manual AZ
annotations in the corpus:
acc = no. of correctly classified sentencestotal no. ofsentences in the corpus
p = no. of sentences correctly identified as Classitotal no. of sentences identified as Classi
r = no. of sentences correctly identified as Classitotal no. of sentences in Classi
f = 2?p?rp+r
We used 10-fold cross validation for all the meth-
ods to avoid the possible bias introduced by rely-
ing on any particular split of the data. More specif-
ically, the data was randomly assigned to ten folds
of roughly the same size. Each fold was used once
as test data and the remaining nine folds as training
data. The results were then averaged.
Following (Dietterich, 1998), we used McNe-
mar?s test (McNemar, 1947) to measure the statisti-
cal significance between the results of different ML
methods. The chosen significance level was .05.
4.2 Results
Table 3 shows the results for the four weakly-
supervised and two supervised methods when 10%
of the training data (i.e. ?700 sentences) has been
labeled. We can see that ASSVM is the best perform-
ing method with an accuracy of 81% and the macro
Table 3: Results when using 10% of the labeled data
Acc. F-score
MF BKG OBJ METH RES CON REL FUT
SVM .77 .74 .84 .68 .71 .82 .64 - -
CRF .70 .65 .75 .46 .48 .78 .76 - -
ASVM .80 .75 .88 .56 .68 .87 .78 .33
ASSVM .81 .76 .86 .56 .76 .88 .76 - -
TSVM .76 .73 .84 .61 .71 .79 .71 - -
SSCRF .73 .67 .76 .48 .52 .81 .78 - -
MF: Macro F-score of the five high frequency categories:
BKG, OBJ, METH, RES, CON.
277
Figure 2: Learning curve for different methods when using 0-100% of the labeled data
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
Labeled
A
cc
u
ra
cy
SVM CRF ASVM ASSVM TSVM SSCRF
Figure 3: Area under learning curves at different intervals
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
SVM CRF ASVM ASSVM TSVM SSCRF
Ar
ea
(0,10%] [10%,20%] [20%,40%] [40%,100%]
F-score of .76 (the macro F-score is calculated for
the 5 scheme categories which are found by all the
methods). ASVM performs nearly as well, with an
accuracy of 80% and F-score of .75. Both methods
outperform supervised SVM with a statistically sig-
nificant difference (p < .001).
TSVM is the lowest performing SVM-based
method. Yielding an accuracy of 76% and F-score
of .73 its performance is lower than that of the super-
vised SVM. However, it does outperform both CRF-
based methods. SSCRF performs better than CRF
with 3% higher accuracy and .02 higher F-score.
The difference in accuracy is statistically significant
(p < .001).
Only one method (ASVM) identifies six out of the
seven possible categories. Other methods identify
five categories. The 1-2 missing categories are very
low in frequency (accounting for 1% of the corpus
data each, see table 2). Looking at the results for
other categories, they seem to reflect the amount of
corpus data available for each category (Table 2),
with RES (Results) being the highest and OBJ (Ob-
jective) the lowest performing category with most
methods. Interestingly, the only method that per-
forms relatively well on OBJ is the supervised SVM.
The best method ASSVM outperforms other meth-
ods most clearly on METH (Method) category. Al-
though METH is a high frequency category (account-
ing for 18% of the corpus data) other methods tend
to confuse it with OBJ, presumably because a single
sentence may contain elements of both (e.g. scien-
tists may describe some of their method when de-
scribing the objective of the study).
Figure 2 shows the learning curve of different
methods (in terms of accuracy) when the percentage
of the labeled data (in the training set) ranges from 0
to 100%. ASSVM outperforms other methods, reach-
ing its best performance of 88% accuracy when us-
ing ?40% of the labeled data. Indeed when using
33% of the labeled data, it performs already equally
well as fully-supervised SVM using 100% of the la-
beled data. The advantage of ASSVM over ASVM
(the second best method) is clear especially when
20-40% of the labeled data is used. SVM and TSVM
tend to perform quite similarly with each other when
more than 25% of the labeled data is used, but when
less data is available, SVM performs better. Look-
ing at the CRF-based methods, SSCRF outperforms
CRF in particular when 10-25% of the labeled data
is used. However, neither of them reaches the per-
formance level of SVM-based methods.
Figure 3 shows the area under the learning curves
(by the trapezoidal rule) at different intervals, which
gives a reasonable approximation to the overall per-
formance of different methods. The area under
ASSVM is the largest at each of the four intervals,
with a value of .08 at (0,10%], .07 at [10%,20%],
278
.20 at [20%, 40%] and .50 at [40%,100%]. The dif-
ference between supervised and weakly-supervised
methods is more significant at (0, 20%] than at
[20%,100%].
4.3 Further analysis of the features
As explained in section 3.1, we employed in our
experiments a collection of features which had per-
formed well in previous supervised AZ experiments.
We conducted further analysis to investigate which
of these features are the most (and the least) useful
for weakly-supervised learning. We took our best
performing method ASSVM and conducted leave-
one-out analysis of the features with 10% of the la-
beled data. The results are shown in Table 4.
Table 4: Leaving one feature out results for ASSVM when
using 10% of the labeled data
Acc. F-score
MF BKG OBJ METH RES CON REL FUT
Location .73 .67 .67 .55 .62 .85 .65 - -
Word .80 .78 .87 .70 .74 .85 .72 - -
Bigram .81 .75 .83 .57 .71 .87 .78 .33 -
Verb .81 .79 .84 .77 .73 .87 .75 - -
VC .79 .75 .86 .62 .72 .84 .70 - -
POS .74 .70 .66 .65 .66 .82 .73 - -
GR .79 .75 .83 .67 .69 .84 .72 - -
Subj .80 .76 .87 .65 .73 .85 .72 - -
Obj .80 .78 .84 .75 .70 .85 .75 - -
Voice .78 .75 .88 .70 .71 .83 .62 - -
? .81 .76 .86 .56 .76 .88 .76 - -
MF: Macro F-score of the five high frequency categories:
BKG, OBJ, METH, RES, CON.
?: Employing all the features.
We can see that the Location feature is by far the
most useful feature for ASSVM. The performance
drops 8% in accuracy and .09 in F-score in the ab-
sence of this feature. Location is particularly im-
portant for BKG (which nearly always appears in the
same location: in the beginning of an abstract) and is
highly useful for METH and CON as well. Removing
POS has almost equally strong effect, in particular
on BKG and METH, suggesting that verb tense is par-
ticularly useful for distinguishing these categories.
Also Voice, Verb class and GR contribute to gen-
eral performance, especially to accuracy. Voice is
particularly important for CON, which differs from
other categories in the sense that it is marked by fre-
quent usage of active voice. Verb class is helpful for
METH, RES and CON while GR is helpful for all high
frequency categories.
Among the least helpful features are those which
suffer from sparse data problems, including e.g.
Word, Bi-gram, and Verb. They perform particularly
badly when applied to low frequency zones. How-
ever, this is not the case when using fully-supervised
methods (i.e. 100% of the labeled data), suggest-
ing that a good performance in fully supervised ex-
periments does not necessarily translate into a good
performance in weakly-supervised experiments, and
that careful feature analysis and selection is impor-
tant when aiming to optimize the performance when
learning from sparse data.
5 Discussion
In our experiments, the majority of weakly-
supervised methods outperformed their correspond-
ing supervised methods when using just 10% of
the labeled data. The SVM-based methods per-
formed better than the CRF-based ones (regardless of
whether they were weakly or fully supervised). Guo
et al (2010) made a similar discovery when com-
paring fully supervised versions of SVM and CRF.
Our best performing weakly-supervised methods
were those based on active learning. Making a good
use of both labeled and unlabeled data, active learn-
ing combined with self-training (ASSVM) proved to
be the most useful method. Given 10% of the la-
beled data, ASSVM obtained an accuracy of 81% and
F-score of .76, outperforming the best supervised
method SVM with a statistically significant differ-
ence. It reached its top performance (88% accuracy)
when using 40% of the labeled data, and performed
equally well as fully supervised SVM (i.e. 100% of
the labeled data) when using just one third of the la-
beled data.
This result is in line with the results of many
other text classification works where active learn-
ing (alone or in combination with other techniques
such as self-training) has proved similarly useful,
e.g. (Lewis and Gale, 1994; Tong and Koller, 2002;
Brinker, 2006; Novak et al, 2006; Esuli and Sebas-
tiani, 2009; Yang et al, 2009).
While active learning iteratively explores the
unknown aspects of the unlabeled data, semi-
supervised learning attempts to make the best use
279
of what it already knows about the data. In our ex-
periments, semi-supervised methods (TSVM and SS-
CRF) did not perform equally well as active learning
? TSVM even produced a lower accuracy than SVM
with the same amount of labeled data ? although
these methods have gained success in related works.
We therefore looked into related works using
TSVM, e.g. (Chapelle and Zien, 2005), and discov-
ered that our dataset is much higher in dimensional-
ity than those employed in many other works. High
dimensional data is more sensitive, and therefore
fine-tuning with unlabeled data may cause a big de-
viation. We also looked into related works using
SSCRF, in particular the work of (Jiao et al, 2006)
who used the same SSCRF as the one we used in our
experiments. Jiao et al (2006) employed a much
larger data set than we did ? one including 5448 la-
beled instances (in 3 classes) and 5210-25145 unla-
beled instances. Given more labeled and unlabeled
data per class we might be able to obtain better per-
formance using SSCRF also on our task. However,
given the high cost of obtaining labeled data meth-
ods not needing it are preferable.
6 Conclusions and future work
Our experiments show that weakly-supervised
learning can be used to identify AZ in scientific
documents with good accuracy when only a limited
amount of labeled data is available. This is helpful
thinking of the real-world application and porting of
the approach to different tasks and domains. To the
best of our knowledge, no previous work has been
done on weakly-supervised learning of information
structure according to schemes of the type we have
focused on (Teufel and Moens, 2002; Mizuta et al,
2006; Lin et al, 2006; Hirohata et al, 2008; Shatkay
et al, 2008; Liakata et al, 2010).
Recently, some work has been done on the related
task of classification of discourse relations in sci-
entific texts: (Hernault et al, 2011) used structural
learning (Ando and Zhang, 2005) for this task. They
obtained 30-60% accuracy on the RST Discourse
Treebank (including 41 relation types) when using
100-10000 labeled and 100000 unlabeled instances.
The accuracy was 20-60% when using the labeled
data only. However, although related, the task of
discourse relation classification differs substantially
from our task in that it focuses on local discourse re-
lations while our task focuses on the global structure
of the scientific document.
In the future, we plan to improve and extend this
work in several directions. First, the approach to
active learning could be improved in various ways.
The query strategy we employed (uncertainty sam-
pling) is a relatively straightforward method which
only considers the best estimate for each unlabeled
instance, disregarding other estimates that may con-
tain useful information. In the future, we plan to
experiment with more sophisticated strategies, e.g.
the margin sampling algorithm by (Scheffer et al,
2001) and the query-by-committee (QBC) algorithm
by (Seung et al, 1992). In addition, there are al-
gorithms designed for reducing the redundancy in
queries which may be worth investigating (Hoi et al,
2006).
Also, (Hoi et al, 2006) shows that Logistic Re-
gression (LR) outperforms SVM when used with ac-
tive learning, yielding higher F-score on the Reuters-
21578 data set (binary classification, 10,788 docu-
ments in total, 100 of them labeled). It would be
interesting to explore whether supervised methods
other than SVM are optimal for active learning when
applied to our task.
Secondly, we plan to investigate other semi-
supervised methods, for example, the Expectation-
Maximization (EM) algorithm. (Lanquillon, 2000)
has shown that EM SVM performs better than super-
vised and transductive SVM on a text classification
task when applied to the dataset of 20 Newsgroups
(20 classes, 4000 documents for testing, 10000 un-
labeled ones), yielding up to ?10% higher accu-
racy when 200-5000 labeled documents are used for
training.
In addition, other combinations of weakly-
supervised methods might be worth looking into,
such as EM+active learning (McCallum and Nigam,
1998) and co-training+EM+active learning (Muslea
et al, 2002), which have proved promising in related
text classification works.
Besides looking for optimal ML strategies, we
plan to look for optimal features for the task. Our
feature analysis showed that not all the features
which had proved promising in fully supervised ex-
periments were equally promising when applied to
weakly-supervised learning from smaller data. We
280
plan to look into ways of reducing the sparse data
problem in features, e.g. by classifying not only
verbs but also other word classes into semantically-
motivated categories.
One the key motivations for developing a weakly-
supervised approach is to facilitate easy porting of
schemes such as AZ to new tasks and domains. Re-
cent research shows that active learning in a target
domain can leverage information from a different
but related (source) domain (Rai et al, 2010). Mak-
ing use of existing annotated datasets in biology,
chemistry, computational linguistics and law (Teufel
and Moens, 2002; Mizuta et al, 2006; Hachey
and Grover, 2006; Teufel et al, 2009) we will ex-
plore optimal ways of combining weakly-supervised
learning with domain-adaptation.
The work presented in this paper has focused on
the abstracts annotated according to the AZ scheme.
In the future, we plan to investigate the usefulness
of weakly-supervised learning for identifying other
schemes of information structure, e.g. (Lin et al,
2006; Hirohata et al, 2008; Shatkay et al, 2008;
Liakata et al, 2010), and not only in scientific ab-
stracts but also in full journal papers which typically
exemplify a larger set of scheme categories.
Finally, an important avenue of future research
is to evaluate the usefulness of weakly-supervised
identification of information structure for NLP tasks
such as summarization and information extraction
(Tbahriti et al, 2006; Ruch et al, 2007), and for
practical tasks such as manual review of scientific
papers for research purposes (Guo et al, 2010).
Acknowledgments
The work reported in this paper was funded by the
Royal Society (UK). YG was funded by the Cam-
bridge International Scholarship.
References
Steven Abney. 2008. Semi-supervised learning for com-
putational linguistics. Chapman & Hall / CRC.
Rie Kubota Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple tasks
and unlabeled data. J. Mach. Learn. Res., 6:1817?
1853.
Klaus Brinker. 2006. On active learning in multi-label
classification. In From Data and Information Analysis
to Knowledge Engineering, pages 206?213.
Olivier Chapelle and Alexander Zien. 2005. Semi-
supervised classification by low density separation.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20(1):37?46.
Ronan Collobert, Fabian Sinz, Jason Weston, and Le?on
Bottou. 2006. Trading convexity for scalability. In
Proceedings of the 23rd international conference on
Machine learning.
J. R. Curran, S. Clark, and J. Bos. 2007. Linguistically
motivated large-scale nlp with c&c and boxer. In Pro-
ceedings of the ACL 2007 Demonstrations Session.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learning
algorithms. Neural Comput., 10:1895?1923.
Andrea Esuli and Fabrizio Sebastiani. 2009. Active
learning strategies for multi-label text classification.
In Proceedings of the 31th European Conference on
IR Research on Advances in Information Retrieval.
Yufan Guo, Anna Korhonen, Maria Liakata, Ilona Silins
Karolinska, Lin Sun, and Ulla Stenius. 2010. Identi-
fying the information structure of scientific abstracts:
an investigation of three different schemes. In Pro-
ceedings of the 2010 Workshop on Biomedical Natural
Language Processing.
Ben Hachey and Claire Grover. 2006. Extractive sum-
marisation of legal texts. Artif. Intell. Law, 14:305?
345.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11:10?18.
T. Hastie and R. Tibshirani. 1998. Classification by pair-
wise coupling. Advances in Neural Information Pro-
cessing Systems, 10.
Hugo Hernault, Danushka Bollegala, and Mitsuru
Ishizuka. 2011. Semi-supervised discourse relation
classification with structural learning. In CICLing (1).
K. Hirohata, N. Okazaki, S. Ananiadou, and M. Ishizuka.
2008. Identifying sections in scientific abstracts us-
ing conditional random fields. In Proceedings of 3rd
International Joint Conference on Natural Language
Processing.
Steven C. H. Hoi, Rong Jin, and Michael R. Lyu. 2006.
Large-scale text categorization by batch mode active
learning. In Proceedings of the 15th international con-
ference on World Wide Web.
F. Jiao, S. Wang, C. Lee, R. Greiner, and D. Schuur-
mans. 2006. Semi-supervised conditional random
fields for improved sequence segmentation and label-
ing. In COLING/ACL.
Carsten Lanquillon. 2000. Learning from labeled and
unlabeled documents: A comparative study on semi-
supervised text classification. In Proceedings of the
281
4th European Conference on Principles of Data Min-
ing and Knowledge Discovery.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the 17th annual international ACM SIGIR confer-
ence on Research and development in information re-
trieval.
M. Liakata, S. Teufel, A. Siddharthan, and C. Batche-
lor. 2010. Corpora for the conceptualisation and zon-
ing of scientific papers. In Proceedings of the Seventh
conference on International Language Resources and
Evaluation (LREC?10).
J. Lin, D. Karakos, D. Demner-Fushman, and S. Khu-
danpur. 2006. Generative content models for struc-
tural analysis of medical abstracts. In Proceedings of
BioNLP-06.
G. S. Mann and A. Mccallum. 2007. Efficient compu-
tation of entropy gradient for semi-supervised condi-
tional random fields. In HLT-NAACL.
Andrew McCallum and Kamal Nigam. 1998. Employ-
ing em and pool-based active learning for text classi-
fication. In Proceedings of the Fifteenth International
Conference on Machine Learning.
A. K. McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
Quinn McNemar. 1947. Note on the Sampling Error
of the Difference Between Correlated Proportions or
Percentages. Psychometrika, 12(2):153?157.
S. Merity, T. Murphy, and J. R. Curran. 2009. Accurate
argumentative zoning with maximum entropy models.
In Proceedings of the 2009 Workshop on Text and Ci-
tation Analysis for Scholarly Digital Libraries.
Y. Mizuta, A. Korhonen, T. Mullen, and N. Collier. 2006.
Zone analysis in biology articles as a basis for in-
formation extraction. International Journal of Med-
ical Informatics on Natural Language Processing in
Biomedicine and Its Applications, 75(6):468?487.
T. Mullen, Y. Mizuta, and N. Collier. 2005. A base-
line feature set for learning rhetorical zones using full
articles in the biomedical domain. Natural language
processing and text mining, 7(1):52?58.
Ion Muslea, Steven Minton, and Craig A. Knoblock.
2002. Active + semi-supervised learning = robust
multi-view learning. In Proceedings of the Nineteenth
International Conference on Machine Learning.
Jorge Nocedal. 1980. Updating Quasi-Newton Matrices
with Limited Storage. Mathematics of Computation,
35(151):773?782.
Bla Novak, Dunja Mladeni, and Marko Grobelnik. 2006.
Text classification with active learning. In From Data
and Information Analysis to Knowledge Engineering,
pages 398?405.
J. C. Platt. 1999a. Probabilistic outputs for support vec-
tor machines and comparisons to regularized likeli-
hood methods. Advances in Large Margin Classiers,
pages 61?74.
John C. Platt. 1999b. Using analytic qp and sparseness
to speed training of support vector machines. In Pro-
ceedings of the 1998 conference on Advances in neural
information processing systems II.
Piyush Rai, Avishek Saha, Hal Daume?, III, and Suresh
Venkatasubramanian. 2010. Domain adaptation
meets active learning. In Proceedings of the NAACL
HLT 2010 Workshop on Active Learning for Natural
Language Processing.
P. Ruch, C. Boyer, C. Chichester, I. Tbahriti, A. Geiss-
buhler, P. Fabry, J. Gobeill, V. Pillet, D. Rebholz-
Schuhmann, C. Lovis, and A. L. Veuthey. 2007. Using
argumentation to extract key sentences from biomedi-
cal abstracts. Int J Med Inform, 76(2-3):195?200.
Tobias Scheffer, Christian Decomain, and Stefan Wro-
bel. 2001. Active hidden markov models for informa-
tion extraction. In Proceedings of the 4th International
Conference on Advances in Intelligent Data Analysis.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In Proceedings of the fifth an-
nual workshop on Computational learning theory.
H. Shatkay, F. Pan, A. Rzhetsky, and W. J. Wilbur. 2008.
Multi-dimensional classification of biomedical text:
Toward automated, practical provision of high-utility
text to diverse users. Bioinformatics, 24(18):2086?
2093.
F. Sinz, 2011. UniverSVM Support Vector Ma-
chine with Large Scale CCCP Functionality.
http://www.kyb.mpg.de/bs/people/fabee/universvm.html.
L. Sun and A. Korhonen. 2009. Improving verb cluster-
ing with automatically acquired selectional preference.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
I. Tbahriti, C. Chichester, Frederique Lisacek, and
P. Ruch. 2006. Using argumentation to retrieve
articles with similar citations. Int J Med Inform,
75(6):488?495.
S. Teufel and M. Moens. 2002. Summarizing scien-
tific articles: Experiments with relevance and rhetor-
ical status. Computational Linguistics, 28:409?445.
S. Teufel, A. Siddharthan, and C. Batchelor. 2009. To-
wards domain-independent argumentative zoning: Ev-
idence from chemistry and computational linguistics.
In Proceedings of EMNLP.
S. Tong and D. Koller. 2001. Support vector machine
active learning with applications to text classification.
Journal of Machine Learning Research, 2:45?66.
Simon Tong and Daphne Koller. 2002. Support vector
machine active learning with applications to text clas-
sification. J. Mach. Learn. Res., 2:45?66.
282
V. N. Vapnik. 1998. Statistical learning theory. Wiley,
New York.
Bishan Yang, Jian-Tao Sun, Tengjiao Wang, and Zheng
Chen. 2009. Effective multi-label active learning for
text classification. In Proceedings of the 15th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining.
283
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1012?1022,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Latent Vector Weighting for Word Meaning in Context
Tim Van de Cruys
RCEAL
University of Cambridge
tv234@cam.ac.uk
Thierry Poibeau
LaTTiCe, UMR8094
CNRS & ENS
thierry.poibeau@ens.fr
Anna Korhonen
Computer Laboratory & RCEAL
University of Cambridge
alk23@cam.ac.uk
Abstract
This paper presents a novel method for the com-
putation of word meaning in context. We make
use of a factorization model in which words, to-
gether with their window-based context words
and their dependency relations, are linked to
latent dimensions. The factorization model al-
lows us to determine which dimensions are im-
portant for a particular context, and adapt the
dependency-based feature vector of the word
accordingly. The evaluation on a lexical substi-
tution task ? carried out for both English and
French ? indicates that our approach is able to
reach better results than state-of-the-art meth-
ods in lexical substitution, while at the same
time providing more accurate meaning repre-
sentations.
1 Introduction
According to the distributional hypothesis of meaning
(Harris, 1954), words that occur in similar contexts
tend to be semantically similar. In the spirit of this by
now well-known adage, numerous algorithms have
sprouted up that try to capture the semantics of words
by looking at their distribution in texts, and compar-
ing those distributions in a vector space model.
Up till now, the majority of computational ap-
proaches to semantic similarity represent the mean-
ing of a word as the aggregate of the word?s contexts,
and hence do not differentiate between the different
senses of a word. The meaning of a word, however, is
largely dependent on the particular context in which
it appears. Take for example the word work in sen-
tences (1) and (2).
(1) The painter?s recent work is a classic example
of art brut.
(2) Equal pay for equal work!
The meaning of work is quite different in both sen-
tences. In sentence (1), work refers to the product of a
creative act, viz. a painting. In sentence (2), it refers
to labour carried out as a source of income. The
NLP community?s standard answer to the ambiguity
problem has always been some flavour of word sense
disambiguation (WSD), which in its standard form
boils down to choosing the best-possible fit from a
pre-defined sense inventory. In recent years, it has
become clear that this is in fact a very hard task to
solve for computers and humans alike (Ide and Wilks,
2006; Erk et al, 2009; Erk, 2010).
With these findings in mind, researchers have
started looking at different methods to tackle lan-
guage?s ambiguity, ranging from coarser-grained
sense inventories (Hovy et al, 2006) and graded
sense assignment (Erk and McCarthy, 2009), over
word sense induction (Schu?tze, 1998; Pantel and Lin,
2002; Agirre et al, 2006), to the computation of indi-
vidual word meaning in context (Erk and Pado?, 2008;
Thater et al, 2010; Dinu and Lapata, 2010). This
research inscribes itself in the same line of thought,
in which the meaning disambiguation of a word is
not just the assignment of a pre-defined sense; in-
stead, the original meaning representation of a word
is adapted ?on the fly?, according to ? and specifi-
cally tailored for ? the particular context in which
it appears. To be able to do so, we build a factor-
ization model in which words, together with their
window-based context words and their dependency
1012
relations, are linked to latent dimensions. The factor-
ization model allows us to determine which dimen-
sions are important for a particular context, and adapt
the dependency-based feature vector of the word ac-
cordingly. The evaluation on a lexical substitution
task ? carried out for both English and French ? indi-
cates that our method is able to reach better results
than state-of-the-art methods in lexical substitution,
while at the same time providing more accurate mean-
ing representations.
The remainder of this paper is organized as follows.
In section 2, we present some earlier work that is
related to the research presented here. Section 3
describes the methodology of our method, focusing
on the factorization model, and the computation of
meaning in context. Section 4 presents a thorough
evaluation on a lexical substitution task, both for
English and French. The last section then draws
conclusions, and presents a number of topics that
deserve further exploration.
2 Related work
One of the best known computational models of se-
mantic similarity is latent semantic analysis ? LSA
(Landauer and Dumais, 1997; Landauer et al, 1998).
In LSA, a term-document matrix is created, that con-
tains the frequency of each word in a particular doc-
ument. This matrix is then decomposed into three
other matrices with a mathematical factorization tech-
nique called singular value decomposition (SVD).
The most important dimensions that come out of the
SVD are said to represent latent semantic dimensions,
according to which nouns and documents can be rep-
resented more efficiently. Our model also applies
a factorization technique (albeit a different one) in
order to find a reduced semantic space.
The nature of a word?s context is a determining
factor in the kind of the semantic similarity that is in-
duced. A broad context window (e.g. a paragraph or
document) yields broad, topical similarity, whereas
a small context window yields tight, synonym-like
similarity. This has lead a number of researchers
(e.g. Lin (1998)) to use the dependency relations that
a particular word takes part in as context features.
An overview of dependency-based semantic space
models is given in Pado? and Lapata (2007).
A number of researchers have exploited the no-
tion of context to differentiate between the different
senses of a word in an unsupervised way (a task la-
beled word sense induction or WSI). Schu?tze (1998)
proposed a context-clustering approach, in which
context vectors are created for the different instances
of a particular word, and those contexts are grouped
into a number of clusters, representing the different
senses of the word. The context vectors are rep-
resented as second-order co-occurrences (i.e. the
contexts of the target word are similar if the words
they in turn co-occur with are similar). Van de Cruys
(2008) proposed a model for sense induction based
on latent semantic dimensions. Using a factorization
technique based on non-negative matrix factorization,
the model induces a latent semantic space according
to which both dependency features and broad con-
textual features are classified. Using the latent space,
the model is able to discriminate between different
word senses. Our approach makes use of a simi-
lar factorization model, but we extend the approach
with a probabilistic framework that is able to adapt
the original vector according to the context of the
instance.
Recently, a number of models emerged that aim
to model the individual meaning of words in context.
Erk and Pado? (2008, 2009) make use of selectional
preferences to express the meaning of a word in con-
text; the meaning of a word in the presence of an
argument is computed by multiplying the word?s vec-
tor with a vector that captures the inverse selectional
preferences of the argument. Thater et al (2009) and
Thater et al (2010) extend the approach based on se-
lectional preferences by incorporating second-order
co-occurrences in their model; their model allows
first-order co-occurrences to act as a filter upon the
second-order vector space, which allows for the com-
putation of meaning in context.
Erk and Pado? (2010) propose an exemplar-based
approach, in which the meaning of a word in context
is represented by the activated exemplars that are
most similar to it. And Mitchell and Lapata (2008)
propose a model for vector composition, focusing on
the different functions that might be used to combine
the constituent vectors. Their results indicate that
a model based on pointwise multiplication achieves
better results than models based on vector addition.
Finally, Dinu and Lapata (2010) propose a proba-
bilistic framework that models the meaning of words
1013
as a probability distribution over latent dimensions
(?senses?). Contextualized meaning is then mod-
eled as a change in the original sense distribution.
The model presented in this paper bears some resem-
blances to their approach; however, while their ap-
proach computes the contextualized meaning directly
within the latent space, our model exploits the latent
space to determine the features that are important
for a particular context, and adapt the original (out-
of-context) dependency-based feature vector of the
target word accordingly. This allows for a more pre-
cise and more distinct computation of word meaning
in context. Secondly, Dinu and Lapata use window-
based context features to build their latent model,
while our approach combines both window-based
and dependency-based features.
3 Methodology
3.1 Non-negative Matrix Factorization
Our model uses non-negative matrix factorization
(Lee and Seung, 2000) in order to find latent dimen-
sions. There are a number of reasons to prefer NMF
over the better known singular value decomposition
used in LSA. First of all, NMF allows us to mini-
mize the Kullback-Leibler divergence as an objec-
tive function, whereas SVD minimizes the Euclidean
distance. The Kullback-Leibler divergence is better
suited for language phenomena. Minimizing the Eu-
clidean distance requires normally distributed data,
and language phenomena are typically not normally
distributed. Secondly, the non-negative nature of
the factorization ensures that only additive and no
subtractive relations are allowed. This proves partic-
ularly useful for the extraction of semantic dimen-
sions, so that the NMF model is able to extract much
more clear-cut dimensions than an SVD model. And
thirdly, the non-negative property allows the resulting
model to be interpreted probabilistically, which is not
straightforward with an SVD factorization.
The key idea is that a non-negative matrix A is
factorized into two other non-negative matrices, W
and H
Ai?j ?Wi?kHk?j (1)
where k is much smaller than i, j so that both in-
stances and features are expressed in terms of a few
components. Non-negative matrix factorization en-
forces the constraint that all three matrices must be
non-negative, so all elements must be greater than or
equal to zero.
Using the minimization of the Kullback-Leibler di-
vergence as an objective function, we want to find the
matrices W and H for which the Kullback-Leibler
divergence between A and WH (the multiplication
of W and H) is the smallest. This factorization is
carried out through the iterative application of update
rules. Matrices W and H are randomly initialized,
and the rules in 2 and 3 are iteratively applied ? alter-
nating between them. In each iteration, each vector is
adequately normalized, so that all dimension values
sum to 1.
Ha? ? Ha?
?
iWia
Ai?
(WH)i??
kWka
(2)
Wia ?Wia
?
?Ha?
Ai?
(WH)i??
vHav
(3)
3.2 Combining syntax and context words
Using an extension of non-negative matrix factor-
ization (Van de Cruys, 2008), it is possible to
jointly induce latent factors for three different modes:
words, their window-based context words, and their
dependency-based context features. As input to
the algorithm, three matrices are constructed that
capture the pairwise co-occurrence frequencies for
the different modes. The first matrix contains co-
occurrence frequencies of words cross-classified
by dependency-based features, the second matrix
contains co-occurrence frequencies of words cross-
classified by words that appear in the word?s context
window, and the third matrix contains co-occurrence
frequencies of dependency-based features cross-
classified by co-occurring context words. NMF is
then applied to the three matrices, and the separate
factorizations are interleaved (i.e. the results of the
former factorization are used to initialize the factor-
ization of the next matrix). A graphical represen-
tation of the interleaved factorization algorithm is
given in figure 1.
When the factorization is finished, the three dif-
ferent modes (words, window-based context words
and dependency-based features) are all represented
according to a limited number of latent factors.
1014
= xW H
= xV G
= xU F
j
i
s
k
i
j
kAwords xdependency relations
B
words xcontext words
Ccontext words xdependency relations
k k
k
k
i
j
i
s
j
s s
Figure 1: A graphical representation of the interleaved
NMF
The factorization that comes out of the NMF model
can be interpreted probabilistically (Gaussier and
Goutte, 2005; Ding et al, 2008). More specifically,
we can transform the factorization into a standard
latent variable model of the form
p(wi, dj) =
K?
z=1
p(z)p(wi|z)p(dj |z) (4)
by introducing two K ?K diagonal scaling matrices
X and Y, such that Xkk = ?iWik and Ykk =?
jHkj . The factorization WH can then be rewritten
as
WH = (WX?1X)(YY?1H)
= (WX?1)(XY)(Y?1H)
(5)
such that WX?1 represents p(wi|z), (Y?1H)T rep-
resents p(dj |z), and XY represents p(z). Using
Bayes? theorem, it is now straightforward to deter-
mine p(z|wi) and p(z|dj).
p(z|wi) =
p(wi|z)p(z)
p(wi)
(6)
p(z|dj) =
p(dj |z)p(z)
p(dj)
(7)
3.3 Meaning in Context
3.3.1 Overview
Using the results of the factorization model de-
scribed above, we can now adapt a word?s feature vec-
tor according to the context in which it appears. Intu-
itively, the contextual features of the word (i.e. the
window-based context words or dependency-based
context features) pinpoint the important semantic di-
mensions of the particular instance, creating a proba-
bility distribution over latent factors. For a number of
context words of a particular instance, we determine
the probability distribution over latent factors given
the context, p(z|C), as the average of the context
words? probability distributions over latent factors
(equation 8).
p(z|C) =
?
wi?C p(z|wi)
|C| (8)
The probability distribution over latent factors
given a number of dependency-based context features
can be computed in a similar fashion, replacing wi
with dj . Additionally, this step allows us to combine
both windows-based context words and dependency-
based context features in order to determine the latent
probability distribution (e.g. by taking a linear com-
bination).
The resulting probability distribution over latent
factors can be interpreted as a semantic fingerprint of
the passage in which the target word appears. Using
this fingerprint, we can now determine a new prob-
ability distribution over dependency features given
the context.
p(d|C) = p(z|C)p(d|z) (9)
The last step is to weight the original probability
vector of the word according to the probability vector
of the dependency features given the word?s context,
by taking the pointwise multiplication of probability
vectors p(d|wi) and p(d|C).
p(d|wi, C) = p(d|wi) ? p(d|C) (10)
Note that this final step is a crucial one in our ap-
proach. We do not just build a model based on latent
factors, but we use the latent factors to determine
which of the features in the original word vector are
the salient ones given a particular context. This al-
lows us to compute an accurate adaptation of the
original word vector in context.
Also note the resemblance to Mitchell and Lap-
ata?s best scoring vector composition model which,
likewise, uses pointwise multiplication. However,
1015
the model presented here has two advantages. First
of all, it allows to take multiple context features into
account, each of which contributes to the probability
distribution over latent factors. Secondly, the target
word and its features do not need to live in the same
vector space (i.e. they do not need to be defined ac-
cording to the same features), as the connections and
the appropriate weightings are determined through
the latent model.
3.3.2 Example
Let us exemplify the procedure with an example.
Say we want to compute the distributionally similar
words to the noun record in the context of example
sentences (3) and (4).
(3) Jack is listening to a record.
(4) Jill updated the record.
First, we extract the context features for both in-
stances, in this case C1 = {listen?1prep(to)} for sen-
tence (3), and C2 = {update?1obj} for sentence (4).1Next, we compute p(z|C1) and p(z|C2) ? the proba-
bility distributions over latent factors given the con-
text ? by averaging over the latent probability dis-
tributions of the individual context features.2 Using
these probability distributions over latent factors, we
can now determine the probability of each depen-
dency feature given the different contexts ? p(d|C1)
and p(d|C2).
The former step yields a general probability dis-
tribution over dependency features that tells us how
likely a particular dependency feature is given the
context that our target word appears in. Our last step
is now to weight the original probability vector of
the target word (the aggregate of dependency-based
context features over all contexts of the target word)
according to the new distribution given the context
in which the target word appears. For the first sen-
tence, features associated with the music sense of
record (or more specifically, the dependency features
associated with latent factors that are related to the
feature {listen?1prep(to)}) will be emphasized, while
1In this example we use dependency features, but the compu-
tations are similar for window-based context words.
2In this case, the sets of context features contain only one
item, so the average probability distribution of the sets is just the
latent probability distribution of their respective item.
features associated with unrelated latent factors are
leveled out. For the second sentence, features that
are associated with the administrative sense of record
(dependency features associated with latent factors
that are related to the feature {update?1obj}) are em-phasized, while unrelated features are played down.
We can now return to our original matrix A and
compute the top similar words for the two adapted
vectors of record given the different contexts, which
yields the results presented below.
1. recordN , C1: album, song, recording, track, cd
2. recordN , C2: file, datum, document, database,
list
4 Evaluation
In this section, we present a thorough evaluation of
the method described above, and compare it with
related methods for meaning computation in context.
In order to test the applicability of the method to
multiple languages, we present evaluation results for
both English and French.
4.1 Datasets
For English, we make use of the SEMEVAL 2007 En-
glish Lexical Substitution task (McCarthy and Nav-
igli, 2007; McCarthy and Navigli, 2009). The task?s
goal is to find suitable substitutes for a target word in
a particular context. The complete data set contains
200 target words (about 50 for each part of speech,
viz. nouns, verbs, adjectives, and adverbs). Each
target word occurs in 10 different sentences, which
yields a total of 2000 sentences. Five annotators pro-
vided suitable substitutes for each target word in the
different contexts.
For French, we developed a small-scale lexical sub-
stitution task ourselves, closely following the guide-
lines of the original English task. We manually se-
lected 10 ambiguous French nouns, and for each noun
we selected 10 different sentences from the FRWaC
corpus (Baroni et al, 2009). Four different native
French speakers were then asked to provide suitable
substitutes for the nouns in context.3
3The task is provided as supplementary material to this paper;
it is also available from the first author?s website.
1016
4.2 Implementational details
The model for English has been trained on part of the
UKWaC corpus (Baroni et al, 2009), covering about
500M words. The corpus has been part of speech
tagged and lemmatized with Stanford Part-Of-Speech
Tagger (Toutanova and Manning, 2000; Toutanova
et al, 2003), and parsed with MaltParser (Nivre et
al., 2006) trained on sections 2-21 of the Wall Street
Journal section of the Penn Treebank extended with
about 4000 questions from the QuestionBank4, so
that dependency triples could be extracted. The sen-
tences of the English lexical substitution task have
been tagged, lemmatized and parsed in the same way.
The model for French has been trained on the French
version of Wikipedia (? 100M words), parsed with
the FRMG parser (Villemonte de La Clergerie, 2010)
for French.
For English, we built different models for each
part of speech (nouns, verbs, adjectives and adverbs),
which yields four models in total. For each model, the
matrices needed for our interleaved NMF factoriza-
tion are extracted from the corpus. The noun model,
for example, was built using 5K nouns, 80K depen-
dency relations, and 2K context words5 (excluding
stop words) with highest frequency in the training
set, which yields matrices of 5K nouns ? 80K de-
pendency relations, 5K nouns ? 2K context words,
and 80K dependency relations ? 2K context words.
The models for the three other parts of speech were
constructed in a similar vein. For French, we only
constructed a model for nouns, as our lexical substi-
tution task for French is limited to this part of speech.
The interleaved NMF model was carried out using
K = 600 (the number of factorized dimensions in
the model), and applying 100 iterations.6 The inter-
leaved NMF algorithm was implemented in Matlab;
the preprocessing scripts and scripts for vector com-
putation in context were written in Python. Cosine
was used as a similarity measure.
4http://maltparser.org/mco/english_
parser/engmalt.html
5We used a fairly large, paragraph-like window of four sen-
tences.
6We experimented with different values (in the range 300?
1500) for K, but the models did not seem to improve much
beyond K = 600; hence, we stuck with 600 factors, due to
speed and memory advantages of a lower number of factors.
4.3 Measures
Up till now, most researchers have interpreted the
lexical substitution task as a ranking problem, in
which the possible substitutes are given beforehand
and the goal is to rank the substitutes according to
their suitability in a particular context, so that sound
substitutes are given a higher rank than their non-
suitable counterparts. This means that all possible
substitutes for a given target word (extracted from the
gold standard) are lumped together, and the system
then has to produce a ranking for the complete set of
substitutes.
We also adopt this approach in our evaluation
framework, but we complement it with the original
evaluation measures of the lexical substitution task,
in which the system is not given a list of possible sub-
stitutes beforehand, but has to come up with the suit-
able candidates itself. This is a much harder task, but
we believe that such an approach is more compelling
in assessing the system?s ability to induce a proper
meaning representation for word usage in context.
We coin the former approach paraphrase ranking,
and the latter one paraphrase induction. In the next
paragraphs, we will describe the actual evaluation
measures that have been used for both approaches.
Paraphrase ranking Following Dinu and Lapata
(2010), we compare the ranking produced by our
model with the gold standard ranking using Kendall?s
?b (which is adjusted for ties). For reasons of com-
parison, we also compute general average precision
(GAP, Kishida (2005)), which was used by Erk and
Pado? (2010) and Thater et al (2010) to evaluate their
rankings. Differences between models are tested for
significance using stratified shuffling (Yeh, 2000),
using a standard number of 10000 iterations.
We compare the results for paraphrase ranking to
two different baselines. The first baseline is a ran-
dom one, in which the gold standard is compared
to an arbitrary ranking. The second baseline is a
dependency-based vector space model that does not
take the context of the particular instance into ac-
count (and thus returns the same ranking for each
instance of the target word). This is a fairly competi-
tive baseline, as noted by other researchers (Erk and
Pado?, 2008; Thater et al, 2009; Dinu and Lapata,
2010).
1017
Paraphrase induction To evaluate the system?s
ability to come up with suitable substitutes from
scratch, we use the measures designed to evaluate
systems that took part in the original English lexical
substitution task (McCarthy and Navigli, 2007). Two
different measures were used, which were coined
best and out-of-ten (oot). The strict best measure
allows the system to give as many candidate substi-
tutes as it considers appropriate, but the credit for
each correct substitute is divided by the total number
of guesses. Recall is then calculated as the average
annotator response frequency of substitutes found by
the system over all items T.
Rbest =
?
s?M?G f(s)
|M | ??s?G f(s)
(11)
whereM is the system?s candidate list7,G is the gold-
standard data, and f(s) is the annotator response
frequency of the candidate.
The out-of-ten measure is more liberal; it allows
the system to give up to ten substitutes, and the credit
for each correct substitute is not divided by the total
number of guesses. The more liberal measure was
introduced to account for the fact that the lexical
substitution task?s gold standard is susceptible to a
considerate amount of variation, and there is only a
limited number of annotators.
P10 =
?
s?M?G f(s)?
s?G f(s)
(12)
where M is the system?s list of 10 candidates, and
G and f(s) are the same as above. Because we only
use the best guess with Rbest, the two measures are
exactly the same except for the number of candidates
M .
4.4 Results
4.4.1 English
Table 1 presents the paraphrase ranking results of
our approach, comparing them to the two baselines
and to a number of previous approaches to meaning
computation in context.
The first two models represent our baselines. The
first baseline is the random baseline, where the can-
didate substitutes are ranked randomly (?b close to
7In our evaluations, we calculate best using the system?s best
guess only, so the candidate list contains only one item.
model ?b GAP
random -0.61 29.98
vectordep 16.57 45.08
EP09 ? 32.2 H
EP10 ? 39.9 H
TFP ? 45.94H
DL 16.56 41.68
NMFcontext 20.64?? 47.60??
NMFdep 22.49?? 48.97??
NMFc+d 22.59?? 49.02??
Table 1: Kendall?s ?b and GAP paraphrase ranking scores
for the English lexical substitution task. Scores marked
with ?H? are copied from the authors? respective papers.
Scores marked with ???? are statistically significant with
p < 0.01 compared to the second baseline.
zero indicates that there is no correlation). The sec-
ond baseline is a standard dependency-based vector
space model, which yields the same ranking for all
instances of a target word. Note that the second base-
line is a rather competitive one.
The next four models represent previous ap-
proaches to meaning computation in context. EP09
is Erk and Pado?s (2009) selectional preference ap-
proach; EP10 is Erk and Pado?s (2010) exemplar-
based approach; TFP stands for Thater et al?s (2010)
approach; and DL is Dinu and Lapata?s (2010) latent
modeling approach. The results are reproduced from
their respective papers, except for Dinu and Lapata?s
approach, which we reimplemented ourselves.8 Note
that the reproduced results (EP09, EP10 and TFP) are
not entirely comparable, because the authors only use
a subset of the lexical substitution task.
The last three models are instantiations of our ap-
proach: NMFcontext is a model that uses window-
based context features, NMFdep is a model that uses
dependency-based context features, and NMFc+d is
a model that uses a linear combination of window-
based and dependency-based context features, giving
equal weight to both.
The three instantiations of our approach reach bet-
ter results than all previous approaches. Moreover,
our approach is the only one able to significantly
8The original paper reports a slightly lower ?b of 16.01 for
their best scoring model.
1018
beat our second (competitive) baseline of a stan-
dard dependency-based vector model. Comparing
our three instantiations, the model that combines
window-based context and dependency-based con-
text scores best, closely followed by the dependency-
based model. The model that only uses window-
based context gets the lowest score of the three, but
is still fairly competitive compared to the previous
approaches. The differences between the models are
statistically significant (p < 0.01), except for the
difference between NMFdep and NMFc+d.
model n v a r
vectordep 15.85 11.68 16.71 25.29
NMFcontext 20.58 16.24 21.00 27.22
NMFdep 21.96 17.33 24.57 28.16
NMFc+d 22.68 17.47 23.84 28.66
Table 2: Kendall?s ?b paraphrase ranking scores for the
English lexical substitution task across different parts of
speech
Table 2 shows the performance of the three model
instantiations on paraphrase ranking across different
parts of speech. The results largely confirm tenden-
cies reported by other researchers (cfr. Dinu and
Lapata (2010)), viz. that verbs are the most difficult,
followed by nouns and adjectives. These parts of
speech also benefit the most from the use of a contex-
tualized model. Adverbs are easier, but there is less
to be gained from using contextualized models.
model Rbest P10
vectordep 8.78 30.21
DL 1.06 7.59
KU 20.65 46.15
IRST2 20.33 68.90
NMFcontext 8.81 30.49
NMFdep 7.73 26.92
NMFc+d 8.96 29.26
Table 3: Rbest and P10 paraphrase induction scores for
the English lexical substitution task
Table 3 shows the performance of the different
models on the paraphrase induction task. Note
once again that our baseline vectordep ? a simple
dependency-based vector space model ? is a highly
competitive one. NMFcontext and NMFc+d are able to
reach marginally better results, but the differences are
not statistically significant. However, all of our mod-
els are able to reach much better results than Dinu
and Lapata?s approach. The results indicate that our
approach, after vector adaptation in context, is still
able to provide accurate similarity calculations across
the complete word space. While other algorithms are
able to rank candidate substitutes at the expense of
accurate similarity calculations, our approach is able
to do both. This is one of the important advantages
of our approach.
For reasons of comparison, we also included the
scores of the best performing models that partici-
pated in the SEMEVAL 2007 lexical substitution task
(KU (Yuret, 2007) and IRST2 (Giuliano et al, 2007),
which got the best scores for Rbest and P10, respec-
tively). These models reach better scores compared
to our models. Note, however, that all participants
of the SEMEVAL 2007 lexical substitution task relied
on a predefined sense inventory (i.e. WordNet, or
a machine readable thesaurus). Our system, on the
other hand, induces paraphrases in a fully unsuper-
vised way. To our knowledge, this is the first time a
fully unsupervised system is tested on the paraphrase
induction task.
model n v a r
vectordep 31.66 23.53 29.91 38.43
NMFcontext 33.73?? 25.21? 28.58 36.45
NMFdep 31.40 25.97?? 20.56 31.48
NMFc+d 33.37? 25.99?? 24.20 35.81
Table 4: P10 paraphrase induction scores for the English
lexical substitution task across different parts of speech.
Scores marked with ???? and ??? are statistically significant
with respectively p < 0.01 and p < 0.05 compared to the
baseline.
Table 4 presents the results for paraphrase induc-
tion (oot) across the different parts of speech. The
results indicate that paraphrase induction works best
for nouns and verbs, with statistically significant im-
provements over the baseline. The differences among
the models themselves are not significant. Adjectives
and adverbs yield lower scores, indicating that their
1019
contextualization yields less precise vectors for mean-
ing computation. Note, however, that the NMFcontext
model is still quite apt for meaning computation,
yielding results that are only slightly lower than the
dependency-based vector space model.
4.4.2 French
This section presents the results on the French lex-
ical substitution task. Table 5 presents the results for
paraphrase ranking, while table 6 shows the models?
performance on the paraphrase induction task.
model Kendall?s ?b GAP
vectordep 7.79 36.46
DL 17.99 41.73
NMFcontext 18.63 44.96
NMFdep 17.15 44.66
NMFc+d 18.40 43.14
Table 5: Kendall?s ?b and GAP paraphrase ranking scores
for the French lexical substitution task
The results for paraphrase ranking in French (ta-
ble 5) show similar tendencies as the results for En-
glish: all of our models are able to improve signifi-
cantly over the dependency-based vector space base-
line. Note, however, thar our models generally score
a bit lower compared to the English results. This drop
in performance is not present for Dinu and Lapata?s
model. The difference might be due to the differ-
ence in corpora size: for the method to operate at full
power, we need to make a good estimate of the co-
occurrences of three modes (words, window-based
context words and dependency-based features), and
thus our methods requires a significant amount of
data. Nevertheless, our approach still yields the best
results, with NMFcontext as the best scoring model.
Finally, the results for paraphrase induction in
French (table 6) interestingly show a significant and
large improvement over the baseline. The improve-
ments indicate once again that the models are able
to carry out precise similarity computations over the
whole word space, while at the same time providing
an adequately adapted contextualized meaning vector.
Dinu and Lapata?s model, which performs similarity
calculations in the latent space, is not able to provide
accurate word vectors, and thus perform worse at the
paraphrase induction task.
model Rbest P10
vectordep 6.38 24.43
DL 0.50 5.34
NMFcontext 10.71 31.42
NMFdep 9.65 28.52
NMFc+d 10.64 35.32
Table 6: Rbest and P10 paraphrase induction scores for
the French lexical substitution task
5 Conclusion
In this paper, we presented a novel method for the
modeling of word meaning in context. We make use
of a factorization model based on non-negative ma-
trix factorization, in which words, together with their
window-based context words and their dependency
relations, are linked to latent dimensions. The factor-
ization model allows us to determine which particular
dimensions are important for a target word in a partic-
ular context. A key feature of the algorithm is that we
adapt the original dependency-based feature vector
of the target word through the latent semantic space.
By doing so, our model is able to make accurate simi-
larity calculations for word meaning in context across
the whole word space. Our evaluation shows that the
approach presented here is able to improve upon the
state-of-the art performance on paraphrase ranking.
Moreover, our approach scores well for both para-
phrase ranking and paraphrase induction, whereas
previous approaches only seem capable of improving
performance on the former task at the expense of the
latter.
During our research, a number of topics surfaced
that we consider worth exploring in the future. First
of all, we would like to further investigate the opti-
mal configuration for combining window-based and
dependency-based contexts. At the moment, the per-
formance of the combined model does not yield a
uniform picture. The results might improve further
if window-based context and dependency-based con-
text are combined in an optimal way. Secondly, we
would like to subject our approach to further evalu-
ation, in particular on a number of different evalua-
tion tasks, such as semantic compositionality. And
thirdly, we would like to transfer the general idea
of the approach presented in this paper to a tensor-
1020
based framework (which is able to capture the multi-
way co-occurrences of words, together with their
window-based and dependency-based context fea-
tures, in a natural way) and investigate whether such
a framework proves beneficial for the modeling of
word meaning in context.
Acknowledgements
The work reported in this paper was funded by
the Isaac Newton Trust (Cambridge, UK), the
EU FP7 project ?PANACEA?, the EPSRC grant
EP/G051070/1 and the Royal Society (UK).
References
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle, and
Aitor Soroa. 2006. Two graph-based algorithms for
state-of-the-art wsd. In Proceedings of the Empirical
Methods in Natural Language Processing (EMNLP)
Conference, pages 585?593, Sydney, Australia.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evaluation,
43(3):209?226.
Chris Ding, Tao Li, and Wei Peng. 2008. On the equiv-
alence between non-negative matrix factorization and
probabilistic latent semantic indexing. Computational
Statistics & Data Analysis, 52(8):3913?3927.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172, Cambridge,
MA, October.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 440?449, Suntec, Singapore.
Katrin Erk and Sebastian Pado?. 2008. A structured vector
space model for word meaning in context. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 897?906, Waikiki,
Hawaii, USA.
Katrin Erk and Sebastian Pado?. 2009. Paraphrase assess-
ment in structured vector space: Exploring parameters
and datasets. In Proceedings of the Workshop on Geo-
metrical Models of Natural Language Semantics, pages
57?65, Athens, Greece.
Katrin Erk and Sebastian Pado?. 2010. Exemplar-based
models for word meaning in context. In Proceedings of
the ACL 2010 Conference Short Papers, pages 92?97,
Uppsala, Sweden.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord. 2009.
Investigations on word senses and word usages. In
Proceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 10?18.
Katrin Erk. 2010. What is word meaning, really? (and
how can distributional models help us describe it?). In
Proceedings of the 2010 Workshop on GEometrical
Models of Natural Language Semantics, pages 17?26.
Eric Gaussier and Cyril Goutte. 2005. Relation between
PLSA and NMF and implications. In Proceedings of
the 28th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 601?602, Salvador, Brazil.
Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.
2007. Fbk-irst: Lexical substitution task exploiting
domain and syntagmatic coherence. In Proceedings of
the Fourth International Workshop on Semantic Evalu-
ations, pages 145?148.
Zellig S. Harris. 1954. Distributional structure. Word,
10(23):146?162.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, pages
57?60, New York, New York, USA.
Nancy Ide and Yorick Wilks. 2006. Making Sense About
Sense. In Word Sense Disambiguation: Algorithms
And Applications, chapter 3. Springer, Dordrecht.
Kazuaki Kishida. 2005. Property of average precision and
its generalization: An examination of evaluation indi-
cator for information retrieval experiments. Technical
report, National Institute of Informatics.
Thomas Landauer and Susan Dumais. 1997. A solution
to Plato?s problem: The Latent Semantic Analysis the-
ory of the acquisition, induction, and representation of
knowledge. Psychology Review, 104:211?240.
Thomas Landauer, Peter Foltz, and Darrell Laham. 1998.
An Introduction to Latent Semantic Analysis. Dis-
course Processes, 25:295?284.
Daniel D. Lee and H. Sebastian Seung. 2000. Algorithms
for non-negative matrix factorization. In Advances in
Neural Information Processing Systems 13, pages 556?
562.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguistics
and 17th International Conference on Computational
Linguistics (COLING-ACL98), Volume 2, pages 768?
774, Montreal, Quebec, Canada.
Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 task 10: English lexical substitution task. In
1021
Proceedings of the 4th International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 48?53.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language resources and
evaluation, 43(2):139?159.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. proceedings of ACL-
08: HLT, pages 236?244.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of LREC-2006, pages 2216?
2219.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161?199.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 613?619, Edmonton, Alberta, Canada.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Stefan Thater, Georgiana Dinu, and Manfred Pinkal. 2009.
Ranking paraphrases in context. In Proceedings of the
2009 Workshop on Applied Textual Inference, pages
44?47, Suntec, Singapore.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings of
the 48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 948?957, Uppsala, Sweden.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proceedings of the
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Corpora
(EMNLP/VLC-2000), pages 63?70.
Kristina Toutanova, Dan Klein, Christopher Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech tag-
ging with a cyclic dependency network. In Proceedings
of HLT-NAACL 2003, pages 252?259.
Tim Van de Cruys. 2008. Using three way data for word
sense discrimination. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 929?936, Manchester.
Eric Villemonte de La Clergerie. 2010. Building factor-
ized TAGs with meta-grammars. In Proceedings of
the 10th International Conference on Tree Adjoining
Grammars and Related Formalisms (TAG+10), pages
111?118, New Haven, Connecticut, USA.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th conference on Computational linguistics,
pages 947?953, Saarbru?cken, Germany.
Deniz Yuret. 2007. Ku: Word sense disambiguation by
substitution. In Proceedings of the Fourth International
Workshop on Semantic Evaluations, pages 207?213.
1022
Proceedings of the EACL 2009 Demonstrations Session, pages 5?8,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
CBSEAS, a Summarization System
Integration of Opinion Mining Techniques to Summarize Blogs
Aure?lien Bossard, Michel Ge?ne?reux and Thierry Poibeau
Laboratoire d?Informatique de Paris-Nord
CNRS UMR 7030 and Universite? Paris 13
93430 Villetaneuse ? France
{firstname.lastname}@lipn.univ-paris13.fr
Abstract
In this paper, we present a novel approach
for automatic summarization. Our system,
called CBSEAS, integrates a new method
to detect redundancy at its very core, and
produce more expressive summaries than
previous approaches. Moreover, we show
that our system is versatile enough to in-
tegrate opinion mining techniques, so that
it is capable of producing opinion oriented
summaries. The very competitive results
obtained during the last Text Evaluation
Conference (TAC 2008) show that our ap-
proach is efficient.
1 Introduction
During the past decade, automatic summarization,
supported by evaluation campaigns and a large re-
search community, has shown fast and deep im-
provements. Indeed, the research in this domain is
guided by strong industrial needs: fast processing
despite ever increasing amount of data.
In this paper, we present a novel approach for
automatic summarization. Our system, called CB-
SEAS, integrates a new method to detect redun-
dancy at its very core, and produce more expres-
sive summaries than previous approaches. The
system is flexible enough to produce opinion ori-
ented summaries by accommodating techniques to
mine documents that express different views or
commentaries. The very competitive results ob-
tained during the last Text Evaluation Conference
(TAC 2008) show that our approach is efficient.
This short paper is structured as follows: we
first give a quick overview of the state of the art.
We then describe our system, focusing on the most
important novel features implemented. Lastly, we
give the details of the results obtained on the TAC
2008 Opinion Pilot task.
2 Related works
Interest in creating automatic summaries has be-
gun in the 1950s (Luhn, 1958). (Edmundson and
Wyllys, 1961) proposed features to assign a score
to each sentence of a corpus in order to rank these
sentences. The ones with the highest scores are
kept to produce the summary. The features they
used were sentence position (in a news article for
example, the first sentences are the most impor-
tant), proper names and keywords in the document
title, indicative phrases and sentence length.
Later on, summarizers aimed at eliminating re-
dundancy, especially for multi-documents summa-
rizing purpose. Identifying redundancy is a criti-
cal task, as information appearing several times in
different documents can be qualified as important.
Among recent approaches, the ?centroid-based
summarization? method developed by (Radev et
al., 2004) consists in identifying the centroid
of a cluster of documents, in other words the
terms which best suit the documents to summa-
rize. Then, the sentences to be extracted are
the ones that contain the greatest number of cen-
troids. Radev implemented this method in an on-
line multi-document summarizer, MEAD.
Radev further improved MEAD using a differ-
ent method to extract sentences: ?Graph-based
centrality? extractor (Erkan and Radev, 2004).
It consists in computing similarity between sen-
tences, and then selecting sentences which are
considered as ?central? in a graph where nodes are
sentences and edges are similarities. Sentence se-
lection is then performed by picking the sentences
which have been visited most after a random walk
on the graph.
The last two systems are dealing with redun-
dancy as a post-processing step. (Zhu et al, 2007),
assuming that redundancy should be the concept
on what is based multi-document summarization,
offered a method to deal with redundancy at the
5
same time as sentence selection. For that purpose,
the authors used a ?Markov absorbing chain ran-
dom walk? on a graph representing the different
sentences of the corpus to summarize.
MMR-MD, introduced by Carbonnel in (Car-
bonell and Goldstein, 1998), is a measure which
needs a passage clustering: all passages consid-
ered as synonyms are grouped into the same clus-
ters. MMR-MD takes into account the similarity
to a query, coverage of a passage (clusters that
it belongs to), content in the passage, similarity
to passages already selected for the summary, be-
longing to a cluster or to a document that has al-
ready contributed a passage to the summary.
The problem of this measure lies in the clus-
tering method: in the literature, clustering is gen-
erally fulfilled using a threshold. If a passage
has a similarity to a cluster centroid higher than
a threshold, then it is added to this cluster. This
makes it a supervised clustering method; an unsu-
pervised clustering method is best suited for au-
tomatic summarization, as the corpora we need
to summarize are different from one to another.
Moreover, sentence synonymy is also dependent
on the corpus granularity and on the user compres-
sion requirement.
3 CBSEAS: A Clustering-Based
Sentence Extractor for Automatic
Summarization
We assume that, in multi-document summariza-
tion, redundant pieces of information are the sin-
gle most important element to produce a good
summary. Therefore, the sentences which carry
those pieces of information have to be extracted.
Detecting these sentences conveying the same in-
formation is the first step of our approach. The de-
veloped algorithm first establishes the similarities
between all sentences of the documents to sum-
marize, then applies a clustering algorithm ? fast
global k-means (Lo?pez-Escobar et al, 2006) ? to
the similarity matrix in order to create clusters in
which sentences convey the same information.
First, our system ranks all the sentences accord-
ing to their similarity to the documents centroid.
We have chosen to build up the documents cen-
troid with the m most important terms, their im-
portance being reflected by the tf/idf of each term.
We then select the n2 best ranked sentences to cre-
ate a n sentences long summary. We do so because
the clustering algorithm we use to detect sentences
for all ejinE
C1 ? ej
for i from 1 to k do
for j from 1 to i
center(Cj)? em|emmaximizes
?
eninCj
sim(em, en)
for all ej in E
ej ? Cl|Clmaximizes sim(center(Cl, ej))
add a new cluster: Ci. It initially contains only its
center, the worst represented element in its cluster.
done
Figure 1: Fast global k-means algorithm
conveying the same information, fast global k-
means, behaves better when it has to group n2
elements into n clusters. The similarity with the
centroid is a weighted sum of terms appearing in
both centroid and sentence, normalized by sen-
tence length.
Similarity between sentences is computed using
a variant of the ?Jaccard? measure. If two terms
are not equal, we test their synonymy/hyperonymy
using the Wordnet taxonomy (Fellbaum, 1998). In
case they are synonyms or hyperonym/hyponym,
these terms are taken into account in the similar-
ity calculation, but weighted respectively half and
quarter in order to reflect that term equality is more
important than term semantic relation. We do this
in order to solve the problem pointed out in (Erkan
and Radev, 2004) (synonymy was not taken into
account for sentence similarity measures) and so
to enhance sentence similarity measure. It is cru-
cial to our system based on redundancy location as
redundancy assumption is dependent on sentence
similarities.
Once the similarities are computed, we cluster
the sentences using fast global k-means (descrip-
tion of the algorithm is in figure 1) using the simi-
larity matrix. It works well on a small data set with
a small number of dimensions, although it has not
yet scaled up as well as we would have expected.
This clustering step completed, we select one
sentence per cluster in order to produce a sum-
mary that contains most of the relevant informa-
tion/ideas in the original documents. We do so by
choosing the central sentence in each cluster. The
central sentence is the one which maximizes the
sum of similarities with the other sentences of its
cluster. It should be the one that characterizes best
the cluster in terms of information vehicled.
6
4 TAC 2008: The Opinion
Summarization Task
In order to evaluate our system, we participated
in the Text Analysis Conference (TAC) that pro-
posed in 2008 an opinion summarization task. The
goal is to produce fluent and well-organized sum-
maries of blogs. These summaries are oriented
by complex user queries, such as ?Why do people
like.....?? or ?Why do people prefer... to...??.
The results were analyzed manually, using the
PYRAMID method (Lin et al, 2006): the PYRA-
MID score of a summary depends on the number
of simple semantic units, units considered as im-
portant by the annotators. The TAC evaluation
for this task also included grammaticality, non-
redundancy, structure/coherence and overall flu-
ency scores.
5 CBSEAS Adaptation to the Opinion
Summarization Task
Blog summarization is very different from a
newswire article or a scientific paper summa-
rization. Linguistic quality as well as reason-
ing structure are variable from one blogger to an-
other. We cannot use generalities on blog struc-
ture, neither on linguistic markers to improve
our summarization system. The other problem
with blogs is the noise due to the use of un-
usual language. We had to clean the blogs in a
pre-processing step: sentences with a ratio num-
ber of frequent words/total number of words below
a given threshold (0.35) were deemed too noisy
and discarded. Frequent words are the one hun-
dred most frequent words in the English language
which on average make up approximately half of
written texts (Fry et al, 2000).
Our system, CBSEAS, is a ?standard? summa-
rization system. We had to adapt it in order to
deal with the specific task of summarizing opin-
ions. All sentences from the set of documents to
summarize were tagged following the opinion de-
tected in the blog post they originated from. We
used for that purpose a two-class (positive or neg-
ative) SVM classifier trained on movie reviews.
The idea behind the opinion classifier is to im-
prove summaries by selecting sentences having
the same opinionated polarity as the query, which
were tagged using a SVM trained on the manually
tagged queries from the training data provided ear-
lier in TAC.
As the Opinion Summarization Task was to pro-
duce a query-oriented summary, the sentence pre-
selection was changed, using the user query in-
stead of the documents centroid. We also changed
the sentence pre-selection ranking measure by
weighting terms according to their lexical cate-
gory; we have chosen to give more weight to
proper names than verbs adjectives, adverbs and
nouns. Indeed, opinions we had to summarize
were mostly on products or people.
While experimenting our system on TAC 2008
training data, we noticed that extracting sentences
which are closest to their cluster center was not
satisfactory. Some other sentences in the same
cluster were best fitted to a query-oriented sum-
mary. We added the sentence ranking used for the
sentence pre-selection to the final sentence extrac-
tor. Each sentence is given a score which is the
distance to the cluster center times the similarity
to the query.
6 TAC 2008 Results on Opinion
Summarization Task
Participants to the Opinion Summarization Task
were allowed to use extra-information given by
TAC organizers. These pieces of information are
called snippets. The snippets contain the relevant
information, and could be used as a stand-alone
dataset. Participants were classified into two dif-
ferent groups: one for those who did not use snip-
pets, and one for those who did. We did not use
snippets at all, as it is a more realistic challenge
to look directly at the blogs with no external help.
The results we present here are those of the partic-
ipants that were not using snippets. Indeed, sys-
tems using snippets obtained much higher scores
than the other systems. We cannot compare our
system to systems using snippets.
Our system obtained quite good results on
the ?opinion task?: the scores can be found on
figure 2. As one can see, our responsiveness
scores are low compared to the others (responsive-
ness score corresponds to the following question:
?How much would you pay for that summary??).
We suppose that despite the grammaticality, flu-
ency and pyramid scores of our summaries, judges
gave a bad responsiveness score to our summaries
because they are too long: we made the choice
to produce summaries with a compression rate of
10% when it was possible, the maximum length
authorized otherwise.
7
Evaluation CBSEAS Mean Best Worst Rank
Pyramid .169 .151 .251 .101 5/20
Grammatic. 5.95 5.14 7.54 3.54 3/20
Non-redun. 6.64 5.88 7.91 4.36 4/20
Structure 3.50 2.68 3.59 2.04 2/20
Fluency 4.45 3.43 5.32 2.64 2/20
Responsiv. 2.64 2.61 5.77 1.68 8/20
Figure 2: Opinion task overall results
Figure 3: Opinion task results
However, we noticed that the quality of our
summaries was very erratic. We assume this is
due to the length of our summaries, as the longest
summaries are the ones which get the worst scores
in terms of pyramid f-score (fig 3). The length of
the summaries is a ratio of the original documents
length. The quality of the summaries would be
decreasing while the number of input sentences is
increasing.
Solutions to fix this problem could be:
? Define a better score for the correspondence
to a user query and remove sentences which
are under a threshold;
? Extract sentences from the clusters that con-
tain more than a predefined number of ele-
ments only.
This would result in improving the pertinence
of the extracted sentences. The users reading the
summaries would also be less disturbed by the
large amount of sentences a too long summary
provides. As the ?opinion summarization? task
was evaluated manually and reflects well the qual-
ity of a summary for an operational use, the con-
clusions of this evaluation are good indicators of
the quality of the summaries produced by our sys-
tem.
7 Conclusion
We presented here a new approach for multi-
document summarization. It uses an unsuper-
vised clustering method to group semantically re-
lated sentences together. It can be compared to
approaches using sentence neighbourhood (Erkan
and Radev, 2004), because the sentences which are
highly related to the highest number of sentences
are those which will be extracted first. How-
ever, our approach is different since sentence se-
lection is directly dependent on redundancy loca-
tion. Also, redundancy elimination, which is cru-
cial in multi-document summarization, takes place
in the same step as sentence selection.
References
Jaime Carbonell and Jade Goldstein. 1998. The use
of MMR, diversity-based reranking for reordering
documents and producing summaries. In SIGIR?98,
pages 335?336, New York, NY, USA. ACM.
Harold P. Edmundson and Ronald E. Wyllys. 1961.
Automatic abstracting and indexing?survey and
recommendations. Commun. ACM, 4(5):226?234.
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Edward Bernard Fry, Jacqueline E. Kress, and
Dona Lee Fountoukidis. 2000. The Reading Teach-
ers Book of Lists. Jossey-Bass, 4th edition.
Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and Jian-
Yun Nie. 2006. An information-theoretic approach
to automatic evaluation of summaries. In Proceed-
ings of HLT-NAACL, pages 463?470, Morristown,
NJ, USA.
Sau?l Lo?pez-Escobar, Jesu?s Ariel Carrasco-Ochoa, and
Jose? Francisco Mart??nez Trinidad. 2006. Fast
global -means with similarity functions algorithm.
In IDEAL, volume 4224 of Springer, Lecture Notes
in Computer Science, pages 512?521.
H.P. Luhn. 1958. The automatic creation of literature
abstracts. IBM Journal, 2(2):159?165.
Dragomir Radev et al 2004. MEAD - a platform for
multidocument multilingual text summarization. In
Proceedings of LREC 2004, Lisbon, Portugal.
Xiaojin Zhu, Andrew Goldberg, Jurgen Van Gael, and
David Andrzejewski. 2007. Improving diversity
in ranking using absorbing random walks. In Pro-
ceedings of HLT-NAACL, pages 97?104, Rochester,
USA.
8
Proceedings of NAACL-HLT 2013, pages 1142?1151,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A Tensor-based Factorization Model of Semantic Compositionality
Tim Van de Cruys
IRIT ? UMR 5505
CNRS
Toulouse, France
tim.vandecruys@irit.fr
Thierry Poibeau?
LaTTiCe ? UMR 8094
CNRS & ENS
Paris, France
thierry.poibeau@ens.fr
Anna Korhonen
Computer Laboratory & DTAL?
University of Cambridge
United Kingdom
anna.korhonen@cl.cam.ac.uk
Abstract
In this paper, we present a novel method for the
computation of compositionality within a distri-
butional framework. The key idea is that com-
positionality is modeled as a multi-way interac-
tion between latent factors, which are automat-
ically constructed from corpus data. We use
our method to model the composition of sub-
ject verb object triples. The method consists
of two steps. First, we compute a latent factor
model for nouns from standard co-occurrence
data. Next, the latent factors are used to induce
a latent model of three-way subject verb object
interactions. Our model has been evaluated on
a similarity task for transitive phrases, in which
it exceeds the state of the art.
1 Introduction
In the course of the last two decades, significant
progress has been made with regard to the automatic
extraction of lexical semantic knowledge from large-
scale text corpora. Most work relies on the distribu-
tional hypothesis of meaning (Harris, 1954), which
states that words that appear within the same contexts
tend to be semantically similar. A large number of
researchers have taken this dictum to heart, giving
rise to a plethora of algorithms that try to capture
the semantics of words by looking at their distribu-
tion in text. Up till now, however, most work on the
automatic acquisition of semantics only deals with
individual words. The modeling of meaning beyond
the level of individual words ? i.e. the combination
of words into larger units ? is to a large degree left
unexplored.
The principle of compositionality, often attributed
to Frege, is the principle that states that the meaning
of a complex expression is a function of the meaning
of its parts and the way those parts are (syntactically)
combined (Frege, 1892). It is the fundamental prin-
ciple that allows language users to understand the
meaning of sentences they have never heard before,
by constructing the meaning of the complex expres-
sion from the meanings of the individual words. Re-
cently, a number of researchers have tried to reconcile
the framework of distributional semantics with the
principle of compositionality (Mitchell and Lapata,
2008; Baroni and Zamparelli, 2010; Coecke et al,
2010; Socher et al, 2012). However, the absolute
gains of the systems remain a bit unclear, and a sim-
ple method of composition ? vector multiplication ?
often seems to produce the best results (Blacoe and
Lapata, 2012).
In this paper, we present a novel method for the
joint composition of a verb with its subject and di-
rect object. The key idea is that compositionality is
modeled as a multi-way interaction between latent
factors, which are automatically constructed from
corpus data. In order to adequately model the multi-
way interaction between a verb and its subject and
objects, a significant part of our method relies on
tensor algebra. Additionally, our method makes use
of a factorization model appropriate for tensors.
The remainder of the paper is structured as follows.
In section 2, we give an overview of previous work
that is relevant to the task of computing composition-
ality within a distributional framework. Section 3
presents a detailed description of our method, in-
cluding an overview of the necessary mathematical
1142
machinery. Section 4 illustrates our method with a
number of detailed examples. Section 5 presents a
quantitative evaluation, and compares our method
to other models of distributional compositionality.
Section 6, then, concludes and lays out a number of
directions for future work.
2 Previous Work
In recent years, a number of methods have been de-
veloped that try to capture compositional phenomena
within a distributional framework. One of the first
approaches to tackle compositional phenomena in a
systematic way is Mitchell and Lapata?s (2008) ap-
proach. They explore a number of different models
for vector composition, of which vector addition (the
sum of each feature) and vector multiplication (the
elementwise multiplication of each feature) are the
most important. They evaluate their models on a
noun-verb phrase similarity task, and find that the
multiplicative model yields the best results, along
with a weighted combination of the additive and mul-
tiplicative model.
Baroni and Zamparelli (2010) present a method
for the composition of adjectives and nouns. In their
model, an adjective is a linear function of one vector
(the noun vector) to another vector (the vector for the
adjective-noun pair). The linear transformation for a
particular adjective is represented by a matrix, and
is learned automatically from a corpus, using partial
least-squares regression.
Coecke et al (2010) present an abstract theoreti-
cal framework in which a sentence vector is a func-
tion of the Kronecker product of its word vectors,
which allows for greater interaction between the dif-
ferent word features. A number of instantiations of
the framework are tested experimentally in Grefen-
stette and Sadrzadeh (2011a) and Grefenstette and
Sadrzadeh (2011b). The key idea is that relational
words (e.g. adjectives or verbs) have a rich (multi-
dimensional) structure that acts as a filter on their
arguments. Our model uses an intuition similar to
theirs.
Socher et al (2012) present a model for composi-
tionality based on recursive neural networks. Each
node in a parse tree is assigned both a vector and
a matrix; the vector captures the actual meaning of
the constituent, while the matrix models the way
it changes the meaning of neighbouring words and
phrases.
Closely related to the work on compositionality
is research on the computation of word meaning in
context. Erk and Pado? (2008, 2009) make use of
selectional preferences to express the meaning of a
word in context; the meaning of a word in the pres-
ence of an argument is computed by multiplying the
word?s vector with a vector that captures the inverse
selectional preferences of the argument. Thater et
al. (2009, 2010) extend the approach based on se-
lectional preferences by incorporating second-order
co-occurrences in their model. And Dinu and La-
pata (2010) propose a probabilistic framework that
models the meaning of words as a probability distri-
bution over latent factors. This allows them to model
contextualized meaning as a change in the original
sense distribution. Dinu and Lapata use non-negative
matrix factorization (NMF) to induce latent factors.
Similar to their work, our model uses NMF ? albeit
in a slightly different configuration ? as a first step
towards our final factorization model.
In general, latent models have proven to be useful
for the modeling of word meaning. One of the best
known latent models of semantics is Latent Seman-
tic Analysis (Landauer and Dumais, 1997), which
uses singular value decomposition in order to auto-
matically induce latent factors from term-document
matrices. Another well known latent model of mean-
ing, which takes a generative approach, is Latent
Dirichlet Allocation (Blei et al, 2003).
Tensor factorization has been used before for the
modeling of natural language. Giesbrecht (2010)
describes a tensor factorization model for the con-
struction of a distributional model that is sensitive to
word order. And Van de Cruys (2010) uses a tensor
factorization model in order to construct a three-way
selectional preference model of verbs, subjects, and
objects. Our underlying tensor factorization ? Tucker
decomposition ? is the same as Giesbrecht?s; and
similar to Van de Cruys (2010), we construct a la-
tent model of verb, subject, and object interactions.
The way our model is constructed, however, is sig-
nificantly different. The former research does not
use any syntactic information for the construction
of the tensor, while the latter makes use of a more
restricted tensor factorization model, viz. parallel
factor analysis (Harshman and Lundy, 1994).
1143
The idea of modeling compositionality by means
of tensor (Kronecker) product has been proposed
in the literature before (Clark and Pulman, 2007;
Coecke et al, 2010). However, the method presented
here is the first that tries to capture compositional
phenomena by exploiting the multi-way interactions
between latent factors, induced by a suitable tensor
factorization model.
3 Methodology
3.1 Mathematical preliminaries
The methodology presented in this paper requires
a number of concepts and mathematical operations
from tensor algebra, which are briefly reviewed in
this section. The interested reader is referred to Kolda
and Bader (2009) for a more thorough introduction
to tensor algebra (including an overview of various
factorization methods).
A tensor is a multidimensional array; it is the gen-
eralization of a matrix to more than two dimensions,
or modes. Whereas matrices are only able to cap-
ture two-way co-occurrences, tensors are able to cap-
ture multi-way co-occurrences.1 Following prevail-
ing convention, tensors are represented by boldface
Euler script notation (X), matrices by boldface capi-
tal letters (X), vectors by boldface lower case letters
(x), and scalars by italic letters (x).
The n-mode product of a tensor X ? RI1?I2?...?IN
with a matrix U ? RJ?In is denoted by X?n U, and
is defined elementwise as
(X?n U)i1...in?1 jin+1...iN =
In
?
in=1
xi1i2...iN u jin (1)
The Kronecker product of matrices A ? RI?J and
B?RK?L is denoted by A?B. The result is a matrix
of size (IK)? (JL), and is defined by
A?B =
?
?
?
?
?
a11B a12B ? ? ? a1JB
a21B a22B ? ? ? a2JB
...
...
. . .
...
aI1B aI2B . . . aIJB
?
?
?
?
?
(2)
1In this research, we limit ourselves to three-way co-
occurrences of verbs, subject, and objects, modelled using a
three-mode tensor.
A special case of the Kronecker product is the
outer product of two vectors a ? RI and b ? RJ , de-
noted a?b. The result is a matrix A ? RI?J obtained
by multiplying each element of a with each element
of b.
Finally, the Hadamard product, denoted A ?B,
is the elementwise multiplication of two matrices
A ? RI?J and B ? RI?J , which produces a matrix
that is equally of size I? J.
3.2 The construction of latent noun factors
The first step of our method consists in the construc-
tion of a latent factor model for nouns, based on their
context words. For this purpose, we make use of non-
negative matrix factorization (Lee and Seung, 2000).
Non-negative matrix factorization (NMF) minimizes
an objective function ? in our case the Kullback-
Leibler (KL) divergence ? between an original matrix
VI?J and WI?KHK?J (the matrix multiplication of
matrices W and H) subject to the constraint that all
values in the three matrices be non-negative. Param-
eter K is set  I,J so that a reduction is obtained
over the original data. The factorization model is
represented graphically in figure 1.
= xV W H
k
k
noun
s
context words
noun
s
context words
Figure 1: Graphical representation of NMF
NMF can be computed fairly straightforwardly,
alternating between the two iterative update rules
represented in equations 3 and 4. The update rules
are guaranteed to converge to a local minimum in the
KL divergence.
Ha? ?Ha?
?i Wia
Vi?
(WH)i?
?k Wka
(3)
Wia?Wia
?? Ha?
Vi?
(WH)i?
?v Hav
(4)
3.3 Modeling multi-way interactions
In our second step, we construct a multi-way interac-
tion model for subject verb object (svo) triples, based
1144
on the latent factors induced in the first step. Our
latent interaction model is inspired by a tensor factor-
ization model called Tucker decomposition (Tucker,
1966), although our own model instantiation differs
significantly. In order to explain our method, we
first revisit Tucker decomposition, and subsequently
explain how our model is constructed.
3.3.1 Tucker decomposition
Tucker decomposition is a multilinear generaliza-
tion of the well-known singular value decomposition,
used in Latent Semantic Analysis. It is also known as
higher order singular value decomposition (HOSVD,
De Lathauwer et al (2000)). In Tucker decomposi-
tion, a tensor is decomposed into a core tensor, multi-
plied by a matrix along each mode. For a three-mode
tensor X ? RI?J?L, the model is defined as
X = G?1 A?2 B?3 C (5)
=
P
?
p=1
Q
?
q=1
R
?
r=1
gpqrap ?bq ? cr (6)
Setting P,Q,R I,J,L, the core tensor G repre-
sents a compressed, latent version of the original ten-
sor X; matrices A ?RI?P, B ?RJ?Q, and C ?RL?R
represent the latent factors for each mode, while
G ? RP?Q?R indicates the level of interaction be-
tween the different latent factors. Figure 2 shows a
graphical representation of Tucker decomposition.2
subjects
verb
s
object
s
=
object
s
k
k
k
verb
s
subjects
k
k
k
Figure 2: A graphical representation of Tucker decompo-
sition
2where P = Q = R = K, i.e. the same number of latent factors
K is used for each mode
3.3.2 Reconstructing a Tucker model from
two-way factors
Computing the Tucker decomposition of a tensor
is rather costly in terms of time and memory require-
ments. Moreover, the decomposition is not unique:
the core tensor G can be modified without affecting
the model?s fit by applying the inverse modification
to the factor matrices. These two drawbacks led us
to consider an alternative method for the construc-
tion of the Tucker model. Specifically, we consider
the factor matrices as given (as the output from our
first step), and proceed to compute the core tensor G.
Additionally, we do not use a latent representation
for the first mode, which means that the first mode is
represented by its original instances.
Our model can be straightforwardly applied to lan-
guage data. The core tensor G models the latent
interactions between verbs, subject, and objects. G
is computed by applying the n-mode product to the
appropriate mode of the original tensor (equation 7),
G=X?2 WT ?3 WT (7)
where XV?N?N is our original data tensor, consisting
of the weighted co-occurrence frequencies of svo
triples (extracted from corpus data), and WN?K is
our latent factor matrix for nouns. Note that we do
not use a latent representation for the verb mode. To
be able to efficiently compute the similarity of verbs
(both within and outside of compositional phrases),
only the subject and object mode are represented by
latent factors, while the verb mode is represented
by its original instances. This means that our core
tensor G will be of size V ?K?K.3 A graphical
representation is given in figure 3.
Note that both tensor X and factor matrices W are
non-negative, which means our core tensor G will
also be non-negative.
3.4 The composition of svo triples
In order to compute the composition of a particular
subject verb object triple ?s,v,o?, we first extract the
appropriate subject vector ws and object vector wo
(both of length K) from our factor matrix W, and
3It is straightforward to also construct a latent factor model
for verbs using NMF, and include it in the construction of our
core tensor; we believe such a model might have interesting
applications, but we save this as an exploration for future work.
1145
subjects
verb
s
object
s
=
object
s
k
k
verb
s
subjectskk
Figure 3: A graphical representation of our model instan-
tiation without the latent verb mode
compute the outer product of both vectors, resulting
in a matrix Y of size K?K.
Y = ws ?wo (8)
Our second and final step is then to weight the
original verb matrix Gv of latent interactions (the
appropriate verb slice of tensor G) with matrix Y,
containing the latent interactions of the specific sub-
ject and object. This is carried out by taking the
Hadamard product of Gv and Y.
Z = Gv ?Y (9)
4 Example
In this section, we present a number of example com-
putations that clarify how our model is able to capture
compositionality. All examples come from actual cor-
pus data, and are computed in a fully automatic and
unsupervised way.
Consider the following two sentences:
(1) The athlete runs a race.
(2) The user runs a command.
Both sentences contain the verb run, but they rep-
resent clearly different actions. When we compute
the composition of both instances of run with their
respective subject and object, we want our model to
show this difference.
To compute the compositional representation of
sentences (1) and (2), we proceed as follows. First,
we extract the latent vectors for subject and object
(wathlete and wrace for the first sentence, wuser and
wcommand for the second sentence) from matrix W.
Next, we compute the outer product of subject and
object ? wathlete ?wrace and wuser ?wcommand ? which
yields matrices Y?athlete,race? and Y?user,command?. By
virtue of the outer product, the matrices Y ? of size
K?K ? represent the level of interaction between the
latent factors of the subject and the latent factors of
the object. We can inspect these interactions by look-
ing up the factor pairs (i.e. matrix cells) with the high-
est values in the matrices Y. Table 1 presents the fac-
tor pairs with highest value for matrix Y?athlete,race?;
table 2 represents the factor pairs with highest value
for matrix Y?user,command?. In order to render the fac-
tors interpretable, we include the three most salient
words for the various factors (i.e. the words with the
highest value for a particular factor).
The examples in tables 1 and 2 give an impression
of the effect of the outer product: semantic features
of the subject combine with semantic features of the
object, indicating the extent to which these features
interact within the expression. In table 1, we notice
that animacy features (28, 195) and a sport feature
(25) combine with a ?sport event? feature (119). In
table 2, we see that similar animacy features (40,
195) and technological features (7, 45) combine with
another technological feature (89).
Similarly, we can inspect the latent interactions of
the verb run, which are represented in the tensor slice
Grun. Note that this matrix contains the verb seman-
tics computed over the complete corpus. The most
salient factor interactions for Grun are represented in
table 3.
Table 3 illustrates that different senses of the verb
run are represented within the matrix Grun. The first
two factor pairs hint at the ?organize? sense of the
verb (run a seminar). The third factor pair repre-
sents the ?transport? sense of the verb (the bus runs
every hour).4 And the fourth factor pair represents
the ?execute? or ?deploy? sense of run (run Linux,
run a computer program). Note that we only show
the factor pairs with the highest value; matrix G con-
tains a value for each pairwise combination of the
latent factors, effectively representing a rich latent
semantics for the verb in question.
The last step is to take the Hadamard product of
matrices Y with verb matrix G, which yields our final
4Obviously, hour is not an object of the verb, but due to
parsing errors it is thus represented.
1146
factors subject object value
?195,119? people (.008), child (.008), adolescent (.007) cup (.007), championship (.006), final (.005) .007
?25,119? hockey (.007), poker (.007), tennis (.006) cup (.007), championship (.006), final (.005) .004
?90,119? professionalism (.007), teamwork (.007), confi-
dence (.006)
cup (.007), championship (.006), final (.005) .003
?28,119? they (.004), pupil (.003), participant (.003) cup (.007), championship (.006), final (.005) .003
Table 1: Factor pairs with highest value for matrix Y?athlete,race?
factors subject object value
?7,89? password (.009), login (.007), username (.007) filename (.007), null (.006), integer (.006) .010
?40,89? anyone (.004), reader (.004), anybody (.003) filename (.007), null (.006), integer (.006) .007
?195,89? people (.008), child (.008), adolescent (.007) filename (.007), null (.006), integer (.006) .006
?45,89? website (.004), Click (.003), site (.003) filename (.007), null (.006), integer (.006) .006
Table 2: Factor pairs with highest value for matrix Y?user,command?
matrices, Zrun,?athlete,race? and Zrun,?user,command?. The
Hadamard product will act as a bidirectional filter
on the semantics of both the verb and its subject
and object: interactions of semantic features that are
present in both matrix Y and G will be highlighted,
while the other interactions are played down. The
result is a representation of the verb?s semantics tuned
to its particular subject-object combination. Note that
this final step can be viewed as an instance of function
application (Baroni and Zamparelli, 2010). Also
note the similarity to Grefenstette and Sadrzadeh?s
(2011a,2011b) approach, who equally make use of
the elementwise matrix product in order to weight
the semantics of the verb.
We can now go back to our original tensor G, and
compute the most similar verbs (i.e. the most similar
tensor slices) for our newly computed matrices Z.5
If we do this for matrix Zrun,?athlete,race?, our model
comes up with verbs finish (.29), attend (.27), and
win (.25). If, instead, we compute the most similar
verbs for Zrun,?user,command?, our model yields execute
(.42), modify (.40), invoke (.39).
Finally, note that the design of our model natu-
rally takes into account word order. Consider the
following two sentences:
(3) man damages car
(4) car damages man
5Similarity is calculated by measuring the cosine of the vec-
torized and normalized representation of the verb matrices.
Both sentences contain the exact same words, but the
process of damaging described in sentences (3) and
(4) is of a rather different nature. Our model is able
to take this difference into account: if we compute
Zdamage,?man,car? following sentence (3), our model
yields crash (.43), drive (.35), ride (.35) as most sim-
ilar verbs. If we do the same for Zdamage,?car,man? fol-
lowing sentence (4), our model instead yields scare
(.26), kill (.23), hurt (.23).
5 Evaluation
5.1 Methodology
In order to evaluate the performance of our tensor-
based factorization model of compositionality, we
make use of the sentence similarity task for transi-
tive sentences, defined in Grefenstette and Sadrzadeh
(2011a). This is an extension of the similarity task
for compositional models developed by Mitchell and
Lapata (2008), and constructed according to the same
guidelines. The dataset contains 2500 similarity
judgements, provided by 25 participants, and is pub-
licly available.6
The data consists of transitive verbs, each paired
with both a subject and an object noun ? thus form-
ing a small transitive sentence. Additionally, a ?land-
mark? verb is provided. The idea is to compose both
the target verb and the landmark verb with subject
and noun, in order to form two small compositional
6http://www.cs.ox.ac.uk/activities/
CompDistMeaning/GS2011data.txt
1147
factors subject object value
?128,181? Mathematics (.004), Science (.004), Economics
(.004)
course (.005), tutorial (.005), seminar (.005) .058
?293,181? organization (.007), association (.007), federa-
tion (.006)
course (.005), tutorial (.005), seminar (.005) .053
?60,140? rail (.011), bus (.009), ferry (.008) third (.004), decade (.004), hour (.004) .038
?268,268? API (.008), Apache (.007), Unix (.007) API (.008), Apache (.007), Unix (.007) .038
Table 3: Factor combinations for Grun
phrases. The system is then required to come up with
a suitable similarity score for these phrases. The cor-
relation of the model?s judgements with human judge-
ments (scored 1?7) is then calculated using Spear-
man?s ? . Two examples of the task are provided in
table 4.
p target subject object landmark sim
19 meet system criterion visit 1
21 write student name spell 6
Table 4: Two example judgements from the phrase simi-
larity task defined by Grefenstette and Sadrzadeh (2011a)
Grefenstette and Sadrzadeh (2011a) seem to cal-
culate the similarity score contextualizing both the
target verb and the landmark verb. Another possibil-
ity is to contextualize only the target verb, and com-
pute the similarity score with the non-contextualized
landmark verb. In our view, the latter option pro-
vides a better assessment of the model?s similar-
ity judgements, since contextualizing low-similarity
landmarks often yields non-sensical phrases (e.g. sys-
tem visits criterion). We provide scores for both
contextualized and non-contextualized landmarks.
We compare our results to a number of different
models. The first is Mitchell and Lapata?s (2008)
model, which computes the elementwise vector mul-
tiplication of verb, subject and object. The second
is Grefenstette and Sadrzadeh?s (2011b) best scoring
model instantiation of the categorical distributional
compositional model (Coecke et al, 2010). This
model computes the outer product of the subject and
object vector, the outer product of the verb vector
with itself, and finally the elementwise product of
both results. It yields the best score on the transitive
sentence similarity task reported to date.
As a baseline, we compute the non-contextualized
similarity score for target verb and landmark. The up-
per bound is provided by Grefenstette and Sadrzadeh
(2011a), based on interannotator agreement.
5.2 Implementational details
All models have been constructed using the UKWAC
corpus (Baroni et al, 2009), a 2 billion word corpus
automatically harvested from the web. From this data,
we accumulate the input matrix V for our first NMF
step. We use the 10K most frequent nouns, cross-
classified by the 2K most frequent context words.7
Matrix V is weighted using pointwise mutual infor-
mation (PMI, Church and Hanks (1990)).
A parsed version of the corpus is available, which
has been parsed with MaltParser (Nivre et al, 2006).
We use this version in order to extract our svo triples.
From these triples, we construct our tensor X, using
1K verbs ? 10K subjects ? 10K objects. Note once
again that the subject and object instances in the sec-
ond step are exactly the same as the noun instances
in the first step. Tensor X has been weighted using a
three-way extension of PMI, following equation 10
(Van de Cruys, 2011).
pmi3(x,y,z) = log
p(x,y,z)
p(x)p(y)p(z)
(10)
We set K = 300 as our number of latent factors.
The value was chosen as a trade-off between a model
that is both rich enough, and does not require an
excessive amount of memory (for the modeling of
the core tensor). The algorithm runs fairly effi-
ciently. Each NMF step is computed in a matter of
seconds, with convergence after 50?100 iterations.
The construction of the core tensor is somewhat more
7We use a context window of 5 words, both before and after
the target word; a stop list was used to filter out grammatical
function words.
1148
evolved, but does not exceed a wall time of 30 min-
utes. Results have been computed on a machine with
Intel Xeon 2.93Ghz CPU and 32GB of RAM.
5.3 Results
The results of the various models are presented in ta-
ble 5; multiplicative represents Mitchell and Lapata?s
(2008) multiplicative model, categorical represents
Grefenstette and Sadrzadeh?s (2011b) model, and
latent represents the model presented in this paper.
model contextualized non-contextualized
baseline .23
multiplicative .32 .34
categorical .32 .35
latent .32 .37
upper bound .62
Table 5: Results of the different compositionality models
on the phrase similarity task
In the contextualized version of the similarity task
(in which the landmark is combined with subject
and object), all three models obtain the same result
(.32). However, in the non-contextualized version
(in which only the target verb is combined with sub-
ject and object), the models differ in performance.
These differences are statistically significant.8 As
mentioned before, we believe the non-contextualized
version of the task gives a better impression of the
systems? ability to capture compositionality. The
contextualization of the landmark verb often yields
non-sensical combinations, such as system visits crite-
rion. We therefore deem it preferable to compute the
similarity of the target verb in composition (system
meets criterion) to the non-contextualized semantics
of the landmark verb (visit).
Note that the scores presented in this evalua-
tion (including the baseline score) are significantly
higher than the scores presented in Grefenstette and
Sadrzadeh (2011b). This is not surprising, since the
corpus we use ? UKWAC ? is an order of magni-
tude larger than the corpus used in their research ?
the British National Corpus (BNC). Presumably, the
scores are also favoured by our weighting measure.
8 p < 0.01; model differences have been tested using stratified
shuffling (Yeh, 2000).
In our experience, PMI performs better than weight-
ing with conditional probabilities.9
6 Conclusion
In this paper, we presented a novel method for the
computation of compositionality within a distribu-
tional framework. The key idea is that composition-
ality is modeled as a multi-way interaction between
latent factors, which are automatically constructed
from corpus data. We used our method to model
the composition of subject verb object combinations.
The method consists of two steps. First, we com-
pute a latent factor model for nouns from standard
co-occurrence data. Next, the latent factors are used
to induce a latent model of three-way subject verb
object interactions, represented by a core tensor. Our
model has been evaluated on a similarity task for tran-
sitive phrases, in which it matches and even exceeds
the state of the art.
We conclude with a number of future work issues.
First of all, we would like to extend our framework in
order to incorporate more compositional phenomena.
Our current model is designed to deal with the latent
modeling of subject verb object combinations. We
would like to investigate how other compositional
phenomena might fit within our latent interaction
framework, and how our model is able to tackle the
computation of compositionality across a differing
number of modes.
Secondly, we would like to further explore the
possibilities of our model in which all three modes
are represented by latent factors. The instantiation
of our model presented in this paper has two latent
modes, using the original instances of the verb mode
in order to efficiently compute verb similarity. We
think a full-blown latent interaction model might
prove to have interesting applications in a number of
NLP tasks, such as the paraphrasing of compositional
expressions.
Finally, we would like to test our method using a
number of different evaluation frameworks. We think
tasks of similarity judgement have their merits, but in
a way are also somewhat limited. In our opinion, re-
search on the modeling of compositional phenomena
within a distributional framework would substantially
9Contrary to the findings of Mitchell and Lapata (2008), who
report a high correlation with human similarity judgements.
1149
benefit from new evaluation frameworks. In particu-
lar, we think of a lexical substitution or paraphrasing
task along the lines of McCarthy and Navigli (2009),
but specifically aimed at the assessment of composi-
tional phenomena.
Acknowledgements
Tim Van de Cruys and Thierry Poibeau are supported
by the Centre National de la Recherche Scientifique
(CNRS, France), Anna Korhonen is supported by the
Royal Society (UK).
References
Brett W. Bader, Tamara G. Kolda, et al 2012. Matlab ten-
sor toolbox version 2.5. http://www.sandia.gov/
~tgkolda/TensorToolbox/.
Marco Baroni and Roberto Zamparelli. 2010. Nouns are
vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1183?1193, Cam-
bridge, MA, October. Association for Computational
Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evaluation,
43(3):209?226.
William Blacoe and Mirella Lapata. 2012. A comparison
of vector-based representations for semantic compo-
sition. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 546?556, Jeju Island, Korea, July. Association
for Computational Linguistics.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. The Journal of Ma-
chine Learning Research, 3:993?1022.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information & lexicography.
Computational Linguistics, 16(1):22?29.
Stephen Clark and Stephen Pulman. 2007. Combining
symbolic and distributional models of meaning. In Pro-
ceedings of the AAAI Spring Symposium on Quantum
Interaction, pages 52?55.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2010. Mathematical foundations for a compositional
distributed model of meaning. Lambek Festschrift, Lin-
guistic Analysis, vol. 36, 36.
Lieven De Lathauwer, Bart De Moor, and Joseph Vande-
walle. 2000. A multilinear singular value decomposi-
tion. SIAM Journal on Matrix Analysis and Applica-
tions, 21(4):1253?1278.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172, Cambridge,
MA, October.
Katrin Erk and Sebastian Pado?. 2008. A structured vector
space model for word meaning in context. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 897?906, Waikiki,
Hawaii, USA.
Katrin Erk and Sebastian Pado?. 2009. Paraphrase assess-
ment in structured vector space: Exploring parameters
and datasets. In Proceedings of the Workshop on Geo-
metrical Models of Natural Language Semantics, pages
57?65, Athens, Greece.
Gottlob Frege. 1892. U?ber Sinn und Bedeutung.
Zeitschrift fu?r Philosophie und philosophische Kritik,
100:25?50.
Eugenie Giesbrecht. 2010. Towards a matrix-based dis-
tributional model of meaning. In Proceedings of the
NAACL HLT 2010 Student Research Workshop, pages
23?28. Association for Computational Linguistics.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011a.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, pages 1394?1404, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011b.
Experimenting with transitive verbs in a discocat. In
Proceedings of the GEMS 2011 Workshop on GEomet-
rical Models of Natural Language Semantics, pages
62?66, Edinburgh, UK, July. Association for Computa-
tional Linguistics.
Zellig S. Harris. 1954. Distributional structure. Word,
10(23):146?162.
Richard A Harshman and Margaret E Lundy. 1994.
Parafac: Parallel factor analysis. Computational Statis-
tics & Data Analysis, 18(1):39?72.
Tamara G. Kolda and Brett W. Bader. 2009. Ten-
sor decompositions and applications. SIAM Review,
51(3):455?500, September.
Tamara G. Kolda and Jimeng Sun. 2008. Scalable tensor
decompositions for multi-aspect data mining. In ICDM
2008: Proceedings of the 8th IEEE International Con-
ference on Data Mining, pages 363?372, December.
Thomas Landauer and Susan Dumais. 1997. A solution
to Plato?s problem: The Latent Semantic Analysis the-
1150
ory of the acquisition, induction, and representation of
knowledge. Psychology Review, 104:211?240.
Daniel D. Lee and H. Sebastian Seung. 2000. Algorithms
for non-negative matrix factorization. In Advances in
Neural Information Processing Systems 13, pages 556?
562.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language resources and
evaluation, 43(2):139?159.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. proceedings of ACL-
08: HLT, pages 236?244.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of LREC-2006, pages 2216?
2219.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings
of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Nat-
ural Language Learning, pages 1201?1211, Jeju Island,
Korea, July. Association for Computational Linguistics.
Stefan Thater, Georgiana Dinu, and Manfred Pinkal. 2009.
Ranking paraphrases in context. In Proceedings of the
2009 Workshop on Applied Textual Inference, pages
44?47, Suntec, Singapore.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings of
the 48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 948?957, Uppsala, Sweden.
Ledyard R. Tucker. 1966. Some mathematical notes on
three-mode factor analysis. Psychometrika, 31(3):279?
311.
Tim Van de Cruys. 2010. A non-negative tensor fac-
torization model for selectional preference induction.
Natural Language Engineering, 16(4):417?437.
Tim Van de Cruys. 2011. Two multivariate generaliza-
tions of pointwise mutual information. In Proceedings
of the Workshop on Distributional Semantics and Com-
positionality, pages 16?20, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th conference on Computational linguistics,
pages 947?953, Saarbru?cken, Germany.
1151
Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 71?79,
Gothenburg, Sweden, April 26 2014.
c?2014 Association for Computational Linguistics
Social and Semantic Diversity:
Socio-semantic Representation of a Scientific Corpus
Elisa Omodei
LATTICE and ISC-PIF
CNRS & ENS & U. Sorbonne Nouvelle
1 rue Mauriece Arnoux
92120 Montrouge France
elisa.omodei@ens.fr
Yufan Guo
University of Washington
Computer Science
Engineering
Box 352350 Seattle, WA 98195-2350
yufanguo@cs.washington.edu
Jean-Philippe Cointet
INRA Sens and ISC-PIF
Cit?e Descartes, 5 boulevard Descartes
77454 Marne-la-Vall?ee Cedex France
75013 Paris France
jphcoi@yahoo.fr
Thierry Poibeau
LATTICE
CNRS & ENS & U. Sorbonne Nouvelle
1 rue Mauriece Arnoux
92120 Montrouge France
thierry.poibeau@ens.fr
Abstract
We propose a new method to extract key-
words from texts and categorize these
keywords according to their informational
value, derived from the analysis of the ar-
gumentative goal of the sentences they ap-
pear in. The method is applied to the ACL
Anthology corpus, containing papers on
the computational linguistic domain pub-
lished between 1980 and 2008. We show
that our approach allows to highlight inter-
esting facts concerning the evolution of the
topics and methods used in computational
linguistics.
1 Introduction
Big data makes it possible to observe in vivo the
dynamics of a large number of different domains.
It is particularly the case in the scientific field,
where researchers produce a prolific literature but
also other kinds of data like numbers, figures, im-
ages and so on. For a number of domains, large
scientific archives are now available over several
decades.
This is for example the case for computational
linguistics. The ACL Anthology contains more
than 24,500 papers, for the most part in PDF for-
mat. The oldest ones date back to 1965 (first edi-
tion of the COLING conference) but it is mostly
after 1980 that data are available in large volumes
so that they can be exploited in evolution studies.
The volume of data increases over time, which
means there is a wide diversity in the number of
papers available depending on the given period of
time. There are similar archives for different do-
mains like, e.g. physics (the APS database pro-
vided by the American Physical Society) or the
bio-medical domain (with Medline).
These scientific archives have already given
birth to a large number of different pieces of work.
Collaboration networks have for example been au-
tomatically extracted so as to study the topology
of the domain (Girvan and Newman, 2002) or
its morphogenesis (Guimera et al., 2005). Ref-
erencing has also been the subject of numerous
studies on inter-citation (Garfield, 1972) and co-
citation (Small, 1973). Other variables can be
taken into account like the nationality of the au-
thors, the projects they are involved in or the re-
search institutions they belong to, but it is the anal-
ysis of the textual content (mostly titles, abstracts
and keywords provided with the papers) that have
attracted the most part of the research in the area
since the seminal work of Callon (Callon et al.,
1986; Callon et al., 1991).
In this paper, our goal is to investigate the evo-
lution of the field of computational linguistics,
which means that text will play a crucial role. Tex-
tual analysis is then mixed with the study of indi-
vidual trajectories in the semantic space: our goal
is to propose possible avenues for the study of the
dynamics of innovation in the computational lin-
71
guistics domain.
The ACL Anthology has been the subject of
several studies in 2012, for the 50 years of the
ACL. More specifically, a workshop called ?Re-
discovering 50 Years of Discoveries? was orga-
nized to examine 50 years of research in NLP
(but, for the reasons given above, the workshop
mostly focused on the evolution of the domain
since 1980). This workshop was also an oppor-
tunity to study a large scientific collection with re-
cent NLP techniques and see how these techniques
can be applied to study the dynamics of a scientific
domain.
The analysis of this kind of data is generally
based on the extraction of key information (au-
thors, keywords) and the discovery of their rela-
tionships. The data can be represented as a graph,
therefore graph algorithmics can be used to study
the topology and the evolution of the graph of col-
laborations or the graph of linked authors. It is
thus possible to observe the evolution of the do-
main, check some hypotheses or common assump-
tions about this evolution and provide a strong em-
pirical basis to epistemology studies.
The paper ?Towards a computational History of
the ACL: 1980-2008? is very relevant from this
point of view (Anderson et al., 2012). The au-
thors try to determine the evolution of the main
sub-domains of research within NLP since 1980
and they obtain very interesting results. For ex-
ample, they show the influence of the American
evaluation campaigns on the domain: when a US
agency sponsored a sub-domain of NLP, one can
observe a quick concentration effect since a wide
number of research groups suddenly concentrated
their efforts on the topic; when no evaluation cam-
paign was organized, research was much more
widespread across the different sub-domains of
NLP. Even if this is partially predictable, it was
not obvious to be able to show this in a collection
of papers as large as the ACL Anthology.
Our study has been profoundly influenced by
the study by Anderson et al. However, our goal
here is to characterize automatically the keywords
based on the information they carry. We will thus
combine keyword extraction with text zoning so
as to categorize the keywords depending on their
context of use.
The rest of the paper is organized as follows.
We first present an analysis of the structure of ab-
stracts so as to better characterize their content by
mixing keyword extraction with text zoning. We
show how these techniques can be applied to the
ACL Anthology in order to examine specific facts,
more specifically concerning the evolution of the
techniques used in the computational linguistics
domain.
2 A Text Zoning Analysis of the ACL
Anthology
The study of the evolution of topics in large cor-
pora is usually done through keyword extraction.
This is also our goal, but we would like to be able
to better characterize these keywords and make a
difference, for example, between keywords refer-
ring to concepts and keywords referring to meth-
ods. Hence, the context of these keywords seems
highly important. Consequently, we propose to
use Text Zoning that can provide an accurate char-
acterization of the argumentative goal of each sen-
tence in a scientific abstract.
2.1 Previous work
The first important contributions in text zoning are
probably the experiments by S. Teufel who pro-
posed to categorize sentences in scientific papers
(and more specifically, in the NLP domain) ac-
cording to different categories (Teufel, 1999) like
BKG: General scientific background, AIM: State-
ments of the particular aim of the current paper or
CTR: Contrastive or comparative statements about
other work. This task is called Rhetorical zoning
or Argumentative zoning since the goal is to iden-
tify the rhetoric or argumentative role of each sen-
tence of the text.
The initial work of Teufel was based on the
manual annotation of 80 papers representing the
different areas of NLP (the corpus was made of
papers published within the ACL conferences or
Computational Linguistics). A classifier was then
trained on this manually annotated corpus. The
author reported interesting results despite ?a 20%
diference between [the] system and human perfor-
mance? (Teufel and Moens, 2002). The learning
method used a Naive Bayesian model since more
sophisticated methods tested by the author did not
obtain better results. Teufel in subsequent publica-
tions showed that the technique can be used to pro-
duce high quality summaries (Teufel and Moens,
2002) or precisely characterize the different cita-
tions in a paper (Ritchie et al., 2008).
The seminal work of Teufel has since then given
72
rise to different kinds of works, on the one hand
to refine the annotation method, and on the other
hand to check its applicability to different scien-
tific domains. Concerning the first point, research
has focused on the identification of relevant fea-
tures for classification, on the evaluation of dif-
ferent learning algorithms for the task and more
importantly on the reduction of the volume of text
to be annotated. Concerning the second point, it
is mostly the biological and bio-medical domains
that have attracted attention, since scientists in
these domains often have to access the literature
?vertically? (i.e. experts may need to have access
to all the methods and protocols that have been
used in a specific domain) (Mizuta et al., 2006;
Tbahriti et al., 2006).
Guo has since developed a similar trend of re-
search to extend the initial work of Teufel (Guo
et al., 2011; Guo et al., 2013): she has tested a
large list of features to analyze the zones, evalu-
ated different learning algorithms for the task and
proposed new methods to decrease the number of
texts to be annotated. The features used for learn-
ing are of three categories: i) positional (location
of the sentence inside the paper), ii) lexical (words,
classes of words, bigrams, etc. are taken into con-
sideration) and iii) syntactic (the different syntac-
tic relations as well as the class of words appear-
ing in subject or object positions are taken into ac-
count). The analysis is thus based on more fea-
tures than in Teufel?s initial work and requires a
parser.
2.2 Application to the ACL Anthology corpus
In our experiment, we only used the abstracts of
the papers. Our hypothesis is that abstracts con-
tain enough information and are redundant enough
to study the evolution of the domain. Taking into
consideration the full text would probably give too
many details and thus introduce noise in the anal-
ysis.
The annotation scheme includes five different
categories, which are the following: OBJEC-
TIVE (objectives of the paper), METHOD (meth-
ods used in the paper), RESULTS (main results),
CONCLUSION (conclusion of the paper), BACK-
GROUND (general context), as in (Reichart and
Korhonen, 2012). These categories are also close
to those of (Mizuta et al., 2006; Guo et al., 2011;
Guo et al., 2013) and have been adapted to ab-
stracts (as opposed to full text
1
). It seems relevant
to take into consideration an annotation scheme
that has already been used by various authors so
that the results are easy to compare to others.
Around one hundred abstracts from the ACL
Anthology have then been manually annotated us-
ing this scheme (?500 sentences; ACL abstracts
are generally quite short since most of them are
related to conference papers). The selection of the
abstracts has been done using stratified sampling
over time and journals, so as to obtain a represen-
tative corpus (papers must be related to different
periods of time and different sub-areas of the do-
main). The annotation has been done according
to the annotation guideline defined by Y. Guo, es-
pecially for long sentences when more than one
category could be applied (preferences are defined
to solve complex cases
2
).
The algorithm defined by (Guo et al., 2011) is
then adapted to our corpus. The analysis is based
on positional, lexical and syntactic features, as ex-
plained above. No domain specific information
was added, which makes the whole process easy
to reproduce. As for parsing, we used the C&C
parser (James Curran and Stephen Clark and Johan
Bos, 2007). All the implementation details can be
found in (Guo et al., 2011), especially concerning
annotation and the learning algorithm. As a result,
each sentence is associated with a tag correspond-
ing to one of the zones defined in the annotation
scheme.
2.3 Results and Discussion
In order to evaluate the text zoning task, a num-
ber of abstracts were chosen randomly (?300 sen-
tences that do not overlap with the training set).
CONCLUSION represented less than 3% of the
sentences and was then dropped for the rest of
the analysis. The four remaining zones are un-
equaly represented: 18.05 % of the sentences re-
fer to BACKGROUND, 14.35% to OBJECTIVE,
14.81 % to RESULT and 52.77 % to METHOD.
Just by looking at these numbers, one can see how
1
The categories used in (Teufel, 1999) were not relevant
since this model focused on full text papers, with a special
emphasis on the novelty of the author?s work and the attitude
towards other people?s work, which is not the case here.
2
The task is to assign the sentence only a single category.
The choice of the category should be made according to the
following priority list: Conclusion > Objective > Result >
Method> Background. The only exception is that when 75%
or more of the sentence belongs to a less preferred category,
then that category will be assigned to the sentence.
73
Table 1: Result of the text zoning analysis (preci-
sion)
Category Precision
Objective 83,87 %
Background 81,25 %
Method 71,05 %
Results 82,05 %
Figure 1: An abstract annotated with text zoning
information. Categories are indicated in bold face.
Most of errors in Korean morphological analysis and
POS ( Part-of-Speech ) tagging are caused by unknown
morphemes . BACKGROUND
This paper presents a generalized unknown morpheme
handling method with POSTAG(POStech TAGger )
which is a statistical/rule based hybrid POS tagging
system . OBJECTIVE
The generalized unknown morpheme guessing is based
on a combination of a morpheme pattern dictionary
which encodes general lexical patterns of Korean
morphemes with a posteriori syllable tri-gram estimation
. METHOD
The syllable tri-grams help to calculate lexical proba-
bilities of the unknown morphemes and are utilized to
search the best tagging result . METHOD
In our scheme , we can guess the POS?s of unknown
morphemes regardless of their numbers and positions
in an eojeol , which was not possible before in Korean
tagging systems . RESULTS
In a series of experiments using three different domain
corpora , we can achieve 97% tagging accuracy regard-
less of many unknown morphemes in test corpora .
RESULTS
methodological issues are important for the do-
main.
We then calculate for each of the categories, the
percentage of sentences that received the right la-
bel, which allows us to calculate precision. The
results are given in table 1.
These results are similar to the state of the art
(Guo et al., 2011), which is positive taking into
consideration the small number of sentences an-
notated for training. The diversity of the features
used makes it easy to transfer the technique from
one domain to the other without any heavy anno-
tation phase. Results are slightly worse for the
METHOD category, probably because this cate-
gory is more diverse and thus more difficult to rec-
ognize. The fact that NLP terms can refer either to
objectives or to methods also contributes render-
ing the recognition of this category more difficult.
Figure 1 shows an abstract annotated by the text
zoning module (the paper is (Lee et al., 2002): it
has been chosen randomly between those contain-
ing the different types of zones). One category
is associated with each sentence but this is some-
times problematic: for example the fact that a hy-
brid method is used is mentioned in a sentence that
is globally tagged as OBJECTIVE by the system.
However, sentences tagged as METHOD contain
relevant keywords like lexical pattern or tri-gram
estimation, which makes it possible to infer that
the approach is hybrid. One can also spot some
problems with digitization, which are typical of
this corpus: the ACL Anthology contains automat-
ically converted files to PDF, which means texts
are not perfect and may contain some digitization
errors.
3 Contribution to the Study of the
Evolution ACL Anthology
As said above, we are largely inspired by (Ander-
son et al., 2012). We think the ACL Anthology
is typical since it contains papers spanning over
more than 30 years: it is thus interesting to use it
as a way to study the main evolutions of the com-
putational linguistics domain. The method can of
course also be applied to other scientific corpora.
3.1 Keyword extraction and characterization
The first step consists in identifying the main key-
words of the domain. We then want to more pre-
cisely categorize these keywords so as to identify
the ones specifically referring to methods for ex-
ample. From this perspective, keywords appear-
ing in the METHOD sections are thus particularly
interesting for us. However, one major problem is
that there is no clear-cut difference between goals
and methods in NLP since most systems are made
of different layers and require various NLP tech-
niques. For example, a semantic analyzer may use
a part-of-speech tagger and a parser, which means
NLP tools can appear as part of the method.
Keyword extraction aims at automatically ex-
tracting relevant keywords from a collection of
texts. A popular approach consists in first extract-
ing typical sequences of tags that are then filtered
according to specific criteria (these criteria can in-
clude the use of external resources but they are
more generally based on scores mixing frequency
and specificity (Bourigault and Jacquemin, 1999;
Frantzi and Ananiadou, 2000)). In this study, we
voluntarily used a minimal approach for keyword
extraction and filtering since we want to keep most
74
Table 2: Most specific keywords found in the METHOD sections.
Methods
Category Method N-grams
Machine learning
Bayesian methods baesyan
Vector Space model space model, vector space, cosine
Genetic algorithms genetic algorithms
HMM hidden markov models, markov model
CRF conditional random fields
SVM support vector machines
MaxEnt maximum entropy model, maximum entropy approach, maximum entropy
Clustering clustering algorithm, clustering method, word clusters, classification problem
Speech & Mach. Trans.
Language models large-vocabulary, n-gram language model, Viterbi
Parallel Corpora parallel corpus, bilingual corpus, phrase pairs, source and target languages, sentence pairs, word pairs,
source sentence
Alignment phrase alignment, alignment algorithm, alignment models, ibm model, phrase translation, translation
candidates, sentence alignment
NLP Methods
POS tagging part-of-speech tagger, part-of-speech tags
Morphology two-level morphology, morphological analyzer, morphological rules
FST finite-state transducers, regular expressions, state automata, rule-based approach
Syntax syntactic categories, syntactic patterns, extraction patterns
Dependency parsing dependency parser, dependency graphs, prague dependency, dependency treebank, derivation trees, parse
trees
Parsing grammar rules, parser output, parsing process, parsed sentences, transfer rules
Semantics logical forms, inference rules, generative lexicon, lexical rules, lexico-syntactic, predicate argument
Applications
IE and IR entity recognition, answer candidates, temporal information, web search, query expansion, google, user
queries, keywords, query terms, term recognition
Discourse generation component, dialogue acts, centering theory, lexical chains, resolution algorithm, generation
process, discourse model, lexical choice
Segmentation machine transliteration, phonological rules, segmentation algorithm, word boundaries
Words and Resource
Lexical knowledge bases lexical knowledge base, semantic network, machine readable dictionaries, eurowordnet, lexical entries,
dictionary entries, lexical units, representation structures, lookup
Word similarity word associations, mutual information, semantic relationships, word similarity, semantic similarity,
semeval-2007, word co-occurrence, synonymy
Corpora brown corpus, dialogue corpus, annotation scheme, tagged corpus
Evaluation Evaluation score, gold standard, evaluation measures, estimation method
Calculation & complexity Software tool development, polynomial time, software tools, series of experiments, system architecture, runtime,
programming language
Constraints relaxation, constraint satisfaction, semantic constraints
of the information for the subsequent text zoning
phase. We thus used NLTK for part-of-speech tag-
ging and from this result extracted the most com-
mon noun phrases. We used a pre-defined set
of grammatical patterns to extract noun phrases
defined as sequences of simple sequences (e.g.
adjectives + nouns, ?phrase pairs?, ?dependency
graph?, etc.) possibly connected to other such pat-
terns through propositions to form longer phrases
(e.g. ?series of experiments?). Only the noun
phrases appearing in more than 10 papers are kept
for subsequent processing.
Candidate keywords are then ranked per zone,
according to their specificity (the zone they are
the most specific of) . Specificity corresponds to
the Kolmogorov-Smirnov test that quantifies a dis-
tance between the empirical distribution functions
of two samples. The test is calculated as follows:
D = max
x
|S
N
1
(x)? S
N
2
(x)| (1)
where S
N
1
(x) et S
N
2
(x) are the empirical distri-
bution function of the two samples (that corre-
spond in our case to the number of occurrences
of the keyword in a given zone, and to the total
number of occurrences of all the keywords in the
same zone, respectively) (Press et al., 2007). A
high value of D for a given keyword means that it
is highly specific of the considered zone. At the
opposite, a low value means that the keyword is
spread over the different zones and not really spe-
cific of any zone.
The first keywords of each category are then
categorized by an expert of the domain. For the
METHOD category, we obtain Table 2. Logically,
given our approach, the table does not contain all
the keywords relevant for the computational lin-
guistics domain, but it contains the mots specific
ones according to the above approach. One should
thus not be surprised not to see all the keywords
used in the domain.
3.2 Evolution of methods over time
The automatic analysis of the corpus allows us to
track the main evolutions of the field over time.
During the last 30 years, the methods used have
changed to a large extent, the most notable fact be-
ing probably the generalization of machine learn-
ing methods since the late 1990s. This is outlined
by the fact that papers in the domain nowadays
nearly always include a section that describes an
experiment and some results.
To confirm this hypothesis, we observe the rel-
ative frequency of sentences tagged as RESULTS
in the papers over time. In the figure 3, we see that
the curve increases almost linearly from the early
1980s until the late 2000s.
75
1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 20080
0.10.2
0.30.4
0.50.6
0.70.8
0.91
NLP Methods
SemanticsParsingDependency parsingSyntaxFSTMorphologyPOS tagging
Year
Relative
 Freque
ncy
1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 20080
0.10.2
0.30.4
0.50.6
0.70.8
0.91
Applications
SegmentationDiscourseIE and IR
Year
Relative
 Freque
ncy
1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 20080
0.10.2
0.30.4
0.50.6
0.70.8
0.91
Machine Learning
ClusteringMaxEntSVMCRFHMMGenetic algorithmsVector Space modelBayesian methods
Year
Relative
 Freque
ncy
1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 20080
0.10.2
0.30.4
0.50.6
0.70.8
0.91
Speech & machine translation specific
AlignmentParallel CorporaLanguage models
Year
Relative
 Freque
ncy
1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 20080
0.10.2
0.30.4
0.50.6
0.70.8
0.91
Resources
CorporaWord similarityLexical knowledge bases
Year
Relative
 Freque
ncy
1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 20080
0.10.2
0.30.4
0.50.6
0.70.8
0.91
Calculation & Complexity
ConstraintsSoftware
Year
Relative
 Freque
ncy
Figure 2: Evolution of the relative frequency of the different groups of methods over time.
It is also possible to make more fine-grained ob-
servations, for example to follow over time the dif-
ferent kinds of methods under consideration. The
results are shown in figure 2. Rule based methods
and manually crafted resources are used all over
the period, while machine learning based meth-
ods are more and more successful after the late
1990s. This is not surprising since we know that
machine learning is now highly popular within the
field. However, symbolic methods are still used,
sometimes in conjunction with learning methods.
The two kinds of methods are thus more comple-
mentary than antagonistic.
One could observe details that should be
checked through a more thorough study. We ob-
serve for example the success of dependency pars-
ing in the end of the 1980s (probably due to the
success of the Tree Adjoining Grammars at the
time) and the new popularity of this area of re-
search in the early 2000s (dependency parsing has
been the subject of several evaluation campaigns
in the 2000s, see for example for the CONLL
shared tasks from 2006 to 2009).
Different machine learning methods have been
popular over time but each of them continues to be
used after a first wave corresponding to their ini-
tial success. Hidden Markov Models and n-grams
are highly popular in the 1990s, probably thanks
to the experiments made by Jelinek and his col-
leagues, which will open the field of statistical ma-
chine translation (Brown et al., 1990). SVM and
CRF have had a more recent success as everybody
knows.
We are also interested in the distribution of
these methods between papers and authors. Fig-
ure 4 shows the average number of keywords
1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 20080
0.05
0.1
0.15
0.2
0.25
Results
Year
Relative
 Freque
ncy
Figure 3: Evolution of the relative frequency of
sentences tagged as RESULTS in the abstracts of
the papers
appearing in the METHOD section of the papers
over time. We see that this number regularly in-
creases, especially during the 1980s, showing pos-
sibly a gradually increasing complexity of the sys-
tems under consideration.
Lastly, figure 5 shows the number of authors
who are specialists of one or several methods.
Most of the authors just mention one method in
their papers and, logically, the curves decrease,
which means that there are few authors who are
really specialists of many methods. This result
should be confirmed by a larger scale study tak-
ing into account a larger number of keywords but
the trend seems however interesting.
3.3 The dynamics of the authors in the
method space
One could say that the results we have reported in
the previous section are not new but rather confirm
some already well known facts. Our method al-
lows to go one step further and try to answer more
76
Figure 4: Evolution of the number of keywords
related to methods over time.
1 9 8 0 2 4 6 .5
5R1
5R9
5R8
5R0
5R2
5R4
esultYariltrvs Fqalncr2rvyvnYaesultYariltrvs Fqalncr4rvyvnYaesultYariltrvs Fqalncr6rvyvnYaesultYariltrvs Fqalncr.rvyvnYaesultYariltrvs Fqalncr?rvyvnYaesultYariltrvs Fqalncr15rvyvnYaesultYariltrvs Fqalncr11rvyvnYaesultYariltrvs Fqalncr19rvyvnYaesultYariltrvs Fqalncr18rvyvnYa
?s? nYrt?r?nultca
?YtvtYu
qt?rt?ry
sultYa
Figure 5: Proportion of authors specialized in
a given number of methods (i.e. mentioning
frequently the name of the method in the ab-
stracts), for different categories of researchers.
challenging questions. How are new methods in-
troduced in the field? Are they mainly brought
by young researchers or is it mainly confirmed re-
searchers who develop new techniques (or import
them from related fields)? Are NLP experts spe-
cialized in one field or in a wide variety of differ-
ent fields?
These questions are of course quite complex.
Each individual has his own expertise and his
own history but we think that automatic meth-
ods can provide some interesting trends over time.
For example, (Anderson et al., 2012) show that
evaluation campaigns have played a central role
at certain periods of time, which does not mean
of course that there was no independent research
outside these campaigns at the time. Our goal
is thus to exhibit some tendencies that could be
interpreted or even make it possible to compare
the evolution of the computational linguistics field
with other fields. Out tools provide some hypothe-
ses that must of course be confirmed by further ob-
servations and analysis. We do not claim that they
provide an exact and accurate view of the domain.
Gene
tic alg
orithm
s HMM
Morp
holog
y
Corpo
ra SVM
Clust
ering
POS 
taggin
g
Baye
sian m
ethod
s
MaxE
nt CRF
Align
ment
Langu
age m
odels
Vecto
r Spa
ce m
odel
00.1
0.20.3
0.40.5
0.60.7
0.80.9
1
Fraction of pionners that are new to the field
Fraction of authors that enter the field in those years
Figure 6: For each ?new method?, number of ?pi-
oneers? not having published any paper before
(compared to the total number of new authors dur-
ing the same period of time).
For this study we only take into account authors
who have published at least 5 papers in the ACL
Anthology, in order to take into consideration au-
thors who have contributed to the domain during a
period of time relevant for the study. We consider
as ?pioneers? the authors of the first 25% of pa-
pers in which a keyword referring to a method is
introduced (for example, the first papers where the
keywords support vector machine or SVM appear).
We then calculate, among this set of authors, the
ones who can be considered as new authors, which
means people who have not published before in
the field. Since there are every year a large number
of new authors (who use standard techniques) we
compare the ratio of new authors using new tech-
niques with the number of authors using already
known techniques over the considered period. Re-
sults are visible in figure 6.
Results are variable depending on the method
under consideration but some of them seem inter-
esting. Papers with the keyword Hidden Markov
Model in the 1990s seem to be largely written
by new comers, probably by researchers having
tested this method in related fields before (and
we know that it was the case of Jelinek?s team
who was largely involved in speech processing, a
domain not so well represented in the ACL An-
thology before the 1990s. Of course, Jelinek and
colleague were confirmed and even highly estab-
lished researchers already at the beginning of the
1990s). We observe a similar patten for genetic
algorithms but the number of authors is too lim-
ited to say if the trend is really meaningful. SVM
also seem to have been popularized by new com-
ers but it is not the case of language models or of
the vector space model. A more thorough study is
of course needed to confirm and better understand
77
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
C
u
m
u
l
a
t
i
v
e
 
D
i
s
t
r
i
b
u
t
i
o
n
 
F
u
n
c
t
i
o
n
Fraction of total production of author already published
Figure 7: Distribution function of the number of
papers already published by ?pioneers? when they
have published their paper on the new method,
compared to the total production of their career.
these results.
We then do a similar experiment to try to de-
termine when, during their career, researchers use
new methods. Practically, we examine at what
point of their career the authors who are character-
ized as ?pioneers? in our study (what refers to the
first authors using a new method) have published
the papers containing new methods (for example,
if an author is one of the first who employed the
keyword SVM, has he done this at the beginning
of his career or later on?). The result is visible in
figure 7 and shows that 60% of pioneers had pub-
lished less than a third of their scientific produc-
tion when they use the new method. We thus ob-
serve a similar set of authors between the pioneers
and researchers having published so far in related
but nevertheless different communities. To con-
firm this result, it would be useful to study other
domains and other corpora (in computer science,
linguistics, cognitive sciences) so as to get a better
picture of the domain, but the task is then highly
challenging.
One may want then to observe the diversity of
methods employed in the domain, especially by
the set of people called ?pioneers? in our study.
Figure 8 shows in blue the number of methods
detected for the pioneers and in red the number of
methods used by all the authors.
We see that pioneers, when taking into consid-
eration the whole set of papers in the ACL An-
thology, are using a larger number of methods.
They are over represented among authors using 3
methods and more. This group of people also con-
tribute to a larger number of sub-areas in the do-
mains compared to the set of other authors.
1 2 3 4 5 6 7 80
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Pioneers proportionTotal authors proportion
Number of methods per author
Propo
rtion 
of aut
hors
Figure 8: Proportion of ?pioneers? experts in a
given number of methods compared to all the other
authors in the corpus.
4 Conclusion
We have presented in this paper an analysis of the
ACL Anthology corpus. Our analysis is based on
the identification of keywords which are catego-
rized according to their informational status. Cate-
gorization is done according to a Text Zoning anal-
ysis of the papers? abstracts, which provides very
relevant information for the study. We have shown
that coupling keyword extraction with Text Zon-
ing makes it possible to observe fine grained facts
in the dynamics of a scientific domain.
These tools only give pieces of information that
should be confirmed by subsequent studies. It
is necessary to go back to the texts themselves,
consult domain experts and probably the larger
context to be able to get a really accurate pic-
ture of the evolution of a scientific domain. This
multi-disciplinary research means that to collabo-
rate with people from other fields is needed, espe-
cially with the history of science and epistemol-
ogy. However, the platforms and the techniques
we have described in this paper are now available
and can be re-used for other kinds of studies, mak-
ing it possible to reproduce similar experiments
across different domains.
References
Ashton Anderson, Dan Jurafsky, and Daniel A. McFar-
land. 2012. Towards a computational history of the
acl: 1980-2008. In Proceedings of the ACL-2012
Special Workshop on Rediscovering 50 Years of Dis-
coveries, pages 13?21, Jeju Island, Core. Associa-
tion for Computational Linguistics.
Didier Bourigault and Christian Jacquemin. 1999.
Term extraction + term clustering: An integrated
platform for computer-aided terminology. In Pro-
ceedings of the Ninth Conference on European
78
Chapter of the Association for Computational Lin-
guistics, EACL ?99, pages 15?22.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine translation.
Computational Linguistics, 16(2):79?85.
Michel Callon, John Law, and Arie Rip. 1986.
Mapping the dynamics of science and technology.
McMillan, London.
Michel Callon, Jean-Pierre Courtial, and Franc?oise
Laville. 1991. Co-word analysis as a tool for de-
scribing the network of interaction between basic
and technological research: The case of polymer
chemistry. Scientometrics, 22(1):155?205.
Katarina Frantzi and Sophia Ananiadou. 2000. Au-
tomatic recognition of multi-word terms:. the C-
value/NC-value method. International Journal on
Digital Libraries, 3(2):115?130.
Eugene Garfield. 1972. Citation Analysis as a Tool in
Journal Evaluation. Science, 178(4060):471?479.
Michelle Girvan and Mark E J Newman. 2002. Com-
munity structure in social and biological networks.
Proceedings of the National Academy of Sciences of
the United States of America, 99:7821?7826.
Roger Guimera, Brian Uzzi, Jarrett Spiro, and Luis
A. Nunes Amaral. 2005. Team Assembly Mech-
anisms Determine Collaboration Network Structure
and Team Performance. Science, 308(5722):697?
702.
Yufan Guo, Anna Korhonen, and Thierry Poibeau.
2011. A weakly-supervised approach to argumenta-
tive zoning of scientific documents. In Proceedings
of the 2011 Conference on Empirical Methods in
Natural Language Processing, pages 273?283, Ed-
inburgh.
Yufan Guo, Roi Reichart, and Anna Korhonen. 2013.
Improved information structure analysis of scien-
tific documents through discourse and lexical con-
straints. In Proceedings of Human Language Tech-
nologies: Conference of the North American Chap-
ter of the Association of Computational Linguistics
(HLT-NAACL), pages 928?937.
James Curran and Stephen Clark and Johan Bos.
2007. Linguistically Motivated Large-Scale NLP
with C&C and Boxer. In Proceedings of the 45th
Meeting of the Association for Computation Linguis-
tics (ACL), pages 33?36.
Gary Geunbae Lee, Jong-Hyeok Lee, and Jeong-
won Cha. 2002. Syllable-pattern-based unknown-
morpheme segmentation and estimation for hybrid
part-of-speech tagging of korean. Computational
Linguistics, 28(1):53?70.
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2006. Zone analysis in biology articles as a
basis for information extraction. International Jour-
nal of Medical Informatics, 75(6):468?487.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2007. Numerical
Recipes 3rd Edition: The Art of Scientific Comput-
ing. Cambridge University Press, New York, NY,
USA, 3 edition.
Roi Reichart and Anna Korhonen. 2012. Docu-
ment and corpus level inference for unsupervised
and transductive learning of information structure of
scientific documents. In Proceedings of COLING
(Posters), pages 995?1006, Mumbai.
Anna Ritchie, Stephen Robertson, and Simone Teufel.
2008. Comparing citation contexts for information
retrieval. In Proeedings of the 17th Conference on
Information and Knowledge Management (CIKM),
pages 213?222, Napa Valley.
Henry G Small. 1973. Co-citation in the scientific lit-
erature: A new measure of the relationship between
two documents. Journal of American Society for In-
formation Science, 24(4):265?269.
Imad Tbahriti, Christine Chichester, Fr?ed?erique
Lisacek, and Patrick Ruch. 2006. Using argumen-
tation to retrieve articles with similar citations: An
inquiry into improving related articles search in the
medline digital library. I. J. Medical Informatics,
75(6):488?495.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: Experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?445.
Simone Teufel. 1999. Argumentative Zoning: Infor-
mation Extraction from Scientific Articles. Univer-
sity of Edinburgh.
79
Proceedings of the First Celtic Language Technology Workshop, pages 28?32,
Dublin, Ireland, August 23 2014.
Processing Mutations in Breton with Finite-State Transducers
Thierry Poibeau
Laboratoire LATTICE (UMR8094)
CNRS & Ecole normale sup?erieure & Universit?e Paris 3 Sorbonne Nouvelle
1 rue Maurice Arnoux 92120 Montrouge France
thierry.poibeau@ens.fr
Abstract
One characteristic feature of Celtic languages is mutation, i.e. the fact that the initial consonant
of words may change according to the context. We provide a quick description of this linguistic
phenomenon for Breton along with a formalization using finite state transducers. This approach
allows an exact and compact description of mutations. The result can be used in various contexts,
especially for spell checking and language teaching.
1 Introduction
Celtic languages (Welsh, Cornish, Irish, Scottish-Gaelic, Manx, etc.) are known to support a common
feature: the initial consonant of different types of words (esp. nouns, adjectives and verbs) is modified
in certain contexts and after certain function words (e.g. prepositions and determiners for nouns and
adjectives; auxiliaries for verbs). This phenomenon known as ?mutation? has been largely studied and
described from a linguistic point of view. Formal descriptions have even been proposed, especially
Mittendorf and Sadler (2006) for Welsh.
In this paper, we investigate mutations in Breton.
1
Our study is largely inspired by the previous
study by Mittendorf and Sadler for Welsh: We share with these authors the idea that ?initial mutation is
close to inflection in nature and is essentially a morphosyntactic phenomenon?. We propose to process
this phenomenon with finite state transducers. In fact, we propose two formalizations: in the first one,
mutations are processed by directly storing the lexical forms with mutations in a dictionary of inflected
forms; in the second one, local rules encoded using finite state transducers are applied dynamically,
depending on the context. We show that this strategy allows for an exact and compact description of the
phenomenon, since transducers directly encode grammar rules.
The paper is organized as follows: we first propose a linguistic description of this phenomenon. We
then explore the two strategies exposed in the previous paragraph: a dictionary of inflected form vs local
grammars encoded using finite state machines. We conclude with a discussion and an overview of the
practical use of this implementation.
1.1 A Quick Description of Mutations in Breton
As said in Wikipedia (http://en.wikipedia.org/wiki/Breton mutations), ?Breton is characterized by initial
consonant mutations, which are changes to the initial sound of a word caused by certain syntactic or
morphological environments. In addition Breton, like French, has a number of purely phonological
sandhi features caused when certain sounds come into contact with others.? The following details are
then added: ?the mutations are divided into four main groups, according to the changes they cause:
soft mutation (in Breton: kemmadurio`u dre vlotaat), hard mutation (kemmadurio`u dre galetaat), spirant
mutation (kemmadurio`u c?hwezhadenni?n) and mixed mutation (kemmadurio`u mesket). There are also
a number of defective (or incomplete) mutations which affect only certain words or certain letters.? A
This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://
creativecommons.org/licenses/by/4.0/
1
Breton is a Celtic language spoken in Brittany (Western France) According to recent studies, 200,000 persons understand
the language but only 35,000 practice it on a daily basis.
28
same word can thus appear differently depending on these classes of changes (for example the noun tad
? father ? becomes da dad ? your father; he zad ? her father; etc. because of the possessive pronouns
da and he that entail different kinds of mutation).
The best approach to give an idea of mutations is to consider some examples. ?Soft mutations? refer
to the fact that after the definite article ar (and its variant an) or the indefinite article ur (or un) the initial
consonant of singular feminine nouns is subject to the following changes:
? K? G, ex. Kador (chair)? Ur gador
? T? D, ex. Taol (table)? Un daol
? P? B, ex. Paner (basket)? Ur baner
? G? C?H, ex. Gavr (goat)? Ur c?havr
? GW?W, ex. Gwern (mast)? Ur wern
Note that in Breton nouns referring to objects and abstract notions can be either masculine or feminine
(there is no neuter case).
Although the phenomenon is well known, its description is not straightforward since it involves a
large number of parameters and different types of information (lexical, morphological, semantic). For
example, plural masculine nouns referring to male persons have the same behavior as singular feminine
nouns (but this is only true for plural masculine nouns referring to people, not for all plural nouns). It is
therefore necessary to distinguish different categories of nouns.
? K? G, ex. Kigerien (butchers)? Ar gigerien
? T? D, ex. Tud (people)? An dud
? P? B, ex. Pesketaerien (fishermen)? Ar besketaerien
? G? C?H, ex. Gellaoued (French)? Ar C?hallaoued
? GW?W, ex. Gwerzherien (sellers)? Ar werzherien
These mutations also affect adjectives, provided that the noun preceding the adjective ends with a vowel
or with the consonant l, m, n, or r.
? K? G, ex. Kaer (nice)? Ur gador gaer (a nice chair)
? T? D, ex. Tev (thick)? Ur wern dev (a thick mast)
? P? B, ex. Paour (poor)? Ur vamm baour (a poor mother)
There are different series of mutations depending on the functional word preceding the noun (and the
adjectives if any). It is one of the main difficulties of the language since this phenomenon changes the
initial of the words: after mutation, words cannot be found anymore directly in the dictionary.
A comprehensive description of this phenomenon can be found in traditional grammars of the lan-
guage: see especially Kervella (1976), Hemon (1984) and Stump (1984) for a formal description of
agreement in Breton.
1.2 Automatic Processing of Mutations in Breton
Two approaches are possible:
? store all the inflected lexical forms and their mutations in a dictionary. Mutation is then considered
as a case of word variation (like the alternation singular/plural);
? compute on the fly the lexical form in context, which is an interesting strategy for text generation
or, more directly, in the context of text authoring (for example to assist students producing texts in
Breton).
In this paper, we consider both approaches since they are both relevant depending on the context.
29
2 Two Competing / Complementary Solutions for Mutations in Breton
The following section describes two ways of processing mutations in Breton. We discuss their interest
and their applicability to the problem.
2.1 A Comprehensive Dictionary of Inflected Forms
This solution is the simplest one: all inflected forms including those with modified initial letters are
included in the dictionary. The dictionary remains manageable and ambiguity introduced by the new
lexical forms is limited. Below is a short extract of a dictionary of inflected forms including lexical
forms after mutation:
kador,kador.N:fs taol,taol.N:fs
gador,kador.N:fs daol,taol.N:fs
c?hador,kador.N:fs zaol,taol.N:fs
The format of the dictionary is the one defined by LADL (Courtois and Silberztein, 1990): inflected
forms are followed by a lemma (separated by a comma). The category of the word can then be found (N
for noun) as well as morphosyntactic features (fs: feminine singular).
However, this solution is not fully satisfactory since it does not explain why a given form is used in
a given context. It would be relevant to provide a more dynamic description of the process taking into
account the different constraints we have seen in the previous section.
2.2 A Finite State Approach
We have seen in the introduction that mutations refer to a simple change in the first letter of certain words
in certain contexts. This phenomenon is essentially local (it does not require to take into account a large
context) so finite state transducers seem to be a relevant choice. These transducers will directly encode
the rules described in the grammar of Breton that just need to be made more formal.
Below (Figure 1) is an example of such a finite state transducer.
Figure 1: Graph MUT-Detfs-K-G
This graph directly encodes all the constraints involved in the process. Elements that appear in boxes
describe a linguistic sequence while elements appearing under the boxes correspond to the rewriting part
of the transducer (i.e. the transduction). Here is a description of the different elements that can be used
for the linguistic description:
? Tags between < and > refer to morphosyntactic categories (DET for determiner, N for noun, A
for adjective, etc.);
? The elements after the colon are morphological features (f: feminine , s: singular...);
? The # sign indicates a separator between words (e.g. blank spaces between words);
? A gray box refers to a subgraph (here LettreMin refers to all lowercase letters; please note that
the sequence described here corresponds to any sequence of letters between separators, i.e. tokens,
because of the recursion on the state itself);
? Other items appearing in a box correspond to characters (or lexical forms);
Here, we see clearly that K becomes G if the sequence is a fem. sing. noun appearing immediately
after a determiner. Notations correspond to the ones defined by the LADL team, see Silberztein (1993)
30
and Paumier (2011) ? other frameworks could of course be used like the Xerox FST toolbox (Beesley
and Karttunen, 2003).
Transducers provide a unified view of the different contraints along with a rewriting process. Recursive
transducers (RTN) make it possible to obtain a concise and effective formalization. Different linguistic
phenomena can be processed using a cascade of automata applied one after the other. For example, it
seems relevant to re-use the graph encoding noun mutations to process adjectives. If all the mutations
for nouns have been encoded and compiled in a single graph called MUT, it is then possible to write the
fllowing transducer (figure 2) for adjectives.
Figure 2: Graph MUT-Adj-K-G
MUT also encodes the constraints on the last vowel of the previous word (only adjectives following a
noun ending with a vowel or with l, m, n or r are subject to this mutation).
2.3 Implementation and evaluation
Local contextual grammars can be encoded using various techniques but finite state transducers seem
to be the most effective and readable way to encode these rules. This is in line with previous work:
for example Mittendorf and Sadler (2006) use the Xerox finite state transducer toolbox to implement
mutations in Welsh. Our proposal is very close to theirs.
Various other platforms allow the manipulation of finite state transducers for local grammars. Scripting
languages (like perl or python) also offer a good solution but these languages are made to manipulate
strings. However for mutations we need to have different information on the words themselves, hence
using a linguistic toolbox seems more appropriate.
The implementation of this linguistic phenomenon using finite state transducers produce a compact
and accurate description. Grammars are easy to modify and maintain. Additionally different grammars
could be developed to take into account local variations and dialects.
3 Discussion
We have presented a practical approach to process mutations in breton. The approach is based on well
known techniques (finite state transducers) that provide an accurate and efficient description of the phe-
nomenon. The technique used reveal the fact that mutation is essentially a morphosyntactic phenomenon,
as said in the introduction.
However, the main challenge does not lie in the proposed formalization. Endangered languages are
generally not well supported (lack of resources and automatic tools) and we think this kind of contribution
could have a positive impact on the evolution of the language. If relevant tools exist, it could be a way to
attract new attention and help language students acquire a good command of the language. Since a large
part of language learners study at home, having dynamic tools assisting text production would be a real
plus.
Adding explanation to the above rules would make it possible to generate suggestions during text
production or text revision. From this perspective, the description we provide could serve as a basis for a
spell checker of the language.
2
Detailed explanations would make the whole system usable for assisting
people during language learning (e.g. to explain why a given sequence in not fully correct in case a word
should appear with a mutation, etc.). This strategy could easily be re-used for other languages and other
linguistic phenomena.
2
Note that different initiatives exist to develop natural language tools for processing Breton. We should cite more specifically
the association Drouizig http://www.drouizig.org/ that has developed a spell checker independently of this study.
31
References
Kenneth R. Beesley and Lauri Karttunen. 2003. Finite State Morphology. CSLI Publications, Stanford.
Blandine Courtois and Max Silberztein, editors. 1990. Dictionnaires lectroniques du franc?ais, volume 87, Paris.
Larousse.
Roparz Hemon. 1984. Grammaire bretonne. Al Liamm, Brest.
Fransez Kervella. 1976. Yezhadur bras ar brezhoneg. Al Liamm, Brest.
Ingo Mittendorf and Louisa Sadler. 2006. A treatment of welsh initial mutations. In Proceedings of the LFG06
Conference, Universit?at Konstanz. CSLI.
S?ebastien Paumier. 2011. Unitex 3.0 User Manual. Universit de Marne la Vall?ee, Marne la Vall?ee.
Max Silberztein. 1993. Dictionnaires lectroniques et analyse automatique de textes : le systme INTEX. Masson,
Paris.
Gregory Stump. 1984. Agreement vs. incorporation in breton. Natural Language and Linguistic Theory, 2:289?
348.
32
