Inferring parts of speech for lexical mappings via the Cyc KB
Tom O?Hara?, Stefano Bertolo, Michael Witbrock, Bj?rn Aldag,
Jon Curtis, with Kathy Panton, Dave Schneider, and Nancy Salay
?Computer Science Department
New Mexico State University
Las Cruces, NM 88001
tomohara@cs.nmsu.edu
Cycorp, Inc.
Austin, TX 78731
{bertolo,witbrock,aldag}@cyc.com
{jonc,panton,daves,nancy}@cyc.com
Abstract
We present an automatic approach to learn-
ing criteria for classifying the parts-of-speech
used in lexical mappings. This will fur-
ther automate our knowledge acquisition sys-
tem for non-technical users. The criteria for
the speech parts are based on the types of
the denoted terms along with morphological
and corpus-based clues. Associations among
these and the parts-of-speech are learned us-
ing the lexical mappings contained in the Cyc
knowledge base as training data. With over
30 speech parts to choose from, the classifier
achieves good results (77.8% correct). Ac-
curate results (93.0%) are achieved in the
special case of the mass-count distinction for
nouns. Comparable results are also obtained
using OpenCyc (73.1% general and 88.4%
mass-count).
1 Introduction
In semantic lexicons, the term lexical mapping de-
scribes the relation between a concept and a phrase
used to refer to it (Onyshkevych and Nirenburg,
1995; Burns and Davis, 1999). Lexical mappings
include associated syntactic information, in partic-
ular, part of speech information for phrase head-
words. The term lexicalize will refer to the process
of producing these mappings, which are referred
to as lexicalizations. Selecting the part of speech
for the lexical mapping is required so that proper
inflectional variations can be recognized and gen-
erated for the term. Although producing lexi-
calizations is often a straightforward task, there
are many cases that can pose problems, especially
when fine-grained speech part categories are used.
For example, the headword ?painting? is a verb
in the phrase ?painting for money? but a noun
in the phrase ?painting for sale.? In cases like
this, semantic or pragmatic criteria, as opposed
to syntactic criteria only, are necessary for deter-
mining the proper part of speech. The headword
part of speech is important for correctly identifying
phrasal variations. For instance, the first term can
also occur in the same sense in ?paint for money.?
However, this does not hold for the second case,
since ?paint for sale? has an entirely different sense
(i.e., a substance rather than an artifact).
When lexical mappings are produced by naive
users, such as in DARPA?s Rapid Knowledge For-
mation (RKF) project, it is desirable that tech-
nical details such as the headword part of speech
be inferred for the user. Otherwise, often complex
and time-consuming clarification dialogs might be
necessary in order to rule out various possibilities.
For example, Cycorp?s Dictionary Assistant was
developed for RKF in order to allow non-technical
users to specify lexical mappings from terms into
the Cyc knowledge base (KB). Currently, when a
new type of activity is described, the user is asked
a series of questions about the ways of referring to
the activity. If the user enters the phrase ?painting
for money,? the system asks whether the phrases
?paint for money? and ?painted for money? are
suitable variations in order to determine whether
?painting? should be treated as a verb. Users find
such clarification dialogs distracting, since they are
more interested in entering domain rather than lin-
guistic knowledge. Regardless, it is often very diffi-
cult to produce prompts that make the distinction
intelligible to a linguistically naive user.
A special case of the lexicalization speech part
classification is the handling of the mass-count dis-
tinction. Having the ability to determine if a con-
cept takes a mass or count noun is useful not only
for parsing, but also for generation of grammat-
ical English. For example, automatically gener-
Collection: PhysicalDevice
Microtheory: ArtifactGVocabularyMt
isa: ExistingObjectType
genls: Artifact ComplexPhysicalObject
SolidTangibleProduct
Microtheory: ProductGMt
isa: ProductType
Figure 1: Type definition for PhysicalDevice.
ated web pages (e.g., based on search terms) oc-
casionally produce ungrammatical term variations
because this distinction is not addressed properly.
Although learner dictionaries provide informa-
tion on the mass-count distinction, they are not
suitable for this task because different senses of a
word are often conflated in the definitions for the
sake of simplicity. In cases like this, the word or
sense might be annotated as being both count and
mass, perhaps with examples illustrating the dif-
ferent usages. This is the case for ?chicken? from
the Cambridge International Dictionary of English
(Procter, 1995), defined as follows:
a type of bird kept on a farm for its eggs or
its meat, or the meat of this bird which is
cooked and eaten
This work describes an approach for automati-
cally inferring the parts of speech for lexical map-
pings, using the existing lexical assertions in the
Cyc KB. We are specifically concerned with select-
ing parts of speech for entries in a semantic lexi-
con, not about determining parts of speech in con-
text. After an overview of the Cyc KB in the next
section, Section 3 discusses the approach taken to
inferring the part of speech for lexicalizations. Sec-
tion 4 then covers the classification results. This is
followed by a comparison to related work in Section
5.
2 Cyc knowledge base
In development since 1984, the Cyc knowledge base
(Lenat, 1995) is the world?s largest formalized rep-
resentation of commonsense knowledge, contain-
ing over 120,000 concepts and more than a mil-
lion axioms.1 Cyc?s upper ontology describes
the most general and fundamental of distinctions
(e.g., tangibility versus intangibility). The lower
ontology contains facts useful for particular appli-
cations, such as web searching, but not necessar-
ily required for commonsense reasoning (e.g., that
1These figures and the results discussed later are
based on Cyc KB version 576 and OpenCyc KB version
567.
?Dubya? refers to President GeorgeW.Bush). The
KB also includes a broad-coverage English lexicon
mapping words and phrases to terms throughout
the KB. A subset of the Cyc KB including parts of
the English lexicon has been made freely available
as part of OpenCyc (www.opencyc.org).
2.1 Ontology
Central to the Cyc ontology is the concept collec-
tion, which corresponds to the familiar notion of
a set, but with membership intensionally defined
(so distinct collections can have identical members,
which is impossible for sets). Every object in the
Cyc ontology is a member (or instance, in Cyc
parlance) of one or more collections. Collection
membership is expressed using the predicate (i.e.,
relation-type) isa, whereas collection subsumption
is expressed using the transitive predicate genls
(i.e., generalization). These predicates correspond
to the set-theoretic notions element of and subset
of respectively and thus are used to form a partially
ordered hierarchy of concepts. For the purposes of
this discussion, the isa and genls assertions on a
Cyc term constitute its type definition.
Figure 1 shows the type definition for Physi-
calDevice, a prototypical denotatum term for count
nouns. The type definition of PhysicalDevice indi-
cates that it is a collection that is a specialization
of Artifact, etc. As is typical for terms referred to
by count nouns, it is an instance of the collection
ExistingObjectType.
Figure 2 shows the type definition for Water, a
prototypical denotation for mass nouns. Although
the asserted type information for Water does not
convey any properties that would suggest a mass
noun lexicalization, the genls hierarchy of collec-
tions does. In particular, the collection Chemical-
CompoundTypeByChemicalSpecies is known to be
a specialization of the collection ExistingStuffType,
via the transitive properties of genls. Thus, by
virtue of being an instance of ChemicalCompound-
TypeByChemicalSpecies, Water is known to be an
instance of ExistingStuffType. This illustrates that
the decision procedure for the lexical mapping
speech parts needs to consider not only asserted,
but also inherited collection membership.
2.2 English lexicon
Natural language lexicons are integrated directly
into the Cyc KB (Burns and Davis, 1999). Though
several lexicons are included in the KB, the English
lexicon is the only one with general coverage. The
mapping from nouns to concepts is done using one
of two general strategies, depending on whether the
Collection: Water
Microtheory: UniversalVocabularyMt
isa: ChemicalCompoundTypeByChemicalSpecies
Microtheory: UniversalVocabularyMt
genls: Individual
Microtheory: NaivePhysicsVocabularyMt
genls: Oxide
Figure 2: Type definition for Water.
mapping is from a name or a common noun phrase.
Several different binary predicates indicate name-
to-term mappings, with the name represented as a
string. For example,
(nameString HEBCompany ?HEB?)
A denotational assertion maps a phrase into a
concept, usually a collection. The phrase is spec-
ified via a lexical word unit (i.e., lexeme concept)
with optional string modifiers. The part of speech
is specified via one of Cyc?s SpeechPart constants.
Syntactic information, such as the wordform vari-
ants and their speech parts, is stored with the Cyc
constant for the word unit. For example, Device-
TheWord, the Cyc constant for the word ?device,?
has a single syntactic mapping since the plural
form is inferable:
Constant: Device-TheWord
Microtheory: GeneralEnglishMt
isa: EnglishWord
posForms: CountNoun
singular: ?device?
The simplest type of denotational mapping asso-
ciates a particular sense of a word with a concept
via the denotation predicate. For example,
(denotation Device-Word CountNoun 0
PhysicalDevice)
This indicates that sense 0 of the count noun ?de-
vice? refers to PhysicalDevice via the associated
wordforms ?device? and ?devices.?
To account for phrasal mappings, three addi-
tional predicates are used, depending on the lo-
cation of the headword in the phrase. These
are compoundString, headMedialString, and mul-
tiWordString for phrases with the headword at the
beginning, the middle, and the end, respectively.
For example,
(compoundString Buy-TheWord (?down?)
Verb BuyDown)
Usage
Predicate OpenCyc Cyc
multiWordString 1123 24606
denotation 2080 16725
compoundString 318 2226
headMedialString 200 942
total 3721 44499
Table 1: Denotational predicate usage in Cyc
English lexicon. This excludes slang and jargon.
Usage
SpeechPart OpenCyc Cyc
CountNoun 2041 21820
MassNoun 566 9993
Adjective 262 6460
Verb 659 2860
AgentiveNoun 81 1389
ProperCountNoun 16 906
Adverb 50 310
ProperMassNoun 1 286
GerundiveNoun 7 275
other 39 185
total 3721 44499
Table 2: Most common speech parts in deno-
tational assertions. The other entry covers 20
infrequently used cases.
This states that ?buy down? refers to BuyDown,
as do ?buys down,? ?buying down,? and ?bought
down? based on the inflections of the verb ?buy.?
Table 1 shows the frequency of the various pred-
icates used in the denotational assertions, exclud-
ing lexicalizations that involve technical, informal
or slang terms. Table 2 shows the most frequent
speech parts from these assertions. This shows
that nearly 50% of the cases use CountNoun for
the headword speech part and that about 25% use
MassNoun. This subset of the denotational asser-
tions forms the basis of the training data used in
the mass versus count noun classifier, as discussed
later. Twenty other speech parts used in the lexi-
con are not shown. Several of these are quite spe-
cialized (e.g., QuantifyingIndexical) and not very
common, mainly occurring in fixed phrases. The
full speech part classifier handles all categories.
3 Inference of default part of
speech
Our method of inferring the part of speech for lexi-
calizations is to apply machine learning techniques
over the lexical mappings from English words or
phrases to Cyc terms. For each target denota-
tum term, the corresponding types and general-
izations are extracted from the ontology. This in-
cludes terms for which the denotatum term is an
instance or specialization, either explicitly asserted
or inferable via transitivity. For simplicity, these
are referred to as ancestor terms. The associa-
tion between the lexicalization parts of speech and
the common ancestor terms forms the basis for the
main criteria used in the lexicalization speech part
classifier and the special case for the mass-count
classifier. In addition, this is augmented with fea-
tures indicating whether known suffixes occur in
the headword as well as with corpus statistics.
3.1 Cyc ancestor term features
There are several possibilities in mapping the Cyc
ancestor terms into a feature vector for use in
machine learning algorithms. The most direct
method is to have a binary feature for each pos-
sible ancestor term, but this would require about
ten thousand features. To prune the list of poten-
tial features, frequency considerations can be ap-
plied, such as taking the most frequent terms that
occur in type definition assertions. Alternatively,
the training data can be analyzed to see which ref-
erence terms are most correlated with the classifi-
cations.
For simplicity, the frequency approach is used
here. The most-frequent 1024 atomic terms are se-
lected, excluding terms used for bookkeeping pur-
poses (e.g., PublicConstant, which mark terms for
public releases of the KB); half of these terms are
taken from the isa assertions, and the other half
from the genls assertions. These are referred to
as the reference terms. For instance, ObjectType
is a type for 21,108 of the denotation terms (out
of 44,449 cases), compared to 20,283 for StuffType.
These occur at ranks 13 and 14, so they are both
included. In contrast, SeparationEvent occurs only
185 times as a generalization term at rank 522, so
it is pruned. See (O?Hara et al, 2003) for more
details on extracting the reference term features.
3.2 Morphology and corpus-based
features
In English, the suffix for a word can provide a good
clue as to the speech part of a word. For exam-
ple, agentive nouns commonly end in ?-or? or ?-er.?
Features to account for this are derived by seeing
whether the headword ends in one of a predefined
set of suffixes and adding the suffix as a value to
an enumerated feature variable corresponding to
suffixes of the given length. Currently, the suffixes
Feature Search Pattern
singular ?singular?
plural ?plural?
count ?many ?plural?? or ?several ?plural??
mass ?much ?singular?? or ?several ?singular??
verb ?must ?head?? or ?could ?head??
adverb ?did ?head?? or ?do ?head?? or
?does ?head?? or ?so ?head?? or
?has ?head? been? or ?have ?head? been?
adjective ?more ?head?? or ?most ?head?? or
?very ?head??
Figure 3: Corpus pattern templates for part-
of-speech clues. The placeholders refer to word-
forms derived from the headword: ?plural? and
?singular? are derived via morphology; ?head? uses
the headword as is.
used are the most-common two to four letter se-
quences found in the headwords.
Often the choice of speech parts for lexicaliza-
tions reflects idiosyncratic usages rather than just
underlying semantics. To account for this, a set of
features is included that is based on the relative
frequency that the denotational headword occurs
in contexts that are indicative of each of the main
speech parts: singular, plural, count, mass, verbal,
adjectival, and adverbial. See Figure 3. These pat-
terns were determined by analyzing part-of-speech
tagged text and seeing which function words co-
occur predominantly in the immediate context for
words of the given grammatical category. Note
that high frequency function words such as ?to?
were not considered because they are usually not
indexed for information retrieval.
These features are derived as follows. Given
a lexical assertion (e.g., (denotation Hound-
TheWord CountNoun 0 Dog)), the headword is
extracted and then the plural or singular variant
wordform is derived for use in the pattern tem-
plates. Corpus checks are done for each, producing
a vector of frequency counts (e.g., ?29, 17, 0, 0, 0,
0, 0?). These counts are then normalized and then
used as numeric features for the machine learning
algorithm. Table 3 shows the results for the hound
example and with a few other cases.
3.3 Sample criteria
We use decision trees for this classification. Part
of the motivation is that the result is readily in-
terpretable and can be incorporated directly by
knowledge-based applications. Decision trees are
induced in a process that recursively splits the
training examples based on the feature that parti-
Head Sing Plural Count Mass Verb Adv Adj
hound .630 .370 0 0 0 0 0
book .613 .371 .011 .001 0 .002 .001
wood .577 .418 0 .004 0 .001 .001
leave .753 .215 0 0 .024 .008 0
fast .924 .003 0 .003 .001 .043 .027
stormy .981 0 0 0 0 0 .019
Table 3: Sample relative frequency values
from corpus checks.
if (genls Event) and
(genls not ? {ConsumingFoodOrDrink,
SeasonOfYear, QualitativeTimeOfDay,
SocialGathering, PrecipitationProcess,
SimpleRepairing, ConflictEvent,
SomethingAppearingSomewhere}) and
(isa not PhysiologicalConditionType) and
(f-Plural ? 0.245) then
if (Suffix ? {ine, een} then Verb
if (Suffix ? {ile, ent} then CountNoun
if (Suffix = ing) then MassNoun
if (Suffix = ion) then
if (f-Mass > 0.026) then MassNoun
else Verb
if (Suffix = ite) then CountNoun
if (Suffix ? {ide, ure, ous} then Verb
if (Suffix = ive) and
(genls Perceiving) then MassNoun
else CountNoun
if (Suffix = ate) then
if (not genls InformationStore) and
(f-Count ? 0.048) and
(f-Adverb ? 0.05) then
if (gens Translocation) then MassNoun
else CountNoun
Figure 4: Sample rule from the general
speech part classifier.
tions the current set of examples to maximize the
information gain (Witten and Frank, 1999). This is
commonly done by selecting the feature that min-
imizes the entropy of the distribution (i.e., yields
least uniform distribution). A fragment of the de-
cision tree is shown to give an idea of the criteria
being considered in the speech part classification.
See Figure 4. In this example, the semantic types
mostly provide exceptions to associations inferred
from the suffixes, with corpus clues used occasion-
ally for differentiation.
4 Evaluation and results
To test out the performance of the speech part clas-
sification, 10-fold cross validation is applied to each
configuration that was considered. Except as noted
below, all the results are produced using Weka?s
J4.8 classifier (Witten and Frank, 1999), which
is an implementation of Quillian?s C4.5 (Quinlan,
1993) decision tree learner. Other classifiers were
considered as well (e.g., Naive Bayes and nearest
neighbor), but J4.8 generally gave the best overall
results.
4.1 Results for mass-count distinction
Table 4 shows the results for the special case mass-
count classification. This shows that the system
achieves an accuracy of 93.0%, an improvement
of 24.4 percentage points over the standard base-
line of always selecting the most frequent case (i.e.,
count noun). Other baselines are included for com-
parison purposes. For example, using the head-
word as the sole feature (just-headwords) performs
fairly well compared to the system based on Cyc;
but, this classifier would lack generalizability, re-
lying simply upon table lookup. (In this case, the
decision tree induction process ran into memory
constraints, so a Naive Bayes classifier was used
instead.) In addition, a system only based on
the suffixes (just-suffixes) performs marginally bet-
ter than always selecting the most common case.
Thus, morphology alone would not be adequate for
this task. The OpenCyc version of the classifier
also performs well. This illustrates that sufficient
data is already available in OpenCyc to allow for
good approximations for such classifications. Note
that for the mass-count experiments and for the
experiments discussed later, the combined system
over full Cyc leads to statistically significant im-
provements compared to the other cases.
4.2 Results for general speech part
classification
Running the same classifier setup over all speech
parts produces the results shown in Table 5. The
overall result is not as high, but there is a similar
improvement over the baselines. Relying solely on
suffixes or on corpus checks performs slightly bet-
ter than the baseline. Using headwords performs
well, but again that amounts to table lookup. In
terms of absolute accuracy it might seem that the
system based on OpenCyc is doing nearly as well
as the system based on full Cyc. This is somewhat
misleading, since the distribution of parts of speech
is simpler in OpenCyc, as shown by the lower en-
tropy value (Jurafsky and Martin, 2000).
5 Related work
There has not been much work in the automatic de-
termination of the preferred lexicalization part of
speech, outside of work related to part-of-speech
tagging (Brill, 1995), which concentrates on the
Dataset Characteristics
OpenCyc Cyc
Instances 2607 30676
Classes 2 2
Entropy 0.76 0.90
Accuracy Figures
OpenCyc Cyc
Baseline 78.3 68.6
Just-headwords 87.5 89.3
Just-suffixes 78.3 71.9
Just-corpus 78.2 68.6
Just-terms 87.4 90.5
Combination 88.4 93.0
Table 4: Mass-count classification over Cyc
lexical mappings. Instances is size of the train-
ing data. Classes is the number of choices. En-
tropy characterizes distribution uniformity. Base-
line uses more frequent case. The just-X en-
tries incorporate a single type: headwords from
lexical mapping, suffixes of headword, corpus co-
occurrence of part-of-speech indicators; and Cyc
reference terms. Combination uses all features ex-
cept for the headwords. For Cyc, it yields a statis-
tically significant improvement over the others at
p < .01 using a paired t-test.
Dataset Characteristics
OpenCyc Cyc
Instances 3721 44499
Classes 16 34
Entropy 1.95 2.11
Accuracy Figures
OpenCyc Cyc
Baseline 54.9 48.6
Just-headwords 61.6 73.8
Just-suffixes 55.6 53.0
Just-corpus 63.1 49.0
Just-terms 68.2 71.3
Combination 73.1 77.8
Table 5: Full speech part classification over
Cyc lexical mappings. All speech parts in Cyc
are used. See Table 4 for legend.
sequences of speech tags rather than the default
tags. Brill uses an error-driven transformation-
based learning approach that learns lists for trans-
forming the initial tags assigned to the sentence.
Unknown words are handled basically via rules
that change the default assignment to another
based on the suffixes of the unknown word. Ped-
ersen and Chen (1995) discuss an approach to
inferring the grammatical categories of unknown
words using constraint solving over the properties
of the known words. Toole (2000) applies decision
trees to a similar problem, distinguishing common
nouns, pronouns, and various types of names, using
a framework analogous to that commonly applied
in named-entity recognition.
In work closer to ours, Woods (2000) describes
an approach to this problem using manually con-
structed rules incorporating syntactic, morpholog-
ical, and semantic tests (via an ontology). For
example, patterns targeting specific stems are ap-
plied provided that the root meets certain semantic
constraints. There has been clustering-based work
in part-of-speech induction, but these tend to tar-
get idiosyncratic classes, such as capitalized words
and words ending in ?-ed? (Clark, 2003).
The special case of classifying the mass-count
distinction has received some attention. Bond and
Vatikiotis-Bateson (2002) infer five types of count-
ability distinctions using NT&T?s Japanese to En-
glish transfer dictionary, including the categories
strongly countable, weakly countable, and plural
only. The countability assigned to a particular
semantic category is based on the most common
case associated with the English words mapping
into the category. Our earlier work (O?Hara et al,
2003) just used semantic features as well but ac-
counted for inheritance of types, achieving 89.5%
with a baseline of 68.2%. Schwartz (2002) uses the
five NT&T countability distinctions when tagging
word occurrences in a corpus (i.e., word tokens),
based primarily on clues provided by determiners.
Results are given in terms of agreement rather than
accuracy; compared to NT&T?s dictionary there is
about 90% agreement for the fully or strong count-
able types and about 40% agreement for the weakly
countable or uncountable types, with half of the
tokens left untagged for countability. Baldwin and
Bond (2003) apply sophisticated preprocessing to
derive a variety of countability clues, such as gram-
matical number of modifiers, co-occurrence of spe-
cific types of determiners and pronouns, and spe-
cific types of prepositions. They achieve 94.6% ac-
curacy using four categories of countability, includ-
ing two categories for types of plural-only nouns.
Since multiple assignments are allowed, negative
agreement is considered as well as positive. When
restricted to just count versus mass nouns, the ac-
curacy is 89.9% (personal communication). Note
that, as with Schwartz, the task is different from
ours and that of Bond and Vatikiotis-Bateson: we
assign countability to word/concept pairs instead
of just to words.
6 Conclusion and future work
This paper shows that an accurate decision pro-
cedure (93.0%) accounting for the mass-count dis-
tinction can be induced from the lexical mappings
in the Cyc KB. The full speech part classifier pro-
duces promising results (77.8%), considering that
it is a much harder task, with over 30 categories to
choose from. The features incorporate semantic in-
formation, in particular Cyc?s ontological types, in
addition to syntactic information (e.g., headword
morphology).
Future work will investigate how the classifiers
can be generalized for classifying word usages in
context, rather than isolated words. This could
complement existing part-of-speech taggers by al-
lowing for more detailed tag types, such as for
count and agentive nouns.
A separate area for future work will be to ap-
ply the techniques to other languages. For exam-
ple, minimal changes to the classifier setup would
be required to handle Romance languages, such
as Italian. The version of the classifier that just
uses Cyc reference terms could be applied as is,
given lexical mappings for the language. For the
combined-feature classifier, we would just need to
change the list of suffixes and the part-of-speech
pattern templates (from Figure 3).
Acknowledgements
The lexicon work at Cycorp has been supported in part
by grants from NIST, DARPA (e.g., RKF), and ARDA
(e.g., AQUAINT). At NMSU, the work was facilitated
by a GAANN fellowship from the Department of Edu-
cation and utilized computing resources made possible
through MII Grants EIA-9810732 and EIA-0220590.
References
Timothy Baldwin and Francis Bond. 2003. Learn-
ing the countability of English nouns from cor-
pus data. In Proc. ACL-03.
Francis Bond and Caitlin Vatikiotis-Bateson.
2002. Using an ontology to determine English
countability. In Proc. COLING-2002, pages 99?
105. Taipei.
Eric Brill. 1995. Transformation-based error-
driven learning and natural language processing:
A case study in part of speech tagging. Compu-
tational Linguistics, 21(4):543?565.
Kathy J. Burns and Anthony B. Davis. 1999.
Building and maintaining a semantically ade-
quate lexicon using Cyc. In Evelyn Viegas, ed-
itor, Breadth and Depth of Semantic Lexicons,
pages 121?143. Kluwer, Dordrecht.
Alexander Clark. 2003. Combining distributional
and morphological information for part of speech
induction. In Proceedings of EACL 2003.
Daniel Jurafsky and James H. Martin. 2000.
Speech and Language Processing. Prentice Hall,
Upper Saddle River, New Jersey.
D. B. Lenat. 1995. Cyc: A large-scale investment
in knowledge infrastructure. Communications of
the ACM, 38(11).
Tom O?Hara, Nancy Salay, Michael Witbrock,
Dave Schneider, Bjoern Aldag, Stefano Bertolo,
Kathy Panton, Fritz Lehmann, Matt Smith,
David Baxter, Jon Curtis, and Peter Wagner.
2003. Inducing criteria for mass noun lexical
mappings using the Cyc KB, and its extension
to WordNet. In Proc. Fifth International Work-
shop on Computational Semantics (IWCS-5).
B. Onyshkevych and S. Nirenburg. 1995. A lexicon
for knowledge-based MT. Machine Translation,
10(2):5?57.
Ted Pedersen and Weidong Chen. 1995. Lexi-
cal acquisition via constraint solving. In Proc.
AAAI 1995 Spring Symposium Series.
Paul Procter, editor. 1995. Cambridge Interna-
tional Dictionary of English. Cambridge Uni-
versity Press, Cambridge.
J. Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann, San Mateo,
California.
Lane O.B. Schwartz. 2002. Corpus-based acquisi-
tion of head noun countability features. Master?s
thesis, Cambridge University, Cambridge, UK.
Janine Toole. 2000. Categorizing unknown words:
Using decision trees to identify names and mis-
spellings. In Proc. ANLP-2000.
Ian H. Witten and Eibe Frank. 1999. Data
Mining: Practical Machine Learning Tools and
Techniques with Java Implementations. Morgan
Kaufmann, San Francisco, CA.
W. Woods. 2000. Aggressive morphology for ro-
bust lexical coverage. In Proc. ANLP-00.
Preposition Semantic Classification via PENN TREEBANK and FRAMENET
Tom O?Hara
Department of Computer Science
New Mexico State University
Las Cruces, NM 88003
tomohara@cs.nmsu.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Abstract
This paper reports on experiments in clas-
sifying the semantic role annotations as-
signed to prepositional phrases in both the
PENN TREEBANK and FRAMENET. In
both cases, experiments are done to see
how the prepositions can be classified
given the dataset?s role inventory, using
standard word-sense disambiguation fea-
tures. In addition to using traditional word
collocations, the experiments incorporate
class-based collocations in the form of
WordNet hypernyms. For Treebank, the
word collocations achieve slightly better
performance: 78.5% versus 77.4% when
separate classifiers are used per preposi-
tion. When using a single classifier for
all of the prepositions together, the com-
bined approach yields a significant gain at
85.8% accuracy versus 81.3% for word-
only collocations. For FrameNet, the
combined use of both collocation types
achieves better performance for the indi-
vidual classifiers: 70.3% versus 68.5%.
However, classification using a single
classifier is not effective due to confusion
among the fine-grained roles.
1 Introduction
English prepositions convey important relations in
text. When used as verbal adjuncts, they are the prin-
ciple means of conveying semantic roles for the sup-
porting entities described by the predicate. Preposi-
tions are highly ambiguous. A typical collegiate dic-
tionary has dozens of senses for each of the common
prepositions. These senses tend to be closely related,
in contrast to the other parts of speech where there
might be a variety of distinct senses.
Given the recent advances in word-sense disam-
biguation, due in part to SENSEVAL (Edmonds and
Cotton, 2001), it would seem natural to apply the
same basic approach to handling the disambiguation
of prepositions. Of course, it is difficult to disam-
biguate prepositions at the granularity present in col-
legiate dictionaries, as illustrated later. Nonetheless,
in certain cases this is feasible.
We provide results for disambiguating preposi-
tions at two different levels of granularity. The
coarse granularity is more typical of earlier work in
computational linguistics, such as the role inventory
proposed by Fillmore (1968), including high-level
roles such as instrument and location. Recently, sys-
tems have incorporated fine-grained roles, often spe-
cific to particular domains. For example, in the Cyc
KB there are close to 200 different types of seman-
tic roles. These range from high-level roles (e.g.,
beneficiaries) through medium-level roles (e.g., ex-
changes) to highly specialized roles (e.g., catalyst).1
Preposition classification using two different se-
mantic role inventories are investigated in this pa-
per, taking advantage of large annotated corpora.
After providing background to the work in Sec-
tion 2, experiments over the semantic role anno-
tations are discussed in Section 3. The results
over TREEBANK (Marcus et al, 1994) are covered
first. Treebank include about a dozen high-level
roles similar to Fillmore?s. Next, experiments us-
ing the finer-grained semantic role annotations in
FRAMENET version 0.75 (Fillmore et al, 2001) are
1Part of the Cyc KB is freely available at www.opencyc.org.
presented. FrameNet includes over 140 roles, ap-
proaching but not quite as specialized as Cyc?s in-
ventory. Section 4 follows with a comparison to
related work, emphasizing work in broad-coverage
preposition disambiguation.
2 Background
2.1 Semantic roles in the PENN TREEBANK
The second version of the Penn Treebank (Marcus
et al, 1994) added additional clause usage informa-
tion to the parse tree annotations that are popular
for natural language learning. This includes a few
case-style relation annotations, which prove useful
for disambiguating prepositions. For example, here
is a simple parse tree with the new annotation for-
mat:
(S (NP-TPC-5 This)
(NP-SBJ every man)
(VP contains
(NP *T*-5)
(PP-LOC within
(NP him))))
This shows that the prepositional phrase (PP) is pro-
viding the location for the state described by the verb
phrase. Treating this as the preposition sense would
yield the following annotation:
This every man contains within
LOC
him
The main semantic relations in TREEBANK are
beneficiary, direction, spatial extent, manner, loca-
tion, purpose/reason, and temporal. These tags can
be applied to any verb complement but normally oc-
cur with clauses, adverbs, and prepositions. Fre-
quency counts for the prepositional phrase (PP) case
role annotations are shown in Table 1.
The frequencies for the most frequent preposi-
tions that have occurred in the prepositional phrase
annotations are shown later in Table 7. The table
is ordered by entropy, which measures the inherent
ambiguity in the classes as given by the annotations.
Note that the Baseline column is the probability of
the most frequent sense, which is a common esti-
mate of the lower bound for classification experi-
ments.
2.2 Semantic roles in FRAMENET
Berkeley?s FRAMENET (Fillmore et al, 2001)
project provides the most recent large-scale anno-
tation of semantic roles. These are at a much finer
granularity than those in TREEBANK, so they should
prove quite useful for applications that learn detailed
semantics from corpora. Table 2 shows the top se-
mantic roles by frequency of annotation. This il-
lustrates that the semantic roles in Framenet can be
quite specific, as in the roles cognizer, judge, and
addressee. In all, there are over 140 roles annotated
with over 117,000 tagged instances.
FRAMENET annotations occur at the phrase level
instead of the grammatical constituent level as in
TREEBANK. The cases that involve prepositional
phrases can be determined by the phrase-type at-
tribute of the annotation. For example, consider the
following annotation.
?S TPOS=?56879338??
?T TYPE=?sense2???/T?
Itpnp hadvhd aat0 sharpaj0
,pun pointedaj0 facenn1 andcjc
?C FE=?BodP? PT=?NP? GF=?Ext??
aat0 featheryaj0 tailnn1 thatcjt
?/C? ?C TARGET=?y?? archedvvd?/C?
?C FE=?Path? PT=?PP? GF=?Comp??
overavp?prp itsdps backnn1
?/C? .pun?/S?
The constituent (C) tags identify the phrases that
have been annotated. The target attribute indicates
the predicating word for the overall frame. The
frame element (FE) attribute indicates one of the se-
mantic roles for the frame, and the phrase type (PT)
attribute indicates the grammatical function of the
phrase. We isolate the prepositional phrase annota-
tion and treat it as the sense of the preposition. This
yields the following annotation:
It had a sharp, pointed face and a feathery
tail that arched over
Path
its back.
The annotation frequencies for the most frequent
prepositions are shown later in Table 8, again or-
dered by entropy. This illustrates that the role dis-
tributions are more complicated, yielding higher en-
tropy values on average. In all, there are over 100
prepositions with annotations, 65 with ten or more
instances each.
Tag Freq Description
pp-loc 17220 locative
pp-tmp 10572 temporal
pp-dir 5453 direction
pp-mnr 1811 manner
pp-prp 1096 purpose/reason
pp-ext 280 spatial extent
pp-bnf 44 beneficiary
Table 1: TREEBANK semantic roles for PP?s. Tag
is the label for the role in the annotations. Freq is
frequency of the role occurrences.
Tag Freq Description
Spkr 8310 speaker
Msg 7103 message
SMov 6778 self-mover
Thm 6403 theme
Agt 5887 agent
Goal 5560 goal
Path 5422 path
Cog 4585 cognizer
Manr 4474 manner
Src 3706 source
Cont 3662 content
Exp 3567 experiencer
Eval 3108 evaluee
Judge 3107 judge
Top 3074 topic
Other 2531 undefined
Cause 2306 cause
Add 2266 addressee
Src-p 2179 perceptual source
Phen 1969 phenomenon
Reas 1789 reason
Area 1328 area
Degr 1320 degree
BodP 1230 body part
Prot 1106 protagonist
Table 2: Common FRAMENET semantic roles. The
top 25 of 141 roles are shown.
3 Classification experiments
The task of selecting the semantic roles for the
prepositions can be framed as an instance of word-
sense disambiguation (WSD), where the semantic
roles serve as the senses for the prepositions.
A straightforward approach for preposition dis-
ambiguation would be to use standard WSD fea-
tures, such as the parts-of-speech of surrounding
words and, more importantly, collocations (e.g., lex-
ical associations). Although this can be highly ac-
curate, it will likely overfit the data and generalize
poorly. To overcome these problems, a class-based
approach is used for the collocations, with WordNet
high-level synsets as the source of the word classes.
Therefore, in addition to using collocations in the
form of other words, this uses collocations in the
form of semantic categories.
A supervised approach for word-sense disam-
biguation is used following Bruce and Wiebe (1999).
The results described here were obtained using the
settings in Figure 1. These are similar to the set-
tings used by O?Hara et al (2000) in the first
SENSEVAL competition, with the exception of the
hypernym collocations. This shows that for the hy-
pernym associations, only those words that occur
within 5 words of the target prepositions are con-
sidered.2
The main difference from that of a standard WSD
approach is that, during the determination of the
class-based collocations, each word token is re-
placed by synset tokens for its hypernyms in Word-
Net, several of which might occur more than once.
This introduces noise due to ambiguity, but given
the conditional-independence selection scheme, the
preference for hypernym synsets that occur for dif-
ferent words will compensate somewhat. O?Hara
and Wiebe (2003) provide more details on the ex-
traction of these hypernym collocations. The fea-
ture settings in Figure 1 are used in two different
configurations: word-based collocations alone, and
a combination of word-based and hypernym-based
collocations. The combination generally produces
2This window size was chosen after estimating that on aver-
age the prepositional objects occur within 2.35+/? 1.26 words
of the preposition and that the average attachment site is within
3.0 +/? 2.98 words. These figures were produced by ana-
lyzing the parse trees for the semantic role annotations in the
PENN TREEBANK.
Features:
POS?2 part-of-speech 2 words to left
POS?1: part-of-speech 1 word to left
POS+1: part-of-speech 1 word to right
POS+2: part-of-speech 2 words to right
Prep preposition being classified
WordColl
i
: word collocation for role i
HypernymColl
i
: hypernym collocation for role i
Collocation Context:
Word: anywhere in the sentence
Hypernym: within 5 words of target preposition
Collocation selection:
Frequency: f(word) > 1
CI threshold: p(c|coll)?p(c)
p(c)
>= 0.2
Organization: per-class-binary
Model selection:
overall classifier: Decision tree
individual classifiers: Naive Bayes
10-fold cross-validation
Figure 1: Feature settings used in the preposi-
tion classification experiments. CI refers to condi-
tional independence; the per-class-binary organiza-
tion uses a separate binary feature per role (Wiebe et
al., 1998).
the best results. This exploits the specific clues pro-
vided by the word collocations while generalizing to
unseen cases via the hypernym collocations.
3.1 PENN TREEBANK
To see how these conceptual associations are de-
rived, consider the differences in the prior versus
class-based conditional probabilities for the seman-
tic roles of the preposition ?at? in TREEBANK. Ta-
ble 3 shows the global probabilities for the roles as-
signed to ?at?. Table 4 shows the conditional prob-
Relation P(R) Example
locative .732 workers at a factory
temporal .239 expired at midnight Tuesday
manner .020 has grown at a sluggish pace
direction .006 CDs aimed at individual investors
Table 3: Prior probabilities of semantic relations for
?at? in TREEBANK. P (R) is the relative frequency.
Example usages are taken from the corpus.
Category Relation P(R|C)
ENTITY#1 locative 0.86
ENTITY#1 temporal 0.12
ENTITY#1 other 0.02
ABSTRACTION#6 locative 0.51
ABSTRACTION#6 temporal 0.46
ABSTRACTION#6 other 0.03
Table 4: Sample conditional probabilities of seman-
tic relations for ?at? in TREEBANK. Category is
WordNet synset defining the category. P (R|C) is
probability of the relation given that the synset cate-
gory occurs in the context.
Relation P(R) Example
addressee .315 growled at the attendant
other .092 chuckled heartily at this admission
phenomenon .086 gazed at him with disgust
goal .079 stationed a policeman at the gate
content .051 angry at her stubbornness
Table 5: Prior probabilities of semantic relations for
?at? in FRAMENET for the top 5 of 40 applicable
roles.
Category Relation P(R|C)
ENTITY#1 addressee 0.28
ENTITY#1 goal 0.11
ENTITY#1 phenomenon 0.10
ENTITY#1 other 0.09
ENTITY#1 content 0.03
ABSTRACTION#6 addressee 0.22
ABSTRACTION#6 other 0.14
ABSTRACTION#6 goal 0.12
ABSTRACTION#6 phenomenon 0.08
ABSTRACTION#6 content 0.05
Table 6: Sample conditional probabilities of seman-
tic relations for ?at? in FRAMENET
abilities for these roles given that certain high-level
WordNet categories occur in the context. These cat-
egory probability estimates were derived by tabulat-
ing the occurrences of the hypernym synsets for the
words occurring within a 5-word window of the tar-
get preposition. In a context with a concrete concept
(ENTITY#1), the difference in the probability dis-
tributions shows that the locative interpretation be-
comes even more likely. In contrast, in a context
with an abstract concept (ABSTRACTION#6), the
difference in the probability distributions shows that
the temporal interpretation becomes more likely.
Therefore, these class-based lexical associations re-
flect the intuitive use of the prepositions.
The classification results for these prepositions
in the PENN TREEBANK show that this approach is
very effective. Table 9 shows the results when all
of the prepositions are classified together. Unlike
the general case for WSD, the sense inventory is
the same for all the words here; therefore, a sin-
gle classifier can be produced rather than individ-
ual classifiers. This has the advantage of allowing
more training data to be used in the derivation of
the clues indicative of each semantic role. Good ac-
curacy is achieved when just using standard word
collocations. Table 9 also shows that significant
improvements are achieved using a combination of
both types of collocations. For the combined case,
the accuracy is 86.1%, using Weka?s J48 classifier
(Witten and Frank, 1999), which is an implementa-
tion of Quinlan?s (1993) C4.5 decision tree learner.
For comparison, Table 7 shows the results for indi-
vidual classifiers created for each preposition (using
Naive Bayes). In this case, the word-only colloca-
tions perform slightly better: 78.5% versus 77.8%
accuracy.
3.2 FRAMENET
It is illustrative to compare the prior probabilities
(i.e., P(R)) for FRAMENET to those seen earlier
for ?at? in TREEBANK. See Table 5 for the most
frequent roles out of the 40 cases that were as-
signed to it. This highlights a difference between
the two sets of annotations. The common tempo-
ral role from TREEBANK is not directly represented
in FRAMENET, and it is not subsumed by another
specific role. Similarly, there is no direct role cor-
responding to locative, but it is partly subsumed by
Dataset Statistics
Instances 26616
Classes 7
Entropy 1.917
Baseline 0.480
Experiment Accuracy STDEV
Word Only 81.1 .996
Combined 86.1 .491
Table 9: Overall results for preposition disambigua-
tion with TREEBANK semantic roles. Instances is
the number of role annotations. Classes is the
number of distinct roles. Entropy measures non-
uniformity of the role distributions. Baseline selects
the most-frequent role. The Word Only experiment
just uses word collocations, whereas Combined uses
both word and hypernym collocations. Accuracy is
average for percent correct over ten trials in cross
validation. STDEV is the standard deviation over the
trails. The difference in the two experiments is sta-
tistically significant at p < 0.01.
Dataset Statistics
Instances 27300
Classes 129
Entropy 5.127
Baseline 0.149
Experiment Accuracy STDEV
Word Only 49.0 0.90
Combined 49.4 0.44
Table 10: Overall results for preposition disam-
biguation with FRAMENET semantic roles. See Ta-
ble 9 for the legend.
Preposition Freq Entropy Baseline Word Only Combined
through 332 1.668 0.438 0.598 0.634
as 224 1.647 0.399 0.820 0.879
by 1043 1.551 0.501 0.867 0.860
between 83 1.506 0.483 0.733 0.751
of 30 1.325 0.567 0.800 0.814
out 76 1.247 0.711 0.788 0.764
for 1406 1.223 0.655 0.805 0.796
on 1927 1.184 0.699 0.856 0.855
throughout 61 0.998 0.525 0.603 0.584
across 78 0.706 0.808 0.858 0.748
from 1521 0.517 0.917 0.912 0.882
Total 6781 1.233 0.609 0.785 0.778
Table 7: Per-word results for preposition disambiguation with TREEBANK semantic roles. Freq gives the
frequency for the prepositions. Entropy measures non-uniformity of the role distributions. The Baseline
experiment selects the most-frequent role. The Word Only experiment just uses word collocations, whereas
Combined uses both word and hypernym collocations. Both columns show averages for percent correct over
ten trials. Total averages the values of the individual experiments (except for Freq).
Prep Freq Entropy Baseline Word Only Combined
between 286 3.258 0.490 0.325 0.537
against 210 2.998 0.481 0.310 0.586
under 125 2.977 0.385 0.448 0.440
as 593 2.827 0.521 0.388 0.598
over 620 2.802 0.505 0.408 0.526
behind 144 2.400 0.520 0.340 0.473
back 540 1.814 0.544 0.465 0.567
around 489 1.813 0.596 0.607 0.560
round 273 1.770 0.464 0.513 0.533
into 844 1.747 0.722 0.759 0.754
about 1359 1.720 0.682 0.706 0.778
through 673 1.571 0.755 0.780 0.779
up 488 1.462 0.736 0.736 0.713
towards 308 1.324 0.758 0.786 0.740
away 346 1.231 0.786 0.803 0.824
like 219 1.136 0.777 0.694 0.803
down 592 1.131 0.764 0.764 0.746
across 544 1.128 0.824 0.820 0.827
off 435 0.763 0.892 0.904 0.899
along 469 0.538 0.912 0.932 0.915
onto 107 0.393 0.926 0.944 0.939
past 166 0.357 0.925 0.940 0.938
Total 10432 1.684 0.657 0.685 0.703
Table 8: Per-word results for preposition disambiguation with FRAMENET semantic roles. See Table 7 for
the legend.
goal. This reflects the bias of FRAMENET towards
roles that are an integral part of the frame under con-
sideration: location and time apply to all frames, so
these cases are not generally annotated.
Table 9 shows the results of classification when
all of the prepositions are classified together. The
overall results are not that high due to the very large
number of roles. However, the combined colloca-
tion approach still shows slight improvement (49.4%
versus 49.0%). Table 8 shows the results when us-
ing individual classifiers. This shows that the com-
bined collocations produce better results: 70.3%
versus 68.5%. Unlike the case with Treebank, the
performance is below that of the individual classi-
fiers. This is due to the fine-grained nature of the
role inventory. When all the roles are considered to-
gether, prepositions are prone to being misclassified
with roles that they might not have occurred with in
the training data, such as whenever other contextual
clues are strong for that role. This is not a problem
with Treebank given its small role inventory.
4 Related work
Until recently, there has not been much work specif-
ically on preposition classification, especially with
respect to general applicability in contrast to spe-
cial purpose usages. Halliday (1956) did some early
work on this in the context of machine translation.
Later work in that area addressed the classification
indirectly during translation. In some cases, the is-
sue is avoided by translating the preposition into a
corresponding foreign function word without regard
to the preposition?s underlying meaning (i.e., direct
transfer). Other times an internal representation is
helpful (Trujillo, 1992). Taylor (1993) discusses
general strategies for preposition disambiguation us-
ing a cognitive linguistics framework and illustrates
them for ?over?. There has been quite a bit of work
in this area but mainly for spatial prepositions (Jap-
kowicz and Wiebe, 1991; Zelinsky-Wibbelt, 1993).
There is currently more interest in this type of
classification. Litkowski (2002) presents manually-
derived rules for disambiguating prepositions, in
particular for ?of?. Srihari et al (2001) present
manually-derived rules for disambiguating preposi-
tions used in named entities.
Gildea and Jurafsky (2002) classify seman-
tic role assignments using all the annotations in
FRAMENET, for example, covering all types of ver-
bal arguments. They use several features derived
from the output of a parser, such as the constituent
type of the phrase (e.g., NP) and the grammatical
function (e.g., subject). They include lexical fea-
tures for the headword of the phrase and the predi-
cating word for the entire annotated frame. They re-
port an accuracy of 76.9% with a baseline of 40.6%
over the FRAMENET semantic roles. However, due
to the conditioning of the classification on the pred-
icating word for the frame, the range of roles for a
particular classification is more limited than in our
case.
Blaheta and Charniak (2000) classify semantic
role assignments using all the annotations in TREE-
BANK. They use a few parser-derived features, such
as the constituent labels for nearby nodes and part-
of-speech for parent and grandparent nodes. They
also include lexical features for the head and al-
ternative head (since prepositions are considered as
the head by their parser). They report an accu-
racy of 77.6% over the form/function tags from the
PENN TREEBANK with a baseline of 37.8%,3 Their
task is somewhat different, since they address all ad-
juncts, not just prepositions, hence their lower base-
line. In addition, they include the nominal and ad-
verbial roles, which are syntactic and presumably
more predictable than the others in this group. Van
den Bosch and Bucholz (2002) also use the Tree-
bank data to address the more general task of assign-
ing function tags to arbitrary phrases. For features,
they use parts of speech, words, and morphological
clues. Chunking is done along with the tagging, but
they only present results for the evaluation of both
tasks taken together; their best approach achieves
78.9% accuracy.
5 Conclusion
Our approach to classifying prepositions according
to the PENN TREEBANK annotations is fairly accu-
rate (78.5% individually and 86.1% together), while
retaining ability to generalize via class-based lexi-
cal associations. These annotations are suitable for
3They target al of the TREEBANK function tags but give
performance figures broken down by the groupings defined in
the Treebank tagging guidelines. The baseline figure shown
above is their recall figure for the ?baseline 2? performance.
default classification of prepositions in case more
fine-grained semantic role information cannot be de-
termined. For the fine-grained FRAMENET roles,
the performance is less accurate (70.3% individu-
ally and 49.4% together). In both cases, the best
accuracy is achieved using a combination of stan-
dard word collocations along with class collocations
in the form of WordNet hypernyms.
Future work will address cross-dataset experi-
ments. In particular, we will see whether the word
and hypernym associations learned over FrameNet
can be carried over into Treebank, given a mapping
of the fine-grained FrameNet roles into the coarse-
grained Treebank ones. Such a mapping would be
similar to the one developed by Gildea and Jurafsky
(2002).
Acknowledgements
The first author is supported by a generous GAANN fellowship
from the Department of Education. Some of the work used com-
puting resources at NMSU made possible through MII Grants
EIA-9810732 and EIA-0220590.
References
Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In Proc. NAACL-00.
Rebecca Bruce and Janyce Wiebe. 1999. Decomposable
modeling in natural language processing. Computa-
tional Linguistics, 25 (2):195?208.
A. Van den Bosch and S. Buchholz. 2002. Shallow pars-
ing on the basis of words only: A case study. In Pro-
ceedings of the 40th Meeting of the Association for
Computational Linguistics (ACL?02), pages 433?440.
Philadelphia, PA, USA.
P. Edmonds and S. Cotton, editors. 2001. Proceedings of
the SENSEVAL 2 Workshop. Association for Compu-
tational Linguistics.
Charles J. Fillmore, Charles Wooters, and Collin F.
Baker. 2001. Building a large lexical databank which
provides deep semantics. In Proceedings of the Pa-
cific Asian Conference on Language, Information and
Computation. Hong Kong.
C. Fillmore. 1968. The case for case. In Emmon Bach
and Rovert T. Harms, editors, Universals in Linguistic
Theory. Holt, Rinehart and Winston, New York.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
M.A.K. Halliday. 1956. The linguistic basis of a
mechanical thesaurus, and its application to English
preposition classification. Mechanical Translation,
3(2):81?88.
Nathalie Japkowicz and Janyce Wiebe. 1991. Translat-
ing spatial prepositions using conceptual information.
In Proc. 29th Annual Meeting of the Assoc. for Com-
putational Linguistics (ACL-91), pages 153?160.
K. C. Litkowski. 2002. Digraph analysis of dictionary
preposition definitions. In Proceedings of the Asso-
ciation for Computational Linguistics Special Interest
Group on the Lexicon. July 11, Philadelphia, PA.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
Proc. ARPA Human Language Technology Workshop.
Tom O?Hara and Janyce Wiebe. 2003. Classifying func-
tional relations in Factotum viaWordNet hypernymas-
sociations. In Proc. Fourth International Conference
on Intelligent Text Processing and Computational Lin-
guistics (CICLing-2003).
TomO?Hara, JanyceWiebe, and Rebecca F. Bruce. 2000.
Selecting decomposable models for word-sense dis-
ambiguation: The GRLING-SDM system. Computers
and the Humanities, 34 (1-2):159?164.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, California.
Rohini Srihari, Cheng Niu, and Wei Li. 2001. A hybrid
approach for named entity and sub-type tagging. In
Proc. 6th Applied Natural Language Processing Con-
ference.
John R. Taylor. 1993. Prepositions: patterns of polysem-
ization and strategies of disambiguation. In Zelinsky-
Wibbelt (Zelinsky-Wibbelt, 1993).
Arturo Trujillo. 1992. Locations in the machine transla-
tion of prepositional phrases. In Proc. TMI-92, pages
13?20.
Janyce Wiebe, Kenneth McKeever, and Rebecca Bruce.
1998. Mapping collocational properties into machine
learning features. In Proc. 6th Workshop on Very
Large Corpora (WVLC-98), pages 225?233,Montreal,
Quebec, Canada. Association for Computational Lin-
guistics SIGDAT.
Ian H.Witten and Eibe Frank. 1999. DataMining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann.
Cornelia Zelinsky-Wibbelt, editor. 1993. The Semantics
of Prepositions: From Mental Processing to Natural
Language Processing. Mouton de Gruyter, Berlin.
Class-based Collocations for Word-Sense Disambiguation
Tom O?Hara
Department of Computer Science
New Mexico State University
Las Cruces, NM 88003-8001
tomohara@cs.nmsu.edu
Rebecca Bruce
Department of Computer Science
University of North Carolina at Asheville
Asheville, NC 28804-3299
bruce@cs.unca.edu
Jeff Donner
Department of Computer Science
New Mexico State University
Las Cruces, NM 88003-8001
jdonner@cs.nmsu.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260-4034
wiebe@cs.pitt.edu
Abstract
This paper describes the NMSU-Pitt-UNCA
word-sense disambiguation system participat-
ing in the Senseval-3 English lexical sample
task. The focus of the work is on using seman-
tic class-based collocations to augment tradi-
tional word-based collocations. Three separate
sources of word relatedness are used for these
collocations: 1) WordNet hypernym relations;
2) cluster-based word similarity classes; and 3)
dictionary definition analysis.
1 Introduction
Supervised systems for word-sense disambigua-
tion (WSD) often rely upon word collocations
(i.e., sense-specific keywords) to provide clues
on the most likely sense for a word given the
context. In the second Senseval competition,
these features figured predominantly among the
feature sets for the leading systems (Mihalcea,
2002; Yarowsky et al, 2001; Seo et al, 2001).
A limitation of such features is that the words
selected must occur in the test data in order for
the features to apply. To alleviate this problem,
class-based approaches augment word-level fea-
tures with category-level ones (Ide and Ve?ronis,
1998; Jurafsky and Martin, 2000). When ap-
plied to collocational features, this approach ef-
fectively uses class labels rather than wordforms
in deriving the collocational features.
This research focuses on the determination
of class-based collocations to improve word-
sense disambiguation. We do not address refine-
ment of existing algorithms for machine learn-
ing. Therefore, a commonly used decision tree
algorithm is employed to combine the various
features when performing classification.
This paper describes the NMSU-Pitt-
UNCA system we developed for the third
Senseval competition. Section 2 presents an
overview of the feature set used in the system.
Section 3 describes how the class-based colloca-
tions are derived. Section 4 shows the results
over the Senseval-3 data and includes detailed
analysis of the performance of the various col-
locational features.
2 System Overview
We use a decision tree algorithm for word-sense
disambiguation that combines features from the
local context of the target word with other lex-
ical features representing the broader context.
Figure 1 presents the features that are used
in this application. In the first Senseval com-
petition, we used the first two groups of fea-
tures, Local-context features and Collocational
features, with competitive results (O?Hara et al,
2000).
Five of the local-context features represent
the part of speech (POS) of words immediately
surrounding the target word. These five fea-
tures are POS?i for i from -2 to +2 ), where
POS+1, for example, represents the POS of the
word immediately following the target word.
Five other local-context features represent
the word tokens immediately surrounding the
target word (Word?i for i from ?2 to +2).
Each Word?i feature is multi-valued; its values
correspond to all possible word tokens.
There is a collocation feature WordColl
s
de-
fined for each sense s of the target word. It
is a binary feature, representing the absence or
presence of any word in a set specifically chosen
for s. A word w that occurs more than once in
the training data is included in the collocation
set for sense s if the relative percent gain in the
conditional probability over the prior probabil-
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
Local-context features
POS: part-of-speech of target word
POS?i: part-of-speech of word at offset i
WordForm: target wordform
Word?i: stem of word at offset i
Collocational features
WordColl
s
: word collocation for sense s
WordColl
?
wordform of non-sense-specific
collocation (enumerated)
Class-based collocational features
HyperColl
s
: hypernym collocation for s
HyperColl
?,i
: non-sense-specific hypernym collo-
cation
SimilarColl
s
: similarity collocation for s
DictColl
s
: dictionary collocation for s
Figure 1: Features for word-sense disambigua-
tion. All collocational features are binary indi-
cators for sense s, except for WordColl
?
.
ity is 20% or higher:
(P (s|w) ? P (s))
P (s) ? 0.20.
This threshold was determined to be effective
via an optimization search over the Senseval-2
data. WordColl
?
represents a set of non-sense-
specific collocations (i.e., not necessarily indica-
tive of any one sense), chosen via the G2 criteria
(Wiebe et al, 1998). In contrast to WordColl
s
,
each of which is a separate binary feature, the
words contained in the set WordColl
?
serve as
values in a single enumerated feature.
These features are augmented with class-
based collocational features that represent in-
formation about word relationships derived
from three separate sources: 1) WordNet
(Miller, 1990) hypernym relations (HyperColl);
2) cluster-based word similarity classes (Simi-
larColl); and 3) relatedness inferred from dictio-
nary definition analysis (DictColl). The infor-
mation inherent in the sources from which these
class-based features are derived allows words
that do not occur in the training data context
to be considered as collocations during classifi-
cation.
3 Class-based Collocations
The HyperColl features are intended to capture
a portion of the information in the WordNet hy-
pernyms links (i.e., is-a relations). Hypernym-
based collocations are formulated by replacing
each word in the context of the target word (e.g.,
in the same sentence as the target word) with
its complete hypernym ancestry from WordNet.
Since context words are not sense-tagged, each
synset representing a different sense of a context
word is included in the set of hypernyms replac-
ing that word. Likewise, in the case of multiple
inheritance, each parent synset is included.
The collocation variable HyperColl
s
for each
sense s is binary, corresponding to the absence
or presence of any hypernym in the set chosen
for s. This set of hypernyms is chosen using the
ratio of conditional probability to prior prob-
ability as described for the WordColl
s
feature
above. In contrast, HyperColl
?,i
selects non-
sense-specific hypernym collocations: 10 sepa-
rate binary features are used based on the G2
selection criteria. (More of these features could
be used, but they are limited for tractability.)
For more details on hypernym collocations, see
(O?Hara, forthcoming).
Word-similarity classes (Lin, 1998) derived
from clustering are also used to expand the
pool of potential collocations; this type of se-
mantic relatedness among words is expressed in
the SimilarColl feature. For the DictColl fea-
tures, definition analysis (O?Hara, forthcoming)
is used to determine the semantic relatedness of
the defining words. Differences between these
two sources of word relations are illustrated by
looking at the information they provide for ?bal-
lerina?:
word-clusters:
dancer:0.115 baryshnikov:0.072
pianist:0.056 choreographer:0.049
... [18 other words]
nicole:0.041 wrestler:0.040
tibetans:0.040 clown:0.040
definition words:
dancer:0.0013 female:0.0013 ballet:0.0004
This shows that word clusters capture a wider
range of relatedness than the dictionary def-
initions at the expense of incidental associa-
tions (e.g., ?nicole?). Again, because context
words are not disambiguated, the relations for
all senses of a context word are conflated. For
details on the extraction of word clusters, see
(Lin, 1998); and, for details on the definition
analysis, see (O?Hara, forthcoming).
When formulating the features SimilarColl
and DictColl, the words related to each con-
text word are considered as potential colloca-
tions (Wiebe et al, 1998). Co-occurrence fre-
Sense Distinctions Precision Recall
Fine-grained .566 .565
Course-grained .660 .658
Table 1: Results for Senseval-3 test data.
99.72% of the answers were attempted. All fea-
tures from Figure 1 were used.
quencies f(s,w) are used in estimating the con-
ditional probability P (s|w) required by the rel-
ative conditional probability selection scheme
noted earlier. However, instead of using a unit
weight for each co-occurrence, the relatedness
weight is used (e.g., 0.056 for ?pianist?); and,
because a given related-word might occur with
more than one context word for the same target-
word sense, the relatedness weights are added.
The conditional probability of the sense given
the relatedness collocation is estimated by di-
viding the weighted frequency by the sum of all
such weighted co-occurrence frequencies for the
word:
P (s|w)? wf (s,w)?
s
?
wf (s?, w)
Here wf(s, w) stands for the weighted co-
occurrence frequency of the related-word collo-
cation w and target sense s.
The relatedness collocations are less reliable
than word collocations given the level of indi-
rection involved in their extraction. Therefore,
tighter constraints are used in order to filter out
extraneous potential collocations. In particular,
the relative percent gain in the conditional ver-
sus prior probability must be 80% or higher, a
threshold again determined via an optimization
search over the Senseval-2 data. In addition,
the context words that they are related to must
occur more than four times in the training data.
4 Results and Discussion
Disambiguation is performed via a decision tree
formulated using Weka?s J4.8 classifier (Witten
and Frank, 1999). For the system used in the
competition, the decision tree was learned over
the entire Senseval-3 training data and then ap-
plied to the test data. Table 1 shows the results
of our system in the Senseval-3 competition.
Table 2 shows the results of 10-fold cross-
validation just over the Senseval-3 training data
(using Naive Bayes rather than decision trees.)
To illustrate the contribution of the three types
Experiment Precision
?Local +Local
Local - .593
WordColl .490 .599
HyperColl .525 .590
DictColl .532 .570
SimilarColl .534 .586
HyperColl+WordColl .525 .611
DictColl+WordColl .501 .606
SimilarColl+WordColl .518 .596
All Collocations .543 .608
#Words: 57 Avg. Entropy: 1.641
Avg. #Senses: 5.3 Baseline: 0.544
Table 2: Results for Senseval-3 training data.
All values are averages, except #Words, which
is the number of distinct word types classified.
Baseline always uses the most-frequent sense.
of class-based collocations, the table shows re-
sults separately for systems developed using a
single feature type, as well as for all features in
combination. In addition, the performance of
these systems are shown with and without the
use of the local features (Local), as well as with
and without the use of standard word colloca-
tions (WordColl). As can be seen, the related-
word and definition collocations perform better
than hypernym collocations when used alone.
However, hypernym collocations perform bet-
ter when combined with other features. Fu-
ture work will investigate ways of ameliorat-
ing such interactions. The best overall system
(HyperColl+WordColl+Local) uses the com-
bination of local-context features, word colloca-
tions, and hypernym collocations. The perfor-
mance of this system compared to a more typi-
cal system for WSD (WordColl+Local) is sta-
tistically significant at p < .05, using a paired
t-test.
We analyzed the contributions of the various
collocation types to determine their effective-
ness. Table 3 shows performance statistics for
each collocation type taken individually over the
training data. Precision is based on the num-
ber of correct positive indicators versus the to-
tal number of positive indicators, whereas recall
is the number correct over the total number of
training instances (7706). This shows that hy-
pernym collocations are nearly as effective as
word collocations. We also analyzed the occur-
rence of unique positive indicators provided by
the collocation types over the training data. Ta-
Total Total
Feature #Corr. #Pos. Recall Prec.
DictColl 273 592 .035 .461
HyperColl 2932 6479 .380 .453
SimilarColl 528 1535 .069 .344
WordColl 3707 7718 .481 .480
Table 3: Collocation performance statistics.
Total #Pos. is number of positive indicators for
the collocation in the training data, and Total
#Corr. is the number of these that are correct.
Unique Unique
Feature #Corr. #Pos. Prec.
DictColl 110 181 .608
HyperColl 992 1795 .553
SimilarColl 198 464 .427
DictColl 1244 2085 .597
Table 4: Analysis of unique positive indicators.
Unique #Pos. is number of training instances
with the feature as the only positive indicator,
and Unique #Corr. is number of these correct.
ble 4 shows how often each feature type is pos-
itive for a particular sense when all other fea-
tures for the sense are negative. This occurs
fairly often, suggesting that the different types
of collocations are complementary and thus gen-
erally useful when combined for word-sense dis-
ambiguation. Both tables illustrate coverage
problems for the definition and related word
collocations, which will be addressed in future
work.
References
Nancy Ide and Jean Ve?ronis. 1998. Introduc-
tion to the special issue on word sense dis-
ambiguation: the state of the art. Computa-
tional Linguistics, 24(1):1?40.
Daniel Jurafsky and James H. Martin. 2000.
Speech and Language Processing. Prentice
Hall, Upper Saddle River, New Jersey.
Dekang Lin. 1998. Automatic retrieval
and clustering of similar words. In Proc.
COLING-ACL 98, pages 768?764, Montreal.
August 10-14.
Rada Mihalcea. 2002. Instance based learning
with automatic feature selection applied to
word sense disambiguation. In Proceedings of
the 19th International Conference on Com-
putational Linguistics (COLING 2002), Tai-
wan. August 26-30.
George Miller. 1990. Introduction. Interna-
tional Journal of Lexicography, 3(4): Special
Issue on WordNet.
Tom O?Hara, Janyce Wiebe, and Rebecca F.
Bruce. 2000. Selecting decomposable models
for word-sense disambiguation: The grling-
sdm system. Computers and the Humanities,
34(1-2):159?164.
Thomas P. O?Hara. forthcoming. Empirical ac-
quisition of conceptual distinctions via dictio-
nary definitions. Ph.D. thesis, Department of
Computer Science, New Mexico State Univer-
sity.
Hee-Cheol Seo, Sang-Zoo Lee, Hae-Chang
Rim, and Ho Lee. 2001. KUNLP sys-
tem using classification information model at
SENSEVAL-2. In Proceedings of the Second
International Workshop on Evaluating Word
Sense Disambiguation Systems (SENSEVAL-
2), pages 147?150, Toulouse. July 5-6.
Janyce Wiebe, Kenneth McKeever, and Re-
becca F. Bruce. 1998. Mapping collocational
properties into machine learning features. In
Proc. 6th Workshop on Very Large Corpora
(WVLC-98), pages 225?233, Montreal, Que-
bec, Canada. Association for Computational
Linguistics. SIGDAT.
Ian H. Witten and Eibe Frank. 1999. Data
Mining: Practical Machine Learning Tools
and Techniques with Java Implementations.
Morgan Kaufmann, San Francisco, CA.
David Yarowsky, Silviu Cucerzan, Radu Flo-
rian, Charles Schafer, and Richard Wicen-
towski. 2001. The Johns Hopkins SENSE-
VAL2 system descriptions. In Proceedings of
the Second International Workshop on Eval-
uating Word Sense Disambiguation Systems
(SENSEVAL-2), pages 163?166, Toulouse.
July 5-6.
Empirical Acquisition of Differentiating Relations from Definitions
Tom O?Hara
Department of Computer Science
New Mexico State University
Las Cruces, NM 88003
tomohara@cs.nmsu.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Abstract
This paper describes a new automatic approach for
extracting conceptual distinctions from dictionary
definitions. A broad-coverage dependency parser is
first used to extract the lexical relations from the def-
initions. Then the relations are disambiguated using
associations learned from tagged corpora. This con-
trasts with earlier approaches using manually devel-
oped rules for disambiguation.
1 Introduction
Large-scale lexicons for computational semantics of-
ten lack sufficient distinguishing information for the
concepts serving to define words. For example,
WordNet (Miller, 1990) recently introduced new re-
lations for domain category and location in Version
2.0, along with 6,000+ instances; however, about
38% of the noun synsets are still not explicitly dis-
tinguished from sibling synsets.
Work on the Extended WordNet project
(Harabagiu et al, 1999) is achieving substan-
tial progress in making the information in WordNet
more explicit. The main goal is to transform the
definitions into a logical form representation suit-
able for drawing inferences; in addition, the content
words in the definitions are being disambiguated. In
the logical form representation, separate predicates
are used for each preposition, as well as for some
other functional words (e.g., conjunctions); thus,
ambiguity in the underlying relations implicit in
the definitions is not being resolved. The work
described here automates the process of relation
disambiguation. This can be used to further the
transformation of WordNet into an explicit lexical
knowledge base.
Earlier approaches to differentia extraction have
predominantly relied upon manually constructed
pattern matching rules for extracting relations from
dictionary definitions (Vanderwende, 1996; Barrie`re,
1997; Rus, 2002). These rules can be very precise,
but achieving broad-coverage can be difficult. Here
a broad coverage dependency parser is first used to
determine the syntactic relations that are present
among the constituents in the sentence. Then the
syntactic relations between sentential constituents
are converted into semantic relations between the
underlying concepts using statistical classification.
Isolating the disambiguation step from the extrac-
tion step in this manner allows for greater flexibil-
ity over earlier approaches. For example, different
parsers can be incorporated without having to re-
work the disambiguation process.
This paper is organized as follows: Section 2 de-
tails the steps in extracting the initial relations from
the definition parse. Section 3 illustrates the disam-
biguation process, the crucial part of this approach.
Section 4 presents an evaluation of the relations that
are extracted from the WordNet definitions. Lastly,
Section 5 compares the approach to previous ap-
proaches that have been tried.
2 Differentia Extraction
The approach to differentia extraction is entirely au-
tomated. This starts with using the Link Grammar
Parser (Sleator and Temperley, 1993), a dependency
parser, to determine the syntactic lexical relations
that occur in the sentence. Dictionary definitions are
often given in the form of sentence fragments with
the headword omitted. For example, the definition
for the beverage sense of ?wine? is ?fermented juice
(of grapes especially).? Therefore, prior to running
the definition analysis, the definitions are converted
into complete sentences, using simple templates for
each part of speech.
After parsing, a series of postprocessing steps is
performed prior to the extraction of the lexical rela-
tions. For the Link Parser, this mainly involves con-
version of the binary dependencies into relational tu-
ples and the realignment of the tuples around func-
tion words. The Link Parser outputs syntactic de-
pendencies among words, punctuation, and sentence
boundary markers. The parser uses quite specialized
syntactic relations, so these are converted into gen-
eral ones prior to the extraction of the relational tu-
ples. For example, the relation A, which is used for
pre-noun adjectives, is converted into modifies. Fig-
ure 1 illustrates the syntactic relations that would
be extracted, along with the original parser output.
The syntactic relationships are first converted
into relational tuples using the format ?source-word,
relation-word , target-word?. This conversion is per-
formed by following the dependencies involving the
content words, ignoring cases involving non-word el-
ements (e.g., punctuation). For example, the first
tuple extracted from the parse would be ?n:wine,
Definition sentence:
Wine is fermented juice (of grapes especially).
Link Grammar parse:
?/////, Wd, 1. n:wine?
?/////, Xp, 10. .?
?1. n:wine, Ss, 2. v:is?
?10. ., RW, 11. /////?
?2. v:is, Ost, 4. n:juice?
?3. v:fermented, A, 4. n:juice?
?4. n:juice, MXs, 6. of?
?5. (, Xd, 6. of?
?6. of, Jp, 7. n:grapes?
?6. of, Xc, 9. )?
Extracted relations:
?1. n:wine, 2. v:is, 4. n:juice?
?3. v:fermented, modifies-3-4, 4. n:juice?
?4. n:juice, 6. of, 7. n:grapes?
Figure 1: Example for relation extraction.
v:is , n:juice?. Certain types of dependencies are
treated specially by converting the syntactic rela-
tionships directly into a relational tuple involving a
special relation-indicating word (e.g., ?modifies?).
The relational tuples extracted from the parse
form the basis for the lexical relations derived from
the definition. Structural ambiguity resolution is not
addressed here, so the first parse returned is used.
The remaining optional step assigns weights to the
relations that are extracted.
When using the relations in applications, it is de-
sirable to have a measure of how relevant the re-
lations are to the associated concepts. One such
measure would be the degree to which the relation
applies to the concept being described as opposed
to sibling concepts. To account for this, cue validi-
ties are used, borrowing from cognitive psychology
(Smith and Medin, 1981). Cue validities can be in-
terpreted as probabilities indicating the degree to
which features apply to a given concept versus sim-
ilar concepts (i.e., P (C|F )).
Cue validities are estimated by calculating the
percentage of times that the feature is associated
with a concept versus the total associations of con-
trasting concepts. This requires a means of deter-
mining the set of contrasting concepts for a given
concept. The simplest way of doing this would be to
just select the set of sibling concepts (e.g., synsets
sharing a common parent in WordNet). However,
due to the idiosyncratic way concepts are special-
ized in knowledge bases, this likely would not include
concepts intuitively considered as contrasting.
To alleviate this problem the most-informative an-
cestor will be used instead of the parent. This is
determined by selecting the ancestor that best bal-
ances frequency of occurrence in a tagged corpus
with specificity. This is similar to Resnik?s (1995)
notion of most-informative subsumer for a pair of
concepts. In his approach, estimated frequencies for
synsets are percolated up the hierarchy, so that the
frequency always increases as one proceeds up the
hierarchy. Therefore the first common ancestor for
a pair is the most-informative subsumer (i.e., has
most information content). Here attested frequen-
cies from SemCor (Miller et al, 1994) are used, so all
ancestors are considered. Specificity is accounted for
by applying a scaling factor to the frequencies that
decreases as one proceeds up the hierarchy. Thus,
?informative? is used more in an intuitive sense rather
than technical.
More details on the extraction process and the
subsequent disambiguation can be found in (O?Hara,
forthcoming).
3 Differentia Disambiguation
After the differentia properties have been extracted
from a definition, the words for the relation source
and object terms are disambiguated to order to re-
duce vagueness in the relationships. In addition, the
relation types are converted from surface-level rela-
tions (e.g., object) or relation-indicating words (e.g.,
prepositions) into the underlying semantic relation.
Since WordNet serves as the knowledge base be-
ing targeted, term disambiguation involves select-
ing the most appropriate synset for both the source
and target terms. The WordNet definitions have re-
cently been sense-tagged as part of the Extended
WordNet (Novischi, 2002), so these annotations are
incorporated. For other dictionaries, use of tradi-
tional word-sense disambiguation algorithms would
be required.
With the emphasis on corpus analysis in computa-
tional linguistics, there has been shift away from re-
lying on explicitly coded knowledge towards the use
of knowledge inferred from naturally occurring text,
in particular text that has been annotated by hu-
mans to indicate phenomena of interest. The Penn
Treebank version II (Marcus et al, 1994) provided
the first large-scale set of case role annotations for
general-purpose text. These are very general roles
akin to Fillmore?s (1968) case roles. The Berkeley
FrameNet (Fillmore et al, 2001) project provides
the most recent large-scale annotation of semantic
roles. These are at a much finer granularity than
those in Treebank, so they should prove quite use-
ful for applications learning detailed semantics from
corpora. O?Hara and Wiebe (2003) explain how
both inventories can be used for preposition disam-
biguation.
The goal of relation disambiguation is to deter-
mine the underlying semantic role indicated by par-
ticular words in a phrase or by word order. For
relations indicated directly by function words, the
disambiguation can be seen as a special case of word-
sense disambiguation (WSD). As an example, refin-
ing the relationship ??dog?, ?with? , ?ears?? into ??dog?,
has-part , ?ears??, is equivalent to disambiguating the
preposition ?with,? given that the senses are the dif-
Local-context features
POS: part of speech of target word
POS?i: part-of-speech of ith word to left
POS+i: part-of-speech of ith word to right
Word: target wordform as is
Word?i: ith word to the left
Word+i: ith word to the right
Collocational features
WordColl
i
: word collocation for sense i
Class-based collocational features
HyperColl
s
: hypernym collocation for sense i
Figure 2: Features for preposition classifier.
ferent relations it can indicate. For relations that are
indicated implicitly (e.g., adjectival modification),
other classification techniques would be required, re-
flecting the more syntactic nature of the task.
A straightforward approach for preposition disam-
biguation would be to use standard WSD features,
such as the parts-of-speech of surrounding words
and, more importantly, collocations (e.g., lexical as-
sociations). Although this can be highly accurate,
it tends to overfit the data and to generalize poorly.
The latter is of particular concern here as the train-
ing data is taken from a different genre (e.g., news-
paper text rather than dictionary definitions). To
overcome these problems, a class-based approach is
used for the collocations, with WordNet high-level
synsets as the source of the word classes. Figure 2
lists the features used for the classifier.
For the application to differentia disambiguation,
the classifiers learned over Treebank and FrameNet
need to be combined. This can be done readily in a
cascaded fashion with the classifier for the most spe-
cific relation inventory (i.e., FrameNet) being used
first and then the other classifiers being applied in
turn whenever the classification is inconclusive. This
has the advantage that new resources can be in-
tegrated into the combined relation classifier with
minimal effort. However, the resulting role inven-
tory will likely be heterogeneous and might be prone
to inconsistent classifications. In addition, the role
inventory could change whenever new annotation re-
sources are incorporated, making the differentia dis-
ambiguation system less predictable.
Alternatively, the annotations can be converted
into a common inventory, and a separate relation
classifier induced over the resulting data. This has
the advantage that the target relation-type inven-
tory remains stable whenever new sources of relation
annotations are introduced. The drawback however
is that annotations from new resources must first be
mapped into the common inventory before incorpo-
ration. The latter approach is employed here. The
common inventory incorporates some of the general
relation types defined by Gildea and Jurafsky (2002)
for their experiments in classifying semantic rela-
tions in FrameNet using a reduced inventory.
Relation Frequency
Theme 0.316
Goal 0.116
Ground 0.080
Category 0.069
Agent 0.069
Cause 0.061
Manner 0.058
Recipient 0.053
Medium 0.039
Characteristic 0.022
Resource 0.021
Means 0.021
Source 0.019
Path 0.017
Experiencer 0.017
Accompaniment 0.011
Area 0.010
Direction 0.001
Table 1: Frequency of relations extracted.
4 Evaluation
The evaluation discussed here assesses the quality of
the information that would be added to the lexicons
with respect to relation disambiguation, which is the
focus of the research. An application-oriented evalu-
ation is discussed in (O?Hara, forthcoming), showing
how using the extracted information improves word-
sense disambiguation.
All the definitions from WordNet 1.7.1 were run
through the differentia-extraction process. This in-
volved 111,223 synsets, of which 10,810 had prepro-
cessing or parse-related errors leading to no relations
being extracted. Table 1 shows the frequency of the
relations in the output from the differentia extrac-
tion process. The most common relation used is
Theme, which occurs four times as much compared
to the annotations. It is usually annotated as the
sense for ?of,? which also occurs with roles Source,
Category, Ground , Agent , Characteristic, and Expe-
riencer . Some of these represent subtle distinctions,
so it is likely that the difference in the text genre is
causing the classifier to use the default more often.
Four human judges were recruited to evaluate ran-
dom samples of the relations that were extracted. To
allow for inter-coder reliability analysis, each evalua-
tor evaluated some samples that were also evaluated
by the others, half as part of a training phase and
half after training. In addition, they also evaluated
a few samples that were manually corrected before-
hand. This provides a baseline against which the
uncorrected results can be measured against. Be-
cause the research only addresses relations indicated
by prepositional phrases, the evaluation is restricted
to these cases. Specifically, the judges rate the as-
signment of relations to the prepositional phrases on
a scale from 1 to 5, with 5 being an exact match.
The evaluation is based on averaging the assess-
Corrected
#Cases 10
#Scores 40
Mean 3.225
stdev 1.625
Score 0.60
Uncorrected
#Cases 15
#Scores 60
Mean 3.033
stdev 1.551
Score 0.58
Table 2: Mean assessment score for all ex-
tracted relationships. 25 relationships were each
evaluated by 4 judges. Mean gives the mean of the
assessment ratings (from 1 to 5). Score gives ratings
relative to scale from 0 to 1.
ment scores over the relationships. Table 2 shows
the results from this evaluation, including the man-
ually corrected as well as the uncorrected subsets
of the relationships. For the corrected output, the
mean assessment value was 3.225, which translates
into an overall score of 0.60. For the uncorrected sys-
tem output, the mean assessment value was 3.033,
which translates into an overall score of 0.58. Al-
though the absolute score is not high, the system?s
output is generally acceptable, as the score for the
uncorrected set of relationships is close to that of the
manually corrected set.
5 Related work
Most of the work addressing differentia extraction
has relied upon manually constructed extraction
rules (Vanderwende, 1996; Barrie`re, 1997; Rus,
2002). Here the emphasis is switched from transfor-
mation patterns for extracting relations into statis-
tical classification for relation disambiguation, given
tagged corpora with examples. This allows for bet-
ter coverage at the expense of precision. Note that
relation disambiguation is not yet addressed in Ex-
tended WordNet (Rus, 2002); for example, preposi-
tions are treated as predicates in the logical form
representation. Their extraction process is also
closely tied into the specifics of the parser, as a trans-
formation rule is developed for each grammar rule.
This work addresses the acquisition of conceptual
distinctions. In principle, it can handle any level
of granularity given sufficient training data; how-
ever, addressing distinctions at the level of near-
synonyms (Edmonds and Hirst, 2002) might require
customized analysis for each cluster of nearly syn-
onymous words. Inkpen and Hirst (2001) discuss
how this can be automated by analyzing specialized
synonymy dictionaries. Decision lists of indicative
keywords are learned for the broad types of prag-
matic distinctions, and these are then manually split
into decision lists for more-specific distinctions.
6 Conclusion
We have presented an empirical methodology for
extracting information from dictionary definitions.
This differs from previous approaches by using data-
driven relation disambiguation, using FrameNet se-
mantic roles annotations mapped into a reduced in-
ventory. All the definitions from WordNet 1.7.1 were
analyzed using this process, and the results evalu-
ated by four human judges. The overall results were
not high, but the evaluation was comparable to re-
lations that were manually corrected before coding.
References
C. Barrie`re. 1997. From Machine Readable Dictio-
naries to a Lexical Knowledge Base of Conceptual
Graphs. Ph.D. thesis, Simon Fraser University.
P. Edmonds and G. Hirst. 2002. Near-synonymy
and lexical choice. Computational Linguistics,
28(2):105?144.
C. Fillmore, C. Wooters, and C. Baker. 2001. Build-
ing a large lexical databank which provides deep
semantics. In Proc. PACLIC-01.
C. Fillmore. 1968. The case for case. In E. Bach
and R. Harms, editors, Universals in Linguistic
Theory. Holt, Rinehart and Winston, New York.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
S. Harabagiu, G. Miller, and D. Moldovan. 1999.
WordNet 2?A morphologically and semantically
enhanced resource. In Proc. SIGLEX Workshop.
D. Inkpen and G. Hirst. 2001. Building a lexical
knowledge-base of near-synonym differences. In
Proc. WordNet and Other Lexical Resources.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIn-
tyre, et al 1994. The Penn Treebank: Annotat-
ing predicate argument structure. In Proc. ARPA
Human Language Technology Workshop.
G. Miller, M. Chodorow, S. Landes, C. Leacock, and
R. Thomas. 1994. Using a semantic concordance
for sense identification. In Proc. ARPA Human
Language Technology Workshop.
G. Miller. 1990. Introduction. International Jour-
nal of Lexicography, 3(4).
A. Novischi. 2002. Accurate semantic annotations
via pattern matching. In Proc. FLAIRS 2002.
T. O?Hara and J. Wiebe. 2003. Preposition se-
mantic classification via Penn Treebank and
FrameNet. In Proc. CoNLL-03.
T. O?Hara. forthcoming. Empirical acquisition of
conceptual distinctions via dictionary definitions.
Ph.D. thesis, New Mexico State University.
P. Resnik. 1995. Disambiguating noun groupings
with respect to WordNet senses. In Proc. WVLC.
V. Rus. 2002. Logic Forms for WordNet Glosses.
Ph.D. thesis, Southern Methodist University.
D. Sleator and D. Temperley. 1993. Parsing English
with a link grammar. In Proc. Workshop on Pars-
ing Technologies.
E. Smith and D. Medin. 1981. Categories and Con-
cepts. Harvard University Press, Cambridge, MA.
L. Vanderwende. 1996. Understanding Noun Com-
pounds using Semantic Information Extracted
from On-Line Dictionaries. Ph.D. thesis, George-
town University.
Exploiting Semantic Role Resources
for Preposition Disambiguation
Tom O?Hara?
University of Maryland, Baltimore County
Janyce Wiebe??
University of Pittsburgh
This article describes how semantic role resources can be exploited for preposition disambigua-
tion. The main resources include the semantic role annotations provided by the Penn Treebank
and FrameNet tagged corpora. The resources also include the assertions contained in the Fac-
totum knowledge base, as well as information from Cyc and Conceptual Graphs. A common
inventory is derived from these in support of definition analysis, which is the motivation for this
work.
The disambiguation concentrates on relations indicated by prepositional phrases, and is
framed as word-sense disambiguation for the preposition in question. A new type of feature for
word-sense disambiguation is introduced, usingWordNet hypernyms as collocations rather than
just words. Various experiments over the Penn Treebank and FrameNet data are presented, in-
cluding prepositions classified separately versus together, and illustrating the effects of filtering.
Similar experimentation is done over the Factotum data, including a method for inferring likely
preposition usage from corpora, as knowledge bases do not generally indicate how relationships
are expressed in English (in contrast to the explicit annotations on this in the Penn Treebank and
FrameNet). Other experiments are included with the FrameNet data mapped into the common
relation inventory developed for definition analysis, illustrating how preposition disambiguation
might be applied in lexical acquisition.
1. Introduction
English prepositions convey important relations in text. When used as verbal adjuncts,
they are the principal means of conveying semantic roles for the supporting entities
described by the predicate. Preposition disambiguation is a challenging problem. First,
prepositions are highly polysemous. A typical collegiate dictionary has dozens of
senses for each of the common prepositions. Second, the senses of prepositions tend
to be closely related to one another. For instance, there are three duplicate role assign-
ments among the twenty senses for of in The Preposition Project (Litkowski and
Hargraves 2006), a resource containing semantic annotations for common prepositions.
? Institute for Language and Information Technologies, Baltimore, MD 21250. E-mail:
tomohara@umbc.edu.
?? Department of Computer Science, Pittsburgh, PA 15260. E-mail: wiebe@cs.pitt.edu.
Submission received: 7 August 2006; accepted for publication: 21 February 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 2
Consider the disambiguation of the usages of on in the following sentences:
(1) The cut should be blocked on procedural grounds.
(2) The industry already operates on very thin margins.
The choice between the purpose and manner meanings for on in these sentences is
difficult. The purpose meaning seems preferred for sentence 1, as grounds is a type of
justification. For sentence 2, the choice is even less clear, though the manner meaning
seems preferred.
This article presents a new method for disambiguating prepositions using infor-
mation learned from annotated corpora as well as knowledge stored in declarative
lexical resources. The approach allows for better coverage and finer distinctions than
in previous work in preposition disambiguation. For instance, a traditional approach
would involvemanually developing rules for on that specify the semantic type of objects
associated with the different senses (e.g., time for temporal). Instead, we infer this based
on lexical associations learned from annotated corpora.
The motivation for preposition disambiguation is to support a system for lexical
acquisition (O?Hara 2005). The focus of the system is to acquire distinguishing infor-
mation for the concepts serving to define words. Large-scale semantic lexicons mainly
emphasize the taxonomic relations among the underlying concepts (e.g., is-a and part-
of ), and often lack sufficient differentiation among similar concepts (e.g., via attributes
or functional relations such as is-used-for). For example, in WordNet (Miller et al 1990),
the standard lexical resource for natural language processing, the only relations for
beagle andAfghan are that they are both a type of hound. Although the size difference can
be inferred from the definitions, it is not represented in the WordNet semantic network.
In WordNet, words are grouped into synonym sets called synsets, which represent
the underlying concepts and serve as nodes in a semantic network. Synsets are ordered
into a hierarchy using the hypernym relation (i.e., is-a). There are several other semantic
relations, such as part-whole, is-similar-to, and domain-of . Nonetheless, in version 2.1 of
WordNet, about 30% of the synsets for noun entries are not explicitly distinguished from
sibling synsets via semantic relations.
To address such coverage problems in lexicons, we have developed an empirical
approach to lexical acquisition, building upon earlier knowledge-based approaches in
dictionary definition analysis (Wilks, Slator, and Guthrie 1996). This involves a two-step
process: Definitions are first analyzed with a broad-coverage parser, and then the result-
ing syntactic relationships are disambiguated using statistical classification. A crucial
part of this process is the disambiguation of prepositions, exploiting online resources
with semantic role usage information. The main resources are the Penn Treebank
(PTB; Marcus et al 1994) and FrameNet (Fillmore, Wooters, and Baker 2001), two
popular corpora providing rich annotations on English text, such as the semantic roles
associatedwith prepositional phrases in context. In addition to the semantic role annota-
tions from PTB and FrameNet, traditional knowledge bases (KBs) are utilized to provide
training data for the relation classification. In particular, the FactotumKB (Cassidy 2000)
is used to provide additional training data for prepositions that are used to convey
particular relationships. Information on preposition usage is not explicitly encoded in
Factotum, so a new corpus analysis technique is employed to infer the associations.
Details on the lexical acquisition process, including application and evaluation, can
be found in O?Hara (2005). This article focuses on the aspects of this method relevant
to the processing of prepositions. In particular, here we specifically address preposition
152
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
disambiguation using semantic role annotations from PTB, FrameNet, and Factotum.
In each case, classification experiments are presented using the respective resources as
training data with evaluation via 10-fold cross validation.
This article is organized as follows. Section 2 presents background information on
the relation inventories used during classification, including one developed specifically
for definition analysis. Section 3 discusses the relation classifiers in depth with results
given for four different inventories. Section 4 discusses related work in relation disam-
biguation, and Section 5 presents our conclusions.
2. Semantic Relation Inventories
The representation of natural language utterances often incorporates the notion of
semantic roles, which are analogous to the slots in a frame-based representation. In
particular, there is an emphasis on the analysis of thematic roles, which serve to tie
the grammatical constituents of a sentence to the underlying semantic representation.
Thematic roles are also called case roles, because in some languages the grammatical
constituents are indicated by case inflections (e.g., ablative in Latin). As used here, the
term ?semantic role? refers to an arbitrary semantic relation, and the term ?thematic
role? refers to a relation intended to capture the semantics of sentences (e.g., event
participation).
Which semantic roles are used varies widely in Natural Language Processing
(NLP). Some systems use just a small number of very general roles, such as beneficiary.
At the other extreme, some systems use quite specific roles tailored to a particular
domain, such as catalyst in the chemical sense.
2.1 Background on Semantic Roles
Bruce (1975) presents an account of early case systems in NLP. For the most part,
those systems had limited case role inventories, along the lines of the cases defined by
Fillmore (1968). Palmer (1990) discusses some of the more contentious issues regarding
case systems, including adequacy for representation, such as reliance solely upon case
information to determine semantics versus the use of additional inference mechanisms.
Barker (1998) provides a comprehensive summary of case inventories in NLP, along
with criteria for the qualitative evaluation of case systems (generality, completeness, and
uniqueness). Linguistic work on thematic roles tends to use a limited number of roles.
Frawley (1992) presents a detailed discussion of twelve thematic roles and discusses
how they are realized in different languages.
During the shift in emphasis away from systems that work in small, self-contained
domains to those that can handle open-ended domains, there has been a trend towards
the use of larger sets of semantic primitives (Wilks, Slator, and Guthrie 1996). The
WordNet lexicon (Miller et al 1990) serves as one example of this. A synset is defined
in terms of its relations with any of the other 100,000+ synsets, rather than in terms of a
set of features like [?ANIMATE]. There has also been a shift in focus from deep under-
standing (e.g., story comprehension) facilitated by specially constructed KBs to shallow
surface-level analysis (e.g., text extraction) facilitated by corpus analysis. Both trends
seem to be behind the increase in case inventories in two relatively recent resources,
namely FrameNet (Fillmore, Wooters, and Baker 2001) and OpenCyc (OpenCyc 2002),
both of which define well over a hundred case roles. However, provided that the case
roles are well structured in an inheritance hierarchy, both paraphrasability and coverage
can be addressed by the same inventory.
153
Computational Linguistics Volume 35, Number 2
2.2 Inventories Developed for Corpus Annotation
With the emphasis on corpus analysis in computational linguistics, there has been a
shift away from relying on explicitly-coded knowledge towards the use of knowledge
inferred from naturally occurring text, in particular text that has been annotated by
humans to indicate phenomena of interest. For example, rather than manually devel-
oping rules for preferring one sense of a word over another based on context, the
most successful approaches have automatically learned the rules based on word-sense
annotations, as evidenced by the Senseval competitions (Kilgarriff 1998; Edmonds and
Cotton 2001).
The Penn Treebank version II (Marcus et al 1994) provided the first large-scale set
of case annotations for general-purpose text. These are very general roles, following
Fillmore (1968). The Berkeley FrameNet (Fillmore, Wooters, and Baker 2001) project
currently provides the most comprehensive set of semantic roles annotations. These are
at a much finer granularity than those in PTB, making them quite useful for applications
learning semantics from corpora. Relation disambiguation experiments for both of these
role inventories are presented subsequently.
2.2.1 Penn Treebank. The original PTB (Marcus, Santorini, and Marcinkiewicz 1993) pro-
vided syntactic annotations in the form of parse trees for text from theWall Street Journal.
This resource is very popular in computational linguistics, particularly for inducing
part-of-speech taggers and parsers. PTB version II (Marcus et al 1994) added 20 func-
tional tags, including a few thematic roles such as temporal, direction, and purpose. These
can be attached to any verb complement but normally occur with clauses, adverbs, and
prepositions.
For example, Figure 1 shows a parse tree using the extended annotation format.
In addition to the usual syntactic constituents such as NP and VP, function tags are
included. For example, the second NP gives the subject. This also shows that the first
prepositional phrase (PP) indicates the time frame, whereas the last PP indicates the
Sentence:
In 1982, Sports & Recreation?s managers and certain passive investors purchased the
company from Brunswick Corp. of Skokie, Ill.
Parse:
(S (PP-TMP In (NP 1982)), temporal extent
(NP-SBJ grammatical subject
(NP (NP (NP Sports) & (NP Recreation) ?s)
managers)
and (NP certain passive investors))
(VP purchased
(NP the company)
(PP-CLR from closely related
(NP (NP Brunswick Corp.)
(PP-LOC of locative
(NP (NP Skokie) , (NP Ill)))
))) .)
Figure 1
Penn Treebank II parse tree annotation sample. The functional tags are shown in boldface.
154
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Table 1
Frequency of Penn Treebank II semantic role annotations. Relative frequencies estimated over the
counts for unique assignments given in the PTB documentation (bkt tags.lst), and descriptions
based on Bies et al (1995). Omits low-frequency benefactive role. The syntactic role annotations
generally have higher frequencies; for example, the subject role occurs 49% of the time (out of
about 240,000 total annotations).
Role Freq. Description
temporal .113 indicates when, how often, or how long
locative .075 place/setting of the event
direction .026 starting or ending location (trajectory)
manner .021 indicates manner, including instrument
purpose .017 purpose or reason
extent .010 spatial extent
location. The second PP is tagged as closely-related, which is one of the miscellaneous
PTB function tags that are more syntactic in nature: ?[CLR] occupy somemiddle ground
between arguments and adjunct? (Bies et al 1995). Frequency information for the
semantic role annotations is shown in Table 1.
2.2.2 FrameNet. FrameNet (Fillmore, Wooters, and Baker 2001) is striving to develop an
English lexicon with rich case structure information for the various contexts that words
can occur in. Each of these contexts is called a frame, and the semantic relations that
occur in each frame are called frame elements (FE). For example, in the communica-
tion frame, there are frame elements for communicator, message, medium, and so forth.
FrameNet annotations occur at the phrase level instead of the grammatical constituent
level as in PTB. Figure 2 shows an example.
Table 2 displays the top 25 semantic roles by frequency of annotation. This shows
that the semantic roles in FrameNet can be quite specific, as with the roles cognizer,
evaluee, and addressee. In all, there are over 780 roles annotated with over 288,000 tagged
instances.
Sentence:
Hewlett-Packard Co has rolled out a new range of ISDN connectivity enabling stand-
alone workstations to communicate over public or private ISDN networks.
Annotation:
Hewlett-Packard Co has rolled out a new range of ISDN connectivity enabling
?C FE=?Communicator? PT=?NP??standalone workstations?/C?
to ?C TARGET=?y??communicate?/C?
?C FE=?Medium? PT=?PP??over public or private ISDN networks?/C? .
Figure 2
FrameNet annotation sample. The constituent (C) tags identify the phrases that have been
annotated. The frame element (FE) attributes indicate the semantic roles, and the phrase type
(PT) attributes indicate the traditional grammatical category for the phrase. For simplicity, this
example is formatted in the earlier FrameNet format, but the information is taken from the
latest annotations (lu5.xml).
155
Computational Linguistics Volume 35, Number 2
Table 2
Common FrameNet semantic roles. The top 25 of 773 roles are shown, representing nearly half of
the total annotations (about 290,000). Descriptions based on FrameNet 1.3 frame documentation.
Role Freq. Description
agent .037 person performing the intentional act
theme .031 object being acted on, affected, etc.
experiencer .029 being who has a physical experience, etc.
goal .028 endpoint of the path
speaker .028 individual that communicates the message
stimulus .026 entity that evokes response
manner .025 manner of performing an action, etc.
degree .024 degree to which event occurs
self-mover .023 volitional agent that moves
message .021 the content that is communicated
path .020 the trajectory of motion, etc.
cognizer .018 person who perceives the event
source .017 the beginning of the path
time .016 the time at which the situation occurs
evaluee .016 thing about which a judgment has been made
descriptor .015 attributes, traits, etc. of the entity
body-part .014 location on the body of the experiencer
content .014 situation or state-of-affairs that attention is focused on
topic .014 subject matter of the communicated message, etc.
item .012 entity whose scalar property is specified
target .011 entity which is hit by a projectile
garment .011 clothing worn
addressee .011 entity that receives a message from the communicator
protagonist .011 person to whom a mental property is attributed
communicator .010 the person who communicates a message
2.3 Other
A recent semantic role resource that is starting to attract interest is the Proposition Bank
(PropBank), developed at the University of Pennsylvania (Palmer, Gildea, and Kings-
bury 2005). It extends the Penn Treebank with information on verb subcategorization.
The focus is on annotating all verb occurrences and all their argument realizations that
occur in the Wall Street Journal, rather than select corpus examples as in FrameNet.
Therefore, the role inventory is heavily verb-centric, for example, with the generic labels
arg0 through arg4 denoting the main verbal arguments to avoid misinterpretations.
Verbal adjuncts are assigned roles based on PTB version II (e.g., argM-LOC and argM-
TMP). PropBank has been used as the training data in recent semantic role labeling
competitions as part of the Conferences on Computational Natural Language Learn-
ing (Carreras and Ma`rquez 2004, 2005). Thus, it is likely to become as influential as
FrameNet in computational semantics.
The Preposition Project similarly adds information to an existing semantic role
resource, namely FrameNet. It is being developed by CL Research (Litkowski and
Hargraves 2006) and endeavors to provide comprehensive syntactic and semantic in-
formation on various usages of prepositions, which often are not represented well
in semantic lexicons (e.g., they are not included at all in WordNet). The Preposition
Project uses the sense distinctions from the Oxford Dictionary of English and integrates
syntactic information about prepositions from comprehensive grammar references.
156
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
2.4 Inventories for Knowledge Representation
This section describes three case inventories: one developed for the Cyc KB (Lenat
1995), one used to define Conceptual Graphs (Sowa 1984), and one for the Factotum
KB (Cassidy 2000). The first two are based on a traditional knowledge representation
paradigm. With respect to natural language processing, these approaches are more
representative of the earlier approaches in which deep understanding is the chief goal.
Factotum is also based on a knowledge representation paradigm, but in a sense also
reflects the empirical aspect of the corpus annotation approach, because the annotations
were developed to address the relations implicit in Roget?s Thesaurus.
In this article, relation disambiguation experiments are only presented for Facto-
tum, given that the others do not readily provide sufficient training data. However, the
other inventories are discussed because each provides relation types incorporated into
the inventory used below for the definition analysis (see Section 3.5).
2.4.1 Cyc. The Cyc system (Lenat 1995) is the most ambitious knowledge representation
project undertaken to date, in development since 1984. The full Cyc KB is propri-
etary, which has hindered its adoption in natural language processing. However, to
encourage broader usage, portions of the KB have been made freely available to the
public. For instance, there is an open-source version of the system called OpenCyc
(www.opencyc.org), which covers the upper part of the KB and also includes the Cyc
inference engine, KB browser, and other tools. In addition, researchers can obtain access
to ResearchCyc, which contains most of the KB except for proprietary information (e.g.,
internal bookkeeping assertions).
Cyc uses a wide range of role types: very general roles (e.g., beneficiary); commonly
occurring situational roles (e.g., victim); and highly specialized roles (e.g., catalyst). Of
the 8,756 concepts in OpenCyc, 130 are for event-based roles (i.e., instances of actor-
slot) with 51 other semantic roles (i.e., other instances of role). Table 3 shows the most
commonly used event-based roles in the KB.
2.4.2 Conceptual Graphs. The Conceptual Graphs (CG) mechanism was introduced by
Sowa (1984) for knowledge representation as part of his Conceptual Structures theory.
The original text listed two dozen or so thematic relations, such as destination and
initiator. In all, 37 conceptual relations were defined. This inventory formed the basis
for most work in Conceptual Graphs. Recently, Sowa (1999) updated the inventory to
allow for better hierarchical structuring and to incorporate the important thematic roles
identified by Somers (1987). Table 4 shows a sample of these roles, along with usage
estimates based on corpus analysis (O?Hara 2005).
2.4.3 Factotum. The Factotum semantic network (Cassidy 2000) developed by Micra,
Inc., makes explicit many of the relations in Roget?s Thesaurus.1 Outside of proprietary
resources such as Cyc, Factotum is themost comprehensive KBwith respect to functional
relations, which are taken here to be non-hierarchical relations, excluding attributes.
OpenCyc does include definitions of many non-hierarchical relations. However, there
are not many instantiations (i.e., relationship assertions), because it concentrates on the
higher level of the ontology.
1 Factotum is based on the public domain version of Roget?s Thesaurus. The latter is freely available via
Project Gutenberg (http://promo.net/pg), thanks to Micra, Inc.
157
Computational Linguistics Volume 35, Number 2
Table 3
Most common event-based roles in OpenCyc. Descriptions based on comments from the
OpenCyc knowledge base (version 0.7). Relative frequencies based on counts obtained
via Cyc?s utility functions.
Role Freq. Description
done-by .178 relates an event to its ?doer?
performed-by .119 doer deliberately does act
object-of-state-change .081 object undergoes some kind of intrinsic change of state
object-acted-on .057 object is altered or affected in event
outputs-created .051 object comes into existence sometime during event
transporter .044 object facilitating conveyance of transportees
transportees .044 object being moved
to-location .041 where the moving object is found when event ends
object-removed .036 object removed from its previous location
inputs .036 pre-existing event participant destroyed or incorporated
into a new entity
products .035 object is one of the intended outputs of event
inputs-destroyed .035 object exists before event and is destroyed during event
from-location .034 where some moving-object in the move is found at the
beginning
primary-object-moving .033 object is in motion at some point during the event, and
this movement is focal
seller .030 agent sells something in the exchange
object-of-possession-transfer .030 rights to use object transferred from one agent to another
transferred-thing .030 object is being moved, transferred, or exchanged in the
event transfer
sender-of-info .030 sender is an agent who is the source of information
transferred
inputs-committed .028 object exists before event and continues to exist
afterwards, and as a result of event, object becomes
incorporated into something created during event
object-emitted .026 object is emitted from the emitter during the emission
event
The Factotum knowledge base is based on the 1911 version of Roget?s Thesaurus
and specifies the relations that hold between the Roget categories and the words listed
in each entry. Factotum incorporates information from other resources as well. For
instance, the Unified Medical Language System (UMLS) formed the basis for the initial
inventory of semantic relations, which was later revised during tagging.
Figure 3 shows a sample from Factotum. This illustrates that the basic Roget or-
ganization is still used, although additional hierarchical levels have been added. The
relations are contained within double braces (e.g., ?{{has subtype}}?) and generally
apply from the category to each word in the synonym list on the same line. For example,
the line with ?{{result of}}? indicates that conversion is the result of transforming,
as shown in the semantic relation listing that would be extracted. There are over 400
different relations instantiated in the knowledge base, which has over 93,000 assertions.
Some of these are quite specialized (e.g., has-brandname). In addition, there are quite a
few inverse relations, becausemost of the relations are not symmetrical. Certain features
of the knowledge representation are ignored during the relation extraction used later.
For example, relation specifications can have qualifier prefixes, such as an ampersand
to indicate that the relationship only sometimes holds.
158
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Table 4
Common semantic roles used in Conceptual Graphs. Inventory and descriptions based on
Sowa (1999, pages 502?510). The term situation is used in place of Sowa?s nexus (i.e., ?fact of
togetherness?), which also covers spatial structures. Freq. gives estimated relative frequencies
from O?Hara (2005).
Role Freq. Description
agent .267 entity voluntarily initiating an action
attribute .155 entity that is a property of some object
characteristic .080 types of properties of entities
theme .064 participant involved with but not changed
patient .061 participant undergoing structural change
location .053 participant of a spatial situation
possession .035 entity owned by some animate being
part .035 object that is a component of some object
origin .035 source of a spatial or ambient situation
experiencer .035 animate goal of an experience
result .032 inanimate goal of an act
instrument .027 resource used but not changed
recipient .019 animate goal of an act
destination .013 goal of a spatial process
point-in-time .011 participant of a temporal situation
path .011 resource of a spatial or ambient situation
accompaniment .011 object participating with another
effector .008 source involuntarily initiating an action
beneficiary .008 entity benefiting from event completion
matter .005 resource that is changed by the event
manner .005 entity that is a property of some process
source .003 present at beginning of activity
resource .003 material necessary for situation
product .003 present at end of activity
medium .003 resource for transmitting information
goal .003 final cause which is purpose or benefit
duration .003 resource of a temporal process
because .003 situation causing another situation
amount .003 a measure of some characteristic
Table 5 shows themost common relations in terms of usage in the semantic network,
and includes others that are used in the experiments discussed later.2 The relative
frequencies just reflect relationships explicitly labeled in the KB data file. For instance,
this does not account for implicit has-subtype relationships based on the hierarchical
organization of the thesaural groups (e.g., ?simple-change, has-subtype, conversion?).
The functional relations are shown in boldface. This excludes the meronym or part-
whole relations (e.g., is-conceptual-part-of ), in line with their classification by Cruse
(1986) as hierarchical relations. The reason for concentrating on the functional relations
is that these are more akin to the roles tagged in PTB and FrameNet.
The information in Factotum complements WordNet through the inclusion of more
functional relations (e.g., non-hierarchical relations such as uses and is-function-of ). For
comparison purposes, Table 6 shows the semantic relation usage in WordNet version
2 The database files and documentation for the semantic network are available from Micra, Inc., via
ftp://micra.com/factotum.
159
Computational Linguistics Volume 35, Number 2
Original data:
A. ABSTRACT RELATION
...
A6 CHANGE (R140 TO R152)
...
A6.1 SIMPLE CHANGE (R140)
...
A6.1.4 CONVERSION (R144)
#144. Conversion.
N. {{has subtype(change, R140)}} conversion, transformation.
{{has case: @R7, initial state, final state}}.
{{has patient: @R3a, object, entity}}.
{{result of}} {{has subtype(process, A7.7)}} converting, transforming.
{{has subtype}} processing.
transition.
Extracted relationships:
?change, has-subtype, conversion? ?change, has-subtype, transformation?
?conversion, has-case, initial state? ?conversion, has-case, final state?
?conversion, has-patient, object? ?conversion, has-patient, entity?
?conversion, is-result-of , converting? ?conversion, is-result-of , transforming?
?process, has-subtype, converting? ?process, has-subtype, transforming?
?conversion, has-subtype, processing?
Figure 3
Sample data from Factotum. Based on version 0.56 of Factotum.
2.1. As can be seen from the table, the majority of the relations are hierarchical.3
WordNet 2.1 averages just about 1.1 non-taxonomic properties per concept (includ-
ing inverses but excluding hierarchical relations such as has-hypernym and is-member-
meronym-of ). OpenCyc provides a much higher average at 3.7 properties per concept,
although with an emphasis on argument constraints and other usage restrictions. Fac-
totum averages 1.8 properties per concept, thus complementing WordNet in terms of
information content.4
2.5 Combining the Different Semantic Role Inventories
It is difficult to provide precise comparisons of the five inventories just discussed. This is
due both to the different nature of the inventories (e.g., developed for knowledge bases
as opposed to being derived from natural language annotations) and due to the way the
3 In WordNet, the is-similar-to relation for adjectives can be considered as hierarchical, as it links satellite
synsets to heads of adjective clusters (Miller 1998). For example, the satellite synsets for ?thirsty? and
?rainless? are both linked to the head synset for ?dry (vs. wet).?
4 These figures are derived by counting the number of relations excluding the instance and subset ones
and then dividing by the number of concepts (i.e., ratio of non-hierarchical relations to concepts). Cyc?s
comments and lexical assertions are also excluded, as these are implicit in Factotum and WordNet.
WordNet?s is-derived-from relations are omitted as lexical in nature (the figure otherwise would be 1.6).
160
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Table 5
Common Factotum semantic roles. These account for 80% of the instances. Boldface relations are
used in the experiments (Section 3.4.2).
Relation Freq. Description
has-subtype .401 inverse of is-a relation
is-property-of .077 object with given salient character
is-caused-by .034 force that is the origin of something
has-property .028 salient property of an object
has-part .022 a part of a physical object
has-high-intensity .018 intensifier for property or characteristic
has-high-level .017 implication of activity (e.g., intelligence)
is-antonym-of .016 generally used for lexical opposition
is-conceptual-part-of .015 parts of other entities (e.g., case relations)
has-metaphor .014 non-literal reference to the word
causesmental .013 motivation (causation in the mental realm)
uses .012 a tool needing active manipulation
is-performed-by .012 human actor for the event
performshuman .011 human role in performing some activity
is-function-of .011 artifact passively performing the function
has-result .010 more specific type of causes
has-conceptual-part .010 generalization of has-part
is-used-in .010 activity or desired effect for the entity
is-part-of .010 distinguishes part from group membership
causes .009 inverse of is-caused-by
has-method .009 method used to achieve some goal
is-caused-bymental .009 inverse of causesmental
has-consequence .008 causation due to a natural association
has-commencement .007 state that commences with the action
is-location-of .007 absolute location of an object
requires .004 object or sub-action needed for an action
is-studied-in .004 inquires into any field of study
is-topic-of .002 communication dealing with given subject
produces .002 what an action yields, generates, etc.
is-measured-by .002 instrument for measuring something
is-job-of .001 occupation title for a job function
is-patient-of .001 action that the object participates in
is-facilitated-by .001 object or sub-action aiding an action
is-biofunction-of .0003 biological function of parts of living things
was-performed-by .0002 is-performed-by occurring in the past
has-consequenceobject .0002 consequence for the patient of an action
is-facilitated-bymental .0001 trait that facilitates some human action
relation listings were extracted (e.g., just including event-based roles from OpenCyc).
As can be seen from Tables 2 and 3, FrameNet tends to refine the roles for agents (e.g.,
communicator) compared to OpenCyc, which in contrast has more refinements of the
object role (e.g., object-removed). The Concept Graphs inventory includes more emphasis
on specialization relations than the others, as can be seen from the top entries in Table 4
(e.g., attribute).
In the next section, we show how classifiers can be automatically developed for
the semantic role inventories just discussed. For the application to dictionary defin-
ition analysis, we need to combine the classifiers learned over PTB, FrameNet, and
Factotum. This can be done readily in a cascaded fashion with the classifier for the
most specific relation inventory (i.e., FrameNet) being used first and then the other
classifiers being applied in turn whenever the classification is inconclusive. This would
161
Computational Linguistics Volume 35, Number 2
Table 6
Semantic relation usage in WordNet. Relative frequencies for semantic relations in WordNet
(173,570 total instances). This table omits lexical relations, such as the is-derived-from relation
(71,914 instances). Frequencies based on analysis of database files for WordNet 2.1.
Relation Freq. Description
has-hypernym .558 superset relation
is-similar-to .130 similar adjective synset
is-member-meronym-of .071 constituent member
is-part-meronym-of .051 constituent part
is-pertainym-of .046 noun that adjective pertains to
is-antonym-of .046 opposing concept
has-topic-domain .038 topic domain for the synset
also-see .019 related entry (for adjectives and verbs)
has-verb-group .010 verb senses grouped by similarity
has-region-domain .008 region domain for the synset
has-attribute .007 related attribute category or value
has-usage-domain .007 usage domain for the synset
is-substance-meronym-of .004 constituent substance
entails .002 action entailed by the verb
causes .001 action caused by the verb
has-participle .001 verb participle
have the advantage that new resources could be integrated into the combined relation
classifier with minimal effort. However, the resulting role inventory would likely be
heterogeneous and might be prone to inconsistent classifications. In addition, the role
inventory could change whenever new annotation resources are incorporated, making
the overall definition analysis system somewhat unpredictable.
Alternatively, the annotations can be converted into a common inventory, and a
separate relation classifier induced over the resulting data. This has the advantage
that the target relation-type inventory remains stable whenever new sources of relation
annotations are introduced. In addition, the classifier will likely be more accurate as
there are more examples per relation type on average. The drawback, however, is that
annotations from new resources must first be mapped into the common inventory
before incorporation.
The latter approach is employed here. The common inventory incorporates some of
the general relation types defined by Gildea and Jurafsky (2002) for their experiments
in classifying semantic relations in FrameNet using a reduced relation inventory. They
defined 18 relations (including a special-case null role for expletives), as shown in
Table 7. These roles served as the starting point for the common relation inventory
we developed to support definition analysis (O?Hara 2005), with half of the roles used
as is and a few others mapped into similar roles. In total, twenty-six relations are
defined, including a few roles based on the PTB, Cyc, and Conceptual Graphs inven-
Table 7
Abstract roles defined by Gildea and Jurafsky based on FrameNet. Taken from Gildea and
Jurafsky (2002).
agent cause degree experiencer force goal
instrument location manner null path patient
percept proposition result source state topic
162
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Table 8
Inventory of semantic relations for definition analysis. This inventory is inspired by the roles in
Table 7 and is primarily based on FrameNet (Fillmore, Wooters, and Baker 2001) and Conceptual
Graphs (Sowa 1999); it also includes roles based on the PTB and Cyc inventories.
Relation Description
accompaniment entity that participates with another entity
agent entity voluntarily performing an action
amount quantity used as a measure of some characteristic
area region in which the action takes place
category general type or class of which the item is an instance
cause non-agentive entity that produces an effect
characteristic general properties of entities
context background for situation or predication
direction either spatial source or goal (same as in PTB)
distance spatial extent of motion
duration period of time that the situation applies within
experiencer entity undergoing some (non-voluntary) experience
goal location that an affected entity ends up in
instrument entity or resource facilitating event occurrence
location reference spatial location for situation
manner property of the underlying process
means action taken to affect something
medium setting in which an affected entity is conveyed
part component of entity or situation
path trajectory which is neither a source nor a goal
product entity present at end of event (same as Cyc products)
recipient recipient of the resource(s)
resource entity utilized during event (same as Cyc inputs)
source initial position of an affected entity
theme entity somehow affected by the event
time reference time for situation
tories. Table 8 shows this role inventory along with a description of each case. In
addition to traditional thematic relations, this includes a few specialization relations,
which are relevant to definition analysis. For example, characteristic corresponds to the
general relation from Conceptual Graphs for properties of entities; and category gen-
eralizes the corresponding FrameNet role, which indicates category type, to subsume
other FrameNet roles related to categorization (e.g., topic). Note that this inventory is
not meant to be definitive and has been developed primarily to address mappings from
FrameNet for the experiments discussed in Section 3.5. Thus, it is likely that additional
roles will be required when additional sources of semantic relations are incorporated
(e.g., Cyc). Themappingswere producedmanually by reviewing the role descriptions in
the FrameNet documentation and checking prepositional usages for each to determine
which of the common inventory roles might be most relevant. As some of the roles with
the same name have frame-specific meanings, in a few cases this involved conflicting
usages (e.g., body-part associated with both area and instrument), which were resolved in
favor of the more common usage.5
5 See www.cs.nmsu.edu/~tomohara/cl-prep-article/relation-mapping.html for the mapping,
covering cases occurring at least 50 times in FrameNet.
163
Computational Linguistics Volume 35, Number 2
3. Preposition Disambiguation
This section presents the results of our experiments on the disambiguation of relations
indicated by prepositional phrases. Results are given for PTB, FrameNet, and Factotum.
The PTB roles are general: For example, for the preposition for, there are six distinctions
(four, with low-frequency pruning). The PTB role disambiguation experiments thus
address a coarse form of sense distinction. In contrast, the FrameNet distinctions are
quite specific: there are 192 distinctions associatedwith for (21 with low-frequency prun-
ing); and, there are 17 distinctions in Factotum (15 with low-frequency pruning). Our
FrameNet and Factotum role disambiguation experiments thus address fine-grained
sense distinctions.
3.1 Overview
A straightforward approach for preposition disambiguation would be to use typical
word-sense disambiguation features, such as the parts-of-speech of surrounding words
and, more importantly, collocations (e.g., lexical associations). Although this can be
highly accurate, it tends to overfit the data and to generalize poorly. The latter is of
particular concern here as the training data is taken from a different genre than the
application data. For example, the PTB data is from newspaper text (specifically, Wall
Street Journal), but the lexical acquisition is based on dictionary definitions. We first
discuss how class-based collocations address this problem and then present the features
used in the experiments.
Before getting into technical details, an informal example will be used to motivate
the use of hypernym collocations. Consider the following purpose role examples, which
are similar to the first example from the introduction.
(3) This contention would justify dismissal of these actions onpurpose
prudential grounds.
(4) Ramada?s stock rose 87.5 cents onpurpose the news.
It turns out that grounds and news are often used as the prepositional object in PTB
when the sense for on is purpose (or reason). Thus, these words would likely be chosen as
collocations for this sense. However, for the sake of generalization, it would be better to
choose theWordNet hypernym subject matter, as that subsumes both words. This would
then allow the following sentence to be recognized as indicating purpose even though
censurewas not contained in the training data.
(5) Senator sets hearing onpurpose censure of Bush.
3.1.1 Class-Based Collocations via Hypernyms. To overcome data sparseness problems, a
class-based approach is used for the collocations, with WordNet synsets as the source
of the word classes. (Part-of-speech tags are a popular type of class-based feature used
in word sense disambiguation (WSD) to capture syntactic generalizations.) Recall that
the WordNet synset hierarchy can be viewed as a taxonomy of concepts. Therefore, in
addition to using collocations in the form of other words, we use collocations in the
form of semantic concepts.
Word collocation features are derived by making two passes over the training
data (e.g., ?on? sentences with correct role indicated). The first pass tabulates the
164
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
co-occurrence counts for each of the context words (i.e., those in a window around the
target word) paired with the classification value for the given training instance (e.g.,
the preposition sense from the annotation). These counts are used to derive conditional
probability estimates of each class value given co-occurrence of the various potential
collocates. The words exceeding a certain threshold are collected into a list associated
with the class value, making this a ?bag of words? approach. In the experiments dis-
cussed below, a potential collocate (coll) is selected whenever the conditional probability
for the class (C) value exceeds the prior probability by a factor greater than 20%:6
P(C|coll)? P(C)
P(C)
? .20 (1)
That is, for a given potential collocation word (coll) to be treated as one of the ac-
tual collocation words, the relative percent change of the class conditional probability
(P(C|coll)) versus the prior probability for the class value (P(C)) must be 20% or higher.
The second pass over the training data determines the value for the collocational feature
of each classification category by checking whether the current context window has any
of the associated collocation words. Note that for the test data, only the second pass is
made, using the collocation lists derived from the training data.
In generalizing this to a class-based approach, the potential collocational words are
replaced with each of their hypernym ancestors fromWordNet. The adjective hierarchy
is relatively shallow, so it is augmented by treating is-similar-to as has-hypernym. For
example, the synset for ?arid? and ?waterless? is linked to the synset for ?dry (vs.
wet).? Adverbs would be included, but there is no hierarchy for them. Because the co-
occurring words are not sense-tagged, this is done for each synset serving as a different
sense of the word. Likewise, in the case of multiple inheritance, each parent synset is
used. For example, given the co-occurring wordmoney, the counts would be updated as
if each of the following tokens were seen (grouped by sense).
1. { medium of exchange#1, monetary system#1, standard#1, criterion#1,
measure#2, touchstone#1, reference point#1, point of reference#1, ref-
erence#3, indicator#2, signal#1, signaling#1, sign#3, communication#2,
social relation#1, relation#1, abstraction#6 }
2. { wealth#4, property#2, belongings#1, holding#2, material possession#1,
possession#2 }
3. { currency#1, medium of exchange#1, monetary system#1, standard#1,
criterion#1, measure#2, touchstone#1, reference point#1, point of -
reference#1, reference#3, indicator#2, signal#1, signaling#1, sign#3,
communication#2, social relation#1, relation#1, abstraction#6 }
Thus, the word token money is replaced by 41 synset tokens. Then, the same two-pass
process just described is performed over the text consisting of the replacement tokens.
Although this introduces noise due to ambiguity, the conditional-probability selection
scheme (Wiebe, McKeever, and Bruce 1998) compensates by selecting hypernym synsets
that tend to co-occur with specific roles.
6 The 20% threshold is a heuristic that is fixed for all experiments. We tested automatic threshold derivation
for Senseval-3 and found that the optimal percentage differed across training sets. As values near 20%
were common, it is left fixed rather than adding an additional feature-threshold refinement step.
165
Computational Linguistics Volume 35, Number 2
Note that there is no preference in the system for choosing either specific or general
hypernyms. Instead, they are inferred automatically based on the word to be disam-
biguated (i.e., preposition for these experiments). Hypernyms at the top levels of the
hierarchy are less likely to be chosen, as they most likely occur with different senses for
the same word (as with relation#1 previously). However, hypernyms at lower levels
tend not to be chosen, as there might not be enough occurrences due to other co-
occurring words. For example, wealth#4 is unlikely to be chosen as a collocation for
the second sense of money, as only a few words map into it, unlike property#2. The
conditional-probability selection scheme (i.e., Equation (1)) handles this automatically
without having to encode heuristics about hypernym rank, and so on.
3.1.2 Classification Experiments. A supervised approach for word-sense disambiguation
is used following Bruce and Wiebe (1999).
For each experiment, stratified 10-fold cross validation is used: The classifiers are
repeatedly trained on 90% of the data and tested on the remainder, with the test sets
randomly selected to form a partition. The results described here were obtained using
the settings in Figure 4, which are similar to the settings used by O?Hara et al (2004)
in the third Senseval competition. The top systems from recent Senseval competitions
(Mihalcea 2002; Grozea 2004) use a variety of lexical features for WSD. Words in the im-
mediate context (Word?i) and their parts of speech (POS?i) are standard features. Word
collocations are also common, but there are various ways of organizing collocations into
features (Wiebe, McKeever, and Bruce 1998). We use the simple approach of having a
single binary feature per sense (e.g., role) that is set true whenever any of the associated
collocation words for that sense are encountered (i.e., per-class-binary).
The main difference of our approach from more typical WSD systems (Mihalcea,
Chklovski, and Kilgarriff 2004) concerns the hypernym collocations. The collocation
context section of Figure 4 shows that word collocations can occur anywhere in the
sentence, whereas hypernym collocations must occur within five words of the target
Features:
Prep: preposition being classified
POS?i: part-of-speech of word at offset i
Word?i: stem of word at offset i
WordCollr: context has word collocation for role r
HypernymCollr: context has hypernym collocation for role r
Collocation context:
Word: anywhere in the sentence
Hypernym: within 5 words of target preposition
Collocation selection:
Frequency: f (word) > 1
Conditional probability: P(C|coll) ? .50
Relative percent change: (P(C|coll)? P(C))/P(C) ? .20
Organization: per-class-binary
Model selection:
C4.5 Decision tree via Weka?s J4.8 classifier (Quinlan 1993; Witten and Frank 1999)
Figure 4
Feature settings used in preposition classification experiments. Aspects that differ from a typical WSD
system are italicized.
166
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
prepositions (i.e., a five-word context window).7 This reduced window size is used
to make the hypernym collocations more related to the prepositional object and the
modified term.
The feature settings in Figure 4 are used in three different configurations: word-
based collocations alone, hypernym collocations alone, and both collocations together.
Combining the two types generally produces the best results, because this balances the
specific clues provided by the word collocations with the generalized clues provided by
the hypernym collocations.
Unlike the general case for WSD, the sense inventory is the same for all the words
being disambiguated; therefore, a single classifier can be produced rather than indi-
vidual classifiers. This has the advantage of allowing more training data to be used
in the derivation of the clues indicative of each semantic role. However, if there were
sufficient annotations for particular preposition, then it would be advantageous to have
a dedicated classifier. For example, the prior probabilities for the roles would be based
on the usages for the given preposition. Therefore, we perform experiments illustrating
the difference when disambiguating prepositions with a single classifier versus the use
of separate classifiers.
3.2 Penn Treebank Classification Experiments
The first set of experiments deals with preposition disambiguation using PTB. When
deriving training data from PTB via the parse tree annotations, the functional tags as-
sociated with prepositional phrases are converted into preposition sense tags. Consider
the following excerpt from the sample annotation for PTB shown earlier:
(6)
(S (PP-TMP In (NP 1982)), temporal extent
(NP-SBJ grammatical subject
(NP (NP (NP Sports) & (NP Recreation) ?s)
managers) ...
Treating temporal as the preposition sense yields the following annotation:
(7) InTMP 1982, Sports & Recreation?s managers ...
The relative frequencies of the roles in the PTB annotations for PPs are shown in Ta-
ble 9. As can be seen, several of the roles do not occur often with PPs (e.g., extent). This
somewhat skewed distribution makes for an easier classification task than the one for
FrameNet.
3.2.1 Illustration with ?at.? As an illustration of the probabilities associated with class-
based collocations, consider the differences in the prior versus class-based conditional
probabilities for the semantic roles of the preposition at in the Penn Treebank (ver-
sion II). Table 10 shows the global probabilities for the roles assigned to at, along with
7 This window size was chosen after estimating that on average the prepositional objects occur within
2.3 ? 1.26 words of the preposition and that the average attachment site is within 3.0 ? 2.98 words. These
figures were produced by analyzing the parse trees for the semantic role annotations in the PTB.
167
Computational Linguistics Volume 35, Number 2
Table 9
Penn Treebank semantic roles for PPs. Omits low-frequency benefactive relation. Freq. is the relative
frequency of the role occurrence (36,476 total instances). Example usages are taken from
the corpus.
Role Freq. Example
locative .472 workers at a factory
temporal .290 expired atmidnight Tuesday
direction .149 has grown at a sluggish pace
manner .050 CDs aimed at individual investors
purpose .030 opened for trading
extent .008 declined by 14%
conditional probabilities for these roles given that certain high-level WordNet synsets
occur in the context. In a context referring to a concrete concept (i.e., entity#1), the
difference in the probability distributions for the locative and temporal roles shows that
the locative interpretation becomes even more likely. In contrast, in a context referring
to an abstract concept (i.e., abstraction#6), the difference in the probability distributions
for the same roles shows that the temporal interpretation becomes more likely. Therefore,
these class-based lexical associations capture commonsense usages of the preposition at.
3.2.2 Results. The classification results for these prepositions in the Penn Treebank show
that this approach is very effective. Table 11 shows the accuracy when disambiguating
the 14 prepositions using a single classifier with 6 roles. Table 11 also shows the per-
class statistics, showing that there are difficulties tagging the manner role (e.g., lowest
F-score). For the single-classifier case, the overall accuracy is 89.3%, using Weka?s J4.8
classifier (Witten and Frank 1999), which is an implementation of Quinlan?s (1993) C4.5
decision tree learner.
For comparison, Table 12 shows the results for individual classifiers created for the
prepositions annotated in PTB. A few prepositions only have small data sets, such as
of which is used more for specialization relations (e.g., category) than thematic ones.
This table is ordered by entropy, which measures the inherent ambiguity in the classes
as given by the annotations. Note that the Baseline column is the probability of the most
frequent sense, which is a common estimate of the lower bound for classification
Table 10
Prior and posterior probabilities of roles for ?at? in the Penn Treebank. P(R) is the relative frequency.
P(R|S) is the probability of the relation given that the synset occurs in the immediate context of
at. RPCR,S is the relative percentage change: (P(R|S)? P(R))/P(R).
Synset
entity#1 abstraction#6
Relation P(R) P(R|S) RPCR,S P(R|S) RPCR,S
locative 73.5 75.5 0.03 67.0 ?0.09
temporal 23.9 22.5 ?0.06 30.6 0.28
manner 2.0 1.5 ?0.25 2.0 0.00
direction 0.6 0.4 ?0.33 0.4 ?0.33
168
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Table 11
Overall preposition disambiguation results over Penn Treebank roles. A single classifier is used for all
the prepositions. # Instances is the number of role annotations. # Classes is the number of distinct
roles. Entropy measures non-uniformity of the role distributions. Baseline is estimated by the
most-frequent role. The Word Only experiment uses just word collocations, Hypernym Only just
uses hypernym collocations, and Both uses both types of collocations. Accuracy is average for
percent correct over ten trials in cross validation. STDEV is the standard deviation over the trials.
Experiment Accuracy STDEV
Word Collocations Only 88.1 0.88
Hypernym Collocations Only 88.2 0.43
Both Collocations 89.3 0.33
Data Set Characteristics
# Instances: 27,308
# Classes: 6
Entropy: 1.831
Baseline: 49.2
Word Only Hypernym Only Both
Class Prec. Rec. F Prec. Rec. F Prec. Rec. F
direction .953 .969 .960 .952 .967 .959 .956 .965 .961
extent .817 .839 .826 .854 .819 .834 .817 .846 .829
locative .879 .967 .921 .889 .953 .920 .908 .932 .920
manner .797 .607 .687 .790 .599 .680 .826 .558 .661
purpose .854 .591 .695 .774 .712 .740 .793 .701 .744
temporal .897 .776 .832 .879 .794 .834 .845 .852 .848
Table 12
Per-preposition disambiguation results over Penn Treebank roles. A separate classifier is used for each
preposition, excluding roles with less than 1% relative frequency. Freq gives the preposition
frequency, and Roles the number of senses. Entropy measures data set uniformity, and Baseline
selects most common role. The Word and Hypernym columns show results when including just
word and hypernym collocations respectively, whereas Both includes both types. Each column
shows averages for percent correct over ten trials. The Mean row averages the values of the
individual experiments.
Prep Freq. Roles Entropy Baseline Word Hypernym Both
through 331 4 1.668 0.438 59.795 62.861 58.592
by 1290 7 1.575 0.479 87.736 88.231 86.655
as 220 3 1.565 0.405 95.113 96.377 96.165
between 87 4 1.506 0.483 77.421 81.032 70.456
of 30 3 1.325 0.567 63.182 82.424 65.606
out 76 4 1.247 0.711 70.238 76.250 63.988
for 1401 6 1.189 0.657 82.444 85.795 80.158
on 1915 5 1.181 0.679 85.998 88.720 79.428
in 14321 7 1.054 0.686 86.404 92.647 86.523
throughout 59 2 0.998 0.525 61.487 35.949 63.923
at 2825 5 0.981 0.735 84.178 90.265 85.561
across 78 2 0.706 0.808 75.000 78.750 77.857
from 1521 5 0.517 0.917 91.649 91.650 91.650
to 3074 5 0.133 0.985 98.732 98.537 98.829
Mean 1944.8 4.43 1.12 0.648 80.0 82.1 78.9
169
Computational Linguistics Volume 35, Number 2
experiments. When using preposition-specific classifiers, the hypernym collocations
surprisingly outperform the other configurations, most likely due to overfitting with
word-based clues: 82.1% versus 80.0% for the word-only case.
3.3 FrameNet Classification Experiments
The second set of experiments perform preposition disambiguation using FrameNet.
A similar preposition word-sense disambiguation experiment is carried out over the
FrameNet semantic role annotations involving prepositional phrases. Consider the sam-
ple annotation shown earlier:
(8) Hewlett-Packard Co has rolled out a new range of ISDN connectivity
enabling ?C FE=?Communicator? PT=?NP??standalone workstations?/C?
to ?C TARGET=?y??communicate?/C? ?C FE=?Medium? PT=?PP??over
public or private ISDN networks?/C?.
The prepositional phrase annotation is isolated and treated as the sense of the preposi-
tion. This yields the following sense annotation:
(9) Hewlett-Packard Co has rolled out a new range of ISDN connectivity
enabling standalone workstations to communicate overMedium public or
private ISDN networks.
Table 13 shows the distribution of common roles assigned to prepositional phrases. The
topic role is the most frequent case not directly covered in PTB.
3.3.1 Illustration with ?at.? See Table 14 for the most frequent roles out of the 124 cases
that were assigned to at, along with the conditional probabilities for these roles given
that certain high-level WordNet synsets occur in the context. In a context referring
to concrete entities, the role place becomes more prominent. However, in an abstract
context, the role time becomes more prominent. Thus, similar behavior to that noted for
PTB in Section 3.2.1 occurs with FrameNet.
3.3.2 Results. Table 15 shows the results of classification when all of the prepositions
are classified together. Due to the exorbitant number of roles (641), the overall results
are low. However, the combined collocation approach still shows slight improvement
(23.3% versus 23.1%). The FrameNet inventory contains many low-frequency relations
Table 13
Most common FrameNet semantic roles for PPs. Relative frequencies for roles assigned to
prepositional phrases in version 1.3 (66,038 instances), omitting cases below 0.01.
Role Freq. Role Freq. Role Freq.
goal .092 theme .022 whole .015
path .071 manner .021 individuals .013
source .043 area .018 location .012
topic .040 reason .018 ground .012
time .037 addressee .017 means .011
place .033 stimulus .017 content .011
170
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Table 14
Prior and posterior probabilities of roles for ?at? in FrameNet. Only the top 5 of 641 applicable roles
are shown. P(R) is the relative frequency for relation. P(R|S) is the probability of the relation given
that the synset occurs in the immediate context of at. RPCR,S is the relative percentage change:
(P(R|S) ? P(R))/P(R).
Synset
entity#1 abstraction#6
Relation P(R) P(R|S) RPCR,S P(R|S) RPCR,S
place 15.6 19.0 21.8 16.8 7.7
time 12.0 11.5 ?4.2 15.1 25.8
stimulus 6.6 5.0 ?24.2 6.6 0.0
addressee 6.1 4.4 ?27.9 3.3 ?45.9
goal 5.5 6.3 14.5 6.0 9.1
Table 15
Preposition disambiguation with all FrameNet roles. All 641 roles are considered. Entropymeasures
data set uniformity, and Baseline selects most common role.
Experiment Accuracy STDEV
Word Collocations Only 23.078 0.472
Hypernym Collocations Only 23.206 0.467
Both Collocations 23.317 0.556
Data Set Characteristics
# Instances: 65,550
# Classes: 641
Entropy: 6.785
Baseline: 9.3
that complicate this type of classification. By filtering out relations that occur in less than
1% of the role occurrences for prepositional phrases, substantial improvement results,
as shown in Table 16. Even with filtering, the classification is challenging (e.g., 18 classes
with entropy 3.82). Table 16 also shows the per-class statistics, indicating that the means
and place roles are posing difficulties for classification.
Table 17 shows the results when using individual classifiers, ordered by entropy.
This illustrates that the role distributions are more complicated than those for PTB,
yielding higher entropy values on average. In all, there are over 360 prepositions
with annotations, 92 with ten or more instances each. (Several of the low-frequency
cases are actually adverbs, such as anywhere, but are treated as prepositions during the
annotation extraction.) The results show that the word collocations produce slightly
better results: 67.8 versus 66.0 for combined collocations. Unlike the case with PTB,
the single-classifier performance is below that of the individual classifiers. This is
due to the fine-grained nature of the role inventory. When all the roles are considered
together, prepositions are sometimes being incorrectly classified using roles that have
not been assigned to them in the training data. This occurs when contextual clues are
stronger for a commonly used role than for the appropriate one. Given PTB?s small role
inventory, this problem does not occur in the corresponding experiments.
3.4 Factotum Classification Experiments
The third set of experiments deals with preposition disambiguation using Factotum.
Note that Factotum does not indicate the way the relationships are expressed in English.
171
Computational Linguistics Volume 35, Number 2
Table 16
Overall results for preposition disambiguation with common FrameNet roles. Excludes roles with less
than 1% relative frequency. Entropymeasures data set uniformity, and Baseline selects most
common role. Detailed per-class statistics are also included, averaged over the 10 folds.
Experiment Accuracy STDEV
Word Collocations Only 73.339 0.865
Hypernym Collocations Only 73.437 0.594
Both Collocations 73.544 0.856
Data Set Characteristics
# Instances: 32974
# Classes: 18
Entropy: 3.822
Baseline: 18.4
Word Only Hypernym Only Both
Class Prec. Rec. F Prec. Rec. F Prec. Rec. F
addressee .785 .332 .443 .818 .263 .386 .903 .298 .447
area .618 .546 .578 .607 .533 .566 .640 .591 .613
content .874 .618 .722 .895 .624 .734 .892 .639 .744
goal .715 .766 .739 .704 .778 .739 .703 .790 .743
ground .667 .386 .487 .684 .389 .494 .689 .449 .541
individuals .972 .947 .959 .961 .945 .953 .938 .935 .936
location .736 .524 .610 .741 .526 .612 .815 .557 .660
manner .738 .484 .584 .748 .481 .584 .734 .497 .591
means .487 .449 .464 .562 .361 .435 .524 .386 .441
path .778 .851 .812 .777 .848 .811 .788 .849 .817
place .475 .551 .510 .483 .549 .513 .474 .576 .519
reason .803 .767 .784 .777 .773 .774 .769 .714 .738
source .864 .980 .918 .865 .981 .919 .860 .978 .915
stimulus .798 .798 .797 .795 .809 .802 .751 .752 .750
theme .787 .811 .798 .725 .847 .779 .780 .865 .820
time .585 .665 .622 .623 .687 .653 .643 .690 .664
topic .831 .836 .833 .829 .842 .835 .856 .863 .859
whole .818 .932 .871 .807 .932 .865 .819 .941 .875
Similarly, WordNet does not indicate this, but it does include definition glosses. For
example,
(10)
Factotum:
?drying, is-function-of , drier?
WordNet:
dryalter remove the moisture from and make dry
dryerappliance an appliance that removes moisture
These definition glosses might be useful in certain cases for inferring the relation markers
(i.e., generalized case markers). As is, Factotum cannot be used to provide training data
for learning how the relations are expressed in English. This contrasts with corpus-
based annotations, such as PTB (Marcus et al 1994) and FrameNet (Fillmore, Wooters,
and Baker 2001), where the relationships are marked in context.
3.4.1 Inferring Semantic Role Markers. To overcome the lack of context in Factotum, the
relation markers are inferred through corpus checks, in particular through proximity
searches involving the source and target terms from the relationship (i.e., ?source,
172
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Table 17
Per-preposition disambiguation results over FrameNet roles. A separate classifier is used for each
preposition, excluding roles with less than 1% relative frequency. Freq gives the preposition
frequency, and Roles the number of senses. Entropy measures data set uniformity, and Baseline
selects most common role. The Word and Hypernym columns show results when including just
word and hypernym collocations, respectively, whereas Both includes both types. Each column
shows averages for percent correct over ten trials. The Mean row averages the values of the
individual experiments.
Prep Freq. Roles Entropy Baseline Word Hypernym Both
with 3758 25 4.201 19.6 59.970 57.809 61.924
of 7339 22 4.188 12.8 85.747 84.663 85.965
between 675 23 4.166 11.4 61.495 56.215 53.311
under 286 26 4.045 25.5 29.567 33.040 33.691
against 557 26 4.028 21.2 53.540 58.885 31.892
for 2678 22 3.988 22.6 58.135 58.839 39.809
by 3348 18 3.929 13.6 62.618 60.854 61.152
on 3579 22 3.877 18.1 61.011 57.671 60.838
at 2685 21 3.790 21.2 61.814 58.501 57.630
in 6071 18 3.717 18.7 54.253 49.953 53.880
as 1123 17 3.346 27.1 53.585 47.186 42.722
to 4741 17 3.225 36.6 71.963 77.751 72.448
behind 254 13 3.222 22.8 47.560 41.045 43.519
over 1157 16 3.190 27.8 47.911 48.548 50.337
after 349 16 2.837 45.8 62.230 65.395 61.944
around 772 15 2.829 45.1 52.463 52.582 49.357
from 3251 14 2.710 51.2 73.268 71.934 75.423
round 389 12 2.633 34.7 46.531 50.733 49.393
into 1923 14 2.208 62.9 79.175 77.366 80.846
during 242 10 2.004 63.6 71.067 75.200 68.233
like 570 9 1.938 62.3 82.554 79.784 85.666
through 1358 10 1.905 66.0 77.800 77.798 79.963
up 745 10 1.880 60.3 76.328 76.328 74.869
off 647 9 1.830 63.8 90.545 86.854 90.423
out 966 8 1.773 60.7 77.383 79.722 78.671
across 894 11 1.763 67.6 80.291 80.095 80.099
towards 673 10 1.754 67.9 65.681 71.171 65.517
down 965 7 1.600 63.2 81.256 81.466 79.141
along 723 9 1.597 72.5 87.281 86.862 86.590
about 1894 8 1.488 72.2 83.214 76.663 83.899
back 405 7 1.462 64.7 88.103 91.149 86.183
past 275 9 1.268 78.9 85.683 86.423 85.573
Mean 1727.9 14.8 2.762 43.8 67.813 67.453 65.966
relation, target?). For example, using AltaVista?s Boolean search,8 this can be done via
?source NEAR target.?
Unfortunately, this technique would require detailed post-processing of the Web
search results, possibly including parsing, in order to extract the patterns. As an ex-
pedient, common prepositions9 are included in a series of proximity searches to find
8 AltaVista?s Boolean search is available at www.altavista.com/sites/search/adv.
9 The common prepositions are determined from the prepositional phrases assigned functional
annotations in the Penn Treebank (Marcus et al 1994).
173
Computational Linguistics Volume 35, Number 2
the preposition occurring most frequently with the given terms. For instance, given the
relationship ?drying, is-function-of, drier?, the following searches would be performed.
(11) drying NEAR drier NEAR in
drying NEAR drier NEAR to
...
drying NEAR drier NEAR ?around?
To account for prepositions that occur frequently (e.g., of ), pointwise mutual infor-
mation (MI) statistics (Manning and Schu?tze 1999, pages 66?68) are used in place of the
raw frequency when rating the potential markers. These are calculated as follows:
MIprep = log2
P(X,Y)
P(X)? P(Y)
? log2
f (source NEAR target NEAR prep)
f (source NEAR target)? f (prep)
(2)
Such checks are done for the 25 most common prepositions to find the preposition
yielding the highest mutual information score. For example, the top three markers for
the ?drying, is-function-of, drier? relationship based on this metric are during, after, and
with.
3.4.2 Method for Classifying Functional Relations. Given the functional relationships in
Factotum along with the inferred relation markers, machine-learning algorithms can
be used to infer what relation most likely applies to terms occurring together with a
particular marker. Note that the main purpose of including the relation markers is to
provide clues for the particular type of relation. Because the source term and target
terms might occur in other relationships, associations based on them alone might not
be as accurate. In addition, the inclusion of these clue words (e.g., the prepositions)
makes the task closer to what would be done in inferring the relations from free text.
The task thus approximates preposition disambiguation, using the Factotum relations
as senses.
Figure 5 gives the feature settings used in the experiments. This is a version of
the feature set used in the PTB and FrameNet experiments (see Figure 4), simplified to
account for the lack of sentential context. Figure 6 contains sample feature specifications
from the experiments discussed in the next section. The top part shows the original
relationships from Factotum; the first example indicates that connaturalize causes simi-
larity. Also included is the most likely relation marker inferred for each instance. This
shows that ?n/a? is used whenever a preposition for a particular relationship cannot be
inferred. This happens in the first example because connaturalize is a rare term.
The remaining parts of Figure 6 illustrate the feature values that would be derived
for the three different experiment configurations, based on the inclusion of word and/or
hypernym collocations. In each case, the classification variable is given by relation.
For brevity, the feature specification only includes collocation features for the most
frequent relations. Sample collocations are also shown for the relations (e.g., vulgar-
ity for is-caused-by). In the word collocation case, the occurrence of similarity is used
to determine that the is-caused-by feature (WC1) should be positive (i.e., ?1?) for the
first two instances. Note that there is no corresponding hypernym collocation due to
conditional probability filtering. In addition, although new is not included as a word
collocation, one of its hypernyms, namely Adj:early#2, is used to determine that the
has-consequence feature (HC3) should be positive in the last instance.
174
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Context:
Source and target terms from relationship (?source, relation, target?)
Features:
POSsource: part-of-speech of the source term
POStarget: part-of-speech of the target term
Prep: preposition serving as relation marker (?n/a? if not inferable)
WordCollr: 1 iff context contains any word collocation for relation r
HypernymCollr: 1 iff context contains any hypernym collocation for relation r
Collocation selection:
Frequency: f (word) > 1
Relative percent change: (P(C|coll)? P(C))/P(C) ? .20
Organization: per-class-binary grouping
Model selection:
Decision tree using Weka?s J4.8 classifier (Witten and Frank 1999)
Figure 5
Features used in Factotum role classification experiments. Simplified version of Figure 4: Context
only consists of the source and target terms.
3.4.3 Results. To make the task more similar to the PTB and FrameNet cases covered
previously, only the functional relations in Factotum are used. These are determined
by removing the hierarchical relations (e.g., has-subtype and has-part) along with the
attribute relations (e.g., is-property-of ). In addition, in cases where there are inverse
functions (e.g., causes and is-caused-by), the most frequently occurring relation of each
inverse pair is used. This is done because the relation marker inference approach does
not account for argument order. The boldface relations in the listing shown earlier in
Table 5 are those used in the experiment. Only single-word source and target terms are
considered to simplify the WordNet hypernym lookup (i.e., no phrasals). The resulting
data set has 5,959 training instances. The data set alo includes the inferred relation
markers (e.g., one preposition per training instance), thus introducing some noise.
Figure 6 includes a few examples from this data set. This shows that the original
relationship ?similarity, is-caused-by, rhyme? from Factotum is augmented with the
by marker prior to classification. Again, these markers are inferred via Web searches
involving the terms from the original relationship.
Table 18 shows the results of the classification. The combined use of both collocation
types achieves the best overall accuracy at 71.2%, which is good considering that the
baseline of always choosing the most common relation (is-caused-by) is 24.2%. This com-
bination generalizes well by using hypernym collocations, while retaining specificity
via word collocations. The classification task is difficult, as suggested by the number
of classes, entropy, and baseline values all being comparable to the filtered FrameNet
experiment (see Table 16).
3.5 Common Relation Inventory Classification Experiments
The last set of experiments investigate preposition disambiguation using FrameNet
mapped into a reduced semantic role inventory. For the application to lexical acqui-
sition, the semantic role annotations are converted into the common relation inventory
discussed in Section 2.5. To apply the common inventory to the FrameNet data, anno-
tations using the 641 FrameNet relations (see Table 2) need to be mapped into those
175
Computational Linguistics Volume 35, Number 2
Relationships from Factotum with inferred markers:
Relationship Marker
?similarity, is-caused-by, connaturalize? n/a
?similarity, is-caused-by, rhyme? by
?approximate, has-consequence, imprecise? because
?new, has-consequence, patented? with
Word collocations only:
Relation POSs POSt Prep WC1 WC2 WC3 WC4 WC5 WC6 WC7
is-caused-by NN VB n/a 1 0 0 0 0 0 0
is-caused-by NN NN by 1 0 0 0 0 0 0
has-consequence NN JJ because 0 0 0 0 0 0 0
has-consequence JJ VBN with 0 0 0 0 0 0 0
Sample collocations:
is-caused-by {bitterness, evildoing, monochrome, similarity, vulgarity}
has-consequence {abrogate, frequently, insufficiency, nonplus, ornament}
Hypernym collocations only:
Relation POSs POSt Prep HC1 HC2 HC3 HC4 HC5 HC6 HC7
is-caused-by NN VB n/a 0 0 0 0 0 0 0
is-caused-by NN NN by 0 0 0 0 0 0 0
has-consequence NN JJ because 0 0 0 0 0 0 0
has-consequence JJ VBN with 0 0 1 0 0 0 0
Sample collocations:
is-caused-by {N:hostility#3, N:inelegance#1, N:humorist#1}
has-consequence {V:abolish#1, Adj:early#2, N:inability#1, V:write#2}
Both collocations:
Relation POSs POSt Prep WC1 ... WC7 HC1 HC2 HC3 ...
is-caused-by NN VB n/a 1 ... 0 0 0 0 ...
is-caused-by NN NN by 1 ... 0 0 0 0 ...
has-consequence NN JJ because 0 ... 0 0 0 0 ...
has-consequence JJ VBN with 0 ... 0 0 0 1 ...
Legend:
POSs & POSt are the parts of speech for the source and target terms; and
WCr & HCr are the word and hypernym collocations as follows:
1. is-caused-by 2. is-function-of 3. has-consequence 4. has-result
5. is-caused-bymental 6. is-performed-by 7. uses
Figure 6
Sample feature specifications for Factotum experiments. Each relationship from Factotum is
augmented with one relational marker inferred via Web searches, as shown at top of figure.
Three distinct sets of feature vectors are shown based on the type of collocation included,
omitting features for low-frequency relations.
176
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Table 18
Functional relation classification over Factotum. This uses the relational source and target terms
with inferred prepositions. The accuracy figures are averages based on 10-fold cross validation.
The gain in accuracy for the combined experiment versus the word experiment is statistically
significant at p < .01 (via a paired t-test).
Experiment Accuracy STDEV
Word Collocations Only 68.4 1.28
Hypernym Collocations Only 53.9 1.66
Both Collocations 71.2 1.78
Data Set Characteristics
# Instances: 5,959
# Classes: 21
Entropy: 3.504
Baseline: 24.2
using the 26 common relations shown in Table 8. Results for the classification of the
FrameNet data mapped into the common inventory are shown in Table 19. As can
be seen, the performance is well above that of the full classification over FrameNet
without filtering (see Table 15). Although the low-frequency role filtering yields the
highest performance (see Table 16), this comes at the expense of having half of the
training instances discarded. Corpus annotations are a costly resource, so such waste
is undesirable. Table 19 also shows the per-class statistics, indicating that the means,
direction, and part roles are handled poorly by the classifier. The latter two are due to the
relatively small training examples for the roles in question, which can be addressed
partly by refining the mapping from FrameNet. However, problems classifying the
means role occur with all classifiers discussed in this article, suggesting that that role
is too subtle to be classified with the feature set currently used.
The results in Table 19 also illustrate that the reduced, common-role inventory has
an additional advantage of improving performance in the classification, compared to a
cascaded approach. This occurs because several of the miscellaneous roles in FrameNet
cover subtle distinctions that are not relevant for definition analysis (e.g., cognizer and
addressee). The common inventory therefore strikes a balance between the overly general
roles in PTB, which are easy to classify, and the overly specialized roles in FrameNet,
which are quite difficult to classify. Nonetheless, a certain degree of classification diffi-
culty is inevitable in order for the inventory to provide adequate coverage of the dif-
ferent distinctions present in dictionary definitions. Note that, by using the annotations
from PTB and FrameNet, the end result is a general-purpose classifier, not one tied into
dictionary text. Thus, it is useful for other tasks besides definition analysis.
This classifier was used to disambiguate prepositions in the lexical acquisition
system we developed at NMSU (O?Hara 2005). Evaluation of the resulting distinctions
was performed by having the output of the system rated by human judges. Manu-
ally corrected results were also evaluated by the same judges. The overall ratings are
not high in both cases, suggesting that some of the distinctions being made are subtle.
For instance, for ?counterintelligence achieved by deleting any information of value?
from the definition of censoring, means is the preferred role for by, but manner is ac-
ceptable. Likewise, characteristic is the preferred role for of, but category is interpretable.
Thus, the judges differed considerably on these cases. However, as the ratings for
the uncorrected output were close to those for the corrected output, the approach is
promising to use for lexical acquisition. If desired, the per-role accuracy results shown
in Table 19 could be incorporated as confidence values assigned to particular relation-
ships extracted from definitions (e.g., 81% for those with source but only 21% when
means used).
177
Computational Linguistics Volume 35, Number 2
4. Related Work
The main contribution of this article concerns the classification methodology (rather
than the inventories for semantic roles), so we will only review other work related
to this aspect. First, we discuss similar work involving hypernyms. Then, we address
preposition classification proper.
Scott and Matwin (1998) use WordNet hypernyms for text classification. They
include a numeric density feature for any synset that subsumes words appearing in
the document, potentially yielding hundreds of features. In contrast, the hypernym
collocations discussed in Section 3.1.1 involve a binary feature for each of the relations
being classified, using indicative synsets based on the conditional probability test. This
test alleviates the need for their maximum height parameter to avoid overly general
hypernyms. Their approach, as well as ours, considers all senses of a word, distrib-
uting the alternative readings throughout the set of features. In comparison, Gildea
Table 19
Results for preposition disambiguation with common roles. The FrameNet annotations are mapped
into the common inventory from Table 8. Entropymeasures data set uniformity, and Baseline
selects most common role. Detailed per-class statistics are also included, averaged over the
10 folds.
Experiment Accuracy STDEV
Word Collocations Only 62.9 0.345
Hypernym Collocations Only 62.6 0.487
Both Collocations 63.1 0.639
Data Set Characteristics
# Instances: 59,615
# Classes: 24
Entropy: 4.191
Baseline: 12.2
Word Only Hypernym Only Both
Class Prec. Rec. F Prec. Rec. F Prec. Rec. F
accompaniment .630 .611 .619 .671 .605 .636 .628 .625 .626
agent .623 .720 .667 .639 .726 .677 .616 .731 .668
area .546 .475 .508 .541 .490 .514 .545 .501 .522
category .694 .706 .699 .695 .700 .697 .714 .718 .716
cause .554 .493 .521 .569 .498 .531 .540 .482 .509
characteristic .595 .468 .523 .607 .474 .530 .584 .490 .532
context .569 .404 .472 .577 .388 .463 .568 .423 .485
direction .695 .171 .272 .701 .189 .294 .605 .169 .260
duration .601 .465 .522 .589 .445 .503 .596 .429 .497
experiencer .623 .354 .449 .606 .342 .435 .640 .378 .474
goal .664 .683 .673 .662 .674 .668 .657 .680 .668
instrument .406 .339 .367 .393 .337 .360 .405 .370 .385
location .433 .557 .487 .427 .557 .483 .417 .553 .475
manner .493 .489 .490 .483 .478 .479 .490 .481 .485
means .235 .183 .205 .250 .183 .210 .254 .184 .212
medium .519 .306 .382 .559 .328 .412 .529 .330 .403
part .539 .289 .368 .582 .236 .323 .526 .301 .380
path .705 .810 .753 .712 .813 .759 .706 .795 .748
product .837 .750 .785 .868 .739 .788 .769 .783 .770
recipient .661 .486 .559 .661 .493 .563 .642 .482 .549
resource .613 .471 .530 .614 .458 .524 .618 .479 .539
source .703 .936 .802 .697 .936 .799 .707 .937 .806
theme .545 .660 .596 .511 .661 .576 .567 .637 .600
time .619 .624 .621 .626 .612 .619 .628 .611 .619
178
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
and Jurafsky (2002) instead just select the first sense for their hypernym features for
relation classification. They report marginal improvements using the features, whereas
configurations with hypernym collocations usually perform best in our preposition
disambiguation experiments.
Mohit and Narayanan (2003) use WordNet hypernyms to generalize patterns for
information extraction inferred from FrameNet annotations by distributing support
from terms co-occurring in annotations for frame elements to the terms for hypernyms.
However, they do not incorporate a filtering stage, as with our conditional probability
test. Mihalcea (2002) shows how hypernym information can be useful in deriving clues
for unsupervised WSD. Patterns for co-occurring words of a given sense are induced
from sense-tagged corpora. Each pattern specifies templates for the co-occurring words
in the immediate context window of the target word, as well as their corresponding
synsets if known (e.g., sense tagged or unambiguous), and similarly the hypernym
synsets if known. To disambiguate a word, the patterns for each of its senses are
evaluated in the context, and the sense with the most support is chosen.
The work here addresses relation disambiguation specifically with respect to those
indicated by prepositional phrases (i.e., preposition word-sense disambiguation). Until
recently, there has been little work on general-purpose preposition disambiguation.
Litkowski (2002) and Srihari, Niu, and Li (2001) present approaches using manually
derived rules. Both approaches account for only a handful of prepositions; in contrast,
for FrameNet we disambiguate 32 prepositions via individual classifiers and over 100
prepositions via the combined classifier. Liu and Soo (1993) present a heuristic approach
for relation disambiguation relying upon syntactic clues as well as occurrence of specific
prepositions. They assign roles to constituents of a sentence from corpus data provided
that sufficient instances are available. Otherwise, a human trainer is used to answer
questions needed by the system for the assignment. They report an 86% accuracy rate
for the assignment of roles to verbal arguments in about 5,000 processed sentences.
Alam (2004) sketches out how the preposition over might be disambiguated into one
of a dozen roles using features based on the head and complement, such as whether the
head is amovement verb orwhether the complement refers to a duration. These features
form the basis for a manually-constructed decision tree, which is interpreted by hand in
an evaluation over sentences from the British National Corpus (BNC), giving a precision
of 93.5%. Boonthum, Toida, and Levinstein (2006), building upon the work of Alam,
show how WordNet can be used to automate the determination of similar head and
complement properties. For example, if both the head and complement refer to people,
with should be interpreted as accompaniment. These features form the basis for a
disambiguation system using manually constructed rules accounting for ten commonly
occurring prepositions. They report a precision of 79% with a recall of 76% over an
inventory of seven roles in a post hoc evaluation that allows for partial correctness.
There have been a fewmachine-learning approaches that are more similar to the ap-
proach used here. Gildea and Jurafsky (2002) perform relation disambiguation using the
FrameNet annotations as training data. They include lexical features for the headword
of the phrase and the predicating word for the entire annotated frame (e.g., the verb
corresponding to the frame under which the annotations are grouped). They also use
several features derived from the output of a parser, such as the constituent type of the
phrase (e.g., NP), the grammatical function (e.g., subject), and a path feature listing part-
of-speech tags from the target word to the phrase being tagged. They report an accuracy
of 78.5% with a baseline of 40.6% over the FrameNet semantic roles. However, by
conditioning the classification on the predicatingword, the range of roles for a particular
classification instance is more limited than in the experiments presented in this article.
179
Computational Linguistics Volume 35, Number 2
Blaheta and Charniak (2000) use the PTB annotations for relation disambiguation. They
use a few parser-derived features, such as the constituent labels for nearby nodes and
part-of-speech for parent and grandparent nodes. They also include lexical features for
the head and alternative head (because prepositions are considered as the head by their
parser). As their classifier tags all adjuncts, they include the nominal and adverbial roles,
which are syntactic and more predictable than the roles occurring with prepositional
phrases.
There have been recent workshops featuring competitions for semantic role tagging
(Carreras and Ma`rquez 2004, 2005; Litkowski 2004). A common approach is to tag all
the semantic roles in a sentence at the same time to account for dependencies, such
as via Hidden Markov Models. To take advantage of accurate Support Vector Machine
classification, Pradhan et al (2005) instead use a postprocessing phrase based on trigram
models of roles. Their system incorporates a large variety of features, building upon sev-
eral different preceding approaches, such as including extensions to the path features
from Gildea and Jurafsky (2002). Their lexical features include the predicate root word,
headwords for the sentence constituents and PPs, as well as their first and last words.
Koomen et al (2005) likewise use a large feature set. They use an optimization phase to
maximize satisfaction of the constraints imposed by the PropBank data set, such as the
number of arguments for particular predicates (e.g., just two for stalk, arg0 and arg1).
Lastly, Ye and Baldwin (2006) show how filtering can be used to constrain the
hypernyms selected to serve as collocations, building upon our earlier work (O?Hara
and Wiebe 2003). They report 87.7% accuracy in a setup similar to ours over PTB
(i.e., a gain of 2 percentage points). They use a different type of collocation feature
than ours: having a binary feature for each potential collocation rather than a single
feature per class. That is, they useOver-Range Binary rather than Per-Class Binary (Wiebe,
McKeever, and Bruce 1998). Moreover, they include several hundred of these features,
rather than our seven (benefactive previously included), which is likely the main source
of improvement. Again, the per-class binary organization is a bag of words approach,
so it works well only with a limited number of potential collocations. Follow-up work
of theirs (Ye and Baldwin 2007) fared well in the recent preposition disambiguation
competition, held as part of SemEval-2007 (Litkowski and Hargraves 2007). Thus, an
immediate area for future work will be to incorporate such improved feature sets. We
will also investigate addressing sentential role constraints as in general semantic role
tagging.
5. Conclusion
This article shows how to exploit semantic role resources for preposition disambigua-
tion. Information about two different types of semantic role resources is provided. The
emphasis is on corpus-based resources providing annotations of naturally occurring
text. The Penn Treebank (Marcus et al 1994) covers general roles for verbal adjuncts and
FrameNet (Fillmore, Wooters, and Baker 2001) includes a wide range of domain-specific
roles for all verbal arguments. In addition, semantic role inventories from knowledge
bases are investigated. Cyc (Lehmann 1996) provides fine-grained role distinctions,
Factotum (Cassidy 2000) includes a variety of functional relations, and work in Concep-
tual Graphs (Sowa 1999) emphasizes roles for attributes. Relations from both types of
resources are considered when developing the inventory of relations used for definition
analysis, as shown in Table 8.
The disambiguation concentrates on relations indicated by prepositional phrases,
and is framed as word-sense disambiguation for the preposition in question. A new
180
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
type of feature for word-sense disambiguation is introduced, using WordNet hyper-
nyms as collocations rather than just words, as is typically done. The full feature set is
shown in Figure 4. Various experiments over the PTB and FrameNet data are presented,
including prepositions classified separately versus together, and illustrating the effects
of filtering. The main results in Tables 11 and 16 show that the combined use of word
and hypernym collocations generally achieves the best performance. For relationships
derived from knowledge bases, the prepositions and other relational markers need to
be inferred from corpora. A method for doing this is demonstrated using Factotum,
with results shown in Table 18. In addition, to account for granularity differences in the
semantic role inventories, the relations are mapped into a common inventory that was
developed based on the inventories discussed in the article. This allows for improved
classification in cases where inventories provide overly specialized relations, such as
those in FrameNet. Classification results are shown in Table 19.
The recent competitions on semantic relation labeling have highlighted the useful-
ness of incorporating a variety of clues for general-purpose relation disambiguation
(Carreras and Ma`rquez 2005). Some of the techniques developed here for preposition
disambiguation can likely help with relation disambiguation in general. For instance,
there are quite a few lexical features, such as in Pradhan et al (2005), which could be
extended to use semantic classes as with our hypernym collocations. In general it seems
that, when lexical features are used in supervised machine learning, it is likely that
corresponding class-based features based on hypernyms can be beneficial for improved
coverage.
Other aspects of this approach are geared specifically to our goal of supporting
lexical acquisition from dictionaries, which was the motivation for the emphasis on
preposition disambiguation. Isolating the preposition annotations allows the classifiers
to be more readily tailored to definition analysis, especially because predicate frames
are not assumed as with other FrameNet relation disambiguation. Future work will
investigate combining the general relation classifiers with preposition disambiguation
classifiers, such as is done in Ye and Baldwin (2006). Future work will also investigate
improvements to the application to definition analysis. Currently, FrameNet roles are
alwaysmapped to the same common inventory role (e.g., place to location). However, this
should account for the frame of the annotation and perhaps other context information.
Lastly, we will also look for more resources to exploit for preposition disambiguation
(e.g., ResearchCyc).
Acknowledgments
The experimentation for this article was
greatly facilitated though the use of
computing resources at New Mexico State
University. We are also grateful for the
extremely helpful comments provided
by the anonymous reviewers.
References
Alam, Yukiko Sasaki. 2004. Decision trees
for sense disambiguation of prepositions:
Case of over. In Proceedings of the
Computational Lexical Semantics Workshop,
pages 52?59, Boston, MA.
Barker, Ken. 1998. Semi-Automatic Recognition
of Semantic Relationships in English Technical
Texts. Ph.D. thesis, Department of
Computer Science, University of Ottawa.
Bies, Ann, Mark Ferguson, Karen Katz,
Robert MacIntyre, Victoria Tredinnick,
Grace Kim, Mary Ann Marcinkiewicz,
and Britta Schasberger. 1995. Bracketing
guidelines for Treebank II style:
Penn Treebank project. Technical
Report MS-CIS-95-06, University of
Pennsylvania.
Blaheta, Don and Eugene Charniak. 2000.
Assigning function tags to parsed text.
In Proceedings of the 1st Annual Meeting
of the North American Chapter of the
American Association for Computational
Linguistics (NAACL-2000), pages
234?240,Seattle, WA.
181
Computational Linguistics Volume 35, Number 2
Boonthum, Chutima, Shunichi Toida, and
Irwin B. Levinstein. 2006. Preposition
senses: Generalized disambiguation
model. In Proceedings of the Seventh
International Conference on Computational
Linguistics and Intelligent Text Processing
(CICLing-2006), pages 196?207,
Mexico City.
Bruce, Bertram. 1975. Case systems for
natural language. Artificial Intelligence,
6:327?360.
Bruce, Rebecca and Janyce Wiebe. 1999.
Decomposable modeling in natural
language processing. Computational
Linguistics, 25(2):195?208.
Carreras, Xavier and Llu??s Ma`rquez. 2004.
Introduction to the CoNLL-2004 shared
task: Semantic role labeling. In Proceedings
of the Eighth Conference on Natural Language
Learning (CoNLL-2004), pages 89?97,
Boston, MA.
Carreras, Xavier and Llu??s Ma`rquez. 2005.
Introduction to the CoNLL-2005 shared
task: Semantic role labeling. In Proceedings
of the Ninth Conference on Natural Language
Learning (CoNLL-2005), pages 152?164,
Ann Arbor, MI.
Cassidy, Patrick J. 2000. An investigation
of the semantic relations in the Roget?s
Thesaurus: Preliminary results. In
Proceedings of the First International
Conference on Intelligent Text Processing
and Computational Linguistics
(CICLing-2000), pages 181?204,
Mexico City.
Cruse, David A. 1986. Lexical Semantics.
Cambridge University Press, Cambridge.
Edmonds, Phil and Scott Cotton, editors.
2001. Proceedings of the Senseval 2 Workshop.
Association for Computational Linguistics,
Toulouse.
Fillmore, Charles. 1968. The case for case.
In Emmon Bach and Rovert T. Harms,
editors, Universals in Linguistic Theory.
Holt, Rinehart and Winston, New York,
pages 1?88.
Fillmore, Charles J., Charles Wooters, and
Collin F. Baker. 2001. Building a large
lexical databank which provides
deep semantics. In Proceedings of the
Pacific Asian Conference on Language,
Information and Computation, pages 3?25,
Hong Kong.
Frawley, William. 1992. Linguistic Semantics.
Lawrence Erlbaum Associates,
Hillsdale, NJ.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Grozea, Cristian. 2004. Finding optimal
parameter settings for high performance
word sense disambiguation. In
Senseval-3: Third International Workshop
on the Evaluation of Systems for the
Semantic Analysis of Text, pages 125?128,
Barcelona.
Kilgarriff, Adam. 1998. Senseval: An exercise
in evaluating word sense disambiguation
programs. In Proceedings of the First
International Conference on Language
Resources and Evaluation (LREC ?98),
pages 581?588, Granada.
Koomen, Peter, Vasin Punyakanok, Dan
Roth, and Wen-tau Yih. 2005. Generalized
inference with multiple semantic role
labeling systems. In Proceedings of the
Ninth Conference on Computational Natural
Language Learning (CoNLL), pages 181?184,
Ann Arbor, MI.
Lehmann, Fritz. 1996. Big posets of
participatings and thematic roles. In
Peter W. Eklund, Gerard Ellis, and
Graham Mann, editors, Conceptual
Structures: Knowledge Representation as
Interlingua, Springer-Verlag, Berlin,
pages 50?74.
Lenat, Douglas B. 1995. Cyc: A large-scale
investment in knowledge infrastructure.
Communications of the ACM, 38(11):33?38.
Litkowski, Kenneth C. 2002. Digraph
analysis of dictionary preposition
definitions. In Proceedings of the
SIGLEX/SENSEVAL Workshop on Word
Sense Disambiguation: Recent Successes
and Future Directions, pages 9?16,
Philadelphia, PA.
Litkowski, Kenneth C. 2004. Senseval-3 task:
Automatic labeling of semantic roles.
In Proceedings of Senseval-3: The Third
International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text,
pages 9?12, Barcelona.
Litkowski, Kenneth C. and Orin Hargraves.
2006. Coverage and inheritance in The
Preposition Project. In Third ACL-SIGSEM
Workshop on Prepositions, pages 37?44,
Trento.
Litkowski, Kenneth C. and Orin Hargraves.
2007. SemEval-2007 task 06: Word-sense
disambiguation of prepositions. In
Proceedings of the Fourth International
Workshop on Semantic Evaluations
(SemEval-2007), pages 24?29, Prague.
Liu, Rey-Long and Von-Wun Soo. 1993. An
empirical study on thematic knowledge
acquisition based on syntactic clues and
heuristics. In Proceedings of the 31st Annual
Meeting of the Association for Computational
182
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Linguistics (ACL-93), pages 243?250,
Columbus, OH.
Manning, Christopher D. and Hinrich
Schu?tze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press,
Cambridge, MA.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre,
Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The
Penn Treebank: Annotating predicate
argument structure. In Proceedings
of the ARPA Human Language
Technology Workshop, pages 110?115,
Plainsboro, NJ.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Mihalcea, Rada. 2002. Instance based
learning with automatic feature
selection applied to word sense
disambiguation. In Proceedings of
the 19th International Conference on
Computational Linguistics (COLING-2002),
pages 1?7, Taiwan.
Mihalcea, Rada, Timothy Chklovski, and
Adam Kilgarriff. 2004. The Senseval-3
English lexical sample task. In
Senseval-3: Third International Workshop
on the Evaluation of Systems for the
Semantic Analysis of Text, pages 25?28,
Barcelona.
Miller, George A., Richard Beckwith,
Christiane Fellbaum, Derek Gross, and
Katherine Miller. 1990. WordNet: An
on-line lexical database. International
Journal of Lexicography, 3(4): Special
Issue on WordNet.
Miller, Katherine. 1998. Modifiers in
WordNet. In Christiane Fellbaum,
editor,WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, MA,
pages 47?67.
Mohit, Behrang and Srini Narayanan.
2003. Semantic extraction with
wide-coverage lexical resources. In
Companion Volume of the Proceedings
of HLT-NAACL 2003 - Short Papers,
pages 64?66, Edmonton.
O?Hara, Thomas P. 2005. Empirical acquisition
of conceptual distinctions via dictionary
definitions. Ph.D. thesis, Department
of Computer Science, New Mexico
State University.
O?Hara, Tom, Rebecca Bruce, Jeff Donner,
and Janyce Wiebe. 2004. Class-based
collocations for word-sense
disambiguation. In Proceedings of
Senseval-3: The Third International
Workshop on the Evaluation of Systems
for the Semantic Analysis of Text,
pages 199?202, Barcelona.
O?Hara, Tom and Janyce Wiebe. 2003.
Classifying functional relations in
Factotum via WordNet hypernym
associations. In Proceedings of the
Fourth International Conference on
Intelligent Text Processing and
Computational Linguistics (CICLing-2003),
pages 347?359, Mexico City.
OpenCyc. 2002. OpenCyc release 0.6b.
Available at www.opencyc.org.
Palmer, Martha, Dan Gildea, and Paul
Kingsbury. 2005. The Proposition Bank:
A corpus annotated with semantic roles.
Computational Linguistics, 31(1):71?106.
Palmer, Martha Stone. 1990. Semantic
Processing for Finite Domains. Cambridge
University Press, Cambridge.
Pradhan, Sameer, Kadri Hacioglu, Valerie
Krugler, Wayne Ward, James H. Martin,
and Daniel Jurafsky. 2005. Support
vector learning for semantic argument
classification.Machine Learning,
60(1?3):11?39.
Quinlan, J. Ross. 1993. C4.5: Programs for
Machine Learning. Morgan Kaufmann,
San Mateo, CA.
Scott, Sam and Stan Matwin. 1998. Text
classification using WordNet hypernyms.
In Proceedings of the COLING/ACL
Workshop on Usage of WordNet in Natural
Language Processing Systems, pages 38?44,
Montreal.
Somers, Harold L. 1987. Valency and Case in
Computational Linguistics. Edinburgh
University Press, Scotland.
Sowa, John F. 1984. Conceptual Structures in
Mind and Machines. Addison-Wesley,
Reading, MA.
Sowa, John F. 1999. Knowledge Representation:
Logical, Philosophical, and Computational
Foundations. Brooks Cole Publishing,
Pacific Grove, CA.
Srihari, Rohini, Cheng Niu, and Wei Li.
2001. A hybrid approach for named
entity and sub-type tagging. In
Proceedings of the 6th Applied Natural
Language Processing Conference,
pages 247?254, Seattle.
Wiebe, Janyce, Kenneth McKeever, and
Rebecca F. Bruce. 1998. Mapping
collocational properties into machine
learning features. In Proceedings of the
6th Workshop on Very Large Corpora
(WVLC-98), pages 225?233, Montreal.
183
Computational Linguistics Volume 35, Number 2
Wilks, Yorick, Brian M. Slator, and Louise
Guthrie. 1996. Electric Words. MIT Press,
Cambridge, MA.
Witten, Ian H. and Eibe Frank. 1999.
Data Mining: Practical Machine
Learning Tools and Techniques with Java
Implementations. Morgan Kaufmann,
San Francisco, CA.
Ye, Patrick and Timothy Baldwin. 2006.
Semantic role labeling of prepositional
phrases. ACM Transactions on Asian
Language Information Processing (TALIP),
5(3):228?244.
Ye, Patrick and Timothy Baldwin.
2007. MELB-YB: Preposition sense
disambiguation using rich semantic
features. In Proceedings of the Fourth
International Workshop on Semantic
Evaluations (SemEval-2007),
pages 241?244, Prague.
184
This article has been cited by:
1. Timothy Baldwin, Valia Kordoni, Aline Villavicencio. 2009. Prepositions in Applications:
A Survey and Introduction to the Special IssuePrepositions in Applications: A Survey and
Introduction to the Special Issue. Computational Linguistics 35:2, 119-149. [Citation] [PDF]
[PDF Plus]
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 68?75,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantically Rich Human-Aided Machine Annotation 
 
Marjorie McShane, Sergei Nirenburg, Stephen Beale, Thomas O?Hara 
Department of Computer Science and Electrical Engineering 
University of Maryland Baltimore County 
1000 Hilltop Circle, Baltimore, Maryland, 21250   USA 
{marge, sergei, sbeale, tomohara}@umbc.edu 
 
Abstract 
This paper describes a semantically rich, 
human-aided machine annotation system 
created within the Ontological Semantics 
(OntoSem) environment using the 
DEKADE toolset. In contrast to main-
stream annotation efforts, this method of 
annotation provides more information at a 
lower cost and, for the most part, shifts 
the maintenance of consistency to the sys-
tem itself. In addition, each tagging effort 
not only produces knowledge resources 
for that corpus, but also leads to im-
provements in the knowledge environ-
ment that will better support subsequent 
tagging efforts.  
1 Introduction 
Corpus tagging is a prerequisite for many machine 
learning methods in NLP but has the drawbacks of 
high cost, inter-annotator inconsistency and the 
insufficient treatment of meaning. A tagging ap-
proach that strives to ameliorate all of these draw-
backs is semantically rich, human-aided machine 
annotation (HAMA), implemented in the OntoSem 
(Ontological Semantics) environment using a tool-
set called DEKADE: the Development, Evaluation, 
Knowledge Acquisition and Demonstration Envi-
ronment of OntoSem. 
In brief, the OntoSem text analyzer takes as in-
put open text and outputs a text-meaning represen-
tation (TMR) that represents its meaning using an 
ontologically grounded, language-independent 
metalanguage (see Nirenburg and Raskin 2004). 
Since the processing leading up to the production 
of TMR includes, in addition to semantic analysis 
proper, preprocessing (roughly, segmentation, 
treatment of named entities and morphology) and 
syntactic analysis, the overall annotation of text in 
this approach includes tags relating to all of the 
above levels. Since the typical input for analysis in 
our practice is genuine sentences, which are on 
average 25 words long and contain all manner of 
complex phenomena, it is not uncommon for the 
automatically generated TMRs to contain errors. 
These errors?which can occur at the level of pre-
processing, syntactic analysis or semantic analy-
sis?can be corrected manually using the 
DEKADE environment, yielding ?gold standard? 
output. Making a human the final arbiter in the 
process means that such long-term complexities as 
treatment of metaphor, metonymy, PP-attachment, 
difficult cases of reference resolution and others 
can be resolved locally while we work on funda-
mental, implementable automatic solutions.  
In this paper we describe the Onto-
Sem/DEKADE environment for the creation of 
gold standard TMRs, which supports the first ever 
annotation effort that:  
 
? produces structures that can be used as input 
for both text generators and general reason-
ing systems: semantically rich representa-
tions of the meaning of text written in a 
language-independent metalanguage; these 
representations cover entities, propositions, 
relations, attributes, speaker attitudes, mo-
dalities, polarity, discourse relations, time, 
reference relations, and more; 
? produces semantic tagging of text largely 
automatically, thus making more realistic 
and affordable the tagging of large amounts 
of text in finite time; 
? almost fully circumvents the pitfalls of man-
ual tagging, including human tagger errors 
and inconsistencies;  
? produces richer semantic annotations than 
manual tagging realistically could, since ma-
nipulating large and complex static knowl-
68
edge sources would be impossible for hu-
mans if starting from scratch (i.e., our meth-
odology effectively turns an essay question 
into a multiple choice one, with most of the 
correct answers already provided); 
? incorporates humans as final arbiters for out-
put of three stages of text analysis (preproc-
essing, syntactic analysis and semantic 
analysis), thus maximally leveraging the 
automated capacity of the system but not re-
quiring of it blanket coverage at this point in 
its development; 
? promises to reduce, over time, the depend-
ence on human input because an important 
side effect of the operation of the human-
assisted machine annotation approach is en-
hancement of the static knowledge resources 
? the lexicon and the ontology ? underlying 
the OntoSem analyzer, so that the quality of 
automatic text analysis will grow as the 
HAMA system operates, leading to an ever 
improving quality of raw, unedited TMRs; 
? (as a corollary to the previous point) be-
comes more cost-efficient over time; and 
? can be cost-effectively extended to other 
languages (including less commonly taught 
languages), with much less work than was 
required for the first language since many of 
the necessary resources are language-
independent. 
 
Our approach to text analysis is a hybrid of 
knowledge-based and corpus-based, stochastic 
methods.   
In the remainder of the paper we will briefly de-
scribe the lay of the land in text annotation (Sec-
tion 2), the OntoSem environment (Section 3), the 
DEKADE environment for creating gold-standard 
TMRs from automatically generated ones (Section 
4), the portability of OntoSem to other languages 
(Section 5), and the broader implications of this 
R&D effort (Section 6).   
2 The Lay of the Land in Annotation 
In addition to the well-known bottlenecks of cost 
and inconsistency, it is widely assumed that low-
level (only syntactic or ?light semantic?) tagging is 
either sufficient or inevitable due to the complexity 
of semantic tagging. Past and ongoing tagging ef-
forts share this point of departure. 
Numerous projects have striven to achieve text 
annotation via a simpler task, like translation, 
sometimes assuming that one language has already 
been tagged (e.g., Pianta and Bentivogli 2003, and 
references therein). But results of such efforts are 
either of low quality, light semantic depth, or re-
main to be reported. Of significant interest is the 
porting of annotations across languages: for exam-
ple, Yarowsky et al 2001 present a method for 
automatic tagging of English and the projection of 
the tags to other languages; however, these tags do 
not include semantics. 
Post-editing of automatic annotation has been 
pursued in various projects (e.g., Brants 2000, and 
Marcus et al 1993). The latter group did an ex-
periment early on in which they found that ?man-
ual tagging took about twice as long as correcting 
[automated tagging], with about twice the inter-
annotator disagreement rate and an error rate that 
was about 50% higher? (Marcus et al 1993). This 
conclusion supports the pursuit of automated tag-
ging methods. The difference between our work 
and the work in the above projects, however, is 
that syntax for us is only a step in the progression 
toward semantics. 
 Interesting time- and cost-related observations 
are provided in Brants 2000 with respect to the 
manual correction of automated POS and syntactic 
tagging of a German corpus (semantics is not ad-
dressed). Although these tasks took approximately 
50 seconds per sentence, with sentences averaging 
17.5 tokens, the actual cost in time and money puts 
each sentence at 10 minutes, by the time two tag-
gers carry out the task, their results are compared, 
difficult issues are resolved, and taggers are trained 
in the first place. Notably, however, this effort 
used students as taggers, not professionals. We, by 
contrast, use professionals to check and correct 
TMRs and thus reduce to practically zero the train-
ing time, the need for multiple annotators (pro-
vided the size of a typical annotation task is 
commensurate with those in current projects), and 
costly correction of errors.  
Among past projects that have addressed se-
mantic annotation are the following: 
 1. Gildea and Jurafsky (2002) created a stochas-
tic system that labels case roles of predicates with 
either abstract (e.g., AGENT, THEME) or domain-
specific (e.g., MESSAGE, TOPIC) roles. The system 
69
trained on 50,000 words of hand-annotated text 
(produced by the FrameNet project). When tasked 
to segment constituents and identify their semantic 
roles (with fillers being undisambiguated textual 
strings) the system scored in the 60?s in precision 
and recall. Limitations of the system include its 
reliance on hand-annotated data, and its reliance on 
prior knowledge of the predicate frame type (i.e., it 
lacks the capacity to disambiguate productively). 
Semantics in this project is limited to case-roles.  
 2. The goal of the ?Interlingual Annotation of 
Multilingual Text Corpora? project 
(http://aitc.aitcnet.org/nsf/iamtc/) is to create a syn-
tactic and semantic annotation representation 
methodology and test it out on six languages (Eng-
lish, Spanish, French, Arabic, Japanese, Korean, 
and Hindi). The semantic representation, however, 
is restricted to those aspects of syntax and seman-
tics that developers believe can be consistently 
handled well by hand annotators for many lan-
guages. The current stage of development includes 
only syntax and light semantics ? essentially, the-
matic roles. 
 3. In the ACE project 
(http://www.ldc.upenn.edu/Projects/ACE/intro.htm
l), annotators carry out manual semantic annotation 
of texts in English, Chinese and Arabic to create 
training and test data for research task evaluations. 
The downside of this effort is that the inventory of 
semantic entities, relations and events is very small 
and therefore the resulting semantic representa-
tions are coarse-grained: e.g., there are only five 
event types. The project description promises more 
fine-grained descriptors and relations among 
events in the future.  
 4. Another response to the insufficiency of syn-
tax-only tagging is offered by the developers of 
PropBank, the Penn Treebank semantic extension. 
Kingsbury et al 2002 report: ?It was agreed that 
the highest priority, and the most feasible type of 
semantic annotation, is coreference and predicate 
argument structure for verbs, participial modifiers 
and nominalizations?, and this is what is included 
in PropBank.  
 To summarize, previous tagging efforts that 
have addressed semantics at all have covered only 
a relatively small subset of semantic phenomena. 
OntoSem, by contrast, produces a far richer anno-
tation, carried out largely automatically, within an 
environment that will improve over time and with 
use.  
3 A Snapshot of OntoSem 
OntoSem is a text-processing environment that 
takes as input unrestricted raw text and carries out 
preprocessing, morphological analysis, syntactic 
analysis, and semantic analysis, with the results of 
semantic analysis represented as formal text-
meaning representations (TMRs) that can then be 
used as the basis for many applications (for details, 
see, e.g., Nirenburg and Raskin 2004, Beale et al 
2003). Text analysis relies on:  
 
? The OntoSem language-independent ontology, 
which is written using a metalanguage of de-
scription and currently contains around 6,000 
concepts, each of which is described by an aver-
age of 16 properties.  
? An OntoSem lexicon for each language proc-
essed, which contains syntactic and semantic 
zones (linked using variables) as well as calls for 
procedural semantic routines when necessary. 
The semantic zone most frequently refers to on-
tological concepts, either directly or with prop-
erty-based modifications, but can also describe 
word meaning extra-ontologically, for example, 
in terms of modality, aspect, time, etc. The cur-
rent English lexicon contains approximately 
25,000 senses, including most closed-class items 
and many of the most frequent and polysemous  
verbs, as targeted by corpus analysis. (An exten-
sive description of the lexicon, formatted as a tu-
torial, can be found at http://ilit.umbc.edu.) 
? An onomasticon, or lexicon of proper names, 
which contains approximately 350,000 entries.  
? A fact repository, which contains real-world 
facts represented as numbered ?remembered in-
stances? of ontological concepts (e.g., SPEECH-
ACT-3366 is the 3366th instantiation of the con-
cept SPEECH-ACT in the world model constructed 
during the processing of some given text(s)). 
? The OntoSem syntactic-semantic analyzer, 
which covers preprocessing, syntactic analysis, 
semantic analysis, and the creation of TMRs. In-
stead of using a large, monolithic grammar of a 
language, which leads to ambiguity and ineffi-
ciency, we use a special lexicalized grammar 
created on the fly for each input sentence (Beale, 
et. al. 2003).  Syntactic rules are generated from 
the lexicon entries of each of the words in the 
sentence, and are supplemented by a small in-
ventory of generalized rules. We augment this 
70
basic grammar with transformations triggered by 
words or features present in the input sentence.  
? The TMR language, which is the metalanguage 
for representing text meaning.  
 
 Creating gold standard TMRs involves running 
text through the OntoSem processors and check-
ing/correcting the output after three stages of 
analysis: preprocessing, syntactic analysis, and 
semantic analysis. These outputs can 
be viewed and edited as text or as vis-
ual representations through the 
DEKADE interface. Although the gold 
standard TMR itself does not reflect 
the results of preprocessing or syntactic 
analysis, the gold standard results of 
those stages of processing are stored in 
the system and can be converted into a 
more traditional annotation format. 
4 TMRs in DEKADE 
TMRs represent propositions con-
nected by discourse relations (since 
space permits only the briefest of descriptions, in-
terested readers are directed to Nirenburg and 
Raskin 2004, Chapter 6 for details). Propositions 
are headed by instances of ontological concepts, 
parameterized for modality, aspect, proposition 
time, overall TMR time, and style. Each proposi-
tion is related to other instantiated concepts using 
ontologically defined relations (which include case 
roles and many others) and attributes. Coreference 
links form an additional layer of linking between 
instantiated concepts. OntoSem microtheories de-
voted to modality, aspect, time, style, reference, 
etc., undergo iterative extensions and improve-
ments in response to system needs as diagnosed 
during the processing of actual texts.    
 We use the following sentence to walk through 
the processes of automatically generating TMRs 
and viewing/editing those TMRs to create a gold-
standard annotated corpus.   
 
The Iraqi government has agreed to let 
U.S. Representative Tony Hall visit the 
country to assess the humanitarian crisis.  
 
Preprocessor. The preprocessor identifies the 
root word, part of speech and morphological fea-
tures of each word; recognizes sentence bounda-
ries, named entities, dates, times and numbers; and 
for named entities, determines the ontological type 
(i.e. HUMAN, PLACE, ORGANIZATION, etc.) of the 
entity as well as its subparts (e.g., the first, last, 
and middle names of a person). For the semi-
automatic creation of gold standard TMRs, much 
ambiguity can be removed at small cost by allow-
ing people to correct spurious part-of-speech tags, 
number and date boundaries, etc., through the 
DEKADE environment at the preprocessor stage 
(see Figure 1). Clicking on w+ permits a new POS 
tag/analysis, and clicking on w-, the more common 
action, removes spurious analyses. Preprocessor 
correction is a conceptually simple and logistically 
fast task that can be carried out by less trained, and 
therefore less expensive, annotators.  
Figure 1. Preprocesor Output Editor. 
Syntax. Syntax output can be viewed and ed-
ited in text or graphic form. The graphic 
viewer/editor presents the sentence using the tradi-
tional metaphor of color-coded labeled arcs. 
Mouse clicks show the components of arcs, permit 
arcs to be deleted along with the orphans they 
would leave, allow for the edges of arcs to be 
moved, etc. (no graphic of the syntax or semantics 
browsers/editors are provided due to space con-
straints).   
One common error in syntax output is spurious 
parses due to contextually incorrect POS or feature 
analysis. As shown above, this can be fixed from 
the outset by correcting the preprocessor. How-
ever, since the preprocessor will always contain 
spurious analyses that can usually be removed 
automatically by the syntactic analyzer, it is not 
necessarily most time efficient to always start with 
preprocessor editing. A more difficult, long-term 
research issue is genuine ambiguity caused, for 
example, by PP-attachments. While such issues are 
71
not likely to be solved computationally in the short 
term, they can be easily resolved when humans are 
used as the final arbiters in the creation of gold 
standard TMRs.  
 When the correct parse is not included in the 
syntactic output, either the necessary lexical 
knowledge is lacking (i.e. there is an unknown 
word or word sense), or an unknown grammatical 
construction has been used. While the syntax-
editing interface permits spot-correction of the 
problem by the addition of the necessary arc(s), a 
more fundamental knowledge-building approach is 
generally preferred ? except when the input is non-
standard, in which case systemic modifications are 
avoided.   
 Semantics. Within the OntoSem environment, 
there are two stages of text-meaning representa-
tions (TMRs): basic and extended. The basic TMR 
shows the basic ontological mappings and depend-
ency structure, whereas the extended TMR shows 
the results of procedural semantics, including ref-
erence resolution, reasoning about time relations, 
etc. The basic and extended stages of TMR crea-
tion can be viewed and edited separately within 
DEKADE.   
TMRs can be viewed and edited in text format 
or graphically. In the latter, concepts are shown as 
nodes and properties are shown as lines connecting 
them. A pretty-printed view of the textual extended 
TMR for our sample sentence, repeated for con-
venience, is as follows (concept names are in small 
caps; instance numbers are appended to them).  
 
The Iraqi government has agreed to let U.S.  
Representative Tony Hall visit the country to  
assess the humanitarian crisis. 
 
AGREE-268 
 textpointer agree 
THEME   MODALITY-200 
 AGENT   GOVERNMENTAL-ORGANIZATION-41 
 TIME   (< FIND-ANCHOR-TIME) 
GOVERNMENTAL-ORGANIZATION-41 
 textpointer government 
 RELATION  NATION-56      
 AGENT-OF  AGREE-268 
NATION-56 
 textpointer Iraq 
 RELATION  GOVERNMENTAL-ORGANIZATION-41 
MODALITY-200 
 textpointer let 
      TYPE    permissive 
SCOPE   TRAVEL-EVENT-272 
      VALUE       1 
TRAVEL-EVENT-272 
 textpointer visit 
 AGENT   SENATOR-4471  
 DESTINATION NATION-57 
 PURPOSE  EVALUATE-69 
 SCOPE-OF  MODALITY-200 
SENATOR-447 
 textpointer Representative Tony Hall2 
 REPRESENTATIVE-OF NATION-40 
NATION-40 
 textpointer U.S. 
 REPRESENTED-BY SENATOR-447 
NATION-57 
 textpointer country    
 COREFER  NATION-56 
EVALUATE-69 
 AGENT   SENATOR-447 
 THEME   DISASTER-EVENT-2 
DISASTER-EVENT-2 
 BENEFICIARY SET-23 
 THEME-OF  EVALUATE-69 
SET-23 
 MEMBER-TYPE HUMAN-1342 
 BENEFICIARY-OF DISASTER-EVENT-2 
 
Within the graphical browser, clicking on concept 
names or properties permits them to be deleted, 
edited, or permits new ones to be added. It also 
shows the expansion of any concept in text format. 
 Evaluating and editing the semantic output is 
the most challenging aspect of creating gold stan-
dard TMRs, since creating formal semantic repre-
sentations is arguably one of the most difficult 
tasks in all of NLP. If a knowledge engineer de-
termines that some aspect of the semantic repre-
sentation is incorrect, the problems can be 
corrected locally or by editing the knowledge re-
sources and rerunning the analyzer. Local correc-
tions are used, for example, in cases of metaphor 
and metonymy, which we do not record in our 
knowledge resources (we are working on a mi-
crotheory of tropes but it is not yet implemented).  
In all other cases, resource supplementation is pre-
ferred; it can be carried out either immediately or 
the problem can be fixed locally, in which case a 
request will be sent to a knowledge acquirer to 
carry out the necessary resource enhancements.  
                                                          
1 The concept SENATOR is defined as a member of a legislative 
assembly. 
2 Collocations of SOCIAL-ROLE + personal name are handled by 
the preprocessor. 
72
 Striking the balance between short-term goals 
(a gold standard TMR for the given text) and long-
term goals (better analysis of any text in the future) 
is always a challenge. For example, if a text con-
tained the word grass in the sense of ?marijuana?, 
and if the lexicon lacked the word ?grass? alto-
gether, we would want to acquire the meaning 
?green lawn cover? as well; however, doing this 
without constraint could mean getting bogged 
down by knowledge acquisition (as with the doz-
ens of idiomatic uses of ?have?) at the expense of 
actually producing gold-standard TMRs. There are 
also cases in which a local solution to semantic 
representation is very easy whereas a fundamental, 
machine-reproducible solution is very difficult. 
Consider the case of relative expressions, like re-
spective and respectively, as used in Smith and 
Matthews pleaded innocent and guilty, respec-
tively. Manually editing a TMR such that the ap-
propriate properties are linked to their heads is 
quite simple, whereas writing a program for this 
non-trivial case of reference resolution is not. 
Thus, in some cases we push through gold standard 
TMR production while keeping track of ? and de-
veloping as time permits ? the more difficult as-
pects of text processing that will enhance TMR 
output in the future.    
The gold standard TMR for the sentence dis-
cussed at length here was produced with only a 
few manual corrections: changing two part of 
speech tags and selecting the correct sense for one 
word. Work took less than the 10 minutes reported 
by Brants 2000 for their non-semantic tagging. 
5 Porting to Other Languages 
Recently the need for tagged corpora for less 
commonly taught languages has received much 
attention. While our group is not currently pursu-
ing such languages, it has in the past: TMRs have 
been automatically generated for languages such as 
Chinese, Georgian, Arabic and Persian. We take a 
short tangent to explain how OntoSem/DEKADE 
can be extended, at relatively low cost, to the anno-
tation of other languages ? showing yet another 
way in which this approach to annotation reaches 
beyond the results for any given text or corpus.  
Whereas it is typical to assume that lexicons are 
language-specific whereas ontologies are lan-
guage-independent, most aspects of the semantic 
structures (sem-strucs) of OntoSem lexicon entries 
are actually language-independent, apart from the 
linking of specific variables to their counterparts in 
the syntactic structure. Stated differently, if we 
consider sem-strucs ? no matter what lexicon they 
originate from ? to be building blocks of the repre-
sentation of word meaning (as opposed to concept 
meaning, as is done in the ontology), then we un-
derstand why building a large OntoSem lexicon for 
English holds excellent promise for future porting 
to other languages: most of the work is already 
done. This conception of cross-linguistic lexicon 
development derives in large part from the Princi-
ple of Practical Effability (Nirenburg and Raskin 
2004), which states that what can be expressed in 
one language can somehow be expressed in all 
other languages, be it by a word, a phrase, etc. (Of 
course, it is not necessary that every nuanced 
meaning be represented in the lexicon of every 
language and, as such, there will be some differ-
ences in the lexical stock of each language: e.g., 
whereas German has a word for white horse which 
will be listed in its lexicon, English will not have 
such a lexical entry, the collocation white horse 
being treated compositionally.) We do not intend 
to trivialize the fact that creating a new lexicon is a 
lot of work. It is, however, compelling to consider 
that a new lexicon of the same quality of our On-
toSem English one could be created with little 
more work than would be required to build a typi-
cal translation dictionary. In fact, we recently car-
ried out an experiment on porting the English 
lexicon to Polish and found that a) much of it could 
be done semi-automatically and b) the manual 
work for a second language is considerably less 
than for the first language (for further discussion, 
see McShane et al 2004).  
To sum up, the OntoSem ontology and the 
DEKADE environment are equally suited to any 
language, and the OntoSem English lexicon and 
analyzer can be configured to new languages with 
much less work required than for their initial de-
velopment. In short, semantic-rich tagging through 
TMR creation could be a realistic option for lan-
guages other than English.  
6 Discussion 
Lack of interannotator agreement presents a sig-
nificant problem in annotation efforts (see, e.g., 
Marcus et al 1993). With the OntoSem semi-
automated approach, there is far less possibility of 
73
interannotator disagreement since people only cor-
rect the output of the analyzer, which is responsi-
ble for consistent and correct deployment of the 
large and complex static resources:  if the knowl-
edge bases are held constant, the analyzer will pro-
duce the same output every time, ensuring 
reproducibility of the annotation.  
Evaluation of annotation has largely centered 
upon the demonstration of interannotator agree-
ment, which is at best a partial standard for evalua-
tion. On the one hand, agreement among 
annotators does not imply the correctness of the 
annotations: all annotators could be mistaken, par-
ticularly as students are most typically recruited for 
the job. On the other hand, there are cases of genu-
ine ambiguity, in which more than one annotation 
is equally correct. Such ambiguity is particularly 
common with certain classes of referring expres-
sions, like this and that, which can refer to chunks 
of text ranging from a noun phrase to many para-
graphs. Genuine ambiguity in the context of corpus 
tagging has been investigated by Poesio and Art-
stein (ms.), among others, who conclude, reasona-
bly, that a system of tags must permit multiple 
possible correct coreference relations and that it is 
useful to evaluate coreference based on corefer-
ence chains rather than individual entities. 
The abovementioned evidence suggests the need 
for ever more complex evaluation metrics which 
are costly to develop and deploy. In fact, evalua-
tion of a complex tagging effort will be almost as 
complex as the core work itself. In our case, TMRs 
need to be evaluated not only for their correctness 
with respect to a given state of knowledge re-
sources but also in the abstract. Speed of gold 
standard TMR creation must also be evaluated, as 
well as the number of mistakes at each stage of 
analysis, and the effect that the correction of output 
at one stage has on the next stage. No methods or 
standards for such evaluation are readily available 
since no work of this type has ever been carried 
out.  
In the face of the usual pressures of time and 
manpower, we have made the programmatic deci-
sion not to focus on all types of evaluation but, 
rather, to concentrate our evaluation metrics on the 
correctness of the automated output of the system, 
the extent to which manual correction is needed, 
and the depth and robustness of  our knowledge 
resources (see Nirenburg et al 2004 for our first 
evaluation effort). We do not deny the ultimate 
desirability of additional aspects of evaluation in 
the future. 
The main source of variation among knowledge 
engineers within our approach lies not in review-
ing/editing annotations as such, but in building the 
knowledge sources that give rise to them. To take 
an actual example we encountered: one member of 
our group described the phrase weapon of mass 
destruction in the lexicon as BIOLOGICAL-WEAPON 
or CHEMICAL-WEAPON, while another described it 
as a WEAPON with the potential to kill a very large 
number of people/animals. While both of these are 
correct, they focus on different salient aspects of 
the collocation. Another example of potential dif-
ferences at the knowledge level has to do with 
grain size: whereas one knowledge engineer re-
viewing a TMR might consider the current lexical 
mapping of neurosurgeon to SURGEON perfectly 
acceptable,  another might consider that this grain 
size is too rough and that, instead, we need a new 
concept NEUROSURGEON, whose special properties 
are ontologically defined. Such cases are to be ex-
pected especially as we work on new specialized 
domains which put greater demands on the depth 
of knowledge encoded about relevant concepts.  
There has been some concern that manual edit-
ing of automated annotation can introduce bias. 
Unfortunately, completely circumventing bias in 
semantic annotation is and will remain impossible 
since the process involves semantic interpretation, 
which often differs among individuals from the 
outset. As such, even agreements among annota-
tors can be questioned by a third (fourth, etc.) 
party.  
At the present stage of development, the TMR 
together with the static (ontology, lexicons) and 
dynamic (analyzer) knowledge sources that are 
used in generating and manipulating it, already 
provide substantial coverage for a broad variety of 
semantic phenomena and represent in a compact 
way practically attainable solutions for most issues 
that have concerned the computational linguistics 
and NLP community for over fifty years. Our 
TMRs have been used as the substrate for ques-
tion-answering, MT, knowledge extraction, and 
were also used as the basis for reasoning in the 
question-answering system AQUA, where they 
supplied knowledge to enable the operation of the  
JTP (Fikes et al, 2003) reasoning module. 
We are creating a database of TMRs paired 
with their corresponding sentences that we believe 
74
will be a boon to machine learning research. Re-
peatedly within the ML community, the creation of 
a high quality dataset (or datasets) for a particular 
domain has sparked development of applications, 
such as  learning semantic parsers, learning lexical 
items, learning about the structure of the underly-
ing domain of discourse, and so on. Moreover, as 
the quality of the raw TMRs increases due to gen-
eral improvements to the static resources (in part, 
as side effects of the operation of the HAMA proc-
ess) and processors (a long-term goal), the net 
benefit of this approach will only increase, as the 
production rate of gold-standard TMRs will in-
crease thus lowering the costs.  
TMRs are a useful medium for semantic repre-
sentation in part because they can capture any con-
tent in any language, and even content not 
expressed in natural language. They can, for ex-
ample, be used for recording the interim and final 
results of reasoning by intelligent agents. We fully 
expect that, as the actual coverage in the ontology 
and the lexicons and the quality of semantic analy-
sis grows, the TMR format will be extended to ac-
commodate these improvements. Such an 
extension, we believe, will largely involve move-
ment toward a finer grain size of semantic descrip-
tion, which the existing formalism should readily 
allow. The metalanguage of TMRs is quite trans-
parent, so that the task of converting them into a 
different representation language (e.g., OWL) 
should not be daunting.   
References  
Stephen Beale, Sergei Nirenburg and Marjorie 
McShane. 2003. Just-in-time grammar. Proceedings 
of the 2003 International Multiconference in Com-
puter Science and Computer Engineering. Las Ve-
gas, Nevada. 
Thorsten Brants. 2000. Inter-annotator agreement for a 
German newspaper corpus. LREC-2000. Athens, 
Greece. 
Richard Fikes, Jessica Jenkins and Gleb Frank. 2003. 
JTP: A system architecture and component library for 
hybrid reasoning. Proceedings of the Seventh World 
Multiconference on Systemics, Cybernetics, and In-
formatics. Orlando, Florida, USA. 
Daniel Gildea and Daniel Jurafsky. 2002. Automatic 
labeling of semantic roles. Computational Linguistics 
28:3, 245-288. 
Paul Kingsbury, Martha Palmer and Mitch Marcus. 
2002. Adding semantic annotation to the Penn Tree-
Bank. (http://www.cis.upenn.edu/~ace/ 
    HLT2002-propbank.pdf.) 
Marcus, Mitchell P., Beatrice Santorini and Mary Ann 
Marcinkiewicz. 1993. Building a large annotated 
corpus of English: the Penn Treebank. Computa-
tional Linguistics 19. 
Marjorie McShane, Margalit Zabludowski, Sergei Ni-
renburg and Stephen Beale. 2004. OntoSem and 
SIMPLE: Two multi-lingual world views. Proceed-
ings of ACL-2004 Workshop on Text Meaning and 
Interpretation. Barcelona, Spain. 
Sergei Nirenburg, Stephen Beale and Marjorie 
McShane. 2004. Evaluating the performance of the 
OntoSem semantic analyzer. Proceedings of the ACL 
Workshop on Text Meaning Representation. Barce-
lona, Spain. 
Sergei Nirenburg and Victor Raskin. 2004. Ontological 
Semantics. The MIT Press.  
Emanuele Pianta and Luisa Bentivogli. 2003. Transla-
tion as annotation. Proceedings of the AI*IA 2003 
Workshop "Topics and Perspectives of Natural Lan-
guage Processing in Italy." Pisa, Italy. 
Massimo Poesio and Ron Artstein. 2005. The reliability 
of anaphoric annotation, reconsidered: Taking ambi-
guity into account. Proceedings of the ACL 2005 
Workshop ?Frontiers in Corpus Annotation II, Pie in 
the Sky?. 
David Yarowsky, Grace Ngai and Richard Wicen-
towski. 2001. Inducing multilingual text analysis 
tools via robust projection across aligned corpora. 
Proceedings of HLT 2001, First International Con-
ference on Human Language Technology Research, 
San Diego, California, USA. 
75
