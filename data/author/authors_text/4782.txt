Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1289?1297,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
On the Use of Virtual Evidence in Conditional Random Fields
Xiao Li
Microsoft Research
One Microsoft Way
Redmond, WA 98052 USA
xiaol@microsoft.com
Abstract
Virtual evidence (VE), first introduced
by (Pearl, 1988), provides a convenient
way of incorporating prior knowledge into
Bayesian networks. This work general-
izes the use of VE to undirected graph-
ical models and, in particular, to condi-
tional random fields (CRFs). We show
that VE can be naturally encoded into a
CRF model as potential functions. More
importantly, we propose a novel semi-
supervised machine learning objective for
estimating a CRF model integrated with
VE. The objective can be optimized us-
ing the Expectation-Maximization algo-
rithm while maintaining the discriminative
nature of CRFs. When evaluated on the
CLASSIFIEDS data, our approach signif-
icantly outperforms the best known solu-
tions reported on this task.
1 Introduction
Statistical approaches to sequential labeling prob-
lems rely on necessary training data to model the
uncertainty of a sequence of events. Human?s
prior knowledge about the task, on the other hand,
often requires minimum cognitive load to spec-
ify, and yet can provide information often com-
plementary to that offered by a limited amount of
training data. Whenever prior knowledge becomes
available, it is desired that such information is in-
tegrated to a probabilistic model to improve learn-
ing.
Virtual evidence (VE), first introduced by Pearl
(1988), offers a principled and convenient way of
incorporating external knowledge into Bayesian
networks. In contrast to standard evidence (also
known as observed variables), VE expresses a
prior belief over values of random variables. It
has been shown that VE can significantly extend
the modeling power of Bayesian networks without
complicating the fundamental inference method-
ology (Bilmes, 2004; Reynolds and Bilmes,
2005).
This work extends the use of VE to undi-
rected graphical models and, in particular, to con-
ditional random fields (CRFs). We show that
VE can be naturally encoded into an undirected
graphical model as potential functions. More im-
portantly, we discuss a semi-supervised machine
learning setting for estimating CRFs with the pres-
ence of VE. As the conditional likelihood objec-
tive of CRFs is not directly maximizable with re-
spect to unlabeled data, we propose a novel semi-
supervised learning objective that can be opti-
mized using the Expectation-Maximization (EM)
algorithm while maintaining the discriminative
nature of CRFs.
We apply our model to the CLASSIFIEDS data
(Grenager et al, 2005). Specifically, we use VE to
incorporate into a CRF model two types of prior
knowledge specified in previous works. The first
is defined based on the notion of prototypes, i.e.,
example words for a given label; and the other as-
sumes that adjacent tokens tend to have the same
label. When unlabeled data becomes available,
we further extend the sparse prototype informa-
tion to other words based on distributional similar-
ity. This results in so-called collocation lists, each
consisting of a relatively large number of noisy
?prototypes? for a label. Given the fact that these
noisy prototypes are often located close to each
other in an input sequence, we create a new type
of VE based on word collocation to reduce ambi-
guity.
1289
We compare our CRF model integrated with VE
with two state-of-the-art models, i.e., constraint-
driven learning (Chang et al, 2007) and gener-
alized expectation criteria (Mann and McCallum,
2008). Experiments show that our approach leads
to sequential labeling accuracies superior to the
best results reported on this task in both supervised
and semi-supervised learning.
2 Related work
There have been various works that make use
of prior knowledge in sequential labeling tasks.
Grenager et al (2005) explicitly constrain the
transition matrix of a hidden Markov model
(HMM) to favor self transitions, assuming that
fields tend to consist of consecutive runs of the
same label.
Prototype-drive learning (Haghighi and Klein,
2006) specifies prior knowledge by providing a
few prototypes (i.e., canonical example words) for
each label. This sparse prototype information is
then propagated to other words based on distri-
butional similarity. The relation between words
and their prototypes are then used as features in
a Markov random field (MRF) model. Since an
MRF model aims to optimize the joint probability
p(x,y) of input and state sequences, it is possible
to apply the EM algorithm for unsupervised/semi-
supervised learning.
Constraint-driven learning (Chang et al, 2007)
expresses several kinds of constraints in a unified
form. In inference, a new decision function is pro-
posed to penalize the violation of the desired con-
straints as follows,
argmax
y
? ? F (x,y) ?
?
k
?
k
d(y, 1
C
k
(x)) (1)
Here ? ? F (x,y) is a linear decision function ap-
plicable to a number of sequential models, such
as HMMs, MRFs and CRFs. Function d is imple-
mented as the Hamming distance (or its approx-
imation) between a hypothesis sequence and the
space of state sequences that satisfy the constraint
C
i
. Due to the nature of the distance function,
their work approximates EM training by finding
the top K hypothesis sequences and using them as
newly labeled instances to update the model. This
process is repeated for a number of iterations in a
self-training fashion (Yarowsky, 1995).
Generalized expectation criteria (Mann and
McCallum, 2008) represent prior knowledge as la-
beled features, and use such information to reg-
ularize semi-supervised learning for CRFs. For-
mally, their learning objective consists of the stan-
dard CRF training objective, plus a Gaussian prior
on model parameters and an additional regulariza-
tion term:1
?
i
log p
?
(y
(i)
|x
(i)
)?
1
2?
2
???
2
??D(p?||p?
?
) (2)
In the last term, p? and p?
?
both refer to conditional
distributions of labels given a feature. While the
former is specified by prior knowledge, and the
latter is estimated from unlabeled data.
Our approach incorporates prior knowledge as
virtual evidence to express preferences over the
values of a set of random variables. The no-
tion of VE was first introduced by Pearl (1998)
and further developed by Bilmes (2004), both in
the context of Bayesian networks. Different from
constraint-driven learning, VE can be formally en-
coded as part of a graphical model. The funda-
mental inference methodology, therefore, does not
need to be altered. Moreover, VE has the flexibil-
ity of representing various kinds of prior knowl-
edge. For example, Reynolds and Bilmes (2005)
use VE that explicitly favors self transitions in dy-
namic Bayesian networks.
This work extends the use of VE to CRFs. In
essence, VE herein can be viewed as probabilistic
constraints in an undirected graph that allow exact
inference. One of the biggest challenges of such a
model lies in the semi-supervised machine learn-
ing setting. Since the entire state sequence of an
unlabeled instance remains hidden, the conditional
likelihood objective of CRFs is not directly opti-
mizable. There have been a number of works that
address this problem for conditional models. For
example, minimum entropy regularization (Grand-
valet and Bengio, 2004; Jiao et al, 2006), aims
to maximize the conditional likelihood of labeled
data while minimizing the conditional entropy of
unlabeled data:
?
i
log p
?
(y
(i)
|x
(i)
)?
1
2?
2
???
2
? ?H(y|x) (3)
This approach generally would result in ?sharper?
models which can be data-sensitive in practice.
Another approach (Suzuki and Isozaki, 2008)
embeds a joint probability model (HMM in their
1We slightly modify the notation here to be consistent
with the rest of the paper.
1290
case) into a CRF model as a new potential func-
tion. Semi-supervised learning is then conducted
by iteratively (1) fixing the HMM and updating
CRF parameters on labeled data and (2) fixing the
CRF model and updating the HMM on unlabeled
data.
Additionally, when unlabeled instances have
partial labeling information, it is possible to op-
timize a marginal distribution of the conditional
likelihood, i.e., p
?
(y
(i)
o
|x), on unlabeled data.
Here y(i)
o
is a subvector of y(i) that denotes the set
of observed state variables. The optimization can
be done in a similar fashion as training a hidden-
state CRF model (Quattoni et al, 2007).
3 Task
We consider the problem of extracting fields from
free-text advertisements. We use the CLASSI-
FIEDS data (Grenager et al, 2005) which consists
of 8767 ads for apartment rental. 302 of the ads
in the CLASSIFIEDS data have been manually-
labeled with 12 fields, including size, rent, neigh-
borhood and so on. The labeled data has been di-
vided into train/dev/test sets with 102/100/100 ads
respectively. The evaluation metric is the token-
level accuracy where tokens include both words
and punctuations.
Our goal in this work is two folds: (1) lever-
age both the training data and the prior knowledge
specified for this task for supervised learning, and
(2) additionally use the unlabeled data for semi-
supervised learning. We exploit two types of prior
knowledge:
? K1: label consistency with prototypes;
? K2: label consistency within a sentence.
K1 involves a set of prototype lists. Each list is
attached with a label and consists of a set of ex-
ample words for that label. In this work, we use
the prototype lists originally defined by Haghighi
and Klein (2006) (HK06) and subsequently used
by Chang et al (2005) (CRR07) and Mann and
McCallum (2008) (MM08). The labels as well as
their prototypes are shown in the first two columns
of Table 1. Our model is desired to be consistent
with such prototype information. Secondly, K2
means that tokens tend to have consistent labels
within a sentence. A similar type of prior knowl-
edge is implemented by CRR07 as a constraint in
inference.
4 Conditional Random Fields
Conditional random fields are a probabilistic
model that directly optimizes the conditional prob-
ability of a state (label) sequence given an input
sequence (Lafferty et al, 2001). Formally, we let
x = (x
1
, x
2
, . . . , x
T
) denote an input sequence
of T tokens, and y = (y
1
, y
2
, . . . , y
T
) the cor-
responding state sequence. We further augment
y with two special states, Start and End,2 repre-
sented by y
0
and y
T+1
respectively. A linear-chain
CRF model is an undirected graphical model as
depicted in Figure 1(a), with the conditional prob-
ability given by
p
?
(y|x) =
1
Z
?
(x)
?
t
?
(t)
?
(x, y
t?1
, y
t
) (4)
The partition function Z
?
(x) normalizes the expo-
nential form to be a probability distribution. ?(t)
?
are a set of potential functions defined on the max-
imum cliques of the graph, i.e., (x, y
t?1
, y
t
) in the
case of a linear-chain CRF model. The potential
functions are typically in the form of
?
(t)
?
(x, y
t?1
, y
t
) = exp
(
? ? f(x, y
t?1
, y
t
, t)
)
(5)
where ? is a weight vector and f is a feature vector
of arbitrary functions of the corresponding clique.
Given a set of labeled examples
{x
(i)
,y
(i)
)}
m
i=1
, we can estimate model pa-
rameters in a supervised machine learning setting.
The objective is to estimate ? that maximizes
the conditional likelihood while regularizing the
model size:
L
1
=
m
?
i=1
log p
?
(y
(i)
|x
(i)
) ?
1
2?
2
???
2 (6)
In this work, we optimize L
1
using stochastic gra-
dient descent and use the accuracy on the develop-
ment set as the stopping criterion.
5 CRFs with Virtual Evidence
A canonical way of using virtual evidence (VE)
in Bayesian networks is to have a directed edge
from a hidden variable h to a VE variable v. The
variable v will always be observed with a partic-
ular value, e.g., v = 1, but the actual value itself
does not matter. The prior knowledge about h is
2Start and End are with regard to a document, which are
different from start and end of a sentence.
1291
xyT EndStart y1 y2
(a)
(b)
x
yT
EndStart y1 y2
vT=1v1=1 v2=1 vT+1=1
Figure 1: Graphical model representations of (a) a
CRF model and (b) a CRF model integrated with
virtual evidence. Solid and empty nodes denote
observed and hidden variables respectively.
expressed via the conditional probability p(v =
1|h). For example, by setting p(v = 1|h = a) >
p(v = 1|h = b), we know that h = a is more
likely a event than h = b. This conditional distri-
bution is not learned from data, Instead, it is pre-
defined in such a way that reflects a prior belief
over the value of h.
VE can be encoded in an undirected graphical
model in a similar fashion. For our task, we mod-
ify the structure of a linear-chain CRF model as
depicted in Figure 1(b) ? we create a sequence
of VE variables, denoted by v
1
, v
2
, . . . , v
T+1
, in
parallel to the state variables. Each v
t
is assigned
a constant 1 (one), and is connected with y
t?1
and y
t
, forming a new set of maximum cliques
(y
t?1
, y
t
, v
t
), t = 1, . . . , T + 1. We create cliques
of size 3 because it is the minimum size required to
represent the prior knowledge used in our task, as
will be discussed shortly. However, it is possible
to have a different graph structure to incorporate
other types of prior knowledge, e.g., using large
cliques to represent constraints that involve more
variables.
Next, in analogy to Equation (5), we define the
corresponding potential functions as follows,
?
(t)
(y
t?1
, y
t
, v
t
) = exp
(
? ? s(y
t?1
, y
t
, v
t
, t)
)
(7)
s is a vector of VE feature functions and ? is the
corresponding weight vector with pre-defined val-
ues. Given the new graphical model in Figure 1(b).
It is natural to model the conditional probability
of the state sequence given both the standard evi-
dence and the VE as follows,
p
?
(y|x,v)
=
1
Z
?
(x,v)
?
t
?
(t)
?
(x, y
t?1
, y
t
)?
(t)
(y
t?1
, y
t
, v
t
)
(8)
Analogous to using p(v = 1|h) in Bayesian net-
works, we can utilize ?(t)(y
t?1
, y
t
,v = 1) to ex-
press preferences over state hypotheses in a CRF
model. In general, the function form of ?(t) may
or may not depend on the input x. Even when ?(t)
does depend on x, the relation is completely deter-
mined by external knowledge/systems (as opposed
to by data). Thus we do not explicitly connect v
with x in the graph.
5.1 Incorporating prior knowledge
Now we show how to represent the prior knowl-
edge introduced in Section 3 using the VE fea-
ture functions. Unless otherwise stated, we as-
sume v
t
= 1 for all t = 1, . . . , T and simply use
v
t
instead of v
t
= 1 in all equations. First, we
define a VE function s
1
that represents K1: label
consistency with prototypes. We let P
l
denote a
prototype list associated with the label l. If x
t
be-
longs to P
l
, we should prefer y
t
= l as opposed to
other values. To this end, for cases where x
t
? P
l
,
we set s
1
as
s
1
(y
t
, v
t
, t) =
{
1 if y
t
= l
0 otherwise (9)
On the other hand, if x
t
is not a prototype, we will
always have s
1
(y
t
, v
t
, t) = 0 for all hypotheses
of y
t
. The impact of this prior knowledge is con-
trolled by the weight of s
1
, denote by ?
1
. At one
extreme where ?
1
= 0, the prior knowledge is
completely ignored in training. At the other ex-
treme where ?
1
? +?, we constrain the values
of state variables to agree with the prior knowl-
edge. Note that although s
1
is implicitly related to
x, we do not write s
1
as a function of x for consis-
tency with the general definition of VE.
1292
To represent K2: label consistency within a sen-
tence, we define a second VE feature function s
2
with weight ?
2
. Assume that we have an exter-
nal system that detects sentence boundaries. If it
is determined that x
t
is not the start of a sentence,
we set s
2
as
s
2
(y
t?1
, y
t
, v
t
, t) =
{
1 if y
t?1
= y
t
0 otherwise (10)
It is easy to see that this would penalize state tran-
sitions within a sentence. On the other hand, if x
t
is a sentence start, we set s
2
(y
t?1
, y
t
, v
t
, t) = 0 for
all possible (y
t?1
, y
t
) pairs. In this work, we use
a simple heuristics to detect sentence boundaries:
we determine that x
t
is the start of a sentence if its
previous token x
t?1
is a period (.), a semi-colon
(;) or an acclamation mark (!), and if x
t
is not a
punctuation.
5.2 Semi-supervised learning
When a large amount of unlabeled data is avail-
able, it is often helpful to leverage such data
to improve learning. However, we cannot di-
rectly optimize p(y|x,v) since the correct state
sequences of the unlabeled data are hidden. One
heuristic approach is to adapt the self-training al-
gorithm (Yarowsky, 1995) to our model. More
specifically, for each input in the unlabeled dataset
{x
(i)
}
n
i=m+1
, we decode the best state sequence,
?
y
(i)
= argmax
y
(i)
p(y
(i)
|x
(i)
,v
(i)
) (11)
Then we use {(x(i), ?y(i))}n
i=m+1
in addition to the
labeled data to train a supervised CRF model. This
approach, however, does not have a theoretical
guarantee on optimality unless certain nontrivial
conditions are satisfied (Abney, 2004).
On the other hand, it is well known that unla-
beled data can be naturally incorporated using a
generative approach that models a joint probabil-
ity (Nigam et al, 2000). This is achieved by max-
imizing a marginal distribution of the joint proba-
bility over hidden variables. Inspired by the gen-
erative approach, we propose to explicitly model
p(y,v|x). In contrast to Equation (8), here we
jointly model y and v but the probability is still
conditioned on x. This ?joint? distribution should
be chosen such that it results in the same condi-
tional distribution p(y|x,v) as defined in Equa-
tion (8). To this end, we define p
?
(y,v|x) as
p
?
(y,v|x)
=
1
Z
?
?
(x)
?
t
?
(t)
?
(x, y
t?1
, y
t
)?
(t)
(y
t?1
, y
t
, v
t
)
(12)
Here Z ?
?
(x) is a normalization function obtained
by summing the numerator over both y and v.
By applying the Bayes rule, it is easy to see that
p(y|x,v) is exactly equal to Equation (8).
Given unlabeled data {x(i)}n
i=m+1
, we aim to
optimize the following objective,3
L
2
=
m+n
?
i=1
log p
?
(v
(i)
|x
(i)
)?
1
2?
2
???
2 (13)
This is essentially the marginal distribution of
p(y,v|x) over hidden variables y. Here we ig-
nore the labels of the dataset {(x(i),y(i))}m
i=1
, but
we do use the label information in initializing the
model which will described in Section 6. To op-
timize such an objective, we apply the EM algo-
rithm in the same fashion as is used in a generative
approach. In other words, we iteratively optimize
Q(?) =
?
y
p
?
g
(y|x,v) log p
?
(y,v|x) where ?g
denotes the model estimated from the previous it-
eration. The gradient of the Q function is straight-
forward to compute with the result given by
?Q(?)
??
k
=
?
t
?
y
t?1
,y
t
f
k
(y
t?1
, y
t
,x, t)?
(
p
?
(y
t?1
, y
t
|x,v) ? p
?
(y
t?1
, y
t
|x)
)
(14)
We keep two sets of accumulators in running the
Forward-Backward algorithm, one for comput-
ing p
?
(y
t?1
, y
t
|x,v) and the other for computing
p
?
(y
t?1
, y
t
|x). Loosely speaking, the model will
converge to a local optimum if the difference be-
tween these two posterior probabilities becomes
trivial.
5.3 Collocation based virtual evidence
Prior knowledge represented by prototypes is typ-
ically sparse. This sparse information, however,
can be propagated across all data based on dis-
tributional similarity (Haghighi and Klein, 2006).
Following the same idea, we extend the prototype
lists as follows. (1) We merge all prototypes in
P
l
into a single word type w
l
. (2) For each word
3In Equation (13), the fact that v is assigned a constant 1
does not mean p(v = 1|x) = 1 (Bilmes, 2004)
1293
Label Prototype lists of HK06 Collocation lists (top examples)
ADDRESS address carlmont [4-digit] street [3-digit] streets
AVAILABLE immediately begin cheaper available
CONTACT [phone] call [time] [email] appointment email see today ...
FEATURES kitchen laundry parking room new covered building garage ...
NEIGHBORHOOD close near shopping transportation center located restaurants ...
PHOTOS pictures image link [url] click view photos
RENT $ month [amount] lease deposit security year agreement ...
RESTRICTIONS pets smoking dog ok sorry please allowed negotiable ...
ROOMMATES roommate respectful drama
SIZE [1-digit] br sq [4-digit] [3-digit] ft bath ba ...
UTILITIES utilities pays electricity water included owner garbage paid
Table 1: Field labels (except other) for the CLASSIFIEDS task, their respective prototype lists specified
by prior knowledge, and collocation lists mined from unlabeled data.
in the corpus, we collect a context vector of the
counts of all words (excluding stop words) that
occur within a window of size k in either direc-
tion, where the window is applied only within sen-
tence boundaries. (3) Latent semantic analysis
(Deerwester et al, 1990) is performed on the con-
structed context vectors. (4) In the resulting latent
semantic space, all words (except stop words) that
have a high enough dot product with w
l
will be
grouped to form a new set, denoted as C
l
, which
is a superset of P
l
. In this regard, C
l
can be viewed
as lists of noisy ?prototypes?. As observed in
HK06, another consequence of this method is that
many neighboring tokens will share the same pro-
totypes.
Differently from previous works, we use C
l
directly as virtual evidence. We could apply
s
1
in Equation (9) when x
t
? C
l
(as opposed
to when x
t
? P
l
). This, however, would con-
taminate our model since C
l
are often noisy.
For example, ?water? is found to be distribu-
tionally similar to the prototypes of utilities.
Although in most cases ?water? indeed means
utilities, it can mean features in the context
of ?water front view?. To maximally reduce
ambiguity, we propose to apply s
1
in Equa-
tion (9) if both of the following conditions hold,
(1) x
t
? C
l
(2) There exists ? s.t. |? ? t| < k, and x
?
? C
l
In other words, we will impose a non-uniform
prior on y
t
if x
t
? C
l
?collocates?, within k
tokens, with another word that belongs to C
l
.
Based on K2, it is reasonable to believe that
neighboring tokens tend to share the same label.
Therefore, knowing that two tokens close to each
other both belong to C
l
would strengthen our
belief that either word is likely to have label l.
We thus refer to this type of virtual evidence
as collocation-based VE, and refer to C
l
as
collocation lists.
6 Evaluation
We use the CLASSIFIEDS data provided by
Grenager et al (2005) and compare with re-
sults reported by CRR07 (Chang et al, 2007) and
MM08 (Mann and McCallum, 2008) for both su-
pervised and semi-supervised learning. Following
all previous works conducted on this task, we to-
kenized both words and punctuations, and created
a number of regular expression tokens for phone
numbers, email addresses, URLs, dates, money
amounts and so on. However, we did not tokenize
newline breaks, as CRR07 did, which might be
useful in determining sentence boundaries. Based
on such tokenization, we extract n-grams, n =
1, 2, 3, from the corpus as features for CRFs.
As described in Section 3, we integrate the prior
knowledge K1 and K2 in our CRF model. The
prototypes that represent K1 are given in Table 1.
CRR07 used the same two kinds of prior knowl-
edge in the form of constraints, and they imple-
mented another constraint on the minimum num-
ber of words in a field chunk. MM08 used almost
the same set of prototypes as labeled features, but
they exploited two sets of 33 additional features
for some experiments. In this regard, the compar-
ison between CRR07, MM08 and the method pre-
sented here cannot be exact. However, we show
that while our prior knowledge is no more than
that used in previous works, our approach is able
1294
# labeled examples
Supervised model 10 25 100
CRR07: HMM 61.6 70.0 76.3
+ Constr in decoding 66.1 73.7 80.4
MM08: CRF 64.6 72.9 79.4
CRF 62.3 71.4 79.1
+ VE in decoding 68.9 74.6 81.1
CRF + VE (auto weights) 48.0 54.8 59.8
+ VE in decoding 66.0 72.5 80.9
Table 2: Token-level accuracy of supervised learn-
ing methods; ?+ VE? refers to the cases where
both kinds of prior knowledge, K1 and K2, are in-
corporated as VE in the CRF model.
to achieve the state-of-art performance.
6.1 Decoding settings
Depending on whether VE is used at test time, we
explore two decoding settings in all experiments:
1. Find y that maximizes p
?
(y|x) as in standard
CRF decoding, ignoring virtual evidence.
2. Find y that maximizes p(y|x,v). We use ?+
VE in decoding? to represent this setting.
These two scenarios are analogous to those in
CRR07 which conducted HMM decoding with-
out/with constraints applied. We use ?+ constr. in
decoding? to represent the latter scenario of their
work. MM08, on the other hand, found no accu-
racy improvement when adding constraints at test
time.
Note that in our second decoding setting, the
weights for the VE feature functions, i.e., ?
1
and
?
2
, are tuned on the development set. This is done
by a greedy search that first finds the best ?
1
, and
then finds the best ?
2
while fixing the value of ?
1
,
both with a step size 0.5.
6.2 Supervised learning results
First, we experimented with a standard CRF
model with VE applied neither in training nor in
decoding. As shown in Table 2, our CRF imple-
mentation performed slightly worse than the im-
plementation by MM08, probably due to slight
difference in tokenization. Secondly, we used the
same CRF model but additionally applied VE in
decoding, corresponding to the second setting in
Section 6.1. This method gave a significant boost
to the tagging performance, yielding the best su-
pervised learning results (shown as bolded in the
# labeled examples
Semi-supervised models 10 25 100
CRR07: HMM + Constr 70.9 74.8 78.6
+ Constr in decoding 74.7 78.5 81.7
MM08: CRF + GE 72.6 76.3 80.1
CRF + VE (Self-train) 69.0 74.2 81.4
+ VE in decoding 69.1 75.2 81.2
CRF + Col-VE (Self-train) 73.1 76.4 81.8
+ Col-VE in decoding 75.7 77.6 82.9
CRF + Col-VE (EM) 78.3 79.1 82.7
+ Col-VE in decoding 78.8 79.5 82.9
Table 3: Token-level accuracy of semi-supervised
learning methods. ?+ Col-VE? refers to cases
where collocation-based VE is integrated in the
CRF model in addition to the VE representing K1
and K2.
table). This proves that the prior knowledge is in-
deed complementary to the information offered by
the training data.
Similar to the second decoding setting that in-
corporates VE, we can have a counterpart setting
at training time. In other words, we can optimize
p
?
(y|x,v) instead of p
?
(y|x) during learning. In
deciding ? = (?
1
, ?
2
), it is possible to learn ?
from data in the same way as how we learn ?.
This, however, might undermine the role of other
useful features since we do not always have suffi-
cient training data to reliably estimate the weight
of prior knowledge. As shown in Table 2, we ex-
perimented with learning ? automatically (shown
as ?auto weights?). While applying VE with such
weights in both training and decoding worked rea-
sonably well, applying VE only in training but not
in decoding yielded very poor performance (prob-
ably due to excessively large estimates of ?
1
and
?
2
). Additionally, we repeated the above experi-
ment with manually specified weights, but did not
find further accuracy improvement over the best
supervised learning results.
6.3 Semi-supervised learning results
One natural way of leveraging the unlabeled data
(more than 8K examples) is to perform semi-
supervised learning in a self-training fashion. To
this end, we used our best supervised model in Ta-
ble 2 to decode the unlabeled examples as well
as the test-set examples (by treating them as un-
labeled). Note that by doing this our comparison
with CRR07 and MM08 cannot be exact as they
1295
sampled the unlabeled examples, with different
rates, for semi-supervised learning, while we used
as much data as possible. We applied the same ?
that was used for the supervised model, and then
combined the newly labeled examples, in addition
to the manually labeled ones, as training data to
learn a supervised CRF model. On this particu-
lar dataset, we did not find it helpful by selecting
automatically labeled data based on a confidence
threshold. We simply used all data available in
self-training. This paradigm is referred to as ?CRF
+ VE (self-train)? in Table 3. When no VE is ap-
plied at test time, this semi-supervised CRF model
significantly outperformed the best model in Ta-
ble 2. When applying VE at test time, however,
the improvement over its supervised counterpart
became trivial.
Next, following Section 5.3, we collected con-
text vectors on the unlabeled data using a win-
dow size k = 3, and extracted the top 50 singular
vectors therefrom.4 We created collocation lists
that contain words close to the merged prototype
words in the latent semantic space. Some exam-
ples are given in the last column of Table 1. We
then augmented the prototype-based VE based on
the following rules: If x
t
belongs to any prototype
list P
l
, we directly apply s
1
in Equation (9); oth-
erwise, we apply s
1
if x
t
and at least one neigh-
bor (within 3 tokens from x
t
) belong to the same
collocation list C
l
. In our experiments, we let
?Col-VE? represent such collocation-based VE.
We conducted self-training using a CRF model in-
tegrated with Col-VE, where ? was tuned a pri-
ori by testing the same model on the develop-
ment set. As shown in the table, ?CRF + Col-VE
(self-train)? gave significant accuracy improve-
ment over ?CRF + VE?, while adding Col-VE at
test time further boosted the performance. The ac-
curacies were already on par with the best results
previously reported on this task.
Finally, we implemented the EM algorithm pro-
posed in Section 5.2 that iteratively optimizes
p(v|x) on all data. The model was initial-
ized by the one obtained from ?CRF + Col-VE
(self-train)?. After the model was initialized,
we performed the EM algorithm until the model
reached a maximum accuracy on the develop-
ment set. Note that in some cases, we observed
a development-set accuracy degradation after the
first iteration of the EM, but the accuracy quickly
4The same configuration is used in HK06.
recovered from the second iteration and kept in-
creasing until a maximum accuracy was reached.5
As shown in the last two rows in Table 3, this
method is clearly advantageous over self-training,
leading to the best tagging accuracies in both de-
coding settings. Our model achieved 2.6%?5.7%
absolute accuracy increases in the three training
settings compared with MM08 which had the best
results without using any constraints in decoding.
When applying VE at test time, our model was
1.2% ? 4.1% better than CRR07 which had the
best overall results. Additionally, when compared
with supervised learning results, our best semi-
supervised model trained on only 10 labeled ex-
amples performed almost as well as a standard su-
pervised CRF model trained on 100 labeled exam-
ples.
7 Conclusions
We have presented the use of virtual evidence as
a principled way of incorporating prior knowledge
into conditional random fields. A key contribu-
tion of our work is the introduction of a novel
semi-supervised learning objective for training a
CRF model integrated with VE. We also found it
useful to create so-called collocation-based VE,
assuming that tokens close to each other tend to
have consistent labels. Our evaluation on the
CLASSIFIEDS data showed that the learning ob-
jective presented here, combined with the use of
collocation-based VE, yielded remarkably good
accuracy performance. In the future, we would
like to see the application of our approach to other
tasks such as (Li et al, 2009).
References
Steven Abney. 2004. Understanding the Yarowsky
algorithm. Association for Computational Linguis-
tics, 30(3):365?395.
Jeff Bilmes. 2004. On soft evidence in Bayesian
networks. Technical Report UWEETR-2004-0016,
University of Washington, Dept. of Electrical Engi-
neering.
Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007.
Guiding semi-supervision with constraint-driven
learning. In Proceedings of ACL.
5The initial degradation is probably due to the fact that
self-training can result in an initial model with decent accu-
racy but low p(v|x); thus the EM algorithm that maximizes
p(v|x) may temporarily decrease the accuracy.
1296
Scott Deerwester, Susan Dumais, Thomas Landauer,
George Furnas, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal
of the American Society of Information Science,
41(6):391?407.
Yves Grandvalet and Yoshua Bengio. 2004. Semi-
supervised learning by entropy minimization. In Ad-
vances in Neural Information Processing Systems.
Trond Grenager, Dan Klein, and Christopher D. Man-
ning. 2005. Unsupervised learning for field seg-
mentation models for information extraction. In
Proceedings of Association of Computational Lin-
guistics.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
HLT-NAACL.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved
sequence segmentation and labeling. In Proceed-
ings of ACL.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning, pages 282?289.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extract-
ing structured information from user queries with
semi-supervised conditional random fields. In Pro-
ceedings of SIGIR.
Gideon Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learn-
ing of conditional random fields. In Proceedings of
ACL.
Kamal Nigam, Andrew Kachites Mccallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classification
from labeled and unlabeled documents using EM.
Machine Learning, 39:103?134.
Judea Pearl. 1988. Probabilistic Reasoning in In-
telligent Systems: Networks of Plausible Inference.
Morgan Kaufmann, 2nd printing edition edition.
A. Quattoni, S. Wang, L.-P. Morency, M. Collins, and
T. Darrell. 2007. Hidden conditional random fields.
IEEE Transaction on Pattern Analysis and Machine
Intellegence, 29(10):1848?1852.
Sheila Reynolds and Jeff Bilmes. 2005. Part-of-speech
tagging using virtual evidence and negative training.
In Proceedings of HLT/EMNLP.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proceedings of
ACL/HLT.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of ACL, pages 189?196.
1297
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1484?1492,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Discovery of Term Variation in Japanese Web Search Queries 
 
 Hisami Suzuki, Xiao Li, and Jianfeng Gao 
Microsoft Research, Redmond 
One Microsoft Way, Redmond, WA 98052 USA 
{hisamis,xiaol,jfgao}@microsoft.com 
 
 
 
 
 
  
 
Abstract 
In this paper we address the problem of identi-
fying a broad range of term variations in Japa-
nese web search queries, where these varia-
tions pose a particularly thorny problem due to 
the multiple character types employed in its 
writing system. Our method extends the tech-
niques proposed for English spelling correc-
tion of web queries to handle a wider range of 
term variants including spelling mistakes, va-
lid alternative spellings using multiple charac-
ter types, transliterations and abbreviations. 
The core of our method is a statistical model 
built on the MART algorithm (Friedman, 
2001). We show that both string and semantic 
similarity features contribute to identifying 
term variation in web search queries; specifi-
cally, the semantic similarity features used in 
our system are learned by mining user session 
and click-through logs, and are useful not only 
as model features but also in generating term 
variation candidates efficiently. The proposed 
method achieves 70% precision on the term 
variation identification task with the recall 
slightly higher than 60%, reducing the error 
rate of a na?ve baseline by 38%.  
1 Introduction 
Identification of term variations is fundamental 
to many NLP applications: words (or more gen-
erally, terms) are the building blocks of NLP ap-
plications, and any robust application must be 
able to handle variations in the surface represen-
tation of terms, be it a spelling mistake, valid 
spelling variation, or abbreviation. In search ap-
plications, term variations can be used for query 
expansion, which generates additional query 
terms for better matching with the terms in the 
document set. Identifying term variations is also 
useful in other scenarios where semantic equiva-
lence of terms is sought, as it represents a very 
special case of paraphrase.  
This paper addresses the problem of identify-
ing term variations in Japanese, specifically for 
the purpose of query expansion in web search, 
which appends additional terms to the original 
query string for better retrieval quality. Query 
expansion has been shown to be effective in im-
proving web search results in English, where dif-
ferent methods of generating the expansion terms 
have been attempted, including relevance feed-
back (e.g., Salton and Buckley, 1990), correction 
of spelling errors (e.g., Cucerzan and Brill, 2004), 
stemming or lemmatization (e.g., Frakes, 1992), 
use of manually- (e.g., Aitchison and Gilchrist, 
1987) or automatically- (e.g., Rasmussen 1992) 
constructed thesauri, and Latent Semantic Index-
ing (e.g., Deerwester et al 1990). Though many 
of these methods can be applied to Japanese 
query expansion, there are unique problems 
posed by Japanese search queries, the most chal-
lenging of which is that valid alternative spel-
lings of a word are extremely common due to the 
multiple script types employed in the language. 
For example, the word for 'protein' can be spelled 
as ??????, ?????, ???, ???? 
and so on, all pronounced tanpakushitsu but us-
ing combinations of different script types. We 
give a detailed description of the problem posed 
by the Japanese writing system in Section 2. 
Though there has been previous work on ad-
dressing specific subsets of spelling alterations 
within and across character types in Japanese, 
there has not been any comprehensive solution 
for the purpose of query expansion.  
Our approach to Japanese query expansion is 
unique in that we address the problem compre-
hensively: our method works independently of 
the character types used, and targets a wide range 
of term variations that are both orthographically 
and semantically similar, including spelling er-
rors, valid alternative spellings, transliterations 
and abbreviations. As described in Section 4, we 
define the problem of term variation identifica-
1484
tion as a binary classification task, and build two 
types of classifiers according to the maximum 
entropy model (Berger et al, 1996) and the 
MART algorithm (Friedman, 2001), where all 
term similarity metrics are incorporated as fea-
tures and are jointly optimized. Another impor-
tant contribution of our approach is that we de-
rive our semantic similarity models by mining 
user query logs, which has been explored for the 
purposes of collecting related words (e.g., Jones 
et al, 2006a), improving search results ranking 
(e.g., Craswell and Szummer, 2007) and learning 
query intention (e.g., Li et al, 2008), but not for 
the task of collecting term variations. We show 
that our semantic similarity models are not only 
effective in the term variation identification task, 
but also for generating candidates of term varia-
tions much more efficiently than the standard 
method whose candidate generation is based on 
edit distance metrics.  
2 Term Variations in Japanese 
In this section we give a summary of the Japa-
nese writing system and the problem it poses for 
identifying term variations, and define the prob-
lem we want to solve in this paper.  
2.1 The Japanese Writing System 
There are four different character types that are 
used in Japanese text: hiragana, katakana, kanji 
and Roman alphabet. Hiragana and katakana are 
the two subtypes of kana characters, which are 
syllabic character sets, each with about 50 basic 
characters. There is a one-to-one correspondence 
between hiragana and katakana characters, and, 
as they are phonetic, they can be unambiguously 
converted into a sequence of Roman characters. 
For example, the word for 'mackerel' is spelled in 
hiragana as ?? or in katakana as ??, both of 
which can be transcribed in Roman characters as 
saba, which is how the word is pronounced. 
Kanji characters, on the other hand, are ideo-
graphic and therefore numerous ? more than 
5,000 are in common usage. One difficulty in 
handling Japanese kanji is that each character has 
multiple pronunciations, and the correct pronun-
ciation is determined by the context in which the 
character is used. For instance, the character ? is 
read as kou in the word ?? ginkou 'bank', gyou 
in ?  'column', and i or okona in ???  itta 
'went' or okonatta 'done' depending on the con-
text in which the word is used.1  Proper name 
readings are particularly difficult to disambiguate, 
as their pronunciation cannot be inferred from 
the context (they tend to have the same grammat-
ical function) or from the dictionary (they tend to 
be out-of-vocabulary). Therefore, in Japanese, 
computing a pronunciation-based edit distance 
metric is not straightforward, as it requires esti-
mating the readings of kanji characters.  
2.2 Term Variation by Character Type 
Spelling variations are commonly observed both 
within and across character types in Japanese. 
Within a character type, the most prevalent is the 
variation observed in katakana words. Katakana 
is used to transliterate words from English and 
other foreign languages, and therefore reflects 
the variations in the sound adaptation from the 
source language. For example, the word 
'spaghetti' is transliterated into six different 
forms (?????? supagetti, ???????
supagettii, ?????? supagettei, ?????
supageti, ?????? supagetii, ?????
supagetei) within a newspaper corpus (Masuya-
ma et al, 2004).  
Spelling variants are also prevalent across 
character types: in theory, a word can be spelled 
using any of the character types, as we have seen 
in the example for the word 'protein' in Section 1. 
Though there are certainly preferred character 
types for spelling each word, variations are still 
very common in Japanese text and search queries. 
Alterations are particularly common among hira-
gana, katakana and kanji (e.g. ??~??~ ? sa-
ba 'mackerel'), and between katakana and Roman 
alphabet (e.g. ??????  fedekkusu fedex). 
This latter case constitutes the problem of transli-
teration, which has been extensively studied in 
the context of machine translation (e.g. Knight 
and Graehl, 1998; Bilac and Tanaka, 2004; Brill 
et al, 2001).  
2.3 Term Variation by Re-write Categories 
Table 1 shows the re-write categories of related 
terms observed in web query logs, drawing on 
our own data analysis as well as on previous 
work such as Jones et al (2006a) and Okazaki et 
al. (2008b). Categories 1 though 9 represent 
strictly synonymous relations; in addition, terms 
in Categories 1 through 5 are also similar ortho-
graphically or in pronunciation. Categories 10 
                                                 
1 In a dictionary of 200K entries, we find that on average 
each kanji character has 2.5 readings, with three characters 
(?,?,?) with as many as 11 readings. 
1485
through 12, on the other hand, specify non-
synonymous relations.  
Different sets out of these categories can be 
useful for different purposes. For example, Jones 
et al(2006a; 2006b) target al of these categories, 
as their goal is to collect related terms as broadly 
as possible for the application of sponsored 
search, i.e., mapping search queries to a small 
corpus of advertiser listings. Okazaki et al 
(2008b) define their task narrowly, to focusing 
on spelling variants and inflection, as they aim at 
building lexical resources for the specific domain 
of medical text.  
For web search, a conservative definition of 
the task as dealing only with spelling errors has 
been successful for English; a more general defi-
nition using related words for query expansion 
has been a mixed blessing as it compromises re-
trieval precision. A comprehensive review on 
this topic is provided by Baeza-Yates and Ribei-
ro-Neto (1999). In this paper, therefore, we adopt 
a working definition of the term variation identi-
fication task as including Categories 1 through 5, 
i.e., those that are synonymous and also similar 
in spelling or in pronunciation.2 This definition is 
reasonably narrow so as to make automatic dis-
covery of term variation pairs realistic, while 
covering all common cases of term variation in 
Japanese, including spelling variants and transli-
terations. It is also appropriate for the purpose of 
query expansion: because term variation defined 
in this manner is based on spelling or pronuncia-
tion similarity, their meaning and function tend 
                                                 
2 In reality, Category 3 (Inflection) is extremely rare in Jap-
anese web queries, because nouns do not inflect in Japanese, 
and most queries are nominals.  
to be completely equivalent, as opposed to Cate-
gories 6 through 9, where synonymy is more 
context- or user-dependent. This will ensure that 
the search results by query expansion will avoid 
the problem of compromised precision.  
3 Related Work 
In information retrieval, the problem of vocabu-
lary mismatch between the query and the terms 
in the document has been addressed in many 
ways, as mentioned in Section 1, achieving vary-
ing degrees of success in the retrieval task. In 
particular, our work is closely related to research 
in spelling correction for English web queries 
(e.g., Cucerzan and Brill, 2004; Ahmad and 
Kondrak, 2005; Li et al, 2006; Chen et al, 2007). 
Among these, Li et al (2006) and Chen et al 
(2007) incorporate both string and semantic simi-
larity in their discriminative models of spelling 
correction, similarly to our approach. In Li et al 
(2006), semantic similarity was computed as dis-
tributional similarity of the terms using query 
strings in the log as context. Chen et al (2007) 
point out that this method suffers from the data 
sparseness problem in that the statistics for rarer 
terms are unreliable, and propose using web 
search results as extended contextual information. 
Their method, however, is expensive as it re-
quires web search results for each query-
candidate pair, and also because their candidate 
set, generated using an edit distance function and 
phonetic similarity from query log data, is im-
practically large and must be pruned by using a 
language model. Our approach differs from these 
methods in that we exploit user query logs to 
derive semantic knowledge of terms, which is 
Categories Example in English Example in Japanese 
1. Spelling mistake aple ~ apple ???? guuguru ~ ???? gu-guru 'google' 
2. Spelling variant color ~ colour ??~??~?; ?????~??????? (Cf. Sec.2.2) 
3. Inflection matrix ~ matrices ?? tsukuru 'make' ~ ??? tsukutta 'made' 
4. Transliteration  ?????? ~ fedex 'Fedex' 
5. Abbreviation/ 
Acronym 
macintosh ~ mac ???? sekaiginkou ~ ?? segin 'World Bank'; ???
??? makudonarudo ~ ??? makku 'McDonald's' 
6. Alias republican party ~ gop ???? furansu ~ ? futsu 'France' 
7. Translation ???????? pakisutantaishikan ~ Pakistan embassy 
8. Synonym carcinoma ~ cancer ? koyomi ~ ????? karendaa 'calendar' 
9. Abbreviation 
    (user specific) 
mini ~ mini cooper ??????? kuronekoyamato ~ ???? kuroneko 
(name of a delivery service company)  
10. Generalization nike shoes ~ shoes ???? ?? shibikku buhin 'Civic parts' ~ ? ?? ku-
ruma buhin 'car parts' 
11. Specification ipod ~ ipod nano ??? toukyoueki 'Tokyo station' ~ ?????? tou-
kyouekijikokuhyou 'Tokyo station timetable' 
12. Related windows ~ microsoft ??? toyota 'Toyota' ~ ??? honda 'Honda' 
Table 1: Categories of Related Words Found in Web Search Logs 
1486
used both for the purpose of generating a candi-
date set efficiently and as features in the term 
variation identification model.  
Acquiring semantic knowledge from a large 
quantity of web query logs has become popular 
in recent years. Some use only query strings and 
their counts for learning word similarity (e.g., 
Sekine and Suzuki, 2007; Komachi and Suzuki, 
2008), while others use additional information, 
such as the user session information (i.e., a set of 
queries issued by the same user within a time 
frame, e.g., Jones et al, 2006a) or the URLs 
clicked as a result of the query (e.g., Craswell 
and Szummer, 2007; Li et al, 2008). This addi-
tional data serves as an approximation to the 
meaning of the query; we use both user session 
and click-through data for discovering term vari-
ations.  
Our work also draws on some previous work 
on string transformation, including spelling nor-
malization and transliteration. In addition to the 
simple Levenshtein distance, we also use genera-
lized string-to-string edit distance (Brill and 
Moore, 2000), which we trained on aligned kata-
kana-English word pairs in the same manner as 
Brill et al (2001). As mentioned in Section 2.2, 
our work also tries to address the individual 
problems targeted by such component technolo-
gies as Japanese katakana variation, English-to-
katakana transliteration and katakana-to-English 
back-transliteration in a unified framework.  
4 Discriminative Model of Identifying 
Term Variation 
Recent work in spelling correction (Ahmed and 
Kondrak, 2005; Li et al, 2006; Chen et al, 2007) 
and normalization (Okazaki et al, 2008b) formu-
lates the task in a discriminative framework:  
??  = argmax??gen  ? ?(?|?) 
This model consists of two components: gen(q) 
generates a list of candidates C(q) for an input 
query q, which are then ranked by the ranking 
function P(c|q). In previous work, gen(q) is typi-
cally generated by using an edit distance function 
or using a discriminative model trained for its 
own purpose (Okazaki et al, 2008b), often in 
combination with a pre-complied lexicon. In the 
current work, we generate the list of candidates 
by learning pairs of queries and their re-write 
candidates automatically from query session and 
click logs, which is far more robust and efficient 
than using edit distance functions. We describe 
our candidate generation method in detail in Sec-
tion 5.1.  
Unlike the spelling correction and normaliza-
tion tasks, our goal is to identify term variations, 
i.e., to determine whether each query-candidate 
pair (q,c) constitutes a term variation or not. We 
formulate this problem as a binary classification 
task. There are various choices of classifiers for 
such a task: we chose to build two types of clas-
sifiers that make a binary decision based on the 
probability distribution P(c|q) over a set of fea-
ture functions fi(q,c). In maximum entropy 
framework, this is defined as:  
? ? ? =
exp ???? ?, ? 
?
?=1
 exp ???? ?, ? 
?
?=1?
 
where ?1,?, ?k are the feature weights. The op-
timal set of feature weights ?* is computed by 
maximizing the log-likelihood of the training 
data. We used stochastic gradient descent for 
training the model with a Gaussian prior.   
The second classifier is built on MART 
(Friedman, 2001), which is a boosting algorithm. 
At each boosting iteration, MART builds a re-
gression tree to model the functional gradient of 
the cost function (which is cross entropy in our 
case), evaluated on all training samples.  MART 
has three main parameters: M, the total number 
of boosting iterations, L, the number of leaf 
nodes for each regression tree, and v, the learning 
rate. The optimal values of these parameters can 
be chosen based on performance on a validation 
set.  In our experiments, we found that the per-
formance of the algorithm is relatively insensi-
tive to these parameters as long as they are in a 
reasonable range: given the training set of a few 
thousand samples or more, as in our experiments, 
M=100, L=15, and v=0.1 usually give good per-
formance. Smaller trees and shrinkage may be 
used if the training data set is smaller. 
The classifiers output a binary decision ac-
cording to P(c|q): positive when P(c|q) > 0.5 and 
negative otherwise.  
5 Experiments 
5.1 Candidate Generation 
We used a set of Japanese query logs collected 
over one year period in 2007 and 2008. More 
specifically, we used two different extracts of log 
data for generating term variation candidate 
pairs:  
Query session data. From raw query logs, we 
extracted pairs of queries q1 and q2 such that they 
are (i) issued by the same user; (ii) q2 follows 
within 3 minutes of issuing q1; and (iii) q2 gener-
ated at least one click of a URL on the result 
1487
page while q1 did not result in any click. We then 
scored each query pair (q1,q2) in this subset using 
the log-likelihood ratio (LLR, Dunning, 1993) 
between q1 and q2, which measures the mutual 
dependence within the context of web search 
queries (Jones et al, 2006a). After applying an 
LLR threshold (LLR > 15) and a count cutoff 
(we used only the top 15 candidate q2 according 
to the LLR value for each q1), we obtained a list 
of 47,139,976 pairs for the 14,929,497 distinct q1, 
on average generating 3.2 candidates per q1
3. We 
took this set as comprising query-candidate pairs 
for our model, along with the set extracted by 
click-through data mining explained below.  
Click-through data. This data extract is based 
on the idea that if two queries led to the same 
URLs being repeatedly clicked, we can reasona-
bly infer that the two queries are semantically 
related. This is similar to computing the distribu-
tional similarity of terms given the context in 
which they appear, where context is most often 
defined as the words co-occurring with the terms. 
Here, the clicked URLs serve as their context.  
One challenge in using the URLs as contex-
tual information is that the contextual representa-
tion in this format is very sparse, as user clicks 
are rare events. To learn query similarities from 
incomplete click-through data, we used the ran-
dom walk algorithm similar to the one described 
in Craswell and Szummer (2007). Figure 1 illu-
strates the basic idea: initially, document ?3 has 
a click-through link consisting of query ?2 only; 
the random walk algorithm adds the link from ?3 
to ?1 , which has a similar click pattern as ?2 . 
Formally, we construct a click graph which is a 
bipartite-graph representation of click-through 
data. We use  ?? ?=1
?  to represent a set of query 
nodes and  ??  ?=1
?
 a set of document nodes. We 
further define an  ? ? ? matrix ? in which ele-
ment ???  represents the click count associated 
with  ?? ,??  . This matrix can be normalized to 
be a query-to-document transition matrix, de-
                                                 
3 We consider each query as an unbreakable term in this 
paper, so term variation is equivalent to query variation. 
noted by ?, where ??? = ?
(1)(?? |??) is the prob-
ability that ??  transits to ??  in one step. Similarly, 
we can normalize the transpose of ?  to be a 
document-to-query transition matrix, denoted by 
?, where ?? ,? = ?
(1)(??|?? ). It is easy to see that 
using ? and ? we can compute the probability of 
transiting from any node to any other node in ? 
steps. In this work, we use a simple measure 
which is the probability that one query transits to 
another in two steps, and the corresponding 
probability matrix is given by ??.  
We used this probability and ranked all pairs 
of queries in the same raw query logs as in the 
query session data described above to generate 
additional candidates for term variation pairs. 
20,308,693 pairs were extracted after applying 
the count cutoff of 5, generating on average 6.8 
candidates for 2,973,036 unique queries. 
It is interesting to note that these two data ex-
tracts are quite complementary: of all the data 
generated, only 4.2% of the pairs were found in 
both the session and click-through data. We be-
lieve that this diversity is attributable to the na-
ture of the extracts: the session data tends to col-
lect the term pairs that are issued by the same 
user as a result of conscious re-writing effort, 
such as typing error corrections and query speci-
fications (Categories 1 and 11 in Table 1), while 
the click-though data collects the terms issued by 
different users, possibly with different intentions, 
and tends to include many spelling variants, syn-
onyms and queries with different specificity 
(Categories 2, 8, 10 and 11).  
5.2 Features 
We used the same set of features for the maxi-
mum entropy and MART models, which are giv-
en in Table 2. They are divided into three main 
types: string similarity features (1-16), semantic 
similarity features (17, 18), and character type 
features (19-39). Among the string similarity 
features, half of them are based on Levenshtein 
distance applied to surface forms (1-8), while the 
other half is based on Levenshtein and string-to-
string edit distance metrics computed over the 
Romanized form of the query, reflecting its pro-
nunciation. The conversion into Roman charac-
ters was done deterministically for kana charac-
ters using a simple mapping table. For Romaniz-
ing kanji characters, we used the function availa-
ble from Windows IFELanguage API (version 
 
Figure 1. Random Walk Algorithm 
1488
2).4 The character equivalence table mentioned in 
the features 3,4,7,8 is a table of 643 pairs of cha-
racters that are known to be equivalent, including 
kanji allography (same kanji in different graphi-
cal styles). The alpha-beta edit distance (11, 12, 
15, 16) is the string-to-string edit distance pro-
posed in Brill and Moore (2001), which we 
trained over about 60K parallel English-to-
katakana Wikipedia title pairs, specifically to 
capture the edit operations between English and 
katakana words, which are different from the edit 
operations between two Japanese words. Seman-
tic similarity features (17, 18) use the LLR score 
from the session data, and the click-though pair 
probability described in the subsection above. 
Finally, features 19-39 capture the script types of 
the query-candidate pair. We first defined six 
basic character types for each query or candidate: 
Hira (hiragana only), Kata (katakana only), Kanji 
(kanji only), Roman (Roman alphabet only), 
MixedNoKanji (includes more than one charac-
ter sets but not kanji) and Mixed (includes more 
than one character sets with kanji). We then de-
rived 21 binary features by concatenating these 
basic character type features for the combination 
                                                 
4 http://msdn.microsoft.com/en-us/library/ms970129.aspx. 
We took the one-best conversion result from the API. The 
conversion accuracy on a randomly sampled 100 kanji que-
ries was 89.6%.  
of query and candidate strings. For example, if 
both the query and candidate are in hiragana, 
BothHira will be on; if the query is Mixed and 
the candidate is Roman, then RomanMixed will 
be on. Punctuation characters and Arabic numer-
als were treated as being transparent to character 
type assignment. The addition of these features is 
motivated by the assumption that appropriate 
types of edit distance operations might depend 
on different character types for the query-
candidate pair.  
Since the dynamic ranges of different features 
can be drastically different, we normalized each 
feature dimension to a normal variable with zero-
mean and unit-variance. We then used the same 
normalized features for both the maximum en-
tropy and the MART classifiers. 
5.3 Training and Evaluation Data 
In order to generate the training data for the bi-
nary classification task, we randomly sampled 
the query session (5,712 samples) and click-
through data (6,228 samples), and manually la-
beled each pair as positive or negative: the posi-
tive label was assigned when the term pair fell 
into Categories 1 through 5 in Table 1; otherwise 
it was assigned a negative label. Only 364 (6.4%) 
and 244 (3.9%) of the samples were positive ex-
amples for the query session and click-through 
data respectively, which makes the baseline per-
String similarity features (16 real-valued features) 
1. Lev distance on surface form 
2. Lev distance on surface form normalized by q1 length 
3. Lev distance on surface form using character equivalence table 
4. Lev distance on surface form normalized by  q1 length using character equivalence table 
5. Lev distance on surface form w/o space 
6. Lev distance on surface form normalized q1 length w/o space 
7. Lev distance on surface form using  character equivalence table w/o space 
8. Lev distance on surface form normalized by q1 using character equivalence table  w/o space 
9. Lev distance on Roman 
10. Lev distance on Roman normalized by q1 length 
11. Alpha-beta edit distance on Roman 
12. Alpha-beta edit distance on Roman normalized by q1 length 
13. Lev distance  on Roman w/o space 
14. Lev distance  on Roman normalized by q1 length w/o space 
15. Alpha-beta edit distance on Roman w/o space 
16. Alpha-beta edit distance on Roman normalized by q1 length w/o space 
Features for semantic similarity (2 real-valued features) 
17. LLR score 
18. Click-though data probability 
Character type features (21 binary features) 
19. BothHira, 20. BothKata, 21. BothRoman, 22. BothKanji, 23. BothMixedNoKanji, 24. BothMixed,  
25. HiraKata, 26. HiraKanji, 27. HiraRoman, 28. HiraMixedNoKanji, 29. HiraMixed, 30. KataKanji, 
31.KataRoman, 32. KataMixedNoKanji, 33. KataMixed, 34. KanjiRoman, 35. KanjiMixedNoKanji,  
36. KanjiMixed, 37. RomanMixedNoKanji, 38. RomanMixed, 39. MixedNoKanjiMixed 
Table 2: Classifier Features 
1489
formance of the classifier quite high (always 
predict the negative label ? the accuracy will be 
95%). Note, however, that these data sets include 
term variation candidates much more efficiently 
than a candidate set generated by the standard 
method that uses an edit distance function with a 
threshold. For example, there is a query-
candidate pair q=???? kafuujouhou 'house-
style information' c= ? ? ? ?  kafunjouhou 
'pollen information') in the session data extract, 
the first one of which is likely to be a mis-
spelling of the second.5 If we try to find candi-
dates for the query ???? using an edit dis-
tance function naively with a threshold of 2 from 
the queries in the log, we end up collecting a 
large amount of completely irrelevant set of can-
didates such as ???? taifuujouhou 'typhoon 
information', ??? kabu jouhou 'stock informa-
tion', ???? kouu jouhou 'rainfall information' 
and so on ? as many as 372 candidates were 
found in the top one million most frequent que-
ries in the query log from the same period; for 
rarer queries these numbers will only be worse. 
Computing the edit distance based on the pro-
nunciation will not help here: the examples 
above are within the edit distance of 2 even in 
terms of Romanized strings.  
Another advantage of generating the annotated 
data using the result of query log data mining is 
that the annotation process is less prone to sub-
jectivity than creating the annotation from 
scratch. As Cucerzan and Brill (2004) point out, 
the process of manually creating a spelling cor-
rection candidate is seriously flawed as the inten-
tion of the original query is completely lost: for 
the query gogle, it is not clear out of context if 
the user meant goggle, google, or gogle. Using 
data mined from query logs solves this problem: 
an annotator can safely assume that if gogle-
goggle appears in the candidate set, it is very 
likely to be a valid term variation intended by the 
user. This makes the annotation more robust and 
efficient: the inter-annotator agreement rate for 
2,000 query pairs by two annotators was 95.7% 
on our data set, each annotator spending only 
about two hours to annotate 2,000 pairs.  
5.4 Results and Discussion 
In order to compare the performance of two clas-
sifiers, we first built maximum entropy and 
MART classifiers as described in Section 4 using 
                                                 
5 ???? does not make any sense in Japanese; on the 
other hand, information about cedar pollen is commonly 
sought after in spring due to widespread pollen allergy.  
all the features in Section 5.2. We run five expe-
riments using different random split of training 
and test data: in each run,  we used 10,000 sam-
ples for training and the remaining 1,940 samples 
for testing, and measured the performance of the 
two classifiers on the task of term variation iden-
tification in terms of the error rate i.e., 1?
accuracy. The results, average over five runs, 
were 4.18 for the maximum entropy model, and 
3.07 for the MART model. In all five runs, the 
MART model outperformed the maximum en-
tropy classifier. This is not surprising given the 
superior performance of tree-boosting algorithms 
previously reported on similar classification 
tasks (e.g., Hastie et al, 2001). In our task where 
different types of features are likely to perform 
better when they are combined (such as semantic 
features and character types features), MART 
would be a better fit than linear classifiers  be-
cause the decision trees generated by MART op-
timally combines features in the local sense. In 
what follows, we only discuss the results pro-
duced by MART for further experiments. Note 
that the baseline classifier, which always predicts 
the label to be negative, achieves 95.04% in ac-
curacy (or 4.96% error rate), which sounds ex-
tremely high, but in fact this baseline classifier is 
useless for the purpose of collecting term varia-
tions, as it learns none of them by classifying all 
samples as negative.  
For evaluating the contribution of different 
types of features in Section 5.2, we performed 
feature ablation experiments using MART. Table 
3 shows the results in error rate by various 
MART classifiers using different combination of 
features. The results in this table are also aver-
aged over five run with random training/test data 
split. From Table 3, we can see that the best per-
formance was achieved by the model using all 
features (line A of the table), which reduces the 
baseline error rate (4.96%) by 38%. The im-
provement is statistically significant according to 
the McNemar test (P < 0.05). Models that use 
string edit distance features only (lines B and C) 
did not perform well: in particular, the model 
that uses surface edit distance features only 
Features Error rate (%) 
A. All features (1-39 in Table 2) 3.07 
B. String features only (1-16) 3.49 
C. Surface string features only (1-8) 4.9 
D. No semantic feats (1-16,19-39) 3.28 
E. No character type feats (1-18) 3.5 
Table 3: Results of Features Ablation Experiments 
Using MART Model 
1490
without considering the term pronunciation per-
formed horribly (line C), which confirms the re-
sults reported by Jones et al (2006b). However, 
unlike Jones et al (2006b), we see a positive 
contribution of semantic features: the use of se-
mantic features reduced the error rate from 3.28 
(line D) to 3.07 (line A), which is statistically 
significant. This may be attributable to the nature 
of semantic information used in our experiments: 
we used the user session and click-though data to 
extract semantic knowledge, which may be se-
mantically more specific than the probability of 
word substitution in a query collection as a 
whole, which is used by Jones et al (2006b). 
Finally, the character type features also contri-
buted to reducing the error rate (lines A and E). 
In particular, the observation that the addition of 
semantic features without the character type fea-
tures (comparing lines B and E) did not improve 
the error rate indicates that the character type 
features are also important in bringing about the 
contribution of semantic features.   
Figure 2 displays the test data precision/recall 
curve of one of the runs of MART that uses all 
features. The x-axis of the graph is the confi-
dence score of classification P(c|q), which was 
set to 0.5 for the results in Table 3. At this confi-
dence, the model achieves 70% precision with 
the recall slightly higher than 60%. In the graph, 
we observe a familiar trade-off between preci-
sion and recall, which is useful for practical ap-
plications that may favor one over the other.  
In order to find out where the weaknesses of 
our classifiers lie, we performed a manual error 
analysis on the same MART run whose results 
are shown in Figure 2. Most of the classification 
errors are false negatives, i.e., the model failed to 
predict a case of term variation as such. The most 
conspicuous error is the failure to capture ab-
breviations, such as failing to capture the altera-
tion between ?????  juujoochuugakkou 
'Juujoo middle school' and ??? juujoochuu, 
which our edit distance-based features fail as the 
length difference between a term and its abbrevi-
ation is significant. Addition of more targeted 
features for this subclass of term variation (e.g., 
Okazaki et al, 2008a) is called for, and will be 
considered in future work. Mistakes in the Ro-
manization of kanji characters were not always 
punished as the query and the candidate string 
may contain the same mistake, but when they 
occurred in either in the query or the candidate 
string (but not in both), the result was destruc-
tive: for example, we assigned a wrong Romani-
zation on ??? as suiginnakari ?mercury lamp?, 
as opposed to the correct suiginntou, which caus-
es the failure to capture the alteration with ??
? suiginntou, (a misspelling of ???). Using 
N-best (N>1) candidate pronunciations for kanji 
terms or using all possible pronunciations for 
kanji characters might reduce this type of error. 
Finally, the features of our models are the edit 
distance functions themselves, rather than the 
individual edit rules or operations. Using these 
individual operations as features in the classifica-
tion task directly has been shown to perform well 
on spelling correction and normalization tasks 
(e.g., Brill and Moore, 2000; Okazaki et al, 
2008b). Okazaki et al?s (2008b) method of gene-
rating edit operations may not be viable for our 
purposes, as they assume that the original and 
candidate strings are very similar in their surface 
representation ? they target only spelling variants 
and inflection in English. One interesting future 
avenue to consider is to use the edit distance 
functions in our current model to select a subset 
of query-candidate pairs that are similar in terms 
of these functions, separately for the surface and 
Romanized forms, and use this subset to align 
the character strings in these query-candidate 
pairs as described in Brill and Moore (2000), and 
add the edit operations derived in this manner to 
the term variation identification classifier as fea-
tures.  
6 Conclusion 
In this paper we have addressed the problem of 
acquiring term variations in Japanese query logs 
for the purpose of query expansion. We generate 
term variation candidates efficiently by mining 
query log data, and our best classifier, based on 
the MART algorithm, can make use of both edit-
distance-based and semantic features, and can 
identify term variation with the precision of 70% 
at the recall slightly higher than 60%. Our next 
 
Figure 2: Precision/Recall Curve of MART 
0
10
20
30
40
50
60
70
80
90
100
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
confidence
p
r
e
c
is
io
n
/
r
e
c
a
ll
 
(
%
)
precision
recall
1491
goal is to use and evaluate the term variation col-
lected by the proposed method in an actual 
search scenario, as well as improving the per-
formance of our classifier by using individual, 
character-dependent edit operations as features in 
classification.  
 
References  
Ahmad, Farooq, and Grzegorz Kondrak. 2005. Learn-
ing a spelling error model from search query logs. 
In Proceedings of EMNLP, pp.955-962.  
Aitchison, J. and A. Gilchrist. 1987. Thesaurus Con-
struction: A Practical Manual. Second edition. 
ASLIB, London. 
Aramaki, Eiji, Takeshi Imai, Kengo Miyo, and Kazu-
hiko Ohe. 2008. Orthographic disambiguation in-
corporating transliterated probability. In Proceed-
ings of IJCNLP, pp.48-55. 
Baeza-Yates, Ricardo, and Berthier Ribeiro-Neto. 
1999. Modern Information Retrieval. Addison 
Wesley. 
Berger, A.L., S. A. D. Pietra, and V. J. D. Pietra. 1996. 
A maximum entropy approach to natural language 
processing. Computational Linguistics, 22(1): 39-
72. 
Bilac, Slaven, and Hozumi Tanaka. 2004. A hybrid 
back-transliteration system for Japanese. In Pro-
ceedings of COLING, pp.597-603. 
Brill, Eric, Gary Kacmarcik and Chris Brockett. 2001. 
Automatically harvesting katakana-English term 
pairs from search engine query logs. In Proceed-
ings of the Sixth Natural Language Processing Pa-
cific Rim Symposium (NLPRS-2001), pp.393-399. 
Brill, Eric, and Robert C. Moore. 2000. An improved 
error model for noisy channel spelling. In Proceed-
ings of ACL, pp.286-293. 
Chen, Qing, Mu Li and Ming Zhou. 2007. Improving 
query spelling correction using web search results. 
In Proceedings of EMNLP-CoNLL, pp.181-189. 
Craswell, Nick, and Martin Szummer. 2007. Random 
walk on the click graph. In Proceedings of SIGIR. 
Cucerzan, Silviu, and Eric Brill. 2004. Spelling cor-
rection as an iterative process that exploits the col-
lective knowledge of web users. In Proceedings of 
EMNLP, pp.293-300. 
Deerwester, S., S.T. Dumais, T. Landauer and 
Harshman. 1990. Indexing by latent semantic anal-
ysis. In Journal of the American Society for Infor-
mation Science, 41(6): 391-407. 
Dunning, Ted. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational 
Linguistics, 19(1): 61-74. 
Frakes, W.B. 1992. Stemming algorithm. In 
W.B.Frakes and R.Baeza-Yates (eds.), Information 
Retrieval: Data Structure and Algorithms, Chapter 
8. Prentice Hall. 
Friedman, J. 2001. Greedy function approximation: a 
gradient boosting machine. Annals of Statistics, 
29(5). 
Jones, Rosie, Benjamin Rey, Omid Madani and Wiley 
Greiner. 2006a. Generating query substitutions. In 
Proceedings of WWW, pp.387?396. 
Jones, Rosie, Kevin Bartz, Pero Subasic and Benja-
min Rey. 2006b. Automatically generating related 
aueries in Japanese. Language Resources and 
Evaluation 40: 219-232.  
Hastie, Trevor, Robert Tibshirani and Jerome Fried-
man. 2001. The Elements of Statistical Learning. 
Springer. 
Knight, Kevin, and Jonathan Graehl. 1998. Machine 
transliteration. Computational Linguistics, 24(4): 
599-612. 
Komachi, Mamoru and Hisami Suzuki. 2008. Mini-
mally supervised learning of semantic knowledge 
from query logs. In Proceedings of IJCNLP, 
pp.358?365. 
Li, Mu, Muhua Zhu, Yang Zhang and Ming Zhou. 
2006. Exploring distributional similarity based 
models for query spelling correction. In Proceed-
ings of COLING/ACL, pp.1025-1032. 
Li, Xiao, Ye-Yi Wang and Alex Acero. 2008. Learn-
ing query intent from regularized click graphs. In 
Proceedings of SIGIR.  
Masuyama, Takeshi, Satoshi Sekine, and Hiroshi Na-
kagawa. 2004. Automatic construction of Japanese 
katakana variant list from large corpus. In Proceed-
ings COLING, pp.1214-1219. 
Okazaki, Naoaki, Mitsuru Ishizuka and Jun?ichi Tsujii. 
2008a. A discriminative approach to Japanese ab-
breviation extraction. In Proceedings of IJCNLP.  
Okazaki, Naoaki, Yoshimasa Tsuruoka, Sophia Ana-
niadou and Jun?ichi Tsujii. 2008b. A discriminative 
candidate generator for string transformations. In 
Proceedings of EMNLP.  
Rasmussen, E. 1992. Clustering algorithm. In 
W.B.Frakes and R.Baeza-Yates (eds.), Information 
Retrieval: Data Structure and Algorithms, Chapter 
16. Prentice Hall. 
Salton, G., and C. Buckley. 1990. Improving retrieval 
performance by relevance feedback. Journal of the 
American Society for Information Science, 41(4): 
288-297. 
Sekine, Satoshi, and Hisami Suzuki. 2007. Acquiring 
ontological knowledge from query logs. In Pro-
ceedings of WWW, pp.1223-1224 
1492
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 861?869,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Semantic Tagging of Web Search Queries   Mehdi Manshadi Xiao Li University of Rochester Microsoft Research Rochester, NY Redmond, WA mehdih@cs.rochester.edu xiaol@microsoft.com       Abstract 
We present a novel approach to parse web search queries for the purpose of automatic tagging of the queries. We will define a set of probabilistic context-free rules, which generates bags (i.e. multi-sets) of words. Us-ing this new type of rule in combination with the traditional probabilistic phrase structure rules, we define a hybrid grammar, which treats each search query as a bag of chunks (i.e. phrases). A hybrid probabilistic parser is used to parse the queries. In order to take contextual information into account, a discriminative model is used on top of the parser to re-rank the n-best parse trees gen-erated by the parser. Experiments show that our approach outperforms a basic model, which is based on Conditional Random Fields. 1 Introduction     Understanding users? intent from web search queries is an important step in designing an intel-ligent search engine.  While it remains a chal-lenge to have a scientific definition of ''intent'', many efforts have been devoted to automatically mapping queries into different domains i.e. topi-cal classes such as product, job and travel (Broder et al 2007; Li et al 2008). This work goes beyond query-level classification. We as-sume that the queries are already classified into the correct domain and investigate the problem of semantic tagging at the word level, which is to assign a label from a set of pre-defined semantic labels (specific to the domain) to every word in the query. For example, a search query in the product domain can be tagged as:  cheap       garmin   streetpilot   c340       gps     |               |              |             |           | SortOrder  Brand      Model      Model    Type 
 Many specialized search engines build their in-dexes directly from relational databases, which contain highly structured information. Given a query tagged with the semantic labels, a search engine is able to compare the values of semantic labels in the query (e.g., Brand = ?garmin?) with its counterpart values in documents, thereby pro-viding users with more relevant search results.     Despite this importance, there has been rela-tively little published work on semantic tagging of web search queries. Allan and Raghavan (2002) and Barr et al (2008) study the linguistic structure of queries by performing part-of-speech tagging.  Pasca et al (2007) use queries as a source of knowledge for extracting prominent attributes for semantic concepts.  On the other hand, there has been much work on extracting structured information from larger text segments, such as addresses (Kushmerick 2001), bibliographic citations (McCallum et al 1999), and classified advertisements (Grenager  et al 2005),  among many others. The most widely used approaches to these problems have been sequential models including hidden Markov models (HMMs), maximum entropy Markov mod-els (MEMMs) (Mccallum 2000), and conditional random fields (CRFs) (Lafferty et al 2001) These sequential models, however, are not op-timal for processing web search queries for the following reasons.. The first problem is that the global constraints and long distance dependencies on state variables are difficult to capture using sequential models. Because of this limitation, Viola and Narasimhand (2007) use a discrimina-tive context-free (phrase structure) grammar for extracting information from semi-structured data and report higher performances over CRFs.      Secondly, sequential models treat the input text as an ordered sequence of words. A web search query, however, is often formulated by a user as a bag of keywords. For example, if a user is look-
861
ing for cheap garmin gps, it is possible that the query comes in any ordering of these three words. We are looking for a model that, once it observes this query, assumes that the other per-mutations of the words in this query are also likely. This model should also be able to handle cases where some local orderings have to be fixed as in the query buses from New York City to Boston, where the words in the phrases from New York city and to Boston have to come in the exact order.  The third limitation is that the sequential mod-els treat queries as unstructured (linear) se-quences of words. The study by Barr et al (2008) on Yahoo! query logs suggests that web search queries, to some degree, carry an underlying lin-guistic structure. As an example, consider a query about finding a local business near some location such as:  seattle wa drugstore 24/7 98109  This query has two constituents: the Business that the user is looking for (24/7 drugstore) and the Neighborhood (seattle wa 98109). The model should not only be able to recognize the two con-stituents but it also needs to understand the struc-ture of each constituent. Note that the arbitrary ordering of the words in the query is a big chal-lenge to understanding the structure of the query. The problem is not only that the two constituents can come in either order, but also that a sub-constituent such as 98109 can also be far from the other words belonging to the same constitu-ent. We are looking for a model that is able to generate a hierarchical structure for this query as shown in figure (1).  The last problem that we discuss here is that the two powerful sequential models i.e. MEMM and CRF are discriminative models; hence they are highly dependent on the training data. Prepar-ing labeled data, however, is very expensive. Therefore in cases where there is no or a small amount of labeled data available, these models do a poor job.   In this paper, we define a hybrid, generative grammar model (section 3) that generates bags of phrases (also called chunks in this paper). The chunks are generated by a set of phrase structure (PS) rules. At a higher level, a bag of chunks is generated from individual chunks by a second type of rule, which we call context-free multiset generating rules. We define a probabilistic ver-sion of this grammar in which every rule has a probability associated with it. Our grammar model eliminates the local dependency assump-tion made by sequential models and the ordering 
constraints imposed by phrase structure gram-mars (PSG). This model better reflects the under-lying linguistic structure of web search queries. The model?s power, however, comes at the cost of increased time complexity, which is exponen-tial in the length of the query. This, is less of an issue for parsing web search queries, as they are usually very short (2.8 words/query in average (Xue et al, 2004)).   Yet another drawback of our approach is due to the context-free nature of the proposed gram-mar model. Contextual information often plays a big role in resolving tagging ambiguities and is one of the key benefits of discriminative models such as CRFs. But such information is not straightforward to incorporate in our grammar model. To overcome this limitation, we further present a discriminative re-ranking module on top of the parser to re-rank the n-best parse trees gen-erated by the parser using contextual features. As seen later, in the case where there is not a large amount of labeled data available, the parser part is the dominant part of the module and performs reasonably well. In cases where there is a large amount of labeled data available, the discrimina-tive re-ranking incorporates into the system and enhances the performance. We evaluate this model on the task of tagging search queries in the product domain. As seen later, preliminary ex-periments show that this hybrid genera-tive/discriminative model performs significantly better than a CRF-based module in both absence and presence of the labeled data. The structure of the paper is as follows. Sec-tion 2 introduces a linguistic grammar formalism that motivates our grammar model. In section 3, we define our grammar model. In section 4 we address the design and implementation of a parser for this kind of grammar. Section 5 gives an example of such a grammar designed for the purpose of automatic tagging of queries. Section 6 discusses motivations for and benefits of run-ning a discriminative re-ranker on top of the parser. In section 7, we explain the evaluations 
 Figure 1. A simple grammar for product domain  
862
and discuss the results. Section 8 summarizes this work and discusses future work. 2 ID/LP Grammar Context-free phrase structure grammars are widely used for parsing natural language. The adequate power of this type of grammar plus the efficient parsing algorithms available  for it has made it very popular.  PSGs treat a sentence as an ordered sequence of words. There are however natural languages that are free word order. For example, a three-word sentence consisting of a subject, an object and a verb in Russian, can occur in all six possible orderings. PSGs  are not a well-suited model for this type of language, since six different PS-rules must be defined in order to cover such a simple structure. To address this issue, Gazdar (1985) introduced the concept of ID/LP rules within the framework of Generalized Phrase Structure Grammar (GPSG). In this framework, Immediate Dominance or ID rules are of the form: (1) A? B, C This rule specifies that a non-terminal A can be rewritten as B and C, but it does not specify the order. Therefore A can be rewritten as both BC and CB. In other words the rule in (1) is equivalent to two PS-rules: (2) A ? BC A ? CB Similarly one ID rule will suffice to cover the simple subject-object-verb structure in Russian: (3) S ? Sub, Obj, Vrb However even in free-word-order languages, there are some ordering restrictions on some of the constituents. For example in Russian an adjective always comes before the noun that it modifies. To cover these ordering restrictions, Gazdar defined Linear Precedence (LP) rules. (4) gives an example of a linear precedence rule: (4) ADJ < N This specifies that ADJ always comes before N when both occur on the right-hand side of a single rule.     Although very intuitive, ID/LP rules are not widely used in the area of natural language processing. The main reason is the time-complexity issue of ID/LP grammar. It has been shown that parsing ID/LP rules is an NP-complete problem (Barton 1985). Since the length of a natural language sentence can easily reach 30-40 (and sometimes even up to 100) words, ID/LP grammar is not a practical model for natural language syntax. In our case, however, 
the time-complexity is not a bottleneck as web search queries are usually very short (2.8 words per query in average). Moreover, the nature of ID rules can be deceptive as it might appear that ID rules allow any reordering of the words in a valid sentence to occur as another vaild sentence of the language. But in general this is not the case. For example consider a grammar with only two ID rules given in (5) and consider S as the start symbol: (5) S ?  B, c B ?  d, e It can be easily verified that dec is a sentence of the language but dce is not. In fact, although the permutation of subconstituents of a constituent is allowed, a subconstituent can not be pulled out from its mother consitutent and freely move within the other constituents. This kind of movement however is a common behaviour in web search queries as shown in figure (1). It means that even ID rules are not powerful enough to model the free-word-order nature of web search queries.  This leads us to define to a new type of grammar model. 3 Our Grammar Model 3.1  The basic model We propose a set of rules in the form: (6) S ?  {B, c} B ?  {D, E} D ?  {d} E ?  {e} which can be used to generate multisets of words. For the notation convenience and consistancy, throughout this paper, we show terminals and non-terminals by lowercase and uppercase letters, respectively and sets and multisets by bold font uppercase letters. Using the rules in (6) a sentence of the language (which is a multiset in this model) can be derived as follows: (7) S ? {B, c} ? {D, E, c} ? {D, e, c}? {d, e, c} Once the set is generated, it can be realized as any of the six permutation of d, e, and c. Therefore a single sequence of derivations can lead to six different strings of words. As another example consider the grammar in (8). (8) Query ?  {Business, Location} Business ?  {Attribute, Business} Location ? {City, State} Business ?  {drugstore} | {Resturant} Attribute? {Chinese} | {24/7} City? {Seattle} | {Portland} State? {WA} | {OR} 
863
where Query is the start symbol and by A ? B|C we mean two differnet rules A ? B and A ? C. Figures (2) and (3) show the tree structures for the queries Restaurant Rochester Chinese MN, and Rochester MN Chinese Restaurant, respectively. As seen in these figures, no matter what the order of the words in the query is, the grammar always groups the words Resturant and Chinese together as the Business and the words Rochester and MN together as the Location. It is important to notice that the above grammars are context-free as every non-terminal A, which occurs on the left-hand side of a rule r, can be replaced with the set of terminals and non-terminals on the right-hand side of r, no matter what the context in which A occurs is.  More formally we define a Context-Free multiSet generating Grammar (CFSG) as a 4-tuple G=(N, T, S, R) where ? N is a set of non-terminals;  ? T is a set of terminals; ? S ? N is a special non-terminal called start symbol,  ? R is a set of rules {Ai? Xj} where Ai is a non-terminal and Xj is a set of terminals and non-terminals. Given two multisets Y and Z over the set N ?  T, we say Y dervies Z (shown as Y ? Z) iff there exists A, W, and X such that: Y = W + {A}1 Z = W + X A? X ? R Here ?* is defined as the reflexive transitive closure of ?. Finally we define the language of multisets generated by the grammar G (shown as L(G)) as L = { X | X is a multiset over N?T and S ?*X} The sequence of ? used to derive X from S is called a derivation of X. Given the above                                                 1 If X and Y are two multisets, X+Y simply means append-ing X to Y. For example {a, b, a} + {b, c, d} = {a, b, a, b, c, d}. 
definitions, parsing a multiset X means to find all (if any) the derivations of  X from S. 2 3.2 Probabilisic CFSG Very often a sentence in the language has more than one derivation, that is the sentence is syntactically ambiguous. One natural way of resolving the ambiguity is using a probabilistic grammar. Analogous to PCFG (Manning and Sch?tze 1999), we define the probabilistic version of a CFSG, in which every rule Ai?Xj has a probability P(Ai?Xj) and for every non-terminal Ai, we have: (9) ?j P(Ai? Xj) = 1 Consider a sentence w1w2?wn, a parse tree T of this sentence, and an interior node v in T labeled with Av and assume that v1, v2, ?vk are the children of the node v in T. We define: (10) ?(v) = P(Av? {Av1? Avk})?(v1) ? ?(vk) with the initial conditions ?(wi)=1. If u is the root of the tree T we have: (11) P(w1w2?wn , T) = ?(u) The parse tree that the probabilistic model assigns to the sentence is defined as: (12) Tmax = argmaxT (P(w1w2?wn , T))  where T ranges over all possible parse trees of the sentence. 4 Parsing Algorithm 4.1 Deterministic parser The parsing algorithm for the CFSG is straight-forward. We used a modified version of the Bot-tom-Up Chart Parser for the phrase structure grammars (Allen 1995, see 3.4). Given the grammar G=(N,T,S,R) and the query q=w1w2?wn, the algorithm in figure (4) is used to parse q. The algorithm is based on the concept of an active arc. An active arc is defined as a 3?                                                2 Every sentence of a language corresponds to a vector of |T| integers where the kth element represents how many times the kth terminal occurs in the multi-set. In fact, the languages defined by grammars are not interesting but the derivations are.   
 Figure 2. A CFSG parse tree  
 Figure 3. A CFSG parse tree  
864
tuple (r, U, I) where r is a rule A ? X in R, U is a subset of X, and I is a subset of {1, 2 ?n} (where n is the number of words in the query). This ac-tive arc tries to find a match to the right-hand side of r (i.e. X) and suggests to replace it with the non-terminal A. U contains the part of the right-hand side that has not been matched yet. There-fore when an arc is newly created U=X. Equiva-lently, X\U3 is the part of the right hand side that has so far been matched with a subset of words in the query, where I stores the positions of these words in q.  An active arc is completed when U=?. Every completed active arc can be reduced to a tuple (A, I), which we call a constituent. A constituent (A, I) shows that the non-terminal A matches the words in the query that are positioned at the numbers in I. Every constituent that is built by the parser is stored in a data structure called chart and remains there throughout the whole process. Agenda is another data structure that temporarily stores the constituents. At initialization step, the constituents (w1, {1}), ? (wn, {n}) are added to both chart and agenda. At each iteration, we pull out a constituent from the agenda and try to find a match to this constituent from the remaining list of terminals and non-terminals on the right-hand side of an active arc. More precisely, given a constituent c=(A, I) and an active arc ? = (r:B?X, U, J), we check if A ? U and I ? J = ?; if so, ? is extendable by c, therefore we extend ? by removing A from U and appending I to J. Note that the extension process keeps a copy of every active arc before it extends it. In practice every active arc and every constituent keep a set of pointers to its children constituents (stored in chart). This information is necessary for the ter-mination step in order to print the parse trees. The algorithm succeeds if there is a constituent in the chart that corresponds to the start symbol and covers all the words in the query, i.e. there is a constituent of the form (S, {1,2,?.n}) in the chart. 4.2 Probabilistic Parser The algorithm given in figure (4) works for a de-terministic grammar. As mentioned before, we use a probabilistic version of the grammar. Therefore the algorithm is modified for the prob-abilistic case. The probabilistic parser keeps a probability p for every active arc and every con-stituent: ? = (r, U, J, p? )                                                 3 A\B is defined as {x | x ? A & x ? B} 
c =(A, I, pc ) When extending ? using c, we have: (13) p? ? p? pc When creating c from the completed active arc ? : (14) pc ? p? p(r) Although search queries are usually short, the running time is still an issue when the length of the query exceeds 7 or 8. Therefore a couple of techniques have been used to make the na?ve al-gorithm more efficient. For example we have used pruning techniques to filter out structures with very low probability. Also, a dynamic pro-gramming version of the algorithm has been used, where for every subset I of the word posi-tions and every non-terminal A only the highest-ranking constituent c=(A, I, p) is kept and the rest are ignored. Note that although more efficient, the dynamic programming version is still expo-nential in the length of the query. 5 A grammar for semantic tagging  As mentioned before, in our system queries are already classified into different domains like movies, books, products, etc. using an automatic query classifier. For every domain we have a schema, which is a set of pre-defined tags. For example figure (5) shows an example of a schema for the product domain. The task defined for this system is to automatically tag the words in the query with the tags defined in the schema: cheap       garmin   streetpilot   c340       gps |                |               |               |            | SortOrder  Brand      Model      Model    Type  
Initialization: For each word wi in q add (wi, {i}) to Chart and to Agenda  For all r: A?X in R, create an active arc (r, X, {}) and add it to the list of active arcs.  Iteration Repeat Pull a constituent c = (A, I) from Agenda For every active arc ? =(r:B?X, U, I)   Extend ? using c if extendable   If U=? add (B, I) to Chart and to Agenda Until Agenda is empty   Termination For every item c=(S, {1..n}) in Chart, return the tree rooted at c. Figure 4. An algorithm for parsing deterministic CFSG 
865
We mentioned that one of the motivations of parsing search queries is to have a deeper under-standing of the structure of the query. The evaluation of such a deep model, however, is not an easy task. There is no Treebank available for web search queries. Furthermore, the definition of the tree structure for a query is quite arbitrary. Therefore even when human resources are avail-able, building such a Treebank is not a trivial task. For these reasons, we evaluate our grammar model on the task of automatic tagging of queries for which we have labeled data available. The other advantage of this evaluation is that there exists a CRF-based module in our system used for the task of automatic tagging. The perform-ance of this module can be considered as the baseline for our evaluation.  We have manually designed a grammar for the purpose of automatic tagging. The resources available for training and testing were a set of search queries from the product domain. There-fore a set of CFSG rules were written for the product domain. We defined very simple and intuitive rules (shown in figure 6) that could eas-ily be generalized to the other domains  Note that Type, Brand, Model, ? could be either pre-terminals generating word tokens, or non-terminals forming the left-hand side of the phrase structure rules. For the product domain, Type and Attribute are generated by a phrase structure grammar. Model and Attribute may also be generated by a set of manually designed regu-
lar expressions. The rest of the tags are simply pre-terminals generating word tokens. Note that we have a lexicon, e.g.., a Brand lexicon, for all the tags except Type and Attribute. The model, however, extends the lexicon by including words discovered from labeled data (if available). The gray color for a non-terminal on the right-hand side (RHS) of some rule means that the non-terminal is optional (see Query rule in figure (6)). We used the optional non-terminals to make the task of defining the grammar easier. For example if we consider a rule with n optional non-terminals on its RHS, without optional non-terminals we have to define 2n different rules to have an equivalent grammar. The parser can treat the optional non-terminals in different ways such as pre-compiling the rules to the equivalent set of rules with no optional non-terminal, or directly handling optional non-terminals during the pars-ing. The first approach results in exponentially many rules in the system, which causes sparsity issues when learning the probability of the rules. Therefore in our system the parser handles op-tional non-terminals directly. In fact, every non-terminal has its own probability for not occurring on the RHS of a rule, therefore the model learns n+1 probabilities for a rule with n optional non-terminals on its RHS: one for the rule itself and one for every non-terminal on its RHS. It means that instead of learning 2n probabilities for 2n dif-ferent rules, the model only learns n+1 probabili-ties. That solves the sparsity problem, but causes another issue which we call short length prefer-ence. This occurs because we have assumed that the probability of a non-terminal being optional is independent of other optional non-terminals. Since for almost all non-terminals on the RHS of the query rule, the probability that the non-terminal does not exist in an instance of a query is higher than 0.5, a null query is the most likely query that the model generates! We solve this problem by conditioning the probabilities on the length of queries. This brings a trade-off between the two other alternatives: ignoring sparsity prob-lem to prevent making many independence as-sumptions and making a lot of independence assumptions to address the sparsity issue.  Unlike sequential models, the grammar model is able to capture critical global con-straints. For example, it is very unlikely for a query to have more than one Type, Brand, etc. This is an important property of the product que-ries that can help to resolve the ambiguity in many cases. In practice, the probability that the model learns for a rule like:  
Query ? {Brand*, Product*, Model*, ?} Brand* ? {Brand} Brand* ? {Brand*, Brand} Type* ? {Type} Type* ? {Type*, Type} Model* ? {Model} Model* ? {Model*, Model} ? Figure 6. A simple grammar for product domain  
Type: Camera, Shoe, Cell phone, ?  Brand: Canon, Nike, At&t, ? Model: dc1700, powershot, ipod nano Attribute: 1GB, 7mpixel, 3X, ? BuyingIntenet: Sale, deal, ? ResearchIntent:  Review, compare, ? SortOrder: Best, Cheap, ? Merchant:  Walmart, Target, ?  Figure 5. Example of schema for product domain  
866
Type* ? {Type*, Type} compared to the rule: Type* ? Type is very small; the model penalizes the occurrence of more than one Type in a query. Figure (7a) shows an example of a parse tree generated for the query ?Canon vs Sony Camera? in which B, Q, and T are abbreviations for Brand, Query, and Type, and U is a special tag for the words that does not fall into any other tag categories and have been left unlabeled in our corpus such as a, the, for, etc. Therefore the parser assigns the tag sequence B U B T to this query. It is true that the word ?vs? plays a critical role in this query, rep-resenting that the user?s intention is to compare the two brands; but as mentioned above in our labeled data such words has left unlabeled. The general model, however, is able to easily capture these sorts of phenomena. A more careful look at the grammar shows that there is another parse tree for this query as shown in figure (7b). These two trees basically represent the same structure and generate the same sequence of tags. The number of trees gen-erated for the same structure increases exponen-tially with the number of equal tags in the tree. To prevent this over-generation we used rules analogous to GPSG?s LP rules such as: B* < B which allows only a unique way of generating a bag of the Brand tags.  Using this LP rule, the only valid tree for the above query is the one in figure (7a). 6 Discriminative re-ranking By using a context-free grammar, we are missing a great source of clues that can help to resolve ambiguity. Discriminative models, on the other hand, allow us to define numerous features, which can cooperate to resolve the ambiguities. Similar studies in parsing natural language sen-
tences (Collins and Koo 2005) have shown that if, instead of taking the most likely tree structure generated by a parser, the n-best parse trees are passed through a discriminative re-ranking mod-ule, the accuracy of the model will increase sig-nificantly. We use the same idea to improve the performance of our model. We run a Support Vector Machine (SVM) based re-ranking module on top of the parser. Several contextual features (such as bigrams) are defined to help in disam-biguation. This combination provides a frame-work that benefits from the advantages of both generative and discriminative models. In particu-lar, when there is no or a very small amount of labeled data, a parser could still work by using unsupervised learning approaches to learn the rules, or by simply using a set of hand-built rules (as we did above for the task of semantic tag-ging). When there is enough labeled data, then a discriminative model can be trained on the la-beled data to learn contextual information and to further enhance the tagging performance.  7 Evaluation Our resources are a set of 21000 manually la-beled queries, a manually designed grammar, a lexicon for every tag (except Type and Attribute), and a set of regular expressions defined for Mod-els and Attributes. Note that with a grammar similar to the one in figure (6), generating a parse tree from a labeled query is straightforward. Then the parser is trained on the trees to learn the pa-rameters of the model (probabilities in this case). We randomly extracted 3000, out of 21000, queries as the test set and used the remaining 18000 for training. We created training sets with different sizes to evaluate the impact of training data size on tagging performance.  Three modules were used in the evaluation: the CRF-based model4, the parser, and the parser plus the SVM-based re-ranking. Figure (8) shows the learning curve of the word-level F-score for all the three modules. As seen in this plot, when there is a small amount of training data, the parser performs better than the CRF module and parser+SVM module performs better than the other two. With a large amount of training data, the CRF and parser almost have the same per-formance. Once again the parser+SVM module                                                 4 The CRF module also uses the lexical resources and regu-lar expressions. In fact, it applies a deterministic context free grammar to the query to find all the possible groupings of words into chunks and uses this information as a set of fea-tures in the system. 
 Figure 7. Two equivalent CFSG parse trees   
867
outperforms the other two. These results show that, as expected, the CRF-based model is more dependent on the training data than the parser. Parser+SVM always performs at least as well as the parser-only module even with a very small set of training data. This is because the rank given to every parse tree by the parser is used as a feature in the SVM module. When there is a very small amount of training data, this feature is dominant and the output of the re-reranking module is basically the same as the parser?s highest-rank output. Table (1) shows the per-formance of all three modules when the whole training set was used to train the system. The first three columns in the table show the word-level precision, recall, and F-score; and the last column represents the query level accuracy (a query is considered correct if all the words in the query have been labeled correctly). There are two rows for the parser+SVM in the table: one for n=2 (i.e. re-ranking the 2-Best trees) and one for n=10. It is interesting to see that even with the re-ranking of only the first two trees generated by the parser, the difference between the accuracy of the parser+SVM module and the parser-only module is quite significant. Re-ranking with a larger number of trees (n>10) did not increase performance significantly. 8 Summary We introduced a novel approach for deep parsing of web search queries. Our approach uses a grammar for generating multisets called a con-text-free multiset generating grammar (CFSG). We used a probabilistic version of this grammar. A parser was designed for parsing this type of grammar. Also a discriminative re-ranking mod-ule based on a support vector machine was used 
to take contextual information into account. We have used this system for automatic tagging of web search queries and have compared it with a CRF-based model designed for the same task.  The parser performs much better when there is a small amount of training data, but an adequate lexicon for every tag. This is a big advantage of the parser model, because in practice providing labeled data is very expensive but very often the lexicons can be easily extracted from the struc-tured data on the web (for example extracting movie titles from imdb or book titles from Ama-zon).  Our hybrid model (parser plus discriminative re-ranking), on the other hand, outperforms the other two modules regardless of the size of the training data.  The main drawback with our approach is to completely ignore the ordering. Note that al-though strict ordering constraints such as those imposed by PSG is not appropriate for modeling query structure, it might be helpful to take order-ing information into account when resolving am-biguity. We leave this for future work. Another interesting and practically useful problem that we have left for future work is to design an unsuper-vised learning algorithm for CFSG similar to its phrase structure counterpart: inside-outside algo-rithm (Baker 1979). Having such a capability, we are able to automatically learn the underlying structure of queries by processing the huge amount of available unlabeled queries. Acknowledgement We need to thank Ye-Yi Wang for his helpful advices. We also thank William de Beaumont for his great comments on the paper. 
References  
Allan, J. and Raghavan, H. (2002) Using Part-of-speech Patterns to Reduce Query Ambiguity, Pro-ceedings of SIGIR 2002, pp. 307-314. Allen, J. F. (1995) Natural Language Understanding, Benjamin Cummings. Baker, J. K. (1979) Trainable grammars for speech recognition. In Jared J. Wolf and Dennis H. Klatt, editors, Speech communication papers presented at the 97th Meeting of the Acoustical Society of America, MIT, Cambridge, MA. Barton, E. (1985) On the complexity of ID/LP rules, Computational Linguistics, Volume 11, Pages 205-218. 
 Figure 8. The learning curve for the three modules  
Train?No?=?18000?Test?No?=?3000? P? R? F? Q?CRF? 0.815? 0.812? 0.813? 0.509?Parser? 0.808? 0.814? 0.811? 0.494?Parser+SVM?(n?=?2)? 0.823? 0.827? 0.825? 0.531?Parser+SVM?(n?=?10)? 0.832? 0.835? 0.833? 0.555?Table 1. The results of evaluating the three modules 
868
Barr, C., Jones, R., Regelson, M., (2008) The Linguis-tic Structure of English Web-Search Queries, In Proceedings of EMNLP-08: conference on Empiri-cal Methods in Natural Language Processing. Broder, A., Fontoura, M., Gabrilovich, E., Joshi, A., Josifovski, V., and Zhang, T. (2007) Robust classi-fication of rare queries using web knowledge. In Proceedings of SIGIR?07 Collins, M., Koo, T., (2005) Discriminative Reranking for Natural Language Parsing, Computational Lin-guistics, v.31 p.25-70. Gazdar, G., Klein, E., Sag, I., Pullum, G., (1985) Gen-eralized Phrase Structure Grammar, Harvard Uni-versity Press. Grenager, T., Klein, D., and Manning, C. (2005) Un-supervised learning of field segmentation models for information extraction, In Proceedings of ACL-05. Kushmerick, N., Johnston, E., and McGuinness, S. (2001). Information extraction by text classifica-tion, In Proceedings of the IJCAI-01 Workshopon Adaptive Text Extraction and Mining. Li, X., Wang, Y., and Acero, A. (2008) Learning query intent from regularized click graphs. In Pro-ceedings of SIGIR?08 Manning, C., Sch?tze, H. (1999) Foundations of Sta-tistical Natural Language Processing, The MIT Press, Cambridge, MA. McCallum, A., Freitag, D., Pereira, F. (2000) Maxi-mum entropy markov models for information ex-traction and segmentation, Proceedings of the Seventeenth International Conference on Machine Learning, Pages: 591 - 598 McCallum, A., Nigam, K., Rennie, J., and Seymore, K. (1999) A machine learning approach to building domain-specific search engines, In IJCAI-1999. Pasca, M., Van Durme, B., and Garera, N.  (2007) The Role of Documents vs. Queries in Extracting Class Attributes from Text, ACM Sixteenth Conference on Information and Knowledge Management (CIKM 2007). Lisboa, Portugal. Viola, P., Narasimhan, M., Learning to extract infor-mation from semi-structured text using a discrimi-native context free grammar SIGIR 2005: 330-337. Xue, GR, HJ Zeng, Z Chen, Y Yu, WY Ma, WS Xi, WG Fan, (2004), Optimizing web search using web click-through data, Proceedings of the thirteenth ACM international conference.   
869
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 21?28,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Learning N-Best Correction Models from Implicit User Feedback  
in a Multi-Modal Local Search Application 
 
 
Dan Bohus, Xiao Li, Patrick Nguyen, Geoffrey Zweig 
Microsoft Research 
One Microsoft Way 
Redmond, WA, 98052 
{dbohus, xiaol, panguyen, gzweig}@microsoft.com 
 
 
 
 
 
 
Abstract 
We describe a novel n-best correction model 
that can leverage implicit user feedback (in 
the form of clicks) to improve performance in 
a multi-modal speech-search application. The 
proposed model works in two stages. First, the 
n-best list generated by the speech recognizer 
is expanded with additional candidates, based 
on confusability information captured via user 
click statistics. In the second stage, this ex-
panded list is rescored and pruned to produce 
a more accurate and compact n-best list. Re-
sults indicate that the proposed n-best correc-
tion model leads to significant improvements 
over the existing baseline, as well as other tra-
ditional n-best rescoring approaches.  
1 Introduction 
Supported by years of research in speech recogni-
tion and related technologies, as well as advances 
in mobile devices, speech-enabled mobile applica-
tions are finally transitioning into day-to-day use. 
One example is Live Search for Windows Mobile 
(2008), a speech-enabled application that allows 
users to get access to local information by speaking 
a query into their device. Several other systems 
operating in similar domains have recently become 
available (TellMeByMobile, 2008; Nuance Mobile 
Search, 2008; V-Lingo Mobile, 2008; VoiceSignal 
Search, 2008.) 
Traditionally, multi-modal systems leverage the 
additional input channels such as text or buttons to 
compensate for the current shortcomings of speech 
recognition technology. For instance, after the user 
speaks a query, the Live Search for Windows Mo-
bile application displays a confirmation screen that 
contains the n-best recognition results. The user 
selects the correct hypothesis using the buttons on 
the device, and only then the system displays the 
corresponding search results (see Figure 1.) 
We argue that ideally multi-modal systems 
could use the additional, more accurate input chan-
nels not only for confirmation or immediate cor-
rection, but also to learn from the interaction and 
improve their performance over time, without ex-
plicit human supervision. For example, in the inte-
raction paradigm described above, apart from 
providing the means for selecting the correct rec-
ognition result from an n-best list, the user click on 
a hypothesis can provide valuable information 
about the errors made by system, which could be 
exploited to further improve performance.  
Consider for instance the following numbers 
from an analysis of logged click data in the Live 
Search for Windows Mobile system. Over a certain 
period of time, the results Beer and Gear were dis-
played together in an n-best list 122 times. Out of 
these cases, Beer was clicked 67% of the time, and 
Gear was never clicked. In 25% of the cases when 
Beer was selected, Gear was incorrectly presented 
above (i.e. higher than) Beer in the n-best list. 
More importantly, there are also 82 cases in which 
Gear appears in an n-best list, but Beer does not. A 
manual inspection reveals that, in 22% of these 
cases, the actual spoken utterance was indeed Beer. 
The clicks therefore indicate that the engine often 
misrecognizes Gear instead of Beer.  
21
Ideally, the system should be able to take advan-
tage of this information and use the clicks to create 
an automatic positive feedback loop. We can envi-
sion several ways in which this could be accom-
plished. A possible approach would be to use all 
the clicked results to adapt the existing language or 
acoustic models. Another, higher-level approach is 
to treat the recognition process as a black-box, and 
use the click feedback (perhaps also in conjunction 
with other high-level information) to post-process 
the results recognition results. 
While both approaches have their merits, in this 
work we concentrate on the latter paradigm. We 
introduce a novel n-best correction model that le-
verages the click data to improve performance in a 
speech-enabled multi-modal application. The pro-
posed model works in two stages. First, the n-best 
list generated by the speech recognizer is expanded 
with additional candidates, based on results confu-
sability information captured by the click statistics. 
For instance, in the 82 cases we mentioned above 
when Gear was present in the n-best list but Beer 
was not, Beer (as well as potentially other results) 
would also be added to form an expanded n-best 
list. The expanded list is then rescored and pruned 
to construct a corrected, more accurate n-best list.  
The proposed approach, described in detail in 
Section 3, draws inspiration from earlier work in 
post-recognition error-correction models (Ringger 
and Allen, 1996; Ringger and Allen, 1997) and n-
best rescoring (Chotimongkol and Rudnicky, 2001; 
Birkenes et al, 2007). The novelty of our approach 
lies in: (1) the use of user click data in a deployed 
multi-modal system for creating a positive feed-
back loop, and (2) the development of an n-best 
correction model based on implicit feedback that 
outperforms traditional rescoring-only approaches. 
Later on, in Section 5, we will discuss in more de-
tail the relationship of the proposed approach to 
these and other works previously reported in the 
literature.  
Before moving on to describe the n-best correc-
tion model in more detail, we give a high-level 
overview of Live Search for Windows Mobile, the 
multi-modal, mobile local search application that 
provided the test-bed for evaluating this work.  
2 Live Search for Windows Mobile  
Live Search for Windows Mobile is an application 
that enables local web-search on mobile devices. In 
its current version, it allows users to find informa-
tion about local businesses and restaurants, to ob-
tain driving directions, explore maps, view current 
traffic, get movie show-times, etc. A number of 
screen-shots are illustrated in Figure 1. 
Recently, Live Search for Windows Mobile has 
been extended with a speech interface (notice the 
Speak button assigned to the left soft-key in Figure 
1.a.) The speech-based interaction with the system 
proceeds as follows: the user clicks the Speak but-
ton and speaks the name of a local business, for 
instance A-B-C Hauling, or a general category such 
as Vietnamese Restaurants. The application end-
points the audio and forwards it over the data 
channel to a server (Figure 1.b.) Recognition is 
performed on the server side, and the resulting n-
best list is sent back to the client application, where 
it is displayed to the user (Figure 1.c.) The user can 
select the correct item from the n-best list, re-speak 
the request, or abandon the interaction altogether 
by pressing Cancel. Once the user selects an item in 
the n-best list, the corresponding search results are 
displayed (Figure 1.d.) 
(a) (b) (c) (d) 
Figure 1. Windows Live Search for Mobile. (a) initial screen; (b) user is speaking a request; (c) n-best list 
is presented; (d) final search results are displayed 
22
Apart from business names, the system also 
handles speech input for addresses, as well as 
compound requests, such as Shamiana Restaurant 
in Kirkland, Washington. For the latter cases, a 
two-tier recognition and confirmation process is 
used. In the first stage a location n-best list is gen-
erated and sent to the client for confirmation. After 
the user selects the location, a second recognition 
stage uses a grammar tailored to that specific loca-
tion to re-recognize the utterance. The client then 
displays the final n-best list from which the user 
can select the correct result. 
Several details about the system architecture and 
the structure of the recognition process have been 
omitted here due to space considerations. For the 
interested reader, a more in-depth description of 
this system is available in (Acero et al, 2008).  
3 Approach 
We now turn our attention to the proposed n-best 
correction model 
3.1 Overview 
The model works in two stages, illustrated in Fig-
ure 2. In the first stage the n-best list produced by 
the speech recognizer is expanded with several 
alternative hypotheses. In the second stage, the 
expanded n-best list is rescored to construct the 
final, corrected n-best list.  
The n-best expansion step relies on a result con-
fusion matrix, constructed from click information. 
The matrix, which we will describe in more detail 
in the following subsection, contains information 
about which result was selected (clicked) by the 
user when a certain result was displayed. For in-
stance, in the example from Figure 2, the matrix 
indicates that when Burlington appeared in the n-
best list, Bar was clicked once, Bowling was 
clicked 13 times, Burger King was clicked twice, 
and Burlington was clicked 15 times (see hashed 
row in matrix.) The last element in the row indi-
cates that there were 7 cases in which Burlington 
was decoded, but nothing (?) was clicked. Essen-
tially, the matrix captures information about the 
confusability of different recognition results.  
The expansion step adds to an n-best list gener-
ated by the recognizer all the results that were pre-
viously clicked in conjunction with any one of the 
items in the given n-best list. For instance, in the 
example from Figure 2, the n-best list contains 
Sterling, Stirling, Burlington and Cooling. Based 
on the confusion matrix, this list will be expanded 
to also include Bar, Bowling, Burger King, Tow-
ing, and Turley. In this particular case, the correct 
recognition result, Bowling, is added in the ex-
panded n-best list.  
In the final step, the expanded list is rescored. In 
the previous example, for simplicity of explana-
tion, a simple heuristic for re-scoring was used: 
add all the counts on the columns corresponding to 
each expanded result. As a consequence, the cor-
Burlington 
Cooling 
Sterling 
Stirling 
0  ?   7    0     0    ?     0    0  ?   1   0    9 
0  ?   4    0     0    ?   10    1  ?   2   2    5 
0  ?   4    0     0    ?     4    1  ?   0   0    9 
B
u
rl
in
g
to
n
 
B
o
w
li
n
g
 
B
u
rg
er
 K
in
g
 
T
o
w
in
g
 
T
u
rl
ey
 
S
ti
rl
in
g
 
B
ar
 
S
te
rl
in
g
 
Sterling 
Stirling 
Burlington 
Cooling + ? 
Bowling  28 
Burlington  15 
Sterling  14 
Towing  3 
Burger King  2 
Stirling  2 
Turley  2 
Bar  1 
 
Bar 
Bowling 
Burger King 
Burlington 
Sterling  
Stirling 
Towing  
Turley 
 
? 
Result Confusion Matrix 
Initial  
N-Best 
Expanded 
N-Best 
Corrected 
(expanded & 
rescored) 
N-Best 
Figure 2. A confusion-based n-best correction model 
1  ? 13    2   15    ?     0    0  ?   0   0    7 
?
 
Stage 1: Expansion Stage 2: Rescoring 
23
rect recognition result, Bowling, was pushed to the 
top of the n-best list.  
We begin by formally describing the construc-
tion of the results confusability matrix and the ex-
pansion process in the next two sub-sections. Then, 
we describe three rescoring approaches. The first 
one is based on an error-correction model con-
structed from the confusion matrix. The other two, 
are more traditional rescoring approaches, based 
on language model adaptation.  
3.2 The Result Confusion Matrix 
The result confusion matrix is computed in a sim-
ple traversal of the click logs. The rows in the ma-
trix correspond to decoded results, i.e. results that 
have appeared in an n-best list. The columns in the 
matrix correspond to clicked (or intended) results, 
i.e. results that the user has clicked on in the n-best 
list. The entries at the intersection of row ? and 
column ? correspond to the number of times result 
? was clicked when result ? was decoded: 
 
?? ,? = #(??????? = ?, ??????? = ?). 
 
In addition, the last column in the matrix, de-
noted ? contains the number of times no result was 
clicked when result ? was displayed: 
 
?? ,? = #(??????? = ?, ??????? = ?). 
 
The rows in the matrix can therefore be used to 
compute the maximum likelihood estimate for the 
conditional probability distribution: 
 
???(?|?) =
?? ,?
 ?? ,??
 . 
 
The full dimensions of the result confusion ma-
trix can grow very large since the matrix is con-
structed at the result level (the average number of 
words per displayed result is 2.01). The number of 
rows equals the number of previously decoded re-
sults, and the number of columns equals the num-
ber of previously clicked results. However, the 
matrix is very sparse and can be stored efficiently 
using a sparse matrix representation. 
3.3 N-Best Expansion 
The first step in the proposed n-best correction 
model is to expand the initial n-best list with all 
results that have been previously clicked in con-
junction with the items in the current n-best list. 
Let?s denote by ? = {??}?=1..?  the initial n-best 
list produced by the speech recognizer. Then, the 
expanded n-best list ?? will contain all ?? , as well 
as all previously clicked results ? such that there 
exists ? with ??? ,? > 0. 
3.4 Confusion Matrix Based Rescoring  
Ideally, we would like to rank the hypotheses in 
the expanded list ?? according to ?(?|?), where ? 
represents the intended result and ? represents the 
acoustics of the spoken utterance. This can be re-
written as follows: 
 
                       ? ? ? =  ?(?|?) ? ?(?|?)? .             [1] 
 
The first component in this model is an error-
correction model ?(?|?). This model describes the 
conditional probability that the correct (or in-
tended) result is ? given that result ? has been de-
coded. While this conditional model cannot be 
constructed directly, we can replace it by a proxy - 
?(?|?), which models the probability that the re-
sult ? will be clicked, given that result ? was de-
coded. As mentioned earlier in subsection 3.2, this 
conditional probability distribution can be com-
puted from the result confusion matrix. In replac-
ing ? ? ?  with ?(?|?), we are making the 
assumption that the clicks correspond indeed to the 
correct, intended results, and to nothing else1. 
Notice that the result confusion matrix is gener-
ally very sparse. The maximum likelihood estima-
tor ???(?|?) will therefore often be inappropriate. 
To address this data sparsity issue, we linearly in-
terpolate the maximum likelihood estimator with 
an overall model ??(?|?): 
 
? ? ? =  ???? ? ? + (1? ?)?? ? ? . 
 
The overall model is defined in terms of two 
constants, ? and ?, as follows: 
 
?? ? ? =  
?, ?? ? = ?
?, ?? ? ? ?
  
 
where ? is the overall probability in the whole 
dataset of clicking on a given decoded result, and 
? is computed such that ?? ? ?  normalizes to 1. 
                                                          
1 While this assumption generally holds, we have also ob-
served cases where it is violated: sometimes users (perhaps 
accidentally) click on an incorrect result; other times the cor-
rect result is in the list but nothing is clicked (perhaps the user 
was simply testing out the recognition capabilities of the sys-
tem, without having an actual information need) 
24
Finally, the ? interpolation parameter is determined 
empirically on the development set.  
The second component in the confusion based 
rescoring model from equation [1] is ?(?|?). This 
is the recognition score for hypothesis ?. The n-
best rescoring model from [1] becomes: 
 
? ? ? =   ???? ? ?? + (1? ?)?? ? ??  ? ?(?? |?)
????
 
3.5 Language Model Based Rescoring 
A more traditional alternative for n-best rescoring 
is to adapt the bigram language model used by the 
system in light of the user click data, and re-rank 
the decoded results by: 
 
? ? ? ? ? ??  ? ? ? ? ?? ?(??) 
 
Here ? ? ??  is the acoustic score assigned by 
the recognizer to hypothesis ?? , and ?(??) is the 
adapted language model score for this hypothesis.  
A simple approach for adapting the system?s 
language model is to add the word sequences of 
the user-clicked results to the original training sen-
tences and to re-estimate the language model ?(?). 
We will refer to this method as maximum likelih-
ood (ML) estimation. A second approach, referred 
to as conditional maximum likelihood (CML) es-
timation, is to adapt the language model such as to 
directly maximize the conditional likelihood of the 
correct result given acoustics, i.e., 
 
? ? ? =
? ? ? ?(?)
 ? ? ?? ?(??)????
 
 
Note that this is the same objective function as 
the one used in Section 3.4, except that here the 
click data is used to estimate the language model 
instead of the error correction model. Again, in 
practice we assume that users click on correct re-
sults, i.e. ? = ?. 
4 Experiments  
We now discuss a number of experiments and the 
results obtained using the proposed n-best correc-
tion approach.  
4.1 Data 
For the purposes of the experiments described be-
low we extracted just over 800,000 queries from 
the server logs in which the recognizer had gener-
ated a simple n-best list2. For each recognition 
event, we collected from the system logs the n-best 
list, and the result clicked by the user (if the user 
clicked on any result).  
In addition, for testing purposes, we also make 
use of 11529 orthographically transcribed user re-
quests. The transcribed set was further divided into 
a development set containing 5680 utterances and 
a test set containing 5849 utterances.  
4.2 Initial N-Best Rescoring 
To tease apart the effects of expansion and rescor-
ing in the proposed n-best correction model, we 
began by using the rescoring techniques on the 
initial n-best lists, without first expanding them. 
Since the actual recognition confidence scores 
?(?? |?) were not available in the system logs, we 
replaced them with an exponential probability den-
sity function based on the rank of the hypothesis:  
 
? ??  ? = 2
??  
 
We then rescored the n-best lists from the test 
set according to the three rescoring models de-
scribed earlier: confusion matrix, maximum like-
lihood (ML), and conditional maximum likelihood 
(CML). We computed the sentence level accuracy 
for the rescored n-best list, at different cutoffs. The 
accuracy was measured by comparing the rescored 
hypotheses against the available transcripts. 
Note that the maximum depth of the n-best lists 
generated by the recognizer is 10; this is the max-
imum number of hypotheses that can be displayed 
on the mobile device. However, the system may 
generate fewer than 10 hypotheses. The observed 
average n-best list size in the test set was 4.2.  
The rescoring results are illustrated in Figure 3 
and reported in Table 1. The X axis in Figure 3 
shows the cutoff at which the n-best accuracy was 
computed. For instance in the baseline system, the 
correct hypothesis was contained in the top result 
in 46.2% of cases, in the top-2 results in 50.5% of 
the cases and in the top-3 results in 51.5% of the 
cases. The results indicate that all the rescoring 
models improve performance relative to the base-
                                                          
2 We did not consider cases where a false-recognition event 
was fired (e.g. if no speech was detected in the audio signal) ? 
in these cases no n-best list is generated. We also did not con-
sider cases where a compound n-best was generated (e.g. for 
compound requests like Shamiana in Kirkland, Washington) 
25
line. The improvement is smallest for the maxi-
mum likelihood (ML) language model rescoring 
approach, but is still statistically significant 
(? = 0.008 in a Wilcoxon sign-rank test.) The con-
fusion-matrix based rescoring and the CML rescor-
ing models perform similarly well, leading to a 1% 
absolute improvement in 1-best and 2-best sen-
tence-level accuracy from the baseline (? < 10?5). 
No statistically significant difference can be de-
tected between these two models. At the same 
time, they both outperform the maximum likelih-
ood rescoring model (? < 0.03). 
4.3 N-Best Correction 
Next, we evaluated the end-to-end n-best correc-
tion approach. The n-best lists were first expanded, 
as described in section 3.3, and the expanded lists 
were ranked using the confusion matrix based res-
coring model described in Section 3.4.  
The expansion process enlarges the original n-
best lists. Immediately after expansion, the average 
n-best size grows from 4.2 to 96.9. The oracle per-
formance for the expanded n-best lists increases to 
59.8% (versus 53.5% in the initial n-best lists.) 
After rescoring, we trimmed the expanded n-best 
lists to a maximum of 10 hypotheses: we still want 
to obey the mobile device display constraint. The 
resulting average n-best size was 7.09 (this is low-
er than 10 since there are cases when the system 
cannot generate enough expansion hypotheses.) 
The sentence-level accuracy of the corrected n-
best lists is displayed in line 4 from Table 1. A di-
rect comparison with the rescoring-only models or 
with the baseline is however unfair, due to the 
larger average size of the corrected n-best lists. To 
create a fair comparison and to better understand 
the performance of the n-best correction process, 
we pruned the corrected n-best lists by eliminating 
all hypotheses with a score below a certain thre-
shold. By varying this rejection threshold, we can 
therefore control the average depth of the resulting 
corrected n-best lists. At a rejection threshold of 
0.004, the average corrected n-best size is 4.15, 
comparable to the baseline of 4.2 .  
The performance for the corresponding cor-
rected (and pruned) n-best lists is shown in line 5 
from Table 1 and illustrated in Figure 4. In contrast 
to a rescoring-only approach, the expansion pro-
cess allows for improved performance at higher 
depths in the n-best list. The maximum n-best per-
formance (while keeping the average n-best size at 
4.15), is 56.5%, a 3% absolute improvement over 
the baseline (? < 10?5).  
Figure 5 provides more insight into the relation-
ship between the sentence-level accuracy of the 
corrected (and pruned) n-best lists and the average 
n-best size (the plot was generated by varying the 
rejection threshold.) The result we discussed above 
can also be observed here: at the same average n-
best size, the n-best correction model significantly 
outperforms the baseline. Furthermore, we can see 
that we can attain the same level of accuracy as the 
baseline system while cutting the average n-best 
size by more than 50%, from 4.22 to 2. In the op-
posite direction, if we are less sensitive to the 
number of items displayed in the n-best list (except 
for the 10-maximum constraint we already obey), 
we can further increase the overall performance by 
another 0.8% absolute to 57.3%; this overall accu-
racy is attained at an average n-best size of 7.09.  
Figure 3. Initial n-best rescoring (test-set) 
Table 1. Test-set sentence-level n-best accuracy; 
(0) baseline; (1)-(3) initial n-best rescoring;  
(4)-(5) expansion + rescoring 
 Model 1-
Best 
2-
Best 
3-
Best 
10-
Best 
0 Baseline 46.2 50.5 51.5 53.5 
1 ML Rescoring  46.8 50.9 52.1 53.5 
2 CML Rescoring 47.4 51.4 52.6 53.5 
3 Confusion Matrix Resc. 47.3 51.5 52.5 53.5 
4 Expansion + Rescoring 
(size=7.09) 
46.8 52.3 54.5 57.3 
5 Expansion + Rescoring 
(size=4.15) 
46.8 52.3 54.4 56.5 
 
26
Finally, we also investigated rescoring the ex-
panded n-best lists using the CML approach. To 
apply CML, an initial ranking of the expanded n-
best lists is however needed. If we use the ranking 
produced by the confusion-matrix based model 
discussed above, no further performance improve-
ments can be observed.  
5 Related work 
The n-best correction model we have described in 
this paper draws inspiration from earlier works on 
post-recognition error correction models, n-best 
rescoring and implicitly supervised learning. In 
this section we discuss some of the similarities and 
differences between the proposed approach and 
previous work. 
The idea of correcting speech recognition errors 
in a post-processing step has been proposed earlier 
by (Ringger and Allen, 1996; Ringger and Allen, 
1997). The authors showed that, in the presence of 
transcribed data, a translation-based post-processor 
can be trained to correct the results of a speech 
recognizer, leading to a 15% relative WER im-
provement in a corpus of TRAINS-95 dialogues.  
The n-best correction approach described here is 
different in two important aspects. First, instead of 
making use of transcripts, the proposed error-
correction model is trained using implicit user 
feedback obtained in a multi-modal interface (in 
this case user clicks in the n-best list.) This is a less 
costly endeavor, as the system automatically ob-
tains the supervision signal directly from the inte-
raction; no transcripts are necessary. Second, the 
approach operates on the entire n-best list, rather 
than only on the top hypothesis; as such, it has ad-
ditional information that can be helpful in making 
corrections. At Figure 2 illustrates, there is a poten-
tial for multiple incorrect hypotheses to point to-
wards and reinforce the same correction 
hypothesis, leading to improved performance (in 
this example, Burlington, Cooling, Sterling and 
Stirling were all highly confusable with Bowling, 
which was the correct hypothesis). 
The n-best correction model we have described 
includes a rescoring step. N-best rescoring ap-
proaches have been investigated extensively in the 
speech recognition community. In the dialog 
community, n-best rescoring techniques that use 
higher-level, dialog features have also been pro-
posed and evaluated (Chotimongkol and Rudnicky, 
2001). Apart from using the click feedback, the 
novelty in our approach lies in the added expansion 
step and in the use of an error-correction model for 
rescoring. We have seen that the confusability-
based n-best expansion process leads to signifi-
cantly improved performance, even if we force the 
model to keep the same average n-best size. 
Finally, the work discussed in this paper has 
commonalities with previous works on lightly su-
pervised learning in the speech community, e.g. 
(Lamel and Gauvain, 2002) and leveraging implicit 
feedback for learning from interaction, e.g. (Baner-
jee and Rudnicky, 2007; Bohus and Rudnicky, 
2007). In all these cases, the goal is to minimize 
the need for manually-labeled data, and learn di-
Figure 5. Overall n-best accuracy as a function of 
the average n-best size  
53.5% 
56.5% 
57.3% 
Figure 4. N-Best correction (test-set) 
27
rectly from the interaction. We believe that in the 
long term this family of learning techniques will 
play a key role towards building autonomous, self-
improving systems. 
6 Conclusion and future work 
We have proposed and evaluated a novel n-best 
correction model that leverages implicit user feed-
back in a multi-modal interface to create a positive 
feedback loop. While the experiments reported 
here were conducted in the context of a local 
search application, the approach is applicable in 
any multi-modal interface that elicits selection in 
an n-best list from the user.  
The proposed n-best correction model works in 
two stages. First, the n-best list generated by the 
speech recognizer is expanded with additional hy-
potheses based on confusability information cap-
tured from previous user clicks. This expanded list 
is then rescored and pruned to create a more accu-
rate and more compact n-best list. Our experiments 
show that the proposed n-best correction approach 
significantly outperforms both the baseline and 
other traditional n-best rescoring approaches, with-
out increasing the average length of the n-best lists.  
Several issues remain to be investigated. The 
models discussed in this paper focus on post-
recognition processing. Other ways of using the 
click data can also be envisioned. For instance, one 
approach would be to add all the clicked results to 
the existing language model training data and 
create an updated recognition language model. In 
the future, we plan to investigate the relationship 
between these two approaches, and to whether they 
can be used in conjunction. Earlier related work 
(Ringger and Allen, 1997) suggests that this should 
indeed be the case. 
Second, the click-based error-correction model 
we have described in section 3.4 operates at the 
result level. The proposed model is essentially a 
sentence level, memory-based translation model. 
In the future, we also plan to investigate word-
level error-correction models, using machine trans-
lation techniques like the ones discussed in (Ring-
ger and Allen, 1997; Li et al, 2008). 
Finally, we plan to investigate how this process 
of learning from implicit feedback in a multi-
modal interface can be streamlined, such that the 
system continuously learns online, with a minimal 
amount of human intervention.  
Acknowledgments 
This work would have not been possible without 
the help of a number of other people. We would 
like to especially thank Oliver Scholz, Julian 
Odell, Christopher Dac, Tim Paek, Y.C. Ju, Paul 
Bennett, Eric Horvitz and Alex Acero for their 
help and for useful conversations and feedback. 
References  
Acero, A., N. Bernstein, et al (2008). "Live Search for 
Mobile: Web Services by Voice on the Cellphone". 
ICASSP'08. Las Vegas, NV. 
Banerjee, S. and A. Rudnicky (2007). "Segmenting 
Meetings into Agenda Items by Extracting Implicit 
Supervision from Human Note-Taking". IUI'2007. 
Honolulu, Hawaii. 
Birkenes, O., T. Matsui, et al (2007). "N-Best Rescor-
ing for Speech Recognition using Penalized Logis-
tic Regression Machines with Garbage Class". 
ICASSP'2007, Honolulu, Hawaii. 
Bohus, D. and A. Rudnicky (2007). "Implicitly-
supervised learning in spoken language interfaces: 
an application to the confidence annotation prob-
lem". SIGdial 2007, Antwerp, Belgium. 
Chotimongkol, A. and A. Rudnicky (2001). "N-best 
Speech Hypotheses Reordering Using Linear Re-
gression". Eurospeech'2001, Aalborg, Denmark. 
Lamel, L. and J.-L. Gauvain (2002). "Lightly Super-
vised and Unsupervised Acoustic Model Training." 
Computer Speech and Language 16: 115-129. 
Li, X., Y.-C. Ju, et al (2008). "Language Modeling for 
Voice Search: a Machine Translation Approach". 
ICASSP'08, Las Vegas, NV. 
Live Search for Windows Mobile (2008): 
 http://mobile.search.live.com 
Nuance Mobile Search (2008): 
http://www.nuance.com/mobilesearch. 
Ringger, E. and J. Allen (1996). "Error Correction via 
Post-Processor for Continuous Speech Recogni-
tion". ICASSP'96, Atlanta, GA. 
Ringger, E. and J. Allen (1997). "Robust Error Correc-
tion of Continuous Speech Recognition". ESCA-
NATO Workshop on Robust Speech Recognition 
for Unknown Communication Channels, Pont-a-
Mousson, France. 
TellMeByMobile (2008): 
http://www.tellme.com/products/tellmebymobile. 
V-Lingo Mobile. (2008): 
http://www.vlingomobile.com/downloads.html. 
VoiceSignal Search. (2008): 
http://www.voicesignal.com/solutions/vsearch.php. 
 
28
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 995?1002, Vancouver, October 2005. c?2005 Association for Computational Linguistics
The Vocal Joystick: A Voice-Based Human-Computer Interface for
Individuals with Motor Impairments?
Jeff A. Bilmes?, Xiao Li?, Jonathan Malkin?, Kelley Kilanski?, Richard Wright?,
Katrin Kirchhoff?, Amarnag Subramanya?, Susumu Harada?, James A.
Landay?, Patricia Dowden?, Howard Chizeck?
?Dept. of Electrical Engineering
?Dept. of Computer Science & Eng.
?Dept. of Linguistics
?Dept. of Speech & Hearing Science
University of Washington
Seattle, WA
Abstract
We present a novel voice-based human-
computer interface designed to enable in-
dividuals with motor impairments to use
vocal parameters for continuous control
tasks. Since discrete spoken commands
are ill-suited to such tasks, our interface
exploits a large set of continuous acoustic-
phonetic parameters like pitch, loudness,
vowel quality, etc. Their selection is opti-
mized with respect to automatic recogniz-
ability, communication bandwidth, learn-
ability, suitability, and ease of use. Pa-
rameters are extracted in real time, trans-
formed via adaptation and acceleration,
and converted into continuous control sig-
nals. This paper describes the basic en-
gine, prototype applications (in particu-
lar, voice-based web browsing and a con-
trolled trajectory-following task), and ini-
tial user studies confirming the feasibility
of this technology.
1 Introduction
Many existing human-computer interfaces (e.g.,
mouse and keyboard, touch screens, pen tablets,
etc.) are ill-suited to individuals with motor
impairments. Specialized (and often expensive)
human-computer interfaces that have been devel-
oped specifically for this target group include sip
and puff switches, head mice, eye-gaze devices, chin
joysticks and tongue switches. While many indi-
viduals with motor impairments have complete use
?This material is based on work supported by the National
Science Foundation under grant IIS-0326382.
of their vocal system, these devices make little use
of it. Sip and puff switches, for example, have low
communication bandwidth, making it impossible to
achieve more complex control tasks.
Natural spoken language is often regarded as
the obvious choice for a human-computer inter-
face. However, despite significant research efforts
in automatic speech recognition (ASR) (Huang et
al., 2001), existing ASR systems are still not suf-
ficiently robust to a wide variety of speaking condi-
tions, noise, accented speakers, etc. ASR-based in-
terfaces are therefore often abandoned by users after
a short initial trial period. In addition, natural speech
is optimal for communication between humans but
sub-optimal for manipulating computers, windows-
icons-mouse-pointer (WIMP) interfaces, or other
electro-mechanical devices (such as a prosthetic ro-
botic arm). Standard spoken language commands
are useful for discrete but not for continuous op-
erations. For example, in order to move a cursor
from the bottom-left to the upper-right of a screen,
the user might have to repeatedly utter ?up? and
?right? or ?stop? and ?go? after setting an initial tra-
jectory and rate, which is quite inefficient. For these
reasons, we are developing alternative and reusable
voice-based assistive technology termed the ?Vocal
Joystick? (VJ).
2 The Vocal Joystick
The VJ approach has three main characteristics:
1) Continuous control parameters: Unlike standard
speech recognition, the VJ engine exploits continu-
ous vocal characteristics that go beyond simple se-
quences of discrete speech sounds (such as syllables
or words) and include e.g., pitch, vowel quality, and
loudness, which are then mapped to continuous con-
995
trol parameters.
2) Discrete vocal commands: Unlike natural speech,
the VJ discrete input language is based on a pre-
designed set of sounds. These sounds are selected
with respect to acoustic discriminability (maximiz-
ing recognizer accuracy), pronounceability (reduc-
ing potential vocal strain), mnemonic characteris-
tics (reducing cognitive load), robustness to environ-
mental noise, and application appropriateness.
3) Reusable infrastructure: Our goal is not to create
a single application but to provide a modular library
that can be incorporated by developers into a variety
of applications that can be controlled by voice. The
VJ technology is not meant to replace standard ASR
but to enhance and be compatible with it.
2.1 Vocal Characteristics
Three continuous vocal characteristics are extracted
by the VJ engine: energy, pitch, and vowel qual-
ity, yielding four specifiable continuous degrees of
freedom. The first of these, localized acoustic en-
ergy, is used for voice activity detection. In addi-
tion, it is normalized relative to the current vowel
detected (see Section 3.3), and is used by our cur-
rent VJ-WIMP application (Section 4) to control the
velocity of cursor movements. For example, a loud
voice causes a faster movement than does a quiet
voice. The second parameter, pitch, is also extracted
but is currently not mapped to any control dimension
in the VJ-WIMP application but will be in the future.
The third parameter is vowel quality. Unlike conso-
nants, which are characterized by a greater degree of
constriction in the vocal tract, vowels have much in-
herent signal energy and are therefore well-suited to
environments where both high accuracy and noise-
robustness are crucial. Vowels can be characterized
using a 2-D space parameterized by F1 and F2, the
first and second vocal-tract formants (resonant fre-
quencies). We initially experimented with directly
extracting F1/F2 and using them for directly spec-
ifying 2-D continuous control. While we have not
ruled out the use of F1/F2 in the future, we have
so far found that even the best F1/F2 detection al-
gorithms available are not yet accurate enough for
precise real-time specification of movement. There-
fore, we classify vowels directly and map them onto
the 2-D vowel space characterized by degree of con-
striction (i.e., tongue height) and tongue body posi-
tion (Figure 1). In our VJ-WIMP application, we use
Deg
ree 
of C
ons
trict
ion Front Central Back
High
Mid
Low
Tongue Body Position
[iy ] [ix ] [uw ]
[ey] [ax ] [ow ]
[ae ] [a] [aa ]
Figure 1: Vowel configurations as a function of their
dominant articulatory configurations.
the four corners of this chart to map to the 4 princi-
ple directions of up, down, left, and right as shown
in Figure 2 (note that the two figures are flipped and
rotated with respect to each other). We have four
different VJ systems running: A) a 4-class system
allowing only the specification of the 4 principle di-
rections; B) a 5-class system that also includes the
phone [ax] to act as a carrier when wishing to vary
only pitch and loudness; C) a 8-class system that in-
cludes the four diagonal directions; and D) a 9-class
system that includes all phones and directions. Most
of the discussion in this paper refers to the 4-class
system.
A fourth vocal characteristic is also extracted
by the VJ engine, namely discrete sounds. These
sounds may correspond to button presses as on a
mouse or joystick. The choice of sounds depends
on the application and are chosen according to char-
acteristic 2 above.
3 The VJ Engine
Our system-level design goals are modularity, low
latency, and maximal computational efficiency. For
this reason, we share common signal processing
operations in multiple signal extraction modules,
which yields real-time performance but leaves con-
siderable computational headroom for the back-end
applications being driven by the VJ engine.
Figure 3 shows the VJ engine architecture having
three modules: signal processing, pattern recogni-
tion, and motion control.
3.1 Signal Processing
The goal of the signal processing module is to ex-
tract low-level acoustic features that can be used in
996
[iy ]
[ix ]
[uw ]
[ey]
[ow ]
[ae ]
[a]
[aa ][ax ]
Figure 2: Vowel-direction mapping: vowels corre-
sponding to directions.
AcousticWaveform FeatureExtraction
Features:EnergyNCCFF1/F2MFCC
SignalProcessing
Energy
VowelClassification
PatternRecognition
PitchTracking
Discrete SoundRecognition
MotionParameters:
xy-directions,Speed,Acceleration,
Motion Control
SpaceTransformationMotion
ComputerInterfaceDriver Adaptation
Figure 3: System organization
estimating the vocal characteristics. The features we
use are energy, normalized cross-correlation coeffi-
cients (NCCC), formant estimates, Mel-frequency
cepstral coefficients (MFCCs), and formant esti-
mates. To extract features, the speech signal is PCM
sampled at a rate of Fs =16,000Hz. Energy is mea-
sured on a frame-by-frame basis with a frame size
of 25ms and a frame step of 10ms. Pitch is ex-
tracted with a frame size of 40ms and a frame step of
10ms. Multiple pattern recognition tasks may share
the same acoustic features: for example, energy and
NCCCs are used for pitch tracking, and energy and
MFCCs can be used in vowel classification and dis-
crete sound recognition. Therefore, it is more ef-
ficient to decouple feature extraction from pattern
recognition, as is shown in Figure 3.
3.2 Pattern Recognition
The pattern recognition module uses the acoustic
features to extract desired parameters. The estima-
tion and classification system must simultaneously
perform energy computation (available from the in-
put), pitch tracking, vowel classification, and dis-
crete sound recognition.
Many state-of-the-art pitch trackers are based on
dynamic programming (DP). This, however, often
requires the meticulous design of local DP cost func-
tions. The forms of these cost functions are usu-
ally empirically determined and/or their parameters
are tuned by algorithms such as gradient descent
(D.Talkin, 1995). Since different languages and ap-
plications may follow very different pitch transition
patterns, the cost functions optimized for certain lan-
guages and applications may not be the most appro-
priate for others. Our VJ system utilizes a graphi-
cal model mechanism to automatically optimize the
parameters of these cost functions, and has been
shown to yield state-of-the-art performance (X.Li et
al., 2004; J.Malkin et al, 2005).
For frame-by-frame vowel classification, our de-
sign constraints are the need for extremely low la-
tency and low computational cost. Probability es-
timates for vowel classes thus need to be obtained
as soon as possible after the vowel has been uttered
or after any small change in voice quality has oc-
curred. It is well known that models of vowel clas-
sification that incorporate temporal dynamics such
as hidden Markov models (HMMs) can be quite ac-
curate. However, the frame-by-frame latency re-
quirements of VJ make HMMs unsuitable for vowel
classification since HMMs estimate the likelihood
of a model based on the entire utterance. An alter-
native is to utilize causal ?HMM-filtering?, which
computes likelihoods at every frame based on all
frames seen so far. We have empirically found,
however, that slightly non-causal and quite local-
ized estimates of the vowel category probability
is sufficient to achieve user satisfaction. Specifi-
cally, we obtain probability estimates of the form
p(Vt|Xt?? , . . . , Xt+? ), where V is a vowel class,
and Xt?? , . . . , Xt+? are feature frames within a
length 2? + 1 window of features centered at time
t. After several empirical trials, we decided on
neural networks for vowel classification because of
the availability of efficient discriminative training al-
gorithms and their computational simplicity. Specif-
ically we use a simple 2-layer multi-layer percep-
tron (Bishop, 1995) whose input layer consists of
26 ? 7 = 182 nodes, where 26 is the dimension of
Xt, the MFCC feature vector, and 2? + 1 = 7 is the
997
number of consecutive frames, and that has 50 hid-
den nodes (the numbers 7 and 50 were determined
empirically). The output layer has 4 output nodes
representing 4 vowel probabilities. During training,
the network is optimized to minimize the Kullback-
Leibler (K-L) divergence between the output and the
true label distribution, thus achieving the aforemen-
tioned probabilistic interpretation.
The VJ engine needs not only to detect that the
user is specifying a vowel (for continuous control)
but also a consonant-vowel-consonant (CVC) pat-
tern (for discrete control) quickly and with a low
probability of confusion (a VJ system also uses C
and CV patterns for discrete commands). Requir-
ing an initial consonant will phonetically distinguish
these sounds from the pure vowel segments used
for continuous control ? the VJ system constantly
monitors for changes that indicate the beginning of
one of the discrete control commands. The vowel
within the CV and CVC patterns, moreover, can help
prevent background noise from being mis-classified
as a discrete sound. Lastly, each such pattern cur-
rently requires an ending silence, so that the next
command (a new discrete sound or continuous con-
trol vowel) can be accurately initiated. In all cases, a
simple threshold-based rejection mechanism is used
to reduce false positives.
To recognize the discrete control signals, HMMs
are employed since, as in standard speech recogni-
tion, time warping is necessary to normalize for dif-
ferent signal durations corresponding to the same
class. Specifically, we embed phone HMMs into
?word? (C, CV, or CVC) HMMs. In this way, it
is possible to train phone models using a training
set that covers all possible phones, and then con-
struct an application-specific discrete command vo-
cabulary without retraining by recombining existing
phone HMMs into new word HMMs. Therefore,
each VJ-driven application can have its own appro-
priate discrete sound set.
3.3 Motion Control: Direction and Velocity
The VJ motion control module receives several pat-
tern recognition parameters and processes them to
produce output more appropriate for determining 2-
D movement in the VJ-WIMP application.
Initial experiments suggested that using pitch to
affect cursor velocity (Igarashi and Hughes, 2001)
would be heavily constrained by an individual?s vo-
cal range. Giving priority to a more universal user-
independent VJ system, we instead focused on rela-
tive energy. Our observation that users often became
quiet when trying to move small amounts confirmed
energy as a natural choice. Drastically different in-
trinsic average energy levels for each vowel, how-
ever, meant that comparing all sounds to a global av-
erage energy would create a large vowel-dependent
bias. To overcome this, we distribute the energy per
frame among the different vowels, in proportion to
the probabilities output by the neural network, and
track the average energy for each vowel indepen-
dently. By splitting the power in this way, there is
no effect when probabilities are close to 1, and we
smooth out changes during vowel transitions when
probabilities are more evenly distributed.
There are many possible options for determining
velocity (a vector capturing both direction and speed
magnitude) and ?acceleration? (a function determin-
ing how the control-to-display ratio changes based
on input parameters), and the different schemes have
a large impact on user satisfaction. Unlike a standard
mouse cursor, where the mapping is from 2-D hand
movement to a 2-D screen, the VJ system maps from
vocal-tract articulatory movement to a 2-D screen,
and the transformation is not as straightforward. All
values are for the current time frame t unless indi-
cated otherwise. First, a raw direction value is cal-
culated for each axis j ? {x, y} as
dj =
?
i
pi ? ?vi, ej? (1)
in which pi = p(Vt = i|Xt??,...,t+? ) is the proba-
bility for vowel i at time t, vi is a unit vector in the
direction of vowel i, ej is the unit-length positive di-
rectional basis vector along the j axis, and ?v, e? is
the projection of vector v onto unit vector e. To de-
termine movement speed, we first calculate a scalar
for each axis j as
sj =
?
i
max
[
0, gi
(
pi ? f(
E
?i
)
)]
? |?vi, ej?|
where E is the energy in the current frame, ?i is the
average energy for vowel i, and f(?) and gi(?) are
functions used for energy normalization and percep-
tual scaling (such as logs and/or cube-roots). This
therefore allocates frame energy to direction based
on the vowel probabilities. Lastly, we calculate the
velocity for axis j at the current frame as
Vj = ? ? s
?
j ? exp(?sj). (2)
998
where ? represents the overall system sensitivity and
the other values (? and ?) are warping constants, al-
lowing the user to control the shape of the accelera-
tion curve. Typically only one of ? and ? is nonzero.
Setting both to zero results in constant-speed move-
ment along each axis, while ? = 1 and ? = 0
gives a linear mapping that will scale motion with
energy but have no acceleration. The current user-
independent system uses ? = 0.6, ? = 1.0 and sets
? = 0. Lastly, the final velocity along axis j is Vjdj .
Future publications will report on systematic evalu-
ations of different f(?) and gi(?) functions.
3.4 Motion Control: User Adaptation
Since vowel quality is used for continuous control,
inaccuracies can arise due to speaker variability ow-
ing to different speech loudness levels, vocal tract
lengths, etc. Moreover, a vowel class articulated by
one user might partially overlap in acoustic space
with a different vowel class from another user. This
imposes limitations on a purely user-independent
vowel classifier. Differences in speaker loudness
alone could cause significant unpredictability. To
mitigate these problems, we have designed an adap-
tation procedure where each user is asked to pro-
nounce four pre-defined vowel sounds, each last-
ing 2-3 seconds, at the beginning of a VJ ses-
sion. We have investigated several novel adaptation
strategies utilizing both neural networks and support
vector machines (SVM). The fundamental idea be-
hind them both is that an initial speaker-independent
transformation of the space is learned using train-
ing data, and is represented by the first layer of a
neural network. Adaptation data then is used to
transform various parameters of the classifier (e.g.,
all or sub-portions of the neural network, or the para-
meters of the SVM). Further details of some of these
novel adaptation strategies appear in (X.Li et al,
2005), and the remainder will appear in forthcom-
ing publications. Also, the average energy values of
each vowel for each user are recorded and used to
normalize the speed control rate mentioned above.
Preliminary evaluations on the data so far collected
show very good results, with adaptation reducing the
vowel classification error rate by 18% for the 4-class
case, and 35% for the 8-class case. Moreover, infor-
mal studies have shown that users greatly prefer the
VJ system after adaptation than before.
4 Applications and Videos
Our overall intent is for VJ to interface with a va-
riety of applications, and our primary application
so far has been to drive a standard WIMP interface
with VJ controls, what we call the VJ-WIMP ap-
plication. The current VJ version allows left but-
ton clicks (press and release, using the consonant
[k]) as well as left button toggles (using consonant
[ch]) to allow dragging. Since WIMP interfaces
are so general, this allows us to indirectly control
a plethora of different applications. Video demon-
strations are available at the URL: http://ssli.
ee.washington.edu/vj.
One of our key VJ applications is vocal web
browsing. The video (dated 6/2005) shows exam-
ples of two web browsing tasks, one as an exam-
ple of navigating the New York Times web site, the
other using Google Maps to select and zoom in on a
target area. Section 5 describes a preliminary evalu-
ation on these tasks. We have also started using the
VJ engine to control video games (third video ex-
ample), have interfaced VJ with the Dasher system
(Ward et al, 2000) (we call it the ?Vocal Dasher?),
and have also used VJ for figure drawing.
Several additional direct VJ-applications have
also been developed. Specifically, we have directly
interfaced the VJ system into a simple blocks world
environment, where more precise object movement
is possible than via the mouse driver. Specifically,
this environment can draw arbitrary trajectories, and
can precisely measure user fidelity when moving an
object along a trajectory. Fidelity depends both on
positional accuracy and task duration. One use of
this environment shows the spatial direction corre-
sponding to vocal effort (useful for training, forth
video example). Another shows a simple robotic
arm being controlled by VJ. We plan to use this
environment to perform formal and precise user-
performance studies in future work.
5 Preliminary User Study
We conducted a preliminary user study1 to evaluate
the feasibility of VJ and to obtain feedback regard-
ing specific difficulties in using the VJ-WIMP sys-
tem. While this study is not accurate in that: 1) it
does not yet involve the intended target population
1The user study presented here used an earlier version of VJ
than the current improved one described in the preceding pages.
999
of individuals with motor impairments, and: 2) the
users had only a small amount of time to practice and
become adept at using VJ, the study is still indica-
tive of the VJ approach?s overall viability as a novel
voice-based human-computer interface method. The
study quantitatively compares VJ performance with
a standard desktop mouse, and provides qualitative
measurement of the user?s perception of the system.
5.1 Experiment Setup
We recruited seven participants ranging from age 22
to 26, none of whom had any motor impairment.
Of the seven participants, two were female and five
were male. All of them were graduate students in
Computer Science, although none of them had pre-
viously heard of or used VJ. Four of the participants
were native English speakers; the other three had an
Asian language as their mother tongue.
We used a Dell Inspiron 9100 laptop with a 3.2
GHz Intel Pentium IV processor running the Fedora
Core 2 operating system, with a 1280 x 800 24-bit
color display. The laptop was equipped with an ex-
ternal Microsoft IntelliMouse connected through the
USB port which was used for all of the tasks in-
volving the mouse. A head-mounted Amanda NC-
61 microphone was used as the audio input device,
while the audio feedback from the laptop was output
through the laptop speakers. The Firefox browser
was used for all of the tasks, with the browser screen
maximized such that the only portion of the screen
which was not displaying the contents of the web
page was the top navigation toolbar which was 30
pixels high.
5.2 Quantitative and Qualitative Evaluation
At the beginning of the quantitative evaluation, each
participant was given a brief description of the VJ
operations and was shown a demonstration of the
system by a practiced experimenter. The participant
was then guided through an adaptation process dur-
ing which she/he was asked to pronounce the four
directional vowels (Section 3.4). After adaptation,
the participant was given several minutes to practice
using a simple target clicking application. The quan-
titative portion of our evaluation followed a within-
participant design. We exposed each participant to
two experimental conditions which we refer to as
input modalities: the mouse and the VJ. Each par-
ticipant completed two tasks on each modality, with
one trial per task.
The first task was a link navigation task (Link),
in which the participants were asked to start from a
specific web page and follow a particular set of links
to reach a destination. Before the trial, the experi-
menter demonstrated the specified sequence of links
to the participant by using the mouse and clicking at
the appropriate links. The participant was also pro-
vided with a sheet of paper for their reference that
listed the sequence of links that would lead them to
the target. The web site we used was a Computer
Science Department student guide and the task in-
volved following six links with the space between
each successive link including both horizontal and
vertical components.
The second task was map navigation (Map), in
which the participant was asked to navigate an on-
line map application from a starting view (showing
the entire USA) to get to a view showing a partic-
ular campus. The size of the map was 400x400
pixels, and the set of available navigation controls
surrounding the map included ten discrete zoom
level buttons, eight directional panning arrows, and
a click inside the map causing the map to be centered
and zoomed in by one level. Before the trial, a prac-
ticed experimenter demonstrated how to locate the
campus map starting from the USA view to ensure
they were familiar with the geography.
For each task, the participants performed one trial
using the mouse, and one trial using a 4-class VJ.
The trials were presented to the participants in a
counterbalanced order. We recorded the completion
time for each trial, as well as the number of false
positives (system interprets a click when the user
did not make a click sound), missed recognitions
(the user makes a click sound but the system fails to
recognize it as a click), and user errors (whenever
the user clicks on an incorrect link). The recorded
trial times include the time used by all of the above
errors including recovery time.
After the completion of the quantitative evalu-
ation, the participants were given a questionnaire
which consisted of 14 questions related to the partic-
ipants? perception of their experience using VJ such
as the degree of satisfaction, frustration, and embar-
rassment. The answers were encoded on a 7-point
Likert scale. We also included a space where the
participants could write in any comments, and an in-
1000
010
20
30
40
50
60
70
80
90
100
Link Map
Task type
T
as
k 
co
m
p
le
ti
o
n
 t
im
e 
(s
ec
o
n
d
s)
Mouse
Vocal Joystick
Figure 4: Task complement times
0
2
4
6
8
10
12
14
16
18
20
M,
 K
ore
a
M,
 N
ort
he
as
t
M,
 M
idw
es
t
M,
 N
ort
he
as
t
F, 
Mi
d-A
tla
nti
c
F, 
Ch
ina
M,
 C
hin
a
Participant (Gender, Origin)
N
um
be
r o
f m
is
se
d 
re
co
gn
iti
on
s
Link
Map
Figure 5: Missed recognitions by participant
formal post-experiment interview was performed to
solicit further feedback.
5.3 Results
Figure 4 shows the task completion times for Link
and Map tasks, Figure 5 shows the breakdown of
click errors by individual participants, Figure 6
shows the average number of false positive and
missed recognition errors for each of the tasks.
There was no instance of user error in any trial. Fig-
ure 7 shows the median of the responses to each of
the fourteen questionnaire questions (error bars in
each plot show ? standard error). In our measure-
ment of the task completion times, we considered
the VJ?s recognition error rate as a fixed factor, and
thus did not subtract the time spent during those er-
rors from the task completion time.
There were several other interesting observations
that were made throughout the study. We noticed
that the participants who had the least trouble with
missed recognitions for the clicking sound were ei-
0
1
2
3
4
5
6
7
8
9
10
Link Map
Task type
N
um
be
r o
f e
rr
or
s
False positive
Missed Recognition
Figure 6: Average number of click errors per task
1.0
2.0
3.0
4.0
5.0
6.0
7.0
Ea
sy 
to l
ear
n
Ea
sy 
to u
se
Dif
f icu
lt to
 co
ntr
ol
Fru
stra
ting Fu
n
Tir
ing
Em
bar
ras
sin
g
Intu
itiv
e
Err
or 
pro
ne
Se
lf-c
ons
cio
us
Se
lf-c
ons
cio
usn
ess
 de
cre
ase
d
Vo
we
l so
und
s d
isti
ngu
ish
abl
e
Ma
p h
ard
er t
han
 se
arc
h
Mo
tion
 ma
tch
ed 
inte
ntio
n
Strongly
agree
Strongly
disagree
Figure 7: Questionnaire results
ther female or with an Asian language background,
as shown in Figure 5. Our hypothesis regarding the
better performance by female participants is that the
original click sound was trained on one of our fe-
male researcher?s voice. We plan also in future work
to determine how the characteristics of different na-
tive language speakers influence VJ, and ultimately
to correct for any bias.
All but one user explicitly expressed their confu-
sion in distinguishing between the [ae] and [aa] vow-
els. Four of the seven participants independently
stated that their performance would probably have
been better if they had been able to practice longer,
and did not attribute their perceived suboptimal per-
formance to the quality of the VJ?s recognition sys-
tem. Several participants reported that they felt their
vocal cords were strained due to having to produce a
loud sound in order to get the cursor to move at the
desired speed. We suspect this is due either to ana-
log gain problems or to their adapted voice being too
loud, and therefore the system calibrating the nor-
mal speed to correspond to the loud voice. We have
since removed this problem by adjusting our adapta-
1001
tion strategy to express preference for a quiet voice.
In summary, the results from our study suggest
that users without any prior experience were able
to perform basic mouse based tasks using the Vocal
Joystick system with relative slowdown of four to
nine times compared to a conventional mouse. We
anticipate that future planned improvements in the
algorithms underlying the VJ engine (to improve ac-
curacy, user-independence, adaptation, and speed)
will further increase the VJ system?s viability, and
combined with practice could improve VJ enough so
that it becomes a reasonable alternative compared to
a standard mouse?s performance.
6 Related Work
Related voice-based interface studies include
(Igarashi and Hughes, 2001; Olwal and Feiner,
2005). Igarashi & Hughes presented a system where
non-verbal voice features control a mouse system ?
their system requires a command-like discrete sound
to determine direction before initiating a movement
command, where pitch is used to control veloc-
ity. We have empirically found an energy-based
mapping for velocity (as used in our VJ system)
both more reliable (no pitch-tracking errors) and
intuitive. Olwal & Feiner?s system moves the mouse
only after recognizing entire words. de Mauro?s
?voice mouse? http://www.dii.unisi.it/
?maggini/research/voice mouse.html
focuses on continuous cursor movements similar
to the VJ scenario; however, the voice mouse
only starts moving after the vocalization has been
completed leading to long latencies, and it is not
easily portable to other applications. Lastly, the
commercial dictation program Dragon by ScanSoft
includes MouseGridTM(Dra, 2004) which allows
discrete vocal commands to recursively 9-partition
the screen, thus achieving log-command access to a
particular screen point. A VJ system, by contrast,
uses continuous aspects of the voice, has change
latency (about 60ms) not much greater than reaction
time, and allows the user to make instantaneous
directional change using one?s voice (e.g., a user
can draw a ?U? shape in one breath).
7 Conclusions
We have presented new voice-based assistive tech-
nology for continuous control tasks and have
demonstrated an initial system implementation of
this concept. An initial user study using a group
of individuals from the non-target population con-
firmed the feasibility of this technology. We plan
next to further improve our system by evaluating a
number of novel pattern classification techniques to
increase accuracy and user-independence, and to in-
troduce additional vocal characteristics (possibilities
include vibrato, degree of nasality, rate of change
of any of the above as an independent parameter)
to increase the available simultaneous degrees of
freedom controllable via the voice. Moreover, we
plan to develop algorithms to decouple unintended
user correlations of these parameters, and to further
advance both our adaptation and acceleration algo-
rithms.
References
C. Bishop. 1995. Neural Networks for Pattern Recogni-
tion. Clarendon Press, Oxford.
2004. Dragon naturally speaking, MousegridTM, Scan-
Soft Inc.
D.Talkin. 1995. A robust algorithm for pitch track-
ing (RAPT). In W.B.Kleign and K.K.Paliwal, editors,
Speech Coding and Synthesis, pp. 495?515, Amster-
dam. Elsevier Science.
X. Huang, A. Acero, and H.-W. Hon. 2001. Spoken Lan-
guage Processing: A Guide to Theory, Algorithm, and
System Development. Prentice Hall.
T. Igarashi and J. F. Hughes. 2001. Voice as sound: Us-
ing non-verbal voice input for interactive control. In
ACM UIST 2001, November.
J.Malkin, X.Li, and J.Bilmes. 2005. A graphical model
for formant tracking. In Proc. IEEE Intl. Conf. on
Acoustics, Speech, and Signal Processing.
A. Olwal and S. Feiner. 2005. Interaction techniques us-
ing prosodic feature of speech and audio localization.
In Proceedings of the 10th International Conference
on Intelligent User Interfaces, pp. 284?286.
D. Ward, A. F. Blackwell, and D. C. MacKay. 2000.
Dasher - a data entry interface using continuous ges-
tures and language models. In ACM UIST 2000.
X.Li, J.Malkin, and J.Bilmes. 2004. A graphical model
approach to pitch tracking. In Proc. Int. Conf. on Spo-
ken Language Processing.
X.Li, J.Bilmes, and J.Malkin. 2005. Maximum mar-
gin learning and adaptation of MLP classifers. In 9th
European Conference on Speech Communication and
Technology (Eurospeech?05), Lisbon, Portugal, Sep-
tember.
1002
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1337?1345,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Understanding the Semantic Structure of Noun Phrase Queries
Xiao Li
Microsoft Research
One Microsoft Way
Redmond, WA 98052 USA
xiaol@microsoft.com
Abstract
Determining the semantic intent of web
queries not only involves identifying their
semantic class, which is a primary focus
of previous works, but also understanding
their semantic structure. In this work, we
formally define the semantic structure of
noun phrase queries as comprised of intent
heads and intent modifiers. We present
methods that automatically identify these
constituents as well as their semantic roles
based on Markov and semi-Markov con-
ditional random fields. We show that the
use of semantic features and syntactic fea-
tures significantly contribute to improving
the understanding performance.
1 Introduction
Web queries can be considered as implicit ques-
tions or commands, in that they are performed ei-
ther to find information on the web or to initiate
interaction with web services. Web users, how-
ever, rarely express their intent in full language.
For example, to find out ?what are the movies of
2010 in which johnny depp stars?, a user may sim-
ply query ?johnny depp movies 2010?. Today?s
search engines, generally speaking, are based on
matching such keywords against web documents
and ranking relevant results using sophisticated
features and algorithms.
As search engine technologies evolve, it is in-
creasingly believed that search will be shifting
away from ?ten blue links? toward understanding
intent and serving objects. This trend has been
largely driven by an increasing amount of struc-
tured and semi-structured data made available to
search engines, such as relational databases and
semantically annotated web documents. Search-
ing over such data sources, in many cases, can
offer more relevant and essential results com-
pared with merely returning web pages that con-
tain query keywords. Table 1 shows a simplified
view of a structured data source, where each row
represents a movie object. Consider the query
?johnny depp movies 2010?. It is possible to re-
trieve a set of movie objects from Table 1 that
satisfy the constraints Year = 2010 and Cast 3
Johnny Depp. This would deliver direct answers to
the query rather than having the user sort through
list of keyword results.
In no small part, the success of such an ap-
proach relies on robust understanding of query in-
tent. Most previous works in this area focus on
query intent classification (Shen et al, 2006; Li
et al, 2008b; Arguello et al, 2009). Indeed, the
intent class information is crucial in determining
if a query can be answered by any structured data
sources and, if so, by which one. In this work, we
go one step further and study the semantic struc-
ture of a query, i.e., individual constituents of a
query and their semantic roles. In particular, we
focus on noun phrase queries. A key contribution
of this work is that we formally define query se-
mantic structure as comprised of intent heads (IH)
and intent modifiers (IM), e.g.,
[IM:Title alice in wonderland] [IM:Year 2010] [IH cast]
It is determined that ?cast? is an IH of the above
query, representing the essential information the
user intends to obtain. Furthermore, there are two
IMs, ?alice in wonderland? and ?2010?, serving as
filters of the information the user receives.
Identifying the semantic structure of queries can
be beneficial to information retrieval. Knowing
the semantic role of each query constituent, we
1337
Title Year Genre Director Cast Review
Precious 2009 Drama Lee Daniels Gabby Sidibe, Mo?Nique,. . .
2012 2009 Action, Sci Fi Roland Emmerich John Cusack, Chiwetel Ejiofor,. . .
Avatar 2009 Action, Sci Fi James Cameron Sam Worthington, Zoe Saldana,. . .
The Rum Diary 2010 Adventure, Drama Bruce Robinson Johnny Depp,Giovanni Ribisi,. . .
Alice in Wonderland 2010 Adventure, Family Tim Burton Mia Wasikowska, Johnny Depp,. . .
Table 1: A simplified view of a structured data source for the Movie domain.
can reformulate the query into a structured form
or reweight different query constituents for struc-
tured data retrieval (Robertson et al, 2004; Kim
et al, 2009; Paparizos et al, 2009). Alternatively,
the knowledge of IHs, IMs and semantic labels of
IMs may be used as additional evidence in a learn-
ing to rank framework (Burges et al, 2005).
A second contribution of this work is to present
methods that automatically extract the semantic
structure of noun phrase queries, i.e., IHs, IMs
and the semantic labels of IMs. In particular, we
investigate the use of transition, lexical, semantic
and syntactic features. The semantic features can
be constructed from structured data sources or by
mining query logs, while the syntactic features can
be obtained by readily-available syntactic analy-
sis tools. We compare the roles of these features
in two discriminative models, Markov and semi-
Markov conditional random fields. The second
model is especially interesting to us since in our
task it is beneficial to use features that measure
segment-level characteristics. Finally, we evaluate
our proposed models and features on manually-
annotated query sets from three domains, while
our techniques are general enough to be applied
to many other domains.
2 Related Works
2.1 Query intent understanding
As mentioned in the introduction, previous works
on query intent understanding have largely fo-
cused on classification, i.e., automatically map-
ping queries into semantic classes (Shen et al,
2006; Li et al, 2008b; Arguello et al, 2009).
There are relatively few published works on un-
derstanding the semantic structure of web queries.
The most relevant ones are on the problem of
query tagging, i.e., assigning semantic labels to
query terms (Li et al, 2009; Manshadi and Li,
2009). For example, in ?canon powershot sd850
camera silver?, the word ?canon? should be tagged
as Brand. In particular, Li et al leveraged click-
through data and a database to automatically de-
rive training data for learning a CRF-based tagger.
Manshadi and Li developed a hybrid, generative
grammar model for a similar task. Both works are
closely related to one aspect of our work, which
is to assign semantic labels to IMs. A key differ-
ence is that they do not conceptually distinguish
between IHs and IMs.
On the other hand, there have been a series of
research studies related to IH identification (Pasca
and Durme, 2007; Pasca and Durme, 2008). Their
methods aim at extracting attribute names, such
as cost and side effect for the concept Drug, from
documents and query logs in a weakly-supervised
learning framework. When used in the context
of web queries, attribute names usually serve as
IHs. In fact, one immediate application of their
research is to understand web queries that request
factual information of some concepts, e.g. ?asiprin
cost? and ?aspirin side effect?. Their framework,
however, does not consider the identification and
categorization of IMs (attribute values).
2.2 Question answering
Query intent understanding is analogous to ques-
tion understanding for question answering (QA)
systems. Many web queries can be viewed as the
keyword-based counterparts of natural language
questions. For example, the query ?california na-
tional? and ?national parks califorina? both imply
the question ?What are the national parks in Cali-
fornia??. In particular, a number of works investi-
gated the importance of head noun extraction in
understanding what-type questions (Metzler and
Croft, 2005; Li et al, 2008a). To extract head
nouns, they applied syntax-based rules using the
information obtained from part-of-speech (POS)
tagging and deep parsing. As questions posed
in natural language tend to have strong syntactic
structures, such an approach was demonstrated to
be accurate in identifying head nouns.
In identifying IHs in noun phrase queries, how-
ever, direct syntactic analysis is unlikely to be as
effective. This is because syntactic structures are
in general less pronounced in web queries. In this
1338
work, we propose to use POS tagging and parsing
outputs as features, in addition to other features, in
extracting the semantic structure of web queries.
2.3 Information extraction
Finally, there exist large bodies of work on infor-
mation extraction using models based on Markov
and semi-Markov CRFs (Lafferty et al, 2001;
Sarawagi and Cohen, 2004), and in particular for
the task of named entity recognition (McCallum
and Li, 2003).
The problem studied in this work is concerned
with identifying more generic ?semantic roles? of
the constituents in noun phrase queries. While
some IM categories belong to named entities such
as IM:Director for the intent class Movie, there
can be semantic labels that are not named entities
such as IH and IM:Genre (again for Movie).
3 Query Semantic Structure
Unlike database query languages such as SQL,
web queries are usually formulated as sequences
of words without explicit structures. This makes
web queries difficult to interpret by computers.
For example, should the query ?aspirin side effect?
be interpreted as ?the side effect of aspirin? or ?the
aspirin of side effect?? Before trying to build mod-
els that can automatically makes such decisions,
we first need to understand what constitute the se-
mantic structure of a noun phrase query.
3.1 Definition
We let C denote a set of query intent classes that
represent semantic concepts such as Movie, Prod-
uct and Drug. The query constituents introduced
below are all defined w.r.t. the intent class of a
query, c ? C, which is assumed to be known.
Intent head
An intent head (IH) is a query segment that cor-
responds to an attribute name of an intent class.
For example, the IH of the query ?alice in won-
derland 2010 cast? is ?cast?, which is an attribute
name of Movie. By issuing the query, the user in-
tends to find out the values of the IH (i.e., cast). A
query can have multiple IHs, e.g., ?movie avatar
director and cast?. More importantly, there can
be queries without an explicit IH. For example,
?movie avatar? does not contain any segment that
corresponds to an attribute name of Movie. Such a
query, however, does have an implicit intent which
is to obtain general information about the movie.
Intent modifier
In contrast, an intent modifier (IM) is a query seg-
ment that corresponds to an attribute value (of
some attribute name). The role of IMs is to impos-
ing constraints on the attributes of an intent class.
For example, there are two constraints implied in
the query ?alice in wonderland 2010 cast?: (1) the
Title of the movie is ?alice in wonderland?; and
(2) the Year of the movie is ?2010?. Interestingly,
the user does not explicitly specify the attribute
names, i.e., Title and Year, in this query. Such
information, however, can be inferred given do-
main knowledge. In fact, one important goal of
this work is to identify the semantic labels of IMs,
i.e., the attribute names they implicitly refer to. We
use Ac to denote the set of IM semantic labels for
the intent class c.
Other
Additionally, there can be query segments that do
not play any semantic roles, which we refer to as
Other.
3.2 Syntactic analysis
The notion of IHs and IMs in this work is closely
related to that of linguistic head nouns and modi-
fiers for noun phrases. In many cases, the IHs of
noun phrase queries are exactly the head nouns in
the linguistic sense. Exceptions mostly occur in
queries without explicit IHs, e.g., ?movie avatar?
in which the head noun ?avatar? serves as an IM
instead. Due to the strong resemblance, it is inter-
esting to see if IHs can be identified by extracting
linguistic head nouns from queries based on syn-
tactic analysis. To this end, we apply the follow-
ing heuristics for head noun extraction. We first
run a POS-tagger and a chunker jointly on each
query, where the POS-tagger/chunker is based on
an HMM system trained on English Penn Tree-
bank (Gao et al, 2001). We then mark the right
most NP chunk before any prepositional phrase
or adjective clause, and apply the NP head rules
(Collins, 1999) to the marked NP chunk.
The main problem with this approach, however,
is that a readily-available POS tagger or chunker is
usually trained on natural language sentences and
thus is unlikely to produce accurate results on web
queries. As shown in (Barr et al, 2008), the lexi-
cal category distribution of web queries is dramat-
ically different from that of natural languages. For
example, prepositions and subordinating conjunc-
tions, which are strong indicators of the syntactic
1339
structure in natural languages, are often missing in
web queries. Moreover, unlike most natural lan-
guages that follow the linear-order principle, web
queries can have relatively free word orders (al-
though some orders may occur more often than
others statistically). These factors make it diffi-
cult to produce reliable syntactic analysis outputs.
Consequently, the head nouns and hence the IHs
extracted therefrom are likely to be error-prone, as
will be shown by our experiments in Section 6.3.
Although a POS tagger and a chunker may not
work well on queries, their output can be used as
features for learning statistical models for seman-
tic structure extraction, which we introduce next.
4 Models
This section presents two statistical models for se-
mantic understanding of noun phrase queries. As-
suming that the intent class c ? C of a query is
known, we cast the problem of extracting the se-
mantic structure of the query into a joint segmen-
tation/classification problem. At a high level, we
would like to identify query segments that corre-
spond to IHs, IMs and Others. Furthermore, for
each IM segment, we would like to assign a se-
mantic label, denoted by IM:a, a ? Ac, indicating
which attribute name it refers to. In other words,
our label set consists of Y = {IH, {IM:a}a?Ac ,
Other}.
Formally, we let x = (x1, x2, . . . , xM ) denote
an input query of length M . To avoid confusion,
we use i to represent the index of a word token
and j to represent the index of a segment in the
following text. Our goal is to obtain
s? = argmax
s
p(s|c,x) (1)
where s = (s1, s2, . . . , sN ) denotes a query seg-
mentation as well as a classification of all seg-
ments. Each segment sj is represented by a tu-
ple (uj, vj , yj). Here uj and vj are the indices of
the starting and ending word tokens respectively;
yj ? Y is a label indicating the semantic role of
s. We further augment the segment sequence with
two special segments: Start and End, represented
by s0 and sN+1 respectively. For notional simplic-
ity, we assume that the intent class is given and
use p(s|x) as a shorthand for p(s|c,x), but keep in
mind that the label space and hence the parameter
space is class-dependent. Now we introduce two
methods of modeling p(s|x).
4.1 CRFs
One natural approach to extracting the semantic
structure of queries is to use linear-chain CRFs
(Lafferty et al, 2001). They model the con-
ditional probability of a label sequence given
the input, where the labels, denoted as y =
(y1, y2, . . . , yM ), yi ? Y , have a one-to-one cor-
respondence with the word tokens in the input.
Using linear-chain CRFs, we aim to find the la-
bel sequence that maximizes
p?(y|x) =
1
Z?(x)
exp
{M+1
?
i=1
? ? f(yi?1, yi,x, i)
}
.
(2)
The partition function Z?(x) is a normalization
factor. ? is a weight vector and f(yi?1, yi,x) is
a vector of feature functions referred to as a fea-
ture vector. The features used in CRFs will be de-
scribed in Section 5.
Given manually-labeled queries, we estimate ?
that maximizes the conditional likelihood of train-
ing data while regularizing model parameters. The
learned model is then used to predict the label se-
quence y for future input sequences x. To obtain s
in Equation (1), we simply concatenate the maxi-
mum number of consecutive word tokens that have
the same label and treat the resulting sequence as a
segment. By doing this, we implicitly assume that
there are no two adjacent segments with the same
label in the true segment sequence. Although this
assumption is not always correct in practice, we
consider it a reasonable approximation given what
we empirically observed in our training data.
4.2 Semi-Markov CRFs
In contrast to standard CRFs, semi-Markov CRFs
directly model the segmentation of an input se-
quence as well as a classification of the segments
(Sarawagi and Cohen, 2004), i.e.,
p(s|x) = 1Z?(x)
exp
N+1
?
j=1
? ? f(sj?1, sj,x) (3)
In this case, the features f(sj?1, sj ,x) are de-
fined on segments instead of on word tokens.
More precisely, they are of the function form
f(yj?1, yj,x, uj , vj). It is easy to see that by
imposing a constraint ui = vi, the model is
reduced to standard linear-chain CRFs. Semi-
Markov CRFs make Markov assumptions at the
segment level, thereby naturally offering means to
1340
CRF features
A1: Transition ?(yi?1 = a)?(yi = b) transiting from state a to b
A2: Lexical ?(xi = w)?(yi = b) current word is w
A3: Semantic ?(xi ? WL)?(yi = b) current word occurs in lexicon L
A4: Semantic ?(xi?1:i ? WL)?(yi = b) current bigram occurs in lexicon L
A5: Syntactic ?(POS(xi) = z)?(yi = b) POS tag of the current word is z
Semi-Markov CRF features
B1: Transition ?(yj?1 = a)?(yj = b) Transiting from state a to b
B2: Lexical ?(xuj :vj = w)?(yj = b) Current segment is w
B3: Lexical ?(xuj :vj 3 w)?(yj = b) Current segment contains word w
B4: Semantic ?(xuj :vj ? L)?(yj = b) Current segment is an element in lexicon L
B5: Semantic max
l?L
s(xuj :vj , l)?(yj = b) The max similarity between the segment and elements in L
B6: Syntactic ?(POS(xuj :vj ) = z)?(yj = b) Current segment?s POS sequence is z
B7: Syntactic ?(Chunk(xuj :vj ) = c)?(yj = b) Current segment is a chunk with phrase type c
Table 2: A summary of feature types in CRFs and segmental CRFs for query understanding. We assume
that the state label is b in all features and omit this in the feature descriptions.
incorporate segment-level features, as will be pre-
sented in Section 5.
5 Features
In this work, we explore the use of transition, lexi-
cal, semantic and syntactic features in Markov and
semi-Markov CRFs. The mathematical expression
of these features are summarized in Table 2 with
details described as follows.
5.1 Transition features
Transition features, i.e., A1 and B1 in Table 2,
capture state transition patterns between adjacent
word tokens in CRFs, and between adjacent seg-
ments in semi-Markov CRFs. We only use first-
order transition features in this work.
5.2 Lexical features
In CRFs, a lexical feature (A2) is implemented as
a binary function that indicates whether a specific
word co-occurs with a state label. The set of words
to be considered in this work are those observed
in the training data. We can also generalize this
type of features from words to n-grams. In other
words, instead of inspecting the word identity at
the current position, we inspect the n-gram iden-
tity by applying a window of length n centered at
the current position.
Since feature functions are defined on segments
in semi-Markov CRFs, we create B2 that indicates
whether the phrase in a hypothesized query seg-
ment co-occurs with a state label. Here the set of
phrase identities are extracted from the query seg-
ments in the training data. Furthermore, we create
another type of lexical feature, B3, which is acti-
vated when a specific word occurs in a hypothe-
sized query segment. The use of B3 would favor
unseen words being included in adjacent segments
rather than to be isolated as separate segments.
5.3 Semantic features
Models relying on lexical features may require
very large amounts of training data to produce
accurate prediction performance, as the feature
space is in general large and sparse. To make our
model generalize better, we create semantic fea-
tures based on what we call lexicons. A lexicon,
denoted as L, is a cluster of semantically-related
words/phrases. For example, a cluster of movie
titles or director names can be such a lexicon. Be-
fore describing how such lexicons are generated
for our task, we first introduce the forms of the
semantic features assuming the availability of the
lexicons.
We let L denote a lexicon, and WL denote the
set of n-grams extracted from L. For CRFs, we
create a binary function that indicates whether any
n-gram in WL co-occurs with a state label, with
n = 1, 2 for A3, A4 respectively. For both A3
and A4, the number of such semantic features is
equal to the number of lexicons multiplied by the
number of state labels.
The same source of semantic knowledge can be
conveniently incorporated in semi-Markov CRFs.
One set of semantic features (B4) inspect whether
the phrase of a hypothesized query segment
matches any element in a given lexicon. A sec-
ond set of semantic features (B5) relax the exact
match constraints made by B4, and take as the fea-
ture value the maximum ?similarity? between the
query segment and all lexicon elements. The fol-
1341
lowing similarity function is used in this work ,
s(xuj :vj , l) = 1? Lev(xuj :vj , l)/|l| (4)
where Lev represents the Levenshtein distance.
Notice that we normalize the Levenshtein distance
by the length of the lexicon element, as we em-
pirically found it performing better compared with
normalizing by the length of the segment. In com-
puting the maximum similarity, we first retrieve a
set of lexicon elements with a positive tf-idf co-
sine distance with the segment; we then evaluate
Equation (4) for each retrieved element and find
the one with the maximum similarity score.
Lexicon generation
To create the semantic features described above,
we generate two types of lexicons leveraging
databases and query logs for each intent class.
The first type of lexicon is an IH lexicon com-
prised of a list of attribute names for the intent
class, e.g., ?box office? and ?review? for the intent
class Movie. One easy way of composing such a
list is by aggregating the column names in the cor-
responding database such as Table 1. However,
this approach may result in low coverage on IHs
for some domains. Moreover, many database col-
umn names, such as Title, are unlikely to appear as
IHs in queries. Inspired by Pasca and Van Durme
(2007), we apply a bootstrapping algorithm that
automatically learns attribute names for an intent
class from query logs. The key difference from
their work is that we create templates that consist
of semantic labels at the segment level from train-
ing data. For example, ?alice in wonderland 2010
cast? is labeled as ?IM:Title IM:Year IH?, and thus
?IM:Title + IM:Year + #? is used as a template. We
select the most frequent templates (top 2 in this
work) from training data and use them to discover
new IH phrases from the query log.
Secondly, we have a set IM lexicons, each com-
prised of a list of attribute values of an attribute
name in Ac. We exploit internal resources to gen-
erate such lexicons. For example, the lexicon for
IM:Title (in Movie) is a list of movie titles gener-
ated by aggregating the values in the Title column
of a movie database. Similarly, the lexicon for
IM:Employee (in Job) is a list of employee names
extracted from a job listing database. Note that
a substantial amount of research effort has been
dedicated to automatic lexicon acquisition from
the Web (Pantel and Pennacchiotti, 2006; Pennac-
chiotti and Pantel, 2009). These techniques can be
used in expanding the semantic lexicons for IMs
when database resources are not available. But we
do not use such techniques in our work since the
lexicons extracted from databases in general have
good precision and coverage.
5.4 Syntactic features
As mentioned in Section 3.2, web queries often
lack syntactic cues and do not necessarily follow
the linear order principle. Consequently, applying
syntactic analysis such as POS tagging or chunk-
ing using models trained on natural language cor-
pora is unlikely to give accurate results on web
queries, as supported by our experimental evi-
dence in Section 6.3. It may be beneficial, how-
ever, to use syntactic analysis results as additional
evidence in learning.
To this end, we generate a sequence of POS tags
for a given query, and use the co-occurrence of
POS tag identities and state labels as syntactic fea-
tures (A5) for CRFs.
For semi-Markov CRFs, we instead examine
the POS tag sequence of the corresponding phrase
in a query segment. Again their identities are com-
bined with state labels to create syntactic features
B6. Furthermore, since it is natural to incorporate
segment-level features in semi-Markov CRFs, we
can directly use the output of a syntactic chunker.
To be precise, if a query segment is determined by
the chunker to be a chunk, we use the indicator of
the phrase type of the chunk (e.g., NP, PP) com-
bined with a state label as the feature, denoted by
B7 in the Table. Such features are not activated if
a query segment is determined not to be a chunk.
6 Evaluation
6.1 Data
To evaluate our proposed models and features, we
collected queries from three domains, Movie, Job
and National Park, and had them manually anno-
tated. The annotation was given on both segmen-
tation of the queries and classification of the seg-
ments according to the label sets defined in Ta-
ble 3. There are 1000/496 samples in the train-
ing/test set for the Movie domain, 600/366 for the
Job domain and 491/185 for the National Park do-
main. In evaluation, we report the test-set perfor-
mance in each domain as well as the average per-
formance (weighted by their respectively test-set
size) over all domains.
1342
Movie Job National Park
IH trailer, box office IH listing, salary IH lodging, calendar
IM:Award oscar best picture IM:Category engineering IM:Category national forest
IM:Cast johnny depp IM:City las vegas IM:City page
IM:Character michael corleone IM:County orange IM:Country us
IM:Category tv series IM:Employer walmart IM:Name yosemite
IM:Country american IM:Level entry level IM:POI volcano
IM:Director steven spielberg IM:Salary high-paying IM:Rating best
IM:Genre action IM:State florida IM:State flordia
IM:Rating best IM:Type full time
IM:Title the godfather
Other the, in, that Other the, in, that Other the, in, that
Table 3: Label sets and their respective query segment examples for the intent class Movie, Job and
National Park.
6.2 Metrics
There are two evaluation metrics used in our work:
segment F1 and sentence accuracy (Acc). The
first metric is computed based on precision and re-
call at the segment level. Specifically, let us as-
sume that the true segment sequence of a query
is s = (s1, s2, . . . , sN ), and the decoded segment
sequence is s? = (s?1, s?2, . . . , s?K). We say that
s?k is a true positive if s?k ? s. The precision
and recall, then, are measured as the total num-
ber of true positives divided by the total num-
ber of decoded and true segments respectively.
We report the F1-measure which is computed as
2 ? prec ? recall/(prec + recall).
Secondly, a sentence is correct if all decoded
segments are true positives. Sentence accuracy is
measured by the total number of correct sentences
divided by the total number of sentences.
6.3 Results
We start with models that incorporate first-order
transition features which are standard for both
Markov and semi-Markov CRFs. We then exper-
iment with lexical features, semantic features and
syntactic features for both models. Table 4 and
Table 5 give a summarization of all experimental
results.
Lexical features
The first experiment we did is to evaluate the per-
formance of lexical features (combined with tran-
sition features). This involves the use of A2 in Ta-
ble 2 for CRFs, and B2 and B3 for semi-Markov
CRFs. Note that adding B3, i.e., indicators of
whether a query segment contains a word iden-
tity, gave an absolute 7.0%/3.2% gain in sentence
accuracy and segment F1 on average, as shown
in the row B1-B3 in Table 5. For both A2 and
B3, we also tried extending the features based on
word IDs to those based on n-gram IDs, where
n = 1, 2, 3. This greatly increased the number of
lexical features but did not improve learning per-
formance, most likely due to the limited amounts
of training data coupled with the sparsity of such
features. In general, lexical features do not gener-
alize well to the test data, which accounts for the
relatively poor performance of both models.
Semantic features
We created IM lexicons from three in-house
databases on Movie, Job and National Parks.
Some lexicons, e.g., IM:State, are shared across
domains. Regarding IH lexicons, we applied the
bootstrapping algorithm described in Section 5.3
to a 1-month query log of Bing. We selected the
most frequent 57 and 131 phrases to form the IH
lexicons for Movie and National Park respectively.
We do not have an IH lexicon for Job as the at-
tribute names in that domain are much fewer and
are well covered by training set examples.
We implemented A3 and A4 for CRFs, which
are based on the n-gram sets created from lex-
icons; and B4 and B5 for semi-Markov CRFs,
which are based on exact and fuzzy match with
lexicon items. As shown in Table 4 and 5, drastic
increases in sentence accuracies and F1-measures
were observed for both models.
Syntactic features
As shown in the row A1-A5 in Table 4, combined
with all other features, the syntactic features (A5)
built upon POS tags boosted the CRF model per-
formance. Table 6 listed the most dominant pos-
itive and negative features based on POS tags for
Movie (features for the other two domains are not
reported due to space limit). We can see that
many of these features make intuitive sense. For
1343
Movie Job National Park Average
Features Acc F1 Acc F1 Acc F1 Acc F1
A1,A2: Tran + Lex 59.9 75.8 65.6 84.7 61.6 75.6 62.1 78.9
A1-A3: Tran + Lex + Sem 67.9 80.2 70.8 87.4 70.5 80.8 69.4 82.8
A1-A4: Tran + Lex + Sem 72.4 83.5 72.4 89.7 71.1 82.3 72.2 85.0
A1-A5: Tran + Lex + Sem + Syn 74.4 84.8 75.1 89.4 75.1 85.4 74.8 86.5
A2-A5: Lex + Sem + Syn 64.9 78.8 68.1 81.1 64.8 83.7 65.4 81.0
Table 4: Sentence accuracy (Acc) and segment F1 (F1) using CRFs with different features.
Movie Job National Park Average
Features Acc F1 Acc F1 Acc F1 Acc F1
B1,B2: Tran + Lex 53.4 71.6 59.6 83.8 60.0 77.3 56.7 76.9
B1-B3: Tran + Lex 61.3 77.7 65.9 85.9 66.0 80.7 63.7 80.1
B1-B4: Tran + Lex + Sem 73.8 83.6 76.0 89.7 74.6 85.3 74.7 86.1
B1-B5: Tran + Lex + Sem 75.0 84.3 76.5 89.7 76.8 86.8 75.8 86.6
B1-B6: Tran + Lex + Sem + Syn 75.8 84.3 76.2 89.7 76.8 87.2 76.1 86.7
B1-B5,B7: Tran + Lex + Sem + Syn 75.6 84.1 76.0 89.3 76.8 86.8 75.9 86.4
B2-B6:Lex + Sem + Syn 72.0 82.0 73.2 87.9 76.5 89.3 73.8 85.6
Table 5: Sentence accuracy (Acc) and segment F1 (F1) using semi-Markov CRFs with different features.
example, IN (preposition or subordinating con-
junction) is a strong indicator of Other, while TO
and IM:Date usually do not co-occur. Some fea-
tures, however, may appear less ?correct?. This
is largely due to the inaccurate output of the POS
tagger. For example, a large number of actor
names were mis-tagged as RB, resulting in a high
positive weight of the feature (RB, IM:Cast).
Positive Negative
(IN, Other), (TO, IM:Date)
(VBD, Other) (IN, IM:Cast)
(CD, IM:Date) (CD, IH)
(RB, IM:Cast) (IN, IM:Character)
Table 6: Syntactic features with the largest posi-
tive/negative weights in the CRF model for Movie
Similarly, we added segment-level POS tag fea-
tures (B6) to semi-Markov CRFs, which lead to
the best overall results as shown by the highlighted
numbers in Table 5. Again many of the dominant
features are consistent with our intuition. For ex-
ample, the most positive feature for Movie is (CD
JJS, IM:Rating) (e.g. 100 best). When syntactic
features based on chunking results (B7) are used
instead of B6, the performance is not as good.
Transition features
In addition, it is interesting to see the importance
of transition features in both models. Since web
queries do not generally follow the linear order
principle, is it helpful to incorporate transition fea-
tures in learning? To answer this question, we
dropped the transition features from the best sys-
tems, corresponding to the last rows in Table 4
and 5. This resulted in substantial degradations
in performance. One intuitive explanation is that
although web queries are relatively ?order-free?,
statistically speaking, some orders are much more
likely to occur than others. This makes it benefi-
cial to use transition features.
Comparison to syntactic analysis
Finally, we conduct a simple experiment by using
the heuristics described in Section 3.2 in extract-
ing IHs from queries. The precision and recall of
IHs averaged over all 3 domains are 50.4% and
32.8% respectively. The precision and recall num-
bers from our best model-based system, i.e., B1-
B6 in Table 5, are 89.9% and 84.6% respectively,
which are significantly better than those based on
pure syntactic analysis.
7 Conclusions
In this work, we make the first attempt to define
the semantic structure of noun phrase queries. We
propose statistical methods to automatically ex-
tract IHs, IMs and the semantic labels of IMs us-
ing a variety of features. Experiments show the ef-
fectiveness of semantic features and syntactic fea-
tures in both Markov and semi-Markov CRF mod-
els. In the future, it would be useful to explore
other approaches to automatic lexicon discovery
to improve the quality or to increase the coverage
of both IH and IM lexicons, and to systematically
evaluate their impact on query understanding per-
formance.
The author would like to thank Hisami Suzuki
and Jianfeng Gao for useful discussions.
1344
References
Jaime Arguello, Fernando Diaz, Jamie Callan, and
Jean-Francois Crespo. 2009. Sources of evidence
for vertical selection. In SIGIR?09: Proceedings of
the 32st Annual International ACM SIGIR confer-
ence on Research and Development in Information
Retrieval.
Cory Barr, Rosie Jones, and Moira Regelson. 2008.
The linguistic structure of English web-search
queries. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1021?1030.
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
2005. Learning to rank using gradient descent. In
ICML?05: Proceedings of the 22nd international
conference on Machine learning, pages 89?96.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Jianfeng Gao, Jian-Yun Nie, Jian Zhang, Endong Xun,
Ming Zhou, and Chang-Ning Huang. 2001. Im-
proving query translation for CLIR using statistical
models. In SIGIR?01: Proceedings of the 24th An-
nual International ACM SIGIR conference on Re-
search and Development in Information Retrieval.
Jinyoung Kim, Xiaobing Xue, and Bruce Croft. 2009.
A probabilistic retrieval model for semistructured
data. In ECIR?09: Proceedings of the 31st Euro-
pean Conference on Information Retrieval, pages
228?239.
John Lafferty, Andrew McCallum, and Ferdando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning, pages 282?289.
Fangtao Li, Xian Zhang, Jinhui Yuan, and Xiaoyan
Zhu. 2008a. Classifying what-type questions by
head noun tagging. In COLING?08: Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pages 481?488.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2008b. Learn-
ing query intent from regularized click graph. In
SIGIR?08: Proceedings of the 31st Annual Interna-
tional ACM SIGIR conference on Research and De-
velopment in Information Retrieval, July.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extract-
ing structured information from user queries with
semi-supervised conditional random fields. In SI-
GIR?09: Proceedings of the 32st Annual Interna-
tional ACM SIGIR conference on Research and De-
velopment in Information Retrieva.
Mehdi Manshadi and Xiao Li. 2009. Semantic tagging
of web search queries. In Proceedings of the 47th
Annual Meeting of the ACL and the 4th IJCNLP of
the AFNLP.
Andrew McCallum and Wei Li. 2003. Early results for
named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003, pages 188?
191.
Donald Metzler and Bruce Croft. 2005. Analysis of
statistical question classification for fact-based ques-
tions. Jounral of Information Retrieval, 8(3).
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally har-vesting semantic relations. In Proceedings
of the 21st International Conference on Computa-
tional Linguis-tics and the 44th annual meeting of
the ACL, pages 113?120.
Stelios Paparizos, Alexandros Ntoulas, John Shafer,
and Rakesh Agrawal. 2009. Answering web queries
using structured data sources. In Proceedings of the
35th SIGMOD international conference on Manage-
ment of data.
Marius Pasca and Benjamin Van Durme. 2007. What
you seek is what you get: Extraction of class at-
tributes from query logs. In IJCAI?07: Proceedings
of the 20th International Joint Conference on Artifi-
cial Intelligence.
Marius Pasca and Benjamin Van Durme. 2008.
Weakly-supervised acquisition of open-domain
classes and class attributes from web documents and
query logs. In Proceedings of ACL-08: HLT.
Marco Pennacchiotti and Patrick Pantel. 2009. Entity
extraction via ensemble semantics. In EMNLP?09:
Proceedings of Conference on Empirical Methods in
Natural Language Processing, pages 238?247.
Stephen Robertson, Hugo Zaragoza, and Michael Tay-
lor. 2004. Simple BM25 extension to multiple
weighted fields. In CIKM?04: Proceedings of the
thirteenth ACM international conference on Infor-
mation and knowledge management, pages 42?49.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
Markov conditional random fields for information
extraction. In Advances in Neural Information Pro-
cessing Systems (NIPS?04).
Dou Shen, Jian-Tao Sun, Qiang Yang, and Zheng Chen.
2006. Building bridges for web query classification.
In SIGIR?06: Proceedings of the 29th Annual Inter-
national ACM SIGIR conference on research and de-
velopment in information retrieval, pages 131?138.
1345
