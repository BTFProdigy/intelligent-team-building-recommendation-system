Proceedings of NAACL HLT 2009: Short Papers, pages 133?136,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Speech Understanding Framework
that Uses Multiple Language Models and Multiple Understanding Models
?Masaki Katsumaru, ?Mikio Nakano, ?Kazunori Komatani,
?Kotaro Funakoshi, ?Tetsuya Ogata, ?Hiroshi G. Okuno
?Graduate School of Informatics, Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto
606-8501, Japan
{katumaru, komatani}@kuis.kyoto-u.ac.jp
{ogata, okuno}@kuis.kyoto-u.ac.jp
?Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako, Saitama
351-0188, Japan
{nakano, funakoshi}@jp.honda-ri.com
Abstract
The optimal combination of language model
(LM) and language understanding model
(LUM) varies depending on available training
data and utterances to be handled. Usually, a
lot of effort and time are needed to find the op-
timal combination. Instead, we have designed
and developed a new framework that uses
multiple LMs and LUMs to improve speech
understanding accuracy under various situa-
tions. As one implementation of the frame-
work, we have developed a method for select-
ing the most appropriate speech understand-
ing result from several candidates. We use
two LMs and three LUMs, and thus obtain six
combinations of them. We empirically show
that our method improves speech understand-
ing accuracy. The performance of the oracle
selection suggests further potential improve-
ments in our system.
1 Introduction
The speech understanding component in a spoken
dialogue system consists of an automatic speech
recognition (ASR) component and a language un-
derstanding (LU) component. To develop a speech
understanding component, we need to prepare an
ASR language model (LM) and a language under-
standing model (LUM) for the dialogue domain
of the system. There are many types of LMs
such as finite-state grammars and N-grams, and
many types of LUMs such as finite-state transduc-
ers (FST), weighted finite-state transducers (WFST),
and keyphrase-extractors (extractor). Selecting a
suitable combination of LM and LUM is necessary
for robust speech understanding against various user
utterances.
Conventional studies of speech understanding
have investigated which LM and LUM give the best
performance by using fixed training and test data
such as the Air Travel Information System (ATIS)
corpus. However, in real system development, re-
sources such as training data for statistical models
and efforts to write finite-state grammars vary ac-
cording to the available human resources or budgets.
Domain-dependent training data are particularly dif-
ficult to obtain. Therefore, in conventional system
development, system developers determine the types
of LM and LUM by trial and error. Every LM and
LUM has some advantages and disadvantages, so it
is difficult for a single combination of LM and LUM
to gain high accuracy except in a situation involv-
ing a lot of training data and effort. Therefore, using
multiple speech understanding methods is a more ef-
fective approach.
In this paper, we propose a speech understand-
ing framework called ?Multiple Language models
and Multiple Understanding models (MLMU)?, in
which multiple LMs and LUMs are used, to achieve
better performance under the various development
situations. It selects the best speech understanding
result from the multiple results generated by arbi-
trary combinations of LMs and LUMs.
So far there have been several attempts to im-
prove ASR and speech understanding using mul-
tiple speech recognizers and speech understanding
modules. ROVER (Fiscus, 1997) tried to improve
ASR accuracy by integrating the outputs of multi-
ple ASRs with different acoustic and language mod-
133
LM 1utterance LM 2
LM n 
resultconfidenceintegrationcomponent
LUcomponent speechunderstandingresultsASRcomponent
LUM n LUM 2
LUM 1
LM: Language Model
LUM: Language Understanding Model
Figure 1: Flow of speech understanding in MLMU
els. The work is different from our study in the fol-
lowing two points: it does not deal with speech un-
derstanding, and it assumes that each ASR is well-
developed and achieves high accuracy for a variety
of speech inputs. Eckert et al (1996) used multiple
LMs to deal with both in-grammar utterances and
out-of-grammar utterances, but did not mention lan-
guage understanding. Hahn et al (2008) used mul-
tiple LUMs, but just a single language model.
2 Speech Understanding Framework
MLMU
MLMU is a framework by which system developers
can use multiple speech understanding methods by
preparing multiple LMs and multiple LUMs. Fig-
ure 1 illustrates the flow of speech understanding in
MLMU. System developers list available LMs and
LUMs for each system?s domain, and the system
understands utterances by using these models. The
framework selects one understanding result from
multiple results or calculates a confidence score of
the result by using the generated multiple under-
standing results.
MLMU can improve speech understanding for the
following reason. The performance of each speech
understanding (a combination of LM and LUM)
might not be very high when either training data for
the statistical model or available expertise and ef-
fort for writing grammar are insufficient. In such
cases, some utterances might not be covered by the
system?s finite-state grammar LM, and probability
estimation in the statistical models may not be very
good. Using multiple speech understanding mod-
els is expected to solve this problem because each
model has different specialities. For example, finite-
state grammar LMs and FST-based LUMs achieve
high accuracy in recognizing and understanding in-
grammar utterances, whereas out-of-grammar utter-
ances are covered by N-gram models and LUMs
based on WFST and keyphrase-extractors. There-
fore it is more possible that the understanding results
of MLMU will include the correct result than a case
when a single understanding model is used.
The understanding results of MLMU will be help-
ful in many ways. We used them to achieve better
understanding accuracy by selecting the most reli-
able one. This selection is based on features con-
cerning ASR results and language understanding re-
sults. It is also possible to delay the selection, hold-
ing multiple understanding result candidates that
will be disambiguated as the dialogue proceeds (Bo-
hus, 2004). Furthermore, confidence scores, which
enable an efficient dialogue management (Komatani
and Kawahara, 2000), can be calculated by ranking
these results or by voting on them, by using multi-
ple speech understanding results. The understanding
results can be used in the discourse understanding
module and the dialogue management module. They
can choose one of the understanding results depend-
ing on the dialogue situation.
3 Implementation
3.1 Available Language Models and Language
Understanding Models
We implemented MLMU as a library of RIME-
TK, which is a toolkit for building multi-domain
spoken dialogue systems (Nakano et al, 2008).
With the current implementation, developers can use
the following LMs:
1. A LM based on finite-state grammar (FSG)
2. A domain-dependent statistical N-gram model
(N-gram)
and the following LUMs:
1. Finite-state transducer (FST)
2. Weighted FST (WFST)
3. Keyphrase-extractor (extractor).
System developers can use multiple finite-state-
grammar-based LMs or N-gram-based LMs, and
134
also multiple FSTs and WFSTs. They can specify
the combination for each domain by preparing LMs
and LUMs. They can specify grammar models when
sufficient human labor is available for writing gram-
mar, and specify statistical models when a corpus for
training models is available.
3.2 Selecting Understanding Result based on
ASR and LU Features
We also implemented a mechanism for selecting one
of the understanding results as the best hypothesis.
The mechanism chooses the result with the highest
estimated probability of correctness. Probabilities
are estimated for each understanding result by using
logistic regression, which uses several ASR and LU
features.
We define Pi as the probability that speech under-
standing result i is correct, and we select one result
based on argmax
i
Pi. We denote each speech un-
derstanding result as i (i = 1,. . . ,6). We constructed
a logistic regression model for Pi. The regression
function can be written as:
Pi = 11 + exp(?(ai1Fi1 + . . . + aimFim + bi)) .(1)
The coefficients ai1, . . . , aim, bi were fitted us-
ing training data. The independent variables
Fi1, Fi2, ..., Fim are listed in Table 1. In the table,
n indicates the number of understanding results, that
is, n = 6 in this paper?s experiment. Here, we denote
the features as Fi1, Fi2, ..., Fim.
Features from Fi1 to Fi3 represent characteristics
of ASR results. The acoustic scores were normal-
ized by utterance durations in seconds. These fea-
tures are used for verifying its ASR result. Features
from Fi4 to Fi9 represent characteristics of LU re-
sults. Features from Fi4 to Fi6 are defined on the
basis of the concept-based confidence scores (Ko-
matani and Kawahara, 2000).
4 Preliminary Experiment
We conducted a preliminary experiment to show the
potential of the framework by using the two LMs
and three LUMs noted in Section 3.1.
Table 1: Features from speech understanding result i
Fi1: acoustic score of ASR
Fi2: difference between Fi1 and acoustic score
of ASR for utterance verification
Fi3: utterance duration [sec.]
Fi4: average confidence scores for concepts in i
Fi5: average of Fi4 ( 1n
?n
i Fi4)
Fi6: proportion of Fi4 (Fi4 /?ni Fi5)
Fi7: average # concepts ( 1n
?n
i #concepti)
Fi8: max. # concepts (max (#concepti) )
Fi9: min. # concepts (min (#concepti) )
4.1 Preparing LMs and LUMs
The finite-state grammar rules were written in sen-
tence units manually. A domain-dependent statisti-
cal N-gram model was trained on 10,000 sentences
randomly generated from the grammar. The vocab-
ulary sizes of the grammar LM and the domain-
dependent statistical LM were both 278. We
also used a domain-independent statistical N-gram
model for obtaining acoustic scores for utterance
verification, which was trained onWeb texts (Kawa-
hara et al, 2004). Its vocabulary size was 60,250.
The grammar used in the FST was the same as the
FSG used as one of the LMs, which was manually
written by a system developer. The WFST-based LU
was based on a method to estimate WFST parame-
ters with a small amount of data (Fukubayashi et al,
2008). Its parameters were estimated by using 105
utterances of just one user. The keyphrase extrac-
tor extracts as many concepts as possible from an
ASR result on the basis of a grammar while ignor-
ing words that do not match the grammar.
4.2 Target Data for Evaluation
We used 3,055 utterances in the rent-a-car reserva-
tion domain (Nakano et al, 2007). We used Julius
(ver. 4.0.2) as the speech recognizer and a 3000-
state phonetic tied-mixture (PTM) triphone model
as the acoustic model1. ASR accuracy in mora ac-
curacy when using the FSG and the N-gram model
were 71.9% and 75.5% respectively. We used con-
cept error rates (CERs) to represent the speech un-
derstanding accuracy, which is calculated as fol-
1http://julius.sourceforge.jp/
135
Table 2: CERs [%] for each speech understanding
method
speech understanding method
(LM + LUM) CER
(1) FSG + FST 26.9
(2) FSG + WFST 29.9
(3) FSG + extractor 27.1
(4) N-gram + FST 35.2
(5) N-gram + WFST 25.3
(6) N-gram + extractor 26.0
selection from (1) through (6) (our method) 22.7
oracle selection 13.5
lows:
CER = # error concepts#concepts in utterances . (2)
We manually annotated whether an understanding
result of each utterance was correct or not, and
used them as training data to fit the coefficients
ai1, . . . , aim, bi.
4.3 Evaluation in Concept Error Rates
We fitted the coefficients of regression functions and
selected understanding results with a 10-fold cross
validation. Table 2 lists the CERs based on combi-
nations of single LM and LUM and by our method.
Of all combinations of single LM and LUM, the best
accuracy was obtained with (5) (N-gram + WFST).
Our method improved by 2.6 points over (5). Al-
though we achieved a lower CER, we used a lot
of data to estimate logistic regression coefficients.
Such a large amount of data may not be available in a
real situation. We will conduct more experiments by
changing the amount of training data. Table 2 also
shows the accuracy of the oracle selection, which
selected the best speech understanding result man-
ually. The CER of the oracle selection was 13.5%,
a significant improvement compared to all combina-
tions of a LM and LUM. There is no combination of
a LM and LUM whose understanding results were
not selected at all in the oracle selection and our
method?s selection. These results show that using
multiple LMs and multiple LUMs can potentially
improve speech understanding accuracy.
5 Ongoing work
We will conduct more experiments in other domains
or with other resources to evaluate the effectiveness
of our framework. We plan to investigate the case
in which a smaller amount of the training data is
used to estimate the coefficients of the logistic re-
gressions. Furthermore, finding a way to calculate
confidence scores of speech understanding results is
on our agenda.
References
Dan Bohus. 2004. Error awareness and recovery in
task-oriented spoken dialogue systems. Ph.D. thesis,
Carnegie Mellon University.
Wieland Eckert, Florian Gallwitz, and Heinrich Nie-
mann. 1996. Combining stochastic and linguistic lan-
guage models for recognition of spontaneous speech.
In Proc. ICASSP, pages 423?426.
Jonathan G. Fiscus. 1997. A post-processing system
to yield reduced word error rates: Recognizer Out-
put Voting Error Reduction (ROVER). In Proc. ASRU,
pages 347?354.
Yuichiro Fukubayashi, Kazunori Komatani, Mikio
Nakano, Kotaro Funakoshi, Hiroshi Tsujino, Tetsuya
Ogata, and Hiroshi G. Okuno. 2008. Rapid prototyp-
ing of robust language understanding modules for spo-
ken dialogue systems. In Proc. IJCNLP, pages 210?
216.
Stefan Hahn, Patrick Lehnen, and Hermann Ney. 2008.
System combination for spoken language understand-
ing. In Proc. Interspeech, pages 236?239.
Tatsuya Kawahara, Akinobu Lee, Kazuya Takeda, Kat-
sunobu Itou, and Kiyohiro Shikano. 2004. Recent
progress of open-source LVCSR Engine Julius and
Japanese model repository. In Proc. ICSLP, pages
3069?3072.
Kazunori Komatani and Tatsuya Kawahara. 2000.
Flexible mixed-initiative dialogue management using
concept-level confidence measures of speech recog-
nizer output. In Proc. COLING, volume 1, pages 467?
473.
Mikio Nakano, Yuka Nagano, Kotaro Funakoshi, Toshi-
hiko Ito, Kenji Araki, Yuji Hasegawa, and Hiroshi Tsu-
jino. 2007. Analysis of user reactions to turn-taking
failures in spoken dialogue systems. In Proc. SIGdial,
pages 120?123.
Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, and
Hiroshi Tsujino. 2008. A framework for building con-
versational agents based on a multi-expert model. In
Proc. SIGdial, pages 88?91.
136
Coling 2010: Poster Volume, pages 579?587,
Beijing, August 2010
Automatic Allocation of Training Data for Rapid Prototyping
of Speech Understanding based on Multiple Model Combination
Kazunori Komatani? Masaki Katsumaru? Mikio Nakano?
Kotaro Funakoshi? Tetsuya Ogata? Hiroshi G. Okuno?
? Graduate School of Informatics, Kyoto University
{komatani,katumaru,ogata,okuno}@kuis.kyoto-u.ac.jp
? Honda Research Institute Japan Co., Ltd.
{nakano,funakoshi}@jp.honda-ri.com
Abstract
The optimal choice of speech understand-
ing method depends on the amount of
training data available in rapid prototyp-
ing. A statistical method is ultimately
chosen, but it is not clear at which point
in the increase in training data a statisti-
cal method become effective. Our frame-
work combines multiple automatic speech
recognition (ASR) and language under-
standing (LU) modules to provide a set
of speech understanding results and se-
lects the best result among them. The
issue is how to allocate training data to
statistical modules and the selection mod-
ule in order to avoid overfitting in training
and obtain better performance. This paper
presents an automatic training data alloca-
tion method that is based on the change
in the coefficients of the logistic regres-
sion functions used in the selection mod-
ule. Experimental evaluation showed that
our allocation method outperformed base-
line methods that use a single ASR mod-
ule and a single LU module at every point
while training data increase.
1 Introduction
Speech understanding in spoken dialogue systems
is the process of extracting a semantic represen-
tation from a user?s speech. That is, it consists
of automatic speech recognition (ASR) and lan-
guage understanding (LU). Because vocabularies
and language expressions depend on individual
systems, it needs to be constructed for each sys-
tem, and accordingly, training data are required
for each. To collect more real training data, which
will lead to higher performance, it is more desir-
able to use a prototype system than that based on
the Wizard-of-Oz (WoZ) method where real ASR
errors cannot be observed, and to use a more ac-
curate speech understanding module. That is, in
the bootstrapping phase, spoken dialogue systems
need to operate before sufficient real data have
been collected.
We have been addressing the issue of rapid pro-
totyping on the basis of the ?Multiple Language
model for ASR and Multiple language Under-
standing (MLMU)? framework (Katsumaru et al,
2009). In MLMU, the most reliable speech un-
derstanding result is selected from candidates pro-
duced by various combinations of multiple ASR
and LU modules using hand-crafted grammar and
statistical models. A grammar-based method is
still effective at an early stage of system devel-
opment because it does not require training data;
Schapire et al (2005) also incorporated human-
crafted prior knowledge into their boosting al-
gorithm. By combining multiple understanding
modules, complementary results can be obtained
by different kinds of ASR and LU modules.
We propose a novel method to allocate avail-
able training data to statistical modules when the
amount of training data increases. The training
data need to be allocated adaptively because there
are several modules to be trained, and they would
cause overfitting without data allocation. There
are speech understanding modules that have lan-
guage models (LMs) for ASR and LU models
579
(LUMs), and a selection module that selects the
most reliable speech understanding result from
multiple candidates in the MLMU framework.
When the amount of available training data is
small, and an LUM and the selection module are
trained on the same data set, they are trained un-
der a closed-set condition, and thus the training
data for the selection module include too many
correct understanding results. In such cases, the
data need to be divided into subdata sets to avoid
overfitting. On the other hand, when the amount
of available training data is large, so that overfit-
ting does not occur, all available data should be
used to train each statistical module to prepare as
much training data as possible.
We therefore develop a method for switching
data allocation policies. More specifically, two
points are automatically determined at which sta-
tistical modules with more parameters start to be
trained. As a result, better overall performance
is achieved at every point while the amount of
training data increases, compared with all combi-
nations of a single ASR module and a single LU
module.
2 Related Work
It is important to consider the amount of available
training data when designing a speech understand-
ing module. Many statistical LU methods have
been studied, e.g., (Wang and Acero, 2006; Jeong
and Lee, 2006; Raymond and Riccardi, 2007;
Hahn et al, 2008; Dinarelli et al, 2009). They
generally outperform grammar-based LU meth-
ods when a sufficient amount of training data is
available; but sufficient training data are not nec-
essarily available during rapid prototyping. Sev-
eral LU methods were constructed using a small
amount of training data (Fukubayashi et al, 2008;
Dinarelli et al, 2009). Fukubayashi et al (2008)
constructed an LU method based on the weighted
finite state transducer (WFST), in which filler
transitions accepting arbitrary inputs and transi-
tion weights were added to a hand-crafted FST.
This method is placed between a grammar-based
method and a statistical method because a sta-
tistically selected weighting scheme is applied
to a hand-crafted grammar model. Therefore,
the amount of training data can be smaller com-
pared with general statistical LU methods, but this
method does not outperform them when plenty of
training data are available. Dinarelli et al (2009)
used a generative model for which overfitting is
less prone to occur than discriminative models
when the amount of training data is small, but
they did not use a grammar-based model, which is
expected to achieve reasonable performance even
when the amount of training data is very small.
Raymond et al (2007) compared the perfor-
mances of statistical LU methods for various
amounts of training data. They used a statis-
tical finite-state transducer (SFST) as a genera-
tive model and a support vector machine (SVM)
and conditional random fields (CRF) as discrim-
inative models. The generative model was more
effective when the amount of data was small,
and the discriminative models were more effec-
tive when it was large. This shows that the perfor-
mance of an LUmethod depends on the amount of
training data available, and therefore, LU meth-
ods need to be switched automatically. Wang et
al. (2002) developed a two-stage speech under-
standing method by applying statistical methods
first and then grammatical rules. They also ex-
amined the performance of the statistical methods
at their first stage for various amounts of train-
ing data and confirmed that the performance is not
very high when a small amount of data is used.
Schapire et al (2005) showed that accuracy
of call classification in spoken dialogue systems
improved by incorporating hand-crafted prior
knowledge into their boosting algorithm. Their
idea is the same as ours in that they improve the
system?s performance by using hand-crafted hu-
man knowledge while only a small amount of
training data is available. We furthermore solve
the data allocation problem because there are mul-
tiple statistical models to be trained in speech
understanding, while their call classification has
only one statistical model.
3 MLMU Framework
MLMU is the framework for selecting the most
reliable speech understanding result from multi-
ple speech understanding modules (Katsumaru et
al., 2009). In this paper, we furthermore adapt the
selection module to the amount of available train-
580
LU model
#1
Language
model #1
LU
modules
ASR
modules
Result:
1
CM
N
CM
MN
CM
?
i
i
CMmaxarg
N
1
Selection module
LU
results
ASR
results
Utterance
ASR: automatic speech recognition
LU: language understanding
CM: confidence measure
M ?
M ?
Speech understanding
Language
model #2
Language
model #N
LU model
#2
LU model
#M
Logistic
regression #1
Logistic
regression #N
Logistic
regression #
Figure 1: Overview of speech understanding framework MLMU
ing data. More specifically, the allocation policy
of training data is changed and thus appropriate
LMs and LUMs are selected as its result.
An overview of MLMU is shown in Figure 1.
MLMU uses multiple LMs for ASR and multi-
ple LUMs and selects the most reliable speech un-
derstanding result from all combinations of them.
We denote a speech understanding module as SUi
(i = 1, . . . , n). Its result is a semantic representa-
tion consisting of a set of concepts. The concept is
either a semantic slot and its value or an utterance
type. Note that n = N ? M , when N LMs and
M LUMs are used. The confidence measure per
utterance for a result of i-th speech understanding
module SUi is denoted as CMi. The speech un-
derstanding result having the highest confidence
measure is selected as the final result for the ut-
terance. That is, the result is the output of SUm
where m = argmaxi CMi.
The confidence measure is calculated by logis-
tic regression based on the features of each speech
understanding result. A logistic regression func-
tion is constructed for each speech understanding
module SUi:
CMi =
1
1 + e?(ai1Fi1+...+ai7Fi7+bi) . (1)
Parameters ai1, . . . , ai7 and bi are determined by
using training data. In the training phase, teacher
signal 1 is given when a speech understanding re-
sult is completely correct; that is, when no error is
contained in the result. Otherwise, 0 is given. We
use seven features, Fi1, Fi2, . . . , Fi7, as indepen-
dent variables. Each feature value is normalized
Table 1: Features of speech understanding result
obtained from SUi
Fi1: Acoustic score normalized by utterance length
Fi2: Difference between Fi1 and normalized acoustic
scores of verification ASR
Fi3: Average concept CM in understanding result
Fi4: Minimum concept CM in understanding result
Fi5: Number of concepts in understanding result
Fi6: Whether any understanding result is obtained
Fi7: Whether understanding result is yes/no
CM: confidence measure
so as to make its mean zero and its variance one.
The features used are listed in Table 1. Com-
pared with those used in our previous paper (Kat-
sumaru et al, 2009), we deleted ones that were
highly correlated with other features and added
ones regarding content of the speech understand-
ing results. Features Fi1 and Fi2 are obtained
from an ASR result. Another ASR with a gen-
eral large vocabulary LM is executed for verifying
the i-th ASR result. Fi2 is the difference between
its score and Fi1 (Komatani et al, 2007). These
two features represent the reliability of the ASR
result. Fi3 and Fi4 are calculated for each concept
in the LU result on the basis of the posterior prob-
ability of the 10-best ASR candidates (Komatani
and Kawahara, 2000). Fi5 is the number of con-
cepts in the LU result. This feature is effective be-
cause the LU results of lengthy utterances tend to
be erroneous in a grammar-based LU. Fi6 repre-
sents the case when an ASR result is not accepted
by the subsequent LU module. In such cases, no
speech understanding result is obtained, which is
581
U1: It is June ninth.
ASR result:
- grammar ?It is June ninth.?
- N-gram ?It is June noon and?
LU result:
- grammar + FST ?month:6 day:9 type:refer-time?
- N-gram + WFST ?month:6 type:refer-time?
U2: I will borrow it on twentieth.
(Underlined part is out-of-grammar.)
ASR result:
- grammar ?Around two pm on twentieth.?
- N-gram ?Around two at ten on twentieth.?
LU result:
- grammar + FST ?day:20 hour:14 type:refer-time?
- N-gram + WFST ?day:20 type:refer-time?
Combination of LM and LUM is denoted as ?LM+LUM?.
Figure 2: Example of speech understanding re-
sults in MLMU framework
regarded as an error. Fi7 is added because affirma-
tive and negative responses, typically ?Yes? and
?No?, tend to be correctly recognized and under-
stood.
Figure 2 depicts an example when multiple
ASRs based on LMs and multiple LUs are used.
In short, the correct speech understanding result is
obtained from a different combination of LMs and
LUMs.
4 Automatic Allocation of Training Data
Using Change in Coefficients
The training data need to be allocated to the
speech understanding modules (i.e., statistical LM
and statistical LUM) and the selection module. If
more data are allocated to the ASR and LU mod-
ules, the performances of these modules are im-
proved, but the overall performance is degraded
because of the low performance of the selection
module. On the other hand, even if more training
data are allocated to the selection module, the per-
formance of each ASR and LU module remains
low.
4.1 Allocation Policy
We focus on the convergence of the logistic re-
gression functions when the amount of training
data increases. The convergence is defined as
the change in their coefficients, which will appear
later as Equation 2, and determines two points
1. All data are used to
train selection modules
2. Data are allocated to SU
and selection modules
3. Data are
not divided
No No
Yes
Yes
Selection module 
first converges?
No over-fitting
occurs?
Amount of training data increases
SU: speech understanding
Figure 3: Flowchart of data allocation
during the increase in training data, and thus three
phases are defined. The flowchart of data alloca-
tion is depicted in Figure 3. The three phases are
explained below.
In the first phase, the first priority is given to
the selection module. This is because the lo-
gistic regression functions used in the selection
module converge with relatively less training data
than those in the statistical ASR and LU mod-
ules for speech understanding; there are eight pa-
rameters for each logistic regression function as
shown in Equation 1, far fewer than for other sta-
tistical models such as N-gram and CRF. The out-
put from a speech understanding module that em-
ploys grammar-based LM and LUM would be the
most reliable in many cases because its perfor-
mance is better than that of other statistical mod-
ules when a very small amount of training data is
available. As a result, equivalent or better perfor-
mance would be achieved than methods using a
single ASR module and a single LU module.
In the second phase, the training data are also
allocated to the speech understanding modules af-
ter the selection module converges. This aims
to improve the performance of the speech under-
standing modules by allocating as much training
data to them as possible. The amount of train-
ing data is fixed in this phase to the amount al-
located to the selection module determined in the
first phase. The remaining data are used to train
the speech understanding modules.
When the performances of all the speech under-
standing modules stabilize, the allocation phase
proceeds to the third one. After this point, we
hypothesize that overfitting does not occur in this
phase because plenty of training data are avail-
able. All available data are used to train all mod-
582
ules without dividing the data in this phase.
4.2 Determining When to Switch Allocation
Policies
Automatic switching from one phase to the next
requires the determination of two points in the
number of training utterances: when the selec-
tion module first converges (konlysel) and when
the speech understanding modules all become sta-
ble (knodiv). These points are determined by fo-
cusing on the changes in the coefficients of the
logistic regression functions when the number of
utterances used as training data increases. We ob-
serve the sum of the changes in the coefficients of
the functions and then identify the points at which
the changes converge. The points are determined
individually by the following algorithm.
Step 1 Construct two logistic regression func-
tions for speech understanding module SUi
by using k and (k + ?k) utterances out of
kmax utterances, where kmax is the amount
of training data available.
Step 2 Calculate the change in coefficients from
the two logistic regression functions by
?i(k) =
?
j
|aij(k + ?k) ? aij(k)|
+|bi(k + ?k) ? bi(k)|, (2)
where aij(k) and bi(k) denote the param-
eters of the logistic regression functions,
shown in Equation 1, for speech understand-
ing module SUi, when k utterances are used
to train the functions.
Step 3 If ?i(k) becomes smaller than threshold
?, consider that the training of the functions
has converged, and record this k as the point
of convergence. If not, return to Step 1 after
k ? k + ?k.
The ?k is the minimum unit of training data con-
taining various utterances. We set it as the number
of utterances in one dialogue session, whose aver-
age was 17. Threshold ? was set to 8, which corre-
sponds to the number of parameters in the logistic
regression functions. No experiments were con-
ducted to determine if better performance could
be achieved with other choices of ?1.
The first point, konlysel, is determined using the
speech understanding module that uses no training
data. Specifically, we used ?grammar+FST? as
method SUi. Here, ?LM+LUM? denotes a com-
bination of LM for ASR and LUM. If the func-
tion converges at k utterances, we set konlysel to
k and fix the k utterances as training data used by
the selection module. The remaining (kmax ? k)
utterances are allocated to the speech understand-
ing modules, that is, the LMs and LUMs. Note
that if k becomes equal to kmax before ?i con-
verges, all training data are allocated to the selec-
tion module; that is, no data are allocated to the
LMs and LUMs. In this case, no output is ob-
tained from statistical speech understanding mod-
ules, and only outputs from the grammar-based
modules are used.
The second point, knodiv , is determined on the
basis of the speech understanding module that
needs the largest amount of data for training. The
amount of data needed depends on the number of
parameters. Specifically, we used ?N-gram+CRF?
as SUi in Equation 2. If the function converges,
we hypothesize that the performance of all the
speech understanding modules stabilize and thus
overfitting does not occur. We then stop the divi-
sion of training data, and use all available data to
train the statistical modules.
5 Experimental Evaluation
5.1 Target Data and Implementation
We used a data set previously collected through
actual dialogues with a rent-a-car reservation sys-
tem (Nakano et al, 2007) with 39 participants.
Each participant performed 8 dialogue sessions,
and 5900 utterances were collected in total. Out
of these utterances, we used 5240 for which the
automatic voice activity detection (VAD) results
agreed with manual annotation. We divided the
utterances into two sets: 2121 with 16 participants
as training data and 3119 with 23 participants as
the test data.
1We do not think the value is very critical after seeing the
results shown in Figure 4.
583
We constructed another rent-a-car reservation
system to evaluate our allocation method. The
system included two language models (LMs)
and four language understanding models (LUMs).
That is, eight speech understanding results in total
were obtained. The two LMs were a grammar-
based LM (?grammar?, hereafter) and a domain-
specific statistical LM (?N-gram?). The grammar
model was described by hand to be equivalent to
the FST model used in LU. The N-gram model
was a class 3-gram and was trained on a tran-
scription of the available training data. The vo-
cabulary size was 281 for the grammar model and
420 for the N-gram model when all the training
data were used. The ASR accuracies of the gram-
mar and N-gram models were 67.8% and 90.5%
for the training data and 66.3% and 85.0% for the
test data when all the training data were used. We
used Julius (ver. 4.1.2) as the speech recognizer
and a gender-independent phonetic-tied mixture
model as the acoustic model (Kawahara et al,
2004). We also used a domain-independent statis-
tical LM with a vocabulary size of 60250, which
was trained on Web documents (Kawahara et al,
2004), as the verification model.
The four LUMs were a finite-state transducer
(FST) model, a weighted FST (WFST) model,
a keyphrase-extractor (Extractor) model, and a
conditional random fields (CRF) model. In the
FST-based LUM, the FST was constructed by
hand. The WFST-based LUM is based on the
method developed by Fukubayashi et al (2008).
The WFSTs were constructed by using the MIT
FST Toolkit (Hetherington, 2004). The weight-
ing scheme used for the test data was selected by
using training data (Fukubayashi et al, 2008). In
the extractor-based LUM, as many parts as pos-
sible in the ASR result were simply transformed
into concepts. As the CRF-based LUM, we used
open-source software, CRF++2, to construct the
LUM. As its features, we use a word in the ASR
result, its first character, its last character, and the
ASR confidence of the word. Its parameters were
estimated by using training data.
The metric used for speech understanding per-
formance was concept understanding accuracy,
2http://crfpp.sourceforge.net/
Table 2: Absolute degradation in oracle accuracy
when each module was removed
Case (A) (B)
With all modules (%) 86.6 90.1
w/o grammar ASR -12.0 -1.1
w/o N-gram ASR -6.1 -7.7
w/o FST LUM -0.4 0.0
w/o WFST LUM -1.2 -0.5
w/o Extractor LUM -0.1 0.0
w/o CRF LUM -0.6 -3.7
(w/o FST & Extractor LUMs) -1.0 -0.1
(A): 141 utterances with 1 participant
(B): 2121 utterances with 16 participants
defined as
1 ? SUB + INS + DEL
no. of concepts in correct results,
where SUB, INS, and DEL denote the numbers of
substitution, insertion, and deletion errors.
5.2 Effectiveness of Using Multiple LMs and
LUMs
We investigated how much the performance of our
framework degraded when one ASR or LU mod-
ule was removed. We used the oracle accuracies,
i.e., when the most appropriate result was selected
by hand. The result reveals the contribution of
each ASR and LU module to the performance of
the framework. A module is regarded as more im-
portant when the accuracy is degraded more when
it is removed than when another one is removed.
Two cases (A) and (B) were defined: when the
amount of available training data was (A) small
and (B) large. We used 141 utterances with 1 par-
ticipant for case (A) and 2121 utterances with 16
participants for case (B). The results are shown in
Table 2.
When a small amount of training data was
available (case (A)), the accuracy was degraded by
12.0 points when the grammar-based ASRmodule
was removed and 6.1 points when the N-gram-
based ASR module was removed. The accuracy
was thus degraded substantially when either ASR
module was removed. This indicates that the two
ASR modules work complementarily.
584
020
40
60
80
100
120
140
160
180
200
0 100 200 300 400 500
C
h
a
n
g
e
s
 
i
n
 
c
o
e
f
f
i
c
i
e
n
t
s
Number of training utterances available
(a) grammar+FST
0
20
40
60
80
100
120
140
160
180
200
0 100 200 300 400 500
C
h
a
n
g
e
s
 
i
n
 
c
o
e
f
f
i
c
i
e
n
t
s
Number of training utterances available
(b) N-gram+CRF
Figure 4: Change in the sum of coefficients ?i when amount of training data increases (?LM+LUM?
denotes combination of LM and LUM)
On the other hand, when a large amount of
training data was available (case (B)), the ac-
curacy was degraded by 1.1 points when the
grammar-based ASR was removed. This means
that it became less important when there are
plenty of training data because the coverage of the
N-gram-based ASR became wider. In short, espe-
cially when the amount of training data is smaller,
speech understanding modules based on a hand-
crafted grammar are more important because of
the low performance of statistical modules.
Concerning the LUMs, the accuracy was de-
graded when any of the LUM modules was re-
moved when a small amount of training data was
available. When a large amount of training data
was available, the module based on CRF in par-
ticular became more important.
5.3 Results and Evaluation of Automatic
Allocation
Figure 4 shows the change in the sum of the co-
efficients, ?i, with the increase in the amount of
training data. In Figure 4(a), the change was very
large while the amount of training data was small,
and decreased dramatically and converged around
one hundred utterances. By applying ? (=8) to ?i,
we set 111 utterances as the first point, konlysel,
up to which all the training data are allocated to
the selection module, as described in Section 4.1.
Similarly, from the results shown in Figure 4(b),
we set 207 utterances as the second point, knodiv,
from which the training data are not divided.
To evaluate our method for allocating training
55
60
65
70
75
80
85
90
50 100 200 400 800 1600
C
o
n
c
e
p
t
 
u
n
d
e
r
s
t
a
n
d
i
n
g
 
a
c
c
u
r
a
c
y
 
[
%
]
Number of training utterances available
Our method
Na?ve allocation
No division
Figure 5: Results of allocation methods
data, we compared it with two baseline methods:
? No-division method: All data available at
each point were used to train both the speech
understanding modules and the selection
module. That is, the same data set was used
to train them.
? Naive-allocation method: Training data
available at each point were allocated equally
to the speech understanding modules and the
selection module.
As shown in Figure 5, our method had the best
concept understanding accuracy when the amount
of training data was small, that is, up to about
278 utterances. This indicates that our method for
allocating the available training data is effective
when the amount of training data is small.
This result is explained more specifically by us-
585
Table 3: Concept understanding accuracy for 141
utterances
Accuracy (%)
Our method 77.9
Naive allocation 73.5
No division 74.1
ing the case in which 141 utterances were used as
the training data. 111 (= konlysel) were secured to
train the selection module and 30 utterances were
allocated to train the speech understanding mod-
ules. As shown in Table 3, the accuracy with our
method was 3.8 points higher than that with the
no-division baseline method. This was achieved
by avoiding the overfitting of the logistic regres-
sion functions; i.e., the data input to the functions
became similar to the test data due to allocation,
so the concept understanding accuracy for the test
set was improved. The accuracy with our method
was 4.4 points higher than that with the naive al-
location baseline method. This was because the
amount of training data allocated to the selection
module was less than our method, and accordingly
the selection module was not trained sufficiently.
5.4 Comparison with methods using a single
ASR and a single LU
Figure 6 plots concept understanding accuracy
with our method against baseline methods using
a single ASR module and a single LU module for
various amounts of training data. Each module for
comparison was constructed by using all available
training data at each point while training data in-
creased; i.e., the same condition as our method.
The accuracies of only three speech understand-
ing modules are shown in the figure, out of the
eight obtained by combining two LMs for ASR
and four LUMs. These three are the ones with the
highest accuracies while the amount of training
data increased. Our method switched the alloca-
tion phase at 111 and 207 utterances, as described
in Section 5.3.
Our method performed equivalently or better
than all baseline methods even when only a small
amount of training data was available. As a result,
our method outperformed all the baseline methods
55
60
65
70
75
80
85
50 100 200 400 800 1600
C
o
n
c
e
p
t
 
u
n
d
e
r
s
t
a
n
d
i
n
g
 
a
c
c
u
r
a
c
y
 
[
%
]
Number of training utterances available
our method
grammar+FST
N-gram+WFST
N-gram+CRF
Figure 6: Comparison with baseline methods us-
ing single speech understanding
at every point while training data increase.
6 Conclusion
We developed a method to automatically allo-
cate training data to statistical modules so as to
avoid performance degradation caused by overfit-
ting. Experimental evaluation showed that speech
understanding accuracies achieved by our method
were equivalent or better than the baseline meth-
ods based on all combinations of a single ASR
module and a single LU module at every point
while training data increase. This includes a case
when a very small amount of training data is avail-
able. We also showed empirically that the training
data should be allocated while an amount of train-
ing data is not sufficient. Our method allocated
available training data on the basis of our alloca-
tion policy described in Section 4.1, and outper-
formed the two baselines where the training data
were equivalently allocated and not allocated.
When plenty of training data were available,
there was no difference between our method and
the speech understanding method that requires the
most training data, i.e., N-gram+CRF, as shown in
Figure 6. It is possible that our method combin-
ing multiple speech understanding modules would
outperform it as Schapire et al (2005) reported.
In their data, there were some examples that only
a hand-crafted rules can parse. Including such a
task as more complicated language understanding
grammar is required, verification of our method in
other tasks is one of the future works.
586
References
Dinarelli, Marco, Alessandro Moschitti, and Giuseppe
Riccardi. 2009. Re-Ranking Models for Spoken
Language Understanding. In Proc. European Chap-
ter of the Association for Computational Linguistics
(EACL), pages 202?210.
Fukubayashi, Yuichiro, Kazunori Komatani, Mikio
Nakano, Kotaro Funakoshi, Hiroshi Tsujino, Tet-
suya Ogata, and Hiroshi G. Okuno. 2008. Rapid
prototyping of robust language understanding mod-
ules for spoken dialogue systems. In Proc. Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP), pages 210?216.
Hahn, Stefan, Patrick Lehnen, and Hermann Ney.
2008. System Combination for Spoken Language
Understanding. In Proc. Annual Conference of the
International Speech Communication Association
(INTERSPEECH), pages 236?239.
Hetherington, Lee. 2004. The MIT Finite-State Trans-
ducer Toolkit for Speech and Language Processing.
In Proc. Int?l Conf. Spoken Language Processing
(ICSLP), pages 2609?2612.
Jeong, Minwoo and Gary Geunbae Lee. 2006. Ex-
ploiting non-local features for spoken language un-
derstanding. In Proc. COLING/ACL 2006 Main
Conference Poster Sessions, pages 412?419.
Katsumaru, Masaki, Mikio Nakano, Kazunori Ko-
matani, Kotaro Funakoshi, Tetsuya Ogata, and Hi-
roshi G. Okuno. 2009. Improving speech un-
derstanding accuracy with limited training data us-
ing multiple language models and multiple under-
standing models. In Proc. Annual Conference of
the International Speech Communication Associa-
tion (INTERSPEECH), pages 2735?2738.
Kawahara, Tatsuya, Akinobu Lee, Kazuya Takeda,
Katsunobu Itou, and Kiyohiro Shikano. 2004. Re-
cent progress of open-source LVCSR engine Julius
and Japanese model repository. In Proc. Int?l Conf.
Spoken Language Processing (ICSLP), pages 3069?
3072.
Komatani, Kazunori and Tatsuya Kawahara. 2000.
Flexible mixed-initiative dialogue management us-
ing concept-level confidence measures of speech
recognizer output. In Proc. Int?l Conf. Computa-
tional Linguistics (COLING), pages 467?473.
Komatani, Kazunori, Yuichiro Fukubayashi, Tetsuya
Ogata, and Hiroshi G. Okuno. 2007. Introducing
utterance verification in spoken dialogue system to
improve dynamic help generation for novice users.
In Proc. 8th SIGdial Workshop on Discourse and
Dialogue, pages 202?205.
Nakano, Mikio, Yuka Nagano, Kotaro Funakoshi,
Toshihiko Ito, Kenji Araki, Yuji Hasegawa, and Hi-
roshi Tsujino. 2007. Analysis of user reactions to
turn-taking failures in spoken dialogue systems. In
Proc. 8th SIGdial Workshop on Discourse and Dia-
logue, pages 120?123.
Raymond, Christian and Giuseppe Riccardi. 2007.
Generative and Discriminative Algorithms for Spo-
ken Language Understanding. In Proc. Annual
Conference of the International Speech Communi-
cation Association (INTERSPEECH), pages 1605?
1608.
Shapire, Robert E., Marie Rochery, Mazin Rahim, and
Narendra Gupta. 2005. Boosting with prior knowl-
edge for call classification. IEEE Trans. on Speech
and Audio Processing, 13(2):174?181.
Wang, Ye-Yi and Alex Acero. 2006. Discrimina-
tive models for spoken language understanding. In
Proc. Int?l Conf. Spoken Language Processing (IN-
TERSPEECH), pages 2426?2429.
Wang, Ye-Yi, Alex Acero, Ciprian Chelba, Brendan
Frey, and Leon Wong. 2002. Combination of Sta-
tistical and Rule-based Approaches for Spoken Lan-
guage Understanding. In Proc. Int?l Conf. Spoken
Language Processing (ICSLP), pages 609?612.
587
