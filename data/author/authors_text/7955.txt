Proceedings of NAACL HLT 2007, Companion Volume, pages 185?188,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Kernel Regression Based Machine Translation
Zhuoran Wang and John Shawe-Taylor
Department of Computer Science
University College London
London, WC1E 6BT
United Kingdom
{z.wang, jst}@cs.ucl.ac.uk
Sandor Szedmak
School of Electronics and Computer Science
University of Southampton
Southampton, SO17 1BJ
United Kingdom
ss03v@ecs.soton.ac.uk
Abstract
We present a novel machine translation
framework based on kernel regression
techniques. In our model, the translation
task is viewed as a string-to-string map-
ping, for which a regression type learning
is employed with both the source and the
target sentences embedded into their ker-
nel induced feature spaces. We report the
experiments on a French-English transla-
tion task showing encouraging results.
1 Introduction
Fig. 1 illustrates an example of phrase alignment
for statistical machine translation (SMT). A rough
linear relation is shown by the co-occurences of
phrases in bilingual sentence pairs, which motivates
us to introduce a novel study on the SMT task:
If we define the feature space Hx of our source
language X as all its possible phrases (i.e. informa-
tive blended word n-grams), and define the mapping
?x : X ? Hx, then a sentence x ? X can be ex-
pressed by its feature vector ?x(x) ? Hx. Each
component of ?x(x) is indexed by a phrase with the
value being the frequency of it in x. The definition
of the feature space Hy of our target language Y can
be made in a similar way, with corresponding map-
ping ?y : Y ? Hy. Now in the machine translation
task, given S = {(xi, yi) : xi ? X , yi ? Y, i =
1, . . . ,m}, a set of sample sentence pairs where yi
is the translation of xi, we are trying to learn W a
matrix represented linear operator, such that:
?y(y) = f(x) = W?x(x) (1)
we return
to marked
questions
marqu?es
nous
revenous
aux
questions
Figure 1: Phrase alignment in SMT
to predict the translation y for a new sentence x.
Comparing with traditional methods, this model
gives us a theoretical framework to capture higher-
dimensional dependencies within the sentences. To
solve the multi-output regression problem, we inves-
tigate two models, least squares regression (LSR)
similar to the technique presented in (Cortes et al,
2005), and maximum margin regression (MMR) in-
troduced in (Szedmak et al, 2006).
The rest of the paper is organized as follows. Sec-
tion 2 gives a brief review of the regression models.
Section 3 details the solution to the pre-image prob-
lem. We report the experimental results in Section
4, with discussions in Section 5.
2 Kernel Regression with Vector Outputs
2.1 Kernel Induced Feature Space
In the practical learning process, only the inner prod-
ucts of the feature vectors are needed (see Section
2.2, 2.3 and 3), so we can perform the so-called
kernel trick to avoid dealing with the very high-
dimensional feature vectors explicitly. That is, for
x, z ? X , a kernel function is defined as:
?x(x, z) = ??x(x),?x(z)? = ?x(x)??x(z) (2)
185
Similarly, a kernel function ?y(?, ?) is defined in Hy.
In our case, the blended n-spectrum string ker-
nel (Lodhi et al, 2002) that compares two strings
by counting how many (contiguous) substrings of
length from 1 up to n they have in common, is a good
choice for the kernel function to induce our feature
spaces Hx and Hy implicitly, even though it brings
in some uninformative features (word n-grams) as
well, when compared to our original definition.
2.2 Least Squares Regression
A basic method to solve the problem in Eq. 1 is least
squares regression that seeks the matrix W mini-
mizing the squared loss in Hy on the training set S:
min ?WMx ?My?2F (3)
where Mx = [?x(x1), ...,?x(xm)], My =
[?y(y1), ...,?y(ym)], and ? ? ?F denotes the Frobe-
nius norm.
Differentiating the expression and setting it to
zero gives:
2WMxM?x ? 2MyM?x = 0
? W = MyK?1x M?x (4)
where Kx = M?x Mx = (?x(xi, xj)1?i,j?m) is the
Gram matrix.
2.3 Maximum Margin Regression
An alternative solution to our regression learn-
ing problem is proposed in (Szedmak et al,
2006), called maximum margin regression. If L2-
normalized feature vectors are used in Eq. 1, de-
noted by ??x(?) and ??y(?), MMR solves the follow-
ing optimization:
min 12?W?
2
F + C
m
?
i=1
?i (5)
s.t. ???y(yi),W??x(xi)?Hy ? 1? ?i,
?i > 0, i = 1, . . . ,m.
where C > 0 is the regularization coefficient, and
?i are the slack variables. The Lagrange dual form
with dual variables ?i gives:
min
m
?
i,j=1
?i?j ??x(xi, xj)??y(yi, yj)?
m
?
i=1
?i
s.t. 0 ? ?i ? C, i = 1, . . . ,m. (6)
where ??x(?, ?) and ??y(?, ?) denote the kernel func-
tions associated to the respective normalized feature
vectors.
This dual problem can be solved efficiently with
a perceptron algorithm based on an incremental
subgradient method, of which the bounds on the
complexity and achievable margin can be found in
(Szedmak et al, 2006).
Then according to Karush-Kuhn-Tucker theory,
W is expressed as:
W =
m
?
i=1
?i??y(yi)??x(xi)? (7)
In practice, MMR works better when the distribu-
tion of the training points are symmetrical. So we
center the data before normalizing them. If ?Sx =
1
m
?m
i=1 ?x(xi) is the centre of mass of the source
sentence sample set {xi} in the feature space, the
new feature map is given by ??x(?) = ?x(?) ? ?Sx .
The similar operation is performed on ?y(?) to ob-
tain ??y(?). Then the L2-normalizations of ??x(?) and
??y(?) yield our final feature vectors ??x(?) and ??y(?).
3 Pre-image Solution
To find the pre-image sentence y = f?1(x) can be
achieved by seeking yt that has the minimum loss
between its feature vector ?y(yt) and our prediction
f(x). That is (Eq. 8: LSR, Eq. 9: MMR):
yt = arg min
y?Y(x)
?W?x(x)? ?y(y)?2
= arg min
y?Y(x)
?y(y, y)? 2ky(y)K?1x kx(x) (8)
yt = arg min
y?Y(x)
1? ???y(y),W??x(x)?Hy
= arg max
y?Y(x)
m
?
i=1
?i??y(yi, y)??x(xi, x) (9)
where Y(x) ? Y is a finite set covering all po-
tential translations for the given source sentence
x, and kx(?) = (?x(?, xi)1?i?m) and ky(?) =
(?y(?, yi)1?i?m) are m? 1 column matrices.
A proper Y(x) can be generated according to a
lexicon that contains possible translations for every
component (word or phrase) in x. But the size of it
will grow exponentially with the length of x, which
poses implementation problem for a decoding algo-
rithm.
186
In earlier systems, several heuristic search meth-
ods were developed, of which a typical example
is Koehn (2004)?s beam search decoder for phrase-
based models. However, in our case, because of the
?y(y, y) item in Eq. 8 and the normalization opera-
tion in MMR, neither the expression in Eq. 8 nor
the one in Eq. 9 can be decomposed into a sum
of subfunctions each involving feature components
in a local area only. It means we cannot estimate
exactly how well a part of the source sentence is
translated, until we obtain a translation for the entire
sentence, which prevents us doing a straightforward
beam search similar to (Koehn, 2004).
To simplify the situation, we restrict the reorder-
ing (distortion) of phrases that yield the output sen-
tences by only allowing adjacent phrases to ex-
change their positions. (The discussion of this strat-
egy can be found in (Tillmann, 2004).) We use x[i:j]
and y[i:j] to denote the substrings of x and y that be-
gin with the ith word and end with the jth. Now, if
we go back to the implementation of a beam search,
the current distortion restriction guarantees that in
each expansion of the search states (hypotheses) we
have x[1:lx] translated to a y[1:ly], either like state (a)
or like state (b) in Fig. 2, where lx is the number of
words translated in the source sentence, and ly is the
number of words obtained in the translation.
We assume that if y is a good translation of x,
then y[1:ly] is a good translation of x[1:lx] as well. So
we can expect that the squared loss ?W?x(x[1:lx])?
?y(y[1:ly])?2 in the LSR is small, or the inner prod-
uct ???y(y[1:ly]),W??x(x[1:lx])?Hy in the MMR is
large, for the hypothesis yielding a good translation.
According to Eq. 8 and Eq. 9, the hypotheses in the
search stacks can thus be reranked with the follow-
ing score functions (Eq. 10: LSR, Eq. 11: MMR):
Score(x[1:lx], y[1:ly]) = (10)
?y(y[1:ly], y[1:ly])? 2ky(y[1:ly])K?1x kx(x[1:lx])
Score(x[1:lx], y[1:ly]) =
m
?
i=1
?i??y(yi, y[1:ly])??x(xi, x[1:lx]) (11)
Therefore, to solve the pre-image problem, we
just employ the same beam search algorithm as
(Koehn, 2004), except we limit the derivation of new
hypotheses with the distortion restriction mentioned
nous revenous aux questions
we return to questions
marqu?es ?(a)
(b)
marked
?nous revenous aux questions
we return to questions
marqu?es
Figure 2: Search states with the limited distortion.
above. However, our score functions will bring
more runtime complexities when compared with tra-
ditional probabilistic methods. The time complexity
of a naive implementation of the blended n-spectrum
string kernel between two sentences si and sj is
O(n|si||sj|), where |?| denotes the length of the sen-
tence. So the score function in Eq. 11 results in an
average runtime complexity of O(mnlyl), where l is
the average length of the sentences yi in the training
set. Note here ??x(x[1:lx], xi) can be pre-computed
for lx from 1 to |x| before the beam search, which
calls for O(m|x|) space. The average runtime com-
plexity of the score function in Eq. 10 will be the
same if we pre-compute K?1x kx(x[1:lx]).
4 Experimental Results
4.1 Resource Description
Baseline System To compare with previous work,
we take Pharaoh (Koehn, 2004) as a baseline system,
with its default settings (translation table size 10,
beam size 100). We train a trigram language model
with the SRILM toolkit (Stocke, 2002). Whilst, the
parameters for the maximum entropy model are de-
veloped based on the minimum error rate training
method (Och, 2003).
In the following experiments, to facilitate com-
parison, each time we train our regression models
and the language model and translation model for
Pharaoh on a common corpus, and use the same
phrase translation table as Pharaoh?s to decode our
systems. According to our preliminary experiments,
with the beam size of 100, the search errors of our
systems can be limited within 1.5%.
Corpora To evaluate our models, we randomly
take 12,000 sentences from the French-English por-
tion of the 1996?2003 Europarl corpus (Koehn,
2005) for scaling-up training, 300 for test (Test), and
300 for the development of Pharaoh (Dev). Some
187
Vocabulary Words Perplexity
Fr En Fr En Dev Test
4k 5084 4039 43k 39k 32.25 31.92
6k 6426 5058 64k 59k 30.81 29.03
8k 7377 5716 85k 79k 29.91 28.94
10k 8252 6339 106k 98k 27.55 27.09
12k 9006 6861 127k 118k 27.19 26.41
Table 1: Statistics of the corpora.
characteristics of the corpora are summarized in Ta-
ble 1.
4.2 Results
Based on the 4k training corpus, we test the per-
formance of the blended n-spectrum string kernel in
LSR and MMR using BLEU score, with n increas-
ing from 2 to 7. Fig. 3 shows the results. It can be
found that the performance becomes stable when n
reaches a certain value. Finally, we choose the 3-
spectrum for LSR, and the 5-spectrum for MMR.
Then we scale up the training set, and compare the
performance of our models with Pharaoh in Fig. 4.
We can see that the LSR model performs almost as
well as Pharaoh, whose differences of BLEU score
are within 0.5% when the training set is larger than
6k. But MMR model performs worse than the base-
line. With the training set of 12k, it is outperformed
by Pharaoh by 3.5%.
5 Discussions
Although at this stage the main contribution is
still conceptual, the capability of our approach to
be applied to machine translation is still demon-
strated. Comparable performance to previous work
is achieved by the LSR model.
But a main problem we face is to scale-up the
training set, as in practice the training set for SMT
will be much larger than several thousand sentences.
A method to speed up the training is proposed in
(Cortes et al, 2005). By approximating the Gram
matrix with a n ? m (n ? m) low-rank matrix,
the time complexity of the matrix inversion opera-
tion can be reduced from O(m3) to O(n2m). But
the space complexity of O(nm) in their algorithm is
still too expensive for SMT tasks. Subset selection
techniques could give a solution to this problem, of
2 3 4 5 6 7
26
28
30
32
34
36
38
40
42
MMR
LSR
Figure 3: BLEU(%) versus n-spectrum
4000 6000 8000 10000 12000
30
32
34
36
38
40
42
44
Pharaoh
LSR
MMR
Figure 4: BLEU(%) versus training set size
which we will leave the further exploration to future
work.
Acknowledgements
The authors acknowledge the support of the EU un-
der the IST project No. FP6-033917.
References
C. Cortes, M. Mohri, and J. Weston. 2005. A general re-
gression technique for learning transductions. In Proc.
of ICML?05.
P. Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proc. of AMTA 2004.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit X.
H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini,
and C. Watkins. 2002. Text classification using string
kernels. J. Mach. Learn. Res., 2:419?444.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL?03.
A. Stocke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Proc. of ICSLP?02.
S. Szedmak, J. Shawe-Taylor, and E. Parado-Hernandez.
2006. Learning via linear operators: Maximum mar-
gin regression; multiclass and multiview learning at
one-class complexity. Technical report, PASCAL,
Southampton, UK.
C. Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proc. of HLT-
NAACL?04.
188
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 241?244,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Handling phrase reorderings for machine translation
Yizhao Ni, Craig J. Saunders
?
, Sandor Szedmak and Mahesan Niranjan
ISIS Group
School of Electronics and Computer Science
University of Southampton
Southampton, SO17 1BJ
United Kingdom
yn05r@ecs.soton.ac.uk, craig.saunders@xrce.xerox.com,
{ss03v,mn}@ecs.soton.ac.uk
Abstract
We propose a distance phrase reordering
model (DPR) for statistical machine trans-
lation (SMT), where the aim is to cap-
ture phrase reorderings using a structure
learning framework. On both the reorder-
ing classification and a Chinese-to-English
translation task, we show improved perfor-
mance over a baseline SMT system.
1 Introduction
Word or phrase reordering is a common prob-
lem in bilingual translations arising from dif-
ferent grammatical structures. For example,
in Chinese the expression of the date follows
?Year/Month/Date?, while when translated into
English, ?Month/Date/Year? is often the correct
grammar. In general, the fluency of machine trans-
lations can be greatly improved by obtaining the
correct word order in the target language.
As the reordering problem is computation-
ally expensive, a word distance-based reordering
model is commonly used among SMT decoders
(Koehn, 2004), in which the costs of phrase move-
ments are linearly proportional to the reordering
distance. Although this model is simple and effi-
cient, the content independence makes it difficult
to capture many distant phrase reordering caused
by the grammar. To tackle the problem, (Koehn
et al, 2005) developed a lexicalized reordering
model that attempted to learn the phrase reorder-
ing based on content. The model learns the local
orientation (e.g. ?monotone? order or ?switching?
order) probabilities for each bilingual phrase pair
using Maximum Likelihood Estimation (MLE).
These orientation probabilities are then integrated
into an SMT decoder to help finding a Viterbi?best
local orientation sequence. Improvements by this
?
the author?s new address: Xerox Research Centre Europe
6, Chemin de Maupertuis, 38240 Meylan France.
model have been reported in (Koehn et al, 2005).
However, the amount of the training data for each
bilingual phrase is so small that the model usually
suffers from the data sparseness problem. Adopt-
ing the idea of predicting the orientation, (Zens
and Ney, 2006) started exploiting the context and
grammar which may relate to phrase reorderings.
In general, a Maximum Entropy (ME) framework
is utilized and the feature parameters are tuned
by a discriminative model. However, the training
times for ME models are usually relatively high,
especially when the output classes (i.e. phrase re-
ordering orientations) increase.
Alternative to the ME framework, we propose
using a classification scheme here for phrase re-
orderings and employs a structure learning frame-
work. Our results confirm that this distance phrase
reordering model (DPR) can lead to improved per-
formance with a reasonable time efficiency.
Figure 1: The phrase reordering distance d.
2 Distance phrase reordering (DPR)
We adopt a discriminative model to capture the
frequent distant reordering which we call distance
phrase reordering. An ideal model would consider
every position as a class and predict the position of
the next phrase, although in practice we must con-
sider a limited set of classes (denoted as ?). Using
the reordering distance d (see Figure 1) as defined
by (Koehn et al, 2005), we extend the two class
model in (Xiong et al, 2006) to multiple classes
(e.g. three?class setup ? = {d < 0, d = 0, d >
0}; or five?class setup ? = {d ? ?5,?5 < d <
0, d = 0, 0 < d < 5, d ? 5}). Note that the more
241
classes it has, the closer it is to the ideal model, but
the smaller amount of training samples it would
receive for each class.
2.1 Reordering Probability model and
training algorithm
Given a (source, target) phrase pair (
?
f
j
, e?
i
) with
?
f
j
= [f
j
l
, . . . , f
j
r
] and e?
i
= [e
i
l
, . . . , e
i
r
], the dis-
tance phrase reordering probability has the form
p(o|
?
f
j
, e?
i
) :=
h
(
w
T
o
?(
?
f
j
, e?
i
)
)
?
o
?
??
h
(
w
T
o
?
?(
?
f
j
, e?
i
)
)
(1)
where w
o
= [w
o,0
, . . . , w
o,dim(?)
]
T
is the weight
vector measuring features? contribution to an ori-
entation o ? ?, ? is the feature vector and h is a
pre-defined monotonic function. As the reorder-
ing orientations tend to be interdependent, learn-
ing {w
o
}
o??
is more than a multi?class classifi-
cation problem. Take the five?class setup for ex-
ample, if an example in class d ? ?5 is classified
in class ?5 < d < 5, intuitively the loss should be
smaller than when it is classified in class d > 5.
The output (orientation) domain has an inherent
structure and the model should respect it. Hence,
we utilize the structure learning framework pro-
posed in (Taskar et al, 2003) which is equivalent
to minimising the sum of the classification errors
min
w
1
N
N
?
n=1
?(o,
?
f
n
j
, e?
n
i
,w) +
?
2
?w?
2
(2)
where ? ? 0 is a regularisation parameter,
?(o,
?
f
j
, e?
i
,w) = max{0,max
o
?
6=o
[4(o, o
?
)+
w
T
o
?
?(
?
f
j
, e?
i
)] ?w
T
o
?(
?
f
j
, e?
i
)}
is a structured margin loss function with
4(o, o
?
) =
?
?
?
0 if o = o
?
0.5 if o and o
?
are close in ?
1 else
measuring the distance between pseudo orienta-
tion o
?
and the true one o. Theoretically, this loss
requires that orientation o
?
which are ?far away?
from the true one o must be classified with a large
margin while nearby candidates are allowed to
be classified with a smaller margin. At training
time, we used a perceptron?based structure learn-
ing (PSL) algorithm to learn {w
o
}
o??
which is
shown in Table 1.
2.1.1 Feature Extraction and Application
Following (Zens and Ney, 2006), we consider
different kinds of information extracted from the
Input: The samples
{
o, ?(
?
f
j
, e?
i
)
}
N
n=1
, step size ?
Initialization: k = 0; w
o,k
= 0 ?o ? ?;
Repeat
for n = 1, 2, . . . , N do
for o
?
6= o get
V = max
o
?
{
4(o, o
?
) + w
T
o
?
,k
?(
?
f
j
, e?
i
)
}
o
?
= argmax
o
?
{
4(o, o
?
) + w
T
o
?
,k
?(
?
f
j
, e?
i
)
}
if w
T
o,k
?(
?
f
j
, e?
i
) < V then
w
o,k+1
= w
o,k
+ ??(
?
f
j
, e?
i
)
w
o
?
,k+1
= w
o
?
,k
? ??(
?
f
j
, e?
i
)
k = k + 1
until converge
Output: w
o,k+1
?o ? ?
Table 1: Perceptron-based structure learning.
phrase environment (see Table 2), where given a
sequence s (e.g. s = [f
j
l
?z
, . . . , f
j
l
]), the features
selected are ?
u
(s
|u|
p
) = ?(s
|u|
p
, u), with the
indicator function ?(?, ?), p = {j
l
? z, . . . , j
r
+ z}
and string s
|u|
p
= [f
p
, . . . , f
p+|u|
]. Hence, the
phrase features are distinguished by both the
content u and its start position p. For exam-
ple, the left side context features for phrase
pair (xiang gang, Hong Kong) in Figure 1 are
{?(s
1
0
, ?zhou?), ?(s
1
1
, ?liu?), ?(s
2
0
, ?zhou liu?)}.
As required by the algorithm, we then normalise
the feature vector
?
?
t
=
?
t
???
.
To train the DPR model, the training samples
{(
?
f
n
j
, e?
n
i
)}
N
n=1
are extracted following the phrase
pair extraction procedure in (Koehn et al, 2005)
and form the sample pool, where the instances
having the same source phrase
?
f
j
are considered
to be from the same cluster. A sub-DPR model is
then trained for each cluster using the PSL algo-
rithm. During the decoding, the DPR model finds
the corresponding sub-DPR model for a source
phrase
?
f
j
and generates the reordering probability
for each orientation class using equation (1).
3 Experiments
Experiments used the Hong Kong Laws corpus
1
(Chinese-to-English), where sentences of lengths
between 1 and 100 words were extracted and the
ratio of source/target lengths was no more than
2 : 1. The training and test sizes are 50, 290 and
1, 000 respectively.
1
This bilingual Chinese-English corpus consists of mainly
legal and documentary texts from Hong Kong. The corpus is
aligned at the sentence level which are collected and revised
manually by the author. The full corpus will be released soon.
242
Features for source phrase
?
f
j
Features for target phrase e?
i
Context
Source word n?grams within a window
(length z) around the phrase edge [j
l
] and [j
r
]
Target word n?grams
of the phrase [e
i
l
, . . . , e
i
r
]
Syntactic
Source word class tag n-grams within a
window (length z) around the phrase edge [j
l
] and [j
r
]
Target word class tag
n-grams of the phrase [e
i
l
, . . . , e
i
r
]
Table 2: The environment for the feature extraction. The word class tags are provided by MOSES.
3.1 Classification Experiments
Figure 2: Classification results with respect to d.
We used GIZA++ to produce alignments, en-
abling us to compare using a DPR model against
a baseline lexicalized reordering model (Koehn et
al., 2005) that uses MLE orientation prediction
and a discriminative model (Zens and Ney, 2006)
that utilizes an ME framework. Two orientation
classification tasks are carried out: one with three?
class setup and one with five?class setup. We
discarded points that had long distance reorder-
ing (|d| > 15) to avoid some alignment errors
cause by GIZA++ (representing less than 5% of
the data). This resulted in data sizes shown in Ta-
ble 3. The classification performance is measured
by an overall precision across all classes and the
class-specific F1 measures and the experiments
are are repeated three times to asses variance.
Table 4 depicts the classification results ob-
tained, where we observed consistent improve-
ments for the DPR model over the baseline and
the ME models. When the number of classes
(orientations) increases, the average relative im-
provements of DPR for the switching classes
(i.e. d 6= 0) increase from 41.6% to 83.2% over
the baseline and from 7.8% to 14.2% over the ME
model, which implies a potential benefit of struc-
ture learning. Figure 2 further demonstrate the av-
erage accuracy for each reordering distance d. It
shows that even for long distance reordering, the
DPR model still performs well, while the MLE
baseline usually performs badly (more than half
examples are classified incorrectly). With so many
classification errors, the effect of this baseline in
an SMT system is in doubt, even with a powerful
language model. At training time, training a DPR
model is much faster than training an ME model
(both algorithms are coded in Python), especially
when the number of classes increase. This is be-
cause the generative iterative scaling algorithm of
an ME model requires going through all examples
twice at each round: one is for updating the condi-
tional distributions p(o|
?
f
j
, e?
i
) and the other is for
updating {w
o
}
o??
. Alternatively, the PSL algo-
rithm only goes through all examples once at each
round, making it faster and more applicable for
larger data sets.
3.2 Translation experiments
We now test the effect of the DPR model in an
MT system, using MOSES (Koehn et al, 2005)
as a baseline system. To keep the comparison
fair, our MT system just replaces MOSES?s re-
ordering models with DPR while sharing all other
models (i.e. phrase translation probability model,
4-gram language model (A. Stolcke, 2002) and
beam search decoder). As in classification exper-
iments the three-class setup shows better results
in switching classes, we use this setup in DPR. In
detail, all consistent phrases are extracted from the
training sentence pairs and form the sample pool.
The three-class DPR model is then trained by the
PSL algorithm and the function h(z) = exp(z) is
applied to equation (1) to transform the prediction
scores. Contrasting the direct use of the reorder-
ing probabilities used in (Zens and Ney, 2006),
we utilize the probabilities to adjust the word
distance?based reordering cost, where the reorder-
ing cost of a sentence is computed as P
o
(f , e) =
243
Settings three?class setup five?class setup
Classes d < 0 d = 0 d > 0 d ? ?5 ?5 < d < 0 d = 0 0 < d < 5 d ? 5
Train 181, 583 755, 854 181, 279 82, 677 98, 907 755, 854 64, 881 116, 398
Test 5, 025 21, 106 5, 075 2, 239 2, 786 21, 120 1, 447 3, 629
Table 3: Data statistics for the classification experiments.
System three?class setup task
Precision d < 0 d = 0 d > 0 Training time (hours)
Lexicalized 77.1? 0.1 55.7? 0.1 86.5? 0.1 49.2? 0.3 1.0
ME 83.7? 0.3 67.9? 0.3 90.8? 0.3 69.2? 0.1 58.6
DPR 86.7? 0.1 73.3? 0.1 92.5? 0.2 74.6? 0.5 27.0
System five?class setup task
Precision d ? ?5 ?5 < d < 0 d = 0 0 < d < 5 d ? 5 Training Time (hours)
Lexicalized 74.3? 0.1 44.9? 0.2 32.0? 1.5 86.4? 0.1 29.2? 1.7 46.2? 0.8 1.3
ME 80.0? 0.2 52.1? 0.1 54.7? 0.7 90.4? 0.2 63.9? 0.1 61.8? 0.1 83.6
DPR 84.6? 0.1 60.0? 0.7 61.4? 0.1 92.6? 0.2 75.4? 0.6 68.8? 0.5 29.2
Table 4: Overall precision and class-specific F1 scores [%] using different number of orientation classes.
Bold numbers refer to the best results.
exp{?
?
m
d
m
?p(o|
?
f
j
m
,e?
i
m
)
} with tuning parameter ?.
This distance?sensitive expression is able to fill
the deficiency of the three?class setup of DPR and
is verified to produce better results. For parameter
tuning, minimum-error-rating training (F. J. Och,
2003) is used in both systems. Note that there are
7 parameters needed tuning in MOSES?s reorder-
ing models, while only 1 requires tuning in DPR.
The translation performance is evaluated by four
MT measurements used in (Koehn et al, 2005).
Table 5 shows the translation results, where we
observe consistent improvements on most evalua-
tions. Indeed both systems produced similar word
accuracy, but our MT system does better in phrase
reordering and produces more fluent translations.
4 Conclusions and Future work
We have proposed a distance phrase reordering
model using a structure learning framework. The
classification tasks have shown that DPR is bet-
ter in capturing the phrase reorderings over the
lexicalized reordering model and the ME model.
Moreover, compared with ME DPR is much faster
and more applicable to larger data sets. Transla-
tion experiments carried out on the Chinese-to-
English task show that DPR gives more fluent
translation results, which verifies its effectiveness.
For future work, we aim at improving the predic-
tion accuracy for the five-class setup using a richer
feature set before applying it to an MT system, as
DPR can be more powerful if it is able to provide
more precise phrase position for the decoder. We
will also apply DPR on a larger data set to test its
performance as well as its time efficiency.
Tasks Measure MOSES DPR
BLEU [%] 44.7? 1.2 47.1? 1.3
CH?EN word accuracy 76.5? 0.6 76.1? 1.5
NIST 8.82? 0.11 9.04? 0.26
METEOR [%] 66.1? 0.8 66.4? 1.1
Table 5: Four evaluations for the MT experiments.
Bold numbers refer to the best results.
References
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase?based statistical machine translation models.
In Proc. of AMTA 2004, Washington DC, October.
P. Koehn, A. Axelrod, A. B. Mayne, C. Callison?
Burch, M. Osborne and D. Talbot. 2005. Ed-
inburgh system description for the 2005 IWSLT
speech translation evaluation. In Proc. of IWSLT,
Pittsburgh, PA.
F. J. Och. 2003. SRILM - An Extensible Language
Modeling Toolkit. In Proc. Intl. Conf. Spoken Lan-
guage Processing, Colorado, September.
A. Stolcke. 2002. Minimum error rate training in sta-
tistical machine translation. In Proc. ACL, Japan.
B. Taskar, C. Guestrin, and D.Koller. 2003. Max?
margin Markov networks. In Proc. NIPS, Vancou-
ver, Canada, December.
D. Xiong, Q. Liu and S. Lin. 2006. Maximum En-
tropy Based Phrase Reordering Model for Statistical
Machine Translation. In Proc. of ACL, Sydney, July.
R. Zens and H. Ney. 2006. Discriminative Reordering
Models for Statistical Machine Translation. In Proc.
of ACL, pages 55?63, New York City, June.
244
