Proceedings of the ACL 2010 Conference Short Papers, pages 80?85,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Evaluating Machine Translations using mNCD
Marcus Dobrinkat and Tero Tapiovaara and Jaakko Va?yrynen
Adaptive Informatics Research Centre
Aalto University School of Science and Technology
P.O. Box 15400, FI-00076 Aalto, Finland
{marcus.dobrinkat,jaakko.j.vayrynen,tero.tapiovaara}@tkk.fi
Kimmo Kettunen
Kymenlaakso University of Applied Sciences
P.O. Box 9, FI-48401 Kotka, Finland
kimmo.kettunen@kyamk.fi
Abstract
This paper introduces mNCD, a method
for automatic evaluation of machine trans-
lations. The measure is based on nor-
malized compression distance (NCD), a
general information theoretic measure of
string similarity, and flexible word match-
ing provided by stemming and synonyms.
The mNCD measure outperforms NCD in
system-level correlation to human judg-
ments in English.
1 Introduction
Automatic evaluation of machine translation (MT)
systems requires automated procedures to en-
sure consistency and efficient handling of large
amounts of data. In statistical MT systems, au-
tomatic evaluation of translations is essential for
parameter optimization and system development.
Human evaluation is too labor intensive, time con-
suming and expensive for daily evaluations. How-
ever, manual evaluation is important in the com-
parison of different MT systems and for the valida-
tion and development of automatic MT evaluation
measures, which try to model human assessments
of translations as closely as possible. Furthermore,
the ideal evaluation method would be language in-
dependent, fast to compute and simple.
Recently, normalized compression distance
(NCD) has been applied to the evaluation of
machine translations. NCD is a general in-
formation theoretic measure of string similar-
ity, whereas most MT evaluation measures, e.g.,
BLEU and METEOR, are specifically constructed
for the task. Parker (2008) introduced BAD-
GER, an MT evaluation measure that uses NCD
and a language independent word normalization
method. BADGER scores were directly compared
against the scores of METEOR and word error
rate (WER). The correlation between BADGER
and METEOR were low and correlations between
BADGER and WER high. Kettunen (2009) uses
the NCD directly as an MT evaluation measure.
He showed with a small corpus of three language
pairs that NCD and METEOR 0.6 correlated for
translations of 10?12 MT systems. NCD was not
compared to human assessments of translations,
but correlations of NCD and METEOR scores
were very high for all the three language pairs.
Va?yrynen et al (2010) have extended the work
by including NCD in the ACL WMT08 evaluation
framework and showing that NCD is correlated
to human judgments. The NCD measure did not
match the performance of the state-of-the-art MT
evaluation measures in English, but it presented a
viable alternative to de facto standard BLEU (Pa-
pineni et al, 2001), which is simple and effective
but has been shown to have a number of drawbacks
(Callison-Burch et al, 2006).
Some recent advances in automatic MT evalu-
ation have included non-binary matching between
compared items (Banerjee and Lavie, 2005; Agar-
wal and Lavie, 2008; Chan and Ng, 2009), which
is implicitly present in the string-based NCD mea-
sure. Our motivation is to investigate whether in-
cluding additional language dependent resources
would improve the NCD measure. We experiment
with relaxed word matching using stemming and
a lexical database to allow lexical changes. These
additional modules attempt to make the reference
sentences more similar to the evaluated transla-
tions on the string level. We report an experiment
showing that document-level NCD and aggregated
NCD scores for individual sentences produce very
similar correlations to human judgments.
80
Figure 1: An example showing the compressed
sizes of two strings separately and concatenated.
2 Normalized Compression Distance
Normalized compression distance (NCD) is a sim-
ilarity measure based on the idea that a string x is
similar to another string y when both share sub-
strings. The description of y can reference shared
substrings in the known x without repetition, in-
dicating shared information. Figure 1 shows an
example in which the compression of the concate-
nation of x and y results in a shorter output than
individual compressions of x and y.
The normalized compression distance, as de-
fined by Cilibrasi and Vitanyi (2005), is given in
Equation 1, with C(x) as length of the compres-
sion of x and C(x, y) as the length of the com-
pression of the concatenation of x and y.
NCD(x, y) =
C(x, y)?min {C(x), C(y)}
max {C(x), C(y)}
(1)
NCD computes the distance as a score closer to
one for very different strings and closer to zero for
more similar strings.
NCD is an approximation of the uncomputable
normalized information distance (NID), a general
measure for the similarity of two objects. NID
is based on the notion of Kolmogorov complex-
ity K(x), a theoretical measure for the informa-
tion content of a string x, defined as the shortest
universal Turing machine that prints x and stops
(Solomonoff, 1964). NCD approximates NID by
the use of a compressor C(x) that is an upper
bound of the Kolmogorov complexity K(x).
3 mNCD
Normalized compression distance was not con-
ceived with MT evaluation in mind, but rather it
is a general measure of string similarity. Implicit
non-binary matching with NCD is indicated by
preliminary experiments which show that NCD is
less sensitive to random changes on the character
level than, for instance, BLEU, which only counts
the exact matches between word n-grams. Thus
comparison of sentences at the character level
could account better for morphological changes.
Variation in language leads to several accept-
able translations for each source sentence, which
is why multiple reference translations are pre-
ferred in evaluation. Unfortunately, it is typical
to have only one reference translation. Paraphras-
ing techniques can produce additional translation
variants (Russo-Lassner et al, 2005; Kauchak and
Barzilay, 2006). These can be seen as new refer-
ence translations, similar to pseudo references (Ma
et al, 2007).
The proposed method, mNCD, works analo-
gously to M-BLEU and M-TER, which use the
flexible word matching modules from METEOR
to find relaxed word-to-word alignments (Agar-
wal and Lavie, 2008). The modules are able to
align words even if they do not share the same
surface form, but instead have a common stem or
are synonyms of each other. A similarized transla-
tion reference is generated by replacing words in
the reference with their aligned counterparts from
the translation hypothesis. The NCD score is com-
puted between the translations and the similarized
references to get the mNCD score.
Table 1 shows some hand-picked German?
English candidate translations along with a) the
reference translations including the 1-NCD score
to easily compare with METEOR and b) the simi-
larized references including the mNCD score. For
comparison, the corresponding METEOR scores
without implicit relaxed matching are shown.
4 Experiments
The proposed mNCD and the basic NCD measure
were evaluated by computing correlation to hu-
man judgments of translations. A high correlation
value between an MT evaluation measure and hu-
man judgments indicates that the measure is able
to evaluate translations in a more similar way to
humans.
Relaxed alignments with the METEOR mod-
ules exact, stem and synonym were created
for English for the computation of the mNCD
score. The synonym module was not available
with other target languages.
4.1 Evaluation Data
The 2008 ACL Workshop on Statistical Machine
Translation (Callison-Burch et al, 2008) shared
task data includes translations from a total of 30
MT systems between English and five European
languages, as well as automatic and human trans-
81
Candidate C/ Reference R/ Similarized Reference S 1-NCD METEOR
C There is no effective means to stop a Tratsch, which was already included in the world.
R There is no good way to halt gossip that has already begun to spread. .41 .31
S There is no effective means to stop gossip that has already begun to spread. .56 .55
C Crisis, not only in America
R A Crisis Not Only in the U.S. .51 .44
S A Crisis not only in the America .72 .56
C Influence on the whole economy should not have this crisis.
R Nevertheless, the crisis should not have influenced the entire economy. .60 .37
S Nevertheless, the crisis should not have Influence the entire economy. .62 .44
C Or the lost tight meeting will be discovered at the hands of a gentlemen?
R Perhaps you see the pen you thought you lost lying on your colleague?s desk. .42 .09
S Perhaps you meeting the pen you thought you lost lying on your colleague?s desk. .40 .13
Table 1: Example German?English translations showing the effect of relaxed matching in the 1-mNCD
score (for rows S) compared with METEOR using the exact module only, since the modules stem
and synonym are already used in the similarized reference. Replaced words are emphasized.
lation evaluations for the translations. There are
several tasks, defined by the language pair and the
domain of translated text.
The human judgments include three different
categories. The RANK category has human quality
rankings of five translations for one sentence from
different MT systems. The CONST category con-
tains rankings for short phrases (constituents), and
the YES/NO category contains binary answers if a
short phrase is an acceptable translation or not.
For the translation tasks into English, the re-
laxed alignment using a stem module and the
synonym module affected 7.5% of all words,
whereas only 5.1% of the words were changed in
the tasks from English into the other languages.
The data was preprocessed in two different
ways. For NCD we kept the data as is, which we
called real casing (rc). Since the used METEOR
align module lowercases all text, we restored the
case information in mNCD by copying the correct
case from the reference translation to the similar-
ized reference, based on METEOR?s alignment.
The other way was to lowercase all data (lc).
4.2 System-level correlation
We follow the same evaluation methodology as in
Callison-Burch et al (2008), which allows us to
measure how well MT evaluation measures corre-
late with human judgments on the system level.
Spearman?s rank correlation coefficient ? was
calculated between each MT evaluation measure
and human judgment category using the simplified
equation
? = 1?
6
?
i di
n(n2 ? 1)
(2)
where for each system i, di is the difference be-
tween the rank derived from annotators? input and
the rank obtained from the measure. From the an-
notators? input, the n systems were ranked based
on the number of times each system?s output was
selected as the best translation divided by the num-
ber of times each system was part of a judgment.
We computed system-level correlations for
tasks with English, French, Spanish and German
as the target language1.
5 Results
We compare mNCD against NCD and relate their
performance to other MT evaluation measures.
5.1 Block size effect on NCD scores
Va?yrynen et al (2010) computed NCD between a
set of candidate translations and references at the
same time regardless of the sentence alignments,
analogously to document comparison. We experi-
mented with segmentation of the candidate trans-
lations into smaller blocks, which were individ-
ually evaluated with NCD and aggregated into a
single value with arithmetic mean. The resulting
system-level correlations between NCD and hu-
man judgments are shown in Figure 2 as a function
of the block size. The correlations are very simi-
lar with all block sizes, except for Spanish, where
smaller block size produces higher correlation. An
experiment with geometric mean produced similar
results. The reported results with mNCD use max-
imum block size, similar to Va?yrynen et al (2010).
1The English-Spanish news task was left out as most mea-
sures had negative correlation with human judgments.
82
2 5 10 20 50 100 500 2000 5000
0.0
0.2
0.4
0.6
0.8
1.0
block size in lines
sys
tem
 lev
el c
orre
latio
n w
ith 
hum
an 
judg
eme
nts
into eninto deinto frinto es
Figure 2: The block size has very little effect on
the correlation between NCD and human judg-
ments. The right side corresponds to document
comparison and the left side to aggregated NCD
scores for sentences.
5.2 mNCD against NCD
Table 2 shows the average system level correlation
of different NCD and mNCD variants for trans-
lations into English. The two compressors that
worked best in our experiments were PPMZ and
bz2. PPMZ is slower to compute but performs
slightly better compared to bz2, except for the
Method Parameters R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
M
ea
n
mNCD PPMZ rc .69 .74 .80 .74
NCD PPMZ rc .60 .66 .71 .66
mNCD bz2 rc .64 .73 .73 .70
NCD bz2 rc .57 .64 .69 .64
mNCD PPMZ lc .66 .80 .79 .75
NCD PPMZ lc .56 .79 .75 .70
mNCD bz2 lc .59 .85 .74 .73
NCD bz2 lc .54 .82 .71 .69
Table 2: Mean system level correlations over
all translation tasks into English for variants of
mNCD and NCD. Higher values are emphasized.
Parameters are the compressor PPMZ or bz2 and
the preprocessing choice lowercasing (lc) or real
casing (rc).
Target Lang Corr
Method Parameters EN DE FR ES
mNCD PPMZ rc .69 .37 .82 .38
NCD PPMZ rc .60 .37 .84 .39
mNCD bz2 rc .64 .32 .75 .25
NCD bz2 rc .57 .34 .85 .42
mNCD PPMZ lc .66 .33 .79 .23
NCD PPMZ lc .56 .37 .77 .21
mNCD bz2 lc .59 .25 .78 .16
NCD bz2 lc .54 .26 .77 .15
Table 3: mNCD versus NCD system correlation
RANK results with different parameters (the same
as in Table 2) for each target language. Higher
values are emphasized. Target languages DE, FR
and ES use only the stem module.
lowercased CONST category.
Table 2 shows that real casing improves RANK
correlation slightly throughout NCD and mNCD
variants, whereas it reduces correlation in the cat-
egories CONST, YES/NO as well as the mean.
The best mNCD (PPMZ rc) improves the best
NCD (PPMZ rc) method by 15% in the RANK
category. In the CONST category the best mNCD
(bz2 lc) improves the best NCD (bz2 lc) by 3.7%.
For the total average, the best mNCD (PPMZ rc)
improves the the best NCD (bz2 lc) by 7.2%.
Table 3 shows the correlation results for the
RANK category by target language. As shown al-
ready in Table 2, mNCD clearly outperforms NCD
for English. Correlations for other languages show
mixed results and on average, mNCD gives lower
correlations than NCD.
5.3 mNCD versus other methods
Table 4 presents the results for the selected mNCD
(PPMZ rc) and NCD (bz2 rc) variants along with
the correlations for other MT evaluation methods
from the WMT?08 data, based on the results in
Callison-Burch et al (2008). The results are av-
erages over language pairs into English, sorted
by RANK, which we consider the most signifi-
cant category. Although mNCD correlation with
human evaluations improved over NCD, the rank-
ing among other measures was not affected. Lan-
guage and task specific results not shown here, re-
veal very low mNCD and NCD correlations in the
Spanish-English news task, which significantly
83
Method R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
M
ea
n
DP .81 .66 .74 .73
ULCh .80 .68 .78 .75
DR .79 .53 .65 .66
meteor-ranking .78 .55 .63 .65
ULC .77 .72 .81 .76
posbleu .75 .69 .78 .74
SR .75 .66 .76 .72
posF4gram-gm .74 .60 .71 .68
meteor-baseline .74 .60 .63 .66
posF4gram-am .74 .58 .69 .67
mNCD (PPMZ rc) .69 .74 .80 .74
NCD (PPMZ rc) .60 .66 .71 .66
mbleu .50 .76 .70 .65
bleu .50 .72 .74 .65
mter .38 .74 .68 .60
svm-rank .37 .10 .23 .23
Mean .67 .62 .69 .66
Table 4: Average system-level correlations over
translation tasks into English for NCD, mNCD
and other MT evaluations measures
degrades the averages. Considering the mean of
the categories instead, mNCD?s correlation of .74
is third best together with ?posbleu?.
Table 5 shows the results from English. The ta-
ble is shorter since many of the better MT mea-
sures use language specific linguistic resources
that are not easily available for languages other
than English. mNCD performs competitively only
for French, otherwise it falls behind NCD and
other methods as already shown earlier.
6 Discussion
We have introduced a new MT evaluation mea-
sure, mNCD, which is based on normalized com-
pression distance and METEOR?s relaxed align-
ment modules. The mNCD measure outperforms
NCD in English with all tested parameter com-
binations, whereas results with other target lan-
guages are unclear. The improved correlations
with mNCD did not change the position in the
RANK category of the MT evaluation measures in
the 2008 ACL WMT shared task.
The improvement in English was expected on
the grounds of the synonym module, and indicated
also by the larger number of affected words in the
Method
Target Lang Corr
DE FR ES Mean
posbleu .75 .80 .75 .75
posF4gram-am .74 .82 .79 .74
posF4gram-gm .74 .82 .79 .74
bleu .47 .83 .80 .68
NCD (bz2 rc) .34 .85 .42 .66
svm-rank .44 .80 .80 .66
mbleu .39 .77 .83 .63
mNCD (PPMZ rc) .37 .82 .38 .63
meteor-baseline .43 .61 .84 .58
meteor-ranking .26 .70 .83 .55
mter .26 .69 .73 .52
Mean .47 .77 .72 .65
Table 5: Average system-level correlations for the
RANK category from English for NCD, mNCD
and other MT evaluation measures.
similarized references. We believe there is poten-
tial for improvement in other languages as well if
synonym lexicons are available.
We have also extended the basic NCD measure
to scale between a document comparison mea-
sure and aggregated sentence-level measure. The
rather surprising result is that NCD produces quite
similar scores with all block sizes. The different
result with Spanish may be caused by differences
in the data or problems in the calculations.
After using the same evaluation methodology as
in Callison-Burch et al (2008), we have doubts
whether it presents the most effective method ex-
ploiting all the given human evaluations in the best
way. The system-level correlation measure only
awards the winner of the ranking of five differ-
ent systems. If a system always scored second,
it would never be awarded and therefore be overly
penalized. In addition, the human knowledge that
gave the lower rankings is not exploited.
In future work with mNCD as an MT evalu-
ation measure, we are planning to evaluate syn-
onym dictionaries for other languages than En-
glish. The synonym module for English does
not distinguish between different senses of words.
Therefore, synonym lexicons found with statis-
tical methods might provide a viable alternative
for manually constructed lexicons (Kauchak and
Barzilay, 2006).
84
References
Abhaya Agarwal and Alon Lavie. 2008. METEOR,
M-BLEU and M-TER: evaluation metrics for high-
correlation with human rankings of machine trans-
lation output. In StatMT ?08: Proceedings of the
Third Workshop on Statistical Machine Translation,
pages 115?118, Morristown, NJ, USA. Association
for Computational Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU
in machine translation research. In Proceedings of
EACL-2006, pages 249?256.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christoph Monz, and Josh Schroeder. 2008.
Further meta-evalutation of machine translation.
ACL Workshop on Statistical Machine Translation.
Yee Seng Chan and Hwee Tou Ng. 2009. MaxSim:
performance and effects of translation fluency. Ma-
chine Translation, 23(2-3):157?168.
Rudi Cilibrasi and Paul Vitanyi. 2005. Clustering
by compression. IEEE Transactions on Information
Theory, 51:1523?1545.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
of the main conference on Human Language Tech-
nology Conference of the North American Chap-
ter of the Association of Computational Linguistics,
pages 455?462, Morristown, NJ, USA. Association
for Computational Linguistics.
Kimmo Kettunen. 2009. Packing it all up in search for
a language independent MT quality measure tool. In
In Proceedings of LTC-09, 4th Language and Tech-
nology Conference, pages 280?284, Poznan.
Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007.
Bootstrapping word alignment via word packing. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 304?
311, Prague, Czech Republic, June. Association for
Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W. Zhu.
2001. BLEU: a method for automatic evaluation
of machine translation. Technical Report RC22176
(W0109-022), IBM Research Division, Thomas J.
Watson Research Center.
Steven Parker. 2008. BADGER: A new machine trans-
lation metric. In Metrics for Machine Translation
Challenge 2008, Waikiki, Hawai?i, October. AMTA.
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.
2005. A paraphrase-based approach to machine
translation evaluation. Technical Report LAMP-
TR-125/CS-TR-4754/UMIACS-TR-2005-57, Uni-
versity of Maryland, College Park.
Ray Solomonoff. 1964. Formal theory of inductive
inference. Part I. Information and Control,, 7(1):1?
22.
Jaakko J. Va?yrynen, Tero Tapiovaara, Kimmo Ket-
tunen, and Marcus Dobrinkat. 2010. Normalized
compression distance as an automatic MT evalua-
tion metric. In Proceedings of MT 25 years on. To
appear.
85
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 195?200,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Applying morphological decomposition to statistical machine translation
Sami Virpioja and Jaakko Va?yrynen and Andre? Mansikkaniemi and Mikko Kurimo
Aalto University School of Science and Technology
Department of Information and Computer Science
PO BOX 15400, 00076 Aalto, Finland
{svirpioj,jjvayryn,ammansik,mikkok}@cis.hut.fi
Abstract
This paper describes the Aalto submission
for the German-to-English and the Czech-
to-English translation tasks of the ACL
2010 Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR.
Statistical machine translation has focused
on using words, and longer phrases con-
structed from words, as tokens in the sys-
tem. In contrast, we apply different mor-
phological decompositions of words using
the unsupervised Morfessor algorithms.
While translation models trained using the
morphological decompositions did not im-
prove the BLEU scores, we show that the
Minimum Bayes Risk combination with
a word-based translation model produces
significant improvements for the German-
to-English translation. However, we did
not see improvements for the Czech-to-
English translations.
1 Introduction
The effect of morphological variation in languages
can be alleviated by using word analysis schemes,
which may include morpheme discovery, part-of-
speech tagging, or other linguistic information.
Words are very convenient and even efficient rep-
resentation in statistical natural language process-
ing, especially with English, but morphologically
rich languages can benefit from more fine-grained
information. For instance, statistical morphs dis-
covered with unsupervised methods result in bet-
ter performance in automatic speech recognition
for highly-inflecting and agglutinative languages
(Hirsima?ki et al, 2006; Kurimo et al, 2006).
Virpioja et al (2007) applied morph-based
models in statistical machine translation (SMT)
between several language pairs without gaining
improvement in BLEU score, but obtaining re-
ductions in out-of-vocabulary rates. They uti-
lized morphs both in the source and in the tar-
get language. Later, de Gispert et al (2009)
showed that Minimum Bayes Risk (MBR) com-
bination of word-based and morph-based trans-
lation models improves translation with Arabic-
to-English and Finnish-to-English language pairs,
where only the source language utilized morph-
based models. Similar results have been shown for
Finnish-to-English and Finnish-to-German in per-
formance evaluation of various unsupervised mor-
pheme analysis algorithms in Morpho Challenge
2009 competition (Kurimo et al, 2009).
We continue the research described above and
examine how the level of decomposition affects
both the individual morph-based systems and
MBR combinations with the baseline word-based
model. Experiments are conducted with the
WMT10 shared task data for German-to-English
and Czech-to-English language pairs.
2 Methods
In this work, morphological analyses are con-
ducted on the source language data, and each dif-
ferent analysis is applied to create a unique seg-
mentation of words into morphemes. Translation
systems are trained with the Moses toolkit (Koehn
et al, 2007) from each differently segmented ver-
sion of the same source language to the target lan-
guage. Evaluation with BLEU is performed on
both the individual systems and system combina-
tions, using different levels of decomposition.
2.1 Morphological models for words
Morfessor (Creutz and Lagus, 2002; Creutz and
Lagus, 2007, etc.) is a family of methods for
unsupervised morphological segmentation. Mor-
fessor does not limit the number of morphemes
for each word, making it suitable for agglutina-
tive and compounding languages. An analysis of a
single word is a list of non-overlapping segments,
195
morphs, stored in the model lexicon. We use both
the Morfessor Baseline (Creutz and Lagus, 2005b)
and the Morfessor Categories-MAP (Creutz and
Lagus, 2005a) algorithms.1 Both are formulated
in a maximum a posteriori (MAP) framework, i.e.,
the learning algorithm tries to optimize the prod-
uct of the model prior and the data likelihood.
The generative model applied by Morfessor
Baseline assumes that the morphs are independent.
The resulting segmentation can be influenced by
using explicit priors for the morph lengths and
frequencies, but their effect is usually minimal.
The training data has a larger effect on the re-
sults: A larger data set alows a larger lexicon,
and thus longer morphs and less morphs per word
(Creutz and Lagus, 2007). Moreover, the model
can be trained with or without taking into account
the word frequencies. If the frequencies are in-
cluded, the more frequent words are usually un-
dersegmented compared to a linguistic analysis,
whereas the rare words are oversegmented (Creutz
and Lagus, 2005b). An easy way to control the
amount of segmentation is to weight the training
data likelihood by a positive factor ?. If ? > 1,
the increased likelihood results in longer morphs.
If ? < 1, the morphs will be shorter and the words
more segmented.
Words that are not present in the training data
can be segmented using an algorithm similar to
Viterbi. The algorithm can be modified to allow
new morphs types to be used by using an approx-
imative cost of adding them into the lexicon (Vir-
pioja and Kohonen, 2009). The modification pre-
vents oversegmentation of unseen word forms. In
machine translation, this is important especially
for proper nouns, for which there is usually no
need for translation.
The Morfessor Categories-MAP algorithm ex-
tends the model by imposing morph categories of
stems, prefixes and suffixes, as well as transition
probabilities between them. In addition, it applies
a hierarchical segmentation model that allows it to
construct new stems from smaller pieces of ?non-
morphemes? (Creutz and Lagus, 2007). Due to
these features, it can provide reasonable segmen-
tations also for those words that contain new mor-
phemes. The drawback of the more sophisticated
model is the slower and more complex training al-
gorithm. In addition, the amount of the segmenta-
1The respective software is available at http://www.
cis.hut.fi/projects/morpho/
tion is harder to control.
Morfessor Categories-MAP was applied to sta-
tistical machine translation by Virpioja et al
(2007) and de Gispert et al (2009). However,
Kurimo et al (2009) report that Morfessor Base-
line outperformed Categories-MAP in Finnish-to-
English and German-to-English tasks both with
and without MBR combination, although the dif-
ferences were not statistically significant. In all
the previous cases, the models were trained on
word types, i.e., without using their frequencies.
Here, we also test models trained on word tokens.
2.2 Statistical machine translation
We utilize the Moses toolkit (Koehn et al, 2007)
for statistical machine translation. The default pa-
rameter values are used except with the segmented
source language, where the maximum sentence
length is increased from 80 to 100 tokens to com-
pensate for the larger number of tokens in text.
2.3 Morphological model combination
For combining individual models, we apply Min-
imum Bayes Risk (MBR) system combination
(Sim et al, 2007). N-best lists from multiple
SMT systems trained with different morpholog-
ical analysis methods are merged; the posterior
distributions over the individual lists are interpo-
lated to form a new distribution over the merged
list. MBR hypotheses selection is then performed
using sentence-level BLEU score (Kumar and
Byrne, 2004).
In this work, the focus of the system combina-
tion is not to combine different translation systems
(e.g., Moses and Systran), but to combine systems
trained with the same translation algorithm using
the same source language data with with different
morphological decompositions.
3 Experiments
The German-to-English and Czech-to-English
parts of the ACL WMT10 shared task data were
investigated. Vanilla SMT models were trained
with Moses using word tokens for MBR combi-
nation and comparison purposes. Several different
morphological segmentation models for German
and Czech were trained with Morfessor. Each seg-
mentation model corresponds to a morph-based
SMT model trained with Moses. The word-based
vanilla Moses model is compared to each morph-
based model as well as to several MBR com-
196
binations between word-based translation models
and morph-based translation models. Quantitative
evaluation is carried out using the BLEU score
with re-cased and re-tokenized translations.
4 Data
The data used in the experiments consisted
of Czech-to-English (CZ-EN) and German-to-
English (DE-EN) parallel language data from
ACL WMT10. The data was divided into distinct
training, development, and evaluation sets. Statis-
tics and details are shown in Table 1.
Aligned data from Europarl v5 and News
Commentary corpora were included in training
German-to-English SMT models. The English
part from the same data sets was used for train-
ing a 5-gram language model, which was used in
all translation tasks. The Czech-to-English trans-
lation model was trained with CzEng v0.9 (train-
ing section 0) and News Commentary data. The
monolingual German and Czech parts of the train-
ing data sets were used for training the morph seg-
mentation models with Morfessor.
The data sets news-test2009, news-
syscomb2009 and news-syscombtune2010
from the ACL WMT 2009 and WMT 2010,
were used for development. The news-test2008,
news-test2010, and news-syscombtest2010 data
sets were used for evaluation.
4.1 Preprocessing
All data sets were preprocessed before use. XML-
tags were removed, text was tokenized and char-
acters were lowercased for every training, devel-
opment and evaluation set.
Morphological models for German and Czech
were trained using a corpus that was a combina-
tion of the respective training sets. Then the mod-
els were used for segmenting all the data sets, in-
cluding development and evaluation sets, with the
Viterbi algorithm discussed in Section 2.1. The
modification of allowing new morph types for out-
of-vocabulary words was not applied.
The Moses cleaning script performed additional
filtering on the parallel language training data.
Specifically, sentences with over 80 words were
removed from the vanilla Moses word-based mod-
els. For morph-based models the limit was set
to 100 morphs, which is the maximum limit of
the Giza++ alignment tool. After filtering with a
threshold of 100 tokens, the different morph seg-
mentations for DE-EN training data from com-
bined Europarl and News Commentary data sets
ranged from 1 613 556 to 1 624 070 sentences.
Similarly, segmented CZ-EN training data ranged
from 896 163 to 897 744 sentences. The vanilla
words-based model was trained with 1 609 998
sentences for DE-EN and 897 497 sentences for
CZ-EN.
5 Results
The details of the ACL WMT10 submissions are
shown in Table 2. The results of experiments with
different morphological decompositions and MBR
system combinations are shown in Table 3. The
significances of the differences in BLEU scores
between the word-based model (Words) and mod-
els with different morphological decompositions
was measured by dividing each evaluation data set
into 49 subsets of 41?51 sentences, and using the
one-sided Wilcoxon signed rank test (p < 0.05).
5.1 Segmentation
We created several word segmentations with Mor-
fessor baseline and Morfessor Categories-MAP
(CatMAP). Statistics for the different segmenta-
tions are given in Table 3. The amount of seg-
mentation was measured as the average number of
morphs per word (m/w) and as the percentage of
segmented words (s-%) in the training data. In-
creasing the data likelihood weight ? in Morfes-
sor Baseline increases the amount of segmentation
for both languages. However, it had little effect
on the proportion of segmented words in the three
evaluation data sets: The proportion of segmented
word tokens was 10?11 % for German and 8?9 %
for Czech, whereas the out-of-vocabulary rate was
7.5?7.8 % for German and 4.8?5.6 % for Czech.
Disregarding the word frequency information
in Morfessor Baseline (nofreq) produced more
morphs per word type and segmented nearly
all words in the training data. The Morfessor
CatMAP algorithm created segmentations with the
largest number of morphs per word, but did not
segment as many words as the Morfessor Baseline
without the frequencies.
5.2 Morph-based translation systems
The models with segmented source language per-
formed worse individually than the word-based
models. The change in the BLEU score was statis-
tically significant in almost all segmentations and
197
Data set Statistics Training Development Evaluation
Sentences Words per sentence SM LM TM
DE CZ EN DE CZ EN DE-EN CZ-EN {DE,CZ}-EN {DE,CZ}-EN
Europarl v5 1 540 549 23.2 25.2 x x x
News Commentary 100 269 21.9 18.9 21.5 x x x x x
CzEng v0.9 (training section 0) 803 286 8.3 9.9 x x
news-test2009 2 525 21.7 18.8 23.2 x
news-syscomb2009 502 19.7 17.2 21.1 x
news-syscombtune2010 455 20.2 17.3 21.0 x
news-test2008 2 051 20.3 17.8 21.7 x
news-test2010 2 489 21.7 18.4 22.3 x
news-syscombtest2010 2 034 22.0 18.6 22.6 x
Table 1: Data sets for the Czech-to-English and German-to-English SMT experiments, including the
number of aligned sentences and the average number of words per sentence in each language. The data
sets used for model training, development and evaluation are marked. Training is divided into German
(DE) and Czech (CZ) segmentation model (SM) training, English (EN) language model (LM) training
and German-to-English (DE-EN) and Czech-to-English (CZ-EN) translation model (TM) training.
Submission Segmentation model for source language BLEU-cased
(news-test2010)
aalto DE-EN WMT10 Morfessor Baseline (? = 0.5) 17.0
aalto DE-EN WMT10 CatMAP Morfessor Categories-MAP 16.5
aalto CZ-EN WMT10 Morfessor Baseline (? = 0.5) 16.2
aalto CZ-EN WMT10 CatMAP Morfessor Categories-MAP 15.9
Table 2: Our submissions for the ACL WMT10 shared task in translation. The translation models are
trained from the segmented source language into unsegmented target language with Moses.
all evaluation sets. Morfessor Baseline (? = 0.5)
was the best individual segmented model for both
German and Czech in the sense that it had the
lowest number of significant decreases the BLEU
score compared to the word-based model. Remov-
ing word frequency information with Morfessor
Baseline and using Morfessor CatMAP gave the
lowest BLEU scores with both source languages.
5.3 Translation system combination
For the DE-EN language pair, all MBR system
combinations between each segmented model and
the word-based model had slightly higher BLUE
scores than the individual word-based model.
Nearly all improvements were statistically signifi-
cant.
The BLEU scores for the MBR combinations
in the CZ-EN language pair were mostly not sig-
nificantly different from the individual word-based
model. Two scores were significantly lower.
6 Discussion
We have applied concatenative morphological
analysis, in which each original word token is seg-
mented into one or more non-overlapping morph
tokens. Our results with different levels of seg-
mentation with Morfessor suggest that the optimal
level of segmentation is language pair dependent
in machine translation.
Our approach for handling rich morphology has
not been able to directly improve the translation
quality. We assume that improvements might still
be possible by carefully tuning the amount of seg-
mentation. The experiments in this paper with
different values of the ? parameter for Morfes-
sor Baseline were conducted with the word fre-
quencies. The parameter had little effect on the
proportion of segmented words in the evaluation
data sets, as frequent words were not segmented
at all, and out-of-vocabulary words were likely to
be oversegmented by the Viterbi algorithm. Fu-
ture work includes testing a larger range of val-
ues for ?, also for models trained without the
word frequencies, and using the modification of
the Viterbi algorithm proposed in Virpioja and Ko-
honen (2009).
It might also be helpful to only segment selected
words, where the selection would be based on the
potential benefit in the translation process. In gen-
eral, the direct segmentation of words into morphs
is problematic because it increases the number
of tokens in the text and directly increases both
model training and decoding complexity. How-
ever, an efficient segmentation decreases the num-
ber of types and the out-of-vocabulary rate (Virpi-
oja et al, 2007).
We have replicated here the result that an MBR
combination of a morph-based MT system with
198
Segmentation (DE) Statistics (DE) BLEU-cased (DE-EN)
news-test2008 news-test2010 news-syscombtest2010
m/w s-% No MBR MBR with No MBR No MBR MBR with
Words Words
Words 1.00 0.0% 16.37 - 17.28 13.22 -
Morfessor Baseline (? = 0.5) 1.82 72.4% 15.19? 16.47+ 17.04? 13.28? 13.70+
Morfessor Baseline (? = 1.0) 1.65 61.0% 15.14? 16.54+ 16.87? 11.95? 13.66+
Morfessor Baseline (? = 5.0) 1.24 23.7% 15.04? 16.44? 16.63? 11.78? 13.43+
Morfessor CatMAP 2.25 67.5% 14.21? 16.42? 16.53? 11.15? 13.61+
Morfessor Baseline nofreq 2.24 91.6% 13.98? 16.47+ 16.36? 10.66? 13.58+
Segmentation (CZ) Statistics (CZ) BLEU-cased (CZ-EN)
news-test2008 news-test2010 news-syscombtest2010
m/w s-% No MBR MBR with No MBR No MBR MBR with
Words Words
Words 1.00 0.0% 14.91 - 16.73 12.75 -
Morfessor Baseline (? = 0.5) 1.19 17.7% 13.22? 14.87? 16.01? 12.60? 12.53?
Morfessor Baseline (? = 1.0) 1.09 8.1% 13.33? 14.88? 16.10? 11.29? 12.84?
Morfessor Baseline (? = 5.0) 1.03 2.9% 13.53? 14.83? 15.92? 11.17? 12.85?
Morfessor CatMAP 2.29 71.9% 11.93? 14.86? 15.79? 10.12? 10.79?
Morfessor Baseline nofreq 2.18 90.3% 12.43? 14.96? 15.82? 10.13? 12.89?
Table 3: Results for German-to-English (DE-EN) and Czech-to-English (CZ-EN) translation models.
The source language is segmented with the shown algorithms. The amount of segmentation in the train-
ing data is measured with the average number of morphs per word (m/w) and as proportion of segmented
words (s-%) against the word-based model (Words). The trained translation systems are evaluated in-
dependently (No MBR) and in Minimum Bayes Risk system combination of word-based translation
systems (MBR). Unchanged (?), significantly higher (+) and lower (?) BLEU scores compared to the
word-based translation model (Words) are marked. The best morph-based model for each column is
emphasized.
a word-based MT system can produce a BLEU
score that is higher than from either of the indi-
vidual systems (de Gispert et al, 2009; Kurimo
et al, 2009). With the DE-EN language pair, the
improvement was statistically significant with all
tested segmentation models. However, the im-
provements were not as large as those obtained
before and the results for the CZ-EN language
pair were not significantly different in most cases.
Whether this is due to the different languages,
training data sets, the domain of the evaluation
data sets, or some problems in the model training,
is currently uncertain.
One very different approach for applying dif-
ferent levels of linguistic analysis is factor mod-
els for SMT (Koehn and Hoang, 2007), where
pre-determined factors (e.g., surface form, lemma
and part-of-speech) are stored as vectors for each
word. This provides better integration of mor-
phosyntactic information and more control of the
process, but the translation models are more com-
plex and the number and factor types in each word
must be fixed.
Our submissions to the ACL WMT10 shared
task utilize unsupervised morphological decompo-
sition models in a straightforward manner. The
individual morph-based models trained with the
source language words segmented into morphs
did not improve the vanilla word-based models
trained with the unsegmented source language.
We have replicated the result for the German-
to-English language pair that an MBR combina-
tion of a word-based and a segmented morph-
based model gives significant improvements to the
BLEU score. However, we did not see improve-
ments for the Czech-to-English translations.
Acknowledgments
This work was supported by the Academy of
Finland in the project Adaptive Informatics, the
Finnish graduate school in Language Technology,
and the IST Programme of the European Commu-
nity, under the FP7 project EMIME (213845).
References
Mathias Creutz and Krista Lagus. 2002. Unsuper-
vised discovery of morphemes. In Proceedings of
the Workshop on Morphological and Phonological
Learning of ACL?02, pages 21?30, Philadelphia,
Pennsylvania, USA.
Mathias Creutz and Krista Lagus. 2005a. Inducing the
morphological lexicon of a natural language from
unannotated text. In Proceedings of the AKRR?05,
Espoo, Finland.
199
Mathias Creutz and Krista Lagus. 2005b. Unsu-
pervised morpheme segmentation and morphology
induction from text corpora using Morfessor 1.0.
Technical Report A81, Publications in Computer
and Information Science, Helsinki University of
Technology.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4(1), January.
Adria` de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes risk com-
bination of translation hypotheses from alternative
morphological decompositions. In Proceedings of
Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, Com-
panion Volume: Short Papers, pages 73?76, Boul-
der, USA, June. Association for Computational Lin-
guistics.
Teemu Hirsima?ki, Mathias Creutz, Vesa Siivola, Mikko
Kurimo, Sami Virpioja, and Janne Pylkko?nen.
2006. Unlimited vocabulary speech recognition
with morph language models applied to Finnish.
Computer Speech and Language, 20(4):515?541.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the EMNLP 2007,
pages 868?876, Prague, Czech Republic, June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Annual Meeting of ACL, demonstration ses-
sion, pages 177?180, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the HLT-NAACL 2004, pages
169?176.
Mikko Kurimo, Antti Puurula, Ebru Arisoy, Vesa Si-
ivola, Teemu Hirsima?ki, Janne Pylkko?nen, Tanel
Aluma?e, and Murat Saraclar. 2006. Unlimited vo-
cabulary speech recognition for agglutinative lan-
guages. In Proceedings of the HLT-NAACL 2006,
pages 487?494, New York, USA.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2009.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.
K. C. Sim, W. J. Byrne, M. J. F. Gales, H. Sahbi, and
P. C. Woodl. 2007. Consensus network decoding
for statistical machine translation system combina-
tion. In IEEE Int. Conf. on Acoustics, Speech, and
Signal Processing.
Sami Virpioja and Oskar Kohonen. 2009. Unsuper-
vised morpheme analysis with Allomorfessor. In
Working notes for the CLEF 2009 Workshop, Corfu,
Greece.
Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz,
and Markus Sadeniemi. 2007. Morphology-aware
statistical machine translation based on morphs in-
duced in an unsupervised manner. In Proceedings
of the Machine Translation Summit XI, pages 491?
498, Copenhagen, Denmark, September.
200
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 343?348,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Normalized Compression Distance Based Measures for
MetricsMATR 2010
Marcus Dobrinkat and Jaakko Va?yrynen and Tero Tapiovaara
Adaptive Informatics Research Centre
Aalto University School of Science and Technology
P.O. Box 15400, FI-00076 Aalto, Finland
{marcus.dobrinkat,jaakko.j.vayrynen,tero.tapiovaara}@tkk.fi
Kimmo Kettunen
Kymenlaakso University of Applied Sciences
P.O. Box 9, FI-48401 Kotka, Finland
Kimmo.kettunen@kyamk.fi
Abstract
We present the MT-NCD and MT-mNCD
machine translation evaluation metrics
as submission to the machine transla-
tion evaluation shared task (MetricsMATR
2010). The metrics are based on nor-
malized compression distance (NCD), a
general information theoretic measure of
string similarity, and evaluated against hu-
man judgments from the WMT08 shared
task. The experiments show that 1)
our metric improves correlation to hu-
man judgments by using flexible match-
ing, 2) segment replication is effective,
and 3) our NCD-inspired method for mul-
tiple references indicates improved results.
Generally, the proposed MT-NCD and
MT-mNCD methods correlate competi-
tively with human judgments compared to
commonly used machine translations eval-
uation metrics, for instance, BLEU.
1 Introduction
The quality of automatic machine translation
(MT) evaluation metrics plays an important role
in the development of MT systems. Human eval-
uation would no longer be necessary if automatic
MT metrics correlated perfectly with manual judg-
ments. Besides high correlation with human judg-
ments of translation quality, a good metric should
be language independent, fast to compute and sen-
sitive enough to reliably detect small improve-
ments in MT systems.
Recently there have been some experiments
with normalized compression distance (NCD) as a
method for automatic evaluation of machine trans-
lation. NCD is a general string similarity measure
that has been useful for clustering in various tasks
(Cilibrasi and Vitanyi, 2005).
Parker (2008) introduced BADGER, a machine
translation evaluation metric that uses NCD to-
gether with a language independent word normal-
ization method. Kettunen (2009) independently
applied NCD to the direct evaluation of transla-
tions. He showed with a small corpus of three lan-
guage pairs that the scores of NCD and METEOR
(v0.6) from translations of 10?12 MT systems
were highly correlated.
Va?yrynen et al (2010) have extended the work
by showing that NCD can be used to rank transla-
tions of different MT systems so that the ranking
order correlates with human rankings at the same
level as BLEU (Papineni et al, 2001). For trans-
lations into English, NCD had an overall system-
level correlation of 0.66 whereas the best method,
ULC had an overall correlation of 0.76, and BLEU
had an overall correlation of 0.65. NCD presents
a viable alternative to the de facto standard BLEU.
Both metrics are language independent, simple
and efficient to compute. However, NCD is a
general measure of similarity that has been ap-
plied in many domains. More advanced meth-
ods achieve better correlation with human judg-
ments, but typically use additional language spe-
cific linguistic resources. Dobrinkat et al (2010)
experimented with relaxed word matching, adding
language specific resources to NCD. The metric
called mNCD, which works similarly to mBLEU
(Agarwal and Lavie, 2008), showed improved cor-
relation to human judgments in English, the only
language where a METEOR synonym module was
used.
The motivation for this challenge submission is
to evaluate the MT-NCD and MT-mNCD metric
performance in an open competition with state-of-
343
the-art MT evaluation metrics. Our experiments
and submission build on NCD and mNCD. We ex-
pand NCD to handle multiple references and re-
port experimental results for replicating segments
as a preprocessing step that improves the NCD as
an MT evaluation metric.
2 NCD-based MT evaluation metrics
NCD-based MT evaluation metrics build on the
idea that a string x is similar to another string y,
when both share common substrings. When de-
scribing y, common substrings do not have to be
repeated, but can be referenced to x. This is done
when compressing the concatenation of x and y,
which results in smaller output when more infor-
mation of y is already included in x.
2.1 Normalized Compression Distance
The normalized compression distance, as defined
by Cilibrasi and Vitanyi (2005) is given in Equa-
tion 1, in which C(x) is the length of the compres-
sion of x and C(x, y) is the length of the compres-
sion of the concatenation of x and y.
NCD(x, y) =
C(x, y)?min {C(x), C(y)}
max {C(x), C(y)}
(1)
NCD computes the distance as a score closer to
one for very different strings and closer to zero for
more similar strings. Most MT evaluation met-
rics are defined as similarity measures in contrast
to NCD, which is a distance measure. For eas-
ier comparison with other MT evaluation metrics,
we define the NCD based MT evaluation similar-
ity metric MT-NCD as 1? NCD.
NCD is a practically usable form of the uncom-
putable normalized information distance (NID), a
general metric for the similarity of two objects.
NID is based on the notion of Kolmogorov com-
plexity K(x), a theoretical measure for the algo-
rithmic information content of a string x. It is de-
fined as the shortest universal Turing machine that
prints x and stops (Solomonoff, 1964). NCD ap-
proximates NID by the use of a compressor C(x)
that presents a computable approximation of the
Kolmogorov complexity K(x).
2.2 NCD with multiple references
Most ideas can be described with in different
ways, therefore using only one reference transla-
tion for the evaluation of a candidate sentence is
not ideal and the exploitation of knowledge in sev-
eral different reference translations is helpful for
automatic MT evaluation.
One simple way for handling multiple refer-
ences is to evaluate against each reference indi-
vidually and select the maximum score. Although
this works, it is clearly not optimal. We developed
the NCDm metric, which is inspired by NCD. It
considers all references simultaneously and the
quality of a translation t against multiple refer-
ences R = {r1, . . . , rm} is assessed as
NCDm(t, R) =
max{C(t|R),min
r?R
C(r|t}
max{C(t),min
r?R
C(r)}
(2)
where C(x|y) = C(x, y) ? C(y) approximates
conditional algorithmic information with the com-
pressor C. The NCDm similarity metric with a
single reference (m = 1) is equal to NCD in Equa-
tion 1. Again, we define MT-NCDm as 1?NCDm.
Figure 1 shows how both, the MT-NCDm and
the BLEU metric change with a different num-
ber of references when the translation is varied
from correct to a random sequence of words. The
scores are computed with 249 sentences from the
LDC2010E28Dev data set using the first reference
as the correct translation. A higher score with mul-
tiple references against the correct translation indi-
cates that the measure is able to take into account
information from multiple references at the same
time.
The words in the candidate translation are re-
placed with probability p with a word randomly
selected with uniform probability from a lexicon
created from all reference translations. This simu-
lates partially correct translations. The words are
changed in a simple way without deletions, inser-
tions or word order permutations. The MT-NCDm
score increases with more than one reference
translation and random changes to the sentence re-
duce the score roughly proportional to the number
of changed words. With BLEU, the score is af-
fected more by a small number of changes.
2.3 mNCD
One enhancement to the basic NCD as auto-
matic evaluation metric is mNCD (Dobrinkat et
al., 2010), which provides relaxed word matching
based on the flexible matching modules of ME-
TEOR (Agarwal and Lavie, 2008).
What mNCD does is that it changes the ref-
erence sentence to be more similar to the candi-
344
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
word change probability (p)
MT
 eva
luat
ion 
met
ric s
core
MT?NCDm with 3 referencesMT?NCDm with 2 referencesMT?NCDm with 1 referenceBLEU with 3 referencesBLEU with 2 referencesBLEU with 1 reference
MT?NCDm
BLEU
Figure 1: The MT-NCDm and BLEU scores with
a different number of multiple references against
correct translation with random word change
probability (p).
date, given that some of the words are synonyms
or share the same stem. Subsequent analysis using
any n-gram based automatic analysis should result
in a larger similarity score in the hope that this re-
flects more than just the surface similarity between
the candidate and the reference.
Given suitable Wordnet resources, mNCD
should alleviate the problem of translation vari-
ability especially in absence of multiple refer-
ence translations. Our submission uses the de-
fault METEOR exact stem synonym mod-
ules, which provide synonyms only for English.
We base our submission metric on the MT-NCD
metric and therefore define MT-mNCD as 1 ?
mNCD.
3 MT Evaluation System Description
3.1 System Parameters
The system parameters for the submission metrics
include how candidates and references are prepro-
cessed, the choice of compressor for the NCD it-
self, as well as the granularity of how large seg-
ments are evaluated by NCD and how they are
combined into a final score.
Partly due to time constraints we decided not to
introduce language specific parameters, therefore
we chose those parameter values that perform well
in overall and are simple to compute.
3.1.1 Preprocessing
Character casing For MT-NCD, we did ex-
periments without preprocessing and with lower-
casing candidates and references. On average over
all tasks for language pairs into English, lower-
casing consistently decreased the RANK correla-
tion scores but increased the CONST correlation
scores. No consistent effect could be found for the
language pairs from English. In our submission
metrics we use no preprocessing.
For MT-mNCD the used METEOR matching
module lower-cases the adapted words by default.
After adapting a synonym in a reference, we tried
to keep the casing as it was in the candidate, which
we called real-casing. We use no real-casing for
our submitted MT-mNCD metric as this did not
improve results consistently over all task into En-
glish.
Segment Replication Compression algorithms
may not work optimally with short strings, which
would deteriorate the approximation of Kol-
mogorov complexity. Our hypothesis was that
a replication of a string (?abc?) multiple times
(3 ? ?abc? = ?abcabcabc?) could help the com-
pression algorithm to produce a better estimate of
the algorithmic information. This was tested in
the MT evaluation framework, and correlation be-
tween MT-NCD and human judgments improved
when the segments were replicated two times.
Further replication did not produce improvements.
Results for the MT-NCD metric with replica-
tions one, two and three times are shown in Ta-
ble 1. The results are averages over all used lan-
guages. With two compared to one replication, the
details for each language show that RANK corre-
lation is improved for the target languages English
and French, but degrades for German and Spanish.
CONST andYES/NO correlation improve for all
languages except German. We did not use repli-
cation in our submissions.
3.1.2 Block size
The block size parameter governs the number of
joined segments that are compared with NCD as a
single string. On one extreme, with block size one,
345
R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
T
O
T
A
L
MT-NCD rep 1 .61 .71 .73 .68
MT-NCD rep 2 .62 .73 .75 .70
MT-NCD rep 3 .61 .72 .74 .69
Table 1: Effect of the replication factor on
MT-NCD correlation scores for the bz2 compres-
sor with block size one as average over all lan-
guages.
each segment is evaluated separately and the seg-
ment scores are aggregated to a document score.
This is similar to how other MT metrics, for ex-
ample, BLEU, work. The other extreme is to join
all segments together, with block size equal to the
number of segments, and evaluate it as a single
string, which is similar to document comparison.
For block aggregation we experimented with arith-
metic and geometric mean and obtained very sim-
ilar results. We selected arithmetic mean for the
submission metrics.
Figure 2 shows the block size effect on the cor-
relation between MT-NCD and human judgments
for different target languages. Except for Spanish,
our experiments indicate that the block size value
has little effect. Therefore, and given how other
evaluation metrics work, we chose a block size of
one for our submission metrics. We noticed incon-
sistencies with Spanish in other settings as well
and will investigate these issues further.
3.1.3 Compressor
There are several universal compressors that can
be utilized with NCD, for instance, zlib/gzip, bz2
and PPMZ, which represent different approaches
to compression. In terms of compression rate,
PPMZ is the best of the mentioned methods, but
it is considerably slower to compute compared to
the other methods. In terms of correlation with hu-
man judgments, NCD using bz2 performs slightly
worse than using PPMZ. Given much shorter com-
pression times for bz2 with very little correlation
performance degradation, our choice for the sub-
mission is the more standard bz2 compressor.
3.1.4 Segment Interleaving
Computation of NCD between longer texts (e.g.
documents) may exceed the internal compressor
window size that is present in some compression
2 5 10 20 50 100 500 2000 5000
0.0
0.2
0.4
0.6
0.8
1.0
block size in lines
sys
tem
 lev
el
 
 
 
co
rre
lati
on 
wit
h h
um
an 
judg
eme
nts
into fr
into en
into es
into de
fr
en
es
de
Figure 2: Effect of the block size on the correlation
of MT-NCD to human judgments for the system
level evaluation.
algorithms (Cebrian et al, 2005). In this case,
only a part of the texts to be compared are visible
at any time to the compressor and similarities to
the text outside the window will be missed. One
solution for the MT evaluation task is to use uti-
lize the known parallel segments of candidate and
reference translations. The two segment lists can
be interleaved so that the corresponding segments
are always adjacent and the compression window
size is not exceeded for matching segments.
For our submission, we chose a block size of
one, therefore every segment is evaluated individ-
ually. As a result, segment interleaving does not
have any effect. Segment interleaving is affective
in the block size evaluation and results shown in
Figure 2.
3.2 Evaluation Experiments
We chose parameters and evaluated our metrics
using the WMT08 part of the MetricsMATR 2010
development data, which contains human judg-
ments of the 2008 ACL Workshop on Statistical
Machine Translation (Callison-Burch et al, 2008)
for translations from a total of 30 MT systems be-
tween English and five other European languages.
There are human evaluations and several auto-
matic evaluations for the translations, divided into
several tasks defined by the language pair and the
domain of the translated sentences. For each of
these tasks, the WMT08 data contains about 2 000
346
reference sentences (segments) plus their aligned
translations for 12 to 17 different translation sys-
tems, depending on the language pair.
The human judgments include three categories
which contain evaluations for at most one segment
at a time, not whole documents. In the RANK
category, humans had to rank the output of five
MT systems according to quality. The CONST
category contains rankings for short phrases (con-
stituents), and the YES/NO category contains bi-
nary answers to judge if a short phrase is an ac-
ceptable translation or not.
We report RANK, CONST and YES/NO system
level correlations to human judgments as results of
our metrics for French, Spanish and German both
from and to English. The English?Spanish news
task was left out as most metrics had negative cor-
relation with human judgments.
The evaluation methodology used in Callison-
Burch et al (2008) allows us to measure how each
MT evaluation metric correlates with human judg-
ments on the system level, in which all translations
from each MT system are aggregated into a single
score. The system rankings based on the scores
are compared to human judgments.
Spearman?s rank correlation coefficient ? was
calculated between each MT metric and human
judgment category using the simplified equation:
? = 1?
6
?
i di
n(n2 ? 1)
(3)
where for each system i, di is the difference be-
tween the rank derived from annotators? input and
the rank obtained from the metric. From the anno-
tators? input, the nMT systems were ranked based
on the number of times each system?s output was
selected as the best translation divided by the num-
ber of times each system was part of a judgment.
3.3 Results
The results for WMT08 data for our submitted
metrics are shown in Table 2 and are sorted by the
RANK category separately for language pairs from
English and into English.
For tasks into English, the correlations show
that MT-mNCD improves over the MT-NCD met-
ric in all categories. Also the flexible match-
ing seems to work better for NCD-based metrics
than for BLEU, where mBLEU only improves
the CONST correlation scores. For tasks from
English, MT-mNCD shows slightly higher cor-
relation compared to MT-NCD, except for the
YES/NO category. The standard BLEU correla-
tion score is best of the shown evaluation met-
rics. Relaxed matching using mBLEU does not
improve BLEU?s RANK correlation scores here
either, but CONST and YES/NO correlation per-
forms better relative to BLEU than MT-mNCD
compared to MT-NCD.
R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
T
O
T
A
L
IN
T
O
E
N MT-mNCD .61 .74 .75 .70
MT-NCD .57 .69 .71 .66
mBLEU .50 .76 .70 .65
BLEU .50 .72 .74 .65
F
R
O
M
E
N BLEU .68 .79 .79 .75
MT-mNCD .67 .76 .74 .72
MT-NCD .65 .73 .75 .71
mBLEU .63 .81 .81 .75
Table 2: Average system-level correlations for the
WMT08 data sorted by RANK into English and
from English for our submitted metrics MT-NCD
and MT-mNCD and for BLEU and mBLEU
4 Conclusions
In our submissions, we applied MT-NCD and
MT-mNCD metrics and extended the NCD MT
evaluation metric to handle multiple references.
The reported experiment indicate a possible im-
provement for the multiple references.
We showed that a replication of segments as a
preprocessing step improves the correlation to hu-
man judgments. The string replication might alle-
viate problems in the compressor for short strings
and thus could provide better estimates of the al-
gorithmic information.
The results of our experiments show that re-
laxed matching in MT-mNCD works well with
proper synonym dictionaries, but is less effective
for tasks from English, which only use stemming.
MT-mNCD and MT-NCD are reasonably sim-
ple to compute and utilize standard and widely
used resources, such as the bz2 compression al-
gorithm and WordNet. The metrics perform com-
parable to the de facto standard BLEU. Improve-
ments with language dependent resources, in par-
ticular relaxed matching using synonym dictionar-
ies proved to be useful.
347
References
Abhaya Agarwal and Alon Lavie. 2008. METEOR,
M-BLEU and M-TER: evaluation metrics for high-
correlation with human rankings of machine trans-
lation output. In StatMT ?08: Proceedings of the
Third Workshop on Statistical Machine Translation,
pages 115?118, Morristown, NJ, USA. Association
for Computational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
StatMT ?08: Proceedings of the Third Workshop
on Statistical Machine Translation, pages 70?106,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Manuel Cebrian, Manuel Alfonseca, and Alfonso Or-
tega. 2005. Common pitfalls using the normalized
compression distance: What to watch out for in a
compressor. Communications in Information and
Systems, 5(4):367?384.
Rudi Cilibrasi and Paul Vitanyi. 2005. Clustering
by compression. IEEE Transactions on Information
Theory, 51:1523?1545.
Marcus Dobrinkat, Tero Tapiovaara, Jaakko J.
Va?yrynen, and Kimmo Kettunen. 2010. Evaluating
machine translations using mNCD. In Proceedings
of the ACL-2010 (to appear), Uppsala, Sweden.
Kimmo Kettunen. 2009. Packing it all up in search for
a language independent MT quality measure tool. In
Proceedings of LTC-09, 4th Language and Technol-
ogy Conference, pages 280?284, Poznan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu.
2001. BLEU: a method for automatic evaluation
of machine translation. Technical Report RC22176
(W0109-022), IBM Research Division, Thomas J.
Watson Research Center.
Steven Parker. 2008. BADGER: A new machine trans-
lation metric. In Metrics for Machine Translation
Challenge 2008, Waikiki, Hawai?i, October. AMTA.
Ray Solomonoff. 1964. Formal theory of inductive
inference. Part I. Information and Control, 7(1):1?
22.
Jaakko J. Va?yrynen, Tero Tapiovaara, Kimmo Ket-
tunen, and Marcus Dobrinkat. 2010. Normalized
compression distance as an automatic MT evalua-
tion metric. In Proceedings of MT 25 years on. To
appear.
348
