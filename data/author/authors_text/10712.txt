Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 87?95,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
Automatic Verb Extraction from Historical Swedish Texts
Eva Pettersson
Department of Linguistics and Philology
Uppsala University
Swedish National Graduate School
of Language Technology
eva.pettersson@lingfil.uu.se
Joakim Nivre
Department of Linguistics and Philology
Uppsala University
joakim.nivre@lingfil.uu.se
Abstract
Even though historical texts reveal a lot of
interesting information on culture and social
structure in the past, information access is lim-
ited and in most cases the only way to find the
information you are looking for is to manually
go through large volumes of text, searching
for interesting text segments. In this paper we
will explore the idea of facilitating this time-
consuming manual effort, using existing natu-
ral language processing techniques. Attention
is focused on automatically identifying verbs
in early modern Swedish texts (1550?1800).
The results indicate that it is possible to iden-
tify linguistic categories such as verbs in texts
from this period with a high level of precision
and recall, using morphological tools devel-
oped for present-day Swedish, if the text is
normalised into a more modern spelling be-
fore the morphological tools are applied.
1 Introduction
Historical texts constitute a rich source of data for
researchers interested in for example culture and so-
cial structure over time. It is however a very time-
consuming task to manually search for relevant pas-
sages in the texts available. It is likely that language
technology could substantially reduce the manual
effort involved and thus the time needed to access
this information, by automatically suggesting sec-
tions that may be of interest to the task at hand. The
interesting text segments could be identified using
for example semantic features or morphological and
syntactic cues in the text.
This would however require natural language pro-
cessing tools capable of handling historical texts,
which are in many respects different from contem-
porary written language, concerning both spelling
and syntax. Ideally, one would of course like to have
tools developed specifically for the time period of in-
terest, and emerging efforts to develop resources and
tools for historical languages are therefore welcome.
Despite these efforts, however, it is unlikely that we
will have anything close to complete coverage of dif-
ferent time periods even for a single language within
the foreseeable future.
In this paper, we will therefore instead exam-
ine the possibility of improving information access
in historical texts by adapting language technology
tools developed for contemporary written language.
The work has been carried out in close cooperation
with historians who are interested in what men and
women did for a living in the early modern Swedish
society (1550?1800). We will hence focus on identi-
fying linguistic categories in Swedish texts from this
period. The encouraging results show that you may
successfully analyse historical texts using NLP tools
developed for contemporary language, if analysis is
preceded by an orthographic normalisation step.
Section 2 presents related work and character-
istics of historical Swedish texts. The extraction
method is defined in section 3. In section 4 the ex-
periments are described, while the results are pre-
sented in section 5. Section 6 describes how the verb
extraction tool is used in ongoing historical research.
Finally, conclusions are drawn in section 7.
87
2 Background
2.1 Related Work
There are still not many studies performed on natu-
ral language processing of historical texts. Pennac-
chiotti and Zanzotto (2008) used contemporary dic-
tionaries and analysis tools to analyse Italian texts
from the period 1200?1881. The results showed that
the dictionary only covered approximately 27% of
the words in the oldest text, as compared to 62.5%
of the words in a contemporary Italian newspaper
text. The morphological analyser used in the study
reached an accuracy of 0.48 (as compared to 0.91
for modern text), while the part-of-speech tagger
yielded an accuracy of 0.54 (as compared to 0.97
for modern text).
Rocio et al (1999) used a grammar of contempo-
rary Portuguese to syntactically annotate medieval
Portuguese texts. To adapt the parser to the me-
dieval language, a lexical analyser was added includ-
ing a dictionary and inflectional rules for medieval
Portuguese. This combination proved to be success-
ful for partial parsing of medieval Portuguese texts,
even though there were some problems with gram-
mar limitations, dictionary incompleteness and in-
sufficient part-of-speech tagging.
Oravecz et al (2010) tried a semi-automatic ap-
proach to create an annotated corpus of texts from
the Old Hungarian period. The annotation was per-
formed in three steps: 1) sentence segmentation
and tokenisation, 2) standardisation/normalisation,
and 3) morphological analysis and disambiguation.
They concluded that normalisation is of vital impor-
tance to the performance of the morphological anal-
yser.
For the Swedish language, Borin et al (2007)
proposed a named-entity recognition system adapted
to Swedish literature from the 19th century. The sys-
tem recognises Person Names, Locations, Organisa-
tions, Artifacts (food/wine products, vehicles etc),
Work&Art (names of novels, sculptures etc), Events
(religious, cultural etc), Measure/Numerical expres-
sions and Temporal expressions. The named en-
tity recognition system was evaluated on texts from
the Swedish Literature Bank without any adaptation,
showing problems with spelling variation, inflec-
tional differences, unknown names and structural is-
sues (such as hyphens splitting a single name into
several entities).1 Normalising the texts before ap-
plying the named entity recognition systemmade the
f-score figures increase from 78.1% to 89.5%.
All the results presented in this section indicate
that existing natural language processing tools are
not applicable to historical texts without adaptation
of the tools, or the source text.
2.2 Characteristics of Historical Swedish Texts
Texts from the early modern Swedish period (1550?
1800) differ from present-day Swedish texts both
concerning orthography and syntax. Inflectional dif-
ferences include a richer verb paradigm in historical
texts as compared to contemporary Swedish. The
Swedish language was also strongly influenced by
other languages. Evidence of this is the placement
of the finite verb at the end of relative clauses in a
German-like fashion not usually found in Swedish
texts, as in ...om man i ha?chtelse sitter as compared
to om man sitter i ha?kte (?...if you in custody are?
vs ?...if you are in custody?).
Examples of the various orthographic differences
are the duplication of long vowels in words such as
saak (sak ?thing?) and stoor (stor ?big/large?), the
use of of fv instead of v, as in o?fver (o?ver ?over?),
and gh and dh instead of the present-day g and d, as
in na?ghon (na?gon ?somebody?) and fadhren (fadern
?the father?) (Bergman, 1995).
Furthermore, the lack of spelling conventions
causes the spelling to vary highly between different
writers and text genres, and even within the same
text. There is also great language variation in texts
from different parts of the period.
3 Verb Extraction
In the following we will focus on identifying verbs
in historical Swedish texts from the period 1550?
1800. The study has been carried out in cooper-
ation with historians who are interested in finding
out what men and women did for a living in the
early modern Swedish society. One way to do this
would be to search for occupational titles occurring
in the text. This is however not sufficient since many
people, especially women, had no occupational ti-
tle. Occupational titles are also vague, and may in-
clude several subcategories of work. In the material
1http://litteraturbanken.se/
88
already (manually) analysed by the historians, oc-
cupation is often described as a verb with a direct
object. Hence, automatically extracting and display-
ing the verbs in a text could help the historians in
the process of finding relevant text segments. The
verb extraction process developed for this purpose
is performed in maximally five steps, as illustrated
in figure 1.
The first step is tokenisation. Each token is
then optionally matched against dictionaries cover-
ing historical Swedish. Words not found in the his-
torical dictionaries are normalised to a more mod-
ern spelling before being processed by the morpho-
logical analyser. Finally, the tagger disambiguates
words with several interpretations, yielding a list of
all the verb candidates in the text. In the experi-
ments, we will examine what steps are essential, and
how they are combined to yield the best results.
3.1 Tokenisation
Tokenisation is performed using an in-house stan-
dard tokeniser. The result of the tokenisation is a
text segmented into one token per line, with a blank
line marking the start of a new sentence.
3.2 Historical Dictionaries
After tokenisation, the tokens are optionally
matched against two historical dictionaries dis-
tributed by The Swedish Language Bank:2
? The Medieval Lexical Database
A dictionary describing Medieval Swedish,
containing approximately 54 000 entries from
the following three books:
? K.F. So?derwalls Ordbok O?fver svenska
medeltids-spra?ket, vol I-III (So?derwall,
1918)
? K.F. So?derwalls Ordbok O?fver svenska
medeltids-spra?ket, vol IV-V (So?derwall,
1973)
? C.J. Schlyters Ordbok till Samlingen af
Sweriges Gamla Lagar (Schlyter, 1877)
? Dalin?s Dictionary
A dictionary covering 19th Century Swedish,
created from the printed version of Ordbok
2http://spraakbanken.gu.se/
O?fver svenska spra?ket, vol I?II by Dalin
(1855). The dictionary contains approximately
64 000 entries.
The dictionaries cover medieval Swedish and 19th
century Swedish respectively. We are actually in-
terested in the time period in between these two pe-
riods, but it is assumed that these dictionaries are
close enough to cover words found in the early mod-
ern period as well. It should further be noticed that
the electronically available versions of the dictionar-
ies are still in an early stage of development. This
means that coverage varies between different word
classes, and verbs are not covered to the same ex-
tent as for example nouns. Words with an irregu-
lar inflection (which is often the case for frequently
occurring verbs) also pose a problem in the current
dictionaries.
3.3 Normalisation Rules
Since both the morphological analyser and the tag-
ger used in the experiments are developed for han-
dling modern Swedish written language, running a
text with the old Swedish spelling preserved pre-
sumably means that these tools will fail to assign
correct analyses in many cases. Therefore, the text is
optionally transformed into a more modern spelling,
before running the document through the analysis
tools.
The normalisation procedure differs slightly for
morphological analysis as compared to tagging.
There are mainly two reasons why the same set
of normalisation rules may not be optimally used
both for the morphological analyser and for the tag-
ger. First, since the tagger (unlike the morphological
analyser) is context sensitive, the normalisation rules
developed for the tagger need to be designed to also
normalise words surrounding verbs, such as nouns,
determiners, etc. For the morphological analyser,
the main focus in formulating the rules has been on
handling verb forms. Secondly, to avoid being lim-
ited to a small set of rules, an incremental normalisa-
tion procedure has been used for the morphological
analyser in order to maximise recall without sacri-
ficing precision. In this incremental process, nor-
malisation rules are applied one by one, and the less
confident rules are only applied to words not iden-
tified by the morphological analyser in the previous
89
Figure 1: Overview of the verb extraction experiment
normalisation step. The tagger on the other hand is
robust, always yielding a tag for each token, even in
cases where the word form is not present in the dic-
tionary. Thus, the idea of running the normalisation
rules in an incremental manner is not an option for
the tagger.
The total set of normalisation rules used for the
morphological analyser is 39 rules, while 29 rules
were defined for the tagger. The rules are inspired
by (but not limited to) some of the changes in
the reformed Swedish spelling introduced in 1906
(Bergman, 1995). As a complement to the rules
based on the spelling reform, a number of empiri-
cally designed rules were formulated, based on the
development corpus described in section 4.1. The
empirical rules include the rewriting of verbal end-
ings (e.g. bega?rade ? bega?rde ?requested? and
utviste ? utvisade ?deported?), transforming dou-
ble consonants into a single consonant (vetta ? veta
?know?, pro?vass ? pro?vas ?be tried?) and vice versa
(upsteg ? uppsteg ?rose/ascended?, viste ? visste
?knew?).
3.4 Morphological Analysis and Tagging
SALDO is an electronically available lexical re-
source developed for present-day written Swedish.
It is based on Svenskt AssociationsLexikon (SAL), a
semantic dictionary compiled by Lo?nngren (1992).
The first version of the SALDO dictionary was re-
leased in 2008 and comprises 72 396 lexemes. In-
flectional information conforms to the definitions
in Nationalencyklopedins ordbok (1995), Svenska
Akademiens ordlista o?ver svenska spra?ket (2006)
and Svenska Akademiens grammatik (1999). Apart
from single word entries, the SALDO dictionary
also contains approximately 2 000 multi-word units,
including 1 100 verbs, mainly particle verbs (Borin
et al, 2008). In the experiments we will use SALDO
version 2.0, released in 2010 with a number of words
added, resulting in a dictionary comprising approxi-
mately 100 000 entries.
When running the SALDO morphological anal-
yser alone, a token is always considered to be a verb
if there is a verb interpretation present in the dictio-
nary, regardless of context. For example, the word
fo?r will always be analysed both as a verb (bring)
and as a preposition (for), even though in most cases
the prepositional interpretation is the correct one.
When running the maximum five steps in the verb
extraction procedure, the tagger will disambiguate
in cases where the morphological analyser has pro-
duced both a verb interpretation and a non-verb in-
terpretation. The tagger used in this study is Hun-
POS (Hala?csy et al, 2007), a free and open source
reimplementation of the HMM-based TnT-tagger
by Brants (2000). Megyesi (2008) showed that
the HunPOS tagger trained on the Stockholm-Umea?
Corpus (Gustafson-Capkova? and Hartmann, 2006)
is one of the best performing taggers for Swedish
texts.
4 Experiments
This section describes the experimental setup in-
cluding data preparation and experiments.
90
4.1 Data Preparation
A subset of Per Larssons dombok, a selection of
court records from 1638, was used as a basis for de-
veloping the automatic verb extraction tool. This
text consists of 11 439 tokens in total, and was
printed by Edling (1937). The initial 984 to-
kens of the text were used as development data,
i.e. words used when formulating the normalisation
rules, whereas the rest of the text was used solely for
evaluation.
A gold standard for evaluation was created, by
manually annotating all the verbs in the text. For
the verb annotation to be as accurate as possible, the
same text was annotated by two persons indepen-
dently, and the results analysed and compared until
consensus was reached. The resulting gold standard
includes 2 093 verbs in total.
4.2 Experiment 1: Normalisation Rules
In the first experiment we will compare morpholog-
ical analysis results before and after applying nor-
malisation rules. To investigate what results could
optimally be expected from the morphological anal-
ysis, SALDO was also run on present-day Swedish
text, i.e. the Stockholm-Umea? Corpus (SUC). SUC
is a balanced corpus consisting of a number of dif-
ferent text types representative of the Swedish lan-
guage in the 1990s. The corpus consists of approx-
imately one million tokens, distributed among 500
texts with approximately 2 000 tokens in each text.
Each word in the corpus is manually annotated with
part of speech, lemma and a number of morpho-
logical features (Gustafson-Capkova? and Hartmann,
2006).
4.3 Experiment 2: Morphological Analysis and
Tagging
In the second experiment we will focus on the
combination of morphological analysis and tagging,
based on the following settings:
morph A token is always considered to be a verb
if the morphological analysis contains a verb
interpretation.
tag A token is always considered to be a verb if it
has been analysed as a verb by the tagger.
morph or tag A token is considered to be a verb if
there is a morphological verb analysis or if it
has been analysed as a verb by the tagger.
morph and tag A token is considered to be a verb
if there is a morphological verb analysis and it
has been tagged as a verb.
To further refine the combination of morphologi-
cal analysis and tagging, a more fine-grained dis-
ambiguation method was introduced, where the tag-
ger is only used in contexts where the morphological
analyser has failed to provide an unambiguous inter-
pretation:
morph + tag A token is considered to be a verb if
it has been unambiguously analysed as a verb
by SALDO. Likewise a token is considered not
to be a verb, if it has been given one or more
analyses from SALDO, where none of the anal-
yses is a verb interpretation. If the token has
been given both a verb analysis and a non-verb
analysis by SALDO, the tagger gets to decide.
The tagger also decides for words not found in
SALDO.
4.4 Experiment 3: Historical Dictionaries
In the third experiment, the historical dictionaries
are added, using the following combinations:
medieval A token is considered to be a verb if it has
been unambiguously analysed as a verb by the
medieval dictionary. Likewise a token is con-
sidered not to be a verb, if it has been given
one or more analyses from the medieval dic-
tionary, where none of the analyses is a verb
interpretation. If the token has been given both
a verb analysis and a non-verb analysis by the
medieval dictionary, or if the token is not found
in the dictionary, the token is processed by the
morphological analyser and the tagger as de-
scribed in setting morph + tag.
19c A token is considered to be a verb if it has been
unambiguously analysed as a verb by the 19th
century dictionary. Likewise a token is consid-
ered not to be a verb, if it has been given one
or more analyses from the 19th century dictio-
nary, where none of the analyses is a verb in-
terpretation. If the token has been given both
91
a verb analysis and a non-verb analysis by the
19th century dictionary, or if the token is not
found in the dictionary, the token is processed
by the morphological analyser and the tagger as
described in setting morph + tag.
medieval + 19c A token is considered to be a verb
if it has been unambiguously analysed as a verb
by the medieval dictionary. Likewise a token is
considered not to be a verb, if it has been given
one or more analyses from the medieval dic-
tionary, where none of the analyses is a verb
interpretation. If the token has been given both
a verb analysis and a non-verb analysis by the
medieval dictionary, or if the token is not found
in the dictionary, the token is matched against
the 19th century dictionary before being pro-
cessed by the morphological analyser and the
tagger as described in setting morph + tag.
19c + medieval A token is considered to be a verb
if it has been unambiguously analysed as a verb
by the 19th century dictionary. Likewise a to-
ken is considered not to be a verb, if it has
been given one or more analyses from the 19th
century dictionary, where none of the analyses
is a verb interpretation. If the token has been
given both a verb analysis and a non-verb anal-
ysis by the 19th century dictionary, or if the to-
ken is not found in the dictionary, the token is
matched against the medieval dictionary before
being processed by the morphological analyser
and the tagger as described in setting morph +
tag.
5 Results
5.1 Normalisation Rules
Running the SALDO morphological analyser on the
test text with the old Swedish spelling preserved,
meant that only 30% of the words were analysed
at all. Applying the normalisation rules before the
morphological analysis is performed, drastically in-
creases recall. After only 5 rules have been ap-
plied, recall is increased by 11 percentage units, and
adding another 5 rules increases recall by another
26 percentage units. All in all, recall increases from
30% for unnormalised text to 83% after all normal-
isation rules have been applied, whereas precision
increases from 54% to 66%, as illustrated in table 1.
Recall is still significantly higher for contempo-
rary Swedish texts than for the historical text (99%
as compared to 83% with the best normalisation
settings). Nevertheless, the rapid increase in re-
call when applying the normalisation rules is very
promising, and it is yet to be explored how good re-
sults it is possible to reach if including more normal-
isation rules.
Precision Recall f-score
raw data 0.54 0.30 0.39
5 rules 0.61 0.41 0.49
10 rules 0.66 0.67 0.66
15 rules 0.66 0.68 0.67
20 rules 0.67 0.73 0.70
25 rules 0.66 0.78 0.72
30 rules 0.66 0.79 0.72
35 rules 0.66 0.82 0.73
39 rules 0.66 0.83 0.74
SUC corpus 0.53 0.99 0.69
Table 1: Morphological analysis results using SALDO
version 2.0, before and after incremental application of
normalisation rules, and compared to the Stockholm-
Umea? corpus of contemporary Swedish written language.
5.2 Morphological Analysis and Tagging
Table 2 presents the results of combining the
SALDO morphological analyser and the HunPOS
tagger, using the settings described in section 4.3.
Precision Recall f-score
morph 0.66 0.83 0.74
tag 0.81 0.86 0.83
morph or tag 0.61 0.92 0.74
morph and tag 0.92 0.80 0.85
morph + tag 0.82 0.88 0.85
Table 2: Results for normalised text, combining mor-
phological analysis and tagging. morph = morphological
analysis using SALDO. tag = tagging using HunPOS.
As could be expected, the tagger yields higher
precision than the morphological anlayser, due to
the fact that the morphological analyser renders all
analyses for a word form given in the dictionary, re-
gardless of context. The results of combining the
92
morphological analyser and the tagger are also quite
expected. In the case where a token is considered
to be a verb if there is a morphological verb analy-
sis or it has been analysed as a verb by the tagger,
a very high level of recall (92%) is achieved at the
expense of low precision, whereas the opposite is
true for the case where a token is considered to be
a verb if there is a morphological verb analysis and
it has been tagged as a verb. Using the tagger for
disambiguation only in ambiguous cases yields the
best results. It should be noted that using the morph-
and-tag setting results in the same f-score as the dis-
ambiguation setting. However, the disambiguation
setting performs better in terms of recall, which is of
importance to the historians in the project at hand.
Another advantage of using the disambiguation set-
ting is that the difference between precision and re-
call is less.
5.3 Historical Dictionaries
The results of using the historical dictionaries are
presented in table 3.
Precision Recall f-score
morph + tag 0.82 0.88 0.85
medieval 0.82 0.81 0.81
19c 0.82 0.86 0.84
medieval + 19c 0.81 0.79 0.80
19c + medieval 0.81 0.79 0.80
Table 3: Results for normalised text, combining histor-
ical dictionaries and contemporary analysis tools. me-
dieval = Medieval Lexical Database. 19c = Dalin?s Dic-
tionary. morph = morphological analysis using SALDO.
tag = tagging using HunPOS.
Adding the historical dictionaries did not improve
the verb analysis results; actually the opposite is
true. Studying the results of the analyses from the
medieval dictionary, one may notice that only two
verb analyses have been found when applied to the
test text, and both of them are erroneous in this con-
text (in both cases the word lass ?load? as in the
phrase 6 lass ho?o? ?6 loads of hay?). Furthermore,
the medieval dictionary produces quite a lot of non-
verb analyses for commonly occurring verbs, for ex-
ample skola (noun: ?shool?, verb: ?should/shall?),
kunna (?can/could?), kom (?come?), finna (?find?)
and vara (noun: ?goods?, verb: ?be?). Another rea-
son for the less encouraging results seems to be that
most of the words actually found and analysed cor-
rectly are words that are correctly analysed by the
contemporary tools as well, such as i (?in?), man
(?man/you?), sin (?his/her/its?), honom (?him?) and
in (?into?).
As for the 19th century dictionary, the same prob-
lems apply. For example, a number of frequent
verb forms are analysed as non-verbs (e.g. skall
?should/shall? and ligger ?lies?). There are also
non-verbs repeatedly analysed as verbs, such as
stadgar (?regulations?) and egne (?own?). As was
the case for the medieval dictionary, most of the
words analysed correctly by the 19th century dic-
tionary are commonly occuring words that would
have been correctly analysed by the morphological
analyser and/or the tagger as well, for example och
(?and?), men (?but?) and na?r (?when?).
6 Support for Historical Research
In the ongoing Gender and Work project at the De-
partment of History, Uppsala University, historians
are interested in what men and women did for a liv-
ing in the early modern Swedish Society.3 Informa-
tion on this is registered and made available for re-
search in a database, most often in the form of a verb
and its object(s). The automatic verb extraction tool
was developed in close cooperation with the Gender
and Work participants, with the aim of reducing the
manual effort involved in finding the relevant infor-
mation to enter into the database.
The verb extraction tool was integrated in a pro-
totypical graphical user interface, enabling the his-
torians to run the system on historical texts of their
choice. The interface provides facilities for upload-
ing files, generating a list of all the verbs in the file,
displaying verb concordances for interesting verbs,
and displaying the verb in a larger context. Figure
2 illustrates the graphical user interface, displaying
concordances for the verb anklaga (?accuse?). The
historians found the interface useful and are inter-
ested in integrating the tool in the Gender and Work
database. Further development of the verb extrac-
tion tool is now partly funded by the Gender and
Work project.
3http://gaw.hist.uu.se/
93
Figure 2: Concordances displayed for the verb anklaga (?accuse?) in the graphical user interface.
7 Conclusion
Today historians and other researchers working on
older texts have to manually go through large vol-
umes of text when searching for information on
for example culture or social structure in histori-
cal times. In this paper we have shown that this
time-consuming manual effort could be significantly
reduced using contemporary natural language pro-
cessing tools to display only those text segments
that may be of interest to the researcher. We have
described the development of a tool that automati-
cally identifies verbs in historical Swedish texts us-
ing morphological analysis and tagging, and a proto-
typical graphical user interface, integrating this tool.
The results indicate that it is possible to retrieve
verbs in Swedish texts from the 17th century with
82% precision and 88% recall, using morphological
tools for contemporary Swedish, if the text is nor-
malised into a more modern spelling before the mor-
phological tools are applied (recall may be increased
to 92% if a lower precision is accepted).
Adding electronically available dictionaries cov-
ering medieval Swedish and 19th century Swedish
respectively to the verb extraction tool, did not im-
prove the results as compared to using only contem-
porary NLP tools. This seems to be partly due to
the dictionaries still being in an early stage of devel-
opment, where lexical coverage is unevenly spread
among different word classes, and frequent, irregu-
larly inflected word forms are not covered. It would
therefore be interesting to study the results of the
historical dictionary lookup, when the dictionaries
are more mature.
Since the present extraction tool has been evalu-
ated on one single text, it would also be interesting
to explore how these extraction methods should be
adapted to handle language variation in texts from
different genres and time periods. Due to the lack
of spelling conventions, it would also be interest-
ing to see how the extraction process performs on
texts from the same period and genre, but written by
different authors. Future work also includes experi-
ments on identifying linguistic categories other than
verbs.
94
References
Go?sta Bergman. 1995. Kortfattad svensk spra?khistoria.
Prisma Magnum, 5th ed., Stockholm.
Lars Borin, Markus Forsberg, and Lennart Lo?nngren.
2008. SALDO 1.0 (Svenskt associationslexikon ver-
sion 2). Spra?kbanken, University of Gothenburg.
Lars Borin, Dimitrios Kokkinakis, and Leif-Jo?ran Ols-
son. 2007. Naming the Past: Named Entity and
Anomacy Recognition in 19th Century Swedish Lit-
erature). In: Proceedings of the Workshop on Lan-
guage Technology for Cultural Heritage Data (LaT-
eCH 2007), pages 1?8. Prague, Czech Republic.
Bra Bo?cker. 1995. Nationalencyklopedins ordbok. Bra
Bo?cker, Ho?gana?s.
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger. In: Proceedings of the 6th Applied
Natural Language Processing Conference (ANLP-00),
Seattle, Washington, USA.
Anders Fredrik Dalin. 1850?1855. Ordbok O?fver sven-
ska spra?ket. Vol I?II. Stockholm.
Nils Edling. 1937. Uppla?ndska dombo?cker. ja?mte in-
ledning, fo?rklaringar och register utgivna genom Nils
Edling. Uppsala.
Sofia Gustafson-Capkova? and Britt Hartmann. December
2006. Manual of the Stockholm Umea? Corpus version
2.0. Description of the content of the SUC 2.0 dis-
tribution, including the unfinished documentation by
Gunnel Ka?llgren.
Pe?ter Hala?csy, Andra?s Kornai, and Csaba Oravecz 2007.
HunPos - an open source trigram tagger. In: Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics Companion Vol-
ume Proceedings of the Demo and Poster Sessions,
pages 209?212. Association for Computational Lin-
guistics, Prague, Czech Republic.
Lennart Lo?nngren. 1992. Svenskt associationslexikon,
del I?IV. Department of Linguistics and Philology,
Uppsala University.
Bea?ta B. Megyesi. 2008. The Open Source Tagger
HunPoS for Swedish. Department of Linguistics and
Philology, Uppsala University.
Csaba Oravecz, Ba?lint Sass, and Eszter Simon 2010.
Semi-automatic Normalization of Old Hungarian
Codices. In: Proceedings of the ECAI 2010 Workshop
on Language Technology for Cultural Heritage, Social
Sciences, and Humanities (LaTeCH 2010). Pages 55?
59. 16 August, 2010 Faculty of Science, University of
Lisbon Lisbon, Portugal.
Marco Pennacchiotti and Fabio Massimo Zanzotto 2008.
Natural Language Processing Across Time: An Em-
pirical Investigation on Italian. In: Aarne Ranta and
Bengt Nordstro?m (Eds.): Advances in Natural Lan-
guage Processing. GoTAL 2008, LNAI Volume 5221,
pages 371?382. Springer-Verlag Berlin Heidelberg.
Vitor Rocio, Ma?rio Amado Alves, Jose? Gabriel Lopes,
Maria Francisca Xavier, and Grac?a Vicente. 1999.
Automated Creation of a Partially Syntactically Anno-
tated Corpus of Medieval Portuguese Using Contem-
porary Portuguese Resources. In: Proceedings of the
ATALA workshop on Treebanks, Paris, France.
Carl Johan Schlyter. 1877. Ordbok till Samlingen af
Sweriges Gamla Lagar. Lund.
Svenska Akademien. 2006. Svenska Akademiens or-
dlista o?ver svenska spra?ket. Norstedts Akademiska
Fo?rlag, Stockholm.
Knut Fredrik So?derwall. 1884?1918. Ordbok O?fver
svenska medeltids-spra?ket, vol I?III. Lund.
Knut Fredrik So?derwall. 1953?1973. Ordbok O?fver
svenska medeltids-spra?ket, vol IV?V. Lund.
Ulf Teleman, Staffan Hellberg, and Erik Andersson.
1999. Svenska Akademiens grammatik. Norstedts Or-
dbok, Stockholm.
95
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 65?74,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
Parsing the Past ? Identification of Verb Constructions in Historical Text
Eva Pettersson?, Bea?ta Megyesi and Joakim Nivre
Department of Linguistics and Philology
Uppsala University
?Swedish National Graduate School
of Language Technology
firstname.lastname@lingfil.uu.se
Abstract
Even though NLP tools are widely used for
contemporary text today, there is a lack of
tools that can handle historical documents.
Such tools could greatly facilitate the work
of researchers dealing with large volumes
of historical texts. In this paper we pro-
pose a method for extracting verbs and their
complements from historical Swedish text,
using NLP tools and dictionaries developed
for contemporary Swedish and a set of nor-
malisation rules that are applied before tag-
ging and parsing the text. When evaluated
on a sample of texts from the period 1550?
1880, this method identifies verbs with an
F-score of 77.2% and finds a partially or
completely correct set of complements for
55.6% of the verbs. Although these re-
sults are in general lower than for contem-
porary Swedish, they are strong enough to
make the approach useful for information
extraction in historical research. Moreover,
the exact match rate for complete verb con-
structions is in fact higher for historical
texts than for contemporary texts (38.7%
vs. 30.8%).
1 Introduction
Today there is an abundance of NLP tools that
can analyse contemporary language and extract
information relevant to a particular user need, but
there is a real lack of tools that can handle histor-
ical documents. Historians and other researchers
working with older texts are still mostly forced to
manually search large amounts of text in order to
find the passages of interest to their research. De-
veloping tools to facilitate this process is a great
challenge, however, as historical texts vary greatly
in both spelling and grammar between different
authors, genres and time periods, and even within
the same text, due to the lack of spelling conven-
tions. In addition to this, there is a shortage of
annotated resources that can be used for the de-
velopment and evaluation of new tools.
The work presented in this paper has been car-
ried out in cooperation with historians, who are
studying what men and women did for a living
in the Early Modern Swedish society. Currently,
the historians are manually extracting segments
describing work activities from a number of his-
torical texts, and entering them into a database,
the Gender and Work Database. Their work so far
has shown that the typical segment describing an
activity of work is a verb construction, that is, a
verb together with its complements (A?gren et al,
2011). (Examples of such segments can be found
below in Table 1.) It is very likely that the manual
effort and the time needed by the historians to find
these segments and enter them into the database
could be substantially reduced if verbs and their
complements were automatically extracted and
presented to the historian. This would give a gen-
eral overview of the content of a text, and the
task of the historian would be to select those seg-
ments that are actually describing work activities.
By linking extracted segments back to larger pas-
sages of text, historians would also be able to find
additional segments that were missed by the first
automatic extraction. The core of such a system
would be a component for identifying verb con-
structions in running text.
We propose a method for automatically iden-
tifying verbs and their complements in various
types of historical documents, produced in the
Early Modern Swedish period (1550?1800). The
method is based on using existing NLP tools for
65
contemporary Swedish, in particular a part-of-
speech tagger and a syntactic parser, and auto-
matically normalising the input text into a more
modern orthography before running it through
the tagger and parser. In order to increase the
precision of complement extraction, we use va-
lency dictionaries to filter out unlikely comple-
ments in the output of the syntactic parser. Using
this method, we are able to identify verbs with
an F-score of 77.2% and find a partially or com-
pletely correct set of complements for 55.6% of
the verbs. To our knowledge, extracting verb con-
structions from historical texts is a task that has
not been directly addressed in previous research,
which means that these results are also important
in setting benchmarks for future research.
The paper is structured as follows. Section 2
reviews related work. Section 3 describes the
method for identification of verbs and comple-
ments in more detail. Section 4 presents the data
and evaluation metrics used in our experiments,
and Section 5 discusses the results of the evalua-
tion. Finally, Section 6 concludes the paper.
2 Related Work
Using NLP tools for analysing historical texts is
still to a large extent unexplored. There is how-
ever a growing interest in this area, and there have
been attempts to analyse historical texts (1) by us-
ing contemporary NLP tools as they are, (2) by
using such tools in combination with normalisa-
tion rules and/or dictionaries covering historical
language variation, and (3) by training new tools
on annotated historical corpora.
Pennacchiotti and Zanzotto (2008) concluded
that contemporary NLP tools are not suitable as
they are for analysing historical text. They tried
to use a contemporary dictionary, morphological
analyser and part-of-speech tagger to analyse Ital-
ian texts from the period 1200?1881. In their ex-
periments, the dictionary only covered approxi-
mately 27% of the words in the oldest text, as
compared to 62.5% of the words in a contem-
porary Italian newspaper text. Consequently, the
morphological analyser based on the dictionary
reached an accuracy of only 48%, as compared
to 91% for contemporary text. Similarly, the part-
of-speech tagger used reached an accuracy of only
54%, as compared to 97% for contemporary text.
Oravecz et al (2010) included a standardis-
ation/normalisation step in their work on semi-
automatically annotating a corpus of Old Hun-
garian. Normalisation was performed using a
noisy channel model combined with morphologi-
cal analysis filtering and decision tree reranking.
Combining these methods, they reached a normal-
isation precision of 73.3%.
Rocio et al (1999) used a grammar of contem-
porary Portuguese to syntactically annotate me-
dieval Portuguese texts. A dictionary and inflec-
tional rules for medieval Portuguese were added
to the parser, to make it suitable for handling these
texts. This approach proved to be successful for
partial parsing of medieval Portuguese texts, even
though there were some problems remaining con-
cerning grammar limitations, dictionary incom-
pleteness and insufficient part-of-speech tagging.
Sa?nchez-Marco et al (2011) adapted an ex-
isting NLP tool to deal with Old Spanish. The
adapted tool had an accuracy of 94.5% in find-
ing the right part of speech, and 89.9% accuracy
in finding the complete morphological tag. The
adaptation was performed on the basis of a 20 mil-
lion token corpus of texts from the 12th to the 16th
century, and included expansion of the dictionary,
modification of tokenisation and affixation rules,
and retraining of the tagger. The retraining was
based on a gold standard of 30,000 tokens, where
the tokens were first pre-annotated with the con-
temporary tagger, and then manually corrected.
Adding new words to the dictionary had the high-
est impact on the results. This was done by au-
tomatically generating word forms through map-
ping old spelling variants to their contemporary
counterparts.
Pettersson and Nivre (2011) presented a study
on automatically extracting verbs from Swedish
17th century texts, using contemporary language
technology tools combined with normalisation
of the input text. The verb extraction process
included an iterative process of normalisation
and morphological analysis, followed by part-of-
speech tagging for disambiguation of competing
interpretations and for analysing words still un-
known to the morphological analyser after all nor-
malisation rules had been applied. Using this
method, verbs were extracted with 82% precision
and 88% recall. The study also included the re-
sults of using only the part-of-speech tagger for
verb recognition, i.e., dropping the morphologi-
cal analyser. This resulted in a small decrease in
precision to 81% and in recall to 86%.
66
3 Extraction of Verb Constructions
In this paper, we will focus on adapting existing
NLP tools by adding normalisation rules prior to
processing. We will mainly follow the method-
ology for verb extraction described in Petters-
son and Nivre (2011), but adding the extraction
of not only the verbs themselves, but also their
adherent complements. It would perhaps have
been desirable to use tools specifically trained for
analysing historical texts. This would however be
a resource-demanding task, considering the lack
of annotated data and the language variation, and
is currently not a realistic scenario.
The goal is to automatically extract verbs and
relevant complements from historical texts, in or-
der to give an overview of the contents and present
segments that are possibly describing work activ-
ities. In this context, we use the term complement
in a broad sense and do not impose a sharp dis-
tinction between complements and adjuncts, es-
pecially not for prepositional phrases. This is mo-
tivated by the fact that in the Gender and Work
Database, both segments that would traditionally
be seen as complements and phrases that would
rather be categorised as adjuncts have been con-
sidered relevant.
A closer look at the database shows that 67%
of the entered segments consist of a verb with a
direct object. Other common constructions are
verbs with a prepositional complement (11%),
verbs with both a direct object and a preposi-
tional complement (10%), and (intransitive) verbs
without complements (7%). Table 1 illustrates
the most common construction types found in
the database, which have been used to define the
rules for extracting complements from parsed sen-
tences. There were also a small number of seg-
ments (8 in total), that we were not able to cate-
gorise.
3.1 System Overview
The extraction of verbs and their complements is
basically performed in five steps:
1. Tokenisation
2. Normalisation
3. Part-of-speech tagging
4. Parsing
5. Extraction of verb constructions
Freq Comp Source Text Example
273 dobj dhe ba?rgadhe [Ho?o?]
they harvested [Hay]
47 pcomp [med een ha?st] kio?rtt
driven [with a horse]
43 dobj [det kio?pet] Han
+ [med ha?nness man] giort
pcomp [the bargain] He
made [with her husband]
30 intrans mala
to grind
5 dobj hulpit [Muremest:]
+ [inla?ggia Trappestenar]
infc helped [the Bricklayer]
[to make a Stone Stair]
3 indobj [honom] [Ja?rnet] sa?lltt
+ sold [him] [the Iron]
dobj
1 subc tillsee [att icke barnen
skulle go?ra skada]
see to it [that the children
do not do any harm]
Table 1: Segments describing work activities in the
Gender and Work Database; verbs underlined; com-
plements in brackets. Grammatical functions: dobj =
direct object, pcomp = prepositional complement, in-
trans = intransitive, indobj = indirect object, infc = in-
finitive clause, subc = subordinate clause.
Tokenisation is performed with a simple tokeniser
for Swedish that has not been adapted for histori-
cal texts.
3.2 Normalisation
After tokenisation, each word is normalised to a
more modern spelling using a set of 29 hand-
crafted rules. The rules were developed us-
ing a text sample from Per Larssons dombok,
a selection of court records from 1638 (Edling,
1937), a sample that has not been used in sub-
sequent evaluation. An example of a normalisa-
tion rule is the transformation of the letter com-
bination sch into sk, as in the old spelling schall
that is normalised to the contemporary spelling
skall (?shall/should?). Some additional rules were
also formulated based on the reformed Swedish
spelling introduced in 1906 (Bergman, 1995).
This set of rules includes the transformation of
double vowels into a single vowel, as in so?o?ka,
which is normalised into so?ka (?search?).
67
3.3 Part-of-speech Tagging
The purpose of part-of-speech tagging in our ex-
periments is both to find the verbs in the text and
to prepare for the parsing step, in which the com-
plements are identified. Part-of-speech tagging is
performed using HunPOS (Hala?csy et al, 2007),
a free and open source reimplementation of the
HMM-based TnT-tagger by Brants (2000). The
tagger is used with a pre-trained language model
based on the Stockholm-Umea? Corpus (SUC), a
balanced, manually annotated corpus of different
text types representative of the Swedish language
in the 1990s, comprising approximately one mil-
lion tokens (Gustafson-Capkova? and Hartmann,
2006). Megyesi (2009) showed that the HunPOS
tagger trained on SUC, is one of the best perform-
ing taggers for (contemporary) Swedish texts.
3.4 Parsing
The normalised and tagged input text is parsed us-
ing MaltParser version 1.6, a data-driven depen-
dency parser developed by Nivre et al (2006a).
In our experiments, the parser is run with a pre-
trained model1 for parsing contemporary Swedish
text, based on the Talbanken section of the
Swedish Treebank (Nivre et al, 2006b). The
parser produces dependency trees labeled with
grammatical functions, which can be used to iden-
tify different types of complements.
3.5 Extraction of Verb Constructions
The extraction of verb constructions from the
tagged and parsed text is performed in two steps:
1. Every word form analysed as a verb by the
tagger is treated as the head of a verb con-
struction.
2. Every phrase analysed as a dependent of the
verb by the parser is treated as a complement
provided that it has a relevant grammatical
function.
The following grammatical functions are defined
to be relevant:
1. Subject (SS)
2. Direct object (OO)
3. Indirect object (IO)
1http://maltparser.org/mco/swedish parser/swemalt.html
4. Predicative complement (SP)
5. Prepositional complement (OA)
6. Infinitive complement of object (VO)
7. Verb particle (PL)
Subjects are included only if the verb has been
analysed as a passive verb by the tagger, in which
case the subject is likely to correspond to the di-
rect object in the active voice.
In an attempt to improve precision in the com-
plement extraction phase, we also use valency
dictionaries for filtering the suggested comple-
ments. The valency frame of a verb tells us what
complements the verb is likely to occur with.
The assumption is that this information could
be useful for removing unlikely complements,
i.e., complements that are not part of the valency
frame for the verb in question. The following
example illustrates the potential usefulness of this
method:
J midler tijd kom greffuinnans gotze fougte thijtt
However, the Countess? estate bailiff came there
In this case, the parser analysed the partial noun
phrase greffuinnans gotze (?the Countess? estate?)
as a direct object connected to kom (?came?).
However, since the word kom is present in the va-
lency dictionaries, we know that it is an intransi-
tive verb that does not take a direct object. Hence,
this complement can be removed. The valency
dictionaries used for filtering are:
1. The Lexin dictionary, containing 3,550 verb
lemmas with valency information.2
2. The Parole dictionary, containing 4,308 verb
lemmas with valency information.3
3. An in-house machine translation dictionary,
containing 2,852 verb lemmas with valency
information.
4 Experimental Setup
4.1 Data
In order to evaluate the accuracy of our method,
we have used ten texts from the period 1527?
1737. The text types covered are court records
2http://spraakbanken.gu.se/lexin/valens lexikon.html
3http://spraakdata.gu.se/parole/lexikon/swedish.parole.lexikon.html
68
and documents related to the Church. In total,
there are 444,075 tokens in the corpus, distributed
as follows (number of tokens in parentheses):
Court records:
1. Per Larssons dombok (subset),1638(11,439)
2. Hammerdals tingslag, 1649?1686 (66,928)
3. Revsunds tingslag, 1649?1689 (101,020)
4. Vendels socken, 1615?45 (59,948)
5. Vendels socken, 1736?37 (55,780)
6. O?stra ha?rads i Njudung,1602?1605(34,956)
Documents related to the Church:
1. Va?stera?s recess, 1527 (12,193)
2. 1571 a?rs kyrkoordning (49,043)
3. Uppsala mo?tes beslut, 1593 (26,969)
4. 1686 a?rs kyrkolag (25,799)
A gold standard of 40 randomly selected sen-
tences from each text was compiled, i.e., in total
400 sentences. The gold standard was produced
by manually annotating the sentences regarding
verbs and complements. Because sentences are
much longer in these texts than in contemporary
texts, the 400 sentences together contain a total of
3,105 verbs. Each word form that was interpreted
as a verb was annotated with the tag VB, and com-
plements were enclosed in brackets labeled with
their grammatical function. This is illustrated in
Figure 1, which shows an annotated segment from
the test corpus.
For comparison with contemporary text, we
make use of a subset of the Stockholm-Umea? Cor-
pus of contemporary Swedish text, SUC (Ejerhed
and Ka?llgren, 1997). This subset contains those
segments in SUC that have been syntactically
annotated and manually revised in the Swedish
Treebank. In total, the subset includes approxi-
mately 20,000 tokens. Since the tagger used in the
experiments on historical texts is trained on the
whole of SUC, we had to slightly modify the ex-
traction algorithm in order not to evaluate on the
same data as the tagger has been trained. When
testing the algorithm on contemporary text, we
therefore trained a new model for the tagger, in-
cluding all tokens in SUC except for the tokens
reserved for evaluation.
Anklagadhes/VB1 Was accused/VB1
[SSvb1 [SSvb1
ryttaren the horse-rider
Hindrik Hindrik
Hyldh Hyldh
SSvb1] SSvb1]
hwilken who
[OOvb2 [OOvb2
mo?krenkningh rape
OOvb2] OOvb2]
giordt/VB2 done/VB2
medh with
en a
gienta girl
Elin Elin
Eriksdotter Eriksdotter
i in
Sika?s Sika?s
, ,
hwarfo?re why
ra?tten the Court
honnom him
tilspordhe/VB3 asked/VB3
[OOvb3 [OOvb3
om if
han he
[OOvb4 [OOvb4
dhetta this
OO vb4] OO vb4]
giordt/VB4 done/VB4
hafwer/VB5 has/VB5
OO vb3] OO vb3]
Figure 1: Annotated segment in the test corpus.
4.2 Evaluation Metrics
In order to get a more fine-grained picture of the
system?s performance, we want to evaluate three
different aspects:
1. Identification of verbs
2. Identification of complements
3. Identification of holistic verb constructions
The identification of verbs depends only on the
part-of-speech tagger and can be evaluated using
traditional precision and recall measures, compar-
ing the tokens analysed as verbs by the tagger to
the tokens analysed as verbs in the gold standard.
The identification of complements depends on
both the tagger and the parser and can also be
69
evaluated using precision and recall measures.
In this case, every complement identified by the
parser is compared to the complements annotated
in the gold standard. Precision is the number of
correct complements found by the parser divided
by the total number of complements output by
the parser, while recall is the number of correct
complements found by the parser divided by the
number of complements in the gold standard.
We do not take the labels into account when
assessing complements as correct or incorrect.
The motivation for this is that the overall aim
of the complement extraction is to present verb
expressions to historians, for them to consider
whether they are describing work activities or
not. In this context, only textual strings will
be of interest, and grammatical function labels
are ignored. For example, assume that the gold
standard is:
lefverere [IO honom] [OO Sa?dh]
deliver [IO him] [OO grain]
and that the system produces:
lefverere [OO honom]
deliver [OO him]
In this context, the complement honom (?him?)
will be regarded as correct, even though it has
been analysed as a direct object instead of an
indirect object. On the other hand, the evaluation
of complement identification is strict in that
it requires the complement found to coincide
exactly with the complement in the gold standard.
For example, assume the gold standard is:
effterfra?gat [OA om sinss manss do?dh]
asked [OA about her husband?s death]
and that the system produces:
effterfra?gat [OA om sinss manss]
asked [OA about her husband?s]
In this case, the complement will not be regarded
as correct because it does not cover exactly the
same textual string as the gold standard annota-
tion.
The identification of holistic verb construc-
tions, that is, a verb and all its complements,
depends on the identification of verbs and com-
plements, as well as the optional filtering of
complements using valency dictionaries. Here
we want to evaluate the entire text segment
extracted in a way that is relevant for the intended
application of the system. First of all, this means
that partially correct constructions should be
taken into account. Consider again the earlier
example:
effterfra?gat [OA om sinss manss do?dh]
asked [OA about her husband?s death]
and assume that the system produces:
effterfra?gat [OA om sinss manss]
asked [OA about her husband?s]
As noted above, this complement would be con-
sidered incorrect in the precision/recall evaluation
of complement extraction, even though only one
word is missing as compared to the gold standard,
and the output would probably still be valuable to
the end-user. Secondly, we should consider the to-
tal segment extracted for a verb including all com-
plements, rather than inspecting each complement
separately.
In order to reflect partially correct comple-
ments and take the total segment extracted for
each verb into account, we use a string-based
evaluation method for the identification of holis-
tic verb constructions. In this evaluation, all la-
bels and brackets are removed before comparing
the segments extracted to the segments in the text
corpus and each extracted instance is classified as
falling into one of the four following categories:
? Fully correct complement set (F)
? Partially correct complement set (P)
? Incorrect complement set (I)
? Missing complement set (M)
A complement set is regarded as fully correct if
the output string generated by the system is iden-
tical to the corresponding gold standard string.
Since labels and brackets have been removed,
these analyses will be regarded as identical:
lemnat [IO swaranden] [OO tid]
given [IO the defendant] [OO time]
70
lemnat [OO swaranden tid]
given [OO the defendant time]
A complement set is regarded as partially correct
if the output string generated by the system has a
non-empty overlap with the corresponding gold
standard string. For example, the following three
sets of analyses will be considered as partially
correct (gold standard top, system output bottom):
lefverere [IO honom] [OO Sa?dh]
deliver [IO him] [OO Grain]
lefverere [OO honom]
deliver [OO him]
effterfra?gat [OA om sinss manss do?dh]
asked [OA about her husband?s death]
effterfra?gat [OA om sinss manss]
asked [OA about her husband?s]
betale [PL a?ter] [IO ha?r Mattz] [OO Ra?gen]
pay [PL back] [IO mister Mattz] [OO the Rye]
betale [OO a?ter ha?r Mattz]
pay [OO back mister Mattz]
A (non-empty) complement set is regarded as in-
correct if the output string has no overlap with the
gold standard string. Finally, a complement set is
regarded as missing if the output string is empty
but the gold standard string is not. It is worth not-
ing that the four categories are mutually exclusive.
5 Results and Discussion
In this section, we evaluate the identification of
verbs, complements and holistic verb construc-
tions using the data and metrics described in the
previous section.
5.1 Verbs
Results on the identification of verbs using part-
of-speech tagging, with and without normalisa-
tion, are reported in Table 2. As can be seen, recall
drastically increases when normalisation rules are
applied prior to tagging, even though the normal-
isation rules used in this experiment are formu-
lated based on a subset of one single 17th cen-
tury text, and the test corpus contains samples of
various text types ranging from 1527?1737. Nor-
malisation also has a small positive effect on pre-
cision, and the best result for historical texts is
78.4% precision and 76.0% recall. This is slightly
Precision Recall F-score
Raw 75.4 60.0 66.9
Norm 78.4 76.0 77.2
SUC 99.1 99.1 99.1
Table 2: Identification of verbs by tagging. Raw = Un-
normalised input text. Norm = Normalisation of input
prior to tagging. SUC = Subset of Stockholm-Umea?
corpus of contemporary Swedish texts, as described in
section 4.1.
lower than the results presented by Pettersson and
Nivre (2011) where only 17th century text was
used for evaluation, indicating that the normalisa-
tion rules are somewhat biased towards 17th cen-
tury text, and that the results could be improved
if normalisation were adapted to specific time pe-
riods. It is also worth noting that the results are
substantially lower for historical text than the re-
sults for contemporary text, with precision and re-
call at 99.1%, but still high enough to be useful in
the intended context of application.
Tokens that are still erroneously analysed by
the tagger include the following cases:
? tokens where the old spelling is identical
to an existing, but different, word form
in contemporary language; for example,
the spelling skal would in contemporary
language be considered a noun (?shell?)
but in the old texts this spelling is used
for a word that is nowadays spelled skall
(?shall/should?) and should be regarded as a
verb;
? ambiguous words; for example, past partici-
ples are often spelled the same way as the
corresponding past tense verb, but participles
are not regarded as verb forms in our experi-
ments;4
? tokens that have not been normalised enough
and thus do not correspond to a word form
recognised by the tagger, e.g., the word
form lemnas which in contemporary lan-
guage should be spelled as la?mnas (?be
left?).
4Participles are only used adjectivally in Swedish, as the
perfect tense is formed using a distinct supine form of the
verb.
71
Precision Recall F-score
Raw 24.8 27.5 26.1
Norm 28.3 28.2 28.2
+Valency 33.1 25.5 28.8
SUC 68.2 70.7 69.5
+Valency 71.8 56.2 63.0
Table 3: Identification of complements by parsing.
Raw = Unnormalised input text. Norm = Normalisa-
tion of input prior to tagging and parsing. +Valency =
Adding valency filtering to the setting in the preced-
ing row. SUC = Subset of Stockholm-Umea? corpus of
contemporary Swedish texts, as described in section
4.1.
5.2 Complements
Recall and precision for the identification of com-
plements using parsing are presented in Table 3.
In this case, normalisation has a smaller effect
than in the case of tagging and affects precision
more than recall. Adding a filter that eliminates
unlikely complements based on the valency frame
of the verb in existing dictionaries predictably im-
proves precision at the expense of recall and re-
sults in a small F-score improvement.
Again, the best scores on the historical texts
are much lower than the corresponding results for
contemporary text, with an F-score of 28.8% in
the former case and 69.5% in the latter, but it is
worth remembering that precision and recall on
exactly matching complements is a harsh metric
that is not directly relevant for the intended appli-
cation. Finally, it is worth noting that the valency
filter has a large negative impact on recall for the
modern texts, resulting in a decrease in the F-
score, which indicates that the parser in this case
is quite successful at identifying complements (in
the wide sense) that are not covered by the va-
lency dictionaries.
5.3 Verb Constructions
As argued in section 4.2, precision and recall mea-
sures are not sufficient for evaluating the extrac-
tion of holistic verb constructions. A more rele-
vant assessment is made by counting the number
of fully correct, partially correct, incorrect and
missing complement sets for the verbs identified.
Table 4 summarises the results in accordance with
this metric.
First of all, we see that normalisation again has
a rather small effect on overall results, increas-
F P I M
Raw 32.6 20.3 29.3 17.8
Norm 34.5 19.5 25.2 20.8
+Valency 38.7 16.9 18.9 25.5
SUC 30.3 54.2 9.1 6.4
+Valency 30.8 47.9 6.8 14.6
Table 4: Identification of holistic verb constructions.
F = Fully correct, P = Partially correct, I = Incorrect,
M = Missing. Raw = Unnormalised input text. Norm
= Normalisation of input prior to tagging and parsing.
+Valency = Adding valency filtering to the setting in
the preceding row. SUC = Subset of Stockholm-Umea?
corpus of contemporary Swedish texts, as described in
section 4.1.
ing the number of fully correct constructions and
decreasing the number of incorrect constructions,
but also leading to an increase in the number of
missing complements. Adding the valency fil-
ter to remove unlikely complements has a simi-
lar effect and increases the percentage of correctly
extracted verb constructions to 38.7% while de-
creasing the share of incorrect constructions to
18.9%. However, it also increases the percent-
age of verbs with missing complement sets from
20.8% to 25.5%. This is partly due to the fact
that some of the verbs are used in a slightly dif-
ferent way in historical text as compared to con-
temporary text, meaning that the valency frames
are not as reliable. For example, the verb avsta?
(?refrain?) in the historical corpus is used with a
direct object, as in Anders Andersson afsta?dt sitt
skatte hemman (?Anders Andersson refrained his
homestead?), whereas in a contemporary context
this verb would more likely be used with a prepo-
sitional complement, avsta? fra?n na?gonting (?re-
frain from something?).
In total, 55.6% of the verbs are assigned a fully
or partially correct set of complements. This is
again lower than the result for contemporary texts
(78.7%), but the difference is smaller than for the
previous metrics, which is encouraging given that
the evaluation in this section is most relevant for
the intended application. Moreover, it is worth
noting that the difference is mostly to be found
in the category of partially correct constructions,
where the best result for modern texts is 54.2%,
to be compared to 16.9% for the historical texts.
With respect to fully correct constructions, how-
ever, the results are actually better for the histor-
72
ical texts than for the modern texts, 38.7% vs.
30.8%, a rather surprising positive result.
6 Conclusion
We have presented a method for automatically ex-
tracting verbs and their complements from histor-
ical Swedish texts, more precisely texts from the
Early Modern era (1550?1800), with the aim of
providing language technology support for histor-
ical research. We have shown that it is possible
to use existing contemporary NLP tools and dic-
tionaries for this purpose, provided that the input
text is first (automatically) normalised to a more
modern spelling. With the best configuration of
our tools, we can identify verbs with an F-score
of 77.2% and find a partially or completely cor-
rect set of complements for 55.6% of the verbs.
To the best of our knowledge, these are the first
results of their kind.
In addition to presenting a method for the iden-
tification of verb constructions, we have also pro-
posed a new evaluation framework for such meth-
ods in the context of information extraction for
historical research. As a complement to standard
precision and recall metrics for verbs and their
complements, we have evaluated the text seg-
ments extracted using the categories fully correct,
partially correct, incorrect, and missing. One
important topic for future research is to validate
this evaluation framework by correlating it to the
perceived usefulness of the system when used
by historians working on the Gender and Work
Database. Preliminary experiments using a proto-
type system indicate that this kind of support can
in fact reduce the time-consuming, manual work
that is currently carried out by historians and other
researchers working with older texts.
Another topic for future research concerns the
variation in performance across time periods and
text types. In the current evaluation, court records
and papers related to the Church ranging from
1527 to 1737 have been sampled in the gold stan-
dard. It would be interesting to explore in more
detail how the program performs on the oldest
texts as compared to the youngest texts, and on
court records as compared to the other genres.
References
Maria A?gren, Rosemarie Fiebranz, Erik Lindberg, and
Jonas Lindstro?m. 2011. Making verbs count. The
research project ?Gender and Work? and its method-
ology. Scandinavian Economic History Review,
59(3):271?291. Forthcoming.
Go?sta Bergman. 1995. Kortfattad svensk
spra?khistoria. Prisma Magnum, Stockholm, 5th
edition.
Thorsten Brants. 2000. TnT - a statistical part-of-
speech tagger. In Proceedings of the 6th Applied
Natural Language Processing Conference (ANLP),
Seattle, Washington, USA.
Nils Edling. 1937. Uppla?ndska dombo?cker. Almqvist
& Wiksells.
Eva Ejerhed and Gunnel Ka?llgren. 1997. Stockholm
Umea? Corpus. Version 1.0. Produced by Depart-
ment of Linguistics, Umea? University and Depart-
ment of Linguistics, Stockholm University. ISBN
91-7191-348-3.
Sofia Gustafson-Capkova? and Britt Hartmann. 2006.
Manual of the Stockholm Umea? Corpus version 2.0.
Technical report, December.
Pe?ter Hala?csy, Andra?s Kornai, and Csaba Oravecz.
2007. HunPos - an open source trigram tagger.
In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics, pages
209?212, Prague, Czech Republic.
Bea?ta B. Megyesi. 2009. The open source tagger
HunPos for Swedish. In Proceedings of the 17th
Nordic Conference on Computational Linguistics
(NODALIDA).
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006a.
MaltParser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of the 5th
international conference on Language Resources
and Evaluation (LREC), pages 2216?2219, Genoa,
Italy, May.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006b.
Talbanken05: A Swedish treebank with phrase
structure and dependency annotation. In Proceed-
ings of the 5th international conference on Lan-
guage Resources and Evaluation (LREC, pages 24?
26, Genoa, Italy, May.
Csaba Oravecz, Ba?lint Sass, and Eszter Simon. 2010.
Semi-automatic normalization of Old Hungarian
codices. In Proceedings of the ECAI Workshop on
Language Technology for Cultural Heritage, Social
Sciences, and Humanities (LaTeCH), pages 55?59,
Faculty of Science, University of Lisbon Lisbon,
Portugal, August.
Marco Pennacchiotti and Fabio Massimo Zanzotto.
2008. Natural language processing across time: An
empirical investigation on Italian. In Advances in
Natural Language Processing. GoTAL, LNAI, vol-
ume 5221, pages 371?382.
Eva Pettersson and Joakim Nivre. 2011. Automatic
verb extraction from historical Swedish texts. In
Proceedings of the 5th ACL-HLT Workshop on Lan-
guage Technology for Cultural Heritage, Social Sci-
ences, and Humanities, pages 87?95, Portland, OR,
73
USA, June. Association for Computational Linguis-
tics.
Vito Rocio, Ma?rio Amado Alves, Jose? Gabriel Lopes,
Maria Francisca Xavier, and Grac?a Vicente. 1999.
Automated creation of a partially syntactically an-
notated corpus of Medieval Portuguese using con-
temporary Portuguese resources. In Proceedings of
the ATALA workshop on Treebanks, Paris, France.
Cristina Sa?nchez-Marco, Gemma Boleda, and Llu??s
Padro?. 2011. Extending the tool, or how to an-
notate historical language varieties. In Proceedings
of the 5th ACL-HLT Workshop on Language Tech-
nology for Cultural Heritage, Social Sciences, and
Humanities, pages 1?9, Portland, OR, USA, June.
Association for Computational Linguistics.
74
Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 32?41,
Gothenburg, Sweden, April 26 2014.
c?2014 Association for Computational Linguistics
A Multilingual Evaluation of Three Spelling Normalisation
Methods for Historical Text
Eva Pettersson
1,2
, Be?ta Megyesi
1
and Joakim Nivre
1
(1) Department of Linguistics and Philology
Uppsala University
(2) Swedish National Graduate School
of Language Technology
firstname.lastname@lingfil.uu.se
Abstract
We present a multilingual evaluation of
approaches for spelling normalisation of
historical text based on data from five
languages: English, German, Hungarian,
Icelandic, and Swedish. Three different
normalisation methods are evaluated: a
simplistic filtering model, a Levenshtein-
based approach, and a character-based sta-
tistical machine translation approach. The
evaluation shows that the machine transla-
tion approach often gives the best results,
but also that all approaches improve over
the baseline and that no single method
works best for all languages.
1 Introduction
Language technology for historical text is a field
of research imposing a variety of challenges. Nev-
ertheless, there is an increasing need for natural
language processing (NLP) tools adapted to his-
torical texts, as an aid for researchers in the hu-
manities field. For example, the historians in the
Gender and Work project are studying what men
and women did for a living in the Early Mod-
ern Swedish society (?gren et al., 2011). In this
project, researchers have found that the most im-
portant words in revealing this information are
verbs such as fishing, selling etc. Instead of man-
ually going through written sources from this time
period, it is therefore assumed that an NLP tool
that automatically searches through a number of
historical documents and presents the contained
verbs (and possibly their complements), would
make the process of finding relevant text passages
more effective.
A major challenge in developing language tech-
nology for historical text is that historical language
often is under-resourced with regard to annotated
data needed for training NLP tools. This prob-
lem is further aggravated by the fact that histori-
cal texts may refer to texts from a long period of
time, during which language has changed. NLP
tools trained on 13th century texts may thus not
perform well on texts from the 18th century. Fur-
thermore, historical language usually shows a sub-
stantial variation in spelling and grammar between
different genres, different authors and even within
the same text written by the same author, due to
the lack of spelling conventions.
To deal with the limited resources and the high
degree of spelling variation, one commonly ap-
plied approach is to automatically normalise the
original spelling to a more modern spelling, be-
fore applying the NLP tools. This way, NLP tools
available for the modern language may be used
to analyse historical text. Even though there may
be structural differences as well between histor-
ical and modern language, spelling is the most
striking difference. Moreover, language technol-
ogy tools such as taggers often to some degree
rely on statistics on word form n-grams and to-
ken frequencies, implying that spelling moderni-
sation is an important step for improving the per-
formance of such tools when applied to historical
text. This paper presents an evaluation of three
approaches to spelling normalisation: 1) a filter-
ing approach based on corpus data, 2) an approach
based on Levenshtein edit distance, and 3) an
approach implementing character-based statistical
machine translation (SMT) techniques. These ap-
proaches have previously solely been evaluated in
isolation, without comparison to each other, and
for one or two languages only. We compare the
results of the different methods in a multilingual
evaluation including five languages, and we show
that all three approaches have a positive impact on
normalisation accuracy as compared to the base-
line. There is no single method that yields the
highest normalisation accuracy for all languages,
but for four out of five languages within the scope
32
of our study, the SMT-based approach gives the
best results.
2 Related Work
Spelling normalisation of historical text has pre-
viously been approached using techniques such as
dictionary lookup, edit distance calculations, and
machine translation.
Rayson et al. (2005) tried an approach based on
dictionary lookup, where a mapping scheme from
historical to modern spelling for 16th to 19th cen-
tury English texts was manually created, resulting
in the VARD tool (VARiant Detector) comprising
45,805 entries. The performance of the normal-
isation tool was evaluated on a set of 17th cen-
tury texts, and compared to the performance of
modern spell checkers on the same text. The re-
sults showed that between a third and a half of
all tokens (depending on which test text was used)
were correctly normalised by both VARD and MS
Word, whereas approximately one third of the to-
kens were correctly normalised only when using
VARD. The percentage of tokens correctly nor-
malised only by MS Word was substantially lower;
approximately 6%. VARD was later further devel-
oped into VARD2, combining the original word
list with data-driven techniques in the form of pho-
netic matching against a modern dictionary, and
letter replacement rules based on common spelling
variation patterns (Baron and Rayson, 2008).
Jurish (2008) argued that due to the lack of or-
thographic conventions, spelling generally reflects
the phonetic form of the word to a higher de-
gree in historical text. Furthermore, it is assumed
that phonetic properties are less resistant to di-
achronic change than orthography. Accordingly,
Jurish explored the idea of comparing the simi-
larity between phonetic forms rather than ortho-
graphic forms. For grapheme-to-phoneme conver-
sion, a module of the IMS German Festival text-
to-speech system (Black and Taylor, 1997) was
used, with a rule-set adapted to historical word
forms. Evaluation was performed on a corpus of
historical German verse quotations extracted from
Deutsches W?rterbuch, containing 5,491,982 to-
kens (318,383 types). Without normalisation, ap-
proximately 84% of the tokens were recognised
by a morphological analyser. After normalisa-
tion, 92% of the tokens were recognised. Adding
lemma-based heuristics, coverage increased fur-
ther to 94% of the tokens.
A Levenshtein similarity approach to normal-
isation was presented by Bollmann et al. (2011)
for Early New High German, where Levenshtein-
based normalisation rules were automatically de-
rived from a word-aligned parallel corpus consist-
ing of the Martin Luther Bible in its 1545 edi-
tion and its 1892 version, respectively. Using this
normalisation technique, the proportion of words
with a spelling identical to the modern spelling in-
creased from 65% in the original text to 91% in the
normalised text. This normalisation method was
further evaluated by Bollmann (2013), comparing
the performance of the RFTagger applied to histor-
ical text before and after normalisation. For every
evaluation text, the tagger was trained on between
100 and 1,000 manually normalised tokens, and
evaluated on the remaining tokens in the same text.
For one manuscript from the 15th century, tagging
accuracy was improved from approximately 29%
to 78% using this method.
Another Levenshtein-based approach to nor-
malisation was presented by Pettersson et al.
(2013b), using context-sensitive, weighted edit
distance calculations combined with compound
splitting. This method requires no annotated his-
torical training data, since normalisation candi-
dates are extracted by Levenshtein comparisons
between the original historical word form and
present-day dictionary entries. However, if a cor-
pus of manually normalised historical text is avail-
able, this can optionally be included for dictio-
nary lookup and weighted Levenshtein calcula-
tions, improving precision. This technique was
evaluated for Early Modern Swedish, and in the
best setting, the proportion of words in the his-
torical text with a spelling identical to the mod-
ern gold standard spelling increased from 64.6%
to 86.9%.
Pettersson et al. (2013a) treated the normalisa-
tion task as a translation problem, using character-
based SMT techniques in the spelling normalisa-
tion process. With the SMT-based approach, the
proportion of tokens in the historical text with
a spelling identical to the modern gold standard
spelling increased from 64.6% to 92.3% for Early
Modern Swedish, and from 64.8% to 83.9% for
15th century Icelandic. It was also shown that nor-
malisation had a positive effect on subsequent tag-
ging and parsing.
Language technology for historical text also has
a lot in common with adaptation of NLP tools
33
for handling present-day SMS messages and mi-
croblog text such as Twitter. In both genres there
is a high degree of spelling variation, ad hoc ab-
breviations and ungrammatical structures impos-
ing the problem of data sparseness. Similar meth-
ods for spelling normalisation may thus be used
for both tasks. Han and Baldwin (2011) pre-
sented a method for normalising SMS and Twitter
text based on morphophonemic similarity, com-
bining lexical edit distance, phonemic edit dis-
tance, prefix substring, suffix substring, and the
longest common subsequence. Context was taken
into account by means of dependency structures
generated by the Stanford Parser applied to a cor-
pus of New York Times articles. In the best set-
ting, a token-level F-score of 75.5% and 75.3%
was reported for SMS messages and Twitter texts
respectively.
3 Approaches
3.1 The Filtering Approach
The filtering approach presupposes access to a par-
allel training corpus of token pairs with historical
word forms mapped to their modernised spelling.
In the normalisation process, whenever a token is
encountered that also occurred in the training data,
the most frequent modern spelling associated with
that token in the training corpus is chosen for nor-
malisation. Other tokens are left unchanged.
3.2 The Levenshtein-based Approach
The Levenshtein-based approach was originally
presented by Pettersson et al. (2013b). In its basic
version, no historical training data is needed,
which is an important aspect considering the
common data sparseness issue, as discussed in
Section 1. Instead, a modern language dictionary
or corpus is required, from which normalisation
candidates are extracted based on edit distance
comparisons to the original historical word form.
If there is parallel data available, i.e. the same
text in its historical and its modernised spelling,
this data can be used to make more reliable Lev-
enshtein calculations by assigning weights lower
than 1 to frequently occurring edits observed in
the training data. The weights are then calculated
by comparing the frequency of each edit occurring
in the training corpus to the frequency with which
the specific source characters are left unchanged,
in accordance with the following formula:
Frequency of Unchanged
Frequency of Edit + Frequency of Unchanged
Context-sensitive weights are added to handle ed-
its affecting more than one character. The context-
sensitive weights are calculated by the same for-
mula as the single-character weights, and include
the following operations:
? double deletion: personnes? persons
? double insertion: strait? straight
? single-to-double substitution: juge? judge
? double-to-single substitution: moost? most
For all historical word forms in the training cor-
pus that are not identical in the modern spelling,
all possible single-character edits as well as multi-
character edits are counted for weighting. Hence,
the historical word form personnes, mapped to
the modern spelling persons, will yield weights
for double-to-single deletion of -ne, as illustrated
above, but also for single deletion of -n and single
deletion of -e.
Finally, a tuning corpus is used to set a
threshold for which maximum edit distance
to allow between the original word form and
its normalisation candidate(s). Based on the
average edit distance between the historical
word forms and their modern spelling in the
tuning corpus, the threshold is calculated by the
following formula (where 1.96 times the stan-
dard deviation is added to cover 95% of the cases):
avg editdistance +(1.96?standard deviation)
If several normalisation candidates have the same
edit distance as compared to the source word, the
most frequent candidate is chosen, based on mod-
ern corpus data. If none of the highest-ranked nor-
malisation candidates are present in the corpus, or
if there are several candidates with the same fre-
quency distribution, a final candidate is randomly
chosen.
3.3 The SMT-based Approach
In the SMT-based approach, originally presented
by Pettersson et al. (2013a), spelling normali-
sation is treated as a translation task. To ad-
dress changes in spelling rather than full transla-
tion of words and phrases, character-based trans-
lation (without lexical reordering) is performed,
a well-known technique for transliteration and
34
character-level translation between closely related
languages (Matthews, 2007; Vilar et al., 2007;
Nakov and Tiedemann, 2012). In character-level
SMT, phrases are modeled as character sequences
instead of word sequences, and translation models
are trained on character-aligned parallel corpora
whereas language models are trained on character
N-grams.
Since the set of possible characters in a lan-
guage is far more limited than the number of pos-
sible word forms, and the same corpus will present
a larger quantity of character instances than token
instances, only a rather small amount of parallel
data is needed for training the translation models
and the language models in character-based trans-
lation. Pettersson et al. (2013a) showed that with
a training and tuning set of only 1,000 pairs of his-
torical word forms mapped to modern spelling, a
normalisation accuracy of 76.5% was achieved for
Icelandic, as compared to 83.9% with a full-sized
training corpus of 33,888 token pairs. Their full
experiment on varying the size of the training data
is illustrated in Figure 1.
 76 77
 78 79
 80 81
 82 83
 84 85
 0  5  10  15  20  25  30  35
N
o
r
m
a
l
i
s
a
t
i
o
n
 
a
c
c
u
r
a
c
y
Size of training data (K tokens)
Normalisation accuracy for different sizes of the alignment training data
Figure 1: Normalisation accuracy when varying
the size of the alignment training data.
We use the same set of training data for the SMT
approach as for the filtering approach and for the
assignment of weights in the Levenshtein-based
approach, i.e. a set of token pairs mapping his-
torical word forms to their manually modernised
spelling. These corpora have the format of one to-
ken per line, with blank lines separating sentences.
To fully adapt this format to the format needed
for training the character-based translation mod-
els, the characters within each token are separated
by space. The SMT system will now regard each
character as a word, the full token as a sentence
and the entire sentence as a section.
The SMT engine used is Moses with all its stan-
dard components. A phrase-based model is ap-
plied, where the feature weights are trained us-
ing MERT with BLEU over character-sequences
as the objective function. The maximum size of a
phrase (sequence of characters) is set to 10.
Two different character alignment techniques
are tested: (i) the word alignment toolkit GIZA++
(Och and Ney, 2000), and (ii) a weighted finite
state transducer implemented in the m2m-aligner
(Jiampojamarn et al., 2007). GIZA is run with
standard word alignment models for character un-
igrams and bigrams, whereas the m2m aligner
implements transducer models based on context-
independent single character and multi-character
edit operations. The transducer is trained us-
ing EM on (unaligned) parallel training data, and
the final model can then be used to produce a
Viterbi alignment between given pairs of charac-
ter strings.
An example is given in Figure 2, where the Ice-
landic word forms me?r? me?ur and giallda?
galda have been aligned at a character-level using
the m2m-aligner. In this example, the  symbol
represents empty alignments, meaning insertions
or deletions. The  symbol in the source word
me?r denotes the insertion of u in the target word
me?ur. Likewise, the  symbol in the target word
galda denotes the deletion of i as compared to the
source word giallda. Furthermore, the alignment
of giallda to galda illustrates the inclusion of
multi-character edit operations, where the colon
denotes a 2:1 alignment where both letters l and d
in the source word correspond to the single letter
d in the target word.
m|e|?||r| m|e|?|u|r|
g|i|a|l|l:d|a| g||a|l|d|a|
Figure 2: m2m character-level alignment.
4 Data
In the following, we will describe the data sets
used for running the filtering approach, the Lev-
enshtein edit distance approach, and the character-
based SMT approach for historical spelling nor-
malisation applied to five languages: English, Ger-
man, Hungarian, Icelandic, and Swedish. For
convenience, we use the notions of training, tun-
35
ing and evaluation corpora, which are well-known
concepts within SMT. These data sets have been
created by extracting every 9th sentence from the
total corpus to the tuning corpus, and every 10th
sentence to the evaluation corpus, whereas the rest
of the sentences have been extracted to a training
corpus.
1
In the filtering approach, there is in fact no
distinction between training and tuning corpora,
since both data sets are combined in the dictionary
lookup process. As for the Levenshtein edit dis-
tance approach, the training corpus is used for ex-
tracting single-character and multi-character edits
by comparing the historical word forms to their
modern spelling. The edits extracted from the
training corpus are then weighted based on their
relative frequency in the tuning corpus.
The historical texts used for training and evalu-
ation are required to be available both in their orig-
inal, historical spelling and in a manually mod-
ernised and validated spelling. A modern trans-
lation of a historical text is generally not usable,
since word order and sentence structure have to re-
main the same to enable training and evaluation of
the proposed methods. The access to such data is
very limited, meaning that the data sets used in our
experiments vary in size, genres and time periods
between the languages.
4.1 English
For training, tuning and evaluation in the En-
glish experiments, we use the Innsbruck Cor-
pus of English Letters, a manually normalised
collection of letters from the period 1386?1698.
This corpus is a subset of the Innsbruck Com-
puter Archive of Machine-Readable English Texts,
ICAMET (Markus, 1999). A subset of the British
National Corpus (BNC) is used as the single mod-
ern language resource both for the Levenshtein-
based and for the SMT-based approach. Table 1
presents in more detail the data sets used in the
English experiments.
4.2 German
For training, tuning and evaluation in the German
experiments, we use a manually normalised sub-
set of the GerManC corpus of German texts from
the period 1650?1800 (Scheible et al., 2011). This
subset contains 22 texts from the period 1659?
1780, within the genres of drama, newspaper text,
1
For information on how to access the data sets used in
our experiments, please contact the authors.
Resource Data Tokens Types
Training ICAMET 148,852 18,267
Tuning ICAMET 16,461 4,391
Evaluation ICAMET 17,791 4,573
Lev. dict. BNC 2,088,680 69,153
Lev. freq. BNC 2,088,680 69,153
SMT lm BNC 2,088,680 69,153
Table 1: Language resources for English.
letters, sermons, narrative prose, humanities, sci-
ence och legal documents. The German Parole
corpus is used as the single modern language re-
source both for the Levenshtein-based and for the
SMT-based approach (Teubert (ed.), 2003). Table
2 presents in more detail the data sets used in the
German experiments.
Resource Data Tokens Types
Training GerManC 39,887 9,055
Tuning GerManC 5,418 2,056
Evaluation GerManC 5,005 1,966
Lev. dict. Parole 18,662,243 662,510
Lev. freq. Parole 18,662,243 662,510
SMT lm Parole 18,662,243 662,510
Table 2: Language resources for German.
4.3 Hungarian
For training, tuning and evaluation in the Hungar-
ian experiments, we use a collection of manually
normalised codices from the Hungarian Gener-
ative Diachronic Syntax project, HGDS (Simon,
To appear), in total 11 codices from the time pe-
riod 1440?1541. The Szeged Treebank is used
as the single modern language resource both for
the Levenshtein-based and for the SMT-based ap-
proach (Csendes et al., 2005). Table 3 presents
in more detail the data sets used in the Hungarian
experiments.
Resource Data Tokens Types
Training HGDS 137,669 45,529
Tuning HGDS 17 181 8 827
Evaluation HGDS 17,214 8,798
Lev. dict. Szeged 1,257,089 144,248
Lev. freq. Szeged 1,257,089 144,248
SMT lm Szeged 1,257,089 144,248
Table 3: Language resources for Hungarian.
36
4.4 Icelandic
For training, tuning and evaluation in the Ice-
landic experiments, we use a manually normalised
subset of the Icelandic Parsed Historical Cor-
pus (IcePaHC), a manually tagged and parsed di-
achronic corpus of texts from the time period
1150?2008 (R?gnvaldsson et al., 2012). This sub-
set contains four texts from the 15th century: three
sagas (Vilhj?lm?s saga, Jarlmann?s saga, and Ec-
tor?s saga) and one narrative-religious text (Mi?al-
da?vint?ri). As a dictionary for Levenshtein cal-
culations we use a combination of Beygingar-
l?sing ?slensks N?t?mam?ls, B?N (a database of
modern Icelandic inflectional forms (Bjarnad?t-
tir, 2012)), and all tokens occurring 100 times or
more in the Tagged Icelandic Corpus of Contem-
porary Icelandic texts, M?M (Helgad?ttir et al.,
2012).
2
The frequency-based choice of a final nor-
malisation candidate in the Levenshtein approach,
as well as the training of a language model in the
SMT approach, are done on all tokens occurring
100 times or more in the M?M corpus. Table 4
presents in more detail the data sets used in the
Icelandic experiments.
Resource Data Tokens Types
Training IcePaHC 52,440 9,748
Tuning IcePaHC 6,443 2,270
Evaluation IcePaHC 6,384 2,244
Lev. dict. B?N+M?M 27,224,798 2,820,623
Lev. freq. M?M 21,339,384 9,461
SMT lm M?M 21,339,384 9,461
Table 4: Language resources for Icelandic.
4.5 Swedish
For training, tuning and evaluation in the Swedish
experiments, we use balanced subsets of the Gen-
der and Work corpus (GaW) of court records and
church documents from the time period 1527?
1812 (?gren et al., 2011). As a dictionary for Lev-
enshtein calculations we use SALDO, a lexical re-
source developed for present-day written Swedish
(Borin et al., 2008). For frequency-based choice of
a final normalisation candidate, we use the Stock-
holm Ume? corpus (SUC) of text representative of
the Swedish language in the 1990s (Ejerhed and
K?llgren, 1997). The SUC corpus is also used
2
The B?N database alone is not sufficient for Levenshtein
calculations, since it only contains content words.
to train a language model in the SMT-based ap-
proach. Table 5 presents in more detail the data
sets used in the Swedish experiments.
Resource Data Tokens Types
Training GaW 28,237 7,925
Tuning GaW 2,590 1,260
Evaluation GaW 33,544 8,859
Lev. dict. SALDO 1,110,731 723,138
Lev. freq. SUC 1,166,593 97,670
SMT lm SUC 1,166,593 97,670
Table 5: Language resources for Swedish.
5 Results
Table 6 presents the results for different languages
and normalisation methods, given in terms of nor-
malisation accuracy, i.e. the percentage of tokens
in the normalised text with a spelling identical
to the manually modernised gold standard, and
character error rate (CER), providing a more pre-
cise estimation of the similarity between the nor-
malised token and the gold standard version at a
character level. Table 7 summarises the results in
terms of Precision (Pre), Recall (Rec) and F-score
(F) for the filtering approach, the Levenshtein-
based approach (with and without filtering), and
the best-performing SMT-based approach.
For the Levenshtein experiments, we have used
context-sensitive weights, as described in Section
3.2. In the SMT approach, we run GIZA with
standard word alignment models for character un-
igrams (un) and bigrams (bi). The m2m aligner is
implemented with single character edit operations
(1:1) and multi-character operations (2:2).
The baseline case shows the proportion of to-
kens in the original, historical text that already
have a spelling identical to the modern gold stan-
dard spelling. In the Hungarian text, only 17.1%
of the historial tokens have a modern spelling,
with a character error rate of 0.85. For German
on the other hand, accuracy is as high as 84.4%,
with a character error rate of only 0.16. At a
first glance, the historical spelling in the Hungar-
ian corpus appears to be very similar to the mod-
ern spelling. A closer look however reveals re-
current differences involving single letter substi-
tutions and/or the use of accents, as for fiayval?
fiaival, m?eghalanac?meghal?nak and hazaba?
h?z?ba.
37
English German Hungarian Icelandic Swedish
Acc CER Acc CER Acc CER Acc CER Acc CER
baseline 75.8 0.26 84.4 0.16 17.1 0.85 50.5 0.51 64.6 0.36
filter 91.7 0.20 94.6 0.26 75.0 0.30 81.7 0.25 86.2 0.27
Lev 82.9 0.19 87.3 0.13 31.7 0.71 67.3 0.35 79.4 0.22
Lev+filter 92.9 0.09 95.1 0.06 76.4 0.35 84.6 0.19 90.8 0.10
giza un 94.3 0.07 96.6 0.04 79.9 0.21 71.8 0.30 92.9 0.07
giza bi 92.4 0.09 95.5 0.05 80.1 0.21 71.5 0.30 92.5 0.08
m2m 1:1 un 90.6 0.11 96.0 0.04 79.4 0.21 71.2 0.31 92.3 0.08
m2m 1:1 bi 88.0 0.14 95.6 0.05 79.5 0.21 71.5 0.30 92.2 0.08
m2m 2:2 un 90.7 0.11 96.4 0.04 77.3 0.24 71.0 0.31 91.3 0.09
m2m 2:2 bi 87.5 0.14 95.5 0.05 79.1 0.22 71.4 0.31 92.1 0.08
Table 6: Normalisation results given in accuracy (Acc) and character error rate (CER).
English German Hungarian Icelandic Swedish
Pre Rec F Pre Rec F Pre Rec F Pre Rec F Pre Rec F
filter 93.6 97.8 95.7 95.0 99.6 97.2 77.4 96.0 85.7 89.3 90.6 89.9 87.5 98.3 92.6
Lev 92.7 88.6 90.7 91.0 95.6 93.2 68.0 37.3 48.2 85.4 76.1 80.5 90.5 86.6 88.5
Lev+filter 97.4 95.2 96.3 97.3 97.7 97.5 96.2 78.8 86.7 95.6 88.0 91.7 96.6 93.8 95.2
SMT 98.2 95.9 97.0 98.7 97.9 98.3 98.3 81.3 89.0 82.0 85.2 83.6 98.6 94.1 96.3
Table 7: Normalisation results given in precision (Pre), recall (Rec) and F-score (F).
The Icelandic corpus also has a relatively low
number of tokens with a spelling identical to the
modern spelling. Even though the Hungarian and
Icelandic texts are older than the English, German,
and Swedish texts, the rather low proportion of to-
kens with a modern spelling in the Icelandic cor-
pus is rather surprising, since the Icelandic lan-
guage is generally seen as conservative in spelling.
A closer inspection of the Icelandic corpus reveals
the same kind of subtle single letter divergences
and differences in the use of accents as for Hun-
garian, e.g. ad? a? and hun? h?n.
The simplistic filtering approach (filter), re-
lying solely on previously seen tokens in the
training data, captures frequently occurring word
forms and works surprisingly well, improving
normalisation accuracy by up to 63 percentage
units. The Levenshtein-based approach (Lev)
in its basic version, with no parallel training
data available, also improves normalisation ac-
curacy as compared to the baseline. However,
for all languages, the simplistic filtering approach
yields significantly higher normalisation accuracy
than the more sophisticated Levenshtein-based ap-
proach does. This could be partly explained by
the fact that frequently occurring word forms have
a high chance of being captured by the filter-
ing approach, whereas the Levenshtein-based ap-
proach runs the risk of consistently normalising
high-frequent word forms incorrectly. For exam-
ple, in the English Levenshtein normalisation pro-
cess, the high-frequent word form stonde has con-
sistently been normalised to stone instead of stand,
due to the larger edit distance between stonde and
stand. The even more common word form ben,
which should optimally be normalised to been, has
consistently been left unchanged as ben, since the
BNC corpus, which is used for dictionary lookup
in the English setup, contains the proper name
Ben. The issue of proper names would not be
a problem if a modern dictionary were used for
Levenshtein comparisons instead of a corpus, or if
casing was taken into account in the Levenshtein
comparisons. There would however still be cases
left like stonde being incorrectly normalised to
stone as described above, which would be disad-
vantageous to the Levenshtein-based method. The
low recall figures, especially for Hungarian, also
indicates that there may be old word forms that
are not present in modern dictionaries and thus are
out of reach for the Levenshtein-based method, as
for the previously discussed Hungarian word form
meghal?nak.
In the Lev+filter setting, the filter is used as a
first step in the normalisation process. Only to-
kens that could not be matched through dictio-
nary lookup based on the training corpus are nor-
malised by Levenshtein comparisons. The idea is
38
that combining these two techniques would per-
form better than one approach only, since high-
frequent word forms are consistently normalised
correctly by the filter, whereas previously unseen
tokens are handled through Levenshtein compar-
isons. This combination does indeed perform bet-
ter for all languages, and for Icelandic this is by far
the most successful normalisation method of all.
For the SMT-based approach, it is interesting to
note that the simple unigram models in many cases
perform better than the more advanced bigram and
multi-character models. We also tried adding the
filter to the SMT approach, so that only tokens that
could not be matched through dictionary lookup
based on the training corpus, would be considered
for normalisation by the SMT model. This did
however not have a positive effect on normalisa-
tion accuracy, probably because the training data
has already been taken care of by the SMT model,
so adding the filter only led to redundant informa-
tion and incorrect matches, deteriorating the re-
sults. For four out of five languages, the GIZA un-
igram setting yields the highest normalisation ac-
curacy of all SMT models evaluated. For Hungar-
ian, the GIZA bigram modell performs marginally
better than the unigram model.
From the presented results, it is not obvious
which normalisation approach to choose for a new
language. For Icelandic, the Levenshtein-based
approach combined with the filter leads to the
highest normalisation accuracy. For the rest of
the languages, the SMT-based approach with the
GIZA unigram or bigram setting gives the best re-
sults. Generally, the Levenshtein-based method
could be used for languages lacking access to an-
notated historical data with information on both
original and modernised spelling. If, on the other
hand, such data is available, the filtering approach,
or the combination of filtering and Levenshtein
calculations, would be likely to improve normal-
isation accuracy. Moreover, the effort of training
a character-based SMT system for normalisation
would be likely to further improve the results.
It would be interesting to also compare the re-
sults between the languages, in a language evo-
lution perspective. This is however not feasible
within the scope of this study, due to the differ-
ences in corpus size, genres and covered time pe-
riods, as discussed in Section 4.
6 Conclusion
We have performed a multilingual evaluation
of three approaches to spelling modernisation
of historical text: a simplistic filtering model,
a Levenshtein-based approach and a character-
based statistical machine translation method. The
results were evaluated on historical texts from
five languages: English, German, Hungarian, Ice-
landic and Swedish. We see that all approaches are
successful in increasing the proportion of tokens in
the historical text with a spelling identical to the
modernised gold standard spelling. We conclude
that the proposed methods have the potential of
enabling us to use modern NLP tools for analysing
historical texts. Which approach to choose is not
clear, since the results vary for the different lan-
guages in our study, even though the SMT-based
approach generally works best. If no historical
training data is available, the Levenshtein-based
approach could still be used, since only a mod-
ern dictionary is required for edit distance com-
parisons. If there is a corpus of token pairs with
historical and modern spelling available, training
an SMT model could however result in improved
normalisation accuracy. Since the SMT models
are character-based, only a rather small amount of
training data is needed for this task, as discussed
in Section 3.3.
We believe that our results would be of interest
to several research fields. From a language evolu-
tion perspective, future research would include a
thorough investigation of why certain approaches
work better for some languages but not for other
languages, and what the results would be if the
data sets for the different languages were more
similar with regard to time period, size, genre etc.
The latter could however be problematic, due to
data sparseness. For historians interested in us-
ing modern NLP tools for analysing historical text,
an extrinsic evaluation is called for, comparing
the results of tagging and parsing using modern
tools, before and after spelling normalisation. Fi-
nally, the proposed methods all treat words in iso-
lation in the normalisation process. From a lan-
guage technology perspective, it would be inter-
esting to also explore ways of handling grammat-
ical and structural differences between historical
and modern language as part of the normalisation
process. This would be particularly interesting
when evaluating subsequent tagging and parsing
performance.
39
References
Maria ?gren, Rosemarie Fiebranz, Erik Lindberg, and
Jonas Lindstr?m. 2011. Making verbs count. The
research project ?Gender and Work? and its method-
ology. Scandinavian Economic History Review,
59(3):271?291. Forthcoming.
Alistair Baron and Paul Rayson. 2008. Vard2: A tool
for dealing with spelling variation in historical cor-
pora. In Postgraduate Conference in Corpus Lin-
guistics, Aston University, Birmingham.
Krist?n Bjarnad?ttir. 2012. The Database of Modern
Icelandic Inflection. In AfLaT2012/SALTMIL joint
workshop on Language technology for normalisa-
tion of less-resourced languages, Istanbul, May.
Alan W. Black and Paul Taylor. 1997. Festival speech
synthesis system: system documentation. Technical
report, University of Edinburgh, Centre for Speech
Technology Research.
Marcel Bollmann, Florian Petran, and Stefanie Dipper.
2011. Rule-based normalization of historical texts.
In Proceedings of the Workshop on Language Tech-
nologies for Digital Humanities and Cultural Her-
itage, pages 34?42, Hissar, Bulgaria.
Marcel Bollmann. 2013. POS tagging for historical
texts with sparse training data. In Proceedings of
the 7th Linguistic Annotation Workshop & Interop-
erability with Discourse, pages 11?18, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Lars Borin, Markus Forsberg, and Lennart L?nngren.
2008. Saldo 1.0 (svenskt associationslexikon ver-
sion 2). Spr?kbanken, University of Gothenburg.
C. Csendes, J. Csirik, T. Gyim?thy, and A. Kocsor.
2005. The Szeged Treebank. In Proceedings of
the Eighth International Conference on Text, Speech
and Dialogue (TSD 2005), Karlovy Vary, Czech Re-
public.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus. Version 1.0. Produced by Depart-
ment of Linguistics, Ume? University and Depart-
ment of Linguistics, Stockholm University. ISBN
91-7191-348-3.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twitter.
In Association for Computational Linguistics, edi-
tor, Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics, pages
368?378, Portland, Oregon, USA, June.
Sigr?n Helgad?ttir, ?sta Svavarsd?ttir, Eir?kur R?gn-
valdsson, Krist?n Bjarnad?ttir, and Hrafn Loftsson.
2012. The Tagged Icelandic Corpus (M?M). In
Proceedings of the Workshop on Language Tech-
nology for Normalisation of Less-Resourced Lan-
guages, pages 67?72.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden markov models to letter-to-phoneme
conversion. In Proceedings of the Annual Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL-HLT
2007), pages 372?379, Rochester, NY, April.
Bryan Jurish. 2008. Finding canonical forms
for historical German text. In Angelika Storrer,
Alexander Geyken, Alexander Siebert, and Kay-
Michael W?rzner, editors, Text Resources and Lex-
ical Knowledge: Selected Papers from the 9th Con-
ference on Natural Language Processing (KON-
VENS 2008), pages 27?37. Mouton de Gruyter,
Berlin.
Manfred Markus, 1999. Manual of ICAMET (Inns-
bruck Computer Archive of Machine-Readable En-
glish Texts). Leopold-Franzens-Universit?t Inns-
bruck.
David Matthews. 2007. Machine transliteration of
proper names. Master?s thesis, School of Informat-
ics.
Preslav Nakov and J?rg Tiedemann. 2012. Combin-
ing word-level and character-level models for ma-
chine translation between closely-related languages.
In Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 301?305, Jeju Island, Korea,
July. Association for Computational Linguistics.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. pages 440?447, Hongkong, China,
October.
Eva Pettersson, Be?ta Megyesi, and Tiedemann J?rg.
2013a. An SMT approach to automatic annotation
of historical text. In Proceedings of the NoDaLiDa
2013 workshop on Computational Historical Lin-
guistics, May.
Eva Pettersson, Be?ta Megyesi, and Joakim Nivre.
2013b. Normalisation of historical text using
context-sensitive weighted Levenshtein distance and
compound splitting. In Proceedings of the 19th
Nordic Conference on Computational Linguistics
(NoDaLiDa), May.
Paul Rayson, Dawn Archer, and Nicholas Smith. 2005.
VARD versus Word ? A comparison of the UCREL
variant detector and modern spell checkers on En-
glish historical corpora. In Proceedings from the
Corpus Linguistics Conference Series on-line e-
journal, volume 1, Birmingham, UK, July.
Eir?kur R?gnvaldsson, Anton Karl Ingason, Einar Freyr
Sigurdsson, and Joel Wallenberg. 2012. The Ice-
landic Parsed Historical Corpus (IcePaHC). In Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Is-
tanbul, Turkey, May. European Language Resources
Association (ELRA).
40
Silke Scheible, Richard J. Whitt, Martin Durrell, and
Paul Bennett. 2011. A Gold Standard Corpus of
Early Modern German. In Proceedings of the 5th
Linguistic Annotation Workshop, pages 124?128,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Eszter Simon. To appear. Corpus building from Old
Hungarian codices. In Katalin ?. Kiss, editor, The
Evolution of Functional Left Peripheries in Hungar-
ian Syntax. Oxford University Press.
Wolfgang Teubert (ed.). 2003. German Parole Corpus.
Electronic resource, Oxford Text Archive.
David Vilar, Jan-Thorsten Peter, and Hermann Ney.
2007. Can we translate letters? In Proceedings of
the Second Workshop on Statistical Machine Trans-
lation, pages 33?39, Prague, Czech Republic, June.
Association for Computational Linguistics.
41
