Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 31?36,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Pronunciation Modeling in Spelling Correction
for Writers of English as a Foreign Language
Adriane Boyd
Department of Linguistics
The Ohio State University
1712 Neil Avenue
Columbus, Ohio 43210, USA
adriane@ling.osu.edu
Abstract
We propose a method for modeling pronunci-
ation variation in the context of spell checking
for non-native writers of English. Spell check-
ers, typically developed for native speakers,
fail to address many of the types of spelling
errors peculiar to non-native speakers, espe-
cially those errors influenced by differences in
phonology. Our model of pronunciation varia-
tion is used to extend a pronouncing dictionary
for use in the spelling correction algorithm
developed by Toutanova and Moore (2002),
which includes models for both orthography
and pronunciation. The pronunciation vari-
ation modeling is shown to improve perfor-
mance for misspellings produced by Japanese
writers of English.
1 Introduction
Spell checkers identify misspellings, select appro-
priate words as suggested corrections, and rank the
suggested corrections so that the likely intended
word is high in the list. Since traditional spell
checkers have been developed with competent na-
tive speakers as the target users, they do not appro-
priately address many types of errors made by non-
native writers and they often fail to suggest the ap-
propriate corrections. Non-native writers of English
struggle with many of the same idiosyncrasies of En-
glish spelling that cause difficulty for native speak-
ers, but differences between English phonology and
the phonology of their native language lead to types
of spelling errors not anticipated by traditional spell
checkers (Okada, 2004; Mitton and Okada, 2007).
Okada (2004) and Mitton and Okada (2007) in-
vestigate spelling errors made by Japanese writers
of English as a foreign language (JWEFL). Okada
(2004) identifies two main sources of errors for
JWEFL: differences between English and Japanese
phonology and differences between the English al-
phabet and the Japanese romazi writing system,
which uses a subset of English letters. Phonolog-
ical differences result in number of distinctions in
English that are not present in Japanese and romazi
causes difficulties for JWEFL because the Latin let-
ters correspond to very different sounds in Japanese.
We propose a method for creating a model of
pronunciation variation from a phonetically untran-
scribed corpus of read speech recorded by non-
native speakers. The pronunciation variation model
is used to generate multiple pronunciations for each
canonical pronunciation in a pronouncing dictionary
and these variations are used in the spelling correc-
tion approach developed by Toutanova and Moore
(2002), which uses statistical models of spelling er-
rors that consider both orthography and pronuncia-
tion. Several conventions are used throughout this
paper: a word is a sequence of characters from the
given alphabet found in the word list. A word list
is a list of words. A misspelling, marked with *, is
a sequence of characters not found in the word list.
A candidate correction is a word from the word list
proposed as a potential correction.
2 Background
Research in spell checking (see Kukich, 1992, for
a survey of spell checking research) has focused
on three main problems: non-word error detec-
tion, isolated-word error correction, and context-
dependent word correction. We focus on the first
two tasks. A non-word is a sequence of letters that
31
is not a possible word in the language in any con-
text, e.g., English *thier. Once a sequence of let-
ters has been determined to be a non-word, isolated-
word error correction is the process of determining
the appropriate word to substitute for the non-word.
Given a sequence of letters, there are thus two
main subtasks: 1) determine whether this is a non-
word, 2) if so, select and rank candidate words as
potential corrections to present to the writer. The
first subtask can be accomplished by searching for
the sequence of letters in a word list. The second
subtask can be stated as follows (Brill and Moore,
2000): Given an alphabet ?, a word list D of strings
? ??, and a string r /? D and ? ??, find w ? D
such that w is the most likely correction. Minimum
edit distance is used to select the most likely candi-
date corrections. The general idea is that a minimum
number of edit operations such as insertion and sub-
stitution are needed to convert the misspelling into a
word. Words requiring the smallest numbers of edit
operations are selected as the candidate corrections.
2.1 Edit Operations and Edit Weights
In recent spelling correction approaches, edit op-
erations have been extended beyond single charac-
ter edits and the methods for calculating edit opera-
tion weights have become more sophisticated. The
spelling error model proposed by Brill and Moore
(2000) allows generic string edit operations up to a
certain length. Each edit operation also has an asso-
ciated probability that improves the ranking of can-
didate corrections by modeling how likely particu-
lar edits are. Brill and Moore (2000) estimate the
probability of each edit from a corpus of spelling er-
rors. Toutanova and Moore (2002) extend Brill and
Moore (2000) to consider edits over both letter se-
quences and sequences of phones in the pronuncia-
tions of the word and misspelling. They show that
including pronunciation information improves per-
formance as compared to Brill and Moore (2000).
2.2 Noisy Channel Spelling Correction
The spelling correction models from Brill and
Moore (2000) and Toutanova and Moore (2002) use
the noisy channel model approach to determine the
types and weights of edit operations. The idea be-
hind this approach is that a writer starts out with the
intended word w in mind, but as it is being writ-
ten the word passes through a noisy channel result-
ing in the observed non-word r. In order to de-
termine how likely a candidate correction is, the
spelling correction model determines the probabil-
ity that the word w was the intended word given the
misspelling r: P (w|r). To find the best correction,
the wordw is found for whichP (w|r) is maximized:
argmaxw P (w|r). Applying Bayes? Rule and dis-
carding the normalizing constant P (r) gives the cor-
rection model:
argmaxw P (w|r) = argmaxw P (w)P (r|w)
P (w), how probable the word w is overall, and
P (r|w), how probable it is for a writer intending to
write w to output r, can be estimated from corpora
containing misspellings. In the following experi-
ments, P (w) is assumed be equal for all words to fo-
cus this work on estimating the error model P (r|w)
for JWEFL.1
Brill and Moore (2000) allow all edit operations
? ? ? where ? is the alphabet and ?, ? ? ??, with
a constraint on the length of ? and ?. In order to
consider all ways that a word w may generate r with
the possibility that any, possibly empty, substring ?
of w becomes any, possibly empty, substring ? of
r, it is necessary to consider all ways that w and r
may be partitioned into substrings. This error model
over letters, called PL, is approximated by Brill and
Moore (2000) as shown in Figure 1 by considering
only the pair of partitions of w and r with the max-
imum product of the probabilities of individual sub-
stitutions. Part(w) is all possible partitions of w,
|R| is number of segments in a particular partition,
and Ri is the ith segment of the partition.
The parameters for PL(r|w) are estimated from
a corpus of pairs of misspellings and target words.
The method, which is described in detail in Brill and
Moore (2000), involves aligning the letters in pairs
of words and misspellings, expanding each align-
ment with up to N neighboring alignments, and cal-
culating the probability of each ? ? ? alignment.
Since we will be using a training corpus that con-
sists solely of pairs of misspellings and words (see
section 3), we would have lower probabilities for
1Of course, P (w) is not equal for all words, but it is not
possible to estimate it from the available training corpus, the
Atsuo-Henry Corpus (Okada, 2004), because it contains only
pairs of words and misspellings for around 1,000 target words.
32
PL(r|w) ? maxR?Part(r),T?Part(w)
|R|?
i=1
P (Ri ? Ti)
PPHL(r|w) ?
?
pronw
1
|pronw|
max
pronr
PPH(pronw|pronr)P (pronr|r)
Figure 1: Approximations of PL from Brill and Moore (2000) and PPHL from Toutanova and Moore (2002)
? ? ? than would be found in a corpus with mis-
spellings observed in context with correct words. To
compensate, we approximate P (? ? ?) by assign-
ing it a minimum probability m:
P (? ? ?) =
{
m+ (1?m) count(???)count(?) if ? = ?
(1?m) count(???)count(?) if ? 6= ?
2.2.1 Extending to Pronunciation
Toutanova and Moore (2002) describe an extension
to Brill and Moore (2000) where the same noisy
channel error model is used to model phone se-
quences instead of letter sequences. Instead of the
word w and the non-word r, the error model con-
siders the pronunciation of the non-word r, pronr,
and the pronunciation of the word w, pronw. The
error model over phone sequences, called PPH , is
just like PL shown in Figure 1 except that r and w
are replaced with their pronunciations. The model is
trained like PL using alignments between phones.
Since a spelling correction model needs to rank
candidate words rather than candidate pronuncia-
tions, Toutanova and Moore (2002) derive an er-
ror model that determines the probability that a
word w was spelled as the non-word r based on
their pronunciations. Their approximation of this
model, called PPHL, is also shown in Figure 1.
PPH(pronw|pronr) is the phone error model de-
scribed above and P (pronr|r) is provided by the
letter-to-phone model described below.
2.3 Letter-To-Phone Model
A letter-to-phone (LTP) model is needed to predict
the pronunciation of misspellings for PPHL, since
they are not found in a pronouncing dictionary. Like
Toutanova and Moore (2002), we use the n-gram
LTP model from Fisher (1999) to predict these pro-
nunciations. The n-gram LTP model predicts the
pronunciation of each letter in a word considering
up to four letters of context to the left and right. The
most specific context found for each letter and its
context in the training data is used to predict the pro-
nunciation of a word. We extended the prediction
step to consider the most probable phone for the top
M most specific contexts.
We implemented the LTP algorithm and trained
and evaluated it using pronunciations from CMU-
DICT. A training corpus was created by pairing the
words from the size 70 CMUDICT-filtered SCOWL
word list (see section 3) with their pronunciations.
This list of approximately 62,000 words was split
into a training set with 80% of entries and a test set
with the remaining 20%. We found that the best per-
formance is seen when M = 3, giving 95.5% phone
accuracy and 74.9% word accuracy.
2.4 Calculating Final Scores
For a misspelling r and a candidate correction w,
the letter model PL gives the probability that w was
written as r due to the noisy channel taking into ac-
count only the orthography. PPH does the same for
the pronunciations of r and w, giving the probability
that pronw was output was pronr. The pronuncia-
tion model PPHL relates the pronunciations mod-
eled by PPH to the orthography in order to give the
probability that r was written as w based on pronun-
ciation. PL and PPHL are then combined as follows
to calculate a score for each candidate correction.
SCMB(r|w) = logPL(r|w) + ?logPPHL(r|w)
3 Resources and Data Preparation
Our spelling correction approach, which includes
error models for both orthography and pronuncia-
tion (see section 2.2) and which considers pronun-
ciation variation for JWEFL requires a number of
resources: 1) spoken corpora of American English
(TIMIT, TIMIT 1991) and Japanese English (ERJ,
see below) are used to model pronunciation vari-
ation, 2) a pronunciation dictionary (CMUDICT,
CMUDICT 1998) provides American English pro-
nunciations for the target words, 3) a corpus of
33
spelling errors made by JWEFL (Atsuo-Henry Cor-
pus, see below) is used to train spelling error mod-
els and test the spell checker?s performance, and 4)
Spell Checker Oriented Word Lists (SCOWL, see
below) are adapted for our use.
The English Read by Japanese Corpus (Mine-
matsu et al, 2002) consists of 70,000 prompts con-
taining phonemic and prosodic cues recorded by 200
native Japanese speakers with varying English com-
petence. See Minematsu et al (2002) for details on
the construction of the corpus.
The Atsuo-Henry Corpus (Okada, 2004) in-
cludes a corpus of spelling errors made by JWEFL
that consists of a collection of spelling errors from
multiple corpora.2 For use with our spell checker,
the corpus has been cleaned up and modified to fit
our task, resulting in 4,769 unique misspellings of
1,046 target words. The data is divided into training
(80%), development (10%), and test (10%) sets.
For our word lists, we use adapted versions of the
Spell Checker Oriented Word Lists.3 The size 50
word lists are used in order to create a general pur-
pose word list that covers all the target words from
the Atsuo-Henry Corpus. Since the target pronun-
ciation of each item is needed for the pronunciation
model, the word list was filtered to remove words
whose pronunciation is not in CMUDICT. After fil-
tering, the word list contains 54,001 words.
4 Method
This section presents our method for modeling pro-
nunciation variation from a phonetically untran-
scribed corpus of read speech. The pronunciation-
based spelling correction approach developed in
Toutanova and Moore (2002) requires a list of pos-
sible pronunciations in order to compare the pro-
nunciation of the misspelling to the pronunciation
of correct words. To account for target pronuncia-
tions specific to Japanese speakers, we observe the
pronunciation variation in the ERJ and generate ad-
ditional pronunciations for each word in the word
list. Since the ERJ is not transcribed, we begin
by adapting a recognizer trained on native English
2Some of the spelling errors come from an elicitation task,
so the distribution of target words is not representative of typi-
cal JWEFL productions, e.g., the corpus contains 102 different
misspellings of albatross.
3SCOWL is available at http://wordlist.sourceforge.net.
speech. First, the ERJ is recognized using a mono-
phone recognizer trained on TIMIT. Next, the most
frequent variations between the canonical and rec-
ognized pronunciations are used to adapt the recog-
nizer. The adapted recognizer is then used to rec-
ognize the ERJ in forced alignment with the canon-
ical pronunciations. Finally, the variations from the
previous step are used to create models of pronun-
ciation variation for each phone, which are used to
generate multiple pronunciations for each word.
4.1 Initial Recognizer
A monophone speech recognizer was trained on all
TIMIT data using the HiddenMarkovModel Toolkit
(HTK).4 This recognizer is used to generate a phone
string for each utterance in the ERJ. Each recog-
nized phone string is then aligned with the canon-
ical pronunciation provided to the speakers. Correct
alignments and substitutions are considered with no
context and insertions are conditioned on the previ-
ous phone. Due to restrictions in HTK, deletions are
currently ignored.
The frequency of phone alignments for all utter-
ances in the ERJ are calculated. Because of the low
phone accuracy of monophone recognizers, espe-
cially on non-native speech, alignments are observed
between nearly all pairs of phones. In order to focus
on the most frequent alignments common to multi-
ple speakers and utterances, any alignment observed
less than 20% as often as the most frequent align-
ment for that canonical phone is discarded, which re-
sults in an average of three variants of each phone.5
4.2 Adapting the Recognizer
Now that we have probability distributions over ob-
served phones, the HMMs trained on TIMIT are
modified as follows to allow the observed varia-
tion. To allow, for instance, variation between p
and th, the states for th from the original recog-
nizer are inserted into the model for p as a separate
path. The resulting phone model is shown in Fig-
ure 2. The transition probabilities into the first states
4HTK is available at http://htk.eng.cam.ac.uk.
5There are 119 variants of 39 phones. The cutoff of 20%
was chosen to allow a few variations for most phones. A small
number of phones have no variants (e.g., iy, w) while a few
have over nine variants (e.g., ah, l). It is not surprising that
phones that are well-known to be difficult for Japanese speakers
(cf. Minematsu et al, 2002) are the ones with the most variation.
34
. 6
. 4 th-1 th-2 th-3
p-1 p-2 p-3
Figure 2: Adapted phone model for p accounting for vari-
ation between p and th
of the phones come from the probability distribution
observed in the initial recognition step. The transi-
tion probabilities between the three states for each
variant phone remain unchanged. All HMMs are
adapted in this manner using the probability distri-
butions from the initial recognition step.
The adapted HMMs are used to recognize the ERJ
Corpus for a second time, this time in forced align-
ment with the canonical pronunciations. The state
transitions indicate which variant of each phone was
recognized and the correspondences between the
canonical phones and recognized phones are used
to generate a new probability distribution over ob-
served phones for each canonical phone. These are
used to find the most probable pronunciation varia-
tions for a native-speaker pronouncing dictionary.
4.3 Generating Pronunciations
The observed phone variation is used to generate
multiple pronunciations for each pronunciation in
the word list. The OpenFst Library6 is used to find
the most probable pronunciations in each case. First,
FSTs are created for each phone using the proba-
bility distributions from the previous section. Next,
an FST is created for the entire word by concate-
nating the FSTs for the pronunciation from CMU-
DICT. The pronunciations corresponding to the best
n paths through the FST and the original canon-
ical pronunciation become possible pronunciations
in the extended pronouncing dictionary. The size 50
word list contains 54,001 words and when expanded
to contain the top five variations of each pronuncia-
tion, there are 255,827 unique pronunciations.
5 Results
In order to evaluate the effect of pronunciation vari-
ation in Toutanova and Moore (2002)?s spelling cor-
rection approach, we compare the performance of
the pronunciation model and the combined model
6OpenFst is available at http://www.openfst.org/.
with and without pronunciation variation.
We implemented the letter and pronunciation
spelling correction models as described in sec-
tion 2.2. The letter error model PL and the phone
error model PPH are trained on the training set.
The development set is used to tune the parameters
introduced in previous sections.7 In order to rank
the words as candidate corrections for a misspelling
r, PL(r|w) and PPHL(r|w) are calculated for each
word in the word list using the algorithm described
in Brill and Moore (2000). Finally, PL and PPHL
are combined using SCMB to rank each word.
5.1 Baseline
The open source spell checker GNU Aspell8 is used
to determine the baseline performance of a tradi-
tional spell checker using the same word list. An
Aspell dictionary was created with the word list de-
scribed in section 3. Aspell?s performance is shown
in Table 1. The 1-Best performance is the percent-
age of test items for which the target word was the
first candidate correction, 2-Best is the percentage
for which the target was in the top two, etc.
5.2 Evaluation of Pronunciation Variation
The effect of introducing pronunciation variation us-
ing the method described in section 4 can be eval-
uated by examining the performance on the test set
for PPHL with and without the additional variations.
The results in Table 1 show that the addition of pro-
nunciation variations does indeed improve the per-
formance of PPHL across the board. The 1-Best,
3-Best, and 4-Best cases for PPHL with variation
show significant improvement (p<0.05) over PPHL
without variation.
5.3 Evaluation of the Combined Model
We evaluated the effect of including pronunciation
variation in the combined model by comparing the
performance of the combined model with and with-
out pronunciation variation, see results in Table 1.
Despite the improvements seen in PPHL with pro-
nunciation variation, there are no significant differ-
ences between the results for the combined model
with and without variation. The combined model
7The values are: N = 3 for the letter model, N = 4 for the
phone model, m = 80%, and ? = 0.15 in SCMB .
8GNU Aspell is available at http://aspell.net.
35
Model 1-Best 2-Best 3-Best 4-Best 5-Best 6-Best
Aspell 44.1 54.0 64.1 68.3 70.0 72.5
Letter (L) 64.7 74.6 79.6 83.2 84.0 85.3
Pronunciation (PHL) without Pron. Var. 47.9 60.7 67.9 70.8 75.0 77.3
Pronunciation (PHL) with Pron. Var. 50.6 62.2 70.4 73.1 76.7 78.2
Combined (CMB) without Pron. Var. 64.9 75.2 78.6 81.1 82.6 83.2
Combined (CMB) with Pron. Var. 65.5 75.0 78.4 80.7 82.6 84.0
Table 1: Percentage of Correct Suggestions on the Atsuo-Henry Corpus Test Set for All Models
Rank Aspell L PHL CMB
1 enemy enemy any enemy
2 envy envy Emmy envy
3 energy money Ne any
4 eye emery gunny deny
5 teeny deny ebony money
6 Ne any anything emery
7 deny nay senna nay
8 any ivy journey ivy
Table 2: Misspelling *eney, Intended Word any
with variation is also not significantly different from
the letter model PL except for the drop in the 4-Best
case.
To illustrate the performance of each model, the
ranked lists in Table 2 give an example of the can-
didate corrections for the misspelling of any as
*eney. Aspell preserves the initial letter of the mis-
spelling and vowels in many of its candidates. PL?s
top candidates also overlap a great deal in orthogra-
phy, but there is more initial letter and vowel varia-
tion. As we would predict, PPHL ranks any as the
top correction, but some of the lower-ranked candi-
dates for PPHL differ greatly in length.
5.4 Summary of Results
The noisy channel spelling correction approach de-
veloped by Brill and Moore (2000) and Toutanova
and Moore (2002) appears well-suited for writers
of English as a foreign language. The letter and
combined models outperform the traditional spell
checker Aspell by a wide margin. Although in-
cluding pronunciation variation does not improve
the combined model, it leads to significant improve-
ments in the pronunciation-based model PPHL.
6 Conclusion
We have presented a method for modeling pronun-
ciation variation from a phonetically untranscribed
corpus of read non-native speech by adapting a
monophone recognizer initially trained on native
speech. This model allows a native pronouncing
dictionary to be extended to include non-native pro-
nunciation variations. We incorporated a pronounc-
ing dictionary extended for Japanese writers of En-
glish into the spelling correction model developed
by Toutanova and Moore (2002), which combines
orthography-based and pronunciation-based mod-
els. Although the extended pronunciation dictio-
nary does not lead to improvement in the combined
model, it does leads to significant improvement in
the pronunciation-based model.
Acknowledgments
I would like to thank Eric Fosler-Lussier, the Ohio
State computational linguistics discussion group,
and anonymous reviewers for their helpful feedback.
References
Brill, Eric and Robert C. Moore (2000). An Improved
Error Model for Noisy Channel Spelling Correction.
In Proceedings of ACL 2000.
CMUDICT (1998). CMU Pronouncing Dictionary
version 0.6. http://www.speech.cs.cmu.edu/cgi-bin/
cmudict.
Fisher, Willam (1999). A statistical text-to-phone func-
tion using ngrams and rules. In Proceedings of ICASSP
1999.
Kukich, Karen (1992). Technique for automatically cor-
recting words in text. ACM Computing Surveys 24(4).
Minematsu, N., Y. Tomiyama, K. Yoshimoto,
K. Shimizu, S. Nakagawa, M. Dantsuji, and S. Makino
(2002). English Speech Database Read by Japanese
Learners for CALL System Development. In
Proceedings of LREC 2002.
Mitton, Roger and Takeshi Okada (2007). The adapta-
tion of an English spellchecker for Japanese writers.
In Symposium on Second Language Writing.
Okada, Takeshi (2004). A Corpus Analysis of Spelling
Errors Made by Japanese EFL Writers. Yamagata En-
glish Studies 9.
TIMIT (1991). TIMIT Acoustic-Phonetic Continuous
Speech Corpus. NIST Speech Disc CD1-1.1.
Toutanova, Kristina and Robert Moore (2002). Pronunci-
ation Modeling for Improved Spelling Correction. In
Proceedings of ACL 2002.
36
Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 40?47,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Identifying non-referential it: a machine learning approach incorporating
linguistically motivated patterns
Adriane Boyd
Department of Linguistics
The Ohio State University
1712 Neil Ave.
Columbus, OH 43210
adriane@ling.osu.edu
Whitney Gegg-Harrison & Donna Byron
Department of Computer Science and Engineering
The Ohio State University
2015 Neil Ave.
Columbus, OH 43210
{geggharr,dbyron}@cse.osu.edu
Abstract
In this paper, we present a machine learn-
ing system for identifying non-referential
it. Types of non-referential it are ex-
amined to determine relevant linguistic
patterns. The patterns are incorporated
as features in a machine learning system
which performs a binary classification of
it as referential or non-referential in a
POS-tagged corpus. The selection of rel-
evant, generalized patterns leads to a sig-
nificant improvement in performance.
1 Introduction
The automatic classification of it as either referen-
tial or non-referential is a topic that has been rel-
atively ignored in the computational linguistics lit-
erature, with only a handful of papers mentioning
approaches to the problem. With the term ?non-
referential it?, we mean to refer to those instances
of it which do not introduce a new referent. In the
previous literature these have been called ?pleonas-
tic?, ?expletive?, and ?non-anaphoric?. It is impor-
tant to be able to identify instances of non-referential
it to generate the correct semantic interpretation of
an utterance. For example, one step of this task is to
associate pronouns with their referents. In an auto-
mated pronoun resolution system, it is useful to be
able to skip over these instances of it rather than at-
tempt an unnecessary search for a referent for them,
The authors would like to thank the GE Foundation Faculty
for the Future grant for their support of this project. We would
also like to thank Detmar Meurers and Erhard Hinrichs for their
helpful advice and feedback.
only to end up with inaccurate results. The task of
identifying non-referential it could be incorporated
into a part-of-speech tagger or parser, or viewed as
an initial step in semantic interpretation.
We develop a linguistically-motivated classifi-
cation for non-referential it which includes four
types of non-referential it: extrapositional, cleft,
weather/condition/time/place, and idiomatic, each
of which will be discussed in more detail in Section
2. A subset of the BNC Sampler Corpus (Burnard,
1995) was chosen for our task because of its ex-
tended tagset and high tagging accuracy. Non-
referential it makes up a significant proportion of the
occurrences of it in our corpus, which contains a se-
lection of written texts of various genres, approxi-
mately one-third prose fiction, one-third newspaper
text, and one-third other non-fiction. In our corpus,
there are 2337 instances of it, 646 of which are non-
referential (28%). It appears in over 10% of the sen-
tences in our corpus. The corpus is described in fur-
ther detail in Section 3.
Previous research on this topic is fairly lim-
ited. Paice and Husk (1987) introduces a rule-based
method for identifying non-referential it and Lappin
and Leass (1994) and Denber (1998) describe rule-
based components of their pronoun resolution sys-
tems which identify non-referential it. Evans (2001)
describes a machine learning system which classi-
fies it into seven types based on the type of referent.
Their approaches are described in detail in Section
4. In Section 5 we describe our system which com-
bines and extends elements of the systems developed
by Paice and Husk (1987) and Evans (2001), and the
results are presented in Section 6.
40
2 Classification
The first step is to create a classification system for
all instances of it. Though the goal is the binary clas-
sification of it as referential or non-referential, an
annotation scheme is used which gives more detail
about each instance of non-referential it, since they
occur in a number of constructions. The main types
of non-referential it are taken from the Cambridge
Grammar of the English Language in the section
on ?Special uses of it?, Section 2.5, Huddleston and
Pullum (2002). Five main uses are outlined: extra-
positional, cleft, weather/condition/time/place, id-
iomatic, and predicative. As noted in the Cambridge
Grammar, predicative it seems to be more referen-
tial that the other types of non-referential it. Pred-
icative it can typically be replaced with a demonstra-
tive pronoun. Consider the example: It is a dreary
day. It can be replaced with This with no change in
grammaticality and no significant change in mean-
ing: This is a dreary day. In contrast, replacing the
other types of it with this results in nonsense, e.g.,
*This seems that the king is displeased.
For our purposes, if a particular it can be re-
placed with a demonstrative pronoun and the result-
ing sentence is still grammatical and has no signif-
icant change in meaning, this it is referential and
therefore annotated as referential. The demonstra-
tive pronoun replacement test is not quite perfect
(e.g., *This is a dreary day in Paris), but no such
instances of predicative it were found in the corpus
so predicative it is always classified as referential.
This leaves four types of it, each of which are de-
scribed in detail below. The main examples for each
type are taken from the corpus. See Section 3 for
details about the corpus.
2.1 Extrapositional
When an element of a sentence is extraposed, it is
often inserted as a placeholder in the original posi-
tion of the now extraposed element. Most often, it
appears in the subject position, but it can also ap-
pear as an object. Example (1) lists a few instances
of extrapositional it from our corpus.
(1) a. It has been confirmed this week that politi-
cal parties will no longer get financial sub-
sidies.
b. She also made it clear that Conductive Ed-
ucation is not the only method.
c. You lead life, it seems to me, like some rit-
ual that demands unerring performance.
The extraposed element is typically a subordinate
clause, and the type of clause depends on lexical
properties of verbs and adjectives in the sentence,
see (2).
(2) * It was difficult that X.
It was difficult to X.
* It was clear to X.
It was clear that X.
As (1c) shows, extrapositional it can also appear
as part of a truncated extrapositional phrase as a kind
of parenthetical comment embedded in a sentence.
2.2 Cleft
It appears as the subject of it-cleft sentences. When
an it-cleft sentence is formed, the foregrounded
phrase becomes the complement of the verb be and
the rest of sentence is backgrounded in a relative
clause. The foregrounded phrase in a cleft sentence
can be a noun phrase, prepositional phrase, adjective
phrase, adverb phrase, non-finite clause, or content
clause.
(3) a. It was the military district commander
who stepped in to avoid bloodshed. (noun
phrase)
b. It is on this point that the views of the
SACP and some Soviet policymakers di-
vide. (prepositional phrase)
c. ?Tis glad I am to ?ear it, me lord. (adjective
phrase)
Additionally, the foregrounded phrase can some-
times be fronted:
(4) He it was who ushered in the new head of state.
More context than the immediate sentence is
needed to accurately identify it-cleft sentences.
First, clefts with a foregrounded noun phrase are am-
biguous between cleft sentences (5a) and sentences
where the noun phrase and relative clause form a
constituent (5b).
41
(5) a. A: I heard that the general stepped in to
avoid bloodshed.
B: No, it was the military district comman-
der who stepped in.
b. A: Was that the general being interviewed
on the news?
B: No, it was the military district comman-
der who stepped in to avoid bloodshed.
Due to this ambiguity, we expect that it may be
difficult to classify clefts. In addition, there are dif-
ficulties because the relative clause does not always
appear in full. In various situations the relative pro-
noun can be omitted, the relative clause can be re-
duced, or the relative clause can be omitted entirely.
2.3 Weather/Condition/Time/Place
It appears as the subject of weather and other
related predicates involving condition, time, and
place/distance:
(6) a. It was snowing steadily outside.
b. It was about midnight.
c. It was no distance to Mutton House.
d. It was definitely not dark.
2.4 Idiomatic
In idioms, it can appear as the subject, object, or
object of a preposition.
(7) a. After three weeks it was my turn to go to
the delivery ward at Fulmer.
b. Cool it!
c. They have not had an easy time of it.
2.5 General Notes
Non-referential it is most often the subject of a sen-
tence, but in extrapositional and idiomatic cases, it
can also be the object. Idioms are the only cases
where non-referential it is found as the object of a
preposition.
3 Corpus
The BNC Sampler Corpus (Burnard, 1995) was cho-
sen for its extended tagset and high tagging accu-
racy. The C7 tagset used for this corpus has a unique
Prose fiction 32%
Newspaper text 38%
Other non-fiction 30%
Table 1: Text types in our corpus
# of Instances % of Inst.
Extrapositional 477 20.4%
Cleft 119 5.1%
Weather 69 2.9%
Idiomatic 46 2.0%
Referential 1626 69.6%
Total 2337 100%
Table 2: Instances of it in our corpus
tag for it, which made the task of identifying all oc-
currences of it very simple. We chose a subset con-
sisting of 350,000 tokens from written texts in vari-
ety of genres. The breakdown by text type can be
seen in Table 1.
The two lead authors independently annotated
each occurence with one of the labels shown in Ta-
ble 2 and then came to a joint decision on the fi-
nal annotation. The breakdown of the instances of it
in our corpus is shown in Table 2. There are 2337
occurrences of it, 646 of which are non-referential
(28%). Ten percent of the corpus, taken from all
sections, was set aside as test data. The remaining
section, which contains 2100 instances of it, became
our training data.
4 Previous Research
Paice and Husk (1987) reports a simple rule-based
system that was used to identify non-referential it in
the technical section of the Lancaster-Oslo/Bergen
Corpus. Because of the type of text, the distribution
of types of non-referential it is somewhat limited, so
they only found it necessary to write rules to match
extrapositional and cleft it (although they do men-
tion two idioms found in the corpus). The corpus
was plain text, so their rules match words and punc-
tuation directly.
Their patterns find it as a left bracket and search
for a right bracket related to the extrapositional and
cleft grammatical patterns (to, that, etc.). For the
extrapositional instances, there are lists of words
which are matched in between it and the right
42
Accuracy 92%
Precision 93%
Recall 97%
Table 3: Paice and Husk (1987): Results
Accuracy 79%
Precision 80%
Recall 31%
Table 4: Replicating Paice and Husk (1987)
bracket. The word lists are task-status words (STA-
TUS), state-of-knowledge words (STATE), and a list
of prepositions and related words (PREP), which is
used to rule out right brackets that could potentially
be objects of prepositions. Patterns such as ?it STA-
TUS to? and ?it !PREP that? were created. The
left bracket can be at most 27 words from the right
bracket and there can be either zero or two or more
commas or dashes between the left and right brack-
ets. Additionally, their system had a rule to match
parenthetical it: there is a match when it appears im-
mediately following a comma and another comma
follows within four words. Their results, shown in
Table 3, are impressive.
We replicated their system and ran it on our test-
ing data, see Table 4. Given the differences in text
types, it is not surprising that their system did not
perform as well on our corpus. The low recall seems
to show the limitations of fixed word lists, while the
reasonably high precision shows that the simple pat-
terns tend to be accurate in the cases where they ap-
ply.
Lappin and Leass (1994) and Denber (1998) men-
tion integrating small sets of rules to match non-
referential it into their larger pronoun resolution sys-
tems. Lappin and Leass use two words lists and
a short set of rules. One word list is modal adjec-
tives (necessary, possible, likely, etc.) and the other
is cognitive verbs (recommend, think, believe, etc.).
Their rules are as follows:
It is Modaladj that S
It is Modaladj (for NP) to VP
It is Cogv-ed that S
It seems/appears/means/follows (that) S
NP makes/finds it Modaladj (for NP) to VP
Accuracy 71%
Precision 73%
Recall 69%
Table 5: Evans (2001): Results, Binary Classifica-
tion
It is time to VP
It is thanks to NP that S
Their rules are mainly concerned with extraposi-
tional it and they give no mention of cleft it. They
give no direct results for this component of their
system, so it is not possible to give a comparison.
Denber (1998) includes a slightly revised and ex-
tended version of Lappin and Leass?s system and
adds in detection of weather/time it. He suggests
using WordNet to extend word lists.
Evans (2001) begins by noting that a significant
percentage of instances of it do not have simple
nominal referents and describes a system which uses
a memory-based learning (MBL) algorithm to per-
form a 7-way classification of it by type of refer-
ent. We consider two of his categories, pleonas-
tic and stereotypic/idiomatic, to be non-referential.
Evans created a corpus with texts from the BNC
and SUSANNE corpora and chose to use a memory-
based learning algorithm. A memory-based learn-
ing algorithm classifies new instances on the basis of
their similarity to instances seen in the training data.
Evans chose the k-nearest neighbor algorithm from
the Tilburg Memory-Based Learner (TiMBL) pack-
age (Daelemans et al, 2003) with approximately 35
features relevant to the 7-way classification. Al-
though his system was created for the 7-way classi-
fication task, he recognizes the importance of the bi-
nary referential/non-referential distinction and gives
the results for the binary classification of pleonastic
it, see Table 5. His results for the classification of
idiomatic it (33% precision and 0.7% recall) show
the limitations of a machine learning system given
sparse data.
We replicated Evans?s system with a simplified set
of features to perform the referential/non-referential
classification of it. We did not include features that
would require chunking or features that seemed rel-
evant only for distinguishing kinds of referential it.
The following thirteen features are used:
43
Accuracy 76%
Precision 57%
Recall 60%
Table 6: Replicating Evans (2001)
1-8. four preceding and following POS tags
9-10. lemmas of the preceding and following verbs
11. lemma of the following adjective
12. presence of that following
13. presence of an immediately preceding preposi-
tion
Using our training and testing data with the same
algorithm from TiMBL, we obtained results similar
to Evans?s, shown in Table 6. The slightly higher
accuracy is likely due to corpus differences or the
reduced feature set which ignores features largely
relevant to other types of it.
Current state-of-the-art reference resolution sys-
tems typically include filters for non-referential
noun phrases. An example of such a system is Ng
and Cardie (2002), which shows the improvement
in reference resolution when non-referential noun
phrases are identified. Results are not given for the
specific task of identifying non-referential it, so a di-
rect comparison is not possible.
5 Method
As seen in the previous section, both rule-based
and machine learning methods have been shown to
be fairly effective at identifying non-referential it.
Rule-based methods look for the grammatical pat-
terns known to be associated with non-referential it
but are limited by fixed word lists; machine learning
methods can handle open classes of words, but are
less able to generalize about the grammatical pat-
terns associated with non-referential it from a small
training set.
Evans?s memory-based learning system showed a
slight integration of rules into the machine learning
system by using features such as the presence of fol-
lowing that. Given the descriptions of types of non-
referential it from Section 2, it is possible to create
more specific rules which detect the fixed grammat-
ical patterns associated with non-referential it such
as it VERB that or it VERB ADJ to. Many of these
patterns are similar to Paice and Husk?s, but hav-
ing part-of-speech tags allows us to create more gen-
eral rules without reference to specific lexical items.
If the results of these rule matches are integrated
as features in the training data for a memory-based
learning system along with relevant verb and ad-
jective lemmas, it becomes possible to incorporate
knowledge about grammatical patterns without cre-
ating fixed word lists. The following sections exam-
ine each type of non-referential it and describe the
patterns and features that can be used to help auto-
matically identify each type.
5.1 Extrapositional it
Extrapositional it appears in a number of fairly fixed
patterns, nine of which are shown below. Interven-
ing tokens are allowed between the words in the pat-
terns. F4-6 are more general versions of F1-3 but
are not as indicative of non-referential it, so it useful
to keep them separate even though ones that match
F1-3 will also match F4-6. F7 applies when it is the
object of a verb. To simplify patterns like F8, all
verbs in the sentence are lemmatized with morpha
(Minnen et al, 2001) before the pattern matching
begins.
F1 it VERB ADJ that
F2 it VERB ADJ
what/which/where/whether/why/how
F3 it VERB ADJ to
F4 it VERB that
F5 it VERB what/which/where/whether/why/how
F6 it VERB to
F7 it ADJ that/to
F8 it be/seem as if
F9 it VERB COMMA
For each item above, the feature consists of the
distance (number of tokens) between it and the end
of the match (the right bracket such that or to).
By using the distance as the feature, it is possible
to avoid specifying a cutoff point for the end of a
match. The memory-based learning algorithm can
adapt to the training data. As discussed in Sec-
tion 2.1, extraposition is often lexically triggered,
so the specific verbs and adjectives in the sentence
are important for its classification. For this reason,
it is necessary to include information about the sur-
rounding verbs and adjectives. The nearby full verbs
44
(as opposed to auxiliary and modal verbs) are likely
to give the most information, so we add features for
the immediately preceding full verb (for F7), the
following full verb (for F1-F6), and the following
adjective (for F1-3,7). The verbs were lemmatized
with morpha and added as features along with the
following adjective.
F10 lemma of immediately preceding full verb
F11 lemma of following full verb within current
sentence
F12 following adjective within current sentence
5.2 Cleft it
Two patterns are used for cleft it:
F13 it be who/which/that
F14 it who/which/that
As mentioned in the previous section, all verbs in
the sentence are lemmatized before matching. Like-
wise, these features are the distance between it and
the right bracket. Feature F14 is used to match a
cleft it in a phrase with inverted word order.
5.3 Weather/Condition/Time/Place it
Ideally, the possible weather predicates could be
learned automatically from the following verbs, ad-
jectives, and nouns, but the list is so open that it
is better in practice to specify a fixed list. The
weather/time/place/condition predicates were taken
from the training set and put into a fixed list. Some
generalizations were made (e.g., adding the names
of all months, weekdays, and seasons), but the list
contains mainly the words found in the training set.
There are 46 words in the list. As Denber men-
tioned, WordNet could be used to extend this list.
A feature is added for the distance to the nearest
weather token.
The following verb lemma feature (F10) added
for extrapositional it is the lemma of the follow-
ing full verb, but in many cases the verb following
weather it is the verb be, so we also added a binary
feature for whether the following verb is be.
F15 distance to nearest weather token
F16 whether the following verb is be
5.4 Idiomatic it
Idioms can be identified by fixed patterns. All verbs
in the sentence are lemmatized and the following
patterns, all found as idioms in our training data, are
used:
if/when it come to pull it off
as it happen fall to it
call it a NOUN ask for it
on the face of it be it not for
have it not been for like it or not
Short idiom patterns such as ?cool it? and ?watch
it? were found to overgeneralize, so only idioms in-
cluding at least three words were used. A binary
feature was added for whether an idiom pattern was
matched for the given instance of it (F17). In addi-
tion, two common fixed patterns were included as a
separate feature:
it be ... time
it be ... my/X?s turn
F17 whether an idiom pattern was matched
F18 whether an additional fixed pattern was
matched
5.5 Additional Restrictions
There are a few additional restrictions on the pattern
matches involving length and punctuation. The first
restriction is on the distance between the instance
of it and the right bracket (that, to, who, etc.). On
the basis of their corpus, Paice and Husk decided
that the right bracket could be at most 27 words
away from it. Instead of choosing a fixed distance,
features based on pattern matches are the distance
(number of tokens) between it and the right bracket.
The system looks for a pattern match between it
and the end of the sentence. The end of a sentence
is considered to be punctuation matching any of the
following: . ; : ? ! ) ] . (Right parenthesis or
bracket is only included if a matching left parenthe-
sis or bracket has not been found before it.) If there
is anything in paired parentheses in the remainder of
the sentence, it is omitted. Quotes are not consistent
indicators of a break in a sentence, so they are ig-
nored. If the end of a sentence is not located within
50 tokens, the sentence is truncated at that point and
the system looks for the patterns within those tokens.
45
As Paice and Husk noted, the presence of a sin-
gle comma or dash between it and the right bracket
is a good sign that the right bracket is not rele-
vant to whether the instance of it is non-referential.
When there are either zero or two or more commas
or dashes it is difficult to come to any conclusion
without more information. Therefore, when the to-
tal comma count or total dash count between it and
the right bracket is one, the pattern match is ignored.
Additionally, unless it occurs in an idiom, it is
also never the object of a preposition, so there is
an additional feature for whether it is preceded by
a preposition.
F19 whether the previous word is a preposition
Finally, the single preceding and five following
simplified part-of-speech tags were also included.
The part-of-speech tags were simplified to their first
character in the C7 tagset, adverb (R) and nega-
tive (X) words were ignored, and only the first in-
stance in a sequence of tokens of the same simplified
type (e.g., the first of two consecutive verbs) was in-
cluded in the set of following tags.
F20-25 surrounding POS tags, simplified
6 Results
Training and testing data were generated from our
corpus using the the 25 features described in the
previous section. Given Evans?s success and the
limited amount of training data, we chose to also
use TiMBL?s k-nearest neighbor algorithm (IB1).
In TiMBL, the distance metric can be calculated
in a number of ways for each feature. The nu-
meric features use the numeric metric and the re-
maining features (lemmas, POS tags) use the de-
fault overlap metric. Best performance is achieved
with gain ratio weighting and the consideration of
2 nearest distances (neighbors). Because of overlap
in the features for various types of non-referential
it and sparse data for cleft, weather, and idiomatic
it, all types of non-referential it were considered at
the same time and the output was a binary classifi-
cation of each instance of it as referential or non-
referential. The results for our TiMBL classifier
(MBL) are shown in Table 7 alongside our results
using a decision tree algorithm (DT, described be-
low) and the results from our replication of Evans
Our MBL
Classifier
Our DT
Classifier
Repl. of
Evans
Accuracy 88% 81% 76%
Precision 82% 82% 57%
Recall 71% 42% 60%
Table 7: Results
Extrapositional 81%
Cleft 45%
Weather 57%
Idiomatic 60%
Referential 94%
Table 8: Recall by Type for MBL Classifier
(2001). All three systems were trained and evalu-
ated with the same data.
All three systems perform a binary classifica-
tion of each instance of it as referential or non-
referential, but each instance of non-referential it
was additionally tagged for type, so the recall for
each type can be calculated. The recall by type can
been seen in Table 8 for our MBL system. Given that
the memory-based learning algorithm is using previ-
ously seen instances to classify new ones, it makes
sense that the most frequent types have the highest
recall. As mentioned in Section 2.2, clefts can be
difficult to identify.
Decision tree algorithms seem suited to this kind
of task and have been used previously, but C4.5
(Quinlan, 1993) decision tree algorithm did not per-
form as well as TiMBL on our data, compare the
TiMBL results (MBL) with the C4.5 results (DT) in
Table 7. This may be because the verb and adjective
lemma features (F10-F12) had hundreds of possible
values and were not as useful in a decision tree as in
the memory-based learning algorithm.
With the addition of more relevant, generalized
grammatical patterns, the precision and accuracy
have increased significantly, but the same cannot be
said for recall. Because many of the patterns are
designed to match specific function words as the
right bracket, cases where the right bracket is omit-
ted (e.g., extraposed clauses with no overt comple-
mentizers, truncated clefts, clefts with reduced rela-
tive clauses) are difficult to match. Other problem-
atic cases include sentences with a lot of intervening
46
material between it and the right bracket or simple
idioms which cannot be easily differentiated. The
results for cleft, weather, and idiomatic it may also
be due in part to sparse data. When only 2% of the
instances of it are of a certain type, there are fewer
than one hundred training instances, and it can be
difficult for the memory-based learning method to
be very successful.
7 Conclusion
The accurate classification of it as referential or non-
referential is important for natural language tasks
such as reference resolution (Ng and Cardie, 2002).
Through an examination of the types of construc-
tions containing non-referential it, we are able to de-
velop a set of detailed grammatical patterns associ-
ated with non-referential it. In previous rule-based
systems, word lists were created for the verbs and
adjectives which often occur in these patterns. Such
a system can be limited because it is unable to adapt
to new texts, but the basic grammatical patterns
are still reasonably consistent indicators of non-
referential it. Given a POS-tagged corpus, the rele-
vant linguistic patterns can be generalized over part-
of-speech tags, reducing the dependence on brittle
word lists. A machine learning algorithm is able
to adapt to new texts and new words, but it is less
able to generalize about the linguistic patterns from
a small training set. To be able to use our knowl-
edge of relevant linguistic patterns without having to
specify lists of words as indicators of certain types
of it, we developed a machine learning system which
incorporates the relevant patterns as features along-
side part-of-speech and lexical information. Two
short lists are still used to help identify weather it
and a few idioms. The k-nearest neighbors algo-
rithm from the Tilburg Memory Based Learner is
used with 25 features and achieved 88% accuracy,
82% precision, and 71% recall for the binary classi-
fication of it as referential or non-referential.
Our classifier outperforms previous systems in
both accuracy and precision, but recall is still a prob-
lem. Many instances of non-referential it are diffi-
cult to identify because typical clues such as com-
plementizers and relative pronouns can be omitted.
Because of this, subordinate and relative clauses
cannot be consistently identified given only a POS-
tagged corpus. Improvements could be made in the
future by integrating chunking or parsing into the
pattern-matching features used in the system. This
would help in identifying extrapositional and cleft it.
Knowledge about context beyond the sentence level
will be needed to accurately identify certain types of
cleft, weather, and idiomatic constructions.
References
L. Burnard, 1995. Users reference guide for the British
National Corpus. Oxford.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2003. TiMBL: Tilburg Mem-
ory Based Learner, version 5.0, Reference Guide. ILK
Technical Report 03-10. Technical report.
Michel Denber. 1998. Automatic resolution of anaphora
in English. Technical report, Imaging Science Divi-
son, Eastman Kodak Co.
Richard Evans. 2001. Applying machine learning to-
ward an automatic classification of It. Literary and
Linguistic Computing, 16(1):45 ? 57.
Rodney D. Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press, Cambridge.
Shalom Lappin and Herbert J. Leass. 1994. An Algo-
rithm for Pronominal Anaphora Resolution. Compu-
tational Linguistics, 20(4):535?561.
Guido Minnen, John Caroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207?223.
Vincent Ng and Claire Cardie. 2002. Identifying
anaphoric and non-anaphoric noun phrases to improve
coreference resolution. Proceedings of the 19th In-
ternational Conference on Computational Linguistics
(COLING-2002).
C. D. Paice and G. D. Husk. 1987. Towards an automatic
recognition of anaphoric features in English text; the
impersonal pronoun ?it?. Computer Speech and Lan-
guage, 2:109 ? 132.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
47
Proceedings of the Linguistic Annotation Workshop, pages 41?44,
Prague, June 2007. c?2007 Association for Computational Linguistics
Discontinuity Revisited: An Improved Conversion to
Context-Free Representations
Adriane Boyd
Department of Linguistics
The Ohio State University
1712 Neil Ave.
Columbus, OH 43210
adriane@ling.osu.edu
Abstract
This paper introduces a new, reversible
method for converting syntactic structures
with discontinuous constituents into tradi-
tional syntax trees. The method is applied
to the Tiger Corpus of German and results
for PCFG parsing requiring such context-
free trees are provided. A labeled depen-
dency evaluation shows that the new conver-
sion method leads to better results by pre-
serving local relationships and introducing
fewer inconsistencies into the training data.
1 Introduction
Unlike traditional treebanks, the Negra and Tiger
Corpora (Brants et al, 2002) allow crossing
branches in the syntactic annotation to handle cer-
tain features of German. In order to use the Ne-
gra or Tiger Corpus data to train a PCFG parser, it
is necessary to convert the syntactic annotation into
context-free syntax trees. In previous work (see sec-
tion 3.1), a non-reversible method has been used that
raises nodes in the tree to eliminate discontinuities.
This method effectively introduces inconsistencies
into the data and disrupts the grammatical depen-
dency annotation in the trees. This paper presents
a new, reversible method for converting Negra and
Tiger syntactic structures into context-free syntax
trees appropriate for training a PCFG parser. A re-
versible conversion allows the original grammati-
cal dependency relations to be reconstructed from
the PCFG parser output. This paper focuses on the
newer, larger Tiger Corpus, but methods and results
are very similar for the Negra Corpus.
2 Tiger Corpus
The Tiger Corpus was a joint project between Saar-
land University, the University of Stuttgart, and Uni-
versity of Potsdam. The Tiger Corpus Version 2 con-
tains 50,474 sentences of newspaper text. The Tiger
annotation combines features from phrase structure
grammar and dependency grammar using a tree-like
syntactic structure with grammatical functions la-
beled on the edges of the tree (Brants et al, 2002).
Flat sentence structures are used in many places
to avoid attachment ambiguities and non-branching
phrases are not allowed. The annotation scheme
emphasizes the use of the tree structure to encode
all grammatical relations in local trees regardless of
whether a grammatical dependency is local within in
the sentence. This leads to the use of discontinuous
constituents to handle flexible word order, extraposi-
tion, partial constituent fronting, and other phenom-
ena. An example of a Tiger tree with discontinuous
constituents (both VPs) is shown in Figure 1.
3 Conversion to Context-Free Syntax
Trees
For research involving PCFG parsing models trained
on Tiger Corpus data, it is necessary to convert the
syntax graphs with crossing branches into traditional
syntax trees in order to extract context-free grammar
rules from the data. Approximately 30% of sen-
tences in Tiger contain at least one discontinuous
constituent.
3.1 Existing Tiger Corpus Conversion
In previous research, crossing branches have been
resolved by raising non-head nodes out of discon-
41
Mit
APPR
dem
ART
Bau
NN
soll
VMFIN
1997
CARD
begonnen
VVPP
werden
VAINF
AC NK NK
PP
OP MO HD
VP
OC HD
VP
OCHD
S
?Construction should start in 1997.?
(lit. with the construction should 1997 begun be)
Figure 1: Discontinuous Tiger tree
tinuous constituents until no more branches cross.
The converted sentence from Figure 1 is shown in
Figure 2. In any sentence, multiple nodes could
each be raised one or more times, so it is difficult
to automatically reconstruct the original sentence.
Previous work on PCFG parsing using Negra or
Tiger has either used the provided Penn Treebank-
style versions of the corpora included with Ne-
gra and Tiger Version 1 (Dubey and Keller, 2003;
Dubey, 2004) or used a program provided with the
Negra/Tiger Annotate software (Plaehn and Brants,
2000) which performs the raising algorithm (Ku?bler,
2005; Ku?bler et al, 2006). This conversion will be
referred to as the ?raising method?.
3.2 A New Approach to Eliminating
Discontinuities
The raising method has the advantages of preserving
the number of nodes in the tree, but it is not easily
reversible and disrupts local trees. Raising non-head
nodes is not an ideal way of eliminating disconti-
nuities because it does not preserve the relationship
between a head and a dependent that is represented
in a local tree in the Tiger annotation. After raising
one or more nodes in 30% of the sentences in the
corpus, local trees are no longer consistent across
the treebank. Some VPs may contain all their ob-
jects while others do not. For example, in Figure 2
the PP object Mit dem Bau is no longer in the local
tree with its head begonnen. The PCFG has lessened
chance of capturing generalizations from the result-
ing inconsistent training data.
Preferable to the raising method is a conversion
that is reversible and that preserves local trees as
Mit
APPR
dem
ART
Bau
NN
soll
VMFIN
1997
CARD
begonnen
VVPP
werden
VAINF
AC NK NK
PP
OP HD OC
S
MO HD
VP
OC HD
VP
Figure 2: Result of conversion by raising
Mit
APPR
dem
ART
Bau
NN
soll
VMFIN
1997
CARD
begonnen
VVPP
werden
VAINF
OC HD OC
S
OC
VP*
OP
VP*
AC NK NK
PP
OC HD
VP*
MO HD
VP*
Figure 3: Result of conversion by splitting
much as possible. The new approach to the con-
version involves splitting discontinuous nodes into
smaller ?partial nodes?. Each subset of the original
children with a continuous terminal yield becomes a
partial node. In this way, it is possible to remove
crossing branches while preserving the parent re-
lationships from the original tree. Because partial
nodes retain their original parents, the reverse con-
version is greatly simplified.
In order to make the conversion easily reversible,
the partial nodes need to be marked in some way
so that they can be identified in the reverse conver-
sion. A simple method is to use a single mark (*)
on all partial nodes.1 For example, a discontinu-
ous VP with the children NN-OA (noun acc. obj.)
and VVINF-HD (infinitive) would be converted into
a VP* with an NN-OA child and a VP* with a
VVINF-HD child. The method of creating partial
nodes with a single mark will be called the ?splitting
method?. It is completely reversible unless there are
two discontinuous sisters with the same label. While
it is not unusual for a Tiger tree to have multiple dis-
1This approach was inspired by Joakim Nivre?s paper
Pseudo-Projective Dependency Parsing (Nivre, 2005), in which
non-projective dependency structures are converted to easier-to-
parse projective dependency structures in a way that limits the
number of new labels introduced, but is mostly reconstructible.
42
continuous nodes with same label (as in Figure 1),
two nodes with the same label are never sisters so the
conversion is reversible for all sentences. Each tree
is converted with the following algorithm, which is a
postorder traversal that starts at the root node of the
tree. The postorder traversal guarantees that every
child of a node is continuous before the node itself
is evaluated, so splitting the node under considera-
tion into partial nodes will resolve the discontinuity.
SPLIT-DISC-NODES(Node)
for each Child of Node
SPLIT-DISC-NODES(Child)
if Node?s terminal yield is discontinuous
Children := immediate children of Node
ContSets := divide Children into subsets
with continuous terminal yields
for each ChildSubset in ContSets
PNode := new node
PNode?s label := Node?s label with mark (*)
PNode?s parent := Node?s parent
for each Child in ChildSubset
Child?s parent := PNode
remove Node from tree
The splitting conversion of the sentence from Fig-
ure 1 can be seen in Figure 3. To convert the split
version back to the original version, the tree is ex-
amined top-down, rejoining any marked sister nodes
with the same label.
4 Results
All parsing was performed using the unlexicalized
parsing model from the left corner parser LoPar
Schmid (2000). The input data was labeled with per-
fect tags from the corpus to prevent errors in tagging
from affecting the parsing results.
4.1 Data Preparation
For the following experiments, the Tiger Corpus
Version 2 was divided into training, development,
and testing sections. Following the data split from
Dubey (2004), 90% of the corpus was used as train-
ing data, 5% as development data, and 5% as test
data. In preprocessing, all punctuation was removed
because it is not attached within the sentence. 6.5%
of sentences are excluded because they contain no
annotation beyond the word level or because they
contain multiple root nodes. After preprocessing,
there are 42,612 sentences in the training set. For
evaluation, only sentences with 40 words or fewer
are used, leaving 2,312 test sentences. The raised
version is created using the Annotate software and
the split version is created using the method de-
scribed in section 3.2. For the split version, partial
nodes are rejoined before evaluation.
In the Penn Treebank-style versions of the corpus
appropriate for training a PCFG parser, each edge la-
bel has been joined with the phrase or POS label on
the phrase or word immediately below it. Because of
this, the edge labels for single-word arguments (e.g.,
pronoun subjects) are attached to the POS tag of
the word, which provides the parser with the perfect
grammatical function label when perfect lexical tags
are provided. This amounts to providing the perfect
grammatical function labels for approximately one-
third of arguments in Tiger, so to avoid this prob-
lem, non-branching phrase nodes are introduced for
single-word arguments. Phrase nodes are introduced
above all single-word subjects, accusative objects,
dative objects, and genitive objects. The category of
the inserted phrase depends on the POS tag on the
word (NP, VP, or AP as appropriate).
4.2 Experiment 1: Reversibility of Splitting
Conversion
All sentences in the test set were converted into syn-
tax trees by splitting discontinuous nodes according
to the algorithm in section 3.2. All 2,312 sentences
in the test set can be converted back to their original
versions with no errors. The most frequently split
nodes are VP (?55%) and NP (?20%).
4.3 Experiment 2: Labeled Dependency
Evaluation
A labeled dependency evaluation is chosen instead
of a typical PARSEVAL evaluation for two reasons:
1) PARSEVAL is unable to evaluate trees with dis-
continuous constituents; 2) a bracketing evaluation
examines all types of brackets in the sentence and
may not reflect how accurately significant grammat-
ical dependencies have been identified.
It is useful to look at an evaluation on gram-
matical functions that are important for determining
the functor-argument structure of the sentence. In
this evaluation, subjects, accusative objects, prepo-
43
Raised Split
GF P R F P R F
Subj 74.8 71.6 73.2 74.7 73.5 74.1
AccObj 46.3 48.9 47.4 49.2 53.7 51.4
PPObj 20.4 10.7 15.6 31.9 15.6 23.8
DatObj 20.1 11.5 15.8 25.5 14.3 19.9
Table 1: Labeled Dependency Evaluation
sitional objects, and dative objects are considered as
part of labeled dependency triples consisting of the
lexical head verb, the grammatical function label,
and the dependent phrase bearing the grammatical
function label. The internal structure of the depen-
dent phrase is not considered.
In Tiger annotation, the head of an argument is
the sister marked with the grammatical function la-
bel HD. HD labels are found with an f-score of 99%
by the parser, so this evaluation mainly reflects how
well the arguments in the dependency triple are iden-
tified. This evaluation uses lexical heads, so if the
sister with the label HD is a phrase, then a recursive
search for heads within that phrase finds the lexical
head. For 5.7% of arguments in the gold standard, it
is not possible to find a lexical head. Further meth-
ods could be applied to find the remaining heads
heuristically, but the additional parameters this in-
troduces for the evaluation are avoided by ignoring
these cases.
The results for a labeled dependency evaluation
on important grammatical function labels are shown
in Table 4.3. Grammatical functions are listed in or-
der of decreasing frequency. The results for subjects
remain similar between the raised and split version,
as expected, and the results for all other types of ar-
guments improve 4-8% for the split version.
Subjects are rarely affected by the raising method
because S nodes are rarely discontinuous, so it is not
surprising that the results for subjects are similar for
both methods. However, VPs are by far the most
frequently discontinuous nodes, and since the rais-
ing method can move an object away from its head,
the difference between the two conversion methods
is most evident in the object relations. Data sparsity
plays a role in the lower scores for the objects, since
there are approximately twice as many subjects as
accusative objects and twelve times as many sub-
jects as dative objects.
5 Future Work
Further research will extend the dependency evalu-
ation presented in this paper to include more or all
of the grammatical functions. There is significant
work on a dependency conversion for Negra by the
Partial Parsing Project (Daum et al, 2004) that could
be adapted for this purpose.
6 Conclusion
By using an improved conversion method to re-
move crossing branches from the Negra/Tiger cor-
pora, it is possible to generate trees without cross-
ing branches that can be converted back to the orig-
inal format with no errors. This is a significant im-
provement over the previously used conversion by
raising, which was not reversible and had the ef-
fect of introducing inconsistencies into the corpus.
The new splitting conversion method shows a 4-8%
improvement in a labeled dependency evaluation on
accusative, prepositional, and dative objects.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius and George Smith, 2002. The TIGER Tree-
bank. In Proceedings of TLT 2002.
Michael Daum, Kilian Foth and Wolfgang Menzel, 2004.
Automatic transformation of phrase treebanks to de-
pendency trees. In Proceedings of LREC 2004.
Amit Dubey, 2004. Statistical Parsing for German: Mod-
eling Syntactic Properties and Annotation Differences.
Ph.D. thesis, Universita?t des Saarlandes.
Amit Dubey and Frank Keller, 2003. Probabilistic Pars-
ing Using Sister-Head Dependencies. In Proceedings
of ACL 2006.
Sandra Ku?bler, 2005. How do treebank annotation
schemes influence parsing results? Or how not to com-
pare apples and oranges. In Proceedings of RANLP
2005.
Sandra Ku?bler, Erhard W. Hinrichs and Wolfgang Maier,
2006. Is it really that difficult to parse German? In
Proceedings of EMNLP 2006.
Joakim Nivre, 2005. Pseudo-Projective Dependency
Parsing. In Proceedings of ACL 2005.
Oliver Plaehn and Thorsten Brants, 2000. Annotate ? An
Efficient Interactive Annotation Tool. In Proceedings
of ANLP 2000.
Helmut Schmid, 2000. LoPar: Design and Implementa-
tion. Technical report, Universita?t Stuttgart.
44
Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 24?32,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Revisiting the impact of different annotation schemes on PCFG parsing:
A grammatical dependency evaluation
Adriane Boyd
Department of Linguistics
The Ohio State University
1712 Neil Avenue
Columbus, Ohio 43210, USA
adriane@ling.osu.edu
Detmar Meurers
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
Wilhelmstrasse 19
72074 Tu?bingen, Germany
dm@sfs.uni-tuebingen.de
Abstract
Recent parsing research has started addressing
the questions a) how parsers trained on differ-
ent syntactic resources differ in their perfor-
mance and b) how to conduct a meaningful
evaluation of the parsing results across such
a range of syntactic representations. Two Ger-
man treebanks, Negra and Tu?Ba-D/Z, consti-
tute an interesting testing ground for such re-
search given that the two treebanks make very
different representational choices for this lan-
guage, which also is of general interest given
that German is situated between the extremes
of fixed and free word order. We show that
previous work comparing PCFG parsing with
these two treebanks employed PARSEVAL
and grammatical function comparisons which
were skewed by differences between the two
corpus annotation schemes. Focusing on the
grammatical dependency triples as an essen-
tial dimension of comparison, we show that
the two very distinct corpora result in compa-
rable parsing performance.
1 Introduction
Syntactically annotated corpora have been produced
for a range of languages and they differ significantly
regarding which language properties are encoded
and how they are represented. Between the two ex-
tremes of constituency treebanks for English and de-
pendency treebanks for free word order languages
such as Czech lie languages such as German, for
which two different treebanks have explored differ-
ent options for encoding topology and dependency,
Negra (Brants et al, 1999) and Tu?Ba-D/Z (Telljo-
hann et al, 2005).
Recent research has started addressing the ques-
tion of how parsers trained on these different syntac-
tic resources differ in their performance. Such work
must also address the question of how to conduct a
meaningful evaluation of the parsing results across
such a range of syntactic representations. In this pa-
per, we show that previous work comparing PCFG
parsing for the two German treebanks used represen-
tations which cannot adequately be compared using
the given PARSEVAL measures and that a grammat-
ical dependency evaluation is more meaningful than
the grammatical function evaluation provided.
We present the first comparison of Negra and
Tu?Ba-D/Z using a labeled dependency evaluation
based on the grammatical function labels provided
in the corpora. We show that, in contrast to previ-
ous literature, a labeled dependency evaluation es-
tablishes that PCFG parsers trained on the two cor-
pora give similar parsing performance. The focus on
labeled dependencies also provides a direct link to
recent work on dependency-based evaluation (e.g.,
Clark and Curran, 2007) and dependency parsing
(e.g., CoNLL shared tasks 2006, 2007).
1.1 Previous work
The question of how to evaluate parser output has
naturally already arisen in earlier work on parsing
English. As discussed by Lin (1995) and others, the
PARSEVAL evaluation typically used to analyze the
performance of statistical parsing models has many
drawbacks. Bracketing evaluation may count a sin-
gle error multiple times and does not differentiate
between errors that significantly affect the interpre-
tation of the sentence and those that are less crucial.
24
It also does not allow for evaluation of particular
syntactic structures or provide meaningful informa-
tion about where the parser is failing. In addition,
and most directly relevant for this paper, PARSE-
VAL scores are difficult to compare across syntactic
annotation schemes (Carroll et al, 2003).
At the same time, previous research on PCFG
parsing using treebank training data present PAR-
SEVAL measures in comparing the parsing per-
formance for different languages and annotation
schemes, reporting a number of striking differences.
For example, Levy and Manning (2003), Ku?bler
(2005), and Ku?bler et al (2006) highlight the sig-
nificant effect of language properties and annotation
schemes for German and Chinese treebanks. In re-
lated work, parser enhancements that provide a sig-
nificant performance boost for English, such as head
lexicalization, are reported not to provide the same
kind of improvement, if any, for German (Dubey and
Keller, 2003; Dubey, 2004; Ku?bler et al, 2006).
Previous work has compared the similar Negra
and Tiger corpora of German to the very different
Tu?Ba-D/Z corpus. Ku?bler et al (2006) compares
the Negra and Tu?Ba-D/Z corpora of German using
a PARSEVAL evaluation and an evaluation on core
grammatical function labels that is included to ad-
dress concerns about the PARSEVAL measure.1 Us-
ing the Stanford Parser (Klein and Manning, 2002),
which employs a factored PCFG and dependency
model, they claim that the model trained on Tu?Ba-
D/Z consistently outperforms that trained on Ne-
gra in PARSEVAL and grammatical function evalu-
ations. Dubey (2004) also includes an evaluation on
grammatical function for statistical models trained
on Negra, but obtains very different results from
Ku?bler et al (2006).2
In recent related work, Rehbein and van Genabith
(2007a) demonstrate using the Tiger and Tu?Ba-D/Z
1The evaluation is based only on the grammatical function;
it does not identify the dependency pair that it labels.
2While the focus of Ku?bler et al (2006) is on comparing
parsing results across corpora, Dubey (2004) focuses on im-
proving parsing for Negra, including corpus-specific enhance-
ments leading to better results. This difference in focus and
additional differences in experimental setup mean that a fine-
grained comparison of the results is inappropriate ? the rele-
vant point here is that the gap between the results (23% for sub-
jects, 35% for accusative objects) warrants further attention in
the context of comparing parsing results across corpora.
corpora of German that PARSEVAL is inappropri-
ate for comparisons of the output of PCFG parsers
trained on different treebank annotation schemes be-
cause PARSEVAL scores are affected by the ratio
of terminal to non-terminal nodes. A dependency-
based evaluation on triples of the form word-POS-
head shows better results for the parser trained
on Tiger even though the much lower PARSEVAL
scores, if meaningful, would predict that the out-
put for Tiger is of lower quality. However, their
dependency-based evaluation does not make use
of the grammatical function labels, which are pro-
vided in the corpora and closely correspond to the
representations used in recent work on formalism-
independent evaluation of parsers (e.g., Clark and
Curran, 2007).3
Addressing these issues, we resolve the apparent
discrepancy between Ku?bler et al (2006) and Dubey
(2004) and establish a firm grammatical function
comparison of Negra and Tu?Ba-D/Z. We also ex-
tend the evaluation to a labeled dependency evalu-
ation based on grammatical relations for both cor-
pora. Such an evaluation, which abstracts away from
the specifics of the annotation schemes, shows that,
in contrast to the claims made in Ku?bler et al (2006),
the parsing results for PCFG parsers trained on these
heterogeneous corpora are very similar.
2 The corpora used
As motivated in the introduction, the work discussed
in this paper is based on two German corpora, Ne-
gra and Tu?Ba-D/Z, which differ significantly in the
syntactic representations used ? thereby offering an
interesting test bed for investigating the influence of
an annotation scheme on the parsers trained.
2.1 Negra
The Negra corpus (Brants et al, 1999) consists of
newspaper text from the Frankfurter Rundschau, a
German newspaper. Version 2 of the corpus contains
20,602 sentences. It uses the STTS tag set (Schiller
et al, 1995) for part-of-speech annotation. There are
25 non-terminal node labels and 46 edge labels.
The syntactic annotation of Negra combines fea-
tures from phrase structure grammar and depen-
3Their evaluation also introduces an additional level of com-
plexity by finding heads heuristically rather than relying on the
head labels present on some elements in each corpus.
25
dency grammar using a tree-like syntactic structure
with grammatical functions labeled on the edges of
the tree. Flat sentence structures are used in many
places to avoid attachment ambiguities and non-
branching phrases are not used.
The annotation scheme emphasizes the use of the
tree structure to encode grammatical dependencies,
representing a head and all its dependents within a
local tree regardless of whether a dependent is real-
ized near its head or not, e.g., because it has been
extraposed or fronted. Since traditional syntax trees
do not permit the crossing branches needed to li-
cense discontinuous constituents, Negra uses a ?syn-
tax graph? data structure to represent the annotation.
An example of a syntax graph with a discontinuous
constituent (VP) due to a fronted dative object (NP)
is shown in Figure 1.
Dieser
PDAT
Meinung
NN
kann
VMFIN
ich
PPER
nur
ADV
voll
ADJD
zustimmen
VVINF
.
$.
NK NK
NP
DA MO HD
VP
OCHD SB MO
S
VROOT
this opinion can I only completely agree
Figure 1: Negra tree for ?I can only agree with this opin-
ion completely.?
Negra uses flat NP and PP annotation with no
marked heads. For example, both Dieser and Mein-
ung in Figure 1 have the grammatical function label
?NK?. Since unary branching is not used in Negra, a
bare noun or pronoun argument is not dominated by
an NP node, as shown by the pronoun ich above.
A verbal head in Negra is always marked with the
edge label ?HD? and its arguments are its sisters in
the local tree. The subject is always the sister of the
finite verb, which is a daughter of S. If the finite verb
is the main verb in the clause, the objects are also its
sisters, i.e., the finite verb, subject and objects are
all daughters of S. If the main verb is an auxiliary
governing a non-finite main verb, the non-finite verb
and its objects and modifiers form a VP where the
objects are sisters of the non-finite verb as in Fig-
ure 1. The VP is then a sister of the finite verb.
The finite verb in a German declarative clause ap-
pears in the so-called verb-second position, immedi-
ately following the fronted constituent. As a result,
the VP in Negra is discontinuous whenever one of
its children has been fronted, as in the common word
orders exemplified in (1a) and (1b).
(1) a. Die
the
Tu?r
door
hat
has
Anna
Anna
wieder
again
zugeschlagen.
slammed-shut
?Anna slammed the door shut again.?
b. Wieder
again
hat
has
Anna
Anna
die
the
Tu?r
door
zugeschlagen.
slammed-shut
?Anna slammed the door shut again.?
The sentence we saw in Figure 1 contains a dis-
continuous VP with a fronted dative object (Dieser
Meinung). The dative object and a modifier (voll)
form a VP with the non-finite verb (zustimmen).
2.2 Tu?Ba-D/Z
The Tu?Ba-D/Z corpus, version 2, (Telljohann et al,
2005) consists of 22,091 sentences of newspaper
text from the German newspaper die tageszeitung.
Like Negra, it uses the STTS tag set (Schiller et al,
1995) for part-of-speech annotation. Syntactically it
uses 27 non-terminal node labels and 47 edge labels.
The syntactic annotation incorporates a topologi-
cal field analysis of the German clause (Reis, 1980;
Ho?hle, 1986), which segments a sentence into topo-
logical units depending on the position of the finite
verb (verb-first, verb-second, verb-last). In a verb-
first and verb-second sentence, the finite verb is the
left bracket (LK), whereas in a verb-last subordinate
clause, the subordinating conjunction occupies that
field. In all clauses, the non-finite verb cluster forms
the right bracket (VC), and arguments and modifiers
can appear in the middle field (MF) between the two
brackets. Extraposed material is found to the right
of the right bracket, and in a verb-second sentence
one constituent appears in the fronted field (VF) pre-
ceding the finite verb. By specifying constraints on
the elements that can occur in the different fields,
the word order in any type of German clause can be
concisely characterized.
Each clause in the Tu?Ba-D/Z corpus is divided
into topological fields at the top level, and each topo-
logical field contains phrase-level annotation. An
26
example sentence from Tu?Ba-D/Z is shown in Fig-
ure 2, where the topological fields VF, LK, MF, and
VC are visible under the SIMPX clause node.
Daf?r
PROP
wird
VAFIN
Andrea
NE
Fischer
NE
wenig
PIAT
Zeit
NN
haben
VAINF
.
$.
HD
PX
HD
VXFIN
- -
NX
- HD
NX
HD
VXINF
OA-MOD
VF
HD
LK
-
EN-ADD
OV
VC
ON OA
MF
- - - -
SIMPX
VROOT
for it will Andrea Fischer little time have
Figure 2: Tu?Ba-D/Z tree for ?Andrea Fischer will have
little time for it.?
Edge labels are used to mark heads and gram-
matical functions, even though it can be nontrivial
to figure out which grammatical function belongs
to which head given that heads and their arguments
often are in separate topological fields. For exam-
ple, in Figure 2 the subject noun chunk (NX) has
the edge label ON (object - nominative) and the ob-
ject noun chunk has the edge label OA (object - ac-
cusative); both are realized within the middle field
(MF), while the finite verb (VXFIN) marked as HD
(head) is in the left sentence bracket (LK). This is-
sue becomes relevant in section 3.4.2, discussing an
evaluation based on labeled dependency triples.
Where Negra uses discontinuous constituents,
Tu?Ba-D/Z uses special edge labels to annotate gram-
matical relations which are not locally realized. For
example, the fronted prepositional phrase (PX) in
Figure 2 has the edge label OA-MOD which needs
to be matched with the noun phrase (NX) with label
OA that is found in the MF field.
2.3 Comparing Negra and Tu?Ba-D/Z
To give an impression of how the different anno-
tation schemes affect the appearance of a typical
tree in the two corpora, Table 1 provides statistics
on average sentence length and the number of non-
terminals per sentence.
Negra Tu?Ba-D/Z
No. of Sentences 20,602 22,091
Terminals/Sentence 17.2 17.3
Non-terminals/Sentence 7.0 20.7
Table 1: General Characteristics of the Corpora
While the sentences in Negra and Tu?Ba-D/Z on
average have the same number of words, the average
Tu?Ba-D/Z sentence has nearly three times as many
non-terminal nodes as the average Negra sentence.
This difference is mainly due to the extra level of
topological fields annotation and the use of more
contoured structures in many places where Negra
uses flatter structures.
3 Experiments
The goal of the following experiments is a compar-
ison of parsing performance across different types
of evaluation metrics for parsers trained on Negra
(Ver. 2) and Tu?Ba-D/Z (Ver. 2).
3.1 Data Preparation
Following Ku?bler et al (2006), only sentences with
fewer than 35 words were used, which results in
20,002 sentences for Negra and 21,365 sentences
for Tu?Ba-D/Z. Because punctuation is not attached
within the sentence in the corpus annotation, punc-
tuation was removed.
To be able to train PCFG parsing models, it is nec-
essary to convert the syntax graphs encoding trees
with discontinuities in Negra into traditional syntax
trees. Around 30% of sentences in Negra contain at
least one discontinuity. To remove discontinuities,
we used the conversion program included with the
Negra corpus annotation tools (Brants and Plaehn,
2000), the same tool used in Ku?bler et al (2006),
which raises non-head elements to a higher tree un-
til there are no more discontinuities. For example,
for the discontinuous tree with a fronted object we
saw in Figure 1, the PP containing the fronted NP
Dieser Meinung is raised to become a daughter of
the top S node.4
Additionally, the edge labels used in both corpora
need to be folded into the node labels to become a
4An alternate method that avoids certain problems with this
raising method is discussed in Boyd (2007).
27
part of context-free grammar rules used by a PCFG
parser. In the Penn Treebank-style versions of the
corpora appropriate for training a PCFG parser, each
edge label is joined with the phrase or POS label
on the phrase or word immediately below it. Both
corpora include edge labels above all phrases and
words. However the flatter structures in Negra result
in 39 different edge labels on words while Tu?Ba-D/Z
has only 5.
Unlike Ku?bler et al (2006), which ignored edge
labels on words, we incorporate all edge labels
present in both corpora. As a consequence of this,
providing a parser with perfect lexical tags would
also provide the edge label for that word. Tu?Ba-D/Z
does not annotate grammatical functions other than
HD on words, but Negra includes many grammati-
cal functions on words. Including edge labels in the
perfect lexical tags would artificially boost the re-
sults of a grammatical function evaluation for Negra
since it amounts to providing the correct grammati-
cal function for the 38% of arguments in Negra that
are single words.
To avoid this problem, we introduced non-
branching phrasal nodes into Negra to prevent the
correct grammatical function label from being pro-
vided with the perfect lexical tag in the cases
of single-word arguments, which are mostly bare
nouns and pronouns. We added phrasal nodes above
all single-word subject, accusative object, dative ob-
ject, and genitive object5 arguments, with the cate-
gory of the inserted phrase depending on the POS
tag on the word. The introduced phrasal node is
given the word?s original grammatical function la-
bel; the grammatical function label of the word itself
becomes NK for NPs and HD for APs and VPs. In
total, 14,580 nodes were inserted into Negra in this
way. Tu?Ba-D/Z has non-branching phrases above all
single-word arguments, so that no such modification
was needed.6
3.2 Experimental Setup
We trained unlexicalized PCFG parsing models us-
ing LoPar (Schmid, 2000). Unlexicalized models
5Genitive objects are modified for the sake of consistency
among arguments even though there are too few genitive objects
to provide reliable results in the evaluation.
6The addition of edge labels to terminal POS labels results
in 337 lexical tags for Negra and 91 for Tu?Ba-D/Z.
were used to minimize the impact of other corpus
differences on parsing. A ten-fold cross validation
was performed for all experiments.7
3.3 PARSEVAL Evaluation
As a reference point for comparison with previous
work, the PARSEVAL results8 are given in Table 2.
Negra Tu?Ba-D/Z
Unlabeled Precision 78.69 89.92
Unlabeled Recall 82.29 86.48
Labeled Precision 64.08 75.36
Labeled Recall 67.01 72.47
Coverage 97.00 99.90
Table 2: PARSEVAL Evaluation
The parser trained on Tu?Ba-D/Z performs much
better than the one trained on Negra on all labeled
and unlabeled bracketing scores. As we saw in
section 2, Negra and Tu?Ba-D/Z use very different
syntactic annotation schemes, resulting in over 2.5
times as many non-terminals per sentence in Tu?Ba-
D/Z as in Negra with the additional unary nodes.
As mentioned previously, Rehbein and van Genabith
(2007a) showed that PARSEVAL is affected by the
ratio of terminal to non-terminal nodes, so these re-
sults are not expected to indicate the quality of the
parses. The comparison with grammatical function
and dependency evaluations we turn to next show-
cases that PARSEVAL does not provide a meaning-
ful evaluation metric across annotation schemes.
3.4 Dependency Evaluation
Complementing the issue of the ratio of terminals
to non-terminals raised in the last section, one can
question whether counting all brackets in the sen-
tence equally, as done by the PARSEVAL metric,
provides a good measure of how accurately the ba-
sic functor-argument structure of the sentence has
been captured in a parse. Thus, it is useful to per-
7Our experimental setup is designed to support a compari-
son between Negra and Tu?Ba-D/Z for the three evaluation met-
rics and is intended to be comparable to the setup of Ku?bler
et al (2006). For Negra, Dubey (2004) explores a range of pars-
ing models and the corpus preparation he uses differs from the
one discussed in this paper so that a discussion of his results is
beyond the scope of the corpus comparison in this paper.
8Scores were calculated using evalb.
28
form an evaluation based on the grammatical func-
tion labels that are important for determining the
functor-argument structure of the sentence: subjects,
accusative objects, and dative objects.9 The first
step in an evaluation of functor-argument structure
is to identify whether an argument bears the correct
grammatical function label.
3.4.1 Grammatical Function Label Evaluation
Ku?bler et al (2006) present the results shown in Ta-
ble 3 for the parsing performance of the unlexical-
ized model of the Stanford Parser (Klein and Man-
ning, 2002). In this grammatical function label eval-
uation, Tu?Ba-D/Z outperforms Negra for subjects,
accusative objects, and dative objects based on an
evaluation of phrasal arguments.
Negra Tu?Ba-D/Z
Prec Rec F Prec Rec F
Subj 52.50 58.02 55.26 66.82 75.93 72.38
Acc 35.14 36.30 35.72 43.84 47.31 45.58
Dat 8.38 3.58 5.98 24.46 9.96 17.21
Table 3: Grammatical Function Label Evaluation for
Phrasal Arguments from Ku?bler et al (2006)
Note that this grammatical function label evalua-
tion is restricted to labels on phrases; grammatical
function labels on words are ignored in training and
testing. This results in an unbalanced comparison
between Negra and Tu?Ba-D/Z since, as discussed
in section 2, Tu?Ba-D/Z includes unary-branching
phrases above all single-word arguments whereas
Negra does not. In effect, single-word arguments
in Negra ? mainly pronouns and bare nouns ? are
not considered in the evaluation from Ku?bler et al
(2006). The result is thus a comparison of multi-
word arguments in Negra to both single- and multi-
word arguments in Tu?Ba-D/Z. Recall from section
3.1 that this is not a minor difference: single-word
arguments account for 38% of subjects, accusative
objects, and dative objects in Negra.
As discussed in the data preparation section, Ne-
gra was modified for our experiment so as not to
9Genitive objects are also annotated in both corpora, but
they are too infrequent to provide meaningful results. As dis-
cussed in Rehbein and van Genabith (2007b), labels such as
subject (SB for Negra, ON for Tu?Ba-D/Z) are not necessarily
comparable in all instances, but such cases are infrequent.
provide the parser with the grammatical function la-
bels for single word phrases as part of the perfect
tags provided. This evaluation handles multiple cat-
egories of arguments, not just NPs, so it focuses
solely on the grammatical function labels, ignoring
the phrasal categories. For example, in Negra an NP-
OA in a parse is considered a correct accusative ob-
ject even if the OA label in the gold standard has the
category MPN. The results are shown in Table 4.
Negra Tu?Ba-D/Z
Prec Rec F Prec Rec F
Subj 69.69 69.12 69.42 65.74 72.24 68.99
Acc 48.17 50.97 49.57 41.37 46.81 44.09
Dat 20.93 15.22 18.08 21.40 11.51 16.46
Table 4: Grammatical Function Label Evaluation
In contrast to the results for NP grammatical func-
tions of Ku?bler et al (2006) we saw in Table 3, Ne-
gra and Tu?Ba-D/Z perform quite similarly overall,
with Negra slightly outperforming Tu?Ba-D/Z for all
types of arguments.
These results also form a clear contrast to the
PARSEVAL results we saw in Table 2. Contrary
to the finding in Ku?bler et al (2006), the PAR-
SEVAL evaluation does not echo the grammatical
function label evaluation. In keeping with the re-
sults from Rehbein and van Genabith (2007a), we
find that PARSEVAL is not an adequate predictor of
performance in an evaluation targeting the functor-
argument structure of the sentence for comparisons
between PCFG parsers trained on corpora with dif-
ferent annotation schemes.
3.4.2 Labeled Dependency Triple Evaluation
While determining the grammatical function of an
element is an important part of determining the
functor-argument structure of a sentence, the other
necessary component is determining the head of
each function. To evaluate whether both the functor
and the argument have been correctly found, an eval-
uation of labeled dependency triples is needed. As
in the previous section, we focus on the grammatical
function labels for arguments of verbs. To complete
a labeled dependency triple for each argument, we
additionally need to locate the lexical verbal head.
In Negra, the head is the sister of an argu-
ment marked with the function label ?HD?, however
29
heads are only marked for a subset of the phrase cat-
egories: S, VP, AP, and AVP.10 This subset includes
the phrase categories that contain verbs and their ar-
guments, S and VP. In our experiment, the parser
finds the HD grammatical function labels with a very
high f-score: 99.5% precision and 96.5% recall. If
the sister with the label HD is a word, then that word
is the lexical head for the purposes of this depen-
dency evaluation. If the sister with the label HD is
a phrase, then a recursive search for heads within
that phrase finds a lexical head. In 3.2% of cases in
the gold standard, it is not possible to find a lexical
head for an argument. Further methods could be ap-
plied to find the remaining heads heuristically, but
we avoid the additional parameters this introduces
for this evaluation by ignoring these cases.
For Tu?Ba-D/Z, finding the head is not as simple
because the verbal head and its arguments are in dif-
ferent topological fields. To create a parallel com-
parison to Negra, the finite verb from the local clause
is chosen as the head for all subjects. The (finite or
non-finite) main full verb is designated as the head
for the accusative and dative objects. It is possible
to automatically find an appropriate head verb for all
but 2.7% of subjects, accusative objects, and dative
objects.11 As with Negra, only cases where a head
verb can be found in the gold standard are consid-
ered in the evaluation.
As in the grammatical function evaluation in the
previous section, only the grammatical function la-
bel, not the phrase category is considered in the eval-
uation. The results for the labeled dependency eval-
uation are shown in Table 5. The parser trained on
Negra outperforms the one trained on Tu?Ba-D/Z for
all types of arguments.
4 Discussion of Results
Comparing PARSEVAL scores for a parser trained
on the Negra and the Tu?Ba-D/Z corpus with a gram-
matical function and a labeled dependency evalua-
10However, some strings labeled as S and VP do not contain
a head and thus lack a daughter with a HD function label.
11The relative numbers of instances where a lexical head is
not found are comparable for Negra and Tu?Ba-D/Z. Heads are
not found for approximately 4% of subjects, 1% of accusative
objects, and 1% of dative objects. These instances are fre-
quently due to elision of the verb in headlines and coordinated
clauses.
Negra Tu?Ba-D/Z
Prec Rec F Prec Rec F
Subj 72.84 69.03 70.93 60.52 65.98 63.25
Acc 47.96 48.80 48.38 37.39 40.83 39.11
Dat 19.56 14.01 16.79 19.32 10.39 14.85
Table 5: Labeled Dependency Evaluation
tion, we confirm that the PARSEVAL scores do not
correlate with the scores in the other two evalua-
tions, which given their closeness to the semantic
functor argument structure make meaningful targets
for evaluating parsers.
Shifting the focus to the grammatical function
evaluation, we showed that a grammatical function
evaluation based on phrasal arguments as provided
by Ku?bler et al (2006) is inadequate for compar-
ing parsers trained on the Negra and Tu?Ba-D/Z cor-
pora. By introducing non-branching phrase nodes
above single-word arguments in Negra, it is possi-
ble to provide a balanced comparison for the gram-
matical function label evaluation between Negra and
Tu?Ba-D/Z on both phrasal and single-word argu-
ments. The models trained on both corpora perform
very similarly in the grammatical function evalua-
tion, in contrast to the claims in Ku?bler et al (2006).
When the grammatical function label evaluation
is extended into a labeled dependency evaluation by
finding the verbal head to complete the labeled de-
pendency triple, the parser trained on Negra outper-
forms that trained on Tu?Ba-D/Z. The more signifi-
cant drop in results for Tu?Ba-D/Z compared to the
grammatical function label evaluation may be due
to the fact that a verbal lexical head in Tu?Ba-D/Z is
not in the same local tree as its dependents, whereas
it is in Negra. The presence of intervening topolog-
ical field nodes in Tu?Ba-D/Z may make it difficult
for the parser to consistently identify the elements
of the dependency triple across several subtrees.
The Negra corpus annotation scheme makes it
simple to identify the heads of verb arguments, but
the flat NP and PP structures make it difficult to ex-
tend a labeled dependency analysis beyond verb ar-
guments. On the other hand, Tu?Ba-D/Z has marked
heads in NPs and PPs, but it is not as easy to pair
verb arguments with their heads because the verbs
are in separate topological fields from their argu-
30
ments. For a constituent-based corpus annotation
scheme to lend itself to a thorough labeled depen-
dency evaluation, heads should be marked clearly
for all phrase categories and all non-head elements
need to have marked grammatical functions.
The presence of topological field nodes in Tu?Ba-
D/Z deserves more discussion in relation to a gram-
matical dependency evaluation. The corpus con-
tains two very different types of nodes in its syntac-
tic trees: nodes such as NP and PP that correspond
to constituents and nodes such as VF (Vorfeld) and
MF (Mittelfeld) that correspond to word order do-
mains. Constituents such as NP have grammatical
relations to other elements in the sentence and have
identifiable heads within them, whereas nodes en-
coding word order domains have neither.12 While
constituents and word order domains sometimes co-
incide, such as the Vorfeld normally consisting of a
single constituent, this is not the general case. For
example, the Mittelfeld often contains multiple con-
stituents which each stand in different grammatical
relations to the verb(s) in the left and right sentence
brackets (LK and VC).
Returning to the issue of finding dependencies be-
tween constituents, the intervening word order do-
main nodes can make it non-trivial to determine
these relations in Tu?Ba-D/Z. For example, word or-
der domain nodes will always intervene between a
verb and its arguments. In order to have all gram-
matical dependencies directly encoded in the tree-
bank, it would be preferable for corpus annotation
schemes to ensure that a homogeneous constituency
representation can be easily obtained.
5 Future Work
An evaluation on arguments of verbs is just a first
step in working towards a more complete labeled
dependency evaluation. Because Negra and Tu?Ba-
D/Z do not have parallel uses of many grammatical
function labels beyond arguments of verbs, a more
detailed evaluation on more types of dependency re-
lations will require a complex dependency conver-
sion method to provide comparable results.
12While the focus in this work is on unlexicalized parsing,
this also calls into question the effect of head lexicalization for
a corpus that contains elements that by their nature are not the
types of elements that have heads.
Since previous work on head-lexicalized pars-
ing models for German has focused on PARSEVAL
evaluations, it would also be useful to perform a la-
beled dependency evaluation to determine what ef-
fect head lexicalization has on particular construc-
tions for the parsers. Because of the concerns dis-
cussed in the previous section and the difference in
which types of clauses have marked heads in Negra
and Tu?Ba-D/Z, the effect of head lexicalization on
the parsing results may differ for the two corpora.
6 Conclusion
Addressing the general question of how to compare
parsing results for different annotation schemes, we
revisited the comparison of PCFG parsing results for
the Negra and Tu?Ba-D/Z corpora. We show that
these different annotation schemes lead to very sig-
nificant differences in PARSEVAL scores for un-
lexicalized PCFG parsing models, but grammatical
function label and labeled dependency evaluations
for arguments of verbs show that this difference does
not carry over to measures which are relevant to the
semantic functor-argument structure. In contrast to
Ku?bler et al (2006) a grammatical function evalua-
tion on subjects, accusative objects, and dative ob-
jects establishes that Negra and Tu?Ba-D/Z perform
similarly when all types of words and phrases ap-
pearing as arguments are taken into consideration. A
labeled dependency evaluation based on grammati-
cal relations, which links this work to current work
on formalism-independent parser evaluation (e.g.,
Clark and Curran, 2007), shows that the parsing per-
formance for Negra and Tu?Ba-D/Z is comparable.
References
Adriane Boyd, 2007. Discontinuity Revisited: An Im-
proved Conversion to Context-Free Representations.
In Proceedings of the Linguistic Annotation Workshop
(LAW). Prague, Czech Republic.
Thorsten Brants and Oliver Plaehn, 2000. Interactive
Corpus Annotation. In Proceedings of the Second In-
ternational Conference on Language Resources and
Evaluation (LREC). Athens, Greece.
Thorsten Brants, Wojciech Skut and Hans Uszkoreit,
1999. Syntactic Annotation of a German Newspaper
Corpus. In Proceedings of the ATALA Treebank Work-
shop. Paris, France.
John Carroll, Guido Minnen and Ted Briscoe, 2003.
Parser evaluation: using a grammatical relation anno-
tation scheme. In A. Abeille? (ed.), Treebanks: Build-
ing and Using Parsed Corpora, Kluwer, Dordrecht.
31
Stephen Clark and James Curran, 2007. Formalism-
Independent Parser Evaluation with CCG and Dep-
Bank. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics (ACL).
Prague, Czech Republic.
Amit Dubey, 2004. Statistical Parsing for German: Mod-
eling Syntactic Properties and Annotation Differences.
Ph.D. thesis, Universita?t des Saarlandes.
Amit Dubey and Frank Keller, 2003. Probabilistic Pars-
ing Using Sister-Head Dependencies. In Proceedings
of the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL). Sapporo, Japan.
Tilman Ho?hle, 1986. Der Begriff ?Mittelfeld?, An-
merkungen u?ber die Theorie der topologischen Felder.
In Akten des Siebten Internationalen Germanistenkon-
gresses 1985. Go?ttingen, Germany.
Dan Klein and Christopher D. Manning, 2002. Fast Exact
Inference with a Factored Model for Natural Language
Parsing. In Advances in Neural Information Process-
ing Systems 15 (NIPS). Vancouver, British Columbia,
Canada.
Sandra Ku?bler, 2005. How do treebank annotation
schemes influence parsing results? Or how not to com-
pare apples and oranges. In Proceedings of the Con-
ference on Recent Advances in Natural Language Pro-
cessing (RANLP). Borovets, Bulgaria.
Sandra Ku?bler, Erhard W. Hinrichs and Wolfgang Maier,
2006. Is it really that difficult to parse German? In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP). Sydney,
Australia.
Roger Levy and Christopher Manning, 2003. Is it harder
to parse Chinese, or the Chinese Treebank? In Pro-
ceedings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL).
Dekang Lin, 1995. A Dependency-based Method for
Evaluating Broad-Coverage Parsers. In Proceedings
of the International Joint Conference on Artificial In-
telligence (IJCAI). Montreal, Quebec, Canada.
Ines Rehbein and Josef van Genabith, 2007a. Treebank
Annotation Schemes and Parser Evaluation for Ger-
man. In Proceedings of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning. Prague,
Czech Republic.
Ines Rehbein and Josef van Genabith, 2007b. Why is it so
difficult to compare treebanks? TIGER and Tu?Ba-D/Z
revisited. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories (TLT). Bergen, Norway.
Marga Reis, 1980. On Justifying Topological Frames:
?Positional Field? and the Order of Nonverbal Con-
stituents in German. Documentation et Recherche
en Linguistique Allemande Contemporaine Vincennes
(DRLAV), 22/23.
Anne Schiller, Simone Teufel and Christine Thielen,
1995. Guidelines fu?r das Tagging deutscher Textcor-
pora mit STTS. Technical report, Universita?t Stuttgart,
Universita?t Tu?bingen, Germany.
Helmut Schmid, 2000. LoPar: Design and Implemen-
tation. Arbeitspapiere des Sonderforschungsbereiches
340 No. 149, Universita?t Stuttgart.
Heike Telljohann, ErhardW. Hinrichs, Sandra Ku?bler and
Heike Zinsmeister, 2005. Stylebook for the Tu?bingen
Treebank of Written German (Tu?Ba-D/Z). Technical
report, Seminar fu?r Sprachwissenschaft, Universita?t
Tu?bingen, Germany.
32
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 10?18,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Enhancing Authentic Web Pages for Language Learners
Detmar Meurers1, Ramon Ziai1,
Luiz Amaral2, Adriane Boyd3, Aleksandar Dimitrov1, Vanessa Metcalf3, Niels Ott1
1 Universita?t Tu?bingen
2 University of Massachusetts Amherst
3 The Ohio State University
Abstract
Second language acquisition research since
the 90s has emphasized the importance of
supporting awareness of language categories
and forms, and input enhancement techniques
have been proposed to make target language
features more salient for the learner.
We present an NLP architecture and web-
based implementation providing automatic vi-
sual input enhancement for web pages. Learn-
ers freely choose the web pages they want to
read and the system displays an enhanced ver-
sion of the pages. The current system supports
visual input enhancement for several language
patterns known to be problematic for English
language learners, as well as fill-in-the-blank
and clickable versions of such pages support-
ing some learner interaction.
1 Introduction
A significant body of research into the effectiveness
of meaning-focused communicative approaches to
foreign language teaching has shown that input
alone is not sufficient to acquire a foreign lan-
guage, especially for older learners (cf., e.g., Light-
bown and Spada, 1999). Recognizing the important
role of consciousness in second-language learning
(Schmidt, 1990), learners have been argued to ben-
efit from (Long, 1991) or even require (Lightbown,
1998) a so-called focus on form to overcome incom-
plete or incorrect knowledge of specific forms or
regularities. Focus on form is understood to be ?an
occasional shift of attention to linguistic code fea-
tures? (Long and Robinson, 1998, p. 23).
In an effort to combine communicative and struc-
turalist approaches to second language teaching,
Rutherford and Sharwood Smith (1985) argued for
the use of consciousness raising strategies drawing
the learner?s attention to specific language proper-
ties. Sharwood Smith (1993, p. 176) coined the term
input enhancement to refer to strategies highlighting
the salience of language categories and forms.
Building on this foundational research in second
language acquisition and foreign language teaching,
in this paper we present an NLP architecture and a
system for automatic visual input enhancement of
web pages freely selected by language learners. We
focus on learners of English as a Second Language
(ESL), and the language patterns enhanced by the
system include some of the well-established diffi-
culties: determiners and prepositions, the distinction
between gerunds and to-infinitives, wh-question for-
mation, tense in conditionals, and phrasal verbs.
In our approach, learners can choose any web
page they like, either by using an ordinary search-
engine interface to search for one or by entering the
URL of the page they want to enhance. In contrast to
textbooks and other pre-prepared materials, allow-
ing the learner to choose up-to-date web pages on
any topic they are interested in and enhancing the
page while keeping it intact (with its links, multi-
media, and other components working) clearly has
a positive effect on learner motivation. Input en-
hanced web pages also are attractive for people out-
side a traditional school setting, such as in the vol-
untary, self-motivated pursuit of knowledge often
referred to as lifelong learning. The latter can be
particularly relevant for adult immigrants, who are
10
already functionally living in the second language
environment, but often stagnate in their second lan-
guage acquisition and lack access or motivation to
engage in language classes or other explicit lan-
guage learning activities. Nevertheless, they do use
the web to obtain information that is language-based
and thus can be enhanced to also support language
acquisition while satisfying information needs.
In terms of paper organization, in section 2 we
first present the system architecture and in 2.1 the
language phenomena handled, before considering
the issues involved in evaluating the approach in 2.2.
The context of our work and related approaches are
discussed in section 3, and we conclude and discuss
several avenues for future research in section 4.
2 The Approach
The WERTi system (Working with English Real
Texts interactively) we developed follows a client-
server paradigm where the server is responsible for
fetching the web page and enriching it with annota-
tions, and the client then receives the annotated web
page and transforms it into an enhanced version.
The client here is a standard web browser, so on the
learner?s side no additional software is needed.
The system currently supports three types of input
enhancement: i) color highlighting of the pattern or
selected parts thereof, ii) a version of the page sup-
porting identification of the pattern through clicking
and automatic color feedback, and iii) a version sup-
porting practice, such as a fill-in-the-blank version
of the page with automatic color feedback.
The overall architecture is shown in Figure 1.
Essentially, the automated input enhancement pro-
cess consists of the following steps:
1. Fetch the page.
2. Find the natural language text portions in it.
3. Identify the targeted language pattern.
4. Annotate the web page, marking up the lan-
guage patterns identified in the previous step.
5. Transform the annotated web page into the out-
put by visually enhancing the targeted pattern
or by generating interaction possibilities.
Steps 1?4 take place on the server side, whereas step
5 happens in the learner?s browser.1 As NLP is only
involved in step 3, we here focus on that step.
1As an alternative to the server-based fetching of web pages,
Server
UIMA                     
Browser                                         
URL Fetching
HTML Annotation
Identifying text in HTML page
Tokenization
Sentence Boundary Detection
POS Tagging
Pattern-specific NLP
Colorize Click Practice
Figure 1: Overall WERTi architecture. Grey components
are the same for all patterns and activities, cf. section 2.1.
While the first prototype of the WERTi system2
presented at CALICO (Amaral, Metcalf and Meur-
ers, 2006) and EUROCALL (Metcalf and Meurers,
2006) was implemented in Python, the current sys-
tem is Java-based, with all NLP being integrated in
the UIMA framework (Ferrucci and Lally, 2004).
UIMA is an architecture for the management and
analysis of unstructured information such as text,
which is built on the idea of referential annotation
and can be seen as an NLP analysis counterpart
to current stand-off encoding standards for anno-
tated corpora (cf., e.g., Ide et al 2000). The input
we are developing a Firefox plugin, leaving only the NLP up to
the server. This increases compatibility with web pages using
dynamically generated contents and special session handling.
2http://purl.org/icall/werti-v1
11
can be monotonically enriched while passing from
one NLP component to the next, using a flexible
data repository common to all components (Go?tz
and Suhre, 2004). Such annotation-based processing
is particularly useful in the WERTi context, where
keeping the original text intact is essential for dis-
playing it in enhanced form.
A second benefit of using the UIMA framework is
that it supports a flexible combination of individual
NLP components into larger processing pipelines.
To obtain a flexible approach to input enhancement
in WERTi, we need to be able to identify and an-
alyze phenomena from different levels of linguistic
analysis. For example, lexical classes can be iden-
tified by a POS tagger, whereas other patterns to be
enhanced require at least shallow syntactic chunk-
ing. The more diverse the set of phenomena, the
less feasible it is to handle all of them within a
single processing strategy or formalism. Using the
UIMA framework, we can re-use the same basic
processing (e.g., tokenizing, POS tagging) for all
phenomena and still be able to branch into pattern-
specific NLP in a demand-driven way. Given that
NLP components in UIMA include self-describing
meta-information, the processing pipeline to be run
can dynamically be obtained from the module con-
figuration instead of being hard-wired into the core
system. The resulting extensible, plugin-like archi-
tecture seems particularly well-suited for the task of
visual input enhancement of a wide range of hetero-
geneous language properties.
Complementing the above arguments for the
UIMA-based architecture of the current WERTi sys-
tem, a detailed discussion of the advantages of an
annotation-based, demand-driven NLP architecture
for Intelligent Computer-Assisted Language Learn-
ing can be found in Amaral, Meurers, and Ziai (To
Appear), where it is employed in an Intelligent Lan-
guage Tutoring System.
2.1 Implemented Modules
The modules implemented in the current system
handle a number of phenomena commonly judged
as difficult for second language learners of English.
In the following we briefly characterize each mod-
ule, describing the nature of the language pattern,
the required NLP, and the input enhancement results,
which will be referred to as activities.
Lexical classes
Lexical classes are the most basic kind of linguis-
tic category we use for input enhancement. The in-
ventory of lexical categories to be used and which
ones to focus on should be informed by second
language acquisition research and foreign language
teaching needs. The current system focuses on func-
tional elements such as prepositions and determiners
given that they are considered to be particularly dif-
ficult for learners of English (cf. De Felice, 2008 and
references therein).
We identify these functional elements using the
LingPipe POS tagger (http://alias-i.com/
lingpipe) employing the Brown tagset (Francis
and Kucera, 1979). As we show in section 2.2, the
tagger reliably identifies prepositions and determin-
ers in native English texts such as those expected for
input enhancement.
The input enhancement used for lexical classes is
the default set of activities provided by WERTi. In
the simplest case, Color, all automatically identified
instances in the web page are highlighted by color-
ing them; no learner interaction is required. This is
illustrated by Figure 2, which shows the result of en-
hancing prepositions in a web page from the British
Figure 2: Screenshot of color activity for prepositions, cf.
http://purl.org/icall/werti-color-ex
12
newspaper The Guardian.3
In this and the following screenshots, links al-
ready present in the original web page appear in light
blue (e.g., Vauban in Germany). This raises an im-
portant issue for future research, namely how to de-
termine the best visual input enhancement for a par-
ticular linguistic pattern given a specific web page
with its existing visual design features (e.g., bold-
facing in the text or particular colors used to indicate
links), which includes the option of removing or al-
tering some of those original visual design features.
A more interactive activity type is Click, where
the learner during reading can attempt to identify in-
stances of the targeted language form by clicking on
it. Correctly identified instances are colored green
by the system, incorrect guesses red.
Thirdly, input can be turned into Practice activi-
ties, where in its simplest form, WERTi turns web
pages into fill-in-the-blank activities and provides
immediate color coded feedback for the forms en-
tered by the learner. The system currently accepts
only the form used in the original text as correct.
In principle, alternatives (e.g., other prepositions)
can also be grammatical and appropriate. The ques-
tion for which cases equivalence classes of target an-
swers can automatically be determined is an interest-
ing question for future research.4
Gerunds vs. to-infinitives
Deciding when a verb is required to be realized as
a to-infinitive and when as a gerund -ing form can be
difficult for ESL learners. Current school grammars
teach students to look for certain lexical clues that
reliably indicate which form to choose. Examples
of such clues are prepositions such as after and of,
which can only be followed by a gerund.
In our NLP approach to this language pattern, we
use Constraint Grammar rules (Karlsson et al, 1995)
on top of POS tagging, which allow for straightfor-
ward formulation of local disambiguation rules such
as: ?If an -ing form immediately follows the prepo-
sition by, select the gerund reading.? Standard POS
3Given the nature of the input enhancement using colors, the
highlighting in the figure is only visible in a color printout.
4The issue bears some resemblance to the task of identify-
ing paraphrases (Androutsopoulos and Malakasiotis, 2009) or
classes of learner answers which differ in form but are equiva-
lent in terms of meaning (Bailey and Meurers, 2008).
tagsets for English contain a single tag for all -ing
forms. In order to identify gerunds only, we in-
troduce all possible readings for all -ing forms and
wrote 101 CG rules to locally disambiguate them.
The to-infinitives, on the other hand, are relatively
easy to identify based on the surface form and re-
quire almost no disambiguation.
For the implementation of the Constraint Gram-
mar rules, we used the freely available CG3 system.5
While simple local disambiguation rules are suffi-
cient for the pattern discussed here, through iterative
application of rules, Constraint Grammar can iden-
tify a wide range of phenomena without the need to
provide a full grammatical analysis.
The Color activity resulting from input enhance-
ment is similar to that for lexical classes described
above, but the system here enhances both verb forms
and clue phrases. Figure 3 shows the system high-
lighting gerunds in orange, infinitives in purple, and
clue phrases in blue.
Figure 3: Color activity for gerunds vs. to-infinitives, cf.
http://purl.org/icall/werti-color-ex2
For the Click activity, the web page is shown
with colored gerund and to-infinitival forms and the
learner can click on the corresponding clue phrases.
For the Practice activity, the learner is presented
with a fill-in-the-black version of the web page, as
in the screenshot in Figure 4. For each blank, the
learner needs to enter the gerund or to-infinitival
form of the base form shown in parentheses.
Wh-questions
Question formation in English, with its particu-
lar word order, constitutes a well-known challenge
for second language learners and has received sig-
nificant attention in the second language acquisi-
5http://beta.visl.sdu.dk/cg3.html
13
Figure 4: Practice activity for gerunds vs. to-infinitives,
cf. http://purl.org/icall/werti-cloze-ex
tion literature (cf., e.g., White et al, 1991; Spada
and Lightbown, 1993). Example (1) illustrates the
use of do-support and subject-aux inversion in wh-
questions as two aspects challenging learners.
(1) What do you think it takes to be successful?
In order to identify the wh-question patterns, we
employ a set of 126 hand-written Constraint Gram-
mar rules. The respective wh-word acts as the lex-
ical clue to the question as a whole, and the rules
then identify the subject and verb phrase based on
the POS and lexical information of the local context.
Aside from the Color activity highlighting the rel-
evant parts of a wh-question, we adapted the other
activity types to this more complex language pattern.
The Click activity prompts learners to click on either
the subject or the verb phrase of the question. The
Practice activity presents the words of a wh-question
in random order and requires the learner to rearrange
them into the correct one.
Conditionals
English has five types of conditionals that are used
for discussing hypothetical situations and possible
outcomes. The tenses used in the different condi-
tional types vary with respect to the certainty of the
outcome as expressed by the speaker/writer. For ex-
ample, one class of conditionals expresses high cer-
tainty and uses present tense in the if -clause and fu-
ture in the main clause, as in example (2).
(2) If the rain continues, we will return home.
The recognition of conditionals is approached us-
ing a combination of shallow and deep methods. We
first look for lexical triggers of a conditional, such as
the word if at the beginning of a sentence. This first
pass serves as a filter to the next, more expensive
processing step, full parsing of the candidate sen-
tences using Bikel?s statistical parser (Bikel, 2002).
The parse trees are then traversed to identify and
mark the verb forms and the trigger word.
For the input enhancement, we color all relevant
parts of a conditional, namely the trigger and the
verb forms. The Click activity for conditionals re-
quires the learner to click on exactly these parts. The
Practice activity prompts users to classify the condi-
tional instances into the different classes.
Phrasal verbs
Another challenging pattern for English language
learners are phrasal verbs consisting of a verb and
either a preposition, an adverb or both. The meaning
of a phrasal verb often differs considerably from that
of the underlying verb, as in (3) compared to (4).
(3) He switched the glasses without her noticing.
(4) He switched off the light before he went to bed.
This distinction is difficult for ESL learners, who
often confuse phrasal and non-phrasal uses.
Since this is a lexical phenomenon, we ap-
proached the identification of phrasal verbs via a
database lookup in a large online collection of verbs
known to occur in phrasal form.6 In order to find out
about noun phrases and modifying adverbs possibly
occurring in between the verb and its particles, we
run a chunker and use this information in specifying
a filter for such intervening elements.
The visual input enhancement activities targeting
phrasal verbs are the same as for lexical classes, with
the difference that for the Practice activity, learners
have to fill in only the particle, not the particle and
the main verb, since otherwise the missing contents
may be too difficult to reconstruct. Moreover, we
want the activity to focus on distinguishing phrasal
from non-phrasal uses, not verb meaning in general.
2.2 Evaluation issues
The success of a visual input enhancement approach
such as the one presented in this paper depends on
a number of factors, each of which can in principle
6http://www.usingenglish.com/reference/
phrasal-verbs
14
be evaluated. The fundamental but as far as we are
aware unanswered question in second language ac-
quisition research is for which language categories,
forms, and patterns input enhancement can be effec-
tive. As Lee and Huang (2008) show, the study of
visual input enhancement sorely needs more experi-
mental studies. With the help of the WERTi system,
which systematically produces visual input enhance-
ment for a range of language properties, it becomes
possible to conduct experiments in a real-life foreign
language teaching setting to test learning outcomes7
with and without visual input enhancement under a
wide range of parameters. Relevant parameters in-
clude the linguistic nature of the language property
to be enhanced as well as the nature of the input en-
hancement to be used, be it highlighting through col-
ors or fonts, engagement in different types of activi-
ties such as clicking, entering fill-in-the-blank infor-
mation, reordering language material, etc.
A factor closely related to our focus in this pa-
per is the impact of the quality of the NLP analysis.8
For a quantitative evaluation of the NLP, one signif-
icant problem is the mismatch between the phenom-
ena focused on in second language learning and the
available gold standards where these phenomena are
actually annotated. For example, standard corpora
such as the Penn Treebank contain almost no ques-
tions and thus do not constitute a useful gold stan-
dard for wh-question identification. Another prob-
lem is that some grammatical distinctions taught to
language learners are disputed in the linguistic liter-
ature. For example, Huddleston and Pullum (2002,
p. 1120) eliminate the distinction between gerunds
and present participles, combining them into a class
called ?gerund-participle?. And in corpus annota-
tion practice, gerunds are not identified as a class by
the tagsets used to annotate large corpora, making it
unclear what gold standard our gerund identification
component should be evaluated against.
While the lack of available gold standards means
that a quantitative evaluation of all WERTi mod-
ules is beyond the scope of this paper, the deter-
miner and preposition classes focused on in the lex-
ical classes module can be identified using the stan-
7Naturally, online measures of noticing, such as eye tracking
or Event-Related Potentials (ERP) would also be relevant.
8The processing time for the NLP analysis as other relevant
aspect is negligible for most of the activities presented here.
dard CLAWS-7 or Brown tagsets, for which gold-
standard corpora are available. We thus decided
to evaluate this WERTi module against the BNC
Sampler Corpus (Burnard, 1999), which contains
a variety of genres, making it particularly appro-
priate for evaluating a tool such as WERTi, which
learners are expected to use with a wide range of
web pages as input. The BNC Sampler corpus is
annotated with the fine-grained CLAWS-7 tagset9
where, e.g., prepositions are distinguished from sub-
ordinating conjunctions. By mapping the relevant
POS tags from the CLAWS-7 tagset to the Brown
tagset used by the LingPipe tagger as integrated in
WERTi, it becomes possible to evaluate WERTi?s
performance for the specific lexical classes focused
on for input enhancement, prepositions and deter-
miners. For prepositions, precision was 95.07% and
recall 90.52% while for determiners, precision was
97.06% with a recall of 94.07%.
The performance of the POS tagger on this refer-
ence corpus thus seems to be sufficient as basis for
visual input enhancement, but the crucial question
naturally remains whether identification of the target
patterns is reliable in the web pages that language
learners happen to choose. For a more precise quan-
titative study, it will thus be important to try the sys-
tem out with real-life users in order to identify a set
of web pages which can constitute an adequate test
set. Interestingly, which web pages the users choose
depends on the search engine front-end we provide
for them. As discussed under outlook in section 4,
we are exploring the option to implicitly guide them
towards web pages containing enough instances of
the relevant language patterns in text at the appro-
priate reading difficulty.
3 Context and related work
Contextualizing our work, one can view the auto-
matic visual input enhancement approach presented
here as an enrichment of Data-Driven Learning
(DDL). Where DDL has been characterized as an
?attempt to cut out the middleman [the teacher] as
far as possible and to give the learner direct access
to the data? (Boulton 2009, p. 82, citing Tim Johns),
in visual input enhancement the learner stays in con-
9http://www.natcorp.ox.ac.uk/docs/
c7spec.html
15
trol, but the NLP uses ?teacher knowledge? about rel-
evant and difficult language properties to make those
more prominent and noticeable for the learner.
In the context of Intelligent Computer-Assisted
Language Learning (ICALL), NLP has received
most attention in connection with Intelligent Lan-
guage Tutoring Systems, where NLP is used to ana-
lyze learner data and provide individual feedback on
that basis (cf. Heift and Schulze, 2007). Demands
on such NLP are high given that it needs to be able
to handle learner language and provide high-quality
feedback for any sentence entered by the learner.
In contrast, visual input enhancement makes use
of NLP analysis of authentic, native-speaker text and
thus applies the tools to the native language they
were originally designed and optimized for. Such
NLP use, which we will refer to as Authentic Text
ICALL (ATICALL), also does not need to be able
to correctly identify and manipulate all instances of
a language pattern for which input enhancement is
intended. Success can be incremental in the sense
that any visual input enhancement can be beneficial,
so that one can focus on enhancing those instances
which can be reliably identified in a text. In other
words, for ATICALL, precision of the NLP tools is
more important than recall. It is not necessary to
identify and enhance all instances of a given pattern
as long as the instances we do identify are in fact
correct, i.e., true positives. As the point of our sys-
tem is to enhance the reading experience by raising
language awareness, pattern occurrences we do not
identify are not harmful to the overall goal.10
We next turn to a discussion of some interest-
ing approaches in two closely related fields, exercise
generation and reading support tools.
3.1 Exercise Generation
Exercise generation is widely studied in CALL re-
search and some of the work relates directly to the
input enhancement approach presented in this paper.
For instance, Antoniadis et al (2004) describe the
plans of the MIRTO project to support ?gap-filling?
and ?lexical spotting? exercises in combination with
a corpus database. However, MIRTO seems to fo-
10While identifying all instances of a pattern indeed is not
crucial in this context, representativeness remains relevant to
some degree. Where only a skewed subset of a pattern is high-
lighted, learners may not properly conceptualize the pattern.
cus on a general architecture supporting instructor-
determined activity design. Visual input enhance-
ment or language awareness are not mentioned. The
VISL project (Bick, 2005) offers games and visual
presentations in order to foster knowledge of syntac-
tic forms and rules, and its KillerFiller tool can cre-
ate slot-filler exercises from texts. However, Killer-
Filler uses corpora and databases as the text base and
it presents sentences in isolation in a testing setup.
In contrast to such exercise generation systems, we
aim at enhancing the reader?s second language input
using the described web-based mash-up approach.
3.2 Reading Support Tools
Another branch of related approaches consists of
tools supporting the reading of texts in a foreign lan-
guage. For example, the Glosser-RuG project (Ner-
bonne et al, 1998) supports reading of French texts
for Dutch learners with an online, context-dependent
dictionary, as well as morphological analysis and ex-
amples of word use in corpora. A similar system,
focusing on multi-word lexemes, was developed in
the COMPASS project (Breidt and Feldweg, 1997).
More recently, the ALPHEIOS project11 has pro-
duced a system that can look up words in a lexi-
con and provide aligned translations. While such
lexicon-based tools are certainly useful to learners,
they rely on the learner asking for help instead of
enhancing specific structures from the start and thus
clearly differ from our approach.
Finally, the REAP project12 supports learners in
searching for texts that are well-suited for provid-
ing vocabulary and reading practice (Heilman et al,
2008). While it differs in focus from the visual input
enhancement paradigm underlying our approach, it
shares with it the emphasis on providing the learner
with authentic text in support of language learning.
4 Conclusion and Outlook
In this paper we presented an NLP architecture and
a concrete system for the enhancement of authen-
tic web pages in order to support language aware-
ness in ESL learners. The NLP architecture is flexi-
ble enough to integrate any processing approach that
lends itself to the treatment of the language phe-
11http://alpheios.net
12http://reap.cs.cmu.edu
16
nomenon in question, without confining the devel-
oper to a particular formalism. The WERTi system
illustrates this with five language patterns typically
considered difficult for ESL learners: lexical classes,
gerunds vs. to-infinitives, wh-questions, condition-
als and phrasal verbs.
Looking ahead, we already mentioned the funda-
mental open question where input enhancement can
be effective in section 2.2. A system such as WERTi,
systematically producing visual input enhancement,
can help explore this question under a wide range of
parameters in a real-life language teaching setting.
A more specific future research issue is the auto-
matic computation of equivalence classes of target
forms sketched in section 2.1. Not yet mentioned
but readily apparent is the goal to integrate more
language patterns known to be difficult for language
learners into WERTi (e.g., active/passive, tense and
aspect distinctions, relative clauses), and to explore
the approach for other languages, such as German.
A final important avenue for future research con-
cerns the starting point of the system, the step where
learners search for a web page they are interested
in and select it for presentation with input enhance-
ment. Enhancing of patterns presupposes that the
pages contain instances of the pattern. The less
frequent the pattern, the less likely we are to find
enough instances of it in web pages returned by the
standard web search engines typically used by learn-
ers to find pages of interest to them. The issue is re-
lated to research on providing learners with texts at
the right level of reading difficulty (Petersen, 2007;
Miltsakaki and Troutt, 2008), but the focus for us
is on ensuring that texts which include instances of
the specific language pattern targeted by a given in-
put enhancement are ranked high in the search re-
sults. Ott (2009) presents a search engine prototype
which, in addition to the content-focused document-
term information and traditional readability mea-
sures, supports indexing based on a more general no-
tion of a text model into which the patterns relevant
to input enhancement can be integrated ? an idea we
are exploring further (Ott and Meurers, Submitted).
Acknowledgments
We benefited from the feedback we received at
CALICO 06, EUROCALL 06, and the ICALL
course13 at ESSLLI 09, where we discussed our
work on the Python-based WERTi prototype. We
would like to thank Chris Hill and Kathy Corl
for their enthusiasm and encouragement. We are
grateful to Magdalena Leshtanska, Emma Li, Iliana
Simova, Maria Tchalakova and Tatiana Vodolazova
for their good ideas and WERTi module contribu-
tions in the context of a seminar at the University of
Tu?bingen in Summer 2008. Last but not least, the
paper benefited from two helpful workshop reviews.
References
Luiz Amaral, Vanessa Metcalf, and Detmar Meur-
ers. 2006. Language awareness through re-use
of NLP technology. Presentation at the CALICO
Workshop on NLP in CALL ? Computational and
Linguistic Challenges, May 17, 2006. University
of Hawaii. http://purl.org/dm/handouts/
calico06-amaral-metcalf-meurers.pdf.
Luiz Amaral, Detmar Meurers, and Ramon Ziai.
To Appear. Analyzing learner language: To-
wards a flexible NLP architecture for intelligent
language tutors. Computer-Assisted Language
Learning. http://purl.org/dm/papers/
amaral-meurers-ziai-10.html.
Ion Androutsopoulos and Prodromos Malakasiotis.
2009. A survey of paraphrasing and textual entailment
methods. Technical report, NLP Group, Informatics
Dept., Athens University of Economics and Business,
Greece. http://arxiv.org/abs/0912.3747.
Georges Antoniadis, Sandra Echinard, Olivier Kraif,
Thomas Lebarbe?, Mathieux Loiseau, and Claude Pon-
ton. 2004. NLP-based scripting for CALL activities.
In Proceedings of the COLING Workshop on eLearn-
ing for CL and CL for eLearning, Geneva.
Stacey Bailey and Detmar Meurers. 2008. Diagnosing
meaning errors in short answers to reading compre-
hension questions. In (Tetreault et al, 2008), pages
107?115.
Eckhard Bick. 2005. Grammar for fun: IT-based gram-
mar learning with VISL. In P. Juel, editor, CALL for
the Nordic Languages, pages 49?64. Samfundslitter-
atur, Copenhagen.
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In Pro-
ceedings of the Second Int. Conference on Human
Language Technology Research, San Francisco.
Alex Boulton. 2009. Data-driven learning: Reasonable
fears and rational reassurance. Indian Journal of Ap-
plied Linguistics, 35(1):81?106.
13http://purl.org/dm/09/esslli/
17
Elisabeth Breidt and Helmut Feldweg. 1997. Accessing
foreign languages with COMPASS. Machine Transla-
tion, 12(1?2):153?174.
L. Burnard, 1999. Users Reference Guide for the BNC
Sampler. Available on the BNC Sampler CD.
Rachele De Felice. 2008. Automatic Error Detection in
Non-native English. Ph.D. thesis, St Catherine?s Col-
lege, University of Oxford.
Catherine Doughty and J. Williams, editors. 1998. Fo-
cus on form in classroom second language acquisition.
Cambridge University Press, Cambridge.
David Ferrucci and Adam Lally. 2004. UIMA: an ar-
chitectural approach to unstructured information pro-
cessing in the corporate research environment. Natu-
ral Language Engineering, 10(3?4):327?348.
W. Nelson Francis and Henry Kucera, 1979. Brown cor-
pus manual. Dept. of Linguistics, Brown University.
Thilo Go?tz and Oliver Suhre. 2004. Design and im-
plementation of the UIMA Common Analysis System.
IBM Systems Journal, 43(3):476?489.
Trude Heift and Mathias Schulze. 2007. Errors and In-
telligence in Computer-Assisted Language Learning:
Parsers and Pedagogues. Routledge.
Michael Heilman, Le Zhao, Juan Pino, and Maxine Eske-
nazi. 2008. Retrieval of reading materials for vocab-
ulary and reading practice. In (Tetreault et al, 2008),
pages 80?88.
Rodney Huddleston and Geoffrey K. Pullum. 2002. The
Cambridge Grammar of the English Language. Cam-
bridge University Press.
Nancy Ide, Patrice Bonhomme, and Laurent Romary.
2000. XCES: An XML-based encoding standard for
linguistic corpora. In Proceedings of the 2nd Int. Con-
ference on Language Resources and Evaluation.
Tim Johns. 1994. From printout to handout: Grammar
and vocabulary teaching in the context of data-driven
learning. In T. Odlin, editor, Perspectives on Pedagog-
ical Grammar, pages 293?313. CUP, Cambridge.
Fred Karlsson, Atro Voutilainen, Juha Heikkila?, and
Arto Anttila, editors. 1995. Constraint Grammar:
A Language-Independent System for Parsing Unre-
stricted Text. Mouton de Gruyter, Berlin, New York.
Sang-Ki Lee and Hung-Tzu Huang. 2008. Visual in-
put enhancement and grammar learning: A meta-
analytic review. Studies in Second Language Acqui-
sition, 30:307?331.
Patsy M. Lightbown and Nina Spada. 1999. How lan-
guages are learned. Oxford University Press, Oxford.
Patsy M. Lightbown. 1998. The importance of timing
in focus on form. In (Doughty and Williams, 1998),
pages 177?196.
Michael H. Long and Peter Robinson. 1998. Focus on
form: Theory, research, and practice. In (Doughty and
Williams, 1998), pages 15?41.
M. H. Long. 1991. Focus on form: A design feature
in language teaching methodology. In K. De Bot,
C. Kramsch, and R. Ginsberg, editors, Foreign lan-
guage research in cross-cultural perspective, pages
39?52. John Benjamins, Amsterdam.
Vanessa Metcalf and Detmar Meurers. 2006.
Generating web-based English preposition
exercises from real-world texts. Presenta-
tion at EUROCALL, Sept. 7, 2006. Granada,
Spain. http://purl.org/dm/handouts/
eurocall06-metcalf-meurers.pdf.
Eleni Miltsakaki and Audrey Troutt. 2008. Real time
web text classification and analysis of reading diffi-
culty. In (Tetreault et al, 2008), pages 89?97.
John Nerbonne, Duco Dokter, and Petra Smit. 1998.
Morphological processing and computer-assisted lan-
guage learning. Computer Assisted Language Learn-
ing, 11(5):543?559.
Niels Ott and Detmar Meurers. Submitted. Information
retrieval for education: Making search engines lan-
guage aware. http://purl.org/dm/papers/
ott-meurers-10.html.
Niels Ott. 2009. Information retrieval for language learn-
ing: An exploration of text difficulty measures. Mas-
ter?s thesis, International Studies in Computational
Linguistics, University of Tu?bingen.
Sarah E. Petersen. 2007. Natural Language Processing
Tools for Reading Level Assessment and Text Simplifi-
cation for Bilingual Education. Ph.D. thesis, Univer-
sity of Washington.
William E. Rutherford and Michael Sharwood Smith.
1985. Consciousness-raising and universal grammar.
Applied Linguistics, 6(2):274?282.
Richard W. Schmidt. 1990. The role of conscious-
ness in second language learning. Applied Linguistics,
11:206?226.
Michael Sharwood Smith. 1993. Input enhancement in
instructed SLA: Theoretical bases. Studies in Second
Language Acquisition, 15:165?179.
Nina Spada and Patsy M. Lightbown. 1993. Instruction
and the development of questions in l2 classrooms.
Studies in Second Language Acquisition, 15:205?224.
Joel Tetreault, Jill Burstein, and Rachele De Felice, ed-
itors. 2008. Proceedings of the Third Workshop on
Innovative Use of NLP for Building Educational Ap-
plications. ACL, Columbus, Ohio, June.
Lydia White, Nina Spada, Patsy M. Lightbown, and Leila
Ranta. 1991. Input enhancement and L2 question for-
mation. Applied Linguistics, 12(4):416?432.
18
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 208?215,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Informing Determiner and Preposition Error Correction with Word Clusters
Adriane Boyd Marion Zepf Detmar Meurers
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
{adriane,mzepf,dm}@sfs.uni-tuebingen.de
Abstract
We extend our n-gram-based data-driven pre-
diction approach from the Helping Our Own
(HOO) 2011 Shared Task (Boyd and Meur-
ers, 2011) to identify determiner and preposi-
tion errors in non-native English essays from
the Cambridge Learner Corpus FCE Dataset
(Yannakoudakis et al, 2011) as part of the
HOO 2012 Shared Task. Our system focuses
on three error categories: missing determiner,
incorrect determiner, and incorrect preposi-
tion. Approximately two-thirds of the errors
annotated in HOO 2012 training and test data
fall into these three categories. To improve
our approach, we developed a missing deter-
miner detector and incorporated word cluster-
ing (Brown et al, 1992) into the n-gram pre-
diction approach.
1 Introduction
We extend our n-gram-based prediction approach
(Boyd and Meurers, 2011) from the HOO 2011
Shared Task (Dale and Kilgarriff, 2011) for the HOO
2012 Shared Task. This approach is an extension
of the preposition prediction approach presented
in Elghafari, Meurers and Wunsch (2010), which
uses a surface-based approach to predict preposi-
tions in English using frequency information from
web searches to choose the most likely preposition
in a given context. For each preposition in the text,
the prediction algorithm considers up to three words
of context on each side of the preposition, building
a 7-gram with a preposition slot in the middle:
rather a question the scales falling
For each prediction task, a cohort of queries is con-
structed with each of the candidate prepositions in
the slot to be predicted:
1. rather a question of the scales falling
2. rather a question to the scales falling
3. rather a question in the scales falling
. . .
9. rather a question on the scales falling
In Elghafari, Meurers and Wunsch (2010), the
queries are submitted to the Yahoo search engine
and in Boyd and Meurers (2011), the search engine
is replaced with the ACL Anthology Reference Cor-
pus (ARC, Bird et al, 2008), which contains texts of
the same genre as the HOO 2011 data. If no hits are
found for any of the 7-gram queries, shorter over-
lapping n-grams are used to approximate the 7-gram
query. For instance, a 7-gram may be approximated
by two overlapping 6-grams:
[rather a question of the scales falling]
?
[rather a question of the scales]
[a question of the scales falling]
If there are still no hits, the overlap backoff will
continue reducing the n-gram length until it reaches
3-grams with one word of context on each side of
the candidate correction. If no hits are found at
the 3-gram level, the Boyd and Meurers (2011) ap-
proach predicts the original token, effectively mak-
ing no modifications to the original text. The ap-
proach from Elghafari, Meurers and Wunsch (2010),
addressing a prediction task rather than a correction
task (i.e., the original token is masked), predicted the
most frequent preposition of if no hits were found.
208
Elghafari, Meurers and Wunsch (2010) showed
this surface-based approach to be competitive with
published state-of-the-art machine learning ap-
proaches using complex feature sets (Gamon et al,
2008; De Felice, 2008; Tetreault and Chodorow,
2008; Bergsma et al, 2009). For a set of nine fre-
quent prepositions (of, to, in, for, on, with, at, by,
from), they accurately predicted 76.5% on native
data from section J of the British National Corpus.
For these nine prepositions, De Felice (2008) iden-
tified a baseline of 27% for the task of choosing
a preposition in a slot (choose of ) and her system
achieved 70.1% accuracy. Humans performing the
same task agree 89% of the time (De Felice, 2008).
For the academic texts in the HOO 2011 Shared
Task, Boyd and Meurers (2011) detected 67% of de-
terminer and preposition substitution errors (equiva-
lent to detection recall in the current task) and pro-
vided the appropriate correction for approximately
half of the detected cases. We achieved a detection
F-score of approximately 80% and a correction F-
score of 44% for the four function word prediction
tasks we considered (determiners, prepositions, con-
junctions, and quantifiers).
2 Our Approach
For the 2012 shared task corpus, we do not have
the advantage of access to a genre-specific reference
corpus such as the ARC used for the first challenge,
so we instead use the Google Web 1T 5-gram Cor-
pus (Web1T5, Brants and Franz, 2006), which con-
tains 1-gram to 5-gram counts for a web corpus with
approximately 1 trillion tokens and 95 billion sen-
tences. Compared to our earlier approach, using the
Web1T5 corpus reduces the size of available context
by going from 7-grams to 5-grams, but we are inten-
tionally keeping the corpus resources and algorithm
simple. We are particularly interested in exploring
the space between surface forms and abstractions
by incorporating information from word clustering,
an issue which is independent from the choice of a
more sophisticated learning algorithm.
Rozovskaya and Roth (2011) compared a range of
learning algorithms for the task of correcting errors
made by non-native writers, including an averaged
perceptron algorithm (Rizzolo and Roth, 2007) and
an n-gram count-based approach (Bergsma et al,
2009), which is similar to our approach. They found
that the count-based approach performs nearly as
well as the averaged perceptron approach when
trained with ten times as much data. Without access
to a large multi-genre corpus even a tenth the size
of the Web1T5 corpus, we chose to use Web1T5.
Our longest queries thus are 5-grams with at least
one word of context on each side of the candidate
function word and the shortest are 3-grams with
one word of context on each side. A large multi-
genre corpus would improve the results by support-
ing access to longer n-grams, and it would also make
deeper linguistic analysis such as part-of-speech tag-
ging feasible.
Table 1 shows the sets of determiners and prepo-
sitions for each of the three categories addressed by
our system: missing determiner (MD), incorrect de-
terminer (RD), and incorrect preposition (RT). The
function word lists are compiled from all single-
word corrections of these types in the training data.
The counts show the frequency of the error types in
the test data, along with the total frequency of func-
tion word candidates.
The following sections describe the main exten-
sions to our system for the 2012 shared task: a sim-
ple correction probability model, a missing deter-
miner detector, and the addition of hierarchical word
clustering to the prediction approach.
2.1 Correction Probability Model
To adapt the system for the CLC FCE learner data,
we added a simple correction probability model to
the n-gram predictor that multiplies the counts for
each n-gram by the probability of a particular re-
placement in the training data. The model includes
both correct and incorrect occurrences of each can-
didate, ignoring any corrections that make up less
than 0.5% of the corrections for a particular token.
For instance, the word among has the following cor-
rection probabilities: among 0.7895, from 0.1053,
between 0.0526. Even such a simplistic probability
model has a noticeable effect on the system perfor-
mance, improving the overall correction F-score by
approximately 3%. The preposition substitution er-
ror detection F-score alone improves by 9%.
Prior to creating the probability model, we exper-
imented with the addition of a bias toward the origi-
nal token, which we hoped would reduce the number
209
Category # Errors Candidate Corrections # Occurrences
Original Revised
MD 125 131 a, an, another, any, her, his, its, my, our, that,
the, their, these, this, those, which, your
-
RD 39 37 a, an, another, any, her, his, its, my, our, that,
the, their, these, this, those, which, your
1924
RT 136 148 about, after, against, along, among, around, as,
at, before, behind, below, between, by,
concerning, considering, during, for, from, in,
into, like, near, of, off, on, onto, out, outside,
over, regarding, since, through, throughout, till,
to, toward, towards, under, until, via, with,
within, without
2202
Table 1: Single-Word Prepositions and Determiners with Error and Overall Frequency in Test Data
of overcorrections generated by our system. With-
out the probability model, a bias toward the original
token improves the results, however, with the prob-
ability model, the bias is no longer useful.
2.2 Word Clustering
In the 2011 shared task, we observed that data spar-
sity issues are magnified in non-native texts because
the n-gram context may contain additional errors
or other infrequent or unusual n-gram sequences.
We found that abstracting to part-of-speech tags
and lemmas in certain contexts leads to small im-
provements in system performance. For the 2012
shared task, we explore the effects of abstracting to
word clusters derived from co-occurrence informa-
tion (Brown et al, 1992), another type of abstraction
relevant to our n-gram prediction approach. We hy-
pothesize that replacing tokens in the n-gram context
in our prediction tasks with clusters will reduce the
data sparsity for non-native text.
Clusters derived from co-occurrence frequencies
offer an attractive type of abstraction that occupy
a middle ground between relatively coarse-grained
morphosyntactic abstractions such as part-of-speech
tags and fine-grained abstractions such as lemmas.
For determiner and preposition prediction, part-of-
speech tags clearly retain too few distinctions. For
example, the choice of a/an before a noun phrase de-
pends on the onset of the first word in the phrase, in-
formation which is not preserved by part-of-speech
tagging. Likewise, preposition selection may be de-
pendent on lexical specifications (e.g., phrasal verbs
such as depend on) or on semantic or world knowl-
edge (cf. Wechsler, 1994).
Brown et al (1992) present a hierarchical word
clustering algorithm that can handle a large num-
ber of classes and a large vocabulary. The algorithm
clusters a vocabulary into C clusters given a corpus
to estimate the parameters of an n-gram language
model. Summarized briefly, the algorithm first cre-
ates C clusters for the C most frequent words in
the corpus. Then, a cluster is added containing the
next most frequent word. After the new cluster is
added, the pair of clusters is merged for which the
loss in average mutual information is smallest, re-
turning the number of clusters to C. The remaining
words in the vocabulary are added one by one and
pairs of clusters are merged in the same fashion un-
til all words have been divided into C clusters.
Using the implementation from Liang (2005),1
we generate word clusters for the most frequent
100,000 tokens in the ukWaC corpus (Baroni et al,
2009). We convert all tokens to lower case, replace
all lower frequency words with a single unique to-
ken, and omit from the clustering the candidate cor-
rections from Table 1 along with the low frequency
tokens. Our corpus is the first 18 million sentences
from ukWaC.2 After converting all tokens to lower-
case and omitting the candidate function words, a
total of 75,333 tokens are clustered.
We create three sets of clusters with sizes 500,
1000, and 2000. Due to time constraints, we did not
yet explore larger sizes. Brown et al (1992) report
that the words in a cluster appear to share syntac-
tic or semantic features. The clusters we obtained
appear to be overwhelmingly semantic in nature.
1Available at http://cs.stanford.edu/?pliang/software
2Those sentences in the file ukwac dep parsed 01.
210
Cluster ID Selected Cluster Members
(1) 00100 was..., woz, wasn?t, was, wasnt
(2) 0111110111101 definetly, definatly, assuredly, definately, undoubtedly, certainly, definitely
(3) 1001110100 extremely, very, incredibly, inordinately, exceedingly, awfully
(4) 1110010001 john, richard, peter, michael, andrew, david, stephen
(5) 11101001001 12.30pm, 7am, 2.00pm, 4.00pm, weekday, tuesdays
Table 2: Sample Clusters from ukWaC with 2000 Clusters
Table 2 shows examples from the set of 2000 clus-
ters. Examples (1) and (2) show how tokens with
errors in tokenization or misspellings are clustered
with tokens with standard spelling and standard tok-
enization. Such clusters may be useful for the shared
task by allowing the system to abstract away from
spelling errors in the learner essays. Examples (3)?
(5) show semantically similar clusters.
An excerpt of the hierarchical cluster tree for the
cluster ID from example (3) is shown in Figure 1.
The tree shows a subset of the clusters for cluster
IDs beginning with the sequence 1001110. Each bi-
nary branch appends a 0 or 1 to the cluster ID as
shown in the edge labels. The cluster 1001110100
(extremely, very) is found in the left-most leaf of
the right branch. A few of the most frequent clus-
ter members are shown for each leaf of the tree.
In our submissions to the shared task, we included
five different cluster settings: 1) using the original
word-based approach with no clusters, 2) using only
2000 clusters, 3) using the word-based approach ini-
tially and backing off to 2000 clusters if no hits are
found, 4) backing off to 1000 clusters, and 5) back-
ing off to 500 clusters. The detailed results will be
presented in section 3.
2.3 Missing Determiner Detector
We newly developed a missing determiner detector
to identify those places in the learner text where
a determiner is missing. Since determiners mostly
occur in noun phrases, we extract all noun phrases
from the text and put them through a two-stage clas-
sifier. For a single-stage classifier, always predict-
ing ?no error? leads to a very high baseline accu-
racy of 98%. Therefore, we first filter out those
noun phrases which already contain a determiner, a
possessive pronoun, another possessive token (e.g.,
?s), or an existential there, or whose head is a pro-
noun. This prefiltering reduces the baseline accu-
racy to 93.6%, but also filters out 10% of learner er-
rors (false negatives), which thus cannot be detected
in stage two.
In the second stage, a decision tree classifier de-
cides for every remaining noun phrase whether a de-
terminer is missing. From the 203 features we orig-
inally extracted to inform the classification, the chi
squared algorithm selected 30. Almost all of the se-
lected features capture properties of either the head
of the noun phrase, its first word, or the token im-
mediately preceding the noun phrase. We follow
Minnen et al (2000) in defining the head of a noun
phrase as the rightmost noun, or if there is no noun,
the rightmost token. As suggested by Han et al
(2004), the classifier considers the parts of speech
of these three words, while the features that record
the respective literal word were discarded.
We also experimented with using the entire noun
phrase and its part-of-speech tag sequence as fea-
tures (Han et al, 2004), which proved not to be
helpful due to the limited size of the training data.
We replaced the part-of-speech tag sequence with a
number of boolean features that each indicate equiv-
alence with a particular sequence. Of these features
only the one that checks whether the whole noun
phrase consists of a single common noun in the sin-
gular was included in the final feature set. Addi-
tionally, the selected features include countability
information from noun countability lists generated
by Baldwin and Bond (2003), which assign nouns
to one or more countability classes: countable, un-
countable/mass noun, bipartite, or plural only.
The majority of the 30 selected features refer to
the position of one of the three tokens (head, first
word, and preceding token) in the cluster hierarchy
described in section 2.2. The set of 500 clusters
proved not to be fine-grained enough, so we used
211
1001110
10011101
100111011
1001110111
. . .
1001110110
slightly
significantly
0 1
100111010
1001110101
10011101011
. . .
10011101010
terribly
quite
0 1
1001110100
extremely
very
0 1
0 1
10011100
100111001
more
100111000
fewer
less
0 1
0 1
Figure 1: Hierarchical Clustering Subtree for Cluster Prefix 1001110
the set of 1000 clusters. To take full advantage of the
hierarchical nature of the cluster IDs, we extract pre-
fixes of all possible lengths (1?18 characters) from
the cluster ID of the respective token. For the head
and the first word, prefixes of length 3?14 were se-
lected by the attribute selector, in addition to a prefix
of length 6 for the preceding token?s cluster ID.
Among the discarded features are many extracted
from the context surrounding the noun phrase, in-
cluding the parts of speech and cluster membership
of three words to the left and right of the noun
phrase, excluding the immediately preceding token.
Features referring to possible sister conjuncts of the
noun phrase, the next 3rd person pronoun in a fol-
lowing sentence, or previous occurrences of the head
in the text also turned out not to be useful. The per-
formance of the classifier was only marginally af-
fected by the reduction in the number of features.
We conclude from this that missing determiner de-
tection is sufficiently informed by local features.
In order to increase the robustness of the classifier,
we generated additional data from the written por-
tion of the BNC by removing a determiner in 20% of
all sentences. The resulting rate of errors is roughly
equal to the rate of errors in the learner texts and the
addition of the BNC data increases the amount of
training data by a factor of 13. We trained a classifier
on both datasets (referred to as HOO-BNC below).
It achieves an F-score of 46.7% when evaluated on
30% of the shared task training data, which was held
out from the classifier training data. On the revised
test data, it reaches an F-score of 44.5%.
3 Results
The following two sections discuss our overall re-
sults for the shared task and our performance on the
three error types targeted by our system.
3.1 Overall
Figure 2 shows the overall recognition and correc-
tion F-score for the cluster settings described in
section 2.2. With the missing determiner detec-
tor HOO-BNC described in section 2.3, these cor-
respond to runs #5?9 submitted to the shared task.
For the unrevised data, Run #6 (2000 clusters only)
gives our best result for overall detection F-score
(30.26%) and Run #7 (2000 cluster backoff) for cor-
rection F-score (18.44%). For the revised data, Run
212
  0
  5
  10
  15
  20
  25
  30
N
o 
Cl
us
te
rs
2,
00
0 
Cl
us
te
rs
2,
00
0 
Ba
ck
of
f
1,
00
0 
Ba
ck
of
f
50
0 
Ba
ck
of
f
F?
Sc
or
e
Cluster Settings
Recognition
Correction
Figure 2: Recognition and Correction F-Score with Clustering
#7 (2000 cluster backoff) has our best overall detec-
tion F-score (32.21%) and Run #5 (no clusters) has
our best overall correction F-score (22.46%).
Runs using clusters give the best results in two
other metrics reported in the shared task results for
the revised data. Run #6 (2000 clusters only) gives
the best results for determiner correction F-score and
Run #2 (2000 cluster backoff), which differs only
from Run #7 in the choice of missing determiner de-
tector, gives the best results for preposition detection
and recognition F-scores.
The detailed results for Runs #5?9 with the re-
vised data are shown in Figure 2. This graph shows
that the differences between the systems with and
without clusters are very small. The recognition F-
score is best with 2000 cluster backoff and the cor-
rection F-score is best with no clusters. In both
cases, the difference between the top two results is
less than 0.01. There is, however, a noticeable in-
crease in performance as the number of clusters in-
creases, which indicates that a larger number of clus-
ters may improve results further. The set of 2000
clusters may still retain too few distinctions for this
task.
3.2 Targeted Error Types
Our system handles three of the six error types in the
shared task: missing determiner (MD), incorrect de-
terminer (RD), and incorrect preposition (RT). The
recognition and correction F-scores for our best-
forming run for each type are shown in Figure 3.
  0
  5
  10
  15
  20
  25
  30
  35
  40
  45
M
D
R
D R
T
F?
Sc
or
e
Error Type
Recognition
Correction
Figure 3: Recognition and Correction F-Score for the
Targeted Error Types
In a comparison of performance on individual er-
ror types in the shared task, our system does best
on the task for which it was originally developed,
213
preposition prediction. We place 4th in recognition
and 3rd in correction F-score for this error type. For
missing determiner (MD) and incorrect determiner
(RD) errors, our system is ranked similarly as in our
overall performance (4th?6th).
For the sake of replicability, as the HOO 2012 test
data is not publicly available, we include our results
on the HOO training data for the preposition and de-
terminer substitution errors in Table 3.
Error No Clusters
Type Recognition Correction
Prec Rec Prec Rec
RT 32.69 29.94 24.85 22.77
RD 10.63 18.56 8.37 14.61
Error 2000 Backoff
Type Recognition Correction
Prec Rec Prec Rec
RT 25.87 35.60 18.26 25.13
RD 9.71 23.65 7.48 18.23
Table 3: Results for HOO 2012 Training Data
Results are reported for the no cluster and 2000
cluster backoff settings, which show that incorpo-
rating the cluster backoff improves recall at the ex-
pense of precision. Missing determiner errors are
not reported directly as the missing determiner de-
tector was trained on the training data, but see the
evaluation at the end of section 2.3.
4 Discussion and Conclusion
The n-gram prediction approach with the new miss-
ing determiner detector performed well in the HOO
2012 Shared Task, placing 6th in terms of detection
and 5th in terms of correction out of fourteen teams
participating in the shared task. In our best sub-
missions evaluated using the revised test data, we
achieved a detection F-score of 32.71%, a recogni-
tion F-score of 29.21% and a correction F-score of
22.73%. For the three error types addressed by our
approach, our correction F-scores are 39.17% for
missing determiners, 9.23% for incorrect determin-
ers, and 30.12% for incorrect prepositions. Informa-
tion from hierarchical word clustering (Brown et al,
1992) extended the types of abstractions available
to our n-gram prediction approach and improved the
performance of the missing determiner detector.
For the n-gram prediction approach, word clusters
IDs from the hierarchical word clustering replace to-
kens in the surrounding context in order to improve
recall for learner texts which may contain errors
or infrequent token sequences. The use of cluster-
based contexts with 2000 clusters as a backoff from
the word-based approach leads to a very small im-
provement in the overall recognition F-score for the
HOO 2012 Shared Task, but our best overall correc-
tion F-score was obtained using our original word-
based approach. The differences between the word-
based and cluster-based approaches are quite small,
so we did not see as much improvement from the
word cluster abstractions as we had hoped. We
experimented with sets of clusters of several sizes
(500, 1000, 2000) and found that as the number
of clusters becomes smaller, the performance de-
creases, suggesting that a larger number of clusters
may lead to more improvement for this task.
Information from the word cluster hierarchy was
also integrated into our new missing determiner de-
tector, which uses a decision tree classifier to decide
whether a determiner should be inserted in front of
a determiner-less NP. Lexical information from the
extracted noun phrases and surrounding context are
not as useful for the classifier as information about
the position of the tokens in the word cluster hier-
archy. In particular, cluster information appears to
help compensate for lexical sparsity given a rela-
tively small amount of training data.
In future work, we plan to explore additional clus-
tering approaches and to determine when the use of
word cluster abstractions is helpful for the task of
predicting determiners, prepositions, and other func-
tion words. An approach that refers to word clus-
ters in certain contexts or in a customized fashion
for each candidate correction may lead to improved
performance for the task of detecting and correcting
such errors in texts by non-native writers.
References
Timothy Baldwin and Francis Bond, 2003. Learn-
ing the countability of English nouns from corpus
data. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics
(ACL). pp. 463?470.
214
M. Baroni, S. Bernardini, A. Ferraresi and
E. Zanchetta, 2009. The WaCky Wide Web: A
Collection of Very Large Linguistically Processed
Web-Crawled Corpora. Language Resources and
Evaluation, 43(3):209?226.
Shane Bergsma, Dekang Lin and Randy Goebel,
2009. Web-scale N-gram models for lexical dis-
ambiguation. In Proceedings of the 21st interna-
tional jont conference on Artifical intelligence (IJ-
CAI?09). Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
Steven Bird, Robert Dale et al, 2008. The ACL An-
thology Reference Corpus. In Proceedings of the
6th International Conference on Language Re-
sources and Evaluation (LREC). Marrakesh, Mo-
rocco.
Adriane Boyd and Detmar Meurers, 2011. Data-
Driven Correction of Function Words in Non-
Native English. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Genera-
tion ? Helping Our Own (HOO) Challenge. As-
sociation for Computational Linguistics, Nancy,
France.
Thorsten Brants and Alex Franz, 2006. Web 1T
5-gram Version 1. Linguistic Data Consortium.
Philadelphia.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, T. J. Watson, Vincent J. Della Pietra and
Jenifer C. Lai, 1992. Class-Based n-gram Models
of Natural Language. Computational Linguistics,
18(4):467?479.
Robert Dale and Adam Kilgarriff, 2011. Helping
Our Own: The HOO 2011 Pilot Shared Task. In
Proceedings of the 13th European Workshop on
Natural Language Generation. Nancy, France.
Rachele De Felice, 2008. Automatic Error Detection
in Non-native English. Ph.D. thesis, Oxford.
Anas Elghafari, Detmar Meurers and Holger Wun-
sch, 2010. Exploring the Data-Driven Prediction
of Prepositions in English. In Proceedings of the
23rd International Conference on Computational
Linguistics (COLING). Beijing.
Michael Gamon, Jianfeng Gao et al, 2008. Us-
ing Contextual Speller Techniques and Language
Modeling for ESL Error Correction. In Proceed-
ings of the Third International Joint Conference
on Natural Language Processing. Hyderabad.
Na-Rae Han, Martin Chodorow and Claudia Lea-
cock, 2004. Detecting Errors in English Arti-
cle Usage with a Maximum Entropy Classifier
Trained on a Large, Diverse Corpus. In Proceed-
ings of the 4th International Conference on Lan-
guage Resources and Evaluation (LREC). Lisbon.
Percy Liang, 2005. Semi-Supervised Learning for
Natural Language. Master?s thesis, Massachusetts
Institute of Technology.
Guido Minnen, Francis Bond and Ann Copestake,
2000. Memory-based learning for article gener-
ation. In Proceedings of the 2nd Workshop on
Learning Language in Logic and the 4th Confer-
ence on Computational Natural Language Learn-
ing. volume 7, pp. 43?48.
Nick Rizzolo and Dan Roth, 2007. Modeling Dis-
criminative Global Inference. In Proceedings of
the First International Conference on Semantic
Computing (ICSC). IEEE, Irvine, California, pp.
597?604.
Alla Rozovskaya and Dan Roth, 2011. Algorithm
Selection and Model Adaptation for ESL Cor-
rection Tasks. In Proceedings of the 49th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies (ACL-HLT). Portland, Oregon.
Joel Tetreault and Martin Chodorow, 2008. Native
Judgments of Non-Native Usage: Experiments in
Preposition Error Detection. In Proceedings of the
22nd International Conference on Computational
Linguistics (COLING). Manchester.
Stephen Wechsler, 1994. Preposition Selection Out-
side the Lexicon. In Raul Aranovich, William
Byrne, Susanne Preuss and Martha Senturia
(eds.), Proceedings of the Thirteenth West Coast
Conference on Formal Linguistics. CSLI Publica-
tions, Stanford, California, pp. 416?431.
H. Yannakoudakis, T. Briscoe and B. Medlock,
2011. A new dataset and method for automati-
cally grading ESOL texts. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Tech-
nologies (ACL-HLT).
215
