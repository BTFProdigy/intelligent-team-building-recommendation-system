Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 551?560, Prague, June 2007. c?2007 Association for Computational Linguistics
Parsimonious Data-Oriented Parsing
Willem Zuidema
Institute for Logic, Language and
Computation, University of Amsterdam
Plantage Muidergracht 24
1018 TV, Amsterdam, the Netherlands
jzuidema@science.uva.nl
Abstract
This paper explores a parsimonious ap-
proach to Data-Oriented Parsing. While al-
lowing, in principle, all possible subtrees
of trees in the treebank to be productive
elements, our approach aims at finding a
manageable subset of these trees that can
accurately describe empirical distributions
over phrase-structure trees. The proposed
algorithm leads to computationally much
more tracktable parsers, as well as linguis-
tically more informative grammars. The
parser is evaluated on the OVIS and WSJ
corpora, and shows improvements on effi-
ciency, parse accuracy and testset likelihood.
1 Data-Oriented Parsing
Data-Oriented Parsing (DOP) is a framework for
statistical parsing and language modeling originally
proposed by Scha (1990). Some of its innovations,
although radical at the time, are now widely ac-
cepted: the use of fragments from the trees in an
annotated corpus as the symbolic grammar (now
known as ?treebank grammars?, Charniak, 1996)
and inclusion of all statistical dependencies between
nodes in the trees for disambiguation (the ?all-
subtrees approach?, Collins & Duffy, 2002).
The best known instantiations of the DOP-
framework are due to Bod (1998; 2001; 2003),
using the Probabilistic Tree Substitution Grammar
(PTSG) formalism. Bod has advocated a maximal-
ist approach to DOP, inducing grammars that con-
tain all subtrees of all parse trees in the treebank,
and using them to parse unknown sentences where
all of these subtrees can potentially contribute to the
most probable parse. Although Bod?s empirical re-
sults have been excellent, his maximalism poses im-
portant computational challenges that, although not
necessarily unsolvable, threaten both the scalability
to larger treebanks and the cognitive plausibility of
the models.
In this paper I explore a different approach to
DOP, that I will call ?Parsimonious Data-Oriented
Parsing? (P-DOP). This approach remains true to
Scha?s original program, by allowing, in principle,
all possible subtrees of trees in the treebank to be
the productive elements. But unlike Bod?s approach,
P-DOP aims at finding a succinct subset of such el-
ementary trees, chosen such that it can still accu-
rately describe observed distributions over phrase-
structure trees. I will demonstrate that P-DOP leads
to computationally more tracktable parsers, as well
as linguistically more informative grammars. More-
over, as P-DOP is formulated as an enrichment
of the treebank Probabilistic Context-free Grammar
(PCFG), it allows for much easier comparison to al-
ternative approaches to statistical parsing (Collins,
1997; Charniak, 1997; Johnson, 1998; Klein and
Manning, 2003; Petrov et al, 2006).
2 Independence Assumptions in PCFGs
Parsing with treebank PCFGs, in its simplest form,
involves the following steps: (1) a treebank is cre-
ated by extracting phrase-structure trees from an an-
notated corpus, and split in a train- and a testset;
(2) a PCFG is read off from all productions in the
trainset trees, with weights proportional to their fre-
551
quency in the treebank (the ?relative frequency esti-
mate?); (3) a standard PCFG parser is used to find
for each yield of the test-set trees the most probable
parse; (4) these parses are compared to the test-set
trees to count matching brackets, labels and trees.
PCFGs incorporate a strong statistical indepen-
dence assumption: that the expansion of a nonter-
minal node is only dependent on the node?s label.
All state-of-the-art wide-coverage parsers relax this
assumption in some way, for instance by (i) chang-
ing the parser in step (3), such that the application
of rules is conditioned on other steps in the deriva-
tion process (Collins, 1997; Charniak, 1997), or
by (ii) enriching the nonterminal labels in step (1)
with context-information (Johnson, 1998; Klein and
Manning, 2003), along with suitable backtransforms
in step (4). These two approaches often turn out to
be equivalent, although for some conditionings it is
not trivial to work out the equivalent enrichment and
vice versa, especially when combined with smooth-
ing. Interesting recent work has focused on the au-
tomatic induction of enrichments (Matzuzaki et al,
2005; Prescher, 2005), leading to extremely accurate
parsers (Petrov et al, 2006).
DOP relaxes the independence assumption by
changing the class of probabilistic grammars in-
duced in step (2). In DOP1 (Bod, 1998), a PTSG
is induced, which consists, subject to some heuris-
tic constraints, of all subtrees1 of the treebank
trees with a weight proportional to their frequency.
PTSGs allow multiple derivations to yield the same
parse; in DOP1 the sum of their probabilities gives
the probability of the parse. The relation between
DOP and enrichment/conditioning models was clar-
ified by Joshua Goodman, who devised an efficient
PCFG transform of the DOP1 model (Goodman,
1996). The size of the PCFG resulting from this
transform is linear in the number of nonterminals to-
kens in the corpus. Goodman?s transform, in com-
bination with a range of heuristics, allowed Bod
(2003) to run the DOP model on the Penn Treebank
WSJ benchmark and obtain some of the best results
obtained with a generative model.
The computational challenges for DOP are far
from solved, however. The difference with style
1A subtree t? of a parse tree t is a tree such that every node
i? in t? equals a node i in t, and i? either has no daughters or the
same daughter nodes as i.
(ii) enrichment is that we derive many more rules
from every original tree than the number of CFG-
productions it contains. This is one reason why the
relative frequency estimator for DOP is inconsistent
(Johnson, 2002). But worse, perhaps, the size of the
grammar remains gigantic2 , making it difficult for
many in the field to replicate Bod?s results.
In this paper, we develop a parsimonious ap-
proach to DOP, that avoids many of the computa-
tional problems of the maximalist approach but tries
to maintain its excellent empirical performance. Our
approach starts, both conceptually and technically,
with an analysis of where the PCFG independence
assumption breaks down when modeling empirical
distributions. In section 2 we derive equations for
the expected frequency of arbitrary subtrees under a
distribution defined by a given PCFG, and use them
to measure how much observed subtree-frequencies
deviate from expectation. In section 4 we generalize
this analysis to PTSGs. In section 5 we discuss an al-
gorithm for estimating PTSGs from a treebank, that
is based on minimizing the differences between ex-
pected and observed subtree-frequencies. We then
proceed with discussing PTSGs induced from var-
ious treebanks, and in section 6 the use of these
PTSGs for parsing.
3 Deviations from a PCFG distribution
PCFGs can be viewed as PTSGs where the elemen-
tary trees are restricted to depth 1; we therefore start
by repeating the definition of PTSGs (Bod, 1998),
and use notation appropriate for PTSGs throughout.
An PTSG is a 5-tuple ?Vn, Vt, S, T, w?, where Vn isthe set of non-terminal symbols; Vt is the set of ter-minal symbols; S ? Vn is the start symbol; T is a setof elementary trees, such that for every ? ? T the
unique root node r(?) ? Vn, the (possibly empty)set of internal nodes i(?) ? Vn and the set of leafnodes l(?) ? Vn ? Vt; finally, w : T ? [0, 1] is aprobability (weight) distribution over the elementary
trees, such that for any ? ? T , ?? ??R(?) w(? ?) = 1,where R(?) is the set of elementary trees with the
same root label as ? . It will prove useful to also
define the set of all possible trees ? over the defined
2Sections 2-21 of WSJ contain 1676821 productions. Of
these,106 are lexical productions, and 36151 top-productions,
leaving approx. 640000 internal productions which yield about
2.5 ? 106 rules in Goodman?s transform.
552
alphabets (with the same conditions on root, internal
and leaf nodes as for T ), and the set of all possible
complete parse trees ? (with r(t) = S and all leaf
nodes l(t) ? Vt). Obviously, T ? ? and ? ? ?.The substitution operation ? is defined if the left-
most nonterminal leaf in ?1 is identical to the rootof ?2. Performing substitution ?1 ? ?2 yields t3, if t3is identical to ?1 with the leftmost nonterminal leafreplaced by ?2. A derivation is a sequence of ele-mentary trees, where the first tree ? ? T has root-
label S and every next tree combines through sub-
stitution with the result of the substitutions before
it. In this paper, we are only concerned with gram-
mars that define proper probability distributions over
trees, such that the probability of all derivations sum
up to 1 and no probability mass gets lost in deriva-
tions that never reach a terminal yield. That is, we
require (if t(d) is the tree derived by derivation d):
?
d:t(d)??
P (d) = 1. (1)
For simplicity, but without loss of generality, we as-
sume there are no recursions on the start symbol.
In this section, we restrict ourselves to PCFG dis-
tributions, and thus to a T with only depth 1 trees.
The probability of a PCFG rule (conditioned on its
left-hand side) in the conventional notation, P (A 7?
?? . . . ?|A), now corresponds to the probability of a
depth 1 tree (conditioned on its root nonterminal):
P
?
?
A
? ? . . . ?
|A
?
?
Of course, the probability of a (complete) deriva-
tion is simply the product of the (conditional) prob-
abilities of the rules in the derivation. It is useful to
consider, for a given grammar G generating a cor-
pus of N trees, the expected frequency of visiting
nonterminal state X:
EF (X) =
{
N if X = S?
? EF (?)C(X, l(?)) otherwise(2)
where C(X, l(?)) gives the number of occurrences
of nonterminal X among the leaves of elementary
tree ? . Furthermore, the expected usage frequency
of ? is given by
EF (?) = EF (r(?))P (? |r(?))
= EF (r(?))w(?) (3)
Substituting eq (3) into (2) yields a system of
|Vn| linear equations, that can be straightforwardlysolved using standard methods.
We are interested in the empirical deviations from
the distribution defined by a given grammar (for in-
stance, the treebank PCFG), such that we can adjust
the grammar to better model the training data (whilst
avoiding overfitting). In line with the general DOP
approach, we would like to measure this deviation
for every possible subtree. Of course, the condi-
tional probability of an arbitrary subtree is simply
the product of the rule probabilities. The expected
frequency of a subtree is the expected frequency of
its root state, times the conditional probability:
EF (t) = EF (r(t))P (t|r(t)) (4)
Using these equations, we can measure for each
observed subtree in the corpus, the difference be-
tween observed frequency and expected frequency.
This will give high values for overrepresented and
frequent constructions in the corpus, such as sub-
trees corresponding to revenues rose CD % to $ CD
million from $ CD million last year, details weren?t
disclosed, NP-SUBJ declined to comment and con-
tracted and negated auxiliaries such as won?t, can?t
and don?t. The top-10 overrepresented subtrees in
the WSJ20-corpus are given in figure 1.
VP
VBD
?SAID?
SBAR
S
TOP
S
CC
?BUT?
S@1
S
CC
?BUT?
S@1
NP-SBJ
NNP
?MR.?
NNP
SBAR
IN
?THAT?
S
NP
NP PP
IN NP
PP
IN
?OF?
NP
PP-LOC
IN
?IN?
NP
VP@1
RB
?N?T?
VP
TOP
S
CC S@1
NP-SBJ S@2
Figure 1: Top-10 overrepresented subtrees (excluding subtrees
with punctuation) in sentences of length ? 20, including punc-
tuation, in sections 2-21 of the WSJ-corpus (transformed to
Chomsky Normal Form, whereby newly created nonterminals
are marked with an @). Measured are the deviations from
the expected frequencies according to the treebank PCFG (of
this selection), as in equation (4) but with EF (r(t)) replaced
by the empirical frequency o(r(t)). Observed frequencies are
(deviations between brackets): 461 (+408.2), 554 (+363.8),
556 (+361.7), 479 (+348.2), 332 (+314.3), 415 (+313.3), 460
(+305.1), 389 (+283.0), 426 (+277.2), 295 (+266.1).
Of course, there are also many subtrees that oc-
cur much less frequently than the grammar predicts,
such as for instance subtrees corresponding to in-
frequent or non-occurring variations of the frequent
553
ones, e.g. revenues rose CD from $ CD million from
$ CD million. Underrepresented subtrees found in
the WSJ20 corpus, include (VP (VBZ ?IS?) NP)),
which occurs only once, even though it is predicted
152.7 times more often (in all other VP?s with ?IS?,
the NP is labeled NP-PRD); and (PP (IN ?IN?) NP)),
which occurs 38 times but is expected 121.0 times
more often (IN NP-constructions are usually labeled
PP-LOC).
Given such statistics, how do we improve the
grammar such that it better models the data? PCFG
enrichment models (Klein and Manning, 2003;
Schmid, 2006) split (and merge) nonterminals;
in automatic enrichment methods (Prescher, 2005;
Petrov et al, 2006) these transformations are per-
formed so as to maximize data likelihood (under
some constraints). The treebank PCFG-distribution
thereby changes, such that the deviations from fig-
ure 1 mostly disappear. For instance, the overrepre-
sentation of ?but? as the sentence-initial CC in the
second and third subtree of that figure, is dealt with
in (Schmid, 2006) by splitting the CC-category into
CC/BUT and CC/AND. However, also when a range
of such transformations is applied, some subtrees are
still greatly overrepresented. Figure 2 gives the top-
10 overrepresented subtrees of the same treebank,
enriched with Schmid?s enrichment program tmod.
In DOP, larger subtrees can be explicitly repre-
sented as units. This is the approach we take in
this paper, which involves switching from PCFGs
to PTSGs. However, we cannot simply add over-
represented trees to the treebank PCFG; as is clear
from figure 2, many of the overrepresented subtrees
are in fact spurious variations of the same construc-
tions (e.g. ?$ CD million?, ?a JJ NN?). To reach our
goal of finding the minimal set of subtrees that ac-
curately models the empirical distribution over trees,
we will thus need to consider a series of PTSGs, find
the subtrees that are still overrepresented and adapt
the grammar accordingly.
4 Deviations from an PTSG distribution
4.1 Expected Frequencies: An Example
Once we allow T to contain elementary trees of
depth larger than 1, the equations above become
more difficult. The reason is that now multiple
derivations may give rise to the same parse tree, and,
NP-SBJ/3S/BASE
NNP
?MR.?
NNP@1
QP/$
$ QP/$@1
CD CD@1
?MILLION?
NP-SBJ/BASE
NNP
?MR.?
NNP@1
]QP/$
$
?$?
QP/$@1
CD CD@1
?MILLION?
QP/$@1
CD CD@1
?MILLION?
NP/BASE
DT/A NP/BASE@1
JJ NN
VP/FIN
MD VP/FIN@1
RB/NOT VP/INF
NP/BASE
DT/A
?A?
NP/BASE@1
JJ NN
TOP
NP-SBJ/3S/BASE
NNP
?MR.?
NNP@1
S/FIN/.@1
NP/BASE
NNP NP/BASE@1
NNP@1 NP/BASE@2
Figure 2: Top-10 overrepresented subtrees (excluding subtrees
with punctuation) in the WSJ20 corpus, enriched with the tmod
program (Schmid, 2006). Empirical frequencies are as fol-
lows (deviations between brackets): 262 (+207.6), 235 (+158.4)
207 (+156.4), 228 (+153.5), 237 (+141.0), 190 (+134.2), 153
(+126.5), 166 (+117.8), 139 (+110.0), 111 (+103.8).
as a corrolary, a specific subtree can emerge in many
different ways. Consider an PTSG that consists of
all subtrees of the trees t1, t2 and t3 in figure 3, andthe expected frequency of the subtree t?.
t1 =S
B
x
A
y
t2 =S
A
x
B
C
y
D
x
t3 =S
A
y
B
C
x
D
y
t? =B
C D
y
Figure 3: Three example treebank trees and the focal subtree
It is clear that t? might arise in many different
ways. For instance, it emerges in the derivation with
elementary trees ?1 ??4 ??5 from figure 4, but also inderivations ?2 ? ?4 and ?3 ? ?5. Note that in none ofthese derivations elementary tree t? itself was used.
?1 =S
A
x
B
C D
?2 = S
A
y
B
C D
y
?3 = S
A
x
B
C
y
D
?4 = C
x
?5 =D
y
?6 =B
C D
Figure 4: Some elementary trees extracted from the trees in fig 3
4.2 Expected Frequency: Usage & Occurrence
Hence, when using PTSGs, we need to distinguish
between the expected usage frequency of an elemen-
tary tree (written as Eu(?)), and the expected occur-
rence frequency (Eo(t)) of the corresponding sub-
tree. Moreover, not all nonterminal nodes in a de-
554
rived tree are necessarily ?visited? substitution sites.
The expected frequency of visiting a nonterminal
state X as substitution site depends on the usage fre-
quencies:
EF (X) =
{
N if X = S?
? Eu(?)C(X, l(?)) otherwise(5)
Relating usage frequencies to weights is still sim-
ple (compare equation 3):
Eu(?) = EF (r(?))w(?) (6)
And hence: w(?) = Eu(?)/?? ?:r(?)=r(? ?) Eu(? ?).The expected frequency of a complete tree is not
simply a product anymore, but the sum of the differ-
ent derivation probabilities (where der(t) gives the
set of derivations of t):
Eo(t) =
?
d?der(t)
?
??d
w(?) if t ? ? (7)
4.3 Expected Frequency of Arbitrary Subtrees
Most complex is the expected occurrence frequency
of an arbitrary subtree t. From the example above it
is clear that it is not necessary that the root of t is a
substitution site. Analogous to equation (4), we need
the expected frequency of arriving at some state ? in
the derivation process that is still consistent with ex-
panding to something that contains t, and multiply it
with the probability that this expansion indeed hap-
pens:
Eo(t) =
?
?
EF (?)P (t|?) (8)
To be able to define the states ?, we redefine the
set of derivations der(t) of a subtree t, such that the
derivations der(t?) of our example tree from figure 3
are the following: d1 = B ? ?6 ? ?5, d2 = ?6 ? ?5,
d3 = B ? t? and d4 = t?. Only if a derivation startswith a single nonterminal is the root node consid-
ered a substitution site. The states ? correspond to
the first elements of each of these derivations, i.e.
?B, ?6, B, t??.As was clear from the example in section 4.1,
we need to consider all supertrees of the trees in
the derivation of t for calculating the expected fre-
quency of a state and the probability of expanding
from that state to form t. It is useful to distin-
guish, as do Bod & Kaplan (Bod, 1998, ch. 10) two
types of supertree-subtree relations, depending on
whether nodes must be removed from the root down-
ward, or from the leaves (?frontier?) upward. ?Root-
subtrees? of t are those subtrees headed by any of
t?s internal nodes and everything below. ?Frontier-
subtrees? are those subtrees headed by t?s root-node,
pruned at any number (? 0) of internal nodes. Using
? to indicate left-most substitution, we can write:
? t1 is a root-subtree of t1, and t1 is a root-subtreeof t2, if ?t3, such that t3 ? t1 = t2;
? t1 is a frontier-subtree of t1, and t1 is a frontier-
subtree of t2, if ?t3 . . . tn, such that t1 ? t3 . . . ?
tn = t2.
? t? is the x-frontier-subtree of t, t? = fsx(t), if
x is a set of nodes in t, such that if t is pruned
at each i ? x it equals t?.
We use the notation st(t) for the set of subtrees of
t, rs(t) for the set of root-subtrees of t and fs(t) for
the set of frontier-subtrees of t. Thus defined, the set
of all subtrees of t is the set of all frontier-subtrees
of all root-subtrees of t: st(t) = {t?|?t??(t?? ?
rs(t) ? t? ? fs(t??)). We further define the sets of
root-supertrees, frontier-supertrees, and supertrees
as follows: (i) f? sx(t) = {t?|t = fsx(t?)}, (ii)
f? s(t) = {t?|t ? fs(t?)} (iii) s?t(t) = {t?|t ? st(t?)}.
If there are only terminals in the yield of t, the ex-
pected frequency of a state ? is now simply the sum
of the expected usage frequencies of those elemen-
tary trees that have ? at their frontier (i.e. that ? is a
root-subtree of):
EF (?) =
?
? ?:??rs(? ?)
Eu(? ?) if l(t) ? Vt (9)
If there are nonterminals in the yield of t, as in the
example, we need to also consider elementary trees
that have these nonterminals already expanded. To
see why, consider again the example of section 4.1
and check that also elementary tree ?3 contributes tothe expected frequency of t?. If we take this into
account, and write nt(t) for the nonterminal nodes
in the yield of t, the final expression for the expected
frequency of state ? becomes:
EF (?) =
?
??f?s(?)
?
? ?? ?rsnt(t)(?)
Eu(? ?) (10)
555
Finally, the probability of expanding a state ?
such that t emerges is again simplest if t has no non-
terminals as leaves. Remember that a state ? was the
first element of a derivation of t; the probability of
expanding to t is simply the product of the weights
of the remaining elementary trees in the derivation
(if states are unique for a derivation):
P (t|?) =
?
??rest(d)
w(?) if l(t) ? Vt (11)
If there are nonterminals among the leaves of t,
however, we need again to sum over possible expan-
sions at those nonterminal leaves:
P (t|?) =
?
??rest(d)
?
? ?? ?fsx(t)(?)
w
(
? ?
) (12)
Substituting equations (9) and (12) into equa-
tion (8) gives a general expression for the expected
occurrence frequency of an arbitary subtree t:
Eo(t) =
?
d?der(t)
( ?
??rest(d)
?
? ?? ?fsx(t)(?)
w
(
? ?
)
?
??
r?s(first(d))
?
? ?? ?fsx(t)(?)
Eu(? ?)
)
. (13)
5 Minimizing deviations: estimation
The equations just derived can be used to learn an
PTSG from a treebank, using an estimation proce-
dure we call ?push-n-pull? (pnp). This procedure
was described in some detail elsewhere (Zuidema,
2006b); here I only sketch the basic idea. Given
an initial setting of the parameters (all depth 1 el-
ementary trees at their empirical frequency), the
method calculates the expected frequency of all
complete and incomplete trees. If a tree t?s ex-
pected frequency Eo(t) is higher than its observed
frequency o(t), the method subtracts the difference
from the tree?s score, and distributes (?pushes?) it
over the elementary trees involved in all its deriva-
tions (der(t)). If it is lower, it ?pulls? the difference
from all its derivations.
The ?score? of an elementary tree ? is the al-
gorithm?s estimate of the usage frequency u(?).
The amounts of score that are pushed or pulled are
capped by the requirement that ?? u(?) ? 0; more-
over, the learning rate parameter ? determines the
fraction of the expected-observed difference that is
actually pushed or pulled. Finally, the method in-
cludes a bias (B) for moving probability mass to
smaller elementary trees, to avoid overfitting (its ef-
fects become smaller as more data gets observed).
Because smaller elementary trees will be involved
in other derivations as well, the push and pull opera-
tions will shift probabilities between different parse
trees. Suppose a given complete tree is the only tree
with nonzero frequency of all trees that can be built
from the same components. This tree will continue
to ?pull? until it has in fact reached its appropriate
frequency. Similarly, if a given tree does have zero
observed frequency, it will continue to leak score to
other derivations with the same components.
NP/BASE
DT/A NP/BASE@1
JJ NN
NP-SBJ/3S/BASE
NNP
?MR.?
NNP1
NP/BASE
DT/THE
?THE?
NP/BASE@1
JJ NN
NP-SBJ/BASE
NNP
?MR.?
NNP1
NP/GEN/BASE
NNP NP/GEN/BASE@1
NNP1 POS
??S?
NP/PP
NP/BASE PP/OF/NP
IN/OF
?OF?
NP/BASE1
NP/BASE
NNP NP/BASE@1
NNP1 NNP2
NP/BASE
JJ NP/BASE@1
JJ1 NNS
NP
QP/$
$ QP/$@1
CD CD1
?MILLION?
VP/FIN
MD VP/FIN@1
ADVP/V
RB/V
VP/INF
Figure 5: Top-10 elementary trees of depth>1, excluding those
with punctuation, from running pnp on the enriched WSJ20.
The output of the push-n-pull algorithm is an
PTSG, with the same set of elementary trees as the
DOP models of Bod (1998; 2001). This set is very
large. However, unlike those existing DOP models,
the score distribution over these elementary trees is
extremely skewed: relatively few trees receive high
scores, and there is a very long tail of trees with low
scores. In Zuidema (2006b) we give a qualitative
analysis of the subtrees with the highest scores as in-
duced from the ATIS treebank, which include many
of its frequent constructions including show me NP,
I would like NP, ights from NP to NP. The top-10
larger elementary trees that result from running pnp
on a randomly selected trainset of about 8000 sen-
tences of the Dutch OVIS treebank (Veldhuijzen van
Zanten et al, 1999), can be glossed as: Yes, from NP
to NP, No thank you very much, I want to VP-INF,
556
No thank you, I want PP to VP-INF, I want PP, I
want Prep NP-LOC Prep NP-LOC, Yes please, At
CD o?clock. In figure 5 we give the top-10 elemen-
tary trees resulting from the WSJ20-corpus.
Figure 6: Log-log curves of (i) subtree frequencies against rank
(for 106 subtrees from WSJ20), (ii) pnp-scores against rank,
and (iii) the same for the top-10000 depth>1-subtrees.
Figure 6 shows some characteristics of this last
grammar. Shown are log-log plots, such as com-
monly used to visualise the Zipf-distributions in nat-
ural language. The top curve plots log(frequency)
against log(rank) for each subtree of the trees in
the corpus, which shows the approximate Zipfian
behavior. The second curve from above plots the
log(score) against log(rank) for these same subtrees.
As can be observed, the score-distribution follows
the frequency distribution only for the most frequent
subtrees (all of depth 1), but then deviates from it
downwards. The bottom curve ? an almost straight
line in this log-log space ? gives the log(score) vs
log(rank) of subtrees with a depth>1.
Figure 7: Subtree frequencies against pnp-scores, including
subsets pnp1000 (dark/blue) and pnp10000 (light/green).
Figure 7 further illustrates the difference between
the score- and the frequency-distributions, by plot-
ting for each subtree, log-frequency (y-axis) against
log-score (x-axis). The subtrees clearly fall into two
categories: those where the scores correlate strongly
with frequency (the depth 1 subtrees) and the larger
subtrees that vary greatly in how strong scores corre-
late with frequency. Only larger subtrees that receive
relatively high scores should be used in parsing.
Weights are proportional to subtree-frequencies
in the DOP1 and related ?maximalist? models.
The differences between the frequency and score-
distributions thus illustrate a very important differ-
ence between maximalist and parsimonious DOP.
The characteristics of the score distribution allow
P-DOP to throw away most of the subtrees without
significantly affecting the distribution over complete
parse trees that the grammar defines. This is the ap-
proach we take for evaluating parsing performance:
we take as our baseline the treebank PCFG, and then
add the n larger elementary trees with the highest
scores from our induced PTSG.
6 Parsing Results
For our parsing results we use BitPar, a fast
and freely available general PCFG parser (Schmid,
2004). In our first experiments we used the OVIS
corpus, with semantic tags and punctuation re-
moved, and all trees (train- and testset) binarized.
As a baseline experiment, we read off the treebank
PCFG as decribed in section 2. The recall, precision
and complete match results are in table 1, labeled tb-
pcfg. For comparison, we also show the results ob-
tained with two versions of the DOP model, DOP1
(Bod, 1998) and DOP* (Zollmann and Sima?an,
2005) on the same treebank.
We ran the pnp program as described above on
the trainset, with parameters B = 1.0, ? = 0.1 and
d = 4. This run yielded a single PTSG that was used
in 4 parsing runs. For these experiments, we added
increasingly many of the depth>1 elementary trees
from the PTSG, with minimum scores of 7.0, 1.0,
0.5, and 0.075. The added elementary trees were
first converted to PCFG rules, by labeling all inter-
nal nodes with a unique address label and reading
off the CFG-productions. Each rule received a score
equal to the score of the elementary tree it derived
from. A copy of each rule, with the label removed,
was also added with a negative score, BitPar auto-
557
matically sums (and substracts) and normalizes the
frequency information provided with each rule. Bit-
Par was then run on the testset sentences, with the
option to output the n best parses with n = 10 by
default. These parses were then read in in a post-
processing program, which removes address labels,
sums probabilities of equivalent parses and outputs
the most probable parse for each sentence (this is the
same approximation of MPP, albeit with smaller n,
as used in most of Bod?s DOP results). The results of
these experiments are also in table 1, labeled pnpN ,
where N is the number of elementary trees added.
model # rules LR LP CM
tb-pcfg 3000 93.45 95.5 85.84
DOP1 1.4 ? 106 (87.55)
DOP* (< 50000) (87.7)
pnp100 3000+100 93.63 95.65 86.55
pnp763 3000+763 93.5 95.52 86.75
pnp1517 3000+1517 93.78 95.83 87.36
pnp11411 3000+11411 94.26 96.4 87.77
Table 1: Results on the Dutch OVIS tree bank, with semantic
tags and punctuation removed. Reported are evalb scores on a
random testset of 1000 sentences (a second testset of 1000 sen-
tences is kept for later evaluations). The trainset for both the
treebank grammar and the pnp program consists of the remain-
ing 8049 trees. Coverage in all cases in 989 sentences out of
1000. Results in brackets are from Zollman & Sima?an, 2005,
using a different train-test set split.
As these experiments show, adding larger elemen-
tary trees from the induced PTSG, in order of their
assigned scores, monotonously increases the parse
accuracy of the treebank PCFG. Although the final
grammar is at least 5 times larger than the origi-
nal treebank PCFG, and the parser therefore slower,
the grammar is orders of magnitude smaller than the
corresponding maximalist DOP models and shows
comparable parse accuracy.
For a second set of parsing experiments, we used
the WSJ portion of the Penn Tree Bank (Marcus et
al., 1993) and Helmut Schmid?s enrichment program
tmod (Schmid, 2006). Schmid?s program enriches
nonterminal labels in the treebank, using features in-
spired by (Klein and Manning, 2003). After enrich-
ment, Schmid obtained excellent parsing scores with
the treebank PCFG. In table 2, as model tb-pcfg, we
give our baseline results. These are slightly lower
than Schmid?s, for two reasons: (i) our implemen-
tation ignores the upper/lower case distinction, and
(ii) we do not use Schmid?s word class automaton
for unknown words (the only smoothing used is the
built-in feature of the BitPar parser, which extracts
an open-class-tag file from the lexicon file). Because
our interest here is in the principles of enrichment
we have not attempted to adapt these techniques for
our implementation.
As before, we ran the pnp program on the train-
set, the enriched sections 2-21 of the WSJ. For
computational reasons, pnp is only run on trees
with a yield of length (including punctuation) ?
20. This run, which took several days on a ma-
chine with 1.5Gb RAM, again produced a very large
PTSG, from which we extracted the 1000 and 10000
depth>1 elementary trees with the highest scores
for parsing experiments. Parsing and postprocess-
ing is performed as before, with the MPP approxi-
mated from the best n = 20 parses. Results from
these experiments are shown in table 2, as models
pnp1000 and pnp10000. With a small number of
added trees, we see a small drop in the parsing per-
formance, which we interpret as evidence that our
additions somewhat disturb the nicely tuned prob-
ability distribution of the treebank PCFG without
providing many advantages, because the most fre-
quent constructions have already been addressed in
the manual PCFG enrichment. However, with 10000
added subtrees we see an increase in parse accuracy,
providing evidence that pnp has learned potential
enrichments that go beyond the manual enrichment.
model LR LP F1 CM
tb-pcfg 83.27 83.53 83.40 26.58
pnp1000 83.20 83.47 83.33 26.70
pnp10000 83.56 83.99 83.77 26.93
Table 2: Results on the WSJ section of the Penn Tree Bank,
where nonterminals are enriched with features using Helmut
Schmid?s tmod program (Schmid, 2006). Reported are evalb
scores (ignoring punctuation) on 1699 sentences ? 100, includ-
ing punctuation, from section 22. Sections 02-21 were the train
set for the treebank PCFG; only trees with a yield (including
punctuation) of length ? 20 were used for the pnp program.
Coverage in all cases is 1691 (excluding failed parses gives
F1 = 85.19 for the tb-pcfg-baseline, and 85.54 for pnp10000).
In figure 8(left) we plot the difference in parse
accuracy between the treebank PCFG and our best
model per testset sentence. To make the plot more
informative, the sentences are ordered by increasing
558
difference. Hence, on the left are sentences where
the treebank PCFG scores better, and at the right
the sentence where pnp10000 scores best. As is
clear from this graph, for most sentences there is
no difference, but there are small and about equally
sized subsets of sentences for which one or the other
model scores better. We have briefly analysed these
sentences, but not found a clear pattern. In fig-
ure 8(right) we plot in a similar way the difference
in log-likelihood that the parsers assign to each sen-
tence. Here we see a clear pattern: only very few
sentences receive slightly higher likelihood under
the PCFG model. For a good portion of the sen-
tences, however, the pnp10000 model assigns them
somewhat and in some cases much higher likeli-
hood. The highest likelihood gains are due to a small
number of frequent multiword expressions, such as
?New York Stock Exchange Composite Trading?,
which P-DOP treats as a unit; all of the other gains
in likelihood are also due to the use of depth>1 ele-
mentary trees, including some non-contiguous con-
structions such as revenues rose CD % to $ CD mil-
lion from $ CD million.
-60
-40
-20
 0
 20
 40
 60
 0  200  400  600  800  1000  1200  1400  1600  1800
F-score difference (pnp-tbg)
-10
 0
 10
 20
 30
 40
 50
 60
 70
 0  200  400  600  800  1000  1200  1400  1600  1800
Log-likelihood difference (pnp-tbg)
Figure 8: Per sentence difference in f-score (left) and log-
likelihood (right) of the sentences of WSJ section 22. The x-
axis gives the sentence-rank when sentences are ordered from
small to large on y-axis value.
7 Discussion and Conclusions
We set out to develop a parsimonious approach to
Data-Oriented Parsing, where all subtrees can poten-
tially become units in a probabilistic grammar but
only if the statistics require it. The grammars re-
sulting from our algorithm are orders of magnitude
smaller than those used in Bod?s maximalist DOP.
Although our parsing results are not yet at the level
of the best results obtained by Bod, our results in-
dicate that we are getting closer and that we already
induce linguistically more plausible grammars.
Could P-DOP eventually not only be more effi-
cient, but also more accurate than maximalist DOP
models? Bod has argued that the explanation for
DOP?s excellent results is that it takes into account
all possible dependencies between productions in
a tree, and not just those from an a-priori chosen
subset (e.g. lexical, head, parent features). Non-
head dependencies in non-contiguous natural lan-
guage constructions, like more ... than, as in more
freight than cargo, are typically excluded in the en-
richment/conditioning approaches discussed in sec-
tion 2. Bod wants to include any dependency a pri-
ori, and then ?let the statistics decide?.
Although the inclusion of all dependencies must
somehow explain the performance difference be-
tween Bod?s best generative model and manually en-
riched PCFG models, this explanation is not entirely
satisfactory. Zuidema (2006a) shows that also the
estimator (Bod, 2003) uses is biased and inconsis-
tent, and will, even in the limit of infinite data, not
correctly identify many possible distributions over
trees. This is not just a theoretical problem. For
instance, in the Penn Tree Bank the construction
won?t VP is annotated as (VP (MD wo) (RB n?t) VP).
There is a strong dependency between the two mor-
phemes: wo doesn?t exist as an independent word,
and strongly predicts n?t. However, Bod?s estimator
will continue to reserve probability mass for other
combinations with the same POS-tags such as wo
not, even with an infinite data set only containing
will not and wo n?t. Because in parsing the strings
are given, this particular example will not harm the
parse accuracy results. The example might be di-
agnostic for other cases that do, however, and cer-
tainly will have impact when DOP is used as lan-
guage model. P-DOP, in contrast, does converge to
grammars that treat won?t as a single unit.
The exact relation of P-DOP to other DOP mod-
els, including S-DOP (Bod, 2003), Backoff-DOP
(Sima?an and Buratto, 2003), DOP* (Zollmann and
Sima?an, 2005) and ML-DOP (Bod, 2006; based on
Expectation Maximization) and not dissimilar au-
tomatic enrichment models such as (Petrov et al,
2006), remains a topic for future work.
Acknowledgments Funded by the Netherlands
Organisation for Scientific Research (EW), project
nr. 612.066.405; many thanks to Reut Tsarfaty,
Remko Scha, Rens Bod and three anonymous re-
viewers for their comments.
559
References
Rens Bod. 1998. Beyond Grammar: An experience-based theory of language. CSLI, Stanford, CA.
Rens Bod. 2001. What is the minimal set of fragmentsthat achieves maximal parse accuracy? In Proceed-ings ACL-2001.
Rens Bod. 2003. An efficient implementation of a newDOP model. In Proceedings EACL?03.
Rens Bod. 2006. An all-subtrees approach to unsuper-vised parsing. Proceedings ACL-COLING?06.
Eugene Charniak. 1996. Tree-bank grammars. Techni-
cal report, Department of Computer Science, BrownUniversity
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-ings of the fourteenth national conference on artificialintelligence, Menlo Park. AAAI Press/MIT Press.
Michael Collins and Nigel Duffy. 2002. New rankingalgorithms for parsing and tagging: Kernels over dis-crete structures, and the voted perceptron. In Proceed-ings ACL 2002.
Michael Collins. 1997. Three generative, lexicalizedmodels for statistical parsing. In Philip R. Cohen
and Wolfgang Wahlster, editors, Proceedings ACL?97,pages 16?23.
Joshua Goodman. 1996. Efficient algorithms for parsingthe DOP model. In Proceedings EMNLP, pages 143?152.
Mark Johnson. 1998. PCFG models of linguis-tic tree representations. Computational Linguistics,24(4):613?632.
Mark Johnson. 2002. The DOP estimation method isbiased and inconsistent. Computational Linguistics,28(1):71?76.
Dan Klein and Christopher D. Manning. 2003. Accurateunlexicalized parsing. In Proceedings ACL?03.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.1993. Building a large annotated corpus of En-glish: The Penn Treebank. Computational Linguistics,
19(2).
T. Matzuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-bilistic CFG with latent annotations. In ProceedingsACL?05, pages 75?82.
Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings ACL-COLING?06, pages 443?440.
Detlef Prescher. 2005. Inducing head-driven PCFGs
with latent heads: Refining a tree-bank grammar forparsing. In Proceedings ECML?05.
Remko Scha. 1990. Taaltheorie en taaltechnolo-gie; competence en performance. In R. de Kort
and G.L.J. Leerdam, editors, Computertoepassin-gen in de Neerlandistiek, pages 7?22. LVVN,Almere, the Netherlands. English translation at
http://iaaa.nl/rs/LeerdamE.html.
Helmut Schmid. 2004. Efficient parsing of highly am-biguous context-free grammars with bit vectors. InProceedings COLING 2004.
Helmut Schmid. 2006. Trace prediction and recovery
with unlexicalized PCFGs and slash features. In Pro-ceedings of COLING-ACL 2006.
Khalil Sima?an. 2002. Computational complexity ofprobabilistic disambiguation. Grammars, 5(2):125?
151.
Khalil Sima?an and Luciano Buratto. 2003. Backoff pa-rameter estimation for the DOP model. In Proceedingsof the 14th European Conference on Machine Learn-ing (ECML?03, Cavtat-Dubrovnik, Croatia, number2837 in Lecture Notes in Artificial Intelligence, pages373?384. Springer Verlag, Berlin, Germany.
Gert Veldhuijzen van Zanten, Gosse Bouma, Khalil
Sima?an, Gertjan van Noord, and Remko Bonnema.1999. Evaluation of the NLP components of theOVIS2 spoken dialogue system. In van Eynde, Schu-
urman, and Schelkens, editors, Computational Lin-guistics in the Netherlands 1998, pages 213?229.Rodopi, Amsterdam.
Andreas Zollmann and Khalil Sima?an. 2005. A consis-
tent and efficient estimator for data-oriented parsing.Journal of Automata, Languages and Combinatorics,10(2/3):367?388.
Willem Zuidema. 2006a. Theoretical evalua-
tion of estimation methods for Data-OrientedParsing. In Proceedings EACL 2006 (Con-ference Companion), pages 183?186. Associa-
tion for Computational Linguistics. Erratum onhttp://staff.science.uva.nl/?jzuidema/research.
Willem Zuidema. 2006b. What are the productive unitsof natural language grammar? A DOP approach to the
automatic identification of constructions. In Proceed-ings of the 10th International Conference on Compu-tational Natural Language Learning (CONLL-X).
560
Theoretical Evaluation of Estimation Methods for Data-Oriented Parsing
Willem Zuidema
Institute for Logic, Language and Computation
University of Amsterdam
Plantage Muidergracht 24, 1018 TV, Amsterdam, the Netherlands.
jzuidema@science.uva.nl
Abstract
We analyze estimation methods for Data-
Oriented Parsing, as well as the theoret-
ical criteria used to evaluate them. We
show that all current estimation methods
are inconsistent in the ?weight-distribution
test?, and argue that these results force us
to rethink both the methods proposed and
the criteria used.
1 Introduction
Stochastic Tree Substitution Grammars (hence-
forth, STSGs) are a simple generalization of Prob-
abilistic Context Free Grammars, where the pro-
ductive elements are not rewrite rules but elemen-
tary trees of arbitrary size. The increased flexibil-
ity allows STSGs to model a variety of syntactic
and statistical dependencies, using relatively com-
plex primitives but just a single and extremely sim-
ple global rule: substitution. STSGs can be seen as
Stochastic Tree Adjoining Grammars without the
adjunction operation.
STSGs are the underlying formalism of most in-
stantiations of an approach to statistical parsing
known as ?Data-Oriented Parsing? (Scha, 1990;
Bod, 1998). In this approach the subtrees of the
trees in a tree bank are used as elementary trees of
the grammar. In most DOP models the grammar
used is an STSGwith, in principle, all subtrees1 of
the trees in the tree bank as elementary trees. For
disambiguation, the best parse tree is taken to be
the most probable parse according to the weights
of the grammar.
Several methods have been proposed to decide
on the weights based on observed tree frequencies
1A subtree t? of a parse tree t is a tree such that every node
i? in t? equals a node i in t, and i? either has no daughters or
the same daughter nodes as i.
in a tree bank. The first such method is now known
as ?DOP1? (Bod, 1993). In combination with
some heuristic constraints on the allowed subtrees,
it has been remarkably successful on small tree
banks. Despite this empirical success, (Johnson,
2002) argued that it is inadequate because it is bi-
ased and inconsistent. His criticism spearheaded
a number of other methods, including (Bonnema
et al, 1999; Bod, 2003; Sima?an and Buratto,
2003; Zollmann and Sima?an, 2005), and will be
the starting point of our analysis. As it turns out,
the DOP1 method really is biased and inconsis-
tent, but not for the reasons Johnson gives, and it
really is inadequate, but not because it is biased
and inconsistent. In this note, we further show that
alternative methods that have been proposed, only
partly remedy the problems with DOP1, leaving
weight estimation as an important open problem.
2 Estimation Methods
The DOP model and STSG formalism are de-
scribed in detail elsewhere, for instance in (Bod,
1998). The main difference with PCFGs is that
multiple derivations, using elementary trees with
a variety of sizes, can yield the same parse tree.
The probability of a parse p is therefore given by:
P (p) =
?
d:d?=p P (d), where d? is the tree derived
by derivation d, P (d) = ?t?d w(t) and w(t) gives
the weights of elementary trees t, which are com-
bined in the derivation d (here treated as a multi-
set).
2.1 DOP1
In Bod?s original DOP implementation (Bod,
1993; Bod, 1998), henceforth DOP1, the weights
of an elementary tree t is defined as its relative
frequency (relative to other subtrees with the same
root label) in the tree bank. That is, the weight
183
wi = w(ti) of an elementary tree ti is given by:
wi =
fi
?
j:r(tj)=r(ti)(fj)
, (1)
where fi = f(ti) gives the frequency of subtree ti
in a corpus, and r(ti) is the root label of ti.
In his critique of this method, (Johnson, 2002)
considers a situation where there is an STSG G
(the target grammar) with a specific set of sub-
trees (t1 . . . tN ) and specific values of the weights
(w1 . . . wN ) . He evaluates an estimation proce-
dure which produces a grammar G? (the estimated
grammar), by looking at the difference between
the weights of G and the expected weights of G?.
Johnson?s test for consistency is thus based on
comparing the weight-distributions between target
grammar and estimated grammar2. I will therefore
refer to this test as the ?weight-distribution test?.
t1 = S
A
a
A
a
t2 =S
A
a
A
t3 =S
A A
a
t5 =S
A
a
t4 =S
A A
t6 =S
A
t7 =A
a
Figure 1: The example of (Johnson, 2002)
(Johnson, 2002) looks at an example grammar
G ? STSG with the subtrees as in figure 1. John-
son considers the case where the weights of all
trees of the target grammar G are 0, except for
w7, which is necessarily 1, and w4 and w6 which
are w4 = p and w6 = 1 ? p. He finds that the
expected values of the weights w4 and w6 of the
estimated grammar G? are:
E[w?4] =
p
2 + 2p, (2)
E[w?6] =
1 ? p
2 + 2p, (3)
which are not equal to their target values for all
values of p where 0 < p < 1. This analysis
thus shows that DOP1 is unable to recover the true
weights of the given STSG, and hence the incon-
sistency of the estimator with respect to the class
of STSGs.
Although usually cited as showing the inad-
equacy of DOP1, Johnson?s example is in fact
2More precisely, it is based on evaluating the estimator?s
behavior for any weight-distribution possible in the STSG
model. (Prescher et al, 2003) give a more formal treatment
of bias and consistency in the context of DOP.
not suitable to distinguish DOP1 from alternative
methods, because no possible estimation proce-
dure can recover the true weights in the case con-
sidered. In the example there are only two com-
plete trees that can be observed in the training
data, corresponding to the trees t1 and t5. It is
easy to see that when generating examples with
the grammar in figure 1, the relative frequencies3
f1 . . . f4 of the subtrees t1 . . . t4 must all be the
same, and equal to the frequency of the complete
tree t1 which can be composed in the following
ways from the subtrees in the original grammar:
t1 = t2 ? t7 = t3 ? t7 = t4 ? t7 ? t7. (4)
It follows that the expected frequencies of each of
these subtrees are:
E[f1] = E[f2] = E[f3] = E[f4] (5)
= w1 + w2w7 + w3w7 + w4w7w7
Similarly, the other frequencies are given by:
E[f5] = E[f6] = w5 + w6w7 (6)
E[f7] = 2 (w1 + w2w7 + w3w7
+w4w7w7) + w5 + w6w7
= 2E[f1] + E[f5]. (7)
From these equations it is immediately clear
that, regardless of the amount of training data,
the problem is simply underdetermined. The val-
ues of 6 weights w1 . . . w6 (w7 = 1) given only
2 frequencies f1 and f5 (and the constraint that
?6
i=1(fi) = 1) are not uniquely defined, and no
possible estimation method will be able to reliably
recover the true weights.
The relevant test is whether for all possible
STSGs and in the limit of infinite data, the ex-
pected relative frequencies of trees given the es-
timated grammar, equal the observed relative fre-
quencies. I will refer to this test as the ?frequency-
distribution test?. As it turns out, the DOP1
method also fails this more lenient test. The easi-
est way to show this, using again figure 1, is as fol-
lows. The weights w?1 . . . w?7 of grammar G? will ?
by definition ? be set to the relative frequencies of
the corresponding subtrees:
w?i =
{ fi
P6
j=1 fj
for i = 1 . . . 6
1 for i = 7. (8)
3Throughout this paper I take frequencies fi to be relative
to the size of the corpus.
184
The grammar G? will thus produce the complete
trees t1 and t5 with expected frequencies:
E[f ?1] = w?1 + w?2w?7 + w?3w?7 + w?4w?7w?7
= 4 f1
?6
j=1 fj
(9)
E[f ?5] = w?5 + w?6w?7 = 2
f5
?6
j=1 fj
. (10)
Now consider the two possible complete trees
t1 and t5, and the fraction of their frequencies
f1/f5. In the estimated grammar G? this fraction
becomes:
E[f ?1]
E[f ?5]
=
4n f1P6
j=1 fj
2n f5P6
j=1 fj
= 2f1f5
. (11)
That is, in the limit of infinite data, the estima-
tion procedure not only ?understandably? fails to
find the target grammar amongst the many gram-
mars that could have produced the observed fre-
quencies, it in fact chooses a grammar that could
never have produced these observed frequencies
at all. This example shows the DOP1 method is
biased and inconsistent for the STSG class in the
frequency-distribution test4.
2.2 Correction-factor approaches
Based on similar observation, (Bonnema et al,
1999; Bod, 2003) propose alternative estimation
methods, which involve a correction factor to
move probability mass from larger subtrees to
smaller ones. For instance, Bonnema et al replace
equation (1) with:
wi = 2?N(ti)
fi
?
j:r(tj)=r(ti)(fj)
, (12)
where N(ti) gives the number of internal nodes
in ti (such that 2?N(ti) is inversely proportional
to the number of possible derivations of ti). Sim-
ilarly, (Bod, 2003) changes the way frequencies
fi are counted, with a similar effect. This ap-
proach solves the specific problem shown in equa-
tion (11). However, the following example shows
that the correction-factor approaches cannot solve
the more general problem.
4Note that there are settings of the weights w1 . . . w7 that
generate a frequency-distribution that could also have been
generated with a PCFG. The example given applies to such
distribution as well, and therefore also shows the inconsis-
tency of the DOP1 method for PCFG distributions.
t1 = S
A
a
A
b
t2 = S
A
b
A
a
t3 = S
A
a
A
a
t4 = S
A
b
A
b
t5 =S
A
a
A
t6 =S
A A
b
t7 =S
A
b
A
t8 =S
A A
a
t9 =S
A A
t10 =A
a
t11 =A
b
Figure 2: Counter-example to the correction-
factor approaches
Consider the STSG in figure 2. The expected
frequencies f1 . . . f4 are here given by:
E[f1] = w1 + w5w11 + w6w10 + w9w10w11
E[f2] = w2 + w7w10 + w8w11 + w9w11w10
E[f3] = w3 + w5w10 + w8w10 + w9w10w10
E[f4] = w4 + w6w11 + w7w11 + w9w11w11
(13)
Frequencies f5 . . . f11 are again simple com-
binations of the frequencies f1 . . . f4. Observa-
tions of these frequencies therefore do not add
any extra information, and the problem of find-
ing the weights of the target grammar is in general
again underdetermined. But consider the situation
where f3 = f4 = 0 and f1 > 0 and f2 > 0.
This constrains the possible solutions enormously.
If we solve the following equations for w3 . . . w11
with the constraint that probabilities with the same
root label add up to 1: (i.e. ?9i=1(wi) = 1,
w10 + w11 = 1):
w3 + w5w10 + w8w10 + w9w10w10 = 0
w4 + w6w11 + w7w11 + w9w11w11 = 0,
we find, in addition to the obvious w3 = w4 = 0,
the following solutions: w10 = w6 = w7 = w9 =
0 ? w11 = w5 = w8 = w9 = 0 ? w5 =
w6 = w7 = w8 = w9 = 0. That is, if we ob-
serve no occurrences of trees t3 and t4 in the train-
ing sample, we know that at least one subtree in
each derivation of these strings must have weight
zero. However, any estimation method that uses
the (relative) frequencies of subtrees and a (non-
zero) correction factor that is based on the size of
the subtrees, will give non-zero probabilities to all
weights w5 . . . w11 if f1 > 0 and f2 > 0, as we
assumed. In other words, these weight estimation
methods for STSGs are also biased and inconsis-
tent in the frequency-distribution test.
185
2.3 Shortest derivation estimators
Because the STSG formalism allows elementary
trees of arbitrary size, every parse tree in a tree
bank could in principle be incorporated in an
STSG grammar. That is, we can define a trivial
estimator with the following weights:
wi =
{
fi if ti is an observed parse tree
0 otherwise
(14)
Such an estimator is not particularly interesting,
because it does not generalize beyond the training
data. It is a point to note, however, that this esti-
mator is unbiased and consistent in the frequency-
distribution test. (Prescher et al, 2003) prove that
any unbiased estimator that uses the ?all subtrees?
representation has the same property, and con-
clude that lack of bias is not a desired property.
(Zollmann and Sima?an, 2005) propose an esti-
mator based on held-out estimation. The training
corpus is split into an estimation corpus EC and a
held out corpus HC . The HC corpus is parsed
by searching for the shortest derivation of each
sentence, using only fragments from EC . The
elementary trees of the estimated STSG are as-
signed weights according to their usage frequen-
cies u1, . . . , uN in these shortest derivations:
wi =
ui
?
j:r(tj)=r(ti) uj
. (15)
This approach solves the problem with bias de-
scribed above, while still allowing for consistency,
as Zollmann & Sima?an prove. However, their
proof only concerns consistency in the frequency-
distribution test. As the corpus EC grows to be
infinitely large, every parse tree in HC will also
be found in EC , and the shortest derivation will
therefore in the limit only involve a single ele-
mentary tree: the parse tree itself. Target STSGs
with non-zero weights on smaller elementary trees
will thus not be identified correctly, even with an
infinitely large training set. In other words, the
Zollmann & Sima?an method, and other methods
that converge to the ?complete parse tree? solution
such as LS-DOP (Bod, 2003) and BackOff-DOP
(Sima?an and Buratto, 2003), are inconsistent in
the weight-distribution test.
3 Discussion & Conclusions
A desideratum for parameter estimation methods
is that they converge to the correct parameters with
infinitely many data ? that is, we like an estima-
tor to be consistent. The STSG formalism, how-
ever, allows for many different derivations of the
same parse tree, and for many different grammars
to generate the same frequency-distribution. Con-
sistency in the weight-distribution test is there-
fore too stringent a criterion. We have shown that
DOP1 and methods based on correction factors
also fail the weaker frequency-distribution test.
However, the only current estimation methods
that are consistent in the frequency-distribution
test, have the linguistically undesirable property
of converging to a distribution with all probabil-
ity mass in complete parse trees. Although these
method fail the weight-distribution test for the
whole class of STSGs, we argued earlier that this
test is not the appropriate test either. Both estima-
tion methods for STSGs and the criteria for eval-
uating them, thus require thorough rethinking. In
forthcoming work we therefore study yet another
estimator, and the linguistically motivated evalua-
tion criterion of convergence to a maximally gen-
eral STSG consistent with the training data5.
References
Rens Bod. 1993. Using an annotated corpus as a stochastic
grammar. In Proceedings EACL?93, pp. 37?44.
Rens Bod. 1998. Beyond Grammar: An experience-based
theory of language. CSLI, Stanford, CA.
Rens Bod. 2003. An efficient implementation of a new DOP
model. In Proceedings EACL?03.
Remko Bonnema, Paul Buying, and Remko Scha. 1999.
A new probability model for data oriented parsing. In
Paul Dekker, editor, Proceedings of the Twelfth Amster-
dam Colloquium. ILLC, University of Amsterdam.
Mark Johnson. 2002. The DOP estimation method is biased
and inconsistent. Computational Linguistics, 28(1):71?
76.
D. Prescher, R. Scha, K. Sima?an, and A. Zollmann. 2003.
On the statistical consistency of DOP estimators. In Pro-
ceedings CLIN?03, Antwerp, Belgium.
Remko Scha. 1990. Taaltheorie en taaltechnologie; compe-
tence en performance. In R. de Kort and G.L.J. Leerdam,
eds, Computertoepassingen in de Neerlandistiek, pages 7?
22. LVVN, Almere.http://iaaa.nl/rs/LeerdamE.html.
Khalil Sima?an and Luciano Buratto (2003). Backoff pa-
rameter estimation for the DOP model. In Proceedings
ECML?03, pp. 373?384. Berlin: Springer Verlag.
Andreas Zollmann and Khalil Sima?an. 2005. A consistent
and efficient estimator for data-oriented parsing. Journal
of Automata, Languages and Combinatorics. In press.
5The author is funded by NWO, project nr. 612.066.405,
and would like to thank the anonymous reviewers and several
colleagues for comments.
186
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 701?709,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Unsupervised Methods for Head Assignments
Federico Sangati, Willem Zuidema
Institute for Logic, Language and Computation
University of Amsterdam, the Netherlands
{f.sangati,zuidema}@uva.nl
Abstract
We present several algorithms for assign-
ing heads in phrase structure trees, based
on different linguistic intuitions on the role
of heads in natural language syntax. Start-
ing point of our approach is the obser-
vation that a head-annotated treebank de-
fines a unique lexicalized tree substitution
grammar. This allows us to go back and
forth between the two representations, and
define objective functions for the unsu-
pervised learning of head assignments in
terms of features of the implicit lexical-
ized tree grammars. We evaluate algo-
rithms based on the match with gold stan-
dard head-annotations, and the compar-
ative parsing accuracy of the lexicalized
grammars they give rise to. On the first
task, we approach the accuracy of hand-
designed heuristics for English and inter-
annotation-standard agreement for Ger-
man. On the second task, the implied lex-
icalized grammars score 4% points higher
on parsing accuracy than lexicalized gram-
mars derived by commonly used heuris-
tics.
1 Introduction
The head of a phrasal constituent is a central
concept in most current grammatical theories and
many syntax-based NLP techniques. The term is
used to mark, for any nonterminal node in a syn-
tactic tree, the specific daughter node that fulfills
a special role; however, theories and applications
differ widely in what that special role is supposed
to be. In descriptive grammatical theories, the
role of the head can range from the determinant of
agreement or the locus of inflections, to the gover-
nor that selects the morphological form of its sis-
ter nodes or the constituent that is distributionally
equivalent to its parent (Corbett et al, 2006).
In computational linguistics, heads mainly
serve to select the lexical content on which the
probability of a production should depend (Char-
niak, 1997; Collins, 1999). With the increased
popularity of dependency parsing, head annota-
tions have also become a crucial level of syntac-
tic information for transforming constituency tree-
banks to dependency structures (Nivre et al, 2007)
or richer syntactic representations (e.g., Hocken-
maier and Steedman, 2007).
For the WSJ-section of the Penn Treebank, a set
of heuristic rules for assigning heads has emerged
from the work of (Magerman, 1995) and (Collins,
1999) that has been employed in a wide variety of
studies and proven extremely useful, even in rather
different applications from what the rules were
originally intended for. However, the rules are
specific to English and the treebank?s syntactic an-
notation, and do not offer much insights into how
headedness can be learned in principle or in prac-
tice. Moreover, the rules are heuristic and might
still leave room for improvement with respect to
recovering linguistic head assignment even on the
Penn WSJ corpus; in fact, we find that the head-
assignments according to the Magerman-Collins
rules correspond only in 85% of the cases to de-
pendencies such as annotated in PARC 700 De-
pendency Bank (see section 5).
Automatic methods for identifying heads are
therefore of interest, both for practical and more
fundamental linguistic reasons. In this paper we
investigate possible ways of finding heads based
on lexicalized tree structures that can be extracted
from an available treebank. The starting point
of our approach is the observation that a head-
annotated treebank (obeying the constraint that ev-
ery nonterminal node has exactly one daughter
marked as head) defines a unique lexicalized tree
substitution grammar (obeying the constraint that
every elementary tree has exactly one lexical an-
chor). This allows us to go back and forth between
701
the two representations, and define objective func-
tions for the unsupervised learning of head assign-
ments in terms of features of the implicit Lexical-
ized Tree Substitution Grammars.
Using this grammar formalism (LTSGs) we will
investigate which objective functions we should
optimize for recovering heads. Should we try to
reduce uncertainty about the grammatical frames
that can be associated with a particular lexical
item? Or should we assume that linguistic head
assignments are based on the occurrence frequen-
cies of the productive units they imply?
We present two new algorithms for unsuper-
vised recovering of heads ? entropy minimization
and a greedy technique we call ?familiarity max-
imization? ? that can be seen as ways to opera-
tionalize these last two linguistic intuitions. Both
algorithms are unsupervised, in the sense that they
are trained on data without head annotations, but
both take labeled phrase-structure trees as input.
Our work fits well with several recent ap-
proaches aimed at completely unsupervised learn-
ing of the key aspects of syntactic structure: lex-
ical categories (Schu?tze, 1993), phrase-structure
(Klein and Manning, 2002; Seginer, 2007),
phrasal categories (Borensztajn and Zuidema,
2007; Reichart and Rappoport, 2008) and depen-
dencies (Klein and Manning, 2004).
For the specific task addressed in this paper ?
assigning heads in treebanks ? we only know of
one earlier paper: Chiang and Bikel (2002). These
authors investigated a technique for identifying
heads in constituency trees based on maximiz-
ing likelihood, using EM, under a Tree Insertion
Grammar (TIG)model1. In this approach, headed-
ness in some sense becomes a state-split, allowing
for grammars that more closely match empirical
distributions over trees. The authors report some-
what disappointing results, however: the automat-
ically induced head-annotations do not lead to sig-
nificantly more accurate parsers than simple left-
most or rightmost head assignment schemes2.
In section 2 we define the grammar model we
will use. In section 3 we describe the head-
assignment algorithms. In section 4, 5 and 6 we
1The space over the possible head assignments that these
authors consider ? essentially regular expressions over CFG
rules ? is more restricted than in the current work where we
consider a larger ?domain of locality?.
2However, the authors? approach of using EM for induc-
ing latent information in treebanks has led to extremely ac-
curate constituency parsers, that neither make use of nor pro-
duce headedness information; see (Petrov et al, 2006)
then describe our evaluations of these algorithms.
2 Lexicalized Tree Grammars
In this section we define Lexicalised Tree Substi-
tution Grammars (LTSGs) and show how they can
be read off unambiguously from a head-annotated
treebank. LTSGs are best defined as a restriction
of the more general Probabilistic Tree Substitution
Grammars, which we describe first.
2.1 Tree Substitution Grammars
A tree substitution grammar (TSG) is a 4-tuple
?Vn, Vt, S, T ? where Vn is the set of nonterminals;
Vt is the set of of terminals; S ? Vn is the start
symbol; and T is the set of elementary trees, hav-
ing root and internal nodes in Vn and leaf nodes in
Vn?Vt. Two elementary trees ? and ? can be com-
bined by means of the substitution operation ? ??
to produce a new tree, only if the root of ? has the
same label of the leftmost nonterminal leaf of ?.
The combined tree corresponds to ? with the left-
most nonterminal leaf replaced with ?. When the
tree resulting from a series of substitution opera-
tions is a complete parse tree, i.e. the root is the
start symbol and all leaf nodes are terminals, we
define the sequence of the elementary trees used
as a complete derivation.
A probabilistic TSG defines a probabilistic
space over the set of elementary trees: for every
? ? T , P (?) ? [0, 1] and
?
? ?:r(? ?)=r(?) P (?
?) =
1, where r(?) returns the root node of ? . Assum-
ing subsequent substitutions are stochastically in-
dependent, we define the probability of a deriva-
tion as the product of the probability of its elemen-
tary trees. If a derivation d consists of n elemen-
tary trees ?1 ? ?2 ? . . . ? ?n, we have:
P (d) =
n?
i=1
P (?i) (1)
Depending on the set T of elementary trees, we
might have different derivations producing the
same parse tree. For any given parse tree t, we
define ?(t) as the set of its derivations licensed by
the grammar. Since any derivation d ? ?(t) is a
possible way to construct the parse tree, we will
compute the probability of a parse tree as the sum
of the probabilities of its derivations:
P (t) =
?
d??(t)
?
??d
P (?) (2)
702
Lexicalized Tree Substitution Grammars are de-
fined as TSGs with the following contraint on the
set of elementary trees T : every ? in T must have
at least one terminal (the lexical anchor) among
its leaf nodes. In this paper, we are only con-
cerned with single-anchored LTSGs, in which all
elementary trees have exactly one lexical anchor.
Like TSGs, LTSGs have a weak generative ca-
pacity that is context-free; but whereas PTSGs are
both probabilistically and in terms of strong gen-
erative capacity richer than PCFGs (Bod, 1998),
LTSG are more restricted (Joshi and Schabes,
1991). This limits the usefulness of LTSGs for
modeling the full complexity of natural language
syntax; however, computationally, LTSGs have
many advantages over richer formalisms and for
the current purposes represent a useful compro-
mise between linguistic adequacy and computa-
tional complexity.
2.2 Extracting LTSGs from a head-annotated
corpus
In this section we will describe a method for as-
signing to each word token that occurs in the cor-
pus a unique elementary tree. This method de-
pends on the annotation of heads in the treebank,
such as for instance provided for the Penn Tree-
bank by the Magerman-Collins head-percolation
rules. We adopt the same constraint as used in this
scheme, that each nonterminal node in every parse
tree must have exactly one of its children anno-
tated as head. Our method is similar to (Chiang,
2000), but is even simpler in ignoring the distinc-
tion between arguments and adjuncts (and thus the
sister-adjunction operation). Figure 1 shows an
example parse tree enriched with head-annotation:
the suffix -H indicates that the specific node is the
head of the production above it.
S
NP
NNP
Ms.
NNP-H
Haag
VP-H
V-H
plays
NP
NNP-H
Elianti
Figure 1: Parse tree of the sentence ?Ms. Haag
plays Elianti? annotated with head markers.
Once a parse tree is annotated with head mark-
ers in such a manner, we will be able to extract
for every leaf its spine. Starting from each lexical
production we need to move upwards towards the
root on a path of head-marked nodes until we find
the first internal node which is not marked as head
or until we reach the root of the tree. In the ex-
ample above, the verb of the sentence ?plays? is
connected through head-marked nodes to the root
of the tree. In this way we can extract the 4 spines
from the parse tree in figure 1, as shown in fig-
ure 2.
NNP
Ms.
NP
NNP-H
Haag
S-H
VP-H
V-H
plays
NP
NNP-H
Elianti
Figure 2: The lexical spines of the tree in fig. 1.
It is easy to show that this procedure yields a
unique spine for each of its leaves, when applied
to a parse tree where all nonterminals have a single
head-daughter and all terminals are generated by a
unary production. Having identified the spines, we
convert them to elementary trees, by completing
every internal node with the other daughter nodes
not on the spine. In this way we have defined a
way to obtain a derivation of any parse tree com-
posed of lexical elementary trees. The 4 elemen-
tary trees completed from the previous paths are in
figure 3 with the substitution sites marked with ?.
NNP
Ms.
NP
NNP? NNP-H
Haag
S-H
NP? VP-H
V-H
plays
NP?
NP
NNP-H
Elianti
Figure 3: The extracted elementary trees.
3 Head Assignment Algorithms
We investigate two novel approaches to automat-
ically assign head dependencies to a training cor-
pus where the heads are not annotated: entropy
minimization and familiarity maximization. The
baselines for our experiments will be given by the
Magerman and Collins scheme together with the
random, the leftmost daughter, and the rightmost
daughter-based assignments.
703
3.1 Baselines
The Magerman-Collins scheme, and very similar
versions, are well-known and described in detail
elsewhere (Magerman, 1995; Collins, 1999; Ya-
mada and Matsumoto, 2003); here we just men-
tion that it is based on a number of heuristic rules
that only use the labels of nonterminal nodes and
the ordering of daughter nodes. For instance if the
root label of a parse tree is S, the head-percolation
scheme will choose to assign the head marker to
the first daughter from the left, labeled with TO.
If no such label is present, it will look for the first
IN. If no IN is found, it will look for the first VP,
and so on. We used the freely available software
?Treep? (Chiang and Bikel, 2002) to annotate the
Penn WSJ treebank with heads.
We consider three other baselines, that are ap-
plicable to other treebanks and other languages as
well: RANDOM, where, for every node in the tree-
bank, we choose a random daughter to be marked
as head; LEFT, where the leftmost daughter is
marked; and RIGHT, where the rightmost daughter
is marked.
3.2 Minimizing Entropy
In this section we will describe an entropy based
algorithm, which aims at learning the simplest
grammar fitting the data. Specifically, we take a
?supertagging? perspective (Bangalore and Joshi,
1999) and aim at reducing the uncertainty about
which elementary tree (supertag) to assign to a
given lexical item. We achieve this by minimizing
an objective function based on the general defini-
tion of entropy in information theory.
The entropy measure that we are going to de-
scribe is calculated from the bag of lexicalized el-
ementary trees T extracted from a given training
corpus of head annotated parse trees. We define
Tl as a discrete stochastic variable, taking as val-
ues the elements from the set of all the elementary
trees having l as lexical anchor {?l1 , ?l2 , . . . , ?ln}.
Tl thus takes n possible values with specific prob-
abilities; its entropy is then defined as:
H(Tl) = ?
n?
i=1
p(?li) log2 p(?li) (3)
The most intuitive way to assign probabilities to
each elementary tree is considering its relative fre-
quency in T . If f(?) is the frequency of the frag-
ment ? and f(l) is the total frequency of fragments
with l as anchor we will have:
p(?lj ) =
f(?lj )
f(lex(?lj ))
=
f(?lj )
n?
i=1
f(?li))
(4)
We will then calculate the entropy H(T ) of our
bag of elementary trees by summing the entropy of
each single discrete stochastic variable Tl for each
choice of l:
H(T ) =
|L |?
l=1
H(Tl) (5)
In order to minimize the entropy, we apply a
hill-climbing strategy. The algorithm starts from
an already annotated tree-bank (for instance using
the RANDOM annotator) and iteratively tries out
a random change in the annotation of each parse
tree. Only if the change reduces the entropy of the
entire grammar it is kept. These steps are repeated
until no further modification which could reduce
the entropy is possible. Since the entropy measure
is defined as the sum of the function p(?) log2 p(?)
of each fragment ? , we do not need to re-calculate
the entropy of the entire grammar, when modify-
ing the annotation of a single parse tree. In fact:
H(T ) = ?
|L |?
l=1
n?
i=1
p(?li) log2 p(?li)
= ?
|T |?
j=1
p(?j) log2 p(?j)
(6)
For each input parse tree under consideration,
the algorithm selects a non-terminal node and tries
to change the head annotation from its current
head-daughter to a different one. As an example,
considering the parse tree of figure 1 and the inter-
nal node NP (the leftmost one), we try to annotate
its leftmost daughter as the new head. When con-
sidering the changes that this modification brings
on the set of the elementary trees T , we understand
that there are only 4 elementary trees affected, as
shown in figure 4.
After making the change in the head annotation,
we just need to decrease the frequencies of the old
trees by one unit, and increase the ones of the new
trees by one unit. The change in the entropy of our
grammar can therefore be computed by calculat-
ing the change in the partial entropy of these four
704
NP
NNP NNP
Haag
NNP
Ms.
NP
NNP
Ms.
NNP
NNP
Haag
?h ?d ? ?h ?
?
d
Figure 4: Lexical trees considered in the EN-
TROPY algorithm when changing the head ass-
ingnment from the second NNP to the first NNP
of the leftmost NP node of figure 1. ?h is the old
head tree; ?d the old dependent tree; ? ?d the new
dependent tree; ? ?h the new head tree.
elementary trees before and after the change. If
such change results in a lower entropy of the gram-
mar, the new annotation is kept, otherwise we go
back to the previous annotation. Although there is
no guarantee our algorithm finds the global min-
imum, it is very efficient and succeeds in drasti-
cally minimize the entropy from a random anno-
tated corpus.
3.3 Maximizing Familiarity
The main intuition behind our second method is
that we like to assign heads to a tree t in such
a way that the elementary trees that we can ex-
tract from t are frequently observed in other trees
as well. That is, we like to use elementary trees
which are general enough to occur in many possi-
ble constructions.
We start with building the bag of all one-anchor
lexicalized elementary trees from the training cor-
pus, consistent with any annotation of the heads.
This operation is reminiscent of the extraction of
all subtrees in Data-Oriented Parsing (Bod, 1998).
Fortunately, and unlike DOP, the number of possi-
ble lexicalised elementary trees is not exponential
in sentence length n, but polynomial: it is always
smaller than n2 if the tree is binary branching.
Next, for each node in the treebank, we need
to select a specific lexical anchor, among the ones
it dominates, and annotate the nodes in the spine
with head annotations. Our algorithm selects the
lexical anchor which maximizes the frequency of
the implied elementary tree in the bag of elemen-
tary trees. In figure 5, algorithm 1 (right) gives the
pseudo-code for the algorithm, and the tree (left)
shows an example of its usage.
3.4 Spine and POS-tag reductions
The two algorithms described in the previous two
sections are also evaluated when performing two
possible generalization operations on the elemen-
tary trees, which can be applied both alone or in
combination:
? in the spine reduction, lexicalized trees are
transformed to their respective spines. This
allows to merge elementary trees that are
slightly differing in argument structures.
? in the POStag reduction, every lexical item
of every elementary tree is replaced by its
POStag category. This allows for merging el-
ementary trees with the same internal struc-
ture but differing in lexical production.
4 Implementation details
4.1 Using CFGs for TSG parsing
When evaluating parsing accuracy of a given
LTSG, we use a CKY PCFG parser. We will
briefly describe how to set up an LTSG parser us-
ing the CFG formalism. Every elementary tree
in the LTSG should be treated by our parser as
a unique block which cannot be further decom-
posed. But to feed it to a CFG-parser, we need
to break it down into trees of depth 1. In order to
keep the integrity of every elementary tree we will
assign to its internal node a unique label. We will
achieve this by adding ?@i? to each i-th internal
node encountered in T .
Finally, we read off a PCFG from the elemen-
tary trees, assigning to each PCFG rule a weight
proportional to the weight of the elementary tree it
is extracted from. In this way the PCFG is equiv-
alent to the original LTSG: it will produce exactly
the same derivation trees with the same probabil-
ities, although we would have to sum over (expo-
nentially) many derivations to obtain the correct
probabilities of a parse tree (derived tree). We ap-
proximate parse probability by computing the n-
best derivations and summing over the ones that
yield the same parse tree (by removing the ?@i?-
labels). We then take the parse tree with highest
probability as best parse of the input sentence.
4.2 Unknown words and smoothing
We use a simple strategy to deal with unknown
words occurring in the test set. We replace all the
words in the training corpus occurring once, and
all the unknown words in the test set, with a spe-
cial *UNKNOWN* tag. Moreover we replace all
the numbers in the training and test set with a spe-
cial *NUMBER* tag.
705
Algorithm 1: MaximizeFamiliarity(N)
Input: a non-terminal node N of a parsetree.
begin
L = null;MAX = ?1;
foreach leaf l underN do
?Nl = lex. tree rooted in N and anchored in l;
F = frequency of ?Nl ;
if F > MAX then
L = l;MAX = F ;
Mark all nodes in the path fromN to L with heads;
foreach substitution siteNi of ?NL do
MaximizeFamiliarity(Ni);
end
Figure 5: Left: example of a parse tree in an instantiation of the ?Familiarity? algorithm. Each arrow,
connecting a word to an internal node, represents the elementary tree anchored in that word and rooted
in that internal node. Numbers in parentheses give the frequencies of these trees in the bag of subtrees
collected from WSJ20. The number below each leaf gives the total frequency of the elementary trees
anchored in that lexical item. Right: pseudo-code of the ?Familiarity? algorithm.
Even with unknown words treated in this way,
the lexicalized elementary trees that are extracted
from the training data are often too specific to
parse all sentences in the test set. A simple strat-
egy to ensure full coverage is to smooth with the
treebank PCFG. Specifically, we add to our gram-
mars all CFG rules that can be extracted from the
training corpus and give them a small weight pro-
portional to their frequency3. This in general will
ensure coverage, i.e. that all the sentences in the
test set can be successfully parsed, but still priori-
tizing lexicalized trees over CFG rules4.
4.3 Corpora
The evaluations of the different models were car-
ried out on the Penn Wall Street Journal corpus
(Marcus et al, 1993) for English, and the Tiger
treebank (Brants et al, 2002) for German. As gold
standard head annotations corpora, we used the
Parc 700 Dependency Bank (King et al, 2003) and
the Tiger Dependency Bank (Forst et al, 2004),
which contain independent reannotations of ex-
tracts of the WSJ and Tiger treebanks.
5 Results
We evaluate the head annotations our algorithms
find in two ways. First, we compare the head
annotations to gold standard manual annotations
3In our implementation, each CFG rule frequency is di-
vided by a factor 100.
4In this paper, we prefer these simple heuristics over more
elaborate techniques, as our goal is to compare the merits of
the different head-assignment algorithms.
of heads. Second, we evaluate constituency pars-
ing performance using an LTSG parser (trained
on the various LTSGs), and a state-of-the-art
parser (Bikel, 2004).
5.1 Gold standard head annotations
Table 1 reports the performance of different al-
gorithms against gold standard head annotations
of the WSJ and the Tiger treebank. These an-
notations were obtained by converting the depen-
dency structures of the PARC corpus (700 sen-
tences from section 23) and the Tiger Dependency
Bank (2000 sentences), into head annotations5.
Since the algorithm doesn?t guarantee that the re-
covered head annotations always follow the one-
head-per-node constraint, when evaluating the ac-
curacy of head annotations of different algorithms,
we exclude the cases in which in the gold cor-
pus no head or multiple heads are assigned to the
daughters of an internal node6, as well as cases in
which an internal node has a single daughter.
In the evaluation against gold standard de-
pendencies for the PARC and Tiger dependency
banks, we find that the FAMILIARITY algorithm
when run with POStags and Spine conversion ob-
tains around 74% recall for English and 69% for
German. The different scores of the RANDOM as-
signment for the two languages can be explained
5This procedure is not reported here for reasons of space,
but it is available for other researchers (together with the ex-
tracted head assignments) at http://staff.science.
uva.nl/?fsangati.
6After the conversion, the percentage of incorrect heads
in PARC 700 is around 9%; in Tiger DB it is around 43%.
706
by their different branching factors: trees in the
German treebank are typically more flat than those
in the English WSJ corpus. However, note that
other settings of our two annotation algorithms do
not always obtain better results than random.
When focusing on the Tiger results, we ob-
serve that the RIGHT head assignment recall is
much better than the LEFT one. This result is in
line with a classification of German as a predomi-
nantly head-final language (in contrast to English).
More surprisingly, we find a relatively low recall
of the head annotation in the Tiger treebank, when
compared to a gold standard of dependencies for
the same sentences as given by the Tiger depen-
dency bank. Detailed analysis of the differences
in head assigments between the two approaches
is left for future work; for now, we note that our
best performing algorithm approaches the inter-
annotation-scheme agreement within only 10 per-
centage points7.
5.2 Constituency Parsing results
Table 2 reports the parsing performances of our
LTSG parser on different LTSGs extracted from
the WSJ treebank, using our two heuristics to-
gether with the 4 baseline strategies (plus the re-
sult of a standard treebank PCFG). The parsing re-
sults are computed on WSJ20 (WSJ sentences up
to length 20), using sections 02-21 for training and
section 22 for testing.
We find that all but one of the head-assignment
algorithms lead to LTSGs that without any fine-
tuning perform better than the treebank PCFG. On
this metric, our best performing algorithm scores
4 percentage points higher than the Magerman-
Collins annotation scheme (a 19% error reduc-
tion). The poor results with the RIGHT assign-
ment, in contrast with the good results with the
LEFT baseline (performing even better than the
Magerman-Collins assignments), are in line with
the linguistic tradition of listing English as a pre-
dominantly head-initial language. A surprising
result is that the RANDOM-assignment gives the
7We have also used the various head-assignments to con-
vert the treebank trees to dependency structures, and used
these in turn to train a dependency parser (Nivre et al, 2005).
Results from these experiments confirm the ordering of the
various unsupervised head-assignment algorithms. Our best
results, with the FAMILIARITY algorithm, give us an Unla-
beled Attachment Score (UAS) of slightly over 50% against
a gold standard obtained by applying the Collins-Magerman
rules to the test set. This is much higher than the three base-
lines, but still considerably worse than results based on su-
pervised head-assignments.
best performing LTSG among the baselines. Note,
however, that this strategy leads to much wield-
ier grammars; with many more elementary trees
than for instance the left-head assignment, the
RANDOM strategy is apparently better equipped
to parse novel sentences. Both the FAMILIAR-
ITY and the ENTROPY strategy are at the level of
the random-head assignment, but do in fact lead to
much more compact grammars.
We have also used the same head-enriched tree-
bank as input to a state-of-the-art constituency
parser8 (Bikel, 2004), using the same training and
test set. Results, shown in table 3, confirm that
the differences in parsing success due to differ-
ent head-assignments are relatively minor, and that
even RANDOM performs well. Surprisingly, our
best FAMILIARITY algorithm performs as well as
the Collins-Magerman scheme.
LFS UFS |T|
PCFG 78.23 82.12 -
RANDOM 82.70 85.54 64k
LEFT 80.05 83.21 46k
Magerman-Collins 79.01 82.67 54k
RIGHT 73.04 77.90 49k
FAMILIARITY 84.44 87.22 42k
ENTROPY-POStags 82.81 85.80 64k
FAMILIARITY-Spine 82.67 85.35 47k
ENTROPY-POStags-Spine 82.64 85.55 64k
Table 2: Parsing accuracy on WSJ20 of the LTSGs
extracted from various head assignments, when
computing the most probable derivations for ev-
ery sentence in the test set. The Labeled F-Score
(LFS) and unlabeled F-Score (UFS) results are re-
ported. The final column gives the total number of
extracted elementary trees (in thousands).
LFS UFS
Magerman-Collins 86.20 88.35
RANDOM 84.58 86.97
RIGHT 81.62 84.41
LEFT 81.13 83.95
FAMILIARITY-POStags 86.27 88.32
FAMILIARITY-POStags-Spine 85.45 87.71
FAMILIARITY-Spine 84.41 86.83
FAMILIARITY 84.28 86.53
Table 3: Evaluation on WSJ20 of various head as-
signments on Bikel?s parser.
8Although we had to change a small part of the code,
since the parser was not able to extract heads from an en-
riched treebank, but it was only compatible with rule-based
assignments. For this reason, results are reported only as a
base of comparison.
707
Gold = PARC 700 % correct
Magerman-Collins 84.51
LEFT 47.63
RANDOM 43.96
RIGHT 40.70
FAMILIARITY-POStags-Spine 74.05
FAMILIARITY-POStags 51.10
ENTROPY-POStags-Spine 43.23
FAMILIARITY-Spine 39.68
FAMILIARITY 37.40
Gold = Tiger DB % correct
Tiger TB Head Assignment? 77.39
RIGHT 52.59
RANDOM 38.66
LEFT 18.64
FAMILIARITY-POStags-Spine 68.88
FAMILIARITY-POStags 41.74
ENTROPY-POStags-Spine 37.99
FAMILIARITY 26.08
FAMILIARITY-Spine 22.21
Table 1: Percentage of correct head assignments against gold standard in Penn WSJ and Tiger.
? The Tiger treebank already comes with built-in head labels, but not for all categories. In this case the
score is computed only for the internal nodes that conform to the one head per node constraint.
6 Conclusions
In this paper we have described an empirical inves-
tigation into possible ways of enriching corpora
with head information, based on different linguis-
tic intuitions about the role of heads in natural lan-
guage syntax. We have described two novel algo-
rithms, based on entropy minimization and famil-
iarity maximization, and several variants of these
algorithms including POS-tag and spine reduction.
Evaluation of head assignments is difficult, as
no widely agreed upon gold standard annotations
exist. This is illustrated by the disparities between
the (widely used) Magerman-Collins scheme and
the Tiger-corpus head annotations on the one
hand, and the ?gold standard? dependencies ac-
cording to the corresponding Dependency Banks
on the other. We have therefore not only evalu-
ated our algorithms against such gold standards,
but also tested the parsing accuracies of the im-
plicit lexicalized grammars (using three different
parsers). Although the ordering of the algorithms
on performance on these various evaluations is dif-
ferent, we find that the best performing strategies
in all cases and for two different languages are
with variants of the ?familiarity? algorithm.
Interestingly, we find that the parsing results are
consistently better for the algorithms that keep the
full lexicalized elementary trees, whereas the best
matches with gold standard annotations are ob-
tained by versions that apply the POStag and spine
reductions. Given the uncertainty about the gold
standards, the possibility remains that this reflects
biases towards the most general headedness-rules
in the annotation practice rather than a linguisti-
cally real phenomenon.
Unsupervised head assignment algorithms can
be used for the many applications in NLP where
information on headedness is needed to convert
constituency trees into dependency trees, or to
extract head-lexicalised grammars from a con-
stituency treebank. Of course, it remains to be
seen which algorithm performs best in any of these
specific applications. Nevertheless, we conclude
that among currently available approaches, i.e.,
our two algorithms and the EM-based approach of
(Chiang and Bikel, 2002), ?familiarity maximiza-
tion? is the most promising approach for automatic
assignments of heads in treebanks.
From a linguistic point of view, our work can
be seen as investigating ways in which distribu-
tional information can be used to determine head-
edness in phrase-structure trees. We have shown
that lexicalized tree grammars provide a promis-
ing methodology for linking alternative head as-
signments to alternative dependency structures
(needed for deeper grammatical structure, includ-
ing e.g., argument structure), as well as to alterna-
tive derivations of the same sentences (i.e. the set
of lexicalized elementary trees need to derive the
given parse tree). In future work, we aim to extend
these results by moving to more expressive gram-
matical formalisms (e.g., tree adjoining grammar)
and by distinguishing adjuncts from arguments.
Acknowledgments We gratefully acknowledge
funding by the Netherlands Organization for Sci-
entific Research (NWO): FS is funded through
a Vici-grant ?Integrating Cognition? (277.70.006)
to Rens Bod and WZ through a Veni-grant ?Dis-
covering Grammar? (639.021.612). We thank
Rens Bod, Yoav Seginer, Reut Tsarfaty and
three anonymous reviewers for helpful comments,
Thomas By for providing us with his dependency
bank and Joakim Nivre and Dan Bikel for help in
adapting their parsers to work with our data.
708
References
S. Bangalore and A.K. Joshi. 1999. Supertagging: An
approach to almost parsing. Computational Linguis-
tics, 25(2):237?265.
D.M. Bikel. 2004. Intricacies of Collins? Parsing
Model. Computational Linguistics, 30(4):479?511.
R. Bod. 1998. Beyond Grammar: An experience-
based theory of language. CSLI, Stanford, CA.
G. Borensztajn, and W. Zuidema. 2007. Bayesian
Model Merging for Unsupervised Constituent La-
beling and Grammar Induction. Technical Report,
ILLC.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER treebank. In Proceed-
ings of the Workshop on Treebanks and Linguistic
Theories, Sozopol.
T. By. 2007. Some notes on the PARC 700 dependency
bank. Natural Language Engineering, 13(3):261?
282.
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proceedings of
the fourteenth national conference on artificial intel-
ligence, Menlo Park. AAAI Press/MIT Press.
D. Chiang and D.M. Bikel. 2002. Recovering
latent information in treebanks. Proceedings of
the 19th international conference on Computational
linguistics-Volume 1, pages 1?7.
D. Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of the 38th Annual Meeting of the ACL.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
G. Corbett, N. Fraser, and S. McGlashan, editors.
2006. Heads in Grammatical Theory. Cambridge
University Press.
M. Forst, N. Bertomeu, B. Crysmann, F. Fouvry,
S. Hansen-Schirra, and V. Kordoni. 2004. To-
wards a dependency-based gold standard for Ger-
man parsers.
J. Hockenmaier and M. Steedman. 2007. CCGbank:
A corpus of ccg derivations and dependency struc-
tures extracted from the penn treebank. Comput.
Linguist., 33(3):355?396.
A.K. Joshi and Y. Schabes. 1991. Tree-adjoining
grammars and lexicalized grammars. Technical re-
port, Department of Computer & Information Sci-
ence, University of Pennsylvania.
T. King, R. Crouch, S. Riezler, M. Dalrymple, and
R. Kaplan. 2003. The PARC 700 dependency bank.
D. Klein and C.D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proceedings of the 40th Annual Meeting
of the ACL.
D. Klein and C.D. Manning. 2004. Corpus-based
induction of syntactic structure: models of depen-
dency and constituency. In Proceedings of the 42nd
Annual Meeting of the ACL.
D.M. Magerman. 1995. Statistical decision-tree mod-
els for parsing. In Proceedings of the 33rd Annual
Meeting of the ACL.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2).
J. Nivre and J. Hall. 2005. MaltParser: A Language-
Independent System for Data-Driven Dependency
Parsing. In Proceedings of the Fourth Workshop
on Treebanks and Linguistic Theories (TLT2005),
pages 137?148.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son,S. Riedel, and D. Yuret. 2007. The conll 2007
shared task on dependency parsing. In Proc. of the
CoNLL 2007 Shared Task., June.
J. Nivre. 2007. Inductive Dependency Parsing. Com-
putational Linguistics, 33(2).
S. Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006. Learning accurate, compact, and interpretable
tree annotation. In Proceedings ACL-COLING?06,
pages 443?440.
R. Reichart and A. Rappoport. 2008. Unsupervised
Induction of Labeled Parse Trees by Clustering with
Syntactic Features. In Proceedings Coling.
H. Schu?tze. 1993. Part-of-speech induction from
scratch. In Proceedings of the 31st annual meeting
of the ACL.
Y. Seginer 2007. Learning Syntactic Structure. Ph.D.
thesis, University of Amsterdam.
H. Yamada, and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines.
In Proceedings of the Eighth International Work-
shop on Parsing Technologies. Nancy, France.
709
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 29?36, New York City, June 2006. c?2006 Association for Computational Linguistics
What are the Productive Units of Natural Language Grammar? A DOP
Approach to the Automatic Identification of Constructions.
Willem Zuidema
Institute for Logic, Language and Computation
University of Amsterdam
Plantage Muidergracht 24, 1018 TV, Amsterdam, the Netherlands.
jzuidema@science.uva.nl
Abstract
We explore a novel computational ap-
proach to identifying ?constructions? or
?multi-word expressions? (MWEs) in an
annotated corpus. In this approach,
MWEs have no special status, but emerge
in a general procedure for finding the best
statistical grammar to describe the train-
ing corpus. The statistical grammar for-
malism used is that of stochastic tree sub-
stitution grammars (STSGs), such as used
in Data-Oriented Parsing. We present an
algorithm for calculating the expected fre-
quencies of arbitrary subtrees given the
parameters of an STSG, and a method
for estimating the parameters of an STSG
given observed frequencies in a tree bank.
We report quantitative results on the ATIS
corpus of phrase-structure annotated sen-
tences, and give examples of the MWEs
extracted from this corpus.
1 Introduction
Many current theories of language use and acquisi-
tion assume that language users store and use much
larger fragments of language than the single words
and rules of combination of traditional linguistic
models. Such fragments are often called construc-
tions, and the theories that assign them a central
role ?construction grammar? (Goldberg, 1995; Kay
and Fillmore, 1999; Tomasello, 2000; Jackendoff,
2002, among others). For construction grammar-
ians, multi-word expressions (MWEs) such as id-
ioms, collocations, fixed expressions and compound
verbs and nouns, are not so much exceptions to the
rule, but rather extreme cases that reveal some fun-
damental properties of natural language.
In the construction grammar tradition, co-
occurrence statistics from corpora have often been
used as evidence for hypothesized constructions.
However, such statistics are typically gathered on
a case-by-case basis, and no reliable procedure ex-
ists to automatically identify constructions. In con-
trast, in computational linguistics, many automatic
procedures are studied for identifying MWEs (Sag
et al, 2002) ? with varying success ? but here they
are treated as exceptions: identifying multi-word ex-
pressions is a pre-processing step, where typically
adjacent words are grouped together after which the
usual procedures for syntactic or semantic analysis
can be applied. In this paper I explore an alter-
native formal and computational approach, where
multi-word constructions have no special status,
but emerge in a general procedure to find the best
statistical grammar to describe a training corpus.
Crucially, I use a formalism known as ?Stochastic
Tree Substitution Grammars? (henceforth, STSGs),
which can represent single words, contiguous and
noncontiguous MWEs, context-free rules or com-
plete parse trees in a unified representation.
My approach is closely related to work in statisti-
cal parsing known as Data-Oriented Parsing (DOP),
an empirically highly successful approach with la-
beled recall and precision scores on the Penn Tree
Bank that are among the best currently obtained
(Bod, 2003). DOP, first proposed in (Scha, 1990),
29
can be seen as an early formalization and combina-
tion of ideas from construction grammar and statis-
tical parsing. Its key innovations were (i) the pro-
posal to use fragments of trees from a tree bank as
the symbolic backbone; (ii) the proposal to allow, in
principle, trees of arbitrary size and shape as the el-
ementary units of combination; (iii) the proposal to
use the occurrence and co-occurrence frequencies as
the basis for structural disambiguation in parsing.
The model I develop in this paper is true to these
general DOP ideals, although it differs in impor-
tant respects from the many DOP implementations
that have been studied since its first inception (Bod,
1993; Goodman, 1996; Bod, 1998; Sima?an, 2002;
Collins and Duffy, 2002; Bod et al, 2003, and many
others). The crucial difference is in the estimation
procedure for choosing the weights of the STSG
based on observed frequencies in a corpus. Existing
DOP models converge to STSGs that either (i) give
all subtrees of the observed trees nonzero weights
(Bod, 1993; Bod, 2003), or (ii) give only the largest
possible fragments nonzero weights (Sima?an and
Buratto, 2003; Zollmann and Sima?an, 2005). The
model in this paper, in contrast, aims at finding the
smallest set of productive units that explain the oc-
currences and co-occurrences in a corpus. Large
subtrees only receive non-zero weights, if they occur
more frequently than can be expected on the basis of
the weights of smaller subtrees.
2 Formalism, Notation and Definitions
2.1 Stochastic Tree Substitution Grammars
STSGs are a simple generalization of Stochas-
tic Context Free Grammars (henceforth, SCFGs),
where the productive units are elementary trees of
arbitrary size instead of the rewrite rules in SCFGs
(which can be viewed as trees of depth 1). STSGs
form a restricted subclass of Stochastic Tree Adjoin-
ing Grammars (henceforth, STAGs) (Resnik, 1992;
Schabes, 1992), the difference being that STSGs
only allow for substitution and not for adjunction
(Joshi and Sarkar, 2003). This limits the genera-
tive capacity to that of context-free grammars, and
means STSGs cannot be fully lexicalized. These
limitations notwithstanding, the close relationship
with STAGs is an attractive feature with extensions
to the class of mildly context-sensitive languages
(Joshi et al, 1991) in mind. Most importantly, how-
ever, STSGs are already able to model a vast range
of statistical dependencies between words and con-
stituents, which allows them to rightly predict the
occurrences of many constructions (Bod, 1998).
For completeness, we include the usual defi-
nitions of STSGs, the substitution operation and
derivation and parse probabilities (Bod, 1998), us-
ing our own notation. An STSG is a 5-tuple
?Vn, Vt, S, T, w?, where Vn is the set of non-terminalsymbols; Vt is the set of terminal symbols; S ? Vn isthe start symbol; T is a set of elementary trees, such
that for every t ? T the unique root node r(t) ? Vn,the set of internal nodes i(t) ? Vn and the set of leafnodes l(t) ? Vn ? Vt; finally, w : T ? [0, 1] is aprobability (weight) distribution over the elementary
trees, such that for any t ? T , ?t??R(t) w(t?) = 1,where R(t) is the set of elementary trees with the
same root label as t. It will prove useful to also de-
fine the set of all possible trees ? over the defined
alphabets (with the same conditions on root, internal
and leaf nodes as for T ), and the set of all possible
complete parse trees ? (with r(t) = S and all leaf
nodes l(t) ? Vt). Obviously, T ? ? and ? ? ?.The substitution operation ? is defined if the left-
most nonterminal leaf in t1 is identical to the root of
t2. Performing substitution t1 ? t2 yields t3, if t3 isidentical to t1 with the leftmost nonterminal leaf re-placed by t2. A derivation is a sequence of elemen-tary trees, where the first tree t ? T has root-label
S and every next tree combines through substitution
with the result of the substitutions before it. The
probability of a derivation d is defined as the prod-
uct of weights of the elementary trees involved:
P (d = t1 ? . . . ? tn) =
n?
i=1
(w (ti)) . (1)
A parse tree is any tree t ? ?. Multiple derivations
can yield the same parse tree; the probability of a
parse tree p equals the sum of the probabilities of
the different derivations that yield that same tree:
P (p) =
?
d:d?=p
(P (d)) , (2)
where d? is the tree derived by derivation d.
In this paper, we are only concerned with gram-
mars that define proper probability distributions over
30
trees, such that the probability of all derivations sum
up to 1 and no probability mass gets lost in deriva-
tions that never reach a terminal yield. We require:
?
p??
P (p) =
?
d:d???
P (d) = 1. (3)
2.2 Usage Frequency and Occurrence
Frequency
In addition to these conventional definitions, we will
make use in this paper of the concepts ?usage fre-
quency? and ?occurrence frequency?. When we
consider an arbitrary subtree t, the usage frequency
u(t) describes the relative frequency with which el-
ementary tree t is involved in a set of derivations.
Given a grammar G ? STSG, the expected usage
frequency is:
u(t) =
?
d:t?d
(P (d) C (t, d)) , (4)
where C (t, d) gives the number of occurrences of
t in d. The set of derivations, and hence usage fre-
quency, is usually considered hidden information.
The occurrence frequency f(t) describes the rela-
tive frequency with which t occurs as a subtree of a
set of parse trees, which is usually assumed to be
observable information. If grammar G is used to
generate trees, it will create a tree bank where each
parse tree will occur with an expected frequency as
in equation (2). More generally, the expected oc-
currence frequency f(t) (relative to the number n of
complete trees in the tree bank) of a subtree t is:
E[f(t)] =
?
p:t?p?
(P (p)C (t, p?)) , (5)
where p? is the multiset of all subtrees of p.
Hence, w(t), u(t) and f(t) all assign values (the
latter two not necessarily between 0 and 1) to trees.
An important question is how these different val-
ues can be related. For STSGs which have only
elementary trees of depth 1, and are thus equiva-
lent to SCFGs, these relations are straightforward:
the usage frequency of an elementary tree simply
equals its expected frequency, and can be derived
from the weights by multiplying inside and out-
side probabilities (Lari and Young, 1990). Estimat-
ing the weights of an (unconstrained and untrans-
formed) SCFG from an tree bank is straightforward,
as weights, in the limit, simply equal the relative
frequency of each depth-1 subtree (relative to other
depth-1 subtrees with the same root label).
When elementary trees can be of arbitrary depth,
however, many different derivations can yield the
same tree, and a given subtree t can emerge with-
out the corresponding elementary tree ever having
been used. The expected frequencies are sums of
products, and ? if one wants to avoid exhaustively
enumerating all possible parse trees ? surprisingly
difficult to calculate, as will become clear below.
2.3 From weights to usage frequencies and
back
Relating usage frequencies to weights is relatively
simple. With a bit of algebra we can work out the
following relations:
u(t) =
?
?
?
w(t) if r(t) = S
w(t)
?
t? :r(t)?l(t?)
u(t?)Ct?t otherwise
(6)
where C t?t gives the number of occurrences of theroot label r(t) of t among the leaves of t?. The in-
verse relation is straightforward:
w(t) = u(t)?
t??R(t) u(t?)
. (7)
2.4 From usage frequency to expected
frequency
The two remaining problems ? calculating expected
frequencies from weights and estimating the weights
from observed frequencies ? are surprisingly dif-
ficult and heretofore not satisfactorily solved. In
(Zuidema, 2006) we evaluate existing estimation
methods for Data-Oriented Parsing, and show that
they are ill-suited for learning tasks such as stud-
ied in this paper. In the next section, we present a
new algorithm for estimation, which makes use of
a method for calculating expected frequencies that
we sketch in this section. This method makes use of
sub- and supertree relations that we explain first.
We define two types of subtrees of a given tree t,
which, for lack of better terminology, we will call
?twigs? and ?prunes? of t. Twigs are those subtrees
headed by any of t?s internal nodes and everything
31
below. Prunes are those subtrees headed by t?s root-
node, pruned at any number (? 0) of internal nodes.
Using ? to indicate left-most substitution, we write:
? t1 is a twig of t2, if either t1 = t2 or ?t3, suchthat t3 ? t1 = t2;
? t1 is a prune of t2, if either t1 = t2 or ?t3 . . . tn,such that t1 ? t3 . . . ? tn = t2;
? t? = prx(t), if x is a set of nodes in t, such thatif t is pruned at each i ? x it equals t?.
Thus defined, the set of all subtrees st(t) of t cor-
responds to the set of all prunes of all twigs of t:
st(t) = {t??|?t?(t? ? tw(t) ? t?? ? pr(t?)).
We further define the sets of supertwigs, super-
prunes and supertrees as follows:
? t?w(t) = {t?|t ? tw(t?)}
? p?rx(t) = {t?|t = prx(t?)}
? s?t(t) = {t?|t ? st(t?)}.
Using these sets, and the set of derivations D(t) of
the fragment t, a general expression for the expected
frequency of t is:
E[f(t)] =
?
d?D(t)
??
? =
?
??ctw(d1)
?
? ?? dprx(t)(?)
u(? ?)
? =
?
t??
?d2,...,dn?
?
? ?? dprx(t)(t?)
w
(
? ?
) (8)
where ?d1, . . . , dn? is the sequence of elementarytrees in derivation d. A derivation of this equation
is provided on the author?s website1. Note that it
1http://staff.science.uva.nl/?jzuidema. The intuition behind
it is as follows. Observe first that there are many ways in which
an arbitrary fragment t can emerge, many of which do not in-
volve the usage of the elementary tree t. It is useful to partition
the set of all derivations of complete parse trees according to the
substitution sites inside t that they involve, and hence according
to the corresponding derivations of t. The first summation in (8)
simply sums over all these cases.
Each derivation of t involves a first elementary tree d1, andpossibly a sequence of further elementary trees ?d2, . . . , dn?.Roughly speaking, the ?-term in equation (8) describes the fre-
quency with which a d1 will be generated. The ?-term thendescribes the probability that d1 will be expanded as t. Theequation simplifies considerably for those fragments that have
no nonterminal leaves: the set dprx(t) then only contains t, andthe two summations over this set disappear. The equation fur-
ther simplifies if only depth-1 elementary trees have nonzero
weights (i.e. for SCFGs): ? and ? then essentially give outside
and inside probabilities (Lari and Young, 1990). However, for
unconstrained STSGs we need all sums and products in (8).
will, in general, be computationally extremely ex-
pensive to calculate E[f(t)] . We will come back to
computational efficiency issues in the discussion.
3 Estimation: push-n-pull
The goal of this paper is an automatic discovery
procedure for finding ?constructions? based on oc-
currence and co-occurrence frequencies in a corpus.
Now that we have introduced the necessary termi-
nology, we can reformulate this goal as follows:
What are the elementary trees with multiple words
with the highest usage frequency in the STSG esti-
mated from an annotated corpus? Thus phrased, the
crucial next step is to decide on an estimation proce-
dure for learning an STSG from a corpus.
Here we develop an estimation procedure we call
?push-n-pull?. The basic idea is as follows. Given
an initial setting of the parameters, the method cal-
culates the expected frequency of all complete and
incomplete trees. If a tree?s expected frequency is
higher than its observed frequency, the method sub-
tracts the difference from the tree?s score, and dis-
tributes (?pushes?) it over the trees involved in its
derivations. If it is lower, it ?pulls? the difference
from these same derivations. The method includes a
bias for moving probability mass to smaller elemen-
tary trees, to avoid overfitting; its effects become
smaller as more data gets observed.
Because the method for calculating estimated fre-
quency works with usage-frequencies, the push-n-
pull algorithm also uses these as parameters. More
precisely, it manipulates a ?score?, which is the
product of usage frequency and the total number of
parse trees observed. Implicit here is the assumption
that by shifting usage frequencies between different
derivations, the relation with weights remains as in
equation (6). Simulations suggest this is reasonable.
In the current implementation, the method starts
with all frequency mass in the longest derivations,
i.e. in the depth-1 elementary trees. Finally, the cur-
rent implementation is incremental. It keeps track of
the frequencies with which it observes subtrees in a
corpus. For each tree received, it finds all derivations
and all probabilities, updates frequencies and scores
according to the rules sketched above. In pseudo-
code, the push-n-pull algorithm is as follows:
for each observed parse tree p
32
for each depth-1 subtree t in p
update-score(t, 1.0)
for each subtree t of p
? =min(sc(t), B + ?(E[f(t)] ? f(t)))
?? = 0
for each of n derivations d of t
let t? . . . t?? be all elementary trees in d
? =min(sc(t?), . . . , sc(t??),??/n)
??? = ?
for each elementary tree t? in d
update-score(t? , ?)
update-score (t,??)
where sc(t) is the score of t, B is the bias to-
wards smaller subtrees, ? is the learning rate param-
eter and f(t) is the observed frequency of t. ?? thus
gives the actual change in the score of t, based on
the difference between expected and observed fre-
quency, bias, learning rate and how much scores can
be pushed or pulled2. For computational efficiency,
only subtrees with a depth no larger than d = 3 or
d = 4 and only derivations involving 2 elementary
trees are considered.
4 Results
We have implemented the algorithms for calculat-
ing the expected frequency, and the push-n-pull al-
gorithm for estimation. We have evaluated the algo-
rithms on a number of simple example STSGs and
found that the expected frequency algorithm cor-
rectly predicts observed frequencies. We have fur-
ther found that ? unlike existing estimation meth-
ods ? the push-n-pull algorithm converges to STSGs
that closely model the observed frequencies (i.e. that
maximize the likelihood of the data) without putting
all probability mass in the largest elementary trees
(i.e. whilst retaining generalizations about the data).
Here we report first quantitative results on the
ATIS3 corpus (Hemphill et al, 1990). Before pro-
cessing, all trees (train and test set) were converted
to a format that our current implementation requires
(all non-terminal labels are unique, all internal nodes
have two daughters, all preterminal nodes have a
single lexical daughter; all unary productions and
all traces were removed). The set of trees was ran-
domly split in a train set of 462 trees, and a test set
2An important topic for future research is to clarify the rela-
tion between push-n-pull and Expectation Maximization.
of 116 trees. The push-n-pull algorithm was then
run in 10 passes over the train set, with d = 3,
B = 0 and ? = 0.1. By calculating the most proba-
ble parse3 for each yield of the trees in test set, and
running ?evalb? we arrive at the following quantita-
tive results: a string set coverage of 84% (19 failed
parses), labeled recall of 95.07, and labeled preci-
sion of 95.07. We obtained almost identical num-
bers on the same data with a reimplementation of
the DOP1 algorithm (Bod, 1998).
method # rules Cov. LR LP EM
DOP1 77852 84% 95.07 95.07 83.5
p-n-p 58799 84% 95.07 95.07 83.5
Table 1: Parseval scores of DOP1 and push-n-pull
on the same 462-116 random train-testset split of a
treebank derived from the ATIS3 corpus (we empha-
size that all trees, also those of the test-set, were con-
verted to Chomsky Normal Form, whereby unary
production and traces were removed and top-nodes
relabeled ?TOP?. These results are thus not compa-
rable to previous methods evaluated on the ATIS3
corpus.) EM is ?exact match?.
method # rules Cov. LR LP EM
sc > 0.3 8593 77% 80.8 80.8 46.3
sc > 0.1 98443 77% 81.9 81.9 48.8
Table 2: Parseval scores using a p-n-p induced
STSG on the same treebank as in table 1, using a
different random 525-53 train-testset split. Shown
are results were only elementary trees with scores
higher than 0.3 and 0.1 respectively are used.
However, more interesting is a qualitative anal-
ysis of the STSG induced, which shows that, un-
like DOP1, push-n-pull arrives at a grammar that
gives high weights (and scores) to those elementary
3We approximated the most probable parse as follows (fol-
lowing (Bod, 2003)). We first converted the induced STSG to
an isomorph SCFG, by giving the internal nodes of every ele-
mentary tree t unique address-labels, and reading off all CFG
productions (all with weight 1.0, except for the top-production,
which receives the weight of t). An existing SCFG parser
(Schmid, 2004) was then used, with a simple unknown word
heuristic, to generate the Viterbi n-best parses with n = 100,
and, after removing the address labels, all equal parses and their
probabilities were summed, and the one with highest probabil-
ity chosen.
33
trees that best explain the overrepresentation of cer-
tain constructions in the data. For instance, in a run
with d = 4, ? = 1.0, B = 1.0, the 50 elemen-
tary trees with the highest scores, as shown in fig-
ure 1, are all exemplary of frequent formulas in the
ATIS corpus such as ?show me X?, ?I?d like to X?,
?which of these?, ?what is the X?, ?cheapest fare?
and ?flights from X to Y?. In short, the push-n-pull
algorithm ? while starting out considering all possi-
ble subtrees ? converges to a grammar which makes
linguistically relevant generalizations. This allows
for a more compact grammar (58799 rules in the
SCFG reduction, vs. 77852 for DOP1), whilst re-
taining DOP?s excellent empirical performance.
5 Discussion
Calculating E[f(t)] using equation (8) can be ex-
tremely expensive in computational terms. One will
typically want to calculate this value for all subtrees,
the number of which is exponential in the size of the
trees in the training data. For each subtree t, we will
need to consider the set of all its derivations (expo-
nential in the size of t), and for each derivation the
set of supertwigs of the first elementary trees and,
for incompletely lexicalized subtrees, the set of su-
perprunes of all elementary trees in their derivations.
The latter two sets, however, need not be constructed
for every time the expected frequency E[f(t)] is cal-
culated. Instead, we can, as we do in the current im-
plementation, keep track of the two sums for every
change of the weights.
However, there are many further possibilities for
improving the efficiency of the algorithm that are
currently not implemented. Equation (8) remains
valid under various restrictions on the elementary
trees that we are willing to consider as productive
units. Some of these will remove the exponential de-
pendence on the size of the trees in the training data.
For instance, in the case where we restrict the pro-
ductive units (with nonzero weights) to depth-1 trees
(i.e. CFG rules), equation (8) collapses to the prod-
uct of inside and outside probabilities, which can be
calculated using dynamical programming in polyno-
mial time (Lari and Young, 1990). A major topic for
future research is to define linguistically motivated
restrictions that allow for efficient computation.
Another concern is the size of the grammar the
estimation procedure produces, and hence the time
and space efficiency of the resulting parser. Ta-
ble 1 already showed that push-n-pull leads to a
more concise grammar. The reason is that many po-
tential elementary trees receive a score (and weight)
0. More generally, push-n-pull generates extremely
tilted score distributions, which allows for even
more compact but highly accurate approximations.
In table 2 we show, for the d = 4 grammar of fig-
ure 1, that a 10-fold reduction of the grammar size
by pruning elementary trees with low scores, leads
only to a small decrease in the LP and LR measures.
Another interesting question is if and how the
current algorithm can be extended to the full class
of Stochastic Tree-Adjoining Grammars (Schabes,
1992; Resnik, 1992). With the added operation of
adjunction, equation (8) is not valid anymore. Given
the computational complexities that it already gives
rise to, however, it seems that issue of linguisti-
cally motivated restrictions (other than lexicaliza-
tion) should be considered first. Finally, given that
the current approach is dependent on the availability
of a large annotated corpus, an important question
is if and how it can be extended to work with un-
labeled data. That is, can we transform the push-n-
pull algorithm to perform the unsupervised learning
of STSGs? Although most work on unsupervised
grammar learning concerns SCFGs (including some
of our own (Zuidema, 2003)) it is interesting to note
that much of the evidence for construction grammar
in fact comes from the language acquisition litera-
ture (Tomasello, 2000).
6 Conclusions
Theoretical linguistics has long strived to account
for the unbounded productivity of natural language
syntax with as few units and rules of combination
as possible. In contrast, construction grammar and
related theories of grammar postulate a heteroge-
neous and redundant storage of ?constructions?. If
this view is correct, we expect to see statistical sig-
natures of these constructions in the distributional
information that can be derived from corpora of nat-
ural language utterances. How can we recover those
signatures? In this paper we have presented an ap-
proach to identifying the relevant statistical correla-
tions in a corpus based on the assumption that the
34
TOP
VB
?SHOW?
VP*
PRP
?ME?
NP
NP*
DT NNS
NP**
PP-DIR PP-DIR*
(a) The ?show me NP PP? frame,
which occurs very frequently in
the training data and is repre-
sented in several elementary trees
with high weight.
WHNP-1
WDT
?WHICH?
PP
IN
?OF?
NP
DT
?THESE?
NNS
?FLIGHTS?
(b) The complete parse tree
for the sentence ?Which of
these flights?, which occurs
16 times in training data.
TOP
NNS
?FLIGHTS?
NP*
PP-DIR
IN
?FROM?
NP**
NNP NNP*
PP-DIR*
TO
?TO?
NNP**
(c) The frame for ?flights from NP to
NP?
1. ((TOP (VB ?SHOW?) (VP* (PRP ?ME?) (NP (NP* DT NNS) (NP** PP-DIR PP-DIR*)))) 17.79 0.008 30)
2. ((TOP (VB ?SHOW?) (VP* (PRP ?ME?) (NP (NP* DT NNS) NP**))) 10.34 0.004 46
3. (TOP (PRP ?I?) (VP (MD ?WOULD?) (VP* (VB ?LIKE?) (VP** TO VP***)))) 10.02 0.009 20
4. (WHNP-1 (WDT ?WHICH?) (PP (IN ?OF?) (NP (DT ?THESE?) (NNS ?FLIGHTS?)))) 8.80 0.078 16
5. (TOP (WP ?WHAT?) (SQ (VBZ ?IS?) (NP-SBJ (DT ?THE?) (NN ?PRICE?)))) 8.76 0.005 20
6. (TOP (WHNP (WDT ?WHAT?) (NNS ?FLIGHTS?)) (SQ (VBP ?ARE?) (SQ* (EX ?THERE?) SQ**))) 8.25 0.006 36
7. (VP* (PRP ?ME?) (NP (NP* (DT ?THE?) (NNS ?FLIGHTS?)) (NP** (PP-DIR IN NNP) (PP-DIR* TO NNP*)))) 7.90 0.023 18
8. (TOP (WHNP (WDT ?WHAT?) (NNS ?FLIGHTS?)) (SQ (VBP ?ARE?) (SQ* (EX ?THERE?) (SQ** PP-DIR-3 PP-DIR-4)))) 6.64 0.005 26
9. (TOP (PRP ?I?) (VP MD (VP* (VB ?LIKE?) (VP** TO VP***)))) 6.48 0.006 20
10. (TOP (PRP ?I?) (VP (VBP ?NEED?) (NP (NP* DT NN) (NP** PP-DIR NP***)))) 5.01 0.004 10
11. (TOP (VB ?SHOW?) (VP* (PRP ?ME?) (NP (DT ?THE?) NNS))) 4.94 0.002 16
12. (TOP WP (SQ (VBZ ?IS?) (NP-SBJ (DT ?THE?) (NN ?PRICE?)))) 4.91 0.0028 20
13. (TOP (WHNP (WDT ?WHAT?) (NNS ?FLIGHTS?)) (SQ (VBP ?ARE?) (SQ* EX (SQ** PP-DIR-3 PP-DIR-4)))) 4.16 0.003 26
14. (TOP (VB ?SHOW?) (VP* (PRP ?ME?) (NP (NNS ?FLIGHTS?) NP*))) 4.01 0.001 16
15. (TOP (VB ?SHOW?) (VP* (PRP ?ME?) (NP (DT ?THE?) NP*))) 3.94 0.002 12
16. (TOP (WHNP (WDT ?WHAT?) (NNS ?FLIGHTS?)) (SQ (VBP ?ARE?) (SQ* EX SQ**))) 3.92 0.003 36
17. (TOP (PRP ?I?) (VP (VBP ?NEED?) (NP (NP* DT NN) NP**))) 3.85 0.003 14
18. (TOP (WP ?WHAT?) (SQ VBZ (NP-SBJ (DT ?THE?) (NN ?PRICE?)))) 3.79 0.002 20
19. (WHNP-1 (WDT ?WHICH?) (PP (IN ?OF?) (NP (DT ?THESE?) NNS))) 3.65 0.032 16
20. (TOP (VB ?SHOW?) (VP* (PRP ?ME?) (NP NP* (SBAR WDT VP**)))) 3.64 0.002 14
21. (TOP (VB ?SHOW?) (VP* PRP (NP (NP* DT NNS) (NP** PP-DIR PP-DIR*)))) 3.61 0.002 30
22. (TOP (WHNP (WDT ?WHAT?) NNS) (SQ (VBP ?ARE?) (SQ* (EX ?THERE?) (SQ** PP-DIR-3 PP-DIR-4)))) 3.30 0.002 26
23. (VP (MD ?WOULD?) (VP* (VB ?LIKE?) (VP** (TO ?TO?) (VP*** VB* VP****)))) 3.25 0.012 16
24. (TOP (WDT ?WHICH?) VP) 3.1460636 0.001646589 12
25. (TOP (VB ?SHOW?) (VP* (PRP ?ME?) (NP (NP* DT NP**) NP***))) 3.03 0.001 12
26. (TOP (VB ?SHOW?) (VP* (PRP ?ME?) (NP NP* (NP*** PP-DIR PP-DIR*)))) 2.97 0.001 12
27. (PP (IN ?OF?) (NP* (NN* ?FLIGHT?) (NP** NNP (NP*** NNP* NP****)))) 2.95 0.015 8
28. (TOP (VB ?SHOW?) (VP* (PRP ?ME?) (NP (DT ?THE?) (NNS ?FARES?)))) 2.85 0.001 8
29. (VP (VBP ?NEED?) (NP (NP* (DT ?A?) (NN ?FLIGHT?)) (NP** PP-DIR NP***))) 2.77 0.009 12
30. (TOP (VB ?SHOW?) (VP* (PRP ?ME?) (NP NP* (NP** PP-DIR PP-DIR*)))) 2.77 0.001 34
31. (TOP (JJS ?CHEAPEST?) (NN ?FARE?)) 2.74 0.001 6
32. (TOP (VB ?SHOW?) (VP* (PRP ?ME?) (NP (NP* DT NP**) (NP*** PP-DIR PP-DIR*)))) 2.71 0.001 8
33. (TOP (NN ?PRICE?) (PP (IN ?OF?) (NP* (NN* ?FLIGHT?) (NP** NNP NP***)))) 2.69 0.001 6
34. (TOP (NN ?PRICE?) (PP (IN ?OF?) (NP* (NN* ?FLIGHT?) NP**))) 2.68 0.001 8
35. (PP-DIR (IN ?FROM?) (NP (NNP ?WASHINGTON?) (NP* (NNP* ?D?) (NNP** ?C?)))) 2.67 0.006 6
36. (PP-DIR (IN ?FROM?) (NP** (NNP ?NEWARK?) (NP*** (NNP* ?NEW?) (NNP** ?JERSEY?)))) 2.60 0.005 6
37. (S* (PRP ?I?) (VP (MD ?WOULD?) (VP* (VB ?LIKE?) (VP** TO VP***)))) 2.59 0.11 8
38. (TOP (VBZ ?DOES?) (SQ* (NP-SBJ DT (NN ?FLIGHT?)) (VP (VB ?SERVE?) (NN* ?DINNER?)))) 2.48 0.002 8
39. (TOP (PRP ?I?) (VP (MD ?WOULD?) (VP* (VB ?LIKE?) VP**))) 2.37 0.002 20
40. (TOP (WP ?WHAT?) (SQ (VBZ ?IS?) (NP-SBJ DT (NN ?PRICE?)))) 2.33 0.001 20
41. (S* (PRP ?I?) (VP MD (VP* (VB ?LIKE?) (VP** TO VP***)))) 2.33 0.100 8
42. (WHNP**** (PP-TMP (IN* ?ON?) (NNP** ?FRIDAY?)) (PP-LOC (IN** ?ON?) (NP (NNP*** ?AMERICAN?) (NNP**** ?AIRLINES?)))) 2.30 0.086 6
43. (VP* (PRP ?ME?) (NP (NP* (DT ?THE?) NNS) (NP** (PP-DIR IN NNP) (PP-DIR* TO NNP*)))) 2.29 0.007 18
44. (TOP (WHNP* (WDT ?WHAT?) (NNS ?FLIGHTS?)) (WHNP** (PP-DIR (IN ?FROM?) NNP) (WHNP*** (PP-DIR* TO NNP*) (PP-TMP IN* NNP**)))) 2.28 0.001 12
45. (SQ (VBP ?ARE?) (SQ* EX (SQ** (PP-DIR-3 IN NNP) (PP-DIR-4 TO NNP*)))) 2.26 0.015 14
46. (TOP (VB ?SHOW?) (VP* (PRP ?ME?) (NP (NP* DT NNS) (SBAR WDT VP**)))) 2.22 0.001 8
47. (TOP (NNS ?FLIGHTS?) (NP* (PP-DIR (IN ?FROM?) (NP** NNP NNP*)) (PP-DIR* (TO ?TO?) NNP**))) 2.20 0.001 10)
48. ((VP (VBP ?NEED?) (NP (NP* (DT ?A?) (NN ?FLIGHT?)) (NP** (PP-DIR IN NNP) NP***))) 2.1346128 0.007185978 10)
49. ((NP (NP* (DT ?THE?) (NNS ?FLIGHTS?)) (NP** (PP-DIR (IN ?FROM?) (NNP ?BALTIMORE?)) (PP-DIR* (TO ?TO?) (NNP* ?OAKLAND?)))) 2.1335514 0.00381956 10)
50. ((TOP (VB ?SHOW?) (VP* (PRP ?ME?) (NP (NP* DT NNS) (NP** PP-DIR NP***)))) 2.09 0.001 8)
Figure 1: Three examples and a list of the first 50 elementary trees with multiple words of an STSG induced
using the push-n-pull algorithm on the ATIS3 corpus. For use in the current implementation, the parse
trees have been converted to Chomsky Normal Form (all occurrences of A ? B, B ? ? are replaced by
A ? ?; all occurrences of A ? BC? are replaced by A ? BA?, A? ? C?), all non-terminal labels are
made unique for a particular parse tree (address labeling not shown) and all top nodes are replaced by the
non-terminal ?TOP?. Listed are the elementary trees of the induced STSG with for each tree the score, the
weight and the frequency with which it occurs in the training set.
35
corpus is generated by an STSG, and by inferring
the properties of that underlying STSG. Given our
best guess of the STSG that generated the data, we
can start to ask questions like: which subtrees are
overrepresented in the corpus? Which correlations
are so strong that it is reasonable to think of the cor-
related phrases as a single unit? We presented a new
algorithm for estimating weights of an STSG from a
corpus, and reported promising empirical results on
a small corpus.
Acknowledgments
The author is funded by the Netherlands Organi-
sation for Scientific Research (Exacte Wetenschap-
pen), project number 612.066.405. Many thanks to
Yoav Seginer, Rens Bod and Remko Scha and the
anonymous reviewers for very useful comments.
References
Rens Bod, Remko Scha, and Khalil Sima?an, editors.2003. Data-Oriented Parsing. CSLI Publications,University of Chicago Press, Chicago, IL.
Rens Bod. 1993. Using an annotated corpus as a stochas-tic grammar. In Proceedings EACL?93, pages 37?44.
Rens Bod. 1998. Beyond Grammar: An experience-
based theory of language. CSLI, Stanford, CA.
Rens Bod. 2003. An efficient implementation of a newDOP model. In Proceedings EACL?03.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-crete structures, and the voted perceptron. ACL?02.
Adele E. Goldberg. 1995. Constructions: A Construc-
tion Grammar Approach to Argument Structure. TheUniversity of Chicago Press, Chicago, IL.
Joshua Goodman. 1996. Efficient algorithms for parsing
the DOP model. In Proceedings EMNLP?96, p. 143?152.
C.T. Hemphill, J.J. Godfrey, and G.R. Doddington. 1990.
The ATIS spoken language systems pilot corpus. In
Proceedings of the DARPA Speech and Natural Lan-
guage Workshop. Morgan Kaufman, Hidden Valley.
Ray Jackendoff. 2002. Foundations of Language. Ox-ford University Press, Oxford, UK.
Aravind Joshi and Anoop Sarkar. 2003. Tree adjoining
grammars and their application to statistical parsing.In Bod et al (Bod et al, 2003), pages 253?282.
A. Joshi, K. Vijay-Shanker, and D. Weir. 1991. The
convergence of mildly context-sensitive grammar for-malisms. In Peter Sells, Stuart Shieber, and Tom Wa-sow, editors, Foundational issues in natural language
processing, pages 21?82. MIT Press, Cambridge MA.
P. Kay and C. Fillmore. 1999. Grammatical construc-tions and linguistic generalizations. Language, 75:1?33.
K. Lari and S.J. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-gorithm. Computer Speech and Language, 4:35?56.
Philip Resnik. 1992. Probabilistic tree-adjoining gram-mar as a framework for statistical natural languageprocessing. In Proceedings COLING?92, p. 418?424.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword ex-pressions: A pain in the neck for NLP. In Proceedings
CICLing, pages 1?15.
Remko Scha. 1990. Taaltheorie en taaltechnolo-
gie; competence en performance. In R. de Kortand G.L.J. Leerdam, editors, Computertoepassingen
in de Neerlandistiek, pages 7?22. LVVN, Almere.
http://iaaa.nl/rs/LeerdamE.html.
Yves Schabes. 1992. Stochastic lexicalized tree-adjoining grammars. In Proceedings COLING?92,pages 425?432.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
Proceedings COLING?04.
Khalil Sima?an and Luciano Buratto. 2003. Backoff pa-rameter estimation for the DOP model. In Proceedings
ECML?03, pages 373?384.
Khalil Sima?an. 2002. Computational complexity of
probabilistic disambiguation. Grammars, 5(2):125?151.
Michael Tomasello. 2000. The item-based nature of chil-dren?s early syntactic development. Trends in Cogni-
tive Science, 4(4):156?163.
Andreas Zollmann and Khalil Sima?an. 2005. A consis-tent and efficient estimator for data-oriented parsing.
Journal of Automata, Languages and Combinatorics.
Willem Zuidema. 2003. How the poverty of the stimulus
solves the poverty of the stimulus. In Suzanna Becker,Sebastian Thrun, and Klaus Obermayer, editors, Ad-
vances in Neural Information Processing Systems 15,
pages 51?58. MIT Press, Cambridge, MA.
Willem Zuidema. 2006. Theoretical evaluation of esti-mation methods for Data-Oriented Parsing. In Pro-
ceedings EACL?06 (Conference Companion), pages
183?186.
36
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 238?241,
Paris, October 2009. c?2009 Association for Computational Linguistics
A generative re-ranking model for dependency parsing
Federico Sangati, Willem Zuidema and Rens Bod
Institute for Logic, Language and Computation
University of Amsterdam
Science Park 904, 1098 XH Amsterdam, The Netherlands
{f.sangati,zuidema,rens.bod}@uva.nl
Abstract
We propose a framework for dependency
parsing based on a combination of dis-
criminative and generative models. We
use a discriminative model to obtain a k-
best list of candidate parses, and subse-
quently rerank those candidates using a
generative model. We show how this ap-
proach allows us to evaluate a variety of
generative models, without needing differ-
ent parser implementations. Moreover, we
present empirical results that show a small
improvement over state-of-the-art depen-
dency parsing of English sentences.
1 Introduction
Probabilistic generative dependency models de-
fine probability distributions over all valid depen-
dency structures, and thus provide a useful inter-
mediate representation that can be used for many
NLP tasks including parsing and language mod-
eling. In recent evaluations of supervised de-
pendency parsing, however, generative approaches
are consistently outperformed by discriminative
models (Buchholz et al, 2006; Nivre et al,
2007), which treat the task of assigning the cor-
rect structure to a given sentence as a classifica-
tion task. In this category we include both transi-
tion based (Nivre and Hall , 2005) and graph based
parsers (McDonald, 2006).
In this paper, we explore a reranking approach
that combines a generative and a discrimative
model and tries to retain the strengths of both.
The idea of combining these two types of models
through re-ranking is not new, although it has been
mostly explored in constituency parsing (Collins
et al, 2002). This earlier work, however, used the
generative model in the first step, and trained the
discriminative model over its k-best candidates. In
this paper we reverse the usual order of the two
models, by employing a generative model to re-
score the k-best candidates provided by a discrim-
inative model. Moreover, the generative model of
the second phase uses frequency counts from the
training set but is not trained on the k-best parses
of the discriminative model.
The main motivation for our approach is that
it allows for efficiently evaluating many gener-
ative models, differing from one another on (i)
the choice of the linguistic units that are gener-
ated (words, pairs of words, word graphs), (ii) the
generation process (Markov process, top-down,
bottom-up), and (iii) the features that are consid-
ered to build the event space (postags/words, dis-
tance). Although efficient algorithms exist to cal-
culate parse forests (Eisner, 1996a), each choice
gives rise to different parser instantiations.
1.1 A generative model for re-ranking
In our re-ranking perspective, all the generative
model has to do is to compute the probability of
k pre-generated structures, and select the one with
maximum probability. In a generative model, ev-
ery structure can be decomposed into a series of
independent events, each mapped to a correspond-
ing conditioning event. As an example, if a gener-
ative model chooses D as the right dependent of a
certain word H , conditioned uniquely on their rel-
ative position, we can define the event as D is the
right dependent of H , and the conditioning event
as H has a right dependent.
As a preprocessing step, every sentence struc-
ture in the training corpus is decomposed into a se-
ries of independent events, with their correspond-
ing conditioning events. During this process, our
model updates two tables containing the frequency
of events and their conditioning counterparts.
In the re-ranking phase, a given candidate struc-
ture can be decomposed into independent events
(e1, e2, . . . , en) and corresponding conditioning
events (c1, c2, . . . , cn) as in the training phase.
238
The probability of the structure can then be cal-
culated as
n?
i=1
f(ei)
f(ci) (1)
where f(x) returns the frequency of x previously
stored in the tables.
It is important to stress the point that the only
specificity each generative model introduces is in
the way sentence structures are decomposed into
events; provided a generic representation for the
(conditioning) event space, both training phase
and probability calculation of candidate structures
can be implemented independently from the spe-
cific generative model, through the implementa-
tion of generic tables of (conditioning) events.
In this way the probabilities of candidate struc-
tures are exact probabilities, and do not suf-
fer from possible approximation techniques that
parsers often utilize (i.e., pruning). On the other
hand the most probable parse is selected from the
set of the k candidates generated by the discrimi-
native model, and it will equal with the most prob-
able parse among all possible structures, only for
sufficiently high k.
2 MST discriminative model
In order to generate a set of k-candidate struc-
tures for every test sentence, we use a state-of-
the-art discriminative model (McDonald, 2006).
This model treats every dependency structure as
a set of word-dependent relations, each described
by a high dimensional feature representation. For
instance, if in a certain sentence word i is the
head of word j, v(i, j) is the vector describing
all the features of such relation (i.e., labels of the
two words, their postag, and other information
including words in between them, and ancestral
nodes). During the training phase the model learns
a weight vector w which is then used to find the
best dependency structure y for a given test sen-
tence x. The score that needs to be maximized is
defined as?(i,j)?y w ?v(i, j), and the best candi-
date is called the maximum spanning tree (MST).
Assuming we have the weight vector and we
only consider projective dependency structures,
the search space can be efficiently computed by
using a dynamic algorithm on a compact repre-
sentation of the parse forest (Eisner, 1996a). The
training phase is more complex; for details we re-
fer to (McDonald, 2006). Roughly, the model em-
ploys a large-margin classifier which iterates over
the structures of the training corpus, and updates
the weight vector w trying to keep the score of the
correct structure above the scores of the incorrect
ones by an amount which is proportional to how
much they differ in accuracy.
3 Generative model
3.1 Eisner model
As a generative framework we have chosen to use
a variation of model C in (Eisner, 1996a). In
this approach nodes are generated recursively in
a top-down manner starting from the special sym-
bol EOS (end of sentence). At any given node, left
and right children are generated as two separate
Markov sequences of nodes1, each conditioned on
ancestral and sibling information (which, for now,
we will simply refer to as context).
One of the relevant variations with respect to
the original model is that in our version the direc-
tion of the Markov chain sequence is strictly left
to right, instead of the usual inside outwards.
More formally, given a dependency structure T ,
and any of its node N , the probability of generat-
ing the fragment T (N) of the dependency struc-
ture rooted in N is defined as:
P (T (N)) =
L?
l=1
P (N2l)|context) ? P (T (N2l))
?
R?
r=1
P (N3r)|context) ? P (T (N3r)) (2)
where L and R are the number of left and right
children of N in T (L,R > 0), N2l is the left
daughter of N at position l in T (analogously for
right daughters). The probability of the entire de-
pendency structure T is computed as P (T (EOS)).
In order to illustrate how a dependency struc-
ture can be decomposed into events, we present
in table 1 the list of events and the correspond-
ing conditioning events extracted from the depen-
dency structure illustrated in figure 1. In this sim-
ple example, each node is identified with its word,
and the context is composed of the direction with
respect to the head node, the head node, and the
previously chosen daughter (or NONE if it is the
first). While during the training phase the event
tables are updated with these events, in the test
phase they are looked-up to compute the structure
probability, as in equation 1.
1Every sequence ends with the special symbol EOC.
239
NObama
V
won
Dthe Jpresidential
N
election
EOS
Figure 1: Dependency tree of the sentence
?Obama won the presidential election?.
3.2 Model extension
In equation 2 we have generically defined the
probability of choosing a daughter D based on
specific features associated with D and the con-
text in which it occurs. In our implementation,
this probability is instantiated as in equation 3.
The specific features associated with D are: the
distance2 dist(H,D) between D and its head H ,
the flag term(D) which specifies whether D has
more dependents, and the lexical and postag repre-
sentation of D. The context in which D occurs is
defined by features of the head node H , the previ-
ously chosen sister S, the grandparent G, and the
direction dir (left or right).
Equation 3 is factorized in four terms, each em-
ploying an appropriate backoff reduction list re-
ported in descending priority3.
P (D|context) = (3)
P (dist(H,D), term(D), word(D), tag(D)|H,S,G, dir) =
P (tag(D)|H,S,G, dir)
reduction list:
wt(H), wt(S), wt(G), dir
wt(H), wt(S), t(G), dir{ wt(H), t(S), t(G), dir
t(H), wt(S), t(G), dir
t(H), t(S), t(G), dir
? P (word(D)|tag(D), H, S,G, dir)
reduction list: wt(H), t(S), dirt(H), t(S), dir
? P (term(D)|word(D), tag(D), H, S,G, dir)
reduction list: tag(D), wt(H), t(S), dirtag(D), t(H), t(S), dir
? P (dist(P,D)|term(D), word(D), tag(D), H, S,G, dir)
reduction list: word(D), tag(D), t(H), t(S), dirtag(D), t(H), t(S), dir
2In our implementation distance values are grouped in 4
categories: 1, 2, 3? 6, 7??.
3In the reduction lists, wt(N) stands for the string in-
corporating both the postag and the word of N , and t(N)
stands for its postag. This second reduction is never applied
to closed class words. All the notation and backoff parame-
ters are identical to (Eisner, 1996b), and are not reported here
for reasons of space.
4The counts are extracted from a two-sentence corpus
which also includes ?Obama lost the election.?
Events Freq. Conditioning Events Freq.won L EOS NONE 1 L EOS NONE 2EOC L EOS won 1 L EOS won 1EOC R EOS NONE 2 R EOS NONE 2Obama L won NONE 1 L won NONE 1EOC L won Obama 1 L won Obama 1election R won NONE 1 R won NONE 1EOC R won election 1 R won election 1EOC L Obama NONE 2 L Obama NONE 2EOC R Obama NONE 2 R Obama NONE 2the L election NONE 2 L election NONE 2presidential L election the 1 L election the 2EOC L election presidential 1 L election presidential 1EOC R election NONE 2 R election NONE 2EOC L the NONE 2 L the NONE 2EOC R the NONE 2 R the NONE 2EOC L presidential NONE 1 L presidential NONE 1EOC R presidential NONE 1 R presidential NONE 1
Table 1: Events occurring when generating the de-
pendency structure in figure 1, for the event space
(dependent | direction, head, sister). According to
the reported frequency counts4, the structure has a
associated probability of 1/4.
4 Results
In our investigation, we have tested our model
on the Wall Street Journal corpus (Marcus et al,
1993) with sentences up to 40 words in length,
converted to dependency structures. Although
several algorithms exist to perform such a conver-
sion (Sangati and Zuidema, 2008), we have fol-
lowed the scheme in (Collins, 1999). Section 2-21
was used as training, and section 22 as test set.
The MST discriminative parser was provided with
the correct postags of the words in the test set, and
it was run in second-order5 and projective mode.
Results are reported in table 2, as unlabeled attach-
ment score (UAS). The MST dependency parser
obtains very high results when employed alone
(92.58%), and generates a list of k-best-candidates
which can potentially achieve much better results
(an oracle would score above 95% when selecting
from the first 5-best, and above 99% from the first
1000-best). The decrease in performance of the
generative model, as the number of the candidate
increases, suggests that its performance would be
lower than a discriminative model if used alone.
On the other hand, our generative model is able to
select better candidates than the MST parser, when
their number is limited to a few dozens, yielding a
maximum accuracy for k = 7 where it improves
accuracy on the discriminative model by a 0.51%
(around 7% error reduction).
5The features of every dependency relation include infor-
mation about the previously chosen sister of the dependent.
240
k-best Oracle best Oracle worst Reranked
1 92.58 92.58 92.58
2 94.22 88.66 92.89
3 95.05 87.04 93.02
4 95.51 85.82 93.02
5 95.78 84.96 93.02
6 96.02 84.20 93.06
7 96.23 83.62 93.09
8 96.40 83.06 93.02
9 96.54 82.57 92.97
10 96.64 82.21 92.96
100 98.48 73.30 92.32
1000 99.34 64.86 91.47 91.00%92.00%93.00%94.00%
95.00%96.00%97.00%98.00%99.00%
100.00%
1 2 3 4 5 6 7 8 9 10 100 1000
Oracle-BestRerankedMST
Figure 2: UAS accuracy of the MST discriminative and re-ranking parser on section 22 of the WSJ.
Oracle best: always choosing the best result in the k-best, Oracle worst: always choosing the worst,
Reranked: choosing the most probable candidate according to the generative model.
5 Conclusions
We have presented a general framework for depen-
dency parsing based on a combination of discrim-
inative and generative models. We have used this
framework to evaluate and compare several gener-
ative models, including those of Eisner (1996) and
some of their variations. Consistently with earlier
results, none of these models performs better than
the discriminative baseline when used alone. We
have presented an instantiation of this framework
in which our newly defined generative model leads
to an improvement of the state-of-the-art parsing
results, when provided with a limited number of
best candidates. This result suggests that discrim-
inative and generative model are complementary:
the discriminative model is very accurate to filter
out ?bad? candidates, while the generative model
is able to further refine the selection among the
few best candidates. In our set-up it is now pos-
sible to efficiently evaluate many other generative
models and identify the most promising ones for
further investigation. And even though we cur-
rently still need the input from a discriminative
model, our promising results show that pessimism
about the prospects of probabilistic generative de-
pendency models is premature.
Acknowledgments We gratefully acknowledge
funding by the Netherlands Organization for
Scientific Research (NWO): FS and RB are
funded through a Vici-grant ?Integrating Cogni-
tion? (277.70.006) to RB, and WZ through a Veni-
grant ?Discovering Grammar? (639.021.612) of
NWO. We also thank 3 anonymous reviewers for
useful comments.
References
S. Buchholz, and E. Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In Proc.
of the 10th CoNLL Conference, pp. 149?164.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. Collins, N. Duffy, and F. Park. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In In
Proceedings of the ACL 2002, pp. 263?270.
J. Eisner. 1996a. Three New Probabilistic Models for
Dependency Parsing: An Exploration. In Proc. of
the 16th International Conference on Computational
Linguistics (COLING-96), pp. 340?345.
J. Eisner. 1996b. An Empirical Comparison of Proba-
bility Models for Dependency Grammar. Technical
Report number IRCS-96-11, Univ. of Pennsylvania.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. In Computational Linguistics,
19(2), pp. 313?330.
R. McDonald. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
J. Nivre and J. Hall. 2005. MaltParser: A Language-
Independent System for Data-Driven Dependency
Parsing. In Proc. of the Fourth Workshop on Tree-
banks and Linguistic Theories, pp. 137?148.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son,S. Riedel, and D. Yuret. 2007. The CONLL
2007 shared task on dependency parsing. In Proc.
of the CoNLL 2007 Shared Task Session, pp. 915?
932.
F. Sangati and W. Zuidema. 2008. Unsupervised
Methods for Head Assignments. In Proc. of the
EACL 2009 Conference, pp. 701?709.
241
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 84?95,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP
Federico Sangati and Willem Zuidema
Institute for Logic, Language and Computation
University of Amsterdam
Science Park 904, 1098 XH Amsterdam, The Netherlands
{f.sangati,zuidema}@uva.nl
Abstract
We present a novel approach to Data-Oriented
Parsing (DOP). Like other DOP models, our
parser utilizes syntactic fragments of arbitrary
size from a treebank to analyze new sentences,
but, crucially, it uses only those which are
encountered at least twice. This criterion al-
lows us to work with a relatively small but
representative set of fragments, which can be
employed as the symbolic backbone of sev-
eral probabilistic generative models. For pars-
ing we define a transform-backtransform ap-
proach that allows us to use standard PCFG
technology, making our results easily replica-
ble. According to standard Parseval metrics,
our best model is on par with many state-of-
the-art parsers, while offering some comple-
mentary benefits: a simple generative proba-
bility model, and an explicit representation of
the larger units of grammar.
1 Introduction
Data-oriented Parsing (DOP) is an approach to
wide-coverage parsing based on assigning structures
to new sentences using fragments of variable size
extracted from a treebank. It was first proposed by
Scha in 1990 and formalized by Bod (1992), and
preceded many developments in statistical parsing
(e.g., the ?treebank grammars? of Charniak 1997)
and linguistic theory (e.g., the current popularity
of ?constructions?, Jackendoff 2002). A rich lit-
erature on DOP has emerged since, yielding state-
of-the-art results on the Penn treebank benchmark
test (Bod, 2001; Bansal and Klein, 2010) and in-
spiring developments in related frameworks includ-
ing tree kernels (Collins and Duffy, 2002), reranking
(Charniak and Johnson, 2005) and Bayesian adaptor
and fragment grammars (e.g., Johnson et al, 2007;
O?Donnell et al, 2009; Cohn et al, 2010). By for-
malizing the idea of using large fragments of earlier
language experience to analyze new sentences, DOP
captures an important property of language cogni-
tion that has shaped natural language. It therefore
complements approaches that have focused on prop-
erties like lexicalization or incrementality, and might
bring supplementary strengths in other NLP tasks.
Early versions of DOP (e.g., Bod et al, 2003)
aimed at extracting all subtrees of all trees in the
treebank. The total number of constructions, how-
ever, is prohibitively large for non-trivial treebanks:
it grows exponentially with the length of the sen-
tences, yielding the astronomically large number of
approximately 1048 for section 2-21 of the Penn
WSJ corpus. These models thus rely on a big sample
of fragments, which inevitably includes a substan-
tial portion of overspecialized constructions. Later
DOP models have used the Goodman transforma-
tion (Goodman, 1996, 2003) to obtain a compact
representation of all fragments in the treebank (Bod,
2003; Bansal and Klein, 2010). In this case the
grammatical constructions are no longer explicitly
represented, and substantial engineering effort is
needed to optimally tune the models and make them
efficient.
In this paper we present a novel DOP model
(Double-DOP) in which we extract a restricted yet
representative subset of fragments: those recurring
at least twice in the treebank. The explicit represen-
tation of the fragments allows us to derive simple
84
ways of estimating probabilistic models on top of the
symbolic grammar. This and other implementation
choices aim at making the methodology transparent
and easily replicable. The accuracy of Double-DOP
is well within the range of state-of-the-art parsers
currently used in other NLP-tasks, while offering the
additional benefits of a simple generative probability
model and an explicit representation of grammatical
constructions.
The contributions of this paper are summarized as
follows: (i) we describe an efficient tree-kernel algo-
rithm which allows us to extract all recurring frag-
ments, reducing the set of potential elementary units
from the astronomical 1048 to around 106. (ii) We
implement and compare different DOP estimation
techniques to induce a probability model (PTSG)
on top of the extracted symbolic grammar. (iii)
We present a simple transformation of the extracted
fragments into CFG-rules that allows us to use off-
the-shelf PCFG parsing and inference. (iv) We in-
tegrate Double-DOP with recent state-splitting ap-
proaches (Petrov et al, 2006), yielding an even more
accurate parser and a better understanding of the re-
lation between DOP and state-splitting.
The rest of the paper is structured as follows. In
section 2 we describe the symbolic backbone of the
grammar formalism that we will use for parsing.
In section 3 we illustrate the probabilistic exten-
sion of the grammar, including our transformation
of PTSGs to PCFGs that allows us to use a standard
PCFG parser, and a different transform that allows
us to use a standard implementation of the inside-
outside algorithm. In section 4 we present the ex-
perimental setup and the results.
2 The symbolic backbone
The basic idea behind DOP is to allow arbitrarily
large fragments from a treebank to be the elemen-
tary units of production of the grammar. Fragments
can be combined through substitution to obtain the
phrase-structure tree of a new sentence. Figure 1
shows an example of a complete syntactic tree ob-
tained by combining three elementary fragments. As
in previous work, two fragments fi and fj can be
combined (fi ? fj) only if the leftmost substitution
site X? in fi has the same label as the root node of
fj ; in this case the resulting tree will correspond to
fi with fj replacing X . The DOP formalism is dis-
cussed in detail in e.g., Bod et al (2003).
S
NP? VP
VBD
wore
NP? ?
NP
DT
The
NNP
Free
NNP
French
?
NP
JJ
black
NN
arm
NNS
bands
?
S
NP
DT
The
NNP
Free
NNP
French
VP
VBD
wore
NP
JJ
black
NN
arm
NNS
bands
Figure 1: An example of a derivation of a complete syn-
tactic structure (below) obtained combining three ele-
mentary fragments (above) by means of the substitution
operation ?. Substitution sites are marked with ?.
2.1 Finding Recurring Fragments
The first step to build a DOP model is to define its
symbolic grammar, i.e. the set of elementary frag-
ments in the model. In the current work we explic-
itly extract a subset of fragments from the training
treebank. To limit the fragment set size, we use a
simple but heretofore unexplored constraint: we ex-
tract only those fragments that occur two or more
times in the treebank1. Extracting this particular
set of fragments is not trivial, though: a naive ap-
proach that filters a complete table of fragments to-
gether with their frequencies fails because that set, in
a reasonably sized treebank, is astronomically large.
Instead, we use a dynamic programming algorithm
based on tree-kernel techniques (Collins and Duffy,
2001; Moschitti, 2006; Sangati et al, 2010).
The algorithm iterates over every pair of trees in
1More precisely we extract only the largest shared fragments
for all pairs of trees in the treebank. All subtrees of these ex-
tracted fragments necessarily also occur at least twice, but they
are only explicitly represented in our extracted set if they hap-
pen to form a largest shared fragment from another pair of trees.
Hence, if a large tree occurs twice in the treebank the algorithm
will extract from this pair only the full tree as a fragment and
not all its (exponentially many) subtrees.
85
S
NP
PRP
I
VP
VBP
say
SBAR
S
NP
PRP
they
VP
VBP
are
ADJP
JJ
ready
.
.
S
NP
NNS
Analysts
VP
VBP
say
SBAR
S
NP
NNP
USAir
VP
VBZ
has
NP
JJ
great
NN
promise
.
.
S NP PRP VP VBP SBAR S NP PRP VP VBP ADJP JJ .S ? ?NP ? ?NNSVP ? ?VBP ? ?SBAR ?S ? ?NP ? ?NNPVP ?VBZNPJJ ?NN. ?? ?
Figure 2: Left: example of two trees sharing a single maximum fragment, circled in the two trees. Right: the chart
M which is used in the dynamic algorithm to extract all maximum fragments shared between the two trees. The
highlighted cells in the chart are the ones which contribute to extract the shared fragment. The marked cells are those
for which the corresponding nodes in the two tree have equivalent labels but differ in their lists of child nodes.
the treebank to look for common fragments. Fig-
ure 2 shows an example of a pair of trees ??,?? be-
ing compared. The algorithm builds a chartM with
one column for every indexed non-terminal node ?i
in ?, and one row for every indexed non-terminal
node ?j in ?. Each cellM?i, j? identifies a set of in-
dices corresponding to the largest fragment in com-
mon between the two trees starting from ?i and ?j .
This set is empty if ?i and ?j differ in their labels,
or they don?t have the same list of child nodes. Oth-
erwise (if both the labels and the lists of children
match) the set is computed recursively as follows:
M?i, j? = {?i} ??
? ?
c={1,2,...,|ch(?)|}
M?ch(?i, c), ch(?j , c)?
?
? (1)
where ch(?) returns the indices of ??s children, and
ch(?, c) the index of its cth child.
After filling the chart, the algorithm extracts the
set of recurring fragments, and stores them in a ta-
ble to keep track of their counts. This is done by
converting back each fragment implicitly defined in
every cell-set2, and filtering out those that are prop-
erly contained in others.
In a second pass over the treebank, exact counts
are obtained for each fragment in the extracted set.
2A cell-set containing a single index corresponds to the frag-
ment including the node with that index together with all its
children.
Parse trees in the training corpus are not necessarily
covered entirely by recurring fragments; to ensure
coverage, we also include in the symbolic backbone
of our Double-DOP model all PCFG-productions
not included in the set of extracted fragments.
2.2 Comparison with previous DOP work
Explicit grammars The number of recurring frag-
ments in our symbolic grammar, extracted from
the training sections of the Penn WSJ treebank3, is
around 1 million, and thus is significantly lower than
previous work extracting explicit fragments (e.g.,
Bod, 2001, used more than 5 million fragments up
to depth 14).
When looking at the extracted fragments we ask
if we could have predicted which fragments occur
twice or more. Figure 3 attempts to tackle this ques-
tion by reporting some statistics on the extracted
fragments. The majority of fragments are rather
small with a limited number of words or substitution
sites in the frontier. Yet, there is a significant por-
tion of fragments, in the tail of the distribution, with
more than 10 words or substitution sites. Since the
space of all fragments with such characteristics is
enormously large, selecting big recurring fragments
using random sampling technique is like finding a
needle in a haystack. Hence, random sampling pro-
cesses (like Bod, 2001), will tend to represent fre-
3This is after the treebank has been preprocessed. See also
section 4.
S
NP
PRP
I
VP
VBP
say
SBAR
S
NP
PRP
they
VP
VBP
are
ADJP
JJ
ready
.
.
S
NP
NNS
Analysts
VP
VBP
say
SBAR
S
NP
NNP
USAir
VP
VBZ
has
NP
JJ
great
NN
promise
.
.
S NP PRP VP VBP SBAR S NP PRP VP VBP ADJP JJ .S ? ?NP ? ?NNSVP ? ?VBP ? ?SBAR ?S ? ?NP ? ?NNPVP ?VBZNPJJ ?NN. ?? ?
Figure 2: Left: example of two trees sharing a single maximum fragment, circled in the two trees. Right: the chart
M which is used in the dynamic algorithm to extract all maximum fragments shared between the two trees. The
highlighted cells in the chart are the ones which contribute to extract the shared fragment. The marked cells are those
for which the corresponding nodes in the two tree have equivalent labels but differ in their lists of child nodes.
the treebank to look for common fragments. Fig-
ure 2 shows an example of a pair of trees ??,?? be-
ing compared. The algorithm builds a chartM with
one column for every indexed non-terminal node ?i
in ?, and one row for every indexed non-terminal
node ?j in ?. Each cellM?i, j? identifies a set of in-
dices corresponding to the largest fragment in com-
mon between the two trees starting from ?i and ?j .
This set is empty if ?i and ?j differ in their labels,
or they don?t have the same list of child nodes. Oth-
erwise (if both the labels and the lists of children
match) the set is computed recursively as follows:
M?i, j? = {?i} ??
? ?
c={1,2,...,|ch(?)|}
M?ch(?i, c), ch(?j , c)?
?
? (1)
where ch(?) returns the indices of ??s children, and
ch(?, c) the index of its cth child.
After filling the chart, the algorithm extracts the
set of recurring fragments, and stores them in a ta-
ble to keep track of their counts. This is done by
converting back each fragment implicitly defined in
every cell-set2, and filtering out those that are prop-
erly contained in others.
In a second pass over the treebank, exact counts
are obtained for each fragment in the extracted set.
2A cell-set containing a single index corresponds to the frag-
ment including the node with that index together with all its
children.
Parse trees in the training corpus are not necessarily
covered entirely by recurring fragments; to ensure
coverage, we also include in the symbolic backbone
of our Double-DOP model all PCFG-productions
not included in the set of extracted fragments.
2.2 Comparison with previous DOP work
Explicit grammars The number of recurring frag-
ments in our symbolic grammar, extracted from
the training sections of the Penn WSJ treebank3, is
around 1 million, and thus is significantly lower than
previous work extracting explicit fragments (e.g.,
Bod, 2001, used more than 5 million fragments up
to depth 14).
When looking at the extracted fragments we ask
if we could have predicted which fragments occur
twice or more. Figure 3 attempts to tackle this ques-
tion by reporting some statistics on the extracted
fragments. The majority of fragments are rather
small with a limited number of words or substitution
sites in the frontier. Yet, there is a significant por-
tion of fragments, in the tail of the distribution, with
more than 10 words or substitution sites. Since the
space of all fragments with such characteristics is
enormously large, selecting big recurring fragments
using rando sa pling technique is like finding a
needle in a haystack. Hence, rando sa pling pro-
cesses (like Bod, 2001), ill tend to represent fre-
3This is after the treebank has been preprocessed. See also
section 4.
S NP PRP VP VBP SBAR S NP PRP VP VBP ADJP JJ .S ? ?NP ? ?NNSVP ? ?VBP ? ?SBAR ?S ? ?NP ? ?NNPVP ?VBZNPJJ ?NN. ??
Figure 2: Left: t trees sharing a single maximum fragment, ircled in the two trees. Rig t: t e chart
M which is us i t ic algorithm to extract all maximum fragments shared b tween the two trees. The
highlighted cells in the chart are the ones which contribute to extract the shared fragment. The marked cells are those
for which the corresponding nodes in the two tree have equivalent labels but differ in their lists of child nodes.
the treebank to look for common fragments. Fig-
ure 2 shows an example of a pair of trees ??, ?? be-
i g c mpared. The algorithm builds a chartM with
one column fo every ind xed non-terminal node ?i
in ?, and one row for every ndexed non-terminal
node ?j in ?. Each cellM?i, j? identifies a set of in-
dices corresponding to the l gest agment in com-
mon between the two trees starting from ?i and ?j .
This set is empty if ?i and ?j differ i th ir labels,
or they don?t have the same list of child nodes. Oth-
erwise (if both the labels and the lists of children
match) the set is computed recursively as follows:
M?i, j? = {?i} ??
? ?
c={1,2,...,|ch(?)|}
M?ch(?i, c), ch(?j , c)?
?
? (1)
where ch(?) returns the indices of ??s children, and
ch(?, c) the index of its c child.
After filling the chart, the algorithm extracts the
set of recurring fragments, and stores them in a ta-
ble to keep track of their counts. This is done by
converting back each fragment implicitly defined in
every cell-set2, and filtering out those that are prop-
erly contained in others.
In a second pass over the treebank, exact counts
are obtained for each fragment in the extracted set.
2A cell-set containing a single index corresponds to the frag-
ment including the node with that index together with all its
children.
Parse trees in the training corpus are not necessarily
covered entirely by recurring fragments; to ensure
c verage, we also include in the symb lic backbone
of o r Double-DOP model all PCFG-productions
not included in the set of extracted fragments.
2.2 Comparison with previous DOP work
Explicit g am ars The numb r of recurring frag-
ments in our symbolic grammar, extracted from
the training sections of the Penn WSJ treebank3, is
around 1 million, and thus is significantly lower than
previous work extracti g explicit fragments (e.g.,
Bod, 2001, used more than 5 million fragments up
to d pth 14).
When looking at th extra ted fragments we ask
if we could have predicted which fragments occur
twice or more. Figure 3 attempts to tackle this ques-
tion by reporting some statistics on the extracted
fragments. The majority of fragments are rather
small with a limited number of words or substitution
sites in the frontier. Yet, there is a significant por-
tion of fragments, in the tail of the distribution, with
more than 10 words or substitution sites. Since the
space of all fragments with such characteristics is
enormously large, selecting big recurring fragments
using random sampling technique is like finding a
needle in a haystack. Hence, random sampling pro-
cesses (like Bod, 2001), will tend to represent fre-
3This is after the treebank has been preprocessed. See also
section 4.
86
quent recurring constructions such as from NP to
NP or whether S or not, together with infrequent
overspecialized fragments like from Houston to NP,
while missing large generic constructions such as
everything you always wanted to know about NP but
were afraid to ask. These large constructions are
excluded completely by models that only allow ele-
mentary trees up to a certain depth (typically 4 or 5)
into the symbolic grammar (Zollmann and Sima?an,
2005; Zuidema, 2007; Borensztajn et al, 2009), or
only elementary trees with exactly one lexical an-
chor (Sangati and Zuidema, 2009).
100101102103104105106  0
 
10
 
20
 
30
 
40
 
50
Number of Fragments
Depth
 / Wo
rds / S
ubstit
ution 
SitesDepth Words
Subst
itutio
n Site
s
Figure 3: Distribution of the recurring fragments types
according to several features: depth, number of words,
and number of substitution sites. Their corresponding
curves peak at 4 (depth), 1 (words), and 4 (substitution
sites).
Implicit grammars Goodman (1996, 2003) de-
fined a transformation for some versions of DOP to
an equivalent PCFG-based model, with the number
of rules extracted from each parse tree linear in the
size of the trees. This transform, representing larger
fragments only implicitly, is used in most recent
DOP parsers (e.g., Bod, 2003; Bansal and Klein,
2010). Bod has promoted the Goodman transform as
the solution to the computational challenges of DOP
(e.g., Bod, 2003); it?s important to realize, how-
ever, that the resulting grammars are still very large:
WSJ sections 2-21 yield about 2.5 million rules in
the basic version of Goodman?s transform. More-
over, the transformed grammars differ from untrans-
formed DOP grammars in that larger fragments are
no longer explicitly represented. Rather, informa-
tion about their frequency is distributed over many
CFG-rules: if a construction occurs n times and con-
tains m context-free productions, Goodman?s trans-
form uses the weights of 7nm +m rules to encode
this fact. Thus, the information that the idiomatic
fragment (PP (IN ?out?) (PP (IN ?of?) (NP (NN
?town?))))) occurs 3 times in WSJ sections 2-21, is
distributed over 132 rules. This way, an attractive
feature of DOP, viz. the explicit representation of
the ?productive units? of language, is lost4.
In addition, grammars that implicitly encode all
fragments found in a treebank are strongly biased to
over-represent big constructions: the great majority
of the entire set of fragments belongs in fact to the
largest tree in the treebank5. DOP models relying on
Goodman?s transform, need therefore to counteract
this tendency. Bansal and Klein (2010), for instance,
rely on a sophisticated tuning technique to correctly
adjust the weights of the rules in the grammar. In
our Double-DOP approach, instead, the number of
fragments extracted from each tree varies much less
(it ranges between 4 and 1,759). This comparison is
shown in figure 4.
3 The probabilistic model
Like CFG grammars, our symbolic model produces
extremely many parse trees for a given test sentence.
We therefore need to disambiguate between the pos-
sible parses by means of a probability model that as-
signs probabilities to fragments, and defines a proper
distribution over the set of possible full parse trees.
For every nonterminal X in the treebank we have:
?
f?FX
p(f) = 1 (2)
where FX is the set of fragments in our sym-
bolic grammar rooted in X . A derivation d =
f1, f2, . . . , fn of t is a sequence of the fragments that
through left-most substitution produces t. The prob-
ability of a derivation is computed as the product of
4Bansal and Klein (2010) address this issue for contigu-
ous constructions by extending the Goodman transform with
a ?Packed Graph Encoding? for fragments that ?bottom out in
terminals?. However, constructions with variable slots, such as
whether S or not, are left unchanged.
5In fact, the number of extracted fragments increase expo-
nentially with the size of the tree.
87
510102103105101010201050
0
1?104
2?104
3?104
4?104
Number of fragments
Rank
 of tre
e from
 train
 set
Recu
rring 
fragm
ents
All fr
agme
nts
Figure 4: Number of fragments extracted from each tree
in sections 2-21 of the WSJ treebank, when considering
all-fragments (dotted line) and recurring-fragments (solid
line). Trees on the x-axis are ranked according to the
number of fragments. Note the double logarithmic scale
on the y-axis.
the probability of each of its fragments.
P (d) =
?
f?d
p(f) (3)
In section 3.2 we describe ways of obtaining dif-
ferent probability distributions over the fragments in
our grammar. In the following section we assume a
given probabilistic model, and illustrate how to use
standard PCFG parsing.
3.1 Parsing
It is possible to define a simple transform of our
probabilistic fragment grammar, such that off-the-
shelf parsers can be used. In order to perform
the PTSG/PCFG conversion, every fragment in our
grammar must be mapped to a CFG rule which will
keep the same probability as the original fragment.
The corresponding rule will have as the left hand
side the root of the fragment and as the right hand
side its yield, i.e., a sequence of terminals and non-
terminals (substitution sites).
It might occur that several fragments are mapped
to the same CFG rule6. These are interesting cases
of syntactic ambiguity as shown in figure 5. In order
to resolve this problem we need to map each am-
biguous fragment to two unique CFG rules chained
6In our binarized treebank we have 31,465 fragments types
that are ambiguous in this sense.
by a unique artificial node, as shown at the bottom
of the same figure. To the first CFG rule in the chain
we assign the probability of the fragment, while the
second will receive probability 1, so the product
gives back the original probability. The ambiguous
and unambiguous PTSG/PCFG mappings need to be
stored in a table, in order to convert back the com-
pressed CFG derivations to the original PTSG model
after parsing.
Such a transformed PCFG will generate the same
derivations as the original PTSG grammar with iden-
tical probabilities. In our experiment we use a stan-
dard PCFG parser to produce a list of k-best Viterbi
derivations. These, in turn, will be used to maximize
possible objectives as described in section 3.3.
VP
VBD NP
NP
DT NN
PP
IN
?with?
NP
VP
VBD NP
DT NN
PP
IN
?with?
NP
m m
VP
NODE@7276
VP
NODE@7277
NODE@7276
VBD DT NN ?with? NP
NODE@7277
VBD DT NN ?with? NP
Figure 5: Above: example of 2 ambiguous fragments
mapping to the same CFG rule VP ? VBD DT NN
?with? NP. The first fragment occurs 5 times in the train-
ing treebank, (e.g. in the sentence was an executive with
a manufacturing concern) while the second fragment oc-
curs 4 times (e.g. in the sentence began this campaign
with such high hopes). Below: the two pairs of CFG rules
that are used to map the two fragments to separate CFG
derivations.
3.2 Inducing probability distributions
Relative Frequency Estimate (RFE) The sim-
plest way to assign probabilities to fragments is to
make them proportional to their counts7 in the train-
ing set. When enforcing equation 2, that gives the
7We refer to the counts of each fragment as returned by our
extraction algorithm in section 2.1.
88
Relative Frequency Estimate (RFE):
pRFE(f) =
count(f)?
f ??Froot(f) count(f ?)
(4)
Unlike RFE for PCFGs, however, the RFE for
PTSGs has no clear probabilistic interpretation. In
particular, it does not yield the maximum likelihood
solution, and when used as an estimator for an all-
fragments grammar, it is strongly biased since it as-
signs the great majority of the probability mass to
big fragments (Johnson, 2002). As illustrated in fig-
ure 4 this bias is much weaker when restricting the
set of fragments with our approach. Although this
does not solve all theoretical issues, it makes RFE a
reasonable first choice again.
Equal Weights Estimate (EWE) Various other
ways of choosing the weights of a DOP grammar
have been worked out. The best empirical results
have been reported by Bod (2003) with the EWE
proposed by Goodman (2003). Goodman defined it
for grammars in the Goodman transform, but for ex-
plicit grammars it becomes:
wEWE(f) =
?
t?TB
count(f, t)
|{f ? ? t}| (5)
pEWE(f) =
wEWE(f)?
f ??Froot(f) wEWE(f ?)
(6)
where the first sum is over all parse trees t in the tree-
bank (TB), count(f, t) gives the number of times
fragment f occurs in t, and |{f ? ? t}| is the total
number of subtrees of t that were included in the
symbolic grammar.
Maximum Likelihood (ML) For reestimation,
we can aim at maximizing the likelihood (ML) of
the treebank. For this, it turns out that we can de-
fine another transformation of our PTSG, such that
we can apply standard Inside-Outside algorithm for
PCFGs (Lari and Young, 1990). The original ver-
sion of IO is defined over string rewriting PCFGs,
and maximizes the likelihood of the training set con-
sisting of plain sentences. Reestimation shifts prob-
ability mass between alternative parse trees for a
sentence. In contrast, our grammars consist of frag-
ments of various size, and our training set consists
of parse trees. Reestimation here shifts probability
mass between alternative derivations for a parse tree.
Our transformation approach is illustrated with an
example in figure 6. In step (b) the fragments in
the grammar as well as the original parse trees in
the treebank are ?flattened? into bracket notation. In
step (c) each fragment is transformed into a CFG
rule in the transformed meta-grammar, whose right-
hand side is constituted by the bracket notation of
the fragment. Each substitution site X? is raised to
a meta-nonterminal X ?, and all other symbols, in-
cluding parentheses, become meta-terminals. The
left-hand side of the rule is constituted by the origi-
nal root symbol R of the fragment raised to a meta-
nonterminal R?.
The resulting PCFG generates trees in bracket no-
tation, and we can run an of-the-shelf inside-outside
algorithm by presenting it parse trees from the train
corpus in bracket notation8. In the experiments that
we report below we used the RFE from section 3, to
generate the initial weights for the grammar.
(a)
S
A? B
y
?
A
x =
S
A
x
B
y
(b) ( S A? ( B y ) ) ? ( A x ) = ( S ( A x ) ( B y ) )
(c) S?? ( S A? ( B y ) ) ? A?? ( A x ) =
S?
( S A?
( A x )
( B y ) )
(d) ( S ( A x ) ( B y ) )
Figure 6: Rule and tree transforms that turn PTSG rees-
timation into PCFG reestimation; (a) a derivation of the
sentence x y through successive substitutions of elemen-
tary trees from a PTSG; (b) the same elementary trees
and resulting parse tree in bracket notation; (c) an equiva-
lent derivation with the meta-grammar, where the original
substitution sites reappear as meta-nonterminals (marked
with a prime) and all other symbols as meta-terminals;
(d) the yield of the derivation in c.
8However, the results with inside-outside reported in this pa-
per were obtained with an earlier version of our code that uses
an equivalent but special-purpose implementation.
89
3.3 Maximizing Objectives
MPD The easiest objective in parsing, is to se-
lect the most probable derivation (MPD), obtained
by maximizing equation 3.
MPP A DOP grammar can often generate the
same parse tree t through different derivations
D(t) = d1, d2, . . . dm. The probability of t is there-
fore obtained by summing the probabilities of all its
possible derivations.
P (t) =
?
d?D(t)
p(d) =
?
d?D(t)
?
f?d
p(f) (7)
An intuitive objective for a parser is to select, for
a given sentence, the parse tree with highest proba-
bility according to equation 7, i.e., the most probable
parse (MPP): unfortunately, identifying the MPP is
computationally intractable (Sima?an, 1996). How-
ever, we can approximate the MPP by deriving a list
of k-best derivations, summing up the probabilities
of those resulting in the same parse tree, and select
the tree with maximum probability.
MCP, MRS Following Goodman (1998), Sima?an
(1999, 2003), and others, we also consider other
objectives, in particular, the max constituent parse
(MCP), and the max rule sum (MRS).
MCP maximizes a weighted average of the ex-
pected labeled recall L/NC and (approximated) la-
beled precision L/NG under the given posterior dis-
tribution, where L is the number of correctly labeled
constituents, NC the number of constituents in the
correct tree, and NG the number of constituents in
the guessed tree. Recall is easy to maximize since
the estimated NC is constant. L/NC can be in fact
maximized in:
t? = argmax
t
?
lc?t
P (lc) (8)
where lc ranges over all labeled constituents in t
and P (lc) is the marginalized probability of all the
derivation trees in the grammar yielding the sentence
under consideration which contains lc.
Precision, instead, is harder because the denom-
inator NG depends on the chosen guessed tree.
Goodman (1998) proposes to look at another metric
which is strongly correlated with precision, which is
the mistake rate (NG?L)/NC that we want to min-
imize. We combine recall with mistake rate through
linear interpolation:
t? = argmax
t
E( LNC
? ?NG ? LNC
) (9)
= argmax
t
?
lc?t
P (lc)? ?(1? P (lc)) (10)
where 10 is obtained from 9 assuming NC constant,
and the optimal level for ? has to be evaluated em-
pirically.
Unlike MPP, the MCP can be calculated effi-
ciently using dynamic programming techniques over
the parse forest. However, in line with the aims of
this paper to produce an easily reproducible imple-
mentation of DOP, we developed an accurate ap-
proximation of the MCP using a list of k-best deriva-
tions, such as those that can be obtained with an off-
the-shelf PCFG parser.
We do so by building a standard CYK chart,
where every cell corresponds to a specific span in
the test sentence. We store in each cell the proba-
bility of seeing every label in the grammar yielding
the corresponding span, by marginalizing the prob-
abilities of all the parse trees in the obtained k-best
derivations that contains that label covering the same
span. We then compute the Viterbi-best parse maxi-
mizing equation 10.
We implement max rule sum (MRS) in a similar
way, but do not only keep track of labels in every
cell, but of each CFG rule that span the specific yield
(see also Sima?an, 1999, 2003). We haven?t im-
plemented the max rule product (MRP) where pos-
teriors are multiplied instead of added (Petrov and
Klein, 2007; Bansal and Klein, 2010).
4 Experimental Setup
In order to build and test our Double-DOP model9,
we employ the Penn WSJ Treebank (Marcus et al,
1993). We use sections 2-21 for training, section 24
for development and section 23 for testing.
Treebank binarization We start with some pre-
processing of the treebank, following standard prac-
9The software produced for running our model is publicly
available and included in the supplementary material to this pa-
per. To the best of our knowledge this is the first DOP software
released that can be used to parse the WSJ PTB.
90
S
NP|S
NP|S@NNP|NP
DT|NP
The
NNP|NP
Free
NNP|NP
French
VP|S
VBD|VP
wore
NP|VP
NP|VP@NN|NP
JJ|NP
black
NN|NP
arm
NNS|NP
bands
Figure 7: The binarized version of the tree in figure 1,
with H=1 and P=1.
tice in WSJ parsing. We remove traces and func-
tional tags. We apply a left binarization of the train-
ing treebank as in Matsuzaki et al (2005) and Klein
and Manning (2003), setting the horizontal history
H=1 and the parent labeling P=1. This means that
when a node has more than 2 children, the ith child
(for i ? 3) is conditioned on child i ? 1. Moreover
the labels of all non-lexical nodes are enriched with
the labels of their parent node. Figure 7 shows the
binarized version of the tree structure in figure 1.
Unknownwords We replace words appearing less
than 5 times in the training data by one of 50 un-
known word categories based on the presence of lex-
ical features as implemented in Petrov (2009). In
some of the experiments we also perform a smooth-
ing over the lexical elements assigning low counts
( = 0.01) to open-class ?words, PoS-tags? pairs not
encountered in the training corpus10.
Fragment extraction We extract the symbolic
grammar and fragment frequencies from this prepro-
cessed treebank as explained in section 2. This is
the the most time-consuming step (around 160 CPU
hours11).
In the extracted grammar we have in total
1,029,342 recurring fragments and 17,768 unseen
CFG rules. We test several probability distributions
over the fragments (section 3.2) and various maxi-
mization objectives (section 3.3).
10A PoS-tag is an open class if it rewrites to at least 50 differ-
ent words in the training corpus. A word is an open class word
if it has been seen only with open-class PoS-tags.
11Although our code could still be optimized further, it does
already allow for running the job on M CPUs in parallel, reduc-
ing the time required by a factor M (10 hours with 16-CPUs).
86.086.587.087.588.0  
0
 
0.5
 
1 1.1
5
 
1.5
 
2
F1 / Recall / Precision (%)
?M
ax Co
nst. P
arse
Max 
Rule 
Sum
Max 
Proba
ble Pa
rse
Max 
Proba
ble D
erivat
ion
Precis
ion (M
CP)
F1 sc
ore (M
CP)
Recal
l (MCP
)
Figure 8: Double-DOP results on the development sec-
tion (? 40) with different maximizing objectives.
Parsing We convert our PTSG into a PCFG (sec-
tion 3.1) and use Bitpar12 for parsing. For approx-
imating MPP and other objectives we marginalize
probabilities from the 1,000 best derivations.
4.1 Results
We start by presenting in figure 8 the results we ob-
tain on the development set (section 24). Here we
compare the maximizing objectives presented in sec-
tion 3.3, using RFE to obtain the probability distri-
bution over the fragments. We conclude that, em-
pirically, MCP for ? = 1.15, is the best choice to
maximize F1, followed by MRS, MPP, and MPD.
We also compare the various estimators presented
in section 3.2, on the same development set, keep-
ing MCP with ? = 1.15 as the maximizing objec-
tive. We find that RFE is the best estimator (87.2
F113) followed by EWE (86.8) and ML (86.6). Our
best results with ML are obtained when removing
fragments occurring less than 6 times (apart from
CFG-rules) and when stopping at the second iter-
ation. This filtering is done in order to limit the
number of big fragments in the grammar. It is well
known that IO for DOP tends to assign most of the
probability mass to big fragments, quickly overfit-
ting the training data. It is surprising that EWE and
ML perform worse than RFE, in contrast to earlier
findings (Bod, 2003).
12http://www.ims.uni-stuttgart.de/tcl/
SOFTWARE/BitPar.html
13We computed F1 scores with EvalB (http://nlp.cs.
nyu.edu/evalb/) using parameter file new.prm.
91
 
80
 
81
 
82
 
83
 
84
 
85
 
86
 
87
 
88  1
 
10 2
0
 
50
 
1001
04105106107
F1
Number of fragments
Fragm
ent fr
equen
cy thr
eshol
d
F1
Doub
le-DO
P gra
mmar
 size
Num
ber of
 PCFG
 rules
Figure 9: Performance (on the development set) and size
of Double-DOP when considering only fragments whose
occurring frequency in the training treebank is above a
specific threshold (x-axis). In all cases, all PCFG-rules
are included in the grammars. For instance, at the right-
hand side of the plot a grammar is evaluated which in-
cluded only 6754 fragments with a frequency > 100 as
well as 39227 PCFG rules.
We also investigate how a further restriction on
the set of extracted fragments influences the perfor-
mance of our model. In figure 9 we illustrate the
performance of Double-DOP when restricting the
grammar to fragments having frequencies greater
than 1, 2, . . . , 100. We can notice a rather sharp
decrease in performance as the grammar becomes
more and more compact.
Next, we present some results on various Double-
DOP grammars extracted from the same training
treebank after refining it using the Berkeley state-
splitting model14 (Petrov et al, 2006; Petrov and
Klein, 2007). In total we have 6 increasingly refined
versions of the treebank, corresponding to the 6 cy-
cles of the Berkeley model. We observe in figure 10
that our grammar is able to benefit from the state
splits for the first four levels of refinement, reaching
the maximum score at cycle 4, where we improve
over our base model. For the last two data points, the
treebank gets too refined, and using Double-DOP
model on top of it, no longer improves accuracy.
We have also compared our best Double-DOP
14We use the Berkeley grammar labeler following the base
settings for the WSJ: trees are right-binarized, H=0, and
P=0. Berkeley parser package is available at http://code.
google.com/p/berkeleyparser/
 
74
 
76
 
78
 
80
 
82
 
84
 
86
 
88
 
90
 
92
 
1
 
2
 
3
 
4
 
5
 
6
F1
Berke
ley gr
amma
r/tree
bank 
refine
ment 
level
Berke
ley M
RP
Berke
ley M
PD
Doub
le-DO
P
Doub
le-DO
P Lex
 smoo
th
Figure 10: Comparison on section 24 between the per-
formance of Double-DOP (using RFE and MCP with
? = 1.15, H=0, P=0) and Berkeley parser on different
stages of refinement of the treebank/grammar.
base model and the Berkeley parser on per-category
performance. Here we observe an interesting trend:
the Berkeley parser outperforms Double-DOP on
very frequent categories, while Double-DOP per-
forms better on infrequent ones. A detailed com-
parison is included in table 1.
Finally, in table 2 we present our results on the
test set (section 23). Our best model (according to
the best settings on the development set) performs
slightly worse than the one by Bansal and Klein
(2010) when trained on the original corpus, but out-
performs it (and the version of their model with
additional refinements) when trained on the refined
version, in particular for the exact match score.
5 Conclusions
We have described Double-DOP, a novel DOP ap-
proach for parsing, which uses all constructions re-
curring at least twice in a treebank. This method-
ology is driven by the linguistic intuition that con-
structions included in the grammar should prove to
be reusable in a representative corpus.
The extracted set of fragments is significantly
smaller than in previous approaches. Moreover con-
structions are explicitly represented, which makes
them potentially good candidates as semantic or
translation units to be used in other applications.
Despite earlier reported excellent results with
DOP parsers, they are almost never used in other
92
Category % F1 F1label in gold Berkeley Double-DOPNP 41.42 91.4 89.5VP 20.46 90.6 88.6S 13.38 90.7 87.6PP 12.82 85.5 84.1SBAR 3.47 86.0 82.1ADVP 3.36 82.4 81.0ADJP 2.32 68.0 67.3QP 0.98 82.8 84.6WHNP 0.88 94.5 92.0WHADVP 0.33 92.8 91.9PRN 0.32 83.0 77.9NX 0.29 9.50 7.70SINV 0.28 90.3 88.1SQ 0.14 82.1 79.3FRAG 0.10 26.4 34.3SBARQ 0.09 84.2 88.2X 0.06 72.0 83.3NAC 0.06 54.6 88.0WHPP 0.06 91.7 44.4CONJP 0.04 55.6 66.7LST 0.03 61.5 33.3UCP 0.03 30.8 50.0INTJ 0.02 44.4 57.1
Table 1: Comparison of the performance (per-category
F1 score) on the development set between the Berkeley
parser and the best Double-DOP model.
NLP tasks: where other successful parsers often fea-
ture as components of machine translation, semantic
role labeling, question-answering or speech recogni-
tion systems, DOP is conspicuously absent in these
neighboring fields (but for a possible application of
closely related formalisms see, e.g., Yamangil and
Shieber, 2010). The reasons for this are many, but
most important are probably the computational inef-
ficiency of many instances of the approach, the lack
of downloadable software and the difficulties with
replicating some of the key results.
In this paper we have addressed all three obsta-
cles: our efficient algorithm for identifying the re-
current fragments in a treebank runs in polynomial
time. The transformation to PCFGs that we define
allows us to use a standard PCFG parser, while re-
taining the benefit of explicitly representing larger
fragments. A different transform also allows us to
run the popular inside-outside algorithm. Although
IO results are slightly worse than with the naive
relative frequency estimate, it is important to es-
tablish that the standard method for dealing with
latent information (i.e., the derivations of a given
parse) is not the best choice in this case. We expect
that other re-estimation methods, for instance Vari-
test (? 40) test (all)
Parsing Model F1 EX F1 EX
PCFG Baseline
PCFG (H=1, P=1) 77.6 17.2 76.5 15.9
PCFG (H=1, P=1) Lex smooth. 78.5 17.2 77.4 16.0
FRAGMENT-BASED PARSERS
Zuidema (2007)* 83.8 26.9 - -
Cohn et al (2010) MRS 85.4 27.2 84.7 25.8
Post and Gildea (2009) 82.6 - - -
Bansal and Klein (2010) MCP 88.5 33.0 87.6 30.8
Bansal and Klein (2010) MCP 88.7 33.8 88.1 31.7
+ Additional Refinement
THIS PAPER
Double-DOP 87.7 33.1 86.8 31.0
Double-DOP Lex smooth. 87.9 33.7 87.0 31.5
Double-DOP-Sp 88.8 35.9 88.2 33.8
Double-DOP-Sp Lex smooth. 89.7 38.3 89.1 36.1
REFINEMENT-BASED PARSERS
Collins (1999) 88.6 - 88.2 -
Petrov and Klein (2007) 90.6 39.1 90.1 37.1
Table 2: Summary of the results of different parsers
on the test set (sec 23). Double-DOP experiments use
RFE, MCP with ? = 1.15, H=1, P=1; those on state-
splitting (Double-DOP-Sp) use Berkeley cycle 4, H=0,
P=0. Results from Petrov and Klein (2007) already in-
clude smoothing which is performed similarly to our
smoothing technique (see section 4). (* Results on a de-
velopment set, with sentences up to length 20.)
ational Bayesian techniques, could be formulated in
the same manner.
Finally, the availability of our programs, as well
as the third party software that we use, also ad-
dresses the replicability issue. Where some re-
searchers in the field have been skeptical of the DOP
approach to parsing, we believe that our independent
development of a DOP parser adds credibility to the
idea that an approach that uses very many large sub-
trees, can lead to very accurate parsers.
Acknowledgments
We gratefully acknowledge funding by the
Netherlands Organization for Scientific Research
(NWO): FS is funded through a Vici-grant ?Inte-
grating Cognition? (277.70.006) to Rens Bod, and
WZ through a Veni-grant ?Discovering Grammar?
(639.021.612). We also thank Rens Bod, Gideon
Borensztajn, Jos de Bruin, Andreas van Cranen-
burgh, Phong Le, Remko Scha, Khalil Sima?an and
the anonymous reviewers for very useful comments.
93
References
Mohit Bansal and Dan Klein. 2010. Simple, accu-
rate parsing with an all-fragments grammar. In
Proceedings of the 48th Annual Meeting of the
ACL, pages 1098?1107. Association for Compu-
tational Linguistics, Uppsala, Sweden.
Rens Bod. 1992. A computational model of lan-
guage performance: Data oriented parsing. In
Proceedings COLING?92 (Nantes, France), pages
855?859. Association for Computational Linguis-
tics, Morristown, NJ.
Rens Bod. 2001. What is the minimal set of frag-
ments that achieves maximal parse accuracy? In
Proceedings of the ACL. Morgan Kaufmann, San
Francisco, CA.
Rens Bod. 2003. An efficient implementation of a
new DOP model. In Proceedings of the tenth con-
ference on European chapter of the Association
for Computational Linguistics - Volume 1, EACL
?03, pages 19?26. Association for Computational
Linguistics, Morristown, NJ, USA.
Rens Bod, Khalil Sima?an, and Remko Scha. 2003.
Data-Oriented Parsing. University of Chicago
Press, Chicago, IL, USA.
Gideon Borensztajn, Willem Zuidema, and Rens
Bod. 2009. Children?s Grammars Grow More
Abstract with Age?Evidence from an Automatic
Procedure for Identifying the Productive Units of
Language. Topics in Cognitive Science, 1(1):175?
188.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Pro-
ceedings of the Fourteenth National Conference
on Artificial Intelligence, pages 598?603. AAAI
Press/MIT Press.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proc. 43nd Meeting of Association
for Computational Linguistics (ACL 2005).
Trevor Cohn, Phil Blunsom, and Sharon Goldwa-
ter. 2010. Inducing tree-substitution grammars.
Journal of Machine Learning Research, 11:3053?
3096.
Michael Collins and Nigel Duffy. 2001. Convolu-
tion Kernels for Natural Language. In Thomas G.
Dietterich, Suzanna Becker, and Zoubin Ghahra-
mani, editors, NIPS, pages 625?632. MIT Press.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels
over discrete structures, and the voted percep-
tron. In Proceedings of 40th Annual Meeting of
the ACL, pages 263?270. Association for Compu-
tational Linguistics, Philadelphia, Pennsylvania,
USA.
Michael J. Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D. the-
sis, University of Pennsylvania.
Joshua Goodman. 1996. Efficient algorithms for
parsing the DOP model. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 143?152.
Joshua Goodman. 2003. Efficient parsing of DOP
with PCFG-reductions. In Bod et al (2003).
Joshua T. Goodman. 1998. Parsing inside-out.
Ph.D. thesis, Harvard University, Cambridge,
MA, USA.
Ray Jackendoff. 2002. Foundations of Language.
Oxford University Press, Oxford, UK.
Mark Johnson. 2002. The dop estimation method is
biased and inconsistent. Computational Linguis-
tics, 28:71?76.
Mark Johnson, Thomas L. Griffiths, and Sharon
Goldwater. 2007. Adaptor grammars: A frame-
work for specifying compositional nonparametric
bayesian models. In Advances in Neural Informa-
tion Processing Systems, volume 16, pages 641?
648.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on ACL,
pages 423?430. Association for Computational
Linguistics, Morristown, NJ, USA.
K. Lari and S. J. Young. 1990. The estimation
of stochastic context-free grammars using the
inside-outside algorithm. Computer Speech and
Language, 4:35?56.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a Large An-
notated Corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313?330.
94
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi
Tsujii. 2005. Probabilistic cfg with latent anno-
tations. In ACL ?05: Proceedings of the 43rd
Annual Meeting on ACL, pages 75?82. Associa-
tion for Computational Linguistics, Morristown,
NJ, USA.
Alessandro Moschitti. 2006. Efficient Convolution
Kernels for Dependency and Constituent Syntac-
tic Trees. In ECML, pages 318?329. Machine
Learning: ECML 2006, 17th European Confer-
ence on Machine Learning, Proceedings, Berlin,
Germany.
Timothy J. O?Donnell, Noah D. Goodman, and
Joshua B. Tenenbaum. 2009. Fragment Gram-
mars: Exploring Computation and Reuse in Lan-
guage. Technical Report MIT-CSAIL-TR-2009-
013, MIT.
Slav Petrov. 2009. Coarse-to-Fine Natural Lan-
guage Processing. Ph.D. thesis, University of
California at Bekeley, Berkeley, CA, USA.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, compact,
and interpretable tree annotation. In ACL-44:
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th an-
nual meeting of the ACL, pages 433?440. Associ-
ation for Computational Linguistics, Morristown,
NJ, USA.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Lan-
guage Technologies 2007: The Conference of the
North American Chapter of the ACL; Proceedings
of the Main Conference, pages 404?411. Asso-
ciation for Computational Linguistics, Rochester,
New York.
Matt Post and Daniel Gildea. 2009. Bayesian learn-
ing of a tree substitution grammar. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short
Papers, pages 45?48. Association for Computa-
tional Linguistics, Suntec, Singapore.
Federico Sangati and Willem Zuidema. 2009. Unsu-
pervised Methods for Head Assignments. In Pro-
ceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 701?
709. Association for Computational Linguistics,
Athens, Greece.
Federico Sangati, Willem Zuidema, and Rens Bod.
2010. Efficiently extract recurring tree fragments
from large treebanks. In Proceedings of the
Seventh conference on International Language
Resources and Evaluation (LREC?10). European
Language Resources Association (ELRA), Val-
letta, Malta.
Remko Scha. 1990. Taaltheorie en taaltechnolo-
gie: competence en performance. In Q. A. M.
de Kort and G. L. J. Leerdam, editors, Com-
putertoepassingen in de Neerlandistiek, LVVN-
jaarboek, pages 7?22. Landelijke Vereniging van
Neerlandici, Almere. [Language theory and
language technology: Competence and Perfor-
mance] in Dutch.
Khalil Sima?an. 1996. Computational complexity
of probabilistic disambiguation by means of tree-
grammars. In Proceedings of the 16th conference
on Computational linguistics, pages 1175?1180.
Association for Computational Linguistics, Mor-
ristown, NJ, USA.
Khalil Sima?an. 1999. Learning Efficient Disam-
biguation. Ph.D. thesis, Utrecht University and
University of Amsterdam.
Khalil Sima?an. 2003. On maximizing metrics for
syntactic disambiguation. In Proceedings of the
International Workshop on Parsing Technologies
(IWPT?03).
Elif Yamangil and Stuart M. Shieber. 2010.
Bayesian synchronous tree-substitution grammar
induction and its application to sentence compres-
sion. In Proceedings of the 48th Annual Meeting
of the ACL, ACL ?10, pages 937?947. Associa-
tion for Computational Linguistics, Stroudsburg,
PA, USA.
Andreas Zollmann and Khalil Sima?an. 2005.
A consistent and efficient estimator for data-
oriented parsing. Journal of Automata, Lan-
guages and Combinatorics, 10(2/3):367?388.
Willem Zuidema. 2007. Parsimonious Data-
Oriented Parsing. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL),
pages 551?560. Association for Computational
Linguistics, Prague, Czech Republic.
95
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 729?739,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
The Inside-Outside Recursive Neural Network model for
Dependency Parsing
Phong Le and Willem Zuidema
Institute for Logic, Language, and Computation
University of Amsterdam, the Netherlands
{p.le,zuidema}@uva.nl
Abstract
We propose the first implementation of
an infinite-order generative dependency
model. The model is based on a new
recursive neural network architecture, the
Inside-Outside Recursive Neural Network.
This architecture allows information to
flow not only bottom-up, as in traditional
recursive neural networks, but also top-
down. This is achieved by computing
content as well as context representations
for any constituent, and letting these rep-
resentations interact. Experimental re-
sults on the English section of the Uni-
versal Dependency Treebank show that
the infinite-order model achieves a per-
plexity seven times lower than the tradi-
tional third-order model using counting,
and tends to choose more accurate parses
in k-best lists. In addition, reranking with
this model achieves state-of-the-art unla-
belled attachment scores and unlabelled
exact match scores.
1 Introduction
Estimating probability distributions is the core is-
sue in modern, data-driven natural language pro-
cessing methods. Because of the traditional defi-
nition of discrete probability
Pr(A) ?
the number of times A occurs
the size of event space
counting has become a standard method to tackle
the problem. When data are sparse, smoothing
techniques are needed to adjust counts for non-
observed or rare events. However, successful use
of those techniques has turned out to be an art. For
instance, much skill and expertise is required to
create reasonable reduction lists for back-off, and
to avoid impractically large count tables, which
store events and their counts.
An alternative to counting for estimating prob-
ability distributions is to use neural networks.
Thanks to recent advances in deep learning, this
approach has recently started to look very promis-
ing again, with state-of-the-art results in senti-
ment analysis (Socher et al., 2013), language mod-
elling (Mikolov et al., 2010), and other tasks. The
Mikolov et al. (2010) work, in particular, demon-
strates the advantage of neural-network-based ap-
proaches over counting-based approaches in lan-
guage modelling: it shows that recurrent neu-
ral networks are capable of capturing long histo-
ries efficiently and surpass standard n-gram tech-
niques (e.g., Kneser-Ney smoothed 5-gram).
In this paper, keeping in mind the success of
these models, we compare the two approaches.
Complementing recent work that focused on such
a comparison for the case of finding appropriate
word vectors (Baroni et al., 2014), we focus here
on models that involve more complex, hierarchical
structures. Starting with existing generative mod-
els that use counting to estimate probability distri-
butions over constituency and dependency parses
(e.g., Eisner (1996b), Collins (2003)), we develop
an alternative based on recursive neural networks.
This is a non-trivial task because, to our knowl-
edge, no existing neural network architecture can
be used in this way. For instance, classic recur-
rent neural networks (Elman, 1990) unfold to left-
branching trees, and are not able to process ar-
bitrarily shaped parse trees that the counting ap-
proaches are applied to. Recursive neural net-
works (Socher et al., 2010) and extensions (Socher
et al., 2012; Le et al., 2013), on the other hand,
do work with trees of arbitrary shape, but pro-
cess them in a bottom-up manner. The probabil-
ities we need to estimate are, in contrast, defined
by top-down generative models, or by models that
require information flows in both directions (e.g.,
the probability of generating a node depends on
the whole fragment rooted at its just-generated sis-
729
Figure 1: Inner (i
p
) and outer (o
p
) representations
at the node that covers constituent p. They are vec-
torial representations of p?s content and context,
respectively.
ter).
To tackle this problem, we propose a new ar-
chitecture: the Inside-Outside Recursive Neural
Network (IORNN) in which information can flow
not only bottom-up but also top-down, inward and
outward. The crucial innovation in our architec-
ture is that every node in a hierarchical structure
is associated with two vectors: one vector, the in-
ner representation, representing the content under
that node, and another vector, the outer represen-
tation, representing its context (see Figure 1). In-
ner representations can be computed bottom-up;
outer representations, in turn, can be computed
top-down. This allows information to flow in
any direction, depending on the application, and
makes the IORNN a natural tool for estimating
probabilities in tree-based generative models.
We demonstrate the use of the IORNN by ap-
plying it to an ?-order generative dependency
model which is impractical for counting due to
the problem of data sparsity. Counting, instead, is
used to estimate a third-order generative model as
in Sangati et al. (2009) and Hayashi et al. (2011).
Our experimental results show that our new model
not only achieves a seven times lower perplex-
ity than the third-order model, but also tends to
choose more accurate candidates in k-best lists. In
addition, reranking with this model achieves state-
of-the-art scores on the task of supervised depen-
dency parsing.
The outline of the paper is following. Firstly, we
give an introduction to Eisner?s generative model
in Section 2. Then, we present the third-order
model using counting in Section 3, and propose
the IORNN in Section 4. Finally, in Section 5 we
show our experimental results.
2 Eisner?s Generative Model
Eisner (1996b) proposed a generative model for
dependency parsing. The generation process is
top-down: starting at the ROOT, it generates
left dependents and then right dependents for the
ROOT. After that, it generates left dependents and
right dependents for each of ROOT?s dependents.
The process recursively continues until there is no
further dependent to generate. The whole process
is captured in the following formula
P (T (H)) =
L
?
l=1
P
(
H
L
l
|C
H
L
l
)
P
(
T (H
L
l
)
)
?
R
?
r=1
P
(
H
R
r
|C
H
R
r
)
P
(
T (H
R
r
)
)
(1)
whereH is the current head, T (N) is the fragment
of the dependency parse rooted in N , and C
N
is
the context in which N is generated. H
L
, H
R
are
respectively H?s left dependents and right depen-
dents, plus EOC (End-Of-Children), a special to-
ken to indicate that there are no more dependents
to generate. Thus, P (T (ROOT )) is the proba-
bility of generating the entire dependency struc-
ture T . We refer to ?H
L
l
, C
H
L
l
?, ?H
R
r
, C
H
R
r
? as
?events?, and ?C
H
L
l
?, ?C
H
R
r
? as ?conditioning con-
texts?.
In order to avoid the problem of data sparsity,
the conditioning context in which a dependent D
is generated should capture only part of the frag-
ment generated so far. Based on the amount of
information that contexts hold, we can define the
order of a generative model (see Hayashi et al.
(2011, Table 3) for examples)
? first-order: C
1
D
contains the head H ,
? second-order: C
2
D
contains H and the just-
generated sibling S,
? third-order: C
3
D
contains H , S, the sibling S
?
before S (tri-sibling); or H , S and the grand-
head G (the head of H) (grandsibling) (the
fragment enclosed in the blue doted contour
in Figure 2),
? ?-order: C
?
D
contains all of D?s ancestors,
theirs siblings, and its generated siblings (the
fragment enclosed in the red dashed contour
in Figure 2).
In the original models (Eisner, 1996a), each de-
pendent D is a 4-tuple ?dist, w, c, t?
? dist(H,D) the distance between D and its
headH , represented as one of the four ranges
1, 2, 3-6, 7-?.
730
Figure 2: Example of different orders of context of ?diversified?. The blue dotted shape corresponds
to the third-order outward context, while the red dashed shape corresponds to the?-order left-to-right
context. The green dot-dashed shape corresponds to the context to compute the outer representation.
? word(D) the lowercase version of the word
of D,
? cap(D) the capitalisation feature of the word
of D (all letters are lowercase, all letters are
uppercase, the first letter is uppercase, the
first letter is lowercase),
? tag(D) the POS-tag of D,
Here, to make the dependency complete,
deprel(D), the dependency relation of D (e.g.,
SBJ, DEP), is also taken into account.
3 Third-order Model with Counting
The third-order model we suggest is similar to
the grandsibling model proposed by Sangati et
al. (2009) and Hayashi et al. (2011). It defines
the probability of generating a dependent D =
?dist, d, w, c, t? as the product of the distance-
based probability and the probabilities of gener-
ating each of its components (d, t, w, c, denoting
dependency relation, POS-tag, word and capitali-
sation feature, respectively). Each of these prob-
abilities is smoothed using back-off according to
the given reduction lists (as explained below).
P (D|C
D
)
= P (dist(H,D), dwct(D)|H,S,G, dir)
= P (d(D)|H,S,G, dir)
reduction list:
tw(H), tw(S), tw(G), dir
tw(H), tw(S), t(G), dir{
tw(H), t(S), t(G), dir
t(H), tw(S), t(G), dir
t(H), t(S), t(G), dir
? P (t(D)|d(D), H, S,G, dir)
reduction list:
d(D), dtw(H), t(S), dir
d(D), d(H), t(S), dir
d(D), d(D), dir
? P (w(D)|dt(D), H, S,G, dir)
reduction list:
dtw(H), t(S), dir
dt(H), t(S), dir
? P (c(D)|dtw(D), H, S,G, dir)
reduction list:
tw(D), d(H), dir
tw(D), dir
? P (dist(H,D)|dtwc(D), H, S,G, dir) (2)
reduction list:
dtw(D), dt(H), t(S), dir
dt(D), dt(H), t(S), dir
The reason for generating the dependency rela-
tion first is based on the similarity between rela-
tion/dependent and role/filler: we generate a role
and then choose a filler for that role.
Back-off The back-off parameters are identi-
cal to Eisner (1996b). To estimate the proba-
bility P (A|context) given a reduction list L =
(l
1
, l
2
, ..., l
n
) of context, let
p
i
=
{
count(A,l
i
)+0.005
count(l
i
)+0.5
if i = n
count(A,l
i
)+3p
i+1
count(l
i
)+3
otherwise
then P (A|context) = p
1
.
4 The Inside-Outside Recursive Neural
Network
In this section, we first describe the Recur-
sive Neural Network architecture of Socher et
al. (2010) and then propose an extension we
call the Inside-Outside Recursive Neural Network
(IORNN). The IORNN is a general architecture
for trees, which works with tree-based genera-
tive models including those employed by Eisner
(1996b) and Collins (2003). We then explain how
to apply the IORNN to the?-order model. Note
that for the present paper we are only concerned
with the problem of computing the probability of
731
Figure 3: Recursive Neural Network (RNN).
a tree; we assume an independently given parser is
available to assign a syntactic structure, or multi-
ple candidate structures, to an input string.
4.1 Recursive Neural Network
The architecture we propose can best be under-
stood as an extension of the Recursive Neural Net-
works (RNNs) proposed by Socher et al. (2010),
that we mentioned above. In order to see how
an RNN works, consider the following example.
Assume that there is a constituent with parse tree
(p
2
(p
1
x y) z) (Figure 3), and that x,y, z ? R
n
are the (inner) representations of the three words
x, y and z, respectively. We use a neural network
which consists of a weight matrix W
1
? R
n?n
for
left children and a weight matrix W
2
? R
n?n
for
right children to compute the vector for a parent
node in a bottom up manner. Thus, we compute
p
1
as follows
p
1
= f(W
1
x + W
2
y + b)
where b is a bias vector and f is an activation
function (e.g., tanh or logistic). Having computed
p
1
, we can then move one level up in the hierarchy
and compute p
2
:
p
2
= f(W
1
p
1
+ W
2
z + b)
This process is continued until we reach the root
node. The RNN thus computes a single vector
for each node p in the tree, representing the con-
tent under that node. It has in common with log-
ical semantics that representations for compounds
(here xyz) are computed by recursively applying a
composition function to meaning representations
of the parts. It is difficult to characterise the ex-
pressivity of the resulting system in logical terms,
but recent work suggests it is surprisingly power-
ful (e.g., Kanerva (2009)).
Figure 4: Inside-Outside Recursive Neural Net-
work (IORNN). Black rectangles correspond to in-
ner representations, white rectangles correspond
to outer representations.
4.2 IORNN
We extend the RNN-architecture by adding a sec-
ond vector to each node, representing the context
of the node, shown as white rectangles in figure 4.
The job of this second vector, the outer represen-
tation, is to summarize all information about the
context of node p so that we can either predict its
content (i.e., predict an inner representation), or
pass on this information to the daughters of p (i.e.,
compute outer representations of these daughters).
Outer representations thus allow information to
flow top-down.
We explain the operation of the resulting Inside-
Outside Recursive Neural Network in terms of the
same example parse tree (p
2
(p
1
x y) z) (see Fig-
ure 4). Each node u in the syntactic tree carries
two vectors o
u
and i
u
, the outer representation and
inner representation of the constituent that is cov-
ered by the node.
Computing inner representations Inner repre-
sentations are computed from the bottom up. We
assume for every word w an inner representation
i
w
? R
n
. The inner representation of a non-
terminal node, say p
1
, is given by
i
p
1
= f(W
i
1
i
x
+ W
i
2
i
y
+ b
i
)
where W
i
1
,W
i
2
are n ? n real matrices, b
i
is a
bias vector, and f is an activation function, e.g.
tanh. (This is the same as the computation of
non-terminal vectors in the RNNs.) The inner rep-
resentation of a parent node is thus a function of
the inner representations of its children.
Computing outer representations Outer repre-
sentations are computed from the top down. For a
node which is not the root, say p
1
, the outer repre-
732
sentation is given by
o
p
1
= g(W
o
1
o
p
2
+ W
o
2
i
z
+ b
o
)
where W
o
1
,W
o
2
are n ? n real matrices, b
o
is a
bias vector, and g is an activation function. The
outer representation of a node is thus a function of
the outer representation of its parent and the inner
representation of its sisters.
If there is information about the external context
of the utterance that is being processed, this infor-
mation determines the outer representation of the
root node o
root
. In our first experiments reported
here, no such information was assumed to be avail-
able. In this case, a random value o
?
is chosen at
initialisation and assigned to the root nodes of all
utterances; this value is then adjusted by the learn-
ing process discussed below.
Training Training the IORNN is to minimise an
objective function J(?) which depends on the pur-
pose of usage where ? is the set of parameters. To
do so, we compute the gradient ?J/?? and ap-
ply the gradient descent method. The gradient is
effectively computed thanks to back-propagation
through structure (Goller and K?uchler, 1996). Fol-
lowing Socher et al. (2013), we use AdaGrad
(Duchi et al., 2011) to update the parameters.
4.3 The?-order Model with IORNN
The RNN and IORNN are defined for context-
free trees. To apply the IORNN architecture to
dependency parses we need to adapt the defini-
tions somewhat. In particular, in the generative
dependency model, every step in the generative
story involves the decision to generate a specific
word while the span of the subtree that this word
will dominate only becomes clear when all depen-
dents are generated. We therefore introduce par-
tial outer representation as a representation of the
current context of a word in the generative pro-
cess, and compute the final outer representation
only when all its siblings have been generated.
Consider an example of head h and its depen-
dents x, y (we ignore directions for simplicity) in
Figure 5. Assume that we are in the state in the
generative process where the generation of h is
complete, i.e. we know its inner and outer rep-
resentations i
h
and o
h
. Now, when generating h?s
first dependent x (see Figure 5-a), we first com-
pute x?s partial outer representation (representing
its context at this stage in the process), which is
a function of the outer representation of the head
(representing the head?s context) and the inner rep-
resentation of the head (representing the content of
the head word):
?
o
1
= f(W
hi
i
h
+ W
ho
o
h
+ b
o
)
where W
hi
,W
ho
are n ? n real matrices, b
o
is a
bias vector, f is an activation function.
With the context of the first dependent deter-
mined, we can proceed and generate its content.
For this purpose, we assume a separate weight ma-
trix W, trained (as explained below) to predict a
specific word given a (partial) outer representa-
tion. To compute a proper probability for word
x, we use the softmax function:
softmax(x,
?
o
1
) =
e
u(x,
?
o
1
)
?
w?V
e
u(w,
?
o
1
)
where
[
u(w
1
,
?
o
1
), ..., u(w
|V |
,
?
o
1
)
]
T
= W
?
o
1
+ b
and V is the set of all possible dependents.
Note that since o
h
, the outer representation of
h, represents the entire dependency structure gen-
erated up to that point,
?
o
1
is a vectorial represen-
tation of the ?-order context generating the first
dependent (like the fragment enclosed in the red
dashed contour in Figure 2). The softmax func-
tion thus estimates the probability P (D = x|C
?
D
).
The next step, now that x is generated, is to
compute the partial outer representation for the
second dependent (see Figure 5-b)
?
o
2
= f(W
hi
i
h
+ W
ho
o
h
+ W
dr(x)
i
x
+ b
o
)
where W
dr(x)
is a n ? n real matrix specific for
the dependency relation of x with h.
Next y is generated (using the softmax function
above), and the partial outer representation for the
third dependent (see Figure 5-c) is computed:
?
o
3
= f(W
hi
i
h
+ W
ho
o
h
+
1
2
(
W
dr(x)
i
x
+ W
dr(y)
i
y
)
+ b
o
)
Since the third dependent is the End-of-
Children symbol (EOC), the process of generat-
ing dependents for h stops. We can then return
to x and y to replace the partial outer represen-
tations with complete outer representations
1
(see
1
According to the IORNN architecture, to compute the
outer representation of a node, the inner representations of
the whole fragments rooting at its sisters must be taken into
account. Here, we replace the inner representation of a frag-
ment by the inner representation of its root since the meaning
of a phrase is often dominated by the meaning of its head.
733
Figure 5: Example of applying IORNN to dependency parsing. Black, grey, white boxes are respectively
inner, partial outer, and outer representations. For simplicity, only links related to the current computation
are drawn (see text).
Figure 5-d,e):
o
x
= f(W
hi
i
h
+ W
ho
o
h
+ W
dr(y)
i
y
+ b
o
)
o
y
= f(W
hi
i
h
+ W
ho
o
h
+ W
dr(x)
i
x
+ b
o
)
In general, if u is the first dependent of h then
?
o
u
= f(W
hi
i
h
+ W
ho
o
h
+ b
o
)
otherwise
?
o
u
= f(W
hi
i
h
+ W
ho
o
h
+ b
o
+
1
|
?
S(u)|
?
v?
?
S(u)
W
dr(v)
i
v
)
where
?
S(u) is the set of u?s sisters generated be-
fore it. And, if u is the only dependent of h (ig-
noring EOC) then
o
u
= f(W
hi
i
h
+ W
ho
o
h
+ b
o
)
otherwise
o
u
= f(W
hi
i
h
+ W
ho
o
h
+ b
o
+
1
|S(u)|
?
v?S(u)
W
dr(v)
i
v
)
where S(u) is the set of u?s sisters.
We then continue this process to generate de-
pendents for x and y until the process stops.
Inner Representations In the calculation of the
probability of generating a word, described above,
we assumed inner representations of all possible
words to be given. These are, in fact, themselves a
function of vector representations for the words (in
our case, the word vectors are initially borrowed
from Collobert et al. (2011)), the POS-tags and
capitalisation features. That is, the inner represen-
tation at a node h is given by:
i
h
= f (W
w
w
h
+ W
p
p
h
+ W
c
c
h
)
where W
w
? R
n?d
w
, W
p
? R
n?d
p
, W
c
?
R
n?d
c
, w
h
is the word vector of h, and p
h
, c
h
are
respectively binary vectors representing the POS-
tag and capitalisation feature of h.
Training Training this IORNN is to minimise
the following objective function which is the reg-
ularised cross-entropy
J(?) =?
1
m
?
T?D
?
w?T
log(P (w|
?
o
w
))
+
1
2
(
?
W
??
W
?
2
+ ?
L
??
L
?
2
)
where D is the set of training dependency parses,
m is the number of dependents; ?
W
, ?
L
are
the weight matrix set and the word embeddings
(? = (?
W
, ?
L
)); ?
W
, ?
L
are regularisation hyper-
parameters.
Implementation We decompose a dependent D
into four features: dependency relation, POS-tag,
lowercase version of word, capitalisation feature
of word. We then factorise P (D|C
?
D
) similarly to
Section 3, where each component is estimated by
a softmax function.
5 Experiments
In our experiments, we convert the Penn Treebank
to dependencies using the Universal dependency
annotation (McDonald et al., 2013)
2
; this yields
a dependency tree corpus we label PTB-U. In or-
der to compare with other systems, we also ex-
periment with an alternative conversion using the
head rules of Yamada and Matsumoto (2003)
3
;
this yields a dependency tree corpus we label PTB-
YM. Sections 2-21 are used for training, section
22 for development, and section 23 for testing. For
the PTB-U, the gold POS-tags are used. For the
PTB-YM, the development and test sets are tagged
by the Stanford POS-tagger
4
trained on the whole
2
https://code.google.com/p/uni-dep-tb/
3
http://stp.lingfil.uu.se/
?
nivre/
research/Penn2Malt.html
4
http://nlp.stanford.edu/software/
tagger.shtml
734
Perplexity
3rd-order model 1736.73
?-order model 236.58
Table 1: Perplexities of the two models on PTB-
U-22.
training data, whereas 10-way jackknifing is used
to generate tags for the training set.
The vocabulary for both models, the third-order
model and the ?-order model, is taken as a list
of words occurring more than two times in the
training data. All other words are labelled ?UN-
KNOWN? and every digit is replaced by ?0?. For
the IORNN used by the ?-order model, we set
n = 200, and define f as the tanh activation func-
tion. We initialise it with the 50-dim word embed-
dings from Collobert et al. (2011) and train it with
the learning rate 0.1, ?
W
= 10
?4
, ?
L
= 10
?10
.
5.1 Perplexity
We firstly evaluate the two models on PTB-U-22
using the perplexity-per-word metric
ppl(P ) = 2
?
1
N
?
T?D
log
2
P (T )
where D is a set of dependency parses, N is the
total number of words. It is worth noting that,
the better P estimates the true distribution P
?
of
D, the lower its perplexity is. Because Eisner?s
model with the dist(H,D) feature (Equation 2)
is leaky (the model allocates some probability to
events that can never legally arise), this feature is
discarded (only in this experiment).
Table 1 shows results. The perplexity of the
third-order model is more than seven times higher
than the?-order model. This reflects the fact that
data sparsity is more problematic for counting than
for the IORNN.
To investigate why the perplexity of the third-
order model is so high, we compute the percent-
ages of events extracted from the development
set appearing more than twice in the training set.
Events are grouped according to the reduction lists
in Equation 2 (see Table 2). We can see that re-
ductions at level 0 (the finest) for dependency re-
lations and words seriously suffer from data spar-
sity: more than half of the events occur less than
three times, or not at all, in the training data. We
thus conclude that counting-based models heavily
rely on carefully designed reduction lists for back-
off.
back-off level d t w c
0 47.4 61.6 43.7 87.7
1 69.8 98.4 77.8 97.3
2 76.0, 89.5 99.7
3 97.9
total 76.1 86.6 60.7 92.5
Table 2: Percentages of events extracted from
PTB-U-22 appearing more than twice in the train-
ing set. Events are grouped according to the reduc-
tion lists in Equation 2. d, t, w, c stand for depen-
dency relation, POS-tag, word, and capitalisation
feature.
5.2 Reranking
In the second experiment, we evaluate the two
models in the reranking framework proposed by
Sangati et al. (2009) on PTB-U. We used the MST-
Parser (with the 2nd-order feature mode) (McDon-
ald et al., 2005) to generate k-best lists. Two
evaluation metrics are labelled attachment score
(LAS) and unlabelled attachment score (UAS), in-
cluding punctuation.
Rerankers Given D(S), a k-best list of parses
of a sentence S, we define the generative reranker
T
?
= arg max
T?D(S)
P (T (ROOT ))
which is identical to Sangati et al. (2009).
Moreover, as in many mixture-model-based ap-
proaches, we define the mixture reranker as a com-
bination of the generative model and the MST dis-
criminative model (Hayashi et al., 2011)
T
?
= arg max
T?D(S)
? logP (T (ROOT ))+(1??)s(S, T )
where s(S, T ) is the score given by the MST-
Parser, and ? ? [0, 1].
Results Figure 6 shows UASs of the generative
reranker on the development set. The MSTParser
achieves 92.32% and the Oracle achieve 96.23%
when k = 10. With the third-order model, the
generative reranker performs better than the MST-
Parser when k < 6 and the maximum improve-
ment is 0.17%. Meanwhile, with the ?-order
model, the generative reranker always gains higher
UASs than the MSTParser, and with k = 6, the
difference reaches 0.7%. Figure 7 shows UASs of
the mixture reranker on the same set. ? is opti-
mised by searching with the step-size 0.005. Un-
surprisingly, we observe improvements over the
735
Figure 6: Performance of the generative reranker
on PTB-U-22.
Figure 7: Performance of the mixture reranker on
PTB-U-22. For each k, ? was optimized with the
step-size 0.005.
LAS UAS
MSTParser 89.97 91.99
Oracle (k = 10) 93.73 96.24
Generative reranker with
3rd-order (k = 3) 90.27 (+0.30) 92.27 (+0.28)
?-order (k = 6) 90.76 (+0.79) 92.83 (+0.84)
Mixture reranker with
3rd-order (k = 6) 90.62 (+0.65) 92.62 (+0.63)
?-order (k = 9) 91.02 (+1.05) 93.08 (+1.09)
Table 3: Comparison based on reranking on PTB-
U-23. The numbers in the brackets are improve-
ments over the MSTParser.
generative reranker as the mixture reranker can
combine the advantages of the two models.
Table 3 shows scores of the two rerankers on the
test set with the parameters tuned on the develop-
ment set. Both the rerankers, either using third-
order or ?-order models, outperform the MST-
Parser. The fact that both gain higher improve-
ments with the ?-order model suggests that the
IORNN surpasses counting.
Figure 9: F1-scores of binned HEAD distance
(PTB-U-23).
5.3 Comparison with other systems
We first compare the mixture reranker using the
?-order model against the state-of-the-art depen-
dency parser TurboParser (with the full mode)
(Martins et al., 2013) on PTB-U-23. Table 4 shows
LASs and UASs. When taking labels into account,
the TurboParser outperforms the reranker. But
without counting labels, the two systems perform
comparably, and when ignoring punctuation the
reranker even outperforms the TurboParser. This
pattern is also observed when the exact match met-
rics are used (see Table 4). This is due to the fact
that the TurboParser performs significantly better
than the MSTParser, which generates k-best lists
for the reranker, in labelling: the former achieves
96.03% label accuracy score whereas the latter
achieves 94.92%.
One remarkable point is that reranking with
the ?-order model helps to improve the exact
match scores 4% - 6.4% (see Table 4). Because
the exact match scores correlate with the ability
to handle global structures, we conclude that the
IORNN is able to capture?-order contexts. Fig-
ure 8 shows distributions of correct-head accuracy
over CPOS-tags and Figure 9 shows F1-scores of
binned HEAD distance. Reranking with the ?-
order model is clearly helpful for all CPOS-tags
and dependent-to-head distances, except a minor
decrease on PRT.
We compare the reranker against other systems
on PTB-YM-23 using the UAS metric ignoring
punctuation (as the standard evaluation for En-
glish) (see Table 5). Our system performs slightly
better than many state-of-the-art systems such as
Martins et al. (2013) (a.k.a. TurboParser), Zhang
and McDonald (2012), Koo and Collins (2010).
It outperforms Hayashi et al. (2011) which is a
reranker using a combination of third-order gen-
erative models with a variational model learnt
736
LAS (w/o punc) UAS (w/o punc) LEM (w/o punc) UEM (w/o punc)
MSTParser 89.97 (90.54) 91.99 (92.82) 32.37 (34.19) 42.80 (45.24)
w. ?-order (k = 9) 91.02 (91.51) 93.08 (93.84) 37.58 (39.16) 49.17 (51.53)
TurboParser 91.56 (92.02) 93.05 (93.70) 40.65 (41.72) 48.05 (49.83)
Table 4: Comparison with the TurboParser on PTB-U-23. LEM and UEM are respectively the labelled
exact match score and unlabelled exact match score metrics. The numbers in brackets are scores com-
puted excluding punctuation.
Figure 8: Distributions of correct-head accuracy over CPOS-tags (PTB-U-23).
System UAS
Huang and Sagae (2010) 92.1
Koo and Collins (2010) 93.04
Zhang and McDonald (2012) 93.06
Martins et al. (2013) 93.07
Bohnet and Kuhn (2012) 93.39
Reranking
Hayashi et al. (2011) 92.89
Hayashi et al. (2013) 93.12
MST+?-order (k = 12) 93.12
Table 5: Comparison with other systems on PTB-
YM-23 (excluding punctuation).
on the fly; performs equally with Hayashi et al.
(2013) which is a discriminative reranker using the
stacked technique; and slightly worse than Bohnet
and Kuhn (2012), who develop a hybrid transition-
based and graphical-based approach.
6 Related Work
Using neural networks to process trees was first
proposed by Pollack (1990) in the Recursive Au-
toassociative Memory model which was used for
unsupervised learning. Socher et al. (2010) later
introduced the Recursive Neural Network archi-
tecture for supervised learning tasks such as syn-
tactic parsing and sentiment analysis (Socher et
al., 2013). Our IORNN is an extension of
the RNN: the former can process trees not only
bottom-up like the latter but also top-down.
Elman (1990) invented the simple recurrent
neural network (SRNN) architecture which is ca-
pable of capturing very long histories. Mikolov
et al. (2010) then applied it to language mod-
elling and gained state-of-the-art results, outper-
forming the the standard n-gram techniques such
as Kneser-Ney smoothed 5-gram. Our IORNN
architecture for dependency parsing bears a re-
semblance to the SRNN in the sense that it can
also capture long ?histories? in context represen-
tations (i.e., outer representations in our terminol-
ogy). Moreover, the IORNN can be seen as a gen-
eralization of the SRNN since a left-branching tree
is equivalent to a chain and vice versa.
The idea of letting parsing decisions depend
on arbitrarily long derivation histories is also ex-
plored in Borensztajn and Zuidema (2011) and
is related to parsing frameworks that allow arbi-
trarily large elementary trees (e.g., Scha (1990),
O?Donnell et al. (2009), Sangati and Zuidema
(2011), and van Cranenburgh and Bod (2013)).
Titov and Henderson (2007) were the first
proposing to use deep networks for dependency
parsing. They introduced a transition-based gen-
erative dependency model using incremental sig-
moid belief networks and applied beam pruning
for searching best trees. Differing from them,
our work uses the IORNN architecture to rescore
k-best candidates generated by an independent
737
graph-based parser, namely the MSTParser.
Reranking k-best lists was introduced by
Collins and Koo (2005) and Charniak and Johnson
(2005). Their rerankers are discriminative and for
constituent parsing. Sangati et al. (2009) proposed
to use a third-order generative model for reranking
k-best lists of dependency parses. Hayashi et al.
(2011) then followed this idea but combined gen-
erative models with a variational model learnt on
the fly to rerank forests. In this paper, we also
followed Sangati et al. (2009)?s idea but used an
?-order generative model, which has never been
used before.
7 Conclusion
In this paper, we proposed a new neural network
architecture, the Inside-Outside Recursive Neural
Network, that can process trees both bottom-up
and top-down. The key idea is to extend the RNN
such that every node in the tree has two vectors
associated with it: an inner representation for its
content, and an outer representation for its context.
Inner and outer representations of any constituent
can be computed simultaneously and interact with
each other. This way, information can flow top-
down, bottom-up, inward and outward. Thanks to
this property, by applying the IORNN to depen-
dency parses, we have shown that using an ?-
order generative model for dependency parsing,
which has never been done before, is practical.
Our experimental results on the English section
of the Universal Dependency Treebanks show that
the ?-order generative model approximates the
true dependency distribution better than the tradi-
tional third-order model using counting, and tends
to choose more accurate parses in k-best lists.
In addition, reranking with this model even out-
performs the state-of-the-art TurboParser on unla-
belled score metrics.
Our source code is available at: github.
com/lephong/iornn-depparse.
Acknowledgments
We thank Remko Scha and three anonymous re-
viewers for helpful comments.
References
Marco Baroni, Georgiana Dinu, and Germ?an
Kruszewski. 2014. Don?t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, volume 1.
Bernd Bohnet and Jonas Kuhn. 2012. The best of
both worlds: a graph-based completion model for
transition-based parsers. In Proceedings of the 13th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 77?87.
Association for Computational Linguistics.
Gideon Borensztajn and Willem Zuidema. 2011.
Episodic grammar: a computational model of the
interaction between episodic and semantic memory
in language processing. In Proceedings of the 33d
Annual Conference of the Cognitive Science Soci-
ety (CogSci?11), pages 507?512. Lawrence Erlbaum
Associates.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 173?180.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25?66.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational lin-
guistics, 29(4):589?637.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, pages 2121?2159.
Jason M. Eisner. 1996a. An empirical comparison of
probability models for dependency grammar. Tech-
nical report, University of Pennsylvania Institute for
Research in Cognitive Science.
Jason M Eisner. 1996b. Three new probabilistic mod-
els for dependency parsing: An exploration. In Pro-
ceedings of the 16th conference on Computational
linguistics-Volume 1, pages 340?345. Association
for Computational Linguistics.
Jeffrey L. Elman. 1990. Finding structure in time.
Cognitive science, 14(2):179?211.
Christoph Goller and Andreas K?uchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In International
Conference on Neural Networks. IEEE.
Katsuhiko Hayashi, Taro Watanabe, Masayuki Asa-
hara, and Yuji Matsumoto. 2011. Third-order
variational reranking on packed-shared dependency
738
forests. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1479?1488. Association for Computational
Linguistics.
Katsuhiko Hayashi, Shuhei Kondo, and Yuji Mat-
sumoto. 2013. Efficient stacked dependency pars-
ing by forest reranking. Transactions of the Associ-
ation for Computational Linguistics, 1(1):139?150.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1077?
1086. Association for Computational Linguistics.
Pentti Kanerva. 2009. Hyperdimensional comput-
ing: An introduction to computing in distributed rep-
resentation with high-dimensional random vectors.
Cognitive Computation, 1(2):139?159.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1?11. Association for
Computational Linguistics.
Phong Le, Willem Zuidema, and Remko Scha. 2013.
Learning from errors: Using vector-based composi-
tional semantics for parse reranking. In Proceedings
Workshop on Continuous Vector Space Models and
their Compositionality (at ACL 2013). Association
for Computational Linguistics.
Andre Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the turbo: Fast third-order non-
projective turbo parsers. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
617?622, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of
dependency parsers. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 91?98. Association for Computa-
tional Linguistics.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Os-
car T?ackstr?om, et al. 2013. Universal dependency
annotation for multilingual parsing. Proceedings of
ACL, Sofia, Bulgaria.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Timothy J O?Donnell, Noah D Goodman, and Joshua B
Tenenbaum. 2009. Fragment grammar: Exploring
reuse in hierarchical generative processes. Techni-
cal report, Technical Report MIT-CSAIL-TR-2009-
013, MIT.
Jordan B. Pollack. 1990. Recursive distributed repre-
sentations. Artificial Intelligence, 46(1):77105.
Federico Sangati and Willem Zuidema. 2011. Ac-
curate parsing with compact tree-substitution gram-
mars: Double-DOP. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMLNP?11), pages 84?95. Association
for Computational Linguistics.
Federico Sangati, Willem Zuidema, and Rens Bod.
2009. A generative re-ranking model for depen-
dency parsing. In Proceedings of the 11th Inter-
national Conference on Parsing Technologies, pages
238?241.
Remko Scha. 1990. Taaltheorie en taaltechnolo-
gie; competence en performance. In R. de Kort
and G.L.J. Leerdam, editors, Computertoepassin-
gen in de Neerlandistiek, pages 7?22. LVVN,
Almere, the Netherlands. English translation at
http://iaaa.nl/rs/LeerdamE.html.
Richard Socher, Christopher D. Manning, and An-
drew Y. Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, Seattle, Washington, USA, October.
Ivan Titov and James Henderson. 2007. A latent vari-
able model for generative dependency parsing. In
Proceedings of the 10th International Conference on
Parsing Technologies, pages 144?155.
Andreas van Cranenburgh and Rens Bod. 2013. Dis-
continuous parsing with an efficient and accurate
DOP model. In Proceedings of the International
Conference on Parsing Technologies (IWPT?13).
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of International Conference
on Parsing Technologies (IWPT), pages 195?206.
Hao Zhang and Ryan McDonald. 2012. Generalized
higher-order dependency parsing with cube prun-
ing. In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 320?331. Association for Computational Lin-
guistics.
739
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 11?19,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Learning from errors: Using vector-based compositional semantics for
parse reranking
Phong Le, Willem Zuidema, Remko Scha
Institute for Logic, Language, and Computation
University of Amsterdam, the Netherlands
{p.le,zuidema,scha}@uva.nl
Abstract
In this paper, we address the problem of
how to use semantics to improve syntac-
tic parsing, by using a hybrid reranking
method: a k-best list generated by a sym-
bolic parser is reranked based on parse-
correctness scores given by a composi-
tional, connectionist classifier. This classi-
fier uses a recursive neural network to con-
struct vector representations for phrases in
a candidate parse tree in order to classify
it as syntactically correct or not. Tested on
the WSJ23, our method achieved a statisti-
cally significant improvement of 0.20% on
F-score (2% error reduction) and 0.95% on
exact match, compared with the state-of-
the-art Berkeley parser. This result shows
that vector-based compositional semantics
can be usefully applied in syntactic pars-
ing, and demonstrates the benefits of com-
bining the symbolic and connectionist ap-
proaches.
1 Introduction
Following the idea of compositionality in formal
semantics, compositionality in vector-based se-
mantics is also based on the principle of composi-
tionality, which says that ?The meaning of a whole
is a function of the meanings of the parts and of
the way they are syntactically combined? (Partee,
1995). According to this principle, composing the
meaning of a phrase or sentence requires a syntac-
tic parse tree, which is, in most current systems,
given by a statistical parser. This parser, in turn, is
trained on syntactically annotated corpora.
However, there are good reasons to also con-
sider information flowing in the opposite direc-
tion: from semantics to syntactic parsing. Per-
formance of parsers trained and evaluated on the
Penn WSJ treebank has reached a plateau, as many
ambiguities cannot be resolved by syntactic infor-
mation alone. Further improvements in parsing
may depend on the use of additional sources of in-
formation, including semantics. In this paper, we
study the use of semantics for syntactic parsing.
The currently dominant approach to syntactic
parsing is based on extracting symbolic grammars
from a treebank and defining appropriate proba-
bility distributions over the parse trees that they
license (Charniak, 2000; Collins, 2003; Klein
and Manning, 2003; Petrov et al, 2006; Bod et
al., 2003; Sangati and Zuidema, 2011; van Cra-
nenburgh et al, 2011). An alternative approach,
with promising recent developments (Socher et
al., 2010; Collobert, 2011), is based on us-
ing neural networks. In the present paper, we
combine the ?symbolic? and ?connectionist? ap-
proaches through reranking: a symbolic parser
is used to generate a k-best list which is then
reranked based on parse-correctness scores given
by a connectionist compositional-semantics-based
classifier.
The idea of reranking is motivated by anal-
yses of the results of state-of-the-art symbolic
parsers such as the Brown and Berkeley parsers,
which have shown that there is still considerable
room for improvement: oracle results on 50-best
lists display a dramatic improvement in accuracy
(96.08% vs. 90.12% on F-score and 65.56% vs.
37.22% on exact match with the Berkeley parser).
This suggests that parsers that rely on syntactic
corpus-statistics, though not sufficient by them-
selves, may very well serve as a basis for sys-
tems that integrate other sources of information by
means of reranking.
One important complementary source of infor-
mation is the semantic plausibility of the con-
stituents of the syntactically viable parses. The ex-
ploitation of that kind of information is the topic
of the research we report here. In this work,
we follow up on a proposal by Mark Steedman
11
(1999), who suggested that the realm of seman-
tics lacks the clearcut hierarchical structures that
characterise syntax, and that semantic information
may therefore be profitably treated by the clas-
sificatory mechanisms of neural nets?while the
treatment of syntactic structures is best left to sym-
bolic parsers. We thus developed a hybrid system,
which parses its input sentences on the basis of a
symbolic probabilistic grammar, and reranks the
candidate parses based on scores given by a neural
network.
Our work is inspired by the work of Socher and
colleagues (2010; 2011). They proposed a parser
using a recursive neural network (RNN) for en-
coding parse trees, representing phrases in a vec-
tor space, and scoring them. Their experimental
result (only 1.92% lower than the Stanford parser
on unlabelled bracket F-score for sentences up to a
length of 15 words) shows that an RNN is expres-
sive enough for syntactic parsing. Additionally,
their qualitative analysis indicates that the learnt
phrase features capture some aspects of phrasal se-
mantics, which could be useful to resolve semantic
ambiguity that syntactical information alone can
not. Our work in this paper differs from their work
in that we replace the parsing task by a reranking
task, and thus reduce the object space significantly
to a set of parses generated by a symbolic parser
rather than the space of all parse trees. As a result,
we can apply our method to sentences which are
much longer than 15 words.
Reranking a k-best list is not a new idea.
Collins (2000), Charniak and Johnson (2005), and
Johnson and Ural (2010) have built reranking sys-
tems with performances that are state-of-the-art.
In order to achieve such high F-scores, those
rerankers rely on a very large number of features
selected on the basis of expert knowledge. Unlike
them, our feature set is selected automatically, yet
the reranker achieved a statistically significant im-
provement on both F-score and exact match.
Closest to our work is Menchetti et al (2005)
and Socher et al (2013): both also rely on sym-
bolic parsers to reduce the search space and use
RNNs to score candidate parses. However, our
work differs in the way the feature set for rerank-
ing is selected. In their methods, only the score at
the tree root is considered whereas in our method
the scores at all internal nodes are taken into ac-
count. Selecting the feature set like that gives us a
flexible way to deal with errors accumulated from
the leaves to the root.
Figure 1 shows a diagram of our method. First,
a parser (in this paper: the Berkeley parser) is used
to generate k-best lists of the Wall Street Jour-
nal (WSJ) sections 02-21. Then, all parse trees in
these lists and the WSJ02-21 are preprocessed by
marking head words, binarising, and performing
error-annotation (Section 2). After that, we use
the annotated trees to train our parse-correctness
classifier (Section 3). Finally, those trees and the
classifier are used to train the reranker (Section 4).
2 Experimental Setup
The experiments presented in this paper have the
following setting. We use the WSJ corpus with
the standard splits: sections 2-21 for training, sec-
tion 22 for development, and section 23 for test-
ing. The latest implementation (version 1.7) of the
Berkeley parser1 (Petrov et al, 2006) is used for
generating 50-best lists. We mark head words and
binarise all trees in the WSJ and the 50-best lists
as in Subsection 2.1, and annotate them as in Sub-
section 2.2 (see Figure 2).
2.1 Preprocessing Trees
We preprocess trees by marking head words and
binarising the trees. For head word marking,
we used the head finding rules of Collins (1999)
which are implemented in the Stanford parser.
To binarise a k-ary branching, e.g. P ?
C1 ... H ... Ck where H is the top label of the
head constituent, we use the following method. If
H is not the left-most child, then
P ? C1 @P ; @P ? C2 ... H ... Ck
otherwise,
P ? @P Ck ; @P ? H ... Ck?1
where @P , which is called extra-P , now is the
head of P . We then apply this transformation
again on the children until we reach terminal
nodes. In this way, we ensure that every internal
node has one head word.
2.2 Error Annotation
We annotate nodes (as correct or incorrect) as fol-
lows. Given a parse tree T in a 50-best list and
a corresponding gold-standard tree G in the WSJ,
1https://code.google.com/p/berkeleyparser
12
Figure 1: An overview of our method.
Figure 2: Example for preprocessing trees. Nodes marked with (*) are labelled incorrect whereas the
other nodes are labelled correct.
we first attempt to align their terminal nodes ac-
cording to the following criterion: a terminal node
t is aligned to a terminal node g if they are at
the same position counting from-left-to-right and
they have the same label. Then, a non-terminal
node P [wh] with children C1, ..., Ck is aligned to
a gold-standard non-terminal node P ?[w?h] with
children C?1 , ..., C
?
l (1 ? k, l ? 2 in our case)
if they have the same word head, the same syn-
tactical category, and their children are all aligned
in the right order. In other words, the following
conditions have to be satisfied
P = P ? ; wh = w?h ; k = l
Ci is aligned to C?i , for all i = 1..k
Aligned nodes are annotated as correct whereas
the other nodes are annotated as incorrect.
3 Parse-Correctness Classification
This section describes how a neural network
is used to construct vector representations for
phrases given parse trees and to identify if those
trees are syntactically correct or not. In order to
encode tree structures, we use an RNN2 (see Fig-
ure 3 and Figure 4) which is similar to the one
proposed by Socher and colleagues (2010). How-
ever, unlike their RNN, our RNN can handle unary
branchings, and also takes head words and syntac-
tic tags as input. It is worth noting that, although
we can use some transformation to remove unary
branchings, handling them is helpful in our case
because the system avoids dealing with so many
syntactic tags that would result from the transfor-
2The first neural-network approach attempting to operate
and represent compositional, recursive structure is the Recur-
sive Auto-Associative Memory network (RAAM), which was
proposed by Pollack (1988). In order to encode a binary tree,
the RAAM network contains three layers: an input layer for
two daughter nodes, a hidden layer for their parent node, and
an output layer for their reconstruction. Training the network
is to minimise the reconstruction error such that we can de-
code the information captured in the hidden layer to the orig-
inal tree form. Our RNN differs from the RAAM network in
that its output layer is not for reconstruction but for classifi-
cation.
13
mation. In addition, using a new set of weight ma-
trices for unary branchings makes our RNN more
expressive without facing the problem of sparsity
thanks to a large number of unary branchings in
the treebank.
Figure 3: An RNN attached to the parse tree
shown in the top-right of Figure 2. All unary
branchings share a set of weight matrices, and all
binary branchings share another set of weight ma-
trices (see Figure 4).
An RNN processes a tree structure by repeat-
edly applying itself at each internal node. Thus,
walking bottom-up from the leaves of the tree to
the root, we compute for every node a vector based
on the vectors of its children. Because of this
process, those vectors have to have the same di-
mension. It is worth noting that, because informa-
tion at leaves, i.e. lexical semantics, is composed
according to a given syntactic parse, what a vec-
tor at each internal node captures is some aspects
of compositional semantics of the corresponding
phrase. In the remainder of this subsection, we
describe in more detail how to construct composi-
tional vector-based semantics geared towards the
parse-correctness classification task.
Similar to Socher et al (2010), and Col-
lobert (2011), given a string of words (w1, ..., wl),
we first compute a string of vectors (x1, ..., xl)
representing those words by using a look-up table
(i.e., word embeddings) L ? Rn?|V |, where |V | is
the size of the vocabulary and n is the dimension-
ality of the vectors. This look-up table L could
be seen as a storage of lexical semantics where
each column is a vector representation of a word.
Hence, let bi be the binary representation of word
wi (i.e., all of the entries of bi are zero except the
one corresponding to the index of the word in the
dictionary), then
xi = Lbi ? Rn (1)
We also encode syntactic tags by binary vectors
but put an extra bit at the end of each vector to
mark if the corresponding tag is extra or not (i.e.,
@P or P ).
Figure 4: Details about our RNN for a unary
branching (top) and a binary branching (bottom).
The bias is not shown for the simplicity.
Then, given a unary branching P [wh]? C, we
can compute the vector at the node P by (see Fig-
ure 4-top)
p = f
(
Wuc+Whxh +W?1x?1 +
W+1x+1 +Wttp + bu
)
where c, xh are vectors representing the child C
and the head word, x?1, x+1 are the left and right
neighbouring words of P , tp encodes the syn-
tactic tag of P , Wu,Wh,W?1,W+1 ? Rn?n,
Wt ? Rn?(|T |+1), |T | is the size of the set of
syntactic tags, bu ? Rn, and f can be any acti-
vation function (tanh is used in this case). With
a binary branching P [wh] ? C1 C2, we simply
change the way the children?s vectors added (see
Figure 4-bottom)
p = f
(
Wb1c1 +Wb2c2 +Whxh +W?1x?1 +
W+1x+1 +Wttp + bb
)
Finally, we put a sigmoid neural unit on the
top of each internal node (except pre-terminal
nodes because we are not concerned with POS-
tagging) to detect the correctness of the subparse
tree rooted at that node
y = sigmoid(Wcatp+ bcat) (2)
where Wcat ? R1?n, bcat ? R.
14
3.1 Learning
The error on a parse tree is computed as the sum
of classification errors of all subparses. Hence, the
learning is to minimise the objective
J(?) =
1
N
?
T
?
(y(?),t)?T
1
2
(t? y(?))2 + ????2
(3)
where ? are the model parameters, N is the num-
ber of trees, ? is a regularisation hyperparameter,
T is a parse tree, y(?) is given by Equation 2, and
t is the class of the corresponding subparse (t = 1
means correct). The gradient ?J?? is computed ef-
ficiently thanks to backpropagation through the
structure (Goller and Kuchler, 1996). L-BFGS
(Liu and Nocedal, 1989) is used to minimise the
objective function.
3.2 Experiments
We implemented our classifier in Torch73 (Col-
lobert et al, 2011a), which is a powerful Matlab-
like environment for machine learning. In order to
save time, we only trained the classifier on 10-best
parses of WSJ02-21. The training phase took six
days on a computer with 16 800MHz CPU-cores
and 256GB RAM. The word embeddings given by
Collobert et al (2011b)4 were used as L in Equa-
tion 1. Note that these embeddings, which are the
result of training a language model neural network
on the English Wikipedia and Reuters, have been
shown to capture many interesting semantic simi-
larities between words.
We tested the classifier on the development
set WSJ22, which contains 1, 700 sentences, and
measured the performance in positive rate and
negative rate
pos-rate =
#true pos
#true pos +#false neg
neg-rate =
#true neg
#true neg +#false pos
The positive/negative rate tells us the rate at which
positive/negative examples are correctly labelled
positive/negative. In order to achieve high per-
formance in the reranking task, the classifier must
have a high positive rate as well as a high nega-
tive rate. In addition, percentage of positive exam-
ples is also interesting because it shows the unbal-
ancedness of the data. Because the accuracy is not
3http://www.torch.ch/
4http://ronan.collobert.com/senna/
a reliable measurement when the dataset is highly
unbalanced, we do not show it here. Table 1, Fig-
ure 5, and Figure 6 show the classification results.
pos-rate (%) neg-rate (%) %-Pos
gold-std 75.31 - 1
1-best 90.58 64.05 71.61
10-best 93.68 71.24 61.32
50-best 95.00 73.76 56.43
Table 1: Classification results on the WSJ22 and
the k-best lists.
Figure 5: Positive rate, negative rate, and percent-
age of positive examples w.r.t. subtree depth.
3.3 Discussion
Table 1 shows the classification results on the
gold-standard, 1-best, 10-best, and 50-best lists.
The positive rate on the gold-standard parses,
75.31%, gives us the upper bound of %-pos when
this classifier is used to yield 1-best lists. On the 1-
best data, the classifier missed less than one tenth
positive subtrees and correctly found nearly two
third of the negative ones. That is, our classi-
fier might be useful for avoiding many of the mis-
takes made by the Berkeley parser, whilst not in-
troducing too many new mistakes of its own. This
fact gave us hope to improve parsing performance
when using this classifier for reranking.
Figure 5 shows positive rate, negative rate, and
percentage of positive examples w.r.t. subtree
depth on the 50-best data. We can see that the pos-
itive rate is inversely proportional to the subtree
depth, unlike the negative rate. That is because the
15
Figure 6: Positive rate, negative rate, and percentage of positive samples w.r.t. syntactic categories
(excluding POS tags).
deeper a subtree is, the lower the a priori likeli-
hood that the subtree is positive (we can see this
in the percentage-of-positive-example curve). In
addition, deep subtrees are difficult to classify be-
cause uncertainty is accumulated when propagat-
ing from bottom to top.
4 Reranking
In this section, we describe how we use the above
classifier for the reranking task. First, we need to
represent trees in one vector space, i.e., ?(T ) =
(
?1(T ), ..., ?v(T )
)
for an arbitrary parse tree T .
Collins (2000), Charniak and Johnson (2005), and
Johnson and Ural (2010) set the first entry to the
model score and the other entries to the number of
occurrences of specific discrete hand-chosen prop-
erties (e.g., how many times the word pizza comes
after the word eat) of trees. We here do the same
with a trick to discretize results from the classifier:
we use a 2D histogram to store predicted scores
w.r.t. subtree depth. This gives us a flexible way to
penalise low score subtrees and reward high score
subtrees w.r.t. the performance of the classifier at
different depths (see Subsection 3.3). However,
unlike the approaches just mentioned, we do not
use any expert knowledge for feature selection; in-
stead, this process is fully automatic.
Formally speaking, a vector feature ?(T ) is
computed as following. ?1(T ) is the model score
(i.e., max-rule-sum score) given by the parser,
(
?2(T ), ..., ?v(T )
)
is the histogram of a set of
(y, h) where y is given by Equation 2 and h is the
depth of the corresponding subtree. The domain
of y (i.e., [0, 1]) is split into ?y equal bins whereas
the domain of h (i.e., {1, 2, 3, ...}) is split into ?h
bins such that the i-th (i < ?h) bin corresponds to
subtrees of depth i and the ?h-th bin corresponds
to subtrees of depth equal or greater than ?h. The
parameters ?y and ?h are then estimated on the de-
velopment set.
After extracting feature vectors for parse trees,
we then find a linear ranking function
f(T ) = w>?(T )
such that
f(T1) > f(T2) iff fscore(T1) > fscore(T2)
where fscore(.) is the function giving F-score, and
w ? Rv is a weight vector, which is efficiently
estimated by SVM ranking (Yu and Kim, 2012).
SVM was initially used for binary classification.
Its goal is to find the hyperplane which has the
largest margin to best separate two example sets. It
was then proved to be efficient in solving the rank-
ing task in information retrieval, and in syntactic
parsing (Shen and Joshi, 2003; Titov and Hender-
son, 2006). In our experiments, we used SVM-
16
Rank5 (Joachims, 2006), which runs extremely
fast (less than two minutes with about 38, 000 10-
best lists).
4.1 Experiments
Using the classifier in Section 3, we implemented
the reranker in Torch7, trained it on WSJ02-21.
We used WSJ22 to estimate the parameters ?y and
?h by the grid search and found that ?y = 9 and
?h = 4 yielded the best F-score.
Table 2 shows the results of our reranker on
50-best WSJ23 given by the Berkeley parser, us-
ing the standard evalb. Our method improves
0.20% on F-score for sentences with all length,
and 0.22% for sentences with ? 40 words.
These differences are statistically significant6 with
p-value < 0.003. Our method also improves ex-
act match (0.95% for all sentences as well as for
sentences with ? 40 words).
Parser LR LP LF EX
all
Berkeley parser 89.98 90.25 90.12 37.22
This paper 90.10 90.54 90.32 38.17
Oracle 95.94 96.21 96.08 65.56
? 40 words
Berkeley parser 90.43 90.70 90.56 39.65
This paper 90.57 91.01 90.78 40.50
Oracle 96.47 96.73 96.60 68.51
Table 2: Reranking results on 50-best lists on
WSJ23 (LR is labelled recall, LP is labelled pre-
cision, LF is labelled F-score, and EX is exact
match.)
Table 3 shows the comparison of the three
parsers that use the same hybrid reranking ap-
proach. On F-score, our method performed 0.1%
lower than Socher et al (2013), and 1.5% better
than Menchetti et al (2005). However, our method
achieved the least improvement on F-score over its
corresponding baseline. That could be because our
baseline parser (i.e., the Berkeley parser) performs
much better than the other two baseline parsers;
and hence, detecting errors it makes on candidate
parse trees is more difficult.
5www.cs.cornell.edu/people/tj/svm light/svm rank.html
6We used the ?Significance testing for evalua-
tion statistics? software (http://www.nlpado.de/ sebas-
tian/software/sigf.shtml) given by Pado? (2006).
Parser LF (all) K-best
parser
Menchetti et
al. (2005)
88.8 (0.6) Collins
(1999)
Socher et
al. (2013)
90.4 (3.8) PCFG Stan-
ford parser
This paper 90.3 (0.2) Berkeley
parser
Table 3: Comparison of parsers using the same hy-
brid reranking approach. The numbers in the blan-
kets indicate the improvements on F-score over the
corresponding baselines (i.e., the k-best parsers).
5 Conclusions
This paper described a new reranking method
which uses semantics in syntactic parsing: a sym-
bolic parser is used to generate a k-best list which
is later reranked thanks to parse-correctness scores
given by a connectionist compositional-semantics-
based classifier. Our classifier uses a recursive
neural network, like Socher et al, (2010; 2011), to
not only represent phrases in a vector space given
parse trees, but also identify if these parse trees are
grammatically correct or not.
Tested on WSJ23, our method achieved a
statistically significant improvement on F-score
(0.20%) as well as on exact match (0.95%).
This result, although not comparable to the re-
sults reported by Collins (2000), Charniak and
Johnson (2005), and Johnson and Ural (2010),
shows an advantage of using vector-based com-
positional semantics to support available state-of-
the-art parsers.
One of the limitations of the current paper is the
lack of a qualitative analysis of how learnt vector-
based semantics has affected the reranking results.
Therefore, the need for ?compositional seman-
tics? in syntactical parsing may still be doubted.
In future work, we will use vector-based seman-
tics together with non-semantic features (e.g., the
ones of Charniak and Johnson (2005)) to find out
whether the semantic features are truly helpful or
they just resemble non-semantic features.
Acknowledgments
We thank two anonymous reviewers for helpful
comments.
17
References
Rens Bod, Remko Scha, and Khalil Sima?an. 2003.
Data-Oriented Parsing. CSLI Publications, Stan-
ford, CA.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics, pages 132?139. Association for
Computational Linguistics.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the In-
ternational Workshop on Machine Learning (then
Conference), pages 175?182.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational lin-
guistics, 29(4):589?637.
Ronan Collobert, Koray Kavukcuoglu, and Cle?ment
Farabet. 2011a. Torch7: A matlab-like environment
for machine learning. In BigLearn, NIPS Workshop.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011b. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Ronan Collobert. 2011. Deep learning for efficient
discriminative parsing. In International Conference
on Artificial Intelligence and Statistics (AISTATS).
Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In IEEE Inter-
national Conference on Neural Networks, volume 1,
pages 347?352. IEEE.
Thorsten Joachims. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery
and data mining, pages 217?226. ACM.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the Berkeley and Brown parsers. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 665?668. As-
sociation for Computational Linguistics.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical programming, 45(1-3):503?528.
Sauro Menchetti, Fabrizio Costa, Paolo Frasconi, and
Massimiliano Pontil. 2005. Wide coverage natu-
ral language processing using kernel methods and
neural networks for structured data. Pattern Recogn.
Lett., 26(12):1896?1906, September.
Sebastian Pado?, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
Barbara Partee. 1995. Lexical semantics and compo-
sitionality. In L. R. Gleitman and M. Liberman, ed-
itors, Language. An Invitation to Cognitive Science,
volume 1, pages 311?360. MIT Press, Cambridge,
MA.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 433?
440. Association for Computational Linguistics.
Jordan B Pollack. 1988. Recursive auto-associative
memory. Neural Networks, 1:122.
Federico Sangati and Willem Zuidema. 2011. Ac-
curate parsing with compact tree-substitution gram-
mars: Double-DOP. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 84?95. Association for Computa-
tional Linguistics.
Libin Shen and Aravind K Joshi. 2003. An SVM based
voting algorithm with application to parse reranking.
In Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 9?16. Association for Computational Linguis-
tics.
Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop.
Richard Socher, Cliff C Lin, Andrew Y Ng, and
Christopher D Manning. 2011. Parsing natural
scenes and natural language with recursive neu-
ral networks. In Proceedings of the 26th Inter-
national Conference on Machine Learning (ICML),
volume 2.
18
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing With Composi-
tional Vector Grammars. In Proceedings of the ACL
conference (to appear).
Mark Steedman. 1999. Connectionist sentence
processing in perspective. Cognitive Science,
23(4):615?634.
Ivan Titov and James Henderson. 2006. Loss mini-
mization in parse reranking. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, pages 560?567. Association
for Computational Linguistics.
Andreas van Cranenburgh, Remko Scha, and Federico
Sangati. 2011. Discontinuous Data-Oriented Pars-
ing: A mildly context-sensitive all-fragments gram-
mar. In Proceedings of the Second Workshop on Sta-
tistical Parsing of Morphologically Rich Languages,
pages 34?44. Association for Computational Lin-
guistics.
Hwanjo Yu and Sungchul Kim. 2012. SVM tutorial:
Classification, regression, and ranking. In Grzegorz
Rozenberg, Thomas Ba?ck, and Joost N. Kok, ed-
itors, Handbook of Natural Computing, volume 1,
pages 479?506. Springer.
19
