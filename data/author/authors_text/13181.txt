Proceedings of the 12th Conference of the European Chapter of the ACL, pages 166?174,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
An Alignment Algorithm using Belief Propagation and a Structure-Based
Distortion Model
Fabien Cromie`res
Graduate school of informatics
Kyoto University
Kyoto, Japan
fabien@nlp.kuee.kyoto-u.ac.jp
Sadao Kurohashi
Graduate school of informatics
Kyoto University
Kyoto, Japan
kuro@i.kyoto-u.ac.jp
Abstract
In this paper, we first demonstrate the in-
terest of the Loopy Belief Propagation al-
gorithm to train and use a simple align-
ment model where the expected marginal
values needed for an efficient EM-training
are not easily computable. We then im-
prove this model with a distortion model
based on structure conservation.
1 Introduction and Related Work
Automatic word alignment of parallel corpora is
an important step for data-oriented Machine trans-
lation (whether Statistical or Example-Based) as
well as for automatic lexicon acquisition. Many
algorithms have been proposed in the last twenty
years to tackle this problem. One of the most suc-
cessfull alignment procedure so far seems to be
the so-called ?IBM model 4? described in (Brown
et al, 1993). It involves a very complex distor-
tion model (here and in subsequent usages ?dis-
tortion? will be a generic term for the reordering
of the words occurring in the translation process)
with many parameters that make it very complex
to train.
By contrast, the first alignment model we are
going to propose is fairly simple. But this sim-
plicity will allow us to try and experiment differ-
ent ideas for making a better use of the sentence
structures in the alignment process. This model
(and even more so its subsequents variations), al-
though simple, do not have a computationally ef-
ficient procedure for an exact EM-based training.
However, we will give some theoretical and empir-
ical evidences that Loopy Belief Propagation can
give us a good approximation procedure.
Although we do not have the space to review the
many alignment systems that have already been
proposed, we will shortly refer to works that share
some similarities with our approach. In particu-
lar, the first alignment model we will present has
already been described in (Melamed, 2000). We
differ however in the training and decoding pro-
cedure we propose. The problem of making use
of syntactic trees for alignment (and translation),
which is the object of our second alignment model
has already received some attention, notably by
(Yamada and Knight, 2001) and (Gildea, 2003) .
2 Factor Graphs and Belief Propagation
In this paper, we will make several use of Fac-
tor Graphs. A Factor Graph is a graphical
model, much like a Bayesian Network. The three
most common types of graphical models (Factor
Graphs, Bayesian Network and Markov Network)
share the same purpose: intuitively, they allow to
represent the dependencies among random vari-
ables; mathematically, they represent a factoriza-
tion of the joint probability of these variables.
Formally, a factor graph is a bipartite graph with
2 kinds of nodes. On one side, the Variable Nodes
(abbreviated as V-Node from here on), and on the
other side, the Factor Nodes (abbreviated as F-
Node). If a Factor Graph represents a given joint
distribution, there will be one V-Node for every
random variable in this joint distribution. Each F-
Node is associated with a function of the V-Nodes
to which it is connected (more precisely, a func-
tion of the values of the random variables associ-
ated with the V-Nodes, but for brevity, we will fre-
quently mix the notions of V-Node, Random Vari-
ables and their values). The joint distribution is
then the product of these functions (and of a nor-
malizing constant). Therefore, each F-Node actu-
ally represent a factor in the factorization of the
joint distribution.
As a short example, let us consider a prob-
lem classically used to introduce Bayesian Net-
work. We want to model the joint probability of
the Weather(W) being sunny or rainy, the Sprin-
kle(S) being on or off, and the Lawn(L) being
wet or dry. Figure 1 show the dependencies of
166
Figure 1: A classical example
the variables represented with a Factor Graph and
with a Bayesian Network. Mathematically, the
Bayesian Network imply that the joint probabil-
ity has the following factorization: P (W,L, S) =
P (W ) ? P (S|W ) ? P (L|W,S). The Factor Graph
imply there exist two functions ?1 and ?2 as well
as a normalization constant C such that we have
the factorization: P (W,L, S) = C ? ?2(W,S) ?
?1(L,W,S). If we set C = 1, ?2(W,S) =
P (W ) ? P (S|W ) and ?1(L,W,S) = P (L|W,S),
the Factor Graph express exactly the same factor-
ization as the Bayesian Network.
A reason to use Graphical Models is that we can
use with them an algorithm called Belief Propa-
gation (abbreviated as BP from here on) (Pearl,
1988). The BP algorithm comes in two flavors:
sum-product BP and max-product BP. Each one
respectively solve two problems that arise often
(and are often intractable) in the use of a proba-
bilistic model: ?what are the marginal probabili-
ties of each individual variable?? and ?what is the
set of values with the highest probability??. More
precisely, the BP algorithm will give the correct
answer to these questions if the graph represent-
ing the distribution is a forest. If it is not the case,
the BP algorithm is not even guaranteed to con-
verge. It has been shown, however, that the BP al-
gorithm do converge in many practical cases, and
that the results it produces are often surprisingly
good approximations (see, for example, (Murphy
et al, 1999) or (Weiss and Freeman, 2001) ).
(Yedidia et al, 2003) gives a very good presen-
tation of the sum-product BP algorithm, as well as
some theoretical justifications for its success. We
will just give an outline of the algorithm. The BP
algorithm is a message-passing algorithm. Mes-
sages are sent during several iterations until con-
vergence. At each iteration, each V-Node sends
to its neighboring F-Nodes a message represent-
ing an estimation of its own marginal values. The
message sent by the V-Node Vi to the F-Node Fj
estimating the marginal probability of Vi to take
the value x is :
mV i?Fj(x) =
?
Fk?N(V i)\Fj
mFk?V i(x)
(N(Vi) represent the set of the neighbours of Vi)
Also, every F-Node send a message to its neigh-
boring V-Nodes that represent its estimates of the
marginal values of the V-Node:
mFj?V i(x) =
?
v1,...,vn
?j(v1, .., x, .., vn)?
?
?
V k?N(Fj)\V i
mV k?Fj(vk)
At any point, the belief of a V-Node V i is given
by
bi(x) =
?
Fk?N(V i)
mFk?V i(x)
, bi being normalized so that
?
x bi(x) = 1. The
belief bi(x) is expected to converge to the marginal
probability (or an approximation of it) of Vi taking
the value x .
An interesting point to note is that each message
can be ?scaled? (that is, multiplied by a constant)
by any factor at any point without changing the re-
sult of the algorithm. This is very useful both for
preventing overflow and underflow during compu-
tation, and also sometimes for simplifying the al-
gorithm (we will use this in section 3.2). Also,
damping schemes such as the ones proposed in
(Murphy et al, 1999) or (Heskes, 2003) are use-
ful for decreasing the cases of non-convergence.
As for the max-product BP, it is best explained
as ?sum-product BP where each sum is replaced
by a maximization?.
3 The monolink model
We are now going to present a simple alignment
model that will serve both to illustrate the effi-
ciency of the BP algorithm and as basis for fur-
ther improvement. As previously mentioned, this
model is mostly identical to one already proposed
in (Melamed, 2000). The training and decoding
procedures we propose are however different.
3.1 Description
Following the usual convention, we will designate
the two sides of a sentence pair as French and En-
glish. A sentence pair will be noted (e, f). ei rep-
resents the word at position i in e.
167
In this first simple model, we will pay little at-
tention to the structure of the sentence pair we
want to align. Actually, each sentence will be re-
duced to a bag of words.
Intuitively, the two sides of a sentence pair ex-
press the same set of meanings. What we want to
do in the alignment process is find the parts of the
sentences that originate from the same meaning.
We will suppose here that each meaning generate
at most one word on each side, and we will name
concept the pair of words generated by a mean-
ing. It is possible for a meaning to be expressed
in only one side of the sentence pair. In that case,
we will have a ?one-sided? concept consisting of
only one word. In this view, a sentence pair ap-
pears ?superficially? as a pair of bag of words, but
the bag of words are themselves the visible part of
an underlying bag of concepts.
We propose a simple generative model to de-
scribe the generation of a sentence pair (or rather,
its underlying bag of concepts):
? First, an integer n, representing the number
of concepts of the sentence is drawn from a
distribution Psize
? Then, n concepts are drawn independently
from a distribution Pconcept
The probability of a bag of concepts C is then:
P (C) = Psize(|C|)
?
(w1,w2)?C
Pconcept((w1, w2))
We can alternatively represent a bag of concepts
as a pair of sentence (e, f), plus an alignment a.
a is a set of links, a link being represented as a
pair of positions in each side of the sentence pair
(the special position -1 indicating the empty side
of a one-sided concept). This alternative represen-
tation has the advantage of better separating what
is observed (the sentence pair) and what is hidden
(the alignment). It is not a strictly equivalent rep-
resentation (it also contains information about the
word positions) but this will not be relevant here.
The joint distribution of e,f and a is then:
P (e, f, a) = Psize(|a|)
?
(i,j)?a
Pconcept(ei, fj)
(1)
This model only take into consideration one-
to-one alignments. Therefore, from now on, we
will call this model ?monolink?. Considering
only one-to-one alignments can be seen as a lim-
itation compared to others models that can of-
ten produce at least one-to-many alignments, but
on the good side, this allow the monolink model
to be nicely symmetric. Additionally, as already
argued in (Melamed, 2000), there are ways to
determine the boundaries of some multi-words
phrases (Melamed, 2002), allowing to treat sev-
eral words as a single token. Alternatively, a pro-
cedure similar to the one described in (Cromieres,
2006), where substrings instead of single words
are aligned (thus considering every segmentation
possible) could be used.
With the monolink model, we want to do two
things: first, we want to find out good values for
the distributions Psize and Pconcept. Then we want
to be able to find the most likely alignment a given
the sentence pair (e, f).
We will consider Psize to be a uniform distribu-
tion over the integers up to a sufficiently big value
(since it is not possible to have a uniform distri-
bution over an infinite discrete set). We will not
need to determine the exact value of Psize . The
assumption that it is uniform is actually enough to
?remove? it of the computations that follow.
In order to determine the Pconcept distribution,
we can use an EM procedure. It is easy to
show that, at every iteration, the EM procedure
will require to set Pconcept(we, wf ) proportional
to the sum of the expected counts of the concept
(we, wf ) over the training corpus. This, in turn,
mean we have to compute the conditional expec-
tation:
E((i, j) ? a|e, f) =
?
a|(i,j)?a
P (a|e, f)
for every sentence pair (e, f). This computation
require a sum over all the possible alignments,
whose numbers grow exponentially with the size
of the sentences. As noted in (Melamed, 2000),
it does not seem possible to compute this expecta-
tion efficiently with dynamic programming tricks
like the one used in the IBM models 1 and 2 (as a
passing remark, these ?tricks? can actually be seen
as instances of the BP algorithm).
We propose to solve this problem by applying
the BP algorithm to a Factor Graph representing
the conditional distribution P (a|e, f). Given a
sentence pair (e, f), we build this graph as fol-
lows.
We create a V-node V ei for every position i in
the English sentence. This V-Node can take for
168
Figure 2: A Factor Graph for the monolink model
in the case of a 2-words English sentence and a 3-
words french sentence (F recij nodes are noted Fri-j)
value any position in the french sentence, or the
special position ?1 (meaning this position is not
aligned, corresponding to a one-sided concept).
We create symmetrically a V-node V fj for every
position in the french sentence.
We have to enforce a ?reciprocal love? condi-
tion: if a V-Node at position i choose a position j
on the opposite side, the opposite V-Node at po-
sition j must choose the position i. This is done
by adding a F-Node F reci,j between every opposite
node V ei and V
f
j , associated with the function:
?reci,j (k, l) =
?
??
??
1 if (i = l and j = k)
or (i 6= l and j 6= k)
0 else
We then connect a ?translation probability? F-
Node F tp.ei to every V-Node V
e
i associated with
the function:
?tp.ei (j) =
{?
Pconcept(ei, fj) if j 6= ?1
Pconcept(ei, ?) if j = ?1
We add symmetrically on the French side F-Nodes
F tp.fj to the V-Nodes V
f
j .
It should be fairly easy to see that such a Factor
Graph represents P (a|e, f). See figure 2 for an
example.
Using the sum-product BP, the beliefs of ev-
ery V-Node V ei to take the value j and of every
node V fj to take the value i should converge to the
marginal expectation E((i, j) ? a|e, f) (or rather,
a hopefully good approximation of it).
We can also use max-product BP on the same
graph to decode the most likely alignment. In the
monolink case, decoding is actually an instance of
the ?assignment problem?, for which efficient al-
gorithms are known. However this will not be the
case for the more complex model of the next sec-
tion. Actually, (Bayati et al, 2005) has recently
proved that max-product BP always give the opti-
mal solution to the assignment problem.
3.2 Efficient BP iterations
Applying naively the BP algorithm would lead us
to a complexity of O(|e|2 ? |f |2) per BP iteration.
While this is not intractable, it could turn out to be
a bit slow. Fortunately, we found it is possible to
reduce this complexity to O(|e| ? |f |) by making
two useful observations.
Let us note meij the resulting message from V
e
i
to V fj (that is the message sent by F
rec
i,j to V
f
j af-
ter it received its own message from V ei ). m
e
ij(x)
has the same value for every x different from i:
meij(x 6= i) =
?
k 6=j
bei (k)
mfji(k)
. We can divide all the
messages meij by m
e
ij(x 6= i), so that m
e
ij(x) = 1
except if x = i; and the same can be done for the
messages coming from the French side mfij . It fol-
lows that meij(x 6= i) =
?
k 6=j b
e
i (k) = 1 ? b
e
i (j)
if the bei are kept normalized. Therefore, at ev-
ery step, we only need to compute meij(j), not
meij(x 6= j).
Hence the following algorithm (meij(j) will be
here abbreviated to meij since it is the only value
of the message we need to compute). We describe
the process for computing the English-side mes-
sages and beliefs (meij and b
e
i ) , but the process
must also be done symmetrically for the French-
side messages and beliefs (mfij and b
f
i ) at every
iteration.
0- Initialize all messages and beliefs with:
me(0)ij = 1 and b
e(0)
i (j) = ?
tp.e
i (j)
Until convergence (or for a set number of itera-
tion):
1- Compute the messages meij : m
e(t+1)
ij =
be(t)i (j)/((1 ? b
e(t)
i (j)) ? m
f(t)
ji )
2- Compute the beliefs bei (j):bi(j)
e(t+1) =
?tp.ei (j) ? m
f(t+1)
ji
3- And then normalize the bi(j)e(t+1) so that?
j bi(j)
e(t+1) = 1.
A similar algorithm can be found for the max-
product BP.
3.3 Experimental Results
We evaluated the monolink algorithm with two
languages pairs: French-English and Japanese-
English.
169
For the English-French Pair, we used 200,000
sentence pairs extracted from the Hansard cor-
pus (Germann, 2001). Evaluation was done with
the scripts and gold standard provided during
the workshop HLT-NAACL 20031 (Mihalcea and
Pedersen, 2003). Null links are not considered for
the evaluation.
For the English-Japanese evaluation, we used
100,000 sentence pairs extracted from a corpus of
English/Japanese news. We used 1000 sentence
pairs extracted from pre-aligned data(Utiyama and
Isahara, 2003) as a gold standard. We segmented
all the Japanese data with the automatic segmenter
Juman (Kurohashi and Nagao, 1994). There is
a caveat to this evaluation, though. The reason
is that the segmentation and alignment scheme
used in our gold standard is not very fine-grained:
mostly, big chunks of the Japanese sentence cover-
ing several words are aligned to big chunks of the
English sentence. For the evaluation, we had to
consider that when two chunks are aligned, there
is a link between every pair of words belonging to
each chunk. A consequence is that our gold stan-
dard will contain a lot more links than it should,
some of them not relevants. This means that the
recall will be largely underestimated and the pre-
cision will be overestimated.
For the BP/EM training, we used 10 BP iter-
ations for each sentences, and 5 global EM iter-
ations. By using a damping scheme for the BP
algorithm, we never observed a problem of non-
convergence (such problems do commonly ap-
pears without damping). With our python/C im-
plementation, training time approximated 1 hour.
But with a better implementation, it should be pos-
sible to reduce this time to something comparable
to the model 1 training time with Giza++.
For the decoding, although the max-product BP
should be the algorithm of choice, we found we
could obtain slightly better results (by between 1
and 2 AER points) by using the sum-product BP,
choosing links with high beliefs, and cutting-off
links with very small beliefs (the cut-off was cho-
sen roughly by manually looking at a few aligned
sentences not used in the evaluation, so as not to
create too much bias).
Due to space constraints, all of the results of this
section and the next one are summarized in two
tables (tables 1 and 2) at the end of this paper.
In order to compare the efficiency of the BP
1http://www.cs.unt.edu/ rada/wpt/
training procedure to a more simple one, we reim-
plemented the Competitive Link Algorithm (ab-
breviated as CLA from here on) that is used in
(Melamed, 2000) to train an identical model. This
algorithm starts with some relatively good esti-
mates found by computing correlation score (we
used the G-test score) between words based on
their number of co-occurrences. A greedy Viterbi
training is then applied to improve this initial
guess. In contrast, our BP/EM training do not need
to compute correlation scores and start the training
with uniform parameters.
We only evaluated the CLA on the
French/English pair. The first iteration of
CLA did improve alignment quality, but subse-
quent ones decreased it. The reported score for
CLA is therefore the one obtained during the best
iteration. The BP/EM training demonstrate a clear
superiority over the CLA here, since it produce
almost 7 points of AER improvement over CLA.
In order to have a comparison with a well-
known and state-of-the-art system, we also used
the GIZA++ program (Och and Ney, 1999) to
align the same data. We tried alignments in both
direction and provide the results for the direction
that gave the best results. The settings used were
the ones used by the training scripts of the Moses
system2, which we assumed to be fairly optimal.
We tried alignment with the default Moses settings
(5 iterations of model 1, 5 of Hmm, 3 of model 3,
3 of model 4) and also tried with increased number
of iterations for each model (up to 10 per model).
We are aware that the score we obtained for
model 4 in English-French is slightly worse than
what is usually reported for a similar size of train-
ing data. At the time of this paper, we did not
have the time to investigate if it is a problem of
non-optimal settings in GIZA++, or if the train-
ing data we used was ?difficult to learn from? (it
is common to extract sentences of moderate length
for the training data but we didn?t, and some sen-
tences of our training corpus do have more than
200 words; also, we did not use any kind of pre-
processing). In any case, Giza++ is compared here
with an algorithm trained on the same data and
with no possibilities for fine-tuning; therefore the
comparison should be fair.
The comparison show that performance-wise,
the monolink algorithm is between the model 2
and the model 3 for English/French. Considering
2http://www.statmt.org/moses/
170
our model has the same number of parameters as
the model 1 (namely, the word translation prob-
abilities, or concept probabilities in our model),
these are pretty good results. Overall, the mono-
link model tend to give better precision and worse
recall than the Giza++ models, which was to be
expected given the different type of alignments
produced (1-to-1 and 1-to-many).
For English/Japanese, monolink is at just about
the level of model 1, but model 1,2 and 3 have very
close performances for this language pair (inter-
estingly, this is different from the English/French
pair). Incidentally, these performances are very
poor. Recall was expected to be low, due to the
previously mentioned problem with the gold stan-
dard. But precision was expected to be better. It
could be the algorithms are confused by the very
fine-grained segmentation produced by Juman.
4 Adding distortion through structure
4.1 Description
While the simple monolink model gives interest-
ing results, it is somehow limited in that it do not
use any model of distortion. We will now try to
add a distortion model; however, rather than di-
rectly modeling the movement of the positions of
the words, as is the case in the IBM models, we
will try to design a distortion model based on the
structures of the sentences. In particular, we are
interested in using the trees produced by syntactic
parsers.
The intuition we want to use is that, much like
there is a kind of ?lexical conservation? in the
translation process, meaning that a word on one
side has usually an equivalent on the other side,
there should also be a kind of ?structure conserva-
tion?, with most structures on one side having an
equivalent on the other.
Before going further, we should precise the idea
of ?structure? we are going to use. As we said, our
prime (but not only) interest will be to make use of
the syntactic trees of the sentences to be aligned.
However these kind of trees come in very different
shapes depending on the language and the type of
parser used (dependency, constituents,. . . ). This is
why we decided the only information we would
keep from a syntactic tree is the set of its sub-
nodes. More specifically, for every sub-node, we
will only consider the set of positions it cover in
the underlying sentence. We will call such a set
of positions a P-set. This simplification will allow
Figure 3: A small syntactic tree and the 3 P-Sets it
generates
us to process dependency trees, constituents trees
and other structures in a uniformized way. Fig-
ure 3 gives an example of a constituents tree and
the P-sets it generates.
According to our intuition about the ?conserva-
tion of structure?, some (not all) of the P-sets on
one side should have an equivalent on the other
side. We can model this in a way similar to how
we represented equivalence between words with
concepts. We postulate that, in addition to a bag of
concepts, sentence pairs are underlaid by a set of
P-concepts. P-concepts being actually pairs of P-
sets (a P-set for each side of the sentence pair). We
also allow the existence of one-sided P-concepts.
In the previous model, sentence pairs where
just bag of words underlaid by a or bag of con-
cepts, and there was no modeling of the position
of the words. P-concepts bring a notion of word
position to the model. Intuitively, there should
be coherency between P-concepts and concepts.
This coherence will come from a compatibility
constraint: if a sentence contains a two-sided P-
concept (PSe, PSf ), and if a word we covered
by PSe come from a two-sided concept (we, wf ),
then wf must be covered by PSf .
Let us describe the model more formally. In
the view of this model, a sentence pair is fully de-
scribed by: e and f (the sentences themselves), a
(the word alignment giving us the underlying bag
of concept), se and sf (the sets of P-sets on each
side of the sentence) and as (the P-set algnment
that give us the underlying set of P-concepts).
e,f ,se,sf are considered to be observed (even if
we will need parsing tools to observe se and sf );
a and as are hidden. The probability of a sentence
pair is given by the joint probability of these vari-
ables :P (e, f, se, sf , a, as). By making some sim-
ple independence assumptions, we can write:
P (a, as, e, f,s
e, sf ) = Pml(a, e, f)?
? P (se, sf |e, f) ? P (as|a, s
e, sf )
171
Pml(a, e, f) is taken to be identical to the mono-
link model (see equation (1)). We are not inter-
ested in P (se, sf |e, f) (parsers will deal with it for
us). In our model, P (as|a, se, sf ) will be equal to:
P(as|a, s
e, sf ) = C ?
?
(i,j)?as
Ppc(s
e
i , s
f
j )?
? comp(a, as, s
e, sf )
where comp(a, as, se, sf ) is equal to 1 if the com-
patibility constraint is verified, and 0 else. C is a
normalizing constant. Ppc describe the probability
of each P-concept.
Although it would be possible to learn parame-
ters for the distribution Ppc depending on the char-
acteristics of each P-concepts, we want to keep
our model simple. Therefore, Ppc will have only
two different values. One for the one-sided P-
concepts, and one for the two-sided ones. Con-
sidering the constraint of normalization, we then
have actually one parameter: ? = Ppc(1?sided)Ppc(2?sided) .
Although it would be possible to learn the param-
eter ? during the EM-training, we choose to set
it at a preset value. Intuitively, we should have
0 < ? < 1, because if ? is greater than 1, then
the one-sided P-concepts will be favored by the
model, which is not what we want. Some empiri-
cal experiments showed that all values of ? in the
range [0.5,0.9] were giving good results, which
lead to think that ? can be set mostly indepen-
dently from the training corpus.
We still need to train the concepts probabilities
(used in Pml(a, e, f)), and to be able to decode
the most probable alignments. This is why we are
again going to represent P (a, as|e, f, se, sf ) as a
Factor Graph.
This Factor Graph will contain two instances of
the monolink Factor Graph as subgraph: one for
a, the other for as (see figure 4). More precisely,
we create again a V-Node for every position on
each side of the sentence pair. We will call these
V-Nodes ?Word V-Nodes?, to differentiate them
from the new ?P-set V-Nodes?. We will create a
?P-set V-Node? V ps.ei for every P-set in se, and a
?P-set V-Node? V ps.fj for every P-set in sj . We
inter-connect all of the Word V-Nodes so that we
have a subgraph identical to the Factor Graph used
in the monolink case. We also create a ?monolink
subgraph? for the P-set V-Nodes.
We now have 2 disconnected subgraphs. How-
ever, we need to add F-Nodes between them to en-
force the compatibility constraint between as and
Figure 4: A part of a Factor Graph showing the
connections between P-set V-Nodes and Word V-
Nodes on the English side.The V-Nodes are con-
nected to the French side through the 2 monolink
subgraphs
a. On the English side, for every P-set V-Node
V psek , and for every position i that the correspond-
ing P-set cover, we add a F-Node F comp.ek,i between
V psek and V
e
i , associated with the function:
?comp.ek,i (l, j) =
?
??
??
1 if j ? sfl or
j = ?1 or l = ?1
0 else
We proceed symmetrically on the French side.
Messages inside each monolink subgraph can
still be computed with the efficient procedure de-
scribed in section 3.2. We do not have the space to
describe in details the messages sent between P-set
V-Nodes and Word V-Nodes, but they are easily
computed from the principles of the BP algorithm.
Let NE =
?
ps?se |ps| and NF =
?
ps?sf |ps|.
Then the complexity of one BP iteration will be
O(NG ? ND + |e| ? |f |).
An interesting aspect of this model is that it
is flexible towards enforcing the respect of the
structures by the alignment, since not every P-set
need to have an equivalent in the opposite sen-
tence. (Gildea, 2003) has shown that too strict an
enforcement can easily degrade alignment quality
and that good balance was difficult to find.
Another interesting aspect is the fact that
we have a somehow ?parameterless? distortion
model. There is only one real-valued parameter to
control the distortion: ?. And even this parameter
is actually pre-set before any training on real data.
The distortion is therefore totally controlled by the
two sets of P-sets on each side of the sentence.
Finally, although we introduced the P-sets as
being generated from a syntactic tree, they do
not need to. In particular, we found interest-
ing to use P-sets consisting of every pair of adja-
172
cent positions in a sentence. For example, with
a sentence of length 5, we generate the P-sets
{1,2},{2,3},{3,4} and {4,5}. The underlying in-
tuition is that ?adjacency? is often preserved in
translation (we can see this as another case of
?conservation of structure?). Practically, using P-
sets of adjacent positions create a distortion model
where permutation of words are not penalized, but
gaps are penalized.
4.2 Experimental Results
The evaluation setting is the same as in the previ-
ous section. We created syntactic trees for every
sentences. For English,we used the Dan Bikel im-
plementation of the Collins parser (Collins, 2003).
For French, the SYGMART parser (Chauche?,
1984) and for Japanese, the KNP parser (Kuro-
hashi and Nagao, 1994).
The line SDM:Parsing (SDM standing for
?Structure-based Distortion Monolink?) shows the
results obtained by using P-sets from the trees pro-
duced by these parsers. The line SDM:Adjacency
shows results obtained by using adjacent positions
P-sets ,as described at the end of the previous sec-
tion (therefore, SDM:Adjacency do not use any
parser).
Several interesting observations can be made
from the results. First, our structure-based distor-
tion model did improve the results of the mono-
link model. There are however some surprising
results. In particular, SDM:Adjacency produced
surprisingly good results. It comes close to the
results of the IBM model 4 in both language pairs,
while it actually uses exactly the same parameters
as model 1. The fact that an assumption as simple
as ?allow permutations, penalize gaps? can pro-
duce results almost on par with the complicated
distortion model of model 4 might be an indica-
tion that this model is unnecessarily complex for
languages with similar structure.Another surpris-
ing result is the fact that SDM:Adjacency gives
better results for the English-French language pair
than SDM:Parsing, while we expected that infor-
mation provided by parsers would have been more
relevant for the distortion model. It might be an
indication that the structure of English and French
is so close that knowing it provide only moder-
ate information for word reordering. The con-
trast with the English-Japanese pair is, in this re-
spect, very interesting. For this language pair,
SDM:Adjacency did provide a strong improve-
Algorithm AER P R
Monolink 0.197 0.881 0.731
SDM:Parsing 0.166 0.882 0.813
SDM:Adjacency 0.135 0.887 0.851
CLA 0.26 0.819 0.665
GIZA++ /Model 1 0.281 0.667 0.805
GIZA++ /Model 2 0.205 0.754 0.863
GIZA++ /Model 3 0.162 0.806 0.890
GIZA++ /Model 4 0.121 0.849 0.927
Table 1: Results for English/French
Algorithm F P R
Monolink 0.263 0.594 0.169
SDM:Parsing 0.291 0.662 0.186
SDM:Adjacency 0.279 0.636 0.179
GIZA++ /Model 1 0.263 0.555 0.172
GIZA++ /Model 2 0.268 0.566 0.176
GIZA++ /Model 3 0.267 0.589 0.173
GIZA++ /Model 4 0.299 0.658 0.193
Table 2: Results for Japanese/English.
ment, but significantly less so than SDM:Parsing.
This tend to show that for language pairs that have
very different structures, the information provided
by syntactic tree is much more relevant.
5 Conclusion and Future Work
We will summarize what we think are the 4 more
interesti ng contributions of this paper. BP al-
gorithm has been shown to be useful and flexi-
ble for training and decoding complex alignment
models. An original mostly non-parametrical dis-
tortion model based on a simplified structure of
the sentences has been described. Adjacence con-
straints have been shown to produce very efficient
distortion model. Empirical performances differ-
ences in the task of aligning Japanese and English
to French hint that considering different paradigms
depending on language pairs could be an improve-
ment on the ?one-size-fits-all? approach generally
used in Statistical alignment and translation.
Several interesting improvement could also be
made on the model we presented. Especially,
a more elaborated Ppc, that would take into ac-
count the nature of the nodes (NP, VP, head,..) to
parametrize the P-set algnment probability, and
would use the EM-algorithm to learn those param-
eters.
173
References
M. Bayati, D. Shah, and M. Sharma. 2005. Maxi-
mum weight matching via max-product belief prop-
agation. Information Theory, 2005. ISIT 2005. Pro-
ceedings. International Symposium on, pages 1763?
1767.
Peter E Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer, 1993. The
mathematics of statistical machine translation: pa-
rameter estimation, volume 19, pages 263?311.
J. Chauche?. 1984. Un outil multidimensionnel de
lanalyse du discours. Coling84. Stanford Univer-
sity, California.
M. Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguis-
tics.
Fabien Cromieres. 2006. Sub-sentential alignment us-
ing substring co-occurrence counts. In Proceedings
of ACL. The Association for Computer Linguistics.
U. Germann. 2001. Aligned hansards
of the 36th parliament of canada.
http://www.isi.edu/naturallanguage/download/hansard/.
D. Gildea. 2003. Loosely tree-based alignment for
machine translation. Proceedings of ACL, 3.
T. Heskes. 2003. Stable fixed points of loopy be-
lief propagation are minima of the bethe free energy.
Advances in Neural Information Processing Systems
15: Proceedings of the 2002 Conference.
S. Kurohashi and M. Nagao. 1994. A syntactic analy-
sis method of long japanese sentences based on the
detection of conjunctive structures. Computational
Linguistics, 20(4):507?534.
I. D. Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
I. Melamed. 2002. Empirical Methods for Exploiting
Parallel Texts. The MIT Press.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation
exercise for word alignment. In Rada Mihalcea and
Ted Pedersen, editors, HLT-NAACL 2003 Workshop:
Building and Using Parallel Texts: Data Driven Ma-
chine Translation and Beyond, pages 1?10, Edmon-
ton, Alberta, Canada, May 31. Association for Com-
putational Linguistics.
Kevin P Murphy, Yair Weiss, and Michael I Jordan.
1999. Loopy belief propagation for approximate in-
ference: An empirical study. In Proceedings of Un-
certainty in AI, pages 467?475.
Franz Josef Och and Hermann Ney. 1999. Improved
alignment models for statistical machine translation.
University of Maryland, College Park, MD, pages
20?28.
J. Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann Publishers.
M. Utiyama and H. Isahara. 2003. Reliable measures
for aligning japanese-english news articles and sen-
tences. Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics-Volume
1, pages 72?79.
Y. Weiss and W. T. Freeman. 2001. On the optimality
of solutions of the max-product belief propagation
algorithm in arbitrary graphs. IEEE Trans. on Infor-
mation Theory, 47(2):736?744.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. Proceedings of ACL.
Jonathan S. Yedidia, William T. Freeman, and Yair
Weiss, 2003. Understanding belief propagation and
its generalizations, pages 239?269. Morgan Kauf-
mann Publishers Inc., San Francisco, CA, USA.
174
Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 13?18,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Sub-sentential Alignment Using Substring Co-Occurrence Counts  
 
Fabien Cromieres 
GETA-CLIPS-IMAG 
BP53 38041 Grenoble Cedex 9  
France 
fabien.cromieres@gmail.com 
 
  
 
Abstract 
In this paper, we will present an efficient 
method to compute the co-occurrence 
counts of any pair of substring in a paral-
lel corpus, and an algorithm that make 
use of these counts to create sub-
sentential alignments on such a corpus. 
This algorithm has the advantage of be-
ing as general as possible regarding the 
segmentation of text. 
1 Introduction 
An interesting and important problem in the 
Statistical Machine Translation (SMT) domain is 
the creation of sub-sentential alignment in a par-
allel corpus (a bilingual corpus already aligned at 
the sentence level). These alignments can later be 
used to, for example, train SMT systems or ex-
tract bilingual lexicons. 
Many algorithms have already been proposed 
for sub-sentential alignment. Some of them focus 
on word-to-word alignment ((Brown,97) or 
(Melamed,97)). Others allow the generation of 
phrase-level alignments, such as (Och et al, 
1999), (Marcu and Wong, 2002) or (Zhang, Vo-
gel, Waibel, 2003). However, with the exception 
of Marcu and Wong, these phrase-level align-
ment algorithms still place their analyses at the 
word level; whether by first creating a word-to-
word alignment or by computing correlation co-
efficients between pairs of individual words. 
This is, in our opinion, a limitation of these al-
gorithms; mainly because it makes them rely 
heavily on our capacity to segment a sentence in 
words. And defining what a word is is not as 
easy as it might seem.  In peculiar, in many 
Asians writings systems (Japanese, Chinese or 
Thai, for example), there is not a special symbol 
to delimit words (such as the blank in most non-
Asian writing systems). Current systems usually 
work around this problem by using a segmenta-
tion tool to pre-process the data. There are how-
ever two major disadvantages: 
- These tools usually need a lot of linguistic 
knowledge, such as lexical dictionaries and 
hand-crafted segmentation rules. So using them 
somehow reduces the ?purity? and universality 
of the statistical approach. 
- These tools are not perfect. They tend to be 
very dependent on the domain of the text they 
are used with. Besides, they cannot take advan-
tage of the fact that there exist a translation of the 
sentence in another language.  
(Xu, Zens and Ney,2004) have overcome part 
of these objections by using multiple segmenta-
tions of a Chinese sentence and letting a SMT 
system choose the best one, as well as creating a 
segmentation lexicon dictionary by considering 
every Chinese character to be a word in itself and 
then creating a phrase alignment. However, it is 
probable that this technique would meet much 
more difficulties with Thai, for example (whose 
characters, unlike Chinese, bear no specific sense) 
or even Japanese (which use both ideograms and 
phonetic characters). 
Besides, even for more ?computer-friendly? 
languages, relying too much on typographic 
words may not be the best way to create an 
alignment. For example, the translation of a set 
phrase may contain no word that is a translation 
of the individual words of this set phrase. And 
one could consider languages such as German, 
which tend to merge words that are in relation in 
a single typographic word. For such languages, it 
could be a good thing to be able to create align-
ment at an even more basic level than the typo-
graphic words. 
These thoughts are the main motivations for 
the development of the alignment algorithm we 
will expose in this paper. Its main advantage is 
that it can be applied whatever is the smallest 
13
unit of text we want to consider: typographic 
word or single character. And even when work-
ing at the character level, it can use larger se-
quence of characters to create correct alignments. 
The problem of the segmentation and of the 
alignment will be resolved simultaneously: a sen-
tence and its translation will mutually induce a 
segmentation on one another. Another advantage 
of this algorithm is that it is purely statistical: it 
will not require any information other than the 
parallel corpus we want to align. 
It should be noted here that the phrase-level 
joint-probability model presented in (Marcu and 
Wong) can pretend to have the same qualities. 
However, it was only applied to word-segmented 
texts by its authors. Making use of the EM train-
ing, it is also much more complex than our ap-
proach. 
Before describing our algorithm, we will ex-
plain in detail a method for extracting the co-
occurrence counts of any substring in a parallel 
corpus. Such co-occurrence counts are important 
to our method, but difficult to compute or store 
in the case of big corpora.  
2 Co-Occurrence counting algorithm 
2.1 Notation and definitions 
In the subsequent parts of this paper, a sub-
string will denote indifferently a sequence of 
characters or a sequence of words (or actually a 
sequence of any typographic unit we might want 
to consider). The terms ?elements? will be used 
instead of ?word? or ?characters? to denote the 
fundamental typographic unit we chose for a 
given language. 
In general, the number of co-occurrences of 
two substrings S1 and S2 in a parallel corpus is 
the number of times they have appeared on the 
opposite sides of a bi-sentence in this corpus. It 
will be noted N(S1,S2). In the cases where S1 and 
S2 appears several times in a single bi-sentence 
(n1 and n2 times respectively), we might count 1, 
n1*n2 or min(n1,n2) co-occurrences. We will also 
note N(S1) the number of occurrences of S1 in the 
corpus.  
2.2 The Storage Problem 
Counting word co-occurrences over a parallel 
corpus and storing them in a data structure such 
as a Hash table is a trivial task. But storing the 
co-occurrences counts of every pair of substring 
presents much more technical difficulties. Basi-
cally, the problem is that the number of values to 
be stored is much greater when we consider sub-
strings. For two sentences with N1 and N2 words 
respectively, there are N1*N2 words that co-occur; 
but the number of substrings that co-occur is 
roughly proportional to (N1*N2)^2. Of course, 
most substrings in a pair of sentences are not 
unique in the corpus, which reduces the number 
of values to be stored. Still, in most cases, it re-
mains impractical. For example, the Japanese-
English BTEC corpus has more than 11 million 
unique English (word-) substrings and more than 
8 million unique Japanese (character-) substrings. 
So there are potentially 88,000 billion co-
occurrence values to be stored.  Again, most of 
these substrings do not co-occur in the corpus, so 
that non-zero co-occurrences values are only a 
fraction of this figure. However, a rough estima-
tion we performed showed that there still would 
be close to a billion values to store. 
With a bigger corpus such as the European 
Parliament Corpus (more than 600,000 sentences 
per languages)  we have more than 698 millions 
unique English (word-) substrings and 875 mil-
lions unique French (word-) substrings. And 
things get much worse if we want to try to work 
with characters substrings. 
To handle this problem, we decided not to try 
and store the co-occurrences count beforehand, 
but rather to compute them ?on-the-fly?, when 
they are needed. For that we will need a way to 
compute co-occurrences very efficiently.   We 
will show how to do it with the data structure 
known as Suffix Array.  
2.3 Suffix Arrays 
Suffix Arrays are a data structure allowing for 
(among other things) the efficient computation of 
the number of occurrences of any substring 
within a text. They have been introduced by 
Mamber and Myers (1993) in a bioinformatics 
context. (Callison-Burch, Bannard and Scroeder, 
2005) used them (in a way similar to us) to com-
pute and store phrase translation probabilities 
over very large corpora. 
Basically, a Suffix Array is a very simple data 
structure: it is the sorted list of all the suffixes of 
a text. A suffix is a substring going from one 
starting position in the text to its end. So a text of 
T elements has T suffixes.  
An important point to understand is that we 
won?t have to store the actual suffixes in memory. 
We can describe any suffix by its starting posi-
tion in the text. Hence, every suffix occupies a 
constant space in memory. Actually, a common 
implementation is to represent a suffix by a 
memory pointer on the full text. So, on a ma-
14
chine with 32-bit pointers, the Suffix Array of a 
text of T elements occupy 4*T bytes.  The time 
complexity of the Suffix Array construction is 
O(T*log(T)) if we build the array of the suffixes 
and then sort it. 
We will now describe the property of the Suf-
fix Array that interest us. Let S be a substring. 
Let pf be the position (in the Suffix Array) of the 
first suffix beginning with substring S and pl be 
the position of the last such suffix. Then every 
suffix in the Array between positions pf and pl 
corresponds to an occurrence of S. And every 
occurrence of S in the text corresponds to a suf-
fix between pf and pl.  
pf and pl can be retrieved in O(|S|*log T) with 
a dichotomy search. Beside, N(S)=pl-pf+1; so 
we can compute N(S) in O(|S|*log T). We will 
now see how to compute N(S1,S2) for two sub-
strings S1 and S2 in a parallel corpus. 
2.4 Computing Co-Occurrences using Suf-
fix Array 
A Suffix Array can be created not only from 
one text, but also from a sequence of texts. In the 
present case, we will consider the sequence of 
sentences formed by one side of a parallel corpus. 
The Suffix Array is then the sorted list of all the 
suffixes of all the sentences in the sequence. Suf-
fixes may be represented as a pair of integer (in-
dex of the sentence, position in the sentence) or 
again as a pointer (an example using integer pairs 
is shown on Figure 1). 
We can implement the Suffix Array so that, 
from a suffix, we can determine the index of the 
sentence to which it belongs (the computational 
cost of this is marginal in practical cases and will 
be neglected). We can now compute pf and pl for 
a substring S such as previously, and retrieve the 
sentence indexes corresponding to every suffix 
between positions pf and pl in the Suffix Array, 
This allow us to create an ?occurrence vector?: a 
mapping between sentence indexes and the num-
ber of occurrences of S in those sentences. This 
operation takes O(pl-pf), that is O(N(S)). (Figure 
1. shows an occurrence vector for the substring 
?red car?) 
We can now efficiently compute the co-
occurrence counts of two substrings S1 and S2 in 
a parallel corpus.  
We compute beforehand the two Suffix Arrays 
for the 2 sides of the parallel corpus. We can 
then compute two occurrence vectors V1 and V2 
for S1 and S2 in O(N(S1)+|S1|*log(T1)) and 
O(N(S2)+|S2|*log(T2)) respectively. 
 
With a good implementation, we can use these 
two vectors to obtain N(S1,S2) in 
O(min(size(V1),size(V2))), that is 
O(min(N(S1),N(S2)). 
Hence we can compute NbCoOcc(S1,S2) for 
any substring pair (S1,S2) in 
O(N(S2)+|S2|*log(T2)+N(S1)+|S1|*log(N1))). This 
is much better than a naive approach that takes 
O(T1*T2) by going through the whole corpus. 
Besides, some simple optimizations will substan-
tially improve the average performances. 
2.5 Some Important Optimizations 
There are two ways to improve performances 
when using the previous method for co-
occurrences computing. 
 Firstly, we won?t compute co-occurrences for 
any substrings at random. Typically, in the algo-
rithm described in the following part, we com-
pute N(S1,S2) for every substring pairs in a given 
bi-sentence. So we will compute the occurrence 
vector of a substring only once per sentence. 
Secondly, the time taken to retrieve the co-
occurrence count of two substrings S1 and S2 is 
more or less proportional to their frequency in 
the corpus. This is a problem for the average per-
formance: the most frequent substrings will be 
the one that take longer to compute. This sug-
gests that by caching the occurrence vectors of 
the most frequent substrings (as well as their co-
occurrence counts), we might expect a good im-
provement in performance. (We will see in the 
next sub-section that caching the 200 most fre-
 
A small monolingual corpus 
index sentence 
1 The red car is here 
2 I saw a blue car 
3 I saw a red car   
Occurrence Vector of 
?red car? 
index nbOcc
1 1 
2 0 
3 1 
Suffix Array 
Array 
index 
Suffix Position Suffix 
0 2,3 a blue car 
1 3,4 a red car 
2 2,4 blue car 
3 2,6 car 
4 3,5 car 
5 1,3 car is here 
6 1,5 here 
7 2,1 I saw a blue car 
8 1,1 I saw a red car 
9 1,4 is here 
10 1,2 red car is here 
11 3,5 red car 
12 2,2 saw a blue car 
13 3,3 saw a red car 
14 1,1 The red car is here 
Figure 1. A small corpus, the corresponding suf-
fix array, and an occurrence vector 
15
quent substrings is sufficient to multiply the av-
erage speed by a factor of 50) 
2.6 Practical Evaluation of the Perform-
ances 
We will now test the computational practicality 
of our method. For this evaluation, we will con-
sider the English-Japanese BTEC corpus 
(170,000 bi-sentences, 12MB), and the English-
French Europarl corpus (688,000 bi-sentences, 
180 MB). We also want to apply our algorithm to 
western languages at the character level. How-
ever, working at a character level multiply the 
size of the suffix array by about 5, and increase 
the size of the cached vectors as well. So, be-
cause of memory limitations, we extracted a 
smaller corpus from the Europarl one (100,000 
bi-sentences, 20MB) for experimenting on char-
acters substrings. 
The base elements we will choose for our sub-
strings will be: word/characters for the BTEC, 
word/word for the bigger EuroParl, and 
word/characters for the smaller EuroParl. We 
computed the co-occurrence counts of every sub-
strings pair in a bi-sentence for the 100 first bi-
sentences of every corpus, on a 2.5GHz x86 
computer. We give the average figures for dif-
ferent corpora and caching strategies. 
These results are good enough and show that 
the algorithm we are going to introduce is not 
computationally impracticable. The cache allows 
an interesting trade-off between the perform-
ances and the used memory. We note that the 
proportional speedup depends on the corpus used. 
We did not investigate this point, but the differ-
ent sizes of corpora (inducing different average 
occurrence vectors sizes), and the differences in 
the frequency distribution of words and charac-
ters are probably the main factors. 
3 Sub-sentential alignment 
3.1 The General Principle 
Given two substrings S1 and S2, we can use 
their occurrence and co-occurrence counts to 
compute a correlation coefficient (such as the 
chi-square statistic, the point-wise mutual infor-
mation or the Dice coefficient).  
The basic principle of our sub-sentential align-
ment algorithm will simply be to compute a cor-
relation coefficient between every substring in a 
bi-sentence, and align the substrings with the 
highest correlation. This idea needs, however, to 
be refined. 
First, we have to take care of the indirect asso-
ciation problem. The problem, which was de-
scribed in (Melamed, 1997) in a word-to-word 
alignment context, is as follows: if e1 is the trans-
lation of f1 and f2 has a strong monolingual asso-
ciation with f1, e1 and f2 will also have a strong 
correlation. Melamed assumed that indirect asso-
ciations are weaker than direct ones, and pro-
vided a Competitive Linking Algorithm that does 
not allow for a word already aligned to be linked 
to another one. We will make the same assump-
tion and apply the same solution. So our algo-
rithm will align the substring pairs with the high-
est correlation first, and will forbid the subse-
quent alignment of substrings having a part in 
common with an already aligned substring. A 
side-effect of this procedure is that we will be 
constrained to produce a single segmentation on 
both sentences and a single alignment between 
the components of this segmentation. According 
to the application, this might be what we are 
looking for or not. But it must be noted that, 
most of the time, alignments with various 
granularities are possible, and we will only be 
able to find one of them. We will discuss the is-
sue of the granularity of the alignment in part 3.3. 
Besides, our approach implicitly considers that 
the translation of a substring is a substring (there 
are no discontinuities). This is of course not the 
case in general (for example, the English word 
?not? is usually translated in French by 
?ne?pas?). However, there is most of the time a 
granularity of alignment at which there is no dis-
continuity in the alignment components. 
Also, it is frequent that a word or a sequence 
of words in a sentence has no equivalent in the 
opposite sentence. That is why it will not be 
mandatory for our algorithm to align every ele-
ment of the sentences at all cost. If, at any point, 
the substrings that are yet to be linked have cor-
relation coefficients below a certain threshold, 
the algorithm will not go further.  
So, the algorithm can be described as follow: 
1- Compute a correlation coefficient for all the 
substrings pairs in e and f  and mark all the ele-
ments in e and f as free. 
Corpus Cache 
(cached 
substrg ) 
Allocated 
Memory 
(MB) 
CoOcc 
computed 
(per sec.) 
bisentences 
processed (per 
sec.) 
BTEC 0 22  7k 1.2 
BTEC  200 120 490k 85 
EuroParl 0 270  3k 0.4 
EuroParl 400 700 18k 1.2 
Small 
EuroParl 
0 100 4k 0.04 
Small 
EuroParl  
400 300 30k 0.3 
16
2- Among the substrings which contain only 
free element, find the pair with the highest corre-
lation. If this correlation is not above a certain 
threshold, end the algorithm. Else, output a link 
between the substrings of the pair. 
3- Mark all the elements belonging to the 
linked pair as non-free. 
4- Go back to 2 
It should be noted that correlation coefficients 
are only meaningful data is sufficiently available; 
but many substrings will appear only a couple of 
times in the corpus. That is why, in our experi-
ments we have set to zero the correlation coeffi-
cient of substring pairs that co-occur less than 5 
times (this might be a bit conservative, but the 
BTEC corpus we used being very redundant, it 
was not too much of a restriction). 
3.2 Giving a preference to bigger align-
ments. 
A problem that arose in applying the previous 
algorithm is a tendency to link incomplete sub-
strings. Typically, this happen when a substring 
S1 can be translated by two substrings S2 and S2?, 
S2 and S2? having themselves a common sub-
string. S1 will then be linked to the common part 
of S2 and S2?. For example, the English word 
?museum? has two Japanese equivalents: ??? 
and ???. In the BTEC corpus, the common 
part (?) will have a stronger association with 
?museum?, and so will be linked instead of the 
correct substring (??? or ???). 
To prevent this problem, we have tried to 
modify the correlation coefficients so that they 
slightly penalize shorter alignment. Precisely, for 
a substring pair (S1,S2), we define its area as 
?length of S1?*?length of S2?. We then multiply 
the Dice coefficient by area(S1,S2) and the chi-
square coefficient by log(area(S1,S2)+1). These 
formulas are very empiric, but they created a 
considerable improvement in our experimental 
results. 
Linking the bigger parts of the sentences first 
has another interesting effect: bigger substrings 
present less ambiguity, and so linking them first 
may prevent further ambiguities to arise. For ex-
ample, with the bi-sentence ?the cat on the 
wall?/?le chat sur le mur?. Each ?the? in the 
English sentence will have the same correlation 
with each ?le? in the French sentence, and so the 
algorithm cannot determine which ?the? corre-
spond to which ?le?. But if, for example ?the 
cat? has been previously linked to ?le chat?, 
there is no more ambiguity. 
We mentioned previously the issue of the 
granularity of alignments. These ?alignment size 
penalties? could also be used to tune the granu-
larity of the alignment produced.  
3.3 Experiments and Evaluations 
Although we made some tests to confirm that 
computation time did not prevent our algorithm 
to work with bigger corpus such as the EuroParl 
corpus, we have until now limited deeper ex-
periments to the Japanese-English BTEC Corpus. 
That is why we will only present results for 
this corpus. For comparison, we re-implemented 
the ISA (Integrated Segmentation Alignment) 
algorithm described in (Zhang, Vogel and 
Waibel, 2003). This algorithm is interesting be-
cause it is somehow similar to our own approach, 
in that it can be seen as a generalization of 
Melamed?s Competitive Linking Algorithm. It is 
also fairly easy to implement. A comparison with 
the joint probability model of Marcu and Wong 
(which can also work at the phrase/substring 
level) would have also been very interesting, but 
the difficulty of implementing and adapting the 
algorithm made us delay the experiment. 
After trying different settings, we chose to use 
chi-square statistic as the correlation coefficient 
for the ISA algorithm, and the dice coefficient 
for our own algorithm. ISA settings as well as 
the ?alignment size penalties? of our algorithm 
were also tuned to give the best results possible 
with our test set. For our algorithm, we consid-
ered word-substrings for English and characters 
substrings for Japanese. For the ISA algorithm, 
we pre-segmented the Japanese corpus, but also 
tried to apply it directly to Japanese by consider-
ing characters as words. 
Estimating the quality of an alignment is not an 
easy thing. We tried to compute a precision and a 
recall score in the following manner. Precision 
was such that: 
    Nb of correct links   
 Precision= Nb of outputted links  
Correct link are counted by manual inspection 
of the results. Appreciating what is a correct link 
is subjective; especially here, where we consider 
many-words-to-many-characters links. Overall, 
the evaluation was pretty indulgent, but tried to 
be consistent, so that the comparison would not 
be biased. 
Computing recall is more difficult: for a given 
bi-sentence, multiple alignments with different 
granularities are possible. As we are only trying 
to output one of these alignments, we cannot de-
fine easily a ?gold standard?. What we did was to 
17
count a missed link for every element that was 
not linked correctly and could have been. We 
then compute a recall measure such that: 
                Nb of correct links                 . Recall= Nb of correct links+ Nb of missed links 
These measures are not perfect and induce 
some biases in the evaluation (they tend to favor 
algorithms aligning bigger part of the sentence, 
for example), but we think they still give a good 
summary of the results we have obtained so far. 
As can be seen in the following table, our al-
gorithm performed quite well. We are far from 
the results obtained with a pre-segmentation, but 
considering the simplicity of this algorithm, we 
think these results are encouraging and justify 
our initial ideas. There is still a lot of room for 
improvement: introducing a n-gram language 
model, using multiple iterations to re-estimate 
the correlation of the substrings...  
That is why we are pretty confident that we 
can hope to compete in the end with algorithms 
using pre-segmentation. 
Also, although we did not make any thorough 
evaluation, we also applied the algorithm to a 
subset of the Europarl corpus (cf. 2.6), where 
characters where considered the base unit for 
French. The alignments were mostly satisfying 
(seemingly better than with the BTEC). But 
hardly any sub-word alignments were produced. 
Some variations on the ideas of the algorithm, 
however, allowed us to get interesting (if infre-
quent) results. For example, in the pair (?I would 
like?/ ?Je voudrais?), ?would? was aligned with 
?rais? and ?voud? with ?like?.  
4 Conclusion and future work 
In this paper we presented both a method for 
accessing the co-occurrences count for any sub-
string pair in a parallel corpus and an algorithm 
taking advantage of this method to create sub-
sentential alignments in such a corpus. 
We showed our co-occurrence counting 
method performs well with corpus commonly 
used in Statistical Machine Translation research, 
and so we think it can be a useful tool for the 
statistical processing of parallel corpora. 
Our phrase level alignment algorithm gave en-
couraging results, especially considering there 
are many possibilities for further improvement. 
In the future, we will try to improve the algo-
rithm as well as perform more extensive evalua-
tions on different language pairs. 
References 
Ralph Brown. 1997. Automated Dictionary Extraction 
for Knowledge-Free Example Based Translation, 
Proceedings of the 7th International Conference on 
Theoretical and Methodological Issues in Machine 
Translation, pp. 111-118, Santa-Fe, July 1997.  
Chris Callison-Burch, Colin Bannard and Josh Scroe-
der. 2005. Scaling Phrase-Based Statistical Ma-
chine Translation to Larger Corpora and Longer 
Phrases, Proceedings of 43rd Conference of the As-
sociation for Computational Linguistics (ACL 05), 
Ann Arbor, USA, 2005. 
Philipp Koehn. 2003. Europarl: A Multilingual Cor-
pus for Evaluation of Machine Translation, 
Draft,Unpublished.  
Manber and Myers. 1993. Suffix Array: A New 
Method for On-Line String Searches, SIAM Jour-
nal on Computing, 22(5):935-948. 
Daniel Marcu, William Wong. 2002. A Phrase-Based, 
Joint Probability Model for Statistical Machine 
Translation, Proceedings of the Conference on 
Empirical Methods in Natural Language Process-
ing , Philadelphia, USA, July 2002. 
Dan Melamed. 1997. A Word-to-Word Model of 
Translational Equivalence, Proceedings of 35th 
Conference of the Association for Computational 
Linguistics (ACL 97), Madrid, Spain, 1997. 
Franz Joseph Och, Christophe Tillmann, Hermann 
Ney. 1999. Improved Alignment Models for Statis-
tical Machine Translation. Proceedings of the joint 
conference of Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora, pp 20-
28, University Of Maryland,  
Jia Xu, Richard Zens., Hermann Ney. 2004. Do We 
Need Chinese Word Segmentation for Statistical 
Machine Translation?, Proceedings of the 3rd 
SIGHAN Workshop on Chinese Language Learn-
ing, Barcelona, Spain, pp. 122-128 , July 2004 
Ying Zhang, Stephan Vogel and Alex Waibel. 2003. 
Integrated Phrase Segmentation and Alignment al-
gorithm for Statistical Machine Translation, Pro-
ceedings of International Conference on Natural 
Language Processing and Knowledge Engineering, 
Beijing,China., October 2003 
 
 Precision Recall 
Our algorithm 
(w/o segmentation) 
78% 70% 
ISA 
 (w/o segmentation) 
55% 55% 
ISA + segmentation 98% 95% 
18
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 508?518,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Efficient retrieval of tree translation examples for
Syntax-Based Machine Translation
Fabien Cromieres
Graduate School of Informatics
Kyoto University
Kyoto, Japan
fabien@nlp.kuee.kyoto-u.ac.jp
Sadao Kurohashi
Graduate School of Informatics
Kyoto University
Kyoto, Japan
kuro@i.kyoto-u.ac.jp
Abstract
We propose an algorithm allowing to effi-
ciently retrieve example treelets in a parsed
tree database in order to allow on-the-fly ex-
traction of syntactic translation rules. We also
propose improvements of this algorithm al-
lowing several kinds of flexible matchings.
1 Introduction
The popular Example-Based (EBMT) and Statistical
Machine Translation (SMT) paradigms make use of
the translation examples provided by a parallel bilin-
gual corpus to produce new translations. Most of
these translation systems process the example data
in a similar way: The parallel sentences are first
word-aligned. Then, translation rules are extracted
from these aligned sentences. Finally, the transla-
tion rules are used in a decoding step to translate
sentences. We use the term translation rule in a very
broad sense here, as it may refer to substring pairs as
in (Koehn et al, 2003), synchronous grammar rules
as in (Chiang, 2007) or treelet pairs as in (Quirk et
al., 2005; Nakazawa and Kurohashi, 2008).
As the size of bilingual corpus grow larger, the
number of translation rules to be stored can easily
become unmanageable. As a solution to this prob-
lem in the context of phrase-based Machine Transla-
tion, (Callison-Burch et al, 2005) proposed to pre-
align the example corpora, but delay the rule extrac-
tion to the decoding stage. They showed that using
Suffix Arrays, it was possible to efficiently retrieve
all sentences containing substrings of the sentence
to be translated, and thus extract the needed trans-
lation rules on-the-fly. (Lopez, 2007) proposed an
extension of this method for retrieving discontinu-
ous substrings, making it suitable for systems such
as (Chiang, 2007).
In this paper, we propose a method to apply the
same idea to Syntax-Based SMT and EBMT (Quirk
et al, 2005; Mi et al, 2008; Nakazawa and Kuro-
hashi, 2008). Since Syntax-Based systems usually
work with the parse trees of the source-side sen-
tences, we will need to be able to retrieve effi-
ciently examples trees from fragments (treelets) of
the parse tree of the sentence we want to translate.
We will also propose extensions of this method al-
lowing more flexible matchings.
2 Overview of the method
2.1 Treelet retrieval
We first formalize the setting of this chapter by pro-
viding some definitions.
Definition 2.1 (Treelets). A treelet is a connected
subgraph of a tree. A treelet T1 is a subtreelet of an-
other treelet T2 if T1 is itself a connected subgraph
of T2. We note |T| the number of nodes in a treelet.
If |T| = 1, T is called an elementary treelet. A lin-
ear treelet is a treelet whose nodes have at most 1
child. A subtree rooted at node n of a tree T is a
treelet containing all nodes descendants of n.
Definition 2.2 (Sub- and Supertreelets). If T1 is a
subtreelet of T2 and |T1| = |T2| ? 1, we call T1
an immediate subtreelet of T2. Reciprocally, T2 is
an (immediate) supertreelet of T1. Furthermore, if
T2 and T1 are rooted at the same node in the original
tree, we say that T2 is a descending supertreelet of
T1. Otherwise it is an ascending supertreelet of T1.
508
In treelet retrieval, we are given a certain treelet
type and want to find all of the tokens of this type in
the database DB. Each token of a given treelet type
will be identified by a mapping from the node of the
treelet type to the nodes of the treelet token in the
database.
Definition 2.3 (Matching). Given a treelet T and a
tree database DB, a matching of T in DB is a func-
tion M that associate the treelet T to a tree T in
DB and every node of T to nodes of T in such a
way that: ?n ? T, label(M(n)) = label(n) and
?(n1, n2) ? T s.t n2 is a child of n1, M(n2) is a
child of M(n1).
In the common case where the siblings of a tree
are ordered, a matching must satisfy the additional
restriction: ?n1, n2 ? T, n1 <s n1 ? M(n1) <s
M(n1), where <s is the partial order relation be-
tween nodes meaning ?is a sibling and to the left of?
We note occ(T) (for ?occurrences of T ?) the set
of all possible matchings from T to DB. We will
call computing T the task of finding occ(T). If
|occ(T)| = 0, we call T an empty treelet. Computing
a query tree TQ means computing all of its treelets.
Definition 2.4 (Notations). Although treelets are
themselves trees, we will use the word treelet to
emphasize they are a subpart of a bigger tree. We
will note T a treelet, and T a tree. TQ is the query
tree we want to compute. DB will refer to the set of
trees in our database. We will use a bracket notation
to describe trees or treelets. Thus ?a(b c d(e))? is
the tree at the bottom of figure 2.
2.2 General approach
There exists already a large body of research about
tree pattern matching (Dubiner et al, 1994; Bruno
et al, 2002). However, our problem is quite differ-
ent from finding the tokens of a given treelet in a
database. We actually want to find all the tokens of
all of the treelets of a given query tree. The query
tree itself is unlikely to appear in full even once in
the database. In this respect, our approach will have
many similarities with (Callison-Burch et al, 2005)
and (Lopez, 2007), and can be seen as an extension
of these works.
The basis of the method in (Lopez, 2007) is to
look for the occurrences of continuous substrings us-
ing a Suffix Array, and then intersect them to find the
occurrences of discontinuous substrings. We will
have a similar approach with two variants. The first
variant consists in using an adaptation of the con-
cept of suffix arrays to trees, which we will call Path-
To-Root Arrays (section 3.4), that allows us to find
efficiently the set of occurrences of a linear treelet.
Occurrences of non-linear treelets can then be com-
puted by intersection. The second variant is to use an
inverted index (section 3.5). Then the occurrences of
all treelets, even the linear treelets, are computed by
intersection.
The main additional difficulty in considering trees
instead of strings is that while a string has a
quadratic number of continuous substrings, a tree
has in general an exponential number of treelets (eg.
several trillion for the dependency tree of a 70 words
sentence). There is also an exponential number of
discontinuous substrings, but (Lopez, 2007) only
consider substrings of bounded size, limiting this
problem. We will not try to bound the size of treelets
retrieved. It is therefore crucial to avoid computing
the occurrences of treelets that have no occurrences
in the database, and also to eliminate as much redun-
dant calculation as is possible.
Lopez proposes to use Prefix Trees for avoiding
any redundant or useless computation. We will use
a similar idea but with an hypergraph that we will
call ?computation hypergraph? (section 3.2). This
hypergraph will not only fit the same role as the Pre-
fix Tree of (Lopez, 2007), but also will allow us to
easily implement different search strategies for flex-
ible search (section 6).
2.3 Representing positions
Whether we use a Path-to-Root Array or an inverted
index, we will need a compact way to represent the
position of a node in a tree. It is straightforward to
define such a position for strings, but slightly less for
trees. Especially, if we consider ordered trees, we
will want to be able to compare the relative location
of the nodes by comparing their positions.
The simplest possibility is to use an integer corre-
sponding to the rank of the node in a in-order depth-
first traversal of the tree. It is then easy, for two
nodes b and c, children of a parent node a, to check
if b is on the left of c, or on the left of a, for example.
A more advanced possibility is to use a represen-
tation inspired from (Zhang et al, 2001), in which
509
the position of a node is a tuple consisting of its rank
in a preorder (ie. children last) and a postorder (chil-
dren first) depth-first traversal, and of its distance to
the root. This allows to test easily whether a node
is an ancestor of another, and their distance to each
other. This allows in turn to compute by intersec-
tion the occurrences of discontinuous treelets, much
like what is done in (Lopez, 2007) for discontinuous
strings. This is discussed in section 7.2.
3 Computing treelets incrementally
We describe here in more details how the treelets can
be efficiently computed incrementally.
3.1 Dependence of treelet computation
Let us first define how it is possible to compute a
treelet from two of its subtreelets. Let us consider
a treelet T and two treelets T1 and T2 such that
T = T1 ? T2, where, in the equality and the union,
the treelet are seen as the set of their nodes. There
are two possibilities. If T1?T2 = ?, then the root of
T1 is a child of a node of T2 or vice-versa. We then
say that T1 and T2 form a disjoint coverage (abbrevi-
ated as D-coverage) of T. If T1?T2 6= ?, we will say
that T1 and T2 form an overlapping coverage (abbre-
viated as O-coverage) of T.
Given two treelets T1 and T2 forming a cover-
age of T, we can compute occ(T) from occ(T1) and
occ(T2) by combining their matchings.
Definition 3.1 (compatibility for O-coverage). Let
T be a treelet of TQ. Let T1 and T2 be 2 treelets
forming a O-coverage of T. Let M1 ? occ(T1)
and M2 ? occ(T2). M1 and M2 are compat-
ible if and only if M1|T1?T2 = M2|T1?T2 and
I(M1|T1\T2) ? I(M2|T2\T1) = ?.
In the definition above, |S is the restriction of a func-
tion to a set S and I is the image set of a function.
If the children of a tree are ordered, we must add
the additional restriction: ?(n1, n2) ? (T1 \ T2) ?
(T2 \ T1), n1 <s n2 ?M1(n1) <s M2(n2).
Definition 3.2 (compatibility for D-coverage). Let
T1 and T2 be 2 treelets forming a D-coverage
of T. Let?s suppose that the root n2 of T2 is a
child of node n1 of T1. Let M1 ? occ(T1) and
M2 ? occ(T2). M1 and M2 are compatible if and
only if M2(n2) is a child of M1(n1).
Figure 1: A computing hypergraph for ?a(b c)?.
Definition 3.3 (intersection (?) operation). If two
matchings are compatible, we can form their union,
which is defined as (M1 ? M2)(n) = M1(n)
if n ? T1 and M2(n) else. We note
occ(T1) ? occ(T2) = {M1 ? M2 | M1 ?
occ(T1),M2 ? occ(T2) and M1 is compati-
ble with M2 }. Then,we have the property:
occ(T ) = occ(T1)? occ(T2)
In practice, the intersection operation will be im-
plemented using merge and binary merge algorithms
(Baeza-Yates and Salinger, 2005), following (Lopez,
2007).
3.2 The computation hypergraph
We have seen that it is possible to compute occ(T)
from two subtreelets forming a coverage of T. This
can be represented by a hypergraph in which nodes
are all the treelets of a given query tree, and every
pair of overlapping or adjacent treelet is linked by
an hyperedge to their union treelet. Whenever we
have computed two starting points of an hyper-edge,
we can compute its destination treelet. An example
of a small computation hypergraph is described in
figure 1.
It is very convenient to represent the incremen-
tal computation of the treelets as a traversal of this
hypergraph. First because it contributes to avoid
redundant computations: each treelet is computed
only once, even if it is used to compute several other
treelets. Also, if a query tree contains two distinct
but identical treelets, only one computation will be
done, provided the two treelets are represented by
the same node in the hypergraph. The hypergraph
also allows us to avoid computing empty treelets, as
we describe in next section. This hypergraph there-
fore has the same role for us as the prefix tree used
510
Figure 2: Inclusion DAG for the tree a(bcd(e))
in (Lopez, 2007). Of course, the hypergraph is gen-
erated on-the-fly during the traversal.
Furthermore, different traversals will define dif-
ferent computation strategies, and we will be able to
use some more advanced graph exploration methods
in section 6.
3.3 The Immediate Inclusion DAG
In many cases (but not always: see section 4.3),
the most optimal computation strategy should be
to always compute a treelet from two of its imme-
diate subtreelets. This is because the computation
time will be proportional to the size of the small-
est occurrence set of the two treelets, and thus the
?cheapest? subtreelet is always one of the immedi-
ate subtreelets. With this computation strategy, we
can replace the general computation hypergraph by
a DAG (Directed Acyclic Graph) in which every
treelet point to its immediate supertreelets. An ex-
ample is given on figure 2. We will call this DAG
the (Immediate) Inclusion DAG.
Traversals of the Inclusion DAG should be pruned
when an empty treelet is found, since all of its su-
pertreelets will also be empty. The algorithm 1 pro-
vide a general traversal of the DAG avoiding to com-
pute as many empty treelets as possible. It uses a
queue D of discovered treelets, and a data-structure
C that associate a treelet to those of its subtreelets
that have been already computed. Once a treelet T
has been computed and is found to be non empty, we
discover its immediate supertreelets TS1, TS2, . . . (if
they have not been discovered already) and add T to
C (TS1), C (TS2), . . . . The operation min(C (T)) re-
Algorithm 1: Generic DAG traversal
Add the set of precomputed treelets to D;1
while ?T ? D s.t T ? precomputed or |C(T )| > 22
do
pop T from D;3
if T in precomputed then4
occ(T )? precomputed[T ];5
else6
T1,T2=min(C (T));7
if |occ(T1)| = 0 then8
occ(T )? ?;9
else10
occ(T )? occ(T1)? occ(T2);11
for TS ? supertree(T ) do12
if occ(TS) = undef then13
Add T to C(TS);14
if |occ(T )| > 0 and TS /? D then15
Add TS to D;16
trieve the 2 subtreelets from C (T) that have the least
occurrences. If one of them is empty, we can di-
rectly conclude that T is empty. No treelet whose all
immediate subtreelets are empty is ever put in the
discovered queue, which allows us to prune most of
the empty treelets of the Inclusion DAG.
A treelet in the inclusion DAG can be computed
as soon as two of its antecedents have been com-
puted. To start the computation (or rather, ?seed?
it), it is necessary to know the occurrences of treelet
of smaller size. In the following sections 3.4 and
3.5, we describe two methods for efficiently obtain-
ing the set of occurrences of some initial treelets.
3.4 Path-to-Root Array
We present here a method to compute very effi-
ciently occ(T) when T is linear. This method is sim-
ilar to the use of Suffix Arrays (Manber and My-
ers, 1990) to find the occurrences of continuous sub-
strings in a text.
Definition 3.4 (Paths-to-Root Array). Given a la-
beled tree T and a node n ? T , the path-to-root
of n is the sequence of labels from n to the root.
The Paths-to-Root Array of a set of trees DB is the
lexicographically sorted list of the Path-to-Roots of
every node in DB.
Just as with suffixes, a path-to-root can be rep-
resented compactly by a pointer to its starting node
in DB. We then need to keep the database DB in
511
Pos PtR Pos PtR Pos PtR
1 3:4 a 8 2:2 bf 15 1:3 fba
2 1:7 a 9 1:6 ca 16 3:3 fga
3 2:6 af 10 1:8 da 17 3:2 ga
4 1:4 afba 11 2:7 daf 18 2:1 gbf
5 3:7 ba 12 3:1 ega 19 1:5 gca
6 1:2 ba 13 2:3 f 20 1:1 hba
7 2:5 baf 14 3:8 fba 21 3:5 heba
Figure 3: Path To Root Array for a set of three trees.
?Pos.? is the position of the starting point of a given path-
to-root (noted as indexOfTree:positionInTree), and PtR is
the sequence of labels on this path-to-root. The path-to-
root are sorted in lexicographic order. We can find the set
of occurrences of any linear treelet with a binary search.
For example, the treelet a(b) corresponds to the label se-
quence ?ba?. With a binary search, we find that the path-
to-root starting with ?ba? are between indexes 5 and 7.
The corresponding occurrences are then 3:7, 1:2 and 2:5.
memory to retrieve efficiently the pointed-to path-
to-root. Once the Path-to-Root Array is built, for a
linear treelet T, we can find its occurrences by a bi-
nary search of the first and last path-to-root starting
with the labels of T. See figure 3 for an example.
Memory cost is quite manageable, since we only
need 10 bytes per nodes in total. 5 bytes per pointer
in the array (tree id: 4 bytes, start position: 1 byte),
and 5 bytes per nodes to store the database in mem-
ory (label id:4 bytes, parent position: 1 byte).
All the optimization tricks proposed in (Lopez,
2007) for Suffix Arrays can be used here, espe-
cially the optimization proposed in (Zhang and Vo-
gel, 2005).
3.5 Inverted Index and Precomputation
Instead of a Path-to-Root array, one can simply use
an inverted index. The inverted index associates
with every label the set of its occurrences, each oc-
currences being represented by a tuple containing
the index of the tree, the position of the label in the
tree, and the position of the parent of the label in
the tree. Knowing the position of the parent will
allow to compute treelets of size 2 by intersection
(D-coverage). This is less effective than the Path-
To-Root Array approach, but open the possibilities
for the flexible search discussed in section 6.
Taking the idea further, we can actually con-
sider the possibility of precomputing treelets of size
greater than 1, especially if they appear frequently
in the corpus.
4 Practical implementation of the traversal
4.1 Postorder traversal
The way we choose the treelet to be popped out on
line 3 of algorithm 1 will define different computa-
tion strategies. For concreteness, we describe now a
more specific traversal. We will process treelets in
an order depending on their root node. More pre-
cisely, we consider the nodes of the query tree in the
order given by a depth-first postorder traversal of the
query tree. This way, when a treelet rooted at n is
processed, all of the treelets rooted at a descendant
of n have already been processed.
We can suppose that every processed treelet is as-
signed an index that we note #T. This allows a con-
venient recursive representation of treelets.
Definition 4.1 (Recursive representation). Let T be
a treelet rooted at node n of TQ. We note ni the
ith child of n in TQ. For all i, ti is the subtree of
T rooted at ni. We note ti = ? and #ti = 0 if T
does not contain ni. The recursive representation of
T is then: [n, (#t1,#t2, . . . ,#tm)]. We note T i the
value #ti.
For example, if TQ =?a(b c d(e))? and the treelets
?b? and ?d(e)? have been assigned the indexes 2
and 4, the recursive representation of the treelet ?a(b
d(e))? would be [a,(2,0,4)].
Algorithm 2 describes this ?postorder traversal?.
DNode is a priority queue containing the treelets
rooted atNode discovered so far. The priority queue
pop out the smallest treelets first. Line 14 maintain a
list L of processed treelets and assign the index of T
in L to #T. Line 22 keeps track of the non-empty
immediate supertreelets of every treelet through a
dictionary S. This is used in the procedure compute-
supertreelets (algorithm 3) to generate the immedi-
ate supertreelets of a treelet T given its recursive rep-
resentation. In this procedure, line 6 produces the
512
Algorithm 2: DAG traversal by query-tree pos-
torder
for Node in postorder-traversal(query-tree) do1
Telem = [Node, (0, 0, .., 0)];2
DNode ? Telem;3
while |DNode| > 0 do4
T=pop-first(DNode);5
if T in precomputed then6
occ(T )? precomputed[Node.label];7
else8
T1, T2=min(C (t));9
if |occ(T1)| = 0 then10
occ(T )? ?;11
else12
occ(T )? occ(T1)? occ(T2);13
Append T to L;14
#T ? |L|;15
for TS in compute-supertree(T,#T ) do16
Add T to C(TS);17
if |occ(T )| > 0 then18
if TS /? DNode and19
root(TS)=Node then
Add TS to D;20
for #t in C (T ) do21
Add #T to S(#t);22
descending supertreelets, and line 8 produces the as-
cending supertreelet. Figure 4 describes the content
of all these data structures for a simple run of the
algorithm.
This postorder traversal has several advantages.
A treelet is only processed once all of its immedi-
ate supertreelets have been computed, which is op-
timal to reduce the cost of the ? operation. The
way the procedure compute-supertreelets discover
supertreelets from the info in S has also several
benefit. One is that, by not adding empty treelets
(line 18) to S , we naturally prevent the discovery
of larger empty treelets. Similarly, in the next sec-
tion, we will be able to prevent the discovery of non-
maximal treelets by modifying S . Modifications of
compute-supertreelets will also allow different kind
of retrieval in section 6.
4.2 Pruning non-maximal treelets
We now try to address another aspect of the over-
whelming number of potential treelets in a query
tree. As we said, in most practical cases, most of the
larger treelets in a query tree will be empty. Still, it is
Algorithm 3: compute-supertrees
Input: T ,#T
Output: lst: list of immediate supertreelets of T
m? |root(T)|;1
for i in 1 . . .m do2
for #TS in S(#T i) do3
if root(#TS) 6= root(T) then4
Tnew ? [root(T), T 0, ..#T ?, . . . , Tm];5
Append Tnew to lst;6
Tnew ? [parent(root(T)), (0, . . . ,#T, . . . , 0)];7
Append Tnew to lst;8
possible that some tree exactly identical to the query
tree (or some tree having a very large treelet in com-
mon with the query tree) do exist in the database.
This case is obviously a best case for translation,
but unfortunately could be a worst-case for our al-
gorithm, as it means that all of the (possibly trillions
of) treelets of the query tree will be computed.
To solve this issue, we try to consider a concept
analogous to that of maximal substring, or substring
class, found in Suffix Trees and Suffix Arrays (Ya-
mamoto and Church, 2001). The idea is that in most
cases where a query tree is ?full? (that is all of its
treelets are not empty), most of the larger treelets
will share the same occurrences (in the database
trees that are very similar to the query tree). We for-
malize this as follow:
Definition 4.2 (domination and maximal treelets).
Let T1 be a subtreelet of T2. If for every matching
M1 of occ(T1), there exist a matching M2 of
occ(T2) such that M2|T1 = M1, we say that T1 is
dominated by T2. A treelet is maximal if it is not
dominated by any other treelet.
If T1 is dominated by T2, it means that all occur-
rences of T1 are actually part of an occurrence of
T2. We will therefore be, in general, more interested
by the larger treelet T2 and can prune as many non-
maximal treelets as we want in the traversal. The key
point is that the algorithm has to avoid discovering
most non maximal treelets.The algorithm 2 can eas-
ily be modified to do this. We will use the following
property.
Property 4.1. Given k treelets T1 . . . Tk with k dis-
tinct roots, all the roots being children of a same
node n. We note n(T1 . . . Tk) the treelet whose root
is n, and for which the k subtrees rooted at the k
513
T d e b b(d) [Empty] b(e) b(d e) [Empty] c a a(b) a(b(e)) a(c) a(b c) a(b(e) c)
# 1 2 3 4 5 6 7 8 9 10 11 12 13
R d e b(..) b(1.) b(.2) b(1 2) c a(..) a(3.) a(5.) a(.7) a(3 7) a(5 7)
C - - - 1,3 2,3 4,5 - - 8,3 5,9 7,8 9,11 10,12
S - - 5 - - - - 9,11 10,12 13 12 13 -
Figure 4: A run of the algorithm 2, for the query tree a(b(d e) c). The row ?T? represents the treelets in the order
they are discovered. The row ?#? is the index #T, and the row ?R? is the recursive representation of the treelet. Also
represented are the content of C and S at the end of the computation. When a treelet is poped out of DNode, occ(T) is
computed from the treelets listed in C (T ). If occ(T) is not empty, the entries of the immediate subtreelets of T in S
are updated with #T. We suppose here that |occ( b(d))|=0. Then, b(d e) is marked as empty and neither b(d) nor b(d e)
are added to the entries of their subtreelets in S. This way, when considering treelets rooted at the upper node ?a?, the
algorithm will not discover any of the treelets containing b(d).
children of n are T1 . . . Tk. Let us further suppose
that for all i, Ti is dominated by a descending su-
pertreelet T di (with the possibility that Ti = T di ).
Then n(T1 . . . Tk) is dominated by n(T d1 . . . T dk ).
For example, if b(c) is dominated by b(c d), then
a(b(c) e) will be dominated by a(b(c d) e).
In algorithm 2, after processing each node, we
proceed to a cleaning of the S dictionary in the fol-
lowing way: for every treelet T (considering the
treelets by increasing size) that is dominated by
one of its supertreelets TS ? S(T) and for every
subtreelet T ? of T such that T ? S(T ?), we re-
place T by TS in S(T ?). The procedure compute-
supertreelets, when called during the processing of
the parent node, will thus skip all of the treelets that
are ?trivially? dominated according to property 4.1.
Let?s note that testing for the domination of a
treelet T by one of its supertrelets TS is not a matter
of just testing if |occ(T)| = |occ(TS)|, as would be
the case with substring: a treelet can have less oc-
currences than one of its supertreelets (eg. b(a) has
more occurrences than b in b(a a) ). An efficient way
is to first check that the two treelets occurs in the
same number of sentences, then confirm this with a
systematic check of the definition.
4.3 The case of constituent trees
We have focused our experiments on dependency
trees, but the method can be applied to any tree.
However, the computations strategies we have used
might not be optimal for all kind of trees. In a de-
pendency tree, nodes are labeled by words and most
non-elementary treelets have a small number of oc-
currences. In a constituent tree, many treelets con-
taining only internal nodes have a high frequency
and will be expensive to compute.
If we have enough memory, we can solve this by
precomputing the most common (and therefore ex-
pensive) treelets.
However, it is usually not very interesting to re-
trieve all the occurrences of treelets such as ?NP(Det
NN)? in the context of aMT system. Such very com-
mon pattern are best treated by some pre-computed
rules. What is interesting is the retrieval of lexical-
ized rules. More precisely, we want to retrieve ef-
ficiently treelets containing at least one leaf of the
query tree. Therefore, an alternative computation
strategy would only explore treelets containing at
least one terminal node. We would thus compute
successively ?dog?, ?NN(dog)? ?NP(NN(dog))?,
?NP(Det NN(dog))?, etc.
4.4 Complexity
Processing time will be mainly dependent on two
factors: the number of treelets in a query tree that
need to be computed, and the average time to com-
pute a treelet.
Let NC be the size of the corpus. It can be shown
quite easily that the time needed to compute a treelet
with our method is proportional to its number of oc-
currences, which is itself growing as O(NC).
Let m be the size of the query tree. The number
of treelets needing to be computed is, in the worst
case, exponential in m. In practice, the only case
where most of the treelets are non-empty is when the
database contains trees similar to the query tree in
the database, and this is handled by the modification
of the algorithm is section 4.2. In other cases, most
of the treelets are empty, and empirically, we find
that the number of non-empty treelets in a query tree
514
Database size (#nodes) 6M 60M
Largest non-empty treelet size 4.6 8.7
Processing time (PtR Array) 0.02 s 0.7 s
Processing time (Inv. Index) 0.02 s 0.9 s
Size on disk 40 MB 500 MB
Figure 5: Performances averaged on 100 sentences.
grows approximately as O(m ?N0.5C ). It is also pos-
sible to bound the size of the retrieved treelets (only
retrieving treelets with less than 10 nodes, for exam-
ple), similarly to what is done in (Lopez, 2007). The
number of treelets will then only grows as O(m).
The total processing time of a given query tree
will therefore be on the order of O(m ? N1.5C ) (or
O(m ? NC) if we bound the treelet size). The fact
that this give a complexity worse than linear with
respect to the database size might seem a concern,
but this is actually only because we are retrieving
more and more different types of treelets. The cost
of retrieving one treelet remain linear with respect to
the size of the corpus. We empirically find that even
for very large values of NC , processing time remain
very reasonable (see next section).
It should be also noted that the constant hid-
den in the big-O notation can be (almost) arbitrar-
ily reduced by precomputing more and more of the
most common (and more expensive) treelets (a time-
memory trade-off).
5 Experiments
We conducted experiments on a large database of
2.9 million automatically parsed dependency trees,
with a total of nearly 60 million nodes1. The largest
trees in the database have around 100 nodes. In or-
der to see how performance scale with the size of the
database, we also used a smaller subset of 230,000
trees containing near 6 million nodes.
We computed, using our algorithm, 100 randomly
selected query trees having from 10 to 70 nodes,
with an average of 27 nodes per tree. Table 5
shows the average performances per sentence. Con-
sidering the huge size of the database, a process-
1This database was an aggregate of several Japanese-English
corpora, notably the Yomiuri newspaper corpus (Utiyama and
Isahara, 2003) and the JST paper abstract corpus created at
NICT(www.nict.go.jp) through (Utiyama and Isahara, 2007).
Method Treelet Our
dictionary method
Disk space used 23 GB 500 MB
BLEU 11.6% 12.0%
Figure 6: Comparison with a dictionary-based baseline
(performances averaged over 100 sentences).
ing time below 1 second seems reasonable. The
increase in processing time between the small and
the large database is in line with the explanations
of section 4.4. Path-to-Root Arrays are slightly bet-
ter than Inverted indexes (we suspect a better im-
plementation could increase the difference further).
Both methods use up about the same disk space:
around 500MB. We also find that the approach of
section 4.2 brings virtually no overhead and gives
similar performances whether the query tree is in the
database or not (effectively reducing the worst-case
computation time from days to seconds).
We also conducted a small English-to-Japanese
translation experiment with a simple translation sys-
tem using Synchronous Tree Substitution Grammars
(STSG) for translating dependency trees. The sys-
tem we used is still in an experimental state and
probably not quite at the state-of-the-art level yet.
However, we considered it was good enough for our
purpose, since we mainly want to test our algorithm
is a practical way. As a baseline, from our cor-
pus of 2.9 millions dependency trees, we automat-
ically extracted STSG rules of size smaller than 6
and stored them in a database, considering that ex-
tracting rules of larger sizes would lead to an un-
manageable database size. We compared MT results
using only the rules of size smaller than 6 to using
all the rules computed on-the-fly after treelet retriev-
ing by our method. These results are summarized on
figure 6.
6 Flexible matching
We now describe an extension of the algorithm for
approximate matching of treelets. We consider that
each node of the query tree and database is labeled
by 2 labels (or more) of different generality. For
concreteness, let?s consider dependency trees whose
nodes are labeled by words and the Part-Of-Speech
(POS) of these words. We want to retrieve treelets
515
that match by word or POS with the query tree.
6.1 Processing multi-Label trees
To do this, the inverted index will just need to
include entries for both words and POS. For ex-
ample, the dependency tree ?likes,V:1 (Paul,N:0
Salmon,N:2 (and,CC:3 (Tuna,N:4)))? would pro-
duce the following (node,parents) entries in the in-
verted index: {N:[(0,1) (2,1) (4,3)], Paul:[(0,1)],
Salmon:[(2,1)],. . . }. This allows to search for a
treelet containing any combination of labels, like
?likes(N Salmon(CC(N)))?.
We actually want to compute all of the treelets of
a query tree TQ labeled by words and POS (meaning
each node can be matched by either word or POS).
We can compute TQ without redundant computa-
tions by slightly modifying the algorithm 2. First,
we modify the recursive representation of a treelet
so that it also includes the chosen label of its root
node. Then, the only modifications needed in algo-
rithm 2 are the following: 1- at initialization (line 3),
the elementary treelets corresponding to every pos-
sible labels are added to the discovered treelets set
D; 2- in procedure compute-supertrees, at line 8, we
generate one ascending supertreelet per label.
6.2 Weighted search
While the previous method would allow us to com-
pute as efficiently as possible all the treelets in-
cluded in a multi-labeled query tree, there is still
a problem: even avoiding redundant computations,
the number of treelets to compute can be huge, since
we are computing all combinations of labels. For
each treelet of size m we would have had in a single
label query tree, we now virtually have 2m treelets.
Therefore, it is not reasonable in general to try to
compute all these treelets.
However, we are not really interested in comput-
ing all possible treelets. In our case, the POS la-
bels allow us to retrieve larger examples when none
containing only words would be available. But we
still prefer to find examples matched by words rather
than by POS. We therefore need to tell the algorithm
that some treelets are more important that some oth-
ers. While we have used the Computation Hypertree
representation to compute treelets efficiently, we can
also use it to prioritize the treelets we want to com-
pute. This is easily implemented by giving a weight
POS matchings Without With
Processing time 0.9 s 22 s
Largest non-empty treelet size 8.7 11.4
Treelets of size>8 0.4 102
BLEU 12.0% 12.1%
Figure 7: Effect of POS-matching
to every treelet. We can then modify our traversal
strategy of the Inclusion DAG to compute treelets
having the biggest weights first: we just need to
specify that the treelet popped out on line 3 is the
treelet with the highest score (more generally, we
could consider a A* search).
6.3 Experiments
Using the above ideas, we have made some experi-
ments for computing query dependency trees labeled
with both words and POS. We score the treelets by
giving them a penalty of -1 for each POS they con-
tain, and stop the search when all remaining treelets
have a score lower than -2 (in other words, treelets
are allowed at most 2 POS-matchings). We also re-
quire POS-matched nodes to be non-adjacent.
We only have some small modifications to do to
algorithm 2. In line 3 of algorithm 2, elementary
treelets are assigned a weight of 0 or -1 depend-
ing on whether their label is a word or POS. Line 5
is replaced by ?pop the first treelet with minimal
weight and break the loop if the minimal weight is
inferior to -2?. In compute-supertreelets, we give a
weight to the generated supertreelets by combining
the weights of the child treelets.
Table 7 shows the increase in the size of the
biggest non-empty treelets when allowing 2 nodes
to be matched by POS. It also shows the impact on
BLEU score of using these additional treelets for on-
the-fly rule generation in our simpleMT system. Im-
provement on BLEU is limited, but it might be due
to a very experimental handling of approximately
matched treelet examples in our MT system.
The computation time, while manageable, was
much slower than in the one-label case. This is due
to the increased number of treelets to be computed,
and to the fact that POS-labeled elementary treelets
have a high number of occurrences. It would be
more efficient to use more specific labeling (e.g V-
516
Figure 8: A packed forest.
mvt for verbs of movement instead of V).
7 Additional extensions
We briefly discuss here some additional extensions
to our algorithm that we will not detail for lack of
room and practical experiments.
7.1 Packed forest
Due to parsing ambiguities and automatic parsers
errors, it is often useful to use multiple parses of
a given sentence. These parses can be represented
by a packed forest such as the one in figure 8. Our
method allows the use of packed representation of
both the query tree and the database.
For the inverted index, the only difference is
that now, an occurrence of a label can have more
than one parent. For example, the inverted in-
dex of a database containing the packed forest
of figure 8 would contain the following entries:
{held: [(1,10a),(1,10b)], NP: [(6,9),(7,9),(9,10a)],
VP:[(10,N)], PP:[(8,10b)], a:[(2,6)], talk:[(3,6)],
with:[(4,7) (4,8)], Sharon:[(5,7) (5,8)]}. Where 10a
and 10b are some kind of virtual position that help to
specify that held and NP8 belong to the same chil-
dren list. We could also include a cost on edges
in the inverted index, which would allow to prune
matchings to unlikely parses.
The inverted index can now be used to search in
the trees contained in a packed forest database with-
out any modification. Modifications to the algorithm
in order to handle a packed forest query are similar
to the ones developed in section 6.
7.2 Discontinuous treelets
As we discussed in section 2.3, using a representa-
tion for the position of every node similar to (Zhang
et al, 2001), it is possible to determine the distance
and ancestor relationship of two nodes by just com-
paring their positions. This opens the possibility of
computing the occurrences of discontinuous treelets
in much the same way as is done in (Lopez, 2007)
for discontinuous substrings. We have not studied
this aspect in depth yet, especially since we are not
aware of any MT system making use of discontin-
uous syntax tree examples. This is nevertheless an
interesting future possibility.
8 Related work
As we previously mentioned, (Lopez, 2007) and
(Callison-Burch et al, 2005) propose a method sim-
ilar to ours for the string case.
We are not aware of previous proposals for ef-
ficient on-the-fly retrieving of translation examples
in the case of Syntax-Based Machine Translation.
Among the works involving rule precomputation,
(Zhang et al, 2009) describes a method for effi-
ciently matching precomputed treelets rules. These
rules are organized in a kind of prefix tree that al-
lows efficient matching of packed forests. (Liu et al,
2006) also propose a greedy algorithm for matching
TSC rules to a query tree.
9 Conclusion and future work
We have presented a method for efficiently retriev-
ing examples of treelets contained in a query tree,
thus allowing on-the-fly computation of translation
rules for Syntax-Based systems. We did this by
building on approaches previously proposed for the
case of string examples, proposing an adaptation of
the concept of Suffix Arrays to trees, and formaliz-
ing computation as the traversal of an hypergraph.
This hypergraph allows us to easily formalize dif-
ferent computation strategy, and adapt the methods
to flexible matchings. We still have a lot to do with
respect to improving our implementation, exploring
the different possibilities offered by this framework
and proceeding to more experiments.
Acknowledgments
We thank the anonymous reviewers for their useful
comments.
517
References
R. Baeza-Yates and A. Salinger. 2005. Experimental
analysis of a fast intersection algorithm for sorted se-
quences. In String Processing and Information Re-
trieval, page 1324.
N. Bruno, N. Koudas, and D. Srivastava. 2002. Holis-
tic twig joins: optimal XML pattern matching. In
Proceedings of the 2002 ACM SIGMOD international
conference on Management of data, page 310321.
C. Callison-Burch, C. Bannard, and J. Schroeder. 2005.
Scaling phrase-based statistical machine translation to
larger corpora and longer phrases. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics, pages 255?262. Association for
Computational Linguistics Morristown, NJ, USA.
David Chiang. 2007. Hierarchical Phrase-Based trans-
lation. Computational Linguistics, 33(2):201?228,
June.
M. Dubiner, Z. Galil, and E. Magen. 1994. Faster
tree pattern matching. Journal of the ACM (JACM),
41(2):205213.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of HLT-
NAACL, pages 48?54. Association for Computational
Linguistics.
Z. Liu, H. Wang, and H. Wu. 2006. Example-based
machine translation based on tree?string correspon-
dence and statistical generation. Machine translation,
20(1):25?41.
A. Lopez. 2007. Hierarchical phrase-based translation
with suffix arrays. In Proc. of EMNLP-CoNLL, page
976985.
U. Manber and G. Myers. 1990. Suffix arrays: a
new method for on-line string searches. In Proceed-
ings of the first annual ACM-SIAM symposium on Dis-
crete algorithms, pages 319?327, San Francisco, CA,
USA. Society for Industrial and Applied Mathematics
Philadelphia, PA, USA.
H. Mi, L. Huang, and Q. Liu. 2008. Forest based trans-
lation. Proceedings of ACL-08: HLT, page 192199.
Toshiaki Nakazawa and Sadao Kurohashi. 2008. Syn-
tactical EBMT system for NTCIR-7 patent translation
task. In Proceedings of NTCIR-7 Workshop Meeting,
Tokyo, Japon.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, page 279.
M. Utiyama and H. Isahara. 2003. Reliable measures for
aligning japanese-english news articles and sentences.
In Proceedings of ACL, pages 72?79, Sapporo,Japon.
M. Utiyama and H. Isahara. 2007. A japanese-english
patent parallel corpus. In MT summit XI, pages 475?
482.
M. Yamamoto and K. W. Church. 2001. Using suffix
arrays to compute term frequency and document fre-
quency for all substrings in a corpus. Computational
Linguistics, 27(1):1?30.
Y. Zhang and S. Vogel. 2005. An efficient phrase-to-
phrase alignment model for arbitrarily long phrase and
large corpora. In Proceedings of EAMT, pages 294?
301, Budapest, Hungary.
C. Zhang, J. Naughton, D. DeWitt, Q. Luo, and
G. Lohman. 2001. On supporting containment queries
in relational database management systems. In Pro-
ceedings of the 2001 ACM SIGMOD international
conference on Management of data, page 425436.
H. Zhang, M. Zhang, H. Li, and Chew Lim Tan. 2009.
Fast translation rule matching for syntax-based statis-
tical machine translation. In Proc. of EMNLP, pages
1037?1045.
518
