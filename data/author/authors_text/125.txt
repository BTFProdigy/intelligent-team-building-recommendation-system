A word-grammar based morl)hoh)gieal nalyzer 
for agglutinative languages 
Aduriz 1.+, Agirre E., Aldezabal I., Alegria I., Arregi X., Arriohl J. M., Artola X., Gojenola K., 
Marilxalar A., Sarasola K., Urkia M.+ 
l)ept, of Colllptiier 1Aulgtlages and Systems, University of lhe Basqtlo Cotlnlry, 64.9 P. K., 
E-20080 1)onostia, Basque Counh'y 
tUZEI, Aldapeta 20, E-20009 1)onostia, Basque Country 
+Universidad de Barcelona, Grin Vfii de Isis Cortes CalaiallaS, 585, E-08007 Flarcelona 
j ipgogak @ si.elm, es. 
Abst rac l  
Agglutinative languages presenl rich 
morphology and for sonic applications 
they lleed deep analysis at word level. 
Tile work here presenled proposes a 
model for designing a full nlorpho- 
logical analyzer. 
The model integrates lhe two-level 
fornlalisnl alld a ullificalion-I)asod 
fornialisni. In contrast to other works, 
we propose to separate the treatment of 
sequential and non-sequetTtial mou)ho- 
lactic constraints. Sequential constraints 
are applied in lhe seglllenlalion phase, 
and non-seqtlontial OlleS ill the filial 
feature-combination phase. Early appli- 
cation of sequential nlorpholactic 
coilsli'aiills during tile segnloillaiioi/ 
process nlakes feasible :,ill officienl 
iinplenleilialion of tile full morpho- 
logical analyzer. 
The result of lhis research has been tile 
design and imi)len~entation of a full 
nlorphosynlactic analysis procedure for 
each word in unrestricted Basque texts. 
I n t roduct ion  
Morphological analysis of woMs is a basic 
tool for automatic language processing, and 
indispensable when dealing willl highly 
agglutinative languages like Basque (Aduriz el 
al., 98b). In lhis conlext, some applications, 
like spelling corfeclion, do ilOI need illOl'e lhan 
the seglllOlltation of each word inlo its 
different COlllponenl nlorphellles alollg with 
their morphological information, ltowever, 
there are oiher applications such as lemnializa- 
tion, lagging, phrase recognition, and 
delernlinaiion of clause boundaries (Aduriz el 
al., 95), which need an additional global 
morphological i)arsing j of the whole word. 
Such a complete nlorphological analyzer has 
lo consider three main aspects (l~,ilchie et al, 
92; Sproal, 92): 
1 Morl)hographenfics (also called morpho- 
phonology). This ternl covers orthographic 
variations that occur when linking 
I l lOfphellleS. 
2) morpholactics. Specil'ication of which 
nlorphenles can or cannot combine with 
each other lo form wflid words. 
3) Feature-combination. Specification of how 
these lnorphemes can be grouped and how 
their nlorphosyntactic features can be 
comlfined. 
The system here presented adopts, oil the one 
hand, tile lwo-level fornlalisnl to deal with 
morphogralfilemics and sequential morl)ho- 
lactics (Alegria el al., 96) and, on the other 
hand, a unification-based woM-grammar 2 to 
combine the grammatical information defined 
in nlorphemes and to  tackle complex 
nlorphotactics. This design allowed us to 
develop a full coverage analyzer that processes 
efl'iciently unrestricted texts in Basque. 
The remainder of tills paper is organized sis 
follows. After a brief' description of Basque 
nlorphology, section 2 describes tile 
architecture for morphological processing, 
where the morphosynlactic omponent is 
included. Section 3 specifies tile plaenomena 
covered by the analyzer, explains its desigi~ 
criteria, alld presents implementation and 
ewthialion details. Section d compares file 
I This has also been called mo*7)hOSh,ntactic 
parsitlg. When we use lhc \[(fill #11017~\]lOSyltl~/X WC 
will always refer to il~c lficrarchical structure at 
woM level, conlbining morphology and synlax. 
2 '\]'\]lt3 \[IDl'll\] WOl'd-gF(lllllllUl" should not be confused 
with the synlaclic lilcory presented in (Hudson, 84). 
system with previous works. Finally, the paper 
ends with some concluding renmrks. 
1 Brief description of Basque 
morphology 
These are the most important features of 
Basque morphology (Alegria et al, 96): 
? As prepositional functions are realized by 
case suffixes inside word-fornls, Basque 
presents a relatively high power to generate 
inflected word-forms. For instance, froth a 
single noun a minimum of 135 inflected 
forms can be generated. Therefore, the 
number of simple word-forms covered by 
the current 70,000 dictionary entries woukl 
not be less than 10 million. 
? 77 of the inflected forms are simple 
combinations of number, determination, 
and case marks, not capable of further 
inflection, but the other 58 word-forms 
ending in one of the two possible genitives 
(possessive and locative) can be further 
inflected with the 135 morphemes. This 
kind of recursive construction reveals a 
noun ellipsis inside a noun phrase and 
could be theoretically exteuded ad 
infinitum; however, in practice it is not 
usual to fiud more than two levels of this 
kind of recursion in a word-form. Taking 
into account a single level of noun ellipsis, 
the number of word-forum coukl be 
estimated over half a billion. 
? Verbs offer a lot of grammatical 
information. A verb tbrln conveys informa- 
tion about the subject, the two objects, as 
well as the tense and aspect. For example: 
diotsut (Eng.: 1 am telling you something). 
o Word-formation is very productive in 
Basque. It is very usual to create new 
compounds as well as derivatives. 
As a result of this wealth of infornmtion 
contained within word-forms, complex struc- 
tures have to be built to represent complete 
morphological information at word level. 
2 An architecture for the full 
morphological ana lyzer  
The framework we propose for the 
morphological treatment is shown in Figure 1. 
The morphological nalyzer is the fiont-end to 
all present applications for the processing of 
Basque texts. It is composed of two modules: 
the segmentation module and the 
morphosyntactic analyzer. 
conformant .................. ~ U~atabas N TEZ-conf~ 
\[Segmentation module 
____~| HorphograDhemics 
Morphotactics I 
TEI-FS .............. ~ ~ ~ ~  ~ - p ~  
conformant Cegmented TexN 
Morphosyntactic 
analyzer 
Feature- combination 
Morphotactics II 
TEI-FS \] .............. ~ actically 
Lermnatization, linguistic Analysis tagging tools 
Figure 1. Architecture 1"o1" morphological processing. 
The segmentation ,nodule was previously 
implemented in (Alegria et al, 96). This 
system applies two-level morphology 
(Koskenniemi, 83) for the morphological 
description and obtains, for each word, its 
possible segmentations (one or many) into 
component morphemes. The two-level system 
has the following components: 
? A set of 24 morphograf~hemic rules, 
compiled into transducers (Karttunen, 94). 
? A lexicon made up of around 70,000 items, 
grouped into 120 sublexicons and stored in 
a general lexical database (Aduriz et al, 
98a). 
This module has full coverage of free-running 
texts in Basque, giving an average number of 
2.63 different analyses per word. The result is 
the set of possible morphological segmenta- 
tions of a word, where each morpheme is 
associated with its corresponding features in 
the lexicon: part of speech (POS), 
subcategory, declension case, number, 
definiteness, as well as syntactic function and 
some semantic features. Therefore, the output 
of the segmeutation phase is very rich, as 
shown in Figure 2 with the word amarengan 
(Eng.: on the mother). 
grammar 
mother) 
POS noun) 
subc~t common 
:count: +) 
(an imate  +) 
(nleasurable "-) 
aren 
(of life) 
(POS decl-suffix) 
(definite +) 
(number sing) 
(case genitive) 
(synt-f @nouncomp) 
J gan \] 
(o.1 / 
(POS decl-suf fix) I 
(case inossivo) \] 
(synt-f @adverbial)I 
=> 
amarengan 
(o. the mother) 
POS noun) 
subcat common) 
number sing) 
definite +) 
case inessive) 
count +) 
animate +) 
measurable -) 
synt-f @adverbial) 
iq:e, ure 2. Morphosynlactic analysis eof (unureugun (l{ng.: (m 
The architecture is a modular envhoument that 
allows different ypes of output depending on 
the desired level of analysis. The foundation of 
the architecture lies in the fact lhat TEI- 
confommnt SGML has been adopted for the 
comnmnication allloIlg modules (Ide and 
VCFOIIiS, 95). l~'eature shucluleS coded 
accoMing TIU are used to represent linguistic 
information, illcluding tile input mM outl)ut of 
the morplaological analyzer. This reprcscnta- 
tion rambles the use of SGML-aware parsers 
and tools, and Call he easily filtered into 
different formats (Artola et ill., 00). 
3 Word level morl)hosyntactic analysis 
This section Hrst presents the l~henomena lhat 
must be covered by the morphosyntactic 
analyzer, then explains ils design criteria, and 
finally shows implementation and ewfluation 
details. 
3.1 Phenomena covered by the analyzer 
There are several features that emphasized the 
need of morphosyntactic almlysis in order to 
build up word level information: 
I) Multiplicity of values for the same feature 
in successive morphemes. In the analysis 
of Figure 2 there are two different values 
for the POS (noun and declension suffix), 
two for the case (genitive and inessive), 
and two for the syntactic function 
(@nouncomp and @adverbial). Multiple 
values at moq~hemc-level will have to be 
merged to obtain the word level infer 
mation. 
2) Words with phrase structure. Although the 
segmentation is done for isolated words, 
independently of context, in several cases 
3 l?calurc wtlues starling with the "@" character 
correspond to syntactic functions, like @noullcomp 
(norm complement) or @adverbial. 
the mother) 
tile resulting structure is oquiwflent o the 
aualysis of a phrase, as can be seen i, 
Figure 2. 111 this case, although there are 
two different cases (genitive and inessive), 
lhe case of the full word-form is simply 
inessive. 
3) Noun ellipsis inside word-lbrms. A noun 
ellipsis can occur withi, the word 
(oceasi(mally more than once). This 
information must be made explicit in the 
resulting analysis. For example, Figure 3 
shows the analysis of a single word-forln 
like diotsudumtrel&z (Eng.: with what I am 
lelling you). The first line shows its 
segmentation into four morphemes 
(die tsut+en+ 0 +arekin). The feature 
compl ill tile final analysis conveys the 
information for the verb (l um lelliHg you), 
that carries information about pc'rson, 
number and case o1' subject, object and 
indirect object. The feature comp2 
represents an elided noun and its 
declension stfffix (with). 
4) l)erivation and composition are productive 
in Basque. There arc more than 80 deri- 
w/tion morphemes (especially suffixes) 
intensively used in word-fornlatioll. 
3.2 Design of the word-grammar 
The need to impose hierarchical structure upon 
sequences of morphemes and to build complex 
constructions from them forced us to choose a 
unil'ication mechanism. This task is currently 
unsolwlble using finite-state techniques, clue to 
the growth in size of the resulting network 
(Beesley, 98). We have developed a unifica- 
tion based word-grammar, where each rule 
combines information flom different 
mot+lJlemes giving as a result a feature 
structure for each interpretation of a word- 
fol'nl, treating the previously mentioned cases. 
3 
diotsut 
I am tellh,g you) 
POS verb) 
(tense present) 
(pers-ergative is)\[ 
(pets-dative 2s) 
(pers-absol 3s) 
en 
(what) 
(POS relation) 
(subcat subord) 
(relator relative 
(synt-f @rel-clause 
0 
() 
(POS ellipsis) 
arekin 
(wire) 
(POS declension-suffix)) 
(case sociative) 
(number sing) 
(definite +) 
(synt-f @adverbial) 
=> diotsudanarekin (wi~ what lamtel l ingyou) 
(POS verb-noun_ellipsis) 
(case sociative) 
(number sing) 
(definite +) 
(synt-f @adverbial) 
(compl (POS verb) 
(subcat subord) 
(relator relative) 
(synt-f @tel-clause) 
(tense present) 
(pers-ergative is) 
(pets-dative 2s) 
(pers-absol 3s)) 
(comp2 (POS noun) 
(subcat common) 
(number sing) 
(definite+) 
(synt-f @adverbial)) 
Figure 3. Morphosyntactic analysis of diotxudanarekin (Eng.: with what I am tellittg you) 
As a consequence of the rich naorphology of 
Basque we decided to control morphotactic 
phenomena, as much as possible, in the 
morphological segmentation phase. Alterna- 
tively, a model with minimal morphotactic 
treatment (Ritchie et al, 92) would produce 
too many possible analyses after segmentation, 
which should be reiected in a second phase. 
Therefore, we propose to separate sequential 
morphotactics (i.e., which sequences of 
morphemes can or cannot combine with each 
other to form valid words), which will be 
recognized by the two-level system by means 
of continuation classes, and non-sequential 
morphotactics like long-distance dependencies 
that will be controlled by the word-gmnunar. 
The general linguistic principles used to define 
unification equations in the word-grannnar 
rules are the following: 
1) Information risen from the lemma. The 
POS and semantic features are risen flom 
the lemnm. This principle is applied to 
common nouns, adjectives and adverbs. 
The lemma also gives the mnnber in 
proper nouns, pronouns and determiners 
(see Figure 2). 
2) lnfornmtion risen from case suffixes. 
Simple case suffixes provide information 
on declension case, number and syntactic 
function. For example, tile singular 
genitive case is given by the suffix -tell in 
ama+ren (Eng.: of the mother). For 
compound case suffixes the number and 
determination are taken from the first 
suffix and the case from the second one. 
First, both suffixes are joined and after 
that they are attached to the lemma. 
3) Noun ellipsis. When an ellipsis occurs, the 
POS of the whole word-form is expressed 
by a compound, which indicates both the 
presence of the ellipsis (always a noun) 
and the main POS of the word. 
For instance, the resulting POS is 
verb-noun_e l l ips is  when a noun- 
ellipsis occurs after a verb. All the 
information corresponding to both units, 
the explicit lemma and the elided one, is 
stored (see Figure 3). 
4) Subordination morl~hemes. When a 
subordination morpheme is attached to a 
verb, the verb POS and its featm'es are 
risen as well as the subordhmte relation 
and the syntactic fnnction conveyed by the 
naorpheme. 
5) Degree morphemes attached to adjectives, 
past participles and adverbs. The POS and 
diotsudan 
(diotsut + en) 
(POS verb) 
(tense present) 
(relator relative) 
/ \ / 
diotsut 
(POS verb) 
(tense present 
diotsudanarekin 
(diotsut + en -I 0 + arekin) 
(POS verb-noun_ell ipsis) 
(case sociative) 
arekin 
(0 + arekin) 
(POS noun ellipsis) 
(case sociative) 
en 
(pos 
? . . 
o 
(POS e l l ips i s  re la t ion)  
arekin 
(case sociative) 
Figure 4. Parse tree for diotmuhmarekitl (Eng.: with what I am lellittg yott) 
main features arc taken from the lemma 
and the features corresponding to the 
degrees of comparison (comparative, 
supcrhttive) aft taken from the degree 
morphemes. 
6) l)efiwttion. 1)miwttion suffixes select tile 
POS of the base-form to create the deriw> 
tive anti in most cases to change its POS. 
For instance, the suffix -garri (Eng.: -able) 
is applied to verbs and the derived word is 
an adjective. When the derived form is 
obtained by means o1' a prefix, it does not 
change the POS of the base-form. In both 
cases the morphosyntactic rules add a new 
feature representing the structure of tile 
word as a derivative (root and affixes). 
7) Composition. At the moment, we only 
treat the most freqttent kind of 
composition (noun-noun). Since Basque is 
syntactically characterized as a right-head 
hmguage, the main information of the 
compound is taken from the second 
element. 
8) Order of application of the mofphosyn- 
tactic phenomena. When several morpho- 
syntactic phenomena are applied to the 
same leml l la ,  so as to eliminate 
nonsensical readings, the natural order to 
consider them in Basque is the following: 
lemmas, derbation prefixes, deriwltion 
suffixes, composition and inflection (see 
Figure 4). 
9) Morl)hotactic constraints. Elimination of 
illegal sequences of morphemes, such as 
those due to long-distance dependencies, 
which are difficult to restrict by means of 
conti.uation classes. 
The first and second principles are defined lo 
combine information of previously recognized 
mOrl~hemcs, but all the other principles arc 
related to both feature-combination a d non- 
sequential moq~hotactics. 
3.3 Implementation 
We have chosen the PATR formalism 
(Shiebcr, 86) for the definition of the moqflm- 
syntactic rules. There were two main reasons 
for this choice: 
? The formalism is based o.  unification. 
Unification is adequate for the treatment of 
complex phenomena (e.g., agreement of 
conslituents in case, tmmber and definite- 
hess) and complex linguistic structures. 
? Simplicity. The grammar is not linked to a 
linguistic theory, e.g. GPSG in (Ritchie et 
al., 92)? The fact that PATR is simpler than 
more sophisticated formalisms will allow 
that in @e future the grammar could be 
adapted to any of them. 
25 rules have been defined, distributed in the 
following way: 
? 11 rules for the merging of declension 
morphemes and their combination with the 
main categories, 
? 9 rules for the description of verbal 
subordination morphenles, 
? 2 general fulcs for derivation, 
? 1 rule for each of the following 
phenomeml: ellipsis, degree of COlnpavison 
of adjectives (comparative and SUl)erlative) 
and noun composition. 
3.4 Evaluat ion 
As a cousequence of the size of the lexical 
database and tile extensive treatment of 
nlorphosyntax, the resulting analyzer offers 
full coverage when applied to real texts, 
capable of treating unknown words and non- 
standard forms (dialectal wtriants and typical 
errors). 
We performed four experilnents to ewtluate 
tile efficiency of the implemented analyzer 
(see Table 1). A 10,832-word text was 
randomly selected from newspapers. We 
measured tile number of words per second 
analyzed by the morphosyntactic analyzer and 
also by the whole morphological analyzer 
(results taken on a Sun Ultra 10). Ill the first 
experiment all tile word-t'ornls were analyzed 
one-by-one; while ill tile other three experi- 
ments words with more than one occurrence 
were analyzed only once. Ill the last two 
experimeuts a memory with the analysis of tile 
most frequent word-forms (MFW) in Basque 
was used, so that only word-forms not found 
in the MFW were analyzed. 
Test 
description 
All 
word forms 
Diffcrent 
word forms 
MFW 
10,000 words 
(I 5 Mb) 
MFW 
50,000 words 
(75 mb) 
# words/scc 
analyzed Morphosynt. 
words analyzer 
10,832 
3,692 
1,483 
533 
15,13 
44 40 
111 95 
308 270 
words/see 
Full 
morphological 
analyzer 
13,5 
Table 1. Evaluation results. 
Even when our language is agglutinative, and 
its morphological phenomena need more 
computational resources to build complex and 
deep structures, the results prove tile feasibility 
of implementiug efficiently a fifll 
morphological analyzer, although efficiency 
was not the main concern of our 
implementation. The system is currently being 
applied to unrestricted texts in real-time 
applications. 
4 Related work 
(Koskeniemmi, 83) defined the formalism 
named two-level morphology. Its main 
contributiou was the treatment of 
morl)hographemics and morphotactics. The 
formalisnl has been stmcessfully applied to a 
wide wlriety ot' languages. 
(Karttunen, 94) speeds the two-level model 
compiling two-level rules into lexical 
transducers, also increasing the expressiveness 
of the model 
The morphological analyzer created by 
(Ritchie et al, 92) does not adopt finite state 
mechanisms to control morphotactic 
phenomena. Their two-level implementation 
incorporates a straightforward morphotactics, 
reducing tile number of sublexicons to the 
indispensable (prefixes, lemmas and suffixes). 
This approximation would be highly 
inefficient for agglutinative languages, as it 
would create lnany nonsensical interpretatiolas 
that should be rejected by tile unification 
phase. They use the word-grammar for both 
morphotactics and feature-conlbination. 
ill a similar way, (Trost, 90) make a proposal 
to combine two-level morphology and non- 
sequential morphotactics. 
The PC-Kimmo-V2 system (Antworth, 94) 
presents an architecture similar to ours applied 
to English, using a finite-state segmentation 
phase before applying a unification-based 
grammar. 
(Pr6szdky and Kis, 99) describe a morpho- 
syntactic analyzer for Hungarian, an agglu- 
tinative language. The system clots not use the 
two-level model for segmentation, precom- 
piling suffix-sequences to improve efficiency. 
They claim the need of a word-grammar, 
giving a first outline of its design, although 
they do not describe it in detail. 
(Oflazer, 99) presents a different approach for 
the treatment of Turkish, an agglutinative 
language, applying directly a dependency 
parsing scheme to morpheme groups, that is, 
merging morphosyntax and syntax. Although 
we are currently using a similar model to 
Basque, there are several applications that are 
word-based and need full morphological 
parsing of each word-t'orm, like the word- 
oriented Constraint Graminar formalism for 
disambiguation (Karlsson et aI., 95). 
Conc lus ion  
We propose a model for fllll morphological 
analysis iutegrating two different components. 
On tile one hand, the two-level formalism 
deals with morphographenfics and sequential 
morphotactics and, on the other hand, a 
unil\]cation-based word-grammar combines lhe 
granlll-iatical in\['ornlatioli defined in illoi'- 
phelllOS alld also handles COlllplcx illori)ho- 
tactics. 
Early application of sCqtloniial I/lOrl)hotactic 
conslraints dtu-ing the segmentation process 
avoids all excessive laUlllber of nleaningless 
segmentation possibilities before the 
coulputationally lllOlO expensive unification 
process. Unification permits lhe resohition of a 
wide variety of morl)hological phenonlena, 
like ellipsis, thal force the definition of: 
complex and deep structures Io roprosenl the 
output of the analyzer. 
This design allowed us io develop a full 
coverage allalyzor that processes efficiently 
unrestricted loxis in Basque, a strongly 
agglulinafive langttage. 
The anaiyzcl" has bccll integrated ill a gCllOl'al 
franlework for the l)lOCessing of l~asquc, with 
all the linguistic inodulos communicating by 
l l leallS O\[: foattll'C stltlClll l 'eS ill accord  {o the 
principles of ihe Text Encoding Initiative. 
Acknowledgements  
This research was partially supported by the 
Basque Government, the University of the 
\]71aS(lUe Cotlntry {/lid the CICYq' (Cotllisidn 
lntcrministorial de Ciencia y Tecnologfil). 
References 
Aduriz 1., Aldczabal I., Ansa ()., Arlola X., I)faz de 
Ilarraza A., Insau.~li .I.M. (1998a) EI)BL: a 
Mttlli-l~ttrposed Lexica/ Sttl)l)c;rl .lot the 
Treatment of Ba,s'que. Proceedings of the l;irst 
Inlernational Confcncncc on l Auiguagc Resources 
and Ewduation, Granada. 
Aduriz I., Agirre E., Aldczabal 1., Alegria 1., Ansa 
O., Arrcgi X., Arriola J.M., ArtolaX., I)faz de 
lhu'raza A., Ezciza N., Gqicnola K., Maritxahu" 
A., Maritxalar M., Oronoz M., Sarasola K., 
Soroa A., Urizar R., Urkia M. (1998b) A 
Framework .for the Automatic Pmce.vsi#~g (if" 
Basqtte. Proceedings o1 the First Ii~ternational 
Con \[elel i te on Lall.gtlagc Resources turf 
Evaluation, Granada. 
Aduriz I., Alcgria I., Arriohl J.M., Artola X., l)faz 
do Ilarraza A., Ecciza N., Gojcnola K., 
Maritxalar M. (1995) Di\[.ferelt! Issues in the 
Design qf a lemmatizer/Tagger fo Ba,s'qtte. From 
Tcxls to Tags: Issues in Mullilingual Language 
Analysis. ACL SIGI)AT Workshop, l)ublin. 
Alcgria 1., Art(Ha X., Sarasoht K., Urkia M. (1996) 
Automatic moqdzological analysis of Basque. 
IAtcrary and IAnguistic Computing, 11 (4): 193- 
203. Oxford University. 
Aniworlh E. I.. (1994) Morphological Par, ffng with 
a lhl(fication-ba,s'ed Word Grcmmutr. Norlh 
Te, xas Natural l~anguage Processing Workshop, 
Texas. 
Arlola X., Dfaz de \]larraza A., Ezciza N., Oo.icnohi 
K., Marilxahu' A., Soma A. (2000) A proposal 
for the integration of NLP tools using SGML- 
lagged documeHls. Proceedings of ll~e Second 
Cotfforence or1 Language Resources and 
Evaltmfion (IA~,EC 2000). Athens, Greece 2000. 
Bcesl%, K. (1998)AraDic Morphological Analysis 
(m the lnlernet, l'rocccdings of the International 
Conference on Mulii-IAngual Computing (Arabic 
& lhlglish), Cambridge. 
Hudson R. (1990) English Word Grammmar. 
Oxford: Basil Blackwcll. 
ldc N., Vcronis J. K. (1995) Text-Ettcoding hHtia- 
tire, Bac:kgmtmd and Context. Kluwcr Academic 
Publishers. 
Karlsson F., Voulilaincn A., Heikkiht J., Anltila A. 
(1995) Constrai, t Gnmmmr: A lxm,?tmge- 
i#ldcpcndent System Jor Pm:ffng Um'estricled 
Text, Mouton do Gruyicr ed.. 
Kartmnen 1,. (1994) Con,s'tructin~ l,e.vical 
7)'ansdttcers. Proc. of CO13NG'94, 406-411. 
Koskcnniemi, K, (1983) Two-level Mc;qdlo\[ogy: A 
ge,eral Comptttational Model ./br Word-Form 
Recognition and Pmduclioth University of 
Ilclsinki, l)clmrtmcnt of General IAnguisiics. 
l~ublications " 11. 
()flazcr K (1999) l)epetMe/t O' Parsing, with a, 
E.rtended I:inite State Approac\]t. ACL'99, 
Maryland. 
Pr6sz6ky G., Kis B (1999)A Unificati(m-hascd 
Apl~roach to Moqdto-syntactic I'arsitl<~ of 
Agghttinative and Other (Highly) lnjlectional 
Languages. ACtd99, Ma,yhmd. 
Ritchie G., Pulhnan S. G., FJlack A. W., Russcl G. 
J. (1992) Comlmtational Moudu)logy: Practical 
Mechanism,s'.fi)r the l#lglish l,exico,. ACL-MIT 
Series on Natural Language Processing, MIT 
Press. 
Shicbcr S. M. (1986) At/ lntroductiotz to 
Unification-Based Approaches to Grammar. 
CSLI, Slanford. 
Sproat R. (1992) Morphology anU Computcaion. 
ACL-MIT Press series in Natural Language 
Processing. 
Trost It. (1990) The application of two-level 
morldzo/ogy to rzon-concatenative German 
moqgtology. COIANG'90, Hclsinki. 
7 
Learning Argument/Adjunct Distinction for Basque 
Abstract 
This paper presents experiments performed on 
lexical knowledge acquisition in the form of 
verbal argumental information. The system 
obtains the data from raw corpora after the 
application of a partial parser and statistical 
filters. We used two different statistical filters 
to acquire the argumental information: Mutual 
Information, and Fisher?s Exact test. 
Due to the characteristics of agglutinative 
languages like Basque, the usual classification 
of arguments in terms of their syntactic 
category (such as NP or PP) is not suitable. 
For that reason, the arguments will be 
classified in 48 different kinds of case 
markers, which makes the system fine grained 
if compared to equivalent systems that have 
been developed for other languages. 
This work addresses the problem of 
distinguishing arguments from adjuncts, this 
being one of the most significant sources of 
noise in subcategorization frame acquisition. 
Introduction 
In recent years a considerable effort has been done 
on the acquisition of lexical information. As 
several authors point out, this information is useful 
for a wide range of applications. For example, J. 
Carroll et al (1998) show how adding 
subcategorization information improves the 
performance of a parser. 
With this in mind our aim is to obtain a system 
that automatically discriminates between 
subcategorized elements of verbs (arguments) and 
non-subcategorized ones (adjuncts).  
We have evaluated our system in two ways: 
comparing the results to a gold standard and 
estimating the coverage over sentences in the 
corpus. The purpose was to find out which was the 
impact of each approach on this particular task. 
The two methods of evaluation yield significantly 
different results.  
Basque is the subject of this study. A language 
that, in contrast to languages like English, has 
limited resources in the form of digital corpora, 
computational lexicons, grammars or annotated 
treebanks. Therefore, any effort like the one 
presented here, oriented to create lexical resources, 
has to be driven to do as much automatic work as 
possible, minimizing development costs. 
The paper is divided into 4 sections. The first 
section is devoted to explain the theoretical 
motivations underlying the process. The second 
section is a description of the different stages of 
the system. The third section presents the results 
obtained. The fourth section is a review of 
previous work on automatic subcategorization 
acquisition. Finally, we present the main 
conclusions. 
1 The argument/adjunct distinction 
Talking about Subcategorization Frames (SCF), 
means talking about arguments. Many existing 
systems acquire directly a set of possible SCFs 
without any previous filtering of adjuncts. 
However, adjuncts are a substantial source of noise 
and therefore, in order to avoid this problem, our 
approach addresses the problem of the 
argument/adjunct distinction. 
The argument/adjunct distinction is probably 
one of the most unclear issues in linguistics. The 
distinction has being presented, for example, in the 
generativist tradition, in the following way: 
arguments are those elements participating in the 
event and adjuncts are those elements 
contextualizing or locating the event. 
This definition seems to be quite clear, but 
when we deal with concrete examples it is not the 
Izaskun Aldezabal, Maxux Aranzabe, Koldo 
Gojenola , Kepa Sarasola 
Dept. of Computer Languages and Systems, 
University of the Basque Country, 649 P. K., 
E-20080 Donostia,  
Basque Country 
Aitziber Atutxa 
University of Maryland  
College Park 
Maryland, 20740 
jibatsaa@si.ehu.es 
                     July 2002, pp. 42-50.  Association for Computational Linguistics.
                     ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia,
                  Unsupervised Lexical Acquisition: Proceedings of the Workshop of the
case. For example, if we take two verbs, talk and 
play.  
a. Yesterday I talked with Mary. 
b. Yesterday I played soccer with Mary. 
Here Mary is a participant of the event in both 
cases, therefore under the given definition both 
would be arguments. But this is contradictory to 
what traditional views consider in practice. The 
PP, with Mary, is considered an argument of talk 
but not an argument of play. It is true that there are 
differences between both of them because playing 
does not require two participants (though it can 
have them), while talking (under the sense of 
communicating) seems to require two participants. 
Finer argument/adjunct distinction have also 
been proposed differentiating between basic 
arguments, pseudo-arguments and adjuncts. Basic 
arguments are those required by the verb. Pseudo-
arguments are those that even if they are not 
required by the verb, when appearing they extend 
the verbal semantics, for example, adding new 
participants. And finally adjuncts, which would be 
contextualizers of the event. The most radical view 
is to consider the argument/adjunct distinction as a 
continuum where the elements belonging to the 
extremes of this continuum can be easily classified 
as arguments or adjuncts. On the contrary, the 
elements belonging to the central part of the 
continuum can be easily misclassified. For further 
reference see C. Schutze (1995), J.M. Gawron 
(1986), C. Verspoor (1997), J. Grimshaw (1990), 
and N. Chomsky (1995). 
From the different diagnostics proposed in the 
literature some are quite consistent among various 
authors (R. Grishman et al 1994, C. Pollard and I. 
Sag 1987, C. Verspoor 1997). 
1) The Obligatoriness condition. When a verb 
demands obligatorily the appearance of an 
element, this element will be an argument. 
a. John put the book on the table 
b. *John put the book 
2) Frequency. Arguments of a verb occur more 
frequently with that verb than with the other 
verbs. 
a. I came from home (argument). 
b. I heard it from you (adjunct). 
3) Iterability: Several instances of the same 
adjunct can appear together with a verb, while 
several instances of an argument cannot appear 
with a verb. 
a. I saw you in Washington, in the 
Kenedy Center. 
b. *I saw Alice John (being John and 
Alice two persons) 
4) Relative order: Arguments tend to appear closer 
to the verb than adjuncts.  
a. I put the book on the table at three 
b. *I put at three the book on the 
table 
5) Implicational test:  Arguments are semantically 
implied, even when they are optional. 
   a. I came to your house (from x) 
  b. I heard that (from x) 
The third and fourth tests were not very useful 
to us. Iterability test is quite weak because it seems 
to rely more on some other semantic notions such 
as part/whole relation than in the argument/adjunct 
distinction. For example, sentence 3.a would be 
grammatical due to semantic plausibility. The 
Kennedy Center is a part of Washington, therefore 
to see somebody in the Kennedy Center and see 
him in Washington are not semantically 
incompatible, so it is plausible to say it. In the case 
of 3.b John is not a part of  Alice and therefore it is 
not plausible to see Alice John. But for example it 
is plausible to say I saw you the hand. The relative 
order test is difficult to apply on a language like 
Basque which is a free word order language.  
The first and fifth tests are robust enough to be 
useful in practice. But only the two first 
diagnostics can be captured statistically by the 
application of association measures like Mutual 
Information. We did not come out with any  
straightforward way to apply the fifth test 
computationally. 
Before talking about the different measures 
applied, we will present step by step the whole 
process we pursued for achieving the 
argument/adjunct distinction. 
2 The acquisition process  
Our starting point was a raw newspaper corpus 
from of 1,337,445 words, where there were 
instances of 1,412 verbs. From them, we selected 
640 verbs as statistically relevant because they 
appear in more than 10 sentences.  
As we said earlier, our goal was to distinguish 
arguments from adjuncts. When starting from raw 
corpus, like in this case, it is necessary to get 
instances of verbs together with their dependents 
(arguments and adjuncts). We obtained this 
information applying a partial parser (section 2.1) 
to the corpus. Once we had the dependents, 
statistical measures helped us deciding which were 
arguments and which were adjuncts (section 2.2). 
2.1 The parsing phase 
Aiming to obtain the data against which statistical 
filters will be applied, we analyzed the corpus 
using several available linguistic resources: 
? First, we performed morphological analysis of 
the corpus, based on two-level morphology (K. 
Koskenniemi 1983; I. Alegria et al 1996) and 
disambiguation using the Constraint Grammar 
formalism (Karlsson et al 1995, Aduriz et al 
1997).  
? Second, a shallow parser was applied (I. 
Aldezabal et al 2000), which recognizes basic 
syntactic units including noun phrases, 
prepositional phrases and several types of 
subordinate sentences. 
? The third step consisted in linking each verb 
and its dependents. Basque lacks a robust 
parser as in (T. Briscoe & J. Carroll 1997, D. 
Kawahara et al 2001) and, therefore, we used a 
finite state grammar to link the dependents 
(both arguments and adjuncts) with the verb (I. 
I. Aldezabal et al 2001). This grammar was 
developed using the Xerox Finite State Tool (L. 
Karttunen et al 1997). Figure 1 shows the 
result of the parsing phase. In this case, both 
commitative and inessive cases (PPs) are 
adjuncts, while the ergative NP is an argument. 
The linking of dependents to a verb is not 
trivial considering that Basque is a language 
with free order of constituents, and any element 
appearing between two verbs could be, in 
principle, dependent on any of them. Many 
problems must be taken into account, such as 
ambiguity and determination of clause 
boundaries, among others. We evaluated the 
accuracy up to this point, obtaining a precision 
over dependents of 87% and a recall of 66%. 
So the input data to the next phase was 
relatively noisy.  
2.2 The argument selection phase 
In the data resulting from the shallow parsing 
phase we counted up to 65 different cases (types of 
arguments, including postpositions and different 
types of suffixes). These are divided in two main 
groups: 
? 43 correspond to postpositions. Some of them 
can be directly mapped to English prepositions, 
but in many cases several Basque postpositions 
correspond to just one English preposition (see 
Table 1a.). This set alo contains postpositions 
1)? (a) [ EEBBetako lehendakariak] (b) [UEko 15 herrialdeetako merkataritza ministroekin] 
(c) [bazkaldu behar zuen] (d) [negoziazioen bilgunean] ? 
 
2) ? the president of the USA had to eat with the ministers of Commerce of 15 countries of the UE in
the negotiation center ? 
 
(a)  [EEBB-etako lehendakari-a-k]       (b)  [UE-ko    15 herrialde-etako    merkataritza ministro-ekin]  
     [USA-of         president-the-erg.]        [UE-of    15 countries-of          Commerce ministers-with]  
      NP-ergative(president, singular)                PP(with)-commitative(minister, plural)  
 The president of the USA  with the ministers of Commerce of 15 countries of the UE 
 
 
(c) [bazkaldu behar zuen]                  (d)   [negoziazio-en     bilgune-an] 
        [to eat        had]                                  [negotiation-of     center-in]   
          verb(eat)                                        PP(in)-inessive(center, singular) 
 had to eat in the negotiation center 
Figure 1. Example of the output of the shallow parsing phase: 1) Input (in Basque), 2) English translation,. 
Below (c) Verb phrase and (a,b,d) verbal dependents (phrases), and also under the case+head 
that map to categories other than English 
prepositions, such as adverbs (Table 1b). 
Table 1. Correspondence between English 
prepositions and Basque postpositions. 
 English Basque 
a. to dative (suffix) 
alative (suffix) 
final ablative (suffix) 
b. like -en gisa (suffix) 
gisa 
bezala 
legez 
 
? 22 types of sentential complements (For 
instance, English that complementizer 
corresponds to several subordination suffixes:  
-la, -n, -na, -nik). 
This shows to which extent the range of 
arguments is fine grained, in contrast to other 
works where the range is at the categorial level, 
such as NP or PP (M. Brent 1993, C. Manning 
1993, P. Merlo & M. Leybold 2001). 
Due to the complexity carried by having such a 
high number of cases, we decided to gather 
postpositions that are semantically equivalent or 
almost equivalent (for example, English between 
and among). Even if there are some semantic 
differences between them they do not seem to be 
relevant at the syntactic level. Some linguists were 
in charge of completing this grouping task. Even 
considering the risk of making mistakes when 
grouping the cases, we concluded that the loss of 
accuracy due to having too sparse data 
(consequence of having many cases) would be 
worse than the noise introduced by any mistake in 
the grouping. The resulting set contained 48 cases. 
The complexity is reduced but it is still 
considerable.  
Most of the work on automatic acquisition of 
subcategorization information (J. Carroll & T. 
Briscoe 1997, A. Sarkar & D. Zeman 2000, A. 
Korhonen 2001) apply statistical methods 
(hypothesis testing). Basically the idea is the 
following: they get "possible subcategorization 
frames" from automatically parsed data (either 
completely or partially parsed) or from a 
syntactically annotated corpus. Afterwards a 
statistical filter is employed to decide whether 
those "possible frames" are or not real 
subcategorization frames. These statistical 
methods can be problematic mostly because they 
perform badly on sparse data. In order to avoid as 
much as possible data sparseness, we decided to 
design a system that learns which are the 
arguments of a given verb instead of learning 
whole frames. Frames are combinations of 
arguments, and considering that our system deals 
with 48 cases, the number of combinations was 
high, resulting in sparse data. So we decided to 
work at the level of the argument/adjunct 
distinction. Working on this distinction is also very 
useful to avoid noise in the subcategorization 
frame, because in this task adjuncts are synonyms 
of noise. A system that tries to get 
subcategorization frames without previously 
making the argument/adjunct distinction suffers of 
having sparse and noisy data.  
To accomplish the argument/adjunct distinction 
we applied two measures: Mutual Information 
(MI), and Fisher's Exact Test (for more 
information on these measures, see C. Manning & 
H. Sch?tze 1999). MI is a measure coming from 
Information Theory, defined as the logarithm of 
the ratio between the probability of the co-
occurrence of the verb and the case, and the 
probability of the verb and the case appearing 
together calculated from their independent 
probability. So higher Mutual Information values 
correspond to higher associated verb and cases 
(see table 2). 
Table 2. Examples from MI values for verb-case 
pairs 
verb case MI 
atera(to take/go out) ablative(from) 1.830 
atera(to take/go out) instrumental(with) -0.955 
erabili(to use) gisa(as) 2.255 
erabili(to use) instrumental(with) -0.783 
Mutual Information shows higher values for 
atera-ablative(to go/take out), erabili-gisa (to use-
as). These pairs were manually tagged as 
arguments, therefore Mutual information makes 
the right prediction. On the contrary, atera-
instrumental (to go/take out-with), erabili-
instrumental (to use-with) were manually tagged as 
adjuncts. Mutual information values in table 2 go 
along with the manual tagging for these last pairs 
as well, because the Mutual information values are 
low as should correspond to adjuncts.  
Fisher?s Exact Test is a hypothesis testing 
statistical measure1. We used the left-side version 
of the test (see T. Pederssen, 1996). Under this 
version the test tells us how likely it would be to 
perform the same experiment again and be less 
accurate. That is to say, if you were repeating the 
experiment and there were no relation between the 
verb and the case, you would have a big 
probability of finding a lower co-occurrence 
frequency than the one you observed in your 
experiment. So higher left-side Fisher values tell 
us that there is a correlation between the verb and 
the case (see table 3.) 
Table 3. Examples of Fisher?s Exact Test  values for 
verb-case pairs 
verb Case Fisher 
atera(to take/go out) Ablative(from) 1.0000 
atera(to take/go out) instrumental(with) 0.0003 
erabili(to use) gisa(as) 1.0000 
erabili(to use) instrumental(with) 0.0002 
Fisher?s Exact values show higher values for 
atera-ablative(to go/take out), erabili-gisa (to use-
as). These values predict correctly the association 
between the verbs and cases for these examples. 
The low values for the atera-instrumental (to 
go/take out-with), and erabili-instrumental (to use-
with) pairs, should be interpreted as the non-
association between the verbs and the cases in 
these examples, that is to say, they are adjuncts. 
And again, the prediction would be right according 
to the taggers. 
These tests are broadly used to discover 
associations between words, but they show 
different behaviour depending on the nature of the 
data. We did not want to make any a priori 
decision on the measure employed. On the 
contrary, we aimed to check which test behaved 
better on our data.  
3 Evaluation  
We found in the literature two main approaches to 
evaluate a system like the one proposed in this 
paper (T. Briscoe & J. Carroll 1997, A. Sarkar & 
D. Zeman 2000, A. Korhonen 2001): 
                                                     
1 There are two ways of interpreting Fisher?s test, as one 
or two sided test. In the one sided fashion there is still 
another interpretation, as a right or left sided test. 
 
? Comparing the obtained information with a 
gold standard.  
? Calculating the coverage of the obtained 
information on a corpus. This can give  an 
estimate of how well the information obtained 
could help a parser on that corpus. 
Under the former approach a further distinction 
emerges: using a dictionary as a gold standard, or 
performing manual evaluation, where some 
linguists extract the subcategorization frames 
appearing in a corpus and comparing them with the 
set of subcategorization frames obtained 
automatically.  
We decided to evaluate the system both ways, 
that is to say, using a gold standard and calculating 
the coverage over a corpus. The intention was to 
determine, all things being equal, the impact of 
doing it one way or the other. 
3.1 Evaluation 1: comparison of the results with a 
gold standard 
From the 640 analyzed verbs, we selected 10 for 
evaluation. For each of these verbs we extracted 
from the corpus the list of all their dependents. The 
list was a set of bare verb-case pairs, that is, no 
context was involved and, therefore, as the sense 
of the given verb could not be derived, different 
senses of the verb were taken into account.  We 
provided 4 human annotators/taggers with this list 
and they marked each dependent as either 
argument or adjunct. The taggers accomplished the 
task three times. Once, with the simple guideline 
of the implicational test and obligatoriness test, but 
with no further consensus. The inter-tagger 
agreement was low (57%). The taggers gathered 
and realized that the problem came mostly from 
semantics. While some taggers tagged the verb-
case pairs assuming a concrete semantic domain 
the others took into account a wider rage of senses 
(moreover, in some cases the senses did not even 
match). So the tagging was repeated when all of 
them considered the same semantics to the 
different verbs. The inter-tagger agreement raised 
up to a 80%. The taggers gathered again to discuss, 
deciding over the non clear pairs. 
The list obtained from merging2 the 4 lists in 
one is taken to be our gold standard. Notice that 
                                                     
2 Merging was possible once the annotators agreed on 
the marking of each element. 
when the annotators decided whether a possible 
argument was really an argument or not, no 
context was involved. In other words, they were 
deciding over bare pairs of verbs and cases. 
Therefore different senses of the verb were 
considered because there was no way to 
disambiguate the specific meaning of the verb. So 
the evaluation is an approximation of how well 
would the system perform over any corpus. Table 
4 shows the results in terms of Precision and 
Recall. 
Table 4. Results of Evaluation 1 (context 
independent) 
 Precision Recall F-score 
MI 62% 50% 55% 
Fisher 64% 44% 52% 
 
3.2 Evaluation 2: Calculation of the coverage on a 
corpus 
The initial corpus was divided in two parts, one for 
training the system and another one for evaluating 
it. From the fraction reserved for evaluation we 
extracted 200 sentences corresponding to the same 
10 verbs used in the "gold standard" based 
evaluation. In this case, the task carried out by the 
annotators consisted in extracting, for each of the 
200 sentences, the elements (arguments/adjuncts) 
linked to the corresponding verb. Each element 
was marked as argument or adjunct. Note that in 
this case the annotation takes place inside the 
context of the sentence. In other words, the verb 
shows precise semantics.  
We performed a simple evaluation on the 
sentences (see table 5), calculating precision and 
recall over each argument marked by the 
annotators3. For example, if a verb appeared in a 
sentence with two arguments and the statistical 
filters were recognizing them as arguments, both 
precision and recall would be 100%. If, on the 
contrary, only one was found, then precision 
would be 100%, and recall 50%.  
Table 5. Results of Evaluation 2 (inside context) 
 Precision Recall F-score 
MI 93% 97% 95% 
Fisher 93% 93% 93% 
                                                     
3 The inter-tagger agreement in this case was of  97%.  
3.3 Discussion 
It is obvious that the results attained in the first 
evaluation are different than those in the second 
one. The origin of this difference comes mostly, on 
one hand, from semantics and, on the other hand, 
from the nature of statistics: 
? Semantic source. The former evaluation was 
not contextualized, while the latter used the 
sentence context. Our experience showed us 
that broader semantics (non-contextualized 
evaluation) leads to a situation where the 
number of arguments increases with respect to 
narrower (contextualized evaluation) 
semantics. This happens because in many 
cases different senses of the same verb require 
different arguments. So when the meaning of 
the verb is not specified, different meanings 
have to be taken into account and, therefore, 
the task becomes more difficult. 
? Statistical reason. The disagreement in the 
results comes from the nature of the statistics 
themselves. Any statistical measure performs 
better on the most frequent cases than on the 
less frequent ones. In the first experiment all 
possible arguments are evaluated, including 
the less frequent ones, whereas in the second 
experiment only the possible arguments found 
in the piece of corpus used were evaluated. In 
most of the cases, the possible arguments 
found were the most frequent ones. 
At this point it is important to note that the 
system deals with non-structural cases. In Basque 
there are three structural cases (ergative, absolutive 
and dative) which are special because, when they 
appear, they are always arguments. They 
correspond to the subject, direct object and indirect 
object functions. These cases are not very 
conflictive about argumenthood, mainly because in 
Basque the auxiliary bears information about their 
appearance in the sentence. So they are easily 
recognized and linked to the corresponding verb. 
That is the reason for not including them in this 
work. Precision and recall would improve 
considerably if they were included because they 
are the most frequent cases (as statistics perform 
well over frequent data), and also because the 
shallow parser links them correctly using the 
information carried by the auxiliary. Notice that 
we did not incorporate them because in the future 
we would like to use the subcategorization  
information obtained for helping our parser, and 
the non-structural cases are the most problematic 
ones.    
4 Related work  
Concerning the acquisition of verb 
subcategorization information, there are proposals 
ranging from manual examination of corpora (R. 
Grishman et al 1994) to fully automatic 
approaches.  
Table 3, partially borrowed from A. Korhonen 
(2001), summarizes several systems on 
subcategorization frame acquisition. 
C. Manning (1993) presents the acquisition of 
subcategorization frames from unlabelled text 
corpora. He uses a stochastic tagger and a finite 
state parser to obtain instances of verbs with their 
adjacent elements (either arguments or adjuncts), 
and then a statistical filtering phase produces 
subcategorization frames (from a set of previously 
defined 19 frames) for each verb.  
T. Briscoe and J. Carroll (1997) describe a 
grammar based experiment for the extraction of 
subcategorization frames with their associated 
relative frequencies, obtaining 76.6% precision 
and 43.4% recall. Regarding evaluation, they use 
the ANLT and COMLEX Syntax dictionaries as 
gold standard. They also performed evaluation of 
coverage over a corpus. For our work, we could 
not make use of any previous information on 
subcategorization, because there is nothing like a  
subcategorization dictionary for Basque. 
A. Sarkar and D. Zeman (2000) report results 
on the automatic acquisition of subcategorization 
frames for verbs in Czech, a free word order 
language. The input to the system is a set of 
manually annotated sentences from a treebank, 
where each verb is linked with its dependents 
(without distinguishing arguments and adjuncts). 
The task consists in iteratively eliminating 
elements from the possible frames with the aim of 
removing adjuncts. For evaluation, they give an 
estimate of how many of the obtained frames 
appear in a set of 500 sentences where dependents 
were annotated manually, showing an 
improvement from a baseline of 57% (all elements 
are adjuncts) to 88%. 
Comparing this approach to our work, we must 
point out that Sarkar and Zeman's data does not 
come from raw corpus, and thus they do not deal 
with the problem of noise coming from the parsing 
phase. Their main limitation comes by relying on a 
treebank, which is an expensive resource. 
D. Kawahara et al (2001) use a full syntactic 
parser to obtain a case frame dictionary for 
Japanese, where arguments are distinguished by 
their syntactic case, including their headword 
(selectional restrictions). The resulting case frame 
components are selected by a frequency threshold. 
Table 3. Summary of several systems on subcategorization information. 
Method Number 
of frames 
Number 
of verbs 
Linguistic 
resources 
F-Score 
(evaluation 
based on a 
gold standard) 
Coverage on a 
corpus 
C. Manning (1993) 19 200 POS tagger + simple 
finite state parser 
58  
T. Briscoe & J. 
Carroll (1997) 
161 14 Full parser 55  
A. Sarkar & D. 
Zeman (2000) 
137 914 Annotated treebank - 88 
D. Kawahara et al 
(2001) 
- 23,497 Full parser  82 accuracy 
M. Maragoudakis et 
al. (2001) 
- 47 Simple phrase 
chunker 
77  
This paper - 640 Morph. Analyzer + 
Phrase Chunker + 
Finite State Parser 
55 95 
      
M. Maragoudakis et al (2001) apply a 
morphological analyzer and phrase chunking 
module to acquire subcategorization frames for 
Modern Greek. In contrast to this work, they use 
different machine learning techniques. They claim 
that Bayesian Belief Networks are the best 
learning technique. 
P. Merlo and M. Leybold (2001) present 
learning experiments for automatic distinction of 
arguments and adjuncts, applied to the case of 
prepositional phrases attached to a verb. She uses 
decision trees tested on a set of 400 verb instances 
with a single PP, reaching an accuracy of 86.5% 
over a baseline of 74%. 
Note that both Manning and Merlo and 
Leybold's systems learn from contexts with just 
one PP (maximum) per verb (finite state filter). 
Our system learns from contexts with up to 5 PPs. 
Furthermore, we distinguish 48 different kinds of 
cases, hence the number of combinations is 
considerably bigger.  
Regarding the parsing phase, the systems 
presented so far are heterogeneous. While  
Manning, Merlo and Leybold and Maragoudakis et 
al. use very simple parsing techniques, Briscoe and 
Carroll and Kawahara et al use sophisticated 
parsers. Our system can be placed between these 
two approaches. The result of the shallow parsing 
is not simple in that it relies on a robust 
morphological analysis and disambiguation. 
Remember that Basque is an agglutinative 
language with strong morphology and, therefore, 
this stage is particularly relevant. Moreover, the 
finite state filter we used for parsing is very 
sophisticated (L. Karttunen et al 1997, I. 
Aldezabal et al 2001), compared to Manning's. 
Conclusion  
This work describes an initial effort to obtain 
subcategorization information for Basque. To 
successfully perform this task we had to go deeper 
than mere syntactic categories (NP, PP, ?) 
enriching the set of possible arguments to 48 
different classes. This leads to quite sparse data.  
Together with sparseness, another problem 
common to every subcategorization acquisition 
system is that of noise, coming from adjuncts and 
incorrectly parsed elements. For that reason, we 
defined subcategorization acquisition in terms of 
distinguishing between arguments and adjuncts. 
The system presented was applied to a 
newspaper corpus. Subcategorization acquisition is 
highly associated to semantics in that different 
senses of a verb will most of the times show 
different subcategorization information. Thus, the 
task of learning subcategorization information is 
influenced by the corpus. As for the evaluation of 
this work, we carried out two different kinds of 
evaluation. This way, we verified the relevance of 
semantics in this kind of task. 
For the future, we plan to incorporate the 
information resulting from this work in our parsing 
system. We hope that this will lead to better results 
in parsing. Consequently, we would get better 
subcategorization information, in a bootstrapping 
cycle. We also plan to improve the results by using 
semantic information as proposed in A. Korhonen 
(2001).  
Acknowledgements 
This work has been supported by the Department 
of Economy of the Government of Gipuzkoa, The 
University of the Basque Country, the Department 
of Education of the Basque Government and the 
Commission of Science and Technology of the 
Spanish Government.   
References 
I. Aduriz, J. M. Arriola, X. Artola, A. D?az de 
Ilarraza, K. Gojenola and M. Maritxalar (1997) 
Morphosyntactic disambiguation for Basque based on 
the Constraint Grammar Formalism. Conference on 
Recent Advances in Natural Language Processing 
(RANLP).  
I. Alegria, X. Artola, K. Sarasola and M. Urkia (1996) 
Automatic morphological analysis of Basque. Literary 
and Linguistic Computing. 11 (4), Oxford University. 
I. Aldezabal, K. Gojenola and K. Sarasola (2000) A 
Bootstrapping Approach to Parser Development. 
International Workshop on Parsing Technologies 
(IWPT), Trento. 
I. Aldezabal, M. Aranzabe, A. Atutxa, K. Gojenola, 
M. Oronoz M. and Sarasola K. (2001) Application of 
finite-state transducers to the acquisition of verb 
subcategorization information. Finite State Methods 
in Natural Language Processing, ESSLLI Workshop, 
Helsinki. 
M. R. Brent (1993) From Grammar to Lexicon: 
Unsupervised Learning of Lexical Syntax. 
Computational Linguistics, 19:243-262. 
T. Briscoe and J. Carroll  (1997) Automatic Extraction 
of Subcategorization from Corpora. ANLP-97:356-
363. 
J. Carroll, G. Minnen and T. Briscoe (1998) Can 
Subcategorization Probabilities Help a Statistical 
Parser? Proceedings of the 6th ACL/SIGDAT 
Workshop on Very Large Corpora, Montreal. 
N. Chomsky (1995) The Minimalist Program. 
Cambridge MA, MIT Press. 
T. Dunning  (1993) Accurate Methods for the 
Statistics of Surprise and Coincidence. Computational 
Linguistics 19, 1  
J.M. Gawron (1986) Situations and prepositions. 
Linguistics and Philosophy 9(3), 327-382. 
J. Grimshaw (1990) Argument Structure. Cambridge, 
MA, MIT Press. 
R. Grishman, C. Macleod, A. Meyers (1994) Comlex 
Syntax: Building a Computational Lexicon. COLING-
94. 
F. Karlsson, A. Voutilainen, J. Heikkila, A. Anttila 
(1995) Constraint Grammar: A Language-
independent System for Parsing Unrestricted Text. 
Mouton de Gruyter. 
L. Karttunen, J.P. Chanod, G. Grefenstette, A. Schiller 
(1997) Regular Expressions For Language 
Engineering. Natural Language Engineering. 
D. Kawahara, N. Kaji and S. Kurohashi (2000) 
Japanese Case Structure Analysis by Unsupervised 
Construction of a Case Frame Dictionary. COLING-
2000, Saarbrucken. 
A. Korhonen (2001) Subcategorization acquisition. 
Unpublished  PhD Thesis, University of Cambridge. 
K. Koskenniemi (1983) Two-level Morphology: A 
general Computational Model for Word-Form 
Recognition and Production. PhD thesis, University 
of Helsinki. 
J. Kuhn, J. Eckle-Kohlerm and C. Rohrer (1998) 
Lexicon Acquisition with and for Symbolic NLP-
Systems -- a Bootstrapping Approach. First 
International Conference on Language Resources and 
Evaluation (LREC98), Granada. 
C.D. Manning (1993) Automatic Acquisition of a 
Large Subcategorization Dictionary from Corpora. 
Proceedings of the 31th ACL. 
C.D. Manning and H. Sch?tze (1999) Foundations of 
Statistical Natural Language Processing. The MIT 
Press, Cambridge, Massachusetts.  
M. Maragoudakis, K. Kermanidis, N. Fakotakis and 
G. Kokkinakis (2001) Learning Automatic Acquisition 
of Subcategorization Frames using Bayesian 
Inference and Support Vector Machines. The 2001 
IEEE International Conference on Data Mining, 
IMDC'01, San Jos?. 
P. Merlo and M. Leybold (2001) Automatic 
Distinction of Arguments and Modifiers: the Case of 
Prepositional Phrases. EACL-2001, Toulousse. 
T. Pederssen (1996) Fishing for Exactness In the 
Proceeding of the South-Central SAS User Group 
Conference (SCSUG-96). 
C. Pollard and I. Sag (1987) An information based 
Syntax and Semantics, volume 13. CSLI lecture. 
Notes, Standford University. 
A. Sarkar and D. Zeman (2000) Automatic Extraction 
of Subcategorization Frames for Czech. COLING-
2000, Saarbrucken.  
C. Schutze (1995) PP Attachment and Argumenthood. 
MIT Working Papers in Linguistics. 
C. Verspoor (1997) Contextually-Dependent Lexical 
Semantics. PhD thesis, Brandeis University, MA. 
