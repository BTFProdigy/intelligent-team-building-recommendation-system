The TELRI tool catalogue: structure and prospects
Tomaz? Erjavec
Dept. of Intelligent Systems
Institute ?Joz?ef Stefan?
Jamova 39
SI-1000 Ljubljana, Slovenia
tomaz.erjavec@ijs.si
Tama?s Va?radi
Linguistics Institute
Hungarian Academy of Sciences
P.O.Box 701/518
Budapest H-1399, Hungary
varadi@nytud.hu
Abstract
In the scope of the TELRI concerted
action a working group is investigat-
ing the formation of a tool catalogue
and repository. The idea is similar to
that of the ACL Natural Language Soft-
ware Registry, but the contents should
be mostly limited to corpus processing
tools available free of cost for research
use. The catalogue should also offer
a help-line for installing and using the
software. The paper reports on the set-
up of this catalogue, and concentrates
on the technical issues involved in its
creation, storage and display. This in-
volves the form interface on the Web,
the XML DocBook encoding, and the
XSL stylesheets used to present the cat-
alogue either on the Web or in print.
The paper lists the current entries in the
catalogue and discusses plans for their
expansion and maintenance.
1 Introduction
The ?Trans-European Language Resources In-
frastructure?, TELRI (http://www.telri.de/), is a
pan-European alliance of focal national language
(technology) institutions with the emphasis on
Central and Eastern European and NIS countries.
Some of the main objectives of TELRI is to col-
lect, promote, and make available monolingual
and multilingual language resources and tools
for the extraction of language data and linguis-
tic knowledge; to provide a forum where experts
from academia and industry share and assess tools
and resources; and to make available the expertise
of its partner institutions to the research commu-
nity, to language industry and to the general pub-
lic.
A number of these goals is being served
by the ?TELRI Research Archive of Com-
putational Tools and Resources?, TRACTOR,
(http://www.tractor.de), which features monolin-
gual, bilingual, and multilingual corpora and lex-
ica in a wide variety of languages as well as
corpus- and lexicon-related software. While the
primary aim is to pool the resources of TELRI
partners, TRACTOR also serves other institutions
by making the resources and tools available to the
wider research and educational community.
While the TRACTOR archives already offer a
number of tools, the longer term objective is to
offer a more substantial catalogue of corpus and
lexicon processing software. Furthermore, the
software itself is not necessarily available directly
from TRACTOR, which would also have a more
formalised structure and a well-defined process
of updating and presenting its entries. A closely
related initiative and model for this effort is the
?The Natural Language Software Registry? of the
ACL hosted at DFKI, a new edition of which was
released in 2000 (Declerck et al, 2000). While
the ACL registry offers a much larger array of
tools, the TELRI catalogue should have the ad-
vantage that each entry also contains a pointer to
the TELRI member who is able to offer advice on
installing and using the tool in question.
Other related catalogues on the Web
are the CTI?s Guide to Digital Resources
(http://info.ox.ac.uk/ctitext/resguide/) which has
a section on Text Analysis Tools and Techniques.
However, it does not seem to be maintained any
longer.
The Summer Institute of Linguistics
(http://www.sil.org/computing/catalog/) also
hosts a repository containing more than 60
pieces of software developed at SIL. Most of the
software is available for free download; the latest
update to the pages comes spring 1999.
A view on sharing resources, very much
based on latest standardisation initiatives, has
been developed by the Open Language Archives
Community, OLAC, (Bird and Simons, 2000),
http://www.language-archives.org/. OLAC is an
international project to construct an infrastructure
aimed at opening the whole array of language re-
sources, including texts, recordings, lexicons, an-
notations, software, protocols, models, and for-
mats. OLAC aims to develop community-specific
metadata to link language archives and establish
centralized catalogs. It builds directly on two
other initiatives, namely the the Open Archives
Initiative (developing and promoting interoper-
ability standards for efficient dissemination of
content) and the Dublin Core Metadata Initia-
tive (development of interoperable online meta-
data standards).
In the scope of the TELRI-II concerted action,
a working group has been set up to design a cata-
logue of corpus processing tools, and this paper
reports on the preliminary results of the work-
ing group. The rest of the paper is structured as
follows: Section 2 gives the overall structure of
the catalogue and its entries; Section 3 explains
the pipeline for updating and displaying the cat-
alogue, i.e. the Web form interface for input, ed-
itorial policy, and the stylesheet mechanism for
display; Section 4 lists the current contents of the
catalogue, while Section 5. gives some conclu-
sions and outlines plans for its expansion and fur-
ther maintenance.
2 Catalogue Format
The overall encoding chosen for the catalogue
was DocBook, an SGML/XML DTD primar-
ily used for encoding computer manuals and
other technical documentation. Choosing an
SGML/XML framework follows a similar strand
of research in annotating linguistic resources, as
exemplified in the XML version of the Corpus
Encoding Standard (Nancy et al, 2000) and in
work on syntactic annotation (Nancy and Romary,
2001). An advantage of XML is the possibility
of further standardisation by the use of related
recommendations, i.e. the XML Stylesheet Lan-
guage.
DocBook has a large user base and is well
documented: a reference book has been pub-
lished and is available on-line (Walsh, 1999)
for browsing or downloading. There is also an
interesting public initiative utilising DocBook,
namely the Linux Documentation Project, LDP
(http://www.linuxdoc.org/), which is working on
developing free, high quality documentation for
the GNU/Linux operating system.
Because DocBook is an application of SGML,
and, more recently, XML, many freely available
tools are available to process it. Most importantly,
this includes XSL processors, which can be used
to render DocBook documents in, say, HTML or
PDF; this issue is further elaborated in Section 4.
The complete catalogue is represented as one
<book> element, with introductory matter in
<bookinfo> giving the name, release informa-
tion and some other general information about
the catalogue. The catalogue is then divided (at
present) into three <chapter> elements, each
giving a a certain type of tools we plan to address:
 morpho-syntactic taggers
 concordancers
 aligners
Each catalogue entry is contained in
<sect1>, the top-level section element.
The section, besides containing a <title>
and being marked with an ID, is composed of
two <sect2> elements. The first gives the
information that is common to all sorts of tools,
while the second is tool-type specific.
The information records are encoded as <for-
malpara>, where each such element has a
<title>, followed by the text of the of the
record as a <para>. Various other DocBook el-
ements are used to annotate pieces of informa-
tion, e.g. <address>, <affiliation> and
similar details. Table 1 gives as an example a
complete dummy catalogue entry, where variable
parts are prefixed by ?this is?.
<sect1 id="this_is_name_971886394">
<title><productname>this is name</productname></title>
<sect2><title>Common part</title>
<formalpara><title>Task</title>
<para>this is task
<indexterm><primary>this is task</primary></indexterm></para></formalpara>
<formalpara><title>Author(s)</title>
<para>this is author</para></formalpara>
<formalpara><title>Institute/Company</title>
<para>
<address>
<affiliation><orgname>this is affil</orgname></affiliation>
<street>this is street</street>
<city>this is city</city>
<country>this is country</country></address></para></formalpara>
<formalpara><title>Version</title>
<para>this is version</para></formalpara>
<formalpara><title>Interface</title>
<para>this is interface</para></formalpara>
<sect3><title>Implementation</title>
<formalpara><title>Platform</title>
<para><hardware>this is platform</hardware></para></formalpara>
<formalpara><title>Operating system</title>
<para><envar>this is os</envar></para></formalpara>
<formalpara><title>Language of implementation</title>
<para>this is impl</para></formalpara></sect3>
<sect3><title>License</title>
<formalpara><title>License conditions for research purposes</title>
<para>this is licres</para></formalpara>
<formalpara><title>License conditions for commercial purposes</title>
<para>this is liccom</para></formalpara>
<formalpara><title>Restrictions</title>
<para>this is restrict</para></formalpara></sect3>
<sect3><title>Distribution</title>
<formalpara><title>Availability of source code</title>
<para>
<ulink url="this is source_url">this is source_url</ulink></para></formalpara>
<formalpara><title>Download possibilities and formats</title>
<para>
<ulink url="this is binary_url">this is binary_url</ulink>
</para></formalpara></sect3>
<sect3><title>References</title>
<formalpara><title>Homepage</title>
<para><ulink url="this is homepage">this is home-
page</ulink></para></formalpara>
<formalpara><title>Language of documentation</title>
<para>this is doc_lang</para></formalpara></sect3>
<sect3><title>TELRI helpline</title>
<para>this is helpline</para></sect3>
</sect2>
<sect2><title>Tool specific part</title>
<formalpara><title>Description</title>
<para>this is description</para></formalpara>
</sect2>
</sect1>
Table 1: Example of entry in DocBook produced via the form interace
3 Catalogue input and output
While the initial catalogue was input di-
rectly with an SGML editor and then vali-
dated, the envisioned additions will be per-
formed via a Web form interface, available at
http://gnu.nytud.hu/telri/. Figure 1 displays the
top part of the screenshot of the HTML form de-
signed to collect the specification of description
of catalogue items.
The definition of the particular information
sought about the software tools required some
consideration. Obviously, we would like to have
as detailed a description of each item as possible.
On the other hand, one has to bear in mind that the
TELRI Catalogue will appeal for free voluntary
contributions. Hence, the form should be maxi-
mally easy to fill in with minimal effort in order
to avoid possibly deterring people from contribut-
ing who might otherwise have done so. The cru-
cial factor to consider was to find the right balance
between the set of required and optional items. In
the end, the required information fields were con-
fined to the bare minimum of name, task, descrip-
tion and TELRI helpline. Table 2 displays the full
list of questions used in the HTML form.
The form interface runs a Perl CGI script,
which mails the output, encoded as the above de-
scribed DocBook <sect1> element, to the ed-
itors of the catalogue. After checking, fresh en-
tries are included in the official release of the cat-
alogue.
The DocBook format is suitable for storage
and interchange, but it is, of course, not appro-
priate for displaying the information. However,
one of the benefits of using standardised solutions
is that conversion tools and specifications are, to
a large extent, already available. For presenta-
tion, we have been so far experimenting with the
XML Stylesheet Language, XSL, or, more pre-
cisely, XSLT, the XSL Transformation Language,
(W3C, 2000). XSLT is a recommendation of the
W3C and is a language for transforming XML
documents into other XML documents. There al-
ready exist several freely available XSLT proces-
sors, e.g., Xalan (http://xml.apache.org/xalan/),
produced by the Apache XML Project.
XSLT is most often used to produce HTML
output for viewing on the Web, and so called
Formatted Objects, which are then further
transformed into print formats, usually PDF.
For DocBook XML there exist ready-made
stylesheets for both kinds of output, made by
Norman Walsh and available at on the Web
(http://nwalsh.com/docbook/xsl/). In the current
version we have used these ?out of the box? tools
to render the catalogue, although some slight
modifications would be in order to produce out-
put better tailored to the catalogue application.
Figure 2 contains a sample HTML output of
one item in the Catalogue.
In summary, Figure 3 gives a graphical
overview of the data processing of the TELRI
Catalogue items.
4 Catalogue Contents
The catalogue currently contains only a few sam-
ple entries, which, nevertheless, exemplify the
kinds of software that are to be most relevant for
inclusion into the catalogue:
 tools that at least one TELRI partner has ex-
perience in using and that the partner is will-
ing to support for new users
 tools that are available free of cost, at least
for academic purposes and, preferably, are
open source
 tools that are language independent or adapt
easily to new languages
 tools that are primarily meant for corpus pro-
cessing
At present, the catalogue lists the following
tools:
 The morpho-syntactic tagger TnT (Brants,
2000)
A robust and very efficient statistical part-
of-speech tagger that is trainable on differ-
ent languages and on virtually any tagset. It
is available by a license agreement which is
free of charge for non-commercial purposes.
Distribution is available, in binaries only, for
Linux and SunOS/Solaris.
Figure 1: The TELRI Catalogue HTML form
*name = Name of product
*task = Task of product
author = Name(s) of author(s)
affiliation = Name of company
street = Address of company
city
country
version = Version number
language = Language(s)
*description
licres = License conditions for research purposes
liccom = License conditions for commercial purposes
restrict = License restrictions
source url = URL of source code
binary url = URL of binary files
platform = Supported hardware
os = Supperted operating system(s)
impl = Language of implementation
interface = User interface
homepage = URL of homepage
doc url = URL of documentation
doc lang = Language of documentation
*helpline = TELRI helpline
Table 2: Full list of fields of the Catalogue HTML form
Figure 2: A sample output page of one Catalogue item
user input User?readableformats:
html, pdf etc.
DocBook
XML
Perl/CGI XSLTHTML?form
Figure 3: Overview of the catalogue data processing
 The IMS Corpus Workbench concordancer
(Christ, 1994)
Comprises a powerful Corpus Query Proces-
sor and a graphical user interface. It is avail-
able by a license agreement which is free of
charge for non-commercial purposes. Distri-
bution, in binary form only, is available for
Linux and SunOS/Solaris.
 The Vanilla sentence aligner (Danielsson
and Ridings, 1997)
A simple but useful program that aligns
a parallel corpus by comparing sentence
lengths in characters by dynamic time-
warping. The program assumes that hard
boundaries are correctly aligned and per-
forms alignment on soft boundaries. It is
freely available with C source code distribu-
tion.
 The Twente Word Aligner (Hiemstra, 1998)
The program constructs a bilingual lexicon
from a parallel sentence aligned corpus. The
translations are ranked according to com-
puted confidence. The system uses statisti-
cal measures and works for single words (to-
kens) only. It is available under the GNU
General Public License and is written in C.
 PLUG Word Aligner (Ahrenberg et al,
1998)
The system integrates a set of modules for
knowledge-lite approaches to word align-
ment, with various possibilities to change
configuration and to adapt the system to
other language pairs and text types. The
system takes a parallel sentence aligned cor-
pus as input and produces a list of word and
phrase correspondences in the text (link in-
stances) and additionally a bilingual lexicon
from these instances (type links). It is avail-
able by a license agreement which is free of
charge for non-commercial purposes. Distri-
bution is available, in binary form only, for
Linux and MS Windows.
5 Conclusions
The paper reported on the set-up of the TELRI
corpus-tool catalogue, concentrating on the tech-
nical issues involved in its creation (form inter-
face), storage (DocBook) and display (XSLT). At
present, the input form is operational and the cata-
logue contains a few sample entries and has a pre-
liminary (default) rendering of its contents. The
current version of the catalogue and templates is
available at http://nl.ijs.si/telri/
In the future, we hope to flesh out the cat-
alogue with more tools, and enlist the services
of TELRI experts in providing user support for
them. The catalogue will, where license permits,
also archive a copy of the software, and will con-
tinue with a proactive adoption of the GNU li-
cense and open standards.
The open (non-profit) nature of the tools we
attempt to identify lends them well for pedago-
cial purposes at the graduate and undergraduate
courses in natural language processing, corpus
linguistics and language engineering.
The tool catalogue, as well as TRACTOR,
could also be made a part of the Open Language
Archives Community mentioned in the introduc-
tion. To join OLAC a number of changes and
mappings would have to be defined, say from the
on-line form onto Dublin Core and the OLAC
Metadata Set. The choices currently listed in the
template could also be changed into a controlled
vocabulary to facilitate searching.
The process of catalogue updates is currently
manual. To automate the production of the on-
line version of the catalogue directly from new
form entries would be relatively easy, given suf-
ficient volume to justify this. More challeng-
ing would be (semi)automatic tracking of new
tools that become available via various (OLAC)
archives and announcements.
Acknowledgements
The authors would like to thank Inguna Greitane
for her exposition of the catalogue structure vo-
cabulary, Laurent Romary for his invaluable assis-
tance with everything XSLT; and Victor Nagy for
his technical assistance in preparing the HTML
form and the CGI script.
Thanks also to the anonymous reviewers for
their valuable comments on the previous version
of the paper; for all remaining errors, only the au-
thors are to blame.
The work report here was supported by the
Copernicus TELRI-II concerted action.
References
Lars Ahrenberg, Mikael Andersson, and Magnus
Merkel. 1998. A simple hybrid aligner for gener-
ating lexical correspondences in parallell texts. In
COLING/ACL.
Steven Bird and Gary Simons. 2000. Open language
archives community. ElsNews, 9(4).
Thorsten Brants. 2000. Tnt - a statistical part-
of-speech tagger. In Proceedings of the Sixth
Applied Natural Language Processing Conference
ANLP-2000, Seattle, WA. http://www.coli.uni-
sb.de/?thorsten/tnt/.
Oliver Christ. 1994. A modular and flexible archi-
tecture for an integrated corpus query system. In
Proceedings of COMPLEX ?94: 3rd Conference
on Computational Lexicography and Text Research,
Budapest, Hungary. CMP-LG archive id 9408005.
Pernilla Danielsson and Daniel Ridings. 1997. Prac-
tical presentation of a ?vanilla? aligner. In Pre-
sented at the TELRI Workshop on Alignment and
Exploitation of Texts. Institute Joz?ef Stefan, Ljubl-
jana. http://nl.ijs.si/telri/Vanilla/doc/ljubljana/.
Thierry Declerck, Alexander Werner Jachmann, and
Hans Uszkoreit. 2000. The new edition of the
natural language software registry (an initiative
of acl hosted at dfki). In Second International
Conference on Language Resources and Evalua-
tion, LREC?00, pages 1129?1132. Paris. ELRA.
http://registry.dfki.de/.
Djoerd Hiemstra. 1998. Multilingual domain mod-
eling in Twenty-One: automatic creation of a bi-
directoral translation lexicon from a parallel cor-
pus. In Proceedings Computational Linguistics in
the Nederlands, pages 41?57. Nijmegen.
Ide Nancy and Laurent Romary. 2001. A Com-
mon Framework for Syntactic Annotation. In ACL,
Toulouse.
Ide Nancy, Laurent Romary, and Patrice Bonhomme.
2000. CES/XML : An XML-based Standard for
Linguistic Corpora. In Second International Con-
ference on Language Resources and Evaluation,
LREC?00, pages 825?830. Paris. ELRA.
W3C. 2000. Extensible stylesheet language (XSL)
version 1.0. URL. http://www.w3.org/TR/xsl.
Norman Walsh. 1999. DocBook: The Defini-
tive Guide. O?Reilly & Associates, Inc.
http://docbook.org/.
Multiword Units in an MT Lexicon 
 
 
 Tam?s V?radi 
Linguistics Institute 
Hungarian Academy of Sciences 
 varadi@nytud.hu 
 
 
  
 
Abstract 
Multiword units significantly contribute 
to the robustness of MT systems as they 
reduce the inevitable ambiguity inherent 
in word to word matching. The paper fo-
cuses on a relatively little studied kind of 
MW units which are partially fixed and 
partially productive. In fact, MW units 
will be shown to form a continuum be-
tween completely frozen expression 
where the lexical elements are specified 
at the level of particular word forms and 
those which are produced by syntactic 
rules defined in terms of general part of 
speech categories. The paper will argue 
for the use of local grammars proposed 
by Maurice Gross to capture the produc-
tive regularity of MW units and will il-
lustrate a uniform implementation of 
them in the NooJ grammar development 
framework. 
1 Introduction 
The robustness of MT systems crucially depend 
on the size and quality of their lexical compo-
nenets. It is commonly recognized that word-to-
word equivalents are fraught with ambiguities. 
MW units on the other hand carry, as it were, the 
disambiguating context with them. Hence, the 
more MW units in the lexicon and the longer 
they are, the less noisy and more robust the MT 
lexicon is likely to be. However, not all kinds of 
MW units are amenable to inclusion by itemized 
listing in the lexicon. The paper will focus on 
MW units whose structure contains slots that can 
be filled by more or less open ended lexical 
units. They are treated in paper dictionaries with 
the usual method of exemplification and implica-
tion, which, even if the intended extension of the 
set of expression is clear, is obviously not a vi-
able option in a machine system that cannot rely 
on the linguistic competence and world knowl-
edge that human readers of dictionaries are ex-
pected to bring to the job of interpreting lexical 
entries. 
2 The multi-word unit continuum 
In order to develop first an intuitive grasp of the 
phenomena, consider the following examples. 
1)  English-speaking population 
 French-speaking clients 
 Spanish-speaking students 
It would not be difficult to carry on with fur-
ther examples, each embodying a pattern <lan-
guage-name> speaking <person> or <group of 
persons>. It is a prototypical example for our 
purposes because the words are interdependent 
yet they admit of open-choice in the selection of 
lexical items for certain positions. The phrases 
*speaking students, English-speaking, or English 
population are either not well-formed or does not 
mean the same as the full expression. The mean-
ing of the phrase is predominantly, if perhaps not 
wholly, compositional and for native language 
speakers the structure may seem entirely trans-
parent. However, in a bilingual context this 
transparency does not necessarily carry over to 
the other language. For example, the phrases in 
(1) are expressed in Hungarian as in 2) 
2)Angol nyelv?   lakoss?g 
  English language-Adj  population 
  Fracia nyelv?  ?gyfelek 
  French language-Adj  clients 
  Spanyol nyelv?  di?kok 
  Spanish language-Adj  students 
The Hungarian equivalent bears the same charac-
teristics of semantic compositionality and struc-
tural transparency and is open-ended in the same 
points as the corresponding slots in the English 
73
pattern. It would be extremely wasteful to cap-
ture the bilingual correspondences in an itemized 
manner, particularly as the set of expressions on 
both sides are open-ended anyway.  
At the other end of the scale in terms of pro-
ductivity and compositionality one finds phrases 
like those listed in 3) 
3) English breakfast 
   French fries 
   German measles 
Purely from a formal point of view, the phrases 
in 3) could be captured in the pattern <language 
name><noun> but the co-occurrence relations 
between items in the two sets are limited to the 
extreme so that once they are defined properly, 
we are practically thrown back to the particular 
one-to-one combinations listed in 3). 
Note that if we had a set like 4), where one 
element is shared it would still not make sense 
make sense to factorize the shared word French 
because it enters into idiomatic semantic rela-
tions. In other words, the multi-word expressions 
are semantically non-compositional even in 
terms of English alone. 
4) French bread 
   French horn 
   French dressing 
The set of terms in 5) exemplifies the other end 
of the scale in terms of compositionality and syn-
tactic transparency. They are adduced here to 
exemplify fully regular combinations of words in 
their literal meaning. 
5) French schools 
   French vote 
   French books 
   French drivers 
In between the wholly idiosyncratic expressions 
which need to be listed in the lexicon and the set 
of completely open-choice expressions which 
form the province of syntax, there is a whole 
gamut of expressions that seem to straddle the 
lexicon-syntax divide. They are non-
compositional in meaning to some extent and 
they also include elements that come from a 
more or less open set. Some of these open-choice 
slots in the expressions may be filled with items 
from sets that are either infinite (like numbers) or 
numerous enough to render them hopeless or 
wasteful for listing in a dictionary. For this rea-
son, they are typically not fully specified in dic-
tionaries, which have no of means of represent-
ing them explicitely in any other way than by 
listing. For want of anything better, lexicogra-
phers rely on the linguistic intelligence of their 
readers to infer from a partial list the correct set 
of items that a given lexical unit applies to. Bol-
inger (Bolinger 1965)  elegantly sums up this 
approach as 
Dictionaries do not exist to define, but to help people 
grasp meaning, and for this purpose their main task is 
to supply a series of hints and associations that will re-
late the unknown to something known. 
Adroit use of this technique may be quite suc-
cessful with human readers but is obviously not 
viable for NLP purposes. What is needed is some 
algorithmic module in order to model the encod-
ing/decoding processing that humans do in ap-
plying their mental lexicon. The most economi-
cal and sometimes the only viable means to 
achieve this goal is to integrate some kind of 
rule-based mechanism that would support the 
recognition as well as generation of all the lexi-
cal units that conventional dictionaries evoke 
through well-chosen partial set of data. 
3 Local grammars 
Local Grammars, developed by Maurice Gross 
(Gross 1997), are heavily lexicalized finite state 
grammars devised to capture the intricacies of 
local syntactic or semantic phenomena. In the 
mid-nineties a very efficient tool, INTEX was 
developed at LADL, Paris VII, (Silberztein 
1999) which has two components that are of 
primary importance to us: it contains a complex 
lexical component (Silberztein 1993) and a 
graphical interface which supports the develop-
ment of finite state transducers in the form of 
graphs (Silberztein 1999). 
Local grammars are typically defined in 
graphs which are compiled into efficient finite 
state automata or transducers. Both the lexicon 
and the grammar are implemented in finite state 
transducers. This fact gives us the ideal tool to 
implement the very kind of lexicon we have been 
arguing for, one that includes both static entries 
and lexical grammars. 
The set of expressions discussed in 1) can be 
captured with the graph in Figure 1. It shows a 
simple finite state automaton of a single with 
through three nodes along the way from the ini-
tial symbol on the left to the end symbol on the 
right. It represents all the expressions that match 
as the graph is traversed between the two points. 
Words in angle brackets stand for the lemma 
form, the shaded box represent a subgraph that 
can freely be embedded in graphs. The facility of  
74
 
Figure 1 INTEX/NOOJ graph to capture phrases like Eng-
lish-speaking students 
graph embedding has the practical convenience 
that it allows the reuse of the subgraph in other 
contexts. At a more theoretical level, it intro-
duces the power of recursion into grammars. 
Subgraphs may also be used to represent a se-
mantic class, such as language name in the pre-
sent case, and can be encoded in the dictionary 
with a semantic feature like +LANGNAME. IN-
TEX/NOOJ dictionaries allow an arbitrary num-
ber of semantic features to be represented in the 
lexical entries and they can be used in the defini-
tion of local grammars as well. An alternative 
grammar using semantic features is displayed in 
Figure 2. 
 
Figure 2 Representing the phrases in Figure 1 with seman-
tic features 
Note that to render expressions like in 2) we use 
local grammars containing nodes that range from 
specific word forms through lemmas, lists of 
words, words defined by a semantic class in an 
ontology to syntactic class or even the com-
pletely general placeholder for any word. Such 
flexibility allows us to apply the constraint de-
fined at the right level of generality required to 
cover exactly the set of expressions without 
overgeneration. 
The local grammars defining the kind of par-
tially productive multi-word units that the pre-
sent paper focuses on can typically be defined 
with the nodes being defined in terms of some 
natural semantic class such as the language 
names of examples 2) or names of colours or 
body parts illustrated in 6) 
6a) the lady in black 
6b) a fekete ruh?s h?lgy 
    the black clad lady 
The English expression in 6a) can be imple-
mented with the graph in Figure 3, its Hungarian 
equivalent 6b) is displayed in Figure 4. 
 
Figure 3 Local grammar to cover the expressions like 6a)  
 
Figure 4 Local grammar to cover the expressions like 6b) 
The use of semantic features is merely the first 
step in building an efficient lexicon. At a more 
advanced level, the lexicon would include a sys-
tem of semantic features arranged into typed hi-
erarchy, which would allow use of multiple in-
heritence. 
4 Application of local grammars 
In the present section we provide some examples 
of how rendering multi-word units with local 
grammars can enhance a multi-lingual applica-
tion. 
4.1 Semantic disambiguation 
The use of transducers in INTEX/NOOJ provides 
an intuitive and user-friendly means of semantic 
disambiguation as illustrated in Figure 5. Here 
the appropriate meaning of the specific node is 
defined by its Hungarian equivalent, but of 
course one might just as well have used mono-
lingual tags for the same purpose. 
75
 Figure 5. Semantic disambiguation with the use of local 
grammars 
4.2 Partial automatic translation 
On the analogy of shallow parsing, we may 
compile transducers that produce as output the 
target language equivalent of the chunks recog-
nized. This is illustrated in Figure 6 where the 
expressions ?trade/trading in dollar/yen? etc. are 
rendered as ?doll?rkereskedelem, jenker-
eskedelem? etc. whereas ?trade/trading in To-
kyo/London? etc. are translated as 
?t?ki?i/londoni keresked?s?. Note that the recog-
nized words are stored in a variable captured by 
the labelled brackets and used in the compilation 
of the output. 
 
Figure 5 Partial translation transducers using variables 
4.3 Automatic lexical acquisition 
Local grammars can be used not only for recog-
nition and generation but also for automated 
lexical acquisition. This can be achieved by 
suitably relaxing the constraints on one or more 
of the nodes in a graph and apply it to a large 
corpus. The resulting hit expressions can then be 
manually processed to find the semantic feature 
underlying the expressions or establish further 
subclasses etc. 
As an example, consider Figure 7 containing a 
graph designed to capture expressions describing 
various kinds of graters in English. As Figure 6 
shows the entry for grater in the Oxford Ad-
vanced dictionary (Wehmeier 2005) uses only 
hints through specific examples as to what sort of 
graters there may be in English 
 
Figure 6 Part of the dictionary entry GRATE from OALD7 
The node <MOT> matches an arbitrary word in 
INTEX, the symbol <E> covers an empty ele-
ment, used here in disjunction the syntactic cate-
gory <DET> to turn the latter optional. 
 
Figure 7 Automatic aquisition of multi-word units with 
local grammars 
5 Conclusions 
In the present paper we have highlighted the im-
portance of multi-word units that are partially 
productive. Far from being peripheral, they ap-
pear to be ubiquitous particularly when viewed 
in a multilingual setting. Many of these expres-
sions including such common phrases like a 
twenty year old woman may not be 
viewed as multi-word expressions at all until one 
realizes the syntactic/semantic constraints in-
volved in their structure (e.g. *year old 
woman). More importantly, once their transla-
tion to another language is not entirely transpar-
ent (i.e. they cannot be rendered word by word), 
the crosslingual transfer must be registered. It is 
suitably done in traditional dictionaries through a 
single example, but in an MT system such reli-
ance on the active contribution of the human user 
is not an option. Nor is exhaustive listing, as 
proved by this simple but extremely common 
example. 
76
We have shown how the use of local gram-
mars can provide the flexibility required to cover 
the phenomena of partially productive multi-
word units which form a continuum between fro-
zen multi-word expressions and open-ended pro-
ductive phrases defined by syntactic rules sensi-
tive to part of speech categories only. 
The local grammars were illustrated in some 
multilingual applications using the grammar de-
velopment environment INTEX/NOOJ, which 
provide an intuitive and linguistically sophisti-
cated tool to explore the use of the multi-word 
units in question. 
References 
Bolinger, D. (1965). "The Atomization of Meaning." 
Language 41: 555-573. 
Gross, M. (1997). The Construction of Local Gram-
mars. in  Y. S. Emmanuel Roche (szerk.) Finite 
State Language Processing. MIT Press: 329-352. 
Sag, I. et al 2002 Multiword Expressions: A Pain in 
the Neck for NLP. in Proceedings of the Third In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics CICLING 
2002): 1--15, 
Silberztein, M. (1993). Dictionnaires ?lectorniques et 
analyse automatique de textes: le systeme INTEX. 
Paris, Masson. 
Silberztein, M. (1999). "Text Indexation with IN-
TEX." Computers and the Humanities 33(3): 265-
280. 
Wehmeier, S., (szerk.) (2005). Oxford Advanced 
Learner's Dictionary. Oxford, Oxford University 
Press. 
 
77
 78 
