A Resource-light Approach to Russian Morphology: Tagging Russian using
Czech resources
Jiri Hana and Anna Feldman and Chris Brew
Department of Linguistics
Ohio State University
Columbus, OH 43210
Abstract
In this paper, we describe a resource-light system
for the automatic morphological analysis and tag-
ging of Russian. We eschew the use of extensive
resources (particularly, large annotated corpora and
lexicons), exploiting instead (i) pre-existing anno-
tated corpora of Czech; (ii) an unannotated corpus
of Russian. We show that our approach has benefits,
and present what we believe to be one of the first full
evaluations of a Russian tagger in the openly avail-
able literature.
1 Introduction
Morphological processing and part-of-speech tag-
ging are essential for many NLP tasks, including
machine translation, information retrieval and pars-
ing. In this paper, we describe a resource-light ap-
proach to the tagging of Russian. Because Russian
is a highly inflected language with a high degree
of morpheme homonymy (cf. Table 11) the tags in-
volved are more numerous and elaborate than those
typically used for English. This complicates the tag-
ging task, although as has been previously noted
(Elworthy, 1995), the increased complexity of the
tags does not necessarily translate into a more de-
manding tagging task. Because no large annotated
corpora of Russian are available to us, we instead
chose to use an annotated corpus of Czech. Czech
is sufficiently similar to Russian that it is reasonable
to suppose that information about Czech will be rel-
evant in some way to the tagging of Russian.
The languages share many linguistic properties (free
word order and a rich morphology which plays
a considerable role in determining agreement and
argument relationships). We created a morpho-
logical analyzer for Russian, combined the results
with information derived from Czech and used the
TnT (Brants, 2000) tagger in a number of differ-
1All Russian examples in this paper are transcribed in the
Roman alphabet. Our system is able to analyze Russian texts
in both Cyrillic and various transcriptions.
krasiv-a beautiful (short adjective, feminine)
muz?-a husband (noun, masc., sing., genitive)
husband (noun, masc., sing., accusative)
okn-a window (noun, neuter, sing., genitive)
window (noun, neuter, pl., nominative)
window (noun, neuter, pl., accusative)
knig-a book (noun, fem., sing., nominative)
dom-a house (noun, masc., sing., genitive)
house (noun, masc., pl., nominative)
house (noun, masc., pl., accusative)
skazal-a say (verb, fem., sing., past tense)
dv-a two (numeral, masc., nominative)
Table 1: Homonymy of the a ending
ent ways, including a a committee-based approach,
which turned out to give the best results. To eval-
uate the results, we morphologically annotated (by
hand) a small corpus of Russian: part of the transla-
tion of Orwell?s ?1984? from the MULTEXT-EAST
project (Ve?ronis, 1996).
2 Why TnT?
Readers may wonder why we chose to use TnT,
which was not designed for Slavic languages. The
short answer is that it is convenient and successful,
but the following two sections address the issue in
rather more detail.
2.1 The encoding of lexical information in TnT
TnT records some lexical information in the emis-
sion probabilities of its second order Markov
Model. Since Russian and Czech do not use the
same words we cannot use this information (at least
not directly) to tag Russian. Given this, the move
from Czech to Russian involves a loss of detailed
lexical information. Therefore we implemented a
morphological analyzer for Russian, the output of
which we use to provide surrogate emission proba-
bilities for the TnT tagger (Brants, 2000). The de-
tails are described below in section 4.2.
2.2 The modelling of word order in TnT
Both Russian and Czech have relatively free word
order, so it may seem an odd choice to use a Markov
model (MM) tagger. Why should second order
MM be able to capture useful facts about such lan-
guages? Firstly, even if a language has the poten-
tial for free word order, it may still turn out that
there are recurring patterns in the progressions of
parts-of-speech attested in a training corpus. Sec-
ondly, n-gram models including MM have indeed
been shown to be successful for various Slavic lan-
guages, e.g., Czech (Hajic? et al, 2001) or Slovene
(Dz?eroski et al, 2000); although not as much as
for English. This shows that the transitional in-
formation captured by the second-order MM from
a Czech or Slovene corpus is useful for Czech or
Slovene.2 The present paper shows that transitional
information acquired from Czech is also useful for
Russian.
3 Russian versus Czech
A deep comparative analysis of Czech and Russian
is far beyond the scope of this paper. However, we
would like to mention just a number of the most im-
portant facts. Both languages are Slavic (Czech is
West Slavonic, Russian is East Slavonic). Both have
extensive morphology whose role is important in
determining the grammatical functions of phrases.
In both languages, the main verb agrees in person
and number with subject; adjectives agree in gen-
der, number and case with nouns. Both languages
are free constituent order languages. The word or-
der in a sentence is determined mainly by discourse.
It turns out that the word order in Czech and Russian
is very similar. For instance, old information mostly
precedes new information. The ?neutral? order in
the two languages is Subject-Verb-Object. Here is a
parallel Czech-Russian example from our develop-
ment corpus:
(1) a. [Czech]
Byl
wasMasc.Past
jasny?,
brightMasc.Sg.Nom
studeny?
coldMasc.Sg.Nom
dubnovy?
AprilMasc.Sg.Nom
den
dayMasc.Sg.Nom
i
and
hodiny
clocksFem.P l.Nom
odb??jely
strokeFem.P l.Past
tr?ina?ctou.
thirteenthFem.Sg.Acc
b. [Russian]
2Respectively, and if the techniques in the present paper
generalize, probably also irrespectively.
Byl
wasMasc.Past
jasnyj,
brightMasc.Sg.Nom
xolodnyj
coldMasc.Sg.Nom
aprel?skij
AprilMasc.Sg.Nom
den?
dayMasc.Sg.Nom
i
and
c?asy
clocksPl.Nom
probili
strokePl.Past
trinadtsat?.
thirteenAcc
?It was a bright cold day in April, and the
clocks were striking thirteen.? [from Orwell?s
?1984?]
Of course, not all utterances are so similar. Sec-
tion 5.4 briefly mentions how to improve the utility
of the corpus by eradicating some of the systematic
differences.
4 Realization
4.1 The tag system
We adopted the Czech tag system (Hajic?, 2000) for
Russian. Every tag is represented as a string of 15
symbols each corresponding to one morphological
category. For example, the word vidjela is assigned
the tag VpFS- - -XR-AA- - -, because it is a verb (V),
past participle (p), feminine (F), singular (S), does
not distinguish case (-), possessive gender (-), pos-
sessive number (-), can be any person (X), is past
tense (R), is not gradable (-), affirmative (A), active
voice (A), and does not have any stylistic variants
(the final hyphen).
No. Description Abbr. No. of values
Cz Ru
1 POS P 12 12
2 SubPOS ? detailed POS S 75 32
3 Gender g 11 5
4 Number n 6 4
5 Case c 9 8
6 Possessor?s Gender G 5 4
7 Possessor?s Number N 3 3
8 Person p 5 5
9 Tense t 5 5
10 Degree of comparison d 4 4
11 Negation a 3 3
12 Voice v 3 3
13 Unused 1 1
14 Unused 1 1
15 Variant, Style V 10 2
Table 2: Overview and comparison of the tagsets
The tagset used for Czech (4290+ tags) is larger
than the tagset we use for Russian (about 900 tags).
There is a good theoretical reason for this choice
? Russian morphological categories usually have
fewer values (e.g., 6 cases in Russian vs. 7 in Czech;
Czech often has formal and colloquial variants of
the same morpheme); but there is also an immedi-
ate practical reason ? the Czech tag system is very
elaborate and specifically devised to serve multiple
needs, while our tagset is designed solely to capture
the core of Russian morphology, as we need it for
our primary purpose of demonstrating the portabil-
ity and feasibility of our technique. Still, our tagset
is much larger than the Penn Treebank tagset, which
uses only 36 non-punctuation tags (Marcus et al,
1993).
4.2 Morphological analysis
In this section we describe our approach to a
resource-light encoding of salient facts about the
Russian lexicon. Our techniques are not as rad-
ical as previously explored unsupervised methods
(Goldsmith, 2001; Yarowsky and Wicentowski,
2000), but are designed to be feasible for languages
for which serious morphological expertise is un-
available to us. We use a paradigm-based morphol-
ogy that avoids the need to explicitly create a large
lexicon. The price that we pay for this is overgener-
ation. Most of these analyses look very implausible
to a Russian speaker, but significantly increasing the
precision would be at the cost of greater develop-
ment time than our resource-light approach is able
to commit. We wish our work to be portable at least
to other Slavic languages, for which we assume that
elaborate morphological analyzers will not be avail-
able. We do use two simple pre-processing methods
to decrease the ambiguity of the results handed to
the tagger ? longest ending filtering and an automat-
ically acquired lexicon of stems. These were easy to
implement and surprisingly effective.
Our analyzer captures just a few textbook facts
about the Russian morphology (Wade, 1992), ex-
cluding the majority of exceptions and including in-
formation about 4 declension classes of nouns, 3
conjugation classes of verbs. In total our database
contains 80 paradigms. A paradigm is a set of end-
ings and POS tags that can go with a particular set
of stems. Thus, for example, the paradigm in Table
3 is a set of inflections that go with the masculine
stems ending on the ?hard? consonants, e.g., slon
?elephant?, stol ?table?.
Unlike the traditional notions of stem and ending,
for us a stem is the part of the word that does not
change within its paradigm, and the ending is the
part of the word that follows such a stem. For ex-
ample, the forms of the verb moc?? ?can.INF?: mogu
?1sg?, moz?es?? ?2sg?, moz?et ?3sg?, etc. are analyzed as
0 NNMS1 - - - - - - - - - - y NNMP1 - - - - - - - - - -
a NNMS2 - - - - - - - - - - ov NNMP2 - - - - - - - - - -
u NNMS3 - - - - - - - - - - am NNMP3 - - - - - - - - - -
a NNMS4 - - - - - - - - - - ov NNMP4 - - - - - - - - - -
u NNMS4 - - - - - - - - - 1
e NNMS6 - - - - - - - - - - ax NNMP6 - - - - - - - - - -
u NNMS6 - - - - - - - - - 1
om NNMS7 - - - - - - - - - - ami NNMP7 - - - - - - - - - -
Table 3: A paradigm for ?hard? consonant mascu-
line nouns
the stem mo followed by the endings gu, z?es??, z?et. A
more linguistically oriented analysis would involve
the endings u, es??, et and phonological alternations
in the stem. All stem internal variations are treated
as suppletion.3
Unlike the morphological analyzers that exist for
Russian (Segalovich and Titov, 2000; Segalovich,
2003; Segalovich and Maslov, 1989; Kovalev, 2002;
Mikheev and Liubushkina, 1995; Yablonsky, 1999;
Segalovich, 2003; Kovalev, 2002, among others)
(Segalovich, 2003; Kovalev, 2002; Mikheev and Li-
ubushkina, 1995; Yablonsky, 1999, among others),
our analyzer does not rely on a substantial manu-
ally created lexicon. This is in keeping with our aim
of being resource-light. When analyzing a word,
the system first checks a list of monomorphemic
closed-class words and then segments the word into
all possible prefix-stem-ending triples.4 The result
has quite good coverage (95.4%), but the average
ambiguity is very high (10.9 tags/token), and even
higher for open class words. We therefore have two
strategies for reducing ambiguity.
4.2.1 Longest ending filtering (LEF)
The first approach to ambiguity reduction is based
on a simple heuristic ? the correct ending is usually
one of the longest candidate endings. In English, it
would mean that if a word is analyzed either as hav-
ing a zero ending or an -ing ending, we would con-
sider only the latter; obviously, in the vast majority
of cases that would be the correct analysis. In addi-
tion, we specify that a few long but very rare end-
ings should not be included in the maximum length
calculation (e.g., 2nd person pl. imperative).
3We do in fact have a very similar analysis, the analyzer?s
run-time representation of the paradigms is automatically pro-
duced from a more compact and linguistically attractive spec-
ification of the paradigms. It is possible to specify the ba-
sic paradigms and then specify the subparadigms, exceptions
and paradigms involving phonological changes by referring to
them.
4Currently, we consider only two inflectional prefixes ? neg-
ative ne and superlative nai.
4.2.2 Deriving a lexicon
The second approach uses a large raw corpus5 to
generate an open class lexicon of possible stems
with their paradigms. In this paper, we can only
sketch the method, for more details see (Hana and
Feldman, to appear). It is based on the idea that
open-class lemmata are likely to occur in more than
one form. First, we run the morphological analyzer
on the text (without any filtering), then we add to
the lexicon those entries that occurred with at least a
certain number of distinct forms and cover the high-
est number of forms. If we encounter the word talk-
ing, using the information about paradigms, we can
assume that it is either the -ing form of the lemma
talk or that it is a monomorphemic word (such as
sibling). Based on this single form we cannot really
say more. However, if we also encounter the forms
talk, talks and talked, the former analysis seems
more probable; and therefore, it seems reasonable
to include the lemma talk as a verb into the lexi-
con. If we encountered also talkings, talkinged and
talkinging, we would include both lemmata talk and
talking as verbs.
Obviously, morphological analysis based on such
a lexicon overgenerates, but it overgenerates much
less than if based on the endings alone. For ex-
ample, for the word form partii of the lemma par-
tija ?party?, our analysis gives 8 possibilities ? the
5 correct ones (noun fem sg gen/dat/loc sg and pl
nom/acc) and 3 incorrect ones (noun masc sg loc,
pl nom, and noun neut pl acc; note that only gen-
der is incorrect). Analysis based on endings alone
would allow 20 possibilities ? 15 of them incorrect
(including adjectives and an imperative).
4.3 Tagging
We use the TnT tagger (Brants, 2000), an imple-
mentation of the Viterbi algorithm for second order
Markov models. We train the transition probabili-
ties on Czech (1.5M tokens of the Prague Depen-
dency Treebank (Be?mova? et al, 1999)). We ob-
tain surrogate emission probabilities by running our
morphological analyzer, then assuming a uniform
distribution over the resulting emissions.
5 Experiments
5.1 Corpora
For evaluation purposes, we selected and morpho-
logically annotated (by hand) a small portion from
5We used The Uppsala Russian Corpus (1M tokens), which
is freely available from Uppsala University at http://www.
slaviska.uu.se/ryska/corpus.html.
the Russian translation of Orwell?s ?1984?. This cor-
pus contains 4011 tokens and 1858 types. For devel-
opment, we used another part of ?1984?. Since we
want to work with minimal language resources, the
development corpus is intentionally small ? 1788 to-
kens. We used it to test our hypotheses and tune the
parameters of our tools.
In the following sections, we discuss our experi-
ments and report the results. Note that we do not
report the results for tag position 13 and 14, since
these positions are unused; and therefore, always
trivially correct.
5.2 Morphological analysis
As can be seen from Table 4, morphological anal-
ysis without any filters gives good recall (although
on a non-fiction text it would probably be lower),
but also very high average ambiguity. Both fil-
ters (the longest-ending filter and automatically ac-
quired lexicon) reduce the ambiguity significantly;
the former producing a considerable drop of recall,
the latter retaining high recall. However, we do best
if we first attempt lexical lookup, then apply LEF
to the words not found. This keeps recall reason-
ably high at the same time as decreasing ambiguity.
As expected, performance increases with the size of
the unannotated Russian corpus used to generate the
lexicon. All subsequent experimental results were
obtained using this best filter combination, i.e., the
combination of the lexicon based on the 1Mword
corpus and LEF.
LEF no no no yes yes yes
Lexicon based on 0 100K 1M 0 100K 1M
recall 95.4 94 93.1 84.4 88.3 90.4
avg ambig (tag/word) 10.9 7.0 4.7 4.1 3.5 3.1
Tagging ? accuracy 50.7 62.1 67.5 62.1 66.8 69.4
Table 4: Morph. analysis with various parameters
5.3 Tagging
Table 7 summarizes the results of our taggers on test
data. Our baseline is produced by the morphologi-
cal analyzer without any filters followed by a tagger
randomly selecting a tag among the tags offered by
the morphological analyzer. The direct-full tag col-
umn shows the result of the TNT tagger with transi-
tion probabilities obtained directly from the Czech
corpus and the emission symbols based on the mor-
phological analyzer with the best filters.
To further improve the results, we used two tech-
niques: (i) we modified the training corpus to re-
move some systematic differences between Czech
and Russian (5.4); (ii) we trained batteries of tag-
gers on subtags to address the data sparsity problem
(5.5 and 5.6).
5.4 Russification
We experimented with ?russified? models. We
trained the TnT tagger on the Czech corpus with
modifications that made the structure of training
data look more like Russian. For example, plural
adjectives and participles in Russian, unlike Czech,
do not distinguish gender.
(2) a. Nadan??
Giftedmasc.pl
muz?i
men
soutez?ili.
competedmasc.pl
?Gifted sportsmen were competing.? [Cz]
b. Nadane?
Giftedfem.pl
z?eny
women
soutez?ily.
competedfem.pl
?Gifted women were competing.? [Cz]
c. Nadana?
Giftedneut.pl
de?vc?ata
girlsneut
soute?z?ila.
competingneut.pl
?Gifted girls were competing.? [Cz]
d. Talantlivye
Giftedpl
muz?c?iny/z?ens?c?iny
men/women
sorevnovalis?.
competedpl
?Gifted men/women were competing.?[Ru]
Negation in Czech is in the majority of cases is ex-
pressed by the prefix ne-, whereas in Russian it is
very common to see a separate particle (ne) instead:
(3) a. Nic
nothing
ner?ekl.
not-said
?He didn?t say anything.? [Cz]
b. On
he
nic?ego
nothing
ne
not
skazal.
said
?He didn?t say anything.? [Ru]
In addition, reflexive verbs in Czech are formed by a
verb followed by a reflexive clitic, whereas in Rus-
sian, the reflexivization is the affixation process:
(4) a. Filip
Filip
se
REFL-CL
jes?te?
still
nehol??.
not-shaves
?Filip doesn?t shave yet.? [Cz]
b. Filip
Filip
esc?e
still
ne
not
breet+sja.
shaves+REFL.SUFFIX
?Filip doesn?t shave yet.? [Ru]
Even though auxiliaries and the copula are the forms
of the same verb byt? ?to be?, both in Russian and in
Czech, the use of this verb is different in the two
languages. For example, Russian does not use an
auxiliary to form past tense:
(5) a. Ja?
I
jsem
aux1sg
psal.
wrote
?I was writing/I wrote.? [Cz]
b. Ja
I
pisal.
wrote
?I was writing/I wrote.? [Ru]
It also does not use the present tense copula, except
for emphasis; but it uses forms of the verb byt? in
some other constructions like past passive.
We implemented a number of simple ?russifica-
tions?. The combination of random omission of the
verb byt?, omission of the reflexive clitics, and nega-
tion transformation gave us the best results on the
development corpus. Their combination improves
the overall result from 68.0% to 69.4%. We admit
we expected a larger improvement.
5.5 Sub-taggers
One of the problems when tagging with a large
tagset is data sparsity; with 1000 tags there are
10003 potential trigrams. It is very unlikely that a
naturally occurring corpus will contain all the ac-
ceptable tag combinations with sufficient frequency
to reliably distinguish them from the unacceptable
combinations. However, not all morphological at-
tributes are useful for predicting the attributes of the
succeeding word (e.g., tense is not really useful for
case). We therefore tried to train the tagger on indi-
vidual components of the full tag, in the hope that
each sub-tagger would be able to learn what it needs
for prediction. This move has the additional bene-
fit of making the tag set of each such tagger smaller
and reducing data sparsity. We focused on the first 5
positions ? POS (P), SubPOS (S), gender (g), num-
ber (n), case (c) and person (p). The selection of
the slots is based on our linguistic intuition ? for
example it is reasonable to assume that the infor-
mation about part-of-speech and the agreement fea-
tures (gnc) of previous words should help in pre-
diction of the same slots of the current word; or
information about part-of-speech, case and person
should assist in determining person. On the other
hand, the combination of tense and case is prima fa-
cie unlikely to be much use for prediction. Indeed,
most of our expectations have been met. The perfor-
mance of some of the models on the development
corpus is summarized in Table 5. The bold num-
bers indicate that the tagger outperforms the full-tag
tagger. As can be seen, the taggers trained on indi-
vidual positions are worse than the full-tag tagger
on these positions. This proves that a smaller tagset
does not necessarily imply that tagging is easier ?
see (Elworthy, 1995) for more discussion of this in-
teresting relation. Similarly, there is no improve-
ment from the combination of unrelated slots ? case
and tense (ct) or gender and negation (ga). How-
ever, the combinations of (detailed) part-of-speech
with various agreement features (e.g., Snc) outper-
form the full-tag tagger on at least some of the slots.
full-tag P S g n c
1 (P) 89.0 87.2 ? ? ? ?
2 (S) 86.6 ? 84.5 ? ? ?
3 (g) 81.4 ? ? 78.8 ? ?
4 (n) 92.4 ? ? ? 91.2 ?
5 (c) 80.9 ? ? ? ? 78.4
full-tag Pc gc ga nc cp ct
1 (P) 89.0 87.5 ? ? ? ? ?
2 (S) 86.6 ? ? ? ? ? ?
3 (g) 81.4 ? 80.4 78.7 ? ? ?
4 (n) 92.4 ? ? ? 91.8 ? ?
5 (c) 80.9 80.6 81.1 ? 81.5 79.3 79.5
8 (p) 98.3 ? ? ? ? 96.9 ?
9 (t) 97.0 ? ? ? ? ? 96.1
11 (a) 97.0 ? ? 95.4 ? ? ?
full-tag Pgc Pnc Sgc Snc Sgnc
1 (P) 89.0 87.9 87.5 ? ? ?
2 (S) 86.6 ? ? 86.1 86.4 87.1
3 (g) 81.4 80.3 ? 81.4 ? 82.7
4 (n) 92.4 ? 92.4 ? 93.0 92.8
5 (c) 80.9 81.8 81.4 80.9 82.9 82.3
Table 5: Performance of the TnT tagger trained on
various subtags (development data)
5.6 Combining Sub-taggers
We now need to put the sub-tags back together to
produce estimates of the correct full tags. We can-
not simply combine the values offered by the best
taggers for each slot, because that could yield ille-
gal tags (e.g., nouns in past tense). Instead we select
the best tag from those offered by our morphologi-
cal analyzer using the following formula:
(6) bestTag = argmaxt?TMAval(t)
TMA ? the set of tags offered by MA
val(t) =?14k=0 Nk(t)/Nk
Nk(t) ? # of taggers voting for k-th slot of t
Nk ? the total # of taggers on slot k
That means, that the best tag is the tag that received
the highest average percentage of votes for each of
full-tag all best 1 best 3
overall 69.5 70.3 70.7 71.1
1 (P) 89.0 88.9 89.1 89.2
2 (S) 86.6 86.5 86.9 86.9
3 (g) 81.4 81.8 83.0 83.2
4 (n) 92.4 92.6 93.1 93.2
5 (c) 80.9 82.1 83.0 83.2
6 (G) 98.5 98.5 98.7 98.7
7 (N) 99.6 99.7 99.8 99.8
8 (p) 98.3 98.2 98.4 98.3
9 (t) 97.0 97.0 97.0 97.0
10 (G) 96.0 96.0 96.0 96.0
11 (a) 97.0 97.0 96.9 97.0
12 (v) 97.4 97.3 97.5 97.4
15 (V) 99.1 99.1 99.0 99.0
Table 6: Combining sub-taggers (development data)
Baseline Direct Russified Russified
Tagger random full-tag full-tag voting
Accuracy
Tags 33.6 69.4 72.6 73.5
1 (POS) 63.2 88.5 90.1 90.4
2 (SubPOS) 57.0 86.8 88.1 88.6
3 (Gender) 59.2 82.5 84.5 85.0
4 (Number) 75.9 91.2 92.6 93.4
5 (Case) 47.3 80.4 84.1 85.3
6 (PossGen) 83.4 98.4 98.8 99.0
7 (PossNr) 99.6 99.6 99.6 99.8
8 (Person) 97.1 99.3 98.9 98.9
9 (Tense) 86.6 96.5 97.6 97.6
10 (Grade) 90.1 95.9 96.6 96.6
11 (Neg) 81.4 95.3 95.5 95.5
12 (Voice) 86.4 97.2 97.9 97.9
15 (Variant) 97.0 99.1 99.5 99.5
Table 7: Tagging with various parameters (test data)
its slots. If we cared about certain slots more than
about others we could weight the slots in the val
function.
We ran several experiments, the results of three of
them are summarized in Table 6. All of them work
better than the full-tag tagger. One (?all?) uses all
available subtaggers, other (?best 1?) uses the best
tagger for each slot (therefore voting in Formula 6
reduces to finding a closest legal tag). The best re-
sult is obtained by the third tagger (?best 3?) which
uses the three best taggers for each of the Pgcp slots
and the best tagger for the rest. We selected this tag-
ger to tag the test corpus, for which the results are
summarized in Table 7.
Russian Gloss Correct Xerox Ours
?Clen member noun nom gen
partii party noun gen obl
po prep prep obl acc
vozmoz?nosti possibility noun obl acc
staralsja tried vfin
nje not ptcl
govorit? to-speak vinf
ni nor ptcl
o about prep obl
Bratstvje Brotherhood noun obl
, cm
ni nor ptcl
o about prep obl
knigje book noun obl
Errors 3 1
?Neither the Brotherhood nor the book was a subject
that any ordinary Party member would mention if
there was a way of avoiding it.? [Orwell: ?1984?]
Table 8: Tagging with Xerox & our tagger
5.7 Comparison with Xerox tagger
A tagger for Russian is part of the Xerox language
tools. We could not perform a detailed evaluation
since the tool is not freely available. We used the
online demo version of Xerox?s Disambiguator6 to
tag a few sentences and compared the results with
the results of our tagger. The Xerox tagset is much
smaller than ours, it uses 63 tags, collapsing some
cases, not distinguishing gender, number, person,
tense etc. (However, it uses different tags for dif-
ferent punctuation, while we have one tag for all
punctuation). For the comparison, we translated our
tagset to theirs. On 201 tokens of the testing cor-
pus, the Xerox tagger achieved an accuracy of 82%,
while our tagger obtained 88%; i.e., a 33% reduc-
tion in error rate. A sample analysis is in Table 8.
5.8 Comparison with Czech taggers
The numbers we obtain are significantly worse than
the numbers reported for Czech (Hajic? et al, 2001)
(95.16% accuracy); however, they use an extensive
manually created morphological lexicon (200K+
entries) which gives 100.0% recall on their testing
data. Moreover, they train and test their taggers on
the same language.
6 Ongoing Research
We are currently working on improving both the
morphological analysis and tagging. We would like
6http://www.xrce.xerox.com/
competencies/content-analysis/demos/
russian
to improve the recall of filters following morpholog-
ical analysis, e.g., using n maximal values instead
of 1, using some basic knowledge of derivational
morphology, etc. We are incorporating phonological
conditions on stems into the guesser module as well
as trying to deal with different morphological phe-
nomena specific to Russian, e.g., verb reflexiviza-
tion. However, we try to stay language independent
(at least within Slavic languages) as much as possi-
ble and limit the language dependent components to
a minimum.
Currently, we are working on more sophisticated
russifications that would be still easily portable to
other languages. For example, instead of omitting
auxiliaries randomly, we want to use the syntac-
tic information present in Prague Dependency Tree-
bank to omit only the ?right? ones.
If possible, we would like to avoid entirely throw-
ing away the Czech emission probabilities, because
our intuition tells us that there are useful lexical
similarities between Russian and Czech, and that
some suitable process of cognate detection will al-
low us to transfer information from the Czech to
the Russian emission probabilities. Just as a knowl-
edge of English words is sometimes helpful (mod-
ulo sound changes) when reading German, a knowl-
edge of the Czech lexicon should be helpful (mod-
ulo character set issues) when reading Russian. We
are seeking the right way to operationalize this in-
tuition in our system, bearing in mind that we want
a sufficiently general algorithm to make the method
portable to other languages, for which we assume
we have neither the time nor the expertise to under-
take knowledge-intensive work. A potentially suit-
able cognate algorithm is described by (Kondrak,
2001).
Finally, we would like to extend our work to Slavic
languages for which there are even fewer available
resources than Russian, such as Belarusian, since
this was the original motivation for undertaking the
work in the first place.
Acknowledgements
We thank Erhard Hinrichs and Eric Fosler-Lussier
for giving us feedback on previous versions of the
paper and providing useful suggestions for subtag-
gers and voting; Jan Hajic? for the help with the
Czech tag system and the morphological analyzer;
to the Clippers discussion group for allowing us to
interview ourselves in front of them, and for ensuing
discussion, and to two anonymous EMNLP review-
ers for extremely constructive feedback.
References
Alena Be?mova?, Jan Hajic?, Barbora Hladka?, and
Jarmila Panevova?. 1999. Morphological and syn-
tactic tagging of the prague dependency treebank.
In Proceedings of ATALA Workshop, pages 21?29.
Paris, France.
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger. In Proceedings of ANLP-NAACL,
pages 224?231.
Sas?o Dz?eroski, Tomaz? Erjavec, and Jakub
Zavrel. 2000. Morphosyntactic Tagging of
Slovene:Evaluating Taggers and Tagsets. In Pro-
ceedings of the Second International Conference
on Language Resources and Evaluation, pages
1099?1104.
David Elworthy. 1995. Tagset design and inflected
languages. In EACL SIGDAT workshop ?From
Texts to Tags: Issues in Multilingual Language
Analysis?, pages 1?10, Dublin, April.
John Goldsmith. 2001. Unsupervised Learning of
the Morphology of a Natural Language. Computa-
tional Linguistics, 27(2):153?198.
Jan Hajic?, Pavel Krbec, Pavel Kve?ton?, Karel Oliva,
and Vladim??r Petkevic?. 2001. Serial Combination
of Rules and Statistics: A Case Study in Czech Tag-
ging. In Proceedings of ACL Conference, Toulouse,
France.
Jan Hajic?. 2000. Morphological Tagging: Data
vs. Dictionaries. In Proceedings of ANLP-NAACL
Conference, pages 94?101, Seattle, Washington,
USA.
Jiri Hana and Anna Feldman. to appear. Portable
Language Technology: The case of Czech and Rus-
sian. In Proceedings from the Midwest Computa-
tional Linguistics Colloquium, June 25-26, 2004,
Bloomington, Indiana.
Greg Kondrak. 2001. Identifying cognates by pho-
netic and semantic similarity. In Proceedings of
the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-2001), pages 103?110, June.
Andrey Kovalev. 2002. A Probabilistic Mor-
phological Analyzer for Russian and Ukranian.
http://linguist.nm.ru/stemka/stemka.html.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Andrei Mikheev and Liubov Liubushkina. 1995.
Russian Morphology: An Engineering Approach.
Natural Language Engineering, 3(1):235?260.
Ilya Segalovich and Michail Maslov. 1989.
Dictionary-based Russian morphological analy-
sis and synthesis with generation of morpho-
logical models of unknown words (in Russian).
http://company.yandex.ru/articles/article1.html.
Ilya Segalovich and Vitaly Titov. 2000. Au-
tomatic morphological annotation MYSTEM.
http://corpora.narod.ru/article.html.
Ilya Segalovich. 2003. A fast morpholog-
ical algorithm with unknown word guessing
induced by a dictionary for a web search
engine. http://company.yandex.ru/articles/iseg-las-
vegas.html.
Jean Ve?ronis. 1996. MULTEXT-
EAST (Copernicus 106).
http://www.lpl.univaix.fr/projects/multext-east.
Terence Wade. 1992. A Comprehensive Russian
Grammar. Blackwell. 582 pp.
Serge A. Yablonsky. 1999. Russian Morphological
Analysis. In Proceedings VEXTAL.
David Yarowsky and Richard Wicentowski. 2000.
Minimally supervised morphological analysis by
multimodal alignment. In Proceedings of the 38th
Meeting of the Association for Computational Lin-
guistics, pages 207?216.
Tagging Portuguese with a Spanish Tagger Using Cognates
Jirka Hana
Department of Linguistics
The Ohio State University
hana.1@osu.edu
Anna Feldman
Department of Linguistics
The Ohio State University
afeldman@ling.osu.edu
Luiz Amaral
Department of Spanish and Portuguese
The Ohio State University
amaral.1@osu.edu
Chris Brew
Department of Linguistics
The Ohio State University
cbrew@acm.org
Abstract
We describe a knowledge and resource
light system for an automatic morpholog-
ical analysis and tagging of Brazilian Por-
tuguese.1 We avoid the use of labor in-
tensive resources; particularly, large anno-
tated corpora and lexicons. Instead, we
use (i) an annotated corpus of Peninsular
Spanish, a language related to Portuguese,
(ii) an unannotated corpus of Portuguese,
(iii) a description of Portuguese morphol-
ogy on the level of a basic grammar book.
We extend the similar work that we have
done (Hana et al, 2004; Feldman et al,
2006) by proposing an alternative algo-
rithm for cognate transfer that effectively
projects the Spanish emission probabili-
ties into Portuguese. Our experiments use
minimal new human effort and show 21%
error reduction over even emissions on a
fine-grained tagset.
1 Introduction
Part of speech (POS) tagging is an important step
in natural language processing. Corpora that have
been POS-tagged are very useful both for linguis-
tic research, e.g. finding instances or frequencies
of particular constructions (Meurers, 2004) and for
further computational processing, such as syntac-
tic parsing, speech recognition, stemming, word-
sense disambiguation. Morphological tagging is
the process of assigning POS, case, number, gen-
der and other morphological information to each
word in a corpus. Despite the importance of mor-
phological tagging, there are many languages that
1We thank the anonymous reviewers for their constructive
comments on an earlier version of the paper.
lack annotated resources of this kind, mainly due
to the lack of training corpora which are usually
required for applying standard statistical taggers.
Applications of taggers include syntactic pars-
ing, stemming, text-to-speech synthesis, word-
sense disambiguation, information extraction. For
some of these getting all the tags right is inessen-
tial, e.g. the input to noun phrase chunking does
not necessarily require high accuracy fine-grained
tag resolution.
Cross-language information transfer is not new;
however, most of the existing work relies on par-
allel corpora (e.g. Hwa et al, 2004; Yarowsky
and Ngai, 2001) which are difficult to find, es-
pecially for lesser studied languages. In this pa-
per, we describe a cross-language method that re-
quires neither training data of the target language
nor bilingual lexicons or parallel corpora. We re-
port the results of the experiments done on Brazil-
ian Portuguese and Peninsular Spanish, however,
our system is not tied to these particular languages.
The method is easily portable to other (inflected)
languages. Our method assumes that an anno-
tated corpus exists for the source language (here,
Spanish) and that a text book with basic linguis-
tic facts about the source language is available
(here, Portuguese). We want to test the generality
and specificity of the method. Can the systematic
commonalities and differences between two ge-
netically related languages be exploited for cross-
language applications? Is the processing of Por-
tuguese via Spanish different from the processing
of Russian via Czech (Hana et al, 2004; Feldman
et al, 2006)?
33
Spanish Portuguese
1. sg. canto canto
2. sg. cantas cantas
3. sg. canta canta
1. pl. catamos cantamos
2. pl. cantais cantais
3. pl. cantan cantam
Table 1: Verb conjugation present indicative: -ar
regular verb: cantar ?to sing?
2 Brazilian Portuguese (BP)
vs. Peninsular Spanish (PS)
Portuguese and Spanish are both Romance lan-
guages from the Iberian Peninsula, and share
many morpho-syntactic characteristics. Both lan-
guages have a similar verb system with three main
conjugations (-ar, -er, -ir), nouns and adjectives
may vary in number and gender, and adverbs are
invariable. Both are pro-drop languages, they have
a similar pronominal system, and certain phenom-
ena, such as clitic climbing, are prevalent in both
languages. They also allow rather free constituent
order; and in both cases there is considerable de-
bate in the literature about the appropriate char-
acterization of their predominant word order (the
candidates being SVO and VSO).
Sometimes the languages exhibit near-complete
parallelism in their morphological patterns, as
shown in Table 1.
The languages are also similar in their lexicon and
syntactic word order:
(1) Os
Los
The
estudantes
estudiantes
students
ja?
ya
already
comparam
compraron
bought
os
los
the
livros.
libros.
books
[BP]
[PS]
?The students have already bought the
books.?
One of the main differences is the fact that
Brazilian Portuguese (BP) accepts object drop-
ping, while Peninsular Spanish (PS) doesn?t. In
addition, subjects in BP tend to be overt while in
PS they tend to be omitted.
(2) a. A: O que
What
voce?
you
fez
did
com
with
o
the
livro?
book?
[BP]
A: ?What did you do with the book??
B: Eu
I
dei
gave
para
to
Maria.
Mary
B: ?I gave it to Mary.?
b. A: ?Que?
What
hiciste
did
con
with
el
the
libro?
book?
[PS]
A: ?What did you do with the book??
B: Se
Her.dat
lo
it.acc
di
gave
a
to
Mar??a.
Mary.
B: ?I gave it to Mary.?
Notice also that in the Spanish example (2b) the
dative pronoun se ?her? is obligatory even when
the prepositional phrase a Mar??a ?to Mary? is
present.
3 Resources
3.1 Tagset
For both Spanish and Portuguese, we used posi-
tional tagsets developed on the basis of Spanish
CLiC-TALP tagset (Torruella, 2002). Every tag is
a string of 11 symbols each corresponding to one
morphological category. For example, the Por-
tuguese word partires ?you leave? is assigned the
tag VM0S---2PI-, because it is a verb (V), main
(M), gender is not applicable to this verb form (0),
singular (S), case, possesor?s number and form are
not applicable to this category(-), 2nd person (2),
present (P), indicative (I) and participle type is not
applicable (-).
A comparison of the two tagsets is in Table 2.2
When possible the Spanish and Portuguese tagsets
use the same values, however some differences are
unavoidable. For instance, the pluperfect is a com-
pound verb tense in Spanish, but a separate word
that needs a tag of its own in Portuguese. In ad-
dition, we added a tag for ?treatment? Portuguese
pronouns.
The Spanish tagset has 282 tags, while that for
Portuguese has 259 tags.
3.2 Training corpora
Spanish training corpus. The Spanish corpus
we use for training the transition probabilities as
well as for obtaining Spanish-Portuguese cognate
pairs is a fragment (106,124 tokens, 18,629 types)
of the Spanish section of CLiC-TALP (Torruella,
2Notice that we have 6 possible values for the gender po-
sition: M (masc.), F (fem.), N (neutr., for certain pronouns), C
(common, either M or F), 0 (unspecified for this form within
the category), - (the category does not distinguish gender)
34
No. Description No. of values
Sp Po
1 POS 14 11
2 SubPOS ? detailed POS 30 29
3 Gender 6 6
4 Number 5 5
5 Case 6 6
6 Possessor?s Number 4 4
7 Form 3 3
8 Person 5 5
9 Tense 7 9
10 Mood 8 9
11 Participle 3 3
Table 2: Overview and comparison of the tagsets
2002). CLiC-TALP is a balanced corpus, contain-
ing texts of various genres and styles. We automat-
ically translated the CLiC-TALP tagset into our
system (see Sect. 3.1) for easier detailed evalua-
tion and for comparison with our previous work
that used a similar approach for tagging (Hana
et al, 2004; Feldman et al, 2006).
Raw Portuguese corpus. For automatic lexi-
con acquisition, we use NILC corpus,3 containing
1.2M tokens.
3.3 Evaluation corpus
For evaluation purposes, we selected and manually
annotated a small portion (1,800 tokens) of NILC
corpus.
4 Morphological Analysis
Our morphological analyzer (Hana, 2005) is an
open and modular system. It allows us to com-
bine modules with different levels of manual in-
put ? from a module using a small manually pro-
vided lexicon, through a module using a large lex-
icon automatically acquired from a raw corpus, to
a guesser using a list of paradigms, as the only
resource provided manually. The general strat-
egy is to run modules that make fewer errors and
less overgenerate before modules that make more
errors and overgenerate more. This, for exam-
ple, means that modules with manually created
resources are used before modules with resources
3Nu?cleo Interdisciplinar de Lingu???stica Computacional;
available at http://nilc.icmc.sc.usp.br/nilc/,
we used the version with POS tags assigned by PALAVRAS.
We ignored the POS tags.
automatically acquired. In the experiments below,
we used the following modules ? lookup in a list
of (mainly) closed-class words, a paradigm-based
guesser and an automatically acquired lexicon.
4.1 Portuguese closed class words
We created a list of the most common preposi-
tions, conjunctions, and pronouns, and a number
of the most common irregular verbs. The list con-
tains about 460 items and it required about 6 hours
of work. In general, the closed class words can be
derived either from a reference grammar book, or
can be elicited from a native speaker. This does
not require native-speaker expertise or intensive
linguistic training. The reason why the creation
of such a list took 6 hours is that the words were
annotated with detailed morphological tags used
by our system.
4.2 Portuguese paradigms
We also created a list of morphological paradigms.
Our database contains 38 paradigms. We just en-
coded basic facts about the Portuguese morphol-
ogy from a standard grammar textbook (Cunha
and Cintra, 2001). The paradigms include all three
regular verb conjugations (-ar, -er, -ir), the most
common adjective and nouns paradigms and a rule
for adverbs of manner that end with -mente (anal-
ogous to the English -ly). We ignore majority of
exceptions. The creation of the paradigms took
about 8 h of work.
4.3 Lexicon Acquisition
The morphological analyzer supports a module or
modules employing a lexicon containing informa-
tion about lemmas, stems and paradigms. There is
always the possibility to provide this information
manually. That, however, is very costly. Instead,
we created such a lexicon automatically.
Usually, automatically acquired lexicons and
similar systems are used as a backup for large
high-precision high-cost manually created lexi-
cons (e.g. Mikheev, 1997; Hlava?c?ova?, 2001). Such
systems extrapolate the information about the
words known by the lexicon (e.g. distributional
properties of endings) to unknown words. Since
our approach is resource light, we do not have any
such large lexicon to extrapolate from.
The general idea of our system is very sim-
ple. The paradigm-based Guesser, provides all the
possible analyses of a word consistent with Por-
tuguese paradigms. Obviously, this approach mas-
35
sively overgenerates. Part of the ambiguity is usu-
ally real but most of it is spurious. We use a large
corpus to weed the spurious analyses out of the
real ones. In such corpus, open-class lemmas are
likely to occur in more than one form. Therefore,
if a lemma+paradigm candidate suggested by the
Guesser occurs in other forms in other parts of the
corpus, it increases the likelihood that the candi-
date is real and vice versa. If we encounter the
word cantamos ?we sing? in a Portuguese corpus,
using the information about the paradigms we can
analyze it in two ways, either as being a noun in
the plural with the ending -s, or as being a verb in
the 1st person plural with the ending -amos. Based
on this single form we cannot say more. However
if we also encounter the forms canto, canta, can-
tam the verb analysis becomes much more prob-
able; and therefore, it will be chosen for the lex-
icon. If the only forms that we encounter in our
Portuguese corpus were cantamos and (the non-
existing) cantamo (such as the existing word ramo
and ramos) then we would analyze it as a noun and
not as a verb.
With such an approach, and assuming that the
corpus contains the forms of the verb matar ?to
kill?, mato1sg matas2sg, mata3sg, etc., we would
not discover that there is also a noun mata ?forest?
with a plural form matas ? the set of the 2 noun
forms is a proper subset of the verb forms. A sim-
ple solution is to consider not the number of form
types covered in a corpus, but the coverage of the
possible forms of the particular paradigm. How-
ever this brings other problems (e.g. it penalizes
paradigms with large number of forms, paradigms
with some obsolete forms, etc.). We combine both
of these measures in Hana (2005).
Lexicon Acquisition consists of three steps:
1. A large raw corpus is analyzed with a
lexicon-less MA (an MA using a list of
mainly closed-class words and a paradigm
based guesser);
2. All possible hypothetical lexical entries over
these analyses are created.
3. Hypothetical entries are filtered with aim to
discard as many nonexisting entries as possi-
ble, without discarding real entries.
Obviously, morphological analysis based on
such a lexicon still overgenerates, but it overgener-
ates much less than if based on the endings alone.
Lexicon no yes
recall 99.0 98.1
avg ambig (tag/word) 4.3 3.5
Tagging (cognates) ? accuracy 79.1 82.1
Table 3: Evaluation of Morphological analysis
Consider for example, the form func?o?es ?func-
tions? of the feminine noun func?a?o. The analyzer
without a lexicon provides 11 analyses (6 lemmas,
each with 1 to 3 tags); only one of them is cor-
rect. In contrast, the analyzer with an automati-
cally acquired lexicon provides only two analyses:
the correct one (noun fem. pl.) and an incorrect
one (noun masc. pl., note that POS and number
are still correct). Of course, not all cases are so
persuasive.
The evaluation of the system is in Table 3. The
98.1% recall is equivalent to the upper bound for
the task. It is calculated assuming an oracle-
Portuguese tagger that is always able to select the
correct POS tag if it is in the set of options given
by the morphological analyzer. Notice also that
for the tagging accuracy, the drop of recall is less
important than the drop of ambiguity.
5 Tagging
We used the TnT tagger (Brants, 2000), an im-
plementation of the Viterbi algorithm for second-
order Markov model. In the traditional approach,
we would train the tagger?s transitional and emis-
sion probabilities on a large annotated corpus of
Portuguese. However, our resource-light approach
means that such corpus is not available to us and
we need to use different ways to obtain this infor-
mation.
We assume that syntactic properties of Spanish
and Portuguese are similar enough to be able to
use the transitional probabilities trained on Span-
ish (after a simple tagset mapping).
The situation with the lexical properties as cap-
tured by emission probabilities is more complex.
Below we present three different ways how to ob-
tains emissions, assuming:
1. they are the same: we use the Spanish emis-
sions directly (?5.1).
2. they are different: we ignore the Spanish
emissions and instead uniformly distribute
36
the results of our morphological analyzer.
(?5.2)
3. they are similar: we map the Spanish emis-
sions onto the result of morphological analy-
sis using automatically acquired cognates.
(?5.3)
5.1 Tagging ? Baseline
Our lowerbound measurement consists of training
the TnT tagger on the Spanish corpus and apply-
ing this model directly to Portuguese.4 The overall
performance of such a tagger is 56.8% (see the the
min column in Table 4). That means that half of
the information needed for tagging of Portuguese
is already provided by the Spanish model. This
tagger has seen no Portuguese whatsoever, and is
still much better than nothing.
5.2 Tagging ? Approximating Emissions I
The opposite extreme to the baseline, is to assume
that Spanish emissions are useless for tagging Por-
tuguese. Instead we use the morphological an-
alyzer to limit the number of possibilities, treat-
ing them all equally ? The emission probabilities
would then form a uniform distribution of the tags
given by the analyzer. The results are summarized
in Table 4 (the e-even column) ? accuracy 77.2%
on full tags, or 47% relative error reduction against
the baseline.
5.3 Tagging ? Approximating Emissions II
Although it is true that forms and distributions of
Portuguese and Spanish words are not the same,
they are also not completely unrelated. As any
Spanish speaker would agree, the knowledge of
Spanish words is useful when trying to understand
a text in Portuguese.
Many of the corresponding Portuguese and
Spanish words are cognates, i.e. historically they
descend from the same ancestor root or they are
mere translations. We assume two things: (i) cog-
nate pairs have usually similar morphological and
distributional properties, (ii) cognate words are
similar in form.
Obviously both of these assumptions are ap-
proximations:
1. Cognates could have departed in their mean-
ings, and thus probably also have dif-
4Before training, we translated the Spanish tagset into the
Portuguese one.
ferent distributions. For example, Span-
ish embarazada ?pregnant? vs. Portuguese
embarac?ada ?embarrassed?.
2. Cognates could have departed in their mor-
phological properties. For example, Span-
ish cerca ?near?.adverb vs. Portuguese cerca
?fence?.noun (from Latin circa, circus ?cir-
cle?).
3. There are false cognates ? unrelated,
but similar or even identical words. For
example, Spanish salada ?salty?.adj vs. Por-
tuguese salada ?salad?.noun, Spanish doce
?twelve?.numeral vs. Portuguese doce
?candy?.noun
Nevertheless, we believe that these examples
are true exceptions from the rule and that in major-
ity of cases, the cognates would look and behave
similarly. The borrowings, counter-borrowings
and parallel developments of the various Romance
languages have of course been extensively studied,
and we have no space for a detailed discussion.
Identifying cognates. For the present work,
however, we do not assume access to philologi-
cal erudition, or to accurate Spanish-Portuguese
translations or even a sentence-aligned corpus. All
of these are resources that we could not expect to
obtain in a resource poor setting. In the absence
of this knowledge, we automatically identify cog-
nates, using the edit distance measure (normalized
by word length).
Unlike in the standard edit distance, the cost of
operations is dependent on the arguments. Simi-
larly as Yarowsky and Wicentowski (2000), we as-
sume that, in any language, vowels are more muta-
ble in inflection than consonants, thus for example
replacing a for i is cheaper that replacing s by r.
In addition, costs are refined based on some well
known and common phonetic-orthographic regu-
larities, e.g. replacing a q with c is less costly than
replacing m with, say s. However, we do not want
to do a detailed contrastive morpho-phonological
analysis, since we want our system to be portable
to other languages. So, some facts from a simple
grammar reference book should be enough.
Using cognates. Having a list of Spanish-
Portuguese cognate pairs, we can use these to
map the emission probabilities acquired on Span-
ish corpus to Portuguese.
37
Let?s assume Spanish word ws and Portuguese
word wp are cognates. Let Ts denote the tags that
ws occurs within the Spanish corpus, and let ps(t)
be the emission probability of a tag t (t 6? Ts ?
ps(t) = 0). Let Tp denote tags assigned to the
Portuguese word wp by our morphological ana-
lyzer, and the pp(t) is the even emission proba-
bility: pp(t) = 1|Tp| . Then we can assign the new
emission probability p?p(t) to every tag t ? Tp in
the following way (followed by normalization):
p?p(t) =
ps(t) + pp(t)
2
(1)
Results. This method provides the best results.
The full-tag accuracy is 82.1%, compared to
56.9% for baseline (58% error rate reduction) and
77.2% for even-emissions (21% reduction). The
accuracy for POS is 87.6%. Detailed results are in
column e-cognates of Table 4.
6 Evaluation & Comparison
The best way to evaluate our results would be to
compare it against the TnT tagger used the usual
way ? trained on Portuguese and applied on Por-
tuguese. We do not have access to a large Por-
tuguese corpus annotated with detailed tags. How-
ever, we believe that Spanish and Portuguese are
similar enough (see Sect. 2) to justify our assump-
tion that the TnT tagger would be equally success-
ful (or unsuccessful) on them. The accuracy of
TnT trained on 90K tokens of the CLiC-TALP cor-
pus is 94.2% (tested on 16K tokens). The accuracy
of our best tagger is 82.1%. Thus the error-rate is
more than 3 times bigger (17.9% vs. 5.4%).
Branco and Silva (2003) report 97.2% tagging
accuracy on 23K testing corpus. This is clearly
better than our results, on the other hand they
needed a large Portuguese corpus of 207K tokens.
The details of the tagset used in the experiments
are not provided, so precise comparison with our
results is difficult.
7 Related work
Previous research in resource-light language
learning has defined resource-light in different
ways. Some have assumed only partially tagged
training corpora (Merialdo, 1994); some have be-
gun with small tagged seed wordlists (Cucerzan
and Yarowsky, 1999) for named-entity tagging,
while others have exploited the automatic trans-
fer of an already existing annotated resource in a
min e-even e-cognates
Tag: 56.9 77.2 82.1
POS: 65.3 84.2 87.6
SubPOS: 61.7 83.3 86.9
gender: 70.4 87.3 90.2
number: 78.3 95.3 96.0
case: 93.8 96.8 97.2
possessor?s num: 85.4 96.7 97.0
form: 92.9 99.2 99.2
person: 74.5 91.2 92.7
tense: 90.7 95.1 96.1
mood: 91.5 95.0 96.0
participle: 99.9 100.0 100.0
Table 4: Tagging Brazilian Portuguese
different genres or a different language (e.g. cross-
language projection of morphological and syn-
tactic information in (Yarowsky et al, 2001;
Yarowsky and Ngai, 2001), requiring no direct su-
pervision in the target language).
Ngai and Yarowsky (2000) observe that the to-
tal weighted human and resource costs is the most
practical measure of the degree of supervision.
Cucerzan and Yarowsky (2002) observe that an-
other useful measure of minimal supervision is the
additional cost of obtaining a desired functional-
ity from existing commonly available knowledge
sources. They note that for a remarkably wide
range of languages, there exist a plenty of refer-
ence grammar books and dictionaries which is an
invaluable linguistic resource.
7.1 Resource-light approaches to Romance
languages
Cucerzan and Yarowsky (2002) present a method
for bootstrapping a fine-grained, broad coverage
POS tagger in a new language using only one
person-day of data acquisition effort. Similarly
to us, they use a basic library reference gram-
mar book, and access to an existing monolingual
text corpus in the language, but they also use a
medium-sized bilingual dictionary.
In our work, we use a paradigm-based mor-
phology, including only the basic paradigms from
a standard grammar textbook. Cucerzan and
Yarowsky (2002) create a dictionary of regular in-
flectional affix changes and their associated POS
and on the basis of it, generate hypothesized in-
flected forms following the regular paradigms.
38
Clearly, these hypothesized forms are inaccurate
and overgenerated. Therefore, the authors perform
a probabilistic match from all lexical tokens actu-
ally observed in a monolingual corpus and the hy-
pothesized forms. They combine these two mod-
els, a model created on the basis of dictionary in-
formation and the one produced by the morpho-
logical analysis. This approach relies heavily on
two assumptions: (i) words of the same POS tend
to have similar tag sequence behavior; and (ii)
there are sufficient instances of each POS tag la-
beled by either the morphology models or closed-
class entries. For richly inflectional languages,
however, there is no guarantee that the latter as-
sumption would always hold.
The accuracy of their model is comparable to
ours. On a fine-grained (up to 5-feature) POS
space, they achieve 86.5% for Spanish and 75.5%
for Romanian. With a tagset of a similar size (11
features) we obtain the accuracy of 82.1% for Por-
tuguese.
Carreras et al (2003) present work on develop-
ing low-cost Named Entity recognizers (NER) for
a language with no available annotated resources,
using as a starting point existing resources for a
similar language. They devise and evaluate several
strategies to build a Catalan NER system using
only annotated Spanish data and unlabeled Cata-
lan text, and compare their approach with a classi-
cal bootstrapping setting where a small initial cor-
pus in the target language is hand tagged. It turns
out that the hand translation of a Spanish model is
better than a model directly learned from a small
hand annotated training corpus of Catalan. The
best result is achieved using cross-linguistic fea-
tures. Solorio and Lo?pez (2005) follow their ap-
proach; however, they apply the NER system for
Spanish directly to Portuguese and train a classi-
fier using the output and the real classes.
7.2 Cognates
Mann and Yarowsky (2001) present a method for
inducing translation lexicons based on trasduction
modules of cognate pairs via bridge languages.
Bilingual lexicons within language families are in-
duced using probabilistic string edit distance mod-
els. Translation lexicons for abitrary distant lan-
guage pairs are then generated by a combination
of these intra-family translation models and one
or more cross-family online dictionaries. Simi-
larly to Mann and Yarowsky (2001), we show that
languages are often close enough to others within
their language family so that cognate pairs be-
tween the two are common, and significant por-
tions of the translation lexicon can be induced with
high accuracy where no bilingual dictionary or
parallel corpora may exist.
8 Conclusion
We have shown that a tagging system with a small
amount of manually created resources can be suc-
cessful. We have previously shown that this ap-
proach can work for Czech and Russian (Hana
et al, 2004; Feldman et al, 2006). Here we have
shown its applicability to a new language pair.
This can be done in a fraction of the time needed
for systems with extensive manually created re-
sources: days instead of years. Three resources
are required: (i) a reference grammar (for infor-
mation about paradigms and closed class words);
(ii) a large amount of text (for learning a lexicon;
e.g. newspapers from the internet); (iii) a limited
access to a native speaker ? reference grammars
are often too vague and a quick glance at results
can provide feedback leading to a significant in-
crease of accuracy; however both of these require
only limited linguistic knowledge.
In this paper we proposed an algorithm for cog-
nate transfer that effectively projects the source
language emission probabilities into the target lan-
guage. Our experiments use minimal new human
effort and show 21% error reduction over even
emissions on a fine-grained tagset.
In the near future, we plan to compare the ef-
fectiveness (time and price) of our approach with
that of the standard resource-intensive approach to
annotating a medium-size corpus (on a corpus of
around 100K tokens). A resource-intensive sys-
tem will be more accurate in the labels which it of-
fers to the annotator, so annotator can work faster
(there are fewer choices to make, fewer keystrokes
required). On the other hand, creation of the in-
frastructure for such a system is very time con-
suming and may not be justified by the intended
application.
The experiments that we are running right now
are supposed to answer the question of whether
training the system on a small corpus of a closely
related language is better than training on a larger
corpus of a less related language. Some prelim-
inary results (Feldman et al, 2006) suggest that
using cross-linguistic features leads to higher pre-
39
cision, especially for the source languages which
have target-like properties complementary to each
other.
9 Acknowledgments
We would like to thank Maria das Grac?as
Volpe Nunes, Sandra Maria Alu??sio, and Ricardo
Hasegawa for giving us access to the NILC cor-
pus annotated with PALAVRAS and to Carlos
Rodr??guez Penagos for letting us use the Spanish
part of the CLiC-TALP corpus.
References
Branco, A. and J. Silva (2003). Portuguese-
specific Issues in the Rapid Development of
State-of-the-art Taggers. In Workshop on Tag-
ging and Shallow Processing of Portuguese:
TASHA?2000.
Brants, T. (2000). TnT ? A Statistical Part-
of-Speech Tagger. In Proceedings of ANLP-
NAACL, pp. 224?231.
Carreras, X., L. Ma`rquez, and L. Padro? (2003).
Named Entity Recognition for Catalan Using
Only Spanish Resources and Unlabelled Data.
In Proceedings of EACL-2003.
Cucerzan, S. and D. Yarowsky (1999). Lan-
guage Independent Named Entity Recognition
Combining Morphological and Contextual Ev-
idence. In Proceedings of the 1999 Joint SIG-
DAT Conference on EMNLP and VLC, pp. 90?
99.
Cucerzan, S. and D. Yarowsky (2002). Boot-
strapping a Multilingual Part-of-speech Tagger
in One Person-day. In Proceedings of CoNLL
2002, pp. 132?138.
Cunha, C. and L. F. L. Cintra (2001). Nova
Grama?tica do Portugue?s Contempora?neo. Rio
de Janeiro, Brazil: Nova Fronteira.
Feldman, A., J. Hana, and C. Brew (2006). Experi-
ments in Morphological Annotation Transfer. In
Proceedings of Computational Linguistics and
Intelligent Text Processing (CICLing).
Hana, J. (2005). Knowledge and labor light mor-
phological analysis. Unpublished manuscript.
Hana, J., A. Feldman, and C. Brew (2004). A
Resource-light Approach to Russian Morphol-
ogy: Tagging Russian using Czech resources.
In Proceedings of EMNLP 2004, Barcelona,
Spain.
Hlava?c?ova?, J. (2001). Morphological Guesser
or Czech Words. In V. Matous?ek (Ed.), Text,
Speech and Dialogue, Lecture Notes in Com-
puter Science, pp. 70?75. Berlin: Springer-
Verlag.
Hwa, R., P. Resnik, A. Weinberg, C. Cabezas,
and O. Kolak (2004). Bootstrapping Parsers via
Syntactic Projection across Parallel Texts. Nat-
ural Language Engineering 1(1), 1?15.
Mann, G. S. and D. Yarowsky (2001). Multipath
Translation Lexicon via Bridge Languages. In
Proceedings of NAACL 2001.
Merialdo, B. (1994). Tagging English Text with
a Probabilistic Model. Computational Linguis-
tics 20(2), 155?172.
Meurers, D. (2004). On the Use of Electronic Cor-
pora for Theoretical Linguistics. Case Studies
from the Syntax of German. Lingua.
Mikheev, A. (1997). Automatic Rule Induction
for Unknown Word Guessing. Computational
Linguistics 23(3), 405?423.
Ngai, G. and D. Yarowsky (2000). Rule Writing or
Annotation: Cost-efficient Resource Usage for
Base Noun Phrase Chunking. In Proceedings of
the 38th Meeting of ACL, pp. 117?125.
Solorio, T. and A. L. Lo?pez (2005). Learning
named entity recognition in Portuguese from
Spanish. In Proceedings of Computational Lin-
guistics and Intelligent Text Processing (CI-
CLing).
Torruella, M. (2002). Gu??a para la anotacio?n mor-
folo?gica del corpus CLiC-TALP (Versio?n 3).
Technical Report WP-00/06, X-Tract Working
Paper.
Yarowsky, D. and G. Ngai (2001). Inducing Mul-
tilingual POS Taggers and NP Bracketers via
Robust Projection Across Aligned Corpora. In
Proceedings of NAACL-2001, pp. 200?207.
Yarowsky, D., G. Ngai, and R. Wicentowski
(2001). Inducing Multilingual Text Analy-
sis Tools via Robust Projection across Aligned
Corpora. In Proceedings of HLT 2001, First
International Conference on Human Language
Technology Research.
Yarowsky, D. and R. Wicentowski (2000). Min-
imally supervised morphological analysis by
multimodal alignment. In Proceedings of the
38th Meeting of the Association for Computa-
tional Linguistics, pp. 207?216.
40
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 11?19,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Error-tagged Learner Corpus of Czech
Jirka Hana
Charles University
Prague, Czech Republic
first.last@gmail.com
Alexandr Rosen
Charles University
Prague, Czech Republic
alexandr.rosen@ff.cuni.cz
Svatava ?kodov?
Technical University
Liberec, Czech Republic
svatava.skodova@tul.cz
Barbora ?tindlov?
Technical University
Liberec, Czech Republic
barbora.stindlova@tul.cz
Abstract
The paper describes a learner corpus of
Czech, currently under development. The
corpus captures Czech as used by non-
native speakers. We discuss its structure,
the layered annotation of errors and the an-
notation process.
1 Introduction
Corpora consisting of texts produced by non-
native speakers are becoming an invaluable source
of linguistic data, especially for foreign language
educators. In addition to morphosyntactic tag-
ging and lemmatisation, common in other corpora,
learner corpora can be annotated by information
relevant to the specific nonstandard language of
the learners. Cases of deviant use can be identi-
fied, emended and assigned a tag specifying the
type of the error, all of which helps to exploit the
richness of linguistic data in the texts. However,
annotation of this kind is a challenging tasks, even
more so for a language such as Czech, with its
rich inflection, derivation, agreement, and largely
information-structure-driven constituent order. A
typical learner of Czech makes errors across all
linguistic levels, often targeting the same form
several times.
The proposed annotation scheme is an attempt
to respond to the requirements of annotating a de-
viant text in such a language, striking a compro-
mise between the limitations of the annotation pro-
cess and the demands of the corpus user. The
three-level format allows for successive emenda-
tions, involving multiple forms in discontinuous
sequences. In many cases, the error type fol-
lows from the comparison of the faulty and cor-
rected forms and is assigned automatically, some-
times using information present in morphosyntac-
tic tags, assigned by a tagger. In more complex
cases, the scheme allows for representing relations
making phenomena such as the violation of agree-
ment rules explicit.
After an overview of issues related to learner
corpora in ?2 and a brief introduction to the project
of a learner corpus of Czech in ?3 we present the
concept of our annotation scheme in ?4, followed
by a description of the annotation process in ?5.
2 Learner corpora
A learner corpus, also called interlanguage or L2
corpus, is a computerised textual database of lan-
guage as produced by second language (L2) learn-
ers (Leech, 1998). Such a database is a very pow-
erful resource in research of second language ac-
quisition. It can be used to optimise the L2 learn-
ing process, to assist authors of textbooks and dic-
tionaries, and to tailor them to learners with a par-
ticular native language (L1).
More generally, a learner corpus ? like other
corpora ? serves as a repository of authentic data
about a language (Granger, 1998). In the do-
main of L2 acquisition and teaching of foreign lan-
guages, the language of the learners is called in-
terlanguage (Selinker, 1983).1 An interlanguage
includes both correct and deviant forms. The pos-
sibility to examine learners? errors on the back-
ground of the correct language is the most impor-
tant aspect of learner corpora (Granger, 1998).
Investigating the interlanguage is easier when
the deviant forms are annotated at least by their
correct counterparts, or, even better, by tags mak-
ing the nature of the error explicit. Although
1Interlanguage is distinguished by its highly individual
and dynamic nature. It is subject to constant changes as
the learner progresses through successive stages of acquiring
more competence, and can be seen as an individual and dy-
namic continuum between one?s native and target languages.
11
learner corpora tagged this way exist, the two
decades of research in this field have shown that
designing a tagset for the annotation of errors is a
task highly sensitive to the intended use of the cor-
pus and the results are not easily transferable from
one language to another.
Learner corpora can be classified according to
several criteria:
? Target language (TL): Most learner corpora
cover the language of learners of English as a
second or foreign language (ESL or EFL). The
number of learner corpora for other languages
is smaller but increasing.
? Medium: Learner corpora can capture written
or spoken texts, the latter much harder to com-
pile, thus less common.
? L1: The data can come from learners with the
same L1 or with various L1s.
? Proficiency in TL: Some corpora gather texts of
students at the same level, other include texts of
speakers at various levels. Most corpora focus
on advanced students.
? Annotation: Many learner corpora contain only
raw data, possibly with emendations, with-
out linguistic annotation; some include part-
of-speech (POS) tagging. Several include er-
ror tagging. Despite the time-consuming man-
ual effort involved, the number of error-tagged
learner corpora is growing.
Error-tagged corpora use the following tax-
onomies to classify the type of error:
? Taxonomies marking the source of error: The
level of granularity ranges from broad cate-
gories (morphology, lexis, syntax) to more spe-
cific ones (auxiliary, passive, etc.).
? Taxonomies based on formal types of alterna-
tion of the source text: omission, addition, mis-
formation, mis-ordering.
? Hierarchical taxonomies based on a combina-
tion of various aspects: error domain (formal,
grammatical, lexical, style errors), error cate-
gory (agglutination, diacritics, derivation inflec-
tion, auxiliaries, gender, mode, etc.), word cat-
egory (POS).
? Without error taxonomies, using only correction
as the implicit explanation for an error.
In Table 1 we present a brief summary of ex-
isting learner corpora tagged by POS and/or er-
ror types, including the size of the corpus (in mil-
lions of words or Chinese characters), the mother
tongue of the learners, or ? in case of learners
with different linguistic backgrounds ? the num-
ber of mother tongues (L1), the TL and the learn-
ers? level of proficiency in TL. For an extensive
overview see, for example (Pravec, 2002; Nessel-
hauf, 2004; Xiao, 2008).
Size L1 TL TL proficiency
ICLE ? Internat?l Corpus of Learner English
3M 21 English advanced
CLC ? Cambridge Learner Corpus
30M 130 English all levels
PELCRA ? Polish Learner English Corpus
0.5M Polish English all levels
USE ? Uppsala Student English Corpus
1.2M Swedish English advanced
HKUST ? Hong Kong University of Science
and Technology Corpus of Learner English
25M Chinese English advanced
CLEC ? Chinese Learner English Corpus
1M Chinese English 5 levels
JEFLL ? Japanese EFL Learner Corpus
0.7M Japanese English advanced
FALKO ? Fehlerannotiertes Lernerkorpus
1.2M various German advanced
FRIDA ? French Interlanguage Database
0.2M various French intermediate
CIC ? Chinese Interlanguage Corpus
2M 96 Chinese intermediate
Table 1: Some currently available learner corpora
3 A learner corpus of Czech
In many ways, building a learner corpus of Czech
as a second/foreign language is a unique enter-
prise. To the best of our knowledge, the CzeSL
corpus (Czech as a Second/Foreign Language) is
the first learner corpus ever built for a highly in-
flectional language, and one of the very few us-
ing multi-layer annotation (together with FALKO
? see Table 1). The corpus consists of 4 subcor-
pora according to the learners? L1:
? The Russian subcorpus represents an interlan-
guage of learners with a Slavic L1.
? The Vietnamese subcorpus represents a numer-
ous minority of learners with very few points of
contact between L1 and Czech.
? The Romani subcorpus represents a linguistic
minority with very specific traits in the Czech
cultural context.
? The ?remnant? subcorpus covers texts from
speakers of various L1s.
The whole extent of CzeSL will be two million
words (in 2012). Each subcorpus is again divided
12
into two subcorpora of written and spoken texts;2
this division guarantees the representative charac-
ter of the corpus data. The corpus is based on
texts covering all language levels according to the
Common European Framework of Reference for
Languages, from real beginners (A1 level) to ad-
vanced learners (level B2 and higher). The texts
are elicited during various situations in classes;
they are not restricted to parts of written examina-
tion. This spectrum of various levels and situations
is unique in the context of other learner corpora.
Each text is equipped with the necessary back-
ground information, including sociological data
about the learner (age, gender, L1, country, lan-
guage level, other languages, etc.) and the sit-
uation (test, homework, school work without the
possibility to use a dictionary, etc.).
4 Annotation scheme
4.1 The feasible and the desirable
The error tagging system for CzeSL is designed to
meet the requirements of Czech as an inflectional
language. Therefore, the scheme is:
? Detailed but manageable for the annotators.
? Informative ? the annotation is appropriate to
Czech as a highly inflectional language.
? Open to future extensions ? it allows for more
detailed taxonomy to be added in the future.
The annotators are no experts in Czech as a for-
eign language or in 2L learning and acquisition,
and they are unaware of possible interferences be-
tween languages the learner knows. Thus they
may fail to recognise an interferential error. A
sentence such as Tokio je pe?kn? hrad ?Tokio is a
nice castle? is grammatically correct, but its au-
thor, a native speaker of Russian, was misled by
?false friends? and assumed hrad ?castle? as the
Czech equivalent of Russian gorod ?town, city?.3
Similarly in Je tam hodne? sklepu? ?There are many
cellars.? The formally correct sentence may strike
the reader as implausible in the context, but it is
impossible to identify and emend the error with-
out the knowledge that sklep in Russian means
?grave?, not ?cellar? (= sklep in Czech).
For some types of errors, the problem is to de-
fine the limits of interpretation. The clause kdyby
citila na tebe zlobna is grammatically incorrect,
2Transcripts of the spoken parts will be integrated with
the rest of the corpus at a later stage of the project.
3All examples are authentic.
yet roughly understandable as ?if she felt angry at
you?. In such cases the task of the annotator is in-
terpretation rather than correction. The clause can
be rewritten as kdyby se na tebe c?tila rozzloben?
?if she felt angry at you?, or kdyby se na tebe zlo-
bila ?if she were angry at you?; the former being
less natural but closer to the original, unlike the
latter. It is difficult to provide clear guidelines.
Errors in word order represent another specific
type. Czech constituent order reflects information
structure and it is sometimes difficult to decide
(even in a context) whether an error is present. The
sentence R?dio je taky na skr??ni ?A radio is also on
the wardrobe? suggests that there are at least two
radios in the room, although the more likely inter-
pretation is that among other things, there is also a
radio, which happens to sit on the wardrobe. Only
the latter interpretation would require a different
word order: Taky je na skr??ni r?dio. Similarly
difficult may be decisions about errors labelled as
lexical and modality.
The phenomenon of Czech diglossia is reflected
in the problem of annotating non-standard lan-
guage, usually individual forms with colloquial
morphological endings. The learners may not be
aware of their status and/or an appropriate context
for their use, and the present solution assumes that
colloquial Czech is emended under the rationale
that the author expects the register of his text to be
perceived as unmarked.
On the other hand, there is the primary goal of
the corpus: to serve the needs of the corpus users.
The resulting error typology is a compromise be-
tween the limitations of the annotation process and
the demands of research into learner corpora.
The corpus can be used for comparisons among
learner varieties of Czech, studied as national in-
terlanguages (Russian, Vietnamese, Romani etc.)
using a matrix of statistic deviations. Similarly in-
teresting are the heterogeneous languages of learn-
ers on different stages of acquisition. From the
pedagogical point of view, corpus-based analy-
ses have led to a new inductive methodology of
data-driven learning, based on the usage of con-
cordances in exercises or to support students? in-
dependent learning activities.
4.2 The framework
Annotated learner corpora sometimes use data for-
mats and tools developed originally for annotating
speech. Such environments allow for an arbitrary
13
segmentation of the input and multilevel annota-
tion of segments (Schmidt, 2009). Typically, the
annotator edits a table with columns correspond-
ing to words and rows to levels of annotation. A
cell can be split or more cells merged to allow for
annotating smaller or larger segments. This way,
phenomena such as agreement or word order can
be emended and tagged (L?deling et al, 2005).
However, in the tabular format vertical corre-
spondences between the original word form and its
emended equivalents or annotations at other levels
may be lost. It is difficult to keep track of links
between forms merged into a single cell, spanning
multiple columns, and the annotations of a form
at other levels (rows). This may be a problem for
successive emendations involving a single form,
starting from a typo up to an ungrammatical word
order, but also for morphosyntactic tags assigned
to forms, whenever a form is involved in a multi-
word annotation and its equivalent or tag leaves
the column of the original form.
While in the tabular format the correspondences
between elements at various levels are captured
only implicitly, in our annotation scheme these
correspondences are explicitly encoded. Our for-
mat supports the option of preserving correspon-
dences across levels, both between individual
word forms and their annotations, while allowing
for arbitrary joining and splitting of any number
of non-contiguous segments. The annotation lev-
els are represented as a graph consisting of a set
of parallel paths (annotation levels) with links be-
tween them. Nodes along the paths always stand
for word tokens, correct or incorrect, and in a sen-
tence with nothing to correct the corresponding
word tokens in every pair of neighbouring paths
are linked 1:1. Additionally, the nodes can be as-
signed morphosyntactic tags, syntactic functions
or any other word-specific information. Whenever
a word form is emended, the type of error can be
specified as a label of the link connecting the in-
correct form at level Si with its emended form at
level Si+1. In general, these labelled relations can
link an arbitrary number of elements at one level
with an arbitrary number of elements at a neigh-
bouring level. The elements at one level partic-
ipating in this relation need not form a contigu-
ous sequence. Multiple words at any level are thus
identified as a single segment, which is related to a
segment at a neighbouring level, while any of the
participating word forms can retain their 1:1 links
with their counterparts at other levels. This is use-
ful for splitting and joining word forms, for chang-
ing word order, and for any other corrections in-
volving multiple words. Nodes can also be added
or omitted at any level to correct missing or odd
punctuation signs or syntactic constituents. See
Figure 1 below for an example of this multi-level
annotation scheme.
The option of relating multiple nodes as sin-
gle segments across levels could also be used for
treating morphosyntactic errors in concord and
government. However, in this case there is typ-
ically one correct form involved, e.g., the sub-
ject in subject-predicate agreement, the noun in
adjective-noun agreement, the verb assigning case
to a complement, the antecedent in pronominal
reference. Rather than treating both the correct
and the incorrect form as equals in a 2:2 relation
between the levels, the incorrect form is emended
using a 1:1 link with an option to refer to the cor-
rect form. Such references link pairs of forms at
neighbouring levels rather than the forms them-
selves to enable possible references from a multi-
word unit (or) to another multi-word unit. See Fig-
ure 1 below again, where such references are rep-
resented by arrows originating in labels val.
A single error may result in multiple incorrect
forms as shown in (1). The adjective velk? ?big-
NOM-SG-M(ASC)? correctly agrees with the noun
pes ?dog-NOM-SG-MASC?. However, the case of
the noun is incorrect ? it should be in accusative
rather than nominative. When the noun?s case is
corrected, the case of the adjective has to be cor-
rected as well. Then multiple references are made:
to the verb as the case assigner for the noun, and
to the noun as the source of agreement for the ad-
jective.
(1) a. *Vide?l
saw
velk?
big-NOM-SG-M
pes.
dog-NOM-SG-M
b. Vide?l
saw
velk?ho
big-ACC-SG-M
psa.
dog-ACC-SG-M
?He saw a big dog?
Annotation of learners? texts is often far from
straightforward, and alternative interpretations are
available even in a broader context. The annota-
tion format supports alternatives, but for the time
being the annotation tool does not support local
disjunctions. This may be a problem if the anno-
tator has multiple target hypotheses in mind.
14
4.3 Three levels of annotation
A multi-level annotation scheme calls for some
justification, and once such a scheme is adopted,
the question of the number of levels follows.
After a careful examination of alternatives, we
have arrived at a two-stage annotation design,
based on three levels. A flat, single-stage, two-
level annotation scheme would be appropriate if
we were interested only in the original text and
in the annotation at some specific level (fully
emended sentences, or some intermediate stage,
such as emended word forms). The flat design
could be used even if we insisted on registering
some intermediate stages of the passage from the
original to a fully emended text, and decided to
store such information with the word-form nodes.
However, such information might get lost in the
case of significant changes involving deletions or
additions (e.g., in Czech as a pro-drop language,
the annotator may decide that a misspelled per-
sonal pronoun in the subject position should be
deleted and the information about the spelling er-
ror would lost). The decision to use a multi-level
design was mainly due to our interest in annotat-
ing errors in single forms as well as those spanning
(potentially discontinuous) strings of words.
Once we have a scheme of multiple levels avail-
able, we can provide the levels with theoretical
significance and assign a linguistic interpretation
to each of them. In a world of unlimited re-
sources of annotators? time and experience, this
would be the optimal solution. The first annota-
tion level would be concerned only with errors in
graphemics, followed by levels dedicated to mor-
phemics, morphosyntax, syntax, lexical phenom-
ena, semantics and pragmatics. More realistically,
there could be a level for errors in graphemics and
morphemics, another for errors in morphosyntax
(agreement, government) and one more for every-
thing else, including word order and phraseology.
Our solution is a compromise between corpus
users? expected demands and limitations due to
the annotators? time and experience. The anno-
tator has a choice of two levels of annotation, and
the distinction, based to a large extent on formal
criteria, is still linguistically relevant.
At the level of transcribed input (Level 0), the
nodes represent the original strings of graphemes.
At the level of orthographical and morphological
emendation (Level 1), only individual forms are
treated. The result is a string consisting of cor-
rect Czech forms, even though the sentence may
not be correct as a whole. The rule of ?correct
forms only? has a few exceptions: a faulty form
is retained if no correct form could be used in the
context or if the annotator cannot decipher the au-
thor?s intention. On the other hand, a correct form
may be replaced by another correct form if the au-
thor clearly misspelled the latter, creating an un-
intended homograph with another form. All other
types of errors are emended at Level 2.
4.4 Captured errors
A typical learner of Czech makes errors all along
the hierarchy of theoretically motivated linguistic
levels, starting from the level of graphemics up
to the level of pragmatics. Our goal is to emend
the input conservatively, modifying incorrect and
inappropriate forms and expressions to arrive at
a coherent and well-formed result, without any
ambition to produce a stylistically optimal solu-
tion. Emendation is possible only when the input
is comprehensible. In cases where the input or its
part is not comprehensible, it is left with a partial
or even no annotation.
The taxonomy of errors is rather coarse-grained,
a more detailed classification is previewed for a
later stage and a smaller corpus sample. It follows
the three-level distinction and is based on criteria
as straightforward as possible. Whenever the er-
ror type can be determined from the way the er-
ror is emended, the type is supplied automatically
by a post-processing module, together with mor-
phosyntactic tags and lemmas for the correct or
emended forms (see ? 5.3).
Errors in individual word forms, treated at Level
1, include misspellings (also diacritics and capi-
talisation), misplaced word boundaries, missing or
misused punctuation, but also errors in inflectional
and derivational morphology and unknown stems.
These types of errors are emended manually, but
the annotator is not expected label them by their
type ? the type of most errors at Level 1 is identi-
fied automatically. The only exception where the
error type must be assigned manually is when an
unknown stem or derivation affix is used.
Whenever the lexeme (its stem and/or suffix) is
unknown and can be replaced by a suitable form, it
is emended at Level 1. If possible, the form should
fit the syntactic context. If no suitable form can
be found, the form is retained and marked as un-
known. When the form exists, but is not appro-
15
priate in context, it is emended at Level 2 ? the
reason may be the violation of a syntactic rule or
semantic incompatibility of the lexeme.
Table 2 gives a list of error types emended at
Level 1. Some types actually include subtypes:
words can be incorrectly split or joined, punctu-
ation, diacritics or character(s) can be missing,
superfluous, misplaced or of a wrong kind. The
Links column gives the maximum number of po-
sitions at Level 0, followed by the maximum num-
ber of position at Level 1 that are related by links
for this type of error. The Id column says if the
error type is determined automatically or has to be
specified manually.
Error type Links Id
Word boundary m:n A
Punctuation 0:1, 1:0 A
Capitalisation 1:1 A
Diacritics 1:1 A
Character(s) 1:1 A
Inflection 1:1 A
Unknown lexeme 1:1 M
Table 2: Types of errors at Level 1
Emendations at Level 2 concern errors in agree-
ment, valency and pronominal reference, negative
concord, the choice of a lexical item or idiom,
and in word order. For the agreement, valency
and pronominal reference cases, there is typically
an incorrect form, which reflects some properties
(morphological categories, valency requirements)
of a correct form (the agreement source, syntac-
tic head, antecedent). Table 3 gives a list of error
types emended at Level 2. The Ref column gives
the number of pointers linking the incorrect form
with the correct ?source?.
Error type Links Ref Id
Agreement 1:1 1 M
Valency 1:1 1 M
Pronominal reference 1:1 1 M
Complex verb forms m:n 0,1 M
Negation m:n 0,1 M
Missing constituent 0:1 0 M
Odd constituent 1:0 0 M
Modality 1:1 0 M
Word order m:n 0 M
Lexis & phraseology m:n 0,1 M
Table 3: Types of errors at Level 2
The annotation scheme is illustrated in Figure 1,
using an authentic sentence, split in two halves for
space reasons. There are three parallel strings of
word forms, including punctuation signs, repre-
senting the three levels, with links for correspond-
ing forms. Any emendation is labelled with an er-
ror type.4 The first line is Level 0, imported from
the transcribed original, with English glosses be-
low (forms marked by asterisks are incorrect in
any context, but they may be comprehensible ? as
is the case with all such forms in this example).
Correct words are linked directly with their copies
at Level 1, for emended words the link is labelled
with an error type. In the first half of the sentence,
unk for unknown form, dia for an error in diacrit-
ics, cap for an error in capitalisation. According to
the rules of Czech orthography, the negative parti-
cle ne is joined with the verb using an intermedi-
ate node bnd. A missing comma is introduced at
Level 1, labelled as a punctuation error. All the er-
ror labels above can be specified automatically in
the post-processing step.
Staying with the first half of the sentence, most
forms at Level 1 are linked directly with their
equivalents at Level 2 without emendations. The
reflexive particle se is misplaced as a second posi-
tion clitic, and is put into the proper position using
the link labelled wo for a word-order error.5 The
pronoun ona ? ?she? in the nominative case ? is
governed by the form l?bit se, and should bear the
dative case: j?. The arrow to l?bit makes the rea-
son for this emendation explicit. The result could
still be improved by positioning Praha after the
clitics and before the finite verb nebude, resulting
in a word order more in line with the underlying
information structure of the sentence, but our pol-
icy is to refrain from more subtle phenomena and
produce a grammatical rather than a perfect result.
In the second half of the sentence, there is only
one Level 1 error in diacritics, but quite a few er-
rors at Level 2. Proto ?therefore? is changed to
proto?e ?because? ? a lexical emendation. The
main issue are the two finite verbs bylo and vad?.
The most likely intention of the author is best ex-
pressed by the conditional mood. The two non-
contiguous forms are replaced by the conditional
4The labels for error types used here are simplified for
reasons of space and mnemonics.
5In word-order errors it may be difficult to identify a spe-
cific word form violating a rule. The annotation scheme al-
lows for both se and j? to be blamed. However, here we pre-
fer the simpler option and identify just one, more prominent
word form. Similarly with mi below.
16
auxiliary and the content verb participle in one
step using a 2:2 relation. The intermediate node
is labelled by cplx for complex verb forms. The
prepositional phrase pro mne? ?for me? is another
complex issue. Its proper form is pro me? (homony-
mous with pro mne?, but with ?me? bearing ac-
cusative instead of dative), or pro mne. The ac-
cusative case is required by the preposition pro.
However, the head verb requires that this comple-
ment bears bare dative ? mi. Additionally, this
form is a second position clitic, following the con-
ditional auxiliary (also a clitic) in the clitic cluster.
The change from PP to the bare dative pronoun
and the reordering are both properly represented,
including the pointer to the head verb. What is
missing is an explicit annotation of the faulty case
of the prepositional complement, which is lost
during the Level 1 ? Level 2 transition, the price
for a simpler annotation scheme with fewer lev-
els. It might be possible to amend the PP at Level
1, but it would go against the rule that only forms
wrong in isolation are emended at Level 1.
Bojal jsem se ?e ona se ne bude libit prahu ,
*feared AUX RFL that she RFL not will *like prague ,
unk p bnd dia cap
B?l jsem se , ?e ona se nebude l?bit Prahu ,
wo val val
B?l jsem se , ?e se j? nebude l?bit Praha ,
I was afraid that she would not like Prague,
proto to bylo velm? vad? pro mne? .
therefore it was *very resent for me .
dia
proto to bylo velmi vad? pro mne? .
lex cplx val,wo
proto?e to by mi velmi vadilo .
because I would be very unhappy about it.
Figure 1: Annotation of a sample sentence
4.5 Data Format
To encode the layered annotation described above,
we have developed an annotation schema in the
Prague Markup Language (PML).6 PML is a
6http://ufal.mff.cuni.cz/jazz/pml/
index_en.html
<?xml version="1.0" encoding="UTF-8"?>
<adata xmlns="http://utkl.cuni.cz/czesl/">
<head>
<schema href="adata_schema.xml" />
<references>
<reffile id="w" name="wdata" href="r049.w.xml" />
</references>
</head>
<doc id="a-r049-d1" lowerdoc.rf="w#w-r049-d1">
...
<para id="a-r049-d1p2" lowerpara.rf="w#w-r049-d1p2">
...
<s id="a-r049-d1p2s5">
<w id="a-r049-d1p2w50">
<token>B?l</token>
</w>
<w id="a-r049-d1p2w51">
<token>jsem</token>
</w>
<w id="a-r049-d1p2w52">
<token>se</token>
</w>
...
</s>
...
<edge id="a-r049-d1p2e54">
<from>w#w-r049-d1p2w46</from>
<to>a-r049-d1p2w50</to>
<error>
<tag>unk</tag>
</error>
</edge>
<edge id="a-r049-d1p2e55">
<from>w#w-r049-d1p2w47</from>
<to>a-r049-d1p2w51</to>
</edge>
...
</para>
...
</doc>
</adata>
Figure 2: Portion of the Level 1 of the sample sen-
tence encoded in the PML data format.
generic XML-based data format, designed for the
representation of rich linguistic annotation organ-
ised into levels. In our schema, each of the higher
levels contains information about words on that
level, about the corrected errors and about rela-
tions to the tokens on the lower levels. Level 0
does not contain any relations, only links to the
neighbouring Level 1. In Figure 2, we show a por-
tion (first three words and first two relations) of
the Level 1 of the sample sentence encoded in our
annotation schema.
5 Annotation process
The whole annotation process proceeds as follows:
? A handwritten document is transcribed into
html using off-the-shelf tools (e.g. Open Office
Writer or Microsoft Word).
? The information in the html document is used to
generate Level 0 and a default Level 1 encoded
in the PML format.
? An annotator manually corrects the document
and provides some information about errors us-
ing our annotation tool.
? Error information that can be inferred automat-
ically is added.
17
Figure 3: Sample sentence in the annotation tool.
5.1 Transcription
The original documents are hand-written, usually
the only available option, given that their most
common source are language courses and exams.
The avoidance of an electronic format is also due
to the concern about the use of automatic text-
editing tools by the students, which may signifi-
cantly distort the authentic interlanguage.
Therefore, the texts must be transcribed, which
is very time consuming. While we strive to cap-
ture only the information present in the original
hand-written text, often some interpretation is un-
avoidable. For example, the transcribers have to
take into account specifics of hand-writing of par-
ticular groups of students and even of each indi-
vidual student (the same glyph may be interpreted
as l in the hand-writing of one student, e of an-
other, and a of yet another). When a text allows
multiple interpretation, the transcribers may pro-
vide all variants. For example, the case of initial
letters or word boundaries are often unclear. Ob-
viously, parts of some texts may be completely il-
legible and are marked as such.
Also captured are corrections made by the stu-
dent (insertions, deletions, etc.), useful for investi-
gating the process of language acquisition.
The transcripts are not spell-checked automati-
cally. In a highly inflectional language, deviations
in spelling very often do not only reflect wrong
graphemics, but indicate an error in morphology.
5.2 Annotation
The manual portion of annotation is supported by
an annotation tool we have developed. The anno-
tator corrects the text on appropriate levels, modi-
fies relations between elements (by default all re-
lations are 1:1) and annotates relations with error
tags as needed. The context of the annotated text
is shown both as a transcribed html document and
as a scan of the original document. The tool is
written in Java on top of the Netbeans platform.7
Figure 3 shows the annotation of the sample sen-
tence as displayed by the tool.
5.3 Postprocessing
Manual annotation is followed by automatic post-
processing, providing the corpus with additional
information:
7http://platform.netbeans.org/
18
? Level 1: lemma, POS and morphological cate-
gories (this information can be ambiguous)
? Level 2: lemma, POS and morphological cate-
gories (disambiguated)
? Level 1: type of error (by comparing the origi-
nal and corrected strings), with the exception of
lexical errors that involve lemma changes (e.g.
*kader?nic?ka ? kader?nice ?hair-dresser?)
? Level 2: type of morphosyntactic errors caused
by agreement or valency error (by comparing
morphosyntactic tags at Level 1 and 2)
? Formal error description: missing/extra expres-
sion, erroneous expression, wrong order
? In the future, we plan to automatically tag errors
in verb prefixes, inflectional endings, spelling,
palatalisation, metathesis, etc.
6 Conclusion
Error annotation is a very resource-intensive task,
but the return on investment is potentially enor-
mous. Depending on the annotation scheme, the
corpus user has access to detailed error statistics,
which is difficult to obtain otherwise. An error-
tagged corpus is an invaluable tool to obtain a re-
liable picture of the learners? interlanguage and to
adapt teaching methods and learning materials by
identifying the most frequent error categories in
accordance with the learner?s proficiency level or
L1 background.
We are expecting plentiful feedback from the er-
ror annotation process, which is just starting. As
the goal of a sizable corpus requires a realistic
setup, we plan to experiment with more and less
detailed sets of error types, measuring the time and
inter-annotator agreement. A substantially more
elaborate classification of errors is previewed for a
limited subset of the corpus.
At the same time, the feedback of the annotators
will translate into the ongoing tuning of the an-
notation guidelines, represented by a comprehen-
sive error-tagging manual. We hope in progress in
dealing with thorny issues such as the uncertainty
about the author?s intended meaning, the inference
errors, the proper amount of interference with the
original, or the occurrence of colloquial language.
In all of this, we need to make sure that annotators
handle similar phenomena in the same way.
However, the real test of the corpus will come
with its usage. We are optimistic ? some of the
future users are a crucial part of our team and their
needs and ideas are the driving force of the project.
7 Acknowledgements
We wish to thank other members of the project
team, namely Milena Hn?tkov?, Tom?? Jel?nek,
Vladim?r Petkevic?, and Hana Skoumalov? for their
numerous stimulating ideas, acute insight and im-
portant feedback. We are especially grateful to
Karel ?ebesta, for all of the above and for initi-
ating and guiding this enterprise.
The work described in this paper is funded by
the European Social Fund and the government of
the Czech Republic within the operational pro-
gramme ?Education for Competitiveness? as a part
of the project ?Innovation in Education in the
Field of Czech as a Second Language? (project
no. CZ.1.07/2.2.00/07.0259).
References
Sylviane Granger, editor. 1998. Learner English on
Computer. Addison Wesley Longman, London and
New York.
Geoffrey Leech. 1998. Preface. In Granger Sylviane,
editor, Learner English on Computer, pages xiv?xx.
Addison Wesley Longman, London and New York.
Anke L?deling, Maik Walter, Emil Kroymann, and Pe-
ter Adolphs. 2005. Multi-level error annotation in
learner corpora. In Proceedings of Corpus Linguis-
tics 2005, Birmingham.
Nadja Nesselhauf. 2004. Learner corpora and their po-
tential for language teaching. In John McHardy Sin-
clair, editor, How to use corpora in language teach-
ing, Studies in corpus linguistics, pages 125?152.
Benjamins, Amsterdam/Philadelphia.
Norma A. Pravec. 2002. Survery of learner corpora.
ICAME Journal, 26:81?114.
Thomas Schmidt. 2009. Creating and working with
spoken language corpora in EXMARaLDA. In
LULCL II: Lesser Used Languages & Computer
Linguistics II, pages 151?164.
Larry Selinker. 1983. Interlanguage. In Betty W.
Robinett and Jacquelyn Schachter, editors, Second
Language Learning: Contrastive analysis, error
analysis, and related aspects, pages 173?196. The
University of Michigan Press, Ann Arbor, MI.
Richard Xiao. 2008. Well-known and influential
corpora. In Anke L?deling and Merja Kyt?, ed-
itors, Corpus Linguistics. An International Hand-
book, volume 1 of Handbooks of Linguistics and
Communication Science [HSK] 29.1, pages 383?
457. Mouton de Gruyter, Berlin and New York.
19
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 197?201,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Challenges of Cheap Resource Creation for Morphological Tagging
Jirka Hana
Charles University
Prague, Czech Republic
first.last@gmail.com
Anna Feldman
Montclair State University
Montclair, New Jersey, USA
first.last@montclair.edu
Abstract
We describe the challenges of resource
creation for a resource-light system for
morphological tagging of fusional lan-
guages (Feldman and Hana, 2010). The
constraints on resources (time, expertise,
and money) introduce challenges that are
not present in development of morphologi-
cal tools and corpora in the usual, resource
intensive way.
1 Introduction
Morphological analysis, tagging and lemmatiza-
tion are essential for many Natural Language Pro-
cessing (NLP) applications of both practical and
theoretical nature. Modern taggers and analyz-
ers are very accurate. However, the standard
way to create them for a particular language re-
quires substantial amount of expertise, time and
money. A tagger is usually trained on a large cor-
pus (around 100,000+ words) annotated with the
correct tags. Morphological analyzers usually rely
on large manually created lexicons. For exam-
ple, the Czech analyzer (Hajic?, 2004) uses a lex-
icon with 300,000+ entries. As a result, most of
the world languages and dialects have no realis-
tic prospect for morphological taggers or analyz-
ers created in this way.
We have been developing a method for creat-
ing morphological taggers and analyzers of fu-
sional languages1 without the need for large-scale
knowledge- and labor-intensive resources (Hana et
al., 2004; Hana et al, 2006; Feldman and Hana,
2010) for the target language. Instead, we rely
on (i) resources available for a related language
and (ii) a limited amount of high-impact, low-
1Fusional languages are languages in which several fea-
ture values are realized in one morpheme. For example Indo-
European languages, including Czech, German, Romanian
and Farsi, are predominantly fusional.
cost manually created resources. This greatly re-
duces cost, time requirements and the need for
(language-specific) linguistic expertise.
The focus of our paper is on the creation of re-
sources for the system we developed. Even though
we have reduced the manual resource creation to
the minimum, we have encountered a number of
problems, including training language annotators,
documenting the reasoning behind the tagset de-
sign and morphological paradigms for a specific
language as well as creating support tools to facil-
itate and speed up the manual work. While these
problems are analogous to those that arise with
standard resource creation, the approach to their
solution is often different as we discuss in the fol-
lowing sections.
2 Resource-light Morphology
The details of our system are provided in (Feld-
man and Hana, 2010). Our main assumption is
that a model for the target language can be approx-
imated by language models from one or more re-
lated source languages and that inclusion of a lim-
ited amount of high-impact and/or low-cost man-
ual resources is greatly beneficial and desirable.
We use TnT (Brants, 2000), a second order
Markov Model tagger. We approximate the target-
language emissions by combining the emissions
from the (modified) source language corpus with
information from the output of our resource-light
analyzer (Hana, 2008). The target-language tran-
sitions are approximated by the source language
(Feldman and Hana, 2010).
3 Resource creation
In this section we address the problem of collec-
tion, selection and creation of resources needed
by our system. The following resources must be
available:
? a reference grammar book for information
197
about paradigms and closed class words,
? a large amount of plain text for learning a lex-
icon, e.g. newspapers from the Internet,
? a large annotated training corpus of a related
language,
? optionally, a dictionary (or a native speaker)
to provide analyses of the most frequent
words,
? a non-expert (not a linguist and not a native
speaker) to create the resources listed below,
? limited access to a linguist (to make non-
obvious decisions in the design of the re-
sources),
? limited access to a native speaker (to anno-
tate a development corpus, to answer a lim-
ited number of language specific questions).
and these resources must be created:
? a list of morphological paradigms,
? a list of closed class words with their analy-
ses,
? optionally, a list of the most frequent forms,
? a small annotated development corpus.
For evaluation, an annotated test corpus must
be also created. As this corpus is not part of the
resource-light system per se, it can (and should)
be as large as possible.
3.1 Restrictions
Since our goal is to create resources cheaply and
fast, we intentionally limit (but not completely ex-
clude) the inclusion of any linguist and of anybody
knowing the target language. We also limit the
time of training and encoding of the basic target-
language linguistic information to a minimum.
3.2 Tagset
In traditional settings, a tagset is usually designed
by a linguist, moreover a native speaker. The con-
straints of a resource-light system preclude both of
these qualifications. Instead, we have standardized
the process as much as possible to make it possible
to have the tagset designed by a non-expert.
3.2.1 Positional Tagset
All languages we work with are morphologically
rich. Naturally, such languages require a large
number of tags to capture their morphological
properties. An obvious way to make it manageable
is to use a structured system. In such a system, a
tag is a composition of tags each coming from a
much smaller and simpler atomic tagset tagging a
particular morpho-syntactic property (e.g. gender
or tense). This system has many benefits, includ-
ing the 1) relative easiness for a human annotator
to remember individual positions rather than sev-
eral thousands of atomic symbols; 2) systematic
morphological description; 3) tag decomposabil-
ity; and 4) systematic evaluation.
3.2.2 Tagset Design: Procedure
Instead of starting from scratch each time a tagset
for a new language is created, we have provided
an annotated tagset template. A particular tagset
can deviate from this template, but only if there is
a linguistic reason. The tagset template includes
the following items:
? order of categories (POS, SubPOS, gender,
animacy, number, case, ...) ? not all might
be present in that language; additional cate-
gories might be needed;
? values for each category (N ? nouns, C ? nu-
merals, M ? masculine);
? which categories we do not distinguish, even
though we could (proper vs. common nouns);
? a fully worked out commented example (as
mentioned above).
Such a template not only provides a general
guidance, but also saves a lot of time, because
many of rather arbitrary decisions involved in any
tagset creation are done just once (e.g. symbols de-
noting basic POS categories, should numerals be
included as separate POS, etc.). As stated, a tagset
may deviate from such a template, but only if there
is a specific reason for it.
3.3 Resources for the morphological analyzer
Our morphological analyzer relies on a small set
of morphological paradigms and a list of closed
class and/or most frequent words.
198
3.3.1 Morphological paradigms
For each target language, we create a list of
morphological paradigms. We just encode basic
facts about the target language morphology from
a standard grammar textbook. On average, the
basic morphology of highly inflected languages,
such as Slavic languages, are captured in 70-80
paradigms. The choices on what to cover involve
a balance between precision, coverage and effort.
3.3.2 A list of frequent forms
Entering a lexicon entry is very costly, both in
terms of time and knowledge needed. While it is
usually easy (for a native speaker) to assign a word
to one of the major paradigm groups, it takes con-
siderably more time to select the exact paradigm
variant differing only in one or two forms (in fact,
this may be even idiolect-dependent). For exam-
ple, in Czech, it is easy to see that the word atom
?atom? does not decline according to the neuter
paradigm me?sto ?town?, but it takes more time to
decide to which of the hard masculine inanimate
paradigms it belongs. On the other hand, enter-
ing possible analyses for individual word forms is
usually very straightforward. Therefore, our sys-
tem uses a list of manually provided analyses for
the most common forms.
Note that the process of providing the list of
forms is not completely manual ? the correct anal-
yses are selected from those suggested on the ba-
sis of the words? endings. This can be done rel-
atively quickly by a native speaker or by a non-
native speaker with the help of a basic grammar
book and a dictionary.
3.4 Documentation
Since the main idea of the project is to create
resources quickly for an arbitrarily selected fu-
sional language, we cannot possibly create anno-
tation and language encoding manuals for each
language. So, we created a manual that explains
the annotation and paradigm encoding procedure
in general and describes the main attributes and
possible values that a language consultant needs
to consider when working on a specific language.
The manual has five parts:
1. How to summarize the basic facts about the
morphosyntax of a language;
2. How to create a tagset
3. How to encode morphosyntactic properties of
the target language in paradigms;
4. How to create a list of closed class words.
5. Corpus annotation manual
The instructions are mostly language indepen-
dent (with some bias toward Indo-European lan-
guages), but contain a lot of examples from lan-
guages we have processed so far. These include
suggestions how to analyze personal pronouns,
what to do with clitics or numerals.
3.5 Procedure
The resource creation procedure involves at least
two people: a native speaker who can annotate
a development corpus, and a non-native speaker
who is responsible for the tagset design, morpho-
logical paradigms, and a list of closed class words
or frequent forms. Below we describe our proce-
dure in more detail.
3.5.1 Tagset and MA resources creation
We have realized that even though we do not need
a native speaker, some understanding of at least
basic morphological categories the language uses
is helpful. So, based on our experience, it is bet-
ter to hire a person who speaks (natively or not) a
language with some features in common. For ex-
ample, for Polish, somebody knowing Russian is
ideal, but even somebody speaking German (it has
genders and cases) is much better than a person
speaking only English. In addition, a person who
had created resources for one language performs
much better on the next target language. Knowl-
edge comes with practice.
The order of work is as follows:
1. The annotator is given basic training that usu-
ally includes the following: 1) brief explana-
tion of the purpose of the project; 2) tagset
design; 3) paradigm creation.
2. The annotator summarizes the basic facts
about the morphosyntax of a language,
3. The first version of the tagset is created.
4. The list of paradigms and closed-class words
is compiled. During this process, the tagset is
further adjusted.
199
3.5.2 Corpus annotation
The annotators do not annotate from scratch.
We first run our morphological analyzer on
the selected corpus; the annotators then dis-
ambiguate the output. We have created a
support tool (http://ufal.mff.cuni.cz/
?hana/law.html) that displays the word to be
annotated, its context, the lemma and possible tags
suggested by the morphological analyzer. There is
an option to insert a new lemma and a new tag if
none of the suggested items is suitable. The tags
are displayed together with their natural language
translation.
4 Case studies
Our case studies include Russian via Czech, Rus-
sian via Polish, Russian via Czech and Polish, Por-
tuguese via Spanish, and Catalan via Spanish.
We use these languages to test our hypotheses
and we do not suggest that morphological tagging
of these languages should be designed in the way
we do. Actually, high precision systems that use
manually created resources already exist for these
languages. The main reason for working with
them is that we can easily evaluate our system on
existing corpora.
We experimented with the direct transfer of
transition probabilities, cognates, modifying tran-
sitions to make them more target-like, training a
battery of subtaggers and combining the results
(Reference omitted). Our best result on Russian
is 81.3% precision (on the full 15-slot tag, on all
POSs), and 92.2% (on the detailed POS). We have
also noticed that the most difficult categories are
nouns and adjectives. If we improve on these in-
dividual categories, we will improve significantly
the overall result. The precision of our model
on Catalan is 87.1% and 91.1% on the full tag
and SubPOS, respectively. The Portuguese perfor-
mance is comparable as well.
The resources our experiments have relied upon
include the following:
1. Russian
? Tagset, paradigms, word-list: speaker of
Czech and linguist, some knowledge of
Russian
? Dev corpus: a native speaker & linguist
2. Catalan
? Tagset: modified existing tagset (de-
signed by native speaking linguists)
? paradigms, word-list: linguist speaking
Russian and English
? Dev corpus: a native speaking linguists
3. Portuguese
? Tagset: modified Spanish tagset (de-
signed by native speaking linguists) by
us
? paradigms, word-list: a native speaking
linguist
? Dev corpus: a native speaking linguist
4. Romanian
? Tagset, paradigms, word-list: designed
by a non-linguist, speaker of English
? Dev corpus ? a native speaker
Naturally, we cannot expect the tagging accu-
racy to be 100%. There are many factors that con-
tribute to the performance of the model:
1. target language morphosyntactic complexity,
2. source-language?target-language proximity,
3. quality of the paradigms,
4. quality of the cognate pairs (that are used for
approximating emissions),
5. time spent on language analysis,
6. expertise of language consultants,
7. supporting tools.
5 Summary
We have described challenges of resource creation
for resource-light morphological tagging. These
include creating clear guidelines for tagset design
that can be reusable for an arbitrarily selected lan-
guage; precise formatting instructions; providing
basic linguistic training with the emphasis on mor-
phosyntactic properties of fusional languages; cre-
ating an annotation support tool; and giving timely
and constructive feedback on intermediate results.
6 Acknowledgement
The development of the tagset was supported by
the GAC?R grant P406/10/P328 and by the U.S.
NSF grant # 0916280.
200
References
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger. In Proceedings of 6th Applied Nat-
ural Language Processing Conference and North
American chapter of the Association for Computa-
tional Linguistics annual meeting (ANLP-NAACL),
pages 224?231.
Anna Feldman and Jirka Hana. 2010. A Resource-light
Approach to Morpho-syntactic Tagging, volume 70
of Language and Computers: Studies in Practical
Linguistics. Rodopi, Amsterdam/New York.
Jan Hajic?. 2004. Disambiguation of Rich Inflection:
Computational Morphology of Czech. Karolinum,
Charles University Press, Prague, Czech Republic.
Jirka Hana, Anna Feldman, and Chris Brew. 2004.
A Resource-light Approach to Russian Morphol-
ogy: Tagging Russian Using Czech Resources.
In Proceedings of Empirical Methods for Natural
Language Processing (EMNLP), pages 222?229,
Barcelona, Spain.
Jirka Hana, Anna Feldman, Luiz Amaral, and Chris
Brew. 2006. Tagging Portuguese with a Span-
ish Tagger Using Cognates. In Proceedings of the
Workshop on Cross-language Knowledge Induction
hosted in conjunction with the 11th Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL), pages 33?40, Trento,
Italy.
Jirka Hana. 2008. Knowledge- and labor-light mor-
phological analysis. OSUWPL, 58:52?84.
201
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 10?18,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
A Low-budget Tagger for Old Czech
Jirka Hana
Charles University, MFF
Czech Republic
first.last@gmail.com
Anna Feldman
Montclair State University
USA
first.last@montclair.edu
Katsiaryna Aharodnik
Montclair State University
USA
ogorodnichek@gmail.com
Abstract
The paper describes a tagger for Old Czech
(1200-1500 AD), a fusional language with
rich morphology. The practical restrictions
(no native speakers, limited corpora and lex-
icons, limited funding) make Old Czech an
ideal candidate for a resource-light cross-
lingual method that we have been developing
(e.g. Hana et al, 2004; Feldman and Hana,
2010).
We use a traditional supervised tagger. How-
ever, instead of spending years of effort to cre-
ate a large annotated corpus of Old Czech, we
approximate it by a corpus of Modern Czech.
We perform a series of simple transformations
to make a modern text look more like a text
in Old Czech and vice versa. We also use a
resource-light morphological analyzer to pro-
vide candidate tags. The results are worse
than the results of traditional taggers, but the
amount of language-specific work needed is
minimal.
1 Introduction
This paper describes a series of experiments in an
attempt to create morphosyntactic resources for Old
Czech (OC) on the basis of Modern Czech (MC) re-
sources. The purpose of this work is two-fold. The
practical goal is to create a morphologically anno-
tated corpus of OC which will help in investigation
of various morphosyntactic patterns underpinning
the evolution of Czech. Our second goal is more
theoretical in nature. We wanted to test the resource-
light cross-lingual method that we have been devel-
oping (e.g. Hana et al, 2004; Feldman and Hana,
2010) on a source-target language pair that is di-
vided by time instead of space. The practical restric-
tions (no native speakers, limited corpora and lexi-
cons, limited funding) make OC an ideal candidate
for a resource-light approach.
We understand that the task we chose is hard
given the 500+ years of language evolution. We are
aware of the fact that all layers of the language have
changed, including phonology and graphemics, syn-
tax and vocabulary. Even words that are still used in
MC are often used with different distributions, with
different declensions, with different gender, etc.
Our paper is structured as follows. We first briefly
describe related work and motivate our approach.
Then we outline the relevant aspects of the Czech
language and compare its Modern and Old forms.
Then we describe the corpora and tagsets used in our
experiments. The rest of the paper describes the ac-
tual experiments, the performance of various models
and concludes with a discussion of the results.
2 Related Work
Since there are no morphological taggers devel-
oped specifically for OC, we compare our work
with those for MC. Morc?e (http://ufal.mff.
cuni.cz/morce/) is currently the best tagger,
with accuracy slightly above 95%. It is based on
a statistical (averaged perceptron) algorithm which
relies on a large morphological lexicon containing
around 300K entries. The tool has been trained and
tuned on data from the Prague Dependency Tree-
bank (PDT; Be?mova et al, 1999; Bo?hmova? et al,
2001). The best set of features was selected af-
ter hundreds of experiments were performed. In
10
contrast, the resource-light system we developed is
not as accurate, but the amount of language-specific
work needed is incomparable to that of the state-
of-the-art systems. Language specific work on our
OC tagger, for example, was completed in about 20
hours, instead of several years.
Research in resource-light learning of mor-
phosyntactic properties of languages is not new.
Some have assumed only partially tagged train-
ing corpora (Merialdo, 1994); some have begun
with small tagged seed wordlists (Cucerzan and
Yarowsky, 2002) for named-entity tagging, while
others have exploited the automatic transfer of an
already existing annotated resource in a different
genre or a different language (e.g. cross-language
projection of morphological and syntactic informa-
tion as in (Cucerzan and Yarowsky, 2000; Yarowsky
et al, 2001), requiring no direct supervision in the
target language). The performance of our system is
comparable to the results cited by these researchers.
In our work we wanted to connect to pre-
existing knowledge that has been acquired and sys-
tematized by traditional linguists, e.g. morpholog-
ical paradigms, sound changes, and other well-
established facts about MC and OC.
3 Czech Language
Czech is a West Slavic language with significant in-
fluences from German, Latin and (in modern times)
English. It is a fusional (flective) language with rich
morphology and a high degree of homonymy of end-
ings.
3.1 Old Czech
As a separate language, Czech forms between 1000-
1150 AD; there are very few written documents
from that time. The term Old Czech usually refers
to Czech roughly between 1150 and 1500. It is fol-
lowed by Humanistic Czech (1500-1650), Baroque
Czech (1650-1780) and then Czech of the so-called
National Revival. Old Czech was significantly in-
fluenced by Old Church Slavonic, Latin and Ger-
man. Spelling during this period was not standard-
ized, therefore the same word can have many dif-
ferent spelling variants. However, our corpus was
transliterated ? its pronunciation was recorded using
the rules of the Modern Czech spelling (see Lehec?ka
change example
u? > ou non-init. mu?ka > mouka ?flour?
se? > se se?no > seno ?hay?
o? > uo > u? ko?n? > kuon? > ku?n? ?horse?
s?c? > s?t? s?c???r > s?t??r ?scorpion?
c?s > c c?so > co ?what?
Table 1: Examples of sound/spelling changes from OC to
MC
and Volekova?, 2011, for more details).
3.2 Modern Czech
Modern Czech is spoken by roughly 10 million
speakers, mostly in the Czech Republic. For a more
detailed discussion, see for example (Naughton,
2005; Short, 1993; Janda and Townsend, 2002;
Karl??k et al, 1996). For historical reasons, there
are two variants of Czech: Official (Literary, Stan-
dard) Czech and Common (Colloquial) Czech. The
official variant is based on the 19th-century resur-
rection of the 16th-century Czech. Sometimes it is
claimed, with some exaggeration, that it is the first
foreign language the Czechs learn. The differences
are mainly in phonology, morphology and lexicon.
The two variants are influencing each other, result-
ing in a significant amount of irregularity, especially
in morphology. The Czech writing system is mostly
phonological.
3.3 Differences
Providing a systematic description of differences be-
tween Old and Modern Czech is well beyond the
scope of this paper. Therefore, we just briefly men-
tion a few illustrative examples. For a more detailed
description see (Va?z?ny?, 1964; Dosta?l, 1967; Mann,
1977).
3.3.1 Phonology and Spelling
Examples of some of the more regular changes be-
tween OC and MC spelling can be found in Table 1
(Mann (1977), Boris Lehec?ka p.c.).
3.3.2 Nominal Morphology
The nouns of OC have three genders: feminine,
masculine, and neuter. In declension they distin-
guish three numbers: singular, plural, and dual,
and seven cases: nominative, genitive, dative, ac-
cusative, vocative, locative and instrumental. Voca-
11
category Old Czech Modern Czech
infinitive pe?c-i pe?c-t ?bake?
present 1sg pek-u pec?-u
1du pec?-eve? ?
1pl pec?-em(e/y) pec?-eme
:
imperfect 1sg pec?-iech ?
1du pec?-iechove? ?
1pl pec?-iechom(e/y) ?
:
imperative 2sg pec-i pec?
2du pec-ta ?
2pl pec-te pec?-te
:
verbal noun pec?-enie pec?-en??
Table 2: A fragment of the conjugation of the verb
pe?ci/pe?ct ?bake? (OC based on (Dosta?l, 1967, 74-77))
tive is distinct only for some nouns and only in sin-
gular.
MC nouns preserved most of the features of OC,
but the dual number survives only in a few paired
names of parts of the body, in the declensions of
the words ?two? and ?both? and in the word for
?two hundred?. In Common Czech the dual plural
distinction is completely neutralized. On the other
hand, MC distinguishes animacy in masculine gen-
der, while this distinction is only emerging in late
OC.
3.3.3 Verbal Morphology
The system of verbal forms and constructions was
far more elaborate in OC than in MC. Many forms
disappeared all together (three simple past tenses,
supinum), and some are archaic (verbal adverbs,
plusquamperfectum). Obviously, all dual forms are
no longer in MC. See Table 2 for an example.
4 Corpora
4.1 Modern Czech Corpus
Our MC training corpus is a portion (700K tokens)
of PDT. The corpus contains texts from daily news-
papers, business and popular scientific magazines. It
is manually morphologically annotated.
The tagset (Hajic? (2004)) has more than 4200
tags encoding detailed morphological information.
It is a positional tagset, meaning the tags are se-
quences of values encoding individual morpholog-
ical features and all tags have the same length, en-
coding all the features distinguished by the tagset.
Features not applicable for a particular word have a
N/A value. For example, when a word is annotated
as AAFS4----2A---- it is an adjective (A), long
form (A), feminine (F), singular (S), accusative (4),
comparative (2), not-negated (A).
4.2 Old Czech Corpora
Several steps (e.g., lexicon acquisition) of our
method require a plain text corpus. We used texts
from the Old-Czech Text Bank (STB, http://
vokabular.ujc.cas.cz/banka.aspx), in
total about 740K tokens. This is significantly less
than we have used in other experiments (e.g., 39M
tokens for Czech or 63M tokens for Catalan (Feld-
man and Hana, 2010)).
A small portion (about 1000 words) of the corpus
was manually annotated for testing purposes. Again
this is much less than what we would like to have,
and we plan to increase the size in the near future.
The tagset is a modification of the modern tagset us-
ing the same categories.
5 Method
The main assumption of our method (Feldman and
Hana, 2010) is that a model for the target language
can be approximated by language models from one
or more related source languages and that inclusion
of a limited amount of high-impact and/or low-cost
manual resources is greatly beneficial and desirable.
We use TnT (Brants, 2000), a second order
Markov Model tagger. The language model of such
a tagger consists of emission probabilities (corre-
sponding to a lexicon with usage frequency infor-
mation) and transition probabilities (roughly corre-
sponding to syntax rules with strong emphasis on lo-
cal word-order). We approximate the emission and
transition probabilities by those trained on a mod-
ified corpus of a related language. Below, we de-
scribe our approach in more detail.
12
6 Experiments
We describe three different taggers:
1. a TnT tagger using modified MC corpus as a
source of both transition and emission proba-
bilities (section 6.1);
2. a TnT tagger using modern transitions but
approximating emissions by a uniformly dis-
tributed output of a morphological analyzer
(MA) (sections 6.2 and 6.3); and
3. a combination of both (section 6.4).
6.1 Translation Model
6.1.1 Modernizing OC and Aging MC
Theoretically, we can take the MC corpus, translate
it to OC and then train a tagger, which would proba-
bly be a good OC tagger. However, we do not need
this sophisticated, costly translation because we only
deal with morphology.
A more plausible idea is to modify the MC corpus
so that it looks more like the OC just in the aspects
relevant for morphological tagging. In this case, the
translation would include the tagset, reverse phono-
logical/graphemic changes, etc. Unfortunately, even
this is not always possible or practical. For exam-
ple, historical linguists usually describe phonologi-
cal changes from old to new, not from new to old.1
In addition, it is not possible to deterministically
translate the modern tagset to the older one. So, we
modify the MC training corpus to look more like the
OC corpus (the process we call ?aging?) and also the
target OC corpus to look more like the MC corpus
(?modernizing?).
6.1.2 Creating the Translation Tagger
Below we describe the process of creating a tagger.
As an example we discuss the details for the Trans-
lation tagger. Figure 1 summarizes the discussion.
1. Aging the MC training (annotated) corpus:
? MC to OC tag translation:
Dropping animacy distinction (OC did not
distinguish animacy).
1Note that one cannot simply reverse the rules, as in general,
the function is not a bijection.
? Simple MC to OC form transformations:
E.g., modern infinitives end in -t, OC in-
finitives ended in -ti;
(we implemented 3 transformations)
2. Training an MC tagger. The tagger is trained
on the result of the previous step.
3. Modernizing an OC plain corpus. In this
step we modernize OC forms by applying
sound/graphemic changes such as those in Ta-
ble 1. Obviously, these transformations are not
without problems. First, the OC-to-MC transla-
tions do not always result in correct MC forms;
even worse, they do not always provide forms
that ever existed. Sometimes these transforma-
tions lead to forms that do exist in MC, but are
unrelated to the source form. Nevertheless, we
think that these cases are true exceptions from
the rule and that in the majority of cases, these
OC translated forms will result in existing MC
words and have a similar distribution.
4. Tagging. The modernized corpus is tagged
with the aged tagger.
5. Reverting modernizations. Modernized words
are replaced with their original forms. This
gives us a tagged OC corpus, which can be used
for training.
6. Training an OC tagger. The tagger is trained on
the result of the previous step. The result of this
training is an OC tagger.
The results of the translation model are provided
in Tables 3 (for each individual tag position) and
4 (across various POS categories). What is evident
from these numbers is that the Translation tagger is
already quite good at predicting the POS, subPOS
and number categories. The most challenging POS
category is the category of verbs and the most diffi-
cult feature is case. Based on our previous experi-
ence with other fusional languages, getting the case
feature right is always challenging. Even though
case participates in syntactic agreement in both OC
and MC, this category is more idiosyncratic than,
say, person or tense. Therefore, the MC syntactic
and lexical information provided by the translation
13
STB
old plain
O2M: form translation
tag & form
(back) translation
STB'
plain
STB'
tagged
STB
tagged
tagging
train
Old Czech
HMM tagger
PDT corpus
modern 
annotated
M2O: tag & form translation
PDT ' corpus
annotated
train
HMM tagger
1
2
3
4
5
6
Figure 1: Schema of the Translation Tagger
model might not be sufficient to compute case cor-
rectly. One of the solutions that we explore in this
paper is approximating the OC lexical distribution
by the resource-light morphological analyzer (see
section 6.3).
While most nominal forms and their morpholog-
ical categories (apart from dual) survived in MC,
OC and MC departed in verbs significantly. Thus,
for example, three OC tenses disappeared in MC
and other tenses replaced them. These include the
OC two aorists, supinum and imperfectum. The
transgressive forms are almost not used in MC any-
more either. Instead MC has periphrastic past, pe-
riphrastic conditional and also future. In addition,
these OC verbal forms that disappeared in MC are
unique and non-ambiguous, which makes it even
more difficult to guess if the model is trained on the
MC data. The tagger, in fact, has no way of provid-
ing the right answer. In the subsequent sections we
use a morphological analyzer to address this prob-
lem. Our morphological analyzer uses very basic
hand-encoded facts about the target language.
6.2 Resource-light Morphological Analysis
The Even tagger described in the following section
relies on a morphological analyzer. While it can
use any analyzer, to stay within a resource light
paradigm, we have used our resource-light analyzer
(Hana, 2008; Feldman and Hana, 2010). Our ap-
proach to morphological analysis (Hana, 2008) takes
the middle road between completely unsupervised
systems on the one hand and systems with exten-
sive manually-created resources on the other. It ex-
ploits Zipf?s law (Zipf, 1935, 1949): not all words
and morphemes matter equally. A small number of
words are extremely frequent, while most words are
rare. For example, in PDT, 10% most frequent noun
lemmas cover about 75% of all noun tokens in the
corpus. On the other hand, the less frequent 50% of
noun lemmas cover only 5% of all noun tokens.
Therefore, in our approach, those resources that
are easy to provide and that matter most are created
14
Tags: 70.6
Position 0 (POS ): 91.5
Position 1 (SubPOS ): 88.9
Position 2 (Gender ): 87.4
Position 3 (Number ): 91.0
Position 4 (case ): 82.6
Position 5 (PossGen): 99.5
Position 6 (PossNr ): 99.5
Position 7 (person ): 93.2
Position 8 (tense ): 94.4
Position 9 (grade ): 98.0
Position 10 (negation): 94.4
Position 11 (voice ): 95.9
Table 3: Accuracy of the Translation Model on individual
positions (in %).
All Full: 70.6
SubPOS 88.9
Nouns Full 63.1
SubPOS 99.3
Adjs Full: 60.3
SubPos 93.7
Verbs Full 47.8
SubPOS 62.2
Table 4: Performance of the Translation Model on major
POS categories (in %).
manually or semi-automatically and the rest is ac-
quired automatically. For more discussion see (Feld-
man and Hana, 2010).
Structure The system uses a cascade of modules.
The general strategy is to run ?sure thing? modules
(ones that make fewer errors and that overgener-
ate less) before ?guessing? modules that are more
error-prone and given to overgeneration. Simplify-
ing somewhat the current system for OC contains the
following three levels:
1. Word list ? a list of 250 most frequent OC
words accompanied with their possible analy-
ses. Most of these words are closed class.
2. Lexicon-based analyzer ? the lexicon has been
automatically acquired from a plain corpus us-
ing the knowledge of manually provided infor-
mation about paradigms (see below).
3a. Guesser ? this module analyzes words relying
purely on the analysis of possible endings and
their relations to the known paradigms. Thus
the English word goes would be analyzed not
only as a verb, but also as plural of the po-
tential noun goe, as a singular noun (with the
presumed plural goeses), etc. In Slavic lan-
guages the situation is complicated by high in-
cidence of homonymous endings. For exam-
ple, the Modern Czech ending a has 14 differ-
ent analyses (and that assumes one knows the
morpheme boundary).
Obviously, the guesser has low precision, and
fails to use all kinds of knowledge that it po-
tentially could use. Crucially, however, it has
high recall, so it can be used as a safety net
when the more precise modules fail. It is also
used during lexicon acquisition, another con-
text where its low precision turns out not to be
a major problem.
3b. Modern Czech word list ? a simple analyzer
of Modern Czech; for some words this module
gives the correct answer (e.g., sva?tek ?holiday?,
some proper names).
The total amount of language-specific work needed
to provide OC data for the analyzer (information
about paradigms, analyses of frequent forms) is
about 12 hours and was done by a non-linguist on
the basis of (Va?z?ny?, 1964; Dosta?l, 1967).
The results of the analyzer are summarized in Ta-
ble 5. They show a similar pattern to the results we
have obtained for other fusional languages. As can
be seen, morphological analysis without any filters
(the first two columns) gives good recall but also
very high average ambiguity. When the automat-
ically acquired lexicon and the longest-ending fil-
ter (analyses involving the longest endings are pre-
ferred) are used, the ambiguity is reduced signifi-
cantly but recall drops as well. As with other lan-
guages, even for OC, it turns out that the drop in
recall is worth the ambiguity reduction when the re-
sults are used by our MA-based taggers. Moreover,
as we mentioned in the previous section, the tag-
ger based purely on the MC corpus has no chance
on verbal forms that disappeared from the language
completely.
15
Old Czech te[t
M$
anal\zed Old 
Cz te[t
tagged 
Old Cz te[t
3
tag translation
4
record of the 
original tags
compiling tnt 
emissions
5
tnt
tag back 
translation
eYen OCz 
emissions
Old Czech te[t
tag translation
1
tag translation
2
Cz 
transitions
Cz 
emissions
M$ Creation
)reTuent forms
/e[icon  Paradigms
(nding based *uesser
Modern Czech )orms
Figure 2: Schema of the MA Based Even Tagger
Lexicon & leo no yes
Recall Ambi Recall Ambi
Overall 96.9 14.8 91.5 5.7
Nouns 99.9 26.1 83.9 10.1
Adjectives 96.8 26.5 96.8 8.8
Verbs 97.8 22.1 95.6 6.2
Table 5: Evaluation of the morphological analyzer on Old
Czech
6.3 Even Tagger
The Even tagger (see Figure 2) approximates emis-
sions by using the output of the morphological ana-
lyzer described in the previous section.
The transition probabilities are based on the Aged
Modern Czech corpus (result of step 2 of Figure 1).
This means that the transitions are produced during
the training phase and are independent of the tagged
text. However, the emissions are produced by the
morphological analyzer on the basis of the tagged
text during tagging. The reason why the model
is called Even is that the emissions are distributed
evenly (uniformly; which is a crude approximation
of reality).
The overall performance of the Even tagger drops
down, but it improves on verbs significantly. Intu-
All Full: 67.7
SubPOS 87.0
Nouns Full 44.3
SubPOS 88.6
Adjs Full: 50.8
SubPos 87.3
Verbs Full 74.4
SubPOS 78.9
Table 6: Performance of the Even Tagger on major POS
categories (in %)
itively, this seems natural, because there is a rel-
atively small homonymy among many OC verbal
endings (see Table 2 for an example) so they are
predicted by the morphological analyzer with low
or even no ambiguity.
6.4 Combining the Translation and Even
Taggers
The TranslEven tagger is a combination of the
Translation and Even models. The Even model
clearly performs better on the verbs, while the Trans-
lation model predicts other categories much better.
So, we decided to combine the two models in the fol-
lowing way. The Even model predicts verbs, while
16
the Translation model predicts the other categories.
The TranslEven Tagger gives us a better overall per-
formance and improves the prediction on each indi-
vidual position of the tag. Unfortunately, it slightly
reduces the performance on nouns (see Tables 7 and
8).
All Full: 74.1
SubPOS 90.6
Nouns Full 57.0
SubPOS 91.3
Adjs Full: 60.3
SubPos 93.7
Verbs Full 80.0
SubPOS 86.7
Table 7: Performance of the TranslEven tagger on major
POS categories (in %)
Full tags: 74.1
Position 0 (POS ): 93.0
Position 1 (SubPOS ): 90.6
Position 2 (Gender ): 89.6
Position 3 (Number ): 92.5
Position 4 (case ): 83.6
Position 5 (PossGen): 99.5
Position 6 (PossNr ): 94.9
Position 7 (person ): 94.9
Position 8 (tense ): 95.6
Position 9 (grade ): 98.6
Position 10 (negation): 96.1
Position 11 (voice ): 96.4
Table 8: Performance of the TranslEven tagger on indi-
vidual positions (in %).
7 Discussion
We have described a series of experiments to cre-
ate a tagger for OC. Traditional statistical taggers
rely on large amounts of training (annotated) data.
There is no realistic prospect of annotation for OC.
The practical restrictions (no native speakers, lim-
ited corpora and lexicons, limited funding) make OC
an ideal candidate for a resource-light cross-lingual
method that we have been developing. OC and MC
departed significantly over the 500+ years, at all lan-
guage layers, including phonology, syntax and vo-
cabulary. Words that are still used in MC are often
used with different distributions and have different
morphological forms from OC.
Additional difficulty of this task arises from the
fact that our MC and OC corpora belong to different
genres. While the OC corpus includes poetry, cook-
books, medical and liturgical texts, the MC corpus
is mainly comprised of newspaper texts. We can-
not possibly expect a significant overlap in lexicon
or syntactic constructions. For example, the cook-
books contain a lot of imperatives and second per-
son pronouns which are rare or non-existent in the
newspaper texts.
Even though our tagger does not perform as the
state-of-the-art tagger for Czech, the results are al-
ready useful. Remember that the tag is a combina-
tion of 12 morphological features and if only one of
them is incorrect, the whole positional tag is marked
as incorrect. So, the performance of the tagger
(74%) on the whole tag is not as low in reality. For
example, if one is only interested in detailed POS
information (the tagset that roughly corresponds to
the English Penn Treebank tagset in size), the per-
formance of our system is over 90%.
Acknowledgments
This research was generously supported by
the Grant Agency Czech Republic (project ID:
P406/10/P328) and by the U.S. NSF grants
#0916280, #1033275, and #1048406. We would
like to thank Alena M. C?erna? and Boris Lehec?ka
for annotating the testing corpus and for answering
questions about Old Czech. We also thank Institute
of Czech Language of the Academy of Sciences of
the Czech Republic for the plain text corpus of Old
Czech. Finally, we thank anonymous reviewers for
their insightful comments. All mistakes are ours.
References
Be?mova, A., J. Hajic, B. Hladka?, and J. Panevova?
(1999). Morphological and Syntactic Tagging of
the Prague Dependency Treebank. In Proceedings
of ATALA Workshop, pp. 21?29. Paris, France.
Bo?hmova?, A., J. Hajic, E. Hajic?ova?, and B. Hladka?
(2001). The Prague Dependency Treebank:
Three-Level Annotation Scenario. In A. Abeille?
(Ed.), Treebanks: Building and Using Syntacti-
17
cally Annotated Corpora. Kluwer Academic Pub-
lishers.
Brants, T. (2000). TnT ? A Statistical Part-of-
Speech Tagger. In Proceedings of ANLP-NAACL,
pp. 224?231.
Cucerzan, S. and D. Yarowsky (2000). Language
Independent Minimally Supervised Induction of
Lexical Probabilities. In Proceedings of the 38th
Meeting of the Association for Computational
Linguistics (ACL), Hong Kong, pp. 270?277.
Cucerzan, S. and D. Yarowsky (2002). Bootstrap-
ping a Multilingual Part-of-speech Tagger in One
Person-day. In Proceedings of the 6th Confer-
ence on Natural Language Learning (CoNLL),
pp. 132?138. Taipei, Taiwan.
Dosta?l, A. (1967). Historicka? mluvnice c?eska? II ?
Tvaroslov??. 2. C?asova?n?? [Historical Czech Gram-
mar II - Morphology. 2. Conjugation]. Prague:
SPN.
Feldman, A. and J. Hana (2010). A resource-light
approach to morpho-syntactic tagging. Amster-
dam/New York, NY: Rodopi.
Hajic?, J. (2004). Disambiguation of Rich Inflection:
Computational Morphology of Czech. Praha:
Karolinum, Charles University Press.
Hana, J. (2008). Knowledge- and labor-light mor-
phological analysis. OSUWPL 58, 52?84.
Hana, J., A. Feldman, and C. Brew (2004, July). A
resource-light approach to Russian morphology:
Tagging Russian using Czech resources. In D. Lin
and D. Wu (Eds.), Proceedings of EMNLP 2004,
Barcelona, Spain, pp. 222?229. Association for
Computational Linguistics.
Janda, L. A. and C. E. Townsend (2002). Czech.
Karl??k, P., M. Nekula, and Z. Rus??nova? (1996).
Pr???ruc?n?? mluvnice c?es?tiny [Concise Grammar of
Czech]. Praha: Nakladatelstv?? Lidove? Noviny.
Lehec?ka, B. and K. Volekova? (2011).
(polo)automaticka? poc???tac?ova? transkripce
[(semi)automatic computational transcription].
In Proceedings of the Conference De?jiny c?eske?ho
pravopisu (do r. 1902) [History of the Czech
spelling (before 1902)]. in press.
Mann, S. E. (1977). Czech Historical Grammar.
Hamburg: Buske.
Merialdo, B. (1994). Tagging English Text with
a Probabilistic Model. Computational Linguis-
tics 20(2), 155?171.
Naughton, J. (2005). Czech: An Essential Gram-
mar. Oxon, Great Britain and New York, NY,
USA: Routledge.
Short, D. (1993). Czech. In B. Comrie and G. G.
Corbett (Eds.), The Slavonic Languages, Rout-
ledge Language Family Descriptions, pp. 455?
532. Routledge.
Va?z?ny?, V. (1964). Historicka? mluvnice c?eska? II
? Tvaroslov??. 1. Sklon?ova?n?? [Historical Czech
Grammar II - Morphology. 1. Declension].
Prague: SPN.
Yarowsky, D., G. Ngai, and R. Wicentowski (2001).
Inducing Multilingual Text Analysis via Robust
Projection across Aligned Corpora. In Proceed-
ings of the First International Conference on Hu-
man Language Technology Research (HLT), pp.
161?168.
Zipf, G. K. (1935). The Psychobiology of Language.
Houghton-Mifflin.
Zipf, G. K. (1949). Human Behavior and the Prin-
ciple of Least-Effort. Addison-Wesley.
18
LAW VIII - The 8th Linguistic Annotation Workshop, pages 38?47,
Dublin, Ireland, August 23-24 2014.
Sentence diagrams: their evaluation and combination
Jirka Hana and Barbora Hladk
?
a and Ivana Luk
?
sov
?
a
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Prague, Czech Republic
{hana,hladka,luksova} (at) ufal.mff.cuni.cz
Abstract
The purpose of our work is to explore the possibility of using sentence diagrams produced by
schoolchildren as training data for automatic syntactic analysis. We have implemented a sentence
diagram editor that schoolchildren can use to practice morphology and syntax. We collect their
diagrams, combine them into a single diagram for each sentence and transform them into a form
suitable for training a particular syntactic parser. In this study, the object language is Czech,
where sentence diagrams are part of elementary school curriculum, and the target format is the
annotation scheme of the Prague Dependency Treebank. We mainly focus on the evaluation of
individual diagrams and on their combination into a merged better version.
1 Introduction
Syntactic parsing has been an attractive topic for both theoretical and computational linguists for many
years. In combination with supervised machine learning techniques, several corpus-based parsers have
been implemented (e.g., (Nivre et al., 2007), (de Marneffe et al., 2006), (McDonald et al., 2005)), com-
bined (e.g., (Surdeanu and Manning, 2010)), and adapted (e.g., (McClosky et al., 2010),(Zhang and
Wang, 2009)). The performance of such techniques directly correlates with the size of training data: the
more annotated data, the better. However, the annotation process is very resource consuming, thus we
have been seeking for alternative ways of faster and cheaper annotation. Namely, we have been inspired
by the solution of crowdsourcing, see e.g. (Brabham, 2013).
In Czech schools, practicing morphology and syntax is an obligatory part of the curriculum.
Schoolchildren draw sentence diagrams similar to syntactic trees in dependency grammar theories (Hud-
son, 1984; Sgall et al., 1986; Mel??cuk, 1988), with labeled nodes and edges. Our goal is to collect such
diagrams and transform them into the annotation scheme of the Prague Dependency Treebank (Haji?c
et al., 2006). Thereby we enlarge training data for taggers and parsers of Czech. Traditionally, dia-
grams that we need are only in students? notebooks so they are not accessible to us at all. Since we
require diagrams electronically, we have been developing a sentence diagram editor
?
Capek. We have
designed it both as a CALL (Computer-Assisted Language Learning) system for practicing morphology
and dependency-based syntax and as a crowdsourcing system for getting annotated data. In addition, the
editor can be used for drawing sentence diagrams in any natural language. On the other hand, transfor-
mation rules have to be specified with respect to a particular target annotation scheme. We introduced
this approach in (Hana and Hladk?a, 2012).
Data quality belongs to the most important issues related to crowdsourcing, see e.g. (Sabou et al.,
2012), (Wang et al., 2010), (Hsueh et al., 2009). We discuss the data quality from two aspects: (i)
evaluation of students? diagrams against teachers? and/or other students? diagrams, i.e. we consider how
diagrams are similar; (ii) combination of students? diagrams of one sentence to get a better diagram, i.e.
we deal with multiple, possibly noisy, annotations and we study if they are useful.
Our paper is organized as follows: in Section 2, we describe Czech sentence diagrams and how they
differ from the PDT annotation scheme. We introduce the
?
Capek editor in Section 3. Section 4 introduces
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
38
a tree edit distance metric we use to quantify the difference between diagrams. Section 5 discusses an
algorithm combining alternative diagrams into a single structure. Finally, some initial evaluation and
other statistics are presented in Section 6.
2 Czech sentence diagrams
In the Czech sentence diagrams (hence SDs), a sentence is represented as a type of dependency structure.
1
The structure is a directed acyclic graph (roughly a tree) with labeled nodes. The nodes correspond to
words: one (most common), multiple (auxiliary words are considered markings on their heads, e.g.
preposition and noun, or a complex verb form share a single node) or none (in case of dropped subjects).
The edges capture the dependency relation between nodes (e.g., between an object and its predicate).
The node label expresses the type of dependency, or syntactic function.
Formally, a sentence diagram over a sentence s = w
1
w
2
. . . w
n
is a directed acyclic graph D =
(Nodes,Edges), where Nodes is a partition of s. Moreover, the Nodes set might contain a dummy
node corresponding to a dropped subject. The first node N
1
of an edge E = (N
1
, N
2
) is a child node of
the second node N
2
.
For illustration, let?s consider the sentence in (1) and its diagram in Figure 1:
(1) (?)
I
R?ano
in the morning
p?ujdu
will go
se
with
sv?ym
my
kamar?adem
friend
na houby.
mushrooming.
?I will go mushrooming with my friend in the morning.?
Since our goal is to get more data annotated according to the PDT schema (the so-called a-layer or
surface syntax), we characterize certain aspects of SD with respect to the PDT conventions depicted in
Figure 2:
? Tokenization. There is a 1:1 correspondence between tokens and nodes in PDT; all punctuation
marks have their corresponding nodes. Cf. 8 tokens and 8 nodes in Example 1 and Figure 2. In
SDs, there is an N:1 correspondence between tokens and nodes (N can be 0 for dropped subjects);
punctuation is mostly ignored. Cf. 8 tokens and 6 nodes in Example 1 and Figure 1.
? Multi-token nodes. SDs operate on both single-token (p?ujdu ?will go?) and multi-token nodes (se
kamar?adem ?with friend?, na houby ?for mushrooms?). The tokens inside each multi-token node are
ordered in accordance with their surface word order. Auxiliary words, auxiliary verbs, prepositions,
modals etc. do not have their own nodes and are always part of a multi-token node. PDT handles
single-token nodes only.
? Subject and predicate. In PDT, predicate is the root and the subject depends on it; in Figure 1,
they are on the same level; cf. the nodes for (j?a) p?ujdu ?I will go?.
? PRO subject. SDs introduce nodes for elided subjects (see the --- node in Figure 1), which are
common in Czech. PDT does not represent them explicitly.
? Morphological tags. We adopt the system of positional tags used in PDT to capture morphological
properties of words. Tags are assigned to each token in the sentence, not to the nodes.
? Syntactical tags (functors). Our SDs use 14 syntactical tags (Subject, Predicate, Attribute, Adver-
bial of time/place/manner/degree/means/cause/reason/condition/opposition, Verbal Complement).
PDT distinguishes significantly higher number of functors, but most of the additional tags are used
in rather specific situations that are captured by different means in school syntax (parenthesis, ellip-
sis), are quite technical (punctuation types), etc. In the vast majority of cases, it is trivial to map SD
functors to PDT functors.
1
For expository reasons, in this paper, we ignore complex sentences consisting of multiple clauses. Their SD is a discon-
nected graph where each component is an SD of a single clause. Such sentences and graphs are however part of the evaluation
in Section 6.
39
Figure 1: A sample of sentence diagram
Figure 2: A sample of PDT tree
Figure 3: A possible sentence diagram draw in
?
Capek
3
?
Capek editor
Since we wanted to provide students with a sentence diagram editor that is easy to use, we have de-
cided not to use the TrEd editor,
2
a versatile, flexible but also complex tool, which is used as the main
annotation tool of the Prague Dependency Treebanks. Instead, we decided to implement
?
Capek, a new
system. It exists as a desktop application, written in Java on top of the Netbeans Platform,
3
and as a web
application.
4
Students use the editor in a similar way as they are used to use chalk/pen at school. A simple and
intuitive GUI supports the following operations:
? JOIN Merge two nodes into a single multi-token node.
? SPL Divide a multi-token into nodes corresponding to single tokens.
? INS Create a node for elided subject.
? LINK Link a node to its governing parent node.
? LAB Label a node with syntactic function.
? MLAB Label a token with morphological function.
Intentionally, we did not make
?
Capek to perform any consistency checks, except acyclicity of the graph.
Thus students can create a graph with several components, all nodes can be a subject, etc.
4 Similarity of sentence diagrams
We compute the similarity between sentence diagrams using a tree edit distance. Our definition is based
on a tree edit distance in (Bille, 2005). It assumes two trees T
1
, T
2
and three edit operations: relabeling
a node, deleting a non-root node, and inserting a node. T
1
is transformed into T
2
by a sequence of edit
2
http://ufal.mff.cuni.cz/tred
3
http://platform.netbeans.org
4
http://capek.herokuapp.com/
40
operations S. Each operation has a particular cost, the cost of the sequence S is simply the sum of the
cost of individual operations. Then tree edit distance between two trees is the cost of a cheapest sequence
of operations turning one tree into another.
Our situation is similar, however:
? the compared sentence diagrams are always over the same sentence, i.e. over the same set of tokens
? diagrams are not trees: they are acyclic graphs but unlike trees they might consist of several compo-
nents (either because they capture complex sentences, or because the students did not finish them).
In addition, a diagram usually has two ?roots?: one for the subject and one for predicate. However,
it is trivial to transform them into the corresponding tree, considering the subject to be the daughter
of the predicate.
Thus, we modify the distance from (Bille, 2005). For an example, see Figure 4 with nodes of two
particular diagrams over a 6-token sentence. The arrows show a token-node mapping specified by the
annotator of D
1
:
? Let D
1
and D
2
be sentence diagrams; we are turning D
2
into D
1
.
? We consider the following operations:
? SPL ? detaching a token from a node
? JOIN ? adding a token to a node
? INS ? adding an empty node (used for elided subjects)
? LINK ? linking a node with its parent and removing all inconsistent edges. If manipulating a
non-root node, relink the node to its new parent and remove the edge to its former parent. If
manipulating a root node, like a in Figure 5 a), link the node to its new parent, e.g. to e, see
Figure 5 b). Then the diagram consists of a single cycle. Thus remove the edge from e to its
former parent c and e becomes a root, see Figure 5 c).
? SLAB ? change node syntactic label
All operations are assumed to have the cost of 1. Without loss of generality, we can assume that
operations are performed in stages: first all SPLs, then all JOINs, etc. In Figure 4, first we apply
SPL twice on the nodes [b, c], [d, e, f ] and then JOIN also twice on the nodes [a], [b] and [e], [f ].
? Finally, the measure is normalized by sentence length. Thus, we redefine the tree edit distance
TED(D
1
, D
2
, n) for diagrams D
1
, D
2
and sentence of n tokens as follows:
TED(D
1
, D
2
, n) = (#SPL+ #JOIN + #INS + #LINK + #SLAB)/n
.
? We define the tree edit distance for annotators A
1
, A
2
and a set of sentences S (s
i
? S) as the
average tree distance over those sentences:
TED(A
1
, A
2
, S) =
1
|S|
|S|
?
i=1
TED(D
i
A
1
, D
i
A
2
, |s
i
|)
.
41
Figure 4: Turning nodes of D
2
into nodes of D
1
Figure 5: Linking a root node
5 Combination of sentence diagrams
We deal with sentence diagrams and their differences before transformation into a target annotation
scheme. We propose a majority-voting method to combine m multiple diagrams D
1
, . . . , D
m
created
by m different users over the sentence s = w
1
w
2
. . . w
n
. In some sense, our task is similar to the
task of combination independently-trained syntactic parsers. However, at least to our knowledge, the
experiments performed so far, e.g. (Surdeanu and Manning, 2010), are based on the assumption that all
input parsers build syntactic structures on the same set of nodes. Given that, we address a significantly
different task. We approach it using the concept of assigning each candidate node and edge a score
based on the number of votes it received from the input diagrams. The votes for edges are weighted by a
specific criterion.
To build a final diagram, we first create its set of nodes FinalNodes, then its set of edges FinalEdges
linking nodes in FinalNodes, and finally extend the set of nodes by any empty nodes. The method can
produce both nodes and edges that do not occur in any of the input diagrams.
Building FinalNodes
1. ?t, u ? s . v(t, u) =
?
m
k=1
?([t, u], D
k
), where ?([t, u], D) = 1 if the tokens t and u are in the
same node in the diagram D, and 0 otherwise. We compute the number of votes v(t, u) to measure
user preferences for having token pair t, u in one node. In total, there are
(
|s|
2
)
token pairs.
2. The set FinalNodes is formed as a partition over tokens induced by the v(t, u) equivalence rela-
tion:
FinalNodes = s/eq where eq(t, u)? v(t, u) > m/2
For illustration, we start with the sentence a b c d and three diagrams with nodes displayed in Figure 6.
All of them consist of two nodes, namelyNodes
1
= {[a, b, c], [d]},Nodes
2
= {[a], [b, c, d]},Nodes
3
=
{[a, b], [c, d]}. First, we calculate the votes for each possible token pairs, see Table 1. There are two
candidates with a majority of votes, namely (a, b) and (b, c), both with two votes. Thus, FinalNodes =
{[a, b, c], [d]}. A final diagram consists of n nodes [w
1
], . . . , [w
n
] if there is no candidate with majority
of votes, see Figure 7 and Table 2.
42
Figure 6: Sentence a b c d and nodes in three diagrams
a b c d
a x 2 1 0
b x x 2 1
c x x x 1
d x x x x
Table 1: Two candidates for joining
Figure 7: Sentence a b c d and nodes in three other diagrams
a b c d
a x 1 0 1
b x x 1 0
c x x x 1
d x x x x
Table 2: No candidates with the great majority of votes
Building FinalEdges
1. fn = |FinalNodes|
2. ?D
k=1,...,m
, ?E = (N
1
, N
2
) ? Edges
k
, ?(t, u) ? tokens(N
1
) ? tokens(N
2
) : v
k
(t, u) =
1/(|tokens(N
1
)||tokens(N
2
)|). We compute v
k
(t, u) to measure user preference for having token
t in a node dependent on a node containing u. We take it proportionally to the number of tokens in
two particular nodes.
3. We initialize a set of potential edges as a set of all possible edges over the final nodes. I.e.
PotentialEdges is formed as a variation of fn nodes choose 2. Let p = |PotentialEdges| =
fn(fn? 1). Then weights are assigned to the potential edges:
?E = (N
1
, N
2
) ? PotentialEdges : v
E
=
?
m
k=1
v
k
(t, u), (t, u) ? tokens(N
1
)? tokens(N
2
)
4. Sort PotentialEdges so that v
E
1
? v
E
2
? ? ? ? ? v
E
p
5. FinalEdges := ?
6. until PotentialEdges = ?
? FinalEdges := FinalEdges ? E
1
? PotentialEdges := PotentialEdges \ E
1
? PotentialEdges := PotentialEdges \ ?E
1
? PotentialEdges := PotentialEdges \ {E : E ? FinalEdges has a cycle}
For illustration, we assume three diagrams D
1
, D
2
, D
3
displayed in Figure 8. We compute weights
of token pairs proportionally to the number of tokens in nodes identifying a given edge, e.g. the edge
([a, b], [c]) in D
1
determines two token pairs (a, c) and (b, c), each of them with the weight 1/2. See
Table 3 for other weights. Let FinalNodes = {[a, b], [c], [d]}. There are six possible edges connecting
the final nodes, namely ([a, b], [c]),([c], [a, b]),([a, b], [d]),([d], [a, b]),([c], [d]),([d], [c]). For each of them,
we compute its weight, see Table 4. Then we sort them ? ([a, b], [c]), ([c], [d]), ([a, b], [d]), ([c], [a, b]),
([d], [a, b]), ([d], [c]). Table 5 traces the algorithm for adding edges into a final diagram. Finally, we get
the diagram D in Figure 8.
43
([a, b], [c]) ([c], [a, b]) ([a, b], [d]) ([d], [a, b]) ([c], [d]) ([d], [c])
weight 13/6 0 1/2 0 1 0
Table 4: Computing weights of edges-candidates to be added into a final diagram
1
st
FinalEdges
PotentialEdges ([a, b], [c]) ([c], [d]) ([a, b], [d]) ([c], [a, b]) ([d], [a, b]) ([d], [c])
2
nd
FinalEdges ([a, b], [c])
PotentialEdges ([c], [d]) ([a, b], [d])





([c], [a, b]) ([d], [a, b]) ([d], [c])
3
rd
FinalEdges ([a, b], [c]) ([c], [d])
PotentialEdges





([a, b], [d])





([d], [a, b])




([d], [c])
Table 5: Adding edges into a final diagram
Figure 8: Input diagrams D
1
, D
2
, D
3
and final diagram D
D
1
D
2
D
3
token weight token weight token weight
pair pair pair
(a, c) 1/2 (a, c) 1/4 (a, c) 1/3
(b, c) 1/2 (a, d) 1/4 (b, c) 1/3
(c, d) 1 (b, c) 1/4 (d, c) 1/3
(b, d) 1/4
Table 3: Assigning weights to token pairs
6 Data and initial experiments
We randomly selected a workbench of 101 sentences from a textbook of Czech language for elementary
schools (Stybl??k and Melichar, 2005) with the average length of 8.5 tokens, for details see Figure 9.
These sentences were manually analysed according to the school system with the emphasis placed on
syntactic analysis. Namely, elementary school teachers T1 and T2 and secondary school students S1 and
S2 drew school system diagrams using
?
Capek 1.0. Teachers T1 and T2 are colleagues from the same
school but they were drawing diagrams separately. Students S1 and S2 study at different schools and
they are students neither of T1 nor T2. In Table 6, we present TED for pairs of teachers and students.
As we expected, the teachers? diagrams are the most similar ones and on the other hand, the students?
diagrams are the most different one. Taking teacher T1 as a gold-standard data, student S1 made less
errors that student S2. We analyzed differences in details considering two aspects:
? Do nodes closer to the root node cause more differences? A diagram D2 is transformed into a
diagram D1 by a sequence of operations (SPL, JOIN, INS, LINK, SLAB) where the first operation
Figure 9: Length of sentences in the workbench
Figure 10: TED vs. Sentence length
44
(T1,T2) (T1,S1) (T1,S2) (S1,S2) U1 U2 U3 U4 U5 U6 U7 MV
# of sentences 101 91 101 91 10 10 10 10 10 10 10 10
TED 0.26 0.49 0.56 0.69 0.78 0.63 0.56 0.76 0.38 0.62 1.21 0.40
Table 6: TED for pairs of teachers and students, for pairs of teacher T1 and users U1,...,U7 and their combination MV
Figure 11: First Error Depth
is applied on the node in some depth of D2 (where the depth of a node is the length of the path from
the root to that node). Figure 11 illustrates this depth for pairs of teachers and students. We observe
that the very first operation is applied in the root nodes mostly. So we can suppose that recognizing
predicate and its dependent nodes is the most difficult step for users.
? Do longer sentences cause more difficulties? In Figure 10, we observe that the sentence length does
not influence discrepancies between teachers at all (measured by TED). For students, we can see
peaks for sentences of 12, 15, 17, 23 tokens. However, we suppose that longer sentences do not
cause obstacles for them.
A group of 7 users U1, . . . , U7, graduate and undergraduate students, drew diagrams for 10 (S
10
)
sentences randomly selected from the workbench using
?
Capek 2.0. We merged their analyses using
the MV algorithm. When the final diagrams are compared to the diagrams by the T1 teacher, we get
TED(T
1
,MV (U1, . . . , U8), S
10
) = 0.4. To see whether we built a better final diagram, we computed
TED(T
1
, U
i
, S
10
) for each user ? see columns U1,. . . ,U7 in Table 6. One can see that only one user
(U5) has a slightly better agreement with the T1 diagrams. The user U7 actually managed to have more
than one error (differences from T1) per annotated token.
7 Conclusion
In this paper, we have shown our motivation for getting more syntactically annotated data by sentence
diagrams transformation. We have implemented
?
Capek, a diagram editor, which allows students to per-
form sentence analysis electronically. We can then collect their diagrams easily. The editor is designed
as both a CALL and crowdsourcing system for practicing morphology and syntax and for collecting dia-
grams over a given set of sentences. Both aspects have to deal with a quantitative measure of agreement,
therefore we designed a tree edit distance metric comparing two or multiple diagrams. In addition, we
have formulated an algorithm combining multiple crowdsourced diagrams into a single better diagram.
Finally, we presented the results of a pilot study with promising results.
In the near future, to get more statistically significant results, we plan to address the following issues:
? evaluating the combination algorithm on complex sentences
? specifying the practice of crowdsourcing: how to distribute tasks, and how to assign voting weights
to users based on their past results
? getting more diagrams
45
Acknowledgements
The authors would like to thank numerous persons for their sentence diagrams drawn using
?
Capek. We
gratefully acknowledge support from the Charles University Grant Agency (grant no. 1568314), Charles
University in Prague, Faculty of Mathematics and Physics. This work has been using language resources
developed and/or stored and/or distributed by the LINDAT/CLARIN project of the Ministry of Education,
Youth and Sports of the Czech Republic (project LM2010013).
References
Philip Bille. 2005. A survey on tree edit distance and related problems. Theoretical computer science, 337(1):217?
239.
Daren C. Brabham. 2013. Crowdsourcing. MIT Press.
Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the 8th International Conference on Language Resources
and Evaluation (LREC 2006), pages 449?454.
Jan Haji?c, Eva Haji?cov?a, Jarmila Panevov?a, Petr Sgall, Petr Pajas, Jan
?
St?ep?anek, Ji?r?? Havelka, and Marie Mikulov?a.
2006. Prague Dependency Treebank 2.0. Number LDC2006T01. Linguistic Data Consortium.
Jirka Hana and Barbora Hladk?a. 2012. Getting more data: Schoolkids as annotators. In Proceedings of the 8th
International Conference on Language Resources and Evaluation (LREC 2012), pages 4049?4054,
?
Istanbul,
Turkey. European Language Resources Association.
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani. 2009. Data quality from crowdsourcing: A study of an-
notation selection criteria. In Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural
Language Processing, HLT ?09, pages 27?35, Stroudsburg, PA, USA. Association for Computational Linguis-
tics.
Richard Hudson. 1984. Word Grammar. Blackwell.
David McClosky, Eugene Charniak, and Mark Johnson. 2010. Automatic domain adaptation for parsing. In
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association
for Computational Linguistics, HLT ?10, pages 28?36, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Haji?c. 2005. Non-projective dependency parsing using
spanning tree algorithms. In Proceedings of the Conference on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ?05, pages 523?530, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Igor Mel??cuk. 1988. Dependency syntax: theory and practice. State University of New York Press.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, Glsen Eryigit, Sandra K?ubler, Svetoslav Marinov, and
Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural
Language Engineering, 13(2):95?135.
Marta Sabou, Kalina Bontcheva, and Arno Scharl. 2012. Crowdsourcing research opportunities: Lessons from
natural language processing. In Proceedings of the 12th International Conference on Knowledge Management
and Knowledge Technologies, i-KNOW ?12, pages 17:1?17:8, New York, NY, USA. ACM.
Petr Sgall, Eva Haji?cov?a, and Jarmila Panevov?a. 1986. The Meaning of the Sentence and Its Semantic and
Pragmatic Aspects. Academia/Reidel Publishing Company, Prague, Czech Republic/Dordrecht, Netherlands.
Vlastimil Stybl??k and Ji?r?? Melichar. 2005.
?
Cesk?y jazyk - P?rehled u?civa z?akladn?? ?skoly. Fortuna.
Mihai Surdeanu and Christopher D. Manning. 2010. Ensemble models for dependency parsing: Cheap and
good? In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ?10, pages 649?652, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Aobo Wang, Cong Duy Vu Hoang, and Min-Yen Kan. 2010. Perspectives on crowdsourcing annotations for
natural language processing.
46
Yi Zhang and Rui Wang. 2009. Cross-domain dependency parsing using a deep linguistic grammar. In Proceed-
ings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ?09, pages 378?386, Stroudsburg,
PA, USA. Association for Computational Linguistics.
47
