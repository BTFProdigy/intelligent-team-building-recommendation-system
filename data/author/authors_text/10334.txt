BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 94?95,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics 1
Conditional Random Fields and Support Vector Machines for Disorder 
Named Entity Recognition in Clinical Texts 
Dingcheng Li Karin Kipper-Schuler Guergana Savova 
University of Minnesota Mayo Clinic College of Medicine Mayo Clinic College of Medicine 
Minneapolis, Minnesota, USA Rochester, Minnesota, USA Rochester, Minnesota, USA 
lixxx345@umn.edu schuler.karin@mayo.edu savova.guergana@mayo.edu 
 
Abstract 
We present a comparative study between 
two machine learning methods, Conditional 
Random Fields and Support Vector Ma-
chines for clinical named entity recognition. 
We explore their applicability to clinical 
domain. Evaluation against a set of gold 
standard named entities shows that CRFs 
outperform SVMs. The best F-score with 
CRFs is 0.86 and for the SVMs is 0.64 as 
compared to a baseline of 0.60. 
1 Introduction and background 
Named entity recognition (NER) is the discovery 
of named entities (NEs), or textual mentions that 
belong to the same semantic class. In the biomedi-
cal domain NEs are diseases, signs/symptoms, ana-
tomical signs, and drugs. NER performance is high 
as applied to scholarly text and newswire narra-
tives (Leaman et al, 2008). Clinical free-text, on 
the other hand, exhibits characteristics of both in-
formal and formal linguistic styles which, in turn, 
poses challenges for clinical NER. Conditional 
Random Fields (CRFs) (Lafferty et al, 2001) and 
and Support Vector Machines (SVMs) (Cortes and 
Vapnik, 1995) are machine learning techniques 
which can handle multiple features during learn-
ing. CRFs? main strength lies in their ability to in-
clude various unrelated features, while SVMs? in 
the inclusion of overlapping features.  Our goal is 
to compare CRFs and SVMs performance for 
clinical NER with focus on disease/disorder NEs. 
2 Dataset and features 
Our dataset is a gold standard corpus of 1557 sin-
gle- and multi-word disorder annotations (Ogren et 
al., 2008). For training and testing the CRF and 
SVM models the IOB (inside-outside-begin) nota-
tion (Leaman, 2008) was applied. In our project, 
we used 1265 gold standard annotations for train-
ing and 292 for testing. The features used for the 
learning process are described as follows. Diction-
ary look-up is a binary value feature that represents 
if the NE is in the dictionary (SNOMED-CT). Bag 
of Words (BOW) is a representation of the context 
by the unique words in it. Part-of-speech tags 
(POS) of BOW is the pos tags of the context 
words. Window size is the number of tokens repre-
senting context surrounding the target word. Ori-
entation(left or right) is the location of the feature 
in regard to the target word. Distance is the prox-
imity of the feature in regard to the target word 
Capitalization has one of the four token-based val-
ues: all upper case, all lower case, mixed_case and 
initial upper case. Number features refer to the 
presence or absence of related numbers. Feature 
sets are in Table 1. 
3 Results and discussion 
Figure 1 shows the CRF results. The F-scores, re-
call and precision for the baseline dictionary look-
up are 0.604, 0.468 and 0.852 respectively. When 
BOW is applied in feature combination 2 results 
improve sharply adding 0.15, 0.17 and 0.08 points 
respectively. The F-score, recall and precision im-
prove even further with the capitalization feature to 
0.858, 0.774 and 0.963 respectively. Figure 2 
shows SVM results. The addition of more features 
to the model did not show an upward trend. The 
best results are with feature combination 1 and 3. 
The F-score reaches 0.643, which although an im-
provement over the baseline greatly underperforms 
CRF results. BOW features seem not discrimina-
tive with SVMs. When the window size increases 
to 5, performance decreases as demonstrated in 
feature combinations 2, 4 and 8. Results with fea-
ture combination 4, in particular, has a pronounced 
downward trend. Its F-score is 0.612, a decrease by 
0.031 compared with Test 1 or Test 3. Its recall 
and precision are 0.487 and 0.822 respectively, a 
decrease by 0.036 and 0.01 respectively. This sup-
ports the results achieved with CRFs where a 
smaller window size yields better performance. 
 
94
 2
No Features 
1 dictionary look-up (baseline) 
2 dictionary look-up+BOW+Orientation+distance (Win-
dow 5) 
3 dictionary look-up + BOW + Orientation + distance 
(Window 3) 
4 dictionary look-up + BOW  + POS + Orientation + 
distance (Window 5) 
5 dictionary look-up + BOW +POS + Orientation + dis-
tance (Window 3) 
6 dictionary look-up + BOW +POS + Orientation + dis-
tance (Window 3) + bullet number 
7 dictionary look-up + BOW + POS + Orientation + 
distance(Window 3) + measurement 
8 dictionary look-up + BOW + POS + Orientation + 
distance  (Window 5) + neighboring number 
9 dictionary look-up + BOW +POS + Orientation + dis-
tance (Window 3) + neighboring number 
10 dictionary look-up + BOW +POS + Orientation + dis-
tance (Window 3)+neighboring number+measurement 
11 dictionary look-up+BOW+POS+Orientation (Window 
3)+neighboring number+bullet number + measurement 
12 dictionary look-up + BOW +POS + Orientation 
+distance (Window 3) + neighboring number + bullet 
number + measurement + capitalization 
Table 1: Feature combinations 
 
 
Figure 1: CRF evaluation results 
 
Figure 2: SVM evaluation results 
 
As the results show, context represented by the 
BOW feature plays an important role indicating the 
importance of the words surrounding NEs. On the 
other hand, POS tag features did not bring much 
improvement, which perhaps hints at a hypothesis 
that grammatical roles are not as important as con-
text in clinical text. Thirdly, a small window size is 
more discriminative. Clinical notes are unstruc-
tured free text with short sentences. If a larger win-
dow size is used, many words will share similar 
features. Fourthly, capitalization is highly dis-
criminative. Fifthly, as a finite state machine de-
rived from HMMs, CRFs can naturally consider 
state-to-state dependences and feature-to-state de-
pendences. On the other hand, SVMs do not con-
sider such dependencies. SVMs separate the data 
into categories via a kernel function. They imple-
ment this by mapping the data points onto an opti-
mal linear separating hyperplane. Finally, SVMs 
do not behave well for large number of feature 
values. For large number of feature values, it 
would be more difficult to find discriminative lines 
to categorize the labels. 
4 Conclusion and future work 
We investigated the use of CRFs and SVMs for 
disorder NER in clinical free-text. Our results 
show that, in general, CRFs outperformed SVMs. 
We demonstrated that well-chosen features along 
with dictionary-based features tend to improve the 
CRF model?s performance but not the SVM?s.  
Acknowledgements 
The work was partially supported by a Biomedical 
Informatics and Computational Biology scholar-
ship from the University of Minnesota. 
References 
Corinna Cortes and Vladimir Vapnik. Support-vector 
network. Machine Learning, 20:273-297, 1995. 
John Lafferty, Andrew McCallum and Fernando 
Pereira. Conditional Random Fields: Probabilistic 
Models for Segmenting and Labeling Sequence Data. 
In Proceedings of the Eighteenth International Con-
ference on Machine Learning (ICML-2001), 2001. 
Robert Leaman and Graciela Gonzalez. BANNER: an 
Executable Survey of Advances in Biomedical 
Named Entity Recognition. Pacific Symposium on 
Biocomputing 13:652-663. 2008. 
Philip Ogren, Guergana Savova and Christopher G 
Chute. Constructing evaluation corpora for auto-
mated clinical named entity recognition. Proc LREC 
2008. 
95
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1169?1178,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Pronoun Anaphora Resolution System based on
Factorial Hidden Markov Models
Dingcheng Li
University of Minnesota,
Twin Cities, Minnesosta
lixxx345@umn.edu
Tim Miller
University of Wisconsin
Milwaukee, Wisconsin
tmill@cs.umn.edu
William Schuler
The Ohio State University
Columbus, Ohio
schuler@ling.osu.edu
Abstract
This paper presents a supervised pronoun
anaphora resolution system based on factorial
hidden Markov models (FHMMs). The ba-
sic idea is that the hidden states of FHMMs
are an explicit short-term memory with an an-
tecedent buffer containing recently described
referents. Thus an observed pronoun can find
its antecedent from the hidden buffer, or in
terms of a generative model, the entries in the
hidden buffer generate the corresponding pro-
nouns. A system implementing this model is
evaluated on the ACE corpus with promising
performance.
1 Introduction
Pronoun anaphora resolution is the task of find-
ing the correct antecedent for a given pronominal
anaphor in a document. It is a subtask of corefer-
ence resolution, which is the process of determin-
ing whether two or more linguistic expressions in
a document refer to the same entity. Adopting ter-
minology used in the Automatic Context Extraction
(ACE) program (NIST, 2003), these expressions
are called mentions. Each mention is a reference
to some entity in the domain of discourse. Men-
tions usually fall into three categories ? proper men-
tions (proper names), nominal mentions (descrip-
tions), and pronominal mentions (pronouns). There
is a great deal of related work on this subject, so
the descriptions of other systems below are those
which are most related or which the current model
has drawn insight from.
Pairwise models (Yang et al, 2004; Qiu et al,
2004) and graph-partitioning methods (McCallum
and Wellner, 2003) decompose the task into a col-
lection of pairwise or mention set coreference de-
cisions. Decisions for each pair or each group
of mentions are based on probabilities of features
extracted by discriminative learning models. The
aforementioned approaches have proven to be fruit-
ful; however, there are some notable problems. Pair-
wise modeling may fail to produce coherent parti-
tions. That is, if we link results of pairwise deci-
sions to each other, there may be conflicting corefer-
ences. Graph-partitioning methods attempt to recon-
cile pairwise scores into a final coherent clustering,
but they are combinatorially harder to work with in
discriminative approaches.
One line of research aiming at overcoming the
limitation of pairwise models is to learn a mention-
ranking model to rank preceding mentions for a
given anaphor (Denis and Baldridge, 2007) This ap-
proach results in more coherent coreference chains.
Recent years have also seen the revival of in-
terest in generative models in both machine learn-
ing and natural language processing. Haghighi
and Klein (2007), proposed an unsupervised non-
parametric Bayesian model for coreference resolu-
tion. In contrast to pairwise models, this fully gener-
ative model produces each mention from a combina-
tion of global entity properties and local attentional
state. Ng (2008) did similar work using the same un-
supervised generative model, but relaxed head gen-
eration as head-index generation, enforced agree-
ment constraints at the global level, and assigned
salience only to pronouns.
Another unsupervised generative model was re-
cently presented to tackle only pronoun anaphora
1169
resolution (Charniak and Elsner, 2009). The
expectation-maximization algorithm (EM) was ap-
plied to learn parameters automatically from the
parsed version of the North American News Cor-
pus (McClosky et al, 2008). This model generates a
pronoun?s person, number and gender features along
with the governor of the pronoun and the syntactic
relation between the pronoun and the governor. This
inference process allows the system to keep track of
multiple hypotheses through time, including multi-
ple different possible histories of the discourse.
Haghighi and Klein (2010) improved their non-
parametric model by sharing lexical statistics at the
level of abstract entity types. Consequently, their
model substantially reduces semantic compatibility
errors. They report the best results to date on the
complete end-to-end coreference task. Further, this
model functions in an online setting at mention level.
Namely, the system identifies mentions from a parse
tree and resolves resolution with a left-to-right se-
quential beam search. This is similar to Luo (2005)
where a Bell tree is used to score and store the
searching path.
In this paper, we present a supervised pro-
noun resolution system based on Factorial Hidden
Markov Models (FHMMs). This system is moti-
vated by human processing concerns, by operating
incrementally and maintaining a limited short term
memory for holding recently mentioned referents.
According to Clark and Sengul (1979), anaphoric
definite NPs are much faster retrieved if the an-
tecedent of a pronoun is in immediately previous
sentence. Therefore, a limited short term memory
should be good enough for resolving the majority of
pronouns. In order to construct an operable model,
we also measured the average distance between pro-
nouns and their antecedents as discussed in next sec-
tions and used distances as important salience fea-
tures in the model.
Second, like Morton (2000), the current sys-
tem essentially uses prior information as a dis-
course model with a time-series manner, using a
dynamic programming inference algorithm. Third,
the FHMM described here is an integrated system,
in contrast with (Haghighi and Klein, 2010). The
model generates part of speech tags as simple struc-
tural information, as well as related semantic in-
formation at each time step or word-by-word step.
While the framework described here can be ex-
tended to deeper structural information, POS tags
alone are valuable as they can be used to incorpo-
rate the binding features (described below).
Although the system described here is evaluated
for pronoun resolution, the framework we describe
can be extended to more general coreference resolu-
tion in a fairly straightforward manner. Further, as
in other HMM-based systems, the system can be ei-
ther supervised or unsupervised. But extensions to
unsupervised learning are left for future work.
The final results are compared with a few super-
vised systems as the mention-ranking model (De-
nis and Baldridge, 2007) and systems compared in
their paper, and Charniak and Elsner?s (2009) unsu-
pervised system, emPronouns. The FHMM-based
pronoun resolution system does a better job than the
global ranking technique and other approaches. This
is a promising start for this novel FHMM-based pro-
noun resolution system.
2 Model Description
This work is based on a graphical model framework
called Factorial Hidden Markov Models (FHMMs).
Unlike the more commonly known Hidden Markov
Model (HMM), in an FHMM the hidden state at
each time step is expanded to contain more than one
random variable (as shown in Figure 1). This al-
lows for the use of more complex hidden states by
taking advantage of conditional independence be-
tween substates. This conditional independence al-
lows complex hidden states to be learned with lim-
ited training data.
2.1 Factorial Hidden Markov Model
Factorial Hidden Markov Models are an extension
of HMMs (Ghahramani and Jordan, 1997). HMMs
represent sequential data as a sequence of hidden
states generating observation states (words in this
case) at corresponding time steps t. A most likely
sequence of hidden states can then be hypothesized
given any sequence of observed states, using Bayes
Law (Equation 2) and Markov independence as-
sumptions (Equation 3) to define a full probability as
the product of a Transition Model (?T ) prior prob-
ability and an Observation Model (?O) likelihood
1170
probability.
h?1..T
def= argmax
h1..T
P(h1..T | o1..T ) (1)
def= argmax
h1..T
P(h1..T ) ? P(o1..T |h1..T ) (2)
def= argmax
h1..T
T
?
t=1
P?T (ht |ht?1) ? P?O(ot |ht)
(3)
For a simple HMM, the hidden state corresponding
to each observation state only involves one variable.
An FHMM contains more than one hidden variable
in the hidden state. These hidden substates are usu-
ally layered processes that jointly generate the ev-
idence. In the model described here, the substates
are also coupled to allow interaction between the
separate processes. As Figure 1 shows, the hidden
states include three sub-states, op, cr and pos which
are short forms of operation, coreference feature and
part-of-speech. Then, the transition model expands
the left term in (3) to (4).
P?T (ht |ht?1)
def= P(opt | opt?1, post?1)
?P(crt | crt?1, opt?1)
?P(post | opt, post?1)
(4)
The observation model expands from the right
term in (3) to (5).
P?O(ot |ht)
def= P(ot | post, crt) (5)
The observation state depends on more than one hid-
den state at each time step in an FHMM. Each hid-
den variable can be further split into smaller vari-
ables. What these terms stand for and the motiva-
tions behind the above equations will be explained
in the next section.
2.2 Modeling a Coreference Resolver with
FHMMs
FHMMs in our model, like standard HMMs, can-
not represent the hierarchical structure of a syntac-
tic phrase. In order to partially represent this in-
formation, the head word is used to represent the
whole noun phrase. After coreference is resolved,
the coreferring chain can then be expanded to the
whole phrase with NP chunker tools.
In this system, hidden states are composed of
three main variables: a referent operation (OP),
coreference features (CR) and part of speech tags
(POS) as displayed in Figure 1. The transition model
is defined as Equation 4.
opt-1=
copy
post-1=
VBZ
ot-1=loves
et-1=
per,org
gt-1=
neu,fem
crt-1
opt=
old
post=
PRP
ot=them
gt=
fem,neu
crt
ht-1 ht
et=
org,per
nt-1=
plu,sing
nt=
sing,plu
it-1=
-,2
it=
0,2
Figure 1: Factorial HMM CR Model
The starting point for the hidden state at each time
step is the OP variable, which determines which
kind of referent operations will occur at the current
word. Its domain has three possible states: none,
new and old.
The none state indicates that the present state will
not generate a mention. All previous hidden state
values (the list of previous mentions) will be passed
deterministically (with probability 1) to the current
time step without any changes. The new state signi-
fies that there is a new mention in the present time
step. In this event, a new mention will be added to
the entity set, as represented by its set of feature val-
ues and position in the coreference table. The old
state indicates that there is a mention in the present
time state and that this mention refers back to some
antecedent mention. In such a case, the list of enti-
ties in the buffer will be reordered deterministically,
moving the currently mentioned entity to the top of
the list.
Notice that opt is defined to depend on opt?1
and post?1. This is sometimes called a switching
FHMM (Duh, 2005). This dependency can be use-
ful, for example, if opt?1 is new, in which case opt
has a higher probability of being none or old. If
1171
post?1 is a verb or preposition, opt has more proba-
bility of being old or new.
One may wonder why opt generates post, and
not the other way around. This model only roughly
models the process of (new and old) entity genera-
tion, and either direction of causality might be con-
sistent with a model of human entity generation,
but this direction of causality is chosen to represent
the effect of semantics (referents) generating syn-
tax (POS tags). In addition, this is a joint model in
which POS tagging and coreference resolution are
integrated together, so the best combination of those
hidden states will be computed in either case.
2.3 Coreference Features
Coreference features for this model refer to features
that may help to identify co-referring entities.
In this paper, they mainly include index (I),
named entity type (E), number (N) and gender (G).
The index feature represents the order that a men-
tion was encountered relative to the other mentions
in the buffer. The latter three features are well
known and described elsewhere, and are not them-
selves intended as the contribution of this work. The
novel aspect of this part of the model is the fact that
the features are carried forward, updated after ev-
ery word, and essentially act as a discourse model.
The features are just a shorthand way of represent-
ing some well known essential aspects of a referent
(as pertains to anaphora resolution) in a discourse
model.
Features Values
I positive integers from 1. . .n
G male, female, neutral, unknown
N singular, plural, unknown
E person, location, organization,
GPE, vehicle,
company, facility
Table 1: Coreference features stored with each mention.
Unlike discriminative approaches, generative
models like the FHMM described here do not have
access to all observations at once. This model must
then have a mechanism for jointly considering pro-
nouns in tandem with previous mentions, as well as
the features of those mentions that might be used to
find matches between pronouns and antecedents.
Further, higher order HMMs may contain more
accurate information about observation states. This
is especially true for coreference resolution because
pronouns often refer back to mentions that are far
away from the present state. In this case, we would
need to know information about mentions which are
at least two mentions before the present one. In
this sense, a higher order HMM may seem ideal
for coreference resolution. However, higher order
HMMs will quickly become intractable as the order
increases.
In order to overcome these limitations, two strate-
gies which have been discussed in the last section
are taken: First, a switching variable called OP is
designed (as discussed in last section); second, a
memory of recently mentioned entities is maintained
to store features of mentions and pass them forward
incrementally.
OP is intended to model the decision to use the
current word to introduce a new referent (new), refer
to an antecedent (old), or neither (none). The entity
buffer is intended to model the set of ?activated? en-
tities in the discourse ? those which could plausibly
be referred to with a pronoun. These designs allow
similar benefits as longer dependencies of higher-
order HMMs but avoid the problem of intractability.
The number of mentions maintained must be limited
in order for the model to be tractable. Fortunately,
human short term memory faces effectively similar
limitations and thus pronouns usually refer back to
mentions not very far away.
Even so, the impact of the size of the buffer on
decoding time may be a concern. Since the buffer of
our system will carry forward a few previous groups
of coreference features plus op and pos, the compu-
tational complexity will be exorbitantly high if we
keep high beam size and meanwhile if each feature
interacts with others. Luckily, we have successfully
reduced the intractability to a workable system in
both speed and space with following methods. First,
we estimate the size of buffer with a simple count
of average distances between pronouns and their an-
tecedents in the corpus. It is found that about six is
enough for covering 99.2% of all pronouns.
Secondly, the coreference features we have used
have the nice property of being independent from
one another. One might expect English non-person
entities to almost always have neutral gender, and
1172
thus be modeled as follows:
P(et, gt | et?1, gt?1) = P(gt | gt?1, et) ? P(et | et?1)
(6)
However, a few considerations made us reconsider.
First, exceptions are found in the corpus. Personal
pronouns such as she or he are used to refer to coun-
try, regions, states or organizations. Second, existing
model files made by Bergsma (2005) include a large
number of non-neutral gender information for non-
person words. We employ these files for acquiring
gender information of unknown words. If we use
Equation 6, sparsity and complexity will increase.
Further, preliminary experiments have shown mod-
els using an independence assumption between gen-
der and personhood work better. Thus, we treat each
coreference feature as an independent event. Hence,
we can safely split coreference features into sepa-
rate parts. This way dramatically reduces the model
complexity. Thirdly, our HMM decoding uses the
Viterbi algorithm with A-star beam search.
The probability of the new state of the coreference
table P(crt | crt?1, opt) is defined to be the product
of probabilities of the individual feature transitions.
P(crt | crt?1, opt) = P(it | it?1, opt)?
P(et | et?1, it, opt)?
P(gt | gt?1, it, opt)?
P(nt |nt?1, it, opt)
(7)
This supposes that the features are conditionally in-
dependent of each other given the index variable, the
operator and previous instance. Each feature only
depends on the operator and the corresponding fea-
ture at the previous state, with that set of features
re-ordered as specified by the index model.
2.4 Feature Passing
Equation 7 is correct and complete, but in fact the
switching variable for operation type results in three
different cases which simplifies the calculation of
the transition probabilities for the coreference fea-
ture table.
Note the following observations about corefer-
ence features: it only needs a probabilistic model
when opt is old ? in other words, only when the
model must choose between several antecedents to
re-refer to. gt, et and nt are deterministic except
when opt is new, when gender, entity type, and num-
ber information must be generated for the new entity
being introduced.
When opt is none, all coreference variables (en-
tity features) will be copied over from the previous
time step to the current time step, and the probabil-
ity of this transition is 1.0. When opt is new, it is
changed deterministically by adding the new entity
to the first position in the list and moving every other
entity down one position. If the list of entities is
full, the least recently mentioned entity will be dis-
carded. The values for the top of the feature lists
gt, et, and nt will then be generated from feature-
specific probability distributions estimated from the
training data. When opt is old, it will probabilisti-
cally select a value 1 . . . n, for an entity list contain-
ing n items. The selected value will deterministi-
cally order the gt, nt and et lists. This distribution
is also estimated from training data, and takes into
account recency of mention. The shape of this dis-
tribution varies slightly depending on list size and
noise in the training data, but in general the probabil-
ity of a mention being selected is directly correlated
to how recently it was mentioned.
With this understanding, coreference table tran-
sition probabilities can be written in terms of only
their non-deterministic substate distributions:
P(crt | crt?1, old) = Pold(it | it?1)?
Preorder(et | et?1, it)?
Preorder(gt | gt?1, it)?
Preorder(nt |nt?1, it)
(8)
where the old model probabilistically selects the an-
tecedent and moves it to the top of the list as de-
scribed above, thus deciding how the reordering will
take place. The reorder model actually implements
the list reordering for each independent feature by
moving the feature value corresponding to the se-
lected entity in the index model to the top of that
feature?s list. The overall effect is simply the prob-
abilistic reordering of entities in a list, where each
entity is defined as a label and a set of features.
P(crt | crt?1, new) = Pnew(it | it?1)?
Pnew(gt | gt?1)?
Pnew(nt |nt?1)?
Pnew(et | et?1)
(9)
where the new model probabilistically generates a
1173
feature value based on the training data and puts it
at the top of the list, moves every other entity down
one position in the list, and removes the final item if
the list is already full. Each entity in i takes a value
from 1 to n for a list of size n. Each g can be one of
four values ? male, female, neuter and unknown; n
one of three values ? plural, singular and unknown
and e around eight values.
Note that post is used in both hidden states and
observation states. While it is not considered a
coreference feature as such, it can still play an im-
portant role in the resolving process. Basically, the
system tags parts of speech incrementally while si-
multaneously resolving pronoun anaphora. Mean-
while, post?1 and opt?1 will jointly generate opt.
This point has been discussed in Section 2.2.
Importantly, the pos model can help to imple-
ment binding principles (Chomsky, 1981). It is
applied when opt is old. In training, pronouns
are sub-categorised into personal pronouns, reflex-
ive and other-pronoun. We then define a vari-
able loct whose value is how far back in the list
of antecedents the current hypothesis must have
gone to arrive at the current value of it. If we
have the syntax annotations or parsed trees, then,
the part of speech model can be defined when
opt is old as Pbinding(post | loct, sloct). For ex-
ample, if post ? ref lexive, P(post | loct, sloct)
where loct has smaller values (implying closer men-
tions to post) and sloct = subject should have
higher values since reflexive pronouns always re-
fer back to subjects within its governing domains.
This was what (Haghighi and Klein, 2009) did and
we did this in training with the REUTERS cor-
pus (Hasler et al, 2006) in which syntactic roles
are annotated. We finally switched to the ACE
corpus for the purpose of comparison with other
work. In the ACE corpus, no syntactic roles are
annotated. We did use the Stanford parser to ex-
tract syntactic roles from the ACE corpus. But
the result is largely affected by the parsing accu-
racy. Again, for a fair comparison, we extract simi-
lar features to Denis and Baldridge (2007), which is
the model we mainly compare with. They approx-
imate syntactic contexts with POS tags surround-
ing the pronoun. Inspired by this idea, we success-
fully represent binding features with POS tags be-
fore anaphors. Instead of using P(post | loct, sloct),
we train P(post | loct, posloct) which can play
the role of binding. For example, suppose the
buffer size is 6 and loct = 5, posloct = noun.
Then, P(post = ref lexive | loct, posloct) is usu-
ally higher than P(post = pronoun | loct, posloct),
since the reflexive has a higher probability of refer-
ring back to the noun located in position 5 than the
pronoun.
In future work expanding to coreference resolu-
tion between any noun phrases we intend to inte-
grate syntax into this framework as a joint model of
coreference resolution and parsing.
3 Observation Model
The observation model that generates an observed
state is defined as Equation 5. To expand that equa-
tion in detail, the observation state, the word, de-
pends on its part of speech and its coreference fea-
tures as well. Since FHMMs are generative, we can
say part of speech and coreference features generate
the word.
In actual implementation, the observed model will
be very sparse, since crt will be split into more vari-
ables according to how many coreference features it
is composed of. In order to avoid the sparsity, we
transform the equation with Bayes? law as follows.
P?O(ot |ht) =
P (ot) ? P(ht | ot)
?
o? P (o?)P(ht | o?)
(10)
= P (ot) ? P(post, crt | ot)?
o? P (o?)P(post, crt | o?)
(11)
We define pos and cr to be independent of each
other, so we can further split the above equation as:
P?O(ot |ht)
def= P (ot) ? P(post | ot) ? P(crt | ot)?
o? P (o?) ? P(post | o?) ? P(crt | o?)
(12)
where P(crt | ot) = P(gt | ot)P(nt | ot)P(et | ot) and
P(crt | o?) = P(gt | o?)P(nt | o?)P(et | o?).
This change transforms the FHMM to a hybrid
FHMM since the observation model no longer gen-
erates the data. Instead, the observation model gen-
erates hidden states, which is more a combination
of discriminative and generative approaches. This
way facilitates building likelihood model files of fea-
tures for given mentions from the training data. The
1174
hidden state transition model represents prior proba-
bilities of coreference features associated with each
while this observation model factors in the probabil-
ity given a pronoun.
3.1 Unknown Words Processing
If an observed word was not seen in training, the
distribution of its part of speech, gender, number and
entity type will be unknown. In this case, a special
unknown words model is used.
The part of speech of unknown words
P(post |wt = unkword) is estimated using a
decision tree model. This decision tree is built
by splitting letters in words from the end of the
word backward to its beginning. A POS tag is
assigned to the word after comparisons between
the morphological features of words trained from
the corpus and the strings concatenated from the
tree leaves are made. This method is about as
accurate as the approach described by Klein and
Manning (2003).
Next, a similar model is set up for estimating
P(nt |wt = unkword). Most English words have
regular plural forms, and even irregular words have
their patterns. Therefore, the morphological features
of English words can often be used to determine
whether a word is singular or plural.
Gender is irregular in English, so model-based
predictions are problematic. Instead, we follow
Bergsma and Lin (2005) to get the distribution of
gender from their gender/number data and then pre-
dict the gender for unknown words.
4 Evaluation and Discussion
4.1 Experimental Setup
In this research, we used the ACE corpus (Phase 2) 1
for evaluation. The development of this corpus in-
volved two stages. The first stage is called EDT (en-
tity detection and tracking) while the second stage
is called RDC (relation detection and characteriza-
tion). All markables have named entity types such
as FACILITY, GPE (geopolitical entity), PERSON,
LOCATION, ORGANIZATION, PERSON, VEHI-
CLE and WEAPONS, which were annotated in the
first stage. In the second stage, relations between
1See http://projects.ldc.upenn.edu/ace/
annotation/previous/ for details on the corpus.
named entities were annotated. This corpus include
three parts, composed of different genres: newspa-
per texts (NPAPER), newswire texts (NWIRE) and
broadcasted news (BNEWS). Each of these is split
into a train part and a devtest part. For the train
part, there are 76, 130 and 217 articles in NPA-
PER, NWIRE and BNEWS respectively while for
the test part, there are 17, 29 and 51 articles respec-
tively. Though the number of articles are quite dif-
ferent for three genres, the total number of words are
almost the same. Namely, the length of NPAPER
is much longer than BNEWS (about 1200 words,
800 word and 500 words respectively for three gen-
res). The longer articles involve longer coreference
chains. Following the common practice, we used
the devtest material only for testing. Progress during
the development phase was estimated only by using
cross-validation on the training set for the BNEWS
section. In order to make comparisons with publica-
tions which used the same corpus, we make efforts
to set up identical conditions for our experiments.
The main point of comparison is Denis and
Baldridge (2007), which was similar in that it de-
scribed a new type of coreference resolver using
simple features.
Therefore, similar to their practice, we use all
forms of personal and possessive pronouns that were
annotated as ACE ?markables?. Namely, pronouns
associated with named entity types could be used in
this system. In experiments, we also used true ACE
mentions as they did. This means that pleonastics
and references to eventualities or to non-ACE enti-
ties are not included in our experiments either. In
all, 7263 referential pronouns in training data set
and 1866 in testing data set are found in all three
genres. They have results of three different systems:
SCC (single candidate classifier), TCC (twin candi-
date classifier) and RK (ranking). Besides the three
and our own system, we also report results of em-
Pronouns, which is an unsupervised system based
on a recently published paper (Charniak and Elsner,
2009). We select this unsupervised system for two
reasons. Firstly, emPronouns is a publicly available
system with high accuracy in pronoun resolution.
Secondly, it is necessary for us to demonstrate our
system has strong empirical superiority over unsu-
pervised ones. In testing, we also used the OPNLP
Named Entity Recognizer to tag the test corpus.
1175
During training, besides coreference annotation
itself, the part of speech, dependencies between
words and named entities, gender, number and index
are extracted using relative frequency estimation to
train models for the coreference resolution system.
Inputs for testing are the plain text and the trained
model files. The entity buffer used in these exper-
iments kept track of only the six most recent men-
tions. The result of this process is an annotation
of the headword of every noun phrase denoting it
as a mention. In addition, this system does not
do anaphoricity detection, so the antecedent oper-
ation for non-anaphora pronoun it is set to be none.
Finally, the system does not yet model cataphora,
about 10 cataphoric pronouns in the testing data
which are all counted as wrong.
4.2 Results
The performance was evaluated using the ratio of
the number of correctly resolved anaphors over the
number of all anaphors as a success metrics. All the
standards are consistent with those defined in Char-
niak and Elsner (2009).
During development, several preliminary experi-
ments explored the effects of starting from a simple
baseline and adding more features. The BNEWS
corpus was employed in these development exper-
iments. The baseline only includes part of speech
tags, the index feature and and syntactic roles. Syn-
tactic roles are extracted from the parsing results
with Stanford parser. The success rate of this base-
line configuration is 0.48. This low accuracy is par-
tially due to the errors of automatic parsing. With
gender and number features added, the performance
jumped to 0.65. This shows that number and gen-
der agreements play an important role in pronoun
anaphora resolution. For a more standard compari-
son to other work, subsequent tests were performed
on the gold standard ACE corpus (using the model
as described with named entity features instead of
syntactic role features). As shown in Denis and
Baldridge (2007), they employ all features we use
except syntactic roles. In these experiments, the sys-
tem got better results as shown in Table 2.
The result of the first one is obtained by running
the publicly available system emPronouns2. It is a
2the available system in fact only includes the testing part.
Thus, it may be unfair to compare emPronouns this way with
System BNEWS NPAPER NWIRE
emPronouns 58.5 64.5 60.6
SCC 62.2 70.7 68.3
TCC 68.6 74.7 71.1
RK 72.9 76.4 72.4
FHMM 74.9 79.4 74.5
Table 2: Accuracy scores for emPronouns, the single-
candidate classifier (SCC), the twin-candidate classifier
(TCC), the ranker and FHMM
high-accuracy unsupervised system which reported
the best result in Charniak and Elsner (2009).
The results of the other three systems are those
reported by Denis and Baldridge (2007). As Table 2
shows, the FHMM system gets the highest average
results.
The emPronouns system got the lowest results
partially due to the reason that we only directly
run the existing system with its existing model files
without retraining. But the gap between its results
and results of our system is large. Thus, we may
still say that our system probably can do a better job
even if we train new models files for emPronouns
with ACE corpus.
With almost exactly identical settings, why does
our FHMM system get the highest average results?
The convincing reason is that FHMM is strongly in-
fluenced by the sequential dependencies. The rank-
ing approach ranks a set of mentions using a set of
features, and it also maintains the discourse model,
but it is not processing sequentially. The FHMM
system always maintain a set of mentions as well
as a first-order dependencies between part of speech
and operator. Therefore, context can be more fully
taken into consideration. This is the main reason that
the FHMM approach achieved better results than the
ranking approach.
From the result, one point we may notice is that
NPAPER usually obtains higher results than both
BNEWS and NWIRE for all systems while BNEWS
lower than other two genres. In last section, we
mention that articles in NPAPER are longer than
other genres and also have denser coreference chains
while articles in BENEWS are shorter and have
sparer chains. Then, it is not hard to understand
why results of NPAPER are better while those of
other systems.
1176
BNEWS are poorer.
In Denis and Baldridge (2007), they also reported
new results with a window of 10 sentences for RK
model. All three genres obtained higher results than
those when with shorter ones. They are 73.0, 77.6
and 75.0 for BNEWS,NPAPER and NWIRE respec-
tively. We can see that except the one for NWIRE,
the results are still poorer than our system. For
NWIRE, the RK model got 0.5 higher. The average
of the RK is 75.2 while that of the FHMM system is
76.3, which is still the best.
Since the emPronoun system can output sample-
level results, it is possible to do a paired Student?s
t-test. That test shows that the improvement of our
system on all three genres is statistically significant
(p < 0.001). Unfortunately, the other systems only
report overall results so the same comparison was
not so straightforward.
4.3 Error Analysis
After running the system on these documents, we
checked which pronouns fail to catch their an-
tecedents. There are a few general reasons for er-
rors.
First, pronouns which have antecedents very far
away cannot be caught. Long-distance anaphora res-
olution may pose a problem since the buffer size
cannot be too long considering the complexity of
tracking a large number of mentions through time.
During development, estimation of an acceptable
size was attempted using the training data. It was
found that a mention distance of fourteen would ac-
count for every case found in this corpus, though
most cases fall well short of that distance. Future
work will explore optimizations that will allow for
larger or variable buffer sizes so that longer distance
anaphora can be detected.
A second source of error is simple misjudgments
when more than one candidate is waiting for selec-
tion. A simple case is that the system fails to distin-
guish plural personal nouns and non-personal nouns
if both candidates are plural. This is not a problem
for singular pronouns since gender features can tell
whether pronouns are personal or not. Plural nouns
in English do not have such distinctions, however.
Consequently, demands and Israelis have the same
probability of being selected as the antecedents for
they, all else being equal. If demands is closer to
they, demands will be selected as the antecedent.
This may lead to the wrong choice if they in fact
refers to Israelis. This may require better measures
of referent salience than the ?least recently used?
heuristic currently implemented.
Third, these results also show difficulty resolv-
ing coordinate noun phrases due to the simplistic
representation of noun phrases in the input. Con-
sider this sentence: President Barack Obama and
his wife Michelle Obama visited China last week.
They had a meeting with President Hu in Beijing.
In this example, the pronoun they corefers with the
noun phrase President Barack Obama and his wife
Michelle Obama. The present model cannot repre-
sent both the larger noun phrase and its contained
noun phrases. Since the noun phrase is a coordinate
one that includes both noun phrases, the model can-
not find a head word to represent it.
Finally, while the coreference feature annotations
of the ACE are valuable for learning feature mod-
els, the model training may still give some mislead-
ing results. This is brought about by missing fea-
tures in the training corpus and by the data sparsity.
We solved the problem with add-one smoothing and
deleted interpolation in training models besides the
transformation in the generation order of the obser-
vation model.
5 Conclusion and Future Work
This paper has presented a pronoun anaphora resolu-
tion system based on FHMMs. This generative sys-
tem incrementally resolves pronoun anaphora with
an entity buffer carrying forward mention features.
The system performs well and outperforms other
available models. This shows that FHMMs and
other time-series models may be a valuable model
to resolve anaphora.
Acknowledgments
We would like to thank the authors and maintainers
of ranker models and emPronouns. We also would
like to thank the three anonymous reviewers. The
final version is revised based on their valuable com-
ments. Thanks are extended to Shane Bergsma, who
provided us the gender and number data distribution.
In addition, Professor Jeanette Gundel and our lab-
mate Stephen Wu also gave us support in paper edit-
ing and in theoretical discussion.
1177
References
S Bergsma. 2005. Automatic acquisition of gender
information for anaphora resolution. page 342353.
Springer.
Eugene Charniak and Micha Elsner. 2009. Em works
for pronoun anaphora resolution. In Proceedings of
the Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL-09),
Athens, Greece.
Noam Chomsky. 1981. Lectures on government and
binding. Foris, Dordercht.
H.H. Clark and CJ Sengul. 1979. In search of refer-
ents for nouns and pronouns. Memory & Cognition,
7(1):35?41.
P. Denis and J. Baldridge. 2007. A ranking approach to
pronoun resolution. In Proc. IJCAI.
Kevin Duh. 2005. Jointly labeling multiple sequences:
a factorial HMM approach. In ACL ?05: Proceedings
of the ACL Student Research Workshop, pages 19?24,
Ann Arbor, Michigan.
Zoubin Ghahramani and Michael I. Jordan. 1997. Facto-
rial hidden markov models. Machine Learning, 29:1?
31.
A. Haghighi and D. Klein. 2007. Unsupervised coref-
erence resolution in a nonparametric bayesian model.
In Proceedings of the 45th annual meeting on Associ-
ation for Computational Linguistics, page 848.
A. Haghighi and D. Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 3-
Volume 3, pages 1152?1161. Association for Compu-
tational Linguistics.
A. Haghighi and D. Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385?393. Associa-
tion for Computational Linguistics.
L. Hasler, C. Orasan, and K. Naumann. 2006. NPs
for events: Experiments in coreference annotation. In
Proceedings of the 5th edition of the International
Conference on Language Resources and Evaluation
(LREC2006), pages 1167?1172. Citeseer.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan.
X Luo. 2005. On coreference resolution performance
metrics. pages 25?32. Association for Computational
Linguistics Morristown, NJ, USA.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In IJCAI Workshop on In-
formation Integration on the Web. Citeseer.
David McClosky, Eugene Charniak, and Mark Johnson.
2008. BLLIP North American News Text, Complete.
Linguistic Data Consortium. LDC2008T13.
T.S. Morton. 2000. Coreference for NLP applications.
In Proceedings of the 38th Annual Meeting on Associ-
ation for Computational Linguistics, pages 173?180.
Association for Computational Linguistics.
V. Ng. 2008. Unsupervised models for coreference reso-
lution. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 640?
649. Association for Computational Linguistics.
US NIST. 2003. The ACE 2003 Evaluation Plan. US Na-
tional Institute for Standards and Technology (NIST),
Gaithersburg, MD.[online, pages 2003?08.
L. Qiu, M.Y. Kan, and T.S. Chua. 2004. A public ref-
erence implementation of the rap anaphora resolution
algorithm. Arxiv preprint cs/0406031.
X. Yang, J. Su, G. Zhou, and C.L. Tan. 2004. Im-
proving pronoun resolution by incorporating corefer-
ential information of candidates. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 127. Association for Com-
putational Linguistics.
1178
Proceedings of the TextGraphs-6 Workshop, pages 1?9,
Portland, Oregon, USA, 19-24 June 2011. c?2011 Association for Computational Linguistics
A Combination of Topic Models with Max-margin Learning for Relation
Detection
Dingcheng Li
University of Minnesota
Twin Cities, MN 55455
lixxx345@umn.edu
Swapna Somasundaran
Siemens Corporate Research
Princeton, NJ 08540
swapna.somasundaran@siemens.com
Amit Chakraborty
Siemens Corporate Research
Princeton, NJ 08540
amit.chakraborty@siemens.com
Abstract
This paper proposes a novel application of
a supervised topic model to do entity rela-
tion detection (ERD). We adapt Maximum En-
tropy Discriminant Latent Dirichlet Alloca-
tion (MEDLDA) with mixed membership for
relation detection. The ERD task is refor-
mulated to fit into the topic modeling frame-
work. Our approach combines the benefits of
both, maximum-likelihood estimation (MLE)
and max-margin estimation (MME), and the
mixed membership formulation enables the
system to incorporate heterogeneous features.
We incorporate different features into the sys-
tem and perform experiments on the ACE
2005 corpus. Our approach achieves better
overall performance for precision, recall and
Fmeasure metrics as compared to SVM-based
and LLDA-based models.
1 Introduction
Entity relation detection (ERD) aims at finding rela-
tions between pairs of Named Entities (NEs) in text.
Availability of annotated corpora (NIST, 2003; Dod-
dington et al, 2004) and introduction of shared tasks
(e.g. (Farkas et al, 2010; Carreras and Ma`rquez,
2005)) has spurred a large amount of research in this
field in recent times. Researchers have used super-
vised and semi-supervised approaches (Hasegawa et
al., 2004; Mintz et al, 2009; Jiang, 2009), and ex-
plored rich features (Kambhatla, 2004), kernel de-
sign (Culotta and Sorensen, 2004; Zhou et al, 2005;
Bunescu and Mooney, 2005; Qian et al, 2008) and
inference algorithms (Chan and Roth, 2011), to de-
tect predefined relations between NEs.
In this work, we explore if and how the latent se-
mantics of the text can help in detecting entity rela-
tions. For this, we adapt the Latent Dirichlet Alloca-
tion (LDA) approach to solve the ERD task. Specif-
ically, we present a ERD system based on Maxi-
mum Entropy Discriminant Latent Dirichlet Alloca-
tion (MEDLDA). MEDLDA (Zhu et al, 2009), is
an extension of Latent Dirichlet Allocation (LDA)
that combines capability of capturing latent seman-
tics with the discriminative capabilities of SVM.
There are a number of challenges in employing
the LDA framework for ERD. Latent Dirichlet Allo-
cation and its supervised extensions such as Labeled
LDA (LLDA) (Ramage et al, 2009) and supervised
LDA (sLDA) (Blei and McAuliffe, 2008) are pow-
erful generative models that capture the underlying
semantics of texts. However, they have trouble dis-
covering marginal classes and easily employing rich
feature sets, both of which are important for ERD.
We overcome the first drawback by employing a
MEDLDA framework, which integrates maximum
likelihood estimation (MLE) and maximum margin
estimation (MME). Specifically, it is a combination
of sLDA and support vector machines (SVMs). Fur-
ther, in order to employ rich and heterogeneous fea-
tures we introduce a separate exponential family dis-
tribution for each feature, similar to (Shan et al,
2009), into our MEDLDA model.
We formulate the relation detection task within
the topic model framework as follows. Pairs of NE
mentions1 and the text between them is considered
1Adopting the terminology used in the Automatic Context
Extraction (ACE) program (NIST, 2003), specific NE instances
are called mentions.
1
as mini-document. Each mini-document has a re-
lation type (analogous to the response variable in
the supervised topic model). The topic model in-
fers the topic (relation type) distribution of the mini-
documents. The supervised topic model discovers
a latent topic representation of the mini-documents
and a response parameter distribution. The topic
representation is discovered with observed response
variables during training. During testing, the topic
distribution of each mini-document can form a pre-
diction of the relation types.
We carry out experiments to measure the effec-
tiveness of our approach and compare it to SVM-
based and LLDA-based models, as well as to a pre-
vious work using the same corpora. We also mea-
sure and analyze the effectiveness of incorporating
different features in our model relative to other mod-
els. Our approach exhibits better overall precision,
recall and Fmeasure than baseline systems. We also
find that the MEDLDA-based approach shows con-
sistent capability for incorporation and improvement
due to a variety of heterogeneous features.
The rest of the paper is organized as follows. We
describe the proposed model in Section 2 and the
features that we explore in this work in Section 3.
Section 4 describes the data, experiments, results
and analyses. We discuss the related work in Sec-
tion 5 before concluding in Section 6.
2 MEDLDA for Relation Detection
MEDLDA is an extension of LDA proposed by Zhu,
Ahmed and Xing (2009). LDA is itself unsuper-
vised and the results are often hard to interpret.
However, with the addition of supervised informa-
tion (such as response variables), the resulting topic
models have much better predictive power for classi-
fication and regression. In our work, we use relation
annotations from the ACE (ACE, 2000 2005) corpus
to provide the supervision. NE pairs within a sen-
tence, and the text between them are considered as
a mini-document. Each mini-document is assumed
to be composed of a set of topics. The topic model
trained with these mini-documents given their rela-
tion type label can generate topics biased toward re-
lation types. Thus, the trained topic model will have
good predictive power on relation types.
We first describe the MEDLDA model from (Zhu
et al, 2009) and then describe how we adapt it for
relation detection using mixed membership exten-
sions.
2.1 MEDLDA
Figure 1: MEDLDA
The MEDLDA model described in (Zhu et al,
2009) is illustrated in Figure 12.
Here, ? is a k-dimensional parameter of a Dirich-
let distribution, ?1:k are the parameters for k compo-
nent distribution over the words. Each component
refers to a topic. In a collection of documents D,
each document w1:N is generated from a sequence
of topics z1:N . ? is a k-dimensional topic distribu-
tion variable, which is sampled from a Dirichlet dis-
tribution Dir(?). Like common LDAs, MEDLDA
uses independence assumption for a finite set of ran-
dom variables z1, ..., zn which are independent and
identically distributed, conditioned on the parame-
ter ?. Like sLDA, MEDLDA is a supervised model.
A response variable Y connected to each document
is added for incorporating supervised side informa-
tion. The supervised side information is expected
to make MEDLDA topic discoveries more inter-
pretable. Zhu, Ahmed and Xing?s (2009) MEDLDA
model can be used in both regression and classifi-
cation. Concretely, Y is drawn from ?1:c, a c k-
dimensional vector which can be derived from suit-
able statistical model. In our work, c is the num-
ber of relation types. Note that the plate diagram
for MEDLDA is quite similar to sLDA (Blei and
McAuliffe, 2008). But there is a difference ? sLDA
focuses on building regression models, and thus the
response variable Y in sLDA is generated by a nor-
mal distribution.
Based on the plate diagram, the joint distribution
of latent and observable variables for our MEDLDA-
2(Zhu et al, 2009) do not have this plate digram in their
paper; rather, we create this illustration from the description of
their model.
2
based relation detection is given by
p(?, z,w,y|?, ?1:k, ?1:c)
=
D?
d=1
p(?d|?)?
(
N?
n=1
p(zdn|?d)p(wdn|zdn, ?1:k)
)
? p(yd|zd1:dN , ?1:c) (1)
Another important difference from sLDA lies in
the fact that MEDLDA does joint learning with both
MME and MLE. The joint learning is done in two
stages, unsupervised topic discovery and multi-class
classification (we refer the reader to (Zhu et al,
2009) for details). During training, EM algorithms
are utilized to infer the posterior distribution of the
hidden variables ?, z and ?. In testing, the trained
models are used to predict relation types y.
2.2 Mixed Membership MEDLDA
Although the MEDLDA model described above can
be applied to the relation detection and classification
task, a few modifications are necessary before it can
be effective in predicting relation types. Mainly, a
Figure 2: Mixed Membership MEDLDA
limitation of LDA or other existing topic models is
the difficulty in incorporating rich features. This is
because LDA is designed to handle data points with
homogeneous features such as words. But for rela-
tion detection, like many other NLP tasks, it is im-
portant to have the flexibility of incorporating part-
of-speech tags, named entities, grammatical depen-
dencies and other linguistic features. We overcome
this limitation by introducing a separate exponential
family distribution for each feature similar to (Shan
et al, 2009). Thus, our MEDLDA-based relation
detection model is really a mixed-member Bayesian
network. Figure 2 illustrates our model with this ex-
tension.
Figure 2 is very similar to Figure 1; the only dif-
ference is that the topic component number k is now
kN . The generative process for each document this
model is as follows:
1. Sample a component proportion ?d ?
Dirichlet(?),
2. For each feature like word, part-of-speech,
named entity in the document,
(a) For n ? {1, ..., N}, sample zdn = i ?
Discrete(?d)
(b) For n ? {1,...,N}, sample wdn ?
P (wdn|?dni)
3. Sample the relation type label
from a softmax(z?,?) where yd ?
softmax(
exp(?Th z?)?c?1
h=1 exp(?
T
h z?)
)
In the sampling, index i is the number of the topic
component which ranges from 1 : k. P (wdn|?dni) in
2(b) is an exponential family distribution where i is
from 1...k. Note that now we have ?dni rather than
only ?di since we have drawn separate distributions
for each word (or feature) n.
Now, our MEDLDA-based relation-detection
model can integrate diverse features of different
types or the same features with different parameters.
Following the generative process, parameter es-
timation and inferences can be made with either
Gibbs sampling or variational methods. We use vari-
ational methods since we adapt MEDLDA package3
to mixed-membership MEDLDA and train relation
detection models.
2.3 Relation Detection
With the generative process, inference and parame-
ter estimation in place, we are ready to perform rela-
tion detection. The first step is to perform variational
inference given the testing instances.
In classification, we estimate the probability of
the relation type given topics and the response pa-
rameters, i.e. p(yd|zd1:dN , ?1:c?1). With variational
approximation, we can derive the prediction rule as
F (y, z1:N , ?) = ?T f(y, z?) where f(y, z?) is a fea-
ture vector. Now, SVM can be used to derive the
3this package is downloaded from
http://www.cs.cmu.edu/j?unzhu/medlda.htm
3
prediction rule. The final prediction can be general-
ized exactly the same as Zhu, Ahmed and Xing (Zhu
et al, 2009):
y? = argmaxyE[?
T f(y, Z?)|?, ?] (2)
3 Features
We explore the effectiveness of incorporating fea-
tures into our systems as well as the baselines. For
this, we construct feature sets similar to Jiang and
Zhai (2007) and Zhou (2005). Three kinds of fea-
tures are employed:
1. BOW The Bag of Words (BOW) feature cap-
tures all the words in our mini-document. It
comprises of the words of the two NE mentions
and the words between them.
2. SYN The SYN features are constructed to cap-
ture syntactic, semantic and structural infor-
mation of the mini-document. They include
features such as HM1 (the head word of the
first mention), HM2 (the head word of the sec-
ond mention), ET1, ET2, M1 and M2 (Entity
types and mention types of the two mentions
involved), #MB (number of other mentions in
between the two mentions), #WB (number of
words in between the two mentions).
3. COMP The COMP features are composite fea-
tures that are similar to SYN, but they addition-
ally capture language order and dependencies
between the features mentioned above. These
include features such as HM1HM2 (combining
head word of mention 1 and head word of men-
tion 2) , ET12 (combinations of mention entity
type), ML12 (combination of mention levels),
M1InM2 or M2InM1 (flag indicating whether
M2/M1 is included in M1/M2).
The main intuitions behind employing composite
features, COMP, are as follows. First, they capture
the ordering information. The ordering of words are
not captured by BOW. That is, BOW features as-
sume exchangeability. This works for models based
on random or seeded sampling (e.g. LDA) ? as long
as words sampled are associated with a topic, the
hidden topics of the documents can be discovered.
In the case of ERD, this assumption might work
with symmetric relations. However, when the rela-
tions are asymmetric, ordering information is impor-
tant. Composite features such as HM1HM2 encodes
what mention head word precedes the other. Second,
features such as M1InM2 or M2InM1 capture token
dependencies. Besides exchangeability, LDA-based
models also assume that words are conditionally in-
dependent. Consequently, the system cannot capture
the knowledge that some mentions may be included
in other mentions. By constructing features such as
M1InM2 or M2InM1, we encode the dependency in-
formation explicitly.
4 Experiments
As MEDLDA is a combination of maximum mar-
gin principle with maximum likelihood estimation
for topic modes, we compare it with two baseline
systems. The first, SVM, uses only the maximum
margin principle, while the second, LLDA, uses only
maximum likelihood estimation for topic modeling.
4.1 Data
We use the ACE corpus (Phase 2, 2005) for eval-
uation. The ACE corpus has annotations for both
entities and relations. The corpus has six major re-
lations types, 23 subtypes and 7 entity types. In this
work, we focus only on the six high-level relation
types listed in Table 1. In addition to the the 6 ma-
jor types, we have an additional category, no relation
(NO-REL), that exists between entities that are not
related.
The data for our experiments consists of pairs of
NEs from a sentence, and the gold standard annota-
tion of their relation type (or NO-REL). All relations
in the ACE corpus are intra-sentential and hence we
do not create NE pairs that cross sentence bound-
aries. Also, almost all positive instances are within
two mentions of each other. Hence, we create NE
pairs for only those NEs that have at most 2 interven-
ing NEs in between. This gives us a total of 38,342
relation instances of which 32,640 are negative in-
stances (NO-REL) and 5702 are positive relation in-
stances belonging to one of the 6 categories.
4.2 Experimental Setup
We use 80% of the instances for training and 20%
for testing. The topic numbers and the penalty pa-
rameter of the cost function C are first determined
4
Major Type Definition Example
ART artifact
User, owner, inventor or the makers of the Kursk
manufacturer
GEN-AFF
citizen, resident, religion, U.S. Companies
ethnicity and organization-location
ORG-AFF
employment, founder, ownership, The CEO of Siemens
(Org-affiliation) sports-affiliation, investor-shareholder
student-alumni and membership
PART-WHOLE geographical, subsidiary and so on a branch of U.S bank
PER-SOC
business, family and a spokesman for the senator
(person-social) lasting personal relationship
PHYS (physical) located or near a military base in Germany
Table 1: Relation types for ACE 05 corpus
for each of the models (wherever applicable) using
the training data. Best parameters are determined
for the three conditions: 1) BOW features alone
BOW, 2) BOW plus SYN features (PlusSYN) and 3)
BOW plus SYN and COMP features (PlusCOMP).
All systems achieved their overall best performance
with PlusCOMP features (see Section 4.4 for a de-
tailed analysis).
4.2.1 MEDLDA
The number of topics are determined using the
equation 2K0 + K1 following Zhu, Ahmed and
Xing (2009) and K1 = 2K0. K0 is the number
of topics per class and K1 is the number of topics
shared by all relation types. The choice of topics is
based on the intuition that the shared component K1
should use all class labels to model common latent
structure while non-overlapping components should
model specific characteristics data from each class.
The ratio of topics is based on the understanding that
shared topics may be more than topics of each class.
The specific numbers do not produce much variation
in the final results. We experimented with the fol-
lowing number of topics: 20, 40, 70, 80, 90, 100,
110. BOW, PlusSYN, and PlusCOMP configura-
tions obtain the best performance for 90 topics, 80
topics, and 70 topics respectively.
Since SVMs are employed in the MEDLDA im-
plementation, we need to determine the penalty pa-
rameter of the cost function, C. We used 5 fold cross-
validation to locate the parameter C. The best values
for C are 25, 28, 30 respectively for BOW, PlusSYN
and PlusCOMP configurations. We used a linear
kernel as it is the most commonly used kernel for
text classification tasks. Since MEDLDA is run by
sampling, the result may be different each time. We
ran it 5 times for each setting and took the average
as the final results.
4.2.2 LLDA and SVM
The setting of topics for LLDA is similar to
MEDLDA. As LLDA is also run by sampling, we
ran it 5 times for each setting and took the average
as the final results. In SVMlight, a grid search tool
is provided to locate the the best value for parame-
ter C. The best C for all three conditions was found
to be 1. All other settings for the two models are
similar to those of MEDLDA.
4.3 Results
Prec% Rec% F%
SVM 53.2 35.2 40.3
LLDA 28.3 51.6 36.6
MEDLDA 57.8 53.2 55.4
Table 2: Overall performance of the 3 systems
We present the results of the three systems built
using PlusCOMP, as all systems achieved their best
overall performance using these features. Table 2 re-
ports the precision, recall and Fmeasure of the three
systems averaged across all 7 categories (the best
numbers for each metric are highlighted in bold).
Here we see that MEDLDA outperforms LLDA and
5
Labels
SVM LLDA MEDLDA
Pre% Rec% F% Pre% Rec% F% Pre% Rec% F%
ART 30 8 14 1.5 33 3 49 36 41
GEN-AFF 53 48 50 3 32 6 40 39 40
ORG-AFF 55 35 43 59 58 59 53 59 56
PART-WHOLE 39 08 14 31 82 45 44 52 48
PER-SOC 50 17 25 7 92 13 73 76 75
PHYS 55 35 43 26 47 33 56 19 29
NO-REL 90 95 93 70 17 27 89 91 90
Table 3: Multi-class Classification Results with PlusCOMP for SVM, LLDA and MEDLDA for the six ACE 05
categories and NO-REL
SVM across all metrics. Specifically, there is a 15
percentage point improvement in Fmeasure over the
best performing baseline. This result indicates that
our approach of combining topic model with max-
margin learning is effective for relation detection.
Now, looking at the results for each individual
relationship category (see Table 3; the best num-
bers for each category and metric are highlighted
in bold) we see that the Fmeasure for MEDLDA is
better than that for SVM for 4 out of the 6 ACE re-
lation types; and better than the Fmeasure obtained
by LLDA for all relation types except ORG-AFF.
Specifically, comparing with the best performing
baseline, MEDLDA produces a Fmeasure improve-
ment 27 percentage points for ART, 3 percentage
points for PART-WHOLE and 50 percentage points
for PER-SOC. Also, for four of the six ACE rela-
tion types, MEDLDA achieves the best precision.
Even in the cases where MEDLDA is not the best
performer for a relation category, its performance is
not very poor (unlike, for example, SVM for PART-
WHOLE and LLDA for ART, respectively).
Interestingly, the NO-REL category reveals a
sharp contrast in the performance of SVM and
LLDA. NO-REL is a difficult, catch-all category
that is a mixture of data with diverse distributions.
This is a category where maximum-margin learning
is more effective than maximum-likelihood estima-
tion. Notice that MEDLDA achieves performance
close to SVM for this category. This is because,
even though both LLDA and MEDLDA model hid-
den topics and then employ discovered hidden topics
to predict relation types, MEDLDA does joint infer-
ence of MLE and MME. This joint inference helps
to improve the detection of NO-REL.
Finally, we also compare our system?s results (us-
ing PlusCOMP features) with the results of previ-
ous research on the same corpus (Khayyamian et al,
2009). They use similar experimental settings: ev-
ery pair of entities within a sentence is regarded to
involve a negative relation instance unless it is anno-
tated as positive in the corpus. A similar filter (they
use a distance filter) is used to sift out unrelated neg-
ative instances. Their train/test ratio of data split is
also the same as ours.
Khayyamian, Mirroshandel and Abolhas-
sani (2009) employ state-of-art kernel methods
developed by Collins and Duffy (2002) and only
report Fmeasures over the six ACE relation types.
For clarity, we reproduce their results in Table 4
and repeat MEDLDA Fmeasures from Table 3 in
the last column. The last row (Overall) reports the
macro-averages computed over all relation types for
each system. Here we see that overall, MEDLDA
outperforms all kernels. MEDLDA also performs
better than the best kernel for four of the six relation
types.
4.4 Analysis
As mentioned previously, all three systems achieved
their overall best performance with PlusCOMP fea-
tures. Here, we analyze if informative features are
consistently useful and if the systems can harness
the informative features consistently across all re-
lation types. Figures 3, 4 and 5 illustrate the F-
measures for SVM, LLDA and MEDLDA respec-
tively for the three conditions: BOW, PlusSYN and
PlusCOMP.
6
Labels CD?01 AAP AAPD TSAAPD-0 TSAAPD-01 MEDLDA
ART% 51 49 50 48 47 41
GEN-AFF % 9 10 12 11 11 40
ORG-AFF % 43 43 43 43 45 56
PART-WHOLE % 30 28 29 30 28 48
PER-SOC % 62 58 70 63 73 75
PHYS % 32 36 29 33 33 29
Overall (Avg) 38 37 39 38 40 48
Table 4: F-measures for every kernel in (Khayyamian et al, 2009) and MEDLDA
Figure 3: SVM Fmeausres for 3 feature conditions
Figure 4: LLDA Fmeausres for 3 feature conditions
Figure 5: MEDLDA Fmeausres for 3 feature conditions
Let us first look at the best systems (based on
Fmeasure) for each of the six ACE relation types
in Table 3, and look at what feature set pro-
duces the best result for that system and relation.
MEDLDA is the best performer for ART, PART-
WHOLE and PER-SOC in Table 3. Figure 5 re-
veals that MEDLDA?s best performance for these re-
lation types are obtained using PlusCOMP features.
Similarly SVM obtains the best Fmeasure for GEN-
AFF and PHYS relations and Figure 3 shows that
SVM achieves its best performance for these cate-
gories using PlusCOMP. We also see a similar trend
with LLDA and the ORG-AFF relation type. These
results corroborate intuition from previous research
that informative features are important for relation
type recognition. The only exception to this is the
performance of SVM for NO-REL. This is not sur-
prising, as the features we use are focused on deter-
mining true relation types and NO-REL is a mixture
of all cases (and features) where relations do not ex-
ist.
Further analysis of the figures reveal that even
though there is a general trend towards better per-
formance with addition of more informative fea-
tures, not all systems show consistent improvements
across all relation types with the addition of com-
posite features. That is, some systems get degraded
performance due to feature addition. For example,
in Figure 3, we see that the SVM with PlusCOMP
features is outperformed by SVM with PlusSYN for
ART and SVM with BOW for NO-REL. The gains
from features are also inconsistent in the case of
LLDA (Figure 4). While the LLDA system with
PlusSYN features always improves over the one us-
ing BOW, the performance drops considerably when
using PlusCOMP features for ART and GEN-AFF.
On the other hand, MEDLDA (see Figure 5) shows
more consistent improvement for all relation types
with the addition of more complex features. Also,
7
the gains are more substantial. This is encouraging
and opens up avenues for further exploration.
5 Related Work
Previous research has explored various methods and
features for relationship detection and mining. Ker-
nel methods have been popularly used for rela-
tion detection. Some examples are are dependency
tree kernels (Culotta and Sorensen, 2004), short-
est dependency path kernels (Bunescu and Mooney,
2005), and more recently, convolution tree kernels
(Zhao and Grishman, 2005; Zhang et al, 2006)
context-sensitive convolution tree kernels (Zhou et
al., 2007) and dynamic syntax tree kernels (Qian et
al., 2008). Kernel methods for relation extraction
focus on representing and capturing the structured
information of the text between the entities. In our
MEDLDA model, instead of computing distances
between subtrees, we sample topics based on their
distributions. The sampling is not only on the (mini)
document level, but also on the word level or on the
syntactic or semantic level. Our model focuses on
addressing the underlying semantics more directly
than typical kernel-based methods.
Chan and Roth (2011) employ constraints us-
ing an integer linear programming (ILP) framework.
Using this, they apply rich linguistic and knowledge-
based constraints based on coreference annotations,
a hierarchy of relations, syntacto-semantic structure,
and knowledge from Wikipedia. In our work, we
focus on capturing the latent semantics of the text
between the NEs.
A variety of features have been explored for ERD
in previous research (Zhou et al, 2005; Zhou et al,
2008; Jiang and Zhai, 2007; Miller et al, 2000).
Syntactic features such as POS tags and dependency
path between entities; semantic features such as
Word-Net relations, semantic parse trees and types
of NEs; and structural features such as which entity
came first in the sentence have been found useful for
ERD. We too observe the utility of informative fea-
tures for this task. However, exploration of the fea-
ture space is not the main focus of this work. Rather,
our focus is on whether the models are capable of
incorporating rich features. A fuller exploration of
rich heterogeneous features is the focus of our fu-
ture work.
A closely related task is that of relation min-
ing and discovery, where unsupervised, semi-
supervised approaches have been effectively em-
ployed (Hasegawa et al, 2004; Mintz et al, 2009;
Jiang, 2009). For example, Hasegawa et al (2004)
use clustering and entity type information, while
Mintz et al (2009) employ distant supervision. Our
ERD task is different from these as we focus on
classifying the relation types into predefined relation
types in the ACE05 corpus.
Topic models have been applied previously for a
number of NLP tasks (e.g. (Lin et al, 2006; Titov
and McDonald, 2008). LDAs have also been em-
ployed to reduce feature dimensions in relation de-
tection systems (Hachey, 2006). However, to the
best of our knowledge, this is the first work to make
use of topic models to perform relation detection.
6 Conclusion and Future Work
In this work, we presented a system for en-
tity relation detection based on mixed-membership
MEDLDA. Our approach was motivated by the idea
that combination of max margin and maximum like-
lihood can help to improve relation detection task.
For this, we adapted the existing work on MEDLDA
and mixed membership models and formulated ERD
as a topic detection task. To the best of our knowl-
edge, this is the first work to make full use of topic
models for relation detection.
Our experiments show that the proposed approach
achieves better overall performance than SVM-
based and LLDA-based approaches across all met-
rics. We also experimented with different features
and the effectiveness of the different models for har-
nessing these features. Our analysis show that our
MEDLDA-based approach is able to effectively and
consistently incorporate informative features.
As a model that incorporates maximum-
likelihood, maximum-margin and mixed mem-
bership learning, MEDLDA has the potential of
incorporating rich kernel functions or conditional
topic random fields (CTRF) (Zhu and Xing, 2010).
These are some of the promising directions for our
future exploration.
8
References
ACE. 2000-2005. Automatic Content Extraction.
http://www.ldc.upenn.edu/Projects/ACE/.
D.M. Blei and J. McAuliffe. 2008. Supervised topic
models. Advances in Neural Information Processing
Systems, 20:121?128.
R.C. Bunescu and R.J. Mooney. 2005. A shortest path
dependency kernel for relation extraction. In HLT &
EMNLP.
X. Carreras and L. Ma`rquez. 2005. Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In
CONLL, pages 152?164. ACL.
Y. Chan and D. Roth. 2011. Exploiting syntactico-
semantic structures for relation extraction. In ACL.
M. Collins and N. Duffy. 2002. Convolution kernels for
natural language. Advances in neural information pro-
cessing systems, 1:625?632.
A. Culotta and J. Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, page 423. ACL.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The auto-
matic content extraction (ACE) program?tasks, data,
and evaluation. In Proceedings of LREC, volume 4,
pages 837?840.
R. Farkas, V. Vincze, G. Mo?ra, J. Csirik, and G. Szarvas.
2010. The CoNLL-2010 Shared Task: Learning to De-
tect Hedges and their Scope in Natural Language Text.
In CoNLL-2010, pages 1?12.
B. Hachey. 2006. Comparison of similarity models for
the relation discovery task. In COLING & ACL 2006,
page 25.
T Hasegawa, S Sekine, and Ralph Grishman. 2004. Dis-
covering relations among named entities from large
corpora. In 42nd ACL.
J. Jiang and C.X. Zhai. 2007. A systematic explo-
ration of the feature space for relation extraction. In
NAACL/HLT, pages 113?120.
J. Jiang. 2009. Multi-task transfer learning for weakly-
supervised relation extraction. In 47th ACL & 4th
AFNLP, pages 1012?1020. ACL.
N. Kambhatla. 2004. Combining lexical, syntactic, and
semantic features with maximum entropy models for
extracting relations. In ACL 2004 Interactive poster
and demonstration sessions.
M. Khayyamian, S.A. Mirroshandel, and H. Abolhassani.
2009. Syntactic tree-based relation extraction using a
generalization of Collins and Duffy convolution tree
kernel. In HLT/NAACL,: Student Research Workshop.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you on?
Identifying perspectives at the document and sentence
levels. In CoNLL-2006.
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel. 2000.
A novel use of statistical parsing to extract information
from text. In NAACL.
M Mintz, S Bills, R Snow, and D Jurafsky. 2009. Dis-
tant supervision for relation extraction without labeled
data. In 47th ACL & 4th AFNLP.
US NIST. 2003. The ACE 2003 Evaluation Plan. US Na-
tional Institute for Standards and Technology (NIST),
pages 2003?08.
L. Qian, G. Zhou, F. Kong, Q. Zhu, and P. Qian. 2008.
Exploiting constituent dependencies for tree kernel-
based semantic relation extraction. In 22nd ACL.
D. Ramage, D. Hall, R. Nallapati, and C.D. Manning.
2009. Labeled LDA: A supervised topic model for
credit attribution in multi-labeled corpora. In EMNLP.
H. Shan, A. Banerjee, and N.C. Oza. 2009. Discrim-
inative Mixed-membership Models. In ICDM, pages
466?475. IEEE.
Ivan Titov and Ryan McDonald. 2008. A joint model of
text and aspect ratings for sentiment summarization.
In ACL-08: HLT.
M. Zhang, J. Zhang, J. Su, and G. Zhou. 2006. A com-
posite kernel to extract relations between entities with
both flat and structured features. In 21st ICCL & 44th
ACL.
S. Zhao and R. Grishman. 2005. Extracting relations
with integrated information using kernel methods. In
43rd ACL.
G Zhou, S. Jian, Z. Jie, and Z. Min. 2005. Exploring
various knowledge in relation extraction. In In 43rd
ACL.
G Zhou, M. Zhang, D.H. Ji, and Q Zhu. 2007. Tree
kernel-based relation extraction with context-sensitive
structured parse tree information. In EMNLP/CoNLL-
2007, pages 728?736.
G.D. Zhou, M. Zhang, D.H. Ji, and Q.M. Zhu. 2008.
Hierarchical learning strategy in semantic relation ex-
traction. Information Processing & Management,
44(3):1008?1021.
J. Zhu and E.P. Xing. 2010. Conditional Topic Random
Fields. In ICML. ACM.
J. Zhu, A. Ahmed, and E.P. Xing. 2009. MedLDA: max-
imum margin supervised topic models for regression
and classification. In ICML, pages 1257?1264. ACM.
9
