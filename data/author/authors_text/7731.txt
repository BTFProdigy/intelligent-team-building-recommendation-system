  
InfoXtract: A Customizable Intermediate Level Information 
Extraction Engine?  
 
Rohini K. Srihari 
Cymfony, Inc. 
State University of New York at Buffalo 
rohini@Cymfony.com
Wei Li, Cheng Niu and Thomas Cornell 
Cymfony Inc. 
600 Essjay Road, Williamsville, NY 14221, USA 
{wei, cniu, cornell}@Cymfony.com
 
Keywords: Information Extraction, Named Entity Tagging, Machine Learning, Domain Porting 
 
                                                     
? This work was supported in part by SBIR grants F30602-01-C-0035, F30602-03-C-0156, and 
F30602-02-C-0057 from the Air Force Research Laboratory (AFRL)/IFEA. 
 
Abstract 
Information extraction (IE) systems assist 
analysts to assimilate information from 
electronic documents. This paper focuses on 
IE tasks designed to support information 
discovery applications. Since information 
discovery implies examining large volumes 
of documents drawn from various sources for 
situations that cannot be anticipated a priori, 
they require IE systems to have breadth as 
well as depth. This implies the need for a 
domain-independent IE system that can 
easily be customized for specific domains: 
end users must be given tools to customize 
the system on their own. It also implies the 
need for defining new intermediate level IE 
tasks that are richer than the 
subject-verb-object (SVO) triples produced 
by shallow systems, yet not as complex as the 
domain-specific scenarios defined by the 
Message Understanding Conference (MUC). 
This paper describes a robust, scalable IE 
engine designed for such purposes. It 
describes new IE tasks such as entity profiles, 
and concept-based general events which 
represent realistic goals in terms of what can 
be accomplished in the near-term as well as 
providing useful, actionable information. 
These new tasks also facilitate the correlation 
of output from an IE engine with existing 
structured data. Benchmarking results for the 
core engine and applications utilizing the 
engine are presented. 
1 Introduction 
This paper focuses on new intermediate level 
information extraction tasks that are defined and 
implemented in an IE engine, named InfoXtract. 
InfoXtract is a domain independent, but portable 
information extraction engine that has been designed 
for information discovery applications. 
The last decade has seen great advances in the area 
of IE. In the US, MUC [Chinchor & Marsh 1998] has 
been the driving force for developing this technology.  
The most successful IE task thus far has been 
Named Entity (NE) tagging. The state-of-the-art 
exemplified by systems such as NetOwl [Krupka & 
Hausman 1998], IdentiFinder [Miller et al1998] and 
InfoXtract [Srihari et al2000] has reached near human 
performance, with 90% or above F-measure. On the 
other hand, the deep level MUC IE task Scenario 
Template (ST) is designed to extract detailed 
information for predefined event scenarios of interest. 
It involves filling the slots of complicated templates. It 
is generally felt that this task is too ambitious for 
commercial application at present.  
Information Discovery (ID) is a term which has 
traditionally been used to describe efforts in data 
mining [Han 1999]. The goal is to extract novel 
patterns of transactions which may reveal interesting 
trends. The key assumption is that the data is already 
in a structured form. ID in this paper is defined within 
the context of unstructured text documents; it is the 
ability to extract, normalize/disambiguate, merge and 
link entities, relationships, and events which provides 
significant support for ID applications. Furthermore, 
there is a need to accumulate information across 
documents about entities and events. Due to rapidly 
changing events in the real world, what is of no 
interest one day, may be especially interesting the 
following day. Thus, information discovery 
applications demand breadth and depth in IE 
technology.  
A variety of IE engines, reflecting various goals in 
terms of extraction as well as architectures are now 
available. Among these, the most widely used are the 
GATE system from the University of Sheffield 
[Cunningham et al2003], the IE components from 
Clearforest (www.clearforest.com), SIFT from BBN 
[Miller et al1998], REES from SRA [Aone & 
Ramon-Santacruz 1998] and various tools provided 
by Inxight (www.inxight.com). Of these, the GATE 
system most closely resembles InfoXtract in terms of 
its goals as well as the architecture and customization 
tools. Cymfony differentiates itself by using a hybrid 
  
model that efficiently combines statistical and 
grammar-based approaches, as well as by using an 
internal data structure known as a token-list that can 
represent hierarchical linguistic structures and IE 
results for multiple modules to work on.  
The research presented here focuses on a new 
intermediate level of information extraction which 
supports information discovery. Specifically, it 
defines new IE tasks such as Entity Profile (EP) 
extraction, which is designed to accumulate 
interesting information about an entity across 
documents as well as within a discourse. Furthermore, 
Concept-based General Event (CGE) is defined as a 
domain-independent, representation of event 
information but more feasible than MUC ST.  
InfoXtract represents a hybrid model for extracting 
both shallow and intermediate level IE: it exploits 
both statistical and grammar-based paradigms. A key 
feature is the ability to rapidly customize the IE engine 
for a specific domain and application. Information 
discovery applications are required to process an 
enormous volume of documents, and hence any IE 
engine must be able to scale up in terms of processing 
speed and robustness; the design and architecture of 
InfoXtract reflect this need.  
In the remaining text, Section 2 defines the new 
intermediate level IE tasks. Section 3 presents 
extensions to InfoXtract to support cross-document 
IE. Section 4 presents the hybrid technology. Section 
5 delves into the engineering architecture and 
implementation of InfoXtract. Section 6 discusses 
domain porting. Section 7 presents two applications 
which have exploited InfoXtract, and finally, Section 
8 summarizes the research contributions. 
2 InfoXtract: Defining New IE Tasks 
InfoXtract [Li & Srihari 2003, Srihari et al2000] is a 
domain-independent and domain-portable, inter- 
mediate level IE engine. Figure 1 illustrates the 
overall architecture of the engine. 
A description of the increasingly sophisticated IE 
outputs from the InfoXtract engine is given below: 
 
? NE:  Named Entity objects represent key items 
such as proper names of person, organization, 
product, location, target, contact information 
such as address, email, phone number, URL, time 
and numerical expressions such as date, year and 
various measurements weight, money, 
percentage, etc.  
? CE:  Correlated Entity objects capture relation- 
ship mentions between entities such as the 
affiliation relationship between a person and his 
employer. The results will be consolidated into 
the information object Entity Profile (EP) based 
on co-reference and alias support. 
? EP:  Entity Profiles are complex rich information 
objects that collect entity-centric information, in 
particular, all the CE relationships that a given 
entity is involved in and all the events this entity 
is involved in. This is achieved through 
document-internal fusion and cross-document 
fusion of related information based on support 
from co-reference, including alias association. 
Work is in progress to enhance the fusion by 
correlating the extracted information with 
information in a user-provided existing database. 
? GE:  General Events are verb-centric information 
objects representing ?who did what to whom 
when and where? at the logical level. 
Concept-based GE (CGE) further requires that 
participants of events be filled by EPs instead of 
NEs and that other values of the GE slots (the 
action, time and location) be disambiguated and 
normalized.  
? PE:  Predefined Events are domain specific or 
user-defined events of a specific event type, such 
as Product Launch and Company Acquisition in 
the business domain. They represent a simplified 
version of MUC ST. InfoXtract provides a toolkit 
that allows users to define and write their own 
PEs based on automatically generated PE rule 
templates.  
 
The InfoXtract engine has been deployed both 
internally to support Cymfony?s Brand Dashboard? 
product and externally to a third-party integrator for 
building IE applications in the intelligence domain.  
 
Document Processor
Knowledge Resources
Lexicon
Resources
Grammars
Process
Manager
Tokenlist
Legend
Output
Manager
Source
Document
NLP/IE Processor(s)Tokenizer
Tokenlist
Lexicon Lookup
 POS Tagging
Named Entity
Detection
Shallow
Parsing
Deep Parsing
Relationship
Detection
Document
pool
NE
CE
EP
SVO
Time
Normalization
Alias and
Coreference
Profile/Event
Linking/Merging
Abbreviations
POS = Part of Speech
NE = Named Entity
CE = Correlated Entity
EP = Entity Profile
SVO = Subject-Verb-Object
GE = General Event
PE = Predefined Event
Grammar Module
Procedure or
Statistical Model
Hybrid
Module
GE
Statistical
Models
Location
Normalizationli ti
PE
InfoXtract
Repository
Event
Extraction
Case Restoration
 
Figure 1:  InfoXtract Engine Architecture 
  
3 Hybrid Technology 
InfoXtract represents a hybrid model for IE since it 
combines both grammar formalisms as well as 
machine learning. Achieving the right balance of these 
two paradigms is a major design objective of 
InfoXtract. The core of the parsing and information 
extraction process in InfoXtract is organized very 
simply as a pipeline of processing modules. All 
modules operate on a single in-memory data structure, 
called a token list. A token list is essentially a 
sequence of tree structures, overlaid with a graph 
whose edges define relations that may be either 
grammatical or informational in nature. The nodes of 
these trees are called tokens. InfoXtract?s typical 
mode of processing is to skim along the roots of the 
trees in the token list, building up structure 
?strip-wise?. So even non-terminal nodes behave, in 
the typical case, as complex tokens. Representing a 
marked up text using trees explicitly, rather than 
implicitly as an interpretation of paired bracket 
symbols, has several advantages. For example, it 
allows a somewhat richer organization of the 
information contained ?between the brackets,? 
allowing us to construct direct links from a root node 
to its semantic head, for example. 
The processing modules that act on token lists can 
range from lexical lookup to the application of hand 
written grammars to statistical analysis based on 
machine learning all the way to arbitrary procedures 
written in C++. The configuration of the InfoXtract 
processing pipeline is controlled by a configuration 
file, which handles pre-loading required resources as 
well as ordering the application of modules. Despite 
the variety of implementation strategies available, 
InfoXtract Natural Language Processing (NLP) 
modules are restricted in what they can do to the token 
list to actions of the following three types : 
 
1. Assertion and erasure of token properties 
(features, normal forms, etc.) 
2. Grouping token sequences into higher level 
constituent tokens. 
3. Linking token pairs with a relational link. 
 
Grammatical analysis of the input text makes use of a 
combination of phrase structure and relational 
approaches to grammar. Basically, early modules 
build up structure to a certain level (including 
relatively simple noun phrases, verb groups and 
prepositional phrases), after which further 
grammatical structure is represented by asserting 
relational links between tokens. This mix of phrase 
structural and relational approaches is very similar to 
the approach of Lexical Functional Grammar (LFG) 
[Kaplan & Bresnan 1982], much scaled down.  
Our grammars are written in a formalism 
developed for our own use, and also in a modified 
formalism developed for outside users, based on our 
in-house experiences. In both cases, the formalism 
mixes regular expressions with boolean expressions. 
Actions affecting the token list are implemented as 
side effects of pattern matching. So although our 
processing modules are in the technical sense token 
list transducers, they do not resemble Finite State 
Transducers (FSTs) so much as the regular expression 
based pattern-action rules used in Awk or Lex. 
Grammars can contain (non-recursive) macros, with 
parameters. 
This means that some long-distance dependencies, 
which are very awkward to represent directly in finite 
state automata can be represented very compactly in 
macro form. While this has the advantage of 
decreasing grammar sizes, it does increase the size of 
the resulting automata. Grammars are compiled to a 
special type of finite state automata. These token list 
automata can be thought of as an extension of tree 
walking automata [M?nnich et al2001, Aho & 
Ullman 1971, Engelfriet et al1999]. These are linear 
automata (as opposed to standard finite state tree 
automata [G?cseg &  Steinby 1997], which are more 
naturally thought of as parallel) which run over trees. 
The problem with linear automata on trees is that there 
can be a number of ?next? nodes to move the read 
head to: right sister, left sister, parent, first child, etc. 
So the vocabulary of the automaton is increased to 
include not only symbols that might appear in the text 
(test instructions) but also symbols that indicate where 
to move the read head (directive instructions). We 
have extended the basic tree walking formalism in 
several directions. First we extend the power of test 
instructions to allow them to check features of the 
current node and to perform string matching against 
the semantic head of the current node (so that a 
syntactically complex constituent can be matched 
against a single word). Second, we include symbols 
for action instructions, to implement side effects. 
Finally, we allow movement not only along the root 
sequence (string-automaton style) and branches of a 
tree (tree-walking style) but also along the the 
terminal frontier of the tree and along relational links. 
These extensions to standard tree walking 
automata extend the power of that formalism 
tremendously, and could pose problems. However, the 
grammar formalisms that compile into these token list 
walking automata are restrictive, in the sense that 
there exist many token list transductions that are 
implementable as automata that are not 
implementable as grammars. Also the nature of the 
shallow parsing task itself is such that we only need to 
dip into the reserves of power that this representation 
affords us on relatively rare occasions. As a result, the 
automata that we actually plug into the InfoXtract 
NLP pipeline generally run very fast. 
Recently, we have developed an extended finite 
state formalism named Expert Lexicon, following the 
general trend of lexicalist approaches to NLP. An 
  
expert lexicon rule consists of both grammatical 
components as well as proximity-based keyword 
matching. All Expert Lexicon entries are indexed, 
similar to the case for the finite state tool in INTEX 
[Silberztein 2000]. The pattern matching time is 
therefore reduced dramatically compared to a 
sequential finite state device.  
Some unique features of this formalism include: (i) 
the flexibility of inserting any number of Expert 
Lexicons at any level of the process; (ii) the capability 
of proximity checking within a window size as rule 
constraints in addition to pattern matching using an 
FST call, so that the rule writer can exploit the 
combined advantages of both; and (iii) support for the 
propagation of semantic tagging results, to 
accommodate principles like one sense per discourse. 
Expert lexicons are used in customization of lexicons, 
named entity glossaries, and alias lists, as well as 
concept tagging. 
Both supervised machine learning and unsuper- 
vised learning are used in InfoXtract. Supervised 
learning is used in hybrid modules such as NE [Srihari 
et al2000], NE Normalization [Li et al2002] and 
Co-reference. It is also used in the preprocessing 
module for orthographic case restoration of case 
insensitive input [Niu et al2003]. Unsupervised 
learning involves acquisition of lexical knowledge 
and rules from a raw corpus. The former includes 
word clustering, automatic name glossary acquisition 
and thesaurus construction. The latter involves 
bootstrapped learning of NE and CE rules, similar to 
the techniques used in [Riloff 1996]. The results of 
unsupervised learning can be post-edited and added as 
additional resources for InfoXtract processing.  
 
Table 1: SVO/CE Benchmarking 
 SVO CE 
 CORRECT 196 48 
 INCORRECT 13 0 
 SPURIOUS 10 2 
 MISSING 31 10 
 PRECISION 89.50% 96.0% 
 RECALL 81.67% 82.8% 
 F-MEASURE 85.41% 88.9% 
 
Accuracy 
InfoXtract has been benchmarked using the MUC-7 
data sets which are recognized as standards by the 
research community. Precision and recall figures for 
the person and location entity types were above 90%. 
For organization entity types, precision and recall 
were in the high 80?s reflecting the fact that 
organization names tend to be very domain specific. 
InfoXtract provides the ability to create customized 
named entity glossaries, which will boost the 
performance of organization tagging for a given 
domain. No such customization was done in the 
testing just described. The accuracy of shallow 
parsing is well over 90% reflecting very high 
performance part-of-speech tagging and named entity 
tagging. Table 1 shows the benchmarks for CE 
relationships which are the basis for EPs and for the 
SVO parsing which supports event extraction.  
4 Engineering Architecture 
The InfoXtract engine has been developed as a 
modular, distributed application and is capable of 
processing up to 20 MB per hour on a single 
processor. The system has been tested on very large (> 
1 million) document collections. The architecture 
facilitates the incorporation of the engine into external 
applications requiring an IE subsystem. Requests to 
process documents can be submitted through a web 
interface, or via FTP. The results of processing a 
document can be returned in XML. Since various 
tools are available to automatically populate databases 
based on XML data models, the results are easily 
usable in web-enabled database applications. 
Configuration files enable the system to be used with 
different lexical/statistical/grammar resources, as well 
as with subsets of the available IE modules.  
InfoXtract supports two modes of operation, active 
and passive. It can act as an active retriever of 
documents to process or act as a passive receiver of 
documents to process. When in active mode, 
InfoXtract is capable of retrieving documents via 
HTTP, FTP, or local file system. When in passive 
mode, InfoXtract is capable of accepting documents 
via HTTP. Figure 2 illustrates a multiple processor 
configuration of InfoXtract focusing on the typical 
deployment of InfoXtract within an application. 
 
Server B
Server C
Server A
Processor 4
Processor 6
Processor 2
Document
Retriever
InfoXtract
Controller
Document
Manager
Processor 1
Processor 3
Processor 5
Extracted info
database
Documents
External Content
Provider
Java InfoXtract
(JIX)
External
Application
Figure 2:  High Level Architecture 
 
The architecture facilitates scalability by 
supporting multiple, independent Processors. The 
Processors can be running on a single server (if 
multiple CPUs are available) and on multiple servers. 
The Document Manager distributes requests to 
process documents to all available Processors. Each 
component is an independent application. All direct 
  
inter-module communication is accomplished using 
the Common Object Request Broker Architecture 
(CORBA). CORBA provides a robust, programming 
language independent, and platform neutral 
mechanism for developing and deploying distributed 
applications. Processors can be added and removed 
without stopping the InfoXTract engine. All modules 
are self-registering and will announce their presence 
to other modules once they have completed 
initialization.  
The Document Retriever module is only used in 
the active retriever mode. It is responsible for 
retrieving documents from a content provider and 
storing the documents for use by the InfoXtract 
Controller. The Document Retriever handles all 
interfacing with the content provider?s retrieval 
process, including interface protocol (authentication, 
retrieve requests, etc.), throughput management, and 
document packaging. It is tested to be able to retrieve 
documents from content providers such as Northern 
Light, Factiva, and LexisNexis. Since the Document 
Retriever and the InfoXtract Controller do not 
communicate directly, it is possible to run the 
Document Retriever standalone and process all 
retrieved documents in a batch mode at a later time.  
The InfoXtract Controller module is used only in 
the active retriever mode. It is responsible for 
retrieving documents to be processed, submitting 
documents for processing, storing extracted 
information, and system logging. The InfoXtract 
Controller is a multi-threaded application that is 
capable of submitting multiple simultaneous requests 
to the Document Manager. As processing results are 
returned, they are stored to a repository or database, an 
XML file, or both. 
The Document Manager module is responsible for 
managing document submission to available 
Processors. As Processors are initialized, they register 
with the Document Manager. The Document Manager 
uses a round robin scheduling algorithm for sending 
documents to available Processors. A document queue 
is maintained with a size of four documents per 
Processor. The Processor module forms the core of the 
IE engine. InfoXtract utilizes a multi-level approach 
to NLP. Each level utilizes the results of the previous 
levels in order to achieve more sophisticated parsing. 
The JIX module is a web application that is 
responsible for accepting requests for documents to be 
processed. This module is only used in the passive 
mode. The document requests are received via the 
HTTP Post request. Processing results are returned in 
XML format via the HTTP Post response.  
In Table 2 we present an example of the 
performance that can be expected based on the 
application of all modules within the engine. It should 
be noted that considerably faster processing per 
processor can be achieved if output is restricted to a 
certain IE level, such as named entity tagging only. 
The output in this benchmark includes all major tasks 
such as NE, EP, parsing and event extraction as well 
as XML generation.  
This configuration provides throughput of 
approximately 12,000 documents (avg. 10KB) per 
day. A smaller average document size will increase 
the document throughput. Increased throughput can 
be achieved by dedicating a CPU for each running 
Processor. Each Processor instance requires 
approximately 500 MB of RAM to run efficiently. 
Processing speed increases linearly with additional 
Processors/CPUs, and CPU speed. In the current state, 
with no speed optimization, using a bank of eight 
processors, it is able to process approximately 
100,000 documents per day. Thus, InfoXtract is 
suitable for high volume deployments. The use of 
CORBA provides seamless inter-process and 
over-the-wire communication between modules. 
Computing resources can be dynamically assigned to 
handle increases in document volume.  
 
Table 2:  Benchmark for Efficiency 
Server 
Configuration 
2 CPU @ 1 GHz, 2 GB 
RAM 
Operating System Redhat Linux 7.2 
Document 
Collection Size 
500 Documents, 5 MB 
total size 
Engine 
Configuration 
InfoXtract Controller, 
Document Manager, 
and 2 Processors 
running on a single 
server 
Processing Time 30 Minutes 
 
A standard document input model is used to 
develop effective preprocessing capabilities. 
Preprocessing adapts the engine to the source by 
presenting metadata, zoning information in a 
standardized format and performing restoration tasks 
(e.g. case restoration). Efforts are underway to 
configure the engine such that zone-specific 
processing controls are enabled. For example, zones 
identified as titles or subtitles must be tagged using 
different criteria than running text. The engine has 
been deployed on a variety of input formats including 
HUMINT documents (all uppercase), the Foreign 
Broadcast Information Services feed (FBIS), live 
feeds from content providers such as Factiva (Dow 
Jones/Reuters), LexisNexis, as well as web pages. A 
user-trainable, high-performance case restoration 
module [Niu et al2003] has been developed that 
transforms case insensitive input such as speech 
transcripts into mixed-case before being processed by 
the engine. The case restoration module eliminates the 
need for separate IE engines for case-insensitive and 
case-sensitive documents; this is easier and more cost 
effective to maintain. 
  
5 Corpus-level IE 
Efforts have extended IE from the document level to 
the corpus level. Although most IE systems perform 
corpus-level information consolidation at an 
application level, it is felt that much can be gained by 
doing this as an extended step in the IE engine. A 
repository has been developed for InfoXtract that is 
able to hold the results of processing an entire corpus. 
A proprietary indexing scheme for indexing token-list 
data has been developed that enables querying over 
both the linguistic structures as well as statistical 
similarity queries (e.g., the similarity between two 
documents or two entity profiles). The repository is 
used by a fusion module in order to generate 
cross-document entity profiles as well as for text 
mining operations. The results of the repository 
module can be subsequently fed into a relational 
database to support applications. This has the 
advantage of filtering much of the noise from the 
engine level and doing sophisticated information 
consolidation before populating a relational database. 
The architecture of these subsequent stages is shown 
in Figure 3. 
 
Databases
Fusion
Module
Corpus-
level IEInfoXtract
Text
Mining
FBIS, Newswire
Documents
InfoXtract
Repository 1
InfoXtract
Repository 2
IDP
Figure 3:  Extensions to InfoXtract 
 
Information Extraction has two anchor points: (i) 
entity-centric information which leads to an EP, and 
(ii) action-centric information which leads to an event 
scenario. Compared with the consolidation of 
extracted events into cross-document event scenario, 
cross-document EP merging and consolidation is a 
more tangible task, based mainly on resolving aliases. 
Even with modest recall, the corpus-level EP 
demonstrates tremendous value in collecting 
information about an entity.  This is as shown in Table 
3 for only part of the profile of ?Mohamed Atta? from 
one experiment based on a collection of news articles. 
The extracted EP centralizes a significant amount of 
valuable information about this terrorist. 
6 Domain Porting 
Considerable efforts have been made to keep the core 
engine as domain independent as possible; domain 
specialization or tuning happens with minimum 
change to the core engine, assisted by automatic or 
semi-automatic domain porting tools we have 
developed.  
Cymfony has taken several distinct approaches in 
achieving domain portability: (i) the use of a standard 
document input model, pre-processors and 
configuration scripts in order to tailor input and output 
formats for a given application, (ii) the use of tools in 
order to customize lexicons and grammars, and (iii) 
unsupervised machine learning techniques for 
learning new named entities (e.g. weapons) and 
relationships based on sample seeds provided by a 
user.  
Table 3:  Sample Entity Profile 
Name Mohamed Atta 
Aliases Atta; Mohamed  
Position apparent mastermind;  
ring leader; engineer; leader  
Age 33; 29; 33-year-old; 
34-year-old  
Where-from United Arab Emirates; 
Spain; Hamburg; Egyptian; 
?? 
Modifiers on the first plane; evasive; 
ready; in Spain; in seat 8D? 
Descriptors hijacker; al-Amir; purported 
ringleader; a square-jawed 
33-year-old pilot; ?? 
Association bin Laden; Abdulaziz 
Alomari; Hani Hanjour; 
Madrid; American Media 
Inc.; ?? 
Involved-events move-events (2); 
accuse-events (9), 
convict-events (10), 
confess-events (2), 
arrest-events (3), 
 rent-events (3),  ..... 
 
It has been one of Cymfony?s primary objectives 
to facilitate domain portability [Srihari 1998] [Li & 
Srihari 2000a,b, 2003].  This has resulted in a 
development/customization environment known as 
the Lexicon Grammar Development Environment 
(LGDE). The LGDE permits users to modify named 
entity glossaries, alias lexicons and general-purpose 
lexicons.  It also supports example-based grammar 
writing; users can find events of interest in sample 
documents, process these through InfoXtract and 
modify the constraints in the automatically generated 
rule templates for event detection. With some basic 
training, users can easily use the LGDE to customize 
InfoXtract for their applications. This facilitates 
customization of the system in user applications 
where access to the input data to InfoXtract is 
restricted. 
  
7 Applications 
The InfoXtract engine has been used in two 
applications, the Information Discovery Portal (IDP) 
and Brand Dashboard (www.branddashboard. 
com). The IDP supports both the traditional top-down 
methods of browsing through large volumes of 
information as well as novel, data-driven browsing. A 
sample user interface is shown in Figure 4.  
Users may select ?watch lists? of entities (people, 
organizations, targets, etc.) that they are interested in 
monitoring. Users may also customize the sources of 
information they are interested in processing. 
Top-down methods include topic-centric browsing 
whereby documents are classified by topics of 
interest. IE-based browsing techniques include 
entity-centric and event-centric browsing. 
Entity-centric browsing permits users to track key 
entities (people, organizations, targets) of interest and 
monitor information pertaining to them. Event-centric 
browsing focuses on significant actions including 
money movement and people movement events.  
Visualization of extracted information is a key 
component of the IDP.  The Information Mesh enables 
a user to visualize an entity, its attributes and its 
relation to other entities and events. Starting from an 
entity (or event), relationship chains can be traversed 
to explore related items. Timelines facilitate 
visualization of information in the temporal axis. 
Information Discovery Portal 
Associations 
Who/what is being 
associated with al-
Qaeda ?
Organizations
    Religious
    Political
    Terrorist
     - al-Jihad (34)
     - HAMAS (16)
     - Hizballah (5)
     - ?more
People
Incidents
  - Attacks (125)
  - Bombing (64)
  - Threats (45)
  - ?more
Locations
Weapons
Governments
Overall 
Coverage 
Events Info. Sources Documents 
Track... Organizations People Targets 
al-Qaeda 
Overall Coverage of  al-Qaeda Over Time 
0 10 
20 30 
40 50 
5/7/2001 5/14/2001 5/21/2001 5/28/2001 6/4/2001 6/11/2001 6/18/2001 6/25/2001 7/2/2001 7/9/2001 7/16/2001 7/23/2001 7/30/2001 
# 
Re
por
ts 
Alerts for Week of August 6, 2001 
(3)  new reports of al-Qaeda terrorist activity 
(1)  new report of  bin Laden sighting 
(4)  new quotes by bin Laden 
(1)  new target identified 
Figure 4:  Information Discovery Portal 
Recent efforts have included a tight integration of 
InfoXtract with visualization tools such as the 
Web-based Timeline Analysis System (WebTAS) 
(http://www.webtas.com). The IDP reflects the ability 
for users to select events of interest and automatically 
export them to WebTAS for visualization. Efforts are 
underway to integrate higher-level event scenario 
analysis tools such as the Terrorist Modus Operandi 
Detection System (TMODS) (www.21technologies 
.com) into the IDP. 
 
Brand Dashboard is a commercial application for 
marketing and public relations organizations to 
measure and assess media perception of consumer 
brands. The InfoXtract engine is used to analyze 
several thousand electronic sources of information 
provided by various content aggregators (Factiva, 
LexisNexis, etc.). The engine is focused on tagging 
and generating brand profiles that also capture salient 
information such as the descriptive phrases used in 
describing brands (e.g. cost-saving, non-habit 
forming) as well as user-configurable specific 
messages that companies are trying to promote and 
track (safe and reliable, industry leader, etc.). The 
output from the engine is fed into a database-driven 
web application which then produces report cards for 
brands containing quantitative metrics pertaining to 
brand perception, as well as qualitative information 
describing characteristics. A sample screenshot from 
Brand Dashboard is presented in Figure 5. It depicts a 
report card for a particular brand, highlighting brand 
strength as well as highlighting metrics that have 
changed the most in the last time period. The ?buzz 
box? on the right hand side illustrates 
companies/brands, people, analysts, and messages 
most frequently associated with the brand in question.  
Figure 5:  Report Card from Brand Dashboard 
8 Summary and Future Work 
This paper has described the motivation behind 
InfoXtract, a domain independent, portable, 
intermediate-level IE engine. It has also discussed the 
architecture of the engine, both from an algorithmic 
perspective and software engineering perspective. 
Current efforts to improve InfoXtract include the 
following: support for more diverse input formats, 
more use of metadata in the extraction tasks, support 
 
  
for structured data, and capabilities for processing 
foreign languages. Finally, support for more intuitive 
domain customization tools, especially the 
semi-automatic learning tools is a major focus.  
Acknowledgments 
The authors wish to thank Carrie Pine of AFRL for 
reviewing and supporting this work. 
References 
[Aho & Ullman 1971] Alfred V. Aho and Jeffrey 
D. Ullman. Translations on a context-free grammar. 
Information and Control, 19(5):439?475, 1971. 
[Aone & Ramos-Santacruz 1998] REES: A 
Large-Scale Relation and Event Extraction System.  
url: http://acl.ldc.upenn.edu/A/A00/A00-1011.pdf 
[Chinchor & Marsh 1998] Chinchor, N. & Marsh, 
E. 1998. MUC-7 Information Extraction Task 
Definition (version 5.1), Proceedings of MUC-7.  
[Cunningham et al2003] Hamish Cunningham et 
al.  Developing Language Processing Components 
with GATE: A User Guide. 
http://gate.ac.uk/sale/tao/index.html#annie 
[Engelfriet et al1999] Joost Engelfriet, Hendrik 
Jan Hoogeboom, and Jan-Pascal Van Best. Trips on 
trees. Acta Cybernetica, 14(1):51?64, 1999. 
[G?cseg & Steinby 1997] Ferenc G?cseg and 
Magnus Steinby. Tree languages. In Grzegorz 
Rozenberg and Arto Salomaa, editors, Handbook of 
Formal Languages: Beyond Words, volume 3, pages 
1?68, Berlin, 1997. Springer 
[Han 1999] Han, J. Data Mining. 1999.  In J. 
Urban and P. Dasgupta (eds.), Encyclopedia of 
Distributed Computing, Kluwer Academic Publishers. 
[Hobbs 1993] J. R. Hobbs, 1993.  FASTUS: A 
System for Extracting Information from Text, 
Proceedings of the DARPA workshop on Human 
Language Technology?, Princeton, NJ, 133-137. 
[Kaplan & Bresnan 1982] Ronald M. Kaplan and 
Joan Bresnan. Lexical-Functional Grammar: A formal 
system for grammatical representation. In Joan 
Bresnan, editor, The Mental Representation of 
Grammatical Relations, pages 173?281. The MIT 
Press, Cambridge, MA, 1982.  
[Krupka & Hausman 1998] G. R Krupka and K. 
Hausman, ?IsoQuest Inc: Description of the NetOwl 
Text Extraction System as used for MUC-7?, MUC-7 
[Li et al2002] Li, H., R. Srihari, C. Niu, and W. Li 
(2002).  Localization Normalization for Information 
Extraction.  COLING 2002, 549?555, Taipei, Taiwan. 
[Li, W & R. Srihari 2000a]. A Domain 
Independent Event Extraction Toolkit, Final 
Technical Report, Air Force Research Laboratory, 
Information Directorate, Rome Research Site, New 
York 
[Li, W & R. Srihari 2000b]. Flexible Information 
Extraction Learning Algorithm, Final Technical 
Report, Air Force Research Laboratory, Information 
Directorate, Rome Research Site, New York  
[Li & Srihari 2003] Li, W. and R. K. Srihari (2003) 
Intermediate-Level Event Extraction for Temporal 
and Spatial Analysis and Visualization, Final 
Technical Report AFRL-IF-RS-TR-2002-245, Air 
Force Research Laboratory, Information Directorate, 
Rome Research Site, New York. 
[Miller et al1998] Miller, Scott; Crystal, Michael; 
Fox, Heidi; Ramshaw, Lance; Schwartz, Richard; 
Stone, Rebecca; Weischedel, Ralph; and Annotation 
Group, the 1998. Algorithms that Learn to Extract 
Information; BBN: Description of the SIFT System as 
Used for MUC-7.  
[M?nnich et al2001] Uwe M?nnich, Frank 
Morawietz, and Stephan Kepser. A regular query for 
context-sensitive relations. In Steven Bird, Peter 
Buneman, and Mark Liberman, editors, IRCS 
Workshop Linguistic Databases 2001, pages 
187?195, 2001 
[Niu et al2003] Niu, C., W. Li, J. Ding, and R.K. 
Srihari (to appear 2003).  Orthographic Case 
Restoration Using Supervised Learning Without 
Manual Annotation.  Proceedings of The 16th 
FLAIRS, St. Augustine, FL 
[Riloff 1996] [Automatically Generating 
Extraction Patterns from Untagged Text. AAAI-96. 
[Roche & Schabes 1997] Emmanuel Roche & 
Yves Schabes, 1997. Finite-State Language 
Processing, The MIT Press, Cambridge, MA. 
[Silberztein 1999] Max Silberztein, (1999). 
INTEX: a Finite State Transducer toolbox, in 
Theoretical Computer Science #231:1, Elsevier 
Science 
[Srihari 1998]. A Domain Independent Event 
Extraction Toolkit, AFRL-IF-RS-TR-1998-152 Final 
Technical Report, Air Force Research Laboratory, 
Information Directorate, Rome Research Site, New 
York  
[Srihari et al2000] Srihari, R, C. Niu and W. Li. 
(2000).  A Hybrid Approach for Named Entity and 
Sub-Type Tagging.  In Proceedings of ANLP 2000, 
247?254, Seattle, WA. 
Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 61?69,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
NE Tagging for Urdu based on Bootstrap POS Learning 
 
 
Smruthi Mukund Rohini K. Srihari 
Dept. of Computer Science and Engineering Dept. of Computer Science and Engineering 
University at Buffalo, SUNY University at Buffalo, SUNY 
Amherst, NY, USA Amherst, NY, USA 
smukund@buffalo.edu rohini@cedar.buffalo.edu 
 
 
 
 
 
 
Abstract 
Part of Speech (POS) tagging and Named Ent-
ity (NE) tagging have become important com-
ponents of effective text analysis. In this 
paper, we propose a bootstrapped model that 
involves four levels of text processing for Ur-
du. We show that increasing the training data 
for POS learning by applying bootstrapping 
techniques improves NE tagging results. Our 
model overcomes the limitation imposed by 
the availability of limited ground truth data 
required for training a learning model. Both 
our POS tagging and NE tagging models are 
based on the Conditional Random Field 
(CRF) learning approach. To further enhance 
the performance, grammar rules and lexicon 
lookups are applied on the final output to cor-
rect any spurious tag assignments. We also 
propose a model for word boundary segmen-
tation where a bigram HMM model is trained 
for character transitions among all positions in 
each word. The generated words are further 
processed using a probabilistic language mod-
el. All models use a hybrid approach that 
combines statistical models with hand crafted 
grammar rules. 
1 Introduction 
The work here is motivated by a desire to under-
stand human sentiment and social behavior through 
analysis of verbal communication. Newspapers 
reflect the collective sentiments and emotions of 
the people and in turn the society to which they 
cater to. Not only do they portray an event that has 
taken place as is, but they also reveal details about 
the intensity of fear, imagination, happiness and 
other emotions that people express in relation to 
that event.  Newspaper write ups, when analyzed 
over these factors - emotions, reactions and beha-
vior - can give a broader perspective on the culture, 
beliefs and the extent to which the people in the 
region are tolerant towards other religions. Our 
final goal is to automate this kind of behavioral 
analysis on newspaper articles for the Urdu lan-
guage. Annotated corpus that tag six basic human 
emotions, ?happy?, ?fear?, ?sad?, ?surprise?, ?an-
ger? and ?disgust?, based on the code book devel-
oped using the MPQA standards as guideline, is 
currently being developed.  Articles from two lead-
ing Urdu newswires, BBC Urdu1 and Jung Daily2 
form our corpus.  
In order to achieve our goal, it was required to 
generate the basic tools needed for efficient text 
analysis. This includes NE tagging and its precur-
sor, POS tagging. However, Urdu, despite being 
spoken by over 100 million people, (Gordon, 
2005) is still a less privileged language when it 
comes to the availability of resources on the inter-
net. Developing tools for a language with limited 
resources is a challenge, but necessary, as the vo-
lume of Urdu text on the internet is rising. Huda 
(2001) shows that Urdu has now gained impor-
tance on the web, making it the right time to tackle 
these issues. 
It is useful to first examine some basic proper-
ties of Urdu and how they affect the cascade of 
NLP steps in text analysis. Urdu has the nastaleeq 
and nasq style of writing that is similar to Arabic 
                                                           
1 http://www.bbc.co.uk/urdu/ 
2 http://www.jang.net/urdu/ 
61
and flows from right to left (Ahmad et al, 2001). It 
also adopts some of its vocabulary from Arabic. 
However, the grammar and semantics of the lan-
guage is similar to Hindi and this makes it very 
different from Arabic. For effective text analysis, a 
thorough syntactic and semantic understanding of 
the language is required. Detailed grammatical 
analysis provided by Platts (1909) and Schmidt 
(1999) can be used for this purpose. The first step 
in the information retrieval pipeline is tokeniza-
tion. Unlike English, where the word delimiter is 
mostly a space, Urdu is more complex. There are 
space insertion as well as space deletion problems. 
This makes tokenization a difficult task. The word 
segmentation model that we propose here com-
bines the statistical approach that considers bigram 
transition of characters based on their positions in a 
word and morphological rules with lexicon loo-
kups. 
 POS tagging comes next in the NLP text analy-
sis pipeline. The accuracy of the tagging model 
varies, depending on the tagsets used and the do-
main of the ground truth data. There are two main 
tagsets designed for Urdu, the CRULP tagset3 and 
the U1-tagset (Hardie 2003). The U1-tagset, re-
leased as a part of EMILLE4 corpus, is based on 
the EAGLES standards (Leech and Wilson 1999). 
We decided to use the standards proposed by 
CRULP for the following reasons. 
 
1. The tagset, though not as detailed as the 
one proposed in U1-tagset, covers all the 
basic requirements needed to achieve our 
final goal. 
2. The tagged corpus provided by CRULP is 
newswire material, similar to our final 
corpus. 
 
A person, when asked to identify an NE tagged 
word in a sentence would typically try to first find 
the word associated with a proper noun or a noun, 
and then assign a suitable NE tag based on the con-
text. A similar approach is used in our model, 
where the learning happens on the data that is POS 
tagged as well as NE tagged. Features are learnt 
from the POS tags as well as the NE tags. The final 
output of our complete model returns the POS tags 
                                                           
3 
http://www.crulp.org/Downloads/ling_resources/parallelcorpu
s/Urdu POS Tagset.pdf 
4 http://www.emille.lancs.ac.uk/ 
and NE tags associated with each word. Since we 
have limited data for training both the POS as well 
as the NE models, we propose a technique called 
bootstrapping that helps in maximizing the learn-
ing for efficient tagging. 
The remainder of the paper is organized as fol-
lows. Section 2 discusses the resources assimilated 
for the work followed by tokenization and word 
segmentation in Section 3. Section 4 gives a de-
tailed explanation of our model starting with a 
brief introduction of the learning approach used. 
Rules used for POS tagging and NE tagging are 
mentioned in subsections of Section 4. Section 5 
presents the results and Section 6 concludes the 
paper. In each section, wherever relevant, previous 
work and drawbacks are presented. 
2 Resources  
Based on the style of writing for Urdu, different 
encoding standards have been proposed. Urdu 
Zabta Takthi - the national standard code page for 
Urdu and Unicode - international standard for mul-
tilingual characters are the two proposed and wide-
ly used encoding standards. BBC Urdu and Jung 
Daily are both encoded with Unicode standards 
and are good sources of data. The availability of 
online resources for Urdu is not as extensive as 
other Asian languages like Chinese and Hindi. 
However, Hussain (2008) has done a good job in 
assimilating most of the resources available on the 
internet. The lexicon provided as a part of the 
EMILLE (2003) data set for Urdu has about 
200,000 words. CRL5 has released a lexicon of 
8000 words as a part of their Urdu data collection. 
They also provide an NE tagged data set mostly 
used for morphological analysis. The lexicon in-
cludes POS information as well. CRULP6 has also 
provided a lexicon of 149,466 words that contains 
places, organizations and names of people. As part 
of the Urdu morphological analyzer provided by 
Humayoun (2007), a lexicon of about 4,500 unique 
words is made available. There are a few Urdu-
English dictionaries available online and the first 
online dictionary, compiled by Siddiqi (2008), 
provides about 24,000 words with their meanings 
in English.  
Getting all the resources into one single compi-
lation is a challenge. These resources were brought 
                                                           
5 http://crl.nmsu.edu/Resources/lang_res/urdu.html 
6 http://www.crulp.org/software/ling_resources/wordlist.htm 
62
together and suitably compiled into a format that 
can be easily processed by Semantex (Srihari, 
2008), a text extraction platform provided by Janya 
Inc7. Lists of places, organizations and names of 
famous personalities in Pakistan were also com-
piled using the Urdu-Wikipedia8 and NationalMas-
ter9. A list of most common names in Pakistan was 
composed by retrieving data from the various 
name databases available on the internet.   
The word segmentation model uses the Urdu 
corpus released by CRULP as the training data. 
This dataset is well segmented. POS tagging model 
uses data provided by CRULP and NE tagging 
model uses data provided by CRL. 
3 Word Segmentation and Tokenization  
Urdu is a language that has both the space inser-
tion and space deletion problems. The Urdu word 
segmentation problem as mentioned by Durrani 
(2007) is triggered by its orthographic rules and 
confusions about the definition of a word. Durrani 
summarizes effectively, all the problems associated 
with Urdu word segmentation. Of all the different 
techniques explored to achieve this objective, tra-
ditional techniques like longest and maximum 
matching depend mostly on the availability of a 
lexicon that holds all the morphological forms of a 
word. Such a lexicon is difficult to obtain. It is 
shown by Theeramunkong et al, (2001), that for a 
Thai segmentation system, the efficiency drops 
considerably (from 97% to 82%) making this ap-
proach highly lexicon dependent.  
Statistical based techniques have applied proba-
bilistic models to solve the problem of word seg-
mentation. Bigram and trigram models are most 
commonly employed. Using feature based tech-
niques for POS tagging is also very common. 
These techniques overcome the limitations of sta-
tistical models by considering the context around 
the word for specific words and collocations. There 
are other models that generate segments by consi-
dering word level collation as well as syllable level 
collocation.  
However, for a language like Urdu, a model that 
is purely statistical will fail to yield good segmen-
tation results. A mixed model that considers the 
morphological as well as semantic features of the 
                                                           
7 http://www.janyainc.com/ 
8 http://ur.wikipedia.com/wiki/ 
9 http://www.nationmaster.com/index.php 
language facilitates better performance as shown 
by Durrani (2007) where the word segmentation 
model uses a lexicon for proper nouns and a statis-
tical model that trains over the n-gram probability 
of morphemes. Maximum matching technique is 
used to generate word boundaries of the ortho-
graphic words that are formed and these are later 
verified using the POS information. The segments 
thus generated are ranked and the best ones are 
accepted. Statistical models that consider character 
based, syllable based and word based probabilities 
have shown to perform reasonably well. The Thai 
segmentation problem was solved by Pornprasert-
kul (1994) using the character based approach. In 
our model, we use a combination of character 
based statistical approach and grammar rules with 
lexicon lookups to generate word boundaries. 
Urdu segmentation problem can be looked at as 
an issue of inserting spaces between characters. All 
letters in Urdu, with a few exceptions, have three 
forms - initial, medial and final. (We do not con-
sider the detached form for word formation). 
Words are written by joining the letters together 
and based on the position of the letter in the word, 
suitable forms are applied. This property of word 
formation is the crux of our model. The bigram 
probability of occurrences of each of these charac-
ters, based on their positions, is obtained by train-
ing over a properly segmented training set. For 
unknown characters, unknown character models 
for all the three position of occurrences are also 
trained. The probability of word occurrence is 
noted. Along with this, a lexicon rich enough to 
hold all possible common words is maintained. 
However, this lexicon does not contain proper 
nouns. A new incoming sentence that is not seg-
mented correctly is taken and suitable word boun-
daries are generated by using a combination of 
morphological rules, lexicon lookups, bigram word 
probabilities and bigram HMM character model. 
The following probabilities are estimated and max-
imized at character level using the Viterbi algo-
rithm. The following are the calculated 
probabilities:  
 
(i) )|( )(1)( initialkmedialk chchP ? - is the prob-
bility of character k being in medial 
form given character k-1 is in initial 
form. 
63
(ii) )|( )(1)( initialkfinalk chchP ? - is the proba-
bility of character k being in final form 
given character k-1 is in initial form. 
(iii) )|( )(1)( medialkfinalk chchP ?  - is the proba-
bility of character k being in final form 
given character k-1 is in medial form. 
(iv) )|( )(1)( medialkmedialk chchP ? - is the proba-
bility of character k being in medial 
form given character k-1 is in medial 
form. 
(v) )|( )(1)( finalkinitialk chchP ? - is the proba-
bility of character k being in initial 
form given character k-1 is in final 
form. 
 
Each word thus formed successfully is then veri-
fied for morphological correctness. If the word is 
not valid morphologically, then the window is 
moved back over 3 characters and at every step the 
validity of occurrence of the word is noted. Simi-
larly, the window is moved 3 characters ahead and 
the validity of the word is verified. All words 
formed successfully are taken and further 
processed using a language model that considers 
the bigram occurrence for each word. The un-
known word probability is considered here as well. 
The word with maximum probability is taken as 
valid in the given context.  
Let >< 321 www  be the word formed by the 
moving window. Then, the word selected, ws, is 
given by 
 
(vi) 
??
??
?
??
??
?
=
)(|)(
)(|)(
)(|)(
max
3
2
1
prev
prev
prev
s
wPwP
wPwP
wPwP
w  
where wprev  is the previous word. 
 
It is also noted that the number of times a transi-
tion happens from a syllable set with consonants to 
a syllable set with vowels, in a word, is no longer 
than four in most cases as noted below. This factor 
is also considered for terminating the Viterbi algo-
rithm for each word.  
 
 Ir | aad | ah - three transitions 
 
Some of the morphological rules considered 
while deciding the word boundaries are given be-
low. Word boundary is formed when  
1. The word ends with ''?? - un Gunna 
2. The character transitions over to digits 
3. Punctuations marks are encountered ('-' is 
also included) 
4. No two 'ye' - choti ye come back to back 
5. No characters occur in detached form un-
less they are initials or abbreviations fol-
lowed by a period 
6. If current character is 'alif' and the pre-
vious character is 'ee' - bari ye then the 
word boundary occurs after 'alif' 
Some of the drawbacks seen in this model are 
mainly on account of improper identification of 
proper nouns. If a proper noun is not well seg-
mented, the error propagates through the sentence 
and typically the next two or three words fail to get 
segmented correctly. Also, in Urdu, some words 
can be written in more than one ways. This mostly 
depends on the diacritics and ambiguity between 
bari and choti 'ye'. The training data as well as the 
test data were not normalized before training. The 
model shows a precision of 83%. We realized that 
the efficiency of this model can be improved if 
phoneme level transitions were taken into consid-
eration. Training has to be increased over more 
proper nouns and a lexicon for proper nouns loo-
kup has to be maintained. Diacritics that are typi-
cally used for beautification should be removed. 
Words across the documents need to be normalized 
to one accepted format to assure uniqueness.  This 
involves considerable amount of work and hence, 
in order to prevent the propagation of error into the 
NLP text analysis pipeline, we decided to test our 
subsequent models using pre-segmented data, in-
dependent of our word segmentation model. 
4 Learning Approaches  
A Conditional Random Field (CRF), is an undi-
rected graphical model used for sequential learn-
ing. The tasks of POS tagging and NE tagging are 
both sequential learning tasks and hence this learn-
ing approach is a reasonable choice. What follows 
is a brief outline about CRF. Interested readers are 
referred to Lafferty et al, (2001), for more infor-
mation on CRF.  
4.1 Conditional Random Fields (CRF) 
64
A linear chain CRF defines a single log-linear 
probabilistic distribution over the possible tag se-
quences y for a sentence x 
??
= =
?=
T
t
K
k
tttkk xyytfxZ
xyp
1 1
1 ),,,(exp)(
1)|( ?  
where  fk(t, yt, yt-1, xt) is typically a binary function 
indicating the presence of feature k, ?k is the weight 
of the feature, and Z(x) is a normalization function. 
? ??
= =
?=
y
T
t
K
k
tttkk xyytfxZ
1 1
1 ),,,(exp)( ?  
This modeling allows us to define features on 
states (the POS/NE tags) and edges (pairs of adja-
cent POS/NE tags) combined with observations 
(eg. words and POS tags for NE estimation). The 
weights of the features are determined such that 
they maximize the conditional log-likelihood of the 
training data:  
( )? == i ii xypL 1 )()( )|(log)( ?? .  
For the actual implementation, CRF++10, an 
open source tool that uses the CRF learning algo-
rithm is used. The L-BFGS algorithm11 is used for 
optimization. 
4.2 %E Tagging using POS information 
POS tagging is a precursor for all text analysis 
tasks. Assigning POS tags to words without any 
ambiguity depends on contextual information and 
extracting this information is a challenge. For a 
language like English, several techniques have 
been proposed that can be broadly classified into 
statistical, rule based and hybrid approaches (Ek-
bal, 2007). The general consensus is that ap-
proaches like MEMM and HMM, that work well 
for Hindi, would work well for Urdu as well, since 
Urdu is grammatically similar to Hindi (Platts, 
1909).  However, the linguistic and morphological 
rules used in the post processing steps differ from 
Hindi because of Urdu?s borrowed vocabulary and 
                                                           
10 http://crfpp.sourceforge.net/ 
11 http://www.mcs.anl.gov/index.php 
style of writing from Arabic. Also, the requirement 
for such models to work well is the availability of 
large training data. 
Building NE recognizers for languages like Ur-
du is difficult as there are no concepts like capitali-
zation of characters. Also, most names of people 
have specific meanings associated with them and 
can easily be found in a dictionary with different 
associated meanings. Various learning approaches 
have been proposed for this task, HMM based 
learning approach (Bikel et al, 1999), Maximum 
Entropy Approach (Borthwick, 1999) and CRF 
approach (McCallum, 2003) are the most popular. 
Ashish et al, (2009) show an SVM based approach 
also works well for such tasks. To overcome the 
problem of limited data availability, we present a 
method to increase the amount of training data that 
is available, by using a technique called bootstrap-
ping. 
We do not have a training corpus that is manual-
ly tagged for both POS and NE. Our training data 
consists of two different datasets. The dataset used 
for POS tagging is provided by CRULP and is 
tagged using their tagset. The dataset used for NE 
tagging is provided by CRL as a part of their Urdu 
resource package. The CRL tagset consists of 
LOCATION, PERSON, ORGANIZATION, DATE 
and TIME tags. We use only the first three tags in 
this work. 
Our aim is to achieve effective POS tagging and 
NE tagging by maximizing the use of the available 
training data. The CRULP dataset (which we call 
datasetPOS) is a corpus of 150,000 words that are 
only POS tagged and the CRL dataset (which we 
call datasetNE) is a corpus of 50,000 words that are 
only NE tagged. First, we trained a CRF model on 
datasetNE that uses only the NE information to per-
form NE recognition. This one stage model was 
not effective due to the sparseness of the NE tags 
in the dataset. The model requires more data while 
training. The obvious and frequently tried ap-
proach (Thamar, 2004) is to use the POS informa-
tion.  
Figure 1 shows a two stage model that uses POS 
information to perform NE tagging. The first stage 
POSA performs POS tagging by using a CRF 
trained model to assign POS tags to each word in a 
sentence of datasetNE. The second stage NEA per-
forms NE tagging by using another CRF trained 
model that uses both the POS information as well 
65
as the NE information, to perform effective NE 
tagging. 
 
 
Figure 1. Two stage model for NE tagging using POS 
information 
 
However, although the accuracy of NE tagging 
improved over the one stage model, there was 
scope for further improvement. It is obvious that 
all the NE tagged words should have the proper 
noun (NNP) POS tag associated. But, when POS 
tags were generated for the NE tagged ground truth 
data in datasetNE, most of the words were either 
tagged as adjectives (JJ) or common nouns (NN).  
Most tags that come after case markers (CM) were 
adjectives (JJ) in the training data. Very few ac-
counted for proper nouns after case markers. This 
adversely affected the NE tagger output. It was 
also noticed that the POS tagger tagged most of the 
proper nouns (NNP) as common nouns (NN) be-
cause of the sparseness of the proper noun tag in 
the POS ground truth data set datasetPOS. This ob-
servation made us look to bootstrapping techniques 
for effective learning.  
We propose a four stage model as shown in Fig-
ure 2, for NE tagging. Three of the stages are 
trained using the CRF learning approach and one 
stage uses a rule based approach.  All four stages 
are trained using unigram features on tags and 
words and bigram features on tags. The POS 
tagged dataset, datasetPOS, consists of words and 
associated POS tags and the NE tagged dataset, 
datasetNE, consists of words and associated NE 
tags. We divide both datasets into training and test-
ing partitions. datasetPOS is divided into trainsetPOS 
and testsetPOS and datasetNE is divided into train-
setNE and testsetNE. 
 
 
Figure 2. Four stage model for NE tagging using POS 
information with bootstrapping 
 
In the model shown in Figure 2, POSA stage is a 
CRF based stage that is trained using trainsetPOS. 
Once trained, the POSA stage takes as input a sen-
tence and generates the associated POS tag for 
each word in that sentence.  
In order to increase the NNP tag associations to 
improve NE tagging, we generate POS tags for the 
NE training data in trainsetNE using the POSA 
stage. The POS tags generated at the POSA stage 
are called POSint. The POScorrection stage takes as 
input trainsetNE along with its associated POS tags, 
POSint. At this stage, correction rules - that change 
the POS tags of NE associated words to proper 
noun (NNP), assign Case Markers (CM) before 
and after the NE tags and verify proper tagging of 
Cardinals (CD) - are applied. The corrected POS 
tags are called POScorrected. A consolidated POS 
training set consisting of entries from both train-
setPOS and trainsetNE (with POScorrected generated as 
output from the POScorrection stage) is used to train 
the CRF based POSB stage. This stage is the final 
POS tagging stage. Test data consisting of sen-
tences (words) from testsetNE is sent as input to 
stage POSB and the output generated at stage POSB 
is the POS tag associated with each input word of a 
sentence. The NEB stage is a CRF based NE tagger 
that is trained on a dataset consisting of word and 
associated NE tags from trainsetNE and associated 
POS tags from POScorrected. This stage learns from 
the POS information and the NE information pro-
vided in the training data. Once trained, the NEB 
stage takes as input words from testsetNE and asso-
ciated POS tags (obtained at stage POSB) and ge-
nerates NE tags. 
The domain we are interested in is newswire 
material, and these articles are written in the ?jour-
66
nalistic? or ?news writing? style12. The articles are 
objective and follow a Subject-Object-Verb struc-
ture. Related information is usually presented with-
in close sentence proximity. This makes it possible 
to hand-craft grammar rules for the discovery of 
NE tags with fine granularity. The final POS 
tagged and NE tagged data generated as outputs at 
stage POSB and stage NEB respectively of the four 
stage model, are processed using rules and lexicon 
lookups to further improve the overall tagging ac-
curacy of the model. Rules used are mostly domain 
specific. The rules were applied to the model using 
Semantex. 
4.3 Rules for POS Tagging 
1. Our model tags all the Question Words 
(QW) like ????? - kya as pronoun (PR). All 
such occurrences are assigned QW tag. 
2. If the word is ????? ? kya and the previous 
tag is an adjective (JJ) and the next tag is a 
phrase marker (PM) then assign a light 
verb tag (VBL) else assign a verb (VB) tag 
to the word. 
3. It was observed that there were spurious 
instances of proper nouns getting tagged as 
nouns. In order to correct this error, if a 
word ends with any of the characters 
shown below, and the word was tagged as 
a noun, then the tag on the word was 
changed to a proper noun.  
?%?, ??? ,???, ?()?, ?*+?, 
?,-?, ????, ?  0*- ?, ???? 
4. All valid cardinals were tagged as nouns or 
proper nouns by the model. This was re-
solved by looking for a digit in the string.  
4.4 Rules for %E Tagging 
1. Words like ?????? (court), ??????? (bu-
reau), ????? (army) etc. are looked up. If 
there are any nouns or proper nouns above 
these within a window of two, then the tag 
on this word is ORGANIZATION. 
2. Words like ??????? (organization), ?????? 
are marked ORGANIZATION if the pre-
vious word is a proper noun. 
3. Lexicon look up for names of places is per-
formed and the POS tag of the next word 
that is found is checked. If this tag is a 
                                                           
12 http://en.wikipedia.org/wiki/News_writing 
Case Marker (CM) with a feminine gend-
er, like ???? (main) or ?????, then the 
word is marked with a LOCATION tag. 
4. If a proper noun that is selected ends with 
a suffix ?pur?, ?bad, ?dad? and has the 
same constraint as mentioned in rule 3, 
then the LOCATION tag is assigned to it 
as well. 
5 Results 
The NE tagging performance, for both the two 
stage model and the four stage model, are eva-
luated using Precision (P), Recall (R) and F-Score 
(FS) metrics, the equations for which are given 
below. 
(vii) NEs  taggedof No.
NEs taggedcorrectly  of No. P =  
(viii) setin test  NEs of no. Total
NEs  taggedof No.R =  
(ix) 
 PR
RPFS +=
2  
 
We performed a 10 fold cross validation test to 
determine the performance of the model. The data-
set is divided into 10 subsets of approximately 
equal size. One subset is withheld for testing and 
the remaining 9 subsets are used for training. This 
process is repeated for all 10 subsets and an aver-
age result is computed. The 10 fold validation test 
for NE tagging was performed for both the two 
stage as well as the four stage models. 
 
Set P R FS P R FS
1 48.09 73.25 58.06 60.54 78.7 68.44
2 38.94 72.42 50.65 60.29 80.46 68.93
3 56.98 74.38 64.53 60.54 79.74 68.83
4 38.44 78.05 51.51 60.54 80.79 69.21
5 32.29 75.91 45.31 60.79 80.34 69.21
6 44.82 88.02 59.4 59.31 79.93 68.09
7 45.75 69.75 55.26 61.04 81.73 69.89
8 43.52 71.5 54.11 60.05 80.36 68.74
9 44.64 81.97 57.8 59.93 81.09 68.92
10 44.17 78.18 56.45 60.67 79.22 68.72
Avg 43.764 76.343 55.308 60.37 80.236 68.898
Four Stage ModelTwo Stage Model
 
Table 1. NE tagging results for the two stage and four 
stage models 
 
It can be seen from Table 1 that the four stage 
model outperforms the two stage model with the 
67
average F-Score being 55.31% for the two stage 
model and 68.89% for the four stage model. 
Table 2 shows the POS tagging results for stages 
POSA and POSB. The POSB stage performs margi-
nally better than the POSA stage. 
 
Set P Set P
1 84.38 1 83.97
2 89.32 2 89.84
3 88.09 3 88.48
4 89.45 4 89.66
5 89.66 5 89.76
6 90.57 6 90.63
7 81.1 7 89.24
8 89.47 8 89.5
9 89 9 89.12
10 89.12 10 89.25
Avg 88.016 Avg 88.945
POSB ResultsPOSA Results
 
Table 2. POS tagging results for the two stage (POSA) 
and four stage (POSB) models 
 
Although for POS tagging, the improvement is 
not very significant between the two models, tags 
like light verbs (VBLI), auxiliary verbs (AUXA 
and AUXT), adjectives (JJ), demonstratives (DM) 
and nouns (NN, NNC, NNCM, NNCR) get tagged 
with higher accuracy in the four stage model as 
shown in Table 3. This improvement becomes evi-
dent in the NE test set. Unfortunately, since this 
data has no associated POS tagged ground truth, 
the results cannot be quantified. The trainsetPOS 
training data had very few instances of proper 
nouns (NNP) occurring after case markers (CM) 
and so most of the proper nouns were getting 
tagged as either adjectives (JJ) or common nouns 
(NN). After providing more training data to stage 
POSB, the model could effectively learn proper 
nouns. Spurious tagging of adjectives (JJ) and 
common nouns (NN) reduced while more proper 
nouns (NNP, NNPC) were tagged accurately and 
this allowed the NE stage to apply its learning effi-
ciently to the NE test set thereby improving the NE 
tagging results.  
The two stage model tagged 238 NE tagged 
words as proper nouns out of 403 NE words. The 
four stage model tagged 340 NE tagged words as 
proper nouns out of 403 NE words. The four stage 
model shows an improvement of 25.3% over the 
two stage model. The results reported for NE and 
POS tagging models are without considering rules 
or lexicon lookups. 
 
Tag FS Tag FS
AUXA 0.801 AUXA 0.816
AUXT 0.872 AUXT 0.898
DM 0.48 DM 0.521
JJ 0.751 JJ 0.765
NN 0.85 NN 0.858
NNC 0.537 NNC 0.549
NNCM 0.909 NNCM 0.923
NNCR 0.496 NNCR 0.51
RB 0.785 RB 0.834
VBLI 0.67 VBLI 0.693
VBT 0.553 VBT 0.586
POSA Output POSB Output
 
Table 3. POS tagging results for stages POSA and POSB 
 
In order to further improve the POS tagged re-
sults and NE tagged results, the rules mentioned in 
sections 4.3 and 4.4 and lexicon lookups were ap-
plied. Table 4 shows the result for NE tagging with 
an overall F-Score of 74.67% 
 
Tag P R FS
LOCATION 0.78 0.793 0.786
ORGANIZATION 0.775 0.731 0.752
PERSON 0.894 0.595 0.714
NEA Output
 
Table 4. NE tagging results after applying rules for test 
results in Table 1 
6. Conclusion and Future Work  
This work was undertaken as a precursor to 
achieve our final objective as discussed in Section 
1. The basic idea here is to increase the size of the 
available training data, by using bootstrapping, so 
as to maximize learning for NE tagging. The pro-
posed four stage model shows an F-Score of 68.9% 
for NE tagging which is much higher than that ob-
tained by the simple two stage model. 
A lot of avenues remain to be explored to fur-
ther improve the performance of the model. One 
approach would be to use the bootstrapping tech-
nique for NE data as well. However, the rules re-
quired can be complicated. More hand crafted rules 
and detailed lexicon lookups can result in better 
NE tagging. We have also noticed certain ambigui-
ties in tagging PERSON and LOCATION. Rules 
that resolve this ambiguity can be explored. 
68
References  
Raymond G. Gordon Jr. (ed.). 2005. Ethnologue: Lan-
guages of the World, Fifteenth edition. Dallas, TX.: 
SIL International 
Kashif Huda. 2001. An Overview of Urdu on the Web. 
 Annual of Urdu Studies Vol 20. 
Zaheer Ahmad, Jehanzeb Khan Orakzai, Inam Shamsh-
er, Awais Adnan. 2007. Urdu astaleeq Character 
Recognition. Proceedings of World Academy of 
Science, Engineering and Technology. Volume 26, 
ISSN 2070-3740. 
John T. Platts. 1967. A grammar of the Hindustani or 
Urdu language.  Munshiram Manoharlal Delhi. 
R. L. Schmidt. 1999. Urdu: an essential grammar. 
London: Routledge. 
Sarmad Hussain. 2008. Resources for Urdu Language 
Processing. The 6th Workshop on Asian Language 
Resources. 
P. Baker, A. Hardie, T. McEnery, B.D. Jayaram. 2003. 
Corpus Data for South Asian Language Processing. 
Proceedings of the 10th Annual Workshop for South 
Asian Language Processing, EACL. 
M. Humayoun, H. Hammarstrm, A. Ranta. 2007. Urdu 
Morphology, Orthography and Lexicon Extraction. 
CAASL-2: The Second Workshop on Computational 
Approaches to Arabic Script-based Languages, LSA 
2007 Linguistic Institute, Stanford University. 
Waseem Siddiqi, Shahab Alam. 2008. Online Urdu-
English and English-Urdu dictionary. 
N. Durrani. 2007. Typology of Word and Automatic 
Word Segmentation in Urdu Text Corpus. National 
University of Computer and Emerging Sciences, La-
hore, Pakistan. 
T. Theeramunkong, S. Usanavasin. 2001. on-
Dictionary Based Thai Word Segmentation Using 
decision trees. In proceedings of the First Interna-
tional Conference on Human Language Technology 
Research, San Diego, California, USA. 
A. Pornprasertkul. 1994. Thai Syntactic Analysis. Ph.D 
Thesis, Asian Institute of Technology. 
Ismat Javed. 1981.  ??  ????? ????. Taraqqi Urdu Bureau, 
New Delhi. 
Abdul M. Haq. 1987.  ????  ??? ? ???. Amjuman-e-
Taraqqi Urdu (Hindi). 
Hassan Sajjad. 2007. Statistical Part of Speech Tagger 
for Urdu. National University of Computer and 
Emerging Sciences, Lahore, Pakistan. 
John D. Lafferty, Andrew McCallum, Fernando C.N. 
Pereira. 2001. Conditional Random Fields: Probabi-
listicModels for Segmenting and Labeling Sequence 
Data. Proceedings of the Eighteenth International 
Conference on Machine Learning, pp. 282-289. 
John Chen. 2006. How to use Sequence Tagger. Seman-
tex Documentation, Janya Inc. 
Bikel, D.M., Schwartz, R.L., Weischedel, R.M.1999. 
An Algorithm that Learns What?s in a ame. Ma-
chine Learning 34(1-3), pp. 211?231. 
Borthwick, A. 1999. Maximum Entropy Approach to 
amed Entity Recognition. PhD thesis, New York 
University. 
McCallum, A., Li, W. 2003. Early results for amed 
Entity Recognition with Conditional Random Fields, 
Feature Induction and Web-enhanced Lexicons. In 
Proceedings of CoNLL. 
A. Hardie. 2003. Developing a tagset for automated 
part-of-speech tagging in Urdu. Department of Lin-
guistics and Modern English Language, University 
of Lancaster. 
Leech, G and Wilson, A. 1999. Standards for tagsets. 
Edited version of EAGLES Recommendations for the 
Morphosyntactic Annotation of Corpora. In van Hal-
teren, H (ed.) Syntactic wordclass tagging. Dor-
drecht: Kluwer Academic Publishers. 
Awaghad Ashish Krishnarao, Himanshu Gahlot, Amit 
Srinet and D. S. Kushwaha.  2009. A Comparative 
Study of amed Entity Recognition for Hindi Using 
Sequential Learning Algorithms. In IEEE Interna-
tional Advance Computing Conference (IACC '09), 
Thapar University, India. March 6-7. 
Thamar Solario. 2004. Improvement of amed Entity 
Tagging by Machine Learning, Technical Report 
CCC-04-004, Coordinacin de Ciencias Computatio-
nales. 
Ekbal, A. and Bandyopadhyay, S. 2007. A Hidden 
Markov Model Based amed Entity Recognition Sys-
tem: Bengali and Hindi as Case Studies. Springer 
LNCS, Vol. 4815, pp. 545. 
R. K. Srihari, W. Li, C. Niu and T. Cornell,"InfoXtract: 
A Customizable Intermediate Level Information Ex-
traction Engine," Journal of atural Language En-
gineering, Cambridge U. Press, 14(1), 2008, pp..33-
69. 
 
69
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 46?51,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Making Semantic Topicality Robust Through Term Abstraction?
Paul M. Heider
Department of Linguistics
University at Buffalo
The State University of New York
Buffalo, NY 14260, USA
pmheider@buffalo.edu
Rohini K. Srihari
Janya, Inc.
1408 Sweet Home Road
Suite 1
Amherst, NY 14228
rohini@cedar.buffalo.edu
Abstract
Despite early intuitions, semantic similarity
has not proven to be robust for splitting multi-
party interactions into separate conversations.
We discuss some initial successes with using
thesaural headwords to abstract the seman-
tics of an utterance. This simple profiling
technique showed improvements over base-
line conversation threading models.
1 Introduction
Topic segmentation is the problem of dividing a
document into smaller coherent units. The seg-
ments can be hierarchical or linear; the topics can
be localized or distributed; the documents can be
newswire or chat logs. Of course, each of these
variables is best analyzed as continuous rather than
discrete. Newswire, for instance, is a more formal,
monologue-style genre while a chat log tends to-
wards the informal register with different conversa-
tions interwoven.
We present a topic segmenter which uses seman-
tics to define coherent conversations within a larger,
multi-party document. Using a word?s thesaurus en-
try as a proxy for its underlying semantics provides
a domain-neutral metric for distinguishing conver-
sations. Also, our classifier does not rely on met-
alinguistic properties that may not be robust across
genres.
? The first author was partially funded through a fel-
lowship from the SUNY at Buffalo Department of Linguis-
tics and partially through a research assistantship at Janya,
Inc. (http://www.janyainc.com, Air Force Grant No.s
FA8750-07-C-0077 and FA8750-07-D-0019, Task Order 0004)
2 Background
Most work on lexical cohesion extends from Halli-
day and Hasan (1976). They formalize a text as any
semantic unit realized through sentences. Linguistic
features found to justify binding sentences together
into Halliday and Hasan?s notion of a text include
pronouns (Hobbs, 1979; Kehler, 2000), lexical over-
lap (Hearst, 1997; Kozima, 1993; Morris and Hirst,
1991), cue phrases (Manning, 1998), and discourse
markers (Power et al, 2003; Reynar, 1999; Beefer-
man et al, 1999), among others. Of course, most
of this earlier work assumes the sentences constitut-
ing any text are contiguous. Thus, a document is
comprised of a series of semantic units that progress
from one to the next with no returns to old topics.
Multi-party interactions1 abide by a different set
of assumptions. Namely, a multi-party interaction
can include multiple floors (Aoki et al, 2006). Much
like at a cocktail party, we can expect more than a
single conversation at every given time. These dif-
ferent conversational floors are the major semantic
units a topic segmentation algorithm must recog-
nize. Spoken chat models (Aoki et al, 2006; Aoki
et al, 2003) can make a simplifying assumption that
speakers tend to only participate in one conversation
at a time. However, in text chat models, Elsner and
Charniak (2008) show that speakers seem to partici-
pate in more conversations roughly as a function of
how talkative they are (cf. Camtepe et al, 2005).
In both modalities, speaker tendency to stay on the
same topics is a robust cue for conversational coher-
1See O?Neill and Martin (2003) for an analysis of differ-
ences between two- and multi-party interactions.
46
ence (Elsner and Charniak, 2008; Acar et al, 2005).
Despite the initial intuitions of Halliday and
Hasan (1976), semantic similarity has not proven
to be a robust cue for multi-party topic segmen-
tation. For instance, Acar et al (2005) and Gal-
ley et al (2003) used word repetition in their defi-
nition of coherence but found that words common
to too many conversations hurt modeling perfor-
mance. Elsner and Charniak (2008) used frequency
binning based on the entire document to reduce the
noise introduced by high-frequency words. Un-
fortunately, binning requires a priori knowledge of
the relative frequencies of words.2 Additionally,
those authors used an on-topic/off-topic word list
to bifurcate technical and non-technical utterances.
Again, this technique assumes prior knowledge of
the strongest on-topic cue words.
Since semantic repetition is clearly useful but
simple word repetition is not a reliable measure, we
investigated other measures of semantic relatedness.
Elsner and Charniak (2008) conceded that context-
based measures like LSA (Deerwester et al, 1990)
require a clear notion of document boundary to func-
tion well. Dictionary-based models (Kozima and
Furugori, 1993) are a step in the right direction be-
cause they leverage word co-occurrence within defi-
nitions to measure relatedness. The richer set of con-
nections available in WordNet models should pro-
vide an even better measure of relatedness (Sussna,
1993; Resnik, 1995). Unfortunately, these mea-
sures have unequal distribution by part-of-speech
and uneven density of lemmas by semantic domain.3
Thesaurus-based models (Morris and Hirst, 1991)
provide many of the same advantages as dictionary-
and WordNet-based models.4 In addition to the hier-
archical relations encoded by the thesaurus, we can
treat each thesaural category as one dimension of
a topicality domain similar to the way Elsner and
Charniak leveraged their list of technical terms. In
sum, our model focuses on the abstraction of lem-
mas that is inherent to a thesaurus while limiting
the domain-specific and a priori knowledge required
2One could use frequencies from a general corpus but that
should only perform as well as a graded stop-word list.
3As one reviewer noted, some parts-of-speech may con-
tribute more to a topic profile than others. Unfortunately, this
empirical question must wait to be tested.
4Budanitsky and Hirst (2006) review the advantages.
by a classifier to divide multi-party interactions into
separate conversational floors.
3 Model
At a high level, our chat topic segmenter works like
most other classifiers: each input line is tokenized5,
passed to a feature analyzer, and clustered with re-
lated lines. Unlike traditional topic segmentation
models, each input represents a new utterance in the
chat log. These utterances can range from single
words to multiple sentences. Another aspect of our
model (although not unique to it) is the on-line clas-
sification of text. We aim to model topic segmenta-
tion as if our classifier were sitting in a chat room,
and not as a post-process.
While our feature analyzer focuses on semantic
markers, interlocutor names are also recorded. Two
intuitions were implemented with respect to individ-
uals? names: continued affiliation with old conver-
sations and naming interlocutors to focus attention.
All else being equal, one would assume a speaker
will continue in the conversations she has already
participated in. Moreover, she will most likely con-
tinue with the last conversation she was part of. As
the total number of conversations increases, the like-
lihood of sticking to the last conversation will de-
crease.
The second intuition derives from the observa-
tion in O?Neill and Martin (2003) that speakers ac-
commodate for cocktail-style conversations by using
direct mentions of interlocutors? names. We only
model backward referencing names. That is, if a
speaker uses the name of another user, we assume
that the speaker is overtly affiliating with a conver-
sation of the other user. Forward referencing is dis-
cussed under future work (see Section 6).
Following Budanitsky and Hirst (2006), we base
our notion of semantic topicality on thesaural rela-
tions. Broadly speaking, two utterances are highly
related if their tokenized words (hereafter, lemmas)
co-occur in more of the same thesaural categories
than not. We will defer further explanation of these
features until we have explained our reference the-
sauri in Subsection 3.1. Unfortunately, many desir-
able and robust features are missing from our classi-
fier. See Section 6 for a discussion of future work.
5We used SemantexTM (Srihari et al, 2008).
47
In the final stage of our processing pipeline, we
use a panel of experts to generate a simple weighted
classification. Each feature described above con-
tributes a roughly equal vote towards the final sort-
ing decision. Barring a single strong preference or a
cohort of weak preferences for one conversation, the
model assumes the incoming utterance introduces a
new conversational floor.
3.1 Thesauri
We chose two machine-readable and public-domain
thesauri for our model: Roget?s Thesaurus (1911)
and Moby Thesaurus II (2002). Both are avail-
able from Project Gutenberg (gutenberg.org). In the
compilation notes for Roget?s Thesaurus, the editor
mentions a supplement of 1,000+ words to the orig-
inal work. A rough count shows 1,000 headwords
(the basic grouping level) and 55,000 synonyms (any
word listed under a headword). The second edi-
tion of Moby Thesaurus contains some 30,000 head-
words and 2.5 million synonyms. Moby Thesaurus
includes many newer terms than Roget?s Thesaurus.
Structurally, Roget?s Thesaurus has a distinct advan-
tage over Moby Thesaurus. The former includes a
six-tiered category structure with cross-indexing be-
tween headwords. The latter is only organized into
headword lists.
3.2 Metrics
As we mentioned above, our model uses three pri-
mary metrics in classifying a new utterance: con-
versation affiliations of the current speaker, conver-
sation affiliations of any explicitly mentioned inter-
locutors, and semantic similarity. In the end, all the
conversation affiliation votes are summed with the
one conversation preferred by each of the three the-
saural measures.6 The input line is then merged with
the conversation that received the most votes. De-
tails for deriving the votes follow.
Every conversation a speaker has participated in
receives a vote. Moreover, his last conversation gets
additional votes as a function of his total number
of conversations (see Equation 1). Likewise, every
conversation a named interlocutor has participated
in receives a vote with extra votes given to her last
conversation as a function of how gregarious she is.
6In the long run, a list of conversations ranked by similarity
score would be better than a winner-takes-all return value.
Headword Type Weight Change
Direct Match 1
Co-hyponymous Headword 0.25
Cross-indexed Headword 0.75
Table 1: Spreading Activation Weights.
V ote = 3ln(|Conversationsspeaker|) + 1 (1)
Each utterance is then profiled in terms of the-
saural headwords. Every lemma in an utterance
matching to some headword increments the activa-
tion of that headword by one.7 A conversation?s se-
mantic profile is a summation of the profiles of its
constituent sentences. In order to simulate the drift
of topic in a conversation, the conversation?s seman-
tic profile decays with every utterance. Thus, more
recent headwords will be more activated than head-
words activated near the beginning of a conversa-
tion. Decay is modeled by halving the activation of
a headword in every cycle that it was not topical.
Moreover, a third profile was kept to simulate
spreading activation within the thesaurus. For this
profile, each topical headword is activated. Every
cross-indexed headword listed within this category
is also augmented by a fixed degree. Finally, every
headword that occupies the same thesaurus section
is augmented. An overview of the weight changes
is listed in Table 1. The specific weights fit the au-
thors? intuitions as good baselines. These weights
can easily be trained to generate a better model.
The similarity between a new line (the test) and
a conversation (the base) is computed as the sum of
match bonuses and mismatch penalties in Table 2.8
Table 3 scores an input line (TEST) against two con-
versations (BASE1 and BASE2) with respect to four
headwords (A, B, C, and D). In order to control for
text size, we also computed the average headword
7Most other models include an explicit stop-word list to re-
duce the effect of function words. Our model implicitly relies
on the thesaurus look-up to filter out function words. One ad-
vantage to our approach is the ability to preferentially weight
different headwords or lemma to headword relations.
8Like with Table 1, these numbers reflect the authors? intu-
itions and can be improved through standard machine learning
methods.
48
Headword Test
Present Yes NoAvg? Above Below
Ba
se Yes
Above +1 +0.5 -0.1
Below +0.5 +1 -0.05
No -1 -0.5 +0.0001
Table 2: Similarity Score Calculations.
A B C D Score
TEST high high low 0 ?
BASE1 high low low high+1 +.5 +1 -.1 2.4
BASE2 0 high 0 0-1 +1 -.5 +.0001 -.4999
Table 3: A Example Similarity Scoring for Two Conver-
sations. ?High? and ?low? refers to headword activation.
activation in a conversation. Intuitively, we consider
it best when a headword is activated for both the
base and test condition. Moreover, a headword with
equally above average or equally below average acti-
vation is better than a headword with above average
activation in the base but below average activation
in the test. In the second best case, neither condition
shows any activation in a headword. The penulti-
mately bad condition occurs when the base contains
a headword that the test does not. We do not want
to penalize the test (which is usually smaller) for not
containing everything that the base does. Finally, if
the test condition contains a headword but the base
does not, we want to penalize the conversation most.
4 Dataset
Our primary dataset was distributed by Elsner and
Charniak (2008). They collected conversations from
the IRC (Internet Relay Chat) channel ##LINUX,
a very popular room on freenode.net with widely
ranging topics. University students then annotated
these chat logs into conversations. We take the col-
lection of these annotations to be our gold standard
for topic segmentation with respect to the chat logs.
4.1 Metrics
Elsner and Charniak (2008) use three major mea-
sures to compare annotations: a 1-to-1 comparison,
E&C Annotators Our Model
Mean Max Min
Conversations 81.33 128 50 153
Avg. Length 10.6 16.0 6.2 5.2
Table 4: General statistics for our model as compared
with Elsner and Charniak?s human annotators. Some
numbers are taken from Table 1 (Elsner and Charniak,
2008).
Mean Max Min
Inter-annotator 86.70 94.13 75.50
Our Model 65.17 74.50 53.38
Table 5: Comparative many-to-1 measures for evaluating
differences in annotation granularity. Some numbers are
taken from Table 1 (Elsner and Charniak, 2008).
a loc3 comparison, and a many-to-1 comparison.
The 1-to-1 metric tries to maximize the global con-
versation overlap in two annotations. The loc3 scale
is better at measuring local agreement. This score
calculates accuracy between two annotations for
each window of three utterances. Slight differences
in a conversation?s start and end are minimized. Fi-
nally, the many-to-1 score measures the entropy dif-
ference between annotations. In other words, sim-
plifying a fine-grained analysis to a coarse-grained
analysis will yield good results because of shared
major boundaries. Disagreeing about the major con-
versation boundaries will yield a low score.
5 Analysis
Compared with the gold standard, our model has a
strong preference to split conversations into smaller
units. As is evident from Table 4, our model has
more conversations than the maximally splitting hu-
man annotator. These results are unsurprising given
that our classifier posits a new conversation in the
absence of contrary evidence. Despite a low 1-to-1
score, our many-to-1 score is relatively high (see Ta-
ble 5). We can interpret these results to mean that
our model is splitting gold standard conversations
into smaller sets rather than creating conversations
across gold standard boundaries.
A similar interaction of annotation granularity
shows up in Table 6. Our 1-to-1 measures are just
barely above the baseline, on average. On the other
49
As % of . . . Misclassified All
Error Type Mean Max Min Mean
Mismatch 25.84 23.78 28.13 11.93
Split Error 62.96 63.11 63.89 29.08
Lump Error 11.20 13.11 7.99 5.17
Table 7: Source of misclassified utterances as a percent-
age of misclassified utterances and all utterances.
hand, our loc3 measure jumps much closer to the
human annotators. In other words, the maximum an-
notation overlap of our model and any given human
is poor9 while the local coherence of our annotation
with respect to any human annotation is high. This
pattern is symptomatic of over-splitting, which is ex-
cessively penalized by the 1-to-1 metric.10
We also analyzed the types of errors our model
made while holding the conversation history con-
stant. We simulated a consistent conversation his-
tory by treating the gold standard?s choice as an un-
beatable vote and tabulating the number of times our
model voted with and against the winning conversa-
tion. There were five numbers tabulated: matching
new conversation votes, matching old conversation
votes, mismatching old conversation votes, incorrect
split vote, and incorrect lump vote. The mismatch-
ing old conversation votes occurred when our model
voted for an old conversation but guessed the wrong
conversation. The incorrect split vote occurred when
our model wanted to create a new conversation but
the gold standard voted with an old conversation.
Finally, the incorrect lump vote occurred when our
model matched the utterance with a old conversation
when the gold standard created a new conversation.
Across all six gold standard annotations, nearly
two-thirds of the errors arose from incorrect splitting
votes (see Table 7). In fact, nearly one-third of all
utterances fell into this category.
6 Future Work
The high granularity for what our model considers a
conversation had a huge impact on our performance
9Elsner and Charniak (2008) found their annotators also
tended to disagree on the exact point when a new conversation
begins.
10Aoki et al (2006) present a thorough analysis of conver-
sational features associated with schisming, the splitting off of
new conversations from old conversations.
scores. The high many-to-1 scores imply that more
human-like chunks will improve performance. The
granularity may be very task dependent and so we
will need to be careful not to overfit our model to this
data set and these annotators. New features should
be tested with several chat corpora to better under-
stand the cue trading effects of genre.
At present, our model uses only a minimal set of
features. Discourse cues and temporal cues are two
simple measures that can be added. Our current fea-
tures can also use refinement. For instance, even par-
tially disambiguating the particular sense of the lem-
mas should reduce the noise in our similarity mea-
sures. Ranking the semantic similarity, in contrast
with the current winner-takes-all approach, should
improve our results. Accounting for forward refer-
encing, when a speaker invokes another?s name to
draw them into a conversation, is also important.
Finally, understanding the different voting pat-
terns of each feature system will help us to better
understand the reliability of the different cues. To-
wards this end, we need to monitor and act upon the
strength and type of disagreement among voters.
Acknowledgments
Harish Srinivasan was great help in tokenizing the
data. The NLP Group at Janya, Inc., Jordana Heller,
Jean-Pierre Koenig, Michael Prentice, and three
anonymous reviewers provided useful feedback.
References
Evrim Acar, Seyit Ahmet Camtepe, Mukkai S. Kr-
ishnamoorthy, and Blent Yener. 2005. Model-
ing and multiway analysis of chatroom tensors. In
Paul B. Kantor, Gheorghe Muresan, Fred Roberts,
Daniel Dajun Zeng, Fei-Yue Wang, Hsinchun Chen,
and Ralph C. Merkle, editors, ISI, volume 3495 of
Lecture Notes in Computer Science, pages 256?268.
Springer.
Paul M. Aoki, Matthew Romaine, Margaret H. Szyman-
ski, James D. Thornton, Daniel Wilson, and Allison
Woodruff. 2003. The Mad Hatter?s cocktail party: A
social mobile audio space supporting multiple simul-
taneous conversations. In CHI 03: Proceedings of the
SIGCHI Conference on Human Factors in Computing
Systems, pages 425?432, New York, NY, USA. ACM
Press.
Paul M. Aoki, Margaret H. Szymanski, Luke Plurkowski,
James D. Thornton, Allison Woodruff, and Weilie Yi.
50
Other Annotators E&C Model Our Model E&C Best Baseline
Mean 1-to-1 52.98 40.62 35.77 34.73
Max 1-to-1 63.50 51.12 49.88 56.00
Min 1-to-1 35.63 33.63 28.25 28.62
Mean loc3 81.09 72.75 68.73 62.16
Max loc3 86.53 75.16 72.77 69.05
Min loc3 74.75 70.47 64.45 54.37
Table 6: Metric values for our model as compared with Elsner and Charniak?s human annotators and classifier. Some
numbers are taken from Table 3 (Elsner and Charniak, 2008).
2006. Where?s the ?party? in ?multi-party?? Ana-
lyzing the structure of small-group sociable talk. In
ACM Conference on Computer Supported Coopera-
tive Work, pages 393?402, Banff, Alberta, Canada,
November. ACM Press.
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation. In Ma-
chine Learning, pages 177?210.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating WordNet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32(1):13?47.
Seyit Ahmet Camtepe, Mark K. Goldberg, Malik
Magdon-Ismail, and Mukkai Krishnamoorty. 2005.
Detecting conversing groups of chatters: A model, al-
gorithms, and tests. In IADIS AC, pages 89?96.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by Latent
Semantic Analysis. Journal of the American Society
of Information Science, 41:391?407.
Micha Elsner and Eugene Charniak. 2008. You talk-
ing to me? A corpus and algorithm for conversa-
tion disentanglement. In The Association for Compu-
tational Linguistics: Human Language Technologies
(ACL-HLT 2008), Columbus, Ohio.
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation of
multi-party conversation. In Proceedings of the ACL,
pages 562?569.
Michael Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman Group, New York.
Marti A. Hearst. 1997. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
J. R. Hobbs. 1979. Coherence and coreference. Cogni-
tive Science, 3:67?90.
A. Kehler. 2000. Coherence, Reference, and the Theory
of Grammar. CSLI Publications.
Hideki Kozima and Teiji Furugori. 1993. Similarity
between words computed by spreading activation on
an English dictionary. In Proceedings of 6th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL?93), pages 232?
239, Utrecht.
Hideki Kozima. 1993. Text segmentation based on sim-
ilarity between words. In Proceedings of ACL?93,
pages 286?288, Ohio.
C. D. Manning. 1998. Rethinking text segmentation
models: An information extraction case study. Tech-
nical Report SULTRY?98?07?01, University of Syd-
ney.
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indicator
of the structure of text. Computational Linguistics,
17(1):21?48.
Jacki O?Neill and David Martin. 2003. Text chat in ac-
tion. In GROUP ?03: Proceedings of the 2003 Inter-
national ACM SIGGROUP Conference on Supporting
Group Work, pages 40?49, New York, NY, USA. ACM
Press.
R. Power, D. Scott, and N. Bouayad-Agha. 2003.
Document structure. Computational Linguistics,
29(2):211?260.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 448?453.
Jeffrey C. Reynar. 1999. Statistical models for topic seg-
mentation. In Proceedings of the 37th Annual Meeting
of the ACL, pages 357?364, Maryland, USA, June.
Peter Mark Roget, editor. 1911. Roget?s Thesaurus.
Project Gutenberg.
R. K. Srihari, W. Li, C. Niu, and T. Cornell. 2008. Infox-
tract: A customizable intermediate level information
extraction engine. Journal of Natural Language Engi-
neering, 14(1):33?69.
Michael Sussna. 1993. Word sense disambiguation
for free-text indexing using a massive semantic net-
work. In Proceedings of the Second International
Conference on Information and Knowledge Manage-
ment (CIKMA?93), pages 67?74, Arlington, VA.
Grady Ward, editor. 2002. Moby Thesaurus List. Project
Gutenberg.
51
A Question Answering System Supported by Information Extraction* 
Rohini Srihari 
Cymfony Inc. 
5500 Main Street 
Williamsville, NY 14221 
rohini@cymfony.com 
Wei Li 
Cymfony Inc. 
5500 Main Street 
Williamsville, NY14221 
wei@cymfony.com 
Abstract 
This paper discusses an information 
extraction (IE) system, Textract, in natural 
language (NL) question answering (QA) and 
examines the role of IE in QA application. It 
shows: (i) Named Entity tagging is an 
important component for QA, (ii) an NL 
shallow parser provides a structural basis for 
questions, and (iii) high-level domain 
independent IE can result in a QA 
breakthrough. 
Introduction 
With the explosion of information in Internet, 
Natural language QA is recognized as a 
capability with great potential. Traditionally, 
QA has attracted many AI researchers, but most 
QA systems developed are toy systems or games 
confined to lab and a very restricted omain. 
More recently, Text Retrieval Conference 
(TREC-8) designed a QA track to stimulate the 
research for real world application. 
Due to little linguistic support from text 
analysis, conventional IR systems or search 
engines do not really perform the task of 
information retrieval; they in fact aim at only 
document retrieval. The following quote from the 
QA Track Specifications (www.research.att.com/ 
-singhal/qa-track-spec.txt) in the TREC 
community illustrates this point. 
Current information retrieval systems allow 
us to locate documents hat might contain the 
pertinent information, but most of them leave 
it to the user to extract he useful information 
from a ranked list. This leaves the (often 
unwilling) user with a relatively large 
amount of text o consume. There is an urgent 
need for tools that would reduce the amount 
of text one might have to read in order to 
obtain the desired information. This track 
aims at doing exactly that for a special (and 
popular) class of information seeking 
behavior: QUESTION ANSWERING. People 
have questions and they need answers, not 
documents. Automatic question answering 
will definitely be a significant advance in the 
state-of-art information retrieval technology. 
Kupiec (1993) presented a QA system 
MURAX using an on-line encyclopedia. This 
system used the technology of robust shallow 
parsing but suffered from the lack of basic 
information extraction support. In fact, the most 
siginifcant IE advance, namely the NE (Named 
Entity) technology, occured after Kupiec (1993), 
thanks to the MUC program (MUC-7 1998). 
High-level IE technology beyond NE has not 
been in the stage of possible application until 
recently. 
AskJeeves launched a QA portal 
(www.askjeeves.com). It is equipped with a 
fairly sophisticated natural language question 
parser, but it does not provide direct answers to 
the asked questions. Instead, it directs the user to 
the relevant web pages, just as the traditional 
search engine does. In this sense, AskJeeves has 
only done half of the job for QA. 
We believe that QA is an ideal test bed for 
demonstrating the power of IE. There is a natural 
co-operation between IE and IR; we regard QA 
as one major intelligence which IE can offer IR. 
* This work was supported in part by the SBIR grants F30602-98-C-0043 and F30602-99-C-0102 from Air Force 
Research Laboratory (AFRL)/IFED. 
166 
An important question then is, what type of 
IE can support IR in QA and how well does it 
support it? This forms the major topic of this 
paper. We structure the remaining part of the 
paper as follows. In Section 1, we first give an 
overview of the underlying IE technology which 
our organization has been developing. Section 2 
discusses the QA system. Section 3 describes the 
limitation of the current system. Finally, in 
Section 4, we propose a more sophisticated QA 
system supported by three levels of IE. 
1 Overview of Textract IE 
The last decade has seen great advance and 
interest in the area of IE. In the US, the DARPA 
sponsored Tipster Text Program \[Grishman 
1997\] and the Message Understanding 
Conferences (MUC) \[MUC-7 1998\] have been 
the driving force for developing this technology. 
In fact, the MUC specifications for various IE 
tasks have become de facto standards in the IE 
research community. It is therefore necessary to 
present our IE effort in the context of the MUC 
program. 
MUC divides IE into distinct tasks, 
namely, NE (Named Entity), TE (Template 
Element), TR (Template Relation), CO 
(Co-reference), and ST (Scenario Templates) 
\[Chinchor & Marsh 1998\]. Our proposal for 
three levels of IE is modelled after the MUC 
standards using MUC-style representation. 
However, we have modified the MUC IE task 
definitions in order to make them more useful 
and more practical. More precisely, we propose a
hierarchical, 3-level architecture for developing a 
kernel IE system which is domain-independent 
throughout. 
The core of this system is a state-of-the-art 
NE tagger \[Srihari 1998\], named Textract 1.0. 
The Textract NE tagger has achieved speed and 
accuracy comparable tothat of the few deployed 
NE systems, such as NetOwl \[Krupka & 
Hausman 1998\] and Nymble \[Bikel et al1997\]. 
It is to be noted that in our definition of NE, 
we significantly expanded the type of 
information to be extracted. In addition to all the 
MUC defined NE types (person, organization, 
location, time, date, money and percent), the 
following types/sub-types of information are also 
identified by the TextractNE module: 
? duration, frequency, age 
? number, fraction, decimal, ordinal, math 
equation 
? weight, length, temperature, angle, area, 
capacity, speed, rate 
? product, software 
? address, email, phone, fax, telex, www 
? name (default proper name) 
Sub-type information like company, 
government agency, school (belonging to the 
type organization) and military person, religious 
person (belonging to person) are also identified. 
These new sub-types provide a better foundation 
for defining multiple relationships between the 
identified entities and for supporting question 
answering functionality. For example, the key to 
a question processor is to identify the asking 
point (who, what, when, where, etc.). In many 
cases, the asking point corresponds to an NE 
beyond the MUC definition, e.g. the 
how+adjective questions: how long (duration or 
length), how far (length), how often (frequency), 
how old (age), etc. 
Level-2 IE, or CE (Correlated Entity), is 
concerned with extracting pre-defined multiple 
relationships between the entities. Consider the 
person entity as an example; the TextractCE 
prototype is capable of extracting the key 
relationships uch as age, gender, affiliation, 
position, birthtime, birth__place, spouse, 
parents, children, where.from, address, phone, 
fax, email, descriptors. As seen, the information 
in the CE represents a mini-CV or profile of the 
entity. In general, the CE template integrates and 
greatly enriches the information contained in 
MUC TE and TR. 
The final goal of our IE effort is to further 
extract open-ended general events (GE, or level 3 
IE) for information like who did what (to whom) 
when (or how often) and where. By general 
events, we refer to argument structures centering 
around verb notions plus the associated 
information of time/frequency and location. We 
show an example of our defined GE extracted 
from the text below: 
Julian Hill, a research chemist whose 
accidental discovery of a tough, taffylike 
compound revolutionized everyday life after 
it proved its worth in warfare and courtship, 
167 
died on Sunday in Hockessin, Del. 
\[1\] <GE_TEMPLATE> := 
PREDICATE: die 
ARGUMENTI: Julian Hill 
TIME: Sunday 
LOCATION: Hockessin, Del 
Figure 1 is the overall system architecture for 
the IE system Textract hat our organization has 
been developing. 
Kernet IE Modutes  L|ngui_sti_cLM_odu!es 
I . . . . . . . . . . . . .  I I . . . . . . . . . .  I 
I I I I ,,l l I , ! I 
I I I I 
'l J ' I I 
! I 
I I 
I I I I ,i I i ', I , 
I ! I I , l ? i  , , 
I I I I 
I I 
F L - -  - - - -~  . . . .  . L - -  - -  - -  . - -  - -  - -  | . . . .  
Apptication Modutes 
NE: NIiml~ EnlilyTitl~klll QA: Que~tlon Answering 
CE: Come,led Entity ExtrmClkm BR: In~lllgenl ~ws lng  
GE: Gcn~mI Evenl Ex~ct~on AS; Auio~ SUl len  
co :  ce-  mfcmnc ~1 momial l  s ~ p~ 
Figure 1: Textract IE System Architecture 
The core of the system consists of three 
kernel IE modules and six linguistic modules. 
The multi-level linguistic modules erve as an 
underlying support system for different levels of 
IE. The IE results are stored in a database which 
is the basis for IE-related applications like QA, 
BR (Browsing, threading and visualization) and 
AS (Automatic Summarization). The approach 
to IE taken here, consists of a unique blend of 
machine learning and FST (finite state 
transducer) rule-based system \[Roche & Schabes 
1997\]. By combining machine learning with an 
FST rule-based system, we are able to exploit he 
best of both paradigms while overcoming their 
respective weaknesses \[Srihari 1998, Li & Srihari 
2000\]. 
2 NE-Supported QA 
This section presents the QA system based on 
Named Entity tagging. Out of the 200 questions 
that comprised the TREC-8 QA track 
competition, over 80% asked for an NE, e.g. who 
(PERSON), when (T IME\ [  DATE), where 
(LOCATION), how far (LENGTH). Therefore, 
the NE tagger has been proven to be very helpful. 
Of course, the NE of the targeted type is only 
necessary but not complete in answering such 
questions because NE by nature only extracts 
isolated individual entities from the text. 
Nevertheless, using even crude methods like "the 
nearest NE to the queried key words" or "the NE 
and its related key words within the same line (or 
same paragraph, etc.)", in most cases, the QA 
system was able to extract ext portions which 
contained answers in the top five list. 
Figure 2 illustrates the system design of 
TextractQA Prototype. There are two 
components for the QA prototype: Question 
Processor and Text Processor. The Text Matcher 
module links the two processing results and tries 
to find answers to the processed question. 
Matching is based on keywords, plus the NE 
type and their common location within a same 
sentence. 
Quest ion  Prc~:essor 
i 
: :eXt P r~_~ . . . . . . . . . . . .  ? ~  
i i ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
i . . . .  i 
Figure 2: Textract/QA 1.0 Prototype Architecture 
The general algorithm for question 
answering is as follows: 
168 
Process Question 
Shallow parse question 
Determine Asking Point 
Question expansion (using word lists) 
Process Documents 
Tokenization, POS tagging, NE Indexing 
Shallow Parsing (not yet utilized) 
Text Matcher 
Intersect search engine results with NE 
rank answers 
2.1 Question Processing 
The Question Processing results are a list of 
keywords plus the information for asking point. 
For example, the question: 
\[2\] Who won the 1998 Nobel Peace Prize? 
contains the following keywords: won, 1998, 
Nobel, Peace, Prize. The asking point Who refers 
to the NE type person. The output before 
question expansion is a simple 2-feature template 
as shown below: 
\[3\] asking_point: PERSON 
key_word: { won, 1998, Nobel, 
Peace, Prize } 
The following is an example where the 
asking point does not correspond to any type of 
NE in our definition. 
\[3\] Why did David Koresh ask the FBI for a 
word processor ? 
The system then maps it to the following 
question template : 
\[4\] asking_point: 
key_word: 
REASON 
{ ask, David, Koresh, 
FBI, word, processor }
The question processor scans the question to 
search for question words (wh-words) and maps 
them into corresponding NE types/sub-types or 
pre-defined notions like REASON. 
We adopt wo sets of pattern matching rules 
for this purpose: (i) structure based pattern 
matching rules; (ii) simple key word based 
pattern matching rules (regarded as default rules). 
It is fairly easy to exhaust the second set of rules 
as interrogative question words/phrases form a 
closed set. In comparison, the development of
the first set of rules are continuously being 
fine-tuned and expanded. This strategy of using 
two set of rules leads to the robustness of the 
question processor. 
The first set of rules are based on shallow 
parsing results of the questions, using Cymfony 
FST based Shallow Parser. This parser identifies 
basic syntactic onstructions like BaseNP (Basic 
Noun Phrase), BasePP (Basic Prepositional 
Phrase) and VG (Verb Group). 
The following is a sample of the first set of 
rules: 
\[6\] Name NP (city I country I company) --> 
CITYICOUNTRYICOMPANY 
\[7\] Name NP(person_w) --> PERSON 
\[8\] Name NP(org_w) --> ORGANIZATION 
\[9\] Name NP(NOT person_w, NOT org_w) 
--> NAME 
Rule \[6\] checks the head word of the NP. It 
covers cases like VG\[Name\] NP\[a country\] that 
VG\[is developing\] NP\[a magnetic levitation 
railway system\]. Rule \[7\] works for cases like 
VG\[Name\] NP\[the first private citizen\] VG\[to 
fly\] PP\[in space\] as citizen belongs to the word 
class person_w. Rule \[9\] is a catch-all rule: if the 
NP is not of class person (person_w) or 
organization (org_w), then the asking point is a 
proper name (default NE), often realized in 
English in capitalized string of words. Examples 
include Name a film that has won the Golden 
Bear in the Berlin Film Festival. 
The word lists org_w and person_w are 
currently manually maintained based on 
inspection of large volumes of text. An effort is 
underway to automate the learning of such word 
lists by utilizing machine learning techniques. 
We used the following pattern 
transformations to expand our ruleset: 
(Please) name NP\[X\] 
--> what/which Aux(be) (the name of) NP\[X\] 
--> NP(what/which...X) 
In other words, the four rules are expanded to 
12 rules. For example, Rule \[10\] below 
corresponds to Rule \[6\]; Rule \[11\] is derived 
169 
from Rule \[7\]. 
\[10\] what/which Aux(be) NP (city \[ country \[ 
company) --> 
CITY I COUNTRY \[ COMPANY 
\[11\] NP(what/which ... person_w) --> 
PERSON 
Rule \[10\] extracts the asking point from 
cases like NP\[What\] Aux\[is\] NP\[the largest 
country\] PP\[in the world\]. Rule \[11\] covers the 
following questions: NP\[What costume 
designer\] VG\[decided\] that NP\[Michael 
Jacksonl VG\[should only wear\] NP\[one glove\], 
NP\[Which former Ku Klux Klan member\] 
VG\[won\] NP\[an elected office\] PP\[in the U.S.\], 
NP\[What Nobel laureate\] VG\[was expelled\] 
PP\[from the Philippines\] PP\[before the 
conference\] PP\[on East Timor\], NP\[What 
famous communist leader\] VG\[died\] PP\[in 
Mexico City\], etc. 
As seen, shallow parsing helps us to capture a
variety of natural anguage question expressions. 
However, there are cases where some simple key 
word based pattern matching would be enough to 
capture the asking point. That is our second set 
of rules. These rules are used when the first set of 
rules has failed to produce results. The following 
is a sample of such rules: 
\[ 12\] who/whom --> PERSON 
\[13\] when --> TIME/DATE 
\[14\] where/what place --> LOCATION 
\[15\] what time (of day) --> TIME 
\[16\] what day (of the week) --> DAY 
\[17\] what/which month --> MONTH 
\[18\] what age/how old --> AGE 
\[19\] what brand --> PRODUCT 
\[20\] what --> NAME 
\[21\] how far/tall/high --> LENGTH 
\[22\] how large/hig/small --> AREA 
\[23\] how heavy --> WEIGHT 
\[24\] how rich --> MONEY 
\[25\] how often --> FREQUENCY 
\[26\] how many --> NUMBER 
\[27\] how long --> LENGTH/DURATION 
\[28\] why/for what --> REASON 
In the stage of question expansion, the 
template in \[4\] would be expanded to the 
template shown in \[29\]: 
\[29\] asking_point: 
key_word: 
{because{because of\] 
due to{thanks to{since I 
in order{to VB} 
{ asklaskslasked\[asking, 
David,Koresh,FBI, 
word, processor} 
The last item in the asking._point list attempts 
to find an infinitive by checking the word to 
followed by a verb (with the part-of-speech tag 
VB). As we know, infinitive verb phrases are 
often used in English to explain a reason for some 
action. 
2.2 Text Processing 
On the text processing side, we first send the 
question directly to a search engine in order to 
narrow down the document pool to the first n, say 
200, documents for IE processing. Currently, 
this includes tokenization, POS tagging and NE 
tagging. Future plans include several evels of 
parsing as well; these are required to support CE 
and GE extraction. It should be noted that all 
these operations are extremely robust and fast, 
features necessary for large volume text 
indexing. Parsing is accomplished through 
cascaded finite state transducer grammars. 
2.3 Text Matching 
The Text Matcher attempts to match the question 
template with the processed ocuments for both 
the asking point and the key words. There is a 
preliminary ranking standard built-in the matcher 
in order to find the most probable answers. The 
primary rank is a count of how many unique 
keywords are contained within a sentence. The 
secondary ranking is based on the order that the 
keywords appear in the sentence compared to 
their order in the question. The third ranking is 
based on whether there is an exact match or a 
variant match for the key verb. 
In the TREC-8 QA track competition, 
Cymfony QA accuracy was 66.0%. Considering 
we have only used NE technology to support QA 
in this run, 66.0% is a very encouraging result. 
3 Limitation 
The first limitation comes from the types of 
questions. Currently only wh-questions are 
handled although it is planned that yes-no 
questions will be handled once we introduce CE 
170 
and GE templates to support QA. Among the 
wh-questions, the why-question and 
how-question t are more challenging because the 
asking point cannot be simply mapped to the NE 
types/sub-types. 
The second limitation is from the nature of 
the questions. Questions like Where can l find 
the homepage for Oscar winners or Where can I 
find info on Shakespeare's works might be 
answerable asily by a system based on a 
well-maintained data base of home pages. Since 
our system is based on the processing of the 
underlying documents, no correct answer can be 
provided if there is no such an answer (explicitly 
expressed in English) in the processed 
documents. In TREC-8 QA, this is not a problem 
since every question is guaranteed to have at least 
one answer in the given document pool. 
However, in the real world scenario such as a QA 
portal, it is conceived that the IE results based on 
the processing of the documents hould be 
complemented by other knowledge sources uch 
as e-copy of yellow pages or other manually 
maintained and updated ata bases. 
The third limitation is the lack of linguistic 
processing such as sentence-level parsing and 
cross-sentential co-reference (CO). This problem 
will be gradually solved when high-level IE 
technology is introduced into the system. 
4 Future Work: Multi-level IE Supported QA 
A new QA architecture is under development; i  
will exploit all levels of the IE system, including 
CE and GE. 
The first issue is how much CE can 
contribute to a better support of QA. It is found 
that there are some frequently seen questions 
which can be better answered once the CE 
information is provided. These questions are of 
two types: (i) what/who questions about an NE; 
(ii) relationship questions. 
Questions of the following format require CE 
templates as best answers: who/what is NE? For 
example, Who is Julian Hill? Who is Bill 
Clinton? What is Du Pont? What is Cymfony? 
To answer these questions, the system can simply 
1 For example, How did one make a chocolate cake? 
How+Adjective questions (e.g. how long, how big, 
how old, etc.) are handled fairly well. 
retrieve the corresponding CE template to 
provide an "assembled" answer, as shown below. 
Q: Who is Julian Hill? 
A: name: Julian Werner Hill 
type: PERSON 
age: 91 
gender: MALE 
position: research chemist 
affiliation: Du Pont Co. 
education: Washington University; 
MIT 
Q: What is Du Pont? 
A: name: Du Pont Co, 
type: COMPANY 
staff: Julian Hill; Wallace Carothers. 
Questions specifically about a CE 
relationship include: For which company did 
Julian Hill work? (affiliation relationship) Who 
are employees of Du Pont Co.? (staff 
relationship) What does Julian Hill do? 
(position/profession relationship) Which 
university did Julian Hill graduate from? 
(education relationship), etc. 2 
The next issue is the relationships between 
GE and QA. It is our belief that the GE 
technology will result in a breakthrough for QA. 
In order to extract GE templates, the text 
goes through a series of linguistic processing as 
shown in Figure 1. It should be noted that the 
question processing is designed to go through 
parallel processes and share the same NLP 
resources until the point of matching and ranking. 
The merging of question templates and GE 
templates in Template Matcher are fairly 
straightforward. As they both undergo the same 
NLP processing, the resulting semantic templates 
are of the same form. Both question templates 
and GE templates correspond to fairly 
standard/predictable patterns (the PREDICATE 
value is open-ended, but the structure remains 
stable). More precisely, a user can ask questions 
on general events themselves (did what) and/or 
on the participants of the event (who, whom, 
what) and/or the time, frequency and place of 
events (when, how often, where). This addresses 
2 An alpha version of TextractQA supported by both 
NE and CE has been implemented and is being tested. 
171 
by far the most types of general questions of a 
potential user. 
For example, if a user is interested in 
company acquisition events, he can ask questions 
like: Which companies ware acquired by 
Microsoft in 1999? Which companies did 
Microsoft acquire in 1999? Our system will then 
parse these questions into the templates as shown 
below: 
\[31\] <Q_TEMPLATE> := 
PREDICATE: acquire 
ARGUMENT1: Microsoft 
ARGUMENT2: WHAT(COMPANY) 
TIME: 1999 
If the user wants to know when some 
acquisition happened, he can ask: When was 
Netscape acquired? Our system will then 
translate it into the pattern below: 
\[32\] <QTEMPLATE> := 
PREDICATE: acquire 
ARGUMENT1: WHO 
ARGUMENT2: Netscape 
TIME: WHEN 
Note that WHO, WHAT, WHEN above are 
variable to be instantiated. Such question 
templates serve as search constraints o filter the 
events in our extracted GE template database. 
Because the question templates and the extracted 
GE template share the same structure, a simple 
merging operation would suffice. Nevertheless, 
there are two important questions to be answered: 
(i) what if a different verb with the same meaning 
is used in the question from the one used in the 
processed text? (ii) what if the question asks 
about something beyond the GE (or CE) 
information? These are issues that we are 
currently researching. 
References 
Bikel D.M. et al (1997) Nymble: aHigh-Performance 
Learning Name-finder. "Proceedings of the Fifth 
Conference on Applied Natural Language 
Processing", Morgan Kaufmann Publishers, pp. 
194-201 
Chinchor N. and Marsh E. (1998) MUC- 7 Information 
Extraction Task Definition (version 5.1), 
"Proceedings ofMUC-7". 
Grishman R. (1997) TIPSTER Architecture Design 
Document Version 2.3. Technical report, DARPA 
Krupka G.R. and Hausman K. (1998) IsoQuest Inc.: 
Description of the NetOwl (TM) Extractor System 
as Used for MUC-7, "Proceedings ofMUC-7". 
Kupiec J. (1993) MURAX: A Robust Linguistic 
Approach For Question Answering Using An 
On-Line Encyclopaedia, "Proceedings of 
SIGIR-93 93" Pittsburgh, Penna. 
Li, W & Srihari, R. 2000. Flexible Information 
Extraction Learning Algorithm, Final Technical 
Report, Air Force Research Laboratory, Rome 
Research Site, New York 
MUC-7 (1998) Proceedings of the Seventh Message 
Understanding Conference (MUC-7), published on 
the website _http://www.muc.saic.com/ 
Roche E. and Schabes Y. (1997) Finite-State 
Language Processing, MIT Press, Cambridge, MA 
Srihari R. (1998) A Domain Independent Event 
Extraction Toolkit, AFRL-IF-RS-TR-1998-152 
Final Technical Report, Air Force Research 
Laboratory, Rome Research Site, New York 
172 
A Hybrid Approach for Named Entity and Sub-Type Tagging* 
Rohini Srihari 
Cymfony Net, Inc. 
5500 Main Street 
Williamsville, NY 14260 
rohini @ cymfony.com 
Cheng Niu and Wei Li 
Cymfony Net, Inc. 
5500 Main Street 
Williamsville, NY 14260 
chengniu@cymfony.com 
wei@cymfony.com 
Abstract 
This paper presents a hybrid approach for 
named entity (NE) tagging which combines 
Maximum Entropy Model (MaxEnt), Hidden 
Markov Model (HMM) and handcrafted 
grammatical rules. Each has innate strengths 
and weaknesses; the combination results in a 
very high precision tagger. MaxEnt includes 
external gazetteers in the system. Sub-category 
generation is also discussed. 
Introduction 
Named entity (NE) tagging is a task in which 
location names, person names, organization 
names, monetary amounts, time and percentage 
expressions are recognized and classified in 
unformatted text documents. This task provides 
important semantic information, and is a critical 
first step in any information extraction system. 
Intense research has been focused on 
improving NE tagging accuracy using several 
different echniques. These include rule-based 
systems \[Krupka 1998\], Hidden Markov Models 
(HMM) \[Bikel et al 1997\] and Maximum 
Entropy Models (MaxEnt) \[Borthwick 1998\]. A 
system based on manual rules may provide the 
best performance; however these require 
painstaking intense skilled labor.. Furthermore, 
shifting domains involves significant effort and 
may result in performance degradation. The 
strength of HMM models lie in their capacity for 
modeling local contextual information. HMMs 
have been widely used in continuous peech 
recognition, part-of-speech tagging, OCR, etc., 
and are generally regarded as the most successful 
statistical modelling paradigm in these domains. 
MaxEnt is a powerful tool to be used in situations 
where several ambiguous information sources 
need to be combined. Since statistical techniques 
such as HMM are only as good as the data they 
are trained on, they are required to use back-off 
models to compensate for unreliable statistics. In  
contrast o empirical back-off models used in 
HMMs, MaxEnt provides a systematic method 
by which a statistical model consistent with all 
obtained knowledge can be trained. \[Borthwick 
et al 1998\] discuss atechnique for combining the 
output of several NE taggers in a black box 
fashion by using MaxEnt. They demonstrate the 
superior performance of this system; however, 
the system is computationally inefficient since 
many taggers need to be run. 
In this paper we propose ahybrid method for 
NE tagging which combines all the modelling 
techniques mentioned above. NE tagging is a 
complex task and high-performance systems are 
required in order to be practically usable. 
Furthermore, the task demonstrates 
characteristics that can be exploited by all three 
techniques. For example, time and monetary 
expressions are fairly predictable and hence 
processed most efficiently with handcrafted 
grammar rules. Name, location and organization 
entities are highly variable and thus lend 
themselves tostatistical training algorithms such 
as HMMs. Finally, many conflicting pieces of 
information regarding the class of a tag are 
* This work was supported in part by the SBIR grant F30602-98-C-0043 from Air Force Research Laboratory 
(AFRL)/IFED. 
247 
frequently present. This includes information 
from less than perfect gazetteers. For this, a 
MaxEnt approach works well in utilizing diverse 
sources of information in determining the final 
tag. The structure of our system is shown in 
Figure 1. 
I I 
I to=~m, I~mm 
I I 
l I o , ,=- . . , I  
I MJE3;,= G,,: ;,IE 
1 
I~1 Sb~lued hE Tasp" 
The first module is a rule-based tagger 
containing pattern match rules, or templates, for 
time, date, percentage, and monetary 
expressions. These tags include the standard 
MUC tags \[Chinchor 1998\], as well as several 
other sub-categories defined by our organization. 
More details concerning the sub-categories are 
presented later. The pattern matcher is based on 
Finite State Transducer (FST) technology 
\[Roches & Schabes 1997\] that has been 
implemented in-house. The subsequent modules 
are focused on location, person and organization 
names. The second module assigns tentative 
person and location tags based on external person 
and location gazetteers. Rather than relying on 
simple lookup of the gazetteer which is very error 
prone, this module employs MaxEnt to build a 
statistical model that incorporates gazetteers with 
common contextual information. The core 
module of the system is a bigram-based HMM 
\[Bikel et a1.1997\]. Rules designed to correct 
errors in NE segmentation are incorporated into a 
constrained HMM network. These rules serve as 
constraints on the HMM model and enable it to 
utilize information beyond bigrams and remove 
obvious errors due to the limitation of the training 
corpus. HMM generates the standard MUC tags, 
person, location and organization. Based on 
MaxEnt, the last module derives sub-categories 
such as city, airport, government, etc. from the 
basic tags. 
Section 1 describes the FST rule module. 
Section 2 discusses combining gazetteer 
information using MaxEnt. The constrained 
HMM is described in Section 3. Section 4 
discusses ub-type generation by MaxEnt. The 
experimental results and conclusion are 
presented finally. 
1 FST-based Pattern Matching Rules for 
Textract NE 
The most attractive feature of the FST (Finite 
State Transducer) formalism lies in its superior 
time and space efficiency \[Mohri 1997\] \[Roche 
& Schabes 1997\]. Applying a deterministic FST 
depends linearly only on the input size of the text. 
Our experiments also show that an FST rule 
system is extraordinarily robust. In addition, it 
has been verified by many research programs 
\[Krupka & Hausman 1998\] \[Hobbs 1993\] 
\[Silberztein 1998\] \[Srihari 1998\] \[Li & Srihari 
2000\], that FST is also a convenient tool for 
capturing linguistic phenomena, especially for 
idioms and semi-productive expressions like time 
NEs and numerical NEs. 
The rules which we have currently 
implemented include a grammar for temporal 
expressions (time, date, duration, frequency, age, 
etc.), a grammar for numerical expressions 
(money, percentage, length, weight, etc.), and a 
grammar for other non-MUC NEs (e.g. contact 
information like address, email). 
The following sample pattern rules give an 
idea of what our NE grammars look like. These 
rules capture typical US addresses, like: 5500 
Main St., Williamsville, NY14221; 12345 Xyz 
Avenue, Apt. 678, Los Angeles, CA98765-4321. 
The following notation is used: @ for macro; I
for logical OR; + for one or more; (...) for 
optionality. 
9 -~- 
number = 
uppercase =
0111213141516171819 
@0_9+ 
AIBICIDIEIFIGIHIIIJI 
KILIMINIOIPIQIRISIT 
UIVIWIXIYIZ 
248 
lowercase = a \[ b \[ c I d \[ e I f l g I h \[i I J I k \[ I I 
mln lo lp lq l r l s l t lu lv lw\ [  
x ly l z  
letter = @uppercase \[ @lowercase 
word = @letter+ 
delimiter = (",") .... + 
zip = @0_9 @0_9 @09 @0_9 @0_9 
("-" @0_9 @0_9 @0_9 @0_9) 
street = \[\[St l ST I Rd I RD I Dr I DRI 
Ave\[AVE \] C.")\] I Street\[ 
Road\[Drive\[Avenue 
city = @word (@word) 
state = @uppercase (".") @uppercase (".") 
us-- USA IU.S.AIUSIU.S.I 
(The) United States (of America) 
street_addr = @number @word @street 
apt_addr = \[APT C.") I Apt (".") \[ 
Apartment\] @number 
local_addr = @ street_addr 
(@delimiter @apt_addr) 
address = @ local_addr 
@delimiter @city 
@delimiter @state @zip 
(@delimiter @us) 
Our work is similar to the research on FST 
local grammars at LADL/University Paris VII 
\[Silberztein 1998\] 1, but that research was not 
turned into a functional rule based NE system. 
The rules in our NE grammars cover 
expressions with very predictable patterns. They 
were designed to address the weaknesses of our 
statistical NE tagger. For example, the following 
missings (underlined) and mistagging originally 
made by our statistical NE tagger have all been 
correctly identified by our temporal NE 
grammar. 
began <TIMEX TYPE="DATE">Dec. 15, 
the</TIMEX> space agency 
on Jan. 28, <TIMEX 
TYPE="DATE"> 1986</TIMEX>, 
in September <TIMEX 
TYPE="DATE">1994</TIMEX>on <TIMEX 
1 They have made public their esearch results at their 
website (http://www.ladl.jussieu.fr/index.html), 
including a grammar for certain temporal expressions 
and a grammar for stock exchange sub-language. 
TYPE="TIME">Saturday at</TIMEX> 2:42 
a.m. ES<ENAMEX 
TYPE="PERSON">T.</ENAMEX> 
He left the United States in <TIMEX 
TYPE="DATE">1984 and</TIMEX> moved 
in early <TIMEX TYPE="DATE"> 1962 
and</TIMEX> 
in <TIMEX TYPE="DATE">1987 the 
Bonn</TIMEX> government ruled 
2 Incorporating Gazetteers with the 
Maximum Entropy Model 
We use two gazetteers in our system, one for 
person and one for location. The person gazetteer 
consists of 3,000 male names, 5,000 female 
names and 14,000 family names. The location 
gazetteer consists of 250,000 location ames with 
their categories uch as CITY, PROVINCE, 
COUNTRY, AIRPORT, etc. The containing and 
being-contained relationship among locations is 
also provided. 
The following is a sample line in the location 
gazetteer, which denotes "Aberdeen" as a city in 
"California", and "California" as a province of 
"United States". 
Aberdeen (CITY) California (PROVINCE) 
United States (COUNTRY) 
Although gazetteers obviously contain useful 
name entity information, a straightforward word 
match approach may even degrade the system 
performance since the information from 
gazetteers i  too ambiguous. There are a lot of 
common words that exist in the gazetteers, uch 
as 'T', "A", "Friday", "June", "Friendship", etc. 
Also, there is large overlap between person 
names and location names, such as "Clinton", 
"Jordan", etc. 
Here we propose a machine learning 
approach to incorporate the gazetteer information 
with other common contextual information based 
on MaxEnt. Using MaxEnt, the system may 
learn under what situation the occurrence in 
gazetteers is a reliable vidence for a name entity. 
We first define "LFEATURE" based on 
occurrence in the location gazetteer as follows: 
249 
COUNTRY 
USSTATE 
MULTITOKEN 
of multiple tokens) 
BIGCITY 
in OXFD dictionary) 
COEXIST 
(country name) 
(US state name) 
(a location ame consisting 
(a location ame occurring 
(where COEXIST(A,B) is 
true iff A and B are in the same US state, or in 
the same foreign country) 
OTHER 
There is precedence from the first 
LFEATURE to the last one. Each token in the 
input document is assigned a unique 
"LFEATURE". We also define "NFEATURE" 
based on occurrence in the name gazetteer as 
follows: 
FAMILY 
MALE 
FEMALE 
FAMILYANDMALE 
name) 
FAMILYANDFEMALE 
name) 
OTHER 
(family name) 
(male name) 
(female name) 
(family and male 
(family and female 
With these two extra features, every token in 
the document is regarded as a three-component 
vector (word, LFEATURE, NFEATURE). We 
can build a statistical model to evaluate the 
conditional probability based on these contextual 
and gazetteer features. Here "tag" represents one 
of the three possible tags (Person, Location, 
Other), and history represents any possible 
contextual history. Generally, we have: 
p (tag, history) 
tag 
(1) 
A maximum entropy solution for probability has 
the form \[Rosenfeld 1994\] \[Ratnaparkhi 1998\] 
H ~/i (history,tag) 
p(tag,history) =
Z(history) 
Z(history) = ~.~ 1-I \[~t'ifi(hist?ry'tag ) 
tag i 
(e) 
(3) 
where fi (history, tag) are binary-valued feature 
functions that are dependent on whether the 
feature is applicable to the current contextual 
history. Here is an example of our feature 
function: 
f(history,tag)={~ ifcurrenttokenisaeountryname, ndtagisloeatiOnotherwise 
(4) 
In (2) and (3) a i are weights associated tofeature 
functions. 
The weight evaluation scheme is as follows: 
We first compute the average value of each 
feature function according to a specific training 
corpus. The obtained average observations are 
set as constraints, and the Improved Iterative 
Scaling (IIS) algorithm \[Pietra et al 1995\] is 
employed to evaluate the weights. The resulting 
probability distribution (2) possesses the 
maximum entropy among all the probability 
distributions consistent with the constraints 
imposed by feature function average values. 
In the training stage, our gazetteer module 
contains two sub-modules: feature function 
induction and weight evaluation \[Pietra et al 
1995\]. The structure is shown in Figure 2. 
Rule ~|ect|on Module \[ 
~elect next rule reduce the entropy most 
"-~ Evaluate weiEht for each Selected rule 
IteraUve $?atinB (US) t 
Fig.2, Structure ofMaxEnt learning Process 
We predefine twenty-four feature function 
templates. The following are some examples and 
others have similar structures: 
10 if LFEATURE = , and tag = _ f (history, tag) = 
else 
250 
f(history, tag)={lo 
f(history, tag)={~ 
f(history,tag)={lo 
f(history, tag)={; 
i f  NFEATURE = _ ,  and  tag  = _ 
e l se  
i f  cur rent  word  = _ ,  and  tag  = _ 
e l se  
i f  p rev ious  word  = _ ,  and  tag  = _ 
e l se  
i f  fo l low ing  word  = _ ,  and  tag  = _ 
e l se  
where the symbol .... denotes any possible 
values which may be  inserted into that field. 
Different fields will be filled different values. 
Then, using a training corpus containing 
230,000 tokens, we set up a feature function 
candidate space based on the feature function 
templates. The "Feature Function Induction 
Module" can select next feature function that 
reduces the Kullback-Leibler divergence the 
most \[Pietra et al 1995\]. To make the weight 
evaluation computation tractable at the feature 
function induction stage, when trying a new 
feature function, all previous computed weights 
are held constant, and we only fit one new 
constraint hat is imposed by the candidate 
feature function. Once the next feature function 
is selected, we recalculate the weights by IIS to 
satisfy all the constraints, and thus obtain the next 
tentative probability. The feature function 
induction module will stop when the 
Log-likelihood gain is less than a pre-set 
threshold. 
The gazetteer module recognizes the person 
and location names in the document despite the 
fact that some of them may be embedded in an 
organization ame. For example, "New York 
Fire Department" may be tagged as 
<LOCATION> New York </NE> Fire 
Department. In the input stream for HMM, each 
token being tagged as location is accordingly 
transformed into one of the built-in tokens 
"CITY", "PROVINCE", "COUNTRY". The 
HMM may group "CITY Fire Department" into 
an organization ame. A similar technique is 
applied for person names. 
Since the tagged tokens from the gazetteer 
module are regarded by later modules as either 
person or location names, we require that the 
current module generates results with the highest 
possible precision. For each tagged token we will 
compute the entropy of the answer. If the entropy 
is higher than a pre-set hreshold, the system will 
not be certain enough about the answer, and the 
word will be untagged. The missed location or 
person names may be recognized by the 
following HMM module. 
3 Improving NE Segmentation through 
constrained HMM 
Our original HMM is similar to the Nymble 
\[Bikel et al 1997\] system that is based on bigram 
statistics. To correct some of the leading errors, 
we incorporate manual segmentation rules with 
HMM. These syntactic rules may provide 
information beyond bigram and balance the 
limitation of the training corpus. 
Our manual rules focus on improving the NE 
segmentation. For example, in the token 
sequence "College of William and Mary", we 
have rules based on global sequence checking to 
determine if the words "and" or "of" are common 
words or parts of organization name. 
The output of the rules are some constraints 
on the HMM transition etwork, such as "same 
tags for tokens A, B", or "common word for 
token A". The Viterbi algorithm will select he 
optimized path that is consistent with such 
constraints. 
The manual rules are divided into three 
categories: (i) preposition disambiguation, (ii) 
spurious capitalized word disambiguation, and 
(iii) spurious NE sequence disambiguation. 
The rules of preposition disambiguation are 
responsible for determination of boundaries 
involving prepositions ("of", "and", "'s", etc.). 
For example, for the sequence "A of B", we have 
the following rule: A and B have same tags if the 
lowercase of A and B both occur in OXFD 
dictionary. A "global word sequence checking" 
\[Mikheev, 1999\] is also employed. For the 
sequence "Sprint and MCI", we search the 
document globally. If the word "Sprint" or 
251 
"MCI" occurs individually somewhere lse, we 
mark "and" as a common word. 
The rules of spurious capitalized word 
disambiguation are designed to recognize the 
first word in the sentence. If the first word is 
unknown in the training corpus, but occurs in 
OXFD as a common word in lowercase, HHM's 
unknown word model may be not accurate 
enough. The rules in the following paragraph are 
designed to treat such a situation. 
If the second word of the same sentence is in 
lowercase, the first word is tagged as a common 
word since it never occurs as an isolated NE 
token in the training corpus unless it has been 
recognized as a NE elsewhere in the document. 
If the second word is capitalized, we will check 
globally if the same sequence occurs somewhere 
else. If so, the HMM is constrained to assign the 
same tag to the two tokens. Otherwise, the 
capitalized token is tagged as a common word. 
The rules of spurious NE sequence 
disambiguation are responsible for finding 
spurious NE output from HMM, adding 
constraints, and re-computing NE by HMM. For 
example, in a sequence "Person Organization", 
we will require the same output ag for these two 
tokens and run HMM again. 
4 NE Sub-Type Tagging using Maximum 
Entropy Model 
The output document from constrained HMM 
contains MUC-standard NE.tags such as person, 
location and organization. However, for a real 
information extraction system, the 
MUC-standard NE tag may not be enough and 
further detailed NE information might be 
necessary. We have predefined the following 
sub-types for person, location and organization: 
Person: Military Person 
Religious Person 
Man 
Woman 
Location: City 
Province 
Country 
Continent 
Lake 
River 
Mountain 
Road 
Region 
District 
Airport 
Organization: Company 
Government 
Army 
School 
Association 
Mass Medium 
If a NE is not covered by any of the above 
sub-categories, it should remain a MUC-standard 
tag. Obviously, the sub-categorization requires 
much more information beyond bigram than 
MUC-standard tagging. For example, it is hard 
to recognize CNN as a Mass Media company by 
bigram if the token "CNN" never occurs in the 
training corpus. External gazetteer information is 
critical for some sub-category recognition, and 
trigger word models may also play an important 
role. 
With such considerations, we use the 
Maximum entropy model for sub-categorization, 
since MaxEnt is powerful enough to incorporate 
into the system gazetteer or other information 
sources which might become available at some 
later time. 
Similar to the gazetteer module in Section 2, 
the sub-categorization module in the training 
stage contains two sub-modules, (i) feature 
function induction and (ii) weight evaluation. 
We have the following seven feature function 
templates: 
10 if MUC_tag = _, and tag = _ 
f (history, tag) = else 
{ 10 if MUC_tag = _, LFEATURE = _, and tag = _ 
f (history, tag) = else 
1 if contain word(__), MUC tag(history) = _,and tag = 
f (history, tag ) = - - - 
0 else 
10 if Previous_Word = _, MUC_tag = _,and tag = _ 
f (history, tag)= else 
f(history, tag)= {10 if following_Word= _,MUC_tag = _ ,ande lse  tag=_  
f(history, tag)={lo i fMUC_tag= ,contain_male_name, and tag 
252 
= 1l  if  ,oc_ta  =_,co. .in_fema,e_.ame,a.d,ag=_ f (history, tag ) to else 
We have trained 1,000 feature functions by 
the feature function induction module according 
to the above templates. 
Because much more external gazetteer 
information is necessary for the 
sub-categorization and there is an overlap 
between male and female name gazetteers, the 
result from the current MaxEnt module is not 
sufficiently accurate. Therefore, a conservative 
strategy has been applied. If the entropy of the 
output answer is higher than a threshold, we will 
back-off to the MUC-standard tags. Unlike 
MUC NE categories, local contextual 
information is not sufficient for 
sub-categorization. In the future more external 
gazetteers focusing on recognition of 
government, company, army, etc. will be 
incorporated into our system. And we are 
considering using trigger words \[Rosenfeld, 
1994\] to recognize some sub-categories. For 
example, "psalms" may be a trigger word for 
"religious person", and "Navy" may be a trigger 
word for "military person". 
Experiment and Conclusion 
We have tested our system on MUC-7 dry run 
data; this data consists of 22,000 words and 
represents articles from The New York Times. 
Since a key was provided with the data, it is 
possible to properly evaluate the performance of 
our NE tagger. The scoring program computes 
both the precision and recall, and combines these 
two measures into f-measure as the weighted 
harmonic mean \[Chinchor, 1998\]. The formulas 
are as follows: 
number of correct responses Precision = 
number esponses 
number of  correct responses 
Recall = 
number correct in key 
F - (/32 + 1)Precision * Recall 
(f l2Recall) + Precision 
The score of our system is as follows: 
Recall Precision 
Organization 95 95 
Person 96 
Location 96 
Percentage 
93 
94 
Date 92 91 
Time 92 91 
Money 100 86 
100 75 
F-measure =93.39 
If the gazetteer module is removed from our 
system, and the constrained HMM is restored to 
the standard HMM, the f-measures for person, 
location, and organization are as follows: 
Recall Precision 
Organization 94 92 
Person 95 91 
Location 95 92 
Obviously, our gazetteer model and 
constrained HMM have greatly increased the 
system accuracy on the recognition of persons, 
locations, and organizations. Currently, there are 
some errors in our gazetteers. Some common 
words such as "Changes", "USER", 
"Administrator", etc. are mistakenly included in 
the person name gazetteer. Also, too many 
person names are included into the location 
gazetteer. By cleaning up the gazetteers, we can 
continue improving the precision on person name 
and locations. 
We also ran our NE tagger on the formal test 
files of MUC-7. The following are the results: 
Recall Precision 
Person 92 95 
Organization 85 86 
Location 90 92 
Date 95 85 
253 
Time 79 72 
Money 95 82 
Percentage 97 80 
-Overall F-measure 89 
There is some performance degradation i  
the formal test. This decrease is because that the 
formal test is focused on satellite and rocket 
domains in which our system has not been 
trained. There are some person/location names 
used as spacecraft or robot names (ex. Mir, Alvin, 
Columbia...), and there are many high-tech 
company names which do not occur in our HMM 
training corpus. Since the finding of organization 
names totally relies on the HMM model, it suffers 
most from domain shift (10% degradation). This 
difference implies that gazetteer information may 
be useful in overcoming the domain dependency. 
This paper has demonstrated improved 
performance in an NE tagger by combining 
symbolic and statistical approaches. MaxEnt has 
been demonstrated to be a viable technique for 
integrating diverse sources of information and 
has been used in NE sub-categorization. 
A. Ratnaparkhi, Maximum Entropy Models for 
Natural Language Ambiguity resolution, PHD 
thesis, Univ. of Pennsylvania, (1998) 
S. D. Pietra, Vincent Della Pietra, and John Lafferty, 
Inducing Features of Random Fields, Tech Report, 
Carnegie Mellon University, (1995) 
A. Mikheev, A Knowledge-free Method for 
Capitalized Word Disambiguation, in Proceedings 
of the 37th Annual Meeting of the Association for 
Computational Linguistics, (1999), pp. 159-166 
J. R. Hobbs, 1993. FASTUS: A System for Extracting 
Information from Text, Proceedings ofthe DARPA 
workshop on Human Language Technology", 
Princeton, NJ, pp. 133-137. 
Emmanuel Roche & Yves Schabes, 1997. Finite-State 
Language Processing, The MIT Press, Cambridge, 
MA. 
Li, W & Srihari, R. 2000. Flexible Information 
Extraction Learning Algorithm, Final Technical 
Report, Air Force Research Laboratory, Rome 
Research Site, New York 
M. Silberztein, 1998. Tutorial Notes: Finite State 
Processing with INTEX, COLING-ACL'98, 
Montreal (also available at 
http://www.ladl.\] ussieu.fr) 
M. Mohri,. 1997. Finite-State Transducers in 
Language and Speech Processing, Computational 
Linguistics, Vol. 23, No. 2, pp. 269-311. 
R. Srihari, 1998. A Domain Independent Event 
Extraction Toolkit, AFRL-IF-RS-TR-1998-152 
Final Technical Report, Air Force Research 
Laboratory, Rome Research Site, New York 
References 
G. R Krupka and K. Hausman, "IsoQuest Inc: 
Description of the NetOwl "Fext Extraction System 
as used for MUC-7" in Proceedings of Seventh 
Machine Understanding Conference (MUC-7) 
(1998) 
D. M. Bikel, "Nymble: a high-performance learning 
name-finder" in Proceedings of the Fifth 
Conference on Applied Natural Language 
Processing, 1997, pp. 194-201, Morgan Kaufmann 
Publishers. 
A. Borthwick, et al, Description of the MENE named 
Entity System, In Proceedings of the Seventh 
Machine Understanding Conference (MUC-7) 
(1998) 
R. Rosenfeld, Adaptive Statistical language Modeling, 
PHD thesis, Carnegie Mellon University, (1994) 
254 
 Location Normalization for Information Extraction* 
 
 
Huifeng Li, Rohini K. Srihari, Cheng Niu, and Wei Li 
Cymfony Inc. 
600 Essjay Road, Williamsville, NY 14221, USA 
(hli, rohini, cniu, wei)@cymfony.com 
 
 
Abstract  
 
Ambiguity is very high for location names. For 
example, there are 23 cities named ?Buffalo? in 
the U.S.  Country names such as ?Canada?, 
?Brazil? and ?China? are also city names in the 
USA. Almost every city has a Main Street or 
Broadway. Such ambiguity needs to be handled 
before we can refer to location names for 
visualization of related extracted events. This 
paper presents a hybrid approach for location 
normalization which combines (i) lexical 
grammar driven by local context constraints, (ii) 
graph search for maximum spanning tree and 
(iii) integration of semi-automatically derived 
default senses. The focus is on resolving 
ambiguities for the following types of location 
names: island, town, city, province, and country. 
The results are promising with 93.8% accuracy 
on our test collections. 
 
1 Introduction 
The task of location normalization is to identify 
the correct sense of a possibly ambiguous 
location Named Entity (NE). Ambiguity is very 
serious for location NEs. For example, there are 
23 cities named ?Buffalo?, including the city in 
New York State and in Alabama State. Even 
country names such as ?Canada?, ?Brazil?, and 
?China? are also city names in the USA. Almost 
every city has a Main Street or Broadway. Such 
ambiguity needs to be properly handled before 
converting location names into some normal 
form to support entity profile construction, event 
merging and visualization of extracted events on 
 
*This work was partly supported by a grant from the 
Air Force Research Laboratory?s Information 
Directorate (AFRL/IF), Rome, NY, under contract 
F30602-00-C-0090.  
a map for an Information Extraction (IE) System.  
Location normalization is a special 
application of word sense disambiguation 
(WSD). There is considerable research on WSD. 
Knowledge-based work, such as (Hirst, 1987; 
McRoy, 1992; Ng and Lee, 1996) used 
hand-coded rules or supervised machine learning 
based on annotated corpus to perform WSD. 
Recent work emphasizes corpus-based 
unsupervised approach (Dagon and Itai, 1994; 
Yarowsky, 1992; Yarowsky, 1995) that avoids 
the need for costly truthed training data. Location 
normalization is different from general WSD in 
that the selection restriction often used for WSD 
in many cases is not sufficient to distinguish the 
correct sense from the other candidates.  
For example, in the sentence ?The White 
House is located in Washington?, the selection 
restriction from the collocation ?located in? can 
only determine that ?Washington? should be a 
location name, but is not sufficient to decide the 
actual sense of this location. Location 
normalization depends heavily on co-occurrence 
constraints of geographically related location 
entities mentioned in the same discourse. For 
example, if ?Buffalo?, ?Albany? and ?Rochester? 
are mentioned in the same document, the most 
probable senses of ?Buffalo?, ?Albany? and 
?Rochester? should refer to the cities in New 
York State. There are certain fixed 
keyword-driven patterns from the local context, 
which decide the sense of location NEs. These 
patterns use keywords such as ?city?, ?town?, 
?province?, ?on?, ?in? or other location names. For 
example, the pattern ?X + city? can determine 
sense tags for cases like ?New York city?; and the 
pattern ?City + comma + State? can disambiguate 
cases such as ?Albany, New York? and 
?Shanghai, Illinois?. In the absence of these 
patterns, co-occurring location NEs in the same 
discourse can be good evidence for predicting the 
most probable sense of a location name.  
  
Unrestricted
text Tokenizer
POS Tagging
Shallow Parsing
Coreference
Semantic Parsing
Pragmatic Filter
NE Tagging
LocNZ
Profile
Event
Question 
Answering
O
ut
pu
t(I
E 
D
at
ab
as
e)
Intelligent
Browsing
Summari-
zationVisualization
Kernel IE Modules Linguistic Modules
Note: NE: name entity tagging; LocNZ: location normalization
Application Modules
O
ut
pu
t(I
E 
D
at
ab
as
e)
 
Figure 1. InfoXtract system architecture 
 
 
Event type: Job change
Keyword: hired
Company : Microsoft
Person in: Mary
Position: sales person
Location: Beijing
Date: January 1
Event type: Job change
Keyword: replaced
Company: Microsoft
Person out : he(Dick)
Position: sales person
Location: Beijing
Date: Yesterday
Event 1 Event 2
Event type: Job change
Keyword: hired
Keyword: replaced
Company: Microsoft
Person in: Mary
Person out : he(Dick)
Position: salesperson
Location: <LocationProfile101>
Date: 2000-01-01
Figure 2. Location verification in Event 
merging. 
For choosing the best matching sense set 
within a document, we simply construct a graph 
where each node represents a sense of a location 
NE, and each edge represents the relationship 
between two location name senses. A graph 
spanning algorithm can be used to select the best 
senses from the graph. If there exist nodes that 
cannot be resolved in this step, we will apply 
default location senses that were extracted 
semi-automatically by statistical processing. The 
location normalization module, or ?LocNZ?, is 
applied after the NE tagging module in our 
InfoXtract IE system as shown in Figure 1. 
This paper focuses on how to resolve 
ambiguity for the names of island, town, city, 
province, and country. Three applications of 
LocNZ in Information Extraction are illustrated 
in Section 2. Section 3 presents location sense 
identification using local context; Section 4 
describes disambiguation process using 
information within a document through graph 
processing; Section 5 shows how to 
semi-automatically collect default senses of 
locations from a corpus; Section 6 presents an 
algorithm for location normalization with 
experimental results. The summary and 
conclusions are given in Section 7. Sample text 
and the results of location tagging are given in the 
Appendix. 
 
2   Applications of Location Normalization 
Several applications are enabled through location  
normalization. 
? Event extraction and merging 
Event extraction is an advanced IE task. 
Extracted events can be merged to provide key 
content in a document. The merging process 
consists of several steps including checking 
information compatibility such as checking 
synonyms, name aliases and co-reference of 
anaphors, time and location normalization. Two 
events cannot be merged if there is a conflicting 
condition such as time and location. Figure 2 
shows an example of event merging where the 
events occurred in Microsoft at Beijing, not in 
Seattle. 
? Event visua lization 
Visualization applications can illustrate where an 
event occurred with support of location 
normalization. Figure 3 demonstrates a 
visualized event on a map based on the 
normalized location names associated with the 
events. The input to visualization consists of 
extracted events from a news story pertaining to 
Julian Hill?s life. The arrow points to the city 
where the event occurred. 
? Entity profile construction 
An entity profile is an information object for 
entities such as person, organization and location.  
It is defined as an Attribute Value Matrix (AVM)  
to represent key aspects of information about 
entities, including their relationships with other 
entities. Each attribute slot embodies some 
  
Event type: <Die: Event 200>
Who:       <Julian Werver Hill: PersonProfile 001>
When:     1996-01-0 7
Where :     <Loca t ionPro f i l e103>
Preceding_event:  <hospitalize: Event 260>
Subsequent_event:  <bury:  Event  250>
Event Visualization
;  ; 
; ; 
Predicate: Die
Who: Julian Werner Hill
When:
Where: <LocationProfile 103>
Hockessin, Delaware, USA,
19707,75.688873,39.77604
1996-01-07
Figure 3. Event visulization with location. 
information about the entity in one aspect. Each 
relationship is represented by an attribute slot in 
the Profile AVM.  Sample Profile AVMs 
involving the reference of locations are 
illustrated below. 
<PersonProfile 001> :: 
Name:   Julian Werner Hill  
Position: Research chemist 
Age:        91  
Birth-place: <LocationProfile100> 
Affiliation:  Du Pont Co.  
Education:  MIT  
 
<LocationProfile 100> :: 
Name:   St. Louis  
State:    Missouri 
Country: United States of America  
Zipcode:  63101 
Lattitude : 90.191313  
Longitude:  38.634616 
Related_profiles: <PersonProfile 001>  
 
Several other applications such as question 
answering and classifying documents by location 
areas can also be enabled through LocNZ. 
3 Lexical Grammar Processing in 
Local Context 
Named Entity tagging systems (Krupka and 
Hausman, 1998; Srihari et al, 2000) attempt to 
tag information such as names of people, 
organizations, locations, time, etc. in running 
text.  In InfoXtract, we combine Maximum 
Entropy Model (MaxEnt) and Hidden Markov 
Model for NE tagging (Shrihari et al,, 2000). The 
Maximum Entropy Models incorporate local 
contextual evidence in handling ambiguity of 
information from a location gazetteer. In the 
Tipster Location gazetteer used by InfoXtract, 
there are a lot of common words, such as I, A, 
June, Friendship , etc. Also, there is large overlap 
between person names and location names, such 
as Clinton, Jordan, etc. Using MaxEnt, systems 
learn under what situation a word is a location 
name, but it is very difficult to determine the 
correct sense of an ambiguous location name. If a 
word can represent a city or state at the same 
time, such as New York or Washington, it is 
difficult to decide if it refers to city or state. The 
NE tagger in InfoXtract only assigns the location 
super-type tag NeLOC to the identified location 
words and leaves the task of location sub-type 
tagging such as NeCITY or NeSTATE and its 
normalization to the subsequent module LocNZ. 
For representation of LocNZ results, we add 
an unique zip code and position information 
that is longitude and latitude for the cities for 
event visualization. 
The first step of LocNZ is to use local context 
that is the co-occurring words around a location 
name. Local context can be a reliable source in 
deciding the sense of a location. The following 
are most commonly used patterns for this 
purpose.  
 
(1) location+comma+NP(headed by ?city?)  
e.g. Chicago, an old city  
(2) ?city of? +location1+comma+location2 
e.g. city of Albany, New York 
(3) ?city of? +location 
(4) ?state of?+location  
(5) location1+{,}+location2+{,}+location3 
e.g. (i) Williamsville, New York, USA 
       (ii) New York, Buffalo,USA 
     (6) {on, in}+location 
 e.g. on Strawberry ? NeIsland 
 in Key West ? NeCity 
Patterns (1) , (3), (4) and (6) can be used to decide 
if the location is a city, a state or an island, while 
patterns (2) and (5) can be used to determine both 
the sub-tag and its sense. These patterns are 
implemented in  our finite state transducer 
formalism. 
 4 Maximum Spanning Tree 
Calculation with Global Information 
Although local context can be reliable evidence 
for disambiguating location senses, there are still 
many cases which cannot be captured by the 
above patterns. Information in the entire 
document (i.e. discourse information) should be 
considered. Since all location names in a 
document have meaning relationships among 
them, a way to represent the best sense 
combination within the document is needed.  
The LocNZ process constructs a weighted 
graph where each node represents a location 
sense, and each edge represents similarity weight 
between location names. Apparently there will be 
no links among the different senses of a location 
name, so the graph will be partially complete. We 
calculate the maximum weight spanning tree 
(MaxST) using Kruskal?s MinST algorithm 
(Cormen et al 1990). The nodes on the 
resulting MaxST are the most promising senses 
of the location names.  
We define three criteria for similarity weight 
assignment between two nodes:  
(1) More weight will be given to the edge 
between a city and the province (or the 
country) to which it belongs.  
(2) Distance between location names mentioned 
in the document is taken into consideration. 
The shorter the distance, the more we assign  
the weight between the nodes.  
(3) The number of word occurrences affects the 
weight calculation. For multiple mentions of 
a location name, only one node will be 
represented in the graph. We assume that all 
the same location mentions have the same 
meaning in a document following one sense 
per discourse principle (Gale, Church, and 
Yarowsky, 1992).  
When calculating the weight between two 
location names, the predefined similarity values 
shown in Table 1, the number of location name 
occurrences and the distance between them in a 
text are taken into consideration. After selecting 
each edge, the senses that are connected will be 
chosen, and other senses of the same location 
name will be discarded so that they will not be 
considered again in the MaxST calculation. A 
weight value is calculated with equation (1), 
where sij indicate the jth sense of wordi, a reflects 
the number of location name occurrences in a 
text, and b refers to the distance between the two 
location names. Figure 4 shows the graph for 
calculating MaxST. Dots in a circle mean the 
number of senses of a location name. 
Table 1. Similarity value sim(si,si) between 
location sense pairs. 
Loc1 Loc2 Relationship  Sim(si,si) 
C1 P1 P1 includes C1  5 
IL Ctr1 Ctr1 includes IL 5 
C1 Ctr1 Ctr1 is direct parent 5 
C1 C2 C1 and C2 in same 
province/state 
3 
C1 C2 C1 and C2 in same 
country 
2 
C1 P1 C1 and P1 are in same 
country but C1 is not 
in P1 
2 
C1 Ctr1 Ctr1 is not a direct 
parent of C1  
3 
P1 Ctr1 P1 is in Ctr1 1 
P1 P2 P1 and P2 in same 
country 
1 
Loc1 Loc2 Loc1 and Loc2 are two 
sense nodes of the 
same location name 
-? 
Loc1 Loc2 Other cases 0 
Note: Ci: city; Pi: province/state; IL: island; Ctri: 
country; Loci: location. 
),(),(
/))()((),(
/),(),(),(),(
jijkij
jijkij
jkijjkijjkijjkij
wwdistss
numAllwnumwnumss
numAllsssssssimssScore
=
+=
-+=
b
a
ba
                          (1) 
5 Default Sense Extraction 
In our experiments, we found that the system 
performance suffers greatly from the lack of 
lexical information on default senses. For 
example, people refer to ?Los Angeles? as the 
city at California more than the city in 
Philippines, Chile, Puerto Rico, or the city in 
Texas in the USA. This problem becomes a 
bottleneck in the system performance. As 
mentioned before, a location name usually has a 
dozen senses that need sufficient evidence in a 
document for selecting one sense among them. 
 Canada
{Kansas,
Kentucky,
Country}
Vancouver
{British Columbia
Washington
port in USA
Port in Canada}
New York 
{Prov in USA,
New York City,
?}
Toronto
(Ontorio ,
New South Wales,
Illinois,
?}
Charlottetown
{Prov in USA,
New York City,
?}
Prince Edward Island
{Island in Canada,
Island in South Africa,
Province in Canada}
Quebec
(city in Quebec ,
Quebec Prov,
Connecticut,
?}
3*4 l ines
2*3 lines
4*11 lines
11*10 lines
3*10 lines
8*3 l ines
2*8 lines
2*43*11
3*10
3*33*8
2*10
2*3
8*4
8*11
8*10
8*4
10*4
3*11
 
Figure 4. Graph for calculating maximum weight 
spanning tree. 
But in many cases there is no explicit clue in a 
document, so the system has to choose the default 
senses that most people may refer to under 
common sense.  
The Tipster Gazetteer (http://crl.nmsu.edu/ 
cgi-bin/Tools/CLR/clrcat) used in our system has 
171,039 location entries with 237,916 total 
senses that cover most location names all over the 
world. Each location in the gazetteer may have 
several senses. Among them 30,711 location 
names have more than one sense. Although it has 
ranking tags on some location entries, a lot of 
them have no tags attached or the same rank is 
assigned to the entries of the same name. 
Manually calculating the default senses for over 
30,000 location names will be difficult and it is 
subject to inconsistency due to the different 
knowledge background of the human taggers. To 
solve this problem in calculating the default 
senses of location names, we propose to extract 
the knowledge from a corpus using statistical 
processing method.  
With the TREC-8 (Text Retrieval Conference) 
corpus, we can only extract default senses for 
1687 location names, which cannot satisfy our 
requirement. This result shows that the general 
corpus is not sufficient to suit our purpose due to 
the serious ?data sparseness? problem. Through a 
series of experiments, we found that we could 
download highly useful information from Web 
search engines such as Google, Yahoo, and 
Northern Light by searching ambiguous location 
names in the Gazetteer. Web search engines can 
provide the closest content by their built-in 
ranking mechanisms. Among those engines, we 
found that the Yahoo search engine is the best 
one for our purpose.  We wrote a script to 
download web-pages from Yahoo! using each 
ambiguous location name as a search string.   
In order to derive default senses automatically 
from the downloaded web-pages, we use the 
similarity features and scoring values between 
location-sense pairs described in Section 3. For 
example, if ?Los Angeles? co-occurs with 
?California? in the same web-page, then its sense 
will be most probably set to the city in California 
by the system.   Suppose a location word w has 
several city senses si: Sense(w) indicates the 
default sense of w; sim(wi,xjk) means the 
similarity value between two senses of the  word 
w and the j th co-occuring word xj; num(w) is the   
number of w in the document, and NumAll is the 
total number of locations.  a  is a parameter that 
reflects the importance of the co-occurring 
location names and is determined empirically. 
The default sense of w is wi that maximizes the 
similarity value with all co-occurring location 
names. The maximum similarity should be larger 
than a threshold to keep meaningful default 
senses. The threshold can be determined 
empirically through experimentation. 
)))(/())((*                
*),((maxmax)(
1 11
wnumNumAllxnum
xssimwSense
j
n
j
jkipkmi
-
= ?
= ????
a
(2) 
 
For each of 30,282 ambiguous location names, 
we used the name itself as search term in Yahoo 
to download its corresponding web-page. The 
system produced default senses for 18,446 
location names. At the same time, it discarded the 
remaining location names because the 
corresponding web-pages do not contain 
sufficient evidence to reach the threshold. We  
observed that the results reflect the correct senses 
in most cases, and found that the discarded 
location names have low references in the search 
results of other Web search engines. This means 
they will not appear frequently in text, hence 
minimal impact on system performance. We 
manually modified some of the default sense 
results based on the ranking tags in the Tipster 
Gazetteer and some additional information on 
population of the locations in order to consolidate 
the default senses.  
 6 Algorithm and Experiment 
With the information from local context, 
discourse context and the knowledge of default 
senses, the location normalization process turned 
out to be very efficient and precise. The 
processing flow is divided into 5 steps: 
Step 1. Look up the location gazetteer to 
associate candidate senses for each location NE; 
Step 2. Call the pattern matching sub-module to 
resolve the ambiguity of the NEs involved in 
local patterns like ?Williamsville, New York, 
USA? to retain only one sense for the NE as early 
as possible; 
Step 3. Apply the ?one sense per discourse? 
principle for each disambiguated location name 
to propagate the selected sense to its other 
occurrences within a document; 
Step 4. Call the global sub-module, which is a 
graph search algorithm, to resolve the remaining 
ambiguities; 
Step 5. If the decision score for a location name is 
lower than a threshold, we choose a default sense 
of that name as a result. 
For evaluating the system performance, 53 
documents from a travel site 
(http://www.worldtravelguide.net/navigate/region/na
m.asp), CNN News and New York Times are 
used. Table 2 shows some sample results from 
our test collections. For results shown in Column 
4, we first applied default senses of location 
names available from the Tipster Gazetteer in 
accordance with the rules specified in the 
gazetteer document. If there is no ranking value 
tagged for a location name, we select the first 
sense in the gazetteer as its default. This 
experiment showed accuracy of 42%. For 
Column 5, we tagged the corpus with default 
senses we derived with the method described in 
section 5, and found that it can resolve 78% 
location name ambiguity. Column 6 in Table 2 is 
the result of our LocNZ system using the 
algorithm described above as well as default 
senses we derived. The system showed promising 
results with 93.8% accuracy.  
7 Conclusion 
This paper presents a method of location 
normalization for information extraction with 
experimental results and its applications. In 
future work, we will integrate a expanded 
location gazetteer including names of landmarks, 
mountains and lakes such as Holland Tunnel (in 
New York, not in Holland) and Hoover Dam (in 
Arizona, not in Alabama), to enlarge the system 
coverage, and adjust the scoring weight given in 
Table 1 for better normalization results. Using 
context information other than location names 
can be a subtask for determining specific location 
names such as bridge or area names. 
Table 2. Experimental evaluation for location name normalization. 
 
Correctly tagged locations Document Type No. of 
Ambigu-
ous Loc 
Names 
No. of 
Ambigu-
ous 
senses  
With Tipster 
Gazetteer 
default sense 
and rule only 
With LocNZ 
default senses 
only 
LocNZ 
Precision 
(%) of 
LocNZ  
California Intro. 26 326 13 18 25 96 
Canada Intro. 14 75 13 13 14 100 
Florida Intro 22 221 10 18 20 90 
Texas Intro. 13 153 9 11 12 93 
CNN News 1 27 486 10 23 25 92 
CNN News 2 26 360 10 22 24 92 
CNN News 3 16 113 4 10 14 87.5 
New York Times 1 8 140 1 7 8 100 
New York Times 2 10 119 2 7 10 100 
New York Times 3 18 218 5 13 17 94 
Total 180 2211 77 (42%) 142 (78%) 169 (93.8%)  93.8 
 
 8 Acknowledgement 
The authors wish to thank Carrie Pine of AFRL 
for supporting this work.  Other members of 
Cymfony?s R&D team, including Sargur N. 
Srihari, have also contributed in various ways. 
References 
Cormen, Thomas H., Charles E. Leiserson, and 
Ronald L. Rivest. 1990. Introduction to 
Algorithm. The MIT Press, pp. 504-505. 
Dagon, Ido and Alon Itai. 1994. Word Sense 
Disambiguation Using a Second Language 
Monolingual Corpus. Computational 
Linguistics, Vol.20, pp. 563-596. 
Gale, W.A., K.W. Church, and D. Yarowsky. 
1992. One Sense Per Discourse. In 
Proceedings of the 4th DARPA Speech and 
Natural Language Workshop. pp. 233-237. 
Hirst, Graeme. 1987. Semantic Interpretation 
and the Resolution of Ambiguity. Cambridge 
University Press, Cambridge. 
Krupka, G.R. and K. Hausman.  1998.  IsoQuest 
Inc.: Description of the NetOwl (TM) Extractor 
System as Used for MUC-7.  Proceedings of 
MUC.  
McRoy, Susan W. 1992. Using Multiple 
Knowledge Sources for Word Sense 
Discrimination. Computational Linguistics, 
18(1): 1-30. 
Ng, Hwee Tou and Hian Beng Lee. 1996. 
Integrating Multiple Knowledge Sources to 
Disambiguate Word Sense: an Exemplar-based 
Approach. In Proceedings of 34th Annual 
Meeting of the Association for Computational 
Linguistics, pp. 40-47, California. 
Srihari, Rohini, Cheng Niu, and Wei Li. 2000. A 
Hybrid Approach for Named Entity and 
Sub-Type Tagging. In Proceedings of ANLP 
2000, Seattle. 
Yarowsky, David. 1992. Word-sense 
Disambiguation Using Statistical Models of 
Roget?s Categories Trained on Large Corpora. 
In Proceedings of the 14 th Internaional 
Conference on Computational Linguistics 
(COLING-92), pp. 454-460, Nates, France. 
Yarowsky, David. 1995. Unsupervised Word 
Sense Disambiguation Rivaling Supervised 
Methods. In Proceedings of the 33rd Annual 
Meeting of the Association for Computational 
Linguistics, Cambridge, Massachusetts. 
Appendix: Sample text and tagged result 
Few countries in the world offer as many choices to 
the world traveler as Canada. Whether your passion is 
skiing, sailing, museum-combing or indulging in 
exceptional cuisine, Canada has it all.  
Western Canada is renowned for its stunningly 
beautiful countryside. Stroll through Vancouver's 
Park, overlooking the blue waters of English Bay or 
ski the slopes of world-famous Whistler-Blackcomb, 
surrounded by thousands of hectares of pristine 
forestland. For a cultural experience, you can take an 
Aboriginal nature hike to learn about Canada's First 
Nations' history and cuisine, while outdoorsmen can 
river-raft, hike or heli-ski the thousands of kilometers 
of Canada's backcountry, where the memories of gold 
prospectors and pioneers still flourish today.  
By contrast, Canada mixes the flavor and charm of 
Europe with the bustle of trendy New York. Toronto 
boasts an irresistible array of ethnic restaurants, 
bakeries and shops to tempt the palate, while 
Charlottetown, Canada's birthplace, is located amidst 
the rolling fields and sandy Atlantic beaches of Prince 
Edward Island. Between the two, ancient Quebec City 
is a world unto itself: the oldest standing citadel in 
North America and the heart of Quebec hospitality.  
 
 
Location City Province Country 
Canada - - Canada 
Vancouver Vancouver British 
Columbia 
Canada 
New York New York New York USA 
Toronto Toronto Ontario Canada 
Charlotte- 
town 
Charlotte- 
town 
Prince 
Edward 
Island 
Canada 
Prince 
Edward 
Island 
- Prince 
Edward 
Island 
Canada 
Quebec Quebec Quebec Canada 
 
Bootstrapping for Named Entity Tagging Using Concept-based Seeds 
Cheng Niu, Wei Li, Jihong Ding, Rohini K. Srihari 
Cymfony Inc. 
600 Essjay Road, Williamsville, NY 14221. USA. 
{cniu, wei, jding, rohini}@cymfony.com
 
Abstract 
A novel bootstrapping approach to 
Named Entity ?NE?tagging using con-
cept-based seeds and successive learners 
is presented. This approach only requires 
a few common noun or pronoun seeds 
that correspond to the concept for the tar-
geted NE, e.g. he/she/man/woman for 
PERSON NE. The bootstrapping proce-
dure is implemented as training two suc-
cessive learners. First, decision list is used 
to learn the parsing-based NE rules. Then, 
a Hidden Markov Model is trained on a 
corpus automatically tagged by the first 
learner. The resulting NE system ap-
proaches supervised NE performance for 
some NE types.  
1 Overview 
Recognizing and classifying proper names is a 
fundamental task for information extraction. Three 
types of proper names are defined in the Message 
Understanding Conference (MUC) Named Entity 
(NE) standards, namely, PERSON (PER), 
ORGANIZATION (ORG), and LOCATION 
(LOC). [MUC-7 1998]  
There is considerable research on NE tagging 
using supervised machine learning [e.g. Bikel et al 
1997; Borthwick 1998]. To overcome the knowl-
edge bottleneck of supervised learning, unsuper-
vised machine learning has been applied to NE. 
[Cucchiarelli & Velardi 2001] discussed boosting 
the performance of an existing NE tagger by unsu-
pervised learning based on parsing structures. 
[Cucerzan & Yarowsky 1999], [Collins & Singer 
1999] and [Kim et al 2002] presented various 
techniques using co-training schemes for NE ex-
traction seeded by a small list of proper names or 
hand-crafted NE rules. NE tagging has two tasks: 
(i) NE chunking; (ii) NE classification. Parsing-
supported unsupervised NE learning systems in-
cluding ours only need to focus on NE classifica-
tion, assuming the NE chunks have been 
constructed by the parser.  
This paper presents a new bootstrapping ap-
proach using successive learning and concept-
based seeds.  The successive learning is as follows. 
First, parsing-based NE rules are learned with high 
precision but limited recall. Then, these rules are 
applied to a large raw corpus to automatically gen-
erate a tagged corpus. Finally, a high-performance 
HMM-based NE tagger is trained using this cor-
pus.  
Unlike co-training, our bootstrapping does not 
involve iterative learning between the two learners, 
hence it suffers little from error propagation which 
is commonly associated with iterative learning.  
To derive the parsing-based learner, the system 
only requires a few common noun or pronoun 
seeds that correspond to the concept for the tar-
geted NE, e.g. he/she/man/woman for PERSON 
NE. Such concept-based seeds share grammatical 
structures with the corresponding NEs, hence a 
parser is utilized to support bootstrapping. Since 
pronouns and common nouns occur more often 
than NE instances, the parsing-based NE rules can 
be learned in one iteration to avoid iterative learn-
ing. 
The benchmarking shows that this system ap-
proaches the performance of supervised NE tag-
gers for two of the three proper name NE types in 
MUC, namely, PER NE and LOC NE. This ap-
proach also supports tagging user-defined NE 
types. 
2 Implementation 
Figure 1 shows the overall system architecture. 
Before the bootstrapping is started, a large raw 
training corpus is parsed. The bootstrapping ex-
periment reported in this paper is based on a cor-
pus containing ~100,000 news articles and totally 
~88,000,000 words. The parsed corpus is saved 
into a repository, which supports fast retrieval by 
keyword based indexing scheme. 
 
Repository
(parsed corpus)
Decision List 
NE Learning
HMM 
NE Learning
Concept-based Seeds
parsing-based NE rules
training corpus 
based on tagged NEs
NE tagging using   parsing-based rules
NE 
Tagger
Bootstrapping Procedure
Bootstrapping Procedure
 
Figure 1. Bootstrapping System Architecture 
 
The unsupervised bootstrapping is performed as 
follows: 
1. User provides concept-based seeds; 
2. Retrieve parsing structures involving con-
cept-based seeds from the repository to train 
a decision list for NE classification; 
3.  Apply the learned rules to the NE candidates 
retrieved from the repository; 
4.  Construct an NE annotated corpus using the 
tagged proper names and their neighboring 
words; 
5. Train an HMM based on the annotated cor-
pus. 
A parser is necessary for concept-based NE 
bootstrapping. This is due to the fact that concept-
based seeds only share pattern similarity with the 
corresponding NEs at structural level, not at string 
sequence level.  In fact, the anaphoric function of 
pronouns and common nouns to represent antece-
dent NEs indicates the substitutability of proper 
names by the noun phrases headed by the corre-
sponding common nouns or pronouns. For exam-
ple, this man can substitute the proper name John 
Smith in almost all structural patterns. 
Five binary dependency relationships decoded 
by our parser are used for parsing-based NE rule 
learning:  (i) a Has_Predicate(b): from logical sub-
ject a to verb b; (ii) a Object_Of(b): from logical 
object a to verb b; (iii) a Has_Amod(b): from noun 
a to its adjective modifier b; (iv) a Possess(b): 
from the possessive noun-modifier a to head noun 
b; (v) a IsA(b):  equivalence relation (including 
appositions)  from one NP a to another NP b. 
The concept-based seeds used in the experi-
ments are: (i) he, she, his, her, him, man, woman 
for PER; (ii) city, province, town, village for LOC; 
(iii) company, firm, organization, bank, airline, 
army, committee, government, school, university 
for ORG.  
From the parsed corpus in the repository, all in-
stances (821,267) of the concept-based seeds in-
volved in the five dependency relations are 
retrieved. Each seed instance was assigned a con-
cept tag corresponding to NE. For example, each 
instance of he is marked as PER. The instances 
with concept tagging plus their associated parsing 
relationships are equivalent to an annotated NE 
corpus. Based on this training corpus, the Decision 
List Learning algorithm [Segal & Etzioni 1994] is 
used. The accuracy of each rule was evaluated us-
ing Laplace smoothing as follows, 
No.category  NEnegativepositive
1positive
++
+
=
accuracy
 
As the PER tag dominates the corpus due to the 
high occurrence frequency of he and she, learning 
is biased towards PER as the answer. To correct 
this bias, we employ the following modification 
scheme for instance count. Suppose there are a to-
tal of PERN  PER instances, LOCN  LOC instances, 
ORGN ORG instances, then in the process of rule 
accuracy evaluation, the involved instance count 
for any NE type will be adjusted by the coefficient 
NE
ORGLOCPERmin
N
) , N, N(N . 
A total of 1,290 parsing-based NE rules, shown 
in samples below, are learned, with accuracy 
higher than 0.9.  
 
Possess(wife)   PER 
Has_Predicate(divorce)  PER 
Object_Of(deport)  PER 
Possess(mayor)  LOC 
Has_AMod(coastal)  LOC 
Possess(ceo)  ORG 
Has_AMod(non-profit)  ORG 
Has_AMod(non-governmental)  ORG 
???? 
Due to the unique equivalence nature of the IsA 
relation, we add the following IsA-based rules to 
the top of the decision list: IsA(seed) tag of the 
seed, e.g. IsA(man)  PER 
The parsing-based first learner is used to tag a 
raw corpus. First, we retrieve all the named entity 
candidates associated with at least one of the five 
parsing relationships from the repository. After 
applying the decision list to the retrieved 1,607,709 
NE candidates, 33,104 PER names, 16,426 LOC 
names, and 11,908 ORG names are tagged. In or-
der to improve the bootstrapping performance, we 
use the heuristic one tag per domain for multi-
word NE in addition to the one sense per discourse 
principle [Gale et al1992]. These heuristics are 
found to be very helpful in both increasing positive 
instances (i.e. tag propagation) and decreasing the 
spurious instances (i.e. tag elimination). The tag 
propagation/elimination scheme is adopted from 
[Yarowsky 1995]. After this step, a total of 
367,441 proper names are classified, including 
134,722 PER names, 186,488 LOC names, and 
46,231 ORG names.  
The classified proper name instances lead to the 
construction of an automatically tagged training 
corpus, consisting of the NE instances and their 
two (left and right) neighboring words within the 
same sentence.  
In the final stage, a bi-gram HMM is trained 
based on the above training corpus. The HMM 
training process follows [Bikel 1997].  
3 Benchmarking 
We used the same blind testing corpus of 300,000 
words containing 20,000 PER, LOC and ORG in-
stances to measure performance degradation of 
unsupervised learning from the existing supervised 
NE tagger (Table 1, P for Precision, R for Recall, F 
for F-measure and F/D for F-measure degradation). 
 
Table 1: Supervised-to-Unsupervised NE Degradation 
 Supervised NE Unsupervised NE  
TYPE P R F P R F F/D 
PER 92.3% 93.1% 92.7% 86.6% 88.9% 87.7% 5.0%
LOC 89.0% 87.7% 88.3% 82.9% 81.7% 82.3% 6.0%
ORG 85.7% 87.8% 86.7% 57.1% 48.9% 52.7% 34.0%
 
The performance for PER and LOC are above 
80%, and approaching the performance of super-
vised learning. The reason of the unsatisfactory 
performance of ORG (52.7%) is not difficult to 
understand. There are numerous sub-types of ORG 
that cannot be represented by the less than a dozen 
concept-based seeds used for this experiment.  
In addition to the key NE types in MUC, we 
also tested this method for recognizing user-
defined NE types. We use the following concept-
based seeds for PRODUCT (PRO) NE: car, truck, 
vehicle, product, plane, aircraft, computer, soft-
ware, operating system, database, book, platform, 
network. Table 2 shows the benchmarks for 
PRODUCT tagging. 
 
Table 2: Performance for PRODUCT NE  
TYPE PRECISION RECALL F-MEASURE
PRODUCT 67.27% 72.52% 69.80%
References 
Bikel, D. M. 1997. Nymble: a high-performance learn-
ing name-finder. Proceedings of ANLP?97, 194-201, 
Morgan Kaufmann Publishers. 
Borthwick, A. et al 1998. Description of the MENE 
named Entity System. Proceedings of MUC-7. 
Collins, M. and Y. Singer. 1999. Unsupervised Models    
for Named Entity Classification. Proceedings of the 
Joint SIGAT Conference on EMNLP and 
VLC. ???Association for Computational    Linguis-
tics, 1999. 
Cucchiarelli, A. and P. Velardi. 2001. Unsupervised 
Named Entity Recognition Using Syntactic and Se-
mantic Contextual Evidence. Computational Linguis-
tics, Volume 27, Number 1, 123-131. 
Cucerzan, S. and D. Yarowsky. 1999. Language    Inde-
pendent Named Entity Recognition Combining    
Morphological and Contextual Evidence.     Proceed-
ings of the Joint SIGDAT Conference on    EMNLP 
and VLC, 90-99. 
Gale, W., K. Church, and D. Yarowsky. 1992. One 
Sense Per Discourse. Proceedings of the 4th DARPA 
Speech and Natural Language Workshop. 233-237. 
Kim, J., I. Kang, and K. Choi. 2002. Unsupervised 
Named Entity Classification Models and their En-
sembles. Proceedings of COLING 2002. 
MUC-7, 1998.  Proceedings of the Seventh Message 
Understanding Conference (MUC-7), published on 
the website http://www.muc.saic.com/ 
Segal, R. and O. Etzioni. 1994. Learning decision lists 
using homogeneous rules. Proceedings of the 12th 
National Conference on Artificial Intelligence.  
Yarowsky, David. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Method. Pro-
ceedings of ACL 1995. 
A Bootstrapping Approach to Named Entity Classification Using 
Successive Learners 
Cheng Niu, Wei Li, Jihong Ding, Rohini K. Srihari 
Cymfony Inc. 
600 Essjay Road, Williamsville, NY 14221. USA. 
{cniu, wei, jding, rohini}@cymfony.com 
 
Abstract 
This paper presents a new bootstrapping 
approach to named entity (NE) 
classification. This approach only requires 
a few common noun/pronoun seeds that 
correspond to the concept for the target 
NE type, e.g. he/she/man/woman for 
PERSON NE. The entire bootstrapping 
procedure is implemented as training two 
successive learners: (i) a decision list is 
used to learn the parsing-based high 
precision NE rules; (ii) a Hidden Markov 
Model is then trained to learn string 
sequence-based NE patterns. The second 
learner uses the training corpus 
automatically tagged by the first learner. 
The resulting NE system approaches 
supervised NE performance for some NE 
types. The system also demonstrates 
intuitive support for tagging user-defined 
NE types. The differences of this 
approach from the co-training-based NE 
bootstrapping are also discussed. 
1 Introduction 
Named Entity (NE) tagging is a fundamental task 
for natural language processing and information 
extraction. An NE tagger recognizes and classifies 
text chunks that represent various proper names, 
time, or numerical expressions. Seven types of 
named entities are defined in the Message 
Understanding Conference (MUC) standards, 
namely, PERSON (PER), ORGANIZATION 
(ORG), LOCATION (LOC), TIME, DATE, 
MONEY, and PERCENT1 (MUC-7 1998). 
                                                 
1 This paper only focuses on classifying proper names. Time and 
numerical NEs are not yet explored using this method.  
There is considerable research on NE tagging 
using different techniques. These include systems 
based on handcrafted rules (Krupka 1998), as well 
as systems using supervised machine learning, 
such as the Hidden Markov Model (HMM) (Bikel  
1997) and the Maximum Entropy Model 
(Borthwick 1998).   
The state-of-the-art rule-based systems and 
supervised learning systems can reach near-human 
performance for NE tagging in a targeted domain. 
However, both approaches face a serious 
knowledge bottleneck, making rapid domain 
porting difficult. Such systems cannot effectively 
support user-defined named entities. That is the 
motivation for using unsupervised or weakly-
supervised machine learning that only requires a 
raw corpus from a given domain for this NE 
research. 
(Cucchiarelli & Velardi 2001) discussed 
boosting the performance of an existing NE tagger 
by unsupervised learning based on parsing 
structures. (Cucerzan & Yarowsky 1999), (Collins 
& Singer 1999) and (Kim 2002) presented various 
techniques using co-training schemes for NE 
extraction seeded by a small list of proper names 
or handcrafted NE rules. NE tagging has two tasks: 
(i) NE chunking; (ii) NE classification. Parsing-
supported NE bootstrapping systems including 
ours only focus on NE classification, assuming NE 
chunks have been constructed by the parser.       
The key idea of co-training is the separation of 
features into several orthogonal views. In case of 
NE classification, usually one view uses the 
context evidence and the other relies on the lexicon 
evidence. Learners corresponding to different 
views learn from each other iteratively. 
One issue of co-training is the error propagation 
problem in the process of the iterative learning. 
The rule precision drops iteration-by-iteration. In 
the early stages, only few instances are available 
for learning. This makes some powerful statistical 
models such as HMM difficult to use due to the 
extremely sparse data.  
This paper presents a new bootstrapping 
approach using successive learning and concept-
based seeds.  The successive learning is as follows. 
First, some parsing-based NE rules are learned 
with high precision but limited recall. Then, these 
rules are applied to a large raw corpus to 
automatically generate a tagged corpus. Finally, an 
HMM-based NE tagger is trained using this 
corpus. There is no iterative learning between the 
two learners, hence the process is free of the error 
propagation problem. The resulting NE system 
approaches supervised NE performance for some 
NE types. 
To derive the parsing-based learner, instead of 
seeding the bootstrapping process with NE 
instances from a proper name list or handcrafted 
NE rules as (Cucerzan & Yarowsky 1999), 
(Collins & Singer 1999) and (Kim 2002), the 
system only requires a few common noun or 
pronoun seeds that correspond to the concept for 
the targeted NE, e.g. he/she/man/woman for 
PERSON NE. Such concept-based seeds share 
grammatical structures with the corresponding 
NEs, hence a parser is utilized to support 
bootstrapping. Since pronouns and common nouns 
occur more often than NE instances, richer 
contextual evidence is available for effective 
learning. Using concept-based seeds, the parsing-
based NE rules can be learned in one iteration so 
that the error propagation problem in the iterative 
learning can be avoided.  
This method is also shown to be effective for 
supporting NE domain porting and is intuitive for 
configuring an NE system to tag user-defined NE 
types. 
The remaining part of the paper is organized as 
follows. The overall system design is presented in 
Section 2. Section 3 describes the parsing-based 
NE learning. Section 4 presents the automatic 
construction of annotated NE corpus by parsing-
based NE classification. Section 5 presents the 
string level HMM NE learning. Benchmarks are 
shown in Section 6. Section 7 is the Conclusion. 
2 System Design 
Figure 1 shows the overall system architecture. 
Before the bootstrapping is started, a large raw 
training corpus is parsed by the English parser 
from our InfoXtract system (Srihari et al 2003).  
The bootstrapping experiment reported in this 
paper is based on a corpus containing ~100,000 
news articles and a total of ~88,000,000 words. 
The parsed corpus is saved into a repository, which 
supports fast retrieval by a keyword-based 
indexing scheme. 
Although the parsing-based NE learner is found 
to suffer from the recall problem, we can apply the 
learned rules to a huge parsed corpus. In other 
words, the availability of an almost unlimited raw 
corpus compensates for the modest recall. As a 
result, large quantities of NE instances are 
automatically acquired. An automatically 
annotated NE corpus can then be constructed by 
extracting the tagged instances plus their 
neighboring words from the repository. 
 
Repository
(parsed corpus)
Decision List 
NE Learning
HMM 
NE Learning
Concept-based Seeds
parsing-based NE rules
training corpus 
based on tagged NEs
NE tagging using   parsing-based rules
NE 
Tagger
Bootstrapping Procedure
Bootstrapping Procedure
 
Figure 1. Bootstrapping System Architecture 
 
The bootstrapping is performed as follows: 
1. Concept-based seeds are provided by the 
user. 
2. Parsing structures involving concept-based 
seeds are retrieved from the repository to 
train a decision list for NE classification. 
3. The learned rules are applied to the NE 
candidates stored in the repository. 
4. The proper names tagged in Step 3 and 
their neighboring words are put together as 
an NE annotated corpus. 
5. An HMM is trained based on the annotated 
corpus. 
3 Parsing-based NE Rule Learning 
The training of the first NE learner has three major 
properties: (i) the use of concept-based seeds, (ii) 
support from the parser, and (iii) representation as 
a decision list.  
This new bootstrapping approach is based on 
the observation that there is an underlying concept 
for any proper name type and this concept can be 
easily expressed by a set of common nouns or 
pronouns, similar to how concepts are defined by 
synsets in WordNet (Beckwith 1991).  
Concept-based seeds are conceptually 
equivalent to the proper name types that they 
represent. These seeds can be provided by a user 
intuitively. For example, a user can use pill, drug, 
medicine, etc. as concept-based seeds to guide the 
system in learning rules to tag MEDICINE names. 
This process is fairly intuitive, creating a favorable 
environment for configuring the NE system to the 
types of names sought by the user. 
An important characteristic of concept-based 
seeds is that they occur much more often than 
proper name seeds, hence they are effective in 
guiding the non-iterative NE bootstrapping.  
A parser is necessary for concept-based NE 
bootstrapping. This is due to the fact that concept-
based seeds only share pattern similarity with the 
corresponding NEs at structural level, not at string 
sequence level. For example, at string sequence 
level, PERSON names are often preceded by a set 
of prefixing title words Mr./Mrs./Miss/Dr. etc., but 
the corresponding common noun seeds 
man/woman etc. cannot appear in such patterns. 
However, at structural level, the concept-based 
seeds share the same or similar linguistic patterns 
(e.g. Subject-Verb-Object patterns) with the 
corresponding types of proper names.  
The rationale behind using concept-based seeds 
in NE bootstrapping is similar to that for parsing-
based word clustering (Lin 1998): conceptually 
similar words occur in structurally similar context.  
In fact, the anaphoric function of pronouns and 
common nouns to represent antecedent NEs 
indicates the substitutability of proper names by 
the corresponding common nouns or pronouns. For 
example, this man can be substituted for the proper 
name John Smith in almost all structural patterns. 
Following the same rationale, a bootstrapping 
approach is applied to the semantic lexicon 
acquisition task [Thelen & Riloff. 2002]. 
The InfoXtract parser supports dependency 
parsing based on the linguistic units constructed by 
our shallow parser (Srihari et al 2003). Five types 
of the decoded dependency relationships are used 
for parsing-based NE rule learning.  These are all 
directional, binary dependency links between 
linguistic units:   
 
(1) Has_Predicate: from logical subject to verb 
e.g.  He said she would want him to join.  
he: Has_Predicate(say) 
she: Has_Predicate(want) 
him: Has_Predicate(join) 
(2) Object_Of : from logical object to verb 
e.g.  This company was founded to provide  
new telecommunication services.  
company: Object_Of(found) 
service: Object_Of(provide) 
(3) Has_Amod: from noun to its adjective modifier 
e.g. He is a smart, handsome young man.  
man: Has_AMod(smart) 
man: Has_AMod(handsome) 
man: Has_AMod(young)  
(4) Possess: from the possessive noun-modifier to 
head noun 
e.g. His son was elected as mayor of the city.  
his: Possess(son) 
city: Possess(mayor) 
(5) IsA:  equivalence relation from one NP to 
another NP  
e.g. Microsoft spokesman John Smith is a  
popular man.  
spokesman: IsA(John Smith) 
John Smith: IsA(man) 
 
The concept-based seeds used in the 
experiments are: 
 
1. PER: he, she, his, her, him, man, woman 
2. LOC: city, province, town, village 
3. ORG: company, firm, organization, bank, 
airline, army, committee, government, 
school, university 
4. PRO: car, truck, vehicle, product, plane, 
aircraft, computer, software, operating 
system, data-base, book, platform, network 
 
Note that the last target tag PRO (PRODUCT) 
is beyond the MUC NE standards: we added this 
NE type for the purpose of testing the system?s 
capability in supporting user-defined NE types.  
From the parsed corpus in the repository, all 
instances of the concept-based seeds associated 
with one or more of the five dependency relations 
are retrieved:  821,267 instances in total in our 
experiment. Each seed instance was assigned a 
concept tag corresponding to NE. For example, 
each instance of he is marked as PER. The marked 
instances plus their associated parsing relationships 
form an annotated NE corpus, as shown below: 
 
he/PER:   Has_Predicate(say) 
she/PER:   Has_Predicate(get) 
company/ORG:  Object_Of(compel) 
city/LOC:   Possess(mayor) 
car/PRO:  Object_Of(manufacture) 
HasAmod(high-quality) 
???? 
 
This training corpus supports the Decision List 
Learning which learns homogeneous rules (Segal 
& Etzioni 1994). The accuracy of each rule was 
evaluated using Laplace smoothing: 
 
No.category  NEnegativepositive
1positive
++
+
=accuracy  
 
It is noteworthy that the PER tag dominates the 
corpus due to the fact that the pronouns he and she 
occur much more often than the seeded common 
nouns. So the proportion of NE types in the 
instances of concept-based seeds is not the same as 
the proportion of NE types in the proper name 
instances. For example, considering a running text 
containing one instance of John Smith and one 
instance of a city name Rochester, it is more likely 
that John Smith will be referred to by he/him than 
Rochester by (the) city. Learning based on such a 
corpus is biased towards PER as the answer. 
To correct this bias, we employ the following 
modification scheme for instance count. Suppose 
there are a total of PERN  PER instances, LOCN  
LOC instances, ORGN  ORG instances, PRON  PRO 
instances, then in the process of rule accuracy 
evaluation, the involved instance count for any NE 
type will be adjusted by the coefficient  
NE
PRO,ORGLOCPERmin
N
) N, N, N(N . For example, if 
the number of the training instances of PER is ten 
times that of PRO, then when evaluating a rule 
accuracy, any positive/negative count associated 
with PER will be discounted by 0.1 to correct the 
bias.  
A total of 1,290 parsing-based NE rules are 
learned, with accuracy higher than 0.9. The 
following are sample rules of the learned decision 
list: 
 
Possess(wife)  PER 
Possess(husband)  PER 
Possess(daughter)  PER 
Possess(bravery)  PER 
Possess(father)  PER 
Has_Predicate(divorce)  PER 
Has_Predicate(remarry)  PER 
Possess(brother)  PER 
Possess(son)  PER 
Possess(mother)  PER 
Object_Of(deport)  PER 
Possess(sister)  PER 
Possess(colleague)  PER 
Possess(career)  PER 
Possess(forehead)  PER 
Has_Predicate(smile)  PER 
Possess(respiratory system)  PER 
{Has_Predicate(threaten),   
  Has_Predicate(kill)} PER 
???? 
Possess(concert hall)  LOC 
Has_AMod(coastal)  LOC 
Has_AMod(northern)  LOC 
Has_AMod(eastern)  LOC 
Has_AMod(northeastern)  LOC 
Possess(undersecretary)  LOC 
Possess(mayor)  LOC 
Has_AMod(southern)  LOC 
Has_AMod(northwestern)  LOC 
Has_AMod(populous)  LOC 
Has_AMod(rogue)  LOC 
Has_AMod(southwestern)  LOC 
Possess(medical examiner)  LOC 
Has_AMod(edgy)  LOC 
???? 
Has_AMod(broad-base)  ORG 
Has_AMod(advisory)  ORG 
Has_AMod(non-profit)  ORG 
Possess(ceo)  ORG 
Possess(operate loss)  ORG 
Has_AMod(multinational)  ORG 
Has_AMod(non-governmental)  ORG 
Possess(filings)  ORG 
Has_AMod(interim)  ORG 
Has_AMod(for-profit)  ORG 
Has_AMod(not-for-profit)  ORG 
Has_AMod(nongovernmental)  ORG 
Object_Of(undervalue)  ORG 
???? 
Has_AMod(handheld)  PRO 
Has_AMod(unman)  PRO 
Has_AMod(well-sell)  PRO 
Has_AMod(value-add)  PRO 
Object_Of(refuel)  PRO 
Has_AMod(fuel-efficient)  PRO 
Object_Of(vend)  PRO 
Has_Predicate(accelerate)  PRO 
Has_Predicate(collide)  PRO 
Object_Of(crash)  PRO 
Has_AMod(scalable)  PRO 
Possess(patch)  PRO 
Object_Of(commercialize)PRO  
Has_AMod(custom-design)  PRO 
Possess(rollout)  PRO 
Object_Of(redesign)  PRO 
???? 
 
Due to the unique equivalence nature of the IsA 
relation, the above bootstrapping procedure can 
hardly learn IsA-based rules. Therefore, we add the 
following IsA-based rules to the top of the decision 
list: IsA(seed) tag of the seed, for example: 
 
IsA(man)  PER 
IsA(city)  LOC 
IsA(company)  ORG 
IsA(software)  PRO 
4 Automatic Construction of Annotated 
NE Corpus 
In this step, we use the parsing-based first learner 
to tag a raw corpus in order to train the second NE 
learner. 
One issue with the parsing-based NE rules is 
modest recall. For incoming documents, 
approximately 35%-40% of the proper names are 
associated with at least one of the five parsing 
relations. Among these proper names associated 
with parsing relations, only ~5% are recognized by 
the parsing-based NE rules. 
So we adopted the strategy of applying the 
parsing-based rules to a large corpus (88 million 
words), and let the quantity compensate for the 
sparseness of tagged instances. A repository level 
consolidation scheme is also used to improve the 
recall.  
The NE classification procedure is as follows. 
From the repository, all the named entity 
candidates associated with at least one of the five 
parsing relationships are retrieved. An NE 
candidate is defined as any chunk in the parsed 
corpus that is marked with a proper name Part-Of-
Speech (POS) tag (i.e. NNP or NNPS). A total of 
1,607,709 NE candidates were retrieved in our 
experiment. A small sample of the retrieved NE 
candidates with the associated parsing 
relationships are shown below: 
 
Deep South : Possess(project) 
Ramada : Possess(president) 
Argentina : Possess(first lady) 
???? 
 
After applying the decision list to the above the 
NE candidates, 33,104 PER names, 16,426 LOC 
names, 11,908 ORG names and 6,280 PRO names 
were extracted. 
It is a common practice in the bootstrapping 
research to make use of heuristics that suggest 
conditions under which instances should share the 
same answer. For example, the one sense per 
discourse principle is often used for word sense 
disambiguation (Gale et al 1992). In this research, 
we used the heuristic one tag per domain for multi-
word NE in addition to the one sense per discourse 
principle. These heuristics were found to be very 
helpful in improving the performance of the 
bootstrapping algorithm for the purpose of both 
increasing positive instances (i.e. tag propagation) 
and decreasing the spurious instances (i.e. tag 
elimination). The following are two examples to 
show how the tag propagation and elimination 
scheme works.  
Tyco Toys occurs 67 times in the corpus, and 11 
instances are recognized as ORG, only one 
instance is recognized as PER. Based on the 
heuristic one tag per domain for multi-word NE, 
the minority tag of PER is removed, and all the 67 
instances of Tyco Toys are tagged as ORG. 
Three instances of Postal Service are 
recognized as ORG, and two instances are 
recognized as PER. These tags are regarded as 
noise, hence are removed by the tag elimination 
scheme.  
The tag propagation/elimination scheme is 
adopted from (Yarowsky 1995). After this step, a 
total of 386,614 proper names were recognized, 
including 134,722 PER names, 186,488 LOC 
names, 46,231 ORG names and 19,173 PRO 
names. The overall precision was ~90%. The 
benchmark details will be shown in Section 6. 
The extracted proper name instances then led to 
the construction of a fairly large training corpus 
sufficient for training the second NE learner. 
Unlike manually annotated running text corpus, 
this corpus consists of only sample string 
sequences containing the automatically tagged NE 
instances and their left and right neighboring 
words within the same sentence. The two 
neighboring words are always regarded as common 
words while constructing the corpus. This is based 
on the observation that the proper names usually 
do not occur continuously without any punctuation 
in between. 
A small sample of the automatically 
constructed corpus is shown below: 
 
in <LOC> Argentina </LOC> . 
<LOC> Argentina </LOC> 's 
and <PER> Troy Glaus </PER> walk 
call <ORG> Prudential Associates </ORG> .  
, <PRO> Photoshop </PRO> has 
not <PER> David Bonderman </PER> , 
???? 
 
This corpus is used for training the second NE 
learner based on evidence from string sequences, 
to be described in Section 5 below. 
5 String Sequence-based NE Learning 
String sequence-based HMM learning is set as our 
final goal for NE bootstrapping because of the 
demonstrated high performance of this type of NE 
taggers.  
In this research, a bi-gram HMM is trained 
based on the sample strings in the annotated corpus 
constructed in section 4. During the training, each 
sample string sequence is regarded as an 
independent sentence. The training process is 
similar to (Bikel 1997).  
The HMM is defined as follows: Given a word 
sequence nn00 fwfwsequenceW =  (where 
jf denotes a single token feature which will be 
defined below), the goal for the NE tagging task is 
to find the optimal NE tag sequence 
n210 ttttsequence T = , which maximizes the 
conditional probability sequence)W |sequence Pr(T  
(Bikel 1997). By Bayesian equality, this is 
equivalent to maximizing the joint probability 
sequence) Tsequence,Pr(W . This joint probability 
can be computed by bi-gram HMM as follows:  
 
? ?=
i
)t,f,w|t,f,wPr(
sequence) T sequence,Pr(W 
1i1-i1-iiii
 
The back-off model is as follows,  
 
)t,w|)Pr(tt,t|f,wPr()-(1
)t,f,w|t,f,w(P
)t,f,w|t,f,wPr(
1i1ii1iiii1
1i1-i1-iiii01
1i1-i1-iiii
???
?
?
+
=
?
?  
 
where V denotes the size of the vocabulary, the 
back-off coefficients ??s are determined using the 
Witten-Bell smoothing algorithm. The quantities 
)t,,w|t,f,w(P 1i11iiii0 ??? if , 
)t,t|f,w(P 1iiii0 ? , )t,w|(tP 1i1-ii0 ? ,
)t|f,w(P iii0 , )t|(fP ii0 , )w|(tP 1-ii0 , )(tP i0 , and 
)t|(wP ii0  are computed by the maximum 
likelihood estimation.  
We use the following single token feature set 
for HMM training. The definitions of these 
features are the same as in (Bikel 1997). 
 
 
 
)t | f,w Pr( ) - (1 )t,t | f, w (P 
)t,t |f,w Pr(
iii 2 1iiii02
1iiii
? ? + = 
?
?
 
)w | Pr(t ) -(1 )t ,w|(tP 
)t ,w | Pr(t
1 - i i 3 1i1-ii03
1i1-ii
? ? + = 
?
?
 
)t | (f)Pt | (w Pr ) -(1)t |f,w (P 
)t|f,w Pr(
ii0i i 4 iii04
iii
? ? + = 
) t ( P ) - (1 ) w | (t P ) w | Pr(t i0 5 1 - ii051-ii ? ? + = 
V 
1 ) - (1 )t |(wP)t| Pr(w 6 ii06ii ? ? + = 
twoDigitNum, fourDigitNum, 
containsDigitAndAlpha,  
containsDigitAndDash, 
containsDigitAndSlash,  
containsDigitAndComma,  
containsDigitAndPeriod, otherNum, allCaps,  
capPeriod, initCap, lowerCase, other.  
6 Benchmarking and Discussion 
Two types of benchmarks were measured: (i) the 
quality of the automatically constructed NE 
corpus, and (ii) the performance of the HMM NE 
tagger. The HMM NE tagger is considered to be 
the resulting system for application. The 
benchmarking shows that this system approaches 
the performance of supervised NE tagger for two 
of the three proper name NE types in MUC, 
namely, PER NE and LOC NE. 
We used the same blind testing corpus of 
300,000 words containing 20,000 PER, LOC and 
ORG instances that were truthed in-house 
originally for benchmarking the existing 
supervised NE tagger (Srihari, Niu & Li 2000). 
This has the benefit of precisely measuring 
performance degradation from the supervised 
learning to unsupervised learning. The 
performance of our supervised NE tagger using the 
MUC scorer is shown in Table 1. 
 
Table 1. Performance of Supervised NE Tagger 
Type Precision Recall F-Measure
PERSON 92.3% 93.1% 92.7% 
LOCATION 89.0% 87.7% 88.3% 
ORGANIZATION 85.7% 87.8% 86.7% 
 
To benchmark the quality of the automatically 
constructed corpus (Table 2), the testing corpus is 
first processed by our parser and then saved into 
the repository. The repository level NE 
classification scheme, as discussed in section 4, is 
applied. From the recognized NE instances, the 
instances occurring in the testing corpus are 
compared with the answer key.  
 
Table 2. Quality of the Constructed Corpus 
Type Precision 
PERSON 94.3% 
LOCATION 91.7% 
ORGANIZATION 88.5% 
To benchmark the performance of the HMM 
tagger, the testing corpus is parsed. The noun 
chunks with proper name POS tags (NNP and 
NNPS) are extracted as NE candidates. The 
preceding word and the succeeding word of the NE 
candidates are also extracted.  Then we apply the 
HMM to the NE candidates with their neighboring 
context. The NE classification results are shown in 
Table 3.  
 
Table 3. Performance of the second HMM NE 
Type Precision Recall F-Measure
PERSON 86.6% 88.9% 87.7% 
LOCATION 82.9% 81.7% 82.3% 
ORGANIZATION 57.1% 48.9% 52.7% 
 
Compared with our existing supervised NE 
tagger, the degradation using the presented 
bootstrapping method for PER NE, LOC NE, and 
ORG NE are 5%, 6%, and 34% respectively.  
The performance for PER and LOC are above 
80%, approaching the performance of supervised 
learning. The reason for the low recall of ORG 
(~50%) is not difficult to understand. For PERSON 
and LOCATION, a few concept-based seeds seem 
to be sufficient in covering their sub-types (e.g. the 
sub-types COUNTRY, CITY, etc for 
LOCATION). But there are hundreds of sub-types 
of ORG that cannot be covered by less than a 
dozen concept-based seeds, which we used. As a 
result, the recall of ORG is significantly affected. 
Due to the same fact that ORG contains many 
more sub-types, the results are also noisier, leading 
to lower precision than that of the other two NE 
types. Some threshold can be introduced, e.g. 
perplexity per word, to remove spurious ORG tags 
in improving the precision. As for the recall issue, 
fortunately, in a real-life application, the 
organization type that a user is interested in usually 
is in a fairly narrow spectrum. We believe that the 
performance will be better if only company names 
or military organization names are targeted. 
In addition to the key NE types in MUC, our 
system is able to recognize another NE type, 
namely, PRODUCT (PRO) NE. We instructed our 
truthing team to add this NE type into the testing 
corpus which contains ~2,000 PRO instances. 
Table 4 shows the performance of the HMM on the 
PRO tag. 
 
Table 4. Performance of PRODUCT NE 
TYPE PRECISION RECALL F-MEASURE
PRODUCT 67.3% 72.5% 69.8% 
 
Similar to the case of ORG NEs, the number of 
concept-based seeds is found to be insufficient to 
cover the variations of PRO subtypes. So the 
performance is not as good as PER and LOC NEs. 
Nevertheless, the benchmark shows the system 
works fairly effectively in extracting the user-
specified NEs. It is noteworthy that domain 
knowledge such as knowing the major sub-types of 
the user-specified NE type is valuable in assisting 
the selection of appropriate concept-based seeds 
for performance enhancement. 
The performance of our HMM tagger is 
comparable with the reported performance in 
(Collins & Singer 1999). But our benchmarking is 
more extensive as we used a much larger data set 
(20,000 NE instances in the testing corpus) than 
theirs (1,000 NE instances).  
7 Conclusion 
A novel bootstrapping approach to NE 
classification is presented. This approach does not 
require iterative learning which may suffer from 
error propagation. With minimal human 
supervision in providing a handful of concept-
based seeds, the resulting NE tagger approaches 
supervised NE performance in NE types for 
PERSON and LOCATION. The system also 
demonstrates effective support for user-defined NE 
classification. 
Acknowledgement 
This work was partly supported by a grant from the 
Air Force Research Laboratory?s Information 
Directorate (AFRL/IF), Rome, NY, under contract 
F30602-01-C-0035. The authors wish to thank 
Carrie Pine and Sharon Walter of AFRL for 
supporting and reviewing this work. 
References 
 
Bikel, D. M. 1997. Nymble: a high-performance 
learning name-finder. Proceedings of ANLP 1997, 
194-201, Morgan Kaufmann Publishers. 
Beckwith, R. et al 1991.  WordNet: A Lexical Database 
Organized on Psycholinguistic Principles.  Lexicons: 
Using On-line Resources to build a Lexicon, Uri 
Zernik, editor, Lawrence Erlbaum, Hillsdale, NJ. 
Borthwick, A. et al 1998. Description of the MENE 
named Entity System. Proceedings of MUC-7. 
Collins, M. and Y. Singer. 1999. Unsupervised Models    
for Named Entity Classification. Proceedings of the 
1999 Joint SIGDAT Conference on EMNLP and VLC. 
Cucchiarelli, A. and P. Velardi. 2001. Unsupervised 
Named Entity Recognition Using Syntactic and Se-
mantic Contextual Evidence. Computational 
Linguistics, Volume 27, Number 1, 123-131. 
Cucerzan, S. and D. Yarowsky. 1999. Language    
Independent Named Entity Recognition Combining    
Morphological and Contextual Evidence.     
Proceedings of the 1999 Joint SIGDAT Conference on    
EMNLP  and VLC, 90-99. 
Gale, W., K. Church, and D. Yarowsky. 1992. One 
Sense Per Discourse. Proceedings of the 4th DARPA 
Speech and Natural Language Workshop. 233-237. 
Kim, J., I. Kang, and K. Choi. 2002. Unsupervised 
Named Entity Classification Models and their 
Ensembles. COLING 2002. 
Krupka, G. R. and K. Hausman. 1998. IsoQuest Inc: 
Description of the NetOwl Text Extraction System as 
used for MUC-7. Proceedings of MUC-7. 
Lin, D.K. 1998. Automatic Retrieval and Clustering of 
Similar Words. COLING-ACL 1998. 
MUC-7, 1998.  Proceedings of the Seventh Message 
Understanding Conference (MUC-7).  
Thelen, M. and E. Riloff. 2002. A Bootstrapping 
Method for Learning Semantic Lexicons using 
Extraction Pattern Contexts. Proceedings of EMNLP 
2002.  
Segal, R. and O. Etzioni. 1994. Learning decision lists 
using homogeneous rules. Proceedings of the 12th 
National Conference on Artificial Intelligence.  
Srihari, R., W. Li, C. Niu and T. Cornell. 2003. 
InfoXtract: An Information Discovery Engine 
Supported by New Levels of Information Extraction.  
Proceeding of HLT-NAACL 2003 Workshop on 
Software Engineering and Architecture of Language 
Technology Systems, Edmonton, Canada. 
Srihari, R., C. Niu, & W. Li. 2000. A Hybrid Approach 
for Named Entity and Sub-Type Tagging.  
Proceedings of ANLP 2000, Seattle.  
Yarowsky, David. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Method. ACL 
1995.  
An Expert Lexicon Approach to Identifying English Phrasal Verbs 
 
Wei Li, Xiuhong Zhang, Cheng Niu, Yuankai Jiang, Rohini Srihari  
 
Cymfony Inc. 
600 Essjay Road 
Williamsville, NY 14221, USA 
{wei, xzhang, cniu, yjiang, rohini}@Cymfony.com
 
Abstract 
Phrasal Verbs are an important feature 
of the English language. Properly 
identifying them provides the basis for 
an English parser to decode the related 
structures. Phrasal verbs have been a 
challenge to Natural Language 
Processing (NLP) because they sit at 
the borderline between lexicon and 
syntax. Traditional NLP frameworks 
that separate the lexicon module from 
the parser make it difficult to handle 
this problem properly.  This paper 
presents a finite state approach that 
integrates a phrasal verb expert lexicon 
between shallow parsing and deep 
parsing to handle morpho-syntactic 
interaction. With precision/recall 
combined performance benchmarked 
consistently at 95.8%-97.5%, the 
Phrasal Verb identification problem 
has basically been solved with the 
presented method.    
1 Introduction 
Any natural language processing (NLP) system 
needs to address the issue of handling  multiword 
expressions, including Phrasal Verbs (PV) [Sag 
et al 2002; Breidt et al 1996]. This paper 
presents a proven approach to identifying 
English PVs based on pattern matching using a 
formalism called Expert Lexicon.   
Phrasal Verbs are an important feature of the 
English language since they form about one third 
of the English verb vocabulary. 1  Properly 
                                                     
1 For the verb vocabulary of our system based on  
machine-readable dictionaries and two Phrasal Verb 
dictionaries, phrasal verb entries constitute 33.8% of 
the entries. 
recognizing PVs is an important condition for  
English parsing. Like single-word verbs, each 
PV has its own lexical features including 
subcategorization features that determine its 
structural patterns [Fraser 1976; Bolinger 1971; 
Pelli 1976; Shaked 1994], e.g., look for has 
syntactic subcategorization and semantic features 
similar to those of search; carry?on shares 
lexical features with continue. Such lexical 
features can be represented in the PV lexicon in 
the same way as those for single-word verbs, but 
a parser can only use them when the PV is 
identified. 
Problems like PVs are regarded as ?a pain in 
the neck for NLP? [Sag et al 2002]. A proper 
solution to this problem requires tighter 
interaction between syntax and lexicon than 
traditionally available [Breidt et al 1994].  
Simple lexical lookup leads to severe 
degradation in both precision and recall, as our 
benchmarks show (Section 4). The recall 
problem is mainly due to separable PVs such as 
turn?off which allow for syntactic units to be 
inserted inside the PV compound, e.g., turn it off, 
turn the radio off.  The precision problem is 
caused by the ambiguous function of the particle. 
For example, a simple lexical lookup will mistag 
looked for as a phrasal verb in sentences such as 
He looked for quite a while but saw nothing. 
In short, the traditional NLP framework that 
separates the lexicon module from a parser 
makes it difficult to handle this problem properly.  
This paper presents an expert lexicon approach 
that integrates the lexical module with contextual 
checking based on shallow parsing results.  
Extensive blind benchmarking shows that this 
approach is very effective for identifying phrasal 
verbs, resulting in the precision/recall combined 
F-score of about 96%.   
 The remaining text is structured as follows. 
Section 2 presents the problem and defines the 
task. Section 3 presents the Expert Lexicon 
formalism and illustrates the use of this 
formalism in solving this problem. Section 4 
shows the benchmarking and analysis, followed 
by conclusions in Section 5. 
2 Phrasal Verb Challenges  
This section defines the problems we intend to 
solve, with a checklist of tasks to accomplish.  
2.1 Task Definition 
First, we define the task as the identification of 
PVs in support of deep parsing, not as the parsing 
of the structures headed by a PV. These two are 
separated as two tasks not only because of 
modularity considerations, but more importantly 
based on a natural labor division between NLP 
modules.  
Essential to the second argument is that these 
two tasks are of a different linguistic nature: the 
identification task belongs to (compounding) 
morphology (although it involves a syntactic 
interface) while the parsing task belongs to 
syntax. The naturalness of this division is 
reflected in the fact that there is no need for a 
specialized, PV-oriented parser. The same parser, 
mainly driven by lexical subcategorization 
features, can handle the structural problems for 
both phrasal verbs and other verbs. The 
following active and passive structures involving 
the PVs look after (corresponding to watch) and 
carry?on (corresponding to continue) are 
decoded by our deep parser after PV 
identification: she is being carefully ?looked 
after? (watched); we should ?carry on? (continue) 
the business for a while. 
There has been no unified definition of PVs 
among linguists. Semantic compositionality is 
often used as a criterion to distinguish a PV from 
a syntactic combination between a verb and its 
associated adverb or prepositional phrase 
[Shaked 1994]. In reality, however, PVs reside in 
a continuum from opaque to transparent in terms 
of semantic compositionality [Bolinger 1971]. 
There exist fuzzy cases such as take something 
away2 that may be included either as a PV or as a 
regular syntactic sequence. There is agreement 
                                                     
2  Single-word verbs like ?take? are often 
over-burdened with dozens of senses/uses. Treating 
marginal cases like ?take?away? as independent 
phrasal verb entries has practical benefits in relieving 
the burden and the associated noise involving ?take?.   
on the vocabulary scope for the majority of PVs, 
as reflected in the overlapping of PV entries from 
major English dictionaries.   
English PVs are generally classified into three 
major types. Type I usually takes the form of an 
intransitive verb plus a particle word that 
originates from a preposition. Hence the resulting 
compound verb has become transitive, e.g., look 
for, look after, look forward to, look into, etc. 
Type II typically takes the form of a transitive 
verb plus a particle from the set {on, off, up, 
down}, e.g., turn?on, take?off, wake?up, 
let?down. Marginal cases of particles may also 
include {out, in, away} such as take?away, 
kick ?in, pull?out.3   
Type III takes the form of an intransitive verb 
plus an adverb particle, e.g., get by, blow up, burn 
up, get off, etc. Note that Type II and Type III 
PVs have considerable overlapping in  
vocabulary, e.g., The bomb blew up vs. The 
clown blew up the balloon. The overlapping 
phenomenon can be handled by assigning both a 
transitive feature and an intransitive feature to the 
identified PVs in the same way that we treat the 
overlapping of single-word verbs.   
The first issue in handling PVs is inflection. A 
system for identifying PVs should match the 
inflected forms, both regular and irregular, of the 
leading verb.  
The second is the representation of the lexical 
identity of recognized PVs. This is to establish a 
PV (a compound word) as a syntactic atomic unit 
with all its lexical properties determined by the 
lexicon [Di Sciullo and Williams 1987]. The 
output of the identification module based on a PV 
lexicon should support syntactic analysis and 
further processing. This translates into two 
sub-tasks: (i) lexical feature assignment, and (ii) 
canonical form representation. After a PV is 
identified, its lexical features encoded in the PV 
lexicon should be assigned for a parser to use. 
The representation of a canonical form for an 
identified PV is necessary to allow for individual 
rules to be associated with identified PVs in 
further processing and to facilitate verb retrieval 
in applications. For example, if we use turn_off 
as the canonical form for the PV turn?off, 
identified in both he turned off the radio and he 
                                                     
3 These three are arguably in the gray area. Since they 
do not fundamentally affect the meaning of the 
leading verb, we do not have to treat them as phrasal 
verbs.  In principle, they can also be treated as  adverb 
complements of verbs.   
turned the radio off, a search for turn_off will 
match all and only the mentions of this PV.  
The fact that PVs are separable hurts recall. In 
particular, for Type II, a Noun Phrase (NP) object 
can be inserted inside the compound verb. NP 
insertion is an intriguing linguistic phenomenon 
involving the morpho-syntactic interface: a 
morphological compounding process needs to 
interact with the formation of a syntactic unit.  
Type I PVs also have the separability problem, 
albeit to a lesser degree.  The possible inserted 
units are adverbs in this case, e.g., look 
everywhere for, look carefully after.   
What hurts precision is spurious matches of 
PV negative instances. In a sentence with the 
structure V+[P+NP], [V+P] may be mistagged as 
a PV, as seen in the following pairs of examples 
for Type I and Type II:  
 
(1a) She [looked for] you yesterday. 
(1b) She looked [for quite a while] (but saw  
nothing). 
(2a) She [put on] the coat. 
(2b) She put [on the table] the book she  
borrowed yesterday. 
 
To summarize, the following is a checklist of 
problems that a PV identification system should 
handle: (i) verb inflection, (ii) lexical identity 
representation, (iii) separability, and (iv) 
negative instances. 
2.2 Related Work 
Two lines of research are reported in addressing 
the PV problem: (i) the use of a high-level 
grammar formalism that integrates the 
identification with parsing, and (ii) the use of a 
finite state device in identifying PVs as a lexical 
support for the subsequent parser. Both 
approaches have their own ways of handling the 
morpho-syntactic interface. 
[Sag et al 2002] and [Villavicencio et al 
2002] present their project LinGO-ERG that 
handles PV identification and parsing together. 
LingGO-ERG is based on Head-driven Phrase 
Structure Grammar (HPSG), a unification-based 
grammar formalism. HPSG provides a 
mono-stratal lexicalist framework that facilitates 
handling intricate morpho-syntactic interaction. 
PV-related morphological and syntactic 
structures are accounted for by means of a lexical 
selection mechanism where the verb morpheme 
subcategorizes for its syntactic object in addition 
to its particle morpheme. 
The LingGO-ERG lexicalist approach is 
believed to be effective. However, their coverage 
and testing of the PVs seem preliminary. The 
LinGO-ERG lexicon contains 295 PV entries, 
with no report on benchmarks.  
In terms of the restricted flexibility and 
modifiability of a system, the use of high-level 
grammar formalisms such as HPSG to integrate 
identification in deep parsing cannot be 
compared with the alternative finite state 
approach [Breidt et al 1994]. 
[Breidt et al1994]?s approach is similar to our 
work. Multiword expressions including idioms, 
collocations, and compounds as well as PVs are 
accounted for by using local grammar rules 
formulated as regular expressions. There is no 
detailed description for English PV treatment 
since their work focuses on multilingual, 
multi-word expressions in general. The authors 
believe that the local grammar implementation of 
multiword expressions can work with general 
syntax either implemented in a high-level 
grammar formalism or implemented as a local 
grammar for the required morpho-syntactic 
interaction, but this interaction is not 
implemented into an integrated system and hence 
it is impossible to properly measure performance 
benchmarks. 
There is no report on implemented solutions 
covering the entire English PVs that are fully 
integrated into an NLP system and are well tested 
on sizable real life corpora, as is presented in this 
paper.   
3 Expert Lexicon Approach  
This section illustrates the system architecture 
and presents the underlying Expert Lexicon (EL) 
formalism, followed by the description of the 
implementation details.  
3.1 System Architecture 
Figure 1 shows the system architecture that 
contains the PV Identification Module based on 
the PV Expert Lexicon.  
This is a pipeline system mainly based on 
pattern matching implemented in local grammars 
and/or expert lexicons [Srihari et al2003]. 4 
                                                     
4 POS and NE tagging are hybrid systems involving 
both hand-crafted rules and statistical learning.  
English parsing is divided into two tasks: shallow 
parsing and deep parsing. The shallow parser 
constructs Verb Groups (VGs) and basic Noun 
Phrases (NPs), also called BaseNPs [Church 
1988]. The deep parser utilizes syntactic 
subcategorization features and semantic features 
of a head (e.g., VG) to decode both syntactic and 
logical dependency relationships such as 
Verb-Subject, Verb-Object, Head-Modifier, etc. 
 
 
Part-of-Speech  
(POS) Tagging 
General
Lexicon Lexical lookup 
Named Entity  
(NE) Taggig 
Shallow Parsing 
PV Identification 
Deep parsing 
General
Lexicon 
PV Expert 
Lexicon 
Figure 1. System Architecture 
 
The general lexicon lookup component 
involves stemming that transforms regular or 
irregular inflected verbs into the base forms to 
facilitate the later phrasal verb matching. This 
component also performs indexing of the word 
occurrences in the processed document for 
subsequent expert lexicons.  
The PV Identification Module is placed 
between the Shallow Parser and the Deep Parser. 
It requires shallow parsing support for the 
required syntactic interaction and the PV output 
provides lexical support for deep parsing. 
Results after shallow parsing form a proper 
basis for PV identification. First, the inserted NPs 
and adverbial time NEs are already constructed 
by the shallow parser and NE tagger. This makes 
it easy to write pattern matching rules for 
identifying separable PVs. 
Second, the constructed basic units NE, NP 
and VG provide conditions for 
constraint-checking in PV identification. For 
example, to prevent spurious matches in 
sentences like she put the coat on the table, it is 
necessary to check that the post-particle unit 
should NOT be an NP.  The VG chunking also 
decodes the voice, tense and aspect features that 
can be used as additional constraints for PV 
identification. A sample macro rule 
active_V_Pin that checks the ?NOT passive? 
constraint and the ?NOT time?, ?NOT location? 
constraints is shown in 3.3.  
3.2 Expert Lexicon Formalism 
The Expert Lexicon used in our system is an 
index-based formalism that can associate pattern 
matching rules with lexical entries. It is 
organized like a lexicon, but has the power of a 
lexicalized local grammar.   
All Expert Lexicon entries are indexed, 
similar to the case for the finite state tool in 
INTEX [Silberztein 2000]. The pattern matching 
time is therefore reduced dramatically compared 
to a sequential finite state device [Srihari et al 
2003].5   
The expert lexicon formalism is designed to 
enhance the lexicalization of our system, in 
accordance with the general trend of lexicalist 
approaches to NLP. It is especially beneficial in 
handling problems like PVs and many individual 
or idiosyncratic linguistic phenomena that can 
not be covered by non-lexical approaches.  
Unlike the extreme lexicalized word expert 
system in [Small and Rieger 1982] and similar to 
the IDAREX local grammar formalism [Breidt et 
al.1994], our EL formalism supports a 
parameterized macro mechanism that can be 
used to capture the general rules shared by a set 
of individual entries. This is a particular useful 
mechanism that will save time for computational 
lexicographers in developing expert lexicons, 
especially for phrasal verbs, as shall be shown in 
Section 3.3 below. 
The Expert Lexicon tool provides a flexible 
interface for coordinating lexicons and syntax: 
any number of expert lexicons can be placed at 
any levels, hand-in-hand with other 
non-lexicalized modules in the pipeline 
architecture of our system.    
                                                     
5 Some other unique features of our EL formalism 
include: (i) providing the capability of proximity 
checking as rule constraints in addition to pattern 
matching using regular expressions so that the rule 
writer or lexicographer can exploit the combined 
advantages of both, and (ii) the propagation 
functionality of semantic tagging results, to 
accommodate principles like one sense per discourse. 
3.3 Phrasal Verb Expert Lexicon 
To cover the three major types of PVs, we use the 
macro mechanism to capture the shared patterns. 
For example, the NP insertion for Type II PV is 
handled through a macro called V_NP_P, 
formulated in pseudo code as follows.  
 
V_NP_P($V,$P,$V_P,$F1, $F2,?)  := 
Pattern:  
$V  
NP 
(?right?|?back?|?straight?) 
$P  
NOT NP 
Action:  
$V: %assign_feature($F1, $F2,?)   
%assign_canonical_form($V_P) 
$P: %deactivate 
 
This macro represents cases like Take the coat 
off, please; put it back on, it?s raining now. It 
consists of two parts: ?Pattern? in regular 
expression form (with parentheses for optionality, 
a bar for logical OR, a quoted string for checking 
a word or head word) and ?Action? (signified by 
the prefix %). The parameters used in the macro 
(marked by the prefix $) include the leading verb 
$V, particle $P, the canonical form $V_P, and 
features $Fn.  After the defined pattern is matched, 
a Type II separable verb is identified. The Action 
part ensures that the lexical identity be 
represented properly, i.e. the assignment of the 
lexical features and the canonical form. The 
deactivate action flags the particle as being part 
of the phrasal verb.  
In addition, to prevent a spurious case in (3b), 
the macro V_NP_P checks the contextual 
constraints that no NP (i.e. NOT NP) should 
follow a PV particle. In our shallow parsing, NP 
chunking does not include identified time NEs, 
so it will not block the PV identification in (3c). 
    
(3a) She [put the coat on]. 
(3b) She put the coat [on the table]. 
(3c) She [put the coat on] yesterday. 
 
All three types of PVs when used without NP 
insertion are handled by the same set of macros, 
due to the formal patterns they share. We use a 
set of macros instead of one single macro, 
depending on the type of particle and the voice of 
the verb, e.g., look for calls the macro 
[active_V_Pfor | passive_V_Pfor], fly in calls the 
macro [active_V_Pin | passive_V_Pin], etc.  
The distinction between active rules and 
passive rules lies in the need for different 
constraints. For example, a passive rule needs to 
check the post-particle constraint [NOT NP] to 
block the spurious case in (4b).  
 
(4a) He [turned on] the radio. 
(4b)  The world [had been turned] [on its 
head] again. 
 
As for particles, they also require different 
constraints in order to block spurious matches. 
For example, active_V_Pin (formulated below) 
requires the constraints ?NOT location NOT 
time? after the particle while active_V_Pfor only 
needs to check ?NOT time?, shown in (5) and (6). 
 
(5a) Howard [had flown in] from Atlanta. 
(5b) The rocket [would fly] [in 1999]. 
(6a) She was [looking for] California on the 
map. 
(6b) She looked [for quite a while]. 
 
active_V_Pin($V, in, $V_P,$F1, $F2,?)  := 
Pattern:  
$V NOT passive 
(Adv|time) 
$P  
NOT location NOT time 
Action:  
$V: %assign_feature($F1, $F2, ?)   
%assign_canonical_form($V_P) 
$P: %deactivate 
 
The coding of the few PV macros requires 
skilled computational grammarians and a 
representative development corpus for rule 
debugging. In our case, it was approximately 15 
person-days of skilled labor including data 
analysis, macro formulation and five iterations of 
debugging against the development corpus. But 
after the PV macros are defined, lexicographers 
can quickly develop the PV entries: it only cost 
one person-day to enter the entire PV vocabulary 
using the EL formalism and the implemented 
macros. We used the Cambridge International 
Dictionary of Phrasal Verbs and Collins Cobuild 
Dictionary of Phrasal Verbs as the major 
reference for developing our PV Expert 
Lexicon. 6  This expert lexicon contains 2,590 
entries. The EL-rules are ordered with specific 
rules placed before more general rules. A sample 
of the developed PV Expert Lexicon is shown 
below (the prefix @ denotes a macro call): 
 
abide:  @V_P_by(abide, by, abide_by, V6A, 
APPROVING_AGREEING) 
accede: @V_P_to(accede, to, accede_to, V6A, 
APPROVING_AGREEING) 
add:  @V_P(add, up, add_up, V2A, 
MATH_REASONING); 
 @V_NP_P(add, up, add_up, V6A, 
MATH_REASONING) 
???? 
 
In the above entries, V6A and V2A are 
subcategorization features for transitive and 
intransitive verb respectively, while 
APPROVING_AGREEING and 
MATH_REASONING are semantic features. 
These features provide the lexical basis for the 
subsequent parser. 
The PV identification method as described 
above resolves all the problems in the checklist. 
The following sample output shows the 
identification result: 
 
NP[That]  
VG[could slow: slow_down/V6A/MOVING] 
NP[him]  
down/deactivated . 
4 Benchmarking 
Blind benchmarking was done by two 
non-developer testers manually checking the 
results. In cases of disagreement, a third tester 
was involved in examining the case to help 
resolve it. We ran benchmarking on both the 
formal style and informal style of English text.  
4.1 Corpus Preparation 
Our development corpus (around 500 KB) 
consists of the MUC-7 (Message Understanding 
                                                     
6 Some entries that are listed in these dictionaries do 
not seem to belong to phrasal verb categories, e.g., 
relieve?of (as used in relieve somebody of something), 
remind?of (as used in remind somebody of 
something), etc.  It is generally agreed that such cases 
belong to syntactic patterns in the form of 
V+NP+P+NP that can be captured by 
subcategorization.  We have excluded these cases.  
Conference-7) dryrun corpus and an additional 
collection of news domain articles from TREC 
(Text Retrieval Conference) data.  The PV expert 
lexicon rules, mainly the macros, were developed 
and debugged using the development corpus.    
The first testing corpus (called English-zone 
corpus) was downloaded from a website that is 
designed to teach PV usage in Colloquial English 
(http://www.english-zone.com/phrasals/w-phras
als.html). It consists of 357 lines of sample 
sentences containing 347 PVs. This addresses the 
sparseness problem for the less frequently used 
PVs that rarely get benchmarked in running text 
testing. This is a concentrated corpus involving 
varieties of PVs from text sources of an informal 
style, as shown below.7 
 
"Would you care for some dessert? We have 
ice cream, cookies, or cake." 
Why are you wrapped up in that blanket? 
After John's wife died, he had to get through 
his sadness. 
After my sister cut her hair by herself, we had 
to take her to a hairdresser to even her 
hair out! 
After the fire, the family had to get by without 
a house. 
 
We have prepared two collections from the 
running text data to test written English of a more 
formal style in the general news domain:  (i) the 
MUC-7 formal run corpus (342 KB) consisting 
of 99 news articles, and (ii) a collection of 23,557 
news articles (105MB) from the TREC data.   
4.2 Performance Testing 
There is no available system known to the NLP 
community that claims a capability for PV 
treatment and could thus be used for a reasonable 
performance comparison. Hence, we have 
devised a bottom-line system and a baseline 
system for comparison with our EL-driven 
system. The bottom-line system is defined as a 
simple lexical lookup procedure enhanced with 
the ability to match inflected verb forms but with 
no capability of checking contextual constraints. 
There is no discussion in the literature on what 
                                                     
7 Proper treatment of PVs is most important in parsing 
text sources involving Colloquial English, e.g., 
interviews, speech transcripts, chat room archives. 
There is an increasing demand for NLP applications in 
handling this type of data.    
constitutes a reasonable baseline system for PV. 
We believe that a baseline system should have 
the additional, easy-to-implement ability to jump 
over inserted object case pronouns (e.g., turn it 
on) and adverbs (e.g., look everywhere for) in PV 
identification.  
Both the MUC-7 formal run corpus and the 
English-zone corpus were fed into the 
bottom-line  and the baseline systems as well as 
our EL-driven system described in Section 3.3. 
The benchmarking results are shown in Table 1 
and Table 2. The F-score is a combined measure 
of precision and recall, reflecting the overall 
performance of a system. 
Table 1.  Running Text Benchmarking 1 
 Bottom-line Baseline EL 
Correct 303 334 338 
Missing 58 27 23 
Spurious 33 34 7 
Precision 90.2% 88.4% 98.0% 
Recall 83.9% 92.5% 93.6% 
F-score 86.9% 91.6% 95.8% 
Table 2.  Sampling Corpus Benchmarking 
 Bottom-line Baseline EL 
Correct 215 244 324 
Missing 132 103 23 
Spurious 0 0 0 
Precision 100% 100% 100% 
Recall 62.0% 70.3% 93.4% 
F-score 76.5% 82.6% 96.6% 
 
Compared with the bottom-line performance 
and the baseline performance, the F-score for the 
presented method has surged 9-20 percentage 
points and 4-14 percentage points, respectively. 
The high precision (100%) in Table 2 is due to 
the fact that, unlike running text, the sampling 
corpus contains only positive instances of PV. 
This weakness, often associated with sampling 
corpora, is overcome by benchmarking running 
text corpora (Table 1 and Table 3).   
To compensate for the limited size of the 
MUC formal run corpus, we used the testing 
corpus from the TREC data. For such a large 
testing corpus (23,557 articles, 105MB), it is 
impractical for testers to read every article to 
count mentions of all PVs in benchmarking. 
Therefore, we selected three representative PVs 
look for, turn?on and blow?up and used the 
head verbs (look, turn, blow), including their 
inflected forms, to retrieve all sentences that 
contain those verbs. We then ran the retrieved 
sentences through our system for benchmarking 
(Table 3).  
All three of the blind tests show fairly 
consistent benchmarking results (F-score 
95.8%-97.5%), indicating that these benchmarks 
reflect the true capability of the presented system, 
which targets the entire PV vocabulary instead of 
a selected subset. Although there is still some 
room for further enhancement (to be discussed 
shortly), the PV identification problem is 
basically solved. 
Table 3.  Running Text Benchmarking 2 
 ?look for? ?turn?on? ?blow?up?
 Correct 1138 128 650 
 Missing 76 0 33 
 Spurious 5 9 0 
 Precision 99.6% 93.4% 100.0% 
 Recall 93.7% 100.0% 95.2% 
 F-score 96.6% 97.5% 97.5% 
4.3 Error Analysis 
There are two major factors that cause errors: (i) 
the impact of errors from the preceding modules 
(POS and Shallow Parsing), and (ii) the mistakes 
caused by the PV Expert Lexicon itself.  
The POS errors caused more problems than 
the NP grouping errors because the inserted NP 
tends to be very short, posing little challenge to 
the BaseNP shallow parsing. Some verbs 
mis-tagged as nouns by POS were missed in PV 
identification. 
There are two problems that require the 
fine-tuning of the PV Identification Module. First, 
the macros need further adjustment in their 
constraints. Some constraints seem to be too 
strong or too weak.  For example, in the Type I 
macro, although we expected the possible 
insertion of an adverb, however, the constraint on 
allowing for only one optional adverb and not 
allowing for a time adverbial is still too strong. 
As a result, the system failed to identify 
listening?to and meet?with in the following 
cases: ?was not listening very closely on 
Thursday to American concerns about human 
tights? and ... meet on Friday with his Chinese... 
The second type of problems cannot be solved 
at the macro level. These are individual problems 
that should be handled by writing specific rules 
for the related PV. An example is the possible 
spurious match of the PV have?out in the 
sentence ...still have our budget analysts out 
working the numbers. Since have is a verb with 
numerous usages, we should impose more 
individual constraints for NP insertion to prevent 
spurious matches, rather than calling a common 
macro shared by all Type II verbs.  
4.4 Efficiency Testing 
To test the efficiency of the index-based PV 
Expert Lexicon in comparison with a sequential  
Finite State Automaton (FSA) in the PV 
identification task, we conducted the following 
experiment.  
The PV Expert Lexicon was compiled as a 
regular local grammar into a large automaton that 
contains 97,801 states and 237,302 transitions. 
For a file of 104 KB (the MUC-7 dryrun corpus 
of 16,878 words), our sequential FSA  runner 
takes over 10 seconds for processing on the  
Windows NT platform with a Pentium PC. This 
processing only requires 0.36 second using the 
indexed PV Expert Lexicon module. This is 
about 30 times faster.   
5 Conclusion 
An effective and efficient approach to phrasal 
verb identification is presented. This approach 
handles both separable and inseparable phrasal 
verbs in English. An Expert Lexicon formalism 
is used to develop the entire phrasal verb lexicon 
and its associated pattern matching rules and 
macros.  This formalism allows the phrasal verb 
lexicon to be called between two levels of 
parsing for the required morpho-syntactic 
interaction in phrasal verb identification. 
Benchmarking using both the running text corpus 
and sampling corpus shows that the presented 
approach provides a satisfactory solution to this 
problem. 
In future research, we plan to extend the 
successful experiment on phrasal verbs to other 
types of multi-word expressions and idioms 
using the same expert lexicon formalism. 
Acknowledgment 
This work was partly supported by a grant from 
the Air Force Research Laboratory?s Information 
Directorate (AFRL/IF), Rome, NY, under 
contract F30602-03-C-0044. The authors wish to 
thank Carrie Pine and Sharon Walter of AFRL 
for supporting and reviewing this work. Thanks 
also go to the anonymous reviewers for their 
constructive comments. 
References 
Breidt. E., F. Segond and G. Valetto. 1994. Local 
Grammars for the Description of Multi-Word 
Lexemes and Their Automatic Recognition in 
Text.  Proceedings of Comlex-2380 - Papers 
in Computational Lexicography, Linguistics 
Institute, HAS, Budapest, 19-28. 
Breidt, et al 1996. Formal description of 
Multi-word Lexemes with the Finite State 
formalism: IDAREX. Proceedings of 
COLING 1996, Copenhagen.  
Bolinger, D. 1971. The Phrasal Verb in English.  
Cambridge, Mass., Harvard University Press. 
Church, K. 1988. A stochastic parts program and 
noun phrase parser for unrestricted text. 
Proceedings of ANLP 1988.  
Di Sciullo, A.M. and E. Williams. 1987.  On The 
Definition of Word. The MIT Press, 
Cambridge, Massachusetts. 
Fraser, B. 1976. The Verb Particle Combination 
in English.  New York: Academic Press. 
Pelli, M. G. 1976. Verb Particle Constructions in 
American English.  Zurich: Francke Verlag 
Bern. 
Sag, I., T. Baldwin, F. Bond, A. Copestake and D. 
Flickinger. 2002. Multiword Expressions: A 
Pain in the Neck for NLP. Proceedings of 
CICLING 2002, Mexico City, Mexico, 1-15. 
Shaked, N. 1994. The Treatment of Phrasal 
Verbs in a Natural Language Processing 
System, Dissertation, CUNY. 
Silberztein, M. 2000. INTEX: An FST Toolbox. 
Theoretical Computer Science, Volume 
231(1): 33-46. 
Small, S. and C. Rieger. 1982. Parsing and 
comprehending with word experts (a theory 
and its realisation). W. Lehnert and M. 
Ringle, editors, Strategies for Natural 
Language Processing. Lawrence Erlbaum 
Associates, Hillsdale, NJ.  
Srihari, R., W. Li, C. Niu and T. Cornell. 2003. 
InfoXtract: An Information Discovery Engine 
Supported by New Levels of Information 
Extraction. Proceeding of HLT-NAACL 
Workshop on Software Engineering and 
Architecture of Language Technology 
Systems, Edmonton, Canada. 
Villavicencio, A. and A. Copestake. 2002. 
Verb-particle constructions in a 
computational grammar of English.  
Proceedings of the Ninth International 
Conference on Head-Driven Phrase Structure 
Grammar, Seoul, South Korea. 
Weakly Supervised Learning for Cross-document Person Name 
Disambiguation Supported by Information Extraction  
Cheng Niu, Wei Li, and Rohini K. Srihari 
Cymfony Inc. 
600 Essjay Road, Williamsville, NY 14221, USA. 
{cniu, wei, rohini}@cymfony.com 
 
Abstract 
It is fairly common that different people are 
associated with the same name. In tracking 
person entities in a large document pool, it is 
important to determine whether multiple 
mentions of the same name across documents 
refer to the same entity or not.  Previous 
approach to this problem involves measuring 
context similarity only based on co-occurring 
words. This paper presents a new algorithm 
using information extraction support in 
addition to co-occurring words. A learning 
scheme with minimal supervision is developed 
within the Bayesian framework. Maximum 
entropy modeling is then used to represent the 
probability distribution of context similarities 
based on heterogeneous features.  Statistical 
annealing is applied to derive the final entity 
coreference chains by globally fitting the 
pairwise context similarities. Benchmarking 
shows that our new approach significantly 
outperforms the existing algorithm by 25 
percentage points in overall F-measure. 
1 Introduction 
Cross document name disambiguation is 
required for various tasks of knowledge discovery 
from textual documents, such as entity tracking, 
link discovery, information fusion and event 
tracking.  This task is part of the co-reference task: 
if two mentions of the same name refer to same 
(different) entities, by definition, they should 
(should not) be co-referenced. As far as names are 
concerned, co-reference consists of two sub-tasks: 
(i) name disambiguation to handle the problem of 
different entities happening to use the same name; 
(ii) alias association to handle the problem of the 
same entity using multiple names (aliases). 
Message Understanding Conference (MUC) 
community has established within-document co-
reference standards [MUC-7 1998]. Compared 
with within-document name disambiguation which 
can leverage highly reliable discourse heuristics 
such as one sense per discourse [Gale et al1992], 
cross-document name disambiguation is a much 
harder problem. 
Among major categories of named entities (NEs, 
which in this paper refer to entity names, excluding 
the MUC time and numerical NEs), company and 
product names are often trademarked or uniquely 
registered, and hence less subject to name 
ambiguity. This paper focuses on cross-document 
disambiguation of person names. 
Previous research for cross-document name 
disambiguation applies vector space model (VSM) 
for context similarity, only using co-occurring 
words [Bagga & Baldwin 1998]. A pre-defined 
threshold decides whether two context vectors are 
different enough to represent two different entities. 
This approach faces two challenges: i) it is difficult 
to incorporate natural language processing (NLP) 
results in the VSM framework; 1 ii) the algorithm 
focuses on the local pairwise context similarity, 
and neglects the global correlation in the data: this 
may cause inconsistent results, and hurts the 
performance. 
This paper presents a new algorithm that 
addresses these problems. A learning scheme with 
minimal supervision is developed within the 
Bayesian framework. Maximum entropy modeling 
is then used to represent the probability 
distribution of context similarities based on 
heterogeneous features covering both co-occurring 
words and natural language information extraction 
(IE) results.  Statistical annealing is used to derive 
the final entity co-reference chains by globally 
fitting the pairwise context similarities. 
Both the previous algorithm and our new 
algorithm are implemented, benchmarked and 
                                                     
1 Based on our experiment, only using co-occurring 
words often cannot fulfill the name disambiguation task. 
For example, the above algorithm identifies the 
mentions of Bill Clinton as referring to two different 
persons, one represents his role as U. S. president, and 
the other is strongly associated with the scandal, 
although in both mention clusters, Bill Clinton has been 
mentioned as U.S. president. Proper name 
disambiguation calls for NLP/IE support which may 
have extracted the key person?s identification 
information from the textual documents. 
compared.  Significant performance enhancement 
up to 25 percentage points in overall F-measure is 
observed with the new approach. The generality of 
this algorithm ensures that this approach is also 
applicable to other categories of NEs. 
The remaining part of the paper is structured as 
follows. Section 2 presents the algorithm design 
and task definition. The name disambiguation 
algorithm is described in Sections 3, 4 and 5, 
corresponding to the three key aspects of the 
algorithm, i.e. minimally supervised learning 
scheme, maximum entropy modeling and 
annealing-based optimization. Benchmarks are 
shown in Section 6, followed by Conclusion in 
Section 7. 
2 Task Definition and Algorithm Design 
Given n  name mentions, we first introduce the 
following symbols. iC  refers to the context of the  
i -th mention. iP  refers to the entity for the i -th 
mention. iName  refers to the name string of the i  
-th mention. jiCS ,  refers to the context similarity 
between the i -th mention and the j -th mention, 
which is a subset of the predefined context 
similarity features. ?f  refers to the? -th 
predefined context similarity feature. So jiCS ,  
takes the form of { }?f . 
The name disambiguation task is defined as hard 
clustering of the multiple mentions of the same 
name. Its final solution is represented as { }MK ,  
where K refers to the number of distinct entities, 
and M represents the many-to-one mapping (from 
mentions to a cluster) such that 
( ) K]. [1,j n],[1,i j,iM ??=  
One way of combining natural language IE 
results with traditional co-occurring words is to 
design a new context representation scheme and 
then define the context similarity measure based on 
the new scheme.  The challenge to this approach 
lies in the lack of a proper weighting scheme for 
these high-dimensional heterogeneous features. In 
our research, the algorithm directly models the 
pairwise context similarity. 
For any given context pair, a set of predefined 
context similarity features are defined. Then with n 
mentions of a same name, 2
)1( ?nn  context 
similarities [ ] [ )( )ijniCS ji ,1,,1 , ??  are 
computed. The name disambiguation task is 
formulated as searching for { }MK ,  which 
maximizes the following conditional probability:  
{ }( ) [ ] [ )( )ijniCSMK ji ,1,,1       }{,Pr , ??  
Based on Bayesian Equity, this is equivalent to 
maximizing the following joint probability 
 
{ }( ) [ ] [ )( )
{ }( ) { }( )
{ }( ) { }( )MKMKCS
MKMKCS
ijniCSMK
ij
Ni
ji
ji
ji
,Pr,Pr
,Pr,}{Pr
,1,,1       }{,,Pr
1,1
,1
,
,
,
?
?=
=
?
=
??
(1) 
 
Eq. (1) contains a prior probability distribution 
of name disambiguation { }( )MK ,Pr . Because 
there is no prior knowledge available about what 
solution is preferred, it is reasonable to take an 
equal distribution as the prior probability 
distribution. So the name disambiguation is 
equivalent to searching for { }MK ,  which 
maximizes Expression (2). 
 
{ }( )?
?=
=
1,1
,1
, ,Pr
ij
Ni
ji MKCS      (2) 
 
where 
{ }( ) ( ) ( ) ( )( )





?
==
= otherwise ,Pr
jMiM if ,Pr,Pr
,
,
,
jiji
jiji
ji PPCS
PPCSMKCS
       (3) 
 
To learn the conditional probabilities ( )jiji PPCS =|Pr ,  and ( )jiji PPCS ?|Pr ,  in Eq. 
(3), we use a machine learning scheme which only 
requires minimal supervision. Within this scheme, 
maximum entropy modeling is used to combine 
heterogeneous context features. With the learned 
conditional probabilities in Eq. (3), for a given 
{ }MK ,  candidate, we can compute the conditional 
probability of Expression (2).  In the final step, 
optimization is performed to search for { }MK ,  
that maximizes the value of Expression (2). 
To summarize, there are three key elements in 
this learning scheme: (i) the use of automatically 
constructed corpora to estimate conditional 
probabilities of Eq. (3); (ii) maximum entropy 
modeling for combining heterogeneous context 
similarity features; and (iii) statistical annealing for 
optimization. 
3 Learning Using Automatically Constructed 
Corpora 
This section presents our machine learning 
scheme to estimate the conditional probabilities ( )jiji PPCS =|Pr ,  and ( )jiji PPCS ?|Pr ,  in Eq. 
(3). Considering jiCS ,  is in the form of { }?f , we 
re-formulate the two conditional probabilities as 
{ }( )ji PPf =|Pr ?  and { }( )ji PPf ?|Pr ? . 
The learning scheme makes use of automatically 
constructed large corpora. The rationale is 
illustrated in the figure below. The symbol + 
represents a positive instance, namely, a mention 
pair that refers to the same entity.  The symbol ? 
represents a negative instance, i.e. a mention pair 
that refers to different entities. 
 
Corpus I  Corpus II 
+++++---++++++         ---------------------- 
+-----+++--+++++           --+------------------ 
   ++++++++++--++           --------------+------ 
   +++++++---++++         ----------------------- 
   +++----++++++++         --------+------------- 
 
As shown in the figure, two training corpora are 
automatically constructed. Corpus I contains 
mention pairs of the same names; these are the 
most frequently mentioned names in the document 
pool. It is observed that frequently mentioned 
person names in the news domain are fairly 
unambiguous, hence enabling the corpus to contain 
mainly positive instances.2 Corpus II contains 
mention pairs of different person names, these 
pairs overwhelmingly correspond to negative 
instances (with statistically negligible exceptions). 
Thus, typical patterns of negative instances can be 
learned from Corpus II. We use these patterns to 
filter away the negative instances in Corpus I. The 
purified Corpus I can then be used to learn patterns 
for positive instances. The algorithm is formulated 
as follows. 
Following the observation that different names 
usually refer to different entities, it is safe to derive 
Eq. (4).  
 
( ) ( )2121 }{Pr}{Pr namenamefPPf ?=? ??   
(4) 
 
For ( )21}{Pr PPf =? , we can derive the 
following relation (Eq. 5): 
 
                                                     
2 Based on our data analysis, there is no observable 
difference in linguistic expressions involving frequently 
mentioned vs. occasionally occurring person names.  
Therefore, the use of frequently mentioned names in the 
corpus construction process does not affect the 
effectiveness of the learned model to be applicable to all 
the person names in general. 
( )
( )[
( )]
( )[
( )( )]2121
21
2121
21
21
Pr1*  
}{Pr
Pr*  
}{Pr
}{Pr
namenamePP
PPf
namenamePP
PPf
namenamef
==?
?+
==
==
=
?
?
?
 (5) 
 
So ( )21}{Pr PPf =?  can be determined if 
( ))()(}{Pr 21 PnamePnamef =? , 
( ))()(}{Pr 21 PnamePnamef ?? , and 
( ))()(Pr 2121 PnamePnamePP ==  are all known. 
By using Corpus I and Corpus II to estimate the 
above three probabilities, we achieve Eq. (6.1) and 
Eq. (6.2) 
 
( )21}{Pr PPf =?  
( ) ( ) ( )
X
Xff ??
=
1*}{Pr}{Pr maxEntIImaxEntI ?? .                  
     (6.1) 
 
( ) })({Pr}{Pr maxEntII21 ?? fPPf =?            (6.2) 
 
where ( )}{Pr maxEntI ?f  denotes the maximum 
entropy model of ( ))()(}{Pr 21 PnamePnamef =?  
using Corpus I,  ( )}{Pr maxEntII ?f  denotes the 
maximum entropy model of 
( ))()(}{Pr 21 PnamePnamef ??  using Corpus II, 
and X  stands for the Maximum Likelihood 
Estimation (MLE) of 
( ))()(Pr 2121 PnamePnamePP ==  using Corpus I. 
Maximum entropy modeling is used here due to its 
strength of combining heterogeneous features. 
It is worth noting that ( )}{Pr maxEntI ?f  and 
( )}{Pr maxEntII ?f  can be automatically computed 
using Corpus I and Corpus II. Only X requires 
manual truthing. Because X is context 
independent, the required truthing is very limited 
(in our experiment, only 100 truthed mention pairs 
were used). The details of corpus construction and 
truthing will be presented in the next section. 
4 Maximum Entropy Modeling 
This section presents the definition of context 
similarity features }{ ?f , and how to estimate the 
maximum entropy model of  ( )}{Pr maxEntI ?f  and 
( )}{Pr maxEntII ?f . 
First, we describe how Corpus I and Corpus II 
are constructed. Before the person name 
disambiguation learning starts, a large pool of 
textual documents are processed by an IE engine 
InfoXtract [Srihari et al2003]. The InfoXtract 
engine contains a named entity tagger, an aliasing 
module, a parser and an entity relationship 
extractor. In our experiments, we used ~350,000 
AP and WSJ news articles (a total of ~170 million 
words) from the TIPSTER collection. All the 
documents and the IE results are stored into an IE 
Repository. The top 5,000 most frequently 
mentioned multi-token person names are retrieved 
from the repository. For each name, all the 
contexts are retrieved while the context is defined 
as containing three categories of features: 
 
(i)     The surface string sequence centering around 
a key person name (or its aliases as identified 
by the aliasing module) within a predefined 
window size equal to 50  
tokens to both sides of the key name. 
 
(ii)  The automatically tagged entity names co 
occurring with the key name (or its aliases) 
within the same predefined window as in (i). 
 
(iii) The automatically extracted relationships 
associated with the key name (or its aliases). 
The relationships being utilized are listed 
below: 
 
Age, Where-from, Affiliation, Position, 
Leader-of, Owner-of, Has-Boss, Boss-of, 
Spouse-of, Has-Parent, Parent-of, Has-
Teacher, Teacher-of, Sibling-of, Friend-of, 
Colleague-of, Associated-Entity, Title, 
Address, Birth-Place, Birth-Time, Death-
Time, Education, Degree, Descriptor, 
Modifier, Phone, Email, Fax. 
 
A recent manual benchmarking of the InfoXtract 
relationship extraction in the news domain is 86% 
precision and 67% recall (75% F-measure).    
To construct Corpus I, a person name is 
randomly selected from the list of the top 5,000 
frequently mentioned multi-token names. For each 
selected name, a pair of contexts are extracted, and 
inserted into Corpus I. This process repeats until 
10,000 pairs of contexts are selected. 
It is observed that, in the news domain, the top 
frequently occurring multi-token names are highly 
unambiguous. For example, Bill Clinton 
exclusively stands for the previous U.S. president 
although in real life, although many other people 
may also share this name. Based on manually 
checking 100 sample pairs in Corpus I, we have 
( ) 95.0Pr 21 ?== PPX I , which means for the 100 
sample pairs mentioning the same person name, 
only 5 pairs are found to refer to different person 
entities. Note that the value of X?1  represents the 
estimation of the noise in Corpus I, which is used 
in Eq (6.1) to correct the bias caused by the noise 
in the corpus.  
To construct Corpus II, two person names are 
randomly selected from the same name list. Then a 
context for each of the two names is extracted, and 
this context pair is inserted into Corpus II. This 
process repeats until 10,000 pairs of contexts are 
selected.  
Based on the above three categories of context 
features, four context similarity features are 
defined:  
 
(1)  VSM-based context similarity using co-
occurring words  
 
The surface string sequence centering around the 
key name is represented as a vector, and the word i 
in context j is weighted as follows. 
 
)(log*),(),( idf
Djitfjiweight =   (7) 
 
where ),( jitf is the frequency of word i in the  
j-th surface string sequence; D is the number of 
documents in the pool; and )(idf  is the number of 
documents containing the word i. Then, the cosine 
of the angle between the two resulting vectors is 
used as the context similarity measure.  
 
(2) Co-occurring NE Similarity 
 
The latent semantic analysis (LSA) [Deerwester 
et al1990] is used to compute the co-occurring NE 
similarities.  LSA is a technique to uncover the 
underlining semantics based on co-occurrence 
data. The first step of LSA is to construct word-
vs.-document co-occurrence table. We use 100,000 
documents from the TIPSTER corpus, and select 
the following types of top n most frequently 
mentioned words as base words: 
 
top 20,000 common nouns 
top 10,000 verbs 
top 10,000 adjectives 
top 2,000 adverbs 
top 10,000 person names 
top 15,000 organization names 
top 6,000 location names 
top 5,000 product names 
 
Then, a word-vs.-document co-occurrence table 
Matrix  is built so that 
)(log*),( idf
DjitfMatrixij = . The second step of 
LSA is to perform singular value decomposition 
(SVD) on the co-occurrence matrix.  SVD yields 
the following Matrix  decomposition:  
 
TDSTMatrix 000=    (8)  
 
where T  and D are orthogonal matrices (the row 
vector is called singular vectors), and S  is a 
diagonal matrix with the diagonal elements (called 
singular values) sorted decreasingly. 
The key idea of LSA is to reduce noise or 
insignificant association patterns by filtering the 
insignificant components uncovered by SVD. This 
is done by keeping only top k singular values. In 
our experiment, k is set to 200, following the 
practice reported in [Deerwester et al 1990] and 
[Landauer & Dumais, 1997]. This procedure yields 
the following approximation to the co-occurrence 
matrix: 
TTSDMatrix ?    (9) 
 
where S  is attained from 0S by deleting non-top k 
elements,  and T ( D ) is obtained from 0T ( 0D ) by 
deleting the corresponding columns. 
It is believed that the approximate matrix is more 
proper to induce underlining semantics than the 
original one. In the framework of LSA, the co-
occurring NE similarities are computed as follows: 
suppose the first context in the pair contains NEs 
{ }it0 , and the second context in the pair contains 
NEs { }it1 . Then the similarity is computed as 

 
=
ii
ii
titi
titi
TwTw
TwTwS
10
10
10
10 where iw0 and iw1 are 
term weights defined in Eq (7). 
 
(3) Relationship Similarity 
 
We define four different similarity values based 
on entity relationship sharing: (i) sharing no 
common relationships, (ii) relationship conflicts 
only, (iii) relationship with consistence and 
conflicts, and (iv) relationship with consistence 
only. The  consistency checking between extracted 
relationships is supported by the InfoXtract 
number normalization and time normalization as 
well as entity aliasing procudures. 
 
(4) Detailed Relationship Similarity 
 
For each  relationship type, four different 
similarity values are defined based on sharing of 
that specific relationship i: (i) no sharing of 
relationship i, (ii) conflicts for relationship i, (iii) 
consistence and conflicts for relationship i, and 
(iv) consistence for relationship i. 
 
To facilitate the maximum entropy modeling in 
the later stage, the values of the first and second 
categories of similarity measures are discretized 
into integers. The number of integers being used 
may impact the final performance of the system. If 
the number is too small, significant information 
may be lost during the discretization process. On 
the other hand, if the number is too large, the 
training data may become too sparse. We trained a 
conditional maximum entropy model to 
disambiguate context pairs between Corpus I and 
Corpus II. The performance of this model is used 
to select the optimal number of integers. There is 
no significant  performance change when the 
integer number is within the range of [5,30], with 
12 as the optimal number. 
Now the context similarity for a context pair is a 
vector of similarity features, e.g.  
 
{VSM_Similairty_equal_to_2, 
NE_Similarity_equal_to_1, 
Relationship_Conflicts_only, 
No_Sharing_for_Age, 
   Conflict_for_Affiliation}. 
Besides the four categories of basic context 
similarity features defined above, we define 
induced context similarity features by combining 
basic context similarity features using the logical 
AND operator. With induced features, the context 
similarity vector in the previous example is 
represented as 
{VSM_Similairty_equal_to_2, 
NE_Similarity_equal_to_1, 
Relationship_Conflicts_only, 
No_Sharing_for_Age, 
Conflict_for_Affiliation,  
[VSM_Similairty_equal_to_2 and 
NE_Similarity_equal_to_1], 
[VSM_Similairty=2 and 
Relationship_Conflicts_only],  
??  
[VSM_Similairty_equal_to_2 and 
NE_Similarity_equal_to_1 and 
Relationship_Conflicts_only and 
No_Sharing_for_Age and 
Conflict_for_Affiliation] 
  }. 
The induced features provide direct and fine-
grained information, but suffer from less sampling 
space. Combining basic features and induced 
features under a smoothing scheme, maximum 
entropy modeling may achieve optimal 
performance.  
Now the maximum entropy modeling can be 
formulated as follows: given a pairwise context 
similarity vector }{ ?f  the probability of }{ ?f is 
given as 
 
( )
{ }
?
?
=
?
?
ff
fwZf
1}{Pr maxEnt   (10) 
 
where Z is the normalization factor, fw  is the 
weight associated with feature f . The Iterative 
Scaling algorithm combined with Monte Carlo 
simulation [Pietra, Pietra & Lafferty 1995] is used 
to train the weights in this generative model. 
Unlike the commonly used conditional maximum 
entropy modeling which approximates the feature 
configuration space as the training corpus 
[Ratnaparkhi 1998], Monte Carlo techniques are 
required in the generative modeling to simulate the 
possible feature configurations. The exponential 
prior smoothing scheme [Goodman 2003] is 
adopted. The same training procedure is performed 
using Corpus I and Corpus II to estimate 
( )}{Pr maxEntI if  and ( )}{Pr maxEntII if  respectively. 
5 Annealing-based Optimization  
With the maximum entropy modeling presented 
in the last section, for a given name 
disambiguation candidate solution{ }MK , , we can 
compute the conditional probability of Expression 
(2). Statistical annealing [Neal 1993]-based 
optimization is used to search for { }MK ,  which 
maximizes Expression (2). 
The optimization process consists of two steps. 
First, a local optimal solution{ }0, MK is computed 
by a greedy algorithm. Then by setting { }0, MK as 
the initial state, statistical annealing is applied to 
search for the global optimal solution. 
Given n  same name mentions, assuming the 
input of 2
)1( ?nn  probabilities ( )jiji PPCS =,Pr  
and 2
)1( ?nn  probabilities ( )jiji PPCS ?,Pr , the 
greedy algorithm performs as follows: 
 
1. Set the initial state { }MK , as nK = , 
and [ ]n1,i  ,)( ?= iiM ; 
2. Sort ( )jiji PPCS =,Pr  in decreasing  
order; 
3. Scan the sorted probabilities one by one.  
If the current probability is  ( )jiji PPCS =,Pr , )(  )( jMiM ? , and  
there exist no such l  and m that  
( ) ( ) ( ) ( )jMmMiMlM == ,  
and ( ) ( )mlmljiji PPCSPPCS ?<= ,, PrPr  
then update { }MK ,  by merging cluster 
)(iM and )( jM . 
4.   Output { }MK ,  as a local optimal solution. 
 
Using the output { }0, MK of the greedy 
algorithm as the initial state, the statistical 
annealing is described using the following pseudo-
code:  
 
Set { } { }0,, MKMK = ; 
for( 1.01?*;?? ;?? final0 =<= ) 
   { 
    iterate pre-defined number of times 
    { 
          set { } { }MKMK ,, 1 = ; 
          update { }1, MK  by randomly changing    
          the  number of clusters K and the    
          content of   each cluster.  
            set 
{ }( )
{ }( )?
?
?=
=
?=
=
=
1,1
,1
,
1,1
,1
1,
,Pr
,Pr
ij
Ni
ji
ij
Ni
ji
MKCS
MKCS
x  
           if(x>=1) 
          { 
             set { } { }1,, MKMK =  
          } 
          else 
         { 
             set { } { }1,, MKMK =  with probability  
              ?x . 
         } 
      if 
{ }( )
{ }( ) 1,Pr
,Pr
1,1
,1
0,
1,1
,1
,
>?
?
?=
=
?=
=
ij
Ni
ji
ij
Ni
ji
MKCS
MKCS
 
      set { } { }MKMK ,, 0 =  
   } 
} 
output { }0, MK  as the optimal state. 
6 Benchmarking 
To evaluate the effectiveness of our new 
algorithm, we implemented the previous algorithm 
described in [Bagga & Baldwin 1998] as our 
baseline. The threshold is selected as 0.19 by 
optimizing the pairwise disambiguation accuracy 
using the 80 truthed mention pairs of ?John 
Smith?. To clearly benchmark the performance 
enhancement from IE support, we also 
implemented a system using the same weakly 
supervised learning scheme but only VSM-based 
similarity as the pairwise context similarity 
measure. We benchmarked the three systems for 
comparison. The following three scoring measures 
are implemented. 
 
(1) Precision (P): 

=
iN
P i  ofcluster  output   in the  mentions of #
i  ofcluster  output   in the  mentionscorrect   of #1  
 
(2) Recall (R): 

=
iN
P i  ofcluster  key    in  the  mentions of #
i   ofcluster  output    in  the  mentionscorrect   of #1  
 
(3) F-measure (F): 
RP
RPF
+
=
*2  
 
The name co-reference precision and recall used 
here is adopted from the B_CUBED scoring 
scheme used in [Bagga & Baldwin 1998], which is 
believed to be an appropriate benchmarking 
standard for this task.  
Traditional benchmarking requires manually 
dividing person name mentions into clusters, 
which is labor intensive and difficult to scale up. In 
our experiments, an automatic corpus construction 
scheme is used in order to perform large-scale 
testing for reliable benchmarks. 
The intuition is that in the general news domain, 
some multi-token names associated with mass 
media celebrities is highly unambiguous. For 
example, ?Bill Gates?, ?Bill Clinton?, etc. 
mentioned in the news almost always refer to 
unique entities. Therefore, we can retrieve contexts 
of these unambiguous names, and mix them 
together. The name disambiguation algorithm 
should recognize mentions of the same name. The 
capability of recognizing mentions of an 
unambiguous name is equivalent to the capability 
of disambiguating ambiguous names. 
For the purpose of benchmarking, we 
automatically construct eight testing datasets 
(Testing Corpus I), listed in Table 1. 
Table 1. Constructed Testing Corpus I 
# of Mentions Name 
Set 1a Set 1b 
Mikhail S. Gorbachev 20 50
Dick Cheney 20 10
Dalai Lama 20 10
Bill Clinton 20 10
 Set 2a Set 2b 
Bob Dole 20 50
Hun Sen 20 10
Javier Perez de Cuellar 20 10
Kim Young Sam 20 10
 Set 3a Set 3b 
Jiang Qing 20 10
Ingrid Bergman 20 10
Margaret Thatcher 20 50
Aung San Suu Kyi 20 10
 Set 4a Set 4b 
Bill Gates 20 10
Jiang Zemin 20 10
Boris Yeltsin 20 50
Kim Il Sung 20 10
  
Table 2.  Testing Corpus I Benchmarking 
 P R F P R F 
 Set 1a Set 1b 
Baseline 0.79 0.37 0.58 0.78 0.34 0.56 
VSMOnly 0.86 0.33 0.60 0.78 0.23 0.51 
Full 0.98 0.75 0.86 0.90 0.79 0.85 
  Set 2a Set 2b 
Baseline 0.82 0.58 0.70 0.94 0.50 0.72 
VSMOnly 0.90 0.54 0.72 0.98 0.45 0.71 
Full 0.93 0.84 0.88 1.00 0.93 0.96 
 Set 3a Set 3b 
Baseline 0.84 0.69 0.77 0.80 0.34 0.57 
VSMOnly 0.95 0.72 0.83 0.93 0.29 0.61 
Full 0.95 0.86 0.90 0.98 0.57 0.77 
 Set 4a Set 4b 
Baseline 0.88 0.74 0.81 0.80 0.49 0.64 
VSMOnly 0.93 0.77 0.85 0.88 0.42 0.65 
Full 0.95 0.93 0.94 0.98 0.84 0.91 
Overall P R F 
Baseline 0.83 0.51 0.63 
VSMOnly 0.90 0.47 0.69 
Full 0.96 0.82 0.88 
 
Table 2 shows the benchmarks for each dataset, 
using the three measures just defined. The new 
algorithm when only using VSM-based similarity 
(VSMOnly) outperforms the existing algorithm 
(Baseline) by 5%. The new algorithm using the full 
context similarity measures including IE features 
(Full) significantly outperforms the existing 
algorithm (Baseline) in every test:  the overall F-
measure jumps from 64% to 88%, with 25 
percentage point enhancement.  This performance 
breakthrough is mainly due to the additional 
support from IE, in addition to the optimization 
method used in our algorithm. 
We have also manually truthed an additional 
testing corpus of two datasets containing mentions 
associated with the same name (Testing Corpus II). 
Truthed Dataset 5a contains 25 mentions of Peter 
Sutherland and Truthed Dataset 5b contains 68 
mentions of John Smith. John Smith is a highly 
ambiguous name. With its 68 mentions, they 
represent totally 29 different entities. On the other 
hand, all the mentions of Peter Sutherland are 
found to refer to the same person. The benchmark 
using this corpus is shown below. 
Table 3. Testing Corpus II Benchmarking 
 P R F P R F 
 Set 5a Set 5b 
Baseline 0.96 0.92 0.94 0.62 0.57 0.60 
VSMOnly 0.96 0.92 0.94 0.75 0.51 0.63 
Full 1.00 0.92 0.96 0.90 0.81 0.85 
 
Based on these benchmarks, using either 
manually truthed corpora or automatically 
constructed corpora, using either ambiguous 
corpora or unambiguous corpora, our algorithm 
consistently and significantly outperforms the 
existing algorithm. In particular, our system 
achieves a very high precision (0.96 precision). 
This shows the effective use of IE results which 
provide much more fine-grained evidence than co-
occurring words. It is interesting to note that the 
recall enhancement is greater than the precision 
enhancement (0.31 recall enhancement vs. 0.13 
precision enhancement). This demonstrates the 
complementary nature between evidence from the 
co-occurring words and the evidence carried by IE 
results. The system recall can be further improved 
once the recall of the currently precision-oriented 
IE engine is enhanced over time. 
7 Conclusion 
We have presented a new person name 
disambiguation algorithm which demonstrates a 
successful use of natural language IE support in 
performance enhancement. Our algorithm is 
benchmarked to outperform the previous algorithm 
by 25 percentage points in overall F-measure, 
where the effective use of IE contributes to 20 
percentage points. The core of this algorithm is a 
learning system trained on automatically 
constructed large corpora, only requiring minimal 
supervision in estimating a context-independent 
probability.   
8 Acknowledgements 
This work was partly supported by a grant from 
the Air Force Research Laboratory?s Information 
Directorate (AFRL/IF), Rome, NY, under contract 
F30602-03-C-0170.  The authors wish to thank 
Carrie Pine of AFRL for supporting and reviewing 
this work.   
References  
Bagga, A., and B. Baldwin. 1998. Entity-Based 
Cross-Document Coreferencing Using the 
Vector Space Model. In Proceedings of 
COLING-ACL'98.  
Deerwester, S., S. T. Dumais, G. W. Furnas, T. K. 
Landauer, and R. Harshman. 1990. Indexing by 
Latent Semantic Analysis. In Journal of the 
American Society of Information Science 
Gale, W., K. Church, and D. Yarowsky. 1992.  
One Sense Per Discourse.  In Proceedings of the 
4th DARPA Speech and Natural Language 
Workshop.  
Goodman, J. 2003. Exponential Priors for 
Maximum Entropy Models. 
Landauer, T. K., & Dumais, S. T. 1997. A solution 
to Plato's problem: The Latent Semantic 
Analysis theory of the acquisition, induction, and 
representation of knowledge. Psychological 
Review, 104, 211-240, 1997. 
MUC-7. 1998.  Proceedings of the Seventh 
Message Understanding Conference. 
Neal, R. M. 1993. Probabilistic Inference Using 
Markov Chain Monte Carlo Methods. Technical 
Report, Univ. of Toronto.  
Pietra, S. D., V. D. Pietra, and J. Lafferty. 1995. 
Inducing Features Of Random Fields. In IEEE 
Transactions on Pattern Analysis and Machine 
Intelligence. 
Srihari, R. K., W. Li, C. Niu and T. Cornell. 
InfoXtract: An Information Discovery Engine 
Supported by New Levels of Information 
Extraction. In Proceeding of HLT-NAACL 2003 
Workshop on Software Engineering and 
Architecture of Language Technology Systems, 
Edmonton, Canada. 
Extracting Exact Answers to Questions Based on Structural Links?
Wei Li, Rohini K. Srihari, Xiaoge Li, M. Srikanth, Xiuhong Zhang, Cheng Niu
Cymfony Inc.
600 Essjay Road, Williamsville, NY 14221. USA.
{wei, rohini, xli, srikanth, xzhang, cniu}@cymfony.com
Keywords:  Question Answering, Information Extraction, Semantic Parsing, Dependency Link
?  This  work was partly supported by a grant from the Air Force Research Laboratory?s Information Directorate 
(AFRL/IF), Rome, NY, under contracts F30602-00-C-0037 and F30602-00-C-0090.
Abstract
This paper presents a novel approach to
extracting phrase-level answers in a question 
answering system. This approach uses
structural support provided by an integrated 
Natural Language Processing (NLP) and
Information Extraction (IE) system. Both
questions and the sentence-level candidate
answer strings are parsed by this NLP/IE
system into binary dependency structures.
Phrase-level answer extraction is modelled by 
comparing the structural similarity involving 
the question-phrase and the candidate answer-
phrase.
There are two types of structural support. The 
first type involves predefined, specific entity 
associa tions such as Affiliation, Position, Age 
for a person entity. If a question asks about 
one of these associations, the answer-phrase
can be determined as long as the system
decodes such pre-defined dependency links 
correctly, despite the syntactic difference
used in expressions between the question and 
the candidate answer string. The second type 
involves generic grammatical relationships
such as V-S (verb-subject), V-O (verb-
object).
Preliminary experimental results show an
improvement in both precision and recall in 
extracting phrase-level answers, compared
with a baseline system which only uses Named 
Entity constraints. The proposed methods are 
particularly effective in cases where the
question-phrase does not correspond to a
known named entity type and in cases where 
there are multiple candidate answer-phrases
satisfying the named entity constraints.
Introduction
Natural language Question Answering (QA) is 
recognized as a capability with great potential.
The NIST-sponsored Text Retrieval Conference
(TREC) has been the driving force for developing 
this technology through its QA track since TREC-8
(Voorhees 1999). There has been significant
progress and interest in QA research in recent
years (Voorhees 2000, Pasca and Harabagiu 2001).
QA is different than search engines in two aspects: 
(i) instead of a string of keyword search terms, the 
query is a natural language question, necessitating 
question parsing, (ii) instead of a list of documents 
or URLs, a list of candidate answers at phrase level 
or sentence level are expected to be returned in 
response to a query, hence the need for text
processing beyond keyword indexing, typically
supported by Natural Language Processing (NLP) 
and Information Extraction (IE) (Chinchor and
Marsh 1998, Hovy, Hermjakob and Lin 2001, Li 
and Srihari 2000). Examples of the use of NLP and 
IE in Question Answering include shallow parsing 
(Kupiec, 1993), semantic parsing (Litkowski
1999), Named Entity tagging (Abney et al 2000, 
Srihari and Li 1999) and high-level IE (Srihari 
and Li, 2000).
Identifying exact or phrase-level answers is a
much more challenging task than sentence-level
answers. Good performance on the latter can be 
achieved by using sophisticated passage retrieval 
techniques and/or shallow level NLP/IE
processing (Kwok et al 2001, Clarke et al 2001). 
The phrase-level answer identification involves
sophisticated NLP/IE and it is difficult to apply 
only IR techniques for this task (Prager et al 
1999). These two tasks are closely related. Many 
systems (e.g. Prager et al1999; Clark et al2001) 
take a two-stage approach. The first stage
involves retrieving sentences or paragraphs in
documents as candidate answer strings. Stage
Two focuses on extracting phrase-level exact
answers from the candidate answer strings.
This paper focuses on methods involving Stage 
Two. The input is a sentence pair consisting of a 
question and a sentence-level candidate answer 
string. The output is defined to be a phrase, called 
answer-point, extracted from the candidate
answer string. In order to identify the answer-
point, the pair of strings are parsed by the same 
system to generate binary dependency structures 
for both specific entity associations and generic 
grammatical relationships. An integrated Natural 
Language Processing (NLP) and Information
Extraction (IE) engine is used to extract named 
entities (NE) and their associations and to decode 
grammatical relationships. The system searches
for an answer-point by comparing the structural 
similarity involving the question-phrase and a
candidate answer-phrase. Generic grammatical
relationships are used as a back-off for specific 
entity associations when the question goes beyond 
the scope of the specific associations or when the 
system fails to identify the answer-point which 
meets the specific  entity association constraints. 
The proposed methods are particularly helpful in 
cases where the question-phrase does not
correspond to a known named entity type and in 
cases where there are multiple candidate answer-
points to select from.
The rest of the paper is structured as follows: 
Section 1 presents the NLP/IE engine used,
sections 2 discusses how to identify and formally 
represent what is being asked, section 3 presents 
the algorithm on identifying exact answers
leveraging structural support, section 4 presents 
case studies and benchmarks, and section 5 is the 
conclusion.
Kernel IE Modules Linguistic  Modules
Entity
Association
Named
Entity
Part-Of-
Speech
Asking-point
Identification
O
ut
pu
t(
En
ti
ty
, 
Ph
ra
se
 a
nd
 S
tr
uc
tu
ra
l 
Li
nk
s)
Shallow
Parsing
Semantic
Parsing
Tokenizer
Input
Figure 1: InfoXtract? NLP/IE System Architecture
1 NLP/IE Engine Description
The NLP/IE engine used in the QA system
described here is named InfoXtract?. It consists 
of an NLP component and IE component, each 
consisting of a set of pipeline modules (Figure 1). 
The NLP component serves as underlying support 
for IE. A brief description of these modules is 
given below.
? Part-of-Speech Tagging: tagging syntactic
categories such as noun, verb, adjective, etc. 
? Shallow Parsing: grouping basic linguistic
units as building blocks for structural links, 
such as Basic Noun Phrase, Verb Group, etc. 
? Asking-point Identification: analysis of
question sentences to determine what is being 
asked
? Semantic Parsing: decoding grammatical
dependency relationships at the logical level 
between linguistic units, such as Verb-Subject
(V-S), Verb-Object (V-O), Head-Modifier
(H-M) relationships; both active patterns and 
passive patterns will be parsed into the same 
underlying logical S-V-O relationships
? Named Entity Tagger: classifying proper
names and other phrases to different
categories such as Person, Organization,
Location, Money, etc.
? Entity Association Extractor: relating named 
entities with predefined associations such as 
Affiliation, Position, Age, Spouse, Address,
etc.
The NE tagger in our system is benchmarked to 
achieve close to human performance, around or 
above 90% precision and recall for most
categories of NE. This performance provides
fundamental support to QA. Many questions
require a named entity or information associated 
with a named entity as answers. A subset of the 
NE hierarchy used in our system is illustrated
below:
Person: woman, man
Organization: company, government,
association, school, army, mass-media
Location: city, province, country, continent, 
ocean, lake, etc.
Time Expressions: hour, part-of-day, day-of-
week, date, month, season, year, decade, 
century, duration
Numerical Expressions: percentage, money, 
number, weight, length, area, etc.
Contact expressions: email, address,
telephone, etc.
The Entity Association module correlates named 
entities and extracts their associations with other 
entities or phrases.  These are specific, predefined 
relationships for entities of person and
organization. Currently, our system can extract
the following entity associations with high
precision (over 90%) and modest recall ranging 
from 50% to 80% depending on the size of
grammars written for each specific association. 
Person: affiliation, position, age, spouse,
birth-place, birth-time, etc.
Organization: location, staff, head, products,
found-time, founder, etc.
Entity associations are semantic structures very
useful in supporting QA. For example, from the 
sentence Grover Cleveland , who in June 1886
married 21-year-old Frances Folsom,?the IE
engine can identify the following associations:
Spouse: Grover Cleveland ?Frances Folsom
Spouse: Frances?Grover Cleveland 
Age:  Frances Folsom?21-year-old
A question asking about such an association, say, 
Q11: Who was President Cleveland ?s wife, will be 
parsed into the following association link between 
a question-phrase ?Who? and the entity ?Cleveland? 
(see Section 2): Spouse: Cleveland ? Who. The 
semantic similarity between this structure and the 
structure Spouse: Grover Cleveland ? Frances 
Folsom can determine the answer point to be
?Frances Folsom?.
The Semantic Parsing module decodes the
grammatical dependency relationships: V-S, V-O,
V-C (Verb-Complement), H-M of time, location, 
reason, manner, purpose, result, etc. This module 
extends the shallow parsing module through the 
use of a cascade of handcrafted pattern matching 
rules.  Manual benchmarking shows results with 
the following performance:
H-M: Precision 77.5%
V-O: Precision 82.5%
V-S: Precision 74%
V-C: Precision 81.4%
In our semantic parsing, not only passive patterns
will be decoded into the same underlying
structures as active patterns, but structures for
verbs such as acquire and for de-verbal nouns such 
as acquisition lead to the same dependency links, 
as shown below.
AOL acquired Netscape in 1998. ?
V-S: acquired? AOL
V-O: acquired ? Netscape
H-M: acquired ? in 1998 (time-modifier)
Netscape was acquired by AOL in 1998. ?
V-S: was acquired ? by AOL
V-O: was acquired ? Netscape
H-M: was acquired ? in 1998 (time-modifier)
the acquisition of Netscape by AOL in 1998??
V-S: acquisition ? by AOL
V-O: acquisition ? of Netscape 
H-M: acquired ? in 1998 (time-modifier)
These links can be used as structural support to 
answer questions like Who acquired Netscape or
which company was acquired by AOL.
Obviously, our semantic parser goes one step
further than parsers which only decode syntactic 
relationships. It consumes some surface structure 
variations to provide the power of comparing the 
structural similarity at logical level. However,
compared with the entity association structures 
which sits at deep semantic level, the logical SVO 
(Subject-Verb-Object) structures still cannot
capture semantic relations which are expressed
using different head verbs with different
structures. An example is the pair : X borrows Y 
from Z versus Z lends Y to X.
2 Asking Point Link Identification
Asking point link identification is a crucial step in 
a QA system. It provides the necessary
information decoded from question processing for 
a system to locate the corresponding answer-
points from candidate answer strings. 
The Asking-point (Link) Identification Module is 
charged with the task of parsing wh-phrases in 
their context into three categories: NE Asking-
point, Asking-point Association  Link and
Asking-point Grammar  Link. Asking Point refers
to the question phrases with its constraints  that a 
corresponding answer-point should satisfy in
matching. Asking-point Link is the decoded binary 
relationship from the asking point to another unit 
in the question. 
The identification of the NE asking point is
essentially mapping the wh-phrase to the NE
types or subtypes. For example, which year is 
mapped to [which year]/NeYear, how old mapped 
to [how old]/NeAge, and how long mapped to 
[how long]/NeLength or [how long]/NeDuration, 
etc.
The identification of the Asking-point Association
Link is to decide whether the incoming question 
asks about a predefined association relationship. 
For Asking-point Association  Link, the module 
needs to identify the involved entity and the asked 
association. For example, the Asking-point
Association  Link for How old is John Smith is the 
AGE relationship of the NePerson John Smith,
represented as AGE: John Smith ? [how
old]/NeAge.
The wh-phrases which may or may not be mapped 
to NE asking points and whose dependency links 
are beyond predefined associations lead to Asking-
point Grammar Links, e.g. How did Julian Hill 
discover nylon? This asking-point link is
represented as H-M: discover ? [How]/manner-
modifier. As seen, an asking-point grammar link 
only involves generic grammatical constraints: in 
this case, the constraints for a candidate answer-
point to satisfy during matching are H-M link with 
?discover? as head and a phrase which must be a 
modifier of manner. 
These three types of asking points and their
possible links form a natural hierarchy that can be 
used to facilitate the backoff strategy for the
answer-point extraction module (see Section 3): 
Asking-point Association Link ? Asking-point
Grammar Link ? NE Asking Point.  This
hierarchy defines the sequence of matching steps 
which should be followed during the answer-point
extraction.
The backoff from Asking-point Association  Link 
to Asking-point Grammar  Link is necessary as the 
latter represents more generic structural constraints 
than the former. For example, in the sentence
where is IBM located, the Asking-point
Association Link is LOCATION: IBM ?
[where]/NeLocation while the default Grammar
Link is H-M: located ? [where]/location-
modifier. When the specific association constraints 
cannot be satisfied, the system should attempt to 
locate an answer-point by searching for a location-
modifier of the key verb ?located?.
The NE asking point constraints are also marked 
for asking-point association links and those asking-
point grammar links whose wh-phrases can be
mapped to NE asking points. Backing off to the 
NE asking point is required in cases where the 
asking-point association constraints and
grammatical structural constraints cannot be
satisfied. For How old is John Smith, the asking-
point grammar  link is represented as H-M: John 
Smith ? [how old]/NeAge. If the system cannot 
find a corresponding AGE association or a
modifier of NeAge for the entity John Smith to
satisfy the structural constraints, it will at least 
attempt to locate a candidate answer-point by
enforcing the NE asking point constraints NeAge. 
When there is only one NeAge in the answer
string, the system can extract it as the only
possible answer-point even if the structural
constraints are not honored.
3 Answer Point Identification
The answer-point identification is accomplished 
through  matching the asking-point to candidate 
answer-points using the following back-off
algorithm based on the processing results of the 
question and the sentence-level candidate answer 
string.
(1) if there is Asking-point Association
Link, call Match(asking-point association 
link, candidate answer-point association 
link) to search for the corresponding
association to locate answer-point
(2) if step (1) fails and there is an asking-
point grammar link, call Match(asking-
point grammar link, candidate answer-
point grammar link) to search for the
corresponding grammar link to locate the 
answer-point
(3) if step (2) fails and there is an NE asking 
point, search for the corresponding NEs: 
if there is only one corresponding NE, 
then extract this as the answer-point else 
mark all corresponding NEs as candidate 
answer-points
The function Match(asking-point link, candidate 
answer-point link) is defined as (i) exact match or 
synonym match of the related units (synonym
match currently confined to verb vs. de-verbal
noun); (ii) match the relation type directly (e.g. V-
S matches V-S, AGE matches AGE, etc.); (iii) 
match the type of asking point and answer point 
(e.g. NePerson asking point matches NePerson and
its sub-types NeMan and NeWoman; ?how?
matches manner-modifier; etc.): either through
direct link or indirect link based on conjunctive 
link (ConjLink) or equivalence link (S-P, subject-
predicative or appositive relations between two
NPs).
Step (1) and Step (2) attempt to leverage the
structural support from parsing and high-level
information extraction beyond NE. It is worth
noticing that in our experiment, the structural
support used for answer-point identification only 
checks the binary links involving the asking point 
and the candidate answer points, instead of full 
template matching as proposed in (Srihari and Li, 
2000).
Full template matching is best exemplified by the 
following example. If the incoming question is 
Who won the Nobel Prize in 1991, and the
candidate answer string is John Smith won the
Nobel Prize in 1991, the question template and 
answer template are shown below:
win
V-S: NePerson [Who]
V-O: NP [the Nobel Prize]
H-M: NeYear [1991]
win
V-S: NePerson [John Smith]
V-O: NP [the Nobel Prize]
H-M: NeYear [1991]
The template matching will match the asking point 
Who with the answer point John Smith because for 
all the dependency links in the trees, the
information is all compatible (in this case, exact
match). This is the ideal case of full template
matching and guarantees the high precision of the 
extracted answer point.
However, in practice, full template matching is 
neither realistic for most of cases nor necessary for 
achieving the objective of extracting answer points 
in a two-stage approach. It is not realistic because 
natural language semantic parsing is such a
challenging problem that a perfect dependency tree 
(or full template) which pieces together every
linguistic unit is not always easy to decode. For
InfoXtract,, in most cases, the majority, but not 
all, of the decoded binary dependency links are 
accurate, as shown in the benchmarks above. In 
such situations, insisting on checking every
dependency link of a template tree is too strong a 
condition to meet. On the other hand, it is actually 
not necessary to check all the links in the
dependency trees for full template matching. With 
the modular design and work division between
sentence level candidate answer string generation 
module (Stage One) and answer-point extraction 
from the candidate answer strings (Stage Two), 
all the candidate answer strings are already
determined by previous modules as highly
relevant. In this situation, a simplified partial
template matching, namely, ?asking/answer point 
binary relation matching?, will be sufficient to 
select the answer-point, if present, from the
candidate answer string. In other words, the
system only needs to check this one dependency 
link in extracting the answer-point. For the
previous example, only the asking/answer point 
binary dependency links need to be matched as 
illustrated below:
V-S win?[Who]/NePerson
V-S win?[John Smith]/NeMan
Some sample results are given in section 4 to
illustrate how answer-points are identified based 
on matching binary relations involving
asking/answer points. 
4 Experiments and Results
In order to conduct the feasibility study on the 
proposed method, we selected the first 100
questions from the TREC-8 QA track pool and 
the corresponding first candidate answer
sentences for this preliminary experiment. The 
Stage One processing for generating candidate 
answer sentences was conducted by the existing 
ranking module of our QA system. The Stage
Two processing for answer-point identification
was accomplished by using the algorithm
described in Section 3.
As shown in Table 1, out of the 100 question-
answer pairs we selected, 9 have detected
association links involving asking/answer points, 
44 are found to have grammar links involving 
asking/answer points. 
Table 1: Experiment Results
detected correct fail precision recall
Association
Links 9 8 1 89% 8%
Grammar
Links 44 39 6 89% 39%
NE Points 
(Baseline) 76 41 35 54% 41%
Overall
performance 86 71 14 83% 71%
As for NE asking points, 76 questions were
identified to require some type of NE as answers.
Assume that a baseline answer-point identification 
system only uses NE asking points as constraints, 
out of the 76 questions requiring NEs as answers, 
41 answer-points were identified successfully
because there was only one NE in the answer
string which matches the required NE type. The 
failed cases in matching NE asking point
constraints include two situations: (i) no NE exists 
in the answer string; (ii) multiple NEs satisfy the 
type constraints of NE asking points (i.e. more 
than one candidate answer-points found from the 
answer string) or there is type conflict during the 
matching of NE asking/answer points. Therefore, 
the baseline system would achieve 54% precision 
and 41% recall based on the standard precision and 
recall formulas: 
Precision = Correct / Detected
Recall = Correct / Relevant. 
In comparison, in our answer-point identification 
system which leverages structural support from
both the entity association links and grammar links 
as well as the NE asking points, both the precision 
and recall are raised: from the baseline 54% to 
83% for precision and from 41% to 71% for recall. 
The significant improvement in precision and
recall is attributed to the performance of structural 
matching in identifying exact answers. This
demonstrates the benefits of making use of
sophisticated NLP/IE technology, beyond NE and 
shallow parsing.
Using grammar links alone, exact answers were
identified for 39 out of the 44 candidate answer-
points satisfying the types of grammar links in 100 
cases. During matching, 6 cases failed either due to 
the parsing error or due to the type conflict
between the asking/answer points (e.g. violating 
the type constraints such as manner-modifier on 
the answer-point for ?how? question). The high 
precision and modest recall in using the grammar 
constraints is understandable as the grammar links 
impose very strong constraints on both the nodes 
and the structural type. The high precision
performance indicates that grammar links not
only have the distinguishing power to identify
exact answers in the presence of multiple NE 
options but also recognize answers in the absence 
of asking point types.
Even stronger structural support comes from the 
semantic relations decoded by the entity
association extraction module.  In this case, the 
performance is naturally high-precision (89%)
low-recall (8%) as predefined association links 
are by nature more sparse than generic
grammatical relations.
In the following, we illustrate with some
examples with questions from the TREC-8 QA 
task on how the match function identified in
Section 3 applies to different question types.
Q4: How much did Mercury spend on
advertising in 1993? ? asking-point grammar 
link:
V-O spend ? [How much]/NeMoney
A: Last year the company spent Pounds 12m
on advertising. ? candidate answer-point
grammar link:
V-O spent?[Pounds 12m]/NeMoney
Answer-point Output: Pounds 12m
This case requires (i) exact match in its original 
verb form between spend and spent; (ii) V-O type 
match; and (iii) asking/answer point type
NeMoney match through direct link.
Q63: What nuclear-powered Russian
submarine sank in the Norwegian Sea on April 
7, 1989?? asking-point grammar link: 
H-M submarine?[What]
A: NEZAVISIMAYA GAZETA on the
Komsomolets nuclear-powered submarine
which sank in the Norwegian Sea five years 
ago:? candidate answer-point grammar link:
H-M submarine?Komsomolets
Answer-point Output: Komsomolets
This case requires (i) exact match of submarine;
(ii) H-M type match; and (iii) asking/answer point 
match through direct link:  there are no asking
point type constraints because the asking point 
goes beyond existing NE. This case highlights the 
power of semantic parsing in answer-point
extraction. Since there are no type constraints on 
answer point,1 candidate answer points cannot be 
extracted without bringing in structural context by 
checking the NE type. Most of what-related asking 
points such as those in the patterns
?what/which?N?, ?what type/kind of ?N? go
beyond NE and require this type of structural
relation checking to locate the exact answer. The 
case below is another example.
Q79: What did Shostakovich write for
Rostropovich?? asking-point grammar link: 
V-O write?[What]
A: The Polonaise from Tchaikovsky?s opera
Eugene was a brief but cracking opener and its 
brilliant bluster was no sooner in our ears than 
forcibly contradicted by the bleak depression of 
Shostakovich?s second cello concerto, Op. 126,
a late work written for Rostropovich in 1966 
between the thirteenth and fourteenth
symphonies. ? candidate answer-point
grammar link:
V-O written?[a late work]/NP
S-P [Op. 126]/NP ?[a late work]/NP
Answer-point Output: Op. 126
This case requires (i) exact match in its original 
verb form between ?written? and ?write?;
(ii) V-O type match; and (iii) asking/answer point 
match through indirect link based on equivalence 
link S-P. When there are no NE constraints on the 
answer point, a proper name or an initial-
capitalized NP is preferred over an ordinary,
lower-case NP as an answer point. This heuristic is 
built-in so that ?Op. 126? is output as the answer-
point in this case instead of ?a late work?.
1 Strictly speaking, there are some type constraints on 
the answer point. The type constraints are something to 
the effect of ?a name for a kind of ship? which goes 
beyond the existing NE types defined.
Conclusion
This paper presented an approach to exact answer 
identification to questions using only binary
structural links involving the question-phrases.
Based on the experiments conducted, some
preliminary conclusions can be arrived at.
? The Entity Association extraction helps in 
pinpointing exact answers precisely
? Grammar dependency links enable the
system to not only identify exact answers 
but answer questions not covered by the 
predefined set of available
NEs/Associations
? Binary dependency links instead of full 
structural templates provide sufficient and 
effective structural leverage for extracting 
exact answers 
Some cases remain difficult however, beyond the 
current level of NLP/IE.  For example,
Q92: Who released the Internet worm in the 
late 1980s?? asking point link: 
V-S (released, NePerson[Who])
A: Morris, suspended from graduate studies at 
Cornell University at Syracuse, N,Y,, is
accused of designing and disseminating in
November, 1988, a rogue program or ?worm? 
that immobilized some 6,000 computers linked 
to a research network, including some used by 
NASA and the Air Force.? answer point link:
V-S (disseminating, NePerson[Morris]) 
In order for this case to be handled, the following 
steps are required: (i) the semantic parser should 
be able to ignore the past participle postmodifier 
phrase headed by ?suspended?; (ii) the V-O
dependency should be decoded between ?is
accused? and ?Morris?; (iii) the V-S dependency 
should be decoded between ?designing and
disseminating? and ?Morris? based on the pattern 
rule ?accuse NP of Ving?? V-S(Ving, NP); (iv) 
the conjunctive structure should map the V-S
(?designing and disseminating?, ?Morris?) into two 
V-S links; (v)  ?disseminate? and ?release? should
be linked somehow for synonym expansion.  It 
may be unreasonable to expect an NLP/IE system 
to accomplish all of these, but each of the above 
challenges indicates some directions for further 
research in this topic.
We would like to extend the experiments on a 
larger set of questions to further investigate the 
effectiveness of structural support in extracting
exact answers. The TREC-9 and TREC 2001 QA 
pool and the candidate answer sentences generated 
by both NLP-based or IR-based QA systems would 
be ideal for further testing this method.
5 Acknowledgement
The authors wish to thank Walter Gadz and Carrie 
Pine of AFRL for supporting this work. Thanks 
also go to anonymous reviewers for their valuable 
comments.
References
Abney, S., Collins, M and Singhal, A. (2000) Answer
Extraction. In Proceedings of ANLP -2000, Seattle.
Chinchor, N. and Marsh, E. (1998) MUC -7 Information 
Extraction Task Definition (version 5.1), In
?Proceedings of MUC-7?. Also published at
http://www.muc.saic.com/
Clarke, C. L. A., Cormack, G. V. and Lynam, T. R. 
(2001), Exploiting Redundancy in Question
Answering. In Proceedings of SIGIR?01, New
Orleans, LA.
Hovy, E.H., U. Hermjakob, and Chin-Yew Lin. 2001. 
The Use of External Knowledge of Factoid QA. In 
Proceedings of the 10th Text Retrieval Conference 
(TREC 2001), Gaithersburg, MD, U.S.A., November 
13-16, 2001
Kupiec, J. (1993) MURAX: A Robust Linguistic
Approach For Question Answering Using An On-Line
Encyclopaedia . In Proceedings of SIGIR-93,
Pittsburgh, PA.
Kwok, K. L., Grunfeld, L., Dinstl, N. and Chan, M. 
(2001), TREC2001 Question-Answer, Web and Cross 
Language Experiments using PIRCS. In Proceedings 
of TREC-10, Gaithersburg, MD.
Li, W. and Srihari, R. (2000) A Domain Independent 
Event Extraction Toolkit , Phase 2 Final Technical 
Report, Air Force Research Laboratory/Rome, NY.
Litkowski, K. C. (1999) Question-Answering Using
Semantic Relation Triples. In Proceedings of TREC-
8, Gaithersburg, MD.
Pasca, M. and Harabagiu, S. M. High Performance
Question/Answering. In Proceedings of SIGIR 2001: 
pages 366-374
Prager, J., Radev, D., Brown, E., Coden, A. and Samn, 
V., The use of predictive annotation for question
answering in TREC8. In Proceedings of TREC-8,
Gaithersburg, MD.
Srihari, R. and Li, W. (1999) Information Extraction 
supported Question Answering. In Proceedings of
TREC-8, Gaithersberg, MD.
Srihari, R and Li, W. (2000b). A Question Answering 
System Supported by Information Extraction. In 
Proceedings of ANLP 2000, Seattle.
Voorhees, E. (1999), The TREC-8 Question Answering 
Track Report, In Proceedings of TREC-8,
Gaithersburg, MD. 
Voorhees, E. (2000), Overview of the TREC-9
Question Answering Track , In Proceedings of
TREC-9, Gaithersburg, MD. 
InfoXtract location normalization: a hybrid approach to geographic 
references in information extraction ? 
 
 
Huifeng Li, Rohini K. Srihari, Cheng Niu, and Wei Li 
Cymfony Inc. 
600 Essjay Road, Williamsville, NY 14221, USA 
(hli, rohini, cniu, wei)@cymfony.com 
 
 
                                                     
? This work was partly supported by a grant from the Air Force Research Laboratory?s Information Directorate (AFRL/IF), Rome, 
NY, under contract F30602-01-C-0035. The authors wish to thank Carrie Pine of AFRL for supporting and commenting this work. 
Abstract  
Ambiguity is very high for location names. For 
example, there are 23 cities named ?Buffalo? in the 
U.S.  Based on our previous work, this paper presents 
a refined hybrid approach to geographic references 
using our information extraction engine InfoXtract. 
The InfoXtract location normalization module 
consists of local pattern matching and discourse 
co-occurrence analysis as well as default senses.  
Multiple knowledge sources are used in a number of 
ways: (i) pattern matching driven by local context, 
(ii) maximum spanning tree search for discourse 
analysis, and (iii) applying default sense heuristics 
and extracting default senses from the web. The 
results are benchmarked with 96% accuracy on our 
test collections that consist of both news articles and 
tourist guides. The performance contribution for each 
component of the module is also benchmarked and 
discussed. 
 
1 Introduction 
The task of location normalization is to decode 
geographic references for extracted location  Named 
Entities (NE). Ambiguity is a very serious problem for 
location NEs. For example, there are 23 cities named 
?Buffalo?, including the city in New York State and in 
the state of Alabama. Country names such as 
?Canada?, ?Brazil?, and ?China? are also city names in 
the USA. Such ambiguity needs to be properly 
handled before converting location names into normal 
form to support Entity Profile (EP) construction, 
information merging/consolidation as well as 
visualization of location-stamped extracted events on 
a map.  
Location normalization is a special application of 
word sense disambiguation (WSD). There is 
considerable research on WSD. Knowledge-based 
work, such as [Hirst 1987; McRoy 1992; Ng and 
Lee 1996] used hand-coded rules or supervised 
machine learning based on an annotated corpus to 
perform WSD. Recent work emphasizes a 
corpus-based unsupervised approach [Dagon and 
Itai 1994; Yarowsky 1992; Yarowsky 1995] that 
avoids the need for costly truthed training data.  
Location normalization is different from general 
WSD in that the selection restriction often used for 
WSD in many cases is not sufficient to distinguish 
the correct sense from the other candidates. For 
example, in the sentence ?The White House is 
located in Washington?, the selection restriction 
from the collocation ?located in? can only 
determine that ?Washington? should be a location 
name, but is not sufficient to decide the actual sense 
of this location.  
In terms of local context, we found that there are 
certain fairly predictable keyword-driven patterns 
which can decide the senses of location NEs. These 
patterns use keywords such as ?city?, ?town?, 
?province?, ?on?, ?in? or candidate location subtypes 
that can be assigned from a location gazetteer. For 
example, the pattern ?X + city? can determine sense 
tags for cases like ?New York City?; and the pattern 
?Candidate-city-name + comma + 
Candidate-state-name? can disambiguate cases 
such as ?Albany, New York? and ?Shanghai, 
Illinois?.  
In the absence of these patterns, co-occurring 
location NEs in the same discourse provide 
evidence for predicting the most probable sense of a 
location name. More specifically, location 
normalization depends on co-occurrence 
constraints of geographically related location entities 
mentioned in the same document. For example, if 
?Buffalo?, ?Albany? and ?Rochester? are mentioned in 
the same  document, the most probable senses of 
?Buffalo?, ?Albany? and ?Rochester? should refer to 
the cities in New York State.   
For choosing the best matching sense set within a 
document, we simply construct a graph where each 
node represents a sense of a location NE, and each 
edge represents the relationship between two location 
name senses. A graph  spanning algorithm can be used 
to select the best senses from the graph.  
Last but not least, proper assignment of default 
senses is found to play a significant role in the 
performance of a location normalizer. This involves 
two issues: (i) determining default senses using 
heuristics and/or other methods, such as statistical 
processing for semi-automatic default sense extraction 
from the web [Li et al 2002]; and (ii) setting the 
conditions/thresholds and the proper levels when 
assigning default senses, to coordinate with local and 
discourse evidence for enhanced performance. The 
second issue can be resolved through experimentation. 
In the light of the above overview, this paper 
presents an effective hybrid location normalization 
approach which consists of local pattern matching and 
discourse co-occurrence analysis as well as default 
senses. Multiple knowledge sources are used in a 
number of ways: (i) pattern matching driven by local 
context, (ii) maximum spanning tree search for 
discourse analysis, and (iii) applying heuristics-based 
default senses and web-extracted default senses in 
proper stages.  
In the remaining text, Section 2 introduces the 
background for this research. Section 3 describes our 
previous work in this area and Section 4 presents the 
modified algorithm to address the issues with the 
previous method. Experiment and benchmarks are 
described in Section 5. Section 6 is the conclusion. 
 
2 Background 
The design and implementation of the location 
normalization module is an integrated part of 
Cymfony?s core information extraction (IE) engine 
InfoXtract. InfoXtract extracts and normalizes entities, 
relationships and events from natural language text.  
Figure 1 shows the overall system architecture of 
InfoXtract, involving multiple modules in a pipeline 
structure.  
InfoXtract involves a spectrum of linguistic 
processing and relationship/event extraction. This 
engine, in its current state, involves over 100 levels of 
processing and 12 major components. Some 
components are based on hand-crafted pattern 
matching rules, some are statistical models or 
procedures, and others are hybrid (e.g. NE, 
Co-reference, Location Normalization). The basic 
information extraction task is NE tagging [Krupka 
and Hausman 1998; Srihari et al 2000].  The NE 
tagger identifies and classifies proper names of type 
PERSON, ORGANIZATION, PRODUCT, 
NAMED-EVENTS, LOCATION (LOC) as well as 
numerical expressions such as MEASUREMENT 
(e.g. MONEY, LENGTH, WEIGHT, etc) and time 
expressions (TIME, DATE, MONTH, etc.). 
Parallel to location normalization, InfoXtract also 
involves time normalization and measurement 
normalization.  
   
Document Processor
Knowledge Resources
Lexicon
Resources
Grammars
Process
Manager
Tokenlist
Legend
Output
Manager
Source
Document
Linguistic Processor(s)Tokenizer
Tokenlist
Lexicon Lookup
 POS Tagging
NE Tagging
Shallow
Parsing
Relationship
Extraction
Document
pool
NE
CE
EP
SVO
Time
Normalization
Profile/Event
Consolidation
Event
Extraction
Abbreviations
NE = Named Entity
CE = Correlated Entity
EP = Entity Profile
SVO = Subject-Verb-Object
GE = General Event
PE = Predefined Event
Rule-based
Pattern Matching
Procedure or
Statistical Model
Hybrid
Module
GE
Statistical
Models
PE
IE
Repository
Deep Parsing
Coreference
Location
Normalization
Measurement
Normalization
 Figure 1:  System Architecture of InfoXtract 
 
InfoXtract combines the Maximum Entropy 
Model (MaxEnt) and Hidden Markov Model for 
NE tagging [Srihari et al 2000]. Maximum 
Entropy Models incorporate local contextual 
evidence to handle ambiguity of information from a 
location gazetteer. In the Tipster Location 
Gazetteer used by InfoXtract, there are many 
common words, such as I, A, June, Friendship, etc. 
Also, there is large overlap between person names 
and location names, such as Clinton, Jordan, etc. 
Using MaxEnt, systems learn under what situation 
a word is a location name, but it is very difficult to 
determine the correct sense of an ambiguous 
location name. The NE tagger in InfoXtract only 
assigns the location super-type tag LOC to the 
identified location words and leaves the task of 
location sub-type tagging such as CITY or STATE 
and its disambiguation to the subsequent module 
Location Normalization.  
Beyond NE, the major information objects 
extracted by InfoXtract are Correlated Entity (CE) 
relationships (e.g. AFFILIATION and POSITION), 
Entity Profile (EP) that is a collection of extracted 
entity-centric information, Subject-Verb-Object 
(SVO) which refers to dependency links between 
logical subject/object and its verb governor, General 
Event (GE) on who did what when and where and 
Predefined Event (PE) such as Management 
Succession and Company Acquisition.  
It is believed that these information objects capture 
the key content of the processed text. When 
normalized location, time and measurement NEs are 
associated with information objects (events, in 
particular) based on parsing, co-reference and/or 
discourse propagation, these events are stamped. The 
processing results are stored in IE Repository, a 
dynamic knowledge warehouse used to support 
cross-document consolidation, text mining for hidden 
patterns and IE applications. For example, 
location-stamped events can support information 
visualization on maps (Figure 2); time-stamped 
information objects can support visualization along a 
timeline; measurement-stamped objects will allow 
advanced retrieval such as find all Company 
Acquisition events that involve money amount greater 
than 2 million US dollars. 
 
Event type: <Die: Event 200>
Who:       <Julian Werver Hill: PersonProfile 001>
When:     1996-01-07
Where:    <LocationProfile103>
Preceding_event: <hospitalize: Event 260>
Subsequent_event: <bury: Event 250>
Event Visualization
;  ; 
; ; 
Predicate: DieWho: Julian Werner Hill
When:
Where: <LocationProfile 103>
Hockessin, Delaware, USA,
19707,75.688873,39.77604
1996-01-07
Figure 2:  Location-stamped Information 
Visualization 
 
3 Previous Work and Issues 
This paper is follow-up research based on our previous 
work [Li et al 2002]. Some efficiency and 
performance issues are identified and addressed by the 
modified approach.  
The previous algorithm [Li et al 2002] for location 
normalization consisted of five steps. 
 
Step 1. Look up location names in the 
gazetteer to associate candidate senses for 
each location NE; 
Step 2. Call the pattern matching sub-module 
to resolve the ambiguity of the NEs involved 
in local patterns like ?Williamsville, New 
York, USA? to retain only one sense for the 
NE as early as possible; 
Step 3. Apply the ?one sense per discourse? 
principle [Gale et al1992] for each 
disambiguated location name to propagate 
the selected sense to its other mentions 
within a document; 
Step 4. Call the discourse sub-module, 
which is a graph search algorithm 
(Kruskal?s algorithm), to resolve the 
remaining ambiguities; 
Step 5. If the decision score for a location 
name is lower than a threshold, we choose a 
default sense of that name as a result. 
In this algorithm, Step 2, Step 4, and Step 5 
complement each other, and help produce better 
overall performance.  
Step 2 uses local context that is the co-occurring 
words around a location name. Local context can be 
a reliable source in deciding the sense of a location. 
The following are the most commonly used 
patterns for this purpose.  
 
(1) LOC + ?,? + NP (headed by ?city?)  
e.g. Chicago, an old city  
(2) ?city of? + LOC1 + ?,? + LOC2 
e.g. city of Albany, New York 
(3) ?city of? + LOC 
(4) ?state of? + LOC  
(5) LOC1+ ?,? + LOC2 + ?,? + LOC3 
e.g. (i) Williamsville, New York, USA 
       (ii) New York, Buffalo, USA 
     (6) ?on?/ ?in? + LOC 
 e.g. on Strawberry  ISLAND 
 in Key West  CITY 
 
Patterns (1) , (3), (4) and (6) can be used to decide if 
the location is a city, a state or an island, while 
patterns (2) and (5) can be used to determine both 
the sub-tag and its sense. 
Step 4 constructs a weighted graph where each 
node represents a location sense, and each edge 
represents similarity weight between location 
names. The graph is partially complete since there 
are no links among the different senses of a location 
name. The maximum weight spanning tree (MST) 
is calculated using Kruskal?s MinST algorithm 
[Cormen et al 1990]. The nodes on the resulting 
MST are the most promising senses of the location 
names.  
Figure 3 and Figure 4 show the graphs for 
calculating MST. Dots in a circle mean the number 
of senses of a location name. 
Through experiments, we found an efficiency 
problem in Step 4 which adopted Kruskal?s 
algorithm for MST search to capture the impact of 
location co-occurrence in a discourse. While this 
algorithm works fairly well for short documents (e.g. 
most news articles), there is a serious time complexity 
issue when numerous location names are contained in 
long documents. A weighted graph is constructed by 
linking sense nodes for each location with the sense 
nodes for other locations. In addition, there is also an 
associated performance issue: the value weighting for 
the calculated edges using the previous method is not 
distinctive enough.  We observe that the number of 
location mentions and the distance between the 
location names impact the selection of location senses, 
but the previous method could not reflect these factors 
in distinguishing the weights of candidate senses. 
 
Canada
{Kansas,
Kentucky,
Country}
Vancouver
{British Columbia
Washington
port in USA
Port in Canada}
New York 
{Prov in USA,
New York City,
?}
Toronto
(Ontorio,
New South Wales,
Illinois,
?}
Charlottetown
{Prov in USA,
New York City,
?}
Prince Edward Island
{Island in Canada,
Island in South Africa,
Province in Canada}
Quebec
(city in Quebec,
Quebec Prov,
Connecticut,
?}
3*4 lines
2*3 lines
4*11 lines
11*10 lines
3*10 lines8*3 lines
2*8 lines
2*43*11
3*10
3*33*8
2*10
2*3
8*4
8*11
8*10
8*4
10*4
3*11
 
Figure 3:  Graph and its Spanning Tree 
 
Canada
{Kansas,
Kentucky,
Country}
Vancouver
{British Columbia
Washington
port in USA
Port in Canada}
New York 
{Prov in USA,
New York City,
?}
Toronto
(Ontorio,
New South Wales,
Illinois,
?}
Charlottetown
{city in New York
Port in canada}
Prince Edward Island
{Island in Canada,
Island in South Africa,
Province in Canada}
Quebec
(city in Quebec,
Prov in Canada,
Connecticut,
?}
3.6
3.6
3.66
3.6
3.6
0
1
2
3
4
5
6
7
 
Figure 4:  Max Spanning Tree 
 
Finally, our research shows that default senses play 
a significant role in location normalization. For 
example, people refer to ?Los Angeles? as the city in 
California more than the city in the Philippines, Chile, 
Puerto Rico, or the city in Texas in the USA. 
Unfortunately, the available Tipster Gazetteer 
(http://crl.nmsu.edu/cgi-bin/Tools/CLR/clrcat) does 
not mark default senses for most entries. It has 
171,039 location entries with 237,916  senses, among 
which 30,711 location names are ambiguous. 
Manually tagging the default senses for over 30,000 
location names is difficult; moreover, it is also subject 
to inconsistency due to the different knowledge 
backgrounds of the human taggers. This problem 
was solved by developing a procedure to 
automatically extract default senses from web 
pages using the Yahoo! search engine [Li et al 
2002]. Such a procedure has the advantage of 
enabling ?re-training? of default senses when 
necessary. If the web pages obtained through Yahoo! 
represent a typical North American ?view? of what 
default sense should be assigned to location names, 
it may be desirable to re-train the default senses of 
location names  using other views (e.g. an Asian 
view or African view) when the system needs to 
handle overseas documents that contain many 
foreign location names.  
In addition to the above automatic default sense 
extraction, we later found that a few simple default 
sense heuristics, when used at proper levels, can 
further enhance performance. This finding is 
incorporated in our modified approach described in 
Section 3 below.  
 
4 Modified Hybrid Approach 
To address the issues identified in Section 2, we 
adopt Prim?s algorithm, which traverses each node 
of a graph to choose the most promising senses. 
This algorithm has much less search space and 
shows the advantage of being able to reflect the 
number of location mentions and their distances in 
a document.  
The following is the description of our adapted 
Prim?s algorithm for the weight calculation.  
The weight of each sense of a node is calculated 
by considering the effect of linked senses of other 
location nodes based on a predefined weight table 
(Table 1) for the sense categories of co-occurring 
location names. For example, when a location name 
with a potential city sense co-occurs with a location 
name with a potential state/province sense and the 
city is in the state/province, the impact weight of 
the state/province name on the city name is fairly 
high, with the weight set to 3 as shown in the 3rd 
row of Table 1.   
Table 1. Impact weight of Sense2 on Sense1 
Sense1 Sense2 Condition  Weight 
City City in same state 2 
 City in same country 1 
 State in same state 3 
 Country in country without 
state (e.g. in Europe) 
4 
 
Let W(Si) be the calculated weight of a sense Sj of 
a location; weight(Sj->Si) means the weight of Si 
influenced by sense Sj; Num(Loci) is the number of 
location mentions; and ?/dist(Loci, Locj) is the 
measure of distance between two locations.  The final 
sense of a location is the one that has maximum 
weight. A location name may be mentioned a number 
of times in a document.  For each location name, we 
only count the location mention that has the maximum 
sense weight summation in equation (1) and 
eventually propagate the selected sense of this 
location mention to all its other mentions based on one 
sense per discourse principle.  Equation (2) refers to 
the sense with the maximum weight for Loci. 
 
(1)
( )

=
?
=
m
j
jijij
i
LocLocdistLocNumSSweight
SW
0
),(/*)(*)(
)(
?
 
(2) ))(()( maxarg j
j
i SWLocS =   
wj ??0  
Through experiments, we also found that it is 
beneficial to select default senses when candidate 
location senses in the discourse analysis turn out to be 
of the same weight. We included two kinds of default 
senses: heuristics-based default senses and the default 
senses extracted semi-automatically from the web 
using Yahoo. For the first category of default senses, 
we observe that if a name has a country sense and 
other senses, such as ?China? and ?Canada?, the 
country senses are dominant in most cases. The 
situation is the same for a name with province sense 
and for a name with country capital sense (e.g. London, 
Beijing). The updated algorithm for location 
normalization is as follows. 
 
Step 1. Look up the location gazetteer to 
associate candidate senses for each location 
NE; 
Step 2. If a location has sense of country, then 
select that sense as the default sense of that 
location (heuristics); 
Step 3. Call the pattern matching sub-module 
for local patterns like ?Williamsville, New 
York, USA?; 
Step 4. Apply the ?one sense per discourse? 
principle for each disambiguated location 
name to propagate the selected sense to its 
other mentions within a document; 
Step 5. Apply default sense heuristics for a 
location with province or capital senses; 
Step 6. Call Prim?s algorithm in the 
discourse sub-module to resolve the 
remaining ambiguities (Figure 5); 
Step 7. If the difference between the sense 
with the maximum weight and the sense 
with next largest weight is equal to or lower 
than a threshold, choose the default sense of 
that name from lexicon.  Otherwise, choose 
the sense with the maximum weight as 
output. 
Canada
{Kansas,
Kentucky,
Country}
Vancouver
{British Columbia
Washington
port in USA
Port in Canada}
New York 
{Prov in USA,
New York City,
?}
Toronto
(Ontorio,
New South Wales,
Illinois,
?}
Charlottetown
{Prov in USA,
New York City,
?}
Prince Edward Island
{Island in Canada,
Island in South Africa,
Province in Canada}
Quebec
(city in Quebec,
Quebec Prov,
Connecticut,
?}
 
 Figure 5:  Weight assigned to Sense Nodes 
 
5 Experiment and Benchmark 
With the information from local context, discourse 
context and the knowledge of default senses, the 
location normalization process is  efficient and 
precise.  
The testing documents were randomly selected 
from CNN news and from travel guide web pages. 
Table 2 shows the preliminary testing results using 
different configurations.  
As shown, local patterns (Column 4) alone 
contribute 12% to the overall performance while 
proper use of defaults senses and the heuristics 
(Column 5) can achieve close to 90%. In terms of 
discourse co-occurrence evidence, the new method 
using Prim?s algorithm (Column 7) is clearly better 
than the previous method using Kruskal?s 
algorithm (Column 6), with 13% enhancement 
(from 73.8% to 86.6%). But both methods cannot 
outperform default senses.  Finally, when using all 
three types of evidence, the new hybrid method 
presented in this paper shows significant 
performance enhancement (96% in Column 9) over 
the previous method (81.9% in Column 8), in 
addition to a satisfactory solution to the efficiency 
problem.  
Table 2. Experimental evaluation for location normalization 
File # of 
ambiguous 
location 
names 
# of 
mentions 
Pattern 
hits 
Def-
senses 
 
Kruskal 
Algo. 
only 
Prim 
Algo 
only  
Kruskal 
+Pattern 
+Def 
(previous) 
Prim 
+Pattern 
+Def 
(new) 
Cnn1 26 39 4 20 21 24 26  26 
Cnn2 12 20 5 11 7 10 11 11 
Cnn3 14 29 0 12 10 12 10 14 
Cnn4 8 14 2 8 4 4 4 8 
Cnn5 11 26 1 9 5 8 5 9 
Cnn6 19 35 6 16 11 16 13 18 
Cnn7 11 27 0 11 4 7 6 10 
Calif. 16 30 0 16 16 16 16 16 
Florida 19 28 0 19 19 19 18 19 
Texas 13 13 0 12 13 13 13 12 
Total 149 261 12% 89.9% 73.8% 86.6% 81.9% 96% 
 
We observed that if a file contains more 
concentrated locations, such as the state introductions 
in the travel guides for California, Florida and Texas, 
the accuracy is higher than the relatively short news 
articles from CNN.  
 
6 Conclusion and Future Work 
This paper presented an effective hybrid method of 
location normalization for information extraction with 
promising experimental results. In the future, we will 
integrate an expanded location gazetteer including 
names of landmarks, mountains and lakes such 
as Holland Tunnel (in New York, not in Holland) and 
Hoover Dam (in Arizona, not in Alabama), to enlarge 
the system coverage. Meanwhile, more extensive 
benchmarking is currently being planned in order to 
conduct a detailed analysis of different evidence 
sources and their interaction and contribution to 
system performance.  
References 
Cormen, Thomas H., Charles E. Leiserson, and 
Ronald L. Rivest. 1990. Introduction to Algorithm. 
The MIT Press, 504-505. 
Dagon, Ido and Alon Itai. 1994. Word Sense 
Disambiguation Using a Second Language 
Monolingual Corpus. Computational Linguistics, 
Vol.20, 563-596. 
Gale, W.A., K.W. Church, and D. Yarowsky. 1992. 
One Sense Per Discourse. Proceedings of the 4th 
DARPA Speech and Natural Language Workshop. 
233-237. 
Hirst, Graeme. 1987. Semantic Interpretation and the 
Resolution of Ambiguity. Cambridge University 
Press, Cambridge. 
Huifeng Li, Rohini K. Srihari, Cheng Niu, Wei Li. 
2002. Location Normalization for Information 
Extraction, COLING 2002, Taipei, Taiwan. 
Krupka, G.R. and K. Hausman.  1998.  IsoQuest 
Inc.: Description of the NetOwl (TM) Extractor 
System as Used for MUC-7.  Proceedings of 
MUC.  
McRoy, Susan W. 1992. Using Multiple 
Knowledge Sources for Word Sense 
Discrimination. Computational Linguistics, 
18(1): 1-30. 
Ng, Hwee Tou and Hian Beng Lee. 1996. 
Integrating Multiple Knowledge Sources to 
Disambiguate Word Sense: an Exemplar-based 
Approach. ACL 1996, 40-47, California. 
Srihari, Rohini, Cheng Niu, and Wei Li. 2000. A 
Hybrid Approach for Named Entity and 
Sub-Type Tagging. ANLP 2000, Seattle. 
Yarowsky, David. 1992. Word-sense 
Disambiguation Using Statistical Models of 
Roget?s Categories Trained on Large Corpora. 
COLING 1992, 454-460, Nantes, France. 
Yarowsky, David. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. 
ACL 1995, Cambridge, Massachusetts. 
Question Answering on a Case Insensitive Corpus 
 
Wei Li, Rohini Srihari, Cheng Niu, Xiaoge Li  
 
Cymfony Inc. 
600 Essjay Road 
Williamsville, NY 14221, USA 
{wei, rohini, cniu, xli}@cymfony.com
 
 
 
 
 
 
 
Abstract 
Most question answering (QA) systems 
rely on both keyword index and Named 
Entity (NE) tagging. The corpus from 
which the QA systems attempt to retrieve 
answers is usually mixed case text.  
However, there are numerous corpora that 
consist of case insensitive documents, e.g. 
speech recognition results.  This paper 
presents a successful approach to QA on a 
case insensitive corpus, whereby a 
preprocessing module is designed to 
restore the case-sensitive form. The 
document pool with the restored case then 
feeds the QA system, which remains 
unchanged. The case restoration 
preprocessing is implemented as a Hidden 
Markov Model trained on a large raw 
corpus of case sensitive documents. It is 
demonstrated that this approach leads to 
very limited degradation in QA 
benchmarking (2.8%), mainly due to the 
limited degradation in the underlying 
information extraction support. 
1 Introduction 
Natural language Question Answering (QA) is 
recognized as a capability with great potential. The 
NIST-sponsored Text Retrieval Conference 
(TREC) has been the driving force for developing 
this technology through its QA track since TREC-8 
[Voorhees 1999] [Voorhees 2000]. There has been 
significant progress and interest in QA research in 
recent years [Pasca & Harabagiu. 2001] [Voorhees 
2000]. 
In real-life QA applications, a system should be 
robust enough to handle diverse textual media 
degraded to different degrees. One of the 
challenges from degraded text is the treatment of 
case insensitive documents such as speech 
recognition results, broadcast transcripts, and the 
Foreign Broadcast Information Service (FBIS) 
sources. In the intelligence domain, the majority of 
archives consist of documents in all uppercase.   
The orthographic case information for written 
text is an important information source. In 
particular, the basic information extraction (IE) 
support for QA, namely Named Entity (NE) 
tagging, relies heavily on the case information for 
recognizing proper names. Almost all NE systems 
(e.g. [Bikel et al 1997], [Krupka & Hausman 
1998]) utilize case-related features. When this 
information is not available, if the system is not re-
trained or adapted, serious performance 
degradation will occur. In the case of the statistical 
NE tagger, without adaptation the system simply 
does not work. The degradation for proper name 
NE tagging is more than 70% based on our testing. 
The key issue here is how to minimize the 
performance degradation by adopting some 
strategy for the system adaptation.     
For search engines, the case information is often 
ignored in keyword indexing and retrieval for the 
sake of efficiency and robustness/recall. However, 
QA requires fine-grained text processing beyond 
keyword indexing since, instead of a list of 
documents or URLs, a list of candidate answers at 
phrase level or sentence level is expected to be 
returned in response to a query.  Typically QA is 
supported by Natural Language Processing (NLP) 
and IE [Chinchor & Marsh 1998] [Hovy et al 
2001] [Srihari & Li 2000]. Examples of using NLP 
and IE in Question Answering include shallow 
parsing [Kupiec 1993] [Srihari & Li 2000], deep 
parsing [Li et al 2002] [Litkowski 1999] 
[Voorhees 1999], and IE [Abney et al 2000] 
[Srihari & Li 2000]. Almost all state-of-the-art QA 
systems rely on NE in searching for candidate 
answers. 
For a system based on language models, a 
feature exclusion approach is used to re-train the 
models, excluding features related to the case 
information [Kubala et al 1998] [Miller et al 
2000] [Palmer et al 2000]. In particular, the 
DARPA HUB-4   program evaluates NE systems 
on speech recognizer output in SNOR (Standard 
Normalized Orthographic Representation) that is 
case insensitive and has no punctuations [Chincor 
et al 1998]. Research on case insensitive text has 
so far been restricted to NE and the feature 
exclusion approach [Chieu & Ng 2002] [Kubala et 
al. 1998] [Palmer et al 2000] [Robinson et al 
1999]. When we examine a system beyond the 
shallow processing of NE, the traditional feature 
exclusion approach may not be feasible. A 
sophisticated QA system usually involves several 
components with multiple modules, involving 
NLP/IE processing at various levels. Each 
processing module may involve some sort of case 
information as constraints. It is too costly and 
sometimes impossible to maintain two versions of 
a multi-module QA system for the purpose of 
handling two types of documents, with or without 
case.   
This paper presents a case restoration approach 
to this problem, as applied to QA. The focus is to 
study the feasibility of QA on a case insensitive 
corpus using the presented case restoration 
approach. For this purpose, we use an existing QA 
system as the baseline in experiments; we are not 
concerned with enhancing the QA system itself. A 
preprocessing module is designed to restore the 
case-sensitive form to feed to this QA system. The 
case restoration module is based on a Hidden 
Markov Model (HMM) trained on a large raw 
corpus of case sensitive documents, which are 
drawn from a given domain with no need for 
human annotation. With the plug-in of this 
preprocessing module, the entire QA system with 
its underlying NLP/IE components needs no 
change or adaptation in handling the case 
insensitive corpus. Using the TREC corpus with 
the case information artificially removed, this 
approach has been benchmarked with very good 
results, leading to only 2.8% degradation in QA 
performance. In the literature, this is the first time 
a QA system is applied to case insensitive corpora.  
Although the artificially-made case insensitive 
corpus is an easier case than some real life corpora 
from speech recognition, the insight and 
techniques gained in this research are helpful in 
further exploring solutions of spoken language 
QA. In addition, by using the TREC corpus and the 
TREC benchmarking standards, the QA 
degradation benchmarking is easy to interpret and 
to compare with other QA systems in the 
community.    
The case restoration approach has the following 
advantages: (i) the training corpus is almost 
limitless, resulting in a high performance model, 
with no knowledge bottleneck as faced by many 
supervised learning scenarios, (ii) the case 
restoration approach is applicable no matter 
whether the core system is statistical model, a 
hand-crafted rule system or a hybrid, (iii) when the 
core system consists of multiple modules, as is the 
case for the QA system used in the experiments 
that is based on multi-level NLP/IE, the case 
restoration approach relieves the burden of having 
to re-train or adapt each module in respect of case 
insensitive input, and (iv) the  restoration approach 
reduces the system complexity: the burden of 
handling degraded text (case in this case) is 
reduced to a preprocessing module while all other 
components need no changes. 
The remaining text is structured as follows. 
Section 2 presents the QA system. Section 3 
describes the language model for case restoration. 
Section 4  benchmarks the IE engine and Section 5 
benchmarks the IE-supported QA application. In 
both benchmarking sections, we compare the 
performance degradation from case sensitive input 
to case insensitive input. Section 5 is the 
Conclusion. 
2 Question Answering Based on IE 
We use a QA system supported by increasingly 
sophisticated levels of IE [Srihari & Li 2000] [Li et 
al. 2002]. Figure 1 presents the underlying IE 
engine InfoXtract [Srihari et al 2003] that forms 
the basis for the QA system. The major 
information objects extracted by InfoXtract include 
NEs,1 Correlated Entity (CE) relationships (e.g. 
Affiliation, Position etc.), Subject-Verb-Object 
(SVO) triples, entity profiles, and general or 
predefined events. These information objects 
capture the key content of the processed text, 
preparing a foundation for answering factoid 
questions.  
Document Processor
Knowledge Resources
Lexicon
Resources
Grammars
Process
Manager
Tokenlist
Legend
Output
Manager
Source
Document
Linguistic Processor(s)Tokenizer
Tokenlist
Lexicon Lookup
Pragmatic
Filtering
 POS Tagging
Named Entity
Detection
Shallow
Parsing
Deep
Parsing
Relationship
Detection
Document
pool
NE
CE
EP
SVO
Time
Normalization
Alias/Coreference
Linking
Profile/Event
Linking
Profile/Event
Merge
Abbreviations
NE = Named Entity
CE = Correlated Entity
EP = Entity Profile
SVO=Subject-Verb-Object
GE = General Event
PE = Predefined Event
Grammar Module
Procedure or
Statistical Model
Hybrid
Module
GEStatisticalModels
Location
Normalizationli ti
PE
IE
Index
 
Figure 1: System Architecture of InfoXtract 
Figure 2 shows the architecture of the QA 
system. This system consists of three components: 
(i) Question Processing, (ii) Text Processing, and 
(iii) Answer Ranking. In text processing, the case 
insensitive corpus is first pre-processed for case 
restoration before being parsed by InfoXtract. In 
addition, keyword indexing on the corpus is 
required. For question processing, a special module 
for Asking Point Identification is called for. 
Linking the two processing components is the 
Answer Ranking component that consists of two 
modules: Snippet Retrieval and Feature Ranking.2 
                                                 
1 It is worth noting that there are two types of NE: (i) proper names 
NeName (including NePerson, NeOrganization, NeLocation, etc.) and 
(ii) non-name NEs (NeItem) such as time NE (NeTimex) and 
numerical NE (NeNumex). Close to 40% of the NE questions target 
non-name NEs. Proper name NEs are more subject to the case effect 
because recognizing a name in the running text often requires case 
information. Non-name NEs generally appear in predictable patterns. 
Pattern matching rules that perform case-insensitive matching are 
most effective in capturing them. 
2 There is a third, optional module Answer Point Identification in our 
QA system [10], which relies on deep parsing for generating phrase-
Answer Ranking relies on access to information 
from both the Keyword Index as well as the IE 
Index.  
 
IE Index
Case 
Insensitive
Corpus
Multi-level 
TemplateQuestion
Keyword 
IndexKeyword indexing
InfoXtract Asking PointIdentification
Feature RankingSnippet Retrieval Snippet-levelAnswer
Question Processing
Text Processing
Answer Ranking
Case Restoration InfoXtract
 
Figure 2:  Architecture of QA Based on NLP/IE 
 
Snippet Retrieval 
 
Snippet retrieval generates the top n (we chose 
200) most relevant sentence-level candidate 
answer snippets based on the question processing 
results. 
We use two types of evidence for snippet 
retrieval:  (i) keyword occurrence statistics at 
snippet level (with stop words removed), and (ii) 
the IE results, including NE Asking Points, Asking 
Point CE Link, head word of a phrase, etc. 
If the Question Processing component detects an 
Asking Point CE Link, the system first attempts to 
retrieve snippets that contain the corresponding CE 
relationship.  If it fails, it backs off to the 
corresponding NE Asking Point. This serves as a 
filter in the sense that only the snippets that contain 
at least one NE that matches the NE Asking Point 
are extracted.  For questions that do not contain NE 
Asking Points, the system backs off to keyword-
based snippet retrieval.   
A synonym lexicon is also constructed for query 
expansion to help snippet retrieval.  This includes 
irregular verbs (go/went/gone, etc.), verb-noun 
conversion (develop/development; satisfy/ 
satisfaction; etc.), and a human-modified 
                                                                             
level answers from snippet-level answers. This module was not used 
in the experiments reported in this paper. 
conservative synonym list (e.g. adjust/adapt; 
adjudicate/judge; etc.). 
Factors that contribute to relevancy weighting in 
snippet retrieval include giving more weight to the 
head words of phrases (e.g. ?disaster? in the noun 
phrase ?the costliest disaster?), more weight to 
words that are linked with question words (e.g. 
?calories? in ?How many calories?? and 
?American? in ?Who was the first American in 
space?), and discounting the weight for synonym 
matching. 
 
Feature Ranking 
 
The purpose of Feature Ranking is to re-rank the 
candidate snippets based on a list of ranking 
features. 
Given a list of top n snippets retrieved in the 
previous stage, the Feature Ranking module uses a 
set of re-ranking features to fine-tune relevancy 
measures of the initial list of snippets in order to 
generate the final top five answer strings that are 
required as output. Figure 3 gives the ranking 
model for the Feature Ranking module.   
 
RankingFeature 1
SnippetList QuestionTokenList
Weight - Wq1
RankingFeature 2
SnippetList QuestionTokenList
Weight - Wq2
RankingFeature mWeight - Wqm
SnippetList QuestionTokenList
Ranked List of Answers  
Figure 3: Pipeline for Ranking Features 
For a given question, Q, let {S1, S2,?,Sn} be the 
set of candidate answer snippets. Let {R1, R2, ?, 
Rk} be the ranking features. For a snippet Sj, let the 
ranking feature Ri assign a relevancy of rij 
quantifying the snippet?s relevance to the question. 
The ranking model is given by  
  

=
=
k
i
ijilj rwSQR
1
),(  
where l represents the question type of Q and wil 
gives the weight assigned to the ranking feature. 
Weights wil vary based on question type. 
We use both traditional IR ranking features such 
as Keyword Proximity and Inverse Document 
Frequency (IDF) as well as the ranking features 
supported by NLP/IE, listed below:  
? NE Asking Point  
? Asking Point CE Link 
? Headword Match for Basic Phrases 
? Phrase-Internal Word Order  
? Alias (e.g. ?IBM? and ?International 
Business Machine?) 
? NE Hierarchical Match (e.g. Company vs. 
Organization)  
? Structure-Based Matching (SVO Links, 
Head-Modifier Link, etc.)  
3 Case Restoration 
This section presents the case restoration approach 
[Niu et al 2003] that supports QA on case 
insensitive corpus. The flowchart for using Case 
Restoration as a plug-in preprocessing module to 
IE is shown in Figure 4. 
 
 
Document Input
Tokenization
Case Detection
InfoXtract 
Case 
Restoration 
Module 
No 
Yes 
Case Sensitive? 
Output
 
Figure 4: Case Restoration for IE 
The incoming documents first go through 
tokenization. In this process, the case information 
is recorded as features for each token. This token-
based case information provides basic evidence for 
the optional procedure called Case Detection to 
decide whether the Case Restoration module needs 
to be called.  
A simple bi-gram Hidden Markov Model [Bikel 
et al 1999] is selected as the choice of language 
model for this task. Currently, the system is based 
on a bi-gram model trained on a normal, case 
sensitive raw corpus in the chosen domain. 
Three orthographic tags are defined in this 
model: (i) initial uppercase followed by at least one 
lowercase, (ii) all lowercase, and (iii) all 
uppercase.  
To handle words with low frequency, each word 
is associated with one of five features: (i) 
PunctuationMark (e.g. &, ?, !?), (ii) LetterDot 
(e.g. A., J.P., U.S.A.,?), (iii) Number (e.g. 
102,?), (iv) Letters (e.g. GOOD, MICROSOFT, 
IBM, ?), or (v) Other.  
The HMM is formulated as follows. Given a 
word sequence nn00 fwfw W =  (where 
jf denotes a single token feature which are defined 
as above), the goal for the case restoration task is 
to find the optimal tag sequence n210 tttt T = , 
which maximizes the conditional probability 
W)| Pr(T  [Bikel et al 1999]. By Bayesian equality, 
this is equivalent to maximizing the joint 
probability T)Pr(W, . This joint probability can be 
computed by a bi-gram HMM as 
?
?
=
i
1i1-i1-iiii )t,f,w|t,f,wPr(T)Pr(W, . The 
back-off model is as follows, 
 
)t,w|)Pr(tt,t|f,wPr()-(1
)t,f,w|t,f,w(P
)t,f,w|t,f,wPr(
1i1ii1iiii1
1i1-i1-iiii01
1i1-i1-iiii
???
?
?
+
=
?
?  
)t|f,wPr()-(1)t,t|f,w(P
)t,t|f,wPr(
iii21iiii02
1iiii
?? +=
?
?  
)w|Pr(t)-(1)t,w|(tP
)t,w|Pr(t
1-ii31i1-ii03
1i1-ii
?? +=
?
?  
)t|(f)Pt|(wPr)-(1)t|f,w(P
)t|f,wPr(
ii0ii4iii04
iii
?? +=
 
)t(P)-(1)w|(tP)w|Pr(t i051-ii051-ii ?? +=  
V
1)-(1)t|(wP)t|Pr(w 6ii06ii ?? +=  
where V denotes the size of the vocabulary, the 
back-off coefficients ??s are determined using the 
Witten-Bell smoothing algorithm, and the 
quantities 
)t,f,w|t,f,w(P 1i1i1iiii0 ??? , )t,t|f,w(P 1iiii0 ? , 
)t,w|(tP 1i1-ii0 ? , )t|f,w(P iii0 , )t|(fP ii0 , 
)w|(tP 1-ii0 , )(tP i0 , and )t|(wP ii0  are computed by 
the maximum likelihood estimation. 
A separate HMM is trained for bigrams 
involving unknown words. The training corpus is 
separated into two parts, the words occurring in 
Part I but not in Part II and the words occurring in 
Part II but not in Part I are all replaced by a special 
symbol #Unknown#. Then an HMM for unknown 
words is trained on this newly marked corpus. In 
the stage of tagging, the unknown word model is 
used in case a word beyond the vocabulary occurs. 
4 IE Engine Benchmarking 
A series of benchmarks have been conducted in 
evaluating the approach presented in this paper. 
They indicate that this is a simple but very 
effective method to solve the problem of handling 
case insensitive input for NLP, IE and QA.  
 
Case Restoration 
 
A raw corpus of 7.6 million words in mixed case 
drawn from the general news domain is used in 
training case restoration. A separate testing corpus 
of 0.88 million words drawn from the same 
domain is used for benchmarking. Table 1 gives 
the case restoration performance benchmarks.  The 
overall F-measure is 98% (P for Precision, R for 
Recall and F for F-measure).  
Table 1: Case Restoration Performance 
P R F
0.96 1 0.98
0.97 0.99 0.98
0.93 0.84 0.88
Initial-Upper Case 0.87 0.84 0.85
All-Upper Case 0.77 0.6 0.67
Overall
Lower Case
Non-Lower Case
 
The score that is most important for IE is the  
F-measure of recognizing non-lowercase word. We 
found that the majority of errors involve missing 
the first word in a sentence due to the lack of a 
powerful sentence final punctuation detection 
module in the case restoration stage. But it is found 
that such ?errors? have almost no negative effect on 
the following IE tasks.3    
There is no doubt that the lack of case 
information from the input text will impact the 
NLP/IE/QA performance. The goal of the case 
restoration module is to minimize this impact. A 
series of degradation tests have been run to 
measure the impact. 
 
Degradation Tests on IE and Parsing 
 
Since IE is the foundation for our QA system, the 
IE degradation due to the case insensitive input 
directly affects the QA performance.  
The IE degradation benchmarking is designed as 
follows. We start with a testing corpus drawn from 
normal case sensitive text. We then feed the corpus 
into the IE engine for benchmarking. This is 
normal benchmarking for case sensitive text input 
as a baseline. After that, we artificially remove the 
case information by transforming the corpus into a 
corpus in all uppercase. The case restoration 
module is then plugged in to restore the case 
before feeding the IE engine. By comparing 
benchmarking using case restoration with baseline 
benchmarking, we can calculate the level of 
performance degradation from the baseline in 
handling case insensitive input. 
For NE, an annotated testing corpus of 177,000 
words is used for benchmarking (Table 3), using 
an automatic scorer following Message 
Understanding Conference (MUC) NE standards. 
Table 2: NE Degradation Benchmarking 
Type P R F 
NE on case sensitive input 89.1% 89.7% 89.4%
NE on case insensitive input using 
case restoration  86.8% 87.9% 87.3%
Degradation  2.3% 1.8% 2.1%
 
The overall F-measure for NE degradation, due 
to the loss of case information in the incoming 
corpus, is 2.1%. We have also implemented the 
traditional NE-retraining approach proposed by 
[Kubala et al 1998] [Miller et al 2000] [Palmer et 
al. 2000] and the re-trained NE model leads to 
                                                 
3 In fact, positive effects are observed in some cases. The normal 
English orthographic rule that the first word be capitalized can 
confuse the NE learning system due to the lack of the usual 
orthographic distinction between a candidate proper name and a 
common word.       
6.3% degradation in the NE F-measure, a drop of 
more than four percentage points when compared 
with the case restoration two-step approach. Since 
this comparison between two approaches is based 
on the same testing corpus using the same system, 
the conclusion can be derived that the case 
restoration approach is clearly better than the 
retraining approach for NE.   
Beyond NE, some fundamental InfoXtract 
support  for QA comes from the CE relationships 
and the SVO parsing results. We benchmarked 
their degradation as follows.  
From a processed corpus drawn from the news 
domain, we randomly picked 250 SVO structural 
links and 60 AFFILIATION and POSITION 
relationships for manual checking (Table 3, COR 
for Correct, INC for Incorrect, SPU for Spurious,  
MIS for Missing, and DEG for Degradation). 
Surprisingly, there is almost no statistically 
significant difference in the SVO performance. 
The degradation due to the case restoration was 
only 0.07%. This indicates that parsing is less 
subject to the case factor to a degree that the 
performance differences between a normal case 
sensitive input and a case restored input are not 
obviously detectable. 
Table 3: SVO/CE Degradation Benchmarking 
 SVO CE 
 Baseline
Case 
Restored Baseline 
Case 
Restored 
COR 196 195 48 43 
INC 13 12 0 1 
SPU 10 10 2 2 
MIS 31 33 
DEG 
 
 
 10 14 
DEG 
 
 
 
P 89.50% 89.86% -0.36% 96.0% 93.5% 2.5% 
R 81.67% 81.25% 0.42% 82.8% 74.1% 8.7% 
F 85.41% 85.34% 0.07% 88.9% 82.7% 6.2% 
 
The degradation for CE is about 6%. 
Considering there is absolutely no adaptation of 
the CE module, this degradation is reasonable. 
5 QA Degradation Benchmarking 
The QA experiments were conducted following the 
TREC-8 QA standards in the category of 250-byte 
answer strings. In addition to the TREC-8 
benchmarking standards Mean Reciprocal Rank 
(MRR), we also benchmarked precision for the top 
answer string (Table 4). 
Table 4: QA Degradation Benchmarking-1 
Type Top 1 Precision MRR 
QA on case sensitive corpus  130/198=65.7% 73.9%
QA on case insensitive corpus 124/198=62.6% 71.1%
Degradation  3.1% 2.8%
 
Comparing QA benchmarks with benchmarks 
for the underlying IE engine shows that the limited 
QA degradation is in proportion with the limited 
degradation in NE, CE and SVO. The following 
examples illustrate the chain effect: case 
restoration errors  NE/CE/SVO errors  QA 
errors. 
 
Q137: ?Who is the mayor of Marbella??  
 
This is a CE question, the decoded CE asking 
relationship is CeHead for the location entity 
?Marbella?. In QA on the original case sensitive 
corpus, the top answer string has a corresponding 
CeHead relationship extracted as shown below.  
Input: Some may want to view the results of the 
much-publicised activities of the mayor of 
Marbella, Jesus Gil y Gil, in cleaning up the 
town 
 [NE tagging] 
  Some may want to view the results of the 
much-publicised activities of the mayor of 
<NeCity>Marbella</NeCity> , 
<NeMan>Jesus Gil y Gil</NeMan>, in 
cleaning up the town 
 [CE extraction] 
CeHead: Marbella  Jesus Gil y Gil 
 
In contrast, the case insensitive processing is 
shown below: 
 
Input: SOME MAY WANT TO VIEW THE 
RESULTS OF THE MUCH-PUBLICISED 
ACTIVITIES OF THE MAYOR OF 
MARBELLA, JESUS GIL Y GIL, IN 
CLEANING UP THE TOWN 
 [case restoration] 
 some may want to view the results of the 
much-publicised activities of the mayor of 
marbella , Jesus Gil y Gil, in cleaning up the 
town 
 [NE tagging] 
 some may want to view the results of the 
much-publicised activities of the mayor of 
marbella , <NeMan>Jesus Gil y 
Gil</NeMan> , in cleaning up the town 
The CE module failed to extract the relationship 
for MARBELLA because this relationship is 
defined for the entity type NeOrganization or 
NeLocation which is absent due to the failed case 
restoration for ?MARBELLA?.  The next example 
shows an NE error leading to a problem in QA.  
 
Q119: ?What Nobel laureate was expelled from 
the Philippines before the conference on East 
Timor??  
 
In question processing, the NE Asking Point is 
identified as NePerson. Because Mairead Maguire  
was successfully tagged as NeWoman, the QA 
system got the correct answer string in the 
following snippet: Immigration officials at the 
Manila airport on Saturday expelled Irish Nobel 
peace prize winner Mairead Maguire. However, 
the case insensitive processing fails to tag any 
NePerson in this snippet. As a result the system 
misses this answer string. The process is illustrated 
below.  
 
Input: IMMIGRATION OFFICIALS AT THE 
MANILA AIRPORT ON SATURDAY 
EXPELLED IRISH NOBEL PEACE PRIZE 
WINNER MAIREAD MAGUIRE 
 [case restoration] 
immigration officials at the Manila airport 
on Saturday expelled Irish Nobel Peace Prize 
Winner Mairead Maguire  
 [NE tagging] 
immigration officials at the 
<NeCity>Manila</NeCity> airport on 
<NeDay>Saturday</NeDay> expelled 
<NeProduct>Irish Nobel Peace Prize Winner 
Mairead Maguire </NeProduct> 
 
As shown, errors in case restoration cause 
mistakes in the NE grouping and tagging: Irish 
Nobel Peace Prize Winner Mairead Maguire  is 
wrongly tagged as NeProduct. 
We also found one interesting case where case 
restoration actually leads to QA performance 
enhancement over the original case sensitive 
processing. A correct answer snippet is promoted 
from the 3rd candidate to the top in answering 
Q191 ?Where was Harry Truman born??. This 
process is shown below. 
Input: HARRY TRUMAN (33RD PRESIDENT): 
BORN MAY 8, 1884, IN LAMAR, MO.  
 [case restoration] 
Harry Truman ( 33rd President ) : born May 
8 , 1884  , in Lamar , MO .  
 [NE tagging] 
 <NeMan>Harry Truman</NeMan> ( 
<NeOrdinal>33rd</NeOrdinal> President ) : 
born <NeDay>May 8 , 1884</NeDay> , in 
<NeCity>Lamar , MO</NeCity> .  
 
As shown, LAMAR, MO gets correctly tagged as 
NeCity after case restoration. But LAMAR is mis-
tagged as NeOrg in the original case sensitive 
processing. The original case sensitive snippet is 
Harry Truman (33rd President): Born May 8, 
1884, in Lamar, Mo.  In our NE system, there is 
such a learned pattern as follows: 
 
X , TwoLetterUpperCase  NeCity.   
 
This rule fails to apply to the original text because 
the US state abbreviation appears in a less 
frequently seen format Mo instead of MO. 
However, the restoration HMM assigns all 
uppercase to ?MO? since this is the most frequently 
seen orthography for this token. This difference of 
the restored case from the original case enables the 
NE tagger to tag Lamar, MO as ?NeCity? which 
meets the NE Asking Point constraint 
?NeLocation?. 
 
QA and Case Insensitive Question 
 
We also conducted a test on case insensitive 
questions in addition to case insensitive corpus by 
calling the same case restoration module.  
Table 5: QA Degradation Benchmarking-2 
Type Top 1 Precision MRR
QA on case sensitive corpus  130/198=65.7% 73.9%
QA on case insensitive corpus,  
with case insensitive question 111/198=56.1% 64.4%
Degradation  9.6% 9.5%
 
This research is useful because, when interfacing 
a speech recognizer to a QA system to accept 
spoken questions, the case information is not 
available in the incoming question.4 We want to 
                                                 
4 In addition to missing the case information, there are other aspects of 
spoken questions that require treatment, e.g., lack of punctuation 
marks, spelling mistakes, repetitions. Whether the restoration 
approach is effective calls for more research.  
know how the same case restoration technique 
applies to question processing and gauge the 
degradation effect on the QA performance  
(Table 5). 
We notice that the question processor missed 
two originally detected NE Asking Points and one 
Asking Point CE Link. There are a number of other 
errors due to incorrectly restored case, including 
non-asking-point NEs in the question and grouping 
errors in shallow parsing as shown below for Q26 : 
?What is the name of the ?female? counterpart to 
El Nino, which results in cooling temperatures and 
very dry weather?? (Notation: NP for Noun Phrase, 
VG for Verb Group, PP for Prepositional Phrase 
and AP for Adjective Phrase).  
 
Input: WHAT IS THE NAME OF THE 
"FEMALE" COUNTERPART TO EL 
NINO ? ?  
 [case restoration] 
What is the name of the "Female" 
counterpart to El Nino, ??  
 [question shallow parsing] 
 NP[What] VG[is] NP[the name] PP[of the] " 
AP[Female] " NP[counterpart] PP[to El 
Nino] , ? ?  
 
In the original mixed-case question, after parsing, 
we get the following basic phrase grouping:  
 
NP[What] VG[is] NP[the name] PP[of the " female 
" counterpart] PP[to El Nino] , ? ?  
 
There is only one difference between the case-
restored question and the original mixed-case 
question, i.e. Female vs. female. This difference 
causes the shallow parsing grouping error for the 
PP of the "female" counterpart. This error affects 
the weights of the ranking features Headword 
Matching and Phrase-internal Word Order. As a 
result, the following originally correctly identified 
answer snippet was dropped: the greenhouse effect 
and El Nino -- as well as its "female" counterpart, 
La Nina -- have had a profound effect on weather 
nationwide. 
As question processing results are the starting 
point and basis for snippet retrieval and feature 
ranking, an error in question processing seems to 
lead to greater degradation, as seen in almost 10% 
drop compared with about 3% drop in the case 
when only the corpus is case insensitive.  
A related explanation for this degradation 
contrast is as follows. Due to the information 
redundancy in a large corpus, processing errors in 
some potential answer strings in the corpus can be 
compensated for by correctly processed equivalent 
answer strings. This is due to the fact that the same 
answer may be expressed in numerous ways in the 
corpus.  Some of those ways may be less subject to 
the case effect than others. Question processing 
errors are fatal in the sense that there is no 
information redundancy for its compensation. 
Once it is wrong, it directs the search for answer 
strings in the wrong direction. Since questions 
constitute a subset of the natural language 
phenomena with their own characteristics, case 
restoration needs to adapt to this subset for optimal 
performance, e.g. by including more questions in 
the case restoration training corpus. 
6 Conclusion 
An effective approach to perform QA on case 
insensitive corpus is presented with very little 
degradation (2.8%). This approach uses a high 
performance case restoration module based on 
HMM as a preprocessor for the NLP/IE processing 
of the corpus. There is no need for any changes on 
the QA system and the underlying IE engine which 
were originally designed for handling normal, case 
sensitive corpora. It is observed that the limited 
QA degradation is due to the limited IE 
degradation. 
An observation from the research of handling 
case insensitive questions is that question 
processing degradation has more serious 
consequence affecting the QA performance. The 
current case restoration training corpus is drawn 
from the general news articles which rarely contain 
questions. As a future effort, we plan to focus on 
enhancing the case restoration performance by 
including as many mixed-case questions as 
possible into the training corpus for case 
restoration. 
Acknowledgment 
This work was partly supported by a grant from the 
Air Force Research Laboratory?s Information 
Directorate (AFRL/IF), Rome, NY, under contract 
F30602-03-C-0044. The authors wish to thank 
Carrie Pine and Sharon Walter of AFRL for 
supporting and reviewing this work. 
References 
Abney, S., Collins, M and Singhal. 2000. A. 
Answer Extraction. Proceedings of ANLP-
2000, Seattle. 
Bikel, D.M. et al 1997. Nymble: a High-
Performance Learning Name-finder.  
Proceedings of the Fifth Conference on ANLP, 
Morgan Kaufmann Publishers,  194-201. 
Bikel, D.M., R. Schwartz, and R.M. Weischedel. 
1999. An Algorithm that Learns What?s in a 
Name.  Machine Learning, Vol. 1,3, 1999, 
211-231. 
Chieu, H.L. and H.T. Ng. 2002. Teaching a 
Weaker Classifier: Named Entity Recognition 
on Upper Case Text. Proceedings of ACL-
2002, Philadelphia. 
Chinchor N. and E. Marsh. 1998. MUC-7 
Information Extraction Task Definition 
(version 5.1), Proceedings of MUC-7. 
Hovy, E.H., U. Hermjakob, and Chin-Yew Lin. 
2001. The Use of External Knowledge of 
Factoid QA. Proceedings of TREC-10, 2001, 
Gaithersburg, MD, U.S.A.. 
Krupka, G.R. and K. Hausman. 1998. IsoQuest 
Inc.: Description of the NetOwl (TM) 
Extractor System as Used for MUC-7, 
Proceedings of MUC-7. 
Kubala, F., R. Schwartz, R. Stone and R. 
Weischedel. 1998. Named Entity Extraction 
from Speech.  Proceedings of DARPA 
Broadcast News Transcription and 
Understanding Workshop. 
Kupiec J. 1993. MURAX: A Robust Linguistic 
Approach For Question Answering Using An 
On-Line Encyclopaedia. Proceedings of SIGIR 
Pittsburgh, PA.  
Li, W, R. Srihari, X. Li, M. Srikanth, X. Zhang and 
C. Niu. 2002. Extracting Exact Answers to 
Questions Based on Structural Links. 
Proceedings of Multilingual Summarization 
and Question Answering (COLING-2002 
Workshop), Taipei, Taiwan. 
Litkowski, K. C. 1999. Question-Answering Using 
Semantic Relation Triples. Proceedings of 
TREC-8, Gaithersburg, MD. 
Miller, D., S. Boisen, R. Schwartz, R. Stone, and 
R. Weischedel. 2000. Named Entity Extraction 
from Noisy Input: Speech and OCR. 
Proceedings of ANLP 2000, Seattle. 
Niu, C., W. Li, J. Ding and R. Srihari. 2003.  
Orthographic Case Restoration Using 
Supervised Learning Without Manual 
Annotation.  Proceedings of the 16th 
International FLAIRS Conference 2003, 
Florida  
Chincor, N., P. Robinson and E. Brown. 1998. 
HUB-4 Named Entity Task Definition Version 
4.8.   (www.nist.gov/speech/tests/bnr/hub4_98/ 
hub4_98.htm) 
Palmer, D., M. Ostendorf and J.D. Burger. 2000. 
Robust Information Extraction from 
Automatically Generated Speech 
Transcriptions.  Speech Communications, Vol. 
32, 2000, 95-109. 
Pasca, M. and S.M. Harabagiu. 2001. High 
Performance Question/Answering. 
Proceedings of SIGIR 2001, 366-374. 
Robinson, P., E. Brown, J. Burger, N. Chinchor, A. 
Douthat, L. Ferro, and L. Hirschman. 1999. 
Overview: Information Extraction from 
Broadcast News.  Proceedings of The DARPA 
Broadcast News Workshop Herndon, Virginia. 
Srihari, R and W. Li. 2000. A Question Answering 
System Supported by Information Extraction.  
Proceedings of ANLP 2000, Seattle.   
Srihari, R., W. Li, C. Niu and T. Cornell. 2003. 
InfoXtract: A Customizable Intermediate 
Level Information Extraction Engine. HLT-
NAACL03 Workshop on The Software 
Engineering and Architecture of Language 
Technology Systems (SEALTS). Edmonton, 
Canada  
Voorhees, E. 1999. The TREC-8 Question 
Answering Track Report. Proceedings of 
TREC-8. Gaithersburg, MD.  
Voorhees, E. 2000. Overview of the TREC-9 
Question Answering Track. Proceedings of 
TREC-9. Gaithersburg, MD.  
Context Clustering for Word Sense Disambiguation Based on  
Modeling Pairwise Context Similarities 
Cheng Niu, Wei Li, Rohini K. Srihari, Huifeng Li, Laurie Crist 
Cymfony Inc. 
600 Essjay Road, Williamsville, NY 14221. USA. 
{cniu, wei, rohini, hli, lcrist}@cymfony.com 
 
Abstract 
Traditionally, word sense disambiguation 
(WSD) involves a different context model for 
each individual word. This paper presents a 
new approach to WSD using weakly 
supervised learning. Statistical models are not 
trained for the contexts of each individual 
word, but for the similarities between context 
pairs at category level. The insight is that the 
correlation regularity between the sense 
distinction and the context distinction can be 
captured at category level, independent of 
individual words. This approach only requires 
a limited amount of existing annotated training 
corpus in order to disambiguate the entire 
vocabulary. A context clustering scheme is 
developed within the Bayesian framework. A 
maximum entropy model is then trained to 
represent the generative probability 
distribution of context similarities based on 
heterogeneous features, including trigger 
words and parsing structures. Statistical 
annealing is applied to derive the final context 
clusters by globally fitting the pairwise 
context similarity distribution. Benchmarking 
shows that this new approach significantly 
outperforms the existing WSD systems in the 
unsupervised category, and rivals supervised 
WSD systems. 
1 Introduction 
Word Sense Disambiguation (WSD) is one of the 
central problems in Natural Language Processing. 
The difficulty of this task lies in the fact that 
context features and the corresponding statistical 
distribution are different for each individual word. 
Traditionally, WSD involves modeling the 
contexts for each word.  [Gale et al 1992] uses the 
Na?ve Bayes method for context modeling which 
requires a manually truthed corpus for each 
ambiguous word. This causes a serious Knowledge 
Bottleneck. The situation is worse when 
considering the domain dependency of word 
senses. To avoid the Knowledge Bottleneck, 
unsupervised or weakly supervised learning 
approaches have been proposed. These include the 
bootstrapping approach [Yarowsky 1995] and the 
context clustering approach [Schutze 1998]. 
Although the above unsupervised or weakly 
supervised learning approaches are less subject to 
the Knowledge Bottleneck, some weakness exists: 
i) for each individual keyword, the sense number 
has to be provided and in the bootstrapping case, 
seeds for each sense are also required; ii) the 
modeling usually assumes some form of evidence 
independency, e.g. the vector space model used in 
[Schutze 1998] and [Niu et al 2003]: this limits the 
performance and its potential enhancement; iii) 
most WSD systems either use selectional 
restriction in parsing relations, and/or  trigger 
words which co-occur within a window size of the 
ambiguous word. We previously at-tempted 
combining both types of evidence but only 
achieved limited improvement due to the lack of a 
proper modeling of information over-lapping [Niu 
et al 2003]. 
This paper presents a new algorithm that 
addresses these problems. A novel context 
clustering scheme based on modeling the 
similarities between pairwise contexts at category 
level is presented in the Bayesian framework. A 
generative maximum entropy model is then trained 
to represent the generative probability distribution 
of pairwise context similarities based on 
heterogeneous features that cover both co-
occurring words and parsing structures. Statistical 
annealing is used to derive the final context 
clusters by globally fitting the pairwise context 
similarities. 
This new algorithm only requires a limited 
amount of existing annotated corpus to train the 
generative maximum entropy model for the entire 
vocabulary. This capability is based on the 
observation that a system does not necessarily 
require training data for word A in order to 
disambiguate A.  The insight is that the correlation 
regularity between the sense distinction and the 
context distinction can be captured at category 
level, independent of individual words. 
In what follows, Section 2 formulates WSD as a 
context clustering task based on the pairwise 
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
context similarity model. The context clustering 
algorithm is described in Sections 3 and 4, 
corresponding to the two key aspects of the 
algorithm, i.e. the generative maximum entropy 
modeling and the annealing-based optimization. 
Section 5 describes benchmarks and conclusion. 
2 Task Definition and Algorithm Design 
Given n  mentions of a key word, we first 
introduce the following symbols. iC  refers to the 
i -th context.  iS  refers to the sense of the i -th 
context. jiCS ,  refers to the context similarity 
between the i -th context and the j -th context, 
which is a subset of the predefined context 
similarity features. ?f  refers to the ? -th 
predefined context similarity feature. So jiCS ,  
takes the form of { }?f . 
The WSD task is defined as the hard clustering 
of multiple contexts of the key word. Its final 
solution is represented as { }MK ,  where K refers 
to the number of distinct senses, and M represents 
the many-to-one mapping (from contexts to a 
cluster) such that ( ) K]. [1,j n],[1,i j,iM ??=  
For any given context pair, a set of context 
similarity features are defined. With n mentions of 
the same key word, 2
)1( ?nn  context similarities 
[ ] [ )( )ijniCS ji ,1,,1 , ??  are computed. The WSD task 
is formulated as searching for { }MK ,  which 
maximizes the following conditional probability: 
{ }( ) [ ] [ )( )ijniCSMK ji ,1,,1       }{,Pr , ??  
Based on Bayesian Equity, this is equivalent to 
maximizing the joint probability in Eq. (1), which 
contains a prior probability distribution of WSD, 
{ }( )MK ,Pr .  
 
{ }( ) [ ] [ )( )
{ }( ) { }( )
{ }( ) { }( )MKMKCS
MKMKCS
ijniCSMK
ij
Ni
ji
ji
ji
,Pr,Pr
,Pr,}{Pr
,1,,1       }{,,Pr
1,1
,1
,
,
,
?
?=
=
=
=
??
 (1) 
 
Because there is no prior knowledge available 
about what solution is preferred, it is reasonable to 
take an equal distribution as the prior probability 
distribution. So WSD is equivalent to searching for 
{ }MK ,  which maximizes Expression (2). 
 { }( )?
?=
=
1,1
,1
, ,Pr
ij
Ni
ji MKCS    (2) 
where 
{ }( ) ( ) ( ) ( )( )





?
==
= otherwise ,Pr
jMiM if ,Pr,Pr
,
,
,
jiji
jiji
ji SSCS
SSCSMKCS   
     (3) 
 
To learn the conditional probabilities ( )jiji SSCS =|Pr ,  and ( )jiji SSCS ?|Pr ,  in Eq. (3), a 
maximum entropy model is trained. There are two 
major advantages of this maximum entropy model: 
i) the model is independent of individual words; ii) 
the model takes no information independence 
assumption about the data, and hence is powerful 
enough to utilize heterogeneous features. With the 
learned conditional probabilities in Eq. (3), for a 
given { }MK ,  candidate, we can compute the 
conditional probability of Expression (2).  In the 
final step, optimization is performed to search for 
{ }MK ,  that maximizes the value of Expression 
(2). 
3 Maximum Entropy Modeling 
This section presents the definition of context 
similarity features, and how to estimate the 
generative probabilities of context similarity ( )jiji SSCS =,Pr  and ( )jiji SSCS ?,Pr  using 
maximum entropy modeling. 
Using the Senseval-2 training corpus,1 we have 
constructed Corpus I and Corpus II for each Part-
of-speech (POS) tag. Corpus I is constructed using 
context pairs involving the same sense of a word.  
Corpus II is constructed using context pairs that 
refer to different senses of a word. Each corpus 
contains about 18,000 context pairs. The instances 
in the corpora are represented as pairwise context 
similarities, taking the form of { }?f . The two 
conditional probabilities ( )jiji SSCS =,Pr  and ( )jiji SSCS ?,Pr  can be represented as 
( )}{Pr maxEntI ?f  and ( )}{Pr maxEntII ?f  which are 
generative probabilities by maximum entropy for 
Corpus I and Corpus II. 
We now present how to compute the context 
similarities. Each context contains the following 
two categories of features: 
i) Trigger words centering around the key word 
within a predefined window size equal to 50 
tokens to both sides of the key word. Trigger 
words are learned using the same technique as 
in [Niu et al 2003]. 
ii) Parsing relationships associated with the key 
word automatically decoded by our parser 
                                                     
1 Note that the words that appear in the Senseval-3 
lexical sample evaluation are removed in the corpus 
construction process. 
InfoXtract [Srihari et al 2003]. The 
relationships being utilized are listed below.  
 
Noun: subject-of, object-of, complement-of, 
has-adjective-modifier, has-noun-
modifier, modifier-of, possess, 
possessed-by, appositive-of 
 
Verb: has-subject, has-object, has-
complement, has-adverb-modifier, 
has-prepositional-modifier 
 
Adjective: modifier-of, has-adverb-modifier 
 
Based on the above context features, the 
following three categories of context similarity 
features are defined: 
(1) Context similarity based on a vector space 
model using co-occurring trigger words: the 
trigger words centering around the key word 
are represented as a vector, and the tf*idf 
scheme is used to weigh each trigger word. 
The cosine of the angle between two resulting 
vectors is used as a context similarity 
measure. 
(2) Context similarity based on Latent 
semantic analysis (LSA) using trigger words: 
LSA [Deerwester et al 1990] is a technique 
used to uncover the underlying semantics 
based on co-occurrence data. Using LSA, 
each word is represented as a vector in the 
semantic space. The trigger words are 
represented as a vector summation. Then the 
cosine of the angle between the two resulting 
vector summations is computed, and used as a 
context similarity measure. 
(3) LSA-based Parsing Structure Similarity: 
each relationship is in the form of )(wR? . 
Using LSA, each word w  is represented as 
semantic vector ( )wV . Then, the similarity 
between )( 1wR? and )( 2wR?  is represented as 
the cosine of angle between ( )1wV  and ( )2wV . 
Two special values are assigned to two 
exceptional cases: i) when  no relationship 
?R  is decoded in both contexts; ii) when the 
relationship ?R is decoded only for one 
context. 
To facilitate the maximum entropy modeling in 
the later stage, the resulting similarity measure is 
discretized into 10 integer values. Now the 
pairwise context similarity is a set of similarity 
features, e.g. 
 
{VSM-Similairty-equal-to-2, LSA-Trigger-
Words-Similarity-equal-to-1, LSA-Subject-
Similarity-equal-to-2}. 
 
In addition to the three categories of basic 
context similarity features defined above, we also 
define induced context similarity features by 
combining basic context similarity features using 
the logical AND operator. With induced features, 
the context similarity vector in the previous 
example is represented as 
 
{VSM-Similairty-equal-to-2, LSA- Trigger-
Words-Similarity-equal-to-1, LSA-Subject-
Similarity-equal-to-2,  
[VSM-Similairty-equal-to-2 and LSA-Trigger -
Words-Similarity-equal-to-1], [VSM-Similairty-
equal-to-2 and LSA-Subject-Similarity-equal-to-
2],  
???, 
[VSM-Similairty-equal-to-2 and LSA-Trigger -
Words-Similarity-equal-to-1 and LSA-Subject-
Similarity-equal-to-2]}. 
 
The induced features provide direct and fine-
grained information, but suffer from less sampling 
space. To make the computation feasible, we 
regulate 3 as the maximum number of logical AND 
in the induced features. Combining basic features 
and induced features under a smoothing scheme, 
maximum entropy modeling may achieve optimal 
performance. 
Now the maximum entropy modeling can be 
formulated as follows: given a pairwise context 
similarity }{ ?f , the generative probability of 
}{ ?f in Corpus I or Corpus II is given as 
 
( )
{ }
?
?
=
?
?
ff
fwZf
1}{Pr maxEnt         (4) 
 
where Z is the normalization factor, fw  is the 
weight associated with feature f . The Iterative 
Scaling algorithm combined with Monte Carlo 
simulation [Pietra, Pietra, & Lafferty 1995] is used 
to train the weights in this generative model. 
Unlike the commonly used conditional maximum 
entropy modeling which approximates the feature 
configuration space as the training corpus 
[Ratnaparkhi 1998], Monte Carlo techniques are 
required in the generative modeling to simulate the 
possible feature configurations. The exponential 
prior smoothing scheme [Goodman 2003] is 
adopted. The same training procedure is performed 
using Corpus I and Corpus II to estimate 
( )}{Pr maxEntI if  and ( )}{Pr maxEntII if  respectively. 
4 Statistical Annealing 
With the maximum entropy modeling presented 
above, the WSD task is performed as follows: i) 
for a given set of contexts, the pairwise context 
similarity measures are computed; ii) for each 
context similarity }{ if , the two generative 
probabilities ( )}{Pr maxEntI if  and ( )}{Pr maxEntII if  are 
computed; iii) for a given WSD candidate 
solution{ }MK , , the conditional probability (2) can 
be computed. Optimization based on statistical 
annealing (Neal 1993) is used to search for { }MK ,  
which maximizes Expression (2). 
The optimization process consists of two steps. 
First, a local optimal solution{ }0, MK is computed 
by a greedy algorithm. Then by setting { }0, MK as 
the initial state, statistical annealing is applied to 
search for the global optimal solution. To reduce 
the search time, we set the maximum value of K  
to 5. 
5 Benchmarking and Conclusion 
To enter the Senseval-3 evaluation, we 
implemented the following procedure to map the 
context clusters to Senseval-3 standards: i) process 
the Senseval-3 training corpus and testing corpus 
using our parser; ii) for each word to be 
benchmarked, retrieve the related contexts from 
the corpora and cluster them; iii) Based on 10% of 
the sense tags in the Senseval-3 training corpus 
(10% data correspond roughly to an average of 2-3 
instances for each sense), the context cluster is 
mapped onto the most frequent WSD sense 
associated with the cluster members. By design, 
the context clusters correspond to distinct senses, 
therefore, we do not allow multiple context clusters 
to be mapped onto one sense. In case multiple 
clusters correspond to one sense, only the largest 
cluster is retained; iv), each instance in the testing 
corpus is tagged with the same sense as the one to 
which its context cluster corresponds.  
    We are not able to compare our performance 
with other systems in Senseval-3 because at the 
time of writing, the Senseval-3 evaluation results 
are not publicly available. As a note, compared 
with the Senseval-2 English Lexical Sample 
evaluation, the benchmarks of our new algorithm 
(Table 1) are significantly above the performance 
of the WSD systems in the unsupervised category, 
and rival the performance of the supervised WSD 
systems. 
 
Table 1. Senseval-3 Lexical Sample Evaluation  
Accuracy  
Category Fine grain (%) Coarse grain (%) 
Adjective (5) 49.1 64.8 
Noun (20) 57.9 66.6 
Verb (32) 55.3 66.3 
Average 56.3% 66.4% 
 
6 Acknowledgements 
This work was supported by the Navy SBIR 
program under contract N00178-03-C-1047. 
References  
Gale, W., K. Church, and D. Yarowsky. 1992. A 
Method for Disambiguating Word Senses in a 
Large Corpus. Computers and the Humanities, 
26. 
Yarowsky, D. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. 
In Proceedings of ACL 1995. 
Schutze, H. 1998. Automatic Word Sense 
Disambiguation. Computational Linguistics, 23. 
C. Niu, Zhaohui Zheng, R. Srihari, H. Li, and W. 
Li 2003. Unsupervised Learning for Verb Sense 
Disambiguation Using Both trigger Words and 
Parsing Relations. In Proceeding of PACLING 
2003, Halifax, Canada. 
Deerwester, S., S. T. Dumais, G. W. Furnas, T. K. 
Landauer, and R. Harshman. 1990. Indexing by 
Latent Semantic Analysis. In Journal of the 
American Society of Information Science 
Goodman, J. 2003. Exponential Priors for 
Maximum Entropy Models. 
Neal, R.M. 1993. Probabilistic Inference Using 
Markov Chain Monte Carlo Methods. Technical 
Report, Univ. of Toronto. 
Pietra, S. D., V. D. Pietra, and J. Lafferty. 1995. 
Inducing Features Of Random Fields. In IEEE 
Transactions on Pattern Analysis and Machine 
Intelligence.  
Adwait Ratnaparkhi. (1998). Maximum Entropy 
Models for Natural Language Ambiguity 
Resolution. Ph.D. Dissertation. University of 
Pennsylvania. 
Srihari, R., W. Li, C. Niu and T. Cornell. 2003. 
InfoXtract: A Customizable Intermediate Level 
Information Extraction Engine. In Proceedings 
of HLT/NAACL 2003 Workshop on SEALTS. 
Edmonton, Canada. 
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 33?39, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
 
Abstract 
Traditionally, word sense disambiguation 
(WSD) involves a different context classifi-
cation model for each individual word. This 
paper presents a weakly supervised learning 
approach to WSD based on learning a word 
independent context pair classification 
model. Statistical models are not trained for 
classifying the word contexts, but for classi-
fying a pair of contexts, i.e. determining if a 
pair of contexts of the same ambiguous word 
refers to the same or different senses. Using 
this approach, annotated corpus of a target 
word A can be explored to disambiguate 
senses of a different word B. Hence, only a 
limited amount of existing annotated corpus 
is required in order to disambiguate the entire 
vocabulary. In this research, maximum en-
tropy modeling is used to train the word in-
dependent context pair classification model. 
Then based on the context pair classification 
results, clustering is performed on word men-
tions extracted from a large raw corpus. The 
resulting context clusters are mapped onto 
the external thesaurus WordNet. This ap-
proach shows great flexibility to efficiently 
integrate heterogeneous knowledge sources, 
e.g. trigger words and parsing structures. 
Based on Senseval-3 Lexical Sample stan-
dards, this approach achieves state-of-the-art 
performance in the unsupervised learning 
category, and performs comparably with the 
supervised Na?ve Bayes system. 
1 Introduction 
Word Sense Disambiguation (WSD) is one of the 
central problems in Natural Language Processing. 
The difficulty of this task lies in the fact that con-
text features and the corresponding statistical dis-
tribution are different for each individual word. 
Traditionally, WSD involves training the context 
classification models for each ambiguous word.  
(Gale et al 1992) uses the Na?ve Bayes method for 
context classification which requires a manually 
annotated corpus for each ambiguous word. This 
causes a serious Knowledge Bottleneck. The bot-
tleneck is particularly serious when considering the 
domain dependency of word senses. To overcome 
the Knowledge Bottleneck, unsupervised or weakly 
supervised learning approaches have been pro-
posed. These include the bootstrapping approach 
(Yarowsky 1995) and the context clustering ap-
proach (Sch?tze 1998). 
The above unsupervised or weakly supervised 
learning approaches are less subject to the Knowl-
edge Bottleneck. For example, (Yarowsky 1995) 
only requires sense number and a few seeds for 
each sense of an ambiguous word (hereafter called 
keyword). (Sch?tze 1998) may only need minimal 
annotation to map the resulting context clusters 
onto external thesaurus for benchmarking and ap-
plication-related purposes. Both methods are based 
on trigger words only. 
This paper presents a novel approach based on 
learning word-independent context pair classifica-
tion model. This idea may be traced back to 
(Sch?tze 1998) where context clusters based on 
generic Euclidean distance are regarded as distinct 
word senses. Different from (Sch?tze 1998), we 
observe that generic context clusters may not al-
ways correspond to distinct word senses. There-
fore, we used supervised machine learning to 
model the relationships between the context dis-
tinctness and the sense distinctness. 
Although supervised machine learning is used 
for the context pair classification model, our over-
all system belongs to the weakly supervised cate-
gory because the learned context pair classification 
Word Independent Context Pair Classification Model for Word 
Sense Disambiguation 
 
Cheng Niu, Wei Li, Rohini K. Srihari, and Huifeng Li 
Cymfony Inc. 
600 Essjay Road, Williamsville, NY 14221, USA. 
{cniu, wei, rohini,hli}@cymfony.com 
33
model is independent of the keyword for disam-
biguation. Our system does not need human-
annotated instances for each target ambiguous 
word. The weak supervision is performed by using 
a limited amount of existing annotated corpus 
which does not need to include the target word set.   
The insight is that the correlation regularity be-
tween the sense distinction and the context distinc-
tion can be captured at Part-of-Speech category 
level, independent of individual words or word 
senses. Since context determines the sense of a 
word, a reasonable hypothesis is that there is some 
mechanism in the human comprehension process 
that will decide when two contexts are similar (or 
dissimilar) enough to trigger our interpretation of a 
word in the contexts as one meaning (or as two 
different meanings). We can model this mecha-
nism by capturing the sense distinction regularity 
at category level.  
In the light of this, a maximum entropy model is 
trained to determine if a pair of contexts of the 
same keyword refers to the same or different word 
senses. The maximum entropy modeling is based 
on heterogeneous context features that involve 
both trigger words and parsing structures. To en-
sure the resulting model?s independency of indi-
vidual words, the keywords used in training are 
different from the keywords used in benchmarking. 
For any target keyword, a collection of contexts is 
retrieved from a large raw document pool. Context 
clustering is performed to derive the optimal con-
text clusters which globally fit the local context 
pair classification results. Here statistical annealing 
is used for its optimal performance. In benchmark-
ing, a mapping procedure is required to correlate 
the context clusters with external ontology senses. 
In what follows, Section 2 formulates the maxi-
mum entropy model for context pair classification. 
The context clustering algorithm, including the 
object function of the clustering and the statistical 
annealing-based optimization, is described in Sec-
tion 3. Section 4 presents and discusses bench-
marks, followed by conclusion in Section 5. 
2 Maximum Entropy Modeling for Con-
text Pair Classification 
Given n  mentions of a keyword, we first introduce 
the following symbols. iC  refers to the i -th con-
text.  iS  refers to the sense of the i -th context. 
jiCS ,  refers to the context similarity between the 
i -th context and the j -th context, which is a subset 
of the predefined context similarity features. ?f  
refers to the ? -th predefined context similarity 
feature. So jiCS ,  takes the form of { }?f . 
In this section, we study the context pair classi-
fication task, i.e. given a pair of contexts iC and 
jC  of the same target word, are they referring to 
the same sense? This task is formulated as compar-
ing the following conditional probabilities: ( )jiji CSSS ,Pr =  and ( )jiji CSSS ,Pr ? . Unlike 
traditional context classification for WSD where 
statistical model is trained for each individual 
word, our context pair classification model is 
trained for each Part-of-speech (POS) category. 
The reason for choosing POS as the appropriate 
category for learning the context similarity is that 
the parsing structures, hence the context represen-
tation, are different for different POS categories. 
The training corpora are constructed using the 
Senseval-2 English Lexical Sample training cor-
pus. To ensure the resulting model?s independency 
of individual words, the target words used for 
benchmarking (which will be the ambiguous words 
used in Senseval-3 English Lexicon Sample task) 
are carefully removed in the corpus construction 
process. For each POS category, positive and nega-
tive instances are constructed as follows.  
Positive instances are constructed using context 
pairs referring to the same sense of a word.  Nega-
tive instances are constructed using context pairs 
that refer to different senses of a word.  
For each POS category, we have constructed 
about 36,000 instances, half positive and half nega-
tive. The instances are represented as pairwise con-
text similarities, taking the form of { }?f . 
Before presenting the context similarity features 
we used, we first introduce the two categories of 
the involved context features: 
 
i) Co-occurring trigger words within a prede-
fined window size equal to 50 words to both 
sides of the keyword. The trigger words are 
learned from a TIPSTER document pool con-
taining ~170 million words of AP and WSJ 
news articles. Following (Sch?tze 1998), ?2 is 
used to measure the cohesion between the 
keyword and a co-occurring word.  In our ex-
34
periment, all the words are first sorted based 
on its ?2 with the keyword, and then the top 
2,000 words are selected as trigger words. 
 
ii) Parsing relationships associated with the 
keyword automatically decoded by a broad-
coverage parser, with F-measure (i.e. the pre-
cision-recall combined score) at about 85% 
(reference temporarily omitted for the sake of 
blind review). The logical dependency rela-
tionships being utilized are listed below. 
 
Noun:  subject-of,  
object-of, 
complement-of,  
has-adjective-modifier,  
has-noun-modifier,  
modifier-of,  
possess, 
 possessed-by,  
appositive-of 
 
Verb:   has-subject,  
has-object, 
 has-complement,  
has-adverb-modifier,  
has-prepositional-phrase-modifier 
 
Adjective: modifier-of,  
has-adverb-modifier 
 
Based on the above context features, the follow-
ing three categories of context similarity features 
are defined: 
 
(1)  VSM-based (Vector Space Model based) 
trigger word similarity: the trigger words 
around the keyword are represented as a vec-
tor, and the word i in context j is weighted as 
follows: 
)(log*),(),( idf
Djitfjiweight =  
where ),( jitf  is the frequency of word i in 
the j-th context; D is the number of docu-
ments in the pool; and )(idf  is the number of 
documents containing the word i. D and 
)(idf are estimated using the document pool 
introduced above. The cosine of the angle be-
tween two resulting vectors is used as the 
context similarity measure. 
 
(2)  LSA-based (Latent Semantic Analysis based) 
trigger word similarity: LSA (Deerwester et 
al. 1990) is a technique used to uncover the 
underlying semantics based on co-occurrence 
data. The first step of LSA is to construct 
word-vs.-document co-occurrence matrix. 
Then singular value decomposition (SVD) is 
performed on this co-occurring matrix. The 
key idea of LSA is to reduce noise or insig-
nificant association patterns by filtering the 
insignificant components uncovered by SVD. 
This is done by keeping only the top k singu-
lar values. By using the resulting word-vs.-
document co-occurrence matrix after the fil-
tering, each word can be represented as a vec-
tor in the semantic space. 
  
In our experiment, we constructed the original 
word-vs.-document co-occurring matrix as 
follows: 100,000 documents from the 
TIPSTER corpus were used to construct the 
co-occurring matrix. We processed these 
documents using our POS tagger, and se-
lected the top n most frequently mentioned 
words from each POS category as base 
words: 
 
top 20,000 common nouns 
top 40,000 proper names 
top 10,000 verbs 
top 10,000 adjectives 
top 2,000 adverbs 
 
In performing SVD, we set k (i.e. the number 
of nonzero singular values) as 200, following 
the practice reported in (Deerwester et al 
1990) and (Landauer & Dumais, 1997). 
 
Using the LSA scheme described above, each 
word is represented as a vector in the seman-
tic space. The co-occurring trigger words are 
represented as a vector summation. Then the 
cosine of the angle between the two resulting 
vector summations is computed, and used as 
the context similarity measure. 
 
(3) LSA-based parsing relationship similarity: 
each relationship is in the form of )(wR? . 
Using LSA, each word w  is represented as a 
35
semantic vector ( )wV . The similarity between 
)( 1wR? and )( 2wR?  is represented as the co-
sine of the angle between ( )1wV  and ( )2wV . 
Two special values are assigned to two excep-
tional cases: (i) when no relationship ?R  is 
decoded in both contexts; (ii) when the rela-
tionship ?R is decoded only for one context. 
 
In matching parsing relationships in a context 
pair, if only exact node match counts, very few 
cases can be covered, hence significantly reducing 
the effect of the parser in this task. To solve this 
problem, LSA is used as a type of synonym expan-
sion in matching.  For example, using LSA, the 
following word similarity values are generated: 
 
similarity(good, good)   1.00 
similarity(good, pretty) 0.79 
similarity(good, great) 0.72 
?? 
 
Given a context pair of a noun keyword, suppose 
the first context involves a relationship has-
adjective-modifier whose value is good, and the 
second context involves the same relationship has-
adjective-modifier with the value pretty, then the 
system assigns 0.79 as the similarity value for this 
relationship pair. 
 
To facilitate the maximum entropy modeling in 
the later stage, all the three categories of the result-
ing similarity values are discretized into 10 inte-
gers. Now the pairwise context similarity is 
represented as a set of similarity features, e.g. 
 
{VSM-Trigger-Words-Similairty-equal-to-2,  
  LSA-Trigger-Words-Similarity-equal-to-1,      
  LSA-Subject-Similarity-equal-to-2}. 
 
In addition to the three categories of basic con-
text similarity features defined above, we also de-
fine induced context similarity features by 
combining basic context similarity features using 
the logical and operator. With induced features, the 
context similarity vector in the previous example is 
represented as 
 
{VSM-Trigger-Word-Similairty-equal-to-2,  
  LSA- Trigger-Word-Similarity-equal-to-1,  
  LSA-Subject-Similarity-equal-to-2,  
  [VSM-Similairty-equal-to-2 and  
   LSA-Trigger-Word-Similarity-equal-to-1],    
  [VSM-Similairty-equal-to-2 and  
   LSA-Subject-Similarity-equal-to-2],  
  ??? 
  [VSM-Trigger-Word-Similairty-equal-to-2   
and LSA-Trigger-Word-Similarity-equal-to-1 
and LSA-Subject-Similarity-equal-to-2] 
} 
 
The induced features provide direct and fine-
grained information, but suffer from less sampling 
space. Combining basic features and induced fea-
tures under a smoothing scheme, maximum en-
tropy modeling may achieve optimal performance. 
Using the context similarity features defined 
above, the training corpora for the context pair 
classification model is in the following format: 
 
Instance_0 tag=?positive? {VSM-Trigger-Word-
Similairty-equal-to-2, ?} 
Instance_1 tag=?negative? {VSM-Trigger-Word-
Similairty-equal-to-0, ?} 
????? 
where positive tag denotes a context pair associ-
ated with same sense, and negative tag denotes a 
context pair associated with different senses.  
 
The maximum entropy modeling is used to com-
pute the conditional probabilities ( )jiji CSSS ,Pr =  and ( )jiji CSSS ,Pr ? : once the 
context pair jiCS ,  is represented as }{ ?f , the con-
ditional probability is given as 
 
( )
{ }
?
?
=
?
?
ff
ftwZft ,
1}{Pr         (1) 
where { }jiji SSSSt ?=? , , Z is the normaliza-
tion factor, ftw ,  is the weight associated with tag t 
and feature f . Using the training corpora con-
structed above, the weights can be computed based 
on Iterative Scaling algorithm (Pietra etc. 1995) 
The exponential prior smoothing scheme (Good-
man 2003) is adopted in the training.  
36
3 Context Clustering based on Context 
Pair Classification Results 
Given n  mentions { }iC of a keyword, we use the 
following context clustering scheme. The discov-
ered context clusters correspond to distinct word 
senses.  
For any given context pair, the context similarity 
features defined in Section 2 are computed. With n 
mentions of the same keyword, 2
)1( ?nn  context 
similarities [ ] [ )( )ijniCS ji ,1,,1 , ??  are computed. 
Using the context pair classification model, each 
pair is associated with two scores ( )( )jijiji CSSSsc ,0, Prlog ==  and 
( )( )jijiji CSSSsc ,1, Prlog ==  which correspond to 
the probabilities of two situations: the pair refers to 
the same or different word senses. 
Now we introduce the symbol { }MK ,  which re-
fers to the final context cluster configuration, 
where K refers to the number of distinct sense, and 
M represents the many-to-one mapping (from con-
texts to a sense) such that 
( ) K]. [1,j n],[1,i j,iM ??= Based on the pairwise 
scores { } 0, jisc and  { } 1, jisc , WSD is formulated as 
searching for { }MK , which maximizes the follow-
ing global scores: 
 
{ }( ) ( )
[ ][ )
 MK,c
,1
,n1,i
,
,

?
?
=
ij
jik
jiscs  (2) 
where ( ) ( ) ( )



=
= otherwise
jMiMifjik      ,1
 ,0,  
 
Similar clustering scheme has been used success-
fully for the task of co-reference in (Luo etc. 
2004), (Zelenko, Aone and Tibbetts, 2004a) and 
(Zelenko, Aone and Tibbetts, 2004b). 
In this paper, statistical annealing-based optimi-
zation (Neal 1993) is used to search for { }MK ,  
which maximizes Expression (2). 
The optimization process consists of two steps. 
First, an intermediate solution { }0, MK  is com-
puted by a greedy algorithm. Then by setting 
{ }0, MK as the initial state, statistical annealing is 
applied to search for the global optimal solution. 
The optimization algorithm is as follows. 
1. Set the initial state { }MK , as nK = , and 
[ ]n1,i  ,)( ?= iiM ; 
2. Select a cluster pair for merging that 
maximally increases 
{ }( ) ( )
[ ][ )
 MK,c
,1
,n1,i
,
,

?
?
=
ij
jik
jiscs  
3. If no cluster pair can be merged to in-
crease { }( ) ( )
[ ][ )
 MK,c
,1
,n1,i
,
,

?
?
=
ij
jik
jiscs , output 
{ }MK , as the intermediate solution; 
otherwise, update { }MK ,  by the merge 
and go to step 2. 
 
Using the intermediate solution { }0, MK of the 
greedy algorithm as the initial state, the statistical 
annealing is implemented using the following 
pseudo-code:  
       Set { } { }0,, MKMK = ; 
       for( 1.01?*;?? ;?? final0 =<= ) 
{ 
    iterate pre-defined number of times 
    { 
          set { } { }MKMK ,, 1 = ; 
     update { }1, MK  by randomly changing 
cluster number and cluster contents;  
            set { }( ){ }( )MK,c
MK,c 1
s
sx =  
           if(x>=1) 
           { 
             set { } { }1,, MKMK =  
           } 
          else 
          { 
             set { } { }1,, MKMK =  with probability  
              ?x . 
          } 
         if { }( ) { }( )0MK,cMK,c ss >  
         then set { } { }MKMK ,, 0 =  
     } 
  } 
  output { }0, MK  as the optimal state. 
 
37
4 Benchmarking 
Corpus-driven context clusters need to map to a 
word sense standard to facilitate performance 
benchmark. Using Senseval-3 evaluation stan-
dards, we implemented the following procedure to 
map the context clusters:  
 
i) Process TIPSTER corpus and the origi-
nal unlabeled Senseval-3 corpora (in-
cluding the training corpus and the 
testing corpus) by our parser, and save 
all the parsing results into a repository.  
 
ii) For each keyword, all related contexts in 
Senseval-3 corpora and up-to-1,000 re-
lated contexts in TIPSTER corpus are 
retrieved from the repository.  
 
iii) All the retrieved contexts are clustered 
based on the context clustering algo-
rithm presented in Sect. 2 and 3. 
 
iv) For each keyword sense, three annotated 
contexts from Senseval-3 training cor-
pus are used for the sense mapping. The 
context cluster is mapped onto the most 
frequent word sense associated with the 
cluster members. By design, the context 
clusters correspond to distinct senses, 
therefore, we do not allow multiple con-
text clusters to be mapped onto one 
sense. In case multiple clusters corre-
spond to one sense, only the largest 
cluster is retained.  
 
v) Each context in the testing corpus is 
tagged with the sense to which its con-
text cluster corresponds to. 
 
As mentioned above, Sensval-2 English lexical 
sample training corpora is used to train the context 
pair classification model. And Sensval-3 English 
lexical sample testing corpora is used here for 
benchmarking. There are several keyword occur-
ring in both Senseval-2 and Senseval-3 corpora. 
The sense tags associated with these keywords are 
not used in the context pair classification training 
process.  
In order to gauge the performance of this new 
weakly supervised learning algorithm, we have 
also implemented a supervised Na?ve Bayes sys-
tem following (Gale et al 1992).  This system is 
trained based on the Senseval-3 English Lexical 
Sample training corpus.  In addition, for the pur-
pose of quantifying the contribution from the pars-
ing structures in WSD, we have run our new 
system with two configurations: (i) using only 
trigger words; (ii) using both trigger words and 
parsing relationships. All the benchmarking is per-
formed using the Senseval-3 English Lexical Sam-
ple testing corpus and standards.  
The performance benchmarks for the two sys-
tems in three runs are shown in Table 1, Table 2 
and Table 3. When using only trigger words, this 
algorithm has 8 percentage degradation from the 
supervised Na?ve Bayes system (see Table 1 vs. 
Table 2). When adding parsing structures, per-
formance degradation is reduced, with about 5 per-
centage drop (see Table 3 vs. Table 2). Comparing 
Table 1 with Table 3, we observe about 3% en-
hancement due to the contribution from the parsing 
support in WSD. The benchmark of our algorithm 
using both trigger words and parsing relationships 
is one of the best in unsupervised category of the 
Senseval-3 Lexical Sample evaluation. 
 
Table 1. New Algorithm Using Only Trigger Words  
Accuracy  
Category Fine grain (%) Coarse grain (%) 
Adjective (5) 46.3 60.8 
Noun (20) 54.6 62.8 
Verb (32) 54.1 64.2 
Overall  54.0 63.4 
 
Table 2. Supervised Na?ve Bayes System 
Accuracy  
Category Fine grain (%) Coarse grain (%) 
Adjective (5) 44.7 56.6 
Noun (20) 66.3 74.5 
Verb (32) 58.6 70.0 
Overall 61.6 71.5 
 
Table 3. New Algorithm Using Both Trigger Words and 
Parsing  
Accuracy  
Category Fine grain (%) Coarse grain (%) 
Adjective (5) 49.1 64.8 
Noun (20) 57.9 66.6 
Verb (32) 55.3 66.3 
Overall 56.3 66.4 
38
 
It is noted that Na?ve Bayes algorithm has many 
variation, and its performance has been greatly 
enhanced during recent research. Based on Sen-
seval-3 results, the best Na?ve Bayse system out-
perform our version (which is implemented based 
on Gale et al 1992) by 8%~10%. So the best su-
pervised WSD systems output-perform our weakly 
supervised WSD system by 13%~15% in accuracy. 
5 Conclusion 
We have presented a weakly supervised learning 
approach to WSD. Statistical models are not 
trained for the contexts of each individual word, 
but for context pair classification. This approach 
overcomes the knowledge bottleneck that chal-
lenges supervised WSD systems which need la-
beled data for each individual word. It captures the 
correlation regularity between the sense distinction 
and the context distinction at Part-of-Speech cate-
gory level, independent of individual words and 
senses. Hence, it only requires a limited amount of 
existing annotated corpus in order to disambiguate 
the full target set of ambiguous words, in particu-
lar, the target words that do not appear in the train-
ing corpus.   
The weakly supervised learning scheme can 
combine trigger words and parsing structures in 
supporting WSD. Using Senseval-3 English Lexi-
cal Sample benchmarking, this new approach 
reaches one of the best scores in the unsupervised 
category of English Lexical Sample evaluation. 
This performance is close to the performance for 
the supervised Na?ve Bayes system. 
In the future, we will implement a new scheme 
to map context clusters onto WordNet senses by 
exploring WordNet glosses and sample sentences. 
Based on the new sense mapping scheme, we will 
benchmark our system performance using Senseval 
English all-words corpora.  
 
References  
Deerwester, S., S. T. Dumais, G. W. Furnas, T. K. 
Landauer, and R. Harshman. 1990. Indexing by 
Latent Semantic Analysis. In Journal of the 
American Society of Information Science 
Gale, W., K. Church, and D. Yarowsky. 1992. A 
Method for Disambiguating Word Senses in a 
Large Corpus. Computers and the Humanities, 
26. 
Goodman, J. 2003. Exponential Priors for Maxi-
mum Entropy Models. In Proceedings of HLT-
NAACL 2004. 
Landauer, T. K., & Dumais, S. T. 1997. A solution 
to Plato's problem: The Latent Semantic Analy-
sis theory of the acquisition, induction, and rep-
resentation of knowledge. Psychological 
Review, 104, 211-240, 1997. 
Luo, X., A. Ittycheriah, H. Jing, N. Kambhatla and 
S. Roukos. A Mention-Synchronous Corefer-
ence Resolution Algorithm Based on the Bell 
Tree. In The Proceedings of ACL 2004. 
Neal, R.M. 1993. Probabilistic Inference Using 
Markov Chain Monte Carlo Methods. Technical 
Report, Univ. of Toronto. 
Pietra, S. D., V. D. Pietra, and J. Lafferty. 1995. 
Inducing Features Of Random Fields. In IEEE 
Transactions on Pattern Analysis and Machine 
Intelligence. 
Sch?tze, H. 1998. Automatic Word Sense Disam-
biguation. Computational Linguistics, 23. 
Yarowsky, D. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. 
In Proceedings of ACL 1995.  
Zelenko, D., C. Aone and J. 2004. Tibbetts. 
Coreference Resolution for Information Extrac-
tion. In Proceedings of ACL 2004 Workshop on 
Reference Resolution and its Application.  
Zelenko, D., C. Aone and J. 2004. Tibbetts. Binary 
Integer Programming for Information Extrac-
tion. In Proceedings of ACE 2004 Evaluation 
Workshop.  
 
39
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 168?175,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatically Extracting Nominal Mentions of Events with a
Bootstrapped Probabilistic Classifier?
Cassandre Creswell? and Matthew J. Beal? and John Chen?
Thomas L. Cornell? and Lars Nilsson? and Rohini K. Srihari??
?Janya, Inc.
1408 Sweet Home Road, Suite 1
Amherst NY 14228
{ccreswell,jchen,cornell,
lars,rohini}@janyainc.com
?Dept. of Computer Science and Engineering
University at Buffalo
The State University of New York
Amherst NY 14260
mbeal@cse.buffalo.edu
Abstract
Most approaches to event extraction focus
on mentions anchored in verbs. However,
many mentions of events surface as noun
phrases. Detecting them can increase the
recall of event extraction and provide the
foundation for detecting relations between
events. This paper describes a weakly-
supervised method for detecting nominal
event mentions that combines techniques
from word sense disambiguation (WSD)
and lexical acquisition to create a classifier
that labels noun phrases as denoting events
or non-events. The classifier uses boot-
strapped probabilistic generative models
of the contexts of events and non-events.
The contexts are the lexically-anchored se-
mantic dependency relations that the NPs
appear in. Our method dramatically im-
proves with bootstrapping, and comfort-
ably outperforms lexical lookup methods
which are based on very much larger hand-
crafted resources.
1 Introduction
The goal of information extraction is to generate
a set of abstract information objects that repre-
sent the entities, events, and relations of particu-
lar types mentioned in unstructured text. For ex-
ample, in a judicial domain, relevant event types
might be ARREST, CHARGING, TRIAL, etc.
Although event extraction techniques usually
focus on extracting mentions textually anchored
by verb phrases or clauses, e.g. (Aone and Ramos-
? This work was supported in part by SBIR grant
FA8750-05-C-0187 from the Air Force Research Laboratory
(AFRL)/IFED.
Santacruz, 2000), many event mentions, espe-
cially subsequent mentions of events that are the
primary topic of a document, are referred to with
nominals. Because of this, detecting nominal
event mentions, like those in (1), can increase the
recall of event extraction systems, in particular for
the most important events in a document.1
(1) The slain journalist was a main organizer of the mas-
sive demonstrations that forced Syria to withdraw its
troops from Lebanon last April, after Assad was widely
accused of planningHariri?s assassination in a Febru-
ary car bombing that was similar to today?s blast.
Detecting event nominals is also an important
step in detecting relations between event men-
tions, as in the causal relation between the demon-
strations and the withdrawal and the similarity re-
lation between the bombing and the blast in (1).
Finally, detecting nominal events can improve
detection and coreference of non-named mentions
of non-event entities (e.g. persons, locations, and
organizations) by removing event nominals from
consideration as mentions of entities.
Current extraction techniques for verbally-
anchored events rest on the assumption that most
verb phrases denote eventualities. A system to ex-
tract untyped event mentions can output all con-
stituents headed by a non-auxiliary verb with a
filter to remove instances of to be, to seem, etc.
A statistical or rule-based classifier designed to
detect event mentions of specific types can then
be applied to filter these remaining instances.
Noun phrases, in contrast, can be used to denote
anything?eventualities, entities, abstractions, and
only some are suitable for event-type filtering.
1For example, in the 2005 Automatic Content Extraction
training data, of the 5,349 event mentions, over 35% (1934)
were nominals.
168
1.1 Challenges of nominal event detection
Extraction of nominal mentions of events encom-
passes many of the fundamental challenges of
natural language processing. Creating a general
purpose lexicon of all potentially event-denoting
terms in a language is a labor-intensive task. On
top of this, even utilizing an existing lexical re-
source like WordNet requires sense disambigua-
tion at run-time because event nominals display
the full spectrum of sense distinction behaviors
(Copestake and Briscoe, 1995), including idiosyn-
cratic polysemy, as in (2); constructional poly-
semy, as in (3); coactivation, (4); and copredica-
tion, as in (5).
(2) a. On May 30 a group of Iranian mountaineers hoisted
the Iranian tricolor on the summit.
b. EU Leaders are arriving here for their two-day
summit beginning Thursday.
(3) Things are getting back to normal in the Baywood Golf
Club after a chemical spill[=event]. Clean-up crews
said the chemical spill[=result] was 99 percent water
and shouldn?t cause harm to area residents.
(4) Managing partner Naimoli said he wasn?t concerned
about recent media criticism.
(5) The construction lasted 30 years and was inaugurated
in the presence of the king in June 1684.
Given the breadth of lexical sense phenom-
ena possible with event nominals, no existing ap-
proach can address all aspects. Lexical lookup,
whether using a manually- or automatically-
constructed resource, does not take context into
consideration and so does not allow for vagueness
or unknown words. Purely word-cooccurrence-
based approaches (e.g. (Schu?tze, 1998)) are un-
suitable for cases like (3) where both senses are
possible in a single discourse. Furthermore, most
WSD techniques, whether supervised or unsuper-
vised, must be retrained for each individual lexical
item, a computationally expensive procedure both
at training and run time. To address these limita-
tions, we have developed a technique which com-
bines automatic lexical acquisition and sense dis-
ambiguation into a single-pass weakly-supervised
algorithm for detecting event nominals.
The remainder of this paper is organized as fol-
lows: Section 2 describes our probabilistic clas-
sifier. Section 3 presents experimental results of
this model, assesses its performance when boot-
strapped to increase its coverage, and compares it
to a lexical lookup technique. We describe related
work in Section 4 and present conclusions and im-
plications for future work in Section 5.
2 Weakly-supervised, simultaneous
lexical acquisition and disambiguation
In this section we present a computational method
that learns the distribution of context patterns that
correlate with event vs. non-event mentions based
on unambiguous seeds. Using these seeds we
build two Bayesian probabilistic generative mod-
els of the data, one for non-event nominals and the
other for event nominals. A classifier is then con-
structed by comparing the probability of a candi-
date instance under each model, with the winning
model determining the classification. In Section 3
we show that this classifier?s coverage can be in-
creased beyond the initial labeled seed set by au-
tomatically selecting additional seeds from a very
large unlabeled, parsed corpus.
The technique proceeds as follows. First, two
lexicons of seed terms are created by hand. One
lexicon includes nominal terms that are highly
likely to unambiguously denote events; the other
includes nominal terms that are highly likely to
unambiguously denote anything other than events.
Then, a very large corpus (>150K documents) is
parsed using a broad-coverage dependency parser
to extract all instantiations of a core set of seman-
tic dependency relations, including verb-logical
subject, verb-logical object, subject-nominal pred-
icate, noun phrase-appositive-modifier, etc.
Format of data: Each instantiation is in the
form of a dependency triple, (wa, R,wb), where
R is the relation type and where each argument is
represented just by its syntactic head, wn. Each
partial instantiation of the relation?i.e. either wa
or wb is treated as a wild card ? that can be filled
by any term?becomes a feature in the model. For
every common noun term in the corpus that ap-
pears with at least one feature (including each en-
try in the seed lexicons), the times it appears with
each feature are tabulated and stored in a matrix
of counts. Each column of the matrix represents
a feature, e.g. (occur,Verb-Subj, ?); each row rep-
resents an individual term,2 e.g. murder; and each
entry is the number of times a term appeared with
the feature in the corpus, i.e. as the instantiation of
?. For each row, if the corresponding term appears
in a lexicon it is given that designation, i.e. EVENT
or NONEVENT, or if it does not appear in either
lexicon, it is left unlabeled.
2A term is any common noun whether it is a single or
multiword expression.
169
Probabilistic model: Here we present the de-
tails of the EVENT model?the computations for
the NONEVENT model are identical. The probabil-
ity model is built using a set of seed words labeled
as EVENTs and is designed to address two desider-
ata: (I) the EVENT model should assign high prob-
ability to an unlabeled vector, v, if its features (as
recorded in the count matrix) are similar to the
vectors of the EVENT seeds; (II) each seed term
s should contribute to the model in proportion to
its prevalence in the training data.3 These desider-
ata can be incorporated naturally into a mixture
model formalism, where there are as many com-
ponents in the mixture model as there are EVENT
seed terms. Desideratum (I) is addressed by hav-
ing each component of the mixture model assign-
ing a multinomial probability to the vector, v. For
the ith mixture component built around the ith
seed, s(i), the probability is
p(v|s(i)) =
F?
f=1
(
s(i)f
)vf
,
where s(i)f is defined as the proportion of the times
the seed was seen with feature f compared to the
number of times the seed was seen with any fea-
ture f ? ? F . Thus s(i)f is simply the (i, f)th entry
in a row-sum normalized count matrix,
s(i)f =
s(i)f
?F
f ?=1 s
(i)
f ?
.
Desideratum (II) is realized using a mixture den-
sity by forming a weighted mixture of the above
multinomial distributions from all the provided
seeds i ? E . The weighting of the ith compo-
nent is fixed to be the ratio of the number of oc-
currences of the ith EVENT seed, denoted |s(i)|, to
the total number of all occurrences of event seed
words. This gives more weight to more prevalent
seed words:
p(s(i)) =
|s(i)|
?
i??E |s
(i?)|
.
The EVENT generative probability is then:
p(v|EVENT) =
?
i?E
[
p(s(i)) ? p(v|s(i))
]
.
An example of the calculation for a model with
just two event seeds and three features is given in
Figure 1. A second model is built from the non-
3The counts used here are the number of times a term is
seen with any feature in the training corpus because the in-
dexing tool used to calculate counts does not keep track of
which instances appeared simultaneously with more than one
feature. We do not expect this artifact to dramatically change
the relative seed frequencies in our model.
f1 f2 f3
event seed vector s(1) 3 1 8
event seed vector s(2) 4 6 1
unlabeled mention vector v 2 0 7
p(v|event) =
12
23
?
?
3
12
?2? 1
12
?0? 8
12
?7
+
11
23
?
?
4
11
?2? 6
11
?0? 1
11
?7
= 0.0019
Figure 1: Example of calculating the probability of unla-
beled instance v under the event distribution composed of
two event seeds s(1) and s(2).
event seeds as well, and a corresponding probabil-
ity p(v|NONEVENT) is computed. The following
difference (log odds-ratio)
d(v) = log p(v|EVENT) ? log p(v|NONEVENT)
is then calculated. An instance v encoded as the
vector v is labeled as EVENT or NONEVENT by
examining the sign of d(v). A positive difference
d(v) classifies v as EVENT; a negative value of
d(v) classifies v as NONEVENT. Should d=0 the
classifier is considered undecided and abstains.
Each test instance is composed of a term and
the dependency triples it appears with in context
in the test document. Therefore, an instance can
be classified by (i: word): Find the unlabeled fea-
ture vector in the training data corresponding to
the term and apply the classifier to that vector,
i.e. classify the instance based on the term?s be-
havior summed across many occurrences in the
training corpus; (ii: context): Classify the instance
based only on its immediate test context vector; or
(iii: word+context): For each model, multiply the
probability information from the word vector (=i)
and the context vector (=ii). In our experiments,
all terms in the test corpus appeared at least once
(80% appearing at least 500 times) in the training
corpus, so there were no cases of unseen terms?
not suprising with a training set 1,800 times larger
than the test set. However, the ability to label
an instance based only on its immediate context
means that there is a backoff method in the case of
unseen terms.
3 Experimental Results
3.1 Training, test, and seed word data
In order to train and test the model, we created
two corpora and a lexicon of event and non-event
seeds. The training corpus consisted of 156,000
newswire documents, ?100 million words, from
the Foreign Broadcast Information Service, Lexis
170
Nexis, and other online news archives. The cor-
pus was parsed using Janya?s information extrac-
tion application, Semantex, which creates both
shallow, non-recursive parsing structures and de-
pendency links, and all (wi, R,wj) statistics were
extracted as described in Section 2. From the
1.9 million patterns, (wi, R, ?) and (?, R,wj) ex-
tracted from the corpus, the 48,353 that appeared
more than 300 times were retained as features.
The test corpus was composed of 77 additional
documents (?56K words), overlapping in time
and content but not included in the training set.
These were annotated by hand to mark event nom-
inals. Specifically, every referential noun phrase
headed by a non-proper noun was considered
for whether it denoted an achievement, accom-
plishment, activity, or process (Parsons, 1990).
Noun heads denoting any of these were marked
as EVENT, and all others were left unmarked.
All documents were first marked by a junior an-
notator, and then a non-blind second pass was per-
formed by a senior annotator (first author). Sev-
eral semantic classes were difficult to annotate be-
cause they are particularly prone to coactivation,
including terms denoting financial acts, legal acts,
speech acts, and economic processes. In addition,
for terms like mission, plan, duty, tactic, policy,
it can be unclear whether they are hyponyms of
EVENT or another abstract concept. In every case,
however, the mention was labeled as an event or
non-event depending on whether its use in that
context appeared to be more or less event-like,
respectively. Tests for the ?event-y?ness of the
context included whether an unambiguous word
would be an acceptable substitute there (e.g. funds
[=only non-event] for expenditure [either]).
To create the test data, the annotated documents
were also parsed to automatically extract all com-
mon noun-headed NPs and the dependency triples
they instantiate. Those with heads that aligned
with the offsets of an event annotation were la-
beled as events; the remainder were labeled as
non-events. Because of parsing errors, about 10%
of annotated event instances were lost, that is re-
mained unlabeled or were labeled as non-events.
So, our results are based on the set of recover-
able event nominals as a subset of all common-
noun headed NPs that were extracted. In the
test corpus there were 9,381 candidate instances,
1,579 (17%) events and 7,802 (83%) non-events.
There were 2,319 unique term types; of these, 167
types (7%) appeared both as event tokens and non-
event tokens. Some sample ambiguous terms in-
clude: behavior, attempt, settlement, deal, viola-
tion, progress, sermon, expenditure.
We constructed two lexicons of nominals to use
as the seed terms. For events, we created a list of
95 terms, such as election, war, assassination, dis-
missal, primarily based on introspection combined
with some checks on individual terms in WordNet
and other dictionaries and using Google searches
to judge how ?event-y? the term was.
To create a list of non-events, we used WordNet
and the British National Corpus. First, from the
set of all lexemes that appear in only one synset
in WordNet, all nouns were extracted along with
the topmost hypernym they appear under. From
these we retained those that both appeared on a
lemmatized frequency list of the 6,318 words with
more than 800 occurrences in the whole 100M-
word BNC (Kilgarriff, 1997) and had one of the
hypernyms GROUP, PSYCHOLOGICAL, ENTITY,
POSSESSION. We also retained select terms from
the categories STATE and PHENOMENON were la-
beled non-event seeds. Examples of the 295 non-
event seeds are corpse, electronics, bureaucracy,
airport, cattle.
Of the 9,381 test instances, 641 (6.8%) had a
term that belonged to the seed list. With respect
to types, 137 (5.9%) of the 2,319 term types in the
test data also appeared on the seed lists.
3.2 Experiments
Experiments were performed to investigate the
performance of our models, both when using orig-
inal seed lists, and also when varying the content
of the seed lists using a bootstrapping technique
that relies on the probabilistic framework of the
model. A 1,000-instance subset of the 9,381 test
data instances was used as a validation set; the re-
maining 8,381 were used as evaluation data, on
which we report all results (with the exception of
Table 3 which is on the full test set).
EXP1: Results using original seed sets Prob-
abilistic models for non-events and events were
built from the full list of 295 non-event and 95
event seeds, respectively, as described above.
Table 1 (top half: original seed set) shows the
results over the 8,381 evaluation data instances
when using the three classification methods de-
scribed above: (i) word, (ii) context, and (iii)
word+context. The first row (ALL) reports scores
where all undecided responses are marked as in-
171
B
O
O
T
S
T
R
A
P
P
E
D
O
R
IG
IN
A
L
S
E
E
D
S
E
T
S
E
E
D
S
E
T
EVENT NONEVENT TOTAL AVERAGE
Input Vector Correct Acc (%) Att (%) Correct Acc (%) Att (%) Correct Acc (%) Att (%) Acc (%)
A
L
L
word 1236 87.7 100.0 4217 60.5 100.0 5453 65.1 100.0 74.1
context 627 44.5 100.0 2735 39.2 100.0 3362 40.1 100.0 41.9
word+context 1251 88.8 100.0 4226 60.6 100.0 5477 65.4 100.0 74.7
FA
IR
word 1236 89.3 98.3 4217 60.7 99.6 5453 65.5 99.4 75.0
context 627 69.4 64.2 2735 62.5 62.8 3362 63.6 63.0 65.9
word+context 1251 89.3 99.5 4226 60.7 99.9 5477 65.5 99.8 75.0
A
L
L
word 1110 78.8 100.0 5517 79.1 100.0 6627 79.1 100.0 79.0
context 561 39.8 100.0 2975 42.7 100.0 3536 42.2 100.0 41.3
word+context 1123 79.8 100.0 5539 79.4 100.0 6662 79.5 100.0 79.6
FA
IR
word 1110 80.2 98.3 5517 79.4 99.6 6627 79.5 99.4 79.8
context 561 62.1 64.2 2975 67.9 62.8 3536 66.9 63.0 65.0
word+context 1123 80.2 99.5 5539 79.5 99.9 6662 79.7 99.8 79.9
LEX 1 1114 79.1 100.0 5074 72.8 100.0 6188 73.8 100.0 75.9
total counts 1408 6973 8381
Table 1: (EXP1, EXP3) Accuracies of classifiers in terms of correct classifications, % correct, and % attempted (if allowed to
abstain), on the evaluation test set. (Row 1) Classifiers built from original seed set of size (295, 95); (Row 2) Classifiers built
from 15 iterations of bootstrapping; (Row 3) Classifier built from Lexicon 1. Accuracies in bold are those plotted in related
Figures 2, 3(a) and 3(b).
correct. In the second row (FAIR), undecided an-
swers (d = 0) are left out of the total, so the
number of correct answers stays the same, but the
percentage of correct answers increases.4 Scores
are measured in terms of accuracy on the EVENT
instances, accuracy on the NONEVENT instances,
TOTAL accuracy across all instances, and the sim-
ple AVERAGE of accuracies on non-events and
events (last column). The AVERAGE score as-
sumes that performance on non-events and events
is equally important to us.
?From EXP1, we see that the behavior of a term
across an entire corpus is a better source of infor-
mation about whether a particular instance of that
term refers to an event than its immediate context.
We can further infer that this is because the imme-
diate context only provides definitive evidence for
the models in 63.0% of cases; when the context
model is not penalized for indecision, its accuracy
improves considerably. Nonetheless, in combina-
tion with the word model, immediate context does
not appear to provide much additional information
over only the word. In other words, based only
on a term?s distribution in the past, one can make
a reasonable prediction about how it will be used
when it is seen again. Consequently, it seems that
a well-constructed, i.e. domain customized, lexi-
con can classify nearly as well as a method that
also takes context into account.
EXP2: Results on ACE 2005 event data In ad-
dition to using the data set created specifically for
this project, we also used a subset of the anno-
4Note that Att(%) does not change with bootstrapping?
an artifact of the sparsity of certain feature vectors in the
training and test data, and not the model?s constituents seeds.
Input Vector Acc (%) Att (%)
word 96.1 97.2
context 72.8 63.1
word+context 95.5 98.9
LEX 1 76.5 100.0
Table 2: (EXP2) Results on ACE event nominals: %correct
(accuracy) and %attempted, for our classifiers and LEX 1.
tated training data created for the ACE 2005 Event
Detection and Recognition (VDR) task. Because
only event mentions of specific types are marked
in the ACE data, only recall of ACE event nomi-
nals can be measured rather than overall recall of
event nominals and accuracy on non-event nom-
inals. Results on the 1,934 nominal mentions of
events (omitting cases of d = 0) are shown in Ta-
ble 2. The performance of the hand-crafted Lex-
icon 1 on the ACE data, described in Section 3.3
below, is also included.
The fact that our method performs somewhat
better on the ACE data than on our own data, while
the lexicon approach is worse (7 points higher
vs. 3 points lower, respectively) can likely be ex-
plained by the fact that in creating our introspec-
tive seed set for events, we consulted the annota-
tion manual for ACE event types and attempted
to include in our list any unambiguous seed terms
that fit those types.
EXP3: Increasing seed set via Bootstrapping
There are over 2,300 unlabeled vectors in the train-
ing data that correspond to the words that appear
as lexical heads in the test data. These unlabeled
training vectors can be powerfully leveraged us-
ing a simple bootstrapping algorithm to improve
the individual models for non-events and events,
as follows: Step 1: For each vector v in the unla-
beled portion of training data, row-sum normalize
172
100 1 5 10 15      LEX160
65
70
75
80
85
90 non?events
eventstotal
average
Figure 2: Accuracies vs. iterations of bootstrapping. Bold
symbols on left denote classifier built from initial (295, 95)
seeds; and bold (disconnected) symbols at right are LEX 1.
it to produce v? and compute a normalized mea-
sure of confidence of the algorithm?s prediction,
given by the magnitude of d(v?). Step 2: Add
those vectors most confidently classified as either
non-events or events to the seed set for non-events
or events, according to the sign of d(v?). Step 3:
Recalculate the model based on the new seed lists.
Step 4: Repeat Steps 1?3 until either no more un-
labeled vectors remain or the validation accuracy
no longer increases.
In our experiments we added vectors to each
model such that the ratio of the size of the seed
sets remained constant, i.e. 50 non-events and
16 events were added at each iteration. Using
our validation set, we determined that the boot-
strapping should stop after 15 iterations (despite
continuing for 21 iterations), at which point the
average accuracy leveled out and then began to
drop. After 15 iterations the seed set is of size
(295, 95)+(50, 16)?15 = (1045, 335). Figure 2
shows the change in the accuracy of the model as
it is bootstrapped through 15 iterations.
TOTAL accuracy improves with bootstrapping,
despite EVENT accuracy decreasing, because the
test data is heavily populated with non-events,
whose accuracy increases substantially. The AV-
ERAGE accuracy also increases, which proves that
bootstrapping is doing more than simply shifting
the bias of the classifier to the majority class. The
figure also shows that the final bootstrapped clas-
sifier comfortably outperforms Lexicon 1, impres-
sive because the lexicon contains at least 13 times
more terms than the seed lists.
EXP4: Bootstrapping with a reduced number
of seeds The size of the original seed lists were
chosen somewhat arbitrarily. In order to deter-
mine whether similar performance could be ob-
tained using fewer seeds, i.e. less human effort, we
experimented with reducing the size of the seed
lexicons used to initialize the bootstrapping.
To do this, we randomly selected a fixed frac-
tion, f%, of the (295, 95) available event and non-
event seeds, and built a classifier from this sub-
set of seeds (and discarded the remaining seeds).
We then bootstrapped the classifier?s models us-
ing the 4-step procedure described above, using
candidate seed vectors from the unlabeled train-
ing corpus, and incrementing the number of seeds
until the classifier consisted of (295, 95) seeds.
We then performed 15 additional bootstrapping it-
erations, each adding (50, 16) seeds. Since the
seeds making up the initial classifier are chosen
stochastically, we repeated this entire process 10
times and report in Figures 3(a) and 3(b) the mean
of the total and average accuracies for these 10
folds, respectively. Both plots have five traces,
with each trace corresponding the fraction f =
(20, 40, 60, 80, 100)% of labeled seeds used to
build the initial models. As a point of reference,
note that initializing with 100% of the seed lexicon
corresponds to the first point of the traces in Fig-
ure 2 (where the x-axis is marked with f =100%).
Interestingly, there is no discernible difference
in accuracy (total or average) for fractions f
greater than 20%. However, upon bootstrapping
we note the following trends. First, Figure 3(b)
shows that using a larger initial seed set increases
the maximum achievable accuracy, but this max-
imum occurs after a greater number bootstrap-
ping iterations; indeed the maximum for 100% is
achieved at 15 (or greater) iterations. This reflects
the difference in rigidity of the initial models, with
smaller initial models more easily misled by the
seeds added by bootstrapping. Second, the final
accuracies (total and average) are correlated with
the initial seed set size, which is intuitively satisfy-
ing. Third, it appears from Figure 3(a) that the to-
tal accuracy at the model size (295,95) (or 100%)
is in fact anti-correlated with the size of the ini-
tial seed set, with 20% performing best. This is
correct, but highlights the sometimes misleading
interpretation of the total accuracy: in this case
the model is defaulting to classifying anything as
a non-event (the majority class), and has a consid-
erably impoverished event model.
If one wants to do as well as Lexicon 1 after 15
iterations of bootstrapping then one needs at least
173
EVENT NONEVENT TOTAL AVERAGE
Corr (%) Corr (%) Corr (%) (%)
LEX 1 1256 79.5 5695 73.0 6951 74.1 76.3
LEX 2 1502 95.1 4495 57.6 5997 63.9 76.4
LEX 3 349 22.1 7220 92.5 7569 80.7 57.3
Total 1579 7802 9381
Table 3: Accuracy of several lexicons, showing number and
percentage of correct classifications on the full test set.
an initial seed set of size 60%. An alternative is
to perform fewer iterations, but here we see that
using 100% of the seeds comfortably achieves the
highest total and average accuracies anyway.
3.3 Comparison with existing lexicons
In order to compare our weakly-supervised proba-
bilistic method with a lexical lookup method based
on very large hand-created lexical resources, we
created three lexicons of event terms, which were
used as very simple classifiers of the test data. If
the test instance term belongs to the lexicon, it is
labeled EVENT; otherwise, it is labeled as NON-
EVENT. The results on the full test set using these
lexicons are shown in Table 3.
Lex 1 5,435 entries from NomLex (Macleod et
al., 1998), FrameNet(Baker et al, 1998), CELEX
(CEL, 1993), Timebank(Day et al, 2003).
Lex 2 13,659 entries from WordNet 2.0 hypernym
classes EVENT, ACT, PROCESS, COGNITIVE PRO-
CESS, & COMMUNICATION combined with Lex 1.
Lex 3 Combination of pre-existing lexicons in the
information extraction application from WordNet,
Oxford Advanced Learner?s Dictionary, etc.
As shown in Tables 1 and 3, the relatively
knowledge-poor method developed here using
around 400 seeds performs well compared to the
use of the much larger lexicons. For the task of
detecting nominal events, using Lexicon 1 might
be the quickest practical solution. In terms of ex-
tensibility to other semantic classes, domains, or
languages lacking appropriate existing lexical re-
sources, the advantage of our trainable method is
clear. The primary requirement of this method is
a dependency parser and a system user-developer
who can provide a set of seeds for a class of in-
terest and its complement. It should be possi-
ble in the next few years to create a dependency
parser for a language with no existing linguistic re-
sources (Klein and Manning, 2002). Rather than
having to spend the considerable person-years it
takes to create resources like FrameNet, CELEX,
and WordNet, a better alternative will be to use
weakly-supervised semantic labelers like the one
described here.
4 Related Work
In recent years an array of new approaches have
been developed using weakly-supervised tech-
niques to train classifiers or learn lexical classes
or synonyms, e.g. (Mihalcea, 2003; Riloff and
Wiebe, 2003). Several approaches make use of de-
pendency triples (Lin, 1998; Gorman and Curran,
2005). Our vector representation of the behavior
of a word type across all its instances in a corpus is
based on Lin (1998)?s DESCRIPTION OF A WORD.
Yarowsky (1995) uses a conceptually similar
technique for WSD that learns from a small set of
seed examples and then increases recall by boot-
strapping, evaluated on 12 idiosyncratically poly-
semous words. In that task, often a single disam-
biguating feature can be found in the context of a
polysemous word instance, motivating his use of
the decision list algorithm. In contrast, the goal
here is to learn how event-like or non-event-like
a set of contextual features together are. We do
not expect that many individual features correlate
unambiguously with references to events (or non-
events), only that the presence of certain features
make an event interpretation more or less likely.
This justifies our probabilistic Bayesian approach,
which performs well given its simplicity.
Thelen and Riloff (2002) use a bootstrapping al-
gorithm to learn semantic lexicons of nouns for
six semantic categories, one of which is EVENTS.
For events, only 27% of the 1,000 learned words
are correct. Their experiments were on a much
smaller scale, however, using the 1,700 document
MUC-4 data as a training corpus and using only
10 seeds per category.
Most prior work on event nominals does not try
to classify them as events or non-events, but in-
stead focuses on labeling the argument roles based
on extrapolating information about the argument
structure of the verbal root (Dahl et al, 1987; La-
pata, 2002; Pradhan et al, 2004). Meyers, et al
(1998) describe how to extend a tool for extrac-
tion of verb-based events to corresponding nomi-
nalizations. Hull and Gomez (1996) design a set
of rule-based algorithms to determine the sense of
a nominalization and identify its arguments.
5 Conclusions
We have developed a novel algorithm for label-
ing nominals as events that combines WSD and
lexical acquisition. After automatically bootstrap-
ping the seed set, it performs better than static lex-
icons many times the original seed set size. Also,
174
further bootstrap iterations?? initial seed setfraction (%) ?
20 40 60 80 100 1 5 10 15      LEX164
66
68
70
72
74
76
78
80
82
20%40%60%80%100%
(a) Total Accuracy
further bootstrap iterations?
?
initial seed set
fraction (%) ?
20 40 60 80 100 1 5 10 15      LEX164
66
68
70
72
74
76
78
80
82
20%40%60%80%100%
(b) Average Accuracy
Figure 3: Accuracies of classifiers built from different-sized initial seed sets, and then bootstrapped onwards to the equivalent
of 15 iterations as before. Total (a) and Average (b) accuracies highlight different aspects of the bootstrapping mechanism.
Just as in Figure 2, the initial model is denoted with a bold symbol in the left part of the plot. Also for reference the relevant
Lexicon 1 accuracy (LEX 1) is denoted with a ? at the far right.
it is more robust than lexical lookup as it can also
classify unknown words based on their immediate
context and can remain agnostic in the absence of
sufficient evidence.
Future directions for this work include applying
it to other semantic labeling tasks and to domains
other than general news. An important unresolved
issue is the difficulty of formulating an appropriate
seed set to give good coverage of the complement
of the class to be labeled without the use of a re-
source like WordNet.
References
C. Aone and M. Ramos-Santacruz. 2000. REES: A
large-scale relation and event extraction system. In
6th ANLP, pages 79?83.
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The
Berkeley FrameNet project. In Proc. COLING-ACL.
Centre of Lexical Information, Nijmegen, 1993.
CELEX English database, E25, online edition.
A. Copestake and T. Briscoe. 1995. Semi-productive
polysemy and sense extension. Journal of Seman-
tics, 12:15?67.
D. Dahl, M. Palmer, and R. Passonneau. 1987. Nomi-
nalizations in PUNDIT. In Proc. of the 25th ACL.
D. Day, L. Ferro, R. Gaizauskas, P. Hanks, M. Lazo,
J. Pustejovsky, R. Sauri, A. See, A. Setzer, and
B. Sundheim. 2003. The TIMEBANK corpus. In
Corpus Linguistics 2003, Lancaster UK.
J. Gorman and J. Curran. 2005. Approximate search-
ing for distributional similarity. In Proc. of the
ACL-SIGLEX Workshop on Deep Lexical Acquisi-
tion, pages 97?104.
R. Hull and F. Gomez. 1996. Semantic interpretation
of nominalizations. In Proc. of the 13th National
Conf. on Artificial Intelligence, pages 1062?1068.
A. Kilgarriff. 1997. Putting frequencies in the dictio-
nary. Int?l J. of Lexicography, 10(2):135?155.
D. Klein and C. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proc. of the 40th ACL.
M. Lapata. 2002. The disambiguation of nominalisa-
tions. Computational Linguistics, 28(3):357?388.
D. K. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. of COLING-ACL ?98.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and
R. Reeves. 1998. NOMLEX: A lexicon of nominal-
izations. In Proc. of EURALEX?98.
A. Meyers, C. Macleod, R. Yangarber, R. Grishman,
L. Barrett, and R. Reeves. 1998. Using NOMLEX
to produce nominalization patterns for information
extraction. In Proc. of the COLING-ACL Workshop
on the Computational Treatment of Nominals.
R. Mihalcea. 2003. Unsupervised natural language
disambiguation using non-ambiguous words. In
Proc. of Recent Advances in Natural Language Pro-
cessing, pages 387?396.
T. Parsons. 1990. Events in the Semantics of English.
MIT Press, Boston.
S. Pradhan, H. Sun, W. Ward, J. Martin, and D. Juraf-
sky. 2004. Parsing arguments of nominalizations in
English and Chinese. In Proc. of HLT-NAACL.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proc. EMNLP.
H. Schu?tze. 1998. Automatic word sense disambigua-
tion. Computational Linguistics, 24(1):97?124.
M. Thelen and E. Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extrac-
tion pattern contexts. In Proc. of EMNLP.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. of
the 33rd ACL, pages 189?196.
175
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 797?805,
Beijing, August 2010
Using Cross-Lingual Projections to Generate Semantic Role  
Labeled Corpus for Urdu - A Resource Poor Language 
Smruthi Mukund 
CEDAR 
University at Buffalo 
smukund@buffalo.edu 
Debanjan Ghosh 
Thomson Reuters R&D 
debanjan.ghosh@ 
thomsonreuters.com 
Rohini K. Srihari 
CEDAR 
University at Buffalo 
rohini@cedar.buffalo.edu 
 
Abstract 
In this paper we explore the possibility of 
using cross lingual projections that help 
to automatically induce role-semantic 
annotations in the PropBank paradigm 
for Urdu, a resource poor language. This 
technique provides annotation projections 
based on word alignments. It is relatively 
inexpensive and has the potential to re-
duce human effort involved in creating 
semantic role resources. The projection 
model exploits lexical as well as syntac-
tic information on an English-Urdu paral-
lel corpus. We show that our method ge-
nerates reasonably good annotations with 
an accuracy of 92% on short structured 
sentences. Using the automatically gen-
erated annotated corpus, we conduct pre-
liminary experiments to create a semantic 
role labeler for Urdu. The results of the 
labeler though modest, are promising and 
indicate the potential of our technique to 
generate large scale annotations for Urdu.  
1 Introduction 
Semantic Roles (also known as thematic roles) 
help to understand the semantic structure of a 
document (Fillmore, 1968). At a fundamental 
level, they help to capture the similarities and 
differences in the meaning of verbs via the ar-
guments they define by generalizing over surface 
syntactic configurations.  In turn, these roles aid 
in domain independent understanding as the se-
mantic frames and semantic understanding sys-
tems do not depend on the syntactic configura-
tion for each new application domain. Identify-
ing semantic roles benefit several language 
processing tasks - information extraction (Sur-
deanu et al, 2003), text categorization (Moschitti, 
2008) and finding relations in textual entailment 
(Burchardt and Frank 2006). 
Automatically identifying semantic roles is of-
ten referred to as shallow semantic parsing (Gil-
dea and Jurafsky, 2002). For English, this 
process is facilitated by the existence of two 
main SRL annotated corpora ? FrameNet (Baker 
et al, 1998) and PropBank (Palmer et al, 2005). 
Both datasets mark almost all surface realizations 
of semantic roles. FrameNet has 800 semantic 
frames that cover 120,000 example sentences1. 
PropBank has annotations that cover over 
113,000 predicate-argument structures. Clearly 
English is well supported with resources for se-
mantic roles. However, there are other widely 
spoken resource poor languages that are not as 
privileged. The PropBank based resources avail-
able for languages like Chinese (Xue and Palmer, 
2009), Korean (Palmer et al, 2006) and Spanish 
(Taule, 2008) are only about two-thirds the size 
of the English PropBank.  
Several alternative techniques have been ex-
plored in the literature to generate semantic role 
labeled corpora for resource poor languages as 
providing manually annotated data is time con-
suming and involves intense human labor. Am-
bati and Chen (2007) have conducted an exten-
sive survey and outlined the benefits of using 
parallel corpora to transfer annotations. A wide 
range of annotations from part of speech (Hi and 
Hwa, 2005) and chunks (Yarowsky et al, 2001) 
to word senses (Diab and Resnik, 2002), depen-
dencies (Hwa et al, 2002) and semantic roles 
(Pado and Lapata, 2009) have been successfully 
transferred between languages. FrameNet style 
annotations in Chinese is obtained by mapping 
English FrameNet entries directly to concepts 
listed in HowNet2 (online ontology for Chinese) 
with an accuracy of 68% (Fung and Chen, 2004). 
                                                 
1 Wikipedia - http://en.wikipedia.org/wiki/PropBank 
2 http://www.keenage.com/html/e_index.html 
797
Fung et al (2007) analyze an automatically an-
notated English-Chinese parallel corpus and 
show high cross-lingual agreement for PropBank 
roles (range of 75%-95% based on the roles).  
In this paper we explore the possibility of us-
ing English-Urdu parallel corpora to generate 
SRL annotations for Urdu, a less commonly 
taught language (LCTL). Earlier attempts to gen-
erate SRL corpora using annotation projections 
have been for languages such as German, French 
(Pado and Lapata, 2009) and Italian (Moschitti, 
2009) that have high vocabulary overlap with 
English. Also, German belongs to the same lan-
guage family as English (Germanic family). Ur-
du on the other hand is an Indic language that is 
grammatically very different and shares almost 
no vocabulary with English.  
The technique of cross lingual projections war-
rants good BLEU score that ensures correct word 
alignments. According to NIST 2008 Open Ma-
chine Translation challenge 3 , a 0.2280 best 
BLEU score was achieved for Urdu to English 
translation. This is comparable to the BLEU 
scores achieved for German to English ? 0.253 
and French to English ? 0.3 (Koehn, 2005). But, 
for SRL transfer, perfect word alignment is not 
mandatory as SRL requires semantic correspon-
dence only. According to Fillmore (1982) se-
mantic frames are based on conceptual structures. 
They are generalizations over surface structures 
and hence less prone to syntactic variations. 
Since English and Urdu have a reasonable se-
mantic correspondence (Example 3), we believe 
that the projections when capped with a post 
processing step will considerably reduce the 
noise induced by inaccurate alignments and pro-
duce acceptable mappings. 
Hindi is syntactically similar to Urdu. These 
languages are standardized forms of Hindustani. 
They are free word order languages and follow a 
general SOV (Subject-Object-Verb) structure. 
Projection approach has been used by (Mukerjee 
et al, 2006) and (Sinha, 2009) to transfer verb 
predicates from English onto Hindi. Sinha (2009) 
achieves a 90% F-Measure in verb predicate 
transfer from English to Hindi. This shows that 
using cross lingual transfer approach to obtain 
semantic annotations for Urdu from English is an 
idea worth exploring. 
                                                 
3http://www.itl.nist.gov/iaui/894.01/tests/mt/2008/doc/mt08
_official_results_v0.html 
1.1 Approach 
Our approach leverages existing English 
PropBank annotations provided via the SemLink4 
corpus. SemLink provides annotations for 
VerbNet using the pb (PropBank) attribute. By 
using English-Urdu parallel corpus we acquire 
verb predicates and their arguments. When we 
transfer verb predicates (lemmas), we also 
transfer pb attributes. We obtain annotation 
projections from the parallel corpora as follows:  
1. Take a pair of sentences E (in English) and U 
(in Urdu) that are translations of each other.  
2. Annotate E with semantic roles. 
3. Project the annotations from E onto U using 
word alignment information, lexical 
information and linguistic rules that involve 
syntactic information. 
There are several challenges to the annotation 
projection technique. Dorr (1994) presents some 
major lexical-semantic divergence problems 
applicable in this scenario:  
(a) Thematic Divergence - In some cases, al-
though there exists semantic parallelism, the 
theme of the English sentence captured in 
the subject changes into an object in the Ur-
du sentence (Example 1). 
(b) Conflatational Divergence - Sometimes tar-
get translations spans over a group of words 
(Example 1: plays is mapped to kirdar ada). 
Trying to ascertain this word span for se-
mantic roles is difficult as the alignments 
can be incomplete and very noisy. 
(c) Demotional divergence and Structural di-
vergence - Despite semantic relatedness, in 
some sentence pairs, alignments obtained 
from simple projections generate random 
matchings as the usage is syntactically dis-
similar (Example 2). 
Handling all challenges adds complexity to our 
model. The heuristic rules that we implement are 
guided by linguistic knowledge of Urdu. This 
increases the effectiveness of the alignments. 
 
Example 1: 
I 
(subject) 
am Angry at Reheem 
(object) 
 
Raheem 
(subject)  
mujhe 
(object) 
Gussa dilate hai 
(Raheem brings anger in me) 
                                                 
4 http://verbs.colorado.edu/semlink/ 
798
Example 2: (noun phrase to prepositional pharse) 
Ali attended work today 
 
Ali aaj daftar mein haazir tha 
(Ali was present at work today) 
2 Generating Parallel Corpora 
PropBank provides SRL annotated corpora for 
English. It uses predicate independent labels 
(ARG0, ARG1, etc.) which indicate how a verb 
relates to its arguments. The argument types are 
consistent across all uses of a single verb and do 
not consider the sense of the verb. We use the 
PropBank annotations provided for the Wall 
Street Journal (WSJ) part of the Penn Tree bank 
corpus (Marcus et al, 2004). The arguments of a 
verb are labeled sequentially from ARG0 to 
ARG5 where ARG0 is the proto-typical Agent, 
ARG1 is the proto-typical patient, ARG2 is the 
recipient, and so on. There are other adjunct tags 
in the dataset that are indicated by ARGM that 
include tags for location (ARGM-LOC), tempor-
al tags (ARGM-TMP) etc.  
An Urdu corpus of 6000 sentences corres-
ponding to 317 WSJ articles of Penn Tree Bank 
corpus is provided by CRULP5 (used in the NIST 
2008 machine translation task). We consider 
2350 English sentences with PropBank annota-
tions that have corresponding Urdu translations 
(CRULP corpus) for our experiments. 
2.1 Sentence Alignment 
Sentence alignment is a prerequisite for any pa-
rallel corpora processing. As the first step, we 
had to generate a perfect sentence aligned paral-
lel corpus as the translated sentences, despite 
belonging to the same domain (WSJ ? Penn tree 
bank), had several errors in demarcating the sen-
tence boundaries.  
Sentence alignment between English and Urdu 
is achieved over two iterations. In the first itera-
tion, the length of each sentence is calculated 
based on the occurrence of words belonging to 
important part of speech categories such as prop-
er nouns, adjectives and verbs. Considering main 
POS categories for length assessment helps over-
come the conflatational divergence issue. For 
each English sentence, Urdu sentences with the 
same length are considered to be probable candi-
                                                 
5http://www.crulp.org/ 
dates for alignment. In the second iteration, an 
Urdu-English lexicon is used on the Urdu corpus 
and English translations are obtained. An Eng-
lish-Urdu sentence pair with maximum lexical 
match is considered to be sentence aligned.  
Clearly this method is highly dependent on the 
existence of an exhaustive Urdu-English dictio-
nary. The lexicons that we use to perform loo-
kups are collected by mining Wikipedia and oth-
er online resources (Mukund et al, 2010). How-
ever, lexicon lookups will fail for Out-Of-
Vocabulary words. There could also be a colli-
sion if Urdu sentences have English transliterated 
words (Example 3, ?office?). Such errors are 
manually verified for correctness. 
 
Example 3: 
Kya  aaj tum office gaye ? 
 
Did you go to the office today ? 
2.2 Word Alignment 
In the case of generating word alignments it is 
beneficial to calculate alignments in both transla-
tion directions (English ? Urdu and Urdu - Eng-
lish). This nature of symmetry will help to re-
duce alignment errors. We use the Berkeley 
Aligner6 word alignment package which imple-
ments a joint training model with posterior de-
coding (Liang et al, 2006) to consider bidirec-
tional alignments. Predictions are made based on 
the agreements obtained by two bidirectional 
models in the training phase. The intuitive objec-
tive function that incorporates data likelihood 
and a measure of agreement between the models 
is maximized using an EM-like algorithm. This 
alignment model is known to provide 29% re-
duction in AER over IBM model 4 predictions.  
On our data set the word alignment accuracy 
is 71.3% (calculated over 200 sentence pairs). In 
order to augment the alignment accuracy, we 
added 3000 Urdu-English words and phrases ob-
tained from the Urdu-English dictionary to our 
parallel corpus. The alignment accuracy im-
proved by 3% as the lexicon affects the word co-
occurrence count. 
Word alignment in itself does not produce ac-
curate semantic role projections from English to 
Urdu. This is because the verb predicates in Urdu 
can span more than one token. Semantic roles 
                                                 
6 http://nlp.cs.berkeley.edu/Main.html 
799
can cover sentential constituents of arbitrary 
length, and simply using word alignments for 
projection is likely to result in wrong role spans. 
Also, alignments are not obtained for all words. 
This could lead to missing projections. 
One way to correct these alignment errors is to 
devise token based heuristic rules. This is not 
very beneficial as writing generic rules is diffi-
cult and different errors demand specific rules. 
We propose a method that considers POS, tense 
and chunk information along with word align-
ments to project annotations. 
 
 
Figure 1: Projection model 
 
Our proposed approach can be explained in 
two stages as shown in figure 1. In Stage 1 only 
verb predicates are transferred from English to 
Urdu. Stage 2 involves transfer of arguments and 
depends on the output of Stage 1. Predicate 
transfer cannot rely entirely on word alignments 
(?3). Rules devised around the chunk boundaries 
boost the verb predicate recognition rate. 
Any verb group sequence consisting of a main 
verb and its auxiliaries are marked as a verb 
chunk. Urdu data is tagged using the chunk tag 
set proposed exclusively for Indian languages by 
Bharati et al, (2006). Table 1 shows the tags that 
are important for this task. 
 
Verb Chunk Description 
VGF Verb group is finite  (decided by the auxiliaries) 
VGNF Verb group for non-finite adverbial and adjectival chunk 
VGNN Verb group has a gerund 
Table 1: Verb chunk tags in Urdu 
The sentence aligned parallel corpora that we 
feed as input to our model is POS tagged for both 
English and Urdu. Urdu data is also tagged for 
chunk boundaries and morphological features 
like tense, gender and number information.  
Named Entities are also marked on the Urdu data 
set as they help in tagging the ARGM arguments. 
All the NLP taggers (POS, NE, Chunker, and 
Morphological Analyzer) used in this work are 
detailed in Mukund et al, (2010). 
English data is not chunked using a conven-
tional chunk tagger. Each English sentence is 
split into virtual phrases at boundaries deter-
mined by the following parts of speech ? IN, TO, 
MD, POS, CC, DT, SYM,: (Penn Tree Bank tag-
set). These tags represent positions in a sentence 
that typically mark context transitions (they are 
mostly the closed class words). We show later 
how these approximate chunks assist in correct-
ing predicate mappings. 
We use an Urdu-English dictionary (?2.1) that 
assigns English meanings to Urdu words in each 
sentence. Using translation information from a 
dictionary can help transfer verb predicates when 
the translation equivalent preserves the lexical 
meaning of the source language.  
The first rule that gets applied for predicate 
transfer is based on lexicon lookup. If the Eng-
lish verb is found to be a synonym to an Urdu 
word that is part of a verb chunk, then the lemma 
associated with the English word is transferred to 
the entire verb chunk in Urdu. However not all 
translations? equivalents are lexically synonym-
ous. Sometimes the word used in Urdu is differ-
ent in meaning to that in English but relevant in 
the context (lexical divergence).  
The word alignments considered in proximity 
to the approximate English chunks come to res-
cue in such scenarios. Here, for all the words 
occurring in each Urdu verb chunk, correspond-
ing English aligned words are found from the 
word alignments. If the words that are found be-
long to the same approximate English chunk, 
then the verb predicate of that chunk (if present) 
is projected onto the verb chunk in Urdu. This 
heuristic technique increases the verb projection 
accuracy by about 15% as shown in ?4. 
The Penn tree bank tag set for English part of 
speech has different tags for verbs based on the 
tense information. VBD is used to indicate past 
tense, and VBP and VBZ for present tense. Urdu 
800
also has the tense information associated with the 
verbs in some cases. We exploit this similarity to 
project the verb predicates from English onto 
Urdu. 
The adverbial chunk in Urdu includes pure ad-
verbial phrases. These chunks also form part of 
the verb predicates.  
   S 
 
 
RBP          NP                        VGNF 
 
RB         NN   VB     AUXA    
(??????/dobara)     (???/jaan)  (????/dali)        (???/gayi) 
[English meaning ? Revitalized] 
Figure 2: example for demotional divergence 
 
E.g. consider the English word ?revitalized? 
(figure 2). This is tagged VBD. However, the Ur-
du equivalent of this word is ??????? ??? ???? ???? 
(dobara jaan daali gayi ~ to put life in again). 
The POS tags are ?RB, NN, VB, AUXA? (adverb, 
noun, verb, aspectual auxiliary). The word ?do-
bara? is a part of the adverbial chunk RBP and 
the infinite verb chunk VGNF spans across the 
last two words ?daali gayi?. ?jaan? is a noun 
chunk. This kind of demotional divergence is 
commonly observed in languages like Hindi and 
Urdu. In order to consider this entire phrase to be 
the Urdu equivalent representation of the English 
word ?revitalized?, a rule for adverbial chunk is 
included as the last step to account for un-
accommodated English verbs in the projections. 
In the PropBank corpus, predicate argument re-
lations are marked for almost all occurrences of 
non-copula verbs. We however do not have POS 
tags that help to identify non-copula words. 
Words that can be auxiliary verbs occur as non-
copula verbs in Urdu. We maintain a list of such 
auxiliary verbs. When the verb chunk in Urdu 
contains only one word and belongs to the list, 
we simply ignore the verb chunk and proceed to 
the next chunk. This avoids several false posi-
tives in verb projections.  
Stage 2 of the model includes the transfer of 
arguments. In order to see how well our method 
works, we project all argument annotations from 
English onto Urdu. We do not consider word 
alignments for arguments with proper nouns. The 
double metaphone algorithm (Philips 2000) is 
applied on both English NNP (proper noun) 
tagged words as well as English transliterated 
Urdu (NNP) tagged words. Arguments from 
English are mapped onto Urdu for word pairs 
with the same metaphone code. 
For other arguments, we consider word align-
ments in proximity to verb predicates. The argu-
ment boundaries are determined based on chunk 
and POS information. We observe that our me-
thod projects the annotations associated with 
nouns fairly well. However, when the arguments 
contain adjectives, the boundaries are disconti-
nuous. In such cases, we consider the entire 
chunk without the case marker as a probable 
candidate for the projected argument. We also 
have some issues with the ARGM-MOD argu-
ments in that they overlap with the verb predi-
cates. When the verb predicate that it overlaps 
with is a complex predicate, we consider the en-
tire verb chunk to be the Urdu equivalent candi-
date argument. These rules along with word 
alignments yield fairly accurate projections.  
The rules that we propose are dependent on the 
POS, chunk and tense information that are lan-
guage specific. Hence our method is language 
independent only to the extent that the new lan-
guage considered should have similar syntactic 
structure as Urdu. Indic languages fall in this 
category. 
3 Verb Predicates 
Detecting verb predicates can be a challenging 
task especially if very reliable and efficient tools 
such as POS tagger and chunkers are not availa-
ble. We apply the POS tagger (CRULP tagset, 
88% F-Score) and Chunker (Hindi tagset, 90% 
F-Score) provided by Mukund et al, (2010) on 
the Urdu data set and show that syntactic infor-
mation helps to compensate alignment errors. 
Stanford POS tagger7 (Penn Tree bank tagset) is 
applied on the English data set. 
Predicates can be simple predicates that lie 
within the chunk boundary or complex predicates 
when they span across chunk boundaries. When 
verbs in English are expressed in Urdu/Hindi, in 
several cases, more than one word is used to 
achieve perfect translation. In English the tense 
of the verb is mostly captured by the verb mor-
pheme such as ?asked? ?said? ?saying?. In Ur-
du the tense is mostly captured by the auxiliary 
verbs. So a single word English verb such as 
?talking? would be translated into two words 
                                                 
7 http://nlp.stanford.edu/software/tagger.shtml 
801
?batein karna? where ?karna?~ do is the aux-
iliary verb. However this cannot be generalized 
as there are instances when translations are word 
to word. E.g. ?said? is mapped to a single word 
Urdu verb ?kaha?. 
Complex predicates in Urdu can occur in the 
following POS combinations. /oun+Verb, Ad-
jective+Verb, Verb+Verb, Adverb+Verb. Table 2 
lists the main verb tags present in the Urdu POS 
tagset. (refer Penn Tree bank POS tagset for 
English tags). 
 
Urdu Tags Description 
VB Verb 
VBI Infinitive Verb 
VBL Light Verb 
VBLI Infinitive Light Verb 
VBT Verb to be 
AUXA Aspectual Auxiliary 
AUXT Tense Auxiliary 
Table 2: Verb tags 
 
Auxiliary verbs in Urdu occur alongside VB, 
VBI, VBL or VBLI tags. Sinha (2009) defines 
complex predicates as a group of words consist-
ing of a noun (NN/NNP), an adjective (JJ), a verb 
(VB) or an adverb (RB) followed by a light verb 
(VBL/VBLI). Light verbs are those which contri-
bute to the tense and agreement of the verb (Butt 
and Geuder, 2001). However, despite the exis-
tence of a light verb tag, it is noticed that in sev-
eral sentences, verbs followed by auxiliary verbs 
need to be grouped as a single predicate. Hence, 
we consider such combinations as belonging to 
the complex predicate category.  
E1G- According_VBG to_TO some_DT estimates_NNS 
the_DT rule_NN changes_NNS would_MD cut_VB insid-
er_NN filings_NNS by_IN more_JJR than_IN a_DT 
third_JJ 
URD- [Kuch_QN  andaazon_NN  ke_CM  muta-
biq_NNCM]_NP [kanoon_NN mein_CM]_NP [tabdee-
liayan_NN]_NP[ androni_JJ    drjbndywn_NN  
ko_CM]_NP [ayk_CD thayiy_FR se_CM]_NP [zyada_I 
kam_JJ]_JJP [karey_VBL gi_AUXT]_VGF 
Example 4 
Example 4 demonstrates the existence of a light 
verb in a complex predicate. The English verb 
?cut? is mapped to ??? ???? ??? (kam karey gi) 
belonging to the VBF chunk group.  
EG- Rolls_NNP -_: Royce_NNP Motor_NNP 
Cars_NNPS Inc._NNP said_VBD it_PRP expects_VBZ 
its_PRP$ U.S._NNP sales_NNS to_TO remain_VB 
steady_JJ at_IN about_IN 1 200_CD cars_NNS in_IN 
1990_CD 
URD - [Rolls  Royce motor car inc_NNPC ne_CM]_NP 
[kaha_VB]_VBNF [wo_PRP]_NP [apney_PRRFP$]_NP 
[U.S._NNP ki_CM]_NP [ frwKt_NN ko_CM]_NP 
[1990_CD mein_CM]_NP [takreeban_RB]_RBP [1200_CD 
karon_NN par_CM]_NP [mtwazn_JJ]_JJP [rakhne_VBI 
ki_CM]_VGNN [tawaqqo_NN]_NP [karte_VB 
hai_AUXT]_VGF 
Example 5 
 
In example 5, ?said? corresponds to one Urdu 
word ?????(kaha) that also captures the tense in-
formation (past). However, consider the verb 
?expects?. This is a clear case of noun-verb 
complex predicate where ?expects? is mapped to 
????? ???? ???(tawaqqo karte hai). 
E1G- /ot_RB all_PDT those_DT who_WP wrote_VBD 
oppose_VBP the_DT changes_NNS  
URD -wo tamaam  jinhon ne likha tabdeeliyon ke [mukha-
lif_JJ]_JJP [nahi_RB]_RBP [hain_VBT]_VGF 
Example 6 
 
In example 6, verb predicates are ?wrote? and 
?oppose?. Consider the word ?oppose?. There 
are two ways of representing this word in Urdu. 
As a verb chunk the translation would be ?muk-
halifat nahi karte? and as an adjectival chunk 
?mukhalif nahi hai?. The latter form of repre-
sentation is used widely in the available transla-
tion corpus. The Urdu equivalent of ?oppose? is 
?????? ????(mukhalif hai). 
Another interesting observation in example 6 is 
the existence of discontinuous predicates. 
Though ?oppose? is one word in English, the 
Urdu representation has two words that do not 
occur together. The adverb ?nahi? ~?not? oc-
curs between the adjective and the verb. Statisti-
cally dealing with this issue is extremely chal-
lenging and affects the boundaries of other ar-
guments. Generalizing the rules needed to identi-
fy discontinuous predicates requires more de-
tailed analysis of the corpus ? from the linguistic 
aspect ? and has not been attempted in this paper. 
We however map ? ??? ???? ????? ?(mukhalif nahi 
hai) to the predicate ?oppose?. ?nahi? is treated 
as an argument ARG_NEG in PropBank. 
4 Projection Results 
It is impossible for us to report our projection 
results on the entire data set as we do not have it 
manually annotated. For the purpose of evalua-
tion, we manually annotated 100 long sentences 
(L) and 100 short sentences (S) from the full 
2350 sentence set. All the results are reported on 
802
this 200 set of sentences. Set L has sentences that 
each has more than two verb predicates and sev-
eral arguments. The number of words per sen-
tence here is greater than 55.  S; on the other 
hand has sentences with about 40 words each and 
no complex SOV structures. 
The results shown in Table 3 are for all tags 
(verbs+args) that are projected from English onto 
Urdu. In order to understand why the perfor-
mance over L dips, consider the results in Table 
4 that are for verb projections only. Some long 
sentences in English have Urdu translations that 
do not maintain the same structure. For example 
an English phrase ? ?? might prompt individu-
als to get out of stocks altogether? is written in 
Urdu in a way that the English representation 
would be ?what makes individuals to get out of 
stocks is ??. The Urdu equivalent word for 
?prompt? is missing and the associated lemma 
gets assigned to the Urdu equivalent of ?get? 
(the next lemma). This also affects the argument 
projections. Another reason is the effect of word 
alignments itself. Clearly longer sentences have 
greater alignment errors. 
All tags8 100 long sentences 
100 short 
sentences 
Actual Tags 1267 372 
Correct Tags 943 325 
Found Tags 1212 353 
L :  Precision 77.8% Recall 74.4% F-Score 76% 
S:  Precision 92% Recall 87.4% F-Score 89.7% 
Table 3: when all tags are considered 
 
Comparing the results of Table 4 to Table 3, 
we see that argument projections affect the re-
call. This is because the projections of arguments 
depend not only on the word alignments but also 
on the verb predicates. Incorrect verb predicates 
affect the argument projections. 
Only lemma 100 long sentences 
100 short 
sentences 
Actual Tags 670 240 
Correct Tags 490 208 
Overall Tags 720 257 
L: Precision 68% Recall 73.1% F-Score 70.45% 
S : Precision 80.9% Recall 86.6% F-Score 83.65% 
Table 4: for verb projections only 
Table 5 summarizes the results obtained when 
only the word alignments are considered to 
                                                 
8 Tags -  lemma (verb predicates) + arguments, Actual tags 
? number of tags in the English set, Found tags ? number of 
tags transferred to Urdu, Correct Tags ? number of tags 
correctly transferred 
project all tags. But when virtual phrase bounda-
ries in English are also considered, the F-score 
improves by 8% (Table 6). This is because vir-
tual boundaries in a way mark context switch and 
when considered in proximity to the word align-
ments yield better predicate boundaries. 
100 long sentences : only alignments 
Actual Tags 1267 
Correct Tags 617 
Overall Tags 782 
Precision 78.9% Recall 48.7% F-Score 60.2% 
Table 5: with only word alignments  
 
100 long sentences : alignments + virtual boundaries 
Actual Tags 1267 
Correct Tags 792 
Overall Tags 1044 
Precision 75.8% Recall  62.5% F-Score 68.5% 
Table 6: with word alignments and virtual boundaries 
 
100 
Sentences 
ARG
0 
ARG
1 
ARG
2 
ARG
3 
ARG
M 
Long 124 271 67 25 140 
Found 111 203 36 12 114 
P % 89.5 74.9 53.7 48 81.42 
Short 34 47 4 2 19 
Found 30 45 4 2 19 
P % 88.2 95.7 100 100 100 
Table 7: results of argument projections 
Precision (P) on arguments 
 
Table 7 shows the results of argument projec-
tions over the first 4 arguments of PropBank ? 
ARG0, ARG1, ARG2 and ARG3 (out of 24 argu-
ments, majority are sparse in our test set) and the 
adjunct tag set ARGM.  
5 Automatic Detection 
The size of SRL annotated corpus generated for 
Urdu is limited with only 2350 sentences. To 
explore the possibilities of augmenting this data 
set, we train verb predicate and argument detec-
tion models. The results show great promise in 
generating large-scale automatic annotations. 
5.1 Verb Predicate Detection 
Verb predicate detection happens in two stag-
es. In the first stage, the predicate boundaries are 
marked using a CRF (Lafferty et al, 2001) based 
sequence labeling approach. The training data for 
the model is generated by annotating the auto-
matically annotated Urdu SRL corpus using BI 
803
annotations. E.g. kam B-VG, karne par I-VG. The 
non-verb predicates are labeled ?-1?. The model 
uses POS, chunk and lexical information as fea-
tures. We report the results on a set of 77 sen-
tences containing a mix of short and long sen-
tences.  
Number of verb predicates correctly marked 377 
Num of verb predicates found 484 
Actual num of verb predicates 451 
Precision 77.8% Recall 83.5% F-Score 80.54% 
Table 8: CRF results for verb boundaries 
Every verb predicate is associated with a lemma 
mapped from the English VerbNet map file9. E.g. 
the Urdu verb ???  ????  ??? (kam karne par) has 
the lemma ?lower?. The second stage includes 
assigning these lemmas. Lemma assignment is 
based on lookups from a VerbNet like map file. 
We have compiled a large set of Urdu verb pre-
dicates by mapping translations found in the au-
tomatically annotated corpus to the VerbNet map 
file. This Urdu verb predicate list also accommo-
dates complex predicates that occur along with 
verbs such as ?karna ? to do?, ?paana ? to get?, 
etc. (along with different variations of these 
verbs ? karte, kiya, paate etc.). This verb predi-
cate list (manually corrected) consists of 800 en-
tries. Since our gold standard test set is very 
small, the lemma assignment for all verb predi-
cates is 100% (no pb values and hence no 
senses). This list, however, has to be augmented 
further to meet the standards of the English 
VerbNet map file. 
5.2 Argument Detection 
Argument detection (SRL) is done in two steps: 
(1) argument boundary detection (2) argument 
label assignment. We perform tests for step 2 to 
show how well a standard SVM role detection 
model works on the automatically generated Ur-
du data set. For each pair of correct predicate p 
and an argument i we create a feature representa-
tion F p,a  ~ set T of all arguments. To train a mul-
ti-class role-classifier, given the set T of all ar-
guments, T can be rationalized as T arg i+  (positive 
instances) and T arg i?  (negative instances) for each 
argument i. In this way, individual ONE-vs-ALL 
(Gildea and Jurafsky, 2002) classifier for each 
                                                 
9 http://verbs.colorado.edu/semlink/semlink1.1/vn-
pb/README.TXT 
argument i is trained. In the testing phrase, given 
an unseen sentence, for each argument Fp,q is 
generated and classified by each individual clas-
sifier.  
We created a set of standard SRL features as 
shown in table 9. The results (Tables 10 and 11), 
though not impressive, are promising. We be-
lieve that by increasing the number of samples 
(for each argument) in the training set and intel-
ligently controlling the negative samples, the 
results can be improved significantly. 
Training ? 2270 sentences with 7315 argument instances. 
Test ? 77 sentences with 496 argument instances. (22 dif-
ferent role types) 
BaseLine 
Features 
(BL) 
phrase-type (syntactic category; NP, PP etc.), 
predicate (in our case, verb group), path (syn-
tactic path from the argument constituent to 
the predicate), head words (argument and the 
predicate respectively), position (whether the 
phrase is before or after the predicate)  
Detailed 
Features 
BL + POS (of the first word in the predicate), 
chunk tag of the predicate, POS (of the first 
word of the constituent argument), head word 
(of the verb group in a complex predicate), 
named entity (whether the argument contains 
any named entity, such as location, person, 
organization etc.) 
Table 9: Features for SRL 
 
Kernel/features Precision Recall F-Score 
LK ? BL 71.88 48.25 57.74 
LK ? all 73.91 47.55 57.87 
PK ? BL 74.19 48.25 58.47 
PK ?all (best) 73.47 49.65 59.26 
Table 10: Arg0 performance 
 
Kernel/features Precision Recall F-Score 
LK ? BL 69.35 22.87 34.40 
LK ? all 69.84 23.4 35.05 
PK ? BL 73.77 24.14 36.38 
PK ?all (best) 73.8 26.06 38.52 
Table 11: Arg1 Performances 
(PK - polynomial kernel LK ? Linear kernel) 
6 Conclusion 
In this work, we develop an alignment system 
that is tailor made to fit the SRL problem scope 
for Urdu. Furthermore, we have shown that de-
spite English being a totally different language, 
resources for Urdu can be generated if the subtle 
grammatical nuances of Urdu are accounted for 
while projecting the annotations. We plan to 
work on argument boundary detection and ex-
plore other features for argument detection. The 
lemma set generated for Urdu is being refined for 
finer granularity. 
804
References 
 
Ambati, Vamshi and Chen, Wei, 2007. Cross Lingual Syn-
tax Projection for Resource-Poor Languages. CMU. 
Baker, Collin .F., Charles J. Fillmore, John B. Lowe. 1998. 
The Berkeley Frame Net project. COLI/G-ACL. 
Bharati, Akshar, Dipti Misra Sharma, Lakshmi Bai and 
Rajeev Sangal. 2006. AnnCorra: Annotating Corpora 
Guidelines For POS And Chunk Annotation For Indian 
Language. Technical Report, Language Technologies 
Research Centre IIIT, Hyderabad. 
Burchardt, Aljoscha and Anette Frank. 2006. Approaching 
textual entailment with LFG and FrameNet frames. RTE-
2 Workshop. Venice, Italy. 
Butt, Miriam and Wilhelm Geuder. 2001.  On the 
(semi)lexical status of light verbs. /orbert Corver and 
Henk van Riemsdijk, (Eds.), Semi-lexical Categories: On 
the content of function words and the function of content 
words, Mouton de Gruyter, pp. 323?370, Berlin.  
Diab, Mona and Philip Resnik. 2002. An unsupervised me-
thod for word sense tagging using parallel corpora. 40th 
Annual Meeting of ACL, pp. 255-262, Philadelphia, PA. 
Dorr, Bonnie, J. 1994. Machine Translation Divergences: A 
Formal Description and Proposed Solution. ACL, Vol. 
20(4), pp. 597-631. 
Fillmore, Charles J. 1968. The case for case. Bach, & 
Harms( Eds.), Universals in Linguistic Theory, pp. 1-88. 
Holt, Rinehart, and Winston, New York. 
Fillmore, Charles J. 1982. Frame semantics. Linguistics in 
the Morning Calm, pp.111-137. Hanshin, Seoul, S. Ko-
rea. 
Fung, Pascale and Benfeng Chen. 2004. BiFrameNet: Bilin-
gual frame semantics resources construction by cross-
lingual induction. 20th International Conference on 
Computational Linguistics, pp. 931-935, Geneva, Swit-
zerland. 
Fung, Pascale, Zhaojun Wu, Yongsheng Yang and Dekai 
Wu. 2007. Learning bilingual semantic frames: Shallow 
semantic parsing vs. semantic role projection. 11th Con-
ference on Theoretical and Methodological Issues in 
Machine Translation, pp. 75-84, Skovde, Sweden. 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic labe-
ling of semantic roles. Computational Linguistics, Vol. 
28(3), pp. 245-288. 
Hi, Chenhai and Rebecca Hwa. 2005. A backoff model for 
bootstrapping resources for non-english languages. Joint 
Human Language Technology Conference and Confe-
rence on EM/LP, pp. 851-858, Vancouver, BC. 
Hwa, Rebecca, Philip Resnik, Amy Weinberg, and Okan 
Kolak. 2002. Evaluation translational correspondance us-
ing annotation projection. 40th Annual Meeting of ACL, 
pp. 392-399, Philadelphia, PA. 
Koehn, Phillip. 2005. ?Europarl: A parallel corpus for statis-
tical machine translation,? MT summit, Citeseer. 
Lafferty, John D., Andrew McCallum and C.N. Pereira. 
2001. Conditional Random Fields: Probabilistic Models 
for Segmenting and Labeling Sequence Data. 18th Inter-
national Conference on Machine Learning, pp. 282-289. 
Liang, Percy, Ben Taskar, and Dan Klein. 2006. Alignment 
by Agreement, /AACL.  
Marcus, Mitchell P., Beatrice Santorini and Mary Ann Mar-
cinkiewicz. 2004. Building a large annotated corpus of 
English: The Penn Treebank. Computational Linguistics, 
Vol. 19(2), pp. 313-330.  
Moschitti, Alessandro. 2008. Kernel methods, syntax and 
semantics for relational text categorization. 17th ACM 
CIKM, pp. 253-262, Napa Valley, CA. 
Mukerjee, Amitabh , Ankit Soni and Achala M. Raina. 
2006. Detecting Complex Predicates in Hindi using POS 
Projection across Parallel Corpora. Workshop on Multi-
word Expressions: Identifying and Exploiting Underly-
ing Properties, pp. 11?18. Sydney. 
Mukund, S., Srihari, R. K., and Peterson, E. 2010. An In-
formation Extraction System for Urdu ? A Resource Poor 
Language. Special Issue on Information Retrieval for In-
dian Languages. TALIP. 
Pado, Sebastian and Mirella Lapata. 2009. Cross-Lingual 
annotation Projection of Semantic Roles. Journal of Ar-
tificial Intelligence Research, Vol. 36, pp. 307-340. 
Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 2005. 
The proposition bank: An annotated corpus of semantic 
roles. Computational Linguistics, Vol. 31(1). 
Palmer, Martha, Shijong Ryu, Jinyoung Choi, Sinwon 
Yoon, and Yeongmi Jeon. 2006. Korean Propbank. Lin-
guistic data consortium, Philadelphia. 
Philips, Lawrence. 2000. The Double Metaphone Search 
Algorithm. C/C++ Users Journal. 
Sinha, R. Mahesh K. 2009. Mining Complex Predicates In 
Hindi Using A Parallel Hindi-English Corpus. ACL In-
ternational Joint Conference in /atural Language 
Processing, pp 40. 
Surdeanu, Mihai, Sanda Harabagiu, John Williams and Paul 
Aarseth. 2003. Using predicate-argument structures for 
information extraction. 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pp. 8-15, Sappo-
ro, Japan. 
Taule, Mariona, M. Antonio Marti, and Marta Recasens. 
2008. Ancora: Multi level annotated corpora for Catalan 
and Spanish. 6th International Conference on Language 
Resources and Evaluation, Marrakesh, Morocco. 
Xue, Nianwen and Martha Palmer. 2009. Adding semantic 
roles to the Chinese treebank. /atural Language Engi-
neering, Vol. 15(1), pp. 143-172. 
Yarowsky, David, Grace Ngai and Richard Wicentowski. 
2001. Inducing multi lingual text analysis tools via ro-
bust projection across aligned corpora. 1st Human Lan-
guage Technology Conference, pp. 161-168, San Fran-
cisco, CA. 
805
Coling 2010: Poster Volume, pages 860?868,
Beijing, August 2010
 
 
 
 
A Vector Space Model for Subjectivity Classification in Urdu 
aided by Co-Training 
 
Smruthi Mukund 
CEDAR 
University at Buffalo 
smukund@buffalo.edu 
Rohini K. Srihari 
CEDAR 
University at Buffalo 
rohini@cedar.buffalo.edu 
 
Abstract 
The goal of this work is to produce a 
classifier that can distinguish subjective 
sentences from objective sentences for 
the Urdu language. The amount of la-
beled data required for training automatic 
classifiers can be highly imbalanced es-
pecially in the multilingual paradigm as 
generating annotations is an expensive 
task. In this work, we propose a co-
training approach for subjectivity analy-
sis in the Urdu language that augments 
the positive set (subjective set) and gene-
rates a negative set (objective set) devoid 
of all samples close to the positive ones. 
Using the data set thus generated for 
training, we conduct experiments based 
on SVM and VSM algorithms, and show 
that our modified VSM based approach 
works remarkably well as a sentence lev-
el subjectivity classifier. 
1 Introduction 
Subjectivity tagging involves distinguishing 
sentences that express opinions from sentences 
that present factual information (Banfield 1982; 
Wiebe, 1994). A wide variety of affective 
nuances can be used while delivering a message 
pertaining to an event. Although the factual 
content remains the same, lexical selections and 
grammatical choices can considerably influence 
the affective nature of the text. Recognizing 
sentences that exhibit affective behavior will 
require, at the least, recognizing the structure of 
the sentence and the emotion bearing words.  
To date, much of the research in this area is 
focused on English. A variety of reliable 
resources that facilitate effective sentiment 
analysis and opinion mining, such as polarity 
lexicons (Senti-WordNet 1 ) and contextual 
valence shifters (Kennedy and Inkpen, 2005) are 
available for English. The MPQA corpus of 
10,000 sentences (Wiebe et al, 2005) provides 
detailed annotations for sources of opinions, 
targets, speech events and fragments that indicate 
attitudes for the English newswire data. The 
IMDB corpus contains 10,000 sentences 
categorized as subjective and objective in the 
movie review domain. Clearly, English is well 
supported with resources. There are other widely 
spoken resource poor languages that are not as 
privileged. When we consider social media, 
limiting our analysis to a language like English, 
however universal, will lead to loss of 
information. With the advent of virtual 
keyboards and extended Unicode support, the 
internet is rapidly getting flooded by users who 
use their native language in textual 
communication. There is a pressing need to 
perform non-topical text analysis in the 
multilingual paradigm. 
Subjectivity analysis is a precursor to 
numerous applications performing non-topical 
text analysis like sentiment analysis, emotion 
detection, and opinion extraction (Liu et al, 
2005; Ku et al, 2006; Titov and McDonald, 
2008). Creating the state-of-the-art subjectivity 
classifier using machine learning techniques 
require access to large amounts of annotated 
data. For less commonly taught languages like 
                                                 
1 http://swn.isti.cnr.it/download_1.0/ 
860
 
 
 
 
Urdu, Hindi, Bengali, Spanish and Romanian, 
the resources required to automate subjectivity 
analysis are either very sparse or unavailable. 
Generating annotated corpus for subjectivity 
detection is laborious and time consuming. 
However, several innovative techniques have 
been proposed by researchers in the past to 
generate annotated data and lexical resources for 
subjectivity analysis in resource poor languages. 
Mihalcea et al, (2007) and Banea et al, (2008) 
used machine translation technique to leverage 
English resources for analysis in Romanian and 
Spanish languages. Wan (2009) proposed a co-
training technique that leveraged an available 
English corpus for Chinese sentiment 
classification. Wan (2008) focused on improving 
Chinese sentiment analysis by using both 
Chinese and English lexicons. 
Unfortunately, not much work has been done 
in the area of subjectivity analysis for the Urdu 
language. This language lacks annotated 
resources required to generate even the basic 
NLP tools (POS tagger, NE tagger etc.) needed 
for text analysis. In order to facilitate subjectivity 
analysis in Urdu language, we annotated a small 
set of Urdu newswire articles for emotions (?2). 
The sentence level annotations provided in this 
dataset follow the annotation guidelines 
proposed by Wiebe et al, (2003). Although 
tremendous effort was put into generating this 
corpus, the data set is not very comprehensive 
and contains only about 500 sentences marked 
subjective. This is definitely insufficient to train 
a suitable subjectivity classifier.  
1.1 Issue with unbalanced data set 
A subjectivity classifier is a binary classifier. 
A traditional binary classifier is trained using 
universal representative sets for positive and 
negative categories. But in subjectivity analysis, 
especially for languages like Urdu that have no 
annotated data, generating universal 
representative sets is extremely difficult and 
almost an impossible task. Assimilating the 
negative set is especially a delicate task as the set 
should be carefully pruned of all the positive 
samples. Also, detecting subjectivity in a 
sentence is highly personalized. Annotators are 
sometimes prejudiced while marking samples. 
This bias, however small, produces errors with 
some true positive samples being unintentionally 
missed and categorized as negative. 
Traditionally, research in machine learning has 
assumed the class distribution in the training data 
to be reasonably balanced. However, when the 
training data is highly imbalanced, i.e., the 
number of positive examples is very small, the 
performance of text classification algorithms 
such as linear support vector machine (SVM) 
(Brank and Grobelnik, 2003), na?ve Bayes and 
decision trees (Kubat and Matwin, 1997) are 
adversely affected.  
In order to achieve a balanced training set, 
Japkowicz (2000) duplicates positive examples 
(oversampling) and discards negative ones 
(downsizing). Kubat and Matwin (1997) discard 
all samples that are close to the positive set to 
avoid misclassification. Chan and Stalfo (1998) 
have trained several classifiers on different ba-
lanced data subsets, each constructed to include 
all positive training samples and a set of negative 
samples of comparable size. The predictions are 
combined through stacking.  
For the task of subjectivity analysis, especially 
in the multilingual paradigm where the data set is 
highly unbalanced, using one of the techniques 
proposed above will yield benefit. To the best of 
our knowledge, co-training technique has not 
been applied before for the subjectivity detection 
task, in particular, for the Urdu language. 
1.2 Contribution 
Our first contribution is inspired by the work 
of Luo et al, (2008). We propose a similar co-
training technique that helps to create a likely 
negative set (objective sentences) and a filtered 
positive set (subjective sentences) 
simultaneously from the unlabeled set. We use 
two learning models trained using the linear 
SVM algorithm iteratively. In every iteration of 
co-training, the likely positive samples are 
filtered. The iterative process terminates when no 
more positive samples are found. The final 
negative set is the likely negative set, considered 
as the universal representative set for the non-
subjective category. The likely positive sample 
set is appended to the already existing positive 
set (annotated set). The SVM models are trained 
using part of speech, unigrams and emotion 
bearing words, as features.  
The second contribution of this work includes 
training a state-of-the-art Vector Space Model 
861
 
 
 
 
(VSM) for Urdu newswire data using the data 
sets generated by the co-training method. 
Experiments that use the SVM classifier are also 
performed. The results show that the 
performance of the proposed VSM based 
approach helps to achieve state-of-the-art 
sentence level subjectivity classifier. The F-
Measure of the VSM subjectivity classifier is 
82.72% with 78.7% F-measure for the subjective 
class and 86.7% F-Measure for the objective 
class.  
2 Data Set 
The data set used to generate a subjectivity 
classifier for Urdu newswire articles is obtained 
from BBC Urdu2. The annotating efforts are di-
rected towards achieving the final goal- emotion 
detection in Urdu newswire data and the annota-
tion guidelines are based on the MPQA standards 
set for English.  
The repository of articles provided by BBC is 
huge and needs to be filtered intelligently. Two 
levels of filters are applied. ? date and keyword 
search. The date filter is applied to retrieve ar-
ticles of three years, starting year 2003. The key-
word based filter consists of a set of seed words 
that are commonly used to express emotions in 
Urdu -ghussa (~anger), pyar (~love) etc. Clearly, 
this list will not cover all possible linguistic ex-
pressions that express emotion and opinion. But 
it is definitely a representative of a wide range of 
phenomena that naturally occurs in text express-
ing emotions.  
The data retrieved is parsed using an in-house 
HTML parser to produce clean data. To date, we 
have 500 articles, consisting of 700 sentences 
annotated for emotions. There are nearly 6000 
sentences that do not contain any emotions mak-
ing it highly unbalanced. This data set is divided 
into testing and training sets with 30% and 70% 
of the data respectively. Co-training is performed 
only on the 70% training set that consists of 470 
subjective sentences and about 4000 objective 
sentences. The purpose of co-training here is to 
remove samples that are close to subjective from 
the objective set and create a likely negative set. 
The samples removed are the likely positive set. 
This set of 4000 objective sentences can be con-
sidered as the un-annotated set. 
                                                 
2 http://www.bbc.co.uk/urdu/ 
3 Co-Training 
Identifying sentences that express emotions in 
Urdu newswire data is not trivial. Subjective sen-
tences do not always contain individual expres-
sions that indicate subjectivity. Analysis is high-
ly dependent on the contextual information. 
Wiebe et al, (2001) reported that nearly 44% of 
sentences in the MPQA corpus (English news-
wire data) are subjective. In newswire data, 
though most facts are reported objectively, there 
are cases when the tone of the sentence is very 
intense indicating the existence of emotion. Con-
sider Example 1. 
 
Example 1:  
Political news headline  
 ?????? ?? ????? ? ??????? ????????? ?? ??????? ?? ????
????? ???? ?? ?????? ???? 
[bhart ka pakstan kE sath jame mZakrat sE ankar, 
bharty lykcr snnE kE Kwaha" nhy"] 
[India refuses to have a dialog with Pakistan, In-
dians are not willing to listen to the lecture] 
Common Urdu 
????? ?? ????? ?? ??? ?? ??? ????? ??????? ??  ?????  
[India refuses to talk to Pakistan] 
Clearly, the news headline is extremely in-
tense and strongly expresses the opinion of India 
on Pakistan. However, the statement in common 
Urdu is not as affective.  
 
Example 2: 
?? ???? ???? ???? ??? ???? ???? ??? ?? ???? ???  ??????
??? ??? ???   
[anSary nE kha ?myry ray^E my" eamr shyl ayk 
bd dmaG awr Zdy XKS hy"? ]                                                      
[Ansari said, ?according to me Aamir Sohail is one 
crazy and stubborn man?] 
Statements in quotes that express emotions are 
subjective as shown in example 2. 
 
Consider example 3. Here, identifying the 
words that indicate subjectivity is not straight 
forward. The phrase, ?found it very difficult to 
hide his smile? is indicative of the emotion expe-
rienced by ?Habib Miya?.  
 
Example 3: 
 ???  ??? ??????? ?? ?? ????? ?? ?? ???? ???? ?? ???
?? ?? ???? ??????? ???? ????  
[rqm ky as wSwly pr yh Hbyb mya" kE ly^E bht 
mXkl t|ha kh wh apny mskrahT c|hpa sky"]                                  
 [At this event of money collection, Habib Miyan 
found it very difficult to hide his smile.]  
862
 
 
 
 
 
There are also several false positives that 
make subjective detection hard task. Example 4 
is an objective sentence despite the usage of 
word ?pyar? ~ love, an emotion bearing word.  
 
Example 4:  
 ?? ??? ???? ??? ???????? ?? ??? ????  
[n|Zmam ka nya pyar ka nam anzy pRa hE] 
[The new nickname for Inzaman is Inzi] 
 
Expressive elements in Urdu sentences were 
marked with an inter-annotator agreement of 0.8 
kappa score. Though high, there still exists a bias 
that can influence classification especially when 
the number of sentences in the positive set is rel-
atively less. In order to obtain a reliable positive 
and negative set for training a learning algorithm, 
we adopt a semi-supervised learning technique of 
co-training. Co-training (Blum and Mitchell, 
1998) is similar to self-training in that it increas-
es the amount of labeled data by automatically 
annotating unlabeled data. The intuition here is 
that if the conditional independence assumption 
holds, then on an average each selected docu-
ment will be as informative as a random docu-
ment, and the learning will progress. Co-training 
differs from self-training as it uses multiple 
learners to do the annotation. Each learner offers 
its own perspective that when combined gives 
more information. This technique is especially 
effective when the feature space of a particular 
type of problem can be divided into distinct 
groups and each group contains sufficient infor-
mation to perform the annotation. In other words, 
co-training algorithm involves training two dif-
ferent learning algorithms on two different fea-
ture spaces. The learning of one becomes condi-
tionally independent of the other and the predic-
tion made by each classifier is used on the unla-
beled data set to augment the training data of the 
other.  
A traditional co-training classifier is trained 
and later applied on the same unlabeled data set. 
Theoretically such classifiers are not likely to 
assign confident labels. In this work, the pro-
posed co-training method differs from the tradi-
tional co-training method in that the two classifi-
ers are based not on two different feature spaces 
but on two different training data sets with the 
same feature space.  
 
 
Figure 1: Co-Training model 
 
Figure 1 explains the overall working of the 
model. The negative set (which can also be the 
unlabeled set) is split into two equal parts N1 and 
N2. S represents the positive annotated set. Two 
linear SVM classifiers are trained iteratively to 
purify the negative data set. SVM1 is trained us-
ing S+N1i and SVM2 is trained using S+N2i data 
sets. In every iteration i, N1i data set is evaluated 
using SVM2 model and N2i data set is evaluated 
using SVM1 model. The samples that are classi-
fied as positive in a given iteration i are binned 
into sets P1i and P2i respectively. These samples 
are removed from N1i and N2i data sets to create 
new N1i+1 and N2i+1 sets that are used for training 
in the next iteration i+1. The iterations continue 
until no positive samples are marked by both 
SVM1 and SVM2 models. The final set of likely 
negatives is S = N1k + N2k sets, where N1k and 
N2k are sets created in the last k iteration of the 
algorithm. In order to obtain the likely positive 
set, the final P1 = {P11 + P12 + ?. + P1k} and P2 = 
{P21 + P22 + ?. + P2k} sets are combined and 
tested using the SVMs modeled in the last k ite-
ration of the co-training algorithm. Similar to the 
traditional co-training method the samples that 
are marked positive by both classifiers (P1o = P2o) 
are considered to be the likely positive set L.  
Several features are used to train the SVM 
learning models used for co-training. The best 
performance is obtained when word unigrams, 
parts of speech and likely emotion words are 
used as features.  
This technique of co-training provides us with 
a relatively huge set of likely positive samples 
863
 
 
 
 
(close to 400 sentences). Sentences in this set 
were examined by the annotators and nearly 60% 
of the sentences were subjective or near subjec-
tive in nature (Example 5 and 6). 
 
Labels R % P % IF % AF % 
Unigram 52.63 
 1 18.64 74.57 29.83 
-1 95.4 62.35 75.44 
Unigram+Bigram 50.25 
1 14.40 85 24.63 
-1 98.19 61.82 75.87 
Table 1: Performance of the model using  
un-balanced data set3 
 
Labels R % P % IF % AF % 
Annotated positive + likely positive + likely 
negative 
62.95 
 
1 39 70 50.09 
-1 87.28 67.34 79.9 
Annotated positive + likely negative 55.42 
1 30 61.2 40.26 
-1 86.1 64.23 73.57 
Table 2 ? Performance of the model after  
co-training method 
 
Table 1 shows the performance of the SVM 
model using the unbalanced data set for training. 
Table 2 shows the performance of the same 
model using data generated after co-training.  
 
Example 5:  
??? ???? ??????? ? ?????? ? ????? ?? ??? ?? ??? ?????? ? 
??? ? ????? ?? ????? ????? ??? ?????? ? ???? ????   
[pwtn nE kha kh lwg dwsrw" ky Ank|h my" tnka 
dyk|h lytE hy" lykn apny Ank|h my" pRa Xhtyr an-
hy" n|zr nhy" Ata .] 
[Potan said people who see dust in others eyes 
never realize that it is their eyes that are filled with 
dirt.] 
The above example is a metaphor indicating 
extreme anger. 
 
Example 6: 
?????? ?? ? ???? ???? ?? ?? ???? ???? ?? ??? ?????? ?????? ? 
??? ????? ???? ????? ???? ??? ?? ????? ?? ?? ??  
[e|ta& alrHmn XyK ka khna hE kh barh agst kw an-
hy" an kE byTw" kE samnE mkml |twr pr brhnh kr 
kE pryD kray^y gy^y] 
[etlaalrahman said that on 12th Aug they made him 
parade naked in front of his children.] 
                                                 
3 Convention used across tables -  Label 1: subjective sen-
tences Label -1: objective sentences R: Recall P: Precision 
IF: Individual F-Measure AF: Average F-Measure. 
 
Example 6 indicates extreme sad emotion. Such 
examples were found in the likely positive set. 
4 Features 
Features that are commonly used to train a 
subjectivity classifier for English are word uni-
grams, emotion keywords, part of speech infor-
mation and noun patterns (Pang et al, 2002). 
Due to difference in syntactic structure, vocabu-
lary and style, features that work for English may 
not work for Urdu. Also, Urdu is handicapped by 
the lack of resources required to perform basic 
NLP analysis. However, it is worth exploring the 
English feature set as subjectivity is more a se-
mantic phenomenon. Efforts to generate likely 
emotion word lexicons and subjectivity patterns 
for the Urdu language are underway. The sec-
tions that follow summarize the experimented 
features. 
4.1 Word Unigrams 
Unigram word features are very informative. 
Three different approaches are tried for selecting 
the unigrams. The first method involves selecting 
only those words that occur more than twice in 
the dataset. This eliminates proper nouns (low 
frequency named entities do not generally con-
tribute towards subjectivity detection) and spel-
ling errors (Pang et al, 2002). In the second me-
thod, only words that are adjectives and verbs 
along with the surrounding case markers are ac-
counted for as features. This has the advantage of 
drastically reducing the feature set. The third me-
thod involves including the nouns as well to the 
feature set. A simple list of stop words (common 
Urdu words ? pronouns such as ?us?, ?is?, ?aap?, 
?un?, salutations like ?shabba khair?, ?aadab? and 
honorifics along with punctuations and special 
symbols) are eliminated. The features are 
represented as Boolean features for the SVM 
model. The value is 1 if the feature word appears 
in the sentence to be classified and 0 otherwise. 
The best performance is obtained for the first 
method that considers all words with frequency 
greater than 2. This conforms to what is shown 
by Pang et al, (2002) for classification of Eng-
lish movie reviews. 
4.2 Part of Speech (POS) Information 
The work done by Mukund and Srihari (2009) 
provides suitable POS and NE tagger for Urdu. 
864
 
 
 
 
This POS tagger is used to generate parts of 
speech tags on the acquired data set (?3).  The 
POS tags associated with adjectives, verbs, 
common nouns and auxiliary words are consi-
dered and used as Boolean features for the SVM 
model. The proper noun words are normalized to 
one common word ?nnp? and are assigned the 
common noun tag. For the English language, 
when building a subjectivity classifier for review 
classification, the use of POS information did not 
benefit the system (Kennedy and Inkpen, 2006). 
However, for Urdu, the performance of the co-
training model with POS information showed 
1.2% improvement (table 3). 
4.3 Likely Emotion Lexicon 
In order to facilitate simple keyword based 
detection of subjectivity, access to a lexicon con-
sisting of likely emotion words is needed. Unfor-
tunately, no such lexicon is available off the 
shelf for Urdu. In this work, an Urdu specific 
emotion list is generated that contains transla-
tions from the English emotion list released by 
SemEval (2007) ?Word"et affect Emotion List?. 
Words for each emotion category - sadness (sad), 
fear, joy (happy), surprise, anger and disgust are 
obtained for Urdu by using an Urdu-English dic-
tionary. The list is pruned manually and cor-
rected to remove errors. Simple keyword lookup 
on the Urdu annotated corpus has an emotion 
detection rate of 29.27%. This shows that al-
though the contribution of the emotion lexicon 
for subjectivity classification is not significant, it 
contains information which when used along 
with other features aid subjectivity detection. 
4.4 Patterns 
Extracting syntactic patterns contribute to-
wards the affective orientation of a sentence (Ri-
loff et al, 2003). The Apriori algorithm (Agar-
wal and Srikant, 1994) for learning association 
rules is used here to mine the syntactic word pat-
terns commonly used in the positive and negative 
data set. The length of the candidate item set k = 
4. Starting from a small set of seed words (likely 
emotion words) and the associated POS tags, 
POS sequential patterns like ?adverb verb 
verbtransitive sentencemarker?, ?noun noun ca-
semarker verbtransitive?, etc., that are most 
commonly found in subjectivity set are extracted. 
23 patterns that strongly indicate subjectivity 
were found by this method and included as fea-
tures to train the SVM learning algorithm.  
4.5 Confidence Words 
The confidence word list positively aids the 
VSM classifier (?5). The words in the likely 
emotion list are not the only ones that contribute 
towards the emotion orientation of a sentence 
and also, not all of these words contribute effec-
tively. There are several stop words (eliminated 
while accounting for unigrams) (esp. case mark-
ers) that contribute significantly for categoriza-
tion. In order to identify all the keywords that 
actually contribute to subjectivity categorization, 
a technique proposed by Soucy and Mineau 
(2004) is used.  
The confidence weight of a given word w, 
based on the number of documents it is asso-
ciated with under each category, is measured us-
ing the Wilson Proportion Estimate (Wilson, 
1927). In order to compute the confidence of w 
for a specific category, the number of positive 
and negative documents associated with w has to 
be determined. A document is positive if it be-
longs to that category and negative otherwise. 
Thus, two kinds of word confidence metrics are 
computed, CPOS:w and C"EG:w as given below.  
                  ???     (Eq. 1) 
                   ???    (Eq. 2) 
where n is the total number of positive and nega-
tive documents,  is the ratio of the num-
ber of positive documents which contain w to the 
total number of documents, and  is the 
ratio of the number of negative documents which 
contain w to the total number of documents. The 
normal distribution is used when n > 30.  
Note that equations 1 and 2 give a range of 
values for CPOS:w and C"EG:w. If the lower bound 
of CPOS:w is greater than the upper bound of 
C"EG:w, we say that w is likely to be a word in 
that category. Now, we compute the strength of a 
word Sw in a particular category as 
 
( )
( )nz
nnzppz
n
zp
C
wPOSwPOS
z
wPOS
wPOS 2
2/
2
2/::2/
2/
:
: 1
]4?1?[2?
?
??
?
+
???
?
???
? +??+
=
( )
( )nz
nnzppz
n
zp
C
w"EGw"EG
z
w"EG
w"EG 2
2/
2
2/::2/
2/
:
: 1
]4?1?[2?
?
??
?
+
???
?
???
? +??+
=
wPOSp :?
( )
??
? ?= >                            ;                    0
 ;2log
otherwise
       )w:NEGub(C  )w:POSlb(C ifmPRFSw
"EG:w p ?
865
 
 
 
 
                                                 ??? (Eq. 3) 
where mPRF is given by  
                                                                              ???   (Eq. 4) 
and lb(?) and ub(?) are the lower and upper 
bounds of their arguments, respectively. 
Equations 1 through 4 generated a very good set 
of keywords that are used as category word fea-
tures in the SVM learning model. For VSM, the 
strength value is used as a boost factor along 
with the tf-idf weight when calculating the simi-
larity score (table 3). 
5 Final Subjectivity Classifier 
Wiebe et al, (2005) and Pang et al, (2002) 
have shown that an SVM based approach works 
well for subjectivity classification. Riloff et al, 
(2003) have conducted experiments that use Bag-
Of-Words (BoW) as features to generate a Na?ve 
Bayes subjectivity classifier for the MPQA cor-
pus in English. This method has an accuracy of 
73.3%. Su and Markert (2008) use BoW features 
termed as lexical features on the IMDB corpus to 
generate an accuracy of 60.5%. Das and Ban-
dyopadhyay (2009) use a CRF based approach to 
generate a subjectivity classifier for Bengali data 
with a precision of 72.16% for news and 74.6% 
for blogs domain. The same approach has a pre-
cision of 76.08% and 79.9% on the two domains 
respectively. Impressive results for emotion de-
tection are obtained by Danisman and Alpkocak, 
(2007) who use a VSM based approach. They 
show that their approach works much better than 
a traditional SVM based approach commonly 
used for emotion detection. 
In this work, we conduct subjectivity classifi-
cation experiments using two different learning 
algorithms ? linear SVM and VSM. The best 
performance is obtained using the VSM model as 
shown in table 4. All experiments are conducted 
on the data set obtained after applying the co-
training technique.  
5.1 VSM algorithm 
The final subjectivity classifier is based on the 
VSM approach. Inspired by the work done in 
?Feeler? (Danisman and Alpkocak, 2007), a sim-
ilar technique is used to train the final subjectivi-
ty classifier for Urdu. The algorithm is explained 
in table 3. The similarity metric is modified to 
include the confidence score for each word 
(pt.5). In VSM, documents and queries are 
represented as vectors, and the cosine angle be-
tween them indicates the similarity. 
1.  di = <w1i, w2i, ?. wni> where wki is the weight of 
the kth term in document i , di is the document 
vector. wki is computed using tf-idf weighting 
scheme. 
2. Mj={d1,d2,?,dc} where Mj is each class (subjec-
tive and objective) 
3.  Model vector for an arbitrary class Ej is created 
by taking the mean of dj vectors  
?
?
=
||1 j
ji
M
Md
i
j
j dM
E
 
where |Mj| represents number of documents in Mj. 
4. The whole system is represented with a set of 
model vectors, D={E1,E2,...,Es} where s represents 
the number of distinct classes to be recognized.  
5. The normalized similarity between a given query 
text Q, and a class, Ej, is defined as follows: 
kj
n
k
kqj EconfwEQsim *)(),(
1
?
=
+=  
conf is the confidence factor applied for lexical 
terms found in the word list. 
6. classification result is, 
 )),(max(arg)( jEQsimQVSM =  
Table 3: VSM Algorithm for subjectivity 
 Classification 
 
Labels R % P % IF % AF % 
Before Co-Training (all data) 62.95 
 1 65.85 70.85 67.4 
-1 85.58 83.33 84.44 
After Co-Training (pruned data) 86.73 
1 72.88 85.57 78.72 
-1 91.29 82.60 86.73 
Table 4: VSM approach, using all training data and 
using pruned training data (L+S+true) 
 
The confidence metric (strength) for each term 
is calculated using the Wilson proportion esti-
mate (?4.4) and added to the term score as the 
boost factor. Q is the test set. Model vectors are 
obtained using the data set that consists of true 
set (annotated positive samples), likely positive 
set L and likely negative set N. Sets L and N are 
obtained from the co-training method. The re-
sults are shown in table 4.  
The power of SVM cannot be ignored. Pang et 
al., (2002) use SVM to generate a subjectivity 
(polarity) classifier for English. Our second set 
of experiments is conducted to measure the per-
formance of a linear SVM classifier for subjec-
tivity analysis on the Urdu newswire data. The 
data set used for training is the pruned data set 
)ub(C )lb(C
)lb(C
w:NEGw:POS
w:POS
+=mPRF
866
 
 
 
 
obtained after applying the co-training technique. 
The features used and the performance of the 
model with each feature is documented in table 6.  
Labels R % P % IF % AF % 
Unigrams + POS 64.2 
 1 40.67 71.1 51.75 
-1 88.29 67.74 76.67 
Unigrams + POS + Patterns 65.68 
1 43.22 72.34 54.11 
-1 88.29 68.69 77.26 
Unigrams + POS + Patterns + emotion words  67.31 
1 48.31 70.81 57.43 
-1 85.88 70.09 77.19 
Table 6: SVM classifier on Urdu newswire data 
 
In order to provide a better understanding of 
the power of the VSM technique, we applied this 
model on the IMDB data set. The training data 
consists of 4000 positive (subjective) and 4000 
negative (objective) samples. Since the data set is 
already balanced, we skip the co-training method. 
Our aim here is to test the working of VSM clas-
sifier. The test set consists of 1000 positive and 
1000 negative samples. The classification result 
on this data set is shown in table 5. The results 
are comparable to the state-of-the-art perfor-
mance of English subjectivity classifier that uses 
SVM (Wiebe et al, 2005). 
Labels R % P % IF % AF % 
Balanced training 78.01 
 1 64 90.57 75 
-1 93.18 71.68 81.03 
Table 5: VSM approach on IMDB data set 
6 Analysis of results 
In this work, experiments were conducted us-
ing two different classification approaches; 1. 
VSM based 2. SVM based.  Results in table 4 
indicate that the VSM technique when combined 
with the modified boost factor (confidence 
measure) can be a very powerful technique for 
sentence level classification tasks. When model 
vectors were constructed using the entire training 
set (highly unbalanced), the performance was at 
62% F-Measure with the subjectivity detection 
rate of 70.85%. Post co-training, using the mod-
ified model vectors obtained from the pruned 
data set generated better scores. The increase in 
the recall of negative class and the increase in the 
overall F-Measure can be attributed to (i) in-
crease in the positive samples (~likely positive 
set), and (ii) cleaner negative set (no near posi-
tive samples).  
The results in table 6 for the SVM classifier 
also indicate the benefits of co-training. The sub-
jectivity classification performance show posi-
tive improvement. Although the performance of 
the SVM model is not as good as the VSM mod-
el, addition of each feature shows an improve-
ment in the subjectivity recognition rate. This 
performance indicates that the feature sets ex-
plored definitely contain positive information 
necessary for accurate detection.  
The poor performance of SVM (over VSM) 
can be attributed to 1. lack of balanced data for 
training a traditional SVM model and, 2. small 
number of positive samples. In VSM the problem 
of unbalanced data set in a way is overcome by 
using the confidence score at the time of calcu-
lating similarity. If these factors are compensated, 
the performance of the SVM model will signifi-
cantly improve. 
7 Conclusion 
This research provides interesting insights in 
modeling a subjectivity classifier for Urdu 
newswire data. We show that despite Urdu being 
a resource poor language, techniques like co-
training and statistical techniques based on tf-idf 
and word unigrams coupled with confidence 
measures help model the state-of-the-art subjec-
tivity classifier. We demonstrate the power of the 
co-training technique in generating likely nega-
tive and positive sets. The number of near sub-
jective samples in the likely positive set suggests 
that this method can be used as an adaptive 
learning technique to enable the annotators pro-
duce more samples. For a task like emotion de-
tection, that requires fine grained analysis, sen-
tences need to be analyzed at the semantic level 
and this goes beyond simple keyword based ap-
proach. Our efforts are now concentrated in this 
direction. 
References 
Agrawal R, Srikant R. 1994. Fast Algorithms for Mining 
Association Rules. In Proc. Of the Intl. Conf on Very 
Large databases. Santiago, Chile. Sept. Pp. 478-499. 
Banea, C., Mihalcea, R., Wiebe, J., and Hassan, S. 2008. 
Multilingual subjectivity analysis using machine transla-
tion. In Proceedings of EM"LP-2008.  
Banfield, A. 1982. Unspeakable Sentences. Routledge and 
Kegan Paul, Boston. 
867
 
 
 
 
Blum, A. and Mitchell, T. 1998. Combining labeled and 
unlabeled data with co-training. Proceedings of the ele-
venth annual conference on Computational learning 
theory, ACM. p. 100. 
Brank, J., Grobelnik, M., Milic-Frayling, N., and Mladenic, 
D. 2003. Training text classifiers with SVM on very few 
positive examples. Technical Report MSR-TR-2003-34, 
Microsoft Corp. 
Chan, Philip K. and Stolfo J. Salvatore. 1998. Toward Scal-
able Learning with Non-Uniform Class and Cost Distri-
butions: A Case Study in Credit Card Fraud Detection. 
Proc. 4th Int. Conf. on Knowledge Discovery and Data 
Mining (KDD-98), August 27?31, 1998, New York City, 
New York, USA, pp. 164?168. AAAI Press. 
Danisman, T., and Alpkocak, A. 2008. Feeler: Emotion 
Classification of Text Using Vector Space Model. AISB 
2008 Convention Communication, Interaction and Social 
Intelligence, p. 53. 
Das, A., and Bandyopadhyay, S. 2009. Subjectivity Detec-
tion in English and Bengali: A CRF-based Approach. Se-
venth International Conference on "atural Language 
Processing (ICON 2009), December. Hyderabad, India. 
Japkowicz Nathalie. 2000. Learning from Imbalanced Data 
Sets: A Comparison of Various Strategies. In "athalie 
Japkowicz (ed.), Learning from Imbalanced Data Sets: 
Papers from the AAAI Workshop (Austin, Texas, Mon-
day, July 31, 2000), AAAI Press, Technical Report WS-
00-05, pp. 10?15. 
Kennedy, A, & Inkpen, D. 2005. Sentiment classification of 
movie and product reviews using contextual valence shif-
ters. In Workshop on the analysis of informal and formal 
information exchange during negotiations (FINEXIN 
2005) 
Ku, L. W., Liang, Y. T., and Chen, H. H. 2006. Opinion 
extraction, summarization and tracking in news and blog 
corpora. In Proceedings of AAAI-2006. 
Kubat, Miroslav and Matwin Stan. 1997. Addressing the 
curse of imbalanced training sets: one-sided selection. 
Proc. 14th ICML, Nashville, Tennessee, USA, July 8?12, 
1997, pp. 179?186. 
Liu, B., Hu, M., and Cheng, J. 2005. Opinion observer: 
Analyzing and comparing opinions on the web. In Pro-
ceedings of WWW-2005. 
Luo, N., Yuan, F., and Zuo, W. 2008. Using CoTraining and 
Semantic Feature Extraction for Positive and Unlabeled 
Text Classification. International Seminar on Future In-
formation Technology and Management Engineering. 
Mihalcea, R., Banea, C., and Wiebe, J. 2007. Learning mul-
tilingual subjective language via cross-lingual projec-
tions. In Proceedings of ACL-2007. 
Mukund, S., and Srihari, R.K., 2009.  NE Tagging for Urdu 
based on Bootstrap POS Learning. Third International 
Workshop on Cross Lingual Information Access: Ad-
dressing the Information Need of Multilingual Societies 
(CLIAWS3), NAACL - 2009, Boulder, CO. 
Pang, B., Lee, L., and Vaithyanathan, S. 2002. Thumbs up? 
Sentiment classification using machine learning tech-
niques. In Proceedings of the Conference on EM"LP, 
pages 79?86. 
Riloff, E., Wiebe, J., and Wilson, T. 2003. Learning subjec-
tive nouns using extraction pattern bootstrapping. Pro-
ceedings of the seventh conference on "atural language 
learning at HLT-"AACL 2003 - Volume 4,  Edmonton, 
Canada: Association for Computational Linguistics, pp. 
25-32. 
Soucy, P., and Mineau, G. W. 2005. Beyond tfidf weighting 
for text categorization in the vector space model. Interna-
tional Joint Conference on Artificial Intelligence, Cite-
seer, p. 1130. 
Su, F., and Markert. K. 2008. From words to senses: a case 
study of subjectivity recognition. Proceedings of the 22nd 
International Conference on Computational Linguistics-
Volume 1, ACL, pp. 825-832. 
Titov, I., and McDonald, R. 2008. A joint model of text and 
aspect ratings for sentiment summarization. In Proceed-
ings of ACL-08:HLT. 
Wan, X. 2008. Using bilingual knowledge and ensemble 
techniques for unsupervised Chinese sentiment analysis. 
In Proceedings of EM"LP-2008. 
Wan, X. 2009. Co-Training for Cross-Lingual Sentiment 
Classification. In Proceedings of the Joint Conference of 
the 47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on "atural Language Processing 
of the AF"LP, Association for Computational Linguis-
tics, pp. 235-243. 
Wiebe, J. 1994. Tracking point of view in narrative. Com-
putational Linguistics, 20(2):233-287. 
Weibe, J., Bruce, R., and O?Hara, T. 1999. Development 
and use of a gold standard data set for subjectivity classi-
fications. In Proc. 37th Annual Meeting of the Assoc. for 
Computational Linguistics (ACL-99). 
Wiebe, J., and Riloff, E. 2005. Creating Subjective and 
Objective Sentence Classifiers from Unannotated Texts. 
Proceedings of the 6th International Conference on Intel-
ligent Text Processing and Computational Linguistics. 
Wiebe, J., Wilson, T., and Cardie, C. 2005. Annotating 
expressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, volume 39, issue 2-3, 
pp. 165-210. 
Wilson, B. Edward. 1927. Probable Inference, the Law of 
Succession, and Statistical Inference. Journal of the 
American Statistical Association, Vol. 22, No. 158 (Jun., 
1927), pp. 209-212. 
 
868
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 58?67,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Using Sequence Kernels to identify Opinion Entities in Urdu 
 
 
Smruthi Mukund? and Debanjan Ghosh* Rohini K Srihari 
?SUNY at Buffalo, NY 
smukund@buffalo.edu 
*Thomson Reuters Corporate R&D 
debanjan.ghosh@thomsonreuters.com 
SUNY at Buffalo, NY 
rohini@cedar.buffalo.edu 
 
 
 
 
 
Abstract 
Automatic extraction of opinion holders 
and targets (together referred to as opinion 
entities) is an important subtask of senti-
ment analysis. In this work, we attempt to 
accurately extract opinion entities from 
Urdu newswire. Due to the lack of re-
sources required for training role labelers 
and dependency parsers (as in English) for 
Urdu, a more robust approach based on (i) 
generating candidate word sequences 
corresponding to opinion entities, and (ii) 
subsequently disambiguating these se-
quences as opinion holders or targets is 
presented. Detecting the boundaries of such 
candidate sequences in Urdu is very differ-
ent than in English since in Urdu, 
grammatical categories such as tense, 
gender and case are captured in word 
inflections. In this work, we exploit the 
morphological inflections associated with 
nouns and verbs to correctly identify 
sequence boundaries. Different levels of 
information that capture context are 
encoded to train standard linear and se-
quence kernels. To this end the best per-
formance obtained for opinion entity 
detection for Urdu sentiment analysis is 
58.06% F-Score using sequence kernels 
and 61.55% F-Score using a combination 
of sequence and linear kernels. 
1 Introduction 
Performing sentiment analysis on newswire da-
ta facilitates the development of systems capable 
of answering perspective questions like ?How did 
people react to the latest presidential speech?? and 
?Does General Musharraf support the Indo-Pak 
peace treaty??. The components involved in de-
veloping such systems require accurate identifica-
tion of opinion expressions and opinion entities. 
Several of the approaches proposed in the literature 
to automatically extract the opinion entities rely on 
the use of thematic role labels and dependency 
parsers to provide new lexical features for opinion 
words (Bethard et al, 2004). Semantic roles (SRL) 
also help to mark the semantic constituents (agent, 
theme, proposition) of a sentence. Such features 
are extremely valuable for a task like opinion en-
tity detection. 
English is a privileged language when it comes 
to the availability of resources needed to contribute 
features for opinion entity detection. There are 
other widely spoken, resource poor languages, 
which are still in the infantile stage of automatic 
natural language processing (NLP). Urdu is one 
such language. The main objective of our research 
is to provide a solution for opinion entity detection 
in the Urdu language. Despite Urdu lacking NLP 
resources required to contribute features similar to 
what works for the English language, the perform-
ance of our approach is comparable with English 
for this task (compared with the work of Weigand 
and Klalow, 2010 ~ 62.61% F1). The morphologi-
cal richness of the Urdu language enables us to 
extract features based on noun and verb inflections 
that effectively contribute to the opinion entity ex-
traction task. Most importantly, these features can 
be generalized to other Indic languages (Hindi, 
Bengali etc.) owing to the grammatical similarity 
between the languages. 
58
English has seen extensive use of sequence ker-
nels (string and tree kernels) for tasks such as rela-
tion extraction (Culotta and Sorensen, 2004) and 
semantic role labeling (Moschitti et al, 2008). But, 
the application of these kernels to a task like opin-
ion entity detection is scarcely explored (Weigand 
and Klalow, 2010). Moreover, existing works in 
English perform only opinion holder identification 
using these kernels. What makes our approach 
unique is that we use the power of sequence ker-
nels to simultaneously identify opinion holders and 
targets in the Urdu language. 
Sequence kernels allow efficient use of the 
learning algorithm exploiting massive number of 
features without the traditional explicit feature rep-
resentation (such as, Bag of Words). Often, in case 
of sequence kernels, the challenge lies in choosing 
meaningful subsequences as training samples in-
stead of utilizing the whole sequence. In Urdu 
newswire data, generating candidate sequences 
usable for training is complicated. Not only are the 
opinion entities diverse in that they can be con-
tained within noun phrases or clauses, the clues 
that help to identify these components can be con-
tained within any word group - speech events, 
opinion words, predicates and connectors. 
 
1 Pakistan ke swaat sarhad ke janoobi shahar Banno 
ka havayi adda zarayye ablaagk tavvju ka markaz 
ban gaya hai. 
[ Pakistan?s provincial border?s south city?s airbase 
has become the center of attraction for all reporters.] 
 
Here, the opinion target spans across four noun 
chunks, ? Pakistan?s | provincial border?s | south 
city?s | airbase ?. The case markers (connectors) 
?ke?and?ka ?  indicate the span. 
2  Habib miyan ka ghussa bad gaya aur wo apne aurat 
ko maara. 
[Habib miya?s anger increased and he hit his own 
wife.] 
 
Here, the gender (Masculine) inflection of the verb 
?maara? (hit) indicates that the agent performing 
this action is ? Habib miya?  (Masculine) 
3  Ansari ne kaha ? mere rayee mein Aamir Sohail eek 
badimaak aur Ziddi insaan hai?. 
[Ansari said, ? according to me Aamir Sohail is one 
crazy and stubborn man?] 
 
Here, cues similar to English such as ?mere rayee 
mein ? (according to)  indicate the opinion holder.  
Another interesting behavior here is the presence of 
nested opinion holders. ?kaha? (said)  indicates that 
this statement was made by Ansari only. 
4  Sutlan bahut khush tha, naseer key kaam se.  
[Sultan was very happy with Naseer?s work ] 
 
Here, the target of the expression ?khush? is after the 
verb ?khush tha?(was happy) ? SV O structure  
Table 1: Examples to outline the complexity of the task 
 
Another contributing factor is the free word or-
der of the Urdu language. Although the accepted 
form is SOV, there are several instances where the 
object comes after the verb or the object is before 
the subject. In Urdu newswire data, the average 
number of words in a sentence is 42 (Table 3). 
This generates a large number of candidate se-
quences that are not opinion entities, on account of 
which the data used for training is highly unbal-
anced. The lack of tools such as dependency pars-
ers makes boundary detection for Urdu different 
from English, which in turn makes opinion entity 
extraction a much harder task. Examples shown in 
table 1 illustrate the complexity of the task. 
One safe assumption that can be made for opin-
ion entities is that they are always contained in a 
phrase (or clause) that contains a noun (common 
noun, proper noun or pronoun), which is either the 
subject or the object of the predicate. Based on 
this, we generate candidate sequences by consider-
ing contextual information around noun phrases. In 
example 1 of Table 1, the subsequence that is gen-
erated will consider all four noun phrases ? Paki-
stan?s | provincial border?s | south city?s | 
airbase?  as a single group for opinion entity. 
We demonstrate that investigating postpositions 
to capture semantic relations between nouns and 
predicates is crucial in opinion entity identifica-
tion. Our approach shows encouraging perform-
ance. 
2  Related Work 
Choi et al, (2005) consider opinion entity iden-
tification as an information extraction task and the 
opinion holders are identified using a conditional 
random field (Lafferty et al, 2001) based se-
quence-labeling approach. Patterns are extracted 
using AutoSlog (Riloff et al, 2003). Bloom et al, 
(2006) use hand built lexicons for opinion entity 
identification. Their method is dependent on a 
combination of heuristic shallow parsing and de-
pendency parsing information. Kim and Hovy 
59
(2006) map the semantic frames of FrameNet 
(Baker et al, 1998) into opinion holder and target 
for adjectives and verbs to identify these compo-
nents.  Stoyanov and Cardie (2008) treat the task of 
identifying opinion holders and targets as a co-
reference resolution problem. Kim et al, (2008) 
used a set of communication words, appraisal 
words from Senti-WordNet (Esuli and Sebastiani, 
2006) and NLP tools such as NE taggers and syn-
tactic parsers to identify opinion holders accu-
rately. Kim and Hovy (2006) use structural 
features of the language to identify opinion enti-
ties. Their technique is based on syntactic path and 
dependency features along with heuristic features 
such as topic words and named entities. Weigand 
and Klalow (2010) use convolution kernels that 
use predicate argument structure and parse trees. 
For Urdu specifically, work in the area of clas-
sifying subjective and objective sentences is at-
tempted by Mukund and Srihari, (2010) using a 
vector space model. NLP tools that include POS 
taggers, shallow parser, NE tagger and morpho-
logical analyzer for Urdu is provided by Mukund 
et al, (2010). This is the only extensive work done 
for automating Urdu NLP, although other efforts to 
generate semantic role labels and dependency 
parsers are underway. 
3  Linguistic Analysis for Opinion Entities 
In this section we introduce the different cues 
used to capture the contextual information for cre-
ating candidate sequences in Urdu by exploiting 
the morphological richness of the language. 
Table 2: Case Inflections on Nouns  
Urdu is a head final language with post-
positional case markers. Some post-positions are 
associated with grammatical functions and some 
with specific roles associated with the meaning of 
verbs (Davison, 1999). Case markers play a very 
important role in determining the case inflections 
of nouns. The case inflections that are useful in the 
context of opinion entity detection are ?ergative?, 
?dative?, ?genitive?, ?instrumental?  and ?loca-
tive? . Table 2 outlines the constructs. 
Consider example 1 below. (a) is a case where 
? Ali ? is nominative. However, in (b) ? Ali?  is da-
tive. The case marker ?ko?  helps to identify sub-
jects of certain experiential and psychological 
predicates: sensations, psychological or mental 
states and obligation or compulsion. Such predi-
cates clearly require the subject to be sentient, and 
further, indicate that they are aected in some 
manner, correlating with the semantic properties 
ascribed to the dative?s primary use (Grimm, 
2007). 
 
E xample (1): 
(a)  Ali khush hua  (Ali became happy) 
(b)  Ali ko khushi hui (Ali became happy) 
E xample (2): 
(a) Sadaf kaam karne ki koshish karti hai ( Sadaf 
tries to do work)  
 
Semantic information in Urdu is encoded in a 
way that is very different from English. Aspect, 
tense and gender depend on the noun that a verb 
governs. Example 2 shows the dependency that 
verbs have on nouns without addressing the lin-
guistic details associated with complex predicates. 
In example 2, the verb ?karti?(do)  is feminine 
and the noun it governs ~ Sadaf  is also feminine. 
The doer for the predicate ?karti hai?(does)  is 
? Sadaf? and there exists a gender match. This 
shows that we can obtain strong features if we are 
able to accurately (i) identify the predicates, (ii) 
find the governing noun, and (iii) determine the 
gender. 
In this work, for the purpose of generating can-
didate sequences, we encompass the post-position 
responsible for case inflection in nouns, into the 
noun phrase and group the entire chunk as one sin-
gle candidate. In example 1, the dative inflection 
on ? Ali?  is due to the case marker ? ko?.  Here, ? Ali 
ko?  will always be considered together in all candi-
date sequences that this sentence generates. This 
Case Clitic 
Form 
Examples 
Ergative (ne) Ali  ne ghussa dikhaya ~ 
Ali showed anger  
Accusa-
tive 
(ko) Ali ko mainey maara ~ 
I hit Ali  
Dative (ko,ke) Similar to accusative 
Instru-
mental 
(se) Yeh kaam Ali  se hua ~ 
This work was done by 
Ali  
Genitive (ka, ke, ki) Ali ka ghussa, baap re 
baap! ~ Ali?s anger, oh 
my God!  
Locative (mein, par, 
tak, tale, 
talak) 
Ali mein ghussa zyaada 
hai ~ there is a lot of 
anger in Ali  
60
behavior can also be observed in example 1 of ta-
ble 1.  
We use SemantexTM (Srihari et al, 2008) - an 
end to end NLP framework for Urdu that provides 
POS, NE, shallow parser and morphological ana-
lyzer, to mark tense, mood, aspect, gender and 
number inflections of verbs and case inflections of 
nouns. For ease of parsing, we enclose dative and 
accusative inflected nouns and the respective case 
markers in a tag called POSSE S S . We also enclose 
locative, genitive and ergative inflections and case 
markers in a tag called DOER.  
4  Methodology 
Sequence boundaries are first constructed based 
on the POSSESS, DOER and NP (noun chunk) 
tags prioritized by the position of the tag while 
parsing. We refer to these chunks as ?candidates?  
as they are the possible opinion entity candidates. 
We generate candidate sequences by combining 
these candidates with opinion expressions (Mu-
kund and Srihari, 2010) and the predicates that 
contain or follow the expression words (~khushi in 
(b) of example 1 above). 
We evaluate our approach in two steps: 
(i) Boundary Detection - detecting opinion 
entities that contain both holders and tar-
gets 
(ii) Entity Disambiguation - disambiguating 
opinion holders from opinion targets 
In the following sections, we briefly describe 
our research methodology including sequence 
creation, choice of kernels and the challenges thus 
encountered. 
4.1  Data Set 
The data used for the experiments are newswire 
articles from BBC Urdu1 that are manually anno-
tated to reflect opinion holders, targets, and ex-
pressions (emotion bearing words).  
 
Number of  subjective sentences 824 
Average word length of each sentence 42 
Number of opinion holders  974 
Number of opinion targets 833 
Number of opinion expressions 894 
Table 3: Corpus Statistics 
 
                                                           
1 www.bbc.co.uk/urdu/ 
Table 3 summarizes the corpus statistics. The inter 
annotator agreement established between two an-
notators over 30 documents was found to be 0.85 
using Cohen?s Kappa score (averaged over all 
tags). The agreement is acceptable as tagging emo-
tions is a difficult and a personalized task. 
4.2  Support Vector Machines (SVM) and 
Kernel Methods 
SVMs belong to a class of supervised machine 
learning techniques that merge the nuances of sta-
tistical learning theory, kernel mapping and opti-
mization techniques to discover separating 
hyperplanes. Given a set of positive and negative 
data points, based on structural risk minimization, 
SVMs attempt to find not only a separating hyper-
plane that separates two categories (Vapnik and 
Kotz, 2006) but also maximize the boundary be-
tween them (maximal margin separation tech-
nique). In this work, we propose to use a variation 
of sequence kernels for opinion entity detection. 
4.3  Sequence Kernels 
The lack of parsers that capture dependencies in 
Urdu sentences inhibit the use of ?tree kernels? 
(Weigand and Klalow, 2010). In this work, we ex-
ploit the power of a set of sequence kernels known 
as ?gap sequence string kernels? (Lodhi et al, 
2002). These kernels provide numerical compari-
son of phrases as entire sequences rather than a 
probability at the chunk level. Gap sequence ker-
nels measure the similarity between two sequences 
(in this case a sequence of Urdu words) by count-
ing the number of common subsequences. Gaps 
between words are penalized with suitable use of 
decay factor to compensate for 
matches between lengthy word sequences. 
Formally, let  be the feature space over 
words. Consequently, we declare other disjoint 
feature spaces  (stem words, POS, 
chunks, gender inflections, etc.) 
and
.
For any two-feature 
vectors  let  compute the number 
of common features between s and t. Table 5 lists 
the features used to compute . 
Given two sequences, s and t and the kernel 
function  that calculates the number of 
61
weighted sparse subsequences of length n (say, 
n =2: bigram) common to both s and t, then 
is as shown in eq 1 (Bunescu and 
Mooney, 2005). 
 
(i,j,k are dimensions)                                ------ Eq 1. 
Generating correct sequences is a prior require-
ment for sequence kernels. For example, in the task 
of relation extraction, features included in the 
shortest path between the mentions of the two se-
quences (which hold the relation) play a decisive 
role (Bunescu and Mooney, 2005). Similarly, in 
the task of role labeling (SRL - Moschitti et al, 
2008), syntactic sub-trees containing the arguments 
are crucial in finding the correct associations. Our 
approach to create candidate sequences for opinion 
entity detection in Urdu is explained in the next 
section. 
4.4  Candidate Sequence Generation 
Each subjective sentence in Urdu contains sev-
eral noun phrases with one or more opinion ex-
pressions. The words that express opinions 
(expression words) can be contained within a verb 
predicate (if the predicate is complex) or precede 
the verb predicate. These subjective sentences are 
first pre-processed to mark the morphological in-
flections as mentioned in ?3. 
Table 4: Candidate Sequence Generation 
 
We define training candidate sequences as the 
shortest substring t which is a tuple that contains 
the candidate noun phrase (POSSESS, DOER or 
NP), an emotion expression and the closest predi-
cate. Table 4 outlines the steps taken to create the 
candidate sequences and figure 1 illustrates the 
different tuples for a sample sentence.  
Experiments conducted by Weigand and 
Klakow (2010) consider <candidate, predicate> 
and <candidate, expression> tuples. However, in 
Urdu the sense of expression and predicate are so 
tightly coupled (in many examples they subsume 
each other and hence inseparable), that specifically 
trying to gauge the influence of predicate and 
expression separately on candidates is impossible. 
There are three advantages in our approach to 
creating candidate sequences: (i) by pairing ex-
pressions with their nearest predicates, several un-
necessary candidate sequences are eliminated, (ii) 
phrases that do not contain nouns are automatically 
not considered (see RBP chunk in figure 1), and 
(iii) by considering only one candidate chunk at a 
time in generating the candidate sequence, we en-
sure that the sequence that is generated is short for 
better sequence kernel performance. 
 
4 . 4 .1  Linear Kernel features 
 
For linear kernels we define features explicitly 
based on the lexical relationship between the can-
didate and its context. Table 5 outlines the features 
used.  
 
Feature Sets and Description  
Set 1 
Baseline 
1. head word of candidate 
2. case marker contained within candidate? 
3. expression words  
4. head word of predicate 
5. POS sequence of predicate words 
6. # of NPs between candidate and emotion 
Set 2 7. the DOER 
8. expression right after candidate? 
Set 3  9. gender match between candidate and 
predicate 
10. predicate contains emotion words? 
Set 4  11. POS sequence of candidate 
Set 5   12. ?kah?  feature in the predicate 
13. locative feature? 
14. genitive feature on noun? 
Table 5: Linear Kernel Features 
 
 
 
 
1 A sentence is parsed to extract all likely candi-
date chunks ? POSSESS, DOER, NP in that 
order. 
2 <expression, predicate> t uples are first selected 
based on nearest neighbor rule : 
1. Predicates that are paired with the expres-
sion words either contain the expressions or 
follow the expressions.  
2. Stand alone predicates are simply ignored as 
they do not contribute to the holder identifi-
cation task (they contribute to either the sen-
tence topic or the reason for the emotion). 
3 For each candidate, 
<candidate, expression, predicate> tuples are 
generated without changing the word order.  
(Fig. 1 ? example candidates maintain the same 
word order) 
62
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: Illustration of candidate sequences 
 
4 . 4 .1  Sequence Kernel features 
 
Features commonly used for sequence kernels 
are based on words (such as character-based or 
word-based sequence kernels). In this work, we 
consider to be a feature space over Urdu words 
along with other disjoint features such as POS, 
gender, case inflections. In the kernel, however, for 
each combination (see table 6) the similarity 
matching function that computes the num-
ber of similar features remains the same. 
Table 6: Disjoint feature set for sequence kernels 
 
Sequence kernels are robust and can deal with 
complex structures. There are several overlapping 
features between the feature sets used for linear 
kernel and sequence kernel.  Consider the POS 
path information feature. This is an important fea-
ture for the linear kernel. However this feature  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
need not be explicitly mentioned for the sequence 
kernel as the model internally learns the path in-
formation. In addition, several Boolean features 
explicitly described for the linear kernel (2 and 13 
in table 5) are also learned automatically in the 
sequence kernel by matching subsequences. 
5  Experiments 
The data used for our experiments is explained 
in ?4.1. Figure 2 gives a flow diagram of the entire 
process. LIBSVM?s (Cha ng and Lin, 2001) linear 
kernel is trained using the manually coded features 
mentioned in table 5. We integrated our proposed 
sequence kernel with the same toolkit. This se-
quence kernel uses the features mentioned in table 
6 and the decay factor is set to 0.5. 
 
 
 
 
 
 
 
 
 
Figure 2: Overall Process 
KID Kernel Type 
1 word based kernel (baseline) 
2 word + POS (parts of speech) 
3 word + POS + chunk  
4 word + POS + chunk + gender inflection  
63
The candidate sequence generation algorithm gen-
erated 8,329 candidate sequences (contains all opi-
nion holders and targets ? table 3) that are used for 
training both the kernels. The data is parsed using 
SemantexTM to apply POS, chunk and morphology 
information. Our evaluation is based on the exact 
candidate boundary (whether the candidate is en-
closed in a POSSESS, DOER or NP chunk).All 
scores are averaged over a 5-fold cross validation 
set. 
5.1  Comparison of Kernels 
We apply both linear kernels (LK) and se-
quence kernels (SK) to identify the entities as well 
as disambiguate between the opinion holders and 
targets. Table 7 illustrates the baselines and the 
best results for boundary detection of opinion enti-
ties. ID 1 of table 7 represents the result of using 
LK with feature set 1 (table 5). We interpret this as 
our baseline result. The best F1 score for this clas-
sifier is 50.17%. 
Table 7: Boundary detection of Opinion Entities 
 
Table 8 compares various kernels and combina-
tions. Set 1 of table 8 shows the relative effect of 
feature sets for LK and how each set contributes to 
detecting opinion entity boundaries. Although sev-
eral features are inspired by similar classification 
techniques (features used for SRL and opinion 
mining by Choi et al, (2005) ~ set 1, table 5), the 
free word nature of Urdu language renders these 
features futile. Moreover, due to larger average 
length of each sentence and high occurrences of 
NPs (candidates) in each sentence, the number of 
candidate instances (our algorithm creates 10 se-
quences per sentence on average) is also very high 
as compared to any English corpus.  This makes 
the training corpus highly imbalanced. Interest-
ingly, when features like ? occurrence of postposi-
tions, ?kah? predicate, gender inflections etc. are 
used, classification improves (set 1, Feature set 
1,2,3,4,5, table 8). 
Table 8: Kernel Performance 
 
ID 3 of table 7 displays the baseline result for SK. 
Interestingly enough, the baseline F1 for SK is 
very close to the best LK performance. This shows 
the robustness of SK and its capability to learn 
complex substructures with only words. A se-
quence kernel considers all possible subsequence 
matching and therefore implements a concept of 
partial (fuzzy) matching. Because of its tendency 
to learn all fuzzy matches while penalizing the 
gaps between words intelligently, the performance 
of SK in general has better recall (Wang, 2008). To 
explain the recall situation, consider set 2 of table 
8. This illustrates the effect of disjoint feature 
scopes of each feature (POS, chunk, gender). Each 
feature adds up and expands the feature space of 
sequence kernel and allows fuzzy matching there-
by improving the recall. Hence KID 4 has almost 
20% recall gain over the baseline (SK baseline).  
However, in many cases, this fuzzy matching 
accumulates in wrong classification and lowers 
precision. A fairly straightforward approach to 
overcome this problem is to employ a high preci-
sion kernel in addition to sequence kernel. Another 
limitation of SK is its inability to capture complex 
I
D Kernel 
Features  
(table 
5/6 ) 
Prec. 
(% ) 
Rec. 
(% ) 
F1 
(% ) 
1 LK Baseline (Set 1) 39.58 51.49 44.75  
2 LK(best) Set 1, 2, 3, 4, 5 44.20 57.99 50.17  
3  SK Baseline (KID 1) 58.22 42.75 49.30  
4 SK (best) KID 4 54.00 62.79 58.06  
5 
Best LK 
+ best 
SK 
KID 4, 
Set 1, 2, 
3, 4, 5 
5 8 . 4 3  65 . 0 4  61.55  
Set Kernel KID Prec.  
(% )  
Rec.  
(% )  
F1 
(% )  
Baseline 
(Set 1) 
39.58  51.49  44.75  
Set 1,2 39.91  52.57  45.38  
Set 1, 2, 3 43.55  57.72  49.65  
Set 1,2,3,4 44.10  56.90  49.68  
 
 
 
 
1 
 
 
 
 
LK 
Feature set 
1,2,3,4,5 
4 4 . 2 0  57 . 9 9  50 .17  
Baseline - 
KID 1 
58.22 42.75  49.30  
KID 2 5 8 . 9 8  47.55  52.65 
KID 3 58.18  49.62  53.59  
 
 
2 
 
 
SK 
KID 4 54.00 6 2 . 7 9  58 . 0 6  
KID 1 + 
best LK 
51.44 6 8 . 8 9  58.90  
KID 2 + 
best LK 
5 9 .18  62.98  61.02 
KID 3 + 
best LK 
55.18 68.38  61.07  
 
 
 
3 
 
 
 
SK  +  
LK 
KID 4 + 
best LK 
58.43  65.04 61.55  
64
grammatical structure and dependencies making it 
highly dependent on only the order of the string 
sequence that is supplied. 
We also combine the similarity scores of SK 
and LK to obtain the benefits of both kernels. This 
permits SK to expand the feature space by natu-
rally adding structural features (POS, chunk) re-
sulting in high recall. At the same time, LK with 
strict features (such as the use of ?kah?  verb) or 
rigid word orders (several Boolean features) will 
help maintain acceptable precision. By summing 
the contribution of both kernels, we achieve an F1 
of 61.55% (Set 3, table 8), which is 17.8%, more 
(relative gain ? around 40%) than the LK baseline 
results (ID 1, table 7). 
 
Table 9: Opinion entity disa mbiguation for best features 
Our next sets of experiments are conducted to dis-
ambiguate opinion holders and targets. A large 
number of candidate sequences that are created are 
not candidates for opinion entities. This results in a 
huge imbalance in the data set. Jointly classify 
opinion holders, opinion targets and false candi-
dates with one model can be attempted if this im-
balance in the data set due to false candidates can 
be reduced. However, this has not been attempted 
in this work. In order to showcase the feasibility of 
our method, we train our model only on the gold 
standard candidate sequences that contain opinion 
entities for entity disambiguation. 
The two kernels are applied on just the two 
classes (opinion holder vs. opinion target). Com-
bined kernels identify holders with a 65.26% F1 
(table 9). However, LK performs best for target 
identification (61.23%). We believe that this is due 
to opinion holders and targets sharing similar syn-
tactic structures. Hence, the sequence information 
that SK learns affects accuracy but improves recall. 
6  Challenges 
Based on the error analysis, we observe some 
common mistakes and provide some examples. 
1. Mistakes resulting due to POS tagger and shal-
low chunker errors. 
2. Errors due to heuristic rules for morphological 
analysis. 
3.  Mistakes due to inaccurate identification of ex-
pression words by the subjectivity classifier. 
4. Errors due to complex and unusual sentence 
structures which the kernels failed to capture. 
 
Example (3):  
Is na-insaafi ka badla hamein zaroor layna chahiye. 
[ we  have to certainly take revenge for this injustice. ] 
E xample (4): 
Kya hum dayshadgardi ka shikar banna chahatein 
hai? 
[Do we  want to become victims of terrorism ? ] 
E xample (5): 
Jab secretary kisi aur say baat karke husthi hai, tho 
Pinto ko ghussa aata hai. 
[When the secretary talks to someone and laughs, 
Pinto  gets angry.] 
 
Example 3 is a false positive. The emotion is ?an-
ger?, indicated by ?na-insaafi ka badla? (revenge 
for injustice) and ?zaroor? (certainly) .  But only 
the second expression word is identified accu-
rately. The sequence kernel model determines na-
insaafi (injustice) to be the opinion holder when it 
is actually the reason for the emotion. However, it 
also identifies the correct opinion holder - hamein 
(we) .  Emotions associated with interrogative sen-
tences are not marked (example 4) as there exists 
no one word that captures the overall emotion. 
However, the subjectivity classifier identifies such 
sentences as subjective candidates. This results in 
false negatives for opinion entity detection. The 
target (secretary) in example 5, fails to be detected 
as no candidate sequence that we generate indi-
cates the noun ?secretary?  to be the target. We 
propose to address these issues in our future work. 
7  Conclusion 
We describe an approach to identify opinion en-
tities in Urdu using a combination of kernels. To 
the best of our knowledge this is the first attempt 
where such an approach is used to identify opinion 
entities in a language lacking the availability of 
resources for automatic text processing. The per-
formance for this task for Urdu is equivalent to the 
state of the art performance for English (Weigand 
and Klakow, 2010) on the same task.  
Kernel Opinion 
Entity 
Prec. 
(% ) 
Rec. 
(% ) 
F1 
(% ) 
Holder 58.71 66.67 62.44 LK 
(best) Target 6 5 . 5 3 57.48 61.23 
Holder 60.26 69.46 64.54 SK 
 Target 59.75 49.73 54.28 
Holder 62.90 6 9 . 81 65. 2 6 Both 
kernels Target 60.71 55.44 57.96 
65
References  
Collin F. Baker, Charles J. Fillmore, John B. Lowe. 
1998. The Berkeley FrameN et Project, Proceedings 
of the 17th international conference on Computa-
tional linguistics, August 10-14. Montreal, Quebec, 
Canada 
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios 
Hatzivassiloglou, and Dan Jurafsky. 2004. Automatic 
Extraction of Opinion Propositions and their Holders, 
AAAI Spring Symposium on Exploring Attitude and 
Affect in Text: Theories and Applications. 
Kenneth Bloom, Sterling Stein, and Shlomo Argamon. 
2007. Appraisal Extraction for News Opinion Analy-
sis at NTCIR-6. In Proceedings of NTCIR-6 Work-
shop Meeting, Tokyo, Japan. 
R. C. Bunescu and R. J. M ooney. 2005. A shortest path 
dependency kernel for relation extraction. In Pro-
ceedings of HLT/EMNLP. 
R. C. Bunescu and R. J.  Mooney. 2005. Subsequence 
Kernels for Relation Extraction. NIPS. Vancouver. 
December. 
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM: 
a library for support vector machines. Software 
available at http://www.cs ie.ntu.edu.tw/~cjlin/libsvm 
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth 
Patwardhan. 2005. Identifying Sources of Opinions 
with Conditional Random Fields and Extraction Pat-
terns. In Proceedings of the Conference on Human 
Language Technology and Empirical Methods in 
Natural Language Processing (HLT/EMNLP), Van-
couver, Canada.  
Aaron Culotta and Jeffery Sorensen. 2004. Dependency 
tree kernels for relation extraction. In Proceedings of 
the 42rd Annual Meeting of the Association for 
Computational Linguistics. pp. 423-429.   
Alice Davison. 1999. Syntax  and Morphology in Hindi 
and Urdu: A Lexical Resource.  University of Iowa.  
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiWordNet: A publicly available lexical resource for 
opinion mining. In Proc of LREC. Vol 6, pp 417-422.  
Scott Gimm. 2007. Subject Ma rking in Hindi/Urdu: A 
Study in Case and Agency. ESSLLI Student Session. 
Malaga, Spain. 
Youngho Kim, Seaongchan Kim and Sun-Hyon 
Myaeng. 2008. Extracting Topic-related Opinions 
and their Targets in NTCIR-7. In Proceedings of the 
7th NTCIR Workshop Meeting. Tokyo. Japan. 
John Lafferty, Andrew McCa llum and F. Pereira. 2001. 
Conditional random fields: Probabilistic models for 
segmenting and labeling sequence data. In: Proc. 
18th International Conf. on Machine Learning, Mor-
gan Kaufmann, San Francisco, CA . pp. 282?289 
Huma Lodhi, Craig Saunders, John Shawe-Taylor, 
Nello Cristianini, Chris Watkins. 2002. Text 
classification using string kernels. J. Mach. Learn. 
Res. 2 (March 2002), 419-44. 
Kim, Soo-Min. and Eduard Hovy. 2006. Extracting 
Opinions, Opinion Holders, and Topics Expressed in 
Online News Media Text. In ACL Workshop on Sen-
timent and Subjectivity in Text. 
Alessandro Moschitti, Daniele Pighin, Roberto Basili. 
2008. Tree kernels for semantic role labeling. Com-
putational Linguistics. Vol 34, num 2, pp 193-224. 
Smruthi Mukund and Rohini K. Srihari. 2010. A Vector 
Space Model for Subjectivity Classification in Urdu 
aided by Co-Training, In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics, 
Beijing, China.  
Smruthi Mukund, Rohini K. Srihari and Erik Peterson. 
2010. An Information Extraction System for Urdu ? 
A Resource Poor Language. Special Issue on Infor-
mation Retrieval for Indian Languages. 
Ellen Riloff, Janyce Wiebe and Theresa Wilson. 2003. 
Learning subjective nouns using extraction pattern 
bootstrapping. In Proceedings of the Seventh Confer-
ence on Natural Language Learning (CoNLL-03). 
Rohini K. Srihari, W. Li, C. Niu, and T. Cornell. 2008. 
InfoXtract: A Customizable Intermediate Level In-
formation Extraction Engine, Journal of Natural Lan-
guage Engineering, Cambridge U. Press, 14(1), pp. 
33-69. 
Veselin Stoyanov and Claire Cardie. 2008.  Annotating 
Topic Opinions. In Proceedings of the Sixth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2008), Marrakech, Morocco. 
John Shawe-Taylor and Nello  Cristianni. 2004. Kernel 
methods for pattern analysis. Cambridge University 
Press. 
Mengqiu Wang. 2008. A Re-examination of Depend-
ency Path Kernels for Relation Extraction, In 
Proceedings of IJCNLP 2008. 
Michael Wiegand and Dietrich Klalow. 2010. Convolu-
tion kernels for opinion holder extraction. In Proc. of 
Human Language Technologies: The 2010 Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics. pp 795-
803, ACL 
66
Vladimir Vapnik, S.Kotz . 2006. Estimation of De-
pendences Based on Empirical Data. Springer,  510 
pages. 
67
Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 1?8,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Analyzing Urdu Social Media for Sentiments using Transfer Learning 
with Controlled Translations 
 
 Author 2 
Smruthi Mukund Rohini K Srihari 
CEDAR, Davis Hall, Suite 113 CEDAR, Davis Hall, Suite 113 
University at Buffalo, SUNY, Buffalo, NY University at Buffalo, SUNY, Buffalo, NY 
smukund@buffalo.edu rohini@cedar.buffalo.edu 
 
 
 
 
 
Abstract 
The main aim of this work is to perform sen-
timent analysis on Urdu blog data. We use the 
method of structural correspondence learning 
(SCL) to transfer sentiment analysis learning 
from Urdu newswire data to Urdu blog data. 
The pivots needed to transfer learning from 
newswire domain to blog domain is not trivial 
as Urdu blog data, unlike newswire data is 
written in Latin script and exhibits code-
mixing and code-switching behavior. We con-
sider two oracles to generate the pivots. 1. 
Transliteration oracle, to accommodate script 
variation and spelling variation and 2. Trans-
lation oracle, to accommodate code-switching 
and code-mixing behavior.  In order to identi-
fy strong candidates for translation, we pro-
pose a novel part-of-speech tagging method 
that helps select words based on POS catego-
ries that strongly reflect code-mixing behav-
ior. We validate our approach against a 
supervised learning method and show that the 
performance of our proposed approach is 
comparable. 
1 Introduction 
The ability to break language barriers and under-
stand people's feelings and emotions towards soci-
etal issues can assist in bridging the gulf that exists 
today. Often emotions are captured in blogs or dis-
cussion forums where writers are common people 
empathizing with the situations they describe. As 
an example, the incident where a cricket team vis-
iting Pakistan was attacked caused widespread an-
guish among the youth in that country who thought 
that they will no longer be able to host internation-
al tournaments. The angry emotion was towards 
the failure of the government to provide adequate 
protection for citizens and visitors. Discussion fo-
rums and blogs on cricket, mainly written by Paki-
stani cricket fans, around the time, verbalized this 
emotion. Clearly analyzing blog data helps to esti-
mate emotion responses to domestic situations that 
are common to many societies. 
Traditional approaches to sentiment analysis re-
quire access to annotated data. But facilitating such 
data is laborious, time consuming and most im-
portantly fail to scale to new domains and capture 
peculiarities that blog data exhibits; 1. spelling var-
iations and 2. code mixing and code switching. 3. 
script difference (Nastaliq vs Latin script). In this 
work, we present a new approach to polarity classi-
fication of code-mixed data that builds on a theory 
called structural correspondence learning (SCL) 
for domain adaptation. This approach uses labeled 
polarity data from the base language (in this case, 
Urdu newswire data - source) along with two sim-
ple oracles that provide one-one mapping between 
the source and the target data set (Urdu blog data).  
Subsequent sections are organized as follows. 
Section 2 describes the issues seen in Urdu blog 
data followed by section 3 that explains the con-
cept of structural correspondence learning. Section 
4 details the code mixing and code switching be-
havior seen in blog data. Section 5 describes the 
statistical part of speech (POS) tagger developed 
for blog data required to identify mixing patterns 
followed by the sentiment analysis model in sec-
tion 6. We conclude with section 7 and briefly out-
line analysis and future work in section 8. 
1
2 Urdu Blog Data 
Though non-topical text analysis like emotion de-
tection and sentiment analysis, have been explored 
mostly in the English language, they have also 
gained some exposure in non-English languages 
like Urdu (Mukund and Srihari, 2010), Arabic 
(Mageed et al, 2011) and Hindi (Joshi and 
Bhattacharya, 2012). Urdu newswire data is writ-
ten using Nastaliq script and follows a relatively 
strict grammatical guideline. Many of the tech-
niques proposed either depend heavily on NLP 
features or annotated data. But, data in blogs and 
discussion forums especially written in a language 
like Urdu cannot be analyzed by using modules 
developed for Nastaliq script for the following rea-
sons; (1) the tone of the text in blogs and discus-
sion forums is informal and hence differs in the 
grammatical structure (2) the text is written using 
Latin script (3) the text exhibits code mixing and 
code switching behavior (with English) (4) there 
exists spelling errors which occur mostly due to the 
lack of predefined standards to represent Urdu data 
in Latin script. 
Urdish (Urdu blog data) is the term used for 
Urdu, which is (1) written either in Nastaliq or Lat-
in script, and (2) contains several English 
words/phrases/sentences.  In other words, Urdish is 
a name given to a language that has Urdu as the 
base language and English as the seasoning lan-
guage. With the wide spread use of English key-
boards these days, using Latin script to encode 
Urdu is very common. Data in Urdish is never in 
pure Urdu. English words and phrases are com-
monly used in the flow integrating tightly with the 
base language. Table 1 shows examples of differ-
ent flavors in which Urdu appears in the internet. 
Differ-
ent 
Forms 
of Data 
Main Issues Example Sentence 
1. Urdu 
written 
in Nasta-
liq 
1.  Lack of tools for 
basic operations such 
as segmentation and 
diacritic restoration 
2. Lack of sufficient 
annotated data for 
POS and NE tagging 
3.  Lack of annotated 
data for more ad-
vanced NLP  
??? ?????? ?? ???? 
????? ?? ??? ???? 
[ The soldiers were 
angry with a lot of 
people] 
 
2. Urdu 
written 
in ASCII 
1.  Several variations 
in spellings that need 
to be normalized 
Wo Mulk Jisko Hum 
nay 1000000 logoon 
sey zayada Loogoon 
(Eng-
lish) 
2.  No normalization 
standards 
3.  Preprocessing 
modules needed if 
tools for Urdu in 
Nastaliq are to be 
used 
4.  Developing a 
completely new NLP 
framework needs 
annotated data 
ki Qurbanian dey ker 
hasil kia usi mulk 
main yai kaisa waqt a 
gay hai ? 
 
[Look at what kind of 
time the land that had 
1000000?s of people 
sacrifice their lives is 
experiencing now] 
3. Urd-
ish writ-
ten in 
Nastaliq 
1.  No combined 
parser that deals with 
English and Urdu 
simultaneously 
2.  English is written 
in Urdu but with 
missing diacritics 
?? ??? ??? ????? ?? ?? 
??? ??? ???  
 
[the phones rang one 
after the other in the 
TV station] 
 
4. Urd-
ish writ-
ten in  
ASCII(
English) 
1.  No combined 
parser that deals 
with English and 
Urdu simultaneous-
ly 
2.  Issue of spelling 
variations that need 
to be normalized 
Afsoos key baat hai . 
kal tak jo batain 
Non Muslim bhi 
kartay hoay dartay 
thay abhi this man 
has brought it out in 
the open. 
 
[It is sad to see that 
those words that even 
a non muslim would 
fear to utter till yes-
terday, this man had 
brought it out in the 
open] 
Table 1: Different forms of using Urdu lan-
guage on the internet 
Blog data follows the order shown in example 
4 of table 1. Such a code-switching phenomenon is 
very common in multilingual societies that have 
significant exposure to English. Other languages 
exhibiting similar behaviors are Hinglish (Hindi 
and English), Arabic with English and Spanglish 
(Spanish with English). 
3   Structural Correspondence Learning 
For a problem where domain and data changes 
requires new training and learning, resorting to 
classical approaches that need annotated data be-
comes expensive. The need for domain adaptation 
arises in many NLP tasks ? part of speech tagging, 
semantic role labeling, dependency parsing, and 
sentiment analysis and has gained high visibility in 
the recent years (Daume III and Marcu, 2006; 
Daume III et al, 2007; Blitzer et al, 2006, Pret-
tenhofer and Stein et al, 2010). There exists two 
main approaches; supervised and semi-supervised.  
2
In the supervised domain adaptation approach 
along with labeled source data, there is also access 
to a small amount of labeled target data. Tech-
niques proposed by Gildea (2001), Roark and Bac-
chiani (2003), Daume III (2007) are based on the 
supervised approach. Studies have shown that 
baseline approaches (based on source only, target 
only or union of data) for supervised domain adap-
tion work reasonably well and beating this is sur-
prisingly difficult (Daume III, 2007).  
 In contract, the semi supervised domain adapta-
tion approach has access to labeled data only in the 
source domain (Blitzer et al, 2006; Dredze et al, 
2007; Prettenhofer and Stein et al, 2010). Since 
there is no access to labeled target data, achieving 
baseline performance exhibited in the supervised 
approach requires innovative thinking.  
The method of structural correspondence learn-
ing (SCL) is related to the structural learning para-
digm introduced by Ando and Zhang (2005). The 
basic idea of structural learning is to constrain the 
hypothesis space of a learning task by considering 
multiple different but related tasks on the same 
input space. SCL was first proposed by Blitzer et 
al., (2006) for the semi supervised domain adapta-
tion problem and works as follows (Shimizu and 
Nakagawa, 2007).  
1. A set of pivot features are defined on unla-
beled data from both the source domain and 
the target domain 
2. These pivot features are used to learn a map-
ping from the original feature spaces of both 
domains to a shared, low-dimensional real?
valued feature space. A high inner product in 
this new space indicates a high degree of cor-
respondence along that feature dimension  
3. Both the transformed and the original features 
in the source domain are used to train a learn-
ing model  
4. The effectiveness of the classifier in the source 
domain transfers to the target domain based on 
the mapping learnt 
This approach of SCL was applied in the field of 
cross language sentiment classification scenario by 
Prettenhofer and Stein (2010) where English was 
used as the source language and German, French 
and Japanese as target languages. Their approach 
induces correspondence among the words from 
both languages by means of a small number of 
pivot pairs that are words that process similar se-
mantics in both the source and the target lan-
guages. The correlation between the pivots is mod-
eled by a linear classifier and used as a language 
independent predictor for the two equivalent clas-
ses. This approach solves the classification prob-
lem directly, instead of resorting to a more general 
and potentially much harder problem such as ma-
chine translation. 
The problem of sentiment classification in blog 
data can be considered as falling in the realm of 
domain adaptation. In this work, we approach this 
problem using SCL tailored to accommodate the 
challenges that code-mixed data exhibits. Similar 
to the work done by Prettenhofer and Stein (2010), 
we look at generating pivot pairs that capture code-
mixing and code-switching behavior and language 
change.  
4 Code Switching and Code Mixing 
Code switching refers to the switch that exists from 
one language to another and typically involves the 
use of longer phrases or clauses of another lan-
guage while conversing in a totally different base 
language. Code mixing, on the other hand, is a 
phenomenon of mixing words and other smaller 
units of one language into the structure of another 
language. This is mostly inter-sentential.  
In a society that is bilingual such as that in Pa-
kistan and India, the use of English in the native 
language suggests power, social prestige and the 
status. The younger crowd that is technologically 
well equipped tends to use the switching phenom-
enon in their language, be it spoken or written. 
Several blogs, discussion forums, chat rooms etc. 
hold information that is expressed is intensely code 
mixed. Urdu blog data exhibits mix of Urdu lan-
guage with English. 
There are several challenges associated with 
developing NLP systems for code-switched lan-
guages. Work done by Kumar (1986) and Sinha & 
Thakur, (2005) address issues and challenges asso-
ciated with Hinglish (Hindi ? English) data.  
Dussias (2003) and Celia (1997) give an overview 
of the behavior of code switching occurring in 
Spanish - Spanglish. This phenomenon can be seen 
in other languages like Kannada and English, 
German and English.  Rasul (2006) analyzes the 
linguistic patterns occurring in Urdish (Urdu and 
English) language. He tries to quantize the extent 
to which code-mixing occurs in media data, in par-
ticular television. Most of his rules are based on 
3
what is proposed by Kachru (1978) for Hinglish 
and has a pure linguistic approach with manual 
intervention for both qualitative and quantitative 
analysis.  
Several automated techniques proposed for 
Hinglish and Spanglish are in the context of ma-
chine translation and may not be relevant for a task 
like information retrieval since converting the data 
to one standardized form is not required. A more 
recent work was by Goyal et al, (2003) where they 
developed a bilingual parser for Hindi and English 
by treating the code mixed language as a complete-
ly different variety. However, the credibility of the 
system depends on the availability of WordNet1.  
4.1 Understanding Mixing Patterns 
Performing analysis on data that exhibit code-
switching has been attempted by many across vari-
ous languages. Since the Urdu language is very 
similar to Hindi, in this section we discuss the 
code-mixing behavior based on a whole battery of 
work done by researchers in the Hindi language. 
Researchers have studied the behavior of the 
mixed patterns and generated rules and constraints 
on code-mixing. The study of code mixing with 
Hindi as the base language is attempted by Sinha 
and Thakur (2005) in the context of machine trans-
lation. They categorize the phenomenon into two 
types based on the extent to which mixing happens 
in text in the context of the main verb. Linguists 
such as Kachru (1996) and Poplack (1980) have 
tried to formalize the terminologies used in this 
kind of behavior. Kumar (1986) says that the moti-
vation for assuming that the switching occurs 
based on certain set of rules and constraints are 
based on the fact that users who use this can effec-
tively communicate with each other despite the 
mixed language. In his paper he proposes a set of 
rules and constraints for Hindi-English code 
switching. However, these rules and constraints 
have been countered by examples proposed in the 
literature (Agnihotri, 1998). This does not mean 
that researchers earlier had not considered all the 
possibilities. It only means that like any other lan-
guage, the language of code-mixing is evolving 
over time but at a very fast pace. 
One way to address this problem of code-mixing 
and code switching for our task of sentiment analy-
                                                          
1 http://www.cfilt.iitb.ac.in/wordnet/webhwn/ 
sis in blog data is rely on predefined rules to identi-
fy mixed words. But this can get laborious and the 
rules may be insufficient to capture the latest be-
havior. Our approach is to use a statistical POS 
model to determine part of speech categories of 
words that typically undergo such switches.  
5 Statistical Part of Speech Tagger  
Example 5.1 showcases a typical sentence seen in 
blog data. Example 5.2 shows the issue with 
spelling variations sometimes that occur in the 
same sentence 
Example 5.1: Otherwise humara bhi wohi haal hoga jo 
is time Palestine, Iraq, Afghanistan wagera ka hai ~ 
Otherwise our state will also be like what is in Pales-
tine, Iraq, Afghanistan etc. are experiencing at this time 
Example 5.2: Shariyat ke aitebaar se bhi ghaur kia jaey 
tu aap ko ilm ho jaega key joh haraam khata hai uska 
dil kis tarhan ka hota hey ~ If you look at it from morals 
point of you too you will understand the heart of people 
who cheat 
A statistical POS tagger for blog data has to take 
into consideration spelling variations, mixing pat-
terns and script change. The goal here is not to 
generate a perfect POS tagger for blog data 
(though the idea explained here can be extended 
for further improvisation) but to be able to identify 
POS categories that are candidates for switch and 
mix. The basic idea of our approach is as follows 
1. Train Latin script POS tagger (LS tagger) on 
pure Urdu Latin script data (Example 2 in table 
1 ? using Urdu POS tag set, Muaz et al, 2009) 
2. Train English POS tagger on English data 
(based on English tag sets, Santorini, 1990) 
3. Apply LS tagger and English tagger on Urdish 
data and note the confidence measures of the 
applied tags on each word 
4. Use confidence measures, LS tags, phoneme 
codes (to accommodate spelling variations) as 
features to train a new learning model on Urd-
ish data 
5. Those words that get tagged with the English 
tagset are potential place holders for mixing 
patterns  
 
Word Act Eng LS 
Urdu  
Urd 
CM 
Eng 
CM 
and CC CC NN 0.29 0.99 
most RB RB VM 0.16 0.83 
im-
portant 
JJ JJ VAUX 0.08 0.97 
thing NN NN CC 0.06 0.91 
4
Zardari NNP NNP NN 0.69 0.18 
ko PSP NNP PSP 0.99 0.28 
shoot VB NNP JJ 0.54 0.29 
ker NN NNP NN 0.73 0.29 
dena VM NNP VM 0.83 0.29 
chahiya VAUX NNP VAUX 0.98 0.21 
. SYM . SYM 0.99 0.99 
Table 2. POS tagger with confidence measures 
 
The training data needed to develop LS tagger for 
Urdu is obtained from Hindi. IIIT POS annotated 
corpus for Hindi contains data in the SSF format 
(Shakti Standard Format) (Bharati, 2006). This 
format tries to capture the pronunciation infor-
mation by assigning unique English characters to 
Hindi characters. Since this data is already in Latin 
script with each character capturing a unique pro-
nunciation, changing this data to a form that repli-
cates chat data using heuristic rules is trivial. 
However, this data is highly sanskritized and hence 
need to be changed by replacing Sanskrit words 
with equivalent Urdu words. This replacement is 
done by using online English to Urdu dictionaries 
(www.urduword.com and www.hamariweb.com). 
We have succeeded in replacing 20,000 pure San-
skrit words to Urdu by performing a manual 
lookup. The advantage with this method is that  
1. The whole process of setting up annotation 
guidelines and standards is eliminated.  
2. The replacement of pure Hindi words with Ur-
du words in most cases is one-one and the POS 
assignment is retained without disturbing the 
entire structure of the sentence. 
Our training data now consists of Urdu words writ-
ten in Latin script. We also generate phonemes for 
each word by running the phonetic model. A POS 
model is trained using CRF (Lafferty, 2001) learn-
ing method with current word, previous word and 
the phonemes as features. This model called the 
Latin Script (LS) POS model has an F-score of 
83%.  
English POS tagger is the Stanford tagger that 
has a tagging accuracy of about 98.7%2 . 
5.1 Approach 
Urdish blog data consists of Urdu code-mixed with 
English. Running simple Latin script based Urdu 
POS tagger results in 81.2% accuracy when POS 
tags on the entire corpus is considered and 52.3% 
                                                          
2 http://nlp.stanford.edu/software/tagger.shtml 
accuracy on only the English words. Running Eng-
lish tagger on the entire corpus improves the POS 
tagging accuracy of English words to 79.2% accu-
racy. However, the tagging accuracy on the entire 
corpus reduces considerably ? 55.4%. This indi-
cates that identifying the language of the words 
will definitely improve tagging.  
Identifying the language of the words can be 
done simply by a lexicon lookup. Since English 
words are easily accessible and more enriched, 
English Wordnet3 makes a good source to perform 
this lookup. Running Latin script POS tagger and 
English tagger on the language specific words re-
sulted in 79.82% accuracy for the entire corpus and 
59.2% accuracy for English words. Clearly there is 
no significant gain in the performance. This is on 
account of English equivalent Urdu representation 
of words (e.g. key ~ their, more ~ peacock, bat ~ 
speak). 
Since identifying the language explicitly yields 
less benefit, we showcase a new approach that is 
based on the confidence measures of the taggers. 
We first run the English POS tagger on the entire 
corpus. This tagger is trained using a CRF model. 
Scores that indicate the confidence with which this 
tagger has applied tags to each word in the corpus 
is also estimated (table 2). Next, the Latin script 
tagger is applied on the entire corpus and the con-
fidence scores for the selected tags are estimated. 
So, for each word, there exist two tags, one from 
the English tagger and the other from the Latin 
script Urdish tagger along with their confidence 
scores. This becomes our training corpus. 
The CRF learning model trained on the above 
corpus using features shown in table 3 generates a 
cross validation accuracy is 90.34%. The accuracy 
on the test set is 88.2%, clearly indicating the ad-
vantages of the statistical approach.  
 
Features used to train Urdish POS tagger 
Urdish word 
POS tag generated by LS tagger 
POS tag generated by English tagger 
Confidence measure by LS tagger 
Confidence measure by English tagger 
Double metaphone value 
Previous and next tags for English and Urdu 
Previous and next words 
Confidence priorities 
Table 3. Features used to train the final POS tagger 
for Urdish data 
                                                          
3 http://wordnet.princeton.edu/ 
5
Table 4 illustrates the POS categories used as po-
tential pattern switching place holders  
 
POS Category Example 
noun within a noun 
phrase 
uski life par itna control acha nahi 
hai ~ its not good to control his life 
this much 
Interjection  Comon Reema yaar! ~ Hey Man 
Reema! 
lol! ~ lol 
Adjective Yeh story bahut hi scary or ugly tha 
~ This story was really scary and 
ugly 
Adverb Babra Shareef ki koi bhi film lagti 
hai, hum definitely dekhtai ~ I would 
definitely watch any movie of Babra 
Shareef 
Gerund (tagged as a 
verb by English 
POS tagger) 
Yaha shooting mana hai ~ shooting 
is prohibited here 
Verb Iss movie main I dozed ~ I slept 
through the movie 
Verb Afridi.. Cool off!  
Table 4. POS categories that exhibit pattern switch 
6 Sentiment Polarity Detection 
The main goal of this work is to perform sentiment 
analysis in Urdu blog data. However, this task is 
not trivial owing to all the peculiarities that blog 
data exhibits. The work done on Urdu sentiment 
analysis (Mukund and Srihari, 2010) provided an-
notated data for sentiments in newswire domain. . 
Newspaper data make a good corpus to analyze 
different kinds of emotions and emotional traits of 
the people.  They reflect the collective sentiments 
and emotions of the people and in turn the society 
to which they cater. When specific frames are con-
sidered (such as semantic verb frames) in the con-
text of the triggering entities ? opinion holders 
(entities who express these emotions) and opinion 
targets (entities towards whom the emotion is di-
rected) - performing sentiment analysis becomes 
more meaningful and newspapers make an excel-
lent source to analyze such phenomena (Mukund et 
al., 2011). We use SCL to transfer sentiment anal-
ysis learning from this newswire data to blog data. 
Inspired by the work done by (Prettenhofer and 
Stein, 2010), we rely on oracles to generate pivot 
pairs. A pivot pair {wS, wT} where wS ? 9S (the 
source language ? Urdu newswire data) and wT ? 
VT (the target language ? Urdish data) should satis-
fy two conditions 1. high support and 2. high con-
fidence, making sure that the pairs are predictive of 
the task.  
Prettenhofer and Stein (2010) used a simple 
translation oracle in their experiments. However 
there exist several challenges with Urdish data that 
inhibits the use of a simple translation oracle.  
1. Script difference in the source and target 
languages. Source corpus (Urdu) is written 
in Nastaleeq and the target corpus (Urdish) 
is written in ASCII 
2. Spelling variations in roman Urdu 
3. Frequent use of English words to express 
strong emotions 
We use two oracles to generate pivot pairs.  
The first oracle accommodates the issue with 
spelling variations. Each Urdu word is converted to 
roman Urdu using IPA (1999) guidelines. Using 
the double metaphone algorithm4 phoneme code 
for the Urdu word is determined. This is also ap-
plied to Urdish data at the target end. Words that 
have the same metaphone code across the source 
and target languages are considered pivot pairs.  
The second oracle is a simple translation oracle 
between Urdu and English. Our first experiment 
(experiment 1) is using words that belong to the 
adjective part of speech category as candidates for 
pivots. We augment this set to include words that 
belong to other POS categories shown in table 4 
that exhibit pattern mixing (experiment 2).  
 
6.1 Implementation  
 
The feature used to train the learning algorithm is 
limited to unigrams. For linear classification, we 
use libSVM (Chang and Lin, 2011). The computa-
tional bottleneck of this method is in the SVD de-
composition of the dense parameter matrix W. We 
set the negative values of W to zero to get a sparse 
representation of the matrix. For SVD computation 
the Lanczos algorithm provided by SVDLIBC5 is 
employed. Each feature matrix used in libSVM is 
scaled between -1 and 1 and the final matrix for 
SVD is standardized to zero mean and unit vari-
ance estimated on DS U Du (source subset and tar-
get subset). 
6.2 Results 
The domain of the source data set is limited to 
cricket and movies in order to ensure domain over-
                                                          
4 http://en.wikipedia.org/wiki/Double_Metaphone 
5 http://tedlab.mit.edu/~dr/SVDLIBC 
6
lap between newswire data that we have and blog 
data. In order to benchmark the proposed tech-
nique, our baseline technique is based on the con-
ventional method of supervised learning approach 
on annotated data. Urdish data set used for polarity 
classification contains 705 sentences written in 
ASCII format (example 6.1). This corpus is manu-
ally annotated by one annotator (purely based on 
intuition and does not follow any predefined anno-
tation guidelines) to get 440 negative sentences 
and 265 positive sentences. The annotated corpus 
is purely used for testing and in this work consid-
ered as unlabeled data. A suitable linear kernel 
based support vector machine is modeled on the 
annotated data and a five-fold cross validation on 
this set gives an F-Measure of 64.3%. 
Example 6.1: 
General zia-ul-haq ke zamane mai qabayli elaqe Russia 
ke khilaf jang ka merkaz thea aur general Pervez 
Musharraf ke zamane mai ye qabayli elaqe Pakistan ke 
khilaf jang ka markaz ban gye . ~ negative 
Our first experiment is based on using the se-
cond oracle for translations on only adjectives 
(most obvious choice for emotion words). We use 
438 pivot pairs. The average F-measure for the 
performance is at 55.78% which is still much be-
low the baseline performance of 64.3% if we had 
access to annotated data. However, the results 
show the ability of this method. 
Our second experiment expands the power of 
the second oracle to provide translations to other 
POS categories that exhibit pattern switching. This 
increased the number of pivot pairs to 640. In-
crease in pivots improved the precision. Also we 
see significant improvement in the recall. The new-
ly added pivots brought more sentences under the 
radar of the transfer model. The average F-
Measure increased to 59.71%.  
The approach can be further enhanced by im-
proving the oracle used to select pivot features. 
One way is add more pivot pairs based on the cor-
relation in the topic space across language domains 
(future work).  
7 Conclusion 
In this work we show a way to perform sentiment 
analysis in blog data by using the method of struc-
tural correspondence learning. This method ac-
commodates the various issues with blog data such 
as spelling variations, script difference, pattern 
switching. 
 
Table 5. SCL based polarity classification for Urdish data 
We rely on two oracles, one that takes care of 
spelling variations and the other that provides 
translations. The words that are selected to be 
translated by the second oracle are carefully cho-
sen based on POS categories that exhibit emotions 
and pattern switching. We show that the perfor-
mance of this approach is comparable to what is 
achieved by training a supervised learning model. 
In order to identify the POS categories that exhibit 
pattern switching, we developed a statistical POS 
tagger for Urdish blog data using a method that 
does not require annotated data in the target lan-
guage. Through these two modules (sentiment 
analysis and POS tagger for Urdish data) we suc-
cessfully show that the efforts in performing non-
topical analysis in Urdu newswire data can easily 
be extended to work on Urdish data. 
8 Future work 
Analyzing the test data set for missing and false 
positives, here are some of the examples of where 
the model did not work 
Example 7.1: ?tring tring tring tring.. Phone to bar bar 
bajta hai. Annoying.? ~ tring tring tring tring tring.. 
the phone rings repeatedly. Annoying. 
Example 7.2: ?bookon ko padna tho ab na mumkin hai. 
Yaha thak mere friends mujhe blindee pukarthey hai? ~ 
cannot read books any more. Infact, my friends call me 
blindee. 
Example 7.3: ?Ek Tamana Hai Ke Faqt Mujh Pe 
Mehrban Raho, Tum Kise Or Ko Dekho To Bura Lagta 
Hai? ~ I have this one wish that destiny be kind to me 
If you see someone else I feel bad 
Our method fails to tag sentences like in example 
7.1 where English verbs are used by themselves. 
Our POS tagger fails to capture such stand-alone 
Precision (P %) Recall (R %) F-Measure (F %) 
Phonemes (Roman Urdu) 
37.97 58.82 46.15 
Metaphones based synonym mapping (adjectives) 
50.9 51 50.89 
56.6 56.4 55.62 
58.9 60.64 59.75 
Precision (P %) Recall (R %) F-Measure (F %) 
Metaphones based synonym mapping (adjectives + other 
POS categories) 
54.2 64.3 58.82 
58.4 60.85 59.6 
59.4 62.12 60.73 
7
verbs as verbs but tags them as nouns. Hence, 
GRHVQ?W RFFXU LQ WKH SLYRW VHW  
Our second issue is with Morpho syntactic 
switching, a behavior seen in example 7.2. 
Nadhkarni (1975) and Pandaripande (1983) have 
shown that when two or more languages come into 
contact, there is mutual feature transfer from one 
language to another. The languages influence each 
other considerably and constraints associated with 
free morphemes fail in most cases. The direction 
and frequency of influence depends on the social 
status associated with the languages used in mix-
ing. The language that has a high social status 
tends to use the morphemes of the lower language.  
Example 7.4: Bookon ? in books, Fileon ? in files, 
Companiyaa ? many companies 
Clearly we can see that English words due to their 
frequent contact with Urdu grammatical system 
tend to adopt the morphology associated with the 
base language and used mostly as native Urdu 
words. These are some issues, if addressed, will 
definitely improve the performance of the senti-
ment analysis model in Urdish data. 
References  
Abdul-Mageed, M., Diab, M., and Korayem, M. 2011.  Sub-
jectivity and Sentiment Analysis of Modern Standard Ara-
bic. In proceedings of the 49th Meeting of ACL. Portland, 
Oregon, USA, June 19-24 
Agnihotri, Rama Kant. 1998. Social Psychological Perspec-
tives on Second Language Learning. Sage Publications, 
New Delhi 
Bharati, Askhar, Rajeev Sangal and Dipti M Sharma. 2005. 
Shakti Analyser: SSF Representation 
Blitzer, John, Ryan McDonald, and Fernando Pereira. 2006. 
Domain adaptation with structural correspondence learning. 
In proceedings of the 2006 Conference on EMNLP, pp. 
120?128, Sydney, Australia 
Chang, Chih-Chung, Chih-Jen Lin. 2011. LIBSVM: a library 
for support vector machines. In the ACM Transactions on 
Intelligent Systems and Technology, Vol 2, no 27, pp 1-27 
Dredze, Mark., Blitzer, John., Talukdar,  Partha Pratim., 
Ganchev, Kuzman., Graca, Joao., Pereira, Fernando. 2007. 
Frustratingly Hard Domain Adaptation for Parsing. Shared 
Task  of CoNLL. 
Dussias, P. E. 2003. Spanish-English code-mixing at the auxil-
iary phrase: Evidence from eye-movements. Revista Inter-
nacional de Ling??stica Iberoamerican. Vol  2, pp. 7-34 
Gildea, Daniel and Jurafsky, Dan. 2002. Automatic Labeling 
of Semantic Roles, Computational Linguistics, 28(3):245?
288 
Goyal, P, Manav R. Mital, A. Mukerjee, Achla M. Raina, D. 
Sharma, P. Shukla, and K Vikram.  2003. Saarthaka - A Bi-
lingual Parser for Hindi, English and code-switching struc-
tures. In proceedings of the 11th Conference of the ECAL 
Hal Daume III and Daniel Marcu. 2006. Domain adaptation 
IRU VWDWLVWLFDO FODVVL?HUV Journal of Artificial Intelligence 
Research, Vol 26, pp. 101?126 
Hal Daume III. 2007. Frustratingly easy domain adaptation. In 
proceedings of the 45th Meeting of ACL, pp.  256?263 
International Phonetic Association (IPA). 1999. Handbook of 
the International Phonetic Association: A guide to the use of 
the International Phonetic Alphabet. Cambridge: Cam-
bridge University Press. ISBN 0-521-65236-7 (hb); ISBN 
0-521-63751-1  
Joshi, Adithya and Bhattacharyya, Pushpak. 2012. Cost and 
Benefit of Using WordNet Senses for Sentiment Analysis.  
LREC, Istanbul, Turkey 
Kachru, Braj. 1978. Conjunct verbs; verbs or verb phrases?. 
In proceedings of the XIIth International Congress of Lin-
guistics. pp. 366-70 
Lafferty, John, Andrew McCallum, Pereira. F. 2001. Condi-
tional random fields: Probabilistic models for segmenting 
and labeling sequence data. In proceedings of the 18th In-
ternational Conference on Machine Learning, Morgan 
Kaufmann, San Francisco, CA . pp. 282?289 
Muaz, Ahmed, Aasim Ali, and Sarmad Hussain. 2009. Analy-
sis and Development of Urdu POS Tagged Corpus. In pro-
ceedings of the 7th Workshop on ACL-IJCNLP, Suntec, 
Singapore, pp. 24?31, 6-7 August. 
Mukund, Smruthi, Rohini K. Srihari. 2010. A Vector Space 
Model for Subjectivity Classification in Urdu aided by Co-
Training, In proceedings of the 23rd COLING, Beijing, 
China 
Mukund, Smruthi, Debanjan Ghosh, Rohini K. Srihari, 2011. 
Using Sequence Kernels to Identify Opinion Entities in Ur-
du.  In Proceedings of CONLL 
Nadkarni, Mangesh. 1975. Bilingualism and Syntactic Change 
in Konkani Language, vol. 51, pp. 672 C 683. 
Pandaripande, R. 1981. Syntax and Semantics of the Passive 
Construction in selected South Asian Languages. PhD disser-
tation. University of Illinois, Illinois 
Prettenhofer, Peter and Benno Stein. 2010. Cross-Lingual 
Adaptation Using Structural Correspondence Learning. In 
proceedings of ACL  
Rasul, Sarwat. 2006. Language Hybridization and Code Mix-
ing in Pakistani Talk Shows. Bahaudin Zakriya University 
Journal 2nd Issue. pp. 29-41 
Roark, Brian and Michiel Bacchiani. 2003. Supervised and 
unsupervised PCFG adaptation to novel domains. 
In Proceedings of the 2003 Conference of NAACL, HLT - 
Volume 1 (NAACL '03) 
Rie-K. Ando and Tong Zhang. 2005. A framework for learning 
predictive structures from multiple tasks and unlabeled data. 
In Jornal of Machine Learning. Res., Vol 6, pp. 1817?1853 
Santorini, Beatrice. 1990. Part-of-speech tagging guidelines 
for the Penn Treebank Project. University of Pennsylvania, 
3rd Revision, 2nd Printing. 
Shimizu, Nobuyuki and Nakagawa, Hiroshi. 2007. Structural 
Correspondence Learning for Dependency Parsing. In pro-
ceedings of CoNLL Shared Task Session of EMNLP-
CoNLL.  
Sinha, R.M.K. and Anil Thakur. 2005. Machine Translation of 
Bi-lingual Hindi-English (Hinglish) Text. 10th Machine 
Translation summit (MT Summit X) 
Zentella, Ana Celia. 1997. A bilingual manual on how to raise 
Spanish Children. 
8
