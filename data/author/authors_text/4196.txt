Processing Self Corrections in a speech to speech system 
J S rg  Sp i lker ,  Mar t in  K la rner ,  G f in ther  G6rz  
University of Er langen-Nuremberg - Computer  Science Institute, 
IMMD 8 - Artificial Intell igence, 
Am Weichselgarten 9, 91058 Er langen-  Tennenlohe, Germany 
{ spilker, klarner, goerz}~immd8, inf ormat ik. uni-erlangen, de 
Abstract  
Speech repairs occur often in spontaneous spo- 
ken dialogues. The ability to detect and cor- 
rect those repairs is necessary for any spoken 
language system. We present a framework to 
detect and correct speech repairs where all tel- 
evant levels of information, i.e., acoustics, lexis, 
syntax and semantics can be integrated. The 
basic idea is to reduce the search space for re- 
pairs as soon as possible by cascading filters 
that involve more and more features. At first an 
acoustic module generates hypotheses about the 
existence of a repair. Second a stochastic model 
suggests a correction for every hypothesis. Well 
scored corrections are inserted as new paths in 
the word lattice. Finally a lattice parser decides 
on accepting the repair. 
1 I n t roduct ion  
Spontaneous peech is disfluent. In contrast 
to read speech the sentences aren't perfectly 
planned before they are uttered. Speakers of- 
ten modify their plans while they speak. This 
results in pauses, word repetitions or changes, 
word fragments and restarts. Current mlto- 
rustic speech understanding systems perform 
very well in small domains with restricted 
speech but have great difficulties to deal with 
such disfluencies. A system that copes with 
these self corrections (=repairs) must recognize 
the spoken words and identify the repair to get 
the intended meaning of an utterance. To char- 
acterize a repair it is commonly segmented into 
the following four parts (el. fig.i): 
? reparandum: the "wrong" part of the ut- 
terance 
? interruption point (IP): marker at the end 
of the reparandum 
? editing term: special phrases, which indi- 
cate a repair like "well", "I mean" or filled 
pauses such as "uhln '~, "uh" 
? reparans: the correction of the reparandum 
on Thursday lcannot ? no Ican meet "ah afteronc 
/ 
/ \ - / "" 
Rct)arandmn Interruption- Editing Rcparans 
point Term 
Figure 1: Example of a self repair 
Only if reparandum and editing term are 
known, the utterance can be analyzed in the 
right way. It remains an open question whether 
the two terms should be deleted before a seman- 
tic analysis as suggested sometimes in the liter- 
ature 1. If both terms are marked it is a straight- 
forward preprocessing step to delete reparan- 
dum and editing term. In the Verbmobil 2 cor- 
pus, a corpus dealing with appointment schedul- 
ing a.nd tr~vel planning, nearly 21% of all turns 
contain at least one repair. As a consequence a 
speech understanding system thai; cannot han- 
dle repairs will lose perforlnance on these turns. 
Even if repairs are defined by syntactic and 
semantic well-formedness (Levelt, 1983) we ob- 
serve that most of them are local phenomena.. 
At this point we have to differentiate between 
restarts and other repairs a (modification re- 
pairs). Modification repairs have a strong corre- 
spondence between reparandum and reparans, 
1In most cases a reparaudum could be deleted with- 
out any loss of information. But, for exmnple, if it in- 
troduces an object which is referred to later, a deletion 
is not appropriate. 
>l?his work is part of the VERBMOBIL  project and 
was funded by the German Federal Ministry for Research 
and Technology (BMBF) in the framework of the Verb- 
mobil Project under Grant BMBF 01 IV 701 V0. The 
responsibility for the contents of this study lies with the 
authors. 
SOften a third kind of repair is defined: "abridged 
repairs". These repairs consist solely of an editing term 
and are not repairs in our sense. 
1116 
whereas restarts a.re less structured. In our be- 
lieve there is no nted for a. complete syntactic 
am@sis to detect ~md correct most modification 
repairs. Thus, in wh~tt follows, we will concen- 
tra.te on this ldnd of repa.ir. 
There are two major arguments to process 
repairs before t)arsing. Primarily spontaneous 
speech is not always syntactically well-formed 
even in the absence of sell' corrections. Sec- 
ond (Meta-) rules increase the pa.rsers' search 
space. This is perhaps acceptable for transliter- 
ated speech but not for speech recognizers out- 
put like l~ttices because they represent millions 
of possible spoken utterances. \[n addition, sys- 
tems whk;h a.re not based on a. deep syntactic 
and semantic amdysis e .g .  statistical dialog 
act prediction -- require a repa.ir processing step 
to resolve contr~dictions like the one in tit. 1. 
We propose all algorithm for word lattices 
th,~t divides repa.ir detection a.nd correction in 
three steps (of. fig. 2) l"irst, ~r trigger indi- 
cates potential 1Ps. Second, a sl;ochasl, ic model 
tries to lind an appropria.te repair h)r each IP by 
guessing 1,he mosl; l)robable segmentation, qb 
accomplish this, repair processing is seen as a 
statistical machine translation problem where 
the repa.randum is a transl~tion of the reparans. 
For every repair found, a pa.th representing the 
spcaker.' intended word sequence is inserted 
into the la.ttice. In the last step, a lattice parser 
selects the best pa.th. 
tlll 'llllll'Sday I ?iIIlllt)l IlO \[ CIIII lllCel "ah tiller t)llC 
gpeec\] l  I't'cOgtllZCi 
wllnl Io Slly ICOll i i1  il 
Oll +l'htusday 1 C;lllllOI lit) \] t'311 IIIL'L~I 'liIh alter  111112 
loca l  word  based  scope  dc lec t io l l  o f  lattice ed i t ing  1o represent  res t l l t  
l{cl)ttl' i l l/dtll l l \ ]~.c\])alans !f 
1 ?-ilJ\] 
? wll iii \[o say icoll ill , ii 
on  "\[\]lttlSdlly t'lllllK~l t'atI1 litter I / "lib ' ' tll'\[Cl "t t)llC t J 
se lec l ion  by  
1 l ingu is t i c  a l la lys l s  
s 
011 "l'htll'gday \]C/Ill nlcel "till :tiler olle 
Figure 2: An architecture for repa.ir processing 
2 Repa i r  qh ' iggers  
Because it is impossible for;t  rea.l time speech 
system to check for every word whether it can 
be part of a repair, we use triggers which indi- 
cate the potential existence of a repa.ir. These 
triggers nlllst be immediately detectable for ev- 
ery word in the lattice. Currently we art using 
two different ldnd of triggers4: 
\]. 
. 
Acoustic/prosodic cuts: Spe~kers mark the 
117 in many cases by prosodic signals like 
1)auses, hesitations, etc. A prosodic classi- 
tier 5 determines for every word the proba-  
bi l i ty of  an IP following. If it is above a cer- 
t~dn l;hreshold, the trigger becomes active. 
For a detailed description of the acoustic 
aspects ee (Batliner eL al., 1998). 
Word Dagments are a very strong repair 
indicator. Unfortunately, no speech recog- 
nizer is able to detect word fl:agmtnts to 
date. But there are some interesting ap- 
proaches to detect words which are not in 
the recognizers vocabulary (Klakow et al, 
1999). A word fi'agment is normally an un- 
known word and we hope that it can bt 
distinguished from unfra.gmented unknown 
words by the prosodic classifier. So, cur- 
rently this is a hypol;hetical trigger. We 
will elaborate on it in the evaluation sec- 
tion (cf. sect. 5) to show the impact of this 
trigger. 
If a trigger is active, a. sea, rch for an acceptable 
segmentation into rel)arandum , editing term 
a.nd reparans is initia.ted. 
3 Scope Detect ion  
As mentioned in the introduction reDfir seg- 
mentation is based mainly on a stochastic trans- 
la.tion modtl, l~el'ore we explain it in detail we 
give a short introduction to statistical machine 
translation ?. The fundalnentaJ idea. is the as- 
sumption that a given sentence S in a source 
language (e.g. English) can be translated in any 
^ 
sentence 5/' in a l;~rgel; I,~nguage (e.g. German). 
To every pair (5', ~/') a probability is assigned 
which reflects the likelihood that a tra.nsl~tor 
who sees S will produce \]' as the translation. 
The sta.tistical machine translation problem is 
4 Other triggers cal, be added as well. (Stolcke ct al., 
1999) for example integrate prosodic cues and an ex- 
tended language model in a speech recognizer to detect 
IPs. 
SThe classifier is developed by tile speech group of 
the IMM1) 5. Special thanks to Anton Batliner, Richard 
Iluber and Volker Warnke. 
~A more detailed introduction is given by (Brown el, 
al., 1990) 
I 117 
formul;~ted as: 
5~' = argmaXTI ' (T lS )  
This is reformulated by Bayes' law for a better 
search space reduction, but we are only inter- 
ested in the conditional probability P(TIS ). For 
further processing steps we have to introduce 
the concept of alignment (Brown et al, 1990). 
Let S be the word sequence S1, S 2 . . . .  5,l ~ SI 
and T = ~,T2. . .Tm ~ 77\] ~. We can link a 
word in T to a word in S. This reflects the 
assumption that the word in T is translated 
from the word in S. \]?or example, if S is "On 
Thursday" and T is "Am l)onnerstag" "Am" 
can be linked to "On" but also to "Thursday". 
If each word in T is linked to exactly one word 
in ,S' these links can be described by a vector 
a~ '~ = a l . . .  a,~ with ai E O...l. If the word 51~. 
is linked to Si then aj = i. If it is not connected 
to any word in S then aj = 0. Such a vector 
is called an alignment a. P(T\],5,) can now be 
expressed by 
 '(TIS) = al,5,) (2) 
a is alignment 
Without any further assumptions we can infer 
the tbllowing: 
) 1 * ( -45) ,  
H \])(ajl(t'{-l' r j - l '  ?'"' '5,) ~ 
J--' Tii-', m, ,5,) (3) 
Now we return to self corrections. How can this 
framework help to detect the segments of a re- 
pair? Assulne we have a lattice l)~th where the 
reparandn.  (m)) a,d the reparans( S) are 
given, then (RS, \]{D) can be seen as a. transla- 
tion pair and P(RD\]R,5,) can be expressed ex- 
actly the same way as in equation (2). Hence 
we have a method to score (ITS, P~D) pairs.. But 
the triggers only indicate the interruption point, 
not the complete segmentation. Let us first 
look at editing terms. We assume them to be 
a closed list of short phrases. Thus if an entry 
of the editing term list; is found after an 1P, the 
corresponding words are skipped. Any subse- 
quence of words befbre/after the IP conld be the 
reparanduln/reparans. Because turns ca.n h~we 
an arbitrary length it is impossible to compute 
P(I-~D\]IL5,) for every (RS, H.D) pair. Bug this 
is not necessary at all, if repairs are considered 
as local phenomena. We restrict our search to a 
window of four words before and after the IP. A 
corpus analysis showed that 98% of all repairs 
are within this window. Now we only have to 
compute probabilities for 42 difl'erent pairs. If 
the probability of a (RS, RD) pair is above a 
certain threshold, the segmentation is accepted 
as a repair. 
3.1 Parameter  Est imation 
The conditional probabilities in equation (3) 
cannot be estimated reliably fi'om any corpus 
of realistic size, because there are too many p~> 
rameters. For example both P in the product 
depend on the complete reparans R,5,. There- 
fore we simplify the probabilities by assuming 
that m depends only on l, a.i only on j ,m and 
l and finally RDj on 1L5,,.j. So equation (3) be- 
comes 
P(Z D, siZeS) : 
\]-I (4) 
j=l  
These probabilffies can be directly trained fi'orn 
a nlannally annotated corl)ns , where all repairs 
are labeled with begin, end, liP and editing term 
and for each l'eparandnnl the words are linked 
to the corresponding words in the respective 
reparalls. All distributions are smoothed by a 
simple back-off method (Katz, 1987) to avoid 
zero probabilities with the exception that the 
word replacement probability P(I~I)jIILS,j) is 
smoothed in a more sophisticated way. 
3.2 Smoothing 
Even it" we reduce the number of parameters for 
the word replacement probability by the sim- 
plifications mentioned above there are a lot of 
parameters left. With a vocabulary size of 2500 
words, 25002 paralneters have to be estimated 
for P(I~DjllL5,~j). The corpus 7 contains 3200 
repairs fi'om which we extra.ct about 5000 word 
links. So most of the possible word links never 
occur in the corpus. Some of theln are more 
likely to occur in a repair than others. For ex- 
ample, the replacement of "Thursday" by "\]M- 
clay'" is supposed to be more likely than by "eat' 
ing", even if both replacements are not in the 
training corpus. Of course, this is related to 
7~110006urns with ~240000 words 
1118 
the fact that a, repair is a syntactic and/or se- 
mantic anomaly. We make nse of it by a.dding 
two additional knowledge sources to our model. 
Minimal syntactic information is given by part- 
o f  speech (POS) tags and POS sequences, se- 
mmltic information is given by semantic word 
classes. Ilence the input is not merely a se- 
quence of words but a sequence of triples. Each 
triple has three slots (word, POS tag, seman- 
tic class). In the next section we will describe 
how we ol)tain these two information pieces \[br 
every word in the lattice. With this additional 
informa.tion, P(RDjI1LS',~ j) probability could 1)e 
smoothed by linea.r interpolation of word, POS 
and semantic la.ss replacement \])robabilities. 
= 
n,, l '(Word( l .Dj )ll4r o.rd( n,S'..j) ) 
+/3 ,  
+ 
with a '+\ [3+7=1.  
l'Vord(IM):i ) is the not~tion tbr 1;11(: selector of 
the word slot of the triple a,t position j .  
4 Integration with Lattice 
Processing 
We ca, ll llOW del ;e( ; t  a ,nd cor rec t  a, repa,ir, given a 
sentence a.nnotated with I)()S tag;s an(I seman- 
1;ic classes, l~tll, how ca.n we ('onsl;rucl, such a. 
sequence, from a wor(l la.tl;ic(<? Integrating the 
ntodel in a lattice algoril;h m requires three steps: 
? mapping the word la?tice to a. tag lattice 
? triggering IPs and extra.cting the possible 
rel)ar;md um/reparans l):~irs 
? intr<)ducing new paths to represent tile 
plausible repa.rans 
The tag lattice constrnction is adapted from 
(Samuelsson, 11997). For every word edge and 
every denoted POS tag a corresponding tag 
edge is crea,ted and tim resulting prol)ability 
is determined. \[I' a tag edge already exists, 
tile probabilities of both edges are merged. 
The original words are stored together with 
their unique semantic lass in a associated list. 
Paths through the tag graph a.re scored by a 
IX)S-trigram. If a trigger is active, all paths 
through the word before tim ll' need to be tested 
whether an acceptable rel)air segmentation ex- 
ists. Since the scope model takes at most \['our 
words for reparandum a.nd rel)a.ra.ns in account 
it is sufficient to expand only partial paths. 
l);ach of these partial paths is then processed by 
the scope model. To reduce the se~rch space, 
paths with a low score can be pruned. 
Repair processing is integrated into the Verb- 
mobil system as a. filter process between speech 
recognition a.nd syntactic analysis. This en- 
forces a rep~fir representation that ca.n be into- 
grated into a lattice. It is not possible to lna.rk 
only the words with some additional informa- 
tion, because a rel)air is a phenomenon that (le- 
pends on a path. Imagine that the system has 
detected ~ repair on ~ certain path in the btttice 
and marked all words by their top,fir function. 
Then a search process (e.g. the parser) selects a 
different D~th which shares only the words of the 
repa.randum. But these words are no reparan- 
dum for this path. A solution is to introduce a 
new path in the. lattice where reI)arandum a.nd 
editing terms a.re deleted. As we said betbre, we 
do not want l;o delete these segments, so they 
are stored in a special slot of 1;11o first word of 
the reparans. The original path can now 1)e re- 
construct if necessary. 
To ensure that these new I)aths are coml)~> 
ra.ble to other paths we score the reparandum 
the same wa.y the l)arser does, and add the re- 
suiting wdue to the \[irst wor(l of the reparaits. 
As a result, l>oth the original path a.nd the. one 
wil,h the repair get the sa.me score excel)t one 
word tra.nsition. The (proba.bly bad) transition 
in l, he original path from the last word o\[" the 
rei)arandtnn to the first word of 1;he repa.rans is 
rel)laeed by a. (proba.bly goo(t) transition From 
the repa.ran(hnn~s onset to the rel>arans. \Ve 
take the lattice in fig. 2 to give an example. 
The SCOl)e mo(M has ma.rked " l  ca.nnot" as the 
reparandum, "no" as an editing term, and "l 
ca.n" as the rel)arans. We sum tip the acoustic 
scores of "1", "can" and "no". Then we add the 
maximnm language model scores for the tra.n- 
sition to "1", to "can" given "I", and to "no" 
given 'T' and "can". This score is ~(I(le(1 as an 
offset to the acoustic score of the second "1". 
5 Resu l ts  and  Fur ther  Work  
Due to the different trigger situations we per- 
formed two tests: One where we use only 
acoustic triggers and ~mother where the exis- 
tence of a perfect word fr~gment detector is as- 
sume(1. The input were unsegmented translit- 
era.ted utterance to exclude intluences a word 
1 1 19 
recognizer. We restrict the processing time on 
a SUN/ULTI{A 300MIIZ to 10 seconds. The 
parser was simulated by a word trigram. Train- 
ing and testing were done on two separated 
parts of the German part of the Verbmobil cor- 
pus (12558 turns training / 1737 turns test). 
Detection Correct scope 
Recall Precision Recall Precision 
Test 1 49% 70% 47 % 70% 
Test 2 71% 85% 62% 83% 
A direct comparison to other groups is rather 
difficult due to very different corpora, eval- 
uation conditions and goals. (Nakatani and 
Hirschberg, 1.993) suggest a acoustic/prosodic 
detector to identify IPs but don't discuss the 
problem of finding the correct segmentation i  
depth. Also their results are obtained on a 
corpus where every utterance contains at least 
one repair. (Shriberg, 1994) also addresses the 
acoustic aspects of repairs. Parsing approaches 
like in (Bear et al, 1992; Itindle, 1983; Core and 
Schubert, 1999) must be proved to work with 
lattices rather than transliterated text. An al- 
gorithm which is inherently capable of lattice 
processing is prot)osed by Heeman (Hem-nan, 
1997). He redefines the word recognition prob- 
lem to identify the best sequence of words, cor- 
responding POS tags and special rel)air tags. 
He reports a recall rate of 81% and a precision 
of 83% for detection and 78%/80% tbr correc- 
tion. The test settings are nearly the same as 
test 2. Unibrtunately, nothing is said about the 
processing time of his module. 
We have presented an approach to score po- 
tential reparandum/reparans pairs with a rela- 
tive simple scope model. Our results show that 
repair processing with statistical methods and 
without deep syntactic knowledge is a promis- 
ing approach at least for modification repairs. 
Within this fi'alnework more sophisticated scope 
models can be evaluated. A system integration 
as a filter process is described. Mapping the 
word lattice to a POS tag lattice is not optimal, 
because word inlbrmation is lost in the search 
tbr partial paths. We plan to implement a com- 
bined combined POS/word tagger. 
References  
A. Batliner, R. Kompe, A. Kiettling, M. Mast, 
H. Niemann, and F,. NSth. 1998. M = 
syntax + prosody: A syntactic-prosodic la-
belling schema for large spontaneous speech 
databases. Epeech Communication, 25:193- 
222. 
J. Bear, J. Dowding, and E. Shriberg. 1992. 
Integrating multiple knowledge sources \["or 
detection and correction of repairs ill hu- 
man computer dialogs. In Proc. ACL, pages 
56-63, University of Delaware, Newark, 
Delaware. 
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. 
Della Pietr~, F. Jelinek, J. D. Lafferty, R. L. 
Mercer, and P. S. Roossin. 1990. A sta.tisti- 
cal approach to machine translation. Compu- 
tational Linguistics, 16(2):79-85, June. 
M. G. Core and K. Schubert. 1999. Speech re- 
pairs: A parsing perspective. Satellite meet- 
ing ICPIIS 99. 
P. A. I-Iceman. 1997. Speech Repairs, Into- 
nation Boundaries and Discourse Markers: 
Modeling Epeakcrs' Utterances in ,5'pokcn Di- 
alog. Ph.l). thesis, University of Rochester. 
D. Hindle. 1983. Deterministic parsing of syn- 
tactic nontluencies. In Proc. ACL, MIT, 
Cambridge, Massachusetts. 
S. M. Katz. 1987. Estimation of probabilities 
from sparse data for tile language model con> 
ponent of a speech recognizer. 7)'ansaction 
on Acoustics, ,5'pcech and ,5'ignal 1)rocessing, 
ASSl'-35, March. 
ill). Klakow, G Rose, and X. Aubert. 1999. 
OOV-Detection in Large Vocabulary Sys- 
tem Using Automatically Defined Word- 
Fragments as Fillers. In EUR.OSPEECII '99, 
volume 1, pages 4:9-52, Budapest. 
W. Levelt. 1983. Monitoring and self-repair in 
speech. Cognition, 14:41-104. 
C. Naka.tani and a. tlirschberg. 1993. A speech- 
tirst model for repair detection and correc- 
tion. In P,vc. ACL, Ohio State University, 
Cohmbus, Ohio. 
C. Samuelsson. 1997. A left-to-right tagger for 
word graphs. In Proc. of the 5th Inter'national 
workshop on Parsing technologies, pages 171- 
178, Bosten, Massachusetts. 
E. E. Shriberg. 1994. Preliminaries to a Theory 
of Epeech Disflucncics. Ph.D. thesis, Univer- 
sity of California. 
A. Stolcke, E. Shriberg, D. Hakkani-Tur, and 
G. Tur. 1999. Modeling the prosody of hid- 
den events for improved word recognition. In 
EUROS'PEECII '99, volume 1, pages 307- 
310, Budapest. 
1120 
HYPERBUG: A Scalable Natural Language Generation Approach 
 
 
Martin Klarner 
University Erlangen-Nuremberg 
klarner@cs.fau.de 
 
 
Abstract 
A scalable natural language generation (NLG) 
system called HYPERBUG 1  embedded in an 
agent-based, multimodal dialog system is pre-
sented. To motivate this presentation, several 
scenarios (including a domain shift) are iden-
tified where scalability in dialog systems is 
really needed, and NLG is argued to be one 
way of easing this desired scalability. There-
fore the novel approach to hybrid NLG in the 
HYPERBUG system is described and the scal-
ability of its parts and resources is investi-
gated. Concluding with a few remarks to 
discourse generation, we argue that NLG can 
both contribute to and benefit from scalability 
in dialog systems. 
1 Introduction 
Scalability in dialog systems is, of course, not only a 
matter of the natural language understanding (NLU) 
component, but also of the NLG part of the system.2 We 
nevertheless see a lot of the effort spent in designing 
and implementing spoken dialog systems go into analy-
sis of speech and language; generation is often left aside 
or squeezed in afterwards. Therefore an NLG compo-
nent must fit into the existing dialog system framework 
and answer to the preset priorities in dialog system de-
velopment. 
We present a scalable NLG system embedded in an 
agent-based, multimodal dialog system. To this end, we 
                                                          
1 The acronym stands for hybrid, pragmatically embedded 
realization with bottom-up generation. 
2 In fact, for a working spoken dialog system even more 
components have to be scalable, including the speech recog-
nizer, the speech synthesizer and, most important of all, the 
dialog manager. But for now we concentrate on the opposi-
tion of NLU and NLG. 
first address common scenarios for scalability in dialog 
systems and describe the role of NLG in such systems 
before focusing on a classification of NLG systems in 
general and our hybrid linguistic realization approach in 
particular. After giving an overview of our NLG system 
we will be able to show how our notion of reversibility 
with respect to internal and external resources and the 
interaction and combination of shallow and deep NLG 
in HYPERBUG work towards and profit from scalability 
in our dialog system. 
2 Scalability in Dialog Systems 
Scalability for spoken dialog systems is needed in sev-
eral situations, including the following scenarios: 
1. Enlarging the domain content modifies and extends 
the thematic orientation of the domain. 
2. Refining the domain language extends the linguis-
tic coverage and expressibility of the domain. 
3. Changing the application domain refers to usually 
both of the first two and can lead to completely new 
requirements for a dialog system and its parts. 
4. Changing the discourse domain alters the dis-
course type within the same domain. 
The common consequence of these four scenarios is 
their impact on both NLU and NLG: If scalability is not 
biased between these two parts, one cannot expect the 
system to be scalable as a whole. Especially in the situa-
tion of a domain shift, the degree of automated knowl-
edge acquisition is an important issue to minimize 
costly (both in terms of time and money) manual efforts. 
If these are unavoidable for the NLU component, as it 
will often be the case in a real-world scenario, at least 
the NLG module must automatically benefit from them.  
3 NLG in Dialog Systems 
NLG itself and for its own can be seen as a way to im-
prove the scalability of a dialog system. (Reiter, 1995) 
analyze the costs and benefits of NLG in comparison to 
other technologies and approaches, such as graphics, 
mail merging and human authoring, and argue for a hy-
brid approach of combining NLG and templates in the 
IDAS system. Generally speaking, the application of 
NLG techniques in a real-world system must be justified 
with respect to linguistic and economic requirements. 
If templates are used, a smart approach is needed in all 
scenarios mentioned in section 2 to avoid being forced 
to completely redesign at least the data part of the sys-
tem output component. Nevertheless, fielded dialog 
systems often settle for a shallow generation module 
which relies on canned text and templates because of 
limited financial means and linguistic knowledge. 
3.1 NLG and Dialog Management 
In our spoken dialog system (B?cher et al, 2001), the 
dialog manager (DM) is responsible for the integration 
of user utterances into the discourse context. Moreover, 
the DM initiates system answers to user?s questions and 
system error messages if a user?s goal is unsatisfiable or 
a system task cannot be fulfilled. But these system ut-
terances must be verbalized as well, i.e. translated from 
abstract semantic representations to natural language 
sentences. This task is not placed within the DM, but 
?outsourced? to a component with adequate linguistic 
competence, the NLG module. This way, the DM can be 
designed completely amodal, i.e. it does not need to 
have any linguistic knowledge. Moreover, we can see 
that scenario 4 in section 2 can be separated into a lin-
guistic and a dialog part. The first part corresponds to 
scenario 2, the second is covered by the DM in our sys-
tem; hence we may skip scenario 4 for the remainder of 
this paper.  
3.2 Reversibility in dialog systems 
Given a spoken dialog system in general and an NLG 
component in such a system in particular, we consider 
reversibility a central means to allow for scalability. 
Considering reversibility, we want to introduce two dis-
tinctions: We discriminate between reversibility of al-
gorithms and reversibility of data on the one hand and 
between static (at developing time or at compile time) 
and dynamic reversibility (at runtime) on the other 
hand. In this terminology, reversibility of data means re-
using existing system resources by the NLG component. 
We can classify NLG resources into two groups: The 
language analysis part contains the (syntactic and se-
mantic) lexicon, the morphology component, and the 
grammar, while the DM part comprises the discourse 
memory, the domain model, and the user model.3 
                                                          
3 For a different classification of knowledge resources for 
text planning, see (Maier, 1999). 
4 Scalability in tactical generation 
We focus on a special part of generation, the tactical 
generation or linguistic realization, according to the 
classification in (Reiter and Dale, 2000)4. We are able to 
do so mainly because, as mentioned in 3.1, the DM is 
responsible for content determination in our system. 
This leaves the realization task for the NLG component 
(besides some microplanning, which has to be per-
formed as well). 
4.1 A taxonomy of existing systems 
In this section we will classify existing tactical genera-
tion systems into three groups and address problems 
with scalability in each one of them. 
Shallow generation systems form the first group; e.g. 
COMRIS (Geldof, 2000). The approach taken there 
relies on canned text and templates, and the domain 
dependency of these constructs is inherent. Therefore, in 
scenario 3 of section Fehler! Verweisquelle konnte 
nicht gefunden werden., once a domain shift is pro-
jected, the whole data part of the NLG component must 
be redesigned from scratch. In scenarios 1 and 2 the 
existing resources must be extended only, but even this 
can become a hard task if the existing template database 
is large enough. 
Deep generation systems make up the second group, e. 
g. KPML (Bateman 1997); they often suffer from large 
overgenerating grammars and slow processing time.  
Also often well-founded linguistic knowledge is re-
quired to create and maintain the grammars needed. 
Their problems with scalability arise primarily in sce-
narios 1 and 2, when thematic or linguistic coverage 
must be increased. 
The third group, ?modern? generation5  systems ide-
ally avoid the shortcomings of both of the above men-
tioned classical approaches. We distinguish between 
three types here: NLG with XSLT (Wilcock, 2003), 
which is basically template-based generation from XML 
input; stochastic approaches like (Oh and Rudnicky, 
2000), where the deep generation grammar is replaced 
by a stochastic language model, and hybrid generation 
approaches like D2S (Theune et al, 2000), which 
bridges the gap between NLG and speech synthesis by a 
prosody module. 
                                                          
4 Dale and Reiter distinguish between linguistic and struc-
ture realization, the former corresponding to the content and 
the latter to the structural part of tactical generation. We find 
this distinction somewhat artificial, because the content must 
be already determined for realization, but want to use it 
anyway to further clarify the task carried out by our system.  
5 For these systems, the term ?hybrid? is normally used in 
the literature, but we want to spare it for hybridization be-
tween shallow and deep generation; see 4.2. 
4.2 Hybrid tactical generation 
In our terminology, hybrid generation means the combi-
nation of shallow and deep generation in a single sys-
tem; therefore, hybrid systems are a special case of the 
?modern? approaches, which were mentioned in the 
preceding section and do not necessarily contain any of 
the two ?classical? approaches. For practical needs, we 
focus on how to combine deep (grammar-based) and 
shallow (template-based) generation techniques under 
the term hybrid NLG. We distinguish three types of 
such hybrid NLG systems: 
 Type I: Shallow NLG with deep elements 
 Type II: Deep NLG with shallow elements 
 Type III: Concurring deep and shallow NLG 
Here are two examples of existing systems to illustrate 
the classification just given: D2S fills slots in syntactic 
templates containing derivation trees and therefore can 
be classified as a type I system. (Wahlster, 2000) has 
separate shallow and deep generation modules resulting 
in a system of type III.6 
5 The HYPERBUG Approach 
5.1 System core functionality 
Our approach to realization is to combine all three types 
of hybrid tactical generation mentioned in section 4.2 in 
a single system called HYPERBUG. The goals for its de-
sign and implementation were: 
                                                          
6 Type II was, though theoretically sound and possible, dif-
ficult to find in existing systems. 
1. To re-use existing system resources originally de-
signed for parsing 
2. To generate templates at runtime rather than mere 
sentences 
3. To dynamically reduce the workload on deep gen-
eration and gradually let shallow generation take 
over 
4. To learn the domain-dependent part of the tactical 
generation task while the dialog is running and, ul-
timately, enable automatic adaptation to domain-
shifts 
Figure 1 shows the system core of HYPERBUG. As a 
shallow generation component, we implemented a pow-
erful template engine with recursion and embedded 
deep generation parts, including a lexicon, a morphol-
ogy component, inflection, and constituent aggregation, 
resulting in a system of type I in our classification. 
For the deep generation branch, we decided to settle for 
a combination of a microplanning and a realization 
component: The first module, a ?sentence planner?, in 
essence converts the input discourse representation 
structure (DRS, Kamp and Reyle, 1993) which is pro-
vided by the DM into a different semantic representa-
tion, the extended logical form (ELF) 7 . This ELF 
structure serves as input for the second module, a modi-
fied and improved version of bottom-up generation 
(BUG, van Noord, 1990) in Java, using a unification-
based grammar with feature structures. We also incor-
porated elements of shallow generation in our version of 
BUG: Surface text parts, e.g. proper nouns, may occur 
in the semantic input structure, the ELF. The precom-
                                                          
7  The extensions allowed (as compared to a conventional 
LF) include syntactic functions like tense and mode, topical-
ization information and subordination clause type. 
Figure 1: System core of HYPERBUG 
HYPERBUG
     
 	

  


o
Reversibility and Re-usability of Resources
in NLG and Natural Language Dialog Systems
Martin Klarner
3SOFT GmbH
Frauenweiherstr. 14, D-91058 Erlangen, Germany
martin.klarner@3soft.de
Abstract
Reversibility is a key to efficient and maintain-
able NLG systems. In this paper, we present
a formal definition of reversible NLG systems
and develop a classification of existing natural
language dialog systems in this framework.
1 Introduction
Reversibility is a key factor in building efficient and
maintainable NLG and natural language dialog systems
(NLDSs). But previous formal descriptions of reversibil-
ity are still lacking in coverage and applicability to exist-
ing systems. In this paper, we extend former approaches
to this matter by formally defining reversibility in NLDSs
and developing a proper classification of such systems
in terms of reversibility. After that, existing NLG and
generic dialog systems are used as examples for the fea-
sibility and applicability of our classification.
In our point of view, it is useless to consider reversibil-
ity for an NLG system alone, because parsing and dia-
log management are equally important for developing an
NLDS. Hence, our classification applies to complete di-
alog systems and not only NLG systems.
2 A Formal Description of Reversibility
In this section, we will provide a formal definition of
reversibility which is based on previous work [Neumann
and van Noord, 1994]. To this end, we will first give
a short overview of the results obtained there in sect.
2.1. After that, we will present our extended definition
in sect. 2.2.
2.1 Previous definitions of Reversibility
In [Neumann and van Noord, 1994], a definition of re-
versibility for programs is provided. The authors start
with a definition for computing a relation r in both di-
rections (def. 1).
Definition 1. (Computing a relation in both directions
according to Neumann and van Noord)
A program P computes a relation r in both di-
rections, iff for a given input ?dir, e? it recursively enu-
merates the set
{x | (?e, x? ? r ? dir = 0) ? (?x, e? ? r ? dir = 1)}.
In this definition, the parameter dir denotes the di-
rection in which the input is computed, and e represents
the content of the input for which the appropriate output
has to be obtained.
Let us state a simple corollary to def. 1 which relates
the notion of computing a relation in both directions to
the notion of inverse relations.
Corollary 1. A program P computes the relation r in
both directions, if P computes r and the inverse relation
of r, r?1.
Proof. According to def. 1, P recursively enumerates the
set {x | ?e, x? ? r} for dir = 0 and the set {x | ?x, e? ? r}
for dir = 1. Hence, it computes r for dir = 0 and
(using the standard definition of inverse relations) r?1
for dir = 1.
Based on def. 1, the following definitions for r-
reversibility of programs and relations are provided in
[Neumann and van Noord, 1994] (def. 2).
Definition 2. (Reversibility of programs and relations
according to Neumann and van Noord)
1. A program P is r-reversible if it computes r in
both directions.
2. A relation r is reversible iff an r-reversible pro-
gram exists.
The notion of reversible programs in def. 1 and 2 is
very general: In an extreme case, such a program can
consist of two completely independent parts tied to-
gether only by an initial conditional statement. This
statement decides, depending on the value of the direc-
tion parameter dir, whether the program part computing
relation r (for dir = 0) or the one computing r?1 (for
dir = 1) is used. In our opinion, such a program should
not be called reversible any more. Hence, definitions 1
and 2 are too general.
On the other hand, they are too specific; this is due
to three reasons:
1. Program and data are not distinguished.
2. Thus, different resources and resource types1 are
also not addressed.
1such as linguistic and pragmatic resources
3. The availability time for a program or a resource2
is not considered.
Hence, in the next section we will replace these def-
initions by a more general description of reversibility
for generic program systems before we will describe re-
versibility in current NLDSs.
2.2 Extended definition of reversibility
In this section, we will present our definition of reversibil-
ity. We start with definitions of a generic program sys-
tem and of system and program relations (def. 3).
Definition 3. (Program system, system relations, and
program relations)
1. A program system S consists of a triplet
(COMPS , PROGS , RESS) of
(a) a set of preprocessing programs COMPS =
{C1, . . . , Ck} which are executed before system
start,
(b) a set of runtime programs PROGS =
{P1, . . . , Pl},
(c) and a set of resources RESS = {R1, . . . , Rm}.
2. The set of relations RELS = {r1, . . . , rn} computed
by the programs of PROGS is called the set of sys-
tem relations of S.
3. The set of relations RELP = {r1, . . . , rp} computed
by a single program Pi ? PROGS is called the set
of program relations of P .
By resources we denote every data structure needed by
a runtime program for its execution3. More precisely, the
resource RPi,rk provides a necessary (but not sufficient)
condition for the runtime program Pi to compute one of
its program relations rk. All of these resources must be
available at system start, but they may be generated by
preprocessing programs.
Before we can state our definition of reversibility, we
have to give a formal description of inverse programs and
resources (def. 4).
Definition 4. (Inverse program and inverse resource)
Let S be a program system, R ? RESS a system re-
source, and P ? PROGS a program with a program re-
lation r ? RELP . Let R be a resource needed by P for
computing r.
1. Then every program P?1 computing the inverse re-
lation r?1 is called an inverse program to P with
respect to r.
2. The transformation of a resource R needed by P?1
to compute r?1 is called inverse resource R?1 to
R with respect to r.
A simple corollary relates self-inverse programs to r-
reversible programs.
2i.e. whether it is available only at runtime or already at
compile time
3contrary to the terminology used e.g. in operating sys-
tems programming
Corollary 2. If P ? P?1 holds, i.e. if P is self-inverse
with respect to r, then P is r-reversible.
Proof. If P computes r, P?1 computes r?1, and P ?
P?1 holds, then P computes r?1 as well. Then, accord-
ing to def. 1, P computes r in both directions, and with
def. 2 P is r-reversible.
Algorithmic reversibility
For any program system of def. 3, we define algorithmic
reversibility in the following way (def. 5).
Definition 5. (Algorithmic reversibility)
Let S be a program system, P ? PROGS a program
in S, and r ? RELP a program relation of P .
Then S is algorithmic-reversible in P and r if P
is r-reversible.
Hence, P (and no other program Q ? PROGS with
Q 6= P )4 has to compute r and r?1 as well.
Data reversibility
Data reversibility, the counterpart of algorithmic re-
versibility, can be defined as follows (def. 6).
Definition 6. (Data reversibility)
Let S be a program system, R ? RESS a system re-
source of S, and r ? RELS a system relation of S.
Then S is data-reversible to R and r if two programs
P1, P2 ? PROGS exist which both need R to be executed
and for both of which r ? RELP1 and r?1 ? RELP2
holds.
Thus, P1 must compute r using R, and P2 must com-
pute the inverse relation r?1 (also by using R). If
P1 ? P2 ? P holds, S is also algorithmic-reversible to P
and r.
Static and dynamic reversibility
A different dimension of reversibility dealing with the
availability time of a program or a resource can be de-
scribed as follows (def. 7).
Definition 7. (Static and dynamic reversibility)
Let S be a program system, R ? RESS a system re-
source and r ? RELS a system relation of S.
1. S is static-reversible with respect to R and r if
(a) a program P ? PROGS with r ? RELP exists
which needs R for its execution,
(b) also r?1 ? RELS, P?1 ? PROGS, and R?1 ?
RESS holds,
and additionally
(c) at least one preprocessing program C ?
COMPS is needed for the construction of R?1
from R or of P?1 from P .
2. If no such program C is needed, S is called
dynamic-reversible with respect to R and r.
4By Q 6= P we denote syntactic in-equivalence here. This
is easily decidable, whereas semantic equivalence of programs
is certainly not.
If, under the preconditions of def. 7, the inverse
program P?1 is constructed, S is also algorithmic-
reversible with respect to P and r. However, if the in-
verse resource R?1 is constructed, S is data-reversible
with respect to R and r. Obviously, both algorithmic
and data reversibility can occur simultaneously.
3 Reversibility in Dialog Systems
Consider the special relation sp?s between phonetic and
semantic structures. This is the relation computed by
the analysis part of a natural language dialog system
(NLDS). By applying our definitions of reversibility pre-
sented in sect. 2.2 on sp?s, we face an important ques-
tion of natural language processing: To what extent is
a given NLDS reversible? But before we consider this
question in more detail, we have to define our notion of
an NLDS first. Based on def. 3, we formally describe an
NLDS as follows (def. 8).
Definition 8. (NLDS)
Let rp?s be the relation between phonological and se-
mantic structures and r?1p?s the inverse relation of rp?s.5
An NLDS is a program system S with rp?s ? RELS
and r?1p?s ? rs?p ? RELS.
Hence, an NLDS must contain both the relations rp?s
and rs?p as system relations. This is quite obvious, since
natural language dialog requires both natural language
understanding (NLU) and natural language generation
(NLG).
4 Classification of Reversibility Types
As we have seen in the previous sections, generic pro-
gram systems and NLDSs in particular can be reversible
in two independent dimensions: On the one hand, they
can be static or6 dynamic, and on the other hand, al-
gorithms and/or data can be reversible. Given that a
system may also be not reversible at all in both dimen-
sions just mentioned, we obtain a classification of nine
possible reversibility types.
[Neumann, 1994], however, describes just four types of
reversibility in dialog systems and takes only the gram-
mar as a linguistic resource into account: Type A has
static reversibility (in terms of data and algorithms),
while type B has dynamic data reversibility. Type C has
statistically reversible data and dynamically reversible
algorithms, while type D has dynamic data and algo-
rithmic reversibility.
By further exploring the notions of algorithmic and
data reversibility introduced above, both of which can be
realized in three different variants (none, static, and dy-
namic), we are able to extend the classification in [Neu-
mann, 1994] by two more types: Type E is statically re-
versible in terms of data and algorithms, and type F has
dynamic data and static algorithmic reversibility. Our
5Henceforth, we will denote r?1p?s just rs?p for obvious
simplicity reasons.
6The ?or? here must be read as an ?exclusive or?.
extended classification of reversible dialog systems is de-
picted in fig. 1.
There are three more possible types in our classifi-
cation, all of them without data reversibility: Type G
has statically and type H dynamically reversible algo-
rithms, whereas type I does not have any reversibility
at all. While types G and H are just not desirable for
real-world NLDSs, type I is even unacceptable. Hence
we decided to exclude types G, H, and I from fig. 1 and
depict them separately in fig. 2. However, the legend
displayed there applies to fig. 1 as well.
It has to be pointed out here that any classification of
reversible dialog systems must not be restricted to the
grammar, but has to be extended to the other resources
used in an NLDS as well. Apart from the grammar,
we distinguish five additional system resources: Lexicon
and morphology component are linguistic resources (to-
gether with the grammar), whereas discourse memory,
domain model, and user model are pragmatic system
resources. Hence, the reversibility of an NLDS can be
classified depending on (at least) six different resource
categories. Together with the six reversibility types in-
troduced above, these six resources form a 6-tuple which
enables us to describe the reversibility of an NLDS for-
mally and completely.
Let us take the Conald dialog system [Ludwig, 2003]
as an example. The system lexicon is precompiled into
an NLG lexicon at development time, hence we have
static reversibility of type E here. On the other hand, the
morphology component is used by both the parser and
the generator at runtime in a uniform way (cf. [Klarner
and Ludwig, 2004]), resulting in dynamic reversibility for
this component. Discourse memory and domain model
are used in the dialog manager for pragmatic integra-
tion and by the NLG component. The data structures
are identical, but the algorithms are different. Thus, we
have type B reversibility for these two resources. The
user model, however, is not used for parsing, only for
generation, hence the system is not reversible with re-
spect to the user model.
In table 1 the reversibility types of the different re-
sources are put together. They form a tuple (E, D, A, B,
B, none) completely describing reversibility in Conald.
Resource Type
Lexicon E
Morphology D
Grammar A
Discourse Memory B
Domain Model B
User Model none
Table 1: Reversibility of Conald.
The Amalia system [Gabrilovich et al, 1998] is a typ-
ical example for Prolog-based reversible NLG systems.
The system grammar is first inverted and then compiled
into two different versions, one for parsing and one for
generation. Thus, we have type C reversibility here. The
Parser
Parser
Uniform Source
Algorithm
Uniform
Algorithm
Parser
Parser
Uniform Source
Algorithm
Uniform
Algorithm
Generator
Generator
Generator
Generator
Parsing
Resource
Parsing
Resource
Parsing
Resource
System
Resource
System
Resource
System
Resource
Generation
Resource
Generation
Resource
Generation
Resource
System
Resource
System
Resource
System
Resource
Type A
data: static; algorithms: none
Type B
data: dynamic; algorithms: none
Type C
data: static; algorithms: dynamic
Type D
data: dynamic; algorithms: dynamic
Type E
data: static; algorithms: static
Type F
data: dynamic; algorithms: static
Figure 1: Reversible dialog systems.
Parser
Parser
Uniform
Algorithm
Uniform Source
Algorithm
Generator
Generator
A
A B
B
BA
Parsing
Resource
Parsing
Resource
Generation
Resource
Generation
Resource
Parsing
Resource
Generation
Resource
Legend
uses resource
is compiled into
Type G
data: none; algorithms: static
Type H
Type I
Resource
Program
is compiled into
Program
Figure 2: Not-so-reversible dialog systems.
same applies to the lexicon. As there are no pragmatic
resources and no morphology component, we can skip
their analysis here. Hence, Amalia can be characterized
by the reversibility tuple (C, n/a, C, n/a, n/a, n/a); cf.
table 2.
Resource Type
Lexicon C
Morphology n/a
Grammar C
Discourse Memory n/a
Domain Model n/a
User Model n/a
Table 2: Reversibility of Amalia.
Our third and final example is Trips [Ferguson and
Allen, 1998]. In this system, the Discourse Context and
the Reference component are shared between the Inter-
pretation Manager (which is used for parsing) and the
Generation Manager (cf. [Allen et al, 2001]). This re-
sults in type B for the discourse memory. The same
holds for the ontology of Trips (cf. [Stent, 2001], p.
139): Its domain model is of type B as well. As there
is no specific user model contained in the system, there
is also no degree of reversibility to be found there. For
various reasons, the Generation Manager uses its own
grammar and morphology component (cf. [Stent, 2001],
p. 180 & 182). The NLG lexicon of Trips is obtained
semi-automatically from various system resources and
off-line extraction (cf. [Stent, 2001], p. 180). Hence, we
have type A reversibility here. We therefore conclude
that Trips can be described by the reversibility tuple
(A, none, C, B, B, n/a); cf. table 3.
Resource Type
Lexicon A
Morphology none
Grammar none
Discourse Memory B
Domain Model B
User Model n/a
Table 3: Reversibility of Trips.
5 Re-usability as Static Reversibility of
Resources
Given our definitions of reversibility in sect. 2, we can
view re-using resources in an NLDS as static or dynamic
reversibility of the system for these resources. Compared
to the definition in [Neumann and van Noord, 1994] re-
ferred in sect. 2.1, this is a more general definition which
can be applied to a lot of existing NLDSs.
Let us again use the Conald system as an example,
this time only taking the data structures into account,
in order to search for possible re-use of resources. Two
core linguistic resources of its parsing branch are re-used
in its NLG component Hyperbug [Klarner and Ludwig,
2004]: The system lexicon and the morphology compo-
nent are both used by the parser and the generator, with
static reversibility for the system lexicon and dynamic re-
versibility for the morphology component. As mentioned
in sect. 4, re-use is also done for the pragmatic resources,
namely discourse memory and domain model.
Generally speaking, the more linguistic and pragmatic
resources are re-used in an NLDS, the higher its degree of
reversibility becomes, and the more efficient the system
will be to develop and maintain.
6 Conclusion and Further Work
We have developed a formal description of reversibility
for NLDSs, using definitions for program systems, sys-
tem relations, and system resources. Based on these def-
initions, we have presented a classification of reversible
NLDSs in general and NLG systems in particular. Our
classification extends previous approaches in three di-
mensions: First, it covers static and dynamic reversibil-
ity, second, it considers algorithmic and data reversibil-
ity, and third, it takes the different resources of a dialog
system into account.
The 6-tuple used in our classification can, of course,
be extended to incorporate different linguistic and prag-
matic resources, should they prove useful for an NLDS.
However, we identified the set of resources mentioned
above by thorough investigation of existing systems
based on the results presented in [Maier, 1999] for text
planning; presently, we do not think we need additional
ones.
Unfortunately, our definition of reversibility does not
yet completely reflect all aspects of current NLDSs: For
example, it does not cover systems where preprocessing
and runtime programs cannot be clearly separated, be-
cause such systems allow a flexible choice for a given
resource and/or algorithm to be computed beforehand
(by preprocessing) or at runtime.7 This extended de-
gree of dynamic has yet to be taken into account in our
definitions.
The obvious practical application of our classification
is twofold: First, using it in a descriptive way to analyze
existing systems. Second, and more practical, using it in
a normative way to further develop one?s one NLDS to
be as reversible as possible (i.e., to obtain a ?D? in all
six positions of the 6-tuple of reversibility types). Both
applications are important, but the second is the one we
are going to pursue in the near future.
Acknowledgments
Most of the work described here was done while complet-
ing my PhD thesis [Klarner, 2005] at the Chair for Artifi-
cal Intelligence of the University of Erlangen-Nuremberg.
This is why I want to thank my former colleagues there,
7While such systems are certainly an attractive theoretical
possibility, we are not aware of real-world existing ones so far.
especially Bernd Ludwig and Peter Rei?, for their en-
during cooperation and support. Many thanks to the
anonymous reviewers as well for providing very helpful
comments to the initial version of this paper.
References
[Allen et al, 2001] J. Allen, G. Ferguson, and A. Stent.
An architecture for more realistic conversational sys-
tems. In Proc. 6th Int. Conf. on Intelligent User In-
terfaces (IUI-2001), pages 1?8, Santa Fe, 2001.
[Ferguson and Allen, 1998] George Ferguson and James
Allen. Trips: An intelligent integrated problem-
solving assistant. In Proc. AAAI-98, Madison, WI,
1998.
[Gabrilovich et al, 1998] Evgeniy Gabrilovich, Nissim
Francez, and Shuly Wintner. Natural language gen-
eration with abstract machine. In Proc. INLG-98,
Niagara-on-the-Lake, 1998.
[Klarner and Ludwig, 2004] Martin Klarner and Bernd
Ludwig. Hybrid natural language generation in a
spoken language dialog system. In Susanne Biundo,
Thom Fru?hwirth, and Gu?nther Palm, editors, Proc.
KI-2004, pages 97?112, Ulm, 2004.
[Klarner, 2005] Martin Klarner. Hybride, pragma-
tisch eingebettete Realisierung mittels Bottom-Up-
Generierung in einem natu?rlichsprachlichen Di-
alogsystem. PhD thesis, Erlangen-Nu?rnberg, 2005.
[Ludwig, 2003] Bernd Ludwig. Ein konfigurierbares
Dialogsystem fu?r Mensch-Maschine-Interaktion in
gesprochener Sprache. PhD thesis, Universita?t
Erlangen-Nu?rnberg, 2003.
[Maier, 1999] Elisabeth Maier. Entwurf und Implemen-
tierung von Wissensquellen fu?r die Textplanung ? eine
modulare Architektur. Berlin, 1999.
[Neumann and van Noord, 1994] Gu?nther Neumann
and Gertjaan van Noord. Reversibility and self-
monitoring in natural language generation. In Tomek
Strzalkowski, editor, Reversible Grammars in Natural
Language Processing. Boston, Dordrecht, London,
1994.
[Neumann, 1994] Gu?nter Neumann. A Uniform Compu-
tation Model for Natural Language Parsing and Gen-
eration. PhD thesis, Universita?t des Saarlands, 1994.
[Stent, 2001] Amanda J. Stent. Dialogue Systems as
Conversational Partners: Applying Conversation Acts
Theory to Natural Language Generation for Task-
Oriented Mixed-Initiative Spoken Dialogue. PhD the-
sis, University of Massachusetts Amherst, 2001.
