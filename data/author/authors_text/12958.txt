Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 19?27,
Beijing, August 2010
Robust Measurement and Comparison of Context Similarity for Finding
Translation Pairs
Daniel Andrade?, Tetsuya Nasukawa?, Jun?ichi Tsujii?
?Department of Computer Science, University of Tokyo
{daniel.andrade, tsujii}@is.s.u-tokyo.ac.jp
?IBM Research - Tokyo
nasukawa@jp.ibm.com
Abstract
In cross-language information retrieval it
is often important to align words that are
similar in meaning in two corpora writ-
ten in different languages. Previous re-
search shows that using context similar-
ity to align words is helpful when no
dictionary entry is available. We sug-
gest a new method which selects a sub-
set of words (pivot words) associated with
a query and then matches these words
across languages. To detect word associa-
tions, we demonstrate that a new Bayesian
method for estimating Point-wise Mutual
Information provides improved accuracy.
In the second step, matching is done in
a novel way that calculates the chance of
an accidental overlap of pivot words us-
ing the hypergeometric distribution. We
implemented a wide variety of previously
suggested methods. Testing in two con-
ditions, a small comparable corpora pair
and a large but unrelated corpora pair,
both written in disparate languages, we
show that our approach consistently out-
performs the other systems.
1 Introduction
Translating domain-specific, technical terms from
one language to another can be challenging be-
cause they are often not listed in a general dictio-
nary. The problem is exemplified in cross-lingual
information retrieval (Chiao and Zweigenbaum,
2002) restricted to a certain domain. In this case,
the user might enter only a few technical terms.
However, jargons that appear frequently in the
data set but not in general dictionaries, impair the
usefulness of such systems. Therefore, various
means to extract translation pairs automatically
have been proposed. They use different clues,
mainly
? Spelling distance or transliterations, which
are useful to identify loan words (Koehn and
Knight, 2002).
? Context similarity, helpful since two words
with identical meaning are often used in sim-
ilar contexts across languages (Rapp, 1999).
The first type of information is quite specific; it
can only be helpful in a few cases, and can thereby
engender high-precision systems with low recall,
as described for example in (Koehn and Knight,
2002). The latter is more general. It holds for
most words including loan words. Usually the
context of a word is defined by the words which
occur around it (bag-of-words model).
Let us briefly recall the main idea for using
context similarity to find translation pairs. First,
the degree of association between the query word
and all content words is measured with respect to
the corpus at hand. The same is done for every
possible translation candidate in the target cor-
pus. This way, we can create a feature vector
for the query and all its possible translation can-
didates. We can assume that, for some content
words, we have valid translations in a general dic-
tionary, which enables us to compare the vectors
across languages. We will designate these content
words as pivot words. The query and its trans-
lation candidates are then compared using their
feature vectors, where each dimension in the fea-
ture vector contains the degree of association to
19
one pivot word. We define the degree of associa-
tion, as a measurement for finding words that co-
occur, or which do not co-occur, more often than
we would expect by pure chance.1
We argue that common ways for comparing
similarity vectors across different corpora perform
worse because they assume that degree of associa-
tions are very similar across languages and can be
compared without much preprocessing. We there-
fore suggest a new robust method including two
steps. Given a query word, in the first step we
determine the set of pivots that are all positively
associated with statistical significance. In the sec-
ond step, we compare this set of pivots with the set
of pivots extracted for a possible translation can-
didate. For extracting positively associated piv-
ots, we suggest using a new Bayesian method for
estimating the critical Pointwise Mutual Informa-
tion (PMI) value. In the second step, we use a
novel measure to compare the sets of extracted
pivot words which is based on an estimation of
the probability that pivot words overlap by pure
chance. Our approach engenders statistically sig-
nificant improved accuracy for aligning transla-
tion pairs, when compared to a variety of previ-
ously suggested methods. We confirmed our find-
ings using two very different pairs of comparable
corpora for Japanese and English.
In the next section, we review previous related
work. In Section 3 we explain our method in
detail, and argue that it overcomes subtle weak-
nesses of several previous efforts. In Section 4, we
show with a series of cross-lingual experiments
that our method, in some settings, can lead to con-
siderable improvement in accuracy. Subsequently
in Section 4.2, we analyze our method in contrast
to the baseline by giving two examples. We sum-
marize our findings in Section 5.
2 Related Work
Extracting context similarity for nouns and then
matching them across languages to find trans-
lation pairs was pioneered in (Rapp, 1999) and
(Fung, 1998). The work in (Chiao and Zweigen-
baum, 2002), which can be regarded as a varia-
1For example ?car? and ?tire? are expected to have a high
(positive) degree of association, and ?car? and ?apple? is ex-
pected to have a high (negative) degree of association.
tion of (Fung, 1998), uses tf.idf, but suggests to
normalize the term frequency by the maximum
number of co-occurrences of two words in the cor-
pus. All this work is closely related to our work
because they solely consider context similarity,
whereas context is defined using a word window.
The work in (Rapp, 1999; Fung, 1998; Chiao and
Zweigenbaum, 2002) will form the baselines for
our experiments in Section 4.2 This baseline is
also similar to the baseline in (Gaussier et al,
2004), which showed that it can be difficult to beat
such a feature vector approach.
In principle our method is not restricted to how
context is defined; we could also use, for exam-
ple, modifiers and head words, as in (Garera et
al., 2009). Although, we found in a preliminary
experiment that using a dependency parser to dif-
ferentiate between modifiers and head words like
in (Garera et al, 2009), instead of a bag-of-words
model, in our setting, actually decreased accuracy
due to the narrow dependency window. How-
ever, our method could be combined with a back-
translation step, which is expected to improve
translation quality as in (Haghighi et al, 2008),
which performs indirectly a back-translation by
matching all nouns mutually exclusive across cor-
pora. Notably, there also exist promising ap-
proaches which use both types of information,
spelling distance, and context similarity in a joint
framework, see (Haghighi et al, 2008), or (De?jean
et al, 2002) which include knowledge of a the-
saurus. In our work here, we concentrate on the
use of degrees of association as an effective means
to extract word translations.
In this application, to measure association ro-
bustly, often the Log-Likelihood Ratio (LLR)
measurement is suggested (Rapp, 1999; Morin et
al., 2007; Chiao and Zweigenbaum, 2002). The
occurrence of a word in a document is modeled
as a binary random variable. The LLR measure-
ment measures stochastic dependency between
2Notable differences are that we neglected word order, in
contrast to (Rapp, 1999), as it is little useful to compare it
between Japanese and English. Furthermore in contrast to
(Fung, 1998) we use only one translation in the dictionary,
which we select by comparing the relative frequencies. We
also made a second run of the experiments where we man-
ually selected the correct translations for the first half of the
most frequent pivots ? Results did not change significantly.
20
two such random variables (Dunning, 1993), and
is known to be equal to Mutual Information that is
linearly scaled by the size of the corpus (Moore,
2004). This means it is a measure for how much
the occurrence of word A makes the occurrence
of word B more likely, which we term positive
association, and how much the absence of word
A makes the occurrence of word B more likely,
which we term negative association. However, our
experiments show that only positive association is
beneficial for aligning words cross-lingually. In
fact, LLR can still be used for extracting posi-
tive associations by filtering in a pre-processing
step words with possibly negative associations
(Moore, 2005). Nevertheless a problem which
cannot be easily remedied is that confidence es-
timates using LLR are unreliable for small sample
sizes (Moore, 2004). We suggest a more princi-
pled approach that measures from the start only
how much the occurrence of word A makes the
occurrence of word B more likely, which is des-
ignated as Robust PMI.
Another point that is common to (Rapp, 1999;
Morin et al, 2007; Chiao and Zweigenbaum,
2002; Garera et al, 2009; Gaussier et al, 2004)
is that word association is compared in a fine-
grained way, i.e. they compare the degree of asso-
ciation3 with every pivot word, even when it is low
or exceptionally high. They suggest as a compar-
ison measurement Jaccard similarity, Cosine sim-
ilarity, and the L1 (Manhattan) distance.
3 Our Approach
We presume that rather than similarity between
degree (strength of) of associations, the existence
of common word associations is a more reliable
measure for word similarity because the degrees
of association are difficult to compare for the fol-
lowing reasons:
? Small differences in the degree of associa-
tion are not statistically significant
Taking, for example, two sample sets from
3To clarify terminology, where possible, we will try to
distinguish between association and degree of association.
For example word ?car? has the association ?tire?, whereas
the degree of association with ?tire? is a continuous number,
like 5.6.
the same corpus, we will in general measure
different degrees of association.
? Differences in sub-domains / sub-topics
Corpora sharing the same topic can still dif-
fer in sub-topics.
? Differences in style or language
Differences in word usage. 4
Other information that is used in vector ap-
proaches such as that in (Rapp, 1999) is nega-
tive association, although negative association is
less informative than positive. Therefore, if it is
used at all, it should be assigned a much smaller
weight.
Our approach caters to these points, by first de-
ciding whether a pivot word is positively associ-
ated (with statistical significance) or whether it
is not, and then uses solely this information for
finding translation pairs in comparable corpora. It
is divisible into two steps. In the first, we use a
Bayesian estimated PointwiseMutual Information
(PMI) measurement to find the pivots that are pos-
itively associated with a certain word with high
confidence. In the second step, we compare two
words using their associated pivots as features.
The similarity of feature sets is calculated using
pointwise entropy. The words for which feature
sets have high similarity are assumed to be related
in meaning.
3.1 Extracting positively associated words ?
Feature Sets
To measure the degree of positive association be-
tween two words x and y, we suggest the use
of information about how much the occurrence
of word x makes the occurrence of word y more
likely. We express this using Pointwise Mutual
Information (PMI), which is defined as follows:
PMI(x, y) = log p(x, y)p(x) ? p(y) = log
p(x|y)
p(x) .
Therein, p(x) is the probability that word x oc-
curs in a document; p(y) is defined analogously.
Furthermore, p(x, y) is the probability that both
4For example, ?stop? is not the only word to describe the
fact that a car halted.
21
words occur in the same document. A positive as-
sociation is given if p(x|y) > p(x). In related
works that use the PMI (Morin et al, 2007), these
probabilities are simply estimated using relative
frequencies, as
PMI(x, y) = log
f(x,y)
n
f(x)
n
f(y)
n
,
where f(x), f(y) is the document frequency
of word x and word y, and f(x, y) is the co-
occurrence frequency; n is the number of docu-
ments. However, using relative frequencies to es-
timate these probabilities can, for low-frequency
words, produce unreliable estimates for PMI
(Manning and Schu?tze, 2002). It is therefore nec-
essary to determine the uncertainty of PMI esti-
mates. The idea of defining confidence intervals
over PMI values is not new (Johnson, 2001); how-
ever, the problem is that exact calculation is very
computationally expensive if the number of docu-
ments is large, in which case one can approximate
the binomial approximation for example with a
Gaussian, which is, however only justified if n
is large and p, the probability of an occurrence,
is not close to zero (Wilcox, 2009). We suggest
to define a beta distribution over each probabil-
ity of the binary events that word x occurs, i.e.
[x], and analogously [x|y]. It was shown in (Ross,
2003) that a Bayesian estimate for Bernoulli trials
using the beta distribution delivers good credibil-
ity intervals5, importantly, when sample sizes are
small, or when occurrence probabilities are close
to 0. Therefore, we assume that
p(x|y) ? beta(??x|y, ??x|y), p(x) ? beta(??x, ??x)
where the parameters for the two beta distribu-
tions are set to
??x|y = f(x, y) + ?x|y ,
??x|y = f(y) ? f(x, y) + ?x|y , and
??x = f(x) + ?x, ??x = n ? f(x) + ?x .
Prior information related to p(x) and the con-
ditional probability p(x|y) can be incorporated
5In the Bayesian notation we refer here to credibility in-
tervals instead of confidence intervals.
by setting the hyper-parameters of the beta-
distribtutions.6 These can, for example, be
learned from another unrelated corpora pair and
then weighted appropriately by setting ?+ ?. For
our experiments, we use no information beyond
the given corpora pair; the conditional priors are
therefore set equal to the prior for p(x). Even if
we do not know which word x is, we have a notion
about p(x) because Zipf?s law indicates to us that
we should expect it to be small. A crude estima-
tion is therefore the mean word occurrence proba-
bility in our corpus as
? = 1|all words|
?
x?{all words}
f(x)
n .
We give this estimate a total weight of one obser-
vation. That is, we set
? = ? , ? = 1 ? ? .
From a practical perspective, this can be inter-
preted as a smoothing when sample sizes are
small, which is often the case for p(x|y). Because
we assume that p(x|y) and p(x) are random vari-
ables, PMI is consequently also a random variable
that is distributed according to a beta distribution
ratio.7 For our experiments, we apply a general
sampling strategy. We sample p(x|y) and p(x) in-
dependently and then calculate the ratio of times
PMI > 0 to determine P (PMI > 0).8 We will
refer to this method as Robust PMI (RPMI).
Finally we can calculate, for any word x, the set
of pivot words which have most likely a positive
association with word x. We require that this set
be statistically significant: the probability of one
or more words being not a positive association is
smaller than a certain p-value.9
6The hyper-parameters ? and ?, can be intuitively inter-
preted in terms of document frequency. For example ?x is
the number of times we belief the word x occurs, and ?x the
number of times we belief that x does not occur in a corpus.
Analogously ?x|y and ?x|y can be interpreted with respect
to the subset of the corpus where the word y occurs, instead
of the whole corpus. Note however, that ? and ? do not nec-
essarily have to be integers.
7The resulting distribution for the general case of a beta
distribution ratio was derived in (Pham-Gia, 2000). Unfortu-
nately, it involves the calculation of a Gauss hyper-geometric
function that is computationally expensive for large n.
8For experiments, we used 100, 000 samples for each es-
timate of P (PMI > 0).
9We set, for all of our experiments, the p-value to 0.01.
22
As an alternative for determining the probabil-
ity of a positive association using P (PMI > 0),
we calculate LLR and assume that approximately
LLR ? ?2 with one degree of freedom (Dunning,
1993). Furthermore, to ensure that only positive
association counts, we set the probability to zero
if p(x, y) < p(x) ? p(y), where the probabilities
are estimated using relative frequencies (Moore,
2005). We refer to this as LLR(P); lacking this
correction, it is LLR.
3.2 Comparing Word Feature Sets Across
Corpora
So far, we have explained a robust means to ex-
tract the pivot words that have a positive associa-
tion with the query. The next task is to find a sen-
sible way to use these pivots to compare the query
with candidates from the target corpus. A simple
means to match a candidate with a query is to see
how many pivots they have in common, i.e. using
the matching coefficient (Manning and Schu?tze,
2002) to score candidates. This similarity mea-
sure produces a reasonable result, as we will show
in the experiment section; however, in our error
analysis, we found out that this gives a bias to
candidates with higher frequencies, which is ex-
plainable as follows. Assuming that a word A has
a fixed number of pivots that are positively associ-
ated, then depending on the sample size?the doc-
ument frequency in the corpus?not all of these
are statistically significant. Therefore, not all true
positive associations are included in the feature
set to avoid possible noise. If the document fre-
quency increases, then we can extract more sta-
tistically significant positive associations and the
cardinality of the feature set increases. This con-
sequently increases the likelihood of having more
pivots that overlap with pivots from the query?s
feature set. For example, imagine two candidate
words A and B, for which feature sets of both in-
clude the feature set of the query, i.e. a complete
match, howeverA?s feature set is much larger than
B?s feature set. In this case, the information con-
veyed by having a complete match with the query
word?s feature set is lower in the case of A?s fea-
ture set than in case of B?s feature set. Therefore,
we suggest its use as a basis of our similarity mea-
sure, the degree of pointwise entropy of having an
estimate of m matches, as
Information(m, q, c) = ? log(P (matches = m)).
Therein, P (matches = m) is the likelihood that a
candidate word with c pivots has m matches with
the query word, which has q pivots. Letting w be
the total number of pivot words, we can then cal-
culate that the probability that the candidate with
c pivots was selected by chance
P (matches = m) =
( q
m
)
?
(w?q
c?m
)
(w
c
) .
Note that this probability equals a hypergeometric
distribution.10 The smaller P (matches = m) is,
the less likely it is that we obtain m matches by
pure chance. In other words, if P (matches = m)
is very small, m matches are more than we would
expect to occur by pure chance.11
Alternatively, in our experiments, we also con-
sider standard similarity measurements (Manning
and Schu?tze, 2002) such as the Tanimoto coeffi-
cient, which also lowers the score of candidates
that have larger feature sets.
4 Experiments
In our experiments, we specifically examine trans-
lating nouns, mostly technical terms, which occur
in complaints about cars collected by the Japanese
Ministry of Land, Infrastructure, Transport and
Tourism (MLIT)12, and in complaints about cars
collected by the USA National Highway Traffic
Safety Administration (NHTSA)13. We create for
each data collection a corpus for which a doc-
ument corresponds to one car customer report-
ing a certain problem in free text. The com-
plaints are, in general, only a few sentences long.
10` q
m
? is the number of possible combinations of pivots
which the candidate has in common with the query. There-
fore, ` qm
?
?
`w?q
c?m
? is the number of possible different feature
sets that the candidate can have such that it sharesm common
pivots with the query. Furthermore, `wc
? is the total number
of possible feature sets the candidate can have.
11The discussion is simplified here. It can also be that
P (matches = m) is very small, if there are less occur-
rences of m that we would expect to occur by pure chance.
However, this case can be easily identified by looking at the
gradient of P (matches = m).
12http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html
13http://www-odi.nhtsa.dot.gov/downloads/index.cfm
23
To verify whether our results can be generalized
over other pairs of comparable corpora, we ad-
ditionally made experiments using two corpora
extracted from articles of Mainichi Shinbun, a
Japanese newspaper, in 1995 and English articles
from Reuters in 1997. There are two notable dif-
ferences between those two pairs of corpora: the
content is much less comparable, Mainichi re-
ports more national news than world news, and
secondly, Mainichi and Reuters corpora are much
larger than MLIT/NHTSA.14
For both corpora pairs, we extracted a
gold-standard semi-automatically by looking at
Japanese nouns and their translations with docu-
ment frequency of at least 50 for MLIT/NHTSA,
and 100 for Mainichi/Reuters. As a dictionary we
used the Japanese-English dictionary JMDic15.
In general, we preferred domain-specific terms
over very general terms, i.e. for example for
MLIT/NHTSA the noun ?? ?injection? was
preferred over ???? ?installation?. We ex-
tracted 100 noun pairs for MLIT/NHTSA and
Mainichi/Reuters, each. Each Japanese noun
which is listed in the gold-standard forms a query
which is input into our system. The resulting
ranking of the translation candidates is automat-
ically evaluated using the gold-standard. There-
fore, synonyms that are not listed in the gold stan-
dard are not recognized, engendering a conserva-
tive estimation of the translation accuracy. Be-
cause all methods return a ranked list of trans-
lation candidates, the accuracy is measured us-
ing the rank of the translation listed in the gold-
standard.16 The Japanese corpora are prepro-
cessed with MeCab (Kudo et al, 2004); the En-
glish corpora with Stepp Tagger (Tsuruoka et al,
2005) and Lemmatizer (Okazaki et al, 2008). As
a dictionary we use the Japanese-English dictio-
nary JMDic17. In line with related work (Gaussier
et al, 2004), we remove a word pair (Japanese
noun s, English noun t) from the dictionary, if s
occurs in the gold-standard. Afterwards we define
14MLIT/MLIT has each 20,000 documents.
Mainichi/Reuters corpora 75,935 and 148,043 documents,
respectively.
15http://www.csse.monash.edu.au/ jwb/edict doc.html
16In cases for which there are several translations listed for
one word, the rank of the first is used.
17http://www.csse.monash.edu.au/ jwb/edict doc.html
the pivot words by consulting the remaining dic-
tionary.
4.1 Crosslingual Experiment
We compare our approach used for extract-
ing cross-lingual translation pairs against several
baselines. We compare to LLR + Manhattan
(Rapp, 1999) and our variation LLR(P) + Man-
hattan. Additionally, we compare TFIDF(MSO)
+ Cosine, which is the TFIDF measure, whereas
the Term Frequency is normalized using the max-
imal word frequency and the cosine similarity
for comparison suggested in (Fung, 1998). Fur-
thermore, we implemented two variations of this,
TFIDF(MPO) + Cosine and TFIDF(MPO) + Jac-
card coefficient, which were suggested in (Chiao
and Zweigenbaum, 2002). In fact, TFIDF(MPO)
is the TFIDF measure, whereas the Term Fre-
quency is normalized using the maximal word pair
frequency. The results are displayed in Figure 1.
Our approach clearly outperforms all baselines;
notably it has Top 1 accuracy of 0.14 and Top 20
accuracy of 0.55, which is much better than that
for the best baseline, which is 0.11 and 0.44, re-
spectively.
experiment that are similar to those of our cross-
lingual experi ent, we use the same pivot words
and the same gold standard as that used for the
MLIT/NHTSA experiments, for which a pair (A,
translation of A) is changed to (A, A): that is, the
word becomes the translation of itself. The result
of the monolingual experiment in Table 2 shows
that our method performs slightly worse than the
baseline, LLR + Manhattan, i.e. LLR with L1 nor-
malization and L1 distance(Rapp, 1999). Further-
more, LLR(P) + Manhattan using only positive as-
sociations also performs slightly worse.
Top 1 Top 10 Top 20
LLR + Manhattan 0.94 0.99 0.99
LLR(P) + Man attan 0.89 1.0 1.0
RPMI + Entropy 0.79 0.94 0.95
Table 2: Monolingual NHTSA experiment.
In our main experiment, we compare our ap-
proach used for extracting cross-lingual transla-
ti n pairs ag inst seve al baselines. As before,
we compare LLR + Manhattan (Rapp, 1999) and
the variation LLR(P) + Manhattan. Addition-
ally, we compare TFIDF(MSO) + Cosine, which
is the TFIDF measure, whereas the Term Fre-
quency is normalized using the maximal word fre-
quency and the cosine similarity for comparison
suggested in (Fung, 1998). Furthermore, we im-
plemented two variations of this, TFIDF(MPO) +
Cosine and TFIDF(MPO) + Jaccard coefficient,
which were suggested in (Chiao and Zweigen-
baum, 2002). In fact, TFIDF(MPO) is the TFIDF
measure, whereas the Term Frequency is normal-
ized using the maximal word pair frequency.14
The results are displayed in Figure 1. Our ap-
proach clearly outperforms all baselines; notably
it has top 1 accuracy of 0.14 and top 20 accuracy
of 0.55, which is much better than that for the best
baseline, which is 0.11 and 0.44, respectively.
We next leave the proposed framework con-
stant, but change the mode of estimating positive
associations and the way to match feature sets.
As alternatives for estimating the probability that
there is a positive association, we test LLR(P) and
LLR. As alternatives for comparing feature sets,
we investigate the matching coefficient (match-
ing), cosine similarity (cosine), Tanimoto coeffi-
14We tried, like originally suggested, using maximum
count of every occurring word pair, i.e. (content word, con-
tent word), but using maximum of all pairs (content word,
pivot word) improves always slightly accuracy. Therefore for
we chose the latter as a baseline.
?
??
??
??
??
??
??
? ? ?? ?? ?? ??
???????????????????????????????????????????????????
Figure 1: Percentile ranking of our approach
RPMI + Entropy against various previous sug-
gested methods.
cient (tani), and overlap coefficient (over) (Man-
ning and Schu?tze, 2002). The result of every com-
bination is displayed concisely in Table 3 using the
median rank. In our experience, the median rank
is a good choice of measure of location for our
problem because we have, in general, a skewed
distribution over the ranks. The cases in which
the median ranks are close to RPMI + entropy are
magnified in 4.
It is readily apparent that most alternatives per-
form clearly worse. Looking at Table 4, we can
see that only RPMI + Entropy, and LLR(P) +
Entropy, perform similar. Pointwise entropy in-
creases the accuracy (Top 1) over the matching
coefficient and is clearly superior to other similar-
ity measures. Overlap similarity performs well in
contrast to other standard measurements because
other measures punish words with a high number
of associated pivots too severely. However, our
approach of using pointwise entropy as a measure
of similarity performs best because it more ade-
quately punishes words with a high number of as-
sociated pivots. Finally, LLR(P) presents a clear
edge over LLR, which suggests that indeed only
positive associations seem to matter in a cross-
lingual setting.
Entropy Matching Cosine Tani Over
RPMI 13.0 17.0 24.0, 37.5 36.0
LLR(P) 16.0 15.0 22.5 34.0 25.5
LLR 23.5 22.0 27.5 50.5 50.0
Table 3: Evaluation Matrix
Finally, we aim to clarify whether these re-
sults are specific to a certain type of compara-
ble corpora pair or if they hold more generally.
Therefore, we conduct the same experiments us-
ing the very different comparable corpora pair
Mainichi/Reuters. When comparing to the best
Figure 1: Crosslingual Experiment
MLIT/NHTSA ? Percentile Ranking of RPMI
+ Entropy Against Various Previous Suggested
Methods.
We next leave the proposed framework con-
stant, but change the mode of estimating posi-
tive associations and the way to match feature
sets. As alternatives for estimating the proba-
bility that there is a positive association, we test
LLR(P) and LLR. As alternatives for comparing
feature sets, we investigate the matching coef-
ficient (match), cosine similarity (cosine), Tan-
imoto coefficient (tani), and overlap coefficient
24
(over) (Manning and Schu?tze, 2002). The re-
sult of every combination is displayed concisely
in Table 1 using the median rank18. The cases
in which the median ranks are close to RPMI +
Entropy are magnified in Table 2. We can see
there that RPMI + Entropy, and LLR(P) + En-
tropy perform nearly equally. All other combina-
tions perform worse, especially in Top 1 accuracy.
Finally, LLR(P) presents a clear edge over LLR,
which suggests that indeed only positive associa-
tions seem to matter in a cross-lingual setting.
Entropy Match Cosine Tani Over
RPMI 13.0 17.0 24.0 37.5 36.0
LLR(P) 16.0 15.0 22.5 34.0 25.5
LLR 23.5 22.0 27.5 50.5 50.0
Table 1: Crosslingual experiment MLIT/NHTSA
? Evaluation matrix showing the median ranks of
several combinations of association and similarity
measures.
Top 1 Top 10 Top 20
RPMI + Entropy 0.14 0.46 0.55
RPMI + Matching 0.08 0.41 0.57
LLR(P) + Entropy 0.14 0.46 0.55
LLR(P) + Matching 0.08 0.44 0.55
Table 2: Accuracies for crosslingual experiment
MLIT/NHTSA.
Finally we conduct an another experiment using
the corpora pair Mainichi/Reuters which is quite
different from MLIT/NHTSA. When comparing
to the best baselines in Table 3 we see that our
approach again performs best. Furthermore, the
experiments displayed in Table 4 suggest that Ro-
bust PMI and pointwise entropy are better choices
for positive association measurement and similar-
ity measurement, respectively. We can see that
Top 1 Top 10 Top 20
RPMI + Entropy 0.15 0.38 0.46
LLR(P) + Manhattan 0.10 0.26 0.33
TFIDF(MPO) + Cos 0.05 0.12 0.18
Table 3: Accuracies for crosslingual experiment
Mainichi/Reuters ? Comparison to best baselines.
18A median rank of i, means that 50% of the correct trans-
lations have a rank higher than i.
Top 1 Top 10 Top 20
RPMI + Entropy 0.15 0.38 0.46
RPMI + Matching 0.08 0.30 0.35
LLR(P) + Entropy 0.13 0.36 0.47
LLR(P) + Matching 0.08 0.29 0.37
Table 4: Accuracies for crosslingual experiment
Mainichi/Reuters ? Comparison to alternatives.
the overall best baseline turns out to be LLR(P) +
Manhattan. Comparing the rank from each word
from the gold-standard pairwise, we see that our
approach, RPMI + Entropy, is significantly better
than this baseline in MLIT/NHTSA as well as in
Mainichi/Reuters.19
4.2 Analysis
In this section, we provide two representative ex-
amples extracted from the previous experiments
which sheds light into a weakness of the stan-
dard feature vector approach which was used as a
baseline before. The two example queries and the
corresponding responses of LLR(P) + Manhattan
and our approach are listed in Table 5. Further-
more in Table 6 we list the pivot words with the
highest degree of association (here LLR values)
for the query and its correct translation. We can
see that a query and its translation shares some
pivots which are associated with statistical signif-
icance20. However it also illustrates that the ac-
tual LLR value is less insightful and can hardly be
compared across these two corpora.
Let us analyze the two examples in more de-
tail. In Table 6, we see that the first query ??
?gear?21 is highly associated with??? ?shift?.
However, on the English side we see that gear is
most highly associated with the pivot word gear.
Note that here the word gear is also a pivot word
corresponding to the Japanese pivot word ??
?gear (wheel)?.22 Since in English the word gear
(shift) and gear (wheel) is polysemous, the surface
forms are the same leading to a high LLR value of
19Using pairwise test with p-value 0.05.
20Note that for example, an LLR value bigger than 11.0
means the chances that there is no association is smaller than
0.001 using that LLR ? ?2.
21For a Japanese word, we write the English translation
which is appropriate in our context, immediately after it.
22In other words, we have the entry (??, gear) in our
dictionary but not the entry (??, gear). The first pair is
used as a pivot, the latter word pair is what we try to find.
25
gear. Finally, the second example query ???
?pedal? shows that words which, not necessarily
always, but very often co-occur, can cause rela-
tively high LLR values. The Japanese verb ??
?to press? is associated with ??? with a high
LLR value ? 4 times higher than ?? ?return?
? which is not reflected on the English side. In
summary, we can see that in both cases the degree
of associations are rather different, and cannot be
compared without preprocessing. However, it is
also apparent that in both examples a simple L1
normalization of the degree of associations does
not lead to more similarity, since the relative dif-
ferences remain.
?? ?gear?
Method Top 3 candidates Rank
baseline jolt, lever, design 284
filtering reverse, gear, lever 2
??? ?pedal?
Method Top 3 candidates Rank
baseline mj, toyota, action 176
filtering pedal, situation, occasion 1
Table 5: List of translation suggestions using
LLR(P) + Manhattan (baseline) and our method
(filtering). The third column shows the rank of
the correct translation.
?? gear
Pivots LLR(P) Pivots LLR(P)
?? ?shift? 154 gear 7064
??? ?shift? 144 shift 1270
??? ?come out? 116 reverse 314
??? pedal
Pivots LLR(P) Pivots LLR(P)
?? ?press? 628 floor 1150
?? ?return? 175 stop 573
? ?foot? 127 press 235
Table 6: Shows the three pivot words which have
the highest degree of association with the query
(left side) and the correct translation (right side).
5 Conclusions
We introduced a new method to compare con-
text similarity across comparable corpora using a
Bayesian estimate for PMI (Robust PMI) to ex-
tract positive associations and a similarity mea-
surement based on the hypergeometric distribu-
tion (measuring pointwise entropy). Our experi-
ments show that, for finding cross-lingual trans-
lations, the assumption that words with similar
meaning share positive associations with the same
words is more appropriate than the assumption
that the degree of association is similar. Our ap-
proach increases Top 1 and Top 20 accuracy of
up to 50% and 39% respectively, when compared
to several previous methods. We also analyzed
the two components of our method separately. In
general, Robust PMI yields slightly better per-
formance than the popular LLR, and, in contrast
to LLR, allows to extract positive associations as
well as to include prior information in a principled
way. Pointwise entropy for comparing feature sets
cross-lingually improved the translation accuracy
clearly when compared with standard similarity
measurements.
Acknowledgment
We thank Dr. Naoaki Okazaki and the anony-
mous reviewers for their helpful comments. Fur-
thermore we thank Daisuke Takuma, IBM Re-
search - Tokyo, for mentioning previous work
on statistical corrections for PMI. This work was
partially supported by Grant-in-Aid for Specially
Promoted Research (MEXT, Japan). The first au-
thor is supported by the MEXT Scholarship and
by an IBM PhD Scholarship Award.
References
Chiao, Y.C. and P. Zweigenbaum. 2002. Looking
for candidate translational equivalents in special-
ized, comparable corpora. In Proceedings of the In-
ternational Conference on Computational Linguis-
tics, pages 1?5. International Committee on Com-
putational Linguistics.
De?jean, H., E?. Gaussier, and F. Sadat. 2002. An ap-
proach based on multilingual thesauri and model
combination for bilingual lexicon extraction. In
Proceedings of the International Conference on
Computational Linguistics, pages 1?7. International
Committee on Computational Linguistics.
Dunning, T. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
Fung, P. 1998. A statistical view on bilingual
lexicon extraction: from parallel corpora to non-
parallel corpora. Lecture Notes in Computer Sci-
ence, 1529:1?17.
26
Garera, N., C. Callison-Burch, and D. Yarowsky.
2009. Improving translation lexicon induction from
monolingual corpora via dependency contexts and
part-of-speech equivalences. In Proceedings of the
Conference on Computational Natural Language
Learning, pages 129?137. Association for Compu-
tational Linguistics.
Gaussier, E., J.M. Renders, I. Matveeva, C. Goutte,
and H. Dejean. 2004. A geometric view on bilin-
gual lexicon extraction from comparable corpora.
In Proceedings of the Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 526?
533. Association for Computational Linguistics.
Haghighi, A., P. Liang, T. Berg-Kirkpatrick, and
D. Klein. 2008. Learning bilingual lexicons from
monolingual corpora. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics, pages 771?779. Association for Computa-
tional Linguistics.
Johnson, M. 2001. Trading recall for precision with
confidence-sets. Technical report, Brown Univer-
sity.
Koehn, P. and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proceedings
of ACL Workshop on Unsupervised Lexical Acquisi-
tion, volume 34, pages 9?16. Association for Com-
putational Linguistics.
Kudo, T., K. Yamamoto, and Y. Matsumoto. 2004.
Applying conditional random fields to Japanese
morphological analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 230?237. Association for Com-
putational Linguistics.
Manning, C.D. and H. Schu?tze. 2002. Foundations
of Statistical Natural Language Processing. MIT
Press.
Moore, R.C. 2004. On log-likelihood-ratios and the
significance of rare events. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 333?340. Association for
Computational Linguistics.
Moore, R.C. 2005. A discriminative framework for
bilingual word alignment. In Proceedings of the
Conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 81?88. Association for Computational
Linguistics.
Morin, E., B. Daille, K. Takeuchi, and K. Kageura.
2007. Bilingual terminology mining-using brain,
not brawn comparable corpora. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics, volume 45, pages 664?671. As-
sociation for Computational Linguistics.
Okazaki, N., Y. Tsuruoka, S. Ananiadou, and J. Tsu-
jii. 2008. A discriminative candidate generator for
string transformations. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 447?456. Association for Com-
putational Linguistics.
Pham-Gia, T. 2000. Distributions of the ratios of in-
dependent beta variables and applications. Com-
munications in Statistics. Theory and Methods,
29(12):2693?2715.
Rapp, R. 1999. Automatic identification of word
translations from unrelated English and German
corpora. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics,
pages 519?526. Association for Computational Lin-
guistics.
Ross, T.D. 2003. Accurate confidence intervals for
binomial proportion and Poisson rate estimation.
Computers in Biology and Medicine, 33(6):509?
531.
Tsuruoka, Y., Y. Tateishi, J. Kim, T. Ohta, J. Mc-
Naught, S. Ananiadou, and J. Tsujii. 2005. De-
veloping a robust part-of-speech tagger for biomed-
ical text. Lecture Notes in Computer Science,
3746:382?392.
Wilcox, R.R. 2009. Basic Statistics: Understanding
Conventional Methods and Modern Insights. Ox-
ford University Press.
27
Proceedings of NAACL-HLT 2013, pages 655?660,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Translation Acquisition Using Synonym Sets
Daniel Andrade Masaaki Tsuchida Takashi Onishi Kai Ishikawa
Knowledge Discovery Research Laboratories, NEC Corporation, Nara, Japan
{s-andrade@cj, m-tsuchida@cq,
t-onishi@bq, k-ishikawa@dq}.jp.nec.com
Abstract
We propose a new method for translation ac-
quisition which uses a set of synonyms to ac-
quire translations from comparable corpora.
The motivation is that, given a certain query
term, it is often possible for a user to specify
one or more synonyms. Using the resulting
set of query terms has the advantage that we
can overcome the problem that a single query
term?s context vector does not always reliably
represent a terms meaning due to the context
vector?s sparsity. Our proposed method uses
a weighted average of the synonyms? context
vectors, that is derived by inferring the mean
vector of the von Mises-Fisher distribution.
We evaluate our method, using the synsets
from the cross-lingually aligned Japanese and
English WordNet. The experiments show that
our proposed method significantly improves
translation accuracy when compared to a pre-
vious method for smoothing context vectors.
1 Introduction
Automatic translation acquisition is an important
task for various applications. For example, finding
term translations can be used to automatically up-
date existing bilingual dictionaries, which are an in-
dispensable resource for tasks such as cross-lingual
information retrieval and text mining.
Various previous research like (Rapp, 1999; Fung,
1998) has shown that it is possible to acquire word
translations from comparable corpora.
We suggest here an extension of this approach
which uses several query terms instead of a single
query term. A user who searches a translation for
a query term that is not listed in an existing bilin-
gual dictionary, might first try to find a synonym
of that term. For example, the user might look up
a synonym in a thesaurus1 or might use methods
for automatic synonym acquisition like described
in (Grefenstette, 1994). If the synonym is listed in
the bilingual dictionary, we can consider the syn-
onym?s translations as the translations of the query
term. Otherwise, if the synonym is not listed in the
dictionary either, we use the synonym together with
the original query term to find a translation.
We claim that using a set of synonymous query
terms to find a translation is better than using a single
query term. The reason is that a single query term?s
context vector is, in general, unreliable due to spar-
sity. For example, a low frequent query term tends to
have many zero entries in its context vector. To mit-
igate this problem it has been proposed to smooth
a query?s context vector by its nearest neighbors
(Pekar et al, 2006). However, nearest neighbors,
which context vectors are close the query?s context
vector, can have different meanings and therefore
might introduce noise.
The contributions of this paper are two-fold. First,
we confirm experimentally that smoothing a query?s
context vector with its synonyms leads in deed to
higher translation accuracy, compared to smoothing
with nearest neighbors. Second, we propose a sim-
ple method to combine a set of context vectors that
performs in this setting better than a method previ-
ously proposed by (Pekar et al, 2006).
Our approach to combine a set of context vec-
1Monolingual thesauri are, arguably, easier to construct than
bilingual dictionaries.
655
tors is derived by learning the mean vector of a von
Mises-Fisher distribution. The combined context
vector is a weighted-average of the original context-
vectors, where the weights are determined by the
word occurrence frequencies.
In the following section we briefly show the rela-
tion to other previous work. In Section 3, we explain
our method in detail, followed by an empirical eval-
uation in Section 4. We summarize our results in
Section 6.
2 Related Work
There are several previous works on extract-
ing translations from comparable corpora ranging
from (Rapp, 1999; Fung, 1998), and more re-
cently (Haghighi et al, 2008; Laroche and Langlais,
2010), among others. Essentially, all these meth-
ods calculate the similarity of a query term?s context
vector with each translation candidate?s context vec-
tor. The context vectors are extracted from the com-
parable corpora, and mapped to a common vector
space with the help of an existing bilingual dictio-
nary.
The work in (De?jean et al, 2002) uses cross-
lingually aligned classes in a multilingual thesaurus
to improve the translation accuracy. Their method
uses the probability that the query term and a trans-
lation candidate are assigned to the same class. In
contrast, our method does not need cross-lingually
aligned classes.
Ismail and Manandhar (2010) proposes a method
that tries to improve a query?s context vector by us-
ing in-domain terms. In-domain terms are the terms
that are highly associated to the query, as well as
highly associated to one of the query?s highly asso-
ciated terms. Their method makes it necessary that
the query term has enough highly associated context
terms.2 However, a low-frequent query term might
not have enough highly associated terms.
In general if a query term has a low-frequency in
the corpus, then its context vector is sparse. In that
case, the chance of finding a correct translation is
reduced (Pekar et al, 2006). Therefore, Pekar et al
(2006) suggest to use distance-based averaging to
smooth the context vector of a low-frequent query
2In their experiments, they require that a query word has at
least 100 associated terms.
term. Their smoothing strategy is dependent on the
occurrence frequency of a query term and its close
neighbors. Let us denote q the context vector of the
query word, and K be the set of its close neighbors.
The smoothed context vector q? is then derived by
using:
q? = ? ? q + (1 ? ?) ?
?
x?K
wx ? x , (1)
where wx is the weight of neighbor x, and all
weights sum to one. The context vectors q and x
are interpreted as probability vectors and therefore
L1-normalized. The weight wx is a function of the
distance between neighbor x and query q. The pa-
rameter ? determines the degree of smoothing, and
is a function of the frequency of the query term and
its neighbors:
? = log f(q)logmaxx?K?{q} f(x)
(2)
where f(x) is the frequency of term x. Their method
forms the baseline for our proposed method.
3 Proposed Method
Our goal is to combine the context vectors to one
context vector which is less sparse and more reli-
able than the original context vector of query word
q. We assume that for each occurrence of a word,
its corresponding context vector was generated by
a probabilistic model. Furthermore, we assume that
synonyms are generated by the same probability dis-
tribution. Finally we use the mean vector of that dis-
tribution to represent the combined context vector.
By using the assumption that each occurrence of a
word corresponds to one sample of the probability
distribution, our model places more weight on syn-
onyms that are highly-frequent than synonyms that
occur infrequently. This is motivated by the assump-
tion that context vectors of synonyms that occur with
high frequency in the corpus, are more reliable than
the ones of low-frequency synonyms.
When comparing context vectors, work
like Laroche and Langlais (2010) observed
that often the cosine similarity performs superior
to other distance-measures, like, for example, the
euclidean distance. This suggests that context
vectors tend to lie in the spherical vector space,
656
and therefore the von Mises-Fisher distribution is
a natural choice for our probabilistic model. The
von Mises-Fisher distribution was also successfully
used in the work of (Basu et al, 2004) to cluster
text data.
The von Mises-Fisher distribution with location
parameter ?, and concentration parameter ? is de-
fined as:
p(x|?, ?) = c(?) ? e??x??T ,
where c(?) is a normalization constant, and ||x|| =
||?|| = 1, and ? ? 0. || denotes here the L2-norm.
The cosine-similarity measures the angle between
two vectors, and the von Mises distribution defines
a probability distribution over the possible angles.
The parameter ? of the von Mises distribution is es-
timated as follows (Jammalamadaka and Sengupta,
2001): Given the words x1, ..., xn, we denote the
corresponding context vectors as x1, ...,xn, and as-
sume that each context vector is L2-normalized.
Then, the mean vector ? is calculated as:
? = 1Z
n
?
i=1
xi
n
where Z ensures that the resulting context vector is
L2-normalized, i.e. Z is ||
?n
i=1
xi
n ||. For our pur-
pose, ? is irrelevant and is assumed to be any fixed
positive constant.
Since we assume that each occurrence of a word x
in the corpus corresponds to one observation of the
corresponding word?s context vector x, we get the
following formula:
? = 1Z ? ?
n
?
i=1
f(xi)
?n
j=1 f(xj)
? xi
where Z ? is now ||
?n
i=1
f(xi)
?n
j=1 f(xj)
? xi||. We then
use the vector ? as the combined vector of the
words? context vectors xi.
Our proposed procedure to combine the context
vector of query word q and its synonyms can be sum-
marized as follows:
1. Denote the context vectors of q and its syn-
onyms as x1, ...,xn, and L2-normalize each
context vector.
2. Calculate the weighted average of the vectors
x1, ...,xn, whereas the weights correspond to
the frequencies of each word xi.
3. L2-normalize the weighted average.
4 Experiments
As source and target language corpora we use a cor-
pus extracted from a collection of complaints con-
cerning automobiles compiled by the Japanese Min-
istry of Land, Infrastructure, Transport and Tourism
(MLIT)3 and the USA National Highway Traffic
Safety Administration (NHTSA)4, respectively. The
Japanese corpus contains 24090 sentences that were
POS tagged using MeCab (Kudo et al, 2004). The
English corpus contains 47613 sentences, that were
POS tagged using Stepp Tagger (Tsuruoka et al,
2005), and use the Lemmatizer (Okazaki et al,
2008) to extract and stem content words (nouns,
verbs, adjectives, adverbs).
For creating the context vectors, we calculate the
association between two content words occurring
in the same sentence, using the log-odds-ratio (Ev-
ert, 2004). It was shown in (Laroche and Langlais,
2010) that the log-odds-ratio in combination with
the cosine-similarity performs superior to several
other methods like PMI5 and LLR6. For comparing
two context vectors we use the cosine similarity.
To transform the Japanese and English context
vectors into the same vector space, we use a bilin-
gual dictionary with around 1.6 million entries.7
To express all context vectors in the same vector
space, we map the context vectors in English to con-
text vectors in Japanese.8 First, for all the words
which are listed in the bilingual dictionary we calcu-
late word translation probabilities. These translation
probabilities are calculated using the EM-algorithm
described in (Koehn and Knight, 2000). We then
create a translation matrix T which contains in each
3http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html
4http://www-odi.nhtsa.dot.gov/downloads/index.cfm
5point-wise mutual information
6log-likelihood ratio
7The bilingual dictionary was developed in the course of our
Japanese language processing efforts described in (Sato et al,
2003).
8Alternatively, we could, for example, use canonical corre-
lation analysis to match the vectors to a common latent vector
space, like described in (Haghighi et al, 2008).
657
column the translation probabilities for a word in
English into any word in Japanese. Each context
vector in English is then mapped into Japanese us-
ing the linear transformation described by the trans-
lation matrix T . For word x with context vector x in
English, let x? be its context vector after transforma-
tion into Japanese, i.e. x? = T ? x.
The gold-standard was created by considering
all nouns in the Japanese and English WordNet
where synsets are aligned cross-lingually. This way
we were able to create a gold-standard with 215
Japanese nouns, and their respective English trans-
lations that occur in our comparable corpora.9 Note
that the cross-lingual alignment is needed only for
evaluation. For evaluation, we consider only the
translations that occur in the corresponding English
synset as correct.
Because all methods return a ranked list of trans-
lation candidates, the accuracy is measured using the
rank of the translation listed in the gold-standard.
The inverse rank is the sum of the inverse ranks of
each translation in the gold-standard.
In Table 1, the first row shows the results when us-
ing no smoothing. Next, we smooth the query?s con-
text vector by using Equation (1) and (2). The set of
neighbors K is defined as the k-terms in the source
language that are closest to the query word, with re-
spect to the cosine similarity (sim). The weight wx
for a neighbor x is set to wx = 100.13?sim(x,q) in
accordance to (Pekar et al, 2006). For k we tried
values between 1 and 100, and got the best inverse
rank when using k=19. The resulting method (Top-
k Smoothing) performs consistently better than the
method using no smoothing, see Table 1, second
row. Next, instead of smoothing the query word with
its nearest neighbors, we use as the set K the set of
synonyms of the query word (Syn Smoothing). Ta-
ble 1 shows a clear improvement over the method
that uses nearest neighbor-smoothing. This confirms
our claim that using synonyms for smoothing can
lead to better translation accuracy than using nearest
neighbors. In the last row of Table 1, we compare
our proposed method to combine context vectors of
synonyms (Syn Mises-Combination), with the pre-
9The resulting synsets in Japanese and English, contain in
average 2.2 and 2.8 words, respectively. The ambiguity of a
query term in our gold-standard is low, since, in average, a
query term belongs to only 1.2 different synsets.
vious method (Syn Smoothing). A pair-wise com-
parison of our proposed method with Syn Smooth-
ing shows a statistically significant improvement (p
< 0.01).10
Finally, we also show the result when simply
adding each synonym vector to the query?s context
vector to form a new combined context vector (Syn
Sum).11 Even though, this approach does not use the
frequency information of a word, it performs bet-
ter than Syn Smoothing. We suppose that this is
due to the fact that it actually indirectly uses fre-
quency information, since the log-odds-ratio tends
to be higher for words which occur with high fre-
quency in the corpus.
Method Top1 Top5 Top10 MIR
No Smoothing 0.14 0.30 0.36 0.23
Top-k Smoothing 0.16 0.33 0.43 0.26
Syn Smoothing 0.18 0.35 0.46 0.28
Syn Sum 0.23 0.46 0.57 0.35
Syn Mises-Combination 0.31 0.46 0.55 0.40
Table 1: Shows Top-n accuracy and mean inverse rank
(MIR) for baseline methods which use no synonyms
(No Smoothing, Top-k Smoothing), the proposed method
(Syn Mises-Combination) which uses synonyms, and al-
ternative methods that also use synonyms (Syn Smooth-
ing, Syn Sum).
5 Discussion
We first discuss an example where the query terms
are???? (cruise) and?? (cruise). Both words
can have the same meaning. The resulting trans-
lation candidates suggested by the baseline meth-
ods and the proposed method is shown in Table 2.
Using no smoothing, the baseline method outputs
the correct translation for ???? (cruise) and ?
? (cruise) at rank 10 and 15, respectively. When
combining both queries to form one context vector
our proposed method (Syn Mises-Combination) re-
trieves the correct translation at rank 2. Note that we
considered all nouns that occur three or more times
as possible translation candidates. As can be seen
in Table 2, this also includes spelling mistakes like
?sevice? and ?infromation?.
10We use the sign-test (Wilcox, 2009) to test the hypothesis
that the proposed method ranks higher than the baseline.
11No normalization is performed before adding the context
vectors.
658
Method Query Output Rank
No Smoothing ???? ..., affinity, delco, cruise, sevice, sentrum,... 10
No Smoothing ?? ..., denali, attendant, cruise, abs, tactic,... 15
Top-k Smoothing ???? pillar, multi, cruise, star, affinity,... 3
Top-k Smoothing ?? ..., burnout, dipstick, cruise, infromation, speed, ... 8
Syn Smoothing ???? smoothed with?? ..., affinity, delco, cruise, sevice, sentrum,... 10
Syn Smoothing ?? smoothed with???? ..., alldata, mode, cruise, expectancy, mph,... 8
Syn Sum ????,?? assumption, level, cruise, reimbursment, infromation,... 3
Syn Mises-Combination ????,?? pillar, cruise, assumption, level, speed,... 2
Table 2: Shows the results for ???? and ?? which both have the same meaning ?cruise?. The third column
shows part of the ranked translation candidates separated by comma. The last column shows the rank of the correct
translation ?cruise?. Syn Smoothing uses Equation (1) with q corresponding to the context vector of the query word,
andK contains only the context vector of the term that is used for smoothing.
Finally, we note that some terms in our test set
are ambiguous, and the ambiguity is not resolved by
using the synonyms of only one synset. For exam-
ple, the term ?? (steering, guidance) belongs to
the synset ?steering, guidance? which includes the
terms??? (steering, guidance) and??? (guid-
ance), ?? (guidance). Despite this conflation of
senses in one synset, our proposed method can im-
prove the finding of (one) correct translation. The
baseline system using only?? (steering, guidance)
outputs the correct translation ?steering? at rank 4,
whereas our method using all four terms outputs it
at rank 2.
6 Conclusions
We proposed a new method for translation acquisi-
tion which uses a set of synonyms to acquire transla-
tions. Our approach combines the query term?s con-
text vector with all the context vectors of its syn-
onyms. In order to combine the vectors we use a
weighted average of each context vector, where the
weights are determined by a term?s occurrence fre-
quency.
Our experiments, using the Japanese and English
WordNet (Bond et al, 2009; Fellbaum, 1998), show
that our proposed method can increase the transla-
tion accuracy, when compared to using only a single
query term, or smoothing with nearest neighbours.
Our results suggest that instead of directly search-
ing for a translation, it is worth first looking for syn-
onyms, for example by considering spelling varia-
tions or monolingual resources.
References
S. Basu, M. Bilenko, and R.J. Mooney. 2004. A prob-
abilistic framework for semi-supervised clustering. In
Proceedings of the ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 59?68.
F. Bond, H. Isahara, S. Fujita, K. Uchimoto, T. Kurib-
ayashi, and K. Kanzaki. 2009. Enhancing the
japanese wordnet. In Proceedings of the 7th Workshop
on Asian Language Resources, pages 1?8. Association
for Computational Linguistics.
H. De?jean, E?. Gaussier, and F. Sadat. 2002. An approach
based on multilingual thesauri and model combination
for bilingual lexicon extraction. In Proceedings of the
International Conference on Computational Linguis-
tics, pages 1?7. International Committee on Computa-
tional Linguistics.
S. Evert. 2004. The statistics of word cooccurrences:
word pairs and collocations. Doctoral dissertation, In-
stitut fu?r maschinelle Sprachverarbeitung, Universita?t
Stuttgart.
C. Fellbaum. 1998. Wordnet: an electronic lexical
database. Cambrige, MIT Press, Language, Speech,
and Communication.
P. Fung. 1998. A statistical view on bilingual lexicon ex-
traction: from parallel corpora to non-parallel corpora.
Lecture Notes in Computer Science, 1529:1?17.
G. Grefenstette. 1994. Explorations in automatic the-
saurus discovery. Springer.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein.
2008. Learning bilingual lexicons from monolingual
corpora. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics, pages
771?779. Association for Computational Linguistics.
A. Ismail and S. Manandhar. 2010. Bilingual lexicon
extraction from comparable corpora using in-domain
terms. In Proceedings of the International Conference
on Computational Linguistics, pages 481 ? 489.
659
S.R. Jammalamadaka and A. Sengupta. 2001. Topics in
circular statistics, volume 5. World Scientific Pub Co
Inc.
P. Koehn and K. Knight. 2000. Estimating word trans-
lation probabilities from unrelated monolingual cor-
pora using the em algorithm. In Proceedings of the
National Conference on Artificial Intelligence, pages
711?715. Association for the Advancement of Artifi-
cial Intelligence.
T. Kudo, K. Yamamoto, and Y. Matsumoto. 2004. Ap-
plying conditional random fields to Japanese morpho-
logical analysis. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 230?237. Association for Computational Lin-
guistics.
A. Laroche and P. Langlais. 2010. Revisiting context-
based projection methods for term-translation spotting
in comparable corpora. In Proceedings of the In-
ternational Conference on Computational Linguistics,
pages 617 ? 625.
N. Okazaki, Y. Tsuruoka, S. Ananiadou, and J. Tsujii.
2008. A discriminative candidate generator for string
transformations. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 447?456. Association for Computational Lin-
guistics.
V. Pekar, R. Mitkov, D. Blagoev, and A. Mulloni. 2006.
Finding translations for low-frequency words in com-
parable corpora. Machine Translation, 20(4):247?
266.
R. Rapp. 1999. Automatic identification of word transla-
tions from unrelated English and German corpora. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics, pages 519?526. Asso-
ciation for Computational Linguistics.
K. Sato, T. Ikeda, T. Nakata, and S. Osada. 2003. In-
troduction of a Japanese language processing mid-
dleware used for CRM. In Annual Meeting of the
Japanese Association for Natural Language Process-
ing (in Japanese), pages 109?112.
Y. Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, J. McNaught,
S. Ananiadou, and J. Tsujii. 2005. Developing a ro-
bust part-of-speech tagger for biomedical text. Lecture
Notes in Computer Science, 3746:382?392.
R.R. Wilcox. 2009. Basic Statistics: Understanding
Conventional Methods and Modern Insights. Oxford
University Press.
660
Learning the Optimal use of Dependency-parsing Information for Finding
Translations with Comparable Corpora
Daniel Andrade?, Takuya Matsuzaki?, Jun?ichi Tsujii?
?Department of Computer Science, University of Tokyo
{daniel.andrade, matuzaki}@is.s.u-tokyo.ac.jp
?Microsoft Research Asia, Beijing
jtsujii@microsoft.com
Abstract
Using comparable corpora to find new word
translations is a promising approach for ex-
tending bilingual dictionaries (semi-) auto-
matically. The basic idea is based on the
assumption that similar words have similar
contexts across languages. The context of
a word is often summarized by using the
bag-of-words in the sentence, or by using
the words which are in a certain dependency
position, e.g. the predecessors and succes-
sors. These different context positions are
then combined into one context vector and
compared across languages. However, previ-
ous research makes the (implicit) assumption
that these different context positions should be
weighted as equally important. Furthermore,
only the same context positions are compared
with each other, for example the successor po-
sition in Spanish is compared with the suc-
cessor position in English. However, this is
not necessarily always appropriate for lan-
guages like Japanese and English. To over-
come these limitations, we suggest to perform
a linear transformation of the context vec-
tors, which is defined by a matrix. We de-
fine the optimal transformation matrix by us-
ing a Bayesian probabilistic model, and show
that it is feasible to find an approximate solu-
tion using Markov chain Monte Carlo meth-
ods. Our experiments demonstrate that our
proposed method constantly improves transla-
tion accuracy.
1 Introduction
Using comparable corpora to automatically extend
bilingual dictionaries is becoming increasingly pop-
ular (Laroche and Langlais, 2010; Andrade et al,
2010; Ismail and Manandhar, 2010; Laws et al,
2010; Garera et al, 2009). The general idea is
based on the assumption that similar words have
similar contexts across languages. The context of
a word can be described by the sentence in which
it occurs (Laroche and Langlais, 2010) or a sur-
rounding word-window (Rapp, 1999; Haghighi et
al., 2008). A few previous studies, like (Garera et
al., 2009), suggested to use the predecessor and suc-
cessors from the dependency-parse tree, instead of a
word window. In (Andrade et al, 2011), we showed
that including dependency-parse tree context posi-
tions together with a sentence bag-of-words context
can improve word translation accuracy. However
previous works do not make an attempt to find an
optimal combination of these different context posi-
tions.
Our study tries to find an optimal weighting and
aggregation of these context positions by learning
a linear transformation of the context vectors. The
motivation is that different context positions might
be of different importance, e.g. the direct predeces-
sors and successors from the dependency tree might
be more important than the larger context from the
whole sentence. Another motivation is that depen-
dency positions cannot be always compared across
different languages, e.g. a word which tends to oc-
cur as a modifier in English, can tend to occur in
Japanese in a different dependency position.
As a solution, we propose to learn the optimal
combination of dependency and bag-of-words sen-
tence information. Our approach uses a linear trans-
formation of the context vectors, before comparing
10
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 10?18,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
them using the cosine similarity. This can be con-
sidered as a generalization of the cosine similarity.
We define the optimal transformation matrix by the
maximum-a-posterior (MAP) solution of a Bayesian
probabilistic model. The likelihood function for a
translation matrix is defined by considering the ex-
pected achieved translation accuracy. As a prior, we
use a Dirichlet distribution over the diagonal ele-
ments in the matrix and a uniform distribution over
its non-diagonal elements. We show that it is fea-
sible to find an approximation of the optimal so-
lution using Markov chain Monte Carlo (MCMC)
methods. In our experiments, we compare the pro-
posed method, which uses this approximation, with
the baseline method which uses the cosine similarity
without any linear transformation. Our experiments
show that the translation accuracy is constantly im-
proved by the proposed method.
In the next section, we briefly summarize the most
relevant previous work. In Section 3, we then ex-
plain the baseline method which is based on previ-
ous research. Section 4 explains in detail our pro-
posed method, followed by Section 5 which pro-
vides an empirical comparison to the baseline, and
analysis. We summarize our findings in Section 6.
2 Previous Work
Using comparable corpora to find new translations
was pioneered in (Rapp, 1999; Fung, 1998). The ba-
sic idea for finding a translation for a word q (query),
is to measure the context of q and then to compare
the context with each possible translation candidate,
using an existing dictionary. We will call words
for which we have a translation in the given dic-
tionary, pivot words. First, using the source cor-
pus, they calculate the degree of association of a
query word q with all pivot words. The degree of
association is a measure which is based on the co-
occurrence frequency of q and the pivot word in a
certain context position. A context (position) can be
a word-window (Rapp, 1999), sentence (Utsuro et
al., 2003), or a certain position in the dependency-
parse tree (Garera et al, 2009; Andrade et al, 2011).
In this way, they get a context vector for q, which
contains the degree of association to the pivot words
in different context positions. Using the target cor-
pus, they then calculate a context vector for each
possible translation candidate x, in the same way.
Finally, they compare the context vector of q with
the context vector of each candidate x, and retrieve
a ranked list of possible translation candidates. In
the next section, we explain the baseline which is
based on that previous research.
The general idea of learning an appropriate
method to compare high-dimensional vectors is not
new. Related research is often called ?metric-
learning?, see for example (Xing et al, 2003; Basu
et al, 2004). However, for our objective function it
is difficult to find an analytic solution. To our knowl-
edge, the idea of parameterizing the transformation
matrix, in the way we suggest in Section 4, and to
learn an approximate solution with a fast sampling
strategy is new.
3 Baseline
Our baseline measures the degree of association be-
tween the query word q and each pivot word with
respect to several context positions. As a context
position we consider the predecessors, successors,
siblings with respect to the dependency parse tree,
and the whole sentence (bag-of-words). The depen-
dency information which is used is also illustrated in
Figure 1. As a measure of the degree of association
we use the Log-odds-ratio as proposed in (Laroche
and Langlais, 2010).
Figure 1: Example of the dependency information used
by our approach. Here, from the perspective of ?door?.
Next, we define the context vector which contains
the degree of association between the query and each
pivot in several context positions. First, for each
11
context position i we define a vector qi which con-
tains the degree of association with each pivot word
in the context position i. If we number the pivot
words from 1 to n, then this vector can be writ-
ten as qi = (q1i , . . . , qni ). Note that in our case i
ranges from 1 to 4, representing the context posi-
tions predecessors (1), successors (2), siblings (3),
and the sentence bag-of-words (4). Finally, the com-
plete context vector for the query q is a long vector
q which appends each qi, i.e.: q = (q1, . . . ,q4).
Next, in the same way as before, we create a con-
text vector x for each translation candidate x in the
target language. For simplicity, we assume that each
pivot word in the source language has only one cor-
responding translation in the target language. As
a consequence, the dimensions of q and x are the
same. Finally we can score each translation candi-
date by using the cosine similarity between q and
x.
We claim that all of the context positions (1 to 4)
can contain information which is helpful to identify
translation candidates. However, we do not know
about their relative importance, neither do we know
whether these dependency positions can be com-
pared across language pairs as different as Japanese
and English. The cosine similarity simply weights
all dependency position equally important and ig-
nores problems which might occur when comparing
dependency positions across languages.
4 Proposed Method
Our proposed method tries to overcome the short-
comings of the cosine-similarity by using the fol-
lowing generalization:
sim(q,x) = qAx
T
?qAqT?xAxT , (1)
where A is a positive-definite matrix in Rdn?dn, and
T is the transpose of a vector. This can also be con-
sidered as linear transformation of the vectors using?A before using the normal cosine similarity, see
also (Basu et al, 2004).1
The challenge is to find an appropriate matrix A
which is expected to take the correlations between
1Therefore, exactly speaking A is not the transformation
matrix, however it defines uniquely the transformation matrix?
A.
the different dimensions into account, and which op-
timally weights the different dimensions. Note that,
if we set A to the identity matrix, we recover the
normal cosine similarity, which is our baseline.
Clearly, finding an optimal matrix in Rdn?dn is
infeasible due to the high dimensionality. We will
therefore restrict the structure of A.
Let I be the identity matrix in Rn?n , then we
define the matrix A, as follows:
A =
?
???
d1I z1,2I z1,3I z1,4I
z1,2I d2I z2,3I z2,4I
z1,3I z2,3I d3I z3,4I
z1,4I z2,4I z3,4I d4I
?
???
It is clear from this definition that d1, . . . , d4 weights
the context positions 1 to 4. Furthermore, zi,j can
be interpreted as a the confusion coefficient between
context position i and j. For example, a high value
for z2,3 means that a pivot word which occurs in
the sibling position in Japanese (source language),
might not necessarily occur in the sibling position in
English (target language), but instead in the succes-
sor position. However, in order to reduce the dimen-
sionality of the parameter space further, we assume
that each such zi,j has the same value z. Therefore,
matrix A becomes
A =
?
???
d1I zI zI zI
zI d2I zI zI
zI zI d3I zI
zI zI zI d4I
?
??? .
In the next subsection we will explain how we de-
fine an optimal solution for A.
4.1 Optimal solution for A
We use a Bayesian probabilistic model in order to
define the optimal solution for A. Formally we try
to find the maximum-a-posterior (MAP) solution of
A, i.e.:
argmax
A
p(A|data, ?). (2)
The posterior probability is defined by
p(A|data, ?) ? fauc(data|A) ? p(A|?) . (3)
fauc(data|A) is the (unnormalized) likelihood func-
tion. p(A|?) is the prior that captures our prior be-
liefs about A, and which is parameterized by a hy-
perparameter ?.
12
4.1.1 The likelihood function fauc(data|A)
As a likelihood function we use a modification
of the area under the curve (AUC) of the accuracy-
vs-rank graph. The accuracy-vs-rank graph shows
the translation accuracy at different ranks. data
refers to the part of the gold-standard which is used
for training. Our complete gold-standard contains
443 domain-specific Japanese nouns (query words).
Each Japanese noun in the gold standard corre-
sponds to one pair of the form <Japanese noun
(query), English translations (answers)>. We de-
note the accuracy at rank r, by accr. The accuracy
accr is determined by counting how often the cor-
rect answer is listed in the top r translation candi-
dates suggested for a query, divided by the number
of all queries in data. The likelihood function is
now defined as follows:
fauc(data|A) =
20?
r=1
accr ? (21 ? r) . (4)
That means fauc(data|A) accumulates the accura-
cies at the ranks from 1 to 20, where we weight ac-
curacies at top ranks higher.
4.1.2 The prior p(A|?)
The prior over the transformation matrix is factor-
ized in the following manner:
p(A|?) = p(z|d1, . . . , d4) ? p(d1, . . . , d4|?) .
The prior over the diagonal is defined as a Dirichlet
distribution:
p(d1, . . . , d4|?) = 1B(?)
4?
i=1
d??1i
where ? is the concentration parameter of the sym-
metric Dirichlet, and B(?) is the normalization con-
stant. The prior over the non-diagonal value a is de-
fined as:
p(z|d1, . . . , d4) = 1? ? 1[0,?](z) (5)
where ? = min{d1, . . . , d4}.
First, note that our prior limits the possible matri-
ces A to matrices which have diagonal entries which
are between 0 and 1. This is not a restriction since
the ranking of the translation candidates induced by
the parameterized cosine similarity will not change
if A is multiplied by a constant c > 0 . To see this,
note that
sim(q,x) = q(c ?A)x?q(c ?A)q?x(c ?A)x
= qAx?qAq?xAx .
Second, note that our prior limits A further, by re-
quiring, in Equation (5), that every non-diagonal el-
ement is smaller or equal than any diagonal element.
That requirement is sensible since we do not expect
that a optimal similarity measure between English
and Japanese will prefer context which is similar in
different dependency positions, over context which
is similar in the same context positions. To see this,
imagine the extreme case where for example d1 is 0,
and instead z12 is 1. In that case the similarity mea-
sure would ignore any similarity in the predecessor
position, but would instead compare the predeces-
sors in Japanese with the successors in English.
Finally, note that our prior puts probability mass
over a subset of the positive-definite matrices in
R4?4, and puts no probability mass on matrices
which are not positive-definite. As a consequence,
the similarity measure in Equation (1) is ensured to
be well-defined.
4.2 Training
In the following we explain how we use the training
data in order to find a good solution for the matrix
A.
4.2.1 Setting hyperparameter ?
Recall, that ? weights our prior belief about how
strong we think that the different context positions
should be weighted equally. From a practical point-
of-view, we do not know how strong we should
weight that prior belief. We therefore use empirical
Bayes to estimate ?, that is we use part of the train-
ing data to set ?. First, using half of the training
set, we find the A which maximizes p(A|data, ?)
for several ?. Then, the remaining half of the train-
ing set is used to evaluate fauc(data|A) to find the
best ?. Note that the prior p(A|?) can also be con-
sidered as a regularization to prevent overfitting. In
the next sub-section we will explain how to find an
approximation ofAwhich maximizes p(A|data, ?).
13
4.2.2 Finding a MAP solution for A
Recall that matrix A is defined by using only five
parameters. Since the problem is low-dimensional,
we can therefore expect to find a reasonable solution
using sampling methods. For finding an approxima-
tion of the maximum-a-posteriori (MAP) solution of
p(A|data, ?), we use the following Markov chain
Monte Carlo procedure:
1. Initialize d1, . . . , d4 and z.
2. Leave z constant, and run Simulated-
Annealing to find the d1, . . . , d4 which
maximize p(A|data, ?).
3. Given d1, . . . , d4, sample from the uniform dis-
tribution [1,min(d1, . . . d4)] in order to find the
z which maximizes p(A|data, ?).
The steps 2. and 3. are repeated till the convergence
of the parameters.
Concerning step 2., we use Simulated-
Annealing for finding a (local) maximum of
p(d1, . . . , d4|data, ?) with the following settings:
As a jumping distribution we use a Dirichlet distri-
bution which we update every 1000 iterations. The
cooling rate is set to 1iteration .
For step 2. and 3. it is of utmost importance to
be able to evaluate p(A|data, ?) fast. The com-
putationally expensive part of p(A|data, ?) is to
evaluate fauc(data|A). In order to quickly evalu-
ate fauc(data|A), we need to pre-calculate part of
sim(q, x) for all queries q and all translation can-
didates x. To illustrate the basic idea, consider
sim(q, x) without the normalization of q and xwith
respect to A, i.e.:
sim(q, x) = qAxT = (q1, . . . ,q4)A(x1, . . . ,x4)T .
Let us denote I?dn a block matrix in Rdn?dn whichcontains in each n ? n block the identity matrix ex-
cept in its diagonal; the diagonal of I?dn contains the
n ? n matrix which is zero in all entries. We can
now rewrite matrix A as:
A =
?
???
d1I 0 0 0
0 d2I 0 0
0 0 d3I 0
0 0 0 d4I
?
???+ z ? I?dn .
And finally we can factor out the parameters
(d1, . . . d4) and z in the following way:
sim(q, x) = (d1, . . . , d4)?
?
??
q1xT1...
q4xT4
?
??+z?(qI?dnxT )
By pre-calculating
?
??
q1xT1...
q4xT4
?
?? and qI?dnxT , we can
make the evaluation of each sample, in steps 2. and
3., computationally feasible.
5 Experiments
In the experiments of the present study, we used
a collection of complaints concerning automobiles
compiled by the Japanese Ministry of Land, Infras-
tructure, Transport and Tourism (MLIT)2 and an-
other collection of complaints concerning automo-
biles compiled by the USA National Highway Traf-
fic Safety Administration (NHTSA)3. Both corpora
are publicly available. The corpora are non-parallel,
but are comparable in terms of content. The part
of MLIT and NHTSA which we used for our ex-
periments, contains 24090 and 47613 sentences, re-
spectively. The Japanese MLIT corpus was mor-
phologically analyzed and dependency parsed using
Juman and KNP4. The English corpus NHTSA was
POS-tagged and stemmed with Stepp Tagger (Tsu-
ruoka et al, 2005; Okazaki et al, 2008) and depen-
dency parsed using the MST parser (McDonald et
al., 2005). Using the Japanese-English dictionary
JMDic5, we found 1796 content words in Japanese
which have a translation which is in the English cor-
pus. These content words and their translations cor-
respond to our pivot words in Japanese and English,
respectively.6
2http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html
3http://www-odi.nhtsa.dot.gov/downloads/index.cfm
4http://www-lab25.kuee.kyoto-u.ac.jp/nl-
resource/juman.html and http://www-lab25.kuee.kyoto-
u.ac.jp/nl-resource/knp.html
5http://www.csse.monash.edu.au/ jwb/edict doc.html
6Recall that we assume a one-to-one correspondence be-
tween a pivot in Japanese and English. If a Japanese pivot word
as more than one English translation, we select the translation
for which the relative frequency in the target corpus is closest
to the pivot in the source corpus.
14
5.1 Evaluation
For the evaluation we extract a gold-standard which
contains Japanese and English noun pairs that ac-
tually occur in both corpora.7 The gold-standard
is created with the help of the JMDic dictionary,
whereas we correct apparently inappropriate trans-
lations, and remove general nouns such as ???
(possibility) and ambiguous words such as? (rice,
America). In this way, we obtain a final list of 443
domain-specific Japanese nouns.
Each Japanese noun in the gold-standard corre-
sponds to one pair of the form <Japanese noun
(query), English translations (answers)>. We divide
the gold-standard into two halves. The first half is
used for for learning the matrix A, the second part
is used for the evaluation. In general, we expect that
the optimal transformation matrixA depends mainly
on the languages (Japanese and English) and on the
corpora (MLIT and NHTSA). However, in practice,
the optimal matrix can also vary depending on the
part of the gold-standard which is used for training.
These random variations are especially large, if the
part of the gold-standard which is used for training
or testing is small.
In order to take these random effects into ac-
count, we perform repeated subsampling of the
gold-standard. In detail, we randomly split the gold-
standard into equally-sized training and test set. This
is repeated five times, leading to five training and
five test sets. The performance on each test set is
shown in Table 1. OPTIMIZED-ALL marks the re-
sult of our proposed method, where matrix A is opti-
mized using the training set. The optimization of the
diagonal elements d1, . . . , d4, and the non-diagonal
value z is as described in Section 4.2. Finally, the
baseline method, as described in 3, corresponds to
OPTIMIZED-ALL where d1, . . . , d4 are set to 1,
and z is set to 0. This baseline is denoted as NOR-
MAL. We can see that the overall translation accu-
racy varies across the test sets. However, we see that
in all test sets our proposed method OPTIMIZED-
ALL performs better than the baseline NORMAL.
7Note that if the current query (Japanese noun) is a pivot
word, then the word is not considered as a pivot word.
5.2 Analysis
In the previous section, we showed that the cosine-
similarity is sub-optimal for comparing context vec-
tors which contain information from different con-
text positions. We showed that it is possible to find
an approximation of a matrix A which optimally
weights, and combines the different context posi-
tions. Recall, that the matrix A is described by the
parameters d1 . . . d4 and z, which can interpreted as
context position weights and a confusion coefficient,
respectively. Therefore, by looking at these parame-
ters which we learned using each training set, we can
get some interesting insights. Table 2 shows theses
parameters learned for each training set.
We can see that the parameters, across the train-
ing sets, are not as stable as we wish. For example
the weight for the predecessor position ranges from
0.27 to 0.44. As a consequence, the average values,
shown in the last row of Table 2, have to be inter-
preted with care. We expect that the variance is due
to the limited size of the training set, 220 <query,
answers> pairs.
Nevertheless, we can draw some conclusions with
confidence. For example, we see that the prede-
cessor and successor positions are the most impor-
tant contexts, since the weights for both are al-
ways higher than for the other context positions.
Furthermore, we clearly see that the sibling and
sentence (bag-of-words) contexts, although not as
highly weighted as the former two, can be consid-
ered to be relevant, since each has a weight of around
0.20. Finally, we see that z, the confusion coeffi-
cient, is around 0.03, which is small.8 Therefore,
we verify z?s usefulness with another experiment.
We additionally define the method OPTIMIZED-
DIAG which uses the same matrix as OPTIMIZED-
ALL except that the confusion coefficient z is set
to zero. In Table 1, we can see that the accu-
racy of OPTIMIZED-DIAG is constantly lower than
OPTIMIZED-ALL.
Furthermore, we are interested in the role of the
whole sentence (bag-of-words) information which is
in the context vector (in position d4 of the block vec-
tor). Therefore, we excluded the sentence informa-
8In other words, z is around 17% of its maximal possible
value. The maximal possible value is around 0.18, since, recall
that z is, by definition, smaller or equal to min{d1 . . . d4}.
15
Test Set Method Top-1 Top-5 Top-10 Top-15 Top-20Accuracy Accuracy Accuracy Accuracy Accuracy
1
OPTIMIZED-ALL 0.20 0.37 0.47 0.50 0.54
OPTIMIZED-DIAG 0.20 0.34 0.43 0.48 0.51
NORMAL 0.18 0.32 0.43 0.47 0.50
2
OPTIMIZED-ALL 0.20 0.35 0.43 0.48 0.52
OPTIMIZED-DIAG 0.19 0.33 0.42 0.46 0.52
NORMAL 0.18 0.34 0.42 0.47 0.49
3
OPTIMIZED-ALL 0.17 0.31 0.37 0.44 0.48
OPTIMIZED-DIAG 0.17 0.27 0.36 0.41 0.45
NORMAL 0.16 0.27 0.36 0.41 0.44
4
OPTIMIZED-ALL 0.14 0.30 0.38 0.43 0.46
OPTIMIZED-DIAG 0.14 0.26 0.34 0.4 0.43
NORMAL 0.15 0.29 0.37 0.41 0.44
5
OPTIMIZED-ALL 0.18 0.34 0.42 0.46 0.51
OPTIMIZED-DIAG 0.17 0.30 0.38 0.43 0.48
NORMAL 0.19 0.31 0.40 0.44 0.48
average
OPTIMIZED-ALL 0.18 0.33 0.41 0.46 0.50
OPTIMIZED-DIAG 0.17 0.30 0.39 0.44 0.48
NORMAL 0.17 0.31 0.40 0.44 0.47
Table 1: Shows the accuracy at different ranks for all test sets, and, in the last column, the average over all test sets.
The proposed method OPTIMIZED-ALL is compared to the baseline NORMAL. Furthermore, for analysis, the results
when optimizing only the diagonal are marked as OPTIMIZED-DIAG.
Training Set d1 d2 d3 d4 zpredecessor successor sibling sentence confusion coefficient
1 0.35 0.26 0.19 0.20 0.03
2 0.27 0.29 0.21 0.23 0.03
3 0.35 0.31 0.16 0.18 0.02
4 0.44 0.24 0.17 0.16 0.04
5 0.39 0.28 0.20 0.13 0.03
average 0.36 0.28 0.19 0.18 0.03
Table 2: Shows the parameters which were learned using each training set. d1 . . . d4 are the weights of the context
positions, which sum up to 1. z marks the degree to which it is useful to compare context across different positions.
tion from the context vector. The accuracy results,
averaged over the same test sets as before, are shown
in Table 3. We can see that the accuracies are clearly
lower than before (compare to Table 1). This clearly
justifies to include additionally sentence information
into the context vector. It is also interesting to note
that the average z value is now 0.14.9 This is consid-
erable higher than before, and shows that a bag-of-
words model can partly make the use of z redundant.
However, note that the sentence bag-of-words model
covers a broader context, beyond the direct prede-
cessors, successor and siblings, which explains why
9That is 48% of its maximal possible value. Since for the
dependency positions predecessor, successor and sibling we get
the average weights 0.38, 0.33 and 0.29, respectively.
a small z value is still relevant in the situation where
we include sentence bag-of-words into the context
vector.
Finally, to see why it can be helpful to compare
different dependency positions from the context vec-
tors of Japanese and English, we looked at concrete
examples. We found, for example, that the trans-
lation accuracy of the query word ???? (disc)
improved when using OPTIMIZED-ALL instead of
OPTIMIZED-DIAG. The pivot word ?? (wrap)
tends together with both the Japanese query ??
?? (disc), and with the correct translation ?disc?
in English. However, that pivot word occurs in
Japanese and English in different context positions.
In the Japanese corpus ?? (wrap) tends to occur
16
Method Top-1 Top-5 Top-10 Top-15 Top-20
OPT-DEP 0.13 0.25 0.34 0.38 0.41
NOR-DEP 0.12 0.23 0.29 0.33 0.38
Table 3: The proposed method, but without the sentence
information in the context vector, is denoted OPT-DEP.
The baseline method, but without the sentence informa-
tion in the context vector, is denoted NOR-DEP.
together with the query???? (disc) in sentences
like for example the following:
????? (break)???? (disc)???
(wrap)???? (occured)??
That Japanese sentence can be literally translated as
?A wrap occured in the brake disc.?, where ?wrap?
is the sibling of ?disc? in the dependency tree. How-
ever, in English, considered out of the perspective
of ?disc?, the pivot word ?wrap? tends to occur in a
different dependency position. For example, the fol-
lowing sentence can be found in the English corpus:
?Front disc wraps.?
In English ?wrap? tends to occur as a successor of
?disc?. A non-zero confusion coefficient allows us
to account some degree of similarity to situations
where the query (here ??????(disc)) and the
translation candidate (here ?disc?) tend to occur with
the same pivot word (here ?wrap?), but in different
dependency positions.
6 Conclusions
Finding new translations of single words using com-
parable corpora is a promising method, for exam-
ple, to assist the creation and extension of bilin-
gual dictionaries. The basic idea is to first create
context vectors of the query word, and all the can-
didate translations, and then, in the second step,
to compare these context vectors. Previous work
(Laroche and Langlais, 2010; Fung, 1998; Garera
et al, 2009) suggests that for this task the cosine-
similarity is a good choice to compare context vec-
tors. For example, Garera et al (2009) include the
information of various context positions from the
dependency-parse tree in one context vector, and, af-
terwards, compares these context vectors using the
cosine-similarity. However, this makes the implicit
assumption that all context positions are equally im-
portant, and, furthermore, that context from differ-
ent context positions does not need to be compared
with each other. To overcome these limitations, we
suggested to use a generalization of the cosine simi-
larity which performs a linear transformation of the
context vectors, before applying the cosine similar-
ity. The linear transformation can be described by a
positive-definite matrix A. We defined the optimal
matrix A by using a Bayesian probabilistic model.
We demonstrated that it is feasible to approximate
the optimal matrix A by using MCMC-methods.
Our experimental results suggest that it is bene-
ficial to weight context positions individually. For
example, we found that predecessor and successor
should be stronger weighted than sibling, and sen-
tence information. Whereas, the latter two are also
important, having a total weight of around 40%.
Furthermore, we showed that for languages as dif-
ferent as Japanese and English it can be helpful to
compare also different context positions across both
languages. The proposed method constantly outper-
formed the baseline method. Top 1 accuracy in-
creased by up to 2% percent points and Top 20 by
up to 4% percent points.
For future work, we consider to use different pa-
rameterizations of the matrix A which could lead to
even higher improvement in accuracy. Furthermore,
we consider to include, and weight additional fea-
tures like transliteration similarity.
Acknowledgment
We would like to thank the anonymous reviewers
for their helpful comments. This work was partially
supported by Grant-in-Aid for Specially Promoted
Research (MEXT, Japan). The first author is sup-
ported by the MEXT Scholarship and by an IBM
PhD Scholarship Award.
References
D. Andrade, T. Nasukawa, and J. Tsujii. 2010. Robust
measurement and comparison of context similarity for
finding translation pairs. In Proceedings of the In-
ternational Conference on Computational Linguistics,
pages 19?27.
D. Andrade, T. Matsuzaki, and J. Tsujii. 2011. Effec-
tive use of dependency structure for bilingual lexicon
17
creation. In Proceedings of the International Confer-
ence on Computational Linguistics and Intelligent Text
Processing, Lecture Notes in Computer Science, pages
80?92. Springer Verlag.
S. Basu, M. Bilenko, and R.J. Mooney. 2004. A prob-
abilistic framework for semi-supervised clustering. In
Proceedings of the ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 59?68.
P. Fung. 1998. A statistical view on bilingual lexicon ex-
traction: from parallel corpora to non-parallel corpora.
Lecture Notes in Computer Science, 1529:1?17.
N. Garera, C. Callison-Burch, and D. Yarowsky. 2009.
Improving translation lexicon induction from mono-
lingual corpora via dependency contexts and part-of-
speech equivalences. In Proceedings of the Confer-
ence on Computational Natural Language Learning,
pages 129?137. Association for Computational Lin-
guistics.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein.
2008. Learning bilingual lexicons from monolingual
corpora. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics, pages
771?779. Association for Computational Linguistics.
A. Ismail and S. Manandhar. 2010. Bilingual lexicon
extraction from comparable corpora using in-domain
terms. In Proceedings of the International Conference
on Computational Linguistics, pages 481 ? 489.
A. Laroche and P. Langlais. 2010. Revisiting context-
based projection methods for term-translation spotting
in comparable corpora. In Proceedings of the In-
ternational Conference on Computational Linguistics,
pages 617 ? 625.
F. Laws, L. Michelbacher, B. Dorow, C. Scheible,
U. Heid, and H. Schu?tze. 2010. A linguistically
grounded graph model for bilingual lexicon extrac-
tion. In Proceedings of the International Conference
on Computational Linguistics, pages 614?622. Inter-
national Committee on Computational Linguistics.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics, pages 91?98. Association
for Computational Linguistics.
N. Okazaki, Y. Tsuruoka, S. Ananiadou, and J. Tsujii.
2008. A discriminative candidate generator for string
transformations. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 447?456. Association for Computational Lin-
guistics.
R. Rapp. 1999. Automatic identification of word transla-
tions from unrelated English and German corpora. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics, pages 519?526. Asso-
ciation for Computational Linguistics.
Y. Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, J. McNaught,
S. Ananiadou, and J. Tsujii. 2005. Developing a ro-
bust part-of-speech tagger for biomedical text. Lecture
Notes in Computer Science, 3746:382?392.
T. Utsuro, T. Horiuchi, K. Hino, T. Hamamoto, and
T. Nakayama. 2003. Effect of cross-language IR
in bilingual lexicon acquisition from comparable cor-
pora. In Proceedings of the conference on European
chapter of the Association for Computational Linguis-
tics, pages 355?362. Association for Computational
Linguistics.
E.P. Xing, A.Y. Ng, M.I. Jordan, and S. Russell. 2003.
Distance metric learning with application to clustering
with side-information. Advances in Neural Informa-
tion Processing Systems, pages 521?528.
18
