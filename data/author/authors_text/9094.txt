Document Re-ranking Based on Automatically Acquired  
Key Terms in Chinese Information Retrieval 
Yang Lingpeng, Ji Donghong, Tang Li 
Institute for Infocomm Research 
21, Heng Mui Keng Terrace 
Singapore, 119613 
{lpyang,dhji,tangli}@i2r.a-star.edu.sg 
 
Abstract 
For Information Retrieval, users are more 
concerned about the precision of top ranking 
documents in most practical situations. In this 
paper, we propose a method to improve the 
precision of top N ranking documents by 
reordering the retrieved documents from the 
initial retrieval. To reorder documents, we first 
automatically extract Global Key Terms from 
document set, then use extracted Global Key 
Terms to identify Local Key Terms in a single 
document or query topic, finally we make use 
of Local Key Terms in query and documents to 
reorder the initial ranking documents. The 
experiment with NTCIR3 CLIR dataset shows 
that an average 10%-11% improvement and 
2%-5% improvement in precision can be 
achieved at top 10 and 100 ranking documents 
level respectively.  
1 Introduction 
Information retrieval (IR) is used to retrieve 
relevant documents from a large document set 
for a given query where the query is a simple 
description by natural language. In most 
practical situations, users concern more on the 
precision of top ranking documents than recall 
because users want to acquire relevant 
information from the top ranking documents.   
Traditionally, IR system uses a one-stage or 
a two-stage mechanism to retrieve relevant 
documents from document set. For one stage 
mechanism, IR system only does an initial 
retrieval. For two-stage mechanism, besides 
the initial retrieval, IR system will make use of 
the initial ranking documents to automatically 
do query expansion to form a new query and 
then use the new query to retrieve again to get 
the final ranking documents. The effectiveness 
of query expansion mainly depends on the 
precision of top N (N<50) ranking documents 
in initial retrieval because almost all proposed 
automatic query expansion algorithms make 
use of the information in the top N retrieved. 
Figure 1 demonstrates the general processes of 
a two-stage IR system. 
In this paper, we propose a method to 
improve the precision of top N ranking 
documents by reordering the initially retrieved 
documents in the initial retrieval. To reorder 
documents, we first automatically extract 
Global Key Terms from the document set, then 
use the extracted Global Key Terms to identify 
Local Key Terms in a single document or query 
topic, finally we make use of the Local Key 
Terms in queries and documents to reorder the 
initial ranking documents.  
Although our method is general and can 
apply to any languages, in this paper we?ll only 
focus on the research on Chinese IR system. 
 
F i g .  1  T r a d i t i o n a l  P r o c e s s  o f  t w o - s t a g e s  I R  
O r i g i n a l  Q u e r y  
E x p a n d e d   Q u e r y  
I n i t i a l  R e t r i e v a l  
F i n a l  R e t r i e v a l  
Q u e r y   E x p a n s i o n  
D o c u m e n t  
S e t  
I n i t i a l  R a n k i n g  
D o c u m e n t s  
F i n a l  R a n k i n g  
D o c u m e n t s  
 
 The rest of this paper is organized as 
following. In section 2, we give an overall 
introduction of our proposed method.  In 
section 3, we talk about what are Global Key 
Terms and what are Local Key Terms and how 
to acquire them. In section 4, we describe how 
these terms apply to Chinese IR system to 
improve the precision and quality of IR 
system. In section 5, we evaluate the 
performance of our proposed method and give 
some result analysis. In section 6, we present 
the conclusion and some future work. 
2 Overview of Document Reordering 
in Chinese IR 
For Chinese IR, many retrieval models, 
indexing strategies and query expansion 
strategies have been studied and successfully 
used in IR. Chinese Character, bi-gram, n-
gram (n>2) and word are the most used 
indexing units. (Li. P. 1999) gives out many 
research results on the effectiveness of single 
Chinese Character as   indexing unit and how 
to improve the effectiveness of single Chinese 
Character as indexing unit. (K.L. Kwok. 1997) 
compares three kinds of indexing units (single 
Character, bigram and short-words) and their 
effectiveness. It reports that single character 
indexing is good but not sufficiently 
competitive, while bi-gram indexing works 
surprisingly well and it?s as good as short-
word indexing in precision. (J.Y. Nie, J. Gao, 
J. Zhang and M. Zhou. 2000) suggests that 
word indexing and bi-gram indexing can 
achieve comparable performance but if we 
consider the time and space factors, then it is 
preferable to use words (and characters) as 
indexes. It also suggests that a combination of 
the longest-matching algorithm with single 
character is a good method for Chinese and if 
there is unknown word detection, the 
performance can be further improved. Many 
other papers in literature (Palmer, D. and 
Burger, J, 1997; Chien, L.F, 1995) give similar 
conclusions. Although there are still different 
voices on if bi-gram or word is the best 
indexing unit, bi-gram and word are both 
considered as the most important top two 
indexing units in Chinese IR and they are used 
in many reported Chinese IR systems and 
experiences.   
There are mainly two kinds of retrieval 
models: Vector Space Model (G. Salton and 
M. McGill, 1983) and Probabilistic Retrieval 
(N. Fuhr, 1992). They are both used in a lot of 
experiments and applications. 
For query expansion, almost all of the 
proposed strategies make use of the top N 
documents in initial ranking documents in the 
initial retrieval. Generally, query expansion 
strategy selects M indexing units (M<50) from 
the top N (N<25) documents in initial ranking 
documents according to some kind of measure 
and add these M indexing units to original 
query to form a new query. In such process of 
query expansion, it?s supposed that the top N 
documents are related with original query, but 
in practice, such an assumption is not always 
true. The Okapi approach (S.E. Roberson and 
S.Walker, 2001) supposes that the top R 
documents are related with query and it selects 
N indexing unit from the top R documents to 
form a new query, for example, R=10 and 
N=25. (M. Mitra., Amit. S. and Chris. B, 1998) 
did an experiment on different query topics 
and it is reported the effectiveness of query 
expansion mainly depends on the precision of 
the top N ranking documents. If the top N 
ranking documents are highly related with the 
original query, then query expansion can 
improve the final result. But if the top N 
documents are less related with the original 
query, query expansion cannot improve the 
final result or even reduces the precision of 
final result. These researches conclude that 
whether query expansion is successful or not 
mainly depends on the quality of top N ranking 
documents in the initial retrieval. 
The precision of top N documents in the 
initial ranking documents depends on indexing 
unit and retrieval models and mainly depends 
on indexing unit. As discussed above, bi-gram 
and word both are the most effective indexing 
units in Chinese IR.  
Other effort has been done to improve the 
precision of top N documents. (Qu. Y, 2002) 
proposed a method to re-rank initial relevant 
documents by using individual thesaurus but 
the thesaurus must be constructed manually 
and depends on each query topic.  
In this paper, we propose a new method to 
improve the precision of top N ranking 
documents in initial ranking documents by 
reordering the top M (M > N and M < 1000) 
ranking documents in initially retrieved 
documents. To reorder documents, we try to 
find long terms (more than 2 Chinese 
characters) that generally represent some 
complete concepts in query and documents, 
then we make use of these long terms to re-
weight the top M documents in initial ranking 
documents and reorder them by re-weighted 
value. We adopt a two-stage approach to 
acquire such kinds of long terms. Firstly, we 
acquire Global Key Terms from the whole 
document set; secondly, we use Global Key 
Terms to acquire Local Key Terms in a query 
or a document. After we have acquired Local 
Key Terms, we use them to re-weight the top M 
documents in initial ranking documents. Figure 
2 demonstrates the processes of an IR system 
that integrates with this new method.  
 
F i g .  2  E n h a n c e d  P r o c e s s  o f  I R  
O r i g i n a l  Q u e r y  
E x p a n d e d  Q u e r y  
I n i t i a l  R e t r i e v a l  
F i n a l  R e t r i e v a l  
Q u e r y  E x p a n s i o n  
D o c u m e n t  
S e t  
I n i t i a l  R a n k i n g  
D o c u m e n t s  
F i n a l  R a n k i n g  
D o c u m e n t s  
D o c u m e n t   
R e - O r d e r  
K e y  T e r m   
E x t r a c t i o n  
K e y  T e r m s  R e - O r d e r e d  
D o c u m e n t s  
 
3 Global/Local Key Term Extraction 
The Global /Local Key Term extraction 
concerns the problem of what is a key term. 
Intuitively, key terms in a document are some 
conceptual terms that are prominent in 
document and play main roles in 
discriminating the document from other 
documents. In other words, a key term in a 
document can represent part of the content of 
the document. Generally, from the point of the 
view of conventional linguistic studies, Key 
Terms may be some NPs, NP-Phrases or some 
kind of VPs, adjectives that can represent some 
specific concepts in document content 
representation.  
We define two kinds of Key Terms: Global 
Key Terms which are acquired from the whole 
document set and Local Key Terms which are 
acquired from a single document or a query.  
We adopt a two-stage approach to 
automatically acquire Global Key Terms and 
Local Key Terms. In the first stage, we acquire 
Global Key Terms from document set by using 
a seeding-and-expansion method. In the second 
stage, we make use of acquired Global Key 
Terms to find Local Key Terms in a single 
document or a query. 
3.1  Global Key Terms  
Global Key Terms are terms which are 
extracted from the whole document set and 
they can be regarded to represent the main 
concepts of document set.  
Although the definition of Global Key 
Terms is difficult, we try to give some 
assumptions about a Global Key Term. Before 
we give these assumptions, we first give out 
the definition of Seed and Key Term in a 
document (or document cluster) d. 
The concept Seed is given to reflect the 
prominence of a Chinese Character in a 
document (or document cluster) in some way. 
Suppose r is the reference document set 
(reference document set including document 
set and other statistical large document 
collection), d is a document (or a document 
set), w is an individual Chinese Character in d, 
let Pr(w) and Pd(w) be the probability of w 
occurring in r and d respectively, we adopt 1), 
relative probability or salience of w in d with 
respect to r (Schutze. 1998), as the criteria for 
evaluation of Seed.  
1) Pd(w) / Pr(w) 
We call w a Seed if Pd(w) / Pr(w)?? (?>1).  
   Now we give out the assumptions about a 
Key Terms in document d. 
 
i) a Key Term contains at least a Seed. 
ii) a Key Term occurs at least N (N>1) times in d. 
iii) the length of a Key Term is less than L (L<30). 
iv) a maximal character string meeting i), ii) and iii) is a 
Key Term. 
v) for a Key Term, a real maximal substring meeting i), 
ii) and iiI) without considering their occurrence in all 
those Key Terms containing it is also a Key Terms.  
 
Here a maximal character string meeting i), ii) 
and iii) refers to a adjacent Chinese character 
string meeting i), ii) and iii) while no other 
longer Chinese character strings containing it 
meet i), ii) and iii). A real maximal substring 
meeting i), ii) and iii) refers to a real substring 
meeting i), ii), and iii) while no other longer 
real substrings containing it meet i), ii) and iii). 
We use a kind of seeding-and-expansion-
based statistical strategy to acquire Key Terms 
in document (or document cluster), in which 
we first identify seeds for a Key Term then 
expand from it to get the whole Key Term. 
Fig. 3 describes the procedure to extract 
Key Terms from a document (or document 
cluster) d. 
 
let Fd(t) represents the frequency of t in d; 
let N is a given threshold (N>1); 
K = {}; 
collect Seeds in d into S; 
for all c?S 
{ 
    let Q = {t: t contains c and Fd(t)?N}; 
   while Q ? NIL 
   { 
    max-t  ? the longest string in Q; 
    K ? K + { max-t }; 
    Remove max-t  from Q; 
   for all other t in Q  
   {  
        if t is a substring of max-t  
       {    Fd(t)? Fd(t)- Fd(max-t); 
    if Fd(t)<N 
       removing t from Q; 
    } 
   } 
 } 
} 
return K as Key Terms in document d; 
 
Fig. 3 Key Term Extraction from document d
 
To acquire Global Key Terms, we first 
roughly cluster the whole document set r into 
K (K<2000) document clusters, then we regard 
each document cluster as a large document and  
apply our proposed Key Term Extraction 
algorithm (see Fig. 3) on each document 
cluster and respectively get Key Terms in each 
document cluster. All these Key Terms from 
document clusters form Global Key Terms.  
There are many document clustering 
approaches to cluster document set. K-Means 
and hierarchical clustering are the two usually 
used approaches. In our algorithm, we don?t 
need to use complicated clustering approaches 
because we only need to roughly cluster 
document set r into K document clusters. Here 
we use a simple K-Means approach to cluster 
document set. Firstly, we pick up randomly 
10*K documents from document set r; 
secondly, we use K-Means approach to cluster 
these 10*K documents into K document 
clusters; finally, we insert every other 
document into one of the K document clusters. 
Fig. 4 describes the general process to cluster 
document set r into K document clusters.  
 
let K is the number of documnet clusters to get;  
T?10*K documents randomly pickuped from r;
  
    
cluster T into K clusters {Kj} by using K-Means; 
for any document d in {r-T} 
{ 
    Ki? document cluster which has the maximal 
similarity with d; 
  insert d to document cluster Ki; 
} 
return K document clusters {Kj|1<=j<=K}; 
 
Fig. 4 Cluster document set r into K clusters 
 
Fig. 5 describes the procedure to acquire Global Key 
Terms from document set r. 
 
roughly cluster document set r to K document clusters 
{Kj|1<=j<=K} (See Fig. 4); 
 G = {}; 
 for each Kj  
 
{ 
     extract Key Terms g from Kj ; (See Fig. 3) 
     G ? G + g; 
} 
return G as Global Key Terms in document set r; 
 
Fig. 5 Global Key Terms Acquisition 
 
 In the processing of Global Key Terms 
acquisition, the frequency of each Global Key 
Term is also recorded for further use in 
identifying Local Key Terms - terms in a single 
document or query.  
 
3.2 Local Key Terms 
Unlike Global Key Terms, Local Key Terms 
are not extracted by using Key Term extraction 
algorithm from single document or query, they 
are identified based on Global Key Terms and 
their frequencies.  
Fig.6 describes the procedure of Local Key 
Terms acquisition from a single document or 
query d. 
 
  Given threshold M (M>10), N (N>100) and document d; 
L = {}; 
collect Global Key Terms occurred in d and their 
frequency in document set r into S = <c, tf>; 
for all <c, tf>?S 
{ 
      if  tf  < M  
             remove <c, tf> from S; 
  }; 
for all <c, tf>?S 
{ 
      if  c = c1c2  and  <c1, tf1>?S and <c2, tf2>?S 
 if (tf1 > tf *N  and tf2 >> tf*N)  
                     remove <c, tf> from S; 
  }; 
  while S ? NIL 
  { 
     let Q = {<t, tf>: t is the longest string is S}; 
        find <max-c,max-tf>
 
in Q where max-tf has the 
maximum value; 
  remove <max-t, max-tf>  from S; 
  if max-t occurs in d 
 {  L ? L + max-t; 
     remove all occurrance of max_t in d; 
    for all <b, tf-b>?S where b is a substring of  max-t;  
          if tf-b < max-tf  remove <b,tf-b>  from S; 
 } 
 }; 
 return L as Local Key Terms in document d; 
 
Fig. 6 Local Key Terms Acquisition 
 
Following are some examples of Global Key 
Terms and Local Key Terms in a query. 
 
Example: 
 
Query:  	

 
  
(Find information of the exhibition "Art and Culture of 
the Han Dynasty" in the National Palace Museum) 
Global Key Terms occurred in Query and their 
frequencies in document set: 
 
 (Cha2 Xun2)? 4948 

 (Gu4 Gong1)? 3456 
(Gu4 Gong1 Bo2 Wu4 Yuan4)? 727 
(Bo2 Wu4 Yuan4) ? 772 
 (Yuan4 Suo3) ? 2991 
	 (Zhu3 Ban4) ? 38698 
 (Qian1 Xi3)? 11510 
 (Han4 Dai4) ? 411 
 (Han4 Dai4 Wen3 Wu4) - 173 
 (Han4 Dai4 Wen3 Wu4 Da4 Zhan3) ? 
133 
 (Wen3 Wu4) ? 7088 
 (Wen3 Wu4 Da4 Zhan3) ? 158 
 (Da4 Zhan3) ? 2270 
 (Xiang3 Guan3) ? 67990 
  (Xiang3 Guan3 Nei3 Rong2) ? 148 
 (Nei3 Rong2) ? 31165 
Local Key Terms in Query: 
 (Han4 Dai4 Wen3 Wu4 Da4 Zhan3) 
 (Han4 Dai4 Wen3 Wu4) 
 (Wen3 Wu4) 
 (Da4 Zhan3) 
(Gu4 Gong1 Bo2 Wu4 Yuan4)  
(Bo2 Wu4 Yuan4) 
(Gu4 Gong1) 
 (Xiang3 Guan3) 
 (Nei3 Rong2) 
	 (Zhu3 Ban4) 
 (Qian1 Xi3) 
  (Cha2 Xun2) 
 
From the example, we can see the difference 
between Global Key Terms and Local Key 
Terms. For example,  (Yuan4 Suo3)  and
'RFXPHQW5HUDQNLQJ%DVHGRQ*OREDODQG/RFDO7HUPV
<DQJ/LQJSHQJ,QVWLWXWHIRU,QIRFRPP5HVHDUFK+HQJ0XL.HQJ7HUUDFH6LQJDSRUHOS\DQJ#LUDVWDUHGXVJ
-L'RQJKRQJ7DQJ/L,QVWLWXWHIRU,QIRFRPP5HVHDUFK+HQJ0XL.HQJ7HUUDFH6LQJDSRUH^GKMLWDQJOL`#LUDVWDUHGXVJ
$EVWUDFW
,Q WKLV SDSHUZH SURSRVH DPHWKRG WR LPSURYHWKHSUHFLVLRQRI WRS1 GRFXPHQWVE\ UHRUGHULQJWKH UHWULHYHG GRFXPHQWV LQ WKH LQLWLDO UHWULHYDO7RUHRUGHUWKHGRFXPHQWVZHILUVWDXWRPDWLFDOO\H[WUDFWJOREDONH\WHUPVIURPGRFXPHQWVHWWKHQXVH WKHVH WHUPV DQG WKHLU IUHTXHQF\ WR LGHQWLI\ORFDO NH\ WHUPV LQ D VLQJOH TXHU\ RU GRFXPHQWILQDOO\ZHPDNHXVHRIORFDONH\WHUPVWRUHRUGHUWKH LQLWLDOO\ UHWULHYHG GRFXPHQWV ,Q RXUH[SHULPHQWVEDVHGRQ17&,5&/,5GDWDVHWDQDYHUDJHLPSURYHPHQWFDQEHPDGHIRUWRS  GRFXPHQWV DQG DQ DYHUDJH LPSURYHPHQW FDQ EH PDGH IRU WRS GRFXPHQWV
 ,QWURGXFWLRQ
7R ILQG RXW ZKDW ZH UHDOO\ ZDQW IURP D ODUJHGRFXPHQW VHW LV D KHDGDFKH ,QIRUPDWLRQ UHWULHYDO,5LVXVHGWRUHWULHYHUHOHYDQWGRFXPHQWVIURPDODUJH GRFXPHQW VHW IRU D JLYHQ TXHU\ ZKHUH WKHTXHU\ LVD VLPSOHGHVFULSWLRQE\QDWXUDO ODQJXDJH,QPRVWSUDFWLFDOVLWXDWLRQVXVHUVFRQFHUQPXFKRQWKHSUHFLVLRQRIWRSUDQNLQJGRFXPHQWVWKDQUHFDOOEHFDXVHXVHUVZDQWWRDFTXLUHUHOHYDQWLQIRUPDWLRQIURP WKH WRS UDQNLQJ GRFXPHQWV WR VDYH WKHLUYDOXDEOHWLPH7UDGLWLRQDOO\ ,5 V\VWHP XVHV D RQHVWDJH RU DWZRVWDJH PHFKDQLVP WR UHWULHYH UHOHYDQWGRFXPHQWV IURP GRFXPHQW VHW )RU RQH VWDJHPHFKDQLVP ,5 V\VWHP RQO\ GRHV DQ LQLWLDOUHWULHYDO )RU WZRVWDJH PHFKDQLVP H[FHSW WKHLQLWLDO UHWULHYDO ,5 V\VWHP ZLOO PDNH XVH RI WKHLQLWLDO UDQNLQJ GRFXPHQWV WR DXWRPDWLFDOO\ GRTXHU\H[SDQVLRQWRIRUPDQHZTXHU\DQGWKHQXVHWKH QHZ TXHU\ WR UHWULHYH DJDLQ WR JHW WKH ILQDOUDQNLQJ GRFXPHQWV 7KH HIIHFWLYHQHVV RI TXHU\H[SDQVLRQPDLQO\GHSHQGVRQ WKHSUHFLVLRQRI WRS5 5 UDQNLQJ GRFXPHQWV LQ LQLWLDO UHWULHYDOEHFDXVH DOPRVW DOO SURSRVHG DXWRPDWLF TXHU\H[SDQVLRQDOJRULWKPVPDNHXVHRIWKHLQIRUPDWLRQLQ WKH WRS 5 GRFXPHQWV LQ LQLWLDO UDQNLQJGRFXPHQWV )LJXUH  GHPRQVWUDWHV WKH JHQHUDOSURFHVVHVRIDWZRVWDJH,5V\VWHP
7R LPSURYH WKH SUHFLVLRQ RI WRS 1 UDQNLQJGRFXPHQWV LQ LQLWLDO UHWULHYDO PDQ\ UHVHDUFKHVKDYH EHHQ GRQH RQ WKH UHWULHYDO PRGDOV DQGLQGH[LQJXQLWV
)LJ7UDGLWLRQDO3URFHVVRIWZRVWDJHV,5
2ULJLQDO4XHU\
([SDQGHG4XHU\
,QLWLDO5HWULHYDO
)LQDO5HWULHYDO
4XHU\([SDQVLRQ
'RFXPHQW6HW
,QLWLDO5DQNLQJ'RFXPHQWV
)LQDO5DQNLQJ'RFXPHQWV
 ,Q WKLV SDSHU ZH SURSRVH D PHWKRG WRLPSURYHWKHSUHFLVLRQRIWRS1UDQNLQJGRFXPHQWVLQ WKH LQLWLDO UHWULHYDO E\ UHRUGHULQJ WKH LQLWLDOUDQNLQJ GRFXPHQWV LQ WKH LQLWLDO UHWULHYDO 7RUHRUGHU GRFXPHQWV ZH ILUVW DXWRPDWLFDOO\ H[WUDFWJOREDO NH\ WHUPV IURP GRFXPHQW VHW WKHQ XVHH[WUDFWHGJOREDONH\WHUPVDQGWKHLUIUHTXHQFLHVWRLGHQWLI\ ORFDO NH\ WHUPV LQ D VLQJOH GRFXPHQW RUTXHU\WRSLFILQDOO\ZHPDNHXVHRIORFDONH\WHUPVLQ TXHU\ DQG GRFXPHQWV WR UHRUGHU WKH LQLWLDOUDQNLQJ GRFXPHQWV%\ GRLQJ VR RXUPHWKRG FDQLPSURYH WKH SUHFLVLRQ RI WRS GRFXPHQWV LQ LQLWLDOUDQNLQJ GRFXPHQWV DQG KHOS WR LPSURYH WKHHIIHFWLYHQHVVRITXHU\H[SDQVLRQ$OWKRXJKRXUPHWKRG LV JHQHUDODQGFDQDSSO\WRDQ\ODQJXDJHVLQWKLVSDSHUZH?OORQO\IRFXVRQWKHUHVHDUFKRQ&KLQHVH,5V\VWHP
7KHUHVWRIWKLVSDSHULVRUJDQL]HGDVIROORZLQJ,QVHFWLRQZHJLYHDQRYHUDOOLQWURGXFWLRQRIRXUSURSRVHGPHWKRG  ,Q VHFWLRQZHGHVFULEHZKDWDUHJOREDONH\WHUPVDQGZKDWDUHORFDONH\WHUPVDQGKRZWRDFTXLUHWKHP,QVHFWLRQZHGHVFULEHKRZJOREDONH\WHUPVDQGORFDONH\WHUPVDSSO\WR&KLQHVH ,5 V\VWHP WR LPSURYH WKH SUHFLVLRQ DQGTXDOLW\RI,5V\VWHP,QVHFWLRQZHHYDOXDWHWKHSHUIRUPDQFH RI RXU SURSRVHG PHWKRG DQG JLYHVRPH UHVXOW DQDO\VLV ,Q VHFWLRQ ZH SUHVHQW WKHFRQFOXVLRQDQGVRPHIXWXUHZRUN
 2YHUYLHZ RI 'RFXPHQW 5HRUGHULQJ LQ&KLQHVH,5
)RU &KLQHVH ,5 PDQ\ UHWULHYDO PRGHOV LQGH[LQJVWUDWHJLHVDQGTXHU\H[SDQVLRQVWUDWHJLHVKDYHEHHQVWXGLHG DQG VXFFHVVIXOO\ XVHG LQ ,5 &KLQHVH&KDUDFWHUELJUDPQJUDPQ!DQGZRUGDUHWKHPRVWXVHG LQGH[LQJXQLWV /L3JLYHVRXWPDQ\UHVHDUFKUHVXOWVRQWKHHIIHFWLYHQHVVRIVLQJOH&KLQHVH &KDUDFWHU DV   LQGH[LQJ XQLW DQG KRZ WRLPSURYH WKH HIIHFWLYHQHVV RI VLQJOH &KLQHVH&KDUDFWHU DV LQGH[LQJ XQLW ./ .ZRN A Large-Scale Semantic Structure for Chinese Sentences 
Tang Li 
Institutue for Infocomm Research  
21 Heng Mui Keng Terrace 
Singapore119613 
Tangli@I2R.a-star.edu.sg 
Ji Donghong, Yang Lingpeng 
Institutue for Infocomm Research  
21 Heng Mui Keng Terrace 
Singapore119613 
{dhji, lpyang}@I2R.a-star.edu.sg 
 
Abstract 
Motivated by a systematic analysis of 
Chinese semantic relationships, we 
constructed a Chinese semantic framework 
based on surface syntactic relationships, deep 
semantic relationships and feature structure to 
express dependencies between lexical 
meanings and conceptual structures, and 
relations that underlie those lexical meanings. 
Analyzing the semantic representations of 
10000 Chinese sentences, we provide a model 
of semantically and syntactically annotated 
sentences from which reliable information on 
combinatorial possibilities of each semantic 
item targeted for analysis can be displayed. 
We also propose a semantic argument ? head 
relation, ?basic conceptual structure? and the 
?Head-Driven Principle?. Our results show that 
we can successfully disambiguate some 
troublesome sentences, and minimize the 
redundancy in language knowledge 
descriptions for natural language processing.  
1 Introduction 
To enable computer-based analysis of Chinese 
sentences in natural language texts we have 
developed a semantic framework, taking into 
account concepts used in the Berkeley FrameNet 
Project (Baker, Fillmore, & Lowe 1998; Fillmore 
& Baker 2001) and the Penn Chinese Tree Bank 
(Nianwen Xue; Fei Xia et al 2000). The FrameNet 
Project, as a computational project, is creating a 
lexical resource for English, based on the principle 
of  semantic frames. It has tried to concentrate on 
frames which help to explain the meanings of 
groups of words, rather than frames that cover just 
one word. The representation of the valences of its 
target words and descriptions of the semantic 
frames underlying the meanings of the words 
described are the mainly part of the database. The 
Penn Chinese Tree Bank analyzed the syntactic 
structure of a phrase or sentence for  selected text, 
based on the current research in Chinese syntax 
and the linguistic expertise of those involved in 
this project. Different from Pan?s syntactic 
structures and FrameNet?s semantic frames, our 
object is to record exactly how the semantic 
features relates frames to those syntactic 
constituents. The key task is to determine the 
relationship between the two direct constituents in 
terms of the semantic relationship. The grammar 
functions are also considered for primarily 
identifying the relation. Here, we use methods 
developed for the analysis of semantic 
relationships to produce a framework based on the 
direct component link. Our framework is largely a 
semantic one, but it has adopted some crucial 
principles of syntactic analysis in the semantic 
structure analysis. 
In this paper, we present our model of 
semantically and synactically annotated 10000 
Chinese sentences. The focus is on the analysis of 
the semantic relationships between one word to 
another in a sentence. We also briefly discuss the 
annotation process. 
2 Theoretical Framework and Case Study 
The basic assumption of Frame Semantics 
(Fillmore 1976;1977; Fillmore & Atkins 1992; 
Petruck 1996) as it applies to the description of 
lexical meanings is that each word (in a given 
meaning) evokes a particular frame and possibly 
profiles some element or aspect of that frame. By 
being linked to frames, each word is directly 
connected with other words in its frame(s). where 
word dependence association are needed from 
surface syntactic structures which actually reflect 
the grammatical relationship to the deep semantics 
structure whereby semantic content are put into 
natural language. The meaning of a word, in most 
cases, is best demonstrated by reference to a 
semantic network. Referential meaning on its own 
is insufficient. Word meaning would include the 
other dimensions concerning the structure and 
function of words. Unlike English, in which there 
are two major types of evidence that help to 
determine the syntactic structure of a phrase or 
sentence: morphological information and 
distributional information (such as word order) ,  in 
Chinese the lack of conclusive morphological cues 
makes ambiguity analyses for one sentence more 
likely. Moreover, most Chinese sentences order are 
very flexible. Phrase omission, word  movement, 
ellipsis and binding also make it difficult to 
characterize their grammatical relation. So the 
semantic information provides important clues for 
Chinese sentence analyse. We have to rely on 
semantic knowledge to guide role assignment. 
Thus,  we propose a method allowing a syntactic 
and semantic-based analysis of sequences and 
relationship of semantic items to obtain the 
common distribution of the relationship order. 
3 Method 
The analysis method that will be presented here 
is logically equivalent to the parsing of syntax and 
semantic dependency  with feature constraints. 
The key idea in our method is to avoid the 
complexity hierarchical tree sturcture. We are 
concerned with building structures that reflect  
basic relationships between one word and other in 
a single sentence. We use methods developed for 
the analysis of semantic relationships to produce a 
framework based on the order link. We started 
from an initial analysis based on the surface 
syntactics, then we analyzed deep semantic 
relationships, and attempted to improve it by 
removing the semantic order from the syntactic 
structure and reconnecting them in different places. 
Since many word phrase patterns are difficult for 
computers to recognize, trying to compromise 
between linguistic correctness and engineering 
convenience, we link the difference semantic roles 
on the flat level, while employing a few template 
rules. All semantic words are linked on the same 
level. They are non-hierarchical constructs. This 
flatted representation allows access to various 
levels of syntactic description tree simultaneously. 
In fact, the purpose of generalization is to get a 
regular expression from the original sentence.  
We manually tagged two kind of relationship 
among our large-scale frameworks: 1. syntax-
semantic relationship; 2. semantic feature 
relationship. 
Our framework consists of a set of nodes and a 
set of arcs that join the nodes, with each word or 
concept corresponding to a node and links between 
any two nodes that are directly associated. The 
basic links in the framework are between one word 
item to another based on immediate semantic 
deperdency order. We summarized the immediate 
semantic relationship through a variety of semantic 
relation features such as agent, reason, result and 
so on. The feature of relationship between two 
nodes are labeled on the arc.  
We developed the first fully instantiated 
semantic structure by manually labeling semantic 
representations in a machine-readable format. To 
make sure that our model can deal with various 
kinds of texts in real life situations, we have 
analysed 10000 sentences from large Web site 
corpora based on our formal model. Our aim is not 
to describe in detail any specific, but to capture at 
an abstract level the semantic relations between the 
direct components in a sentence. Our model?s most 
important domain of application is to Chinese 
sentence analysis, but it may also be applicable to 
different languages. This semantic framwork 
constructs a model on the basis of a few rules. 
The present paper indicates how situation types 
are represented, how these representations are 
composed from semantic representations of 
linguistic constituents, and how these type 
differences affect the expression of sentences. 
3.1 Syntax-Semantics Relationship Labeling 
This work flow includes linking and labeling of 
each relation between direct semantic items in 
single sentences, which reflects different semantic 
representation, and descriptions of the relations of 
each frame?s basic conceptual structure in terms of 
semantic actions. A semantic representation is a 
feature that allows one word in the sentence to 
point at some other word to which it is related. A 
word in a sentence may have much direct 
representation, these are differentiated by the 
semantic action. By analyzing the direct se mantic 
representation, we can capture semantic 
relationships between words, reconstructing a  
framework for the order of Chinese sentences. 
In most cases syntactic relationships are 
consistent with semantic relationships. The 
following framework shows show some important 
similarities between the structure of syntactic and 
semantic structure. For example, in 
     ?????. ??I am watching TV.?? 
Syntactically,  ?? ?? I? is subject, directly 
relating to the verbal predicate ????watch?, ??
? ??TV? is object , also links to the verbal 
predicate directly. ????be doing?as a adverb is 
an adjoined predicate ????watch?, there is a 
direct relationship between the two nodes.  
Semantically,  ????I?is the agent and ????
?TV?is the recipients, both of them have a direct 
relationship with the activity ????watch?. So 
we link the different nodes as follows: 
 
In cases where the relationship between syntax 
and semantics is inconsistent, by syntactic analysis, 
if there are multiple syntactic analyses among a 
sentence, we always choose the analysis 
 3.2 ?Head? Determination relationship that is consistent with the semantic 
relationship. For example, the Chinese sentence The basic link is the direct link between two 
semantic units. In addition, a set of general rules 
for determining the directions has been identified. 
???????? 
?many people sit beside the street.? 
The above sentence can be analyzed either of the 
following two syntactic structures.  
1. That between  Head and Its Modifier as a 
Case of Direct Relationship 
type 1: The head (see below), and the modifiers that 
come before it, constitute a type of modification 
relationship, which is one of the typical cases of 
direct relationships, e.g,  
 A. Gao zige de ren 
type 2:      tall  body DE person 
 
     the person with tall body 
B.   (to be compared with the above sentence) 
      ren de gezi gao 
      person DE body tall The two syntactic structures are analyzed with 
difference in the first node and the second node. In 
type 1, ?????beside of the street?is analyzed 
as subject, for type2, the linguist also analyzed it as 
adverb modifier, adjuncting to the predicate ???
?seat?. But when this sentence is analyzed in 
terms of semantics, there is only one relationship 
structure similar as type 2. ?? ?? people? is 
analyzed as agent,  ?????beside of the street?
as localizer, attached to the activity ????seat?. 
This semantic structure is consistent with the 
syntactic structure type 2. Only one structure can 
display both syntax and semantic relationship 
simultaneously. So we choose the second analysis. 
      ?The person?s body is tall.? 
In the above sentence, ren ?person? and gezi 
?body? hold a modification relationship, but gao 
?tall? and ren?person? are related indirectly as the 
relationship between the two words is realized 
through that of gezi ?body?. Therefore, we say that 
the relationship that ren ?person? holds with gezi 
?body? is a direct one, but that with gao is a rather 
indirect one. 
2. That between An Action Verb and Its Patient 
as a Case of a Direct relationship 
In case a head noun is an AGENT of an action 
verb within a modifying phrase, then the 
relationship between the Head none and the action 
verb is a direct one. The following sentences 
illustrate the point. 
If the syntactic relationship is different from the 
semantic relationship, we take no account of the 
syntactic order. In the Chinese sentence  C. chi pingguo de nuhai. 
     Eat apples DE   girl ??????? 
    ?the girl who is eating apples.?     ?she cry so much that her eyes become red.? 
D. (to be compared with the above sentence) Within the surface syntactic structure, adjective 
??? ?red?will be analyzed as  complementation 
and directly associated with main verb  ?? ? 
? cry?  , which indicate  result of predicate. 
Underlying the syntactic structure, ??? ?red?
actually point to ??? ?? eyes? in semantic 
representation. There is no direct semantic 
relationship between   ?? ? ? cry? and ?? ? 
?red?. The semantic network can be analyzed as: 
she cry + her eyes become red, the immediate 
relationship between ?he? as a possessor and 
?belly? as a possession and that between ?belly? as 
entity and ?painful? as description. In this case we 
link the node ?? ? ? red? to ??? ??eyes?
directly based on semantic relationship. 
      nuhai chi pingguo 
      girl    eat apples 
     ?The girl is eating apples.? 
In the above sentence, nuhai ?girl? is an AGENT 
of the action verb chi ?eat?, the two words have a 
direct semantic relationship, therefore we link 
them directly and annotate ?girl? as a head. In 
contrary, the relationship between nuhai ?girl? and 
pingguo ?apples? is of an indirect type. 
3. Other Cases of Direct Relationships 
In case there is neither a modification nor an 
AGENT/PATIENT relationship, the whole phrase, 
which is still directly related to a following 
describing phrase, has to be embedded. E.g.,  
E. ban shiqing yinggai guquan daju. 
Handle problem should care-about overall  
 
situation 
 ?People should care about the overall situation  
when they handle problems.? 
F. chouyan hai shenti.  
    Smoke   harm health 
    ?Smoking harms health.? 
 
G. ta neng daying de shiqing wo ye neng daying. 
     He can accept DE event I also can accept 
The above three head semantic structures clearly 
show us the different relationships among sentence 
and noun phrases with different meaning. The head 
words are connected  to  their modifier through 
arrow arcs. The first SVO relationship is also 
represented by non-head tagging.  
  ?The event that he can accept are also 
acceptable to me.? 
3.3 ?Head? Determination 
Since Chinese lacks morphological cues, the  
grammatical markers (such as ?????) and 
word order are comparatively important cues for 
the relationship determination. We have to rely on 
grammatical and semantic knowledge to guide role 
assignment. 
3.4 Feature Abstracting and Labeling 
Based on the analysis of semantic relationships, 
we have been parsing feature structures to express 
dependencies between semantic features. In our 
analysis model, semantic feature means a variety 
of  detailed semantic relationships. Most of the 
time, semantic features are not so easy to define. 
Some feature typologies have been provided, but 
there is still much discussions about the nature of a 
re in a text. To avoid the confusion of feature 
ification, we p d a method bstract 
the semantic featur tly from se s that 
contain the natura
sentences without s
labeling the sema
categorys include in
the relationship ar
semantic framewo
dimension. For exam
In this study, we have proposed an approach that 
combines ?basic conceptual structure? and our 
?Head-Driven Principle?.  According to the ?Head-
Driven Principle?, most structures are analyzed as 
having a ?Head? which is connected to various 
types of modifiers, such as Head-NP (adjective-
noun, noun-adverbial pairs ???  ), Head-VP 
(adverbial-verb, verb-adverbial, adjective-verb?). 
In our framework, modification is represented by 
attaching tags with arrows to the core semantic 
item whereve the type of modification can be 
clearly identified. Since the SVO is the basic order 
in Chinese, there is no modifier relationship among 
the level of SVO. In our model, ?Subject-Predicate 
Structures? and ?Verb-Object Structures? are 
represented as non-head. In above example, the 
relation linking the ?core? noun and verb with their 
?adjunct? is tagged with an arrow to indicate that it 
is a ?head?. Both A and B label the ?head? as the 
core noun. E labels the ?head? as  the core verb. 
Employing the ?Head-Driven Principle? for the 
construction of semantic models. Some ambiguous 
sentences can be clearly represented. The different 
meaning among sentence or phrase  containing 
same words can also be described . Conside the 
following sentence and phrases: 
       Ta  gezi   bu g
       His stature isn
       He isn?t tall. 
 
              
 
 
 
 
 
statu
he
In traditional anal
constituent in a sen
meaning of the sent
is semantic feature l
thus in our semantic
?tall? semantically, 
marking a semanti
immediate constitu
structure, after featu
to its English co
translation from one
??????? 
?The students like the teachers.? 
??????? 
(the students who like the teachers) 
??????? 
(the teachers who the students like) 
All of above examples containing same meaning 
words can have very different meaning, depending 
on the different word order and grammatical 
marker ?? ??DE? . We use head tagging to 
construct different frameworks for these structures:  In some sentence
features but also th
Similarly we abstrac
features. Thus we ca
to express this level 
    
 
Ta  liang   mi     
he   two   metersropose
e direc
l feature word.
emantic features i
ntic features re
 other sentences,
cs. Thus we co
rk based on 
ple: 
ao. 
?t tall. 
re 
not 
ysis, ?stature? is ju
tence. However, 
ence is ?he is not t
inking ?he? and ?t
 analysis we link o
?stature? is take
c relationship, ra
ent. This Chine
re abstraction, is 
unterpart. It fa
 language into ano
s , there are not o
eir particular valu
ted the values atta
n expand the featu
of detail. For exam
  gao. 
  tall   to a
ntencefeatu
class For those 
nsert , we?ll 
fer to the 
 attached on 
nstructed a 
multi-profile 
tall
st a syntactic 
the essential 
all?, ?stature? 
all? together, 
nly ?he? and 
n as feature 
ther than an 
se semantic 
very similar 
cilitates the 
ther. 
nly semantic 
es included. 
ched on  the 
re structures 
ple 
?He is two meters tall.?  
 Several different sentences which should be 
analyzed as having the same syntactic structure 
may have fundamentally different semantic 
structures. The following three sentences S1, 
S2and S3, for example, should be analyzed as 
having the syntactic structure, but their semantic 
structures are nevertheless represented as S1?, S2? 
and S3? respectively in our framework. 
 
 
 
 
he tall
two 
meters 
In the above framework, ?tall? is the semantic 
feature describing staturs of the agent ?he? , and 
?two meters? express the value of the feature. They 
provid different information at different level, 
constructing a feature sturcture.  NP + V + Adj + NP   
  S1?Ta xiao-tong le duzi 4 The Advantages of Our Semantic Model     
           he laugh-painful ASP belly 
In developing our semantic frameworks, we also 
have articulated a framework of ?Noun-Centrality? 
as a supplement to the widely assumed ?Verb-
Centrality? practice. We can successfully 
disambiguate some troublesome sentences, and 
minimize the redundancy in language knowledge 
description for natural language processing. We 
automatically learn a simpler, less redundant 
representation of the same information.  
          ?He laughed so much that his belly was 
painful.? 
  S2?Wo kan-tou le ni 
         I  see- through ASP you 
         ?I understand you thoroughly.? 
    S3?Ta da po-le beizi 
            She broke up the cup First, comparing syntactic order and semantic 
order, we used the reconstructed original order, 
giving some different order sentences similar 
results. Thus, variations of order in the same 
sentence can reveal the same relationships. 
            She broke up the cup. 
                                                    
S1?? NP         V        Adj     NP                  One semantic structure may correspond to more 
syntactic structures in Chinese, and this 
correspondence can be made specifically clear 
using our approach.  
 
1.Ta da-le 
wo 
 She beat me 
 ?She beat 
me.? 
2.Ta ba wo da-
le 
She BA me beat  
?She beat me.? 
      
3.Wo BEI Ta da-
le 
I BEI she beat 
?I have been 
beaten by her.? 
The above three sentences, their syntactic 
structures are clearly different from each other. 
That is, the direct object wo ?me? appears right 
after the main verb in (1) whereas the same logical 
object has moved to a pre-verbal position with the 
help of a special Chinese preposition BA in (2) and 
to a sentence-initial position with the help of BEI 
in (3). But underlying the difference syntactic 
structures, they share the same basic semantic 
structure, using semantic represented expression, 
the three sentences of above example can be 
described in below. 
AGENT Ta   ?she? 
PATIENT Wo?me? 
ACTION Da  ?beat? 
 
 
S2??NP            V       Adj     NP                  
 
                                                                                                  
 
S3??NP         V   Adj        NP 
                                        
 
On the other hand, many structural ambiguities 
in Chinese sentences are one of the major problems 
in Chinese syntactic analyses. One syntactic 
structure may correspond to two or more semantic 
structures, that is, various forms of structural 
ambiguity are widely observed in Chinese. 
Disregarding the semantic types will cause 
syntactic ambiguity. If this type of information is 
not available during parsing, important clues will 
be missing, and loss of accuracy will result. 
Consider the Chinese sentence 
Ta de yifu zuo de piaoliang. 
   Her cloth do DE beautiful 
 Reading 1:  ?She has made the cloth beautifully 
b) minimal redundancy in language knowledge 
description for natural language processing. 
Reading 2: (Somebody) has made her cloth 
beautifully.? 
We hope to use the minimum analysis method to 
find the semantic order with equal relationship 
among new sentence. We then used the partition 
relationship as a training database to recognize 
new order as similar as these order structures. 
Syntactically, the sentence, with either one of 
the above two semantic interpretations, should be 
analyzed as 
                          S 
We also have been creating feature sets parsing 
feature structures to expressing dependencies 
between semantic features. Furthermore, we 
abstracted the values attached to the features. Thus 
we can expand the feature structures to express this 
level of detail. 
                      /         \ 
               NP               VP 
            /        \            /      \ 
      NP           N      V         Adj (Complement) 
       |               |        |              | 
References  
      Ta de     yifu    zuo  de  piaoliang 
Baker C, Fillmore C, Lower J 1998 The 
Berkeley                Her    cloth      make DE beautiful 
   FrameNet Project, In Proc. of ACL/COLING 
1998.  
But the two semantic structures have to be 
properly represented in a semantics-oriented 
framework. We do so as in type A and type B 
respectively. 
Daniel Gildea and Daniel Jurafsky 2002 Auto 
matic Labeling of Semantic Roles. In Proc. of 
ACL 2000. 
Type A:   Ta de yifu zuo de piaoliang.                Nianwen Xue, Fei Xia 2000 The Bracketing 
Guidelines for the Pann Chinese Treebank, IRCS 
Report 00-08 University of Pennsylvania, Oct 
2000 
                 Her cloth do DE beautiful         
            
Dominique Dutoit, Thierry Poibeau 2002 Inferring 
Knowledge from a Large Semantic Network, In 
Proc. of COLING 2000 
 Type B: Ta     de yifu zuo de piaoliang. James Henderson, Paola Merlo, Ivan Petroff  2002 
Using Syntactic Analysis to Increase Efficiency 
in Visualizing Text Collections, In Proc. of 
ACL/COLING 2002. 
               Her cloth do DE beautiful 
    
 
So under our proposal, the above two different 
types of semantic relations can be clearly 
represented.. 
5 Conclusion 
In this paper we have demonstrated how our 
semantic model can be created to analyze and 
represent the semantic relationships of Chinese 
sentence structures. The semantic model project is 
producing a structured tree bank with a richer set 
of semantic and syntactic relationships of different 
words on the basis of the analysis of lexical 
meanings and conceptual structures that underlie 
those lexical meanings. We developed some 
methods for determining the relationship between 
direct semantic items based on the analysis of 
syntactic and semantic order. The key advantages 
of our semantic model are:  
a) many ambiguous sentences can be clearly 
represented. 
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 154?157,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese word segmentation and named entity recognition based 
on a context-dependent Mutual Information Independence Model 
 
 
Zhang Min    Zhou GuoDong    Yang LingPeng    Ji DongHong 
 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore, 119613 
Email: (mzhang, zhougd, lpyang, dhji)@i2r.a-star.edu.sg 
 
Abstract  
This paper briefly describes our system in the 
third SIGHAN bakeoff on Chinese word 
segmentation and named entity recognition. 
This is done via a word chunking strategy 
using a context-dependent Mutual 
Information Independence Model. 
Evaluation shows that our system performs 
well on all the word segmentation closed 
tracks and achieves very good scalability 
across different corpora. It also shows that 
the use of the same strategy in named entity 
recognition shows promising performance 
given the fact that we only spend less than 
three days in total on extending the system in 
word segmentation to incorporate named 
entity recognition, including training and 
formal testing. 
1  Introduction 
Word segmentation and named entity recognition 
aim at recognizing the implicit word boundaries 
and proper nouns, such as names of persons, 
locations and organizations, respectively in plain 
Chinese text, and are critical in Chinese 
information processing.  However, there exist two 
problems when developing a practical word 
segmentation or named entity recognition system 
for large open applications, i.e. the resolution of 
ambiguous segmentations and the identification 
of OOV words or OOV entity names.  
In order to resolve above problems, we 
developed a purely statistical Chinese word 
segmentation system and a named entity 
recognition system using a three-stage strategy 
under an unified framework.  
The first stage is called known word 
segmentation, which aims to segment an input 
sequence of Chinese characters into a sequence of 
known words (called word atoms in this paper). In 
this paper, all Chinese characters are regarded as 
known words and a word unigram model is 
applied to perform this task for efficiency.  Also, 
for convenience, all the English characters are 
transformed into the Chinese counterparts in 
preprocessing, which will be recovered just 
before outputting results. 
The second stage is the word and/or named 
entity identification and classification on the 
sequence of atomic words in the first step. Here, a 
word chunking strategy is applied to detect words 
and/or entity names by chunking one or more 
atomic words together according to the word 
formation patterns of the word atoms and optional 
entity name formation patterns for named entity 
recognition. The problem of word segmentation 
and/or entity name recognition are re-cast as 
chunking one or more word atoms together to 
form a new word and/or entity name, and a 
discriminative Markov model, named Mutual 
Information Independence Model (MIIM), is 
adopted in chunking. Besides, a SVM plus 
sigmoid model is applied to integrate various 
types of contexts and implement the 
discriminative modeling in MIIM. 
The third step is post processing, which tries 
to further resolve ambiguous segmentations and 
unknown word segmentation. Due to time limit, 
this is only done in Chinese word segmentation. 
No post processing is done on Chinese named 
entity recognition. 
The rest of this paper is as follows: Section 2 
describes the context-dependent Mutual 
Information Independence Model in details while 
purely statistical post-processing in Chinese word 
segmentation is presented in Section 3. Finally, 
we report the results of our system in Chinese 
word segmentation and named entity recognition 
in Section 4 and conclude our work in Section 5. 
154
2 Mutual Information Independence 
Model  
In this paper, we use a discriminative Markov 
model, called Mutual Information Independence 
Model (MIIM) as proposed by Zhou et al(2002), 
for Chinese word segmentation and named entity 
recognition. MIIM is derived from a conditional 
probability model. Given an observation sequence 
n
n oooO L211 = , MIIM finds a stochastic optimal 
state(tag) sequence nn sssS L211 =  that 
maximizes: 
??
==
? +=
n
i
n
i
n
i
i
i
nn OsPSsPMIOSP
1
1
2
1
111 )|(log),()|(log  
We call the above model the Mutual 
Information Independence Model due to its 
Pair-wise Mutual Information (PMI) assumption 
(Zhou et al2002). The above model consists of 
two sub-models: the state transition model 
?
=
?n
i
i
i SsPMI
2
1
1 ),( , which can be computed by 
applying ngram modeling, and the output model 
?
=
n
i
n
i OsP
1
1 )|(log , which can be estimated by any 
probability-based classifier, such as a maximum 
entropy classifier or a SVM plus sigmoid 
classifier (Zhou et al2006).  In this competition, 
the SVM plus sigmoid classifier is used in 
Chinese word segmentation while a simple 
backoff  approach as described in Zhou et al
(2002) is used in named entity recognition. 
Here, a variant of the Viterbi algorithm 
(Viterbi 1967) in decoding the standard Hidden 
Markov Model (HMM) (Rabiner 1989) is 
implemented to find the most likely state 
sequence by replacing the state transition model 
and the output model of the standard HMM with 
the state transition model and the output model of 
the MIIM, respectively. The above MIIM has 
been successfully applied in many applications, 
such as text chunking (Zhou 2004), Chinese word 
segmentation ( Zhou 2005), English named entity 
recognition in the newswire domain (Zhou et al
2002) and the biomedical domain (Zhou et al
2004; Zhou et al2006). 
For Chinese word segmentation and named 
entity recognition by chunking, a word or a entity 
name is regarded as a chunk of one or more word 
atoms and we have: 
? >=< iii wpo , ; iw is the thi ?  word atom in 
the sequence of word atoms nn wwwW L211 = ; 
ip  is the word formation pattern of the word 
atom iw . Here ip  measures the word 
formation power of the word atom iw  and 
consists of: 
o The percentage of iw  occurring as a whole 
word (round to 10%) 
o The percentage of iw  occurring at the 
beginning of other words (round to 10%) 
o The percentage of iw  occurring at the end 
of other words (round to 10%) 
o The length of iw  
o Especially for named entity recognition, 
the percentages of a word occurring in 
different entity types (round to 10%). 
? is : the states are used to bracket and 
differentiate various types of words and 
optional entity types for named entity 
recognition. In this way, Chinese word 
segmentation and named entity recognition 
can be regarded as a bracketing and 
classification process. is  is structural and 
consists of two parts: 
o Boundary category (B): it includes four 
values: {O, B, M, E}, where O means that 
current word atom is a whOle word or 
entity name and B/M/E means that current 
word atom is at the Beginning/in the 
Middle/at the End of a word or entity name. 
o Unit category (W): It is used to denote the 
type of the word or entity name.  
Because of the limited number of boundary 
and unit categories, the current word atom 
formation pattern ip  described above is added 
into the state transition model in MIIM. This 
makes the above MIIM context dependent as 
follows: 
??
==
?
? +=
n
i
n
i
n
i
ii
i
i
nn
OsPppSsPMI
OSP
1
1
2
1
1
1
11
)|(log)|,(
)|(log
 
3 Post Processing in Word 
Segmentation 
The third step is post processing, which tries to 
resolve ambiguous segmentations and false 
unknown word generation raised in the second 
step. Due to time limit, this is only done in 
Chinese word segmentation, i.e. no post 
processing is done on Chinese named entity 
recognition. 
155
A simple pattern-based method is employed to 
capture context information to correct the 
segmentation errors generated in the second steps. 
The pattern is designed as follows: 
<Ambiguous Entry (AE)> | <Left Context, 
Right Context> => <Proper Segmentation> 
The ambiguity entry (AE) means ambiguous 
segmentations or forced-generated unknown 
words. We use the 1st and 2nd words before AE as 
the left context and the 1st and 2nd words after AE 
as the right context. To reduce sparseness, we also 
only use the 1st left and right words as context. 
This means that there are two patterns generated 
for the same context. All the patterns are 
automatically learned from training corpus using 
the following algorithm. 
 
LearningPatterns() 
// Input: training corpus 
// Output: patterns 
BEGIN 
(1) Training a MIIM model using training 
corpus 
(2) Using the MIIM model to segment training 
corpus 
(3) Aligning the training corpus with the 
segmented training corpus 
(4) Extracting error segmentations 
(5) Generating disambiguation patterns using 
the left and right context 
(6) Removing the conflicting entries if two 
patterns have the same left hand side but 
different right hand side. 
END 
 
4 Evaluation 
We first develop our system using the PKU data 
released in the Second SIGHAN Bakeoff last 
year. Then, we train and evaluate it on the Third 
SIGHAN Bakeoff corpora without any 
fine-tuning. We only carry out our evaluation on 
the closed tracks. It means that we do not use any 
additional knowledge beyond the training corpus. 
Precision (P), Recall (R), F-measure (F), OOV 
Recall and IV Recall are adopted to measure the 
performance of word segmentation. Accuracy 
(A), Precision (P), Recall (R) and F-measure (F) 
are adopted to measure the performance of NER. 
Tables 1, 2 and 3 in the next page report the 
performance of our algorithm on different corpus 
in the SIGHAN Bakeoff 02 and Bakeoff 03, 
respectively. For the performance of other 
systems, please refer to 
http://sighan.cs.uchicago.edu/bakeoff2005/data/r
esults.php.htm for the Chinese bakeoff 2005 and 
http://sighan.cs.uchicago.edu/bakeoff2006/longst
ats.html for the Chinese bakeoff 2006.  
Comparison against other systems shows that 
our system achieves the state-of-the-art 
performance on all Chinese word segmentation 
closed tracks and shows good scalability across 
different corpora. The small performance gap 
should be able to overcome by replacing the word 
unigram model with the more powerful word 
bigram model. Due to very limited time of less 
than three days, although our NER system under 
the unified framework as Chinese word 
segmentation does not achieve the 
state-of-the-art, its performance in NER is quite 
promising and provides a good platform for 
further improvement. Error analysis reveals that 
OOV is still an open problem that is far from to 
resolve. In addition, different corpus defines 
different segmentation principles. This will stress 
OOV handling in the extreme. Therefore a system 
trained on one genre usually performances worse 
when faced with text from a different register. 
5 Conclusion 
This paper proposes a purely unified statistical 
three-stage strategy in Chinese word 
segmentation and named entity recognition, 
which are based on a context-dependent Mutual 
Information Independence Model. Evaluation 
shows that our system achieves the 
states-of-the-art segmentation performance and 
provides a good platform for further performance 
improvement of Chinese NER. 
References  
Rabiner L. 1989. A Tutorial on Hidden Markov 
Models and Selected Applications in Speech 
Recognition. IEEE 77(2), pages257-285.  
Viterbi A.J. 1967. Error Bounds for 
Convolutional Codes and an Asymptotically 
Optimum Decoding Algorithm. IEEE 
Transactions on Information Theory, IT 13(2), 
260-269. 
Zhou GuoDong and Su Jain. 2002. Named Entity 
Recognition Using a HMM-based Chunk 
Tagger, Proceedings of the 40th Annual Meeting 
of the Association for Computational 
Linguistics  (ACL?2002). Philadelphia. July 
2002. pp473-480.  
156
Zhou GuoDong, Zhang Jie, Su Jian, Shen Dan and 
Tan ChewLim. 2004. Recognizing Names in 
Biomedical Texts: a Machine Learning 
Approach. Bioinformatics. 20(7): 1178-1190. 
DOI: 10.1093/bioinformatics/bth060. 2004. 
ISSN: 1460-2059 
Zhou GuoDong. 2004. Discriminative hidden 
Markov modeling with long state dependence 
using a kNN ensemble. Proceedings of 20th 
International Conference on Computational 
Linguistics (COLING?2004).  23-27 Aug, 2004, 
Geneva, Switzerland.  
Zhou GuoDong. 2005. A chunking strategy 
towards unknown word detection in Chinese 
word segmentation. Proceedings of 2nd 
International Joint Conference on Natural 
Language Processing (IJCNLP?2005), Lecture 
Notes in Computer Science (LNCS 3651) 
Zhou GuoDong. 2006. Recognizing names in 
biomedical texts using Mutual Information 
Independence Model and SVM plus Sigmod. 
International Journal of Medical Informatics 
(Article in Press). ISSN 1386-5056 
Tables  
Task P R F OOV Recall IV Recall 
CityU 0.9 38 0.952 94.5 0.578 0.967 
MSRA 0.952 0.962 95.7 0.51 0.98 
CKIP 0.94 0.957 94.8 0.502 0.976 
PKU 0.952 0.952 95.2 0.71 0.967 
Table 1: Performance of Word Segmentation on Closed Tracks in the SIGHAN Bakeoff 02  
 
Task P R F OOV Recall IV Recall 
CityU 0.968 0.961 96.5 0.633 0.983 
MSRA 0.961 0.953 95.7 0.499 0.977 
CKIP 0.958 0.941 94.9 0.554 0.976 
UPUC 0.936 0.917 92.6 0.617 0.966 
Table 2: Performance of Word Segmentation on Closed Tracks in the SIGHAN Bakeoff 03 
 
Task A P R F 
MSRA 0.9743 0.8150 0.7882 79.92 
CityU 0.9725 0.8466 0.8061 82.59 
     Table 3: Performance of NER on Closed Tracks in the SIGHAN Bakeoff 03 
 
 
 
157
