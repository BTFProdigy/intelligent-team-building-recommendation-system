Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 458?467,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Automatic Feature Engineering for Answer Selection and Extraction
Aliaksei Severyn
DISI, University of Trento
38123 Povo (TN), Italy
severyn@disi.unitn.it
Alessandro Moschitti
Qatar Computing Research Institue
5825 Doha, Qatar
amoschitti@qf.org.qa
Abstract
This paper proposes a framework for automat-
ically engineering features for two important
tasks of question answering: answer sentence
selection and answer extraction. We represent
question and answer sentence pairs with lin-
guistic structures enriched by semantic infor-
mation, where the latter is produced by auto-
matic classifiers, e.g., question classifier and
Named Entity Recognizer. Tree kernels ap-
plied to such structures enable a simple way to
generate highly discriminative structural fea-
tures that combine syntactic and semantic in-
formation encoded in the input trees. We con-
duct experiments on a public benchmark from
TREC to compare with previous systems for
answer sentence selection and answer extrac-
tion. The results show that our models greatly
improve on the state of the art, e.g., up to 22%
on F1 (relative improvement) for answer ex-
traction, while using no additional resources
and no manual feature engineering.
1 Introduction
Question Answering (QA) systems are typically
built from three main macro-modules: (i) search and
retrieval of candidate passages; (ii) reranking or se-
lection of the most promising passages; and (iii) an-
swer extraction. The last two steps are the most in-
teresting from a Natural Language Processing view-
point since deep linguistic analysis can be carried
out as the input is just a limited set of candidates.
Answer sentence selection refers to the task of se-
lecting the sentence containing the correct answer
among the different sentence candidates retrieved by
a search engine.
Answer extraction is a final step, required for
factoid questions, consisting in extracting multi-
words constituting the synthetic answer, e.g., Barack
Obama for a question: Who is the US president?
The definition of rules for both tasks is conceptually
demanding and involves the use of syntactic and se-
mantic properties of the questions and its related an-
swer passages.
For example, given a question from TREC QA1:
Q: What was Johnny Appleseed?s real
name?
and a relevant passage, e.g., retrieved by a search
engine:
A: Appleseed, whose real name was John
Chapman, planted many trees in the early
1800s.
a rule detecting the semantic links between Johnny
Appleseed?s real name and the correct answer
John Chapman in the answer sentence has to
be engineered. This requires the definition of
other rules that associate the question pattern
real name ?(X) with real name is(X) of
the answer sentence. Although this can be done by
an expert NLP engineer, the effort for achieving the
necessary coverage and a reasonable accuracy is not
negligible.
An alternative to manual rule definition is the use
of machine learning, which often shifts the problem
1We use it as our running example in the rest of the paper.
458
to the easier task of feature engineering. Unfortu-
nately, when the learning task is semantically dif-
ficult such as in QA, e.g., features have to encode
combinations of syntactic and semantic properties.
Thus their extraction modules basically assume the
shape of high-level rules, which are, in any case, es-
sential to achieve state-of-the-art accuracy. For ex-
ample, the great IBM Watson system (Ferrucci et
al., 2010) uses a learning to rank algorithm fed with
hundreds of features. The extraction of some of the
latter requires articulated rules/algorithms, which,
in terms of complexity, are very similar to those
constituting typical handcrafted QA systems. An
immediate consequence is the reduced adaptability
to new domains, which requires a substantial re-
engineering work.
In this paper, we show that tree kernels (Collins
and Duffy, 2002; Moschitti, 2006) can be applied to
automatically learn complex structural patterns for
both answer sentence selection and answer extrac-
tion. Such patterns are syntactic/semantic structures
occurring in question and answer passages. To make
such information available to the tree kernel func-
tions, we rely on the shallow syntactic trees enriched
with semantic information (Severyn et al, 2013b;
Severyn et al, 2013a), e.g., Named Entities (NEs)
and question focus and category, automatically de-
rived by machine learning modules, e.g., question
classifier (QC) or focus classifier (FC).
More in detail, we (i) design a pair of shallow
syntactic trees (one for the question and one for the
answer sentence); (ii) connect them with relational
nodes (i.e., those matching the same words in the
question and in the answer passages); (iii) label the
tree nodes with semantic information such as ques-
tion category and focus and NEs; and (iv) use the NE
type to establish additional semantic links between
the candidate answer, i.e., an NE, and the focus word
of the question. Finally, for the task of answer ex-
traction we also connect such semantic information
to the answer sentence trees such that we can learn
factoid answer patterns.
We show that our models are very effective in pro-
ducing features for both answer selection and ex-
traction by experimenting with TREC QA corpora
and directly comparing with the state of the art,
e.g., (Wang et al, 2007; Yao et al, 2013). The re-
sults show that our methods greatly improve on both
tasks yielding a large improvement in Mean Average
Precision for answer selection and in F1 for answer
extraction: up to 22% of relative improvement in F1,
when small training data is used. Moreover, in con-
trast to the previous work, our model does not rely
on external resources, e.g., WordNet, or complex
features in addition to the structural kernel model.
The reminder of this paper is organized as fol-
lows, Sec. 2 describes our kernel-based classifiers,
Sec. 3 illustrates our question/answer relational
structures also enriched with semantic information,
Sec. 4 describes our model for answer selection and
extraction, Sec. 5 illustrates our comparative exper-
iments on TREC data, Sec. 6 reports on our error
analysis, Sec. 7 discusses the related work, and fi-
nally, Sec. 8 derives the conclusions.
2 Structural Kernels for classification
This section describes a kernel framework where the
input question/answer pairs are handled directly in
the form of syntactic/semantic structures.
2.1 Feature vector approach to object pair
classification
A conventional approach to represent a ques-
tion/answer pairs in linear models consists in defin-
ing a set of similarity features {xi} and computing
the simple scalar product h(x) = w ? x =
?
iwixi,
where w is the model weight vector learned on the
training data. Hence, the learning problem boils
down to estimating individual weights of each of
the similarity features xi. Such features often en-
code various types of lexical, syntactic and semantic
similarities shared between a question and its can-
didate. Previous work used a rich number of distri-
butional semantic, knowledge-based, translation and
paraphrase resources to build explicit feature vector
representations. One evident potential downside of
using feature vectors is that a great deal of structural
information encoded in a given text pair is lost.
2.2 Pair Classification using Structural Kernels
A more versatile approach in terms of the input
representation relies on kernels. A typical ker-
nel machine, e.g., SVM, classifies a test input x
using the following prediction function: h(x) =
?
i ?iyiK(x,xi), where ?i are the model parame-
ters estimated from the training data, yi are target
459
variables, xi are support vectors, andK(?, ?) is a ker-
nel function. The latter can measure the similarity
between question and answer pairs.
We define each question/answer pair x as a triple
composed of a question treeT q and answer sentence
tree T s and a similarity feature vector v , i.e., x =
?T q,T s, v?. Given two triples xi and xj , we define
the following kernel:
K(xi,xj) = KTK(T iq,T
j
q)
+ KTK(T is,T
j
s)
+ Kv(v i, vj),
(1)
where KTK computes a structural kernel, e.g., tree
kernel, and Kv is a kernel over feature vectors, e.g.,
linear, polynomial, gaussian, etc. Structural kernels
can capture the structural representation of a ques-
tion/answer pair whereas traditional feature vectors
can encode some sort of similarity, e.g., lexical, syn-
tactic, semantic, between a question and its candi-
date answer.
We prefer to split the kernel computation over a
question/answer pair into two terms since tree ker-
nels are very efficient and there are no efficient
graph kernels that can encode exhaustively all graph
fragments. It should be noted that the tree kernel
sum does not capture feature pairs. Theoretically,
for such purpose, a kernel product should be used.
However, our experiments revealed that using the
product is actually worse in practice. In contrast,
we solve the lack of feature pairing by annotating
the trees with relational tags which are supposed
to link the question tree fragments with the related
fragments from the answer sentence.
Such relational information is very important to
improve the quality of the pair representation as well
as the implicitly generated features. In the next sec-
tion, we show simple structural models that we used
in our experiments for question and answer pair clas-
sification.
2.3 Partial Tree Kernels
The above framework can use any kernel for
structural data. We use the Partial Tree Kernel
(PTK) (Moschitti, 2006) to compute KTK(?, ?) as it
is the most general convolution tree kernel, which
at the same time shows rather good efficiency. PTK
can be effectively applied to both constituency and
dependency parse trees. It generalizes the syntactic
tree kernel (STK) (Collins and Duffy, 2002), which
maps a tree into the space of all possible tree frag-
ments constrained by the rule that sibling nodes can-
not be separated. In contrast, the PTK fragments
can contain any subset of siblings, i.e., PTK allows
for breaking the production rules in syntactic trees.
Consequently, PTK generates an extremely rich fea-
ture space, which results in higher generalization
ability.
3 Relational Structures
This section introduces relational structures de-
signed to encode syntactic and shallow semantic
properties of question/answer pairs. We first define a
simple to construct shallow syntactic tree represen-
tation derived from a shallow parser. Next, we in-
troduce a relational linking scheme based on a plain
syntactic matching and further augment it with ad-
ditional semantic information.
3.1 Shallow syntactic tree
Our shallow tree structure is a two-level syntactic
hierarchy built from word lemmas (leaves), part-of-
speech tags that organized into chunks identified by
a shallow syntactic parser (Fig. 1). We defined a
similar structure in (Severyn and Moschitti, 2012)
for answer passage reranking, which improved on
feature vector baselines.
This simple linguistic representation is suitable
for building a rather expressive answer sentence se-
lection model. Moreover, the use of a shallow parser
is motivated by the need to generate text spans to
produce candidate answers required by an answer
extraction system.
3.2 Tree pairs enriched with relational links
It is important to establish a correspondence be-
tween question and answer sentence aligning related
concepts from both. We take on a two-level ap-
proach, where we first use plain lexical matching to
connect common lemmas from the question and its
candidate answer sentence. Secondly, we establish
semantic links between NEs extracted from the an-
swer sentence and the question focus word, which
encodes the expected lexical answer type (LAT). We
use the question categories to identify NEs that have
460
Figure 1: Shallow tree representation of the example q/a pair from Sec. 1. Dashed arrows (red) indicate the tree
fragments (red dashed boxes) in the question and its answer sentence linked by the relational REL tag, which is
established via syntactic match on the word lemmas. Solid arrows (blue) connect a question focus word name with the
related named entities of type Person corresponding to the question category (HUM) via a relational tag REL-HUM.
Additional ANS tag is used to mark chunks containing candidate answer (here the correct answer John Chapman).
higher probability to be correct answers following a
mapping defined in Table 1.
Next, we briefly introduce our tree kernel-based
models for building question focus and category
classifiers.
Lexical Answer Type. Question Focus represents
a central entity or a property asked by a question
(Prager, 2006). It can be used to search for semanti-
cally compatible candidate answers, thus greatly re-
ducing the search space (Pinchak, 2006). While sev-
eral machine learning approaches based on manual
features and syntactic structures have been recently
explored, e.g. (Quarteroni et al, 2012; Damljanovic
et al, 2010; Bunescu and Huang, 2010), we opt for
the latter approach where tree kernels handle auto-
matic feature engineering.
To build an automatic Question Focus detector we
use a tree kernel approach as follows: we (i) parse
each question; (ii) create a set of positive trees by
labeling the node exactly covering the focus with
FC tag; (iii) build a set of negative trees by labeling
any other constituent node with FC; (iii) we train
the FC node classifier with tree kernels. At the test
time, we try to label each constituent node with FC
generating a set of candidate trees. Finally, we select
the tree and thus the constituent associated with the
highest SVM score.
Question classification. Our question classification
model is simpler than before: we use an SVM multi-
classifier with tree kernels to automatically extract
the question class. To build a multi-class classifier
we train a binary SVM for each of the classes and
apply a one-vs-all strategy to obtain the predicted
Table 1: Expected Answer Type (EAT) ? named entity
types.
EAT Named Entity types
HUM Person
LOCATION Location
ENTITY Organization, Person, Misc
DATE Date, Time, Number
QUANTITY Number, Percentage
CURRENCY Money, Number
class. We use constituency trees as our input repre-
sentation.
Our question taxonomy is derived from the
UIUIC dataset (Li and Roth, 2002) which defines
6 coarse and 50 fine grain classes. In particular,
our set of question categories is formed by adopt-
ing 3 coarse classes: HUM (human), LOC (loca-
tion), ENTY (entities) and replacing the NUM (nu-
meric) coarse class with 3 fine-grain classes: CUR-
RENCY, DATE, QUANTITY2. This set of question
categories is sufficient to capture the coarse seman-
tic answer type of the candidate answers found in
TREC. Also using fewer question classes results in
a more accurate multi-class classifier.
Semantic tagging. Question focus word specifies
the lexical answer type capturing the target informa-
tion need posed by a question, but to make this piece
of information effective, the focus word needs to
be linked to the target candidate answer. The focus
word can be lexically matched with words present in
2This class is composed by including all the fine-grain
classes from NUMERIC coarse class except for CURRENCY
and DATE.
461
the answer sentence, or the match can be established
using semantic information. Clearly, the latter ap-
proach is more appealing since it helps to alleviate
the lexical gap problem, i.e., it improves the cover-
age of the na?ive string matching of words between a
question and its answer.
Hence, we propose to exploit a question focus
along with the related named entities (according to
the mapping from Table 1) of the answer sentence
to establish relational links between the tree frag-
ments. In particular, once the question focus and
question category are determined, we link the fo-
cus word wfocus in the question, with all the named
entities whose type matches the question class (Ta-
ble 1). We perform tagging at the chunk level and
use a relational tag typed with a question class, e.g.,
REL-HUM. Fig. 1 shows an example q/a pair where
the typed relational tag is used in the shallow syntac-
tic tree representation to link the chunk containing
the question focus name with the named entities of
the corresponding type Person, i.e., Appleseed and
John Chapman.
4 Answer Sentence Selection and Answer
Keyword Extraction
This section describes our approach to (i) answer
sentence selection used to select the most promising
answer sentences; and (ii) answer extraction which
returns the answer keyword (for factoid questions).
4.1 Answer Sentence Selection
We cast the task of answer sentence selection as
a classification problem. Considering a supervised
learning scenario, we are given a set of questions
{qi}Ni=1 where each question qi is associated with
a list of candidate answer sentences {(ri, si)}Ni=1,
with ri ? {?1,+1} indicating if a given candidate
answer sentence si contains a correct answer (+1)
or not (?1). Using this labeled data, our goal is to
learn a classifier model to predict if a given pair of
a question and an answer sentence is correct or not.
We train a binary SVM with tree kernels3 to train an
answer sentence classifier. The prediction scores ob-
tained from a classifier are used to rerank the answer
candidates (pointwise reranking), s.t. the sentences
that are more likely to contain correct answers will
3disi.unitn.it/moschitti/Tree-Kernel.htm
be ranked higher than incorrect candidates. In addi-
tion to the structural representation, we augment our
model with basic bag-of-word features (unigram and
bigrams) computed over lemmas.
4.2 Answer Sentence Extraction
The goal of answer extraction is to extract a text span
from a given candidate answer sentence. Such span
represents a correct answer phrase for a given ques-
tion. Different from previous work that casts the an-
swer extraction task as a tagging problem and apply
a CRF to learn an answer phrase tagger (Yao et al,
2013), we take on a simpler approach using a kernel-
based classifier.
In particular, we rely on the shallow tree represen-
tation, where text spans identified by a shallow syn-
tactic parser serve as a source of candidate answers.
Algorithm 1 specifies the steps to generate training
data for our classifier. In particular, for each ex-
ample representing a triple ?a, Tq, Ts? composed of
the answer a, the question and the answer sentence
trees, we generate a set of training examples E with
every candidate chunk marked with an ANS tag (one
at a time). To reduce the number of generated exam-
ples for each answer sentence, we only consider NP
chunks, since other types of chunks, e.g., VP, ADJP,
typically do not contain factoid answers. Finally, an
original untagged tree is used to generate a positive
example (line 8), when the answer sentence contains
a correct answer, and a negative example (line 10),
when it does not contain a correct answer.
At the classification time, given a question and a
candidate answer sentence, all NP nodes of the sen-
tence are marked with ANS (one at a time) as the
possible answer, generating a set of tree candidates.
Then, such trees are classified (using the kernel from
Eq. 1) and the one with the highest score is selected.
If no tree is classified as positive example we do not
extract any answer.
5 Experiments
We provide the results on two related yet different
tasks: answer sentence selection and answer extrac-
tion. The goal of the former is to learn a model
scoring correct question and answer sentence pairs
to bring in the top positions sentences containing the
correct answers. Answer extraction derives the cor-
462
Algorithm 1 Generate training data for answer ex-
traction
1: for all ?a, Tq, Ts? ?D do
2: E ? ?
3: for all chunk ? extract chunks(Ts) do
4: if not chunk == NP then
5: continue
6: T ?s ? tagAnswerChunk(Ts, chunk)
7: if contains answer(a, chunk) then
8: label? +1
9: else
10: label? ?1
11: e? build example(Tq, T ?s, label)
12: E ? E ? {e}
13: return E
rect answer keywords, i.e., a text span such as multi-
words or constituents, from a given sentence.
5.1 Semantic Annotation
We briefly describe the experiments of training auto-
matic question category and focus classifiers, which
are more extensively described in (Severyn et al,
2013b).
Question Focus detection. We used three datasets
for training and evaluating the performance of our
focus detector: SeCo-600 (Quarteroni et al, 2012),
Mooney GeoQuery (Damljanovic et al, 2010) and
the dataset from (Bunescu and Huang, 2010). The
SeCo dataset contains 600 questions. The Mooney
GeoQuery contains 250 question targeted at ge-
ographical information in the U.S. The first two
datasets are very domain specific, while the dataset
from (Bunescu and Huang, 2010) is more generic
containing the first 2,000 questions from the answer
type dataset from Li and Roth annotated with fo-
cus words. We removed questions with implicit and
multiple focuses.
Question Classification. We used the UIUIC
dataset (Li and Roth, 2002) which contains 5,952
factoid questions 4 to train a multi-class question
classifier.
Table 2 summarizes the results of question focus
and category classification.
4We excluded questions from TREC to ensure there is no
overlap with the data used for testing models trained on TREC
QA.
Table 2: Accuracy (%) of focus (FC) and question classi-
fiers (QC) using PTK.
TASK SET PTK
FC
MOONEY 80.5
SECO-600 90.0
BUNESCU 96.9
QC
UIUIC 85.9
TREC 11-12 78.1
5.2 Answer Sentence Selection
We used the train and test data from (Wang et al,
2007) to enable direct comparison with previous
work on answer sentence selection. The training
data is composed by questions drawn from TREC
8-12 while questions from TREC 13 are used for
testing. The data provided for training comes as
two sets: a small set of 94 questions (TRAIN) that
were manually curated for errors5 and 1,229 ques-
tions from the entire TREC 8-12 that contain at least
one correct answer sentence (ALL). The latter set
represents a more noisy setting, since many answer
sentences are marked erroneously as correct as they
simply match a regular expression. Table 3 summa-
rizes the data used for training and testing.
Table 4 compares our kernel-based structural
model with the previous state-of-the-art systems for
answer sentence selection. In particular, we com-
pare with four most recent state of the art answer
sentence reranker models (Wang et al, 2007; Heil-
man and Smith, 2010; Wang and Manning, 2010;
Yao et al, 2013), which report their performance on
the same questions and candidate sets from TREC
13 as provided by (Wang et al, 2007).
Our simple shallow tree representation (Severyn
and Moschitti, 2012) delivers state-of-the-art ac-
curacy largely improving on previous work. Fi-
nally, augmenting the structure with semantic link-
ing (Severyn et al, 2013b) yields additional im-
provement in MAP and MRR. This suggests the
utility of using supervised components, e.g., ques-
tion focus and question category classifiers coupled
with NERs, to establish semantic mapping between
words in a q/a pair.
5In TREC correct answers are identified by regex matching
using the provided answer pattern files
463
Table 3: Summary of TREC data for answer extraction
used in (Yao et al, 2013).
data questions candidates correct
TRAIN 94 4718 348
ALL 1229 53417 6410
TEST 89 1517 284
Table 4: Answer sentence reranking on TREC 13.
System MAP MRR
Wang et al (2007) 0.6029 0.6852
Heilman & Smith (2010) 0.6091 0.6917
Wang & Manning (2010) 0.5951 0.6951
Yao et al (2013) 0.6319 0.7270
+ WN 0.6371 0.7301
shallow tree (S&M, 2012) 0.6485 0.7244
+ semantic tagging 0.6781 0.7358
It is worth noting that our kernel-based classifier
is conceptually simpler than approaches in the previ-
ous work, as it relies on the structural kernels, e.g.,
PTK, to automatically extract salient syntactic pat-
terns relating questions and answers. Our model
only includes the most basic feature vector (uni- and
bi-grams) and does not rely on external sources such
as WordNet.
5.3 Answer Extraction
Our experiments on answer extraction replicate the
setting of (Yao et al, 2013), which is the most recent
work on answer extraction reporting state-of-the-art
results.
Table 5 reports the accuracy of our model in re-
covering correct answers from a set of candidate an-
swer sentences for a given question. Here the fo-
cus is on the ability of an answer extraction system
to recuperate as many correct answers as possible
from each answer sentence candidate. The set of
extracted candidate answers can then be used to se-
lect a single best answer, which is the final output
of the QA system for factoid questions. Recall (R)
encodes the percentage of correct answer sentences
for which the system correctly extracts an answer
(for TREC 13 there are a total of 284 correct answer
sentences), while Precision (P) reflects how many
answers extracted by the system are actually correct.
Clearly, having a high recall system, allows for cor-
rectly answering more questions. On the other hand,
a high precision system would attempt to answer less
questions (extracting no answers at all) but get them
right.
We compare our results to a CRF model of (Yao et
al., 2013) augmented with WordNet features (with-
out forced voting) 6. Unlike the CRF model which
obtains higher values of precision, our system acts
as a high recall system able to recover most of the
answers from the correct answer sentences. Having
higher recall is favorable to high precision in answer
extraction since producing more correct answers can
help in the final voting scheme to come up with a
single best answer. To solve the low recall problem
of their CRF model, Yao et al (2013) apply fairly
complex outlier resolution techniques to force an-
swer predictions, thus aiming at increasing the num-
ber of extracted answers.
To further boost the number of answers produced
by our system we exclude negative examples (an-
swer sentences not containing the correct answer)
from training, which slightly increases the number
of pairs with correctly recovered answers. Never-
theless, it has a substantial effect on the number of
questions that can be answered correctly (assuming
perfect single best answer selection). Clearly, our
system is able to recover a large number of answers
from the correct answer sentences, while low pre-
cision, i.e., extracting answer candidates from sen-
tences that do not contain a correct answer, can be
overcome by further applying various best answer
selection strategies, which we explore in the next
section.
5.4 Best Answer Selection
Since the final step of the answer extraction module
is to select for each question a single best answer
from a set of extracted candidate answers, an answer
selection scheme is required.
We adopt a simple majority voting strategy, where
we aggregate the extracted answers produced by our
answer extraction model. Answers sharing simi-
lar lemmas (excluding stop words) are grouped to-
gether. The prediction scores obtained by the an-
6We could not replicate the results obtained in (Yao et al,
2013) with the forced voting strategy. Thus such result is not
included in Table 5.
464
Table 5: Results on answer extraction. P/R - precision
and recall; pairs - number of QA pairs with a correctly ex-
tracted answer, q - number of questions with at least one
correct answer extracted, F1 sets an upper bound on the
performance assuming the selected best answer among
extracted candidates is always correct. *-marks the set-
ting where we exclude incorrect question answer pairs
from training.
set P R pairs q F1
Yao et al (2013) 25.7 23.4 73 33 -
+ WN 26.7 24.3 76 35 -
TRAIN 29.6 64.4 183 58 65.2
TRAIN* 15.7 71.8 204 66 74.1
Yao et al (2013) 35.2 35.1 100 38 -
+ WN 34.5 34.7 98 38 -
ALL 29.4 74.6 212 69 77.5
ALL* 15.8 76.7 218 73 82.0
Table 6: Results on finding the best answer with voting.
system set P R F1
Yao et al (2013)
TRAIN
55.7 43.8 49.1
+ forced 54.5 53.9 54.2
+ WN 55.2 53.9 54.5
this work 66.2 66.2 66.2
Yao et al (2013)
ALL
67.2 50.6 57.7
+ forced 60.9 59.6 60.2
+ WN 63.6 62.9 63.3
this work 70.8 70.8 70.8
swer extraction classifier are used as votes to decide
on the final rank to select the best single answer.
Table 6 shows the results after the majority vot-
ing is applied to select a single best answer for each
candidate. A rather na??ve majority voting scheme
already produces satisfactory outcome demonstrat-
ing better results than the previous work. Our vot-
ing scheme is similar to the one used by (Yao et al,
2013), yet it is much simpler since we do not per-
form any additional hand tuning to account for the
weight of the ?forced? votes or take any additional
steps to catch additional answers using outlier detec-
tion techniques applied in the previous work.
6 Discussion and Error Analysis
There are several sources of errors affecting the fi-
nal performance of our answer extraction system: (i)
chunking, (ii) named entity recognition and seman-
tic linking, (iii) answer extraction, (iv) single best
answer selection.
Chunking. Our system uses text spans identified by
a chunker to extract answer candidates, which makes
it impossible to extract answers that lie outside the
chunk boundaries. Nevertheless, we found this to
be a minor concern since for 279 out of total 284
candidate sentences from TREC 13 the answers are
recoverable within the chunk spans.
Semantic linking. Our structural model relies heav-
ily on the ability of NER to identify the relevant en-
tities in the candidate sentence that can be further
linked to the focus word of the question. While
our answer extraction model is working on all the
NP chunks, the semantic tags from NER serve as a
strong cue for the classifier that a given chunk has
a high probability of containing an answer. Typical
off-the-shelf NER taggers have good precision and
low recall, s.t. many entities as potential answers are
missed. In this respect, a high recall entity linking
system, e.g., linking to wikipedia entities (Ratinov
et al, 2011), is required to boost the quality of can-
didates considered for answer extraction. Finally,
improving the accuracy of question and focus clas-
sifiers would allow for having more accurate input
representations fed to the learning algorithm.
Answer Extraction. Our answer extraction model
acts as a high recall system, while it suffers from
low precision in extracting answers for many incor-
rect sentences. Improving the precision without sac-
rificing the recall would ease the successive task of
best answer selection, since having less incorrect an-
swer candidates would result in a better final per-
formance. Introducing additional constraints in the
form of semantic tags to allow for better selection of
answer candidates could also improve our system.
Best Answer Selection. We apply a na??ve majority
voting scheme to select a single best answer from
a set of extracted answer candidates. This step has
a dramatic impact on the final performance of the
answer extraction system resulting in a large drop
of recall, i.e., from 82.0 to 70.8 before and after vot-
ing respectively. Hence, a more involved model, i.e.,
465
performing joint answer sentence re-ranking and an-
swer extraction, is required to yield a better perfor-
mance.
7 Related Work
Tree kernel methods have found many applications
for the task of answer reranking which are reported
in (Moschitti, 2008; Moschitti, 2009; Moschitti and
Quarteroni, 2008; Severyn and Moschitti, 2012).
However, their methods lack the use of important
relational information between a question and a can-
didate answer, which is essential to learn accurate
relational patterns. In this respect, a solution based
on enumerating relational links was given in (Zan-
zotto and Moschitti, 2006; Zanzotto et al, 2009) for
the textual entailment task but it is computationally
too expensive for the large dataset of QA. A few so-
lutions to overcome computational issues were sug-
gested in (Zanzotto et al, 2010).
In contrast, this paper relies on structures directly
encoding the output of question and focus classifiers
to connect focus word and good candidate answer
keywords (represented by NEs) of the answer pas-
sage. This provides more effective relational infor-
mation, which allows our model to significantly im-
prove on previous rerankers. Additionally, previous
work on kernel-based approaches does not target an-
swer extraction.
One of the best models for answer sentence selec-
tion has been proposed in (Wang et al, 2007). They
use the paradigm of quasi-synchronous grammar to
model relations between a question and a candidate
answer with syntactic transformations. (Heilman
and Smith, 2010) develop an improved Tree Edit
Distance (TED) model for learning tree transforma-
tions in a q/a pair. They search for a good sequence
of tree edit operations using complex and com-
putationally expensive Tree Kernel-based heuristic.
(Wang and Manning, 2010) develop a probabilistic
model to learn tree-edit operations on dependency
parse trees. They cast the problem into the frame-
work of structured output learning with latent vari-
ables. The model of (Yao et al, 2013) has reported
an improvement over the Wang?s et al (2007) sys-
tem. It applies linear chain CRFs with features de-
rived from TED and WordNet to automatically learn
associations between questions and candidate an-
swers.
Different from previous approaches that use tree-
edit information derived from syntactic trees, our
kernel-based learning approach also use tree struc-
tures but with rather different learning methods, i.e.,
SVMs and structural kernels, to automatically ex-
tract salient syntactic patterns relating questions and
answers. In (Severyn et al, 2013c), we have shown
that such relational structures encoding input text
pairs can be directly used within the kernel learning
framework to build state-of-the-art models for pre-
dicting semantic textual similarity. Furthermore, se-
mantically enriched relational structures, where au-
tomatic have been previously explored for answer
passage reranking in (Severyn et al, 2013b; Sev-
eryn et al, 2013a). This paper demonstrates that this
model also works for building a reranker on the sen-
tence level, and extends the previous work by apply-
ing the idea of automatic feature engineering with
tree kernels to answer extraction.
8 Conclusions
Our paper demonstrates the effectiveness of han-
dling the input structures representing QA pairs di-
rectly vs. using explicit feature vector representa-
tions, which typically require substantial feature en-
gineering effort. Our approach relies on a kernel-
based learning framework, where structural kernels,
e.g., tree kernels, are used to handle automatic fea-
ture engineering. It is enough to specify the desired
type of structures, e.g., shallow, constituency, de-
pendency trees, representing question and its can-
didate answer sentences and let the kernel learning
framework learn to use discriminative tree fragments
for the target task.
An important feature of our approach is that it
can effectively combine together different types of
syntactic and semantic information, also generated
by additional automatic classifiers, e.g., focus and
question classifiers. We augment the basic struc-
tures with additional relational and semantic infor-
mation by introducing special tag markers into the
tree nodes. Using the structures directly in the ker-
nel learning framework makes it easy to integrate
additional relational constraints and semantic infor-
mation directly in the structures.
The comparison with previous work on a public
466
benchmark from TREC suggests that our approach
is very promising as we can improve the state of the
art in both answer selection and extraction by a large
margin (up to 22% of relative improvement in F1 for
answer extraction). Our approach makes it relatively
easy to integrate other sources of semantic informa-
tion, among which the use of Linked Open Data can
be the most promising to enrich the structural repre-
sentation of q/a pairs.
To achieve state-of-the-art results in answer sen-
tence selection and answer extraction, it is sufficient
to provide our model with a suitable tree structure
encoding relevant syntactic information, e.g., using
shallow, constituency or dependency formalisms.
Moreover, additional semantic and relational infor-
mation can be easily plugged in by marking tree
nodes with special tags. We believe this approach
greatly eases the task of tedious feature engineering
that will find its applications well beyond QA tasks.
Acknowledgements
This research is partially supported by the EU?s
7th Framework Program (FP7/2007-2013) (#288024
LIMOSINE project) and an Open Collaborative Re-
search (OCR) award from IBM Research. The first
author is supported by the Google Europe Fellow-
ship 2013 award in Machine Learning.
References
Razvan Bunescu and Yunfeng Huang. 2010. Towards a
general model of answer typing: Question focus iden-
tification. In CICLing.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over Dis-
crete Structures, and the Voted Perceptron. In ACL.
Danica Damljanovic, Milan Agatonovic, and Hamish
Cunningham. 2010. Identification of the question fo-
cus: Combining syntactic analysis and ontology-based
lookup through the user interaction. In LREC.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya Kalyanpur, Adam Lally,
J. William Murdock, Eric Nyberg, John Prager, Nico
Schlaefer, and Chris Welty. 2010. Building watson:
An overview of the deepqa project. AI Magazine,
31(3).
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments, para-
phrases, and answers to questions. In NAACL.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In COLING.
A. Moschitti and S. Quarteroni. 2008. Kernels on Lin-
guistic Structures for Answer Extraction. In ACL.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
ECML.
Alessandro Moschitti. 2008. Kernel methods, syntax and
semantics for relational text categorization. In CIKM.
Alessandro Moschitti. 2009. Syntactic and semantic ker-
nels for short text pair categorization. In EACL.
Christopher Pinchak. 2006. A probabilistic answer type
model. In In EACL.
John M. Prager. 2006. Open-domain question-
answering. Foundations and Trends in Information
Retrieval, 1(2):91?231.
Silvia Quarteroni, Vincenzo Guerrisi, and Pietro La
Torre. 2012. Evaluating multi-focus natural language
queries over data services. In LREC.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In ACL.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of an-
swer re-ranking. In SIGIR.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013a. Building structures from classifiers
for passage reranking. In CIKM.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013b. Learning adaptable patterns for
passage reranking. In CoNLL.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013c. Learning semantic textual similar-
ity with structural representations. In ACL.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question answer-
ing. In ACL.
Mengqiu Wang, Noah A. Smith, and Teruko Mitaura.
2007. What is the jeopardy model? a quasi-
synchronous grammar for qa. In EMNLP.
Xuchen Yao, Benjamin Van Durme, Peter Clark, and
Chris Callison-Burch. 2013. Answer extraction as se-
quence tagging with tree edit distance. In NAACL.
F. M. Zanzotto and A. Moschitti. 2006. Automatic
Learning of Textual Entailments with Cross-Pair Sim-
ilarities. In COLING.
F. M. Zanzotto, M. Pennacchiotti, and A. Moschitti.
2009. A Machine Learning Approach to Recognizing
Textual Entailment. Natural Language Engineering,
Volume 15 Issue 4, October 2009:551?582.
F. M. Zanzotto, L. Dell?Arciprete, and A. Moschitti.
2010. Efficient graph kernels for textual entail-
ment recognition. FUNDAMENTA INFORMATICAE,
2010.
467
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 664?672,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Encoding Semantic Resources in Syntactic Structures
for Passage Reranking
Kateryna Tymoshenko
Trento RISE
38123 Povo (TN), Italy
k.tymoshenko@trentorise.eu
Alessandro Moschitti
Qatar Computing Research Instit.
5825 Doha, Qatar
amoschitti@qf.org.qa
Aliaksei Severyn
University of Trento
38123 Povo (TN), Italy
severyn@disi.unitn.it
Abstract
In this paper, we propose to use seman-
tic knowledge from Wikipedia and large-
scale structured knowledge datasets avail-
able as Linked Open Data (LOD) for
the answer passage reranking task. We
represent question and candidate answer
passages with pairs of shallow syntac-
tic/semantic trees, whose constituents are
connected using LOD. The trees are pro-
cessed by SVMs and tree kernels, which
can automatically exploit tree fragments.
The experiments with our SVM rank algo-
rithm on the TREC Question Answering
(QA) corpus show that the added relational
information highly improves over the state
of the art, e.g., about 15.4% of relative im-
provement in P@1.
1 Introduction
Past work in TREC QA, e.g. (Voorhees, 2001),
and more recent work (Ferrucci et al., 2010) in
QA has shown that, to achieve human perfor-
mance, semantic resources, e.g., Wikipedia
1
,
must be utilized by QA systems. This requires
the design of rules or machine learning features
that exploit such knowledge by also satisfying
syntactic constraints, e.g., the semantic type of
the answer must match the question focus words.
The engineering of such rules for open domain
QA is typically very costly. For instance, for
automatically deriving the correctness of the
answer passage in the following question/answer
passage (Q/AP) pair (from the TREC corpus
2
):
Q: What company owns the soft drink brand ?Gatorade??
A: Stokely-Van Camp bought the formula and started
marketing the drink as Gatorade in 1967. Quaker Oats Co.
took over Stokely-Van Camp in 1983.
1
http://www.wikipedia.org
2
It will be our a running example for the rest of the paper.
we would need to write the following complex
rules:
is(Quaker Oats Co.,company),
own(Stokely-Van Camp,Gatorade),
took over(Quaker Oats Co.,Stokely-Van Camp),
took over(Y, Z)?own(Z,Y),
and carry out logic unification and resolution.
Therefore, approaches that can automatically
generate patterns (i.e., features) from syntactic
and semantic representations of the Q/AP are
needed. In this respect, our previous work, e.g.,
(Moschitti et al., 2007; Moschitti and Quarteroni,
2008; Moschitti, 2009), has shown that tree
kernels for NLP, e.g., (Moschitti, 2006), can
exploit syntactic patterns for answer passage
reranking significantly improving search engine
baselines. Our more recent work, (Severyn and
Moschitti, 2012; Severyn et al., 2013b; Severyn
et al., 2013a), has shown that using automatically
produced semantic labels in shallow syntactic
trees, such as question category and question
focus, can further improve passage reranking and
answer extraction (Severyn and Moschitti, 2013).
However, such methods cannot solve the class
of examples above as they do not use background
knowledge, which is essential to answer com-
plex questions. On the other hand, Kalyanpur
et al. (2011) and Murdock et al. (2012) showed
that semantic match features extracted from large-
scale background knowledge sources, including
the LOD ones, are beneficial for answer rerank-
ing.
In this paper, we tackle the candidate answer
passage reranking task. We define kernel func-
tions that can automatically learn structural pat-
terns enriched by semantic knowledge, e.g., from
LOD. For this purpose, we carry out the follow-
ing steps: first, we design a representation for the
Q/AP pair by engineering a pair of shallow syn-
tactic trees connected with relational nodes (i.e.,
664
  
NLPAnnotatorsFocus and Question classifiers
NLPAnnotatorsFocus and Question classifiers
syntactic/semantic graphsyntactic/semantic graph train/testdata
Kernel-based rerankerKernel-based reranker
RerankedAP
EvaluationEvaluation
CandidateAPQuestion
UIMA pipeline
Search engineSearch engine
q/a similarity featuresq/a similarity features
Wikipedia link annotatorWikipedia link annotator
WikipediaWikipedia
LOD type annotatorLOD type annotator
LOD datasetsLOD datasets
Figure 1: Kernel-based Answer Passage Reranking System
those matching the same words in the question and
in the answer passages).
Secondly, we use YAGO (Suchanek et al.,
2007), DBpedia (Bizer et al., 2009) and Word-
Net (Fellbaum, 1998) to match constituents from
Q/AP pairs and use their generalizations in our
syntactic/semantic structures. We employ word
sense disambiguation to match the right entities in
YAGO and DBpedia, and consider all senses of an
ambiguous word from WordNet.
Finally, we experiment with TREC QA and sev-
eral models combining traditional feature vectors
with automatic semantic labels derived by statis-
tical classifiers and relational structures enriched
with LOD relations. The results show that our
methods greatly improve over strong IR baseline,
e.g., BM25, by 96%, and on our previous state-
of-the-art reranking models, up to 15.4% (relative
improvement) in P@1.
2 Reranking with Tree Kernels
In contrast to ad-hoc document retrieval, struc-
tured representation of sentences and paragraphs
helps to improve question answering (Bilotti et al.,
2010). Typically, rules considering syntactic and
semantic properties of the question and its candi-
date answer are handcrafted. Their modeling is in
general time-consuming and costly. In contrast,
we rely on machine learning and automatic fea-
ture engineering with tree kernels. We used our
state-of-the-art reranking models, i.e., (Severyn et
al., 2013b; Severyn et al., 2013a) as a baseline.
Our major difference with such approach is that
we encode knowledge and semantics in different
ways, using knowledge from LOD. The next sec-
tions outline our new kernel-based framework, al-
though the detailed descriptions of the most inno-
vative aspects such as new LOD-based representa-
tions are reported in Section 3.
2.1 Framework Overview
Our QA system is based on a rather simple rerank-
ing framework as displayed in Figure 1: given a
question Q, a search engine retrieves a list of can-
didate APs ranked by their relevancy. Next, the
question together with its APs are processed by
a rich NLP pipeline, which performs basic tok-
enization, sentence splitting, lemmatization, stop-
word removal. Various NLP components, em-
bedded in the pipeline as UIMA
3
annotators, per-
form more involved linguistic analysis, e.g., POS-
tagging, chunking, NE recognition, constituency
and dependency parsing, etc.
Each Q/AP pair is processed by a Wikipedia
link annotator. It automatically recognizes n-
grams in plain text, which may be linked to
Wikipedia and disambiguates them to Wikipedia
URLs. Given that question passages are typically
short, we concatenate them with the candidate an-
swers to provide a larger disambiguation context
to the annotator.
These annotations are then used to produce
computational structures (see Sec. 2.2) input to the
reranker. The semantics of such relational struc-
tures can be further enriched by adding links be-
tween Q/AP constituents. Such relational links
can be also generated by: (i) matching lemmas
as in (Severyn and Moschitti, 2012); (ii) match-
ing the question focus type derived by the ques-
tion classifiers with the type of the target NE as
in (Severyn et al., 2013a); or (iii) by matching the
constituent types based on LOD (proposed in this
paper). The resulting pairs of trees connected by
semantic links are then used to train a kernel-based
reranker, which is used to re-order the retrieved
answer passages.
2.2 Relational Q/AP structures
We use the shallow tree representation that we
proposed in (Severyn and Moschitti, 2012) as a
baseline structural model. More in detail, each Q
and its candidate AP are encoded into two trees,
where lemmas constitute the leaf level, the part-
of-speech (POS) tags are at the pre-terminal level
and the sequences of POS tags are organized into
the third level of chunk nodes. We encoded struc-
tural relations using the REL tag, which links the
related structures in Q/AP, when there is a match
3
http://uima.apache.org/
665
Figure 2: Basic structural representations using a shallow chunk tree structure for the Q/AP in the running example. Curved
line indicates the tree fragments in the question and its answer passage linked by the relational REL tag.
between the lemmas in Q and AP. We marked the
parent (POS tags) and grand parent (chunk) nodes
of such lemmas by prepending a REL tag.
However, more general semantic relations, e.g.,
derived from the question focus and category, can
be encoded using the REL-FOCUS-<QC> tag,
where <QC> stands for the question class. In
(Severyn et al., 2013b; Severyn et al., 2013a), we
used statistical classifiers to derive question focus
and categories of the question and of the named
entities in the AP. We again mark (i) the focus
chunk in the question and (ii) the AP chunks con-
taining named entities of type compatible with the
question class, by prepending the above tags to
their labels. The compatibility between the cat-
egories of named entities and questions is evalu-
ated with a lookup to a manually predefined map-
ping (see Table 1 in (Severyn et al., 2013b)). We
also prune the trees by removing the nodes beyond
a certain distance (in terms of chunk nodes) from
the REL and REL-FOCUS nodes. This removes
irrelevant information and speeds up learning and
classification. We showed that such model outper-
forms bag-of-words and POS-tag sequence mod-
els (Severyn et al., 2013a).
An example of a Q/AP pair encoded using shal-
low chunk trees is given in Figure 2. Here, for ex-
ample, the lemma ?drink? occurs in both Q and AP
(we highlighted it with a solid line box in the fig-
ure). ?Company? was correctly recognized as a fo-
cus
4
, however it was misclassified as ?HUMAN?
(?HUM?). As no entities of the matching type
?PERSON? were found in the answer by a NER
system, no chunks were marked as REL-FOCUS
on the answer passage side.
We slightly modify the REL-FOCUS encod-
ing into the tree. Instead of prepending REL-
FOCUS-<QC>, we only prepend REL-FOCUS
to the target chunk node, and add a new node
QC as the rightmost child of the chunk node, e.g,
in Figure 2, the focus node would be marked as
REL-FOCUS and the sequence of its children
would be [WP NN HUM]. This modification in-
4
We used the same approach to focus detection and ques-
tion classification used in (Severyn et al., 2013b)
tends to reduce the feature sparsity.
3 LOD for Semantic Structures
We aim at exploiting semantic resources for build-
ing more powerful rerankers. More specifically,
we use structured knowledge about properties of
the objects referred to in a Q/AP pair. A large
amount of knowledge has been made available as
LOD datasets, which can be used for finding addi-
tional semantic links between Q/AP passages.
In the next sections, we (i) formally define novel
semantic links between Q/AP structures that we
introduce in this paper; (ii) provide basic notions
of Linked Open Data along with three of its most
widely used datasets, YAGO, DBpedia and Word-
Net; and, finally, (iii) describe our algorithm to
generate linked Q/AP structures.
3.1 Matching Q/AP Structures: Type Match
We look for token sequences (e.g., complex nomi-
nal groups) in Q/AP pairs that refer to entities and
entity classes related by isa (Eq. 1) and isSubclas-
sOf (Eq. 2) relations and then link them in the
structural Q/AP representations.
isa : entity ? class? {true, false} (1)
isSubclassOf : class? class? {true, false} (2)
Here, entities are all the objects in the world
both real or abstract, while classes are sets of en-
tities that share some common features. Informa-
tion about entities, classes and their relations can
be obtained from the external knowledge sources
such as the LOD resources. isa returns true if an
entity is an element of a class (false otherwise),
while isSubclassOf(class1,class2) returns true if
all elements of class1 belong also to class2.
We refer to the token sequences introduced
above as to anchors and the entities/classes they
refer to as references. We define anchors to be in
a Type Match (TM) relation if the entities/classes
they refer to are in isa or isSubclassOf relation.
More formally, given two anchors a
1
and a
2
be-
longing to two text passages, p
1
and p
2
, respec-
tively, and given an R(a, p) function, which re-
turns a reference of an anchor a in passage p, we
define TM (r
1
, r
2
) as
666
{
isa (r
1
, r
2
) : if isEntity (r
1
) ? isClass (r
2
)
subClassOf (r
1
, r
2
) : if isClass (r
1
) ? isClass (r
2
)
(3)
where r
1
= R(a
1
, p
1
), r
2
= R(a
2
, p
2
) and isEn-
tity(r) and isClass(r) return true if r is an entity or
a class, respectively, and false otherwise. It should
be noted that, due to the ambiguity of natural lan-
guage, the same anchor may have different refer-
ences depending on the context.
3.2 LOD for linking Q/A structures
LOD consists of datasets published online accord-
ing to the Linked Data (LD) principles
5
and avail-
able in open access. LOD knowledge is repre-
sented following the Resource Description Frame-
work (RDF)
6
specification as a set of statements.
A statement is a subject-predicate-object triple,
where predicate denotes the directed relation, e.g.,
hasSurname or owns, between subject and object.
Each object described by RDF, e.g., a class or
an entity, is called a resource and is assigned a
Unique Resource Identifier (URI).
LOD includes a number of common schemas,
i.e., sets of classes and predicates to be reused
when describing knowledge. For example, one
of them is RDF Schema (RDFS)
7
, which contains
predicates rdf:type and rdfs:SubClassOf
similar to the isa and subClassOf functions above.
LOD contains a number of large-scale cross-
domain datasets, e.g., YAGO (Suchanek et al.,
2007) and DBpedia (Bizer et al., 2009). Datasets
created before the emergence of LD, e.g., Word-
Net, are brought into correspondence with the LD
principles and added to the LOD as well.
3.2.1 Algorithm for detecting TM
Algorithm 1 detects n-grams in the Q/AP struc-
tures that are in TM relation and encodes TM
knowledge in the shallow chunk tree representa-
tions of Q/AP pairs. It takes two text passages, P
1
and P
2
, and a LOD knowledge source, LOD
KS
,
as input. We run the algorithm twice, first with
AP as P
1
and Q as P
2
and then vice versa. For
example, P
1
and P
2
in the first run could be, ac-
cording to our running example, Q and AP candi-
date, respectively, and LOD
KS
could be YAGO,
DBpedia or WordNet.
Detecting anchors. getAnchors(P
2
,LOD
KS
)
in line 1 of Algorithm 1 returns all anchors in the
5
http://www.w3.org/DesignIssues/
LinkedData.html
6
http://www.w3.org/TR/rdf-concepts/
7
http://www.w3.org/TR/rdf-schema/
Algorithm 1 Type Match algorithm
Input: P
1
, P
2
- text passages; LOD
KS
- LOD knowledge
source.
1: for all anchor ? getAnchors(P
2
,LOD
KS
) do
2: for all uri ? getURIs(anchor,P
2
,LOD
KS
) do
3: for all type ? getTypes(uri,LOD
KS
) do
4: for all ch ? getChunks(P
1
) do
5: matchedTokens ? checkMatch(ch,
type.labels)
6: if matchedTokens 6= ? then
7: markAsTM(anchor,P
2
.parseTree)
8: markAsTM(matchedTokens,
P
1
.parseTree)
given text passage, P
2
. Depending on LOD
KS
one may have various implementations of this pro-
cedure. For example, when LOD
KS
is Word-
Net, getAnchor returns token subsequences of the
chunks in P
2
of lengths n-k, where n is the number
of tokens in the chunk and k = [1, .., n? 1).
In case when LOD
KS
is YAGO or DBpedia,
we benefit from the fact that both YAGO and DB-
pedia are aligned with Wikipedia on entity level by
construction and we can use the so-called wikifica-
tion tools, e.g., (Milne and Witten, 2009), to detect
the anchors. The wikification tools recognize n-
grams that may denote Wikipedia pages in plain
text and disambiguate them to obtain a unique
Wikipedia page. Such tools determine whether
a certain n-gram may denote a Wikipedia page(s)
by looking it up in a precomputed vocabulary cre-
ated using Wikipedia page titles and internal link
network (Csomai and Mihalcea, 2008; Milne and
Witten, 2009).
Obtaining references. In line 2 of Algorithm 1
for each anchor, we determine the URIs of enti-
ties/classes it refers to in LOD
KS
. Here again,
we have different strategies for different LOD
KS
.
In case of WordNet, we use the all-senses strat-
egy, i.e., getURI procedure returns a set of URIs
of synsets that contain the anchor lemma.
In case when LOD
KS
is YAGO or DBpedia,
we use wikification tools to correctly disambiguate
an anchor to a Wikipedia page. Then, Wikipedia
page URLs may be converted to DBpedia URIs by
substituting the en.wikipedia.org/wiki/
prefix to the dbpedia.org/resource/; and
YAGO URIs by querying it for subjects of the
RDF triples with yago:hasWikipediaUrl
8
as a predicate and Wikipedia URL as an object.
For instance, one of the anchors detected in
the running example AP would be ?Quaker oats?,
8
yago: is a shorthand for the http prefix http://
yago-knowledge.org/resource/
667
a wikification tool would map it to wiki:
Quaker_Oats_Company
9
, and the respective
YAGO URI would be yago:Quaker_Oats_
Company.
Obtaining type information. Given a uri, if it
is an entity, we look for all the classes it belongs
to, or if it is a class, we look for all classes for
which it is a subclass. This process is incorpo-
rated in the getTypes procedure in line 3 of Algo-
rithm 1. We call such classes types. If LOD
KS
is WordNet, then our types are simply the URIs of
the hypernyms of uri. If LOD
KS
is DBpedia or
YAGO, we query these datasets for the values of
the rdf:type and rdfs:subClassOf prop-
erties of the uri (i.e., objects of the triples with uri
as subject and type/subClassOf as predicates) and
add their values (which are also URIs) to the types
set. Then, we recursively repeat the same queries
for each retrieved type URI and add their results to
the types. Finally, the getTypes procedure returns
the resulting types set.
The extracted URIs returned by getTypes are
HTTP ids, however, frequently they have human-
readable names, or labels, specified by the rdfs:
label property. If no label information for a
URI is available, we can create the label by re-
moving the technical information from the type
URI, e.g., http prefix and underscores. type.labels
denotes a set of type human-readable labels for
a specific type. For example, one of the types
extracted for yago:Quaker_Oats_Company
would have label ?company?.
Checking for TM. Further, the checkMatch
procedure checks whether any of the labels in the
type.labels matches any of the chunks in P
1
re-
turned by getChunks, fully or partially (line 5 of
Algorithm 1). Here, getChunks procedure returns
a list of chunks recognized in P
1
by an external
chunker.
More specifically, given a chunk, ch, and a type
label, type.label, checkMatch checks whether the
ch string matches
10
type.label or its last word(s).
If no match is observed, we remove the first to-
ken from ch and repeat the procedure. We stop
when the match is observed or when no tokens
in ch are left. If the match is observed, check-
Match returns all the tokens remaining in ch as
matchedTokens. Otherwise, it returns an empty
set. For example, the question of the running ex-
9
wiki: is a shorthand for the http prefix http://en.
wikipedia.org/wiki/
10
case-insensitive exact string match
ample contains the chunk ?what company?, which
partially matches the human readable ?company?
label of one of the types retrieved for the ?Quaker
oats? anchor from the answer. Our implemen-
tation of the checkMatch procedure would re-
turn ?company? from the question as one of the
matchedTokens.
If the matchedTokens set is not empty,
this means that TM
(
R
(
anchor, P
2
)
, R
(
matchedTokens, P
1
))
in Eq. 3 returns true.
Indeed, a
1
is an anchor and a
2
is the matched-
Tokens sequence (see Eq. 3), and their respective
references, i.e., URI assigned to the anchor and
URI of one of its types, are either in subClassOf
or in isa relation by construction. Naturally, this
is only one of the possible ways to evaluate the
TM function, and it may be noise-prone.
Marking TM in tree structures. Finally,
if the TM match is observed, i.e., matchedTo-
kens is not an empty set, we mark tree substruc-
tures corresponding to the anchor in the struc-
tural representation of P
2
(P
2
.parseTree) and
those corresponding to matchedTokens in that of
P
1
(P
1
.parseTree) as being in a TM relation. In
our running example, we would mark the substruc-
tures corresponding to ?Quaker oats? anchor in the
answer (our P
2
) and the ?company? matchedTo-
ken in the question (our P
1
) shallow syntactic tree
representations. We can encode TM match infor-
mation into a tree in a variety of ways, which we
describe below.
3.2.2 Encoding TM knowledge in the trees
a
1
and a
2
from Eq. 3 are n-grams, therefore they
correspond to the leaf nodes in the shallow syn-
tactic trees of p
1
and p
2
. We denote the set of
their preterminal parents as N
TM
. We consid-
ered the following strategies of encoding TM re-
lation in the trees: (i) TM node (TM
N
). Add leaf
sibling tagged with TM to all the nodes in N
TM
.
(ii) Directed TM node (TM
ND
). Add leaf sib-
ling tagged with TM-CHILD to all the nodes in
N
TM
corresponding to the anchor, and leaf sib-
lings tagged with TM-PARENT to the nodes cor-
responding to matchedTokens. (iii) Focus TM
(TM
NF
). Add leaf siblings to all the nodes in
N
TM
. If matchedTokens is a part of a question
focus label them as TM-FOCUS. Otherwise, la-
bel them as TM. (iv) Combo TM
NDF
. Encode
using the TM
ND
strategy. If matchedTokens is a
part of a question focus label then also add a child
labeled FOCUS to each of the TM labels. Intu-
668
Figure 3: Fragments of a shallow chunk parse tree anno-
tated in TM
ND
mode.
itively, TM
ND
, TM
NF
, TM
NDF
are likely to re-
sult in more expressive patterns. Fig. 3 shows an
example of the TM
ND
annotation.
3.3 Wikipedia-based matching
Lemma matching for detecting REL may result in
low coverage, e.g., it is not able to match differ-
ent variants for the same name. We remedy this
by using Wikipedia link annotation. We consider
two word sequences (in Q and AP, respectively)
that are annotated with the same Wikipedia link
to be in a matching relation. Thus, we add new
REL tags to Q/AP structural representations as de-
scribed in Sec. 2.2.
4 Experiments
We evaluated our different rerankers encoding sev-
eral semantic structures on passage retrieval task,
using a factoid open-domain TREC QA corpus.
4.1 Experimental Setup
TREC QA 2002/2003. In our experiments, we
opted for questions from years 2002 and 2003,
which totals to 824 factoid questions. The
AQUAINT corpus
11
is used for searching the sup-
porting passages.
Pruning. Following (Severyn and Moschitti,
2012) we prune the shallow trees by removing the
nodes beyond distance of 2 from the REL, REL-
FOCUS or TM nodes.
LOD datasets. We used the core RDF distribu-
tion of YAGO2
12
, WordNet 3.0 in RDF
13
, and the
datasets from the 3.9 DBpedia distribution
14
.
Feature Vectors. We used a subset of the sim-
ilarity functions between Q and AP described in
(Severyn et al., 2013b). These are used along
with the structural models. More explicitly: Term-
overlap features: i.e., a cosine similarity over
question/answer, sim
COS
(Q,AP ), where the in-
put vectors are composed of lemma or POS-tag
11
http://catalog.ldc.upenn.edu/
LDC2002T31
12
http://www.mpi-inf.mpg.de/yago-naga/
yago1_yago2/download/yago2/yago2core_
20120109.rdfs.7z
13
http://semanticweb.cs.vu.nl/lod/wn30/
14
http://dbpedia.org/Downloads39
n-grams with n = 1, .., 4. PTK score: i.e., out-
put of the Partial Tree Kernel (PTK), defined in
(Moschitti, 2006), when applied to the structural
representations of Q and AP, sim
PTK
(Q,AP ) =
PTK(Q,AP ) (note that, this is computed within
a pair). PTK defines similarity in terms of the
number of substructures shared by two trees.
Search engine ranking score: the ranking score of
our search engine assigned to AP divided by a nor-
malizing factor.
SVM re-ranker. To train our models, we use
SVM-light-TK
15
, which enables the use of struc-
tural kernels (Moschitti, 2006) in SVM-light
(Joachims, 2002). We use default parameters and
the preference reranking model described in (Sev-
eryn and Moschitti, 2012; Severyn et al., 2013b).
We used PTK and the polynomial kernel of degree
3 on standard features.
Pipeline. We built the entire processing pipeline
on top of the UIMA framework.We included many
off-the-shelf NLP tools wrapping them as UIMA
annotators to perform sentence detection, tok-
enization, NE Recognition, parsing, chunking and
lemmatization. Moreover, we used annotators
for building new sentence representations starting
from tools? annotations and classifiers for question
focus and question class.
Search engines. We adopted Terrier
16
using the
accurate BM25 scoring model with default param-
eters. We trained it on the TREC corpus (3Gb),
containing about 1 million documents. We per-
formed indexing at the paragraph level by splitting
each document into a set of paragraphs, which are
then added to the search index. We retrieve a list of
50 candidate answer passages for each question.
Wikipedia link annotators. We use the
Wikipedia Miner (WM) (Milne and Witten,
2009)
17
tool and the Machine Linking (ML)
18
web-service to annotate Q/AP pairs with links to
Wikipedia. Both tools output annotation confi-
dence. We use all WM and ML annotations with
confidence exceeding 0.2 and 0.05, respectively.
We obtained these figures heuristically, they are
low because we aimed to maximize the Recall of
the Wikipedia link annotators in order to maxi-
15
http://disi.unitn.it/moschitti/
Tree-Kernel.htm
16
http://terrier.org
17
http://sourceforge.net/projects/
wikipedia-miner/files/wikipedia-miner/
wikipedia-miner_1.1, we use only topic detector
module which detects and disambiguates anchors
18
http://www.machinelinking.com/wp
669
System MRR MAP P@1
BM25 28.02?2.94 0.22?0.02 18.17?3.79
CH+V (CoNLL, 2013) 37.45 0.3 27.91
CH+V+QC+TFC
(CoNLL, 2013)
39.49 0.32 30
CH + V 36.82?2.68 0.30?0.02 26.34?2.17
CH + V+ QC+TFC 40.20?1.84 0.33?0.01 30.85?2.35
CH+V+QC+TFC* 40.50?2.32 0.33?0.02 31.46?2.42
Table 1: Baseline systems
mize the number of TMs. In all the experiments,
we used a union of the sets of the annotations pro-
vided by WM and ML.
Metrics. We used common QA metrics: Precision
at rank 1 (P@1), i.e., the percentage of questions
with a correct answer ranked at the first position,
and Mean Reciprocal Rank (MRR). We also report
the Mean Average Precision (MAP). We perform
5-fold cross-validation and report the metrics aver-
aged across all the folds together with the std.dev.
4.2 Baseline Structural Reranking
In these experiments, we evaluated the accuracy
of the following baseline models: BM25 is the
BM25 scoring model, which also provides the ini-
tial ranking; CH+V is a combination of tree struc-
tures encoding Q/AP pairs using relational links
with the feature vector; and CH+V+QC+TFC is
CH+V extended with the semantic categorial links
introduced in (Severyn et al., 2013b).
Table 1 reports the performance of our base-
line systems. The lines marked with (CoNLL,
2013) contain the results we reported in (Sev-
eryn et al., 2013b). Lines four and five report
the performance of the same systems, i.e., CH+V
and CH+V+QC+TFC, after small improvement
and changes. Note that in our last version, we
have a different set of V features than in (CoNLL,
2013). Finally, CH+V+QC+TFC* refers to the
performance of CH+V+QC+TFC with question
type information of semantic REL-FOCUS links
represented as a distinct node (see Section 2.2).
The results show that this modification yields a
slight improvement over the baseline, thus, in
the next experiments, we add LOD knowledge to
CH+V+QC+TFC*.
4.3 Impact of LOD in Semantic Structures
These experiments evaluated the accuracy of the
following models (described in the previous sec-
tions): (i) a system using Wikipedia to establish
the REL links; and (ii) systems which use LOD
knowledge to find type matches (TM).
The first header line of the Table 2 shows which
baseline system was enriched with the TM knowl-
edge. Type column reports the TM encoding strat-
egy employed (see Section 3.2.2). Dataset column
reports which knowledge source was employed to
find TM relations. Here, yago is YAGO2, db is
DBpedia, and wn is WordNet 3.0. The first re-
sult line in Table 2 reports the performance of
the strong CH+V and CH+V+QC+TFC* base-
line systems. Line with the ?wiki? dataset re-
ports on CH+V and CH+V+QC+TFC* using
both Wikipedia link annotations provided by ML
and MW and hard lemma matching to find the re-
lated structures to be marked by REL (see Sec-
tion 3.3 for details of the Wikipedia-based REL
matching). The remainder of the systems is built
on top of the baselines using both hard lemma and
Wikipedia-based matching. We used bold font to
mark the top scores for each encoding strategy.
The tables show that all the systems ex-
ploiting LOD knowledge, excluding those us-
ing DBpedia only, outperform the strong CH+V
and CH+V+QC+TFC* baselines. Note that
CH+V enriched with TM tags performs com-
parably to, and in some cases even outper-
forms, CH+V+QC+TFC*. Compare, for exam-
ple, the outputs of CH+V+TM
NDF
using YAGO,
WordNet and DBpedia knowledge and those of
CH+V+QC+TFC* with no LOD knowledge.
Adding TM tags to the top-performing base-
line system, CH+V+QC+TFC*, typically re-
sults in further increase in performance. The
best-performing system in terms of MRR and
P@1 is CH+V+QC+TFC*+TM
NF
system us-
ing the combination of WordNet and YAGO2 as
source of TM knowledge and Wikipedia for REL-
matching. It outperforms the CH+V+QC+TFC*
baseline by 3.82% and 4.15% in terms of MRR
and P@1, respectively. Regarding MAP, a num-
ber of systems employing YAGO2 in combina-
tion with WordNet and Wikipedia-based REL-
matching obtain 0.37 MAP score thus outperform-
ing the CH+V+QC+TFC* baseline by 4%.
We used paired two-tailed t-test for evaluating
the statistical significance of the results reported in
Table 2. ? and ? correspond to the significance lev-
els of 0.05 and 0.1, respectively. We compared (i)
the results in the wiki line to those in the none line;
and (ii) the results for the TM systems to those in
the wiki line.
The table shows that we typically obtain bet-
ter results when using YAGO2 and/or WordNet.
In our intuition this is due to the fact that these
resources are large-scale, have fine-grained class
670
Type Dataset CH + V CH + V + QC + TFC*
MRR MAP P@1 MRR MAP P@1
- none 36.82?2.68 0.30?0.02 26.34?2.17 40.50?2.32 0.33?0.02 31.46?2.42
- wiki 39.17?1.29? 0.31?0.01? 28.66?1.43? 41.33?1.17 0.34?0.01 31.46?1.40
TM
N
db 40.60?1.88 0.33?0.01? 31.10?2.99? 40.80?1.01 0.34?0.01 30.37?1.90
TM
N
wn 41.39?1.96? 0.33?0.01? 31.34?2.94 42.43?0.56 0.35?0.01 32.80?0.67
TM
N
wn+db 40.85?1.52? 0.33?0.01? 30.37?2.34 42.37?1.12 0.35?0.01 32.44?2.64
TM
N
yago 40.71?2.07 0.33?0.03? 30.24?2.09? 43.28?1.91? 0.36?0.01? 33.90?2.75
TM
N
yago+db 41.25?1.57? 0.34?0.02? 31.10?1.88? 42.39?1.83 0.35?0.01 32.93?3.14
TM
N
yago+wn 42.01?2.26? 0.34?0.02? 32.07?3.04? 43.98?1.08? 0.36?0.01? 35.24?1.46?
TM
N
yago+wn+db 41.52?1.85? 0.34?0.02? 30.98?2.71? 43.13?1.38 0.36?0.01 33.66?2.77
TM
NF
db 40.67?1.94? 0.33?0.01? 30.85?2.22? 41.43?0.70 0.35?0.01 31.22?1.09
TM
NF
wn 40.95?2.27? 0.33?0.01? 30.98?3.74 42.37?0.98 0.35?0.01 32.56?1.76
TM
NF
wn+db 40.84?2.18? 0.34?0.01? 30.73?3.04 43.08?0.83? 0.36?0.01? 33.54?1.29?
TM
NF
yago 42.01?2.44? 0.34?0.02? 32.07?3.01? 43.82?2.36? 0.36?0.02? 34.88?3.35
TM
NF
yago+db 41.32?1.70? 0.34?0.02? 31.10?2.48? 43.19?1.17? 0.36?0.01? 33.90?1.86
TM
NF
yago+wn 41.69?1.66? 0.34?0.02? 31.10?2.44? 44.32?0.70? 0.36?0.01? 35.61?1.11?
TM
NF
yago+wn+db 41.56?1.41? 0.34?0.02? 30.85?2.22? 43.79?0.73? 0.37?0.01? 34.88?1.69?
TM
ND
db 40.37?1.87 0.33?0.01? 30.37?2.17 41.58?1.02 0.35?0.01? 31.46?1.59
TM
ND
wn 41.13?2.14? 0.33?0.01? 30.73?2.75 42.19?1.39 0.35?0.01 32.32?1.36
TM
ND
wn+db 41.28?1.03? 0.34?0.01? 30.73?0.82? 42.37?1.16 0.36?0.01 32.44?2.71
TM
ND
yago 42.11?3.24? 0.34?0.02? 32.07?4.06? 44.04?2.05? 0.36?0.01? 34.63?2.17?
TM
ND
yago+db 42.28?2.01? 0.35?0.01? 32.44?1.99? 43.77?2.02? 0.37?0.01? 34.27?2.42
TM
ND
yago+wn 42.96?1.45? 0.35?0.01? 33.05?2.04? 44.25?1.32? 0.37?0.00? 34.76?1.61?
TM
ND
yago+wn+db 42.56?1.25? 0.35?0.01? 32.56?1.91? 43.91?1.01? 0.37?0.01? 34.63?1.32?
TM
NDF
db 40.40?1.93? 0.33?0.01? 30.49?1.78? 41.85?1.05 0.35?0.01? 31.83?0.80
TM
NDF
wn 40.84?1.69? 0.33?0.01? 30.49?2.24 41.89?0.99 0.35?0.01 31.71?0.86
TM
NDF
wn+db 41.14?1.29? 0.34?0.01? 30.73?1.40? 42.31?0.92 0.36?0.01 32.32?2.36
TM
NDF
yago 42.31?2.57? 0.35?0.02? 32.68?3.01? 44.22?2.38? 0.37?0.02? 35.00?2.88?
TM
NDF
yago+db 41.96?1.82? 0.35?0.01? 32.32?2.24? 43.82?1.95? 0.37?0.01? 34.51?2.39?
TM
NDF
yago+wn 42.80?1.19? 0.35?0.01? 33.17?1.86? 43.91?0.98? 0.37?0.01? 34.63?0.90?
TM
NDF
yago+wn+db 43.15?0.93? 0.35?0.01? 33.78?1.59? 43.96?0.94? 0.37?0.01? 34.88?1.69?
Table 2: Results in 5-fold cross-validation on TREC QA corpus
taxonomy and contain many synonymous labels
per class/entity thus allowing us to have a good
coverage with TM-links. DBpedia ontology that
we employed in the db experiments is more shal-
low and contains fewer labels for classes, there-
fore the amount of discovered TM matches is
not always sufficient for increasing performance.
YAGO2 provides better coverage for TM relations
between entities and their classes, while Word-
Net contains more relations between classes
19
.
Note that in (Severyn and Moschitti, 2012), we
also used supersenses of WordNet (unsuccess-
fully) whereas here we use hypernymy relations
and a different technique to incorporate semantic
match into the tree structures.
Different TM-knowledge encoding strategies,
TM
N
, TM
ND
, TM
NF
, TM
NDF
produce small
changes in accuracy. We believe, that the differ-
ence between them would become more signifi-
cant when experimenting with larger corpora.
5 Conclusions
This paper proposes syntactic structures whose
nodes are enriched with semantic information
from statistical classifiers and knowledge from
LOD. In particular, YAGO, DBpedia and Word-
Net are used to match and generalize constituents
from QA pairs: such matches are then used in
19
We consider the WordNet synsets to be classes in the
scope of our experiments
syntactic/semantic structures. The experiments
with TREC QA and the above representations
also combined with traditional features greatly im-
prove over a strong IR baseline, e.g., 96% on
BM25, and on previous state-of-the-art rerank-
ing models, up to 15.4% (relative improvement)
in P@1. In particular, differently from previous
work, our models can effectively use semantic
knowledge in statistical learning to rank methods.
These promising results open interesting future
directions in designing novel semantic structures
and using innovative semantic representations in
learning algorithms.
Acknowledgments
This research is partially supported by the EU?s 7
th
Framework Program (FP7/2007-2013) (#288024
LIMOSINE project) and by a Shared University
Research award from the IBM Watson Research
Center - Yorktown Heights, USA and the IBM
Center for Advanced Studies of Trento, Italy. The
third author is supported by the Google Europe
Fellowship 2013 award in Machine Learning.
References
Matthew W. Bilotti, Jonathan L. Elsas, Jaime Car-
bonell, and Eric Nyberg. 2010. Rank learning
for factoid question answering with linguistic and
semantic constraints. In Proceedings of the 19th
671
ACM international Conference on Information and
Knowledge Management (CIKM), pages 459?468.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
S?oren Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. Dbpedia - a crys-
tallization point for the web of data. Web Seman-
tics: Science, Services and Agents on the World Wide
Web, 7(3):154?165, September.
Andras Csomai and Rada Mihalcea. 2008. Linking
documents to encyclopedic knowledge. IEEE Intel-
ligent Systems, 23(5):34?41.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John
Prager, Nico Schlaefer, and Chris Welty. 2010.
Building watson: An overview of the deepqa
project. AI Magazine, 31(3).
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining
(KDD), pages 133?142. ACM.
Aditya Kalyanpur, J William Murdock, James Fan, and
Christopher Welty. 2011. Leveraging community-
built knowledge for type coercion in question an-
swering. In The Semantic Web?ISWC 2011, pages
144?156. Springer.
David Milne and Ian H Witten. 2009. An open-source
toolkit for mining wikipedia. In New Zealand Com-
puter Science Research Student Conference (NZC-
SRSC).
Alessandro Moschitti and Silvia Quarteroni. 2008.
Kernels on linguistic structures for answer extrac-
tion. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguis-
tics on Human Language Technologies: Short Pa-
pers (ACL), pages 113?116.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for ques-
tion/answer classification. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics (ACL), pages 776?783.
Alessandro Moschitti. 2006. Efficient convolution
kernels for dependency and constituent syntactic
trees. In Proceedings of the 17th European Confer-
ence on Machine Learning (ECML), pages 318?329.
Springer.
Alessandro Moschitti. 2009. Syntactic and seman-
tic kernels for short text pair categorization. In
Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL), pages 576?584. Association for
Computational Linguistics.
J William Murdock, Aditya Kalyanpur, Chris Welty,
James Fan, David A Ferrucci, DC Gondek, Lei
Zhang, and Hiroshi Kanayama. 2012. Typing can-
didate answers using type coercion. IBM Journal of
Research and Development, 56(3.4):7?1.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In Proceedings of the 35th in-
ternational ACM SIGIR conference on Research and
development in information retrieval (SIGIR), pages
741?750. ACM.
Aliaksei Severyn and Alessandro Moschitti. 2013. Au-
tomatic feature engineering for answer selection and
extraction. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 458?467.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013a. Building structures from clas-
sifiers for passage reranking. In Proceedings of the
22nd ACM international conference on Conference
on information & knowledge management (CIKM),
pages 969?978. ACM.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013b. Learning adaptable patterns for
passage reranking. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL).
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international con-
ference on World Wide Web (WWW), pages 697?706.
ACM Press.
Ellen M Voorhees. 2001. Overview of the TREC
2001 Question Answering Track. In Proceedings of
TREC, pages 42?51.
672
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 714?718,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning Semantic Textual Similarity with Structural Representations
Aliaksei Severyn(1) and Massimo Nicosia(1) and Alessandro Moschitti1,2
(1)DISI, University of Trento, 38123 Povo (TN), Italy
{severyn,m.nicosia,moschitti}@disi.unitn.it
(2)QCRI, Qatar Foundation, Doha, Qatar
amoschitti@qf.org.qa
Abstract
Measuring semantic textual similarity
(STS) is at the cornerstone of many NLP
applications. Different from the major-
ity of approaches, where a large number
of pairwise similarity features are used to
represent a text pair, our model features
the following: (i) it directly encodes input
texts into relational syntactic structures;
(ii) relies on tree kernels to handle feature
engineering automatically; (iii) combines
both structural and feature vector repre-
sentations in a single scoring model, i.e.,
in Support Vector Regression (SVR); and
(iv) delivers significant improvement over
the best STS systems.
1 Introduction
In STS the goal is to learn a scoring model that
given a pair of two short texts returns a similar-
ity score that correlates with human judgement.
Hence, the key aspect of having an accurate STS
framework is the design of features that can ade-
quately represent various aspects of the similarity
between texts, e.g., using lexical, syntactic and se-
mantic similarity metrics.
The majority of approaches treat input text pairs
as feature vectors where each feature is a score
corresponding to a certain type of similarity. This
approach is conceptually easy to implement and
the STS shared task at SemEval 2012 (Agirre et
al., 2012) (STS-2012) has shown that the best sys-
tems were built following this idea, i.e., a num-
ber of features encoding similarity of an input text
pair were combined in a single scoring model, e.g.,
SVR. Nevertheless, one limitation of using only
similarity features to represent a text pair is that of
low representation power.
The novelty of our approach is that we treat the
input text pairs as structural objects and rely on the
power of kernel learning to extract relevant struc-
tures. To link the documents in a pair we mark the
nodes in the related structures with a special rela-
tional tag. This way effective structural relational
patterns are implicitly encoded in the trees and
can be automatically learned by the kernel-based
machines. We combine our relational structural
model with the features from two best systems of
STS-2012. Finally, we use the approach of classi-
fier stacking to combine several structural models
into the feature vector representation.
The contribution of this paper is as follows: (i) it
provides a convincing evidence that adding struc-
tural features automatically extracted by structural
kernels yields a significant improvement in accu-
racy; (ii) we define a combination kernel that inte-
grates both structural and feature vector represen-
tations within a single scoring model, e.g., Sup-
port Vector Regression; (iii) we provide a sim-
ple way to construct relational structural models
that can be built using off-the-shelf NLP tools;
(iv) we experiment with four structural representa-
tions and show that constituency and dependency
trees represent the best source for learning struc-
tural relationships; and (v) using a classifier stack-
ing approach, structural models can be easily com-
bined and integrated into existing feature-based
STS models.
2 Structural Relational Similarity
The approach of relating pairs of input struc-
tures by learning predictable syntactic transforma-
tions has shown to deliver state-of-the-art results
in question answering, recognizing textual entail-
ment, and paraphrase detection, e.g. (Wang et al,
2007; Wang and Manning, 2010; Heilman and
Smith, 2010). Previous work relied on fairly com-
plex approaches, e.g. applying quasi-synchronous
grammar formalism and variations of tree edit dis-
tance alignments, to extract syntactic patterns re-
lating pairs of input structures. Our approach
is conceptually simpler, as it regards the prob-
lem within the kernel learning framework, where
we first encode salient syntactic/semantic proper-
714
ties of the input text pairs into tree structures and
rely on tree kernels to automatically generate rich
feature spaces. This work extends in several di-
rections our earlier work in question answering,
e.g., (Moschitti et al, 2007; Moschitti and Quar-
teroni, 2008), in textual entailment recognition,
e.g., (Moschitti and Zanzotto, 2007), and more in
general in relational text categorization (Moschitti,
2008; Severyn and Moschitti, 2012).
In this section we describe: (i) a kernel frame-
work to combine structural and vector models; (ii)
structural kernels to handle feature engineering;
and (iii) suitable structural representations for re-
lational learning.
2.1 Structural Kernel Learning
In supervised learning, given labeled data
{(xi, y i)}ni=1, the goal is to estimate a decision
function h(x) = y that maps input examples to
their targets. A conventional approach is to rep-
resent a pair of texts as a set of similarity fea-
tures {fi}, s.t. the predictions are computed as
h(x) = w ? x = ?iwifi, where w is the model
weight vector. Hence, the learning problem boils
down to estimating individual weights of each of
the similarity features fi. One downside of such
approach is that a great deal of similarity infor-
mation encoded in a given text pair is lost when
modeled by single real-valued scores.
A more versatile approach in terms of the input
representation relies on kernels. In a typical kernel
learning approach, e.g., SVM, the prediction func-
tion for a test input x takes on the following form
h(x) =
?
i ?iyiK(x,xi), where ?i are the model
parameters estimated from the training data, yi are
target variables, xi are support vectors, andK(?, ?)
is a kernel function.
To encode both structural representation and
similarity feature vectors of a given text pair in a
single model we define each document in a pair
to be composed of a tree and a vector: ?t, v?.
To compute a kernel between two text pairs xi
and xj we define the following all-vs-all kernel,
where all possible combinations of components,
x(1) and x(2), from each text pair are consid-
ered: K(xi,xj) = K(x(1)i ,x(1)j )+K(x(1)i ,x(2)j )+
K(x(2)i ,x
(1)
j ) + K(x
(2)
i ,x
(2)
j ). Each of the ker-
nel computations K can be broken down into
the following: K(x(1),x(2)) = KTK(t(1), t(2)) +
Kfvec(v(1), v(2)), where KTK computes a struc-
tural kernel and Kfvec is a kernel over feature vec-
tors, e.g., linear, polynomial or RBF, etc. Further
in the text we refer to structural tree kernel models
as TK and explicit feature vector representation as
fvec.
Having defined a way to jointly model text pairs
using structural TK representations along with the
similarity features fvec, we next briefly review
tree kernels and our relational structures.
2.2 Tree Kernels
We use tree structures as our base representation
since they provide sufficient flexibility in repre-
sentation and allow for easier feature extraction
than, for example, graph structures. Hence, we
rely on tree kernels to compute KTK(?, ?). Given
two trees it evaluates the number of substructures
(or fragments) they have in common, i.e., it is a
measure of their overlap. Different TK functions
are characterized by alternative fragment defini-
tions. In particular, we focus on the Syntactic Tree
kernel (STK) (Collins and Duffy, 2002) and a Par-
tial Tree Kernel (PTK) (Moschitti, 2006).
STK generates all possible substructures rooted in
each node of the tree with the constraint that pro-
duction rules can not be broken (i.e., any node in a
tree fragment must include either all or none of its
children).
PTK can be more effectively applied to both con-
stituency and dependency parse trees. It general-
izes STK as the fragments it generates can contain
any subset of nodes, i.e., PTK allows for breaking
the production rules and generating an extremely
rich feature space, which results in higher gener-
alization ability.
2.3 Structural representations
In this paper, we define simple-to-build relational
structures based on: (i) a shallow syntactic tree,
(ii) constituency, (iii) dependency and (iv) phrase-
dependency trees.
Shallow tree is a two-level syntactic hierarchy
built from word lemmas (leaves), part-of-speech
tags (preterminals) that are further organized into
chunks. It was shown to significantly outperform
feature vector baselines for modeling relationships
between question answer pairs (Severyn and Mos-
chitti, 2012).
Constituency tree. While shallow syntactic pars-
ing is very fast, here we consider using con-
stituency structures as a potentially richer source
of syntactic/semantic information.
Dependency tree. We propose to use depen-
dency relations between words to derive an alter-
native structural representation. In particular, de-
715
Figure 1: A phrase dependency-based structural representation of a text pair (s1, s2): A woman with
a knife is slicing a pepper (s1) vs. A women slicing green pepper (s2) with a high semantic similarity
(human judgement score 4.0 out of 5.0). Related tree fragments are linked with a REL tag.
pendency relations are used to link words in a way
that they are always at the leaf level. This reorder-
ing of the nodes helps to avoid the situation where
nodes with words tend to form long chains. This
is essential for PTK to extract meaningful frag-
ments. We also plug part-of-speech tags between
the word nodes and nodes carrying their grammat-
ical role.
Phrase-dependency tree. We explore a phrase-
dependency tree similar to the one defined in (Wu
et al, 2009). It represents an alternative struc-
ture derived from the dependency tree, where the
dependency relations between words belonging to
the same phrase (chunk) are collapsed in a unified
node. Different from (Wu et al, 2009), the col-
lapsed nodes are stored as a shallow subtree rooted
at the unified node. This node organization is par-
ticularly suitable for PTK that effectively runs a
sequence kernel on the tree fragments inside each
chunk subtree. Fig 1 gives an example of our vari-
ation of a phrase dependency tree.
As a final consideration, if a document contains
multiple sentences they are merged in a single tree
with a common root. To encode the structural
relationships between documents in a pair a spe-
cial REL tag is used to link the related structures.
We adopt a simple strategy to establish such links:
words from two documents that have a common
lemma get their parents (POS tags) and grandpar-
ents, non-terminals, marked with a REL tag.
3 Pairwise similarity features.
Along with the direct representation of input text
pairs as structural objects our framework is also
capable of encoding pairwise similarity feature
vectors (fvec), which we describe below.
Baseline features. (base) We adopt similar-
ity features from two best performing systems
of STS-2012, which were publicly released1:
namely, the Takelab2 system (S?aric? et al, 2012)
and the UKP Lab?s system3 (Bar et al, 2012).
Both systems represent input texts with similarity
features combining multiple text similarity mea-
sures of varying complexity.
UKP (U) provides metrics based on match-
ing of character, word n-grams and common
subsequences. It also includes features derived
from Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007) and aggregation of word sim-
ilarity based on lexical-semantic resources, e.g.,
WordNet. In total it provides 18 features.
Takelab (T) includes n-gram matching of vary-
ing size, weighted word matching, length differ-
ence, WordNet similarity and vector space simi-
larity where pairs of input sentences are mapped
into Latent Semantic Analysis (LSA) space. The
features are computed over several sentence rep-
resentations where stop words are removed and/or
lemmas are used in place of raw tokens. The total
number of Takelab?s features is 21. The combined
system consists of 39 features.
Additional features. We also augment the U and
T feature sets, with an additional set of features (A)
which includes: a cosine similarity scores com-
puted over (i) n-grams of part-of-speech tags (up
to 4-grams), (ii) SuperSense tags (Ciaramita and
1Note that only a subset of the features used in the fi-
nal evaluation was released, which results in lower accuracy
when compared to the official rankings.
2http://takelab.fer.hr/sts/
3https://code.google.com/p/dkpro-similarity-
asl/wiki/SemEval2013
716
Altun, 2006), (iii) named entities, (iv) dependency
triplets, and (v) PTK syntactic similarity scores
computed between documents in a pair, where as
input representations we use raw dependency and
constituency trees. To alleviate the problem of do-
main adaptation, where datasets used for training
and testing are drawn from different sources, we
include additional features to represent the com-
bined text of a pair: (i) bags (B) of lemmas, de-
pendency triplets, production rules (from the con-
stituency parse tree) and a normalized length of
the entire pair; and (ii) a manually encoded cor-
pus type (M), where we use a binary feature with
a non-zero entry corresponding to a dataset type.
This helps the learning algorithm to learn implic-
itly the individual properties of each dataset.
Stacking. To integrate multiple TK representa-
tions into a single model we apply a classifier
stacking approach (Fast and Jensen, 2008). Each
of the learned TK models is used to generate pre-
dictions which are then plugged as features into
the final fvec representation, s.t. the final model
uses only explicit feature vector representation. To
obtain prediction scores, we apply 5-fold cross-
validation scheme, s.t. for each of the held-out
folds we obtain independent predictions.
4 Experiments
We present the results of our model tested on the
data from the Core STS task at SemEval 2012.
4.1 Setup
Data. To compare with the best systems of the
STS-2012 we followed the same setup used in
the final evaluation, where 3 datasets (MSRpar,
MSRvid and SMTeuroparl) are used for training
and 5 for testing (two ?surprise? datasets were
added: OnWN and SMTnews). We use the entire
training data to obtain a single model for making
predictions on each test set.
Software. To encode TK models along with the
similarity feature vectors into a single regression
scoring model, we use an SVR framework imple-
mented in SVM-Light-TK4. We use the follow-
ing parameter settings -t 5 -F 1 -W A -C
+, which specifies a combination of trees and fea-
ture vectors (-C +), STK over trees (-F 1) (-F
3 for PTK) computed in all-vs-all mode (-W A)
and polynomial kernel of degree 3 for the feature
vector (active by default).
4http://disi.unitn.it/moschitti/Tree-Kernel.htm
Metrics. We report the following metrics em-
ployed in the final evaluation: Pearson correlation
for individual test sets5 and Mean ? an average
score weighted by the test set size.
4.2 Results
Table 1 summarizes the results of combining TK
models with a strong feature vector model. We
test structures defined in Sec. 2.3 when using STK
and PTK. The results show that: (i) combining
all three features sets (U, T, A) provides a strong
baseline system that we attempt to further improve
with our relational structures; (ii) the generality of
PTK provides an advantage over STK for learn-
ing more versatile models; (iii) constituency and
dependency representations seem to perform bet-
ter than shallow and phrase-dependency trees; (iv)
using structures with no relational linking does not
work; (v) TK models provide a far superior source
of structural similarity than U + T + A that already
includes PTK similarity scores as features, and fi-
nally (vi) the domain adaptation problem can be
addressed by including corpus specific features,
which leads to a large improvement over the pre-
vious best system.
5 Conclusions and Future Work
We have presented an approach where text pairs
are directly treated as structural objects. This pro-
vides a much richer representation for the learning
algorithm to extract useful syntactic and shallow
semantic patterns. We have provided an exten-
sive experimental study of four different structural
representations, e.g. shallow, constituency, de-
pendency and phrase-dependency trees using STK
and PTK. The novelty of our approach is that it
goes beyond a simple combination of tree kernels
with feature vectors as: (i) it directly encodes input
text pairs into relationally linked structures; (ii) the
learned structural models are used to obtain pre-
diction scores thus making it easy to plug into ex-
isting feature-based models, e.g. via stacking; (iii)
to our knowledge, this work is the first to apply
structural kernels and combinations in a regres-
sion setting; and (iv) our model achieves the state
of the art in STS largely improving the best pre-
vious systems. Our structural learning approach
to STS is conceptually simple and does not re-
quire additional linguistic sources other than off-
the-shelf syntactic parsers. It is particularly suit-
able for NLP tasks where the input domain comes
5we also report the results for a concatenation of all five
test sets (ALL)
717
Experiment U T A S C D P STK PTK B M ALL Mean MSRp MSRv SMTe OnWN SMTn
fvec
model
? .7060 .6087 .6080 .8390 .2540 .6820 .4470
? .7589 .6863 .6814 .8637 .4950 .7091 .5395
? ? .8079 .7161 .7134 .8837 .5519 .7343 .5607
? ? ? .8187 .7137 .7157 .8833 .5131 .7355 .5809
TK
models
with STK
and PTK
? ? ? ? ? .8261 .6982 .7026 .8870 .4807 .7258 .5333
? ? ? ? ? .8326 .6970 .7020 .8925 .4826 .7190 .5253
? ? ? ? ? .8341 .7024 .7086 .8921 .4671 .7319 .5495
? ? ? ? ? .8211 .6693 .6994 .8903 .2980 .7035 .5603
? ? ? ? ? .8362 .7026 .6927 .8896 .5282 .7144 .5485
? ? ? ? ? .8458 .7047 .6935 .8953 .5080 .7101 .5834
? ? ? ? ? .8468 .6954 .6717 .8902 .4652 .7089 .6133
? ? ? ? ? .8326 .6693 .7108 .8879 .4922 .7215 .5156
REL tag ? ? ? ? .8218 .6899 .6644 .8726 .4846 .7228 .5684? ? ? ? .8250 .7000 .6806 .8822 .5171 .7145 .5769
domain
adaptation
? ? ? ? ? .8539 .7132 .6993 .9005 .4772 .7189 .6481
? ? ? ? ? .8529 .7249 .7080 .8984 .5142 .7263 .6700
? ? ? ? ? ? .8546 .7156 .6989 .8979 .4884 .7181 .6609
? ? ? ? ? ? .8810 .7416 .7210 .8971 .5912 .7328 .6778
UKP (best system of STS-2012) .8239 .6773 .6830 .8739 .5280 .6641 .4937
Table 1: Results on STS-2012. First set of experiments studies the combination of fvec models from
UKP (U), Takelab (T) and (A). Next we show results for four structural representations: shallow (S),
constituency (C), dependency (D) and phrase-dependency (P) trees with STK and PTK; next row set
demonstrates the necessity of relational linking for two best structures, i.e. C and D (empty circle denotes
a structures with no relational linking.); finally, domain adaptation via bags of features (B) of the entire
pair and (M) manually encoded dataset type show the state of the art results.
as pairs of objects, e.g., question answering, para-
phrasing and recognizing textual entailment.
6 Acknowledgements
This research is supported by the EU?s Seventh
Framework Program (FP7/2007-2013) under the
#288024 LIMOSINE project.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-
Agirre. 2012. Semeval-2012 task 6: A pilot on se-
mantic textual similarity. In *SEM.
Daniel Bar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In SemEval.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
EMNLP.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In
ACL.
Andrew S. Fast and David Jensen. 2008. Why stacked
models perform effective collective classification.
In ICDM.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In IJCAI.
Michael Heilman and Noah A. Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In NAACL.
Alessandro Moschitti and Silvia Quarteroni. 2008.
Kernels on linguistic structures for answer extrac-
tion. In ACL.
Alessandro Moschitti and Fabio Massimo Zanzotto.
2007. Fast and effective kernels for relational learn-
ing from texts. In ICML.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploit-
ing syntactic and shallow semantic kernels for ques-
tion/answer classification. In ACL.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In ECML.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In SIGIR.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic?. 2012. Takelab: Systems
for measuring semantic text similarity. In SemEval.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question answer-
ing. In ACL.
Mengqiu Wang, Noah A. Smith, and Teruko Mitaura.
2007. What is the jeopardy model? a quasi-
synchronous grammar for qa. In EMNLP.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In EMNLP.
718
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1252?1261,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Opinion Mining on YouTube
Aliaksei Severyn
1
, Alessandro Moschitti
3,1
,
Olga Uryupina
1
, Barbara Plank
2
, Katja Filippova
4
1
DISI - University of Trento,
2
CLT - University of Copenhagen,
3
Qatar Computing Research Institute,
4
Google Inc.
severyn@disi.unitn.it, amoschitti@qf.org.qa,
uryupina@gmail.com, bplank@cst.dk, katjaf@google.com
Abstract
This paper defines a systematic approach
to Opinion Mining (OM) on YouTube
comments by (i) modeling classifiers for
predicting the opinion polarity and the
type of comment and (ii) proposing ro-
bust shallow syntactic structures for im-
proving model adaptability. We rely on the
tree kernel technology to automatically ex-
tract and learn features with better gener-
alization power than bag-of-words. An ex-
tensive empirical evaluation on our manu-
ally annotated YouTube comments corpus
shows a high classification accuracy and
highlights the benefits of structural mod-
els in a cross-domain setting.
1 Introduction
Social media such as Twitter, Facebook or
YouTube contain rapidly changing information
generated by millions of users that can dramati-
cally affect the reputation of a person or an orga-
nization. This raises the importance of automatic
extraction of sentiments and opinions expressed in
social media.
YouTube is a unique environment, just like
Twitter, but probably even richer: multi-modal,
with a social graph, and discussions between peo-
ple sharing an interest. Hence, doing sentiment
research in such an environment is highly relevant
for the community. While the linguistic conven-
tions used on Twitter and YouTube indeed show
similarities (Baldwin et al, 2013), focusing on
YouTube allows to exploit context information,
possibly also multi-modal information, not avail-
able in isolated tweets, thus rendering it a valuable
resource for the future research.
Nevertheless, there is almost no work showing
effective OM on YouTube comments. To the best
of our knowledge, the only exception is given by
the classification system of YouTube comments
proposed by Siersdorfer et al (2010).
While previous state-of-the-art models for opin-
ion classification have been successfully applied
to traditional corpora (Pang and Lee, 2008),
YouTube comments pose additional challenges:
(i) polarity words can refer to either video or prod-
uct while expressing contrasting sentiments; (ii)
many comments are unrelated or contain spam;
and (iii) learning supervised models requires train-
ing data for each different YouTube domain, e.g.,
tablets, automobiles, etc. For example, consider a
typical comment on a YouTube review video about
a Motorola Xoom tablet:
this guy really puts a negative spin on
this , and I ?m not sure why , this seems
crazy fast , and I ?m not entirely sure
why his pinch to zoom his laggy all the
other xoom reviews
The comment contains a product name xoom and
some negative expressions, thus, a bag-of-words
model would derive a negative polarity for this
product. In contrast, the opinion towards the prod-
uct is neutral as the negative sentiment is ex-
pressed towards the video. Similarly, the follow-
ing comment:
iPad 2 is better. the superior apps just
destroy the xoom.
contains two positive and one negative word, yet
the sentiment towards the product is negative (the
negative word destroy refers to Xoom). Clearly,
the bag-of-words lacks the structural information
linking the sentiment with the target product.
In this paper, we carry out a systematic study on
OM targeting YouTube comments; its contribution
is three-fold: firstly, to solve the problems outlined
above, we define a classification schema, which
separates spam and not related comments from the
informative ones, which are, in turn, further cate-
gorized into video- or product-related comments
1252
(type classification). At the final stage, differ-
ent classifiers assign polarity (positive, negative or
neutral) to each type of a meaningful comment.
This allows us to filter out irrelevant comments,
providing accurate OM distinguishing comments
about the video and the target product.
The second contribution of the paper is the cre-
ation and annotation (by an expert coder) of a
comment corpus containing 35k manually labeled
comments for two product YouTube domains:
tablets and automobiles.
1
It is the first manu-
ally annotated corpus that enables researchers to
use supervised methods on YouTube for comment
classification and opinion analysis. The comments
from different product domains exhibit different
properties (cf. Sec. 5.2), which give the possibility
to study the domain adaptability of the supervised
models by training on one category and testing on
the other (and vice versa).
The third contribution of the paper is a novel
structural representation, based on shallow syn-
tactic trees enriched with conceptual information,
i.e., tags generalizing the specific topic of the
video, e.g., iPad, Kindle, Toyota Camry. Given the
complexity and the novelty of the task, we exploit
structural kernels to automatically engineer novel
features. In particular, we define an efficient tree
kernel derived from the Partial Tree Kernel, (Mos-
chitti, 2006a), suitable for encoding structural rep-
resentation of comments into Support Vector Ma-
chines (SVMs). Finally, our results show that our
models are adaptable, especially when the struc-
tural information is used. Structural models gen-
erally improve on both tasks ? polarity and type
classification ? yielding up to 30% of relative im-
provement, when little data is available. Hence,
the impractical task of annotating data for each
YouTube category can be mitigated by the use of
models that adapt better across domains.
2 Related work
Most prior work on more general OM has been
carried out on more standardized forms of text,
such as consumer reviews or newswire. The most
commonly used datasets include: the MPQA cor-
pus of news documents (Wilson et al, 2005), web
customer review data (Hu and Liu, 2004), Ama-
zon review data (Blitzer et al, 2007), the JDPA
1
The corpus and the annotation guidelines are pub-
licly available at: http://projects.disi.unitn.
it/iKernels/projects/sentube/
corpus of blogs (Kessler et al, 2010), etc. The
aforementioned corpora are, however, only par-
tially suitable for developing models on social
media, since the informal text poses additional
challenges for Information Extraction and Natu-
ral Language Processing. Similar to Twitter, most
YouTube comments are very short, the language
is informal with numerous accidental and deliber-
ate errors and grammatical inconsistencies, which
makes previous corpora less suitable to train mod-
els for OM on YouTube. A recent study focuses on
sentiment analysis for Twitter (Pak and Paroubek,
2010), however, their corpus was compiled auto-
matically by searching for emoticons expressing
positive and negative sentiment only.
Siersdorfer et al (2010) focus on exploiting user
ratings (counts of ?thumbs up/down? as flagged by
other users) of YouTube video comments to train
classifiers to predict the community acceptance of
new comments. Hence, their goal is different: pre-
dicting comment ratings, rather than predicting the
sentiment expressed in a YouTube comment or its
information content. Exploiting the information
from user ratings is a feature that we have not ex-
ploited thus far, but we believe that it is a valuable
feature to use in future work.
Most of the previous work on supervised senti-
ment analysis use feature vectors to encode doc-
uments. While a few successful attempts have
been made to use more involved linguistic anal-
ysis for opinion mining, such as dependency
trees with latent nodes (T?ackstr?om and McDonald,
2011) and syntactic parse trees with vectorized
nodes (Socher et al, 2011), recently, a comprehen-
sive study by Wang and Manning (2012) showed
that a simple model using bigrams and SVMs per-
forms on par with more complex models.
In contrast, we show that adding structural fea-
tures from syntactic trees is particularly useful for
the cross-domain setting. They help to build a sys-
tem that is more robust across domains. Therefore,
rather than trying to build a specialized system
for every new target domain, as it has been done
in most prior work on domain adaptation (Blitzer
et al, 2007; Daum?e, 2007), the domain adapta-
tion problem boils down to finding a more robust
system (S?gaard and Johannsen, 2012; Plank and
Moschitti, 2013). This is in line with recent ad-
vances in parsing the web (Petrov and McDonald,
2012), where participants where asked to build a
single system able to cope with different yet re-
1253
lated domains.
Our approach relies on robust syntactic struc-
tures to automatically generate patterns that adapt
better. These representations have been inspired
by the semantic models developed for Ques-
tion Answering (Moschitti, 2008; Severyn and
Moschitti, 2012; Severyn and Moschitti, 2013)
and Semantic Textual Similarity (Severyn et al,
2013). Moreover, we introduce additional tags,
e.g., video concepts, polarity and negation words,
to achieve better generalization across different
domains where the word distribution and vocab-
ulary changes.
3 Representations and models
Our approach to OM on YouTube relies on the
design of classifiers to predict comment type and
opinion polarity. Such classifiers are traditionally
based on bag-of-words and more advanced fea-
tures. In the next sections, we define a baseline
feature vector model and a novel structural model
based on kernel methods.
3.1 Feature Set
We enrich the traditional bag-of-word representa-
tion with features from a sentiment lexicon and
features quantifying the negation present in the
comment. Our model (FVEC) encodes each docu-
ment using the following feature groups:
- word n-grams: we compute unigrams and
bigrams over lower-cased word lemmas where
binary values are used to indicate the pres-
ence/absence of a given item.
- lexicon: a sentiment lexicon is a collection of
words associated with a positive or negative senti-
ment. We use two manually constructed sentiment
lexicons that are freely available: the MPQA Lex-
icon (Wilson et al, 2005) and the lexicon of Hu
and Liu (2004). For each of the lexicons, we use
the number of words found in the comment that
have positive and negative sentiment as a feature.
- negation: the count of negation words, e.g.,
{don?t, never, not, etc.}, found in a comment.
2
Our structural representation (defined next) en-
ables a more involved treatment of negation.
- video concept: cosine similarity between a com-
ment and the title/description of the video. Most
of the videos come with a title and a short descrip-
tion, which can be used to encode the topicality of
2
The list of negation words is adopted from
http://sentiment.christopherpotts.net/lingstruc.html
each comment by looking at their overlap.
3.2 Structural model
We go beyond traditional feature vectors by em-
ploying structural models (STRUCT), which en-
code each comment into a shallow syntactic tree.
These trees are input to tree kernel functions
for generating structural features. Our struc-
tures are specifically adapted to the noisy user-
generated texts and encode important aspects of
the comments, e.g., words from the sentiment lexi-
cons, product concepts and negation words, which
specifically targets the sentiment and comment
type classification tasks.
In particular, our shallow tree structure is a
two-level syntactic hierarchy built from word lem-
mas (leaves) and part-of-speech tags that are fur-
ther grouped into chunks (Fig. 1). As full syn-
tactic parsers such as constituency or dependency
tree parsers would significantly degrade in perfor-
mance on noisy texts, e.g., Twitter or YouTube
comments, we opted for shallow structures, which
rely on simpler and more robust components: a
part-of-speech tagger and a chunker. Moreover,
such taggers have been recently updated with
models (Ritter et al, 2011; Gimpel et al, 2011)
trained specifically to process noisy texts show-
ing significant reductions in the error rate on user-
generated texts, e.g., Twitter. Hence, we use the
CMU Twitter pos-tagger (Gimpel et al, 2011;
Owoputi et al, 2013) to obtain the part-of-speech
tags. Our second component ? chunker ? is taken
from (Ritter et al, 2011), which also comes with a
model trained on Twitter data
3
and shown to per-
form better on noisy data such as user comments.
To address the specifics of OM tasks on
YouTube comments, we enrich syntactic trees
with semantic tags to encode: (i) central con-
cepts of the video, (ii) sentiment-bearing words
expressing positive or negative sentiment and (iii)
negation words. To automatically identify con-
cept words of the video we use context words (to-
kens detected as nouns by the part-of-speech tag-
ger) from the video title and video description and
match them in the tree. For the matched words,
we enrich labels of their parent nodes (part-of-
speech and chunk) with the PRODUCT tag. Sim-
ilarly, the nodes associated with words found in
3
The chunker from (Ritter et al, 2011) relies on its own
POS tagger, however, in our structural representations we fa-
vor the POS tags from the CMU Twitter tagger and take only
the chunk tags from the chunker.
1254
Figure 1: Shallow tree representation of the example comment (labeled with product type and
negative sentiment): ?iPad 2 is better. the superior apps just destroy the xoom.? (lemmas are replaced
with words for readability) taken from the video ?Motorola Xoom Review?. We introduce additional tags
in the tree nodes to encode the central concept of the video (motorola xoom) and sentiment-bearing words
(better, superior, destroy) directly in the tree nodes. For the former we add a PRODUCT tag on the chunk
and part-of-speech nodes of the word xoom) and polarity tags (positive and negative) for the latter. Two
sentences are split into separate root nodes S.
the sentiment lexicon are enriched with a polar-
ity tag (either positive or negative), while nega-
tion words are labeled with the NEG tag. It should
be noted that vector-based (FVEC) model relies
only on feature counts whereas the proposed tree
encodes powerful contextual syntactic features in
terms of tree fragments. The latter are automati-
cally generated and learned by SVMs with expres-
sive tree kernels.
For example, the comment in Figure 1 shows
two positive and one negative word from the senti-
ment lexicon. This would strongly bias the FVEC
sentiment classifier to assign a positive label
to the comment. In contrast, the STRUCT model
relies on the fact that the negative word, destroy,
refers to the PRODUCT (xoom) since they form a
verbal phase (VP). In other words, the tree frag-
ment: [S [negative-VP [negative-V
[destroy]] [PRODUCT-NP [PRODUCT-N
[xoom]]]] is a strong feature (induced
by tree kernels) to help the classifier to dis-
criminate such hard cases. Moreover, tree
kernels generate all possible subtrees, thus
producing generalized (back-off) features,
e.g., [S [negative-VP [negative-V
[destroy]] [PRODUCT-NP]]]] or [S
[negative-VP [PRODUCT-NP]]]].
3.3 Learning
We perform OM on YouTube using supervised
methods, e.g., SVM. Our goal is to learn a model
to automatically detect the sentiment and type of
each comment. For this purpose, we build a multi-
class classifier using the one-vs-all scheme. A bi-
nary classifier is trained for each of the classes
and the predicted class is obtained by taking a
class from the classifier with a maximum predic-
tion score. Our back-end binary classifier is SVM-
light-TK
4
, which encodes structural kernels in the
SVM-light (Joachims, 2002) solver. We define a
novel and efficient tree kernel function, namely,
Shallow syntactic Tree Kernel (SHTK), which is
as expressive as the Partial Tree Kernel (PTK)
(Moschitti, 2006a) to handle feature engineering
over the structural representations of the STRUCT
model. A polynomial kernel of degree 3 is applied
to feature vectors (FVEC).
Combining structural and vector models. A
typical kernel machine, e.g., SVM, classifies a
test input x using the following prediction func-
tion: h(x) =
?
i
?
i
y
i
K(x,x
i
), where ?
i
are
the model parameters estimated from the training
data, y
i
are target variables, x
i
are support vec-
tors, and K(?, ?) is a kernel function. The latter
computes the similarity between two comments.
The STRUCT model treats each comment as a tu-
ple x = ?T ,v? composed of a shallow syntactic
tree T and a feature vector v . Hence, for each pair
of comments x
1
and x
2
, we define the following
comment similarity kernel:
K(x
1
,x
2
) = K
TK
(T
1
,T
2
) +K
v
(v
1
, v
2
), (1)
where K
TK
computes SHTK (defined next), and
K
v
is a kernel over feature vectors, e.g., linear,
polynomial, Gaussian, etc.
Shallow syntactic tree kernel. Following the
convolution kernel framework, we define the new
4
http://disi.unitn.it/moschitti/Tree-Kernel.htm
1255
SHTK function from Eq. 1 to compute the similar-
ity between tree structures. It counts the number of
common substructures between two trees T
1
and
T
2
without explicitly considering the whole frag-
ment space. The general equations for Convolu-
tion Tree Kernels is:
TK(T
1
, T
2
) =
?
n
1
?N
T
1
?
n
2
?N
T
2
?(n
1
, n
2
), (2)
where N
T
1
and N
T
2
are the sets of the T
1
?s and
T
2
?s nodes, respectively and ?(n
1
, n
2
) is equal to
the number of common fragments rooted in the n
1
and n
2
nodes, according to several possible defini-
tion of the atomic fragments.
To improve the speed computation of TK, we
consider pairs of nodes (n
1
, n
2
) belonging to the
same tree level. Thus, given H , the height of the
STRUCT trees, where each level h contains nodes
of the same type, i.e., chunk, POS, and lexical
nodes, we define SHTK as the following
5
:
SHTK(T
1
, T
2
) =
H
?
h=1
?
n
1
?N
h
T
1
?
n
2
?N
h
T
2
?(n
1
, n
2
), (3)
where N
h
T
1
and N
h
T
2
are sets of nodes at height h.
The above equation can be applied with any ?
function. To have a more general and expressive
kernel, we use ? previously defined for PTK.
More formally: if n
1
and n
2
are leaves then
?(n
1
, n
2
) = ??(n
1
, n
2
); else ?(n
1
, n
2
) =
?
(
?
2
+
?
~
I
1
,
~
I
2
,|
~
I
1
|=|
~
I
2
|
?
d(
~
I
1
)+d(
~
I
2
)
|
~
I
1
|
?
j=1
?(c
n
1
(
~
I
1j
), c
n
2
(
~
I
2j
))
)
,
where ?, ? ? [0, 1] are decay factors; the large
sum is adopted from a definition of the sub-
sequence kernel (Shawe-Taylor and Cristianini,
2004) to generate children subsets with gaps,
which are then used in a recursive call to ?. Here,
c
n
1
(i) is the i
th
child of the node n
1
;
~
I
1
and
~
I
2
are
two sequences of indexes that enumerate subsets
of children with gaps, i.e.,
~
I = (i
1
, i
2
, .., |I|), with
1 ? i
1
< i
2
< .. < i
|I|
; and d(
~
I
1
) =
~
I
1l(
~
I
1
)
?
~
I
11
+ 1
and d(
~
I
2
) =
~
I
2l(
~
I
2
)
?
~
I
21
+ 1, which penalizes
subsequences with larger gaps.
It should be noted that: firstly, the use of a
subsequence kernel makes it possible to generate
child subsets of the two nodes, i.e., it allows for
gaps, which makes matching of syntactic patterns
5
To have a similarity score between 0 and 1, a normaliza-
tion in the kernel space, i.e.
SHTK(T
1
,T
2
)
?
SHTK(T
1
,T
1
)?SHTK(T
2
,T
2
)
is
applied.
less rigid. Secondly, the resulting SHTK is essen-
tially a special case of PTK (Moschitti, 2006a),
adapted to the shallow structural representation
STRUCT (see Sec. 3.2). When applied to STRUCT
trees, SHTK exactly computes the same feature
space as PTK, but in faster time (on average). In-
deed, SHTK required to be only applied to node
pairs from the same level (see Eq. 3), where the
node labels can match ? chunk, POS or lexicals.
This reduces the time for selecting the matching-
node pairs carried out in PTK (Moschitti, 2006a;
Moschitti, 2006b). The fragment space is obvi-
ously the same, as the node labels of different
levels in STRUCT are different and will not be
matched by PTK either.
Finally, given its recursive definition in Eq. 3
and the use of subsequence (with gaps), SHTK can
derive useful dependencies between its elements.
For example, it will generate the following subtree
fragments: [positive-NP [positive-A
N]], [S [negative-VP [negative-V
[destroy]] [PRODUCT-NP]]]] and so on.
4 YouTube comments corpus
To build a corpus of YouTube comments, we fo-
cus on a particular set of videos (technical reviews
and advertisings) featuring commercial products.
In particular, we chose two product categories:
automobiles (AUTO) and tablets (TABLETS). To
collect the videos, we compiled a list of prod-
ucts and queried the YouTube gData API
6
to re-
trieve the videos. We then manually excluded
irrelevant videos. For each video, we extracted
all available comments (limited to maximum 1k
comments per video) and manually annotated each
comment with its type and polarity. We distin-
guish between the following types:
product: discuss the topic product in general or
some features of the product;
video: discuss the video or some of its details;
spam: provide advertising and malicious links; and
off-topic: comments that have almost no content
(?lmao?) or content that is not related to the video
(?Thank you!?).
Regarding the polarity, we distinguish between
{positive, negative, neutral} sentiments with re-
spect to the product and the video. If the comment
contains several statements of different polarities,
it is annotated as both positive and negative: ?Love
the video but waiting for iPad 4?. In total we have
6
https://developers.google.com/youtube/v3/
1256
annotated 208 videos with around 35k comments
(128 videos TABLETS and 80 for AUTO).
To evaluate the quality of the produced labels,
we asked 5 annotators to label a sample set of one
hundred comments and measured the agreement.
The resulting annotator agreement ? value (Krip-
pendorf, 2004; Artstein and Poesio, 2008) scores
are 60.6 (AUTO), 72.1 (TABLETS) for the senti-
ment task and 64.1 (AUTO), 79.3 (TABLETS) for
the type classification task. For the rest of the
comments, we assigned the entire annotation task
to a single coder. Further details on the corpus can
be found in Uryupina et al (2014).
5 Experiments
This section reports: (i) experiments on individ-
ual subtasks of opinion and type classification; (ii)
the full task of predicting type and sentiment; (iii)
study on the adaptability of our system by learn-
ing on one domain and testing on the other; (iv)
learning curves that provide an indication on the
required amount and type of data and the scalabil-
ity to other domains.
5.1 Task description
Sentiment classification. We treat each com-
ment as expressing positive, negative or
neutral sentiment. Hence, the task is a three-
way classification.
Type classification. One of the challenging as-
pects of sentiment analysis of YouTube data is that
the comments may express the sentiment not only
towards the product shown in the video, but
also the video itself, i.e., users may post posi-
tive comments to the video while being generally
negative about the product and vice versa. Hence,
it is of crucial importance to distinguish between
these two types of comments. Additionally, many
comments are irrelevant for both the product and
the video (off-topic) or may even contain
spam. Given that the main goal of sentiment
analysis is to select sentiment-bearing comments
and identify their polarity, distinguishing between
off-topic and spam categories is not critical.
Thus, we merge the spam and off-topic into
a single uninformative category. Similar to
the opinion classification task, comment type clas-
sification is a multi-class classification with three
classes: video, product and uninform.
Full task. While the previously discussed sen-
timent and type identification tasks are useful to
Task class
AUTO TABLETS
TRAIN TEST TRAIN TEST
Sentiment
positive 2005 (36%) 807 (27%) 2393 (27%) 1872 (27%)
neutral 2649 (48%) 1413 (47%) 4683 (53%) 3617 (52%)
negative 878 (16%) 760 (26%) 1698 (19%) 1471 (21%)
total 5532 2980 8774 6960
Type
product 2733 (33%) 1761 (34%) 7180 (59%) 5731 (61%)
video 3008 (36%) 1369 (26%) 2088 (17%) 1674 (18%)
off-topic 2638 (31%) 2045 (39%) 2334 (19%) 1606 (17%)
spam 26 (>1%) 17 (>1%) 658 (5%) 361 (4%)
total 8405 5192 12260 9372
Full
product-pos. 1096 (13%) 517 (10%) 1648 (14%) 1278 (14%)
product-neu. 908 (11%) 729 (14%) 3681 (31%) 2844 (32%)
product-neg. 554 (7%) 370 (7%) 1404 (12%) 1209 (14%)
video-pos. 909 (11%) 290 (6%) 745 (6%) 594 (7%)
video-neu. 1741 (21%) 683 (14%) 1002 (9%) 773 (9%)
video-neg. 324 (4%) 390 (8%) 294 (2%) 262 (3%)
off-topic 2638 (32%) 2045 (41%) 2334 (20%) 1606 (18%)
spam 26 (>1%) 17 (>1%) 658 (6%) 361 (4%)
total 8196 5041 11766 8927
Table 1: Summary of YouTube comments data
used in the sentiment, type and full classification
tasks. The comments come from two product cate-
gories: AUTO and TABLETS. Numbers in paren-
thesis show proportion w.r.t. to the total number of
comments used in a task.
model and study in their own right, our end goal is:
given a stream of comments, to jointly predict both
the type and the sentiment of each comment. We
cast this problem as a single multi-class classifica-
tion task with seven classes: the Cartesian product
between {product, video} type labels and
{positive, neutral, negative} senti-
ment labels plus the uninformative category
(spam and off-topic). Considering a real-life ap-
plication, it is important not only to detect the po-
larity of the comment, but to also identify if it is
expressed towards the product or the video.
7
5.2 Data
We split all the videos 50% between training
set (TRAIN) and test set (TEST), where each
video contains all its comments. This ensures
that all comments from the same video appear
either in TRAIN or in TEST. Since the number
of comments per video varies, the resulting sizes
of each set are different (we use the larger split
for TRAIN). Table 1 shows the data distribution
across the task-specific classes ? sentiment and
type classification. For the sentiment task we ex-
clude off-topic and spam comments as well
as comments with ambiguous sentiment, i.e., an-
7
We exclude comments annotated as both video and
product. This enables the use of a simple flat multi-
classifiers with seven categories for the full task, instead of
a hierarchical multi-label classifiers (i.e., type classification
first and then opinion polarity). The number of comments as-
signed to both product and video is relatively small (8%
for TABLETS and 4% for AUTO).
1257
notated as both positive and negative.
For the sentiment task about 50% of the
comments have neutral polarity, while the
negative class is much less frequent. Inter-
estingly, the ratios between polarities expressed
in comments from AUTO and TABLETS are very
similar across both TRAIN and TEST. Conversely,
for the type task, we observe that comments from
AUTO are uniformly distributed among the three
classes, while for the TABLETS the majority of
comments are product related. It is likely due
to the nature of the TABLETS videos, that are
more geek-oriented, where users are more prone
to share their opinions and enter involved discus-
sions about a product. Additionally, videos from
the AUTO category (both commercials and user
reviews) are more visually captivating and, be-
ing generally oriented towards a larger audience,
generate more video-related comments. Regard-
ing the full setting, where the goal is to have
a joint prediction of the comment sentiment and
type, we observe that video-negative and
video-positive are the most scarce classes,
which makes them the most difficult to predict.
5.3 Results
We start off by presenting the results for the tradi-
tional in-domain setting, where both TRAIN and
TEST come from the same domain, e.g., AUTO or
TABLETS. Next, we show the learning curves to
analyze the behavior of FVEC and STRUCT mod-
els according to the training size. Finally, we per-
form a set of cross-domain experiments that de-
scribe the enhanced adaptability of the patterns
generated by the STRUCT model.
5.3.1 In-domain experiments
We compare FVEC and STRUCT models on three
tasks described in Sec. 5.1: sentiment, type and
full. Table 2 reports the per-class performance
and the overall accuracy of the multi-class clas-
sifier. Firstly, we note that the performance on
TABLETS is much higher than on AUTO across
all tasks. This can be explained by the follow-
ing: (i) TABLETS contains more training data and
(ii) videos from AUTO and TABLETS categories
draw different types of audiences ? well-informed
users and geeks expressing better-motivated opin-
ions about a product for the former vs. more gen-
eral audience for the latter. This results in the
different quality of comments with the AUTO be-
ing more challenging to analyze. Secondly, we
observe that the STRUCT model provides 1-3%
of absolute improvement in accuracy over FVEC
for every task. For individual categories the F1
scores are also improved by the STRUCT model
(except for the negative classes for AUTO, where
we see a small drop). We conjecture that sentiment
prediction for AUTO category is largely driven
by one-shot phrases and statements where it is
hard to improve upon the bag-of-words and senti-
ment lexicon features. In contrast, comments from
TABLETS category tend to be more elaborated
and well-argumented, thus, benefiting from the ex-
pressiveness of the structural representations.
Considering per-class performance, correctly
predicting negative sentiment is most difficult
for both AUTO and TABLETS, which is proba-
bly caused by the smaller proportion of the neg-
ative comments in the training set. For the type
task, video-related class is substantially more dif-
ficult than product-related for both categories. For
the full task, the class video-negative ac-
counts for the largest error. This is confirmed by
the results from the previous sentiment and type
tasks, where we saw that handling negative sen-
timent and detecting video-related comments are
most difficult.
5.3.2 Learning curves
The learning curves depict the behavior of FVEC
and STRUCT models as we increase the size of
the training set. Intuitively, the STRUCT model
relies on more general syntactic patterns and may
overcome the sparseness problems incurred by the
FVEC model when little training data is available.
Nevertheless, as we see in Figure 2, the learning
curves for sentiment and type classification tasks
across both product categories do not confirm this
intuition. The STRUCT model consistently outper-
forms the FVEC across all training sizes, but the
gap in the performance does not increase when we
move to smaller training sets. As we will see next,
this picture changes when we perform the cross-
domain study.
5.3.3 Cross-domain experiments
To understand the performance of our classifiers
on other YouTube domains, we perform a set of
cross-domain experiments by training on the data
from one product category and testing on the other.
Table 3 reports the accuracy for three tasks
when we use all comments (TRAIN + TEST) from
AUTO to predict on the TEST from TABLETS
1258
Task class
AUTO TABLETS
FVEC STRUCT FVEC STRUCT
P R F1 P R F1 P R F1 P R F1
Sent
positive 49.1 72.1 58.4 50.1 73.9 59.0 67.5 70.3 69.9 71.2 71.3 71.3
neutral 68.2 55.0 61.4 70.1 57.6 63.1 81.3 71.4 76.9 81.1 73.1 77.8
negative 42.0 36.9 39.6 41.3 35.8 38.8 48.3 60.0 54.8 50.2 62.6 56.5
Acc 54.7 55.7 68.6 70.5
Type
product 66.8 73.3 69.4 68.8 75.5 71.7 78.2 95.3 86.4 80.1 95.5 87.6
video 45.0 52.8 48.2 47.8 49.9 48.7 83.6 45.7 58.9 83.5 46.7 59.4
uninform 59.3 48.2 53.1 60.6 53.0 56.4 70.2 52.5 60.7 72.9 58.6 65.0
Acc 57.4 59.4 77.2 78.6
Full
product-pos 34.0 49.6 39.2 36.5 51.2 43.0 48.4 56.8 52.0 52.4 59.3 56.4
product-neu 43.4 31.1 36.1 41.4 36.1 38.4 68.0 67.5 68.1 59.7 83.4 70.0
product-neg 26.3 29.5 28.8 26.3 25.3 25.6 43.0 49.9 45.4 44.7 53.7 48.4
video-pos 23.2 47.1 31.9 26.1 54.5 35.5 69.1 60.0 64.7 64.9 68.8 66.4
video-neu 26.1 30.0 29.0 26.5 31.6 28.8 56.4 32.1 40.0 55.1 35.7 43.3
video-neg 21.9 3.7 6.0 17.7 2.3 4.8 39.0 17.5 23.9 39.5 6.1 11.5
uninform 56.5 52.4 54.9 60.0 53.3 56.3 60.0 65.5 62.2 63.3 68.4 66.9
Acc 40.0 41.5 57.6 60.3
Table 2: In-domain experiments on AUTO and TABLETS using two models: FVEC and STRUCT. The
results are reported for sentiment, type and full classification tasks. The metrics used are precision (P),
recall (R) and F1 for each individual class and the general accuracy of the multi-class classifier (Acc).
AUTOSTRUCTAUTOFVECTABLETSSTRUCTTABLETSFVECAccuracy
55
60
65
70
training size1k 2k 3k 4k 5k ALL
(a) Sentiment classification
AUTOSTRUCTAUTOFVECTABLETSSTRUCTTABLETSFVEC
Accurac
y
40
45
50
55
60
65
70
75
80
training size1k 2k 3k 4k 5k ALL
(b) Type classification
Figure 2: In-domain learning curves. ALL refers
to the entire TRAIN set for a given product cate-
gory, i.e., AUTO and TABLETS (see Table 1)
and in the opposite direction (TABLETS?AUTO).
When using AUTO as a source domain, STRUCT
model provides additional 1-3% of absolute im-
Source Target Task FVEC STRUCT
AUTO TABLETS
Sent 66.1 66.6
Type 59.9 64.1
?
Full 35.6 38.3
?
TABLETS AUTO
Sent 60.4 61.9
?
Type 54.2 55.6
?
Full 43.4 44.7
?
Table 3: Cross-domain experiment. Ac-
curacy using FVEC and STRUCT models
when trained/tested in both directions, i.e.
AUTO?TABLETS and TABLETS?AUTO.
?
de-
notes results statistically significant at 95% level
(via pairwise t-test).
provement, except for the sentiment task.
Similar to the in-domain experiments, we stud-
ied the effect of the source domain size on the tar-
get test performance. This is useful to assess the
adaptability of features exploited by the FVEC and
STRUCT models with the change in the number
of labeled examples available for training. Addi-
tionally, we considered a setting including a small
amount of training data from the target data (i.e.,
supervised domain adaptation).
For this purpose, we drew the learning curves of
the FVEC and STRUCT models applied to the sen-
timent and type tasks (Figure 3): AUTO is used
as the source domain to train models, which are
tested on TABLETS.
8
The plot shows that when
8
The results for the other direction (TABLETS?AUTO)
show similar behavior.
1259
STRUCTFVEC
Source +Target
Accurac
y
62
63
64
65
66
67
68
training size1k 2k 3k 4k 5k 8.5k(ALL) 100 500 1k
(a) Sentiment classification
STRUCTFVEC
Source +Target
Accurac
y
30
35
40
45
50
55
60
65
70
training size1k 2k 3k 4k 5k 8.5k(TRAIN) 13k(ALL) 100 500 1k
(b) Type classification
Figure 3: Learning curves for the cross-domain
setting (AUTO?TABLETS). Shaded area refers to
adding a small portion of comments from the same
domain as the target test data to the training.
little training data is available, the features gener-
ated by the STRUCT model exhibit better adapt-
ability (up to 10% of improvement over FVEC).
The bag-of-words model seems to be affected by
the data sparsity problem which becomes a crucial
issue when only a small training set is available.
This difference becomes smaller as we add data
from the same domain. This is an important ad-
vantage of our structural approach, since we can-
not realistically expect to obtain manual annota-
tions for 10k+ comments for each (of many thou-
sands) product domains present on YouTube.
5.4 Discussion
Our STRUCT model is more accurate since it is
able to induce structural patterns of sentiment.
Consider the following comment: optimus pad
is better. this xoom is just to bulky but optimus
pad offers better functionality. The FVEC bag-
of-words model misclassifies it to be positive,
since it contains two positive expressions (better,
better functionality) that outweigh a single nega-
tive expression (bulky). The structural model, in
contrast, is able to identify the product of interest
(xoom) and associate it with the negative expres-
sion through a structural feature and thus correctly
classify the comment as negative.
Some issues remain problematic even for the
structural model. The largest group of errors are
implicit sentiments. Thus, some comments do not
contain any explicit positive or negative opinions,
but provide detailed and well-argumented criti-
cism, for example, this phone is heavy. Such com-
ments might also include irony. To account for
these cases, a deep understanding of the product
domain is necessary.
6 Conclusions and Future Work
We carried out a systematic study on OM from
YouTube comments by training a set of su-
pervised multi-class classifiers distinguishing be-
tween video and product related opinions. We
use standard feature vectors augmented by shallow
syntactic trees enriched with additional conceptual
information.
This paper makes several contributions: (i) it
shows that effective OM can be carried out with
supervised models trained on high quality annota-
tions; (ii) it introduces a novel annotated corpus
of YouTube comments, which we make available
for the research community; (iii) it defines novel
structural models and kernels, which can improve
on feature vectors, e.g., up to 30% of relative im-
provement in type classification, when little data
is available, and demonstrates that the structural
model scales well to other domains.
In the future, we plan to work on a joint model
to classify all the comments of a given video, s.t. it
is possible to exploit latent dependencies between
entities and the sentiments of the comment thread.
Additionally, we plan to experiment with hierar-
chical multi-label classifiers for the full task (in
place of a flat multi-class learner).
Acknowledgments
The authors are supported by a Google Fac-
ulty Award 2011, the Google Europe Fellowship
Award 2013 and the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) un-
der the grant #288024: LIMOSINE.
1260
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596, December.
Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay, and Li Wang. 2013. How noisy social
media text, how diffrnt social media sources? In
IJCNLP.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL.
Hal Daum?e, III. 2007. Frustratingly easy domain
adaptation. ACL.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: annotation, features, and experiments.
In ACL.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In KDD.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In KDD.
Jason S. Kessler, Miriam Eckert, Lyndsie Clark, and
Nicolas Nicolov. 2010. The 2010 ICWSM JDPA
sentiment corpus for the automotive domain. In
ICWSM-DWC.
Klaus Krippendorf, 2004. Content Analysis: An In-
troduction to Its Methodology, second edition, chap-
ter 11. Sage, Thousand Oaks, CA.
Alessandro Moschitti. 2006a. Efficient convolution
kernels for dependency and constituent syntactic
trees. In ECML.
Alessandro Moschitti. 2006b. Making tree kernels
practical for natural language learning. In EACL,
pages 113?120.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In LREC.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In ACL.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an ex-
perimental study. In ACL.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In SIGIR.
Aliaksei Severyn and Alessandro Moschitti. 2013. Au-
tomatic feature engineering for answer selection and
extraction. In EMNLP.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013. Learning semantic textual simi-
larity with structural representations. In ACL.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
Stefan Siersdorfer, Sergiu Chelaru, Wolfgang Nejdl,
and Jose San Pedro. 2010. How useful are your
comments?: Analyzing and predicting YouTube
comments and comment ratings. In WWW.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In EMNLP.
Anders S?gaard and Anders Johannsen. 2012. Ro-
bust learning in random subspaces: Equipping nlp
for oov effects. In COLING.
Oscar T?ackstr?om and Ryan McDonald. 2011. Semi-
supervised latent variable models for sentence-level
sentiment analysis. In ACL.
Olga Uryupina, Barbara Plank, Aliaksei Severyn,
Agata Rotondi, and Alessandro Moschitti. 2014.
SenTube: A corpus for sentiment analysis on
YouTube social media. In LREC.
Sida Wang and Christopher Manning. 2012. Baselines
and bigrams: Simple, good sentiment and topic clas-
sification. In ACL.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In EMNLP.
1261
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 53?58, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
iKernels-Core: Tree Kernel Learning for Textual Similarity
Aliaksei Severyn1 and Massimo Nicosia1 and Alessandro Moschitti1,2
1University of Trento, DISI, 38123 Povo (TN), Italy
{severyn,m.nicosia,moschitti}@disi.unitn.it
2Qatar Foundation, QCRI, Doha, Qatar
{amoschitti}@qf.org.qa
Abstract
This paper describes the participation of iKer-
nels system in the Semantic Textual Similar-
ity (STS) shared task at *SEM 2013. Different
from the majority of approaches, where a large
number of pairwise similarity features are
used to learn a regression model, our model
directly encodes the input texts into syntac-
tic/semantic structures. Our systems rely on
tree kernels to automatically extract a rich set
of syntactic patterns to learn a similarity score
correlated with human judgements. We ex-
periment with different structural representa-
tions derived from constituency and depen-
dency trees. While showing large improve-
ments over the top results from the previous
year task (STS-2012), our best system ranks
21st out of total 88 participated in the STS-
2013 task. Nevertheless, a slight refinement to
our model makes it rank 4th.
1 Introduction
Comparing textual data to establish the degree of se-
mantic similarity is of key importance in many Nat-
ural Language Processing (NLP) tasks ranging from
document categorization to textual entailment and
summarization. The key aspect of having an accu-
rate STS framework is the design of features that can
adequately represent various aspects of the similar-
ity between texts, e.g. using lexical, syntactic and
semantic similarity metrics.
The majority of approaches to semantic textual
similarity treat the input text pairs as feature vec-
tors where each feature is a score corresponding to a
certain type of similarity. This approach is concep-
tually easy to implement and STS-2012 (Agirre et
al., 2012) has shown that the best systems were built
following this idea, i.e. a number of features encod-
ing similarity of an input text pair were combined in
a single scoring model, such as Linear Regression
or Support Vector Regression (SVR). One potential
limitation of using only similarity features to repre-
sent a text pair is that of low representation power.
The novelty of our approach is that we encode the
input text pairs directly into structural objects, e.g.
trees, and rely on the power of kernel learning to ex-
tract relevant structures. This completely different
from (Croce et al, ), where tree kernels where used
to establish syntactic similarity and then plugged as
similarity features. To link the documents in a pair
we mark the nodes in the related structures with a
special relational tag. In this way effective struc-
tural relational patterns are implicitly encoded in the
trees and can be automatically learned by the kernel-
based machine learning methods. We build our sys-
tems on top of the features used by two best systems
from STS-2012 and combine them with the tree ker-
nel models within the Support Vector Regression to
derive a single scoring model. Since the test data
used for evaluation in STS-2013 (Agirre et al, 2013)
is different from the 2012 data provided for the sys-
tem development, domain adaptation represents an
additional challenge. To address this problem we
augment our feature vector representation with fea-
tures extracted from a text pair as a whole to capture
individual properties of each dataset. Additionally,
we experiment with a corpus type classifier and in-
clude its prediction score as additional features. Fi-
nally, we use stacking to combine several structural
models into the feature vector representation.
53
In the following sections we describe our ap-
proach to combine structural representations with
the pairwise similarity features in a single SVR
learning framework. We then report results on both
STS-2012 and 2013 tasks.
2 Structural Relational Similarity
In this section we first describe the kernel framework
to combine structural and vector models, then we
explain how to construct the tree models and briefly
describe tree kernels we use to automatically extract
the features.
2.1 Structural Kernel Learning
In supervised learning, given the labeled data
{(xi, y i)}ni=1, the goal is to estimate a decision func-
tion h(x) = y that maps input examples to the tar-
get variables. A conventional approach is to rep-
resent a pair of texts as a set of similarity features
{fi}, s.t. the predictions are computed as h(x) =
w ? x =
?
iwifi, wherew is the model weight vec-
tor. Hence, the learning problem boils down to es-
timating the individual weight of each of the sim-
ilarity feature fi. One downside of such approach
is that a great deal of similarity information carried
by a given text pair is lost when modeled by single
real-valued scores.
A more versatile approach in terms of the input
representation relies on kernels. In a typical ker-
nel machine, e.g. SVM, the prediction function for
a test input x takes on the following form h(x) =
?
i ?iyiK(x,xi), where ?i are the model parame-
ters estimated from the training data, yi - target vari-
ables, xi are support vectors, and K(?, ?) is a kernel
function.
To encode both structural representation and sim-
ilarity feature vectors of input text pairs xi in a sin-
gle model, we treat it as the following tuple: xi =
?xai ,x
b
i? = ?(t
a
i , v
a
i ), (t
b
i , v
b
i)?, where x
a
i x
b
i are the
first and the second document of xi, and t and v de-
note tree and vector representations respectively.
To compute a kernel between two text pairs xi
and xj we define the following all-vs-all kernel,
where all possible combinations of documents from
each pair are considered: K(xi,xj) = K(xai ,x
a
j ) +
K(xai ,x
b
j) + K(x
b
i ,x
a
j ) + K(x
b
i ,x
b
j). Each of the
kernel computations K between two documents xa
and xb can be broken down into the following:
K(xa,xb) = KTK(ta, tb) + Kfvec(va, vb), where
KTK computes a tree kernel and Kfvec is a kernel
over feature vectors, e.g. linear, polynomial or RBF,
etc. Further in the text we refer to structural tree
kernel models as TK and explicit feature vector rep-
resentation as fvec.
Having defined a way to jointly model text pairs
using structural TK representations along with the
similarity features fvec, we next briefly review tree
kernels and our relational structures derived from
constituency and dependency trees.
2.2 Tree Kernels
We use tree structures as our base representation
since they provide sufficient flexibility in represen-
tation and allow for easier feature extraction than,
for example, graph structures. We use a Partial Tree
Kernel (PTK) (Moschitti, 2006) to take care of auto-
matic feature extraction and compute KTK(?, ?).
PTK is a tree kernel function that can be ef-
fectively applied to both constituency and depen-
dency parse trees. It generalizes a subset tree ker-
nel (STK) (Collins and Duffy, 2002) that maps a
tree into the space of all possible tree fragments con-
strained by the rule that the sibling nodes from their
parents cannot be separated. Different from STK
where the nodes in the generated tree fragments are
constrained to include none or all of their direct chil-
dren, PTK fragments can contain any subset of the
features, i.e. PTK allows for breaking the production
rules. Consequently, PTK generalizes STK generat-
ing an extremely rich feature space, which results in
higher generalization ability.
2.3 Relational Structures
The idea of using relational structures to jointly
model text pairs was previously proposed in (Sev-
eryn and Moschitti, 2012), where shallow syntactic
structures derived from chunks and part-of-speech
tags were used to represent question/answer pairs.
In this paper, we define novel relational structures
based on: (i) constituency and (ii) dependency trees.
Constituency tree. Each document in a given text
pair is represented by its constituency parse tree.
If a document contains multiple sentences they are
merged in a single tree with a common root. To
encode the structural relationships between docu-
54
Figure 1: A dependency-based structural representation of a text pair. REL tag links related fragments.
ments in a pair a special REL tag is used to link
the related structures. We adopt a simple strategy
to establish such links: words from two documents
that have a common lemma get their parents (POS
tags) and grandparents, non-terminals, marked with
a REL tag.
Dependency tree. We propose to use dependency
relations between words to derive an alternative
structural representation. In particular, dependency
relations are used to link words in a way that words
are always at the leaf level. This reordering of the
nodes helps to avoid the situation where nodes with
words tend to form long chains. This is essential
for PTK to extract meaningful fragments. We also
plug part-of-speech tags between the word nodes
and nodes carrying their grammatical role. Again
a special REL tag is used to establish relations be-
tween tree fragments. Fig. 1 gives an example of
a dependency-based structure taken from STS-2013
headlines dataset.
3 Pairwise similarity features.
Along with the direct representation of input text
pairs as structural objects our framework also en-
codes feature vectors (base), which we describe
below.
3.1 Baseline features
We adopt similarity features from two best perform-
ing systems of STS-2012, which were publicly re-
leased: namely, the Takelab1 system (S?aric? et al,
2012) and the UKP Lab?s system2 (Bar et al, 2012).
Both systems represent input texts with similar-
1http://takelab.fer.hr/sts/
2https://code.google.com/p/dkpro-similarity-
asl/wiki/SemEval2013
ity features which combine multiple text similarity
measures of varying complexity.
UKP provides metrics based on matching of char-
acter, word n-grams and common subsequences. It
also includes features derived from Explicit Seman-
tic Analysis vector comparisons and aggregation of
word similarity based on lexical-semantic resources,
e.g. WordNet. In total it provides 18 features.
Takelab includes n-gram matching of varying size,
weighted word matching, length difference, Word-
Net similarity and vector space similarity where
pairs of input sentences are mapped into Latent Se-
mantic Analysis (LSA) space (Turney and Pantel,
2010). The features are computed over several sen-
tence representations where stop words are removed
and/or lemmas are used in place of raw tokens.
The total number of Takelab?s features is 21. Even
though some of the UKP and Takelab features over-
lap we include all of them in a combined system with
the total of 39 features.
3.2 iKernels features
Here we describe our additional features added to
the fvec representation. First, we note that word
frequencies used to compute weighted word match-
ings and the word-vector mappings to compute LSA
similarities required by Takelab features are pro-
vided only for the vocabulary extracted from 2012
data. Hence, we use both STS-2012 and 2013 data to
obtain the word counts and re-estimate LSA vector
representations. For the former we extract unigram
counts from Google Books Ngrams3, while for the
latter we use additional corpora as described below.
LSA similarity. To construct LSA word-vector
mappings we use the following three sources: (i)
3http://storage.googleapis.com/books/ngrams/books/datasetsv2.html
55
Aquaint4, which consists of more than 1 million
newswire documents, (ii) ukWaC (Baroni et al,
2009) - a 2 billion word corpus constructed from
the Web, and (iii) and a collection of documents
extracted from Wikipedia dump5. To extract LSA
topics we use GenSim6 software. We preprocess
the data by lowercasing, removing stopwords and
words with frequency lower than 5. Finally, we ap-
ply tf-idf weighting. For all representations we fix
the number of dimensions to 250. For all corpora
we use document-level representation, except for
Wikipedia we also experimented with a sentence-
level document representation, which typically pro-
vides a more restricted context for estimating word-
document distributions.
Brown Clusters. In addition to vector represen-
tations derived from LSA, we extract word-vector
mappings using Brown word clusters7 (Turian et al,
2010), where words are organized into a hierarchy
and each word is represented as a bit-string. We
encode each word by a feature vector where each
entry corresponds to a prefix extracted from its bit-
string. We use prefix lengths in the following range:
k = {4, 8, 12, 16, 20}. Finally, the document is rep-
resented as a feature vector composed by the indi-
vidual word vectors.
Term-overlap features. In addition to the word
overlap features computed by UKP and Takelab
systems we also compute a cosine similarity over
the following representations: (i) n-grams of part-
of-speech tags (up to 4-grams), (ii) SuperSense
tags (Ciaramita and Altun, 2006), (iii) named enti-
ties, and (iv) dependency triplets.
PTK similarity. We use PTK to provide a syn-
tactic similarity score between documents in a pair:
PTK(a, b) = PTK(a, b), where as input represen-
tations we use dependency and constituency trees.
Explicit Semantic Analysis (ESA) similarity.
ESA (Gabrilovich and Markovitch, 2007) represents
input documents as vectors of Wikipedia concepts.
To compute ESA features we use Lucene8 to in-
dex documents extracted from a Wikipedia dump.
Given a text pair we retrieve k top documents (i.e.
4http://www.ldc.upenn.edu/Catalog/docs/LDC2002T31/
5http://dumps.wikimedia.org/
6http://radimrehurek.com/gensim/
7http://metaoptimize.com/projects/wordreprs/
8http://lucene.apache.org/
Wikipedia concepts) and compute the metric by
looking at the overlap of the concepts between the
documents: esak(a, b) =
|Wa
?
Wb|
k , where Wa is
the set of concepts retrieved for document a. We
compute esa features with k ? {10, 25, 50, 100}.
3.3 Corpus type features
Here we describe two complementary approaches
(corpus) in an attempt to alleviate the problem of
domain adaptation, where the datasets used for train-
ing and testing are drawn from different sources.
Pair representation. We treat each pair of texts as a
whole and extract the following sets of corpus fea-
tures: plain bag-of-words, dependency triplets, pro-
duction rules of the syntactic parse tree and a length
feature, i.e. a log-normalized length of the combined
text. Each feature set is normalized and added to the
fvec model.
Corpus classifier. We use the above set of features
to train a multi-class classifier to predict for each in-
stance its most likely corpus type. Our categories
correspond to five dataset types of STS-2012. Pre-
diction scores for each of the dataset categories are
then plugged as features into the final fvec repre-
sentation. Our multi-class classifier is a one-vs-all
binary SVM trained on the merged data from STS-
2012. We apply 5-fold cross-validation scheme, s.t.
for each of the held-out folds we obtain independent
predictions. The accuracy (averaged over 5-folds)
on the STS-2012 data is 92.0%.
3.4 Stacking
To integrate multiple TK models into a single model
we apply a classifier stacking approach (Fast and
Jensen, 2008). Each of the learned TK models is
used to generate predictions which are then plugged
as features into the final fvec representation, s.t.
the final model uses only explicit feature vector
representation. We apply a 5-fold cross-validation
scheme to obtain prediction scores in the same man-
ner as described above.
4 Experimental Evaluation
4.1 Experimental setup
To encode TK models along with the similarity fea-
ture vectors into a single regression scoring model,
56
base corpus TK
U T I B O M C D ALL Mean MSRp MSRv SMTe OnWN SMTn
? 0.7060 0.6087 0.6080 0.8390 0.2540 0.6820 0.4470
? 0.7589 0.6863 0.6814 0.8637 0.4950 0.7091 0.5395
? ? 0.8079 0.7161 0.7134 0.8837 0.5519 0.7343 0.5607
? ? ? 0.8187 0.7137 0.7157 0.8833 0.5131 0.7355 0.5809
? ? ? ? 0.8458 0.7047 0.6935 0.8953 0.5080 0.7101 0.5834
? ? ? ? 0.8468 0.6954 0.6717 0.8902 0.4652 0.7089 0.6133
? ? ? ? ? 0.8539 0.7132 0.6993 0.9005 0.4772 0.7189 0.6481
? ? ? ? ? 0.8529 0.7249 0.7080 0.8984 0.5142 0.7263 0.6700
Sys1 ? ? ? ? ? ? 0.8546 0.7156 0.6989 0.8979 0.4884 0.7181 0.6609
Sys3 ? ? ? ? ? ? 0.8810 0.7416 0.7210 0.8971 0.5912 0.7328 0.6778
Sys2 ? ? ? ? ? ? 0.8705 0.7339 0.7039 0.9012 0.5629 0.7376 0.6656
UKPbest 0.8239 0.6773 0.6830 0.8739 0.5280 0.6641 0.4937
Table 1: System configurations and results on STS-2012. Column set base lists 3 feature sets : UKP (U), Takelab
(T) and iKernels (I); corpus type features (corpus) include plain features (B), corpus classifier (O), and manually
encoded dataset category (M); TK contains constituency (C) and dependency-based (D) models. UKPbest is the best
system of STS-2012. First column shows configuration of our three system runs submitted to STS-2013.
we use an SVR framework implemented in SVM-
Light-TK9. We use the following parameter settings
-t 5 -F 3 -W A -C +, which specifies to use
a combination of trees and feature vectors (-C +),
PTK over trees (-F 3) computed in all-vs-all mode
(-W A) and using polynomial kernel of degree 3 for
the feature vector (active by default).
We report the following metrics employed in the
final evaluation: Pearson correlation for individual
test sets10 and Mean ? an average score weighted by
the test set size.
4.2 STS-2012
For STS-2013 task the entire data from STS-2012
was provided for the system development. To com-
pare with the best systems of the previous year we
followed the same setup, where 3 datasets (MSRp,
MSRv and SMTe) are used for training and 5 for test-
ing (two ?surprise? datasets were added: OnWN and
SMTn). We use the entire training data to obtain a
single model.
Table 1 summarizes the results using structural
models (TK), pairwise similarity (base) and corpus
type features (corpus). We first note, that com-
bining all three features sets (U, T and I) provides
a good match to the best system UKPbest. Next,
adding TK models results in a large improvement
beating the top results in STS-2012. Furthermore,
using corpus features results in even greater im-
9http://disi.unitn.it/moschitti/Tree-Kernel.htm
10for STS-2012 we also report the results for a concatenation
of all five test sets (ALL)
provement with the Mean = 0.7416 and Pearson
ALL = 0.8810.
4.3 STS-2013
Below we specify the configuration for each of the
submitted runs (also shown in Table 1) and report the
results on the STS-2013 test sets: headlines (head),
OnWN, FNWN, and SMT:
Sys1: combines base features (U, T and I), TK
models (C and D) and plain corpus type features (B).
We use STS-2012 data to train a single model.
Sys2: different from Sys1 where a single model
trained on the entire data is used to make predictions,
we adopt a different training/test setup to account for
the different nature of the data used for training and
testing. After performing manual analysis of the test
data we came up with the following strategy to split
the training data into two sets to learn two differ-
ent models: STMe and OnWN (model1) and MSRp,
SMTn and STMe (model2); model1 is then used to
get predictions for OnWN, FNWN, while model2 is
used for SMT and headlines.
Sys3: same as Sys1 + a corpus type classifier O as
described in Sec. 3.3.
Table 2 shows the resulting performance of our
systems and the best UMBC system published in the
final ranking. Sys2 appears the most accurate among
our systems, which ranked 21st out of 88. Compar-
ing to the best system across four datasets we ob-
serve that it performs reasonably well on the head-
lines dataset (it is 5th best), while completely fails
on the OnWN and FNWN test sets. After performing
57
error analysis, we found that TK models underper-
form on FNWN and OnWN sets, which appear un-
derrepresented in the training data from STS-2012.
We build a new system (Sys?2), which is based on
Sys2, by making two adjustments in the setup: (i)
we exclude SMTe from training to obtain predictions
on SMT and head and (ii) we remove all TK features
to train a model for FNWN and OnWN. This is mo-
tivated by the observation that text pairs from STS-
2012 yield a paraphrase model, since the texts are
syntactically very similar. Yet, two datasets from
STS-2013 FNWN, and OnWN contain text pairs
where documents exhibit completely different struc-
tures. This is misleading for our syntactic similarity
model learned on the STS-2012.
System head OnWN FNWN SMT Mean Rank
UMBC 0.7642 0.7529 0.5818 0.3804 0.6181 1
Sys2 0.7465 0.5572 0.3875 0.3409 0.5339 21
Sys1 0.7352 0.5432 0.3842 0.3180 0.5188 28
Sys3 0.7395 0.4228 0.3596 0.3294 0.4919 40
Sys?2 0.7538 0.6872 0.4478 0.3391 0.5732 4*
Table 2: Results on STS-2013.
5 Conclusions and Future Work
We have described our participation in STS-2013
task. Our approach treats text pairs as structural
objects which provides much richer representation
for the learning algorithm to extract useful patterns.
We experiment with structures derived from con-
stituency and dependency trees where related frag-
ments are linked with a special tag. Such struc-
tures are then used to learn tree kernel models which
can be efficiently combined with the a feature vector
representation in a single scoring model. Our ap-
proach ranks 1st with a large margin w.r.t. to the
best systems in STS-2012 task, while it is 21st ac-
cording to the final rankings of STS-2013. Never-
theless, a small change in the system setup makes
it rank 4th. Clearly, domain adaptation represents a
big challenge in STS-2013 task. We plan to address
this issue in our future work.
6 Acknowledgements
This research has been supported by the Euro-
pean Community?s Seventh Framework Program
(FP7/2007-2013) under the #288024 LIMOSINE
project.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-
Agirre. 2012. Semeval-2012 task 6: A pilot on se-
mantic textual similarity. In First Joint Conference on
Lexical and Computational Semantics (*SEM).
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Daniel Bar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the Sixth International Work-
shop on Semantic Evaluation (SemEval 2012).
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
EMNLP.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over Dis-
crete Structures, and the Voted Perceptron. In ACL.
Danilo Croce, Paolo Annesi, Valerio Storch, and Roberto
Basili. Unitor: Combining semantic text similarity
functions through sv regression. In SemEval 2012.
Andrew S. Fast and David Jensen. 2008. Why stacked
models perform effective collective classification. In
ICDM.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In IJCAI.
A. Moschitti. 2006. Efficient convolution kernels for
dependency and constituent syntactic trees. In ECML.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of an-
swer re-ranking. In SIGIR.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In ACL.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: vector space models of semantics.
J. Artif. Int. Res., 37(1):141?188.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic?. 2012. Takelab: Systems
for measuring semantic text similarity. In Proceedings
of the Sixth International Workshop on Semantic Eval-
uation (SemEval 2012).
58
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 75?83,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Learning Adaptable Patterns for Passage Reranking
Aliaksei Severyn(1) and Massimo Nicosia(1) and Alessandro Moschitti1,2
(1)DISI, University of Trento, 38123 Povo (TN), Italy
{severyn,m.nicosia,moschitti}@disi.unitn.it
(2)QCRI, Qatar Foundation, 5825 Doha, Qatar
amoschitti@qf.org.qa
Abstract
This paper proposes passage reranking
models that (i) do not require manual fea-
ture engineering and (ii) greatly preserve
accuracy, when changing application do-
main. Their main characteristic is the
use of relational semantic structures rep-
resenting questions and their answer pas-
sages. The relations are established us-
ing information from automatic classifiers,
i.e., question category (QC) and focus
classifiers (FC) and Named Entity Recog-
nizers (NER). This way (i) effective struc-
tural relational patterns can be automati-
cally learned with kernel machines; and
(ii) structures are more invariant w.r.t. dif-
ferent domains, thus fostering adaptability.
1 Introduction
A critical issue for implementing Question An-
swering (QA) systems is the need of designing
answer search and extraction modules specific to
the target application domain. These modules en-
code handcrafted rules based on syntactic patterns
that detect the relations between a question and its
candidate answers in text fragments. Such rules
are triggered when patterns in the question and the
passage are found. For example, given a ques-
tion1:
What is Mark Twain?s real name?
and a relevant passage, e.g., retrieved by a search
engine:
Samuel Langhorne Clemens, better
known as Mark Twain.
the QA engineers typically apply a syntactic parser
to obtain the parse trees of the above two sen-
tences, from which, they extract rules like:
1We use this question/answer pair from TREC QA as a
running example in the rest of the paper.
if the pattern ?What is NP2?s ADJ
name? is in the question and the pat-
tern ?NP1 better known as NP2?
is in the answer passage then associate
the passage with a high score2.
Machine learning has made easier the task of
QA engineering by enabling the automatic learn-
ing of answer extraction modules. However, new
features and training data have to be typically de-
veloped when porting a QA system from a domain
to another. This is even more critical considering
that effective features tend to be as much complex
and similar as traditional handcrafted rules.
To reduce the burden of manual feature engi-
neering for QA, we proposed structural models
based on kernel methods, (Moschitti et al, 2007;
Moschitti and Quarteroni, 2008; Moschitti, 2008)
with passages limited to one sentence. Their main
idea is to: (i) generate question and passage pairs,
where the text passages are retrieved by a search
engine; (ii) assuming those containing the correct
answer as positive instance pairs and all the oth-
ers as negative ones; (iii) represent such pairs with
two syntactic trees; and (ii) learn to rank answer
passages by means of structural kernels applied to
two trees. This enables the automatic engineering
of structural/lexical semantic patterns.
More recently, we showed that such models can
be learned for passages constituted by multiple
sentences on very large-scale (Severyn and Mos-
chitti, 2012). For this purpose, we designed a shal-
low syntactic representation of entire paragraphs
by also improving the pair representation using re-
lational tags.
In this paper, we firstly use our model in (Sev-
eryn and Moschitti, 2012) as the current baseline
and compare it with more advanced structures de-
rived from dependency trees.
2If the point-wise answer is needed rather than the entire
passage, the rule could end with: returns NP1
75
Search Engine
Kernel-based 
reranker
Reranked
answers
Candidate 
answers
Query
Evaluation
UIMA pipeline
NLP 
annotators
Focus and 
Question 
classifiers
syntactic/semantic 
graph
q/a similarity 
features
train/test 
data
Figure 1: Kernel-based Answer Passage Reranking system
Secondly, we enrich the semantic representa-
tion of QA pairs with the categorical informa-
tion provided by automatic classifiers, i.e., ques-
tion category (QC) and focus classifiers (FC) and
Named Entity Recognizers (NER). FC determines
the constituent of the question to be linked to the
named entities (NEs) of the answer passage. The
target NEs are selected based on their compatibil-
ity with the category of the question, e.g., an NE
of type PERSON is compatible with a category of
a question asking for a human (HUM).
Thirdly, we tested our models in a cross-domain
setting since we believe that: (i) the enriched rep-
resentation is supposed to increase the capability
of learning effective structural relational patterns
through kernel machines; and (ii) such structural
features are more invariant with respect to differ-
ent domains, fostering their adaptability.
Finally, the results show that our methods
greatly improve on IR baseline, e.g., BM25, by
40%, and on previous reranking models, up to
10%. In particular, differently from our previous
work such models can effectively use NERs and
the output of different automatic modules.
The rest of the paper is organized as follows,
Sec. 2 describes our kernel-based reranker, Sec. 3
illustrates our question/answer relational struc-
tures; Sec. 5 briefly describes the feature vectors,
and finally Sec. 6 reports the experimental results
on TREC and Answerbag data.
2 Learning to rank with kernels
2.1 QA system
Our QA system is based on a rather simple rerank-
ing framework as displayed in Figure 1: given a
query question a search engine retrieves a list of
candidate passages ranked by their relevancy. Var-
ious NLP components embedded in the pipeline as
UIMA3 annotators are then used to analyze each
question together with its candidate answers, e.g.,
part-of-speech tagging, chunking, named entity
recognition, constituency and dependency pars-
ing, etc. These annotations are then used to
produce structural models (described in Sec. 3),
which are further used by a question focus detector
and question type classifiers to establish relational
links for a given question/answer pair. The result-
ing tree pairs are then used to train a kernel-based
reranker, which outputs the model to refine the ini-
tial ordering of the retrieved answer passages.
2.2 Tree kernels
We use tree structures as our base representation
since they provide sufficient flexibility in repre-
sentation and allow for easier feature extraction
than, for example, graph structures. We rely on
the Partial Tree Kernel (PTK) (Moschitti, 2006) to
handle feature engineering over the structural rep-
resentations. The choice of PTK is motivated by
its ability to generate rich feature spaces over both
constituency and dependency parse trees. It gen-
eralizes a subset tree kernel (STK) (Collins and
Duffy, 2002) that maps a tree into the space of
all possible tree fragments constrained by the rule
that the sibling nodes from their parents cannot be
separated. Different from STK where the nodes
in the generated tree fragments are constrained to
include none or all of their direct children, PTK
fragments can contain any subset of the features,
i.e., PTK allows for breaking the production rules.
Consequently, PTK generalizes STK, thus gener-
ating an extremely rich feature space, which re-
sults in higher generalization ability.
2.3 Preference reranking with kernels
To enable the use of kernels for learning to
rank with SVMs, we use preference reranking
(Joachims, 2002), which reduces the task to bi-
nary classification. More specifically, the problem
of learning to pick the correct candidate hi from
a candidate set {h1, . . . , hk} is reduced to a bi-
nary classification problem by creating pairs: pos-
itive training instances ?h1, h2?, . . . , ?h1, hk? and
negative instances ?h2, h1?, . . . , ?hk, h1?. This set
can then be used to train a binary classifier. At
classification time the standard one-versus-all bi-
narization method is applied to form all possible
3http://uima.apache.org/
76
pairs of hypotheses. These are ranked according
to the number of classifier votes they receive: a
positive classification of ?hk, hi? gives a vote to
hk whereas a negative one votes for hi.
A vectorial representation of such pairs is the
difference between the vectors representing the
hypotheses in a pair. However, this assumes that
features are explicit and already available whereas
we aim at automatically generating implicit pat-
terns with kernel methods. Thus, for keeping im-
plicit the difference between such vectors we use
the following preference kernel:
PK(?h1, h2?, ?h?1, h?2?) = K(h1, h?1)+
K(h2, h?2)?K(h1, h?2)?K(h2, h?1),
(1)
where hi and h?i refer to two sets of hypothe-
ses associated with two rankings and K is a ker-
nel applied to pairs of hypotheses. We represent
the latter as pairs of question and answer passage
trees. More formally, given two hypotheses, hi =
?hi(q), hi(a)? and hi = ?h?i(q), h?i(a)?, whose
members are the question and answer passage
trees, we define K(hi, h?i) as TK(hi(q), h?i(q)) +
TK(hi(a), h?i(a)), where TK can be any tree ker-
nel function, e.g., STK or PTK.
To enable traditional feature vectors it is enough
to add the product (~xh1 ? ~xh2) ? (~xh?1 ? ~xh?2) tothe structural kernel PK , where ~xh is the feature
vector associated with the hypothesis h.
We opted for a simple kernel sum over a prod-
uct, since the latter rarely works in practice. Al-
though in (Moschitti, 2004) the kernel product has
been shown to provide some improvement when
applied to tree kernels over a subcategorization
frame structure, in general, it seems to work well
only when the tree structures are small and derived
rather accurately (Giordani and Moschitti, 2009;
Giordani and Moschitti, 2012).
3 Structural models of Q/A pairs
First, we briefly describe a shallow tree represen-
tation that we use as our baseline model and then
propose a new dependency-based representation.
3.1 Shallow tree structures
In a shallow syntactic representation first explored
for QA in (Severyn and Moschitti, 2012) each
question and its candidate answer are encoded into
a tree where part-of-speech tags are found at the
pre-terminal level and word lemmas at the leaf
level. To encode structural relationships for a
given q/a pair a special REL tag is used to link
the related structures. The authors adopt a sim-
ple strategy to establish such links: lemmas shared
between a question and and answer get their par-
ents (POS tags) and grandparents (chunk labels)
marked by a REL tag.
3.2 Dependency-based structures
Given the ability of PTK to generate a rich set
of structural features from a relatively flat shal-
low tree representation, we propose to use depen-
dency relations between words to derive an al-
ternative structural model. In particular, we use
a variation of the dependency tree, where depen-
dency relations are altered in such a way that the
words are always at the leaf level. This reorder-
ing of the nodes in the dependency tree, s.t. words
do not form long chains, which is typical in the
standard dependency tree representation, is essen-
tial for PTK to extract meaningful fragments. We
also add part-of-speech tags between the words
and the nodes encoding their grammatical roles
(provided by the original dependency parse tree).
Again a special REL tag is used in the same man-
ner as in the shallow representation to establish
structural links between a question and an answer.
Fig. 2 (top) gives an example of a dependency-
based structure for our example q/a pair.
4 Relational Linking
The use of a special tag to mark the related frag-
ments in the question and answer tree represen-
tations has been shown to yield more accurate
relational models (Severyn and Moschitti, 2012).
However, previous approach was based on a na??ve
hard matching between word lemmas.
Below we propose a novel strategy to estab-
lish relational links using named entities extracted
from the answer along with question focus and
category classifiers. In particular, we use a ques-
tion category to link the focus word of a question
with the named entities extracted from the candi-
date answer. For this purpose, we first introduce
our tree kernel-based models for building a ques-
tion focus and category classifiers.
4.1 Question focus detection
The question focus is typically a simple noun rep-
resenting the entity or property being sought by
the question (Prager, 2006). It can be used to
search for semantically compatible candidate an-
77
NER: Person NER: Personfocus
Figure 2: Dependency-based structure DEP (top) for the q/a pair. Q: What is Mark Twain?s real name? A: Samuel Langhorne
Clemens, better known as Mark Twain. Arrows indicate the tree fragments in the question and its answer passage linked by the
relational REL tag. Shallow tree structure CH (bottom) with a typed relation tag REL-FOCUS-HUM to link a question focus
word name with the named entities of type Person corresponding to the question category (HUM).
swers in document passages, thus greatly reduc-
ing the search space (Pinchak, 2006). While sev-
eral machine learning approaches based on man-
ual features and syntactic structures have been
recently explored, e.g. (Quarteroni et al, 2012;
Damljanovic et al, 2010; Bunescu and Huang,
2010), we opt for the latter approach where tree
kernels handle automatic feature engineering.
In particular, to detect the question focus word
we train a binary SVM classifier with tree ker-
nels applied to the constituency tree representa-
tion. For each given question we generate a set
of candidate trees where the parent (node with the
POS tag) of each candidate focus word is anno-
tated with a special FOCUS tag. Trees with the
correctly tagged focus word constitute a positive
example, while the others are negative examples.
To detect the focus for an unseen question we clas-
sify the trees obtained after tagging each candidate
focus word. The tree yielding the highest classifi-
cation score reveals the target focus word.
4.2 Question classification
Question classification is the task of assigning a
question to one of the pre-specified categories. We
use the coarse-grain classes described in (Li and
Roth, 2002): six non-overlapping classes: Abbre-
viations (ABBR), Descriptions (DESC, e.g. def-
initions or explanations), Entity (ENTY, e.g. an-
imal, body or color), Human (HUM, e.g. group
or individual), Location (LOC, e.g. cities or coun-
tries) and Numeric (NUM, e.g. amounts or dates).
These categories can be used to determine the Ex-
pected Answer Type for a given question and find
the appropriate entities found in the candidate an-
swers. Imposing such constraints on the potential
answer keys greatly reduces the search space.
Previous work in Question Classification re-
veals the power of syntactic/semantic tree repre-
sentations coupled with tree kernels to train the
state-of-the-art models (Bloehdorn and Moschitti,
2007). Hence, we opt for an SVM multi-classifier
using tree kernels to automatically extract the
question class. To build a multi-class classifier
we train a binary SVM for each of the classes and
apply a one-vs-all strategy to obtain the predicted
class. We use constituency trees as our input rep-
resentation.
4.3 Linking focus word with named entities
using question class
Question focus captures the target information
need posed by a question, but to make this piece
of information effective, the focus word needs to
be linked to the target candidate answer. The focus
word can be lexically matched with words present
in an answer, or the match can be established us-
ing semantic information. Clearly, the latter ap-
proach is more appealing since it helps to allevi-
ate the lexical gap problem which makes the na?ive
string matching of words between a question and
its answer less reliable.
Hence, we propose to exploit a question cate-
gory (automatically identified by a question type
classifier) along with named entities found in the
answer to establish relational links between the
tree structures of a given q/a pair. In particu-
lar, once the question focus and question category
78
Table 1: Question classes ? named entity types.
Question Category Named Entity types
HUM Person
LOC Location
NUM Date, Time, Money, Percentage
ENTY Organization, Person
are determined, we link the focus word wfocus in
the question, with all the named entities whose
type matches the question class. Table 1 provides
the correspondence between question classes and
named entity types. We perform tagging at the
chunk level and use two types of relational tags:
plain REL-FOCUS and a tag typed with a ques-
tion class, e.g., REL-FOCUS-HUM. Fig. 2 (bot-
tom) shows an example q/a pair where the typed
relational tag is used in the shallow syntactic tree
representation to link the chunk containing the
question focus name with the named entities of the
corresponding type Person (according to the map-
ping defined in Table 1), i.e. samuel langhorne
clemens and mark twain.
5 Feature vector representation
While the primary focus of our study is on the
structural representations and relations between
q/a pairs we also include basic features widely
used in QA:
Term-overlap features. A cosine similarity be-
tween a question and an answer: simCOS(q, a),
where the input vectors are composed of: (i) n-
grams (up to tri-grams) of word lemmas and part-
of-speech tags, and (ii) dependency triplets. For
the latter, we simply hash the string value of the
predicate defining the triple together with its argu-
ment, e.g. poss(name, twain).
PTK score. For the structural representations we
also define a similarity based on the PTK score:
simPTK(q, a) = PTK(q, a), where the input
trees can be both dependency trees and shallow
chunk trees. Note that this similarity is computed
between the members of a q/a pair, thus, it is very
different from the one defined in Eq. 1.
NER relatedness represents a match between a
question category and the related named entity
types extracted from the candidate answer. It
counts the proportion of named entities in the an-
swer that correspond to the question type returned
by the question classifier.
In our study feature vectors serve a complemen-
tary purpose, while the main focus is to study the
virtue of structural representations for reranking.
The effect of a more extensive number of pairwise
similarity features in QA has been studied else-
where, e.g., (Surdeanu et al, 2008).
6 Experiments
We report the results on two QA collections: fac-
toid open-domain QA corpus from TREC and a
community QA corpus Answerbag. Since we fo-
cus on passage reranking we do not carry out an-
swer extraction. The goal is to rank the passage
containing the right answer in the top position.
6.1 Corpora
TREC QA. In the TREC QA tasks, answer pas-
sages containing correct information nuggets, i.e.
answer keys, have to be extracted from a given text
corpus, typically a large corpus from newswire.
In our experiments, we opted for questions from
2002 and 2003 years, which totals to 824 ques-
tions. AQUAINT newswire corpus4 is used for
searching the supporting answers.
Answerbag is a community-driven QA collection
that contains a large portion of questions that have
?professionally researched? answers. Such an-
swers are provided by the website moderators and
allow for training high quality models. From the
original corpus containing 180k question/answer
pairs, we use 1k randomly sampled questions for
testing and 10k for training.
Question Focus. We use 3 datasets for train-
ing and evaluating the performance of our fo-
cus detector: SeCo-600 (Quarteroni et al, 2012),
Mooney GeoQuery (Damljanovic et al, 2010) and
the dataset from (Bunescu and Huang, 2010). The
SeCo dataset contains 600 questions from which
we discarded a subset of multi-focus questions
and non-interrogative queries. The Mooney Geo-
Query contains 250 question targeted at geograph-
ical information in the U.S. The first two datasets
are very domain specific, so we also carried out
experiments with the dataset from (Bunescu and
Huang, 2010), which contains the first 2000 ques-
tions from the answer type dataset from Li and
Roth annotated with focus words. We removed
questions with implicit and multiple focuses.
Question Classification. We used the UIUIC
dataset (Li and Roth, 2002)5 which contains 5952
4http://www.ldc.upenn.edu/Catalog/docs/LDC2002T31/
5although the QC dataset from (Li and Roth, 2002) in-
cludes additional 50 fine grain classes we opted for using only
6 coarse classes that are sufficient to capture the coarse se-
mantic answer type of the candidate answer. This choice also
results in a more accurate multi-class classifier.
79
factoid questions from different sources (USC,
TREC 8, TREC 9, TREC 10). For training the
classifiers we excluded questions from TREC 8 to
ensure there is no overlap with the data used for
testing models trained on TREC QA.
6.2 Models and Metrics
Our models are built applying a kernel-based
reranker to the output of a search engine.
6.2.1 BM25
We use Terrier6 search engine, which provides
BM25 scoring model for indexing and retrieval.
For the TREC QA 2002 and 2003 task we index
AQUAINT corpus treating paragraphs as docu-
ments. The resulting index contains about 12 mil-
lion items. For the Answerbag we index the entire
collection of 180k answers. We retrieve a list of
top 50 candidate answers for each question.
6.2.2 Reranking models
To train our reranking models we used SVM-light-
TK7, which encodes structural kernels in SVM-
light (Joachims, 2002) solver. In particular, we
use PTK on the relational tree structures combined
with the polynomial kernel of degree 3 applied to
the feature vectors. Therefore, different represen-
tations lead to different models described below.
CH - our basic shallow chunk tree (Severyn and
Moschitti, 2012) used as a baseline structural
reranking model.
DEP - dependency tree augmented with POS tags
and reorganized relations suitable for PTK.
V - reranker model using similarity features de-
fined in Sec. 5.
DEP+V, CH+V - a combination of tree structures
and similarity feature vectors.
+FC+QC - relational linking of the question focus
word and named entities of the corresponding type
using Focus and Question classifiers.
+TFC+QC - a typed relational link refined a ques-
tion category.8
6.2.3 Metrics
We report the following metrics, most commonly
used in QA: Precision at rank 1 (P@1), i.e.,
6http://terrier.org/
7http://disi.unitn.it/moschitti/Tree-Kernel.htm
8? is used for showing the results of DEP, DEP+V and
CH+V structural representations that are significantly better
than the baseline model CH, while ? indicates improvement
of +QC+FC and +QC+TFC tagging applied to basic struc-
tural representations, e.g. CH+V and DEP+V.
Table 2: Structural representations on TREC QA.
MODELS MAP MRR P@1
BM25 0.22 28.02 18.17
V 0.22 28.40 18.54
STRUCTURAL REPRESENTATIONS
CH (S&M, 2012) 0.28 35.63 24.88
CH+V 0.30? 37.45? 27.91?
DEP 0.30? 37.87? 28.05?
DEP+V 0.30? 37.64? 28.05?
REFINED RELATIONAL TAG
CH+V+QC+FC 0.32? 39.48? 29.63?
CH+V+QC+TFC 0.32? 39.49? 30.00?
DEP+V+QC+FC 0.31? 37.49 28.56
DEP+V+QC+TFC 0.31? 38.05? 28.93?
the percentage of questions with a correct an-
swer at rank 1, Mean Reciprocal Rank (MRR),
and Mean Average Precision (MAP). The reported
metrics are averages after performing a 5-fold
cross-validation. We used a paired t-test at 95%
confidence to compare the performance of our
models to a baseline.
6.3 Passage Reranking Results
We first evaluate the impact of two different syn-
tactic representations using shallow and depen-
dency trees. Then, we evaluate the accuracy boost
when such structures are enriched with automati-
cally derived tags, e.g., question focus and ques-
tion category and NEs found in the answer pas-
sage.
6.3.1 Structural representations
Table 2 reveals that using V model results in a
small improvement over BM25 baseline. Indeed,
similarity scores that are most often based on
word-overlap measures even when computed over
various q/a representations are fairly redundant to
the search engine similarity score. Instead, using
the structural representations, CH and DEP, gives
a bigger boost in the performance. Interestingly,
having more features in the CH+V model results
in further improvement while DEP+V seems to re-
main insensitive to additional features provided by
the V model.
6.3.2 Semantically Enriched Structures
In the following set of experiments we explore an-
other strategy for linking structures for a given
q/a pair. We automatically detect the question
focus word and link it to the related named en-
tities in the answer, selected accordingly to the
question category identified by the question clas-
sifier (QC+FC). Further refining the relational link
80
Table 3: Accuracy (%) of focus classifiers.
DATASET ST STK STK+BOW PTK
MOONEY 73.0 81.9 81.5 80.5
SECO-600 90.0 94.5 94.5 90.0
BUNESCU 89.7 98.3 98.2 96.9
Table 4: Accuracy (%) of question classifiers.
DATASET STK+BOW PTK
LI & ROTH 86.1 82.2
TREC TEST 79.3 78.1
with the question category yields QC+TFC model.
First, we report the results of training our question
focus detector and question category classifier.
Focus classifier results. Table 3 displays the ac-
curacies obtained by the question focus detector
on 3 datasets using different kernels: the ST (sub-
tree kernel where fragments contain full subtrees
including leaves), STK, STK+bow (bag-of-words
feature vector is added) and PTK. As we can see,
using STK model yields the best accuracy and we
use it in our pipeline to automatically detect the
focus.
Question classifier results. Table 4 contains the
accuracies of the question classifier on the UIUIC
dataset and the TREC questions that we also use
for testing our reranker models. STK+bow per-
forms better than PTK, since here the input rep-
resentation is a plain constituency tree, for which
STK is particularly suited. Hence, we use this
model to predict the question category.
Ranking results. Table 2 (bottom) summarizes
the performance of the CH+V and DEP+V models
when coupled with QC+FC and QC+TFC strate-
gies to establish the links between the structures
in a given q/a pair. CH structural representation
with QC+FC yields an interesting improvement,
while further refining the relational tag by adding
a question category (QC+TFC) gives slightly bet-
ter results.
Integrating the refined relational tag into the
DEP based structures results more problematic,
since the dependency tree is less suitable for repre-
senting multi-word expressions, named entities in
our case. Hence, using the relational tag to mark
the nodes spanning such multi-word entities in the
dependency structure may result in less meaning-
ful features than in CH model, where words in a
phrase are naturally grouped under a chunk node.
A more detailed discussion on the merits of each
model is provided in the Sec. 6.5.
Table 5: Cross-domain experiment: training on Answerbag
and testing on TREC QA.
MODELS MAP MRR P@1
BM25 0.22 27.91 18.08
V 0.23 28.86 18.90
BASIC STRUCTURAL REPRESENTATIONS
CH (S&M, 2012) 0.24 30.25 20.42
CH+V 0.25? 31.31? 21.28?
DEP+V 0.26? 33.26? 22.21?
REFINED RELATIONAL TAG
CH+V+QC+TFC 0.27? 33.53? 22.81?
DEP+V+QC+TFC 0.29? 34.25? 23.45?
6.4 Learning cross-domain pairwise
structural relationships
To test the robustness of the syntactic patterns au-
tomatically learned by our structural models, we
conduct a cross-domain experiment, i.e. we train
a model on Answerbag data and test it on TREC. It
should be noted that unlike TREC data, where the
answers are simply passages containing the cor-
rect answer phrase, answers in Answerbag specif-
ically address a given question and are generated
by humans. Additionally, TREC QA contains only
factoid questions, while Answerbag is a commu-
nity QA corpus with a large portion of non-factoid
questions. Interestingly, the results demonstrate
the robustness of our syntactic relational model
which captures patterns shared across different do-
mains, e.g. TREC and Answerbag data.
Table 5 shows that: (i) models based on depen-
dency structures result in a better generalization
ability extracting more robust syntactic patterns;
and (ii) the strategy to link the question focus with
the related named entities in the answer provides
an interesting improvement over the basic struc-
tural representations.
6.5 Error Analysis
Consider our running example q/a pair from
Sec. 1. As the first candidate answer, the
search engine retrieves the following incorrect
passage: ?The autobiography of Mark Twain?,
Mark Twain. It is relatively short and mentions the
keywords {Mark, Twain} twice, which apparently
results in a high score for the BM25 model. In-
stead, the search engine ranks the correct answer at
position 34. After reranking using the basic CH+V
model the correct answer is promoted by 20 posi-
tions. While using the CH+V+QC+FC model the
correct answer advances to position 6. Below, we
provide the intuition behind the merits of QC+FC
and QC+TFC encoding question focus and ques-
81
tion category into the basic models.
The model learned by the reranker represents a
collection of q/a pairs from the training set (sup-
port vectors) which are matched against each can-
didate q/a pair. We isolated the following pair
from the model that has a high structural similarity
with our running example:
Q: What is Barbie?s full name?
A: The toy is called after Barbie Millicent
Roberts from Willows.
Despite differences in the surface forms of
the words, PTK extracts matching patterns,
e.g. [S NP [VP VBN] [PP IN] REL-NP],
which yields a high similarity score boosting the
rank of the correct candidate. However, we
note that at the same time an incorrect candi-
date answer, e.g. Mark Twain was accused of
racist language., exhibits similar patterns and also
gets a high rank. The basic structural repre-
sentation is not able to encode essential differ-
ences from the correct answer candidate. This
poses a certain limitation on the discriminative
power of CH and DEP representations. Intro-
ducing a focus tag changes the structural repre-
sentation of both q/a pairs, s.t. the correct q/a
pair preserves the pattern (after identifying word
name as focus and question category as HUM,
it is transformed to [S REL-FOCUS-NP [VP
VBN] [PP IN] REL-FOCUS-NP]), while it
is absent in the incorrect candidate. Thus, linking
the focus word with the related NEs in the answer
helps to discriminate between structurally similar
yet semantically different candidates.
Another step towards a more fine-grained struc-
tural representation is to specialize the relational
focus tag (QC+TFC model). We propose to aug-
ment the focus tag with the question category to
avoid matches with other structurally similar but
semantically different candidates. For example, a
q/a pair found in the list of support vectors:
Q: What is Mark Twain?s place of birth?
A: Mark Twain was raised in Hannibal Missouri.
would exhibit high structural similarity even when
relational focus is used (since the relational tag
does not incorporate the question class LOC), but
refining the focus tag with the question class elim-
inates such cases.
7 Related Work
Previous studies similar to ours carry out pas-
sage reranking by exploiting structural informa-
tion, e.g. using subject-verb-object relations (At-
tardi et al, 2001; Katz and Lin, 2003). Un-
fortunately, the large variability of natural lan-
guage makes such triples rather sparse thus dif-
ferent methods explore soft matching (i.e., lexical
similarity) based on answer types and named en-
tity types (Aktolga et al, 2011). Passage reranking
using classifiers of question and answer pairs were
proposed in (Radlinski and Joachims, 2006; Jeon
et al, 2005).
Regarding kernel methods, our work in (Mos-
chitti et al, 2007; Severyn and Moschitti, 2012)
was the first to exploit tree kernels for modeling
answer reranking. However, such method lacks
the use of important relational information be-
tween a question and a candidate answer, which
is essential to learn accurate relational patterns. In
contrast, this paper relies on shallow and depen-
dency trees encoding the output of question and
focus classifiers to connect focus word and NEs of
the answer passage. This provides more effective
relational information, which allows our model to
significantly improve on previous rerankers.
8 Conclusions
This paper shows a viable research direction in
the automatic QA engineering. One of its main
characteristics is the use of structural kernel tech-
nology to induce features from structural seman-
tic representations of question and answer pas-
sage pairs. The same technology is also used to
construct question and focus classifiers, which are
used to derive relational structures.
An interesting result of this paper is that to de-
sign an answer passage reranker for a new do-
main, we can use off-the-shelf syntactic parsers
and NERs along with little training data for the
QC and FC classifiers. This is due to the fact
that: (i) the kernel technology is able to automat-
ically extract effective structural patterns; and (ii)
the extracted patterns are rather robust, e.g., mod-
els learned on Answerbag improve accuracy on
TREC test data.
Acknowledgements
This research is partially supported by the EU?s 7th
Framework Program (FP7/2007-2013) (#288024
LIMOSINE project) and an Open Collaborative
Research (OCR) award from IBM Research.
82
References
Elif Aktolga, James Allan, and David A. Smith. 2011.
Passage reranking for question answering using syn-
tactic structures and answer types. In ECIR.
Giuseppe Attardi, Antonio Cisternino, Francesco
Formica, Maria Simi, and Ro Tommasi. 2001.
Piqasso: Pisa question answering system. In TREC,
pages 599?607.
Stephan Bloehdorn and Alessandro Moschitti. 2007.
Combined syntactic and semantic kernels for text
classification. In ECIR.
Razvan Bunescu and Yunfeng Huang. 2010. Towards
a general model of answer typing: Question focus
identification. In CICLing.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In
ACL.
Danica Damljanovic, Milan Agatonovic, and Hamish
Cunningham. 2010. Identification of the question
focus: Combining syntactic analysis and ontology-
based lookup through the user interaction. In LREC.
Alessandra Giordani and Alessandro Moschitti. 2009.
Syntactic structural kernels for natural language in-
terfaces to databases. In Proceedings of ECML
PKDD, ECML PKDD ?09. Springer-Verlag.
Alessandra Giordani and Alessandro Moschitti. 2012.
Translating questions to sql queries with generative
parsers discriminatively reranked. In Proceedings
of The 24rd International Conference on Computa-
tional Linguistics, India. Coling 2012.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In CIKM.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD),
pages 133?142.
Boris Katz and Jimmy Lin. 2003. Selectively using re-
lations to improve precision in question answering.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In COLING.
Alessandro Moschitti and Silvia Quarteroni. 2008.
Kernels on linguistic structures for answer extrac-
tion. In ACL.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploit-
ing syntactic and shallow semantic kernels for ques-
tion/answer classification. In ACL.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow statistic parsing. In Proceedings
of the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL?04), Main Volume, pages
335?342, Barcelona, Spain, July.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In ECML.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM.
Christopher Pinchak. 2006. A probabilistic answer
type model. In In EACL.
John M. Prager. 2006. Open-domain question-
answering. Foundations and Trends in Information
Retrieval, 1(2):91?231.
Silvia Quarteroni, Vincenzo Guerrisi, and Pietro La
Torre. 2012. Evaluating multi-focus natural lan-
guage queries over data services. In LREC.
Filip Radlinski and Thorsten Joachims. 2006. Query
chains: Learning to rank from implicit feedback.
CoRR.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In SIGIR.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008.
Learning to rank answers on large online QA collec-
tions. In Proceedings of ACL-HLT.
83
